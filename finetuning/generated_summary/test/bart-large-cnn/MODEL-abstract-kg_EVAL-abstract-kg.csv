,paper_id,input,output_bestRouge1,output_bestRougeAvg
0,SP:b19df5243359791fbaad005d6f13d7e9fdb0ff63,"Role - based learning USED-FOR scalable multi - agent learning. Role - based learning USED-FOR decomposing complex tasks. decomposing complex tasks USED-FOR scalable multi - agent learning. roles USED-FOR decomposing complex tasks. role selector USED-FOR role discovery. role selector USED-FOR role space. role space CONJUNCTION temporal resolution. temporal resolution CONJUNCTION role space. it USED-FOR bi - level learning hierarchy. primitive action - observation spaces FEATURE-OF role policies. action effects USED-FOR role selector. learning efficiency CONJUNCTION policy generalization. policy generalization CONJUNCTION learning efficiency. method COMPARE MARL algorithms. MARL algorithms COMPARE method. OtherScientificTerm are joint action spaces, and restricted role action spaces. Material is StarCraft II micromanagement benchmark. ","-based multi-agent reinforcement learning (MARL) is a popular approach for multi-task reinforcement learning. In this paper, the authors propose a role-based MARL algorithm that learns a role selector that selects a role policy that maximizes the impact of each action in the role space. The proposed method is evaluated on the StarCraft II micromanagement benchmark.   ","This paper proposes a new approach to multi-agent role-based learning for StarCraft II micromanagement. The proposed approach is based on a bi-level learning hierarchy, where each agent selects a role from a set of agents, and each agent learns a role policy for each of the agents in the multi-task setting. The role selector selects the agent that is best suited for the task at hand, and the agent is trained to find the best role for the agent in the task. The agent is then trained to perform the task in a way that maximizes the performance of the agent with the best performance in the role space. The authors show that the proposed approach outperforms the state-of-the-art MARL algorithm in terms of learning efficiency and policy generalization."
9,SP:7deb61890d97422a0fe141ca807f968c70ab239a,"stochastic subgradient descent ( SSGD ) method USED-FOR over - parameterized nonsmooth optimization problems. interpolation condition FEATURE-OF over - parameterized nonsmooth optimization problems. composite structure USED-FOR SSGD. composite structure USED-FOR empirical risk minimization problems. stochastic gradient descent ( SGD ) method USED-FOR smooth problems. rates COMPARE rates. rates COMPARE rates. rates USED-FOR stochastic gradient descent ( SGD ) method. SGD USED-FOR smooth and nonsmooth machine learning models. SSGD USED-FOR smooth and nonsmooth machine learning models. SGD CONJUNCTION SSGD. SSGD CONJUNCTION SGD. subgradient method USED-FOR convex and interpolation setting. OtherScientificTerm are convex and strongly - convex objectives, and interpolation. ",This paper studies the convergence of stochastic subgradient descent (SSGD) in over-parameterized nonsmooth optimization problems with an interpolation condition. The authors show that the convergence rate of SSGD in the convex and strongly-convex setting is faster than that of SGD in the non-interpolation setting. The main contribution of the paper is a theoretical analysis of the convergence rates of the SSGD method under the interpolation and convexity conditions.   ,This paper proposes a new stochastic subgradient descent (SSGD) method for over-parameterized nonsmooth optimization problems. The main contribution of the paper is a theoretical analysis of the convergence rate of SSGD in the convex and strongly-convex setting. The authors show that SSGD converges to a convergence rate that is close to the rate of SGD in the nonsmoothed setting. They also show that the rate is also close to SGD for the smooth setting. 
18,SP:c7e0b3fedc0d0409d662dd612b529fdacad2b03e,"non - linear “ reservoir ” layers CONJUNCTION regular transformer layers. regular transformer layers CONJUNCTION non - linear “ reservoir ” layers. Method are transformers, and machine learning. OtherScientificTerm is layers. Metric is wall - clock compute time. Task is machine translation. ",This paper studies the use of transformers in machine translation tasks. The authors propose to use non-linear “reservoir” and “regular transformer” layers to reduce the computational cost of translation. Theoretical analysis is provided to show that reservoir layers are more efficient than regular transformer layers. Experiments are conducted to show the effectiveness of the proposed method. ,"This paper studies the problem of machine translation in the context of transformer-based machine translation. The main contribution of the paper is a theoretical analysis of the time complexity of translation in terms of the number of transformer layers. The authors show that the total number of layers in a transformer can be as large as the total amount of time it takes to translate a translation. They show that this is true for both non-linear transformer layers and regular transformer layers, and that it is also true for the reservoir layers. They also show that for non-Linear transformer layers it is possible to compute the total translation time in the wall-clock fashion. "
27,SP:ba9f1d4738ec67a440346f3ac6c4cf35f7232077,"transformation invariance CONJUNCTION equivariance. equivariance CONJUNCTION transformation invariance. transformation invariance FEATURE-OF network architecture. equivariance FEATURE-OF network architecture. geometry transformation of data FEATURE-OF network robustness. Filter transform USED-FOR steerable CNN. group representation theory USED-FOR steerable CNN. group representation theory USED-FOR function space structure. group representation theory USED-FOR steerable kernel function. function space structure FEATURE-OF steerable kernel function. theory COMPARE filter transform technique. filter transform technique COMPARE theory. group representation theory USED-FOR kernel. filter transform USED-FOR kernel. filter transformed kernels USED-FOR group representation. approach USED-FOR steerable convolution operators. Method are Steerable CNN, and steerable CNN theory. OtherScientificTerm is overfitting. ",This paper studies steerable convolutional neural networks with filter transform. The authors propose to use group representation theory to study the steerable kernel function in steerable CNNs. Theoretical results show that steerable kernels are invariant to transformation invariance and equivariant to equivariance. They also show that filter transformed kernels can be used to represent the group representation of the kernel. ,"This paper studies the problem of steerable CNNs. The authors propose a new group representation theory for steerable kernels, which is based on the theory of group representation. The main contribution of the paper is a theoretical analysis of the group representation of the steerable kernel function. Theoretical analysis is provided for the first time in the literature, and it is shown that the kernel is invariant to transformation invariance and equivariant to equivariance. The paper also provides an empirical analysis for the robustness of the kernel to overfitting."
36,SP:c1116fbb4d058eb6be195b5d13d19a55ba86b602,"Multimodal program synthesis USED-FOR program. user input USED-FOR program. Multimodal program synthesis USED-FOR program synthesis. user input USED-FOR Multimodal program synthesis. noisy signals USED-FOR it. natural language HYPONYM-OF noisy signals. neural model USED-FOR program ’s score. natural language ( NL ) CONJUNCTION input - output examples. input - output examples CONJUNCTION natural language ( NL ). user intent FEATURE-OF multimodal synthesis tasks. input - output examples USED-FOR multimodal synthesis tasks. input - output examples USED-FOR user intent. top - down recurrent neural model PART-OF method. automated program analysis techniques USED-FOR search space. user ’s constraints FEATURE-OF infeasibility of partial programs. automated program analysis techniques USED-FOR it. multimodal synthesis dataset ( STRUCTUREDREGEX ) EVALUATE-FOR method. method COMPARE techniques. techniques COMPARE method. multimodal synthesis dataset ( STRUCTUREDREGEX ) EVALUATE-FOR techniques. accuracy EVALUATE-FOR techniques. accuracy EVALUATE-FOR method. Method is optimal neural synthesis approach. OtherScientificTerm are user - provided constraints, abstract syntax trees, NL input, and syntactically valid programs. Generic is model. ","This paper proposes a method for multimodal program synthesis based on natural language (NL) and input-output examples. The main idea is to use a top-down recurrent neural model to predict the program’s score, which is then used to generate a set of partial programs that satisfy user constraints. The method is evaluated on the STRUCTUREDREGEX program synthesis dataset. ",This paper proposes a novel method for multi-modal program synthesis. The proposed method is based on a top-down recurrent neural model that learns the score of a program’s score from the input-output examples. The model is trained using an automated program analysis technique. The method is evaluated on the STRUCTUREDREGEX dataset and shows that the proposed method outperforms existing methods.
45,SP:55e02d79146bbb42f1ab6d4fafa2db5ddbe599b0,"protease enzymes HYPONYM-OF proteins. substrate specificity landscape FEATURE-OF protease enzyme. sequence motifs PART-OF substrate specificity landscape. methods USED-FOR predicting protease specificity landscapes. sequence patterns USED-FOR methods. protein graph convolutional neural network ( PGCN ) USED-FOR substrate specificity. Rosetta energy function USED-FOR topology and energetic features. structure - based molecular interaction graph USED-FOR substrate specificity. Rosetta energy function USED-FOR structure - based molecular interaction graph. structure - based molecular interaction graph USED-FOR protein graph convolutional neural network ( PGCN ). PGCN USED-FOR specificity. specificity FEATURE-OF NS3/4 protease. Hepatitic C virus FEATURE-OF NS3/4 protease. PGCN COMPARE machine learning models. machine learning models COMPARE PGCN. classification tasks EVALUATE-FOR PGCN. feature importance USED-FOR sub - graph patterns. sub - graph patterns USED-FOR molecular recognition. physical interactions USED-FOR PGCN. PGCN model USED-FOR enzymes. Task is robustness of key life processes. OtherScientificTerm are protease specificity landscapes, mutational changes, and molecular interactions. ",This paper proposes a protein graph convolutional neural network (PGCN) to predict the substrate specificity landscape of a given enzyme. The proposed method is based on a structure-based molecular interaction graph (SIG) and a Rosetta energy function to model the topology and energetic features of the protein. The method is evaluated on a variety of classification tasks and is shown to outperform existing methods.  ,This paper proposes a protein graph convolutional neural network (PGCN) for predicting protease specificity landscapes. The proposed method is based on a structure-based molecular interaction graph and a Rosetta energy function to predict the topology and energetic features of the protease. The method is evaluated on a variety of classification tasks and compared with state-of-the-art methods. 
54,SP:7727eeb7b17ad94ddfa0cf24e64a9626d83a8876,"method USED-FOR overestimation bias. Double Q - learning USED-FOR overestimation bias. Double Q - learning HYPONYM-OF method. approximate Bellman operator USED-FOR non - optimal fixed points. underestimation bias PART-OF double Q - learning. approximate dynamic programming USED-FOR approach. method COMPARE baseline algorithms. baseline algorithms COMPARE method. Atari benchmark tasks EVALUATE-FOR baseline algorithms. Atari benchmark tasks EVALUATE-FOR method. Method are Bellman operation, and deep Q - learning paradigm. Task are value prediction, and learning. OtherScientificTerm is non - optimal stationary solutions. ","This paper studies the overestimation and underestimation bias in deep Q-learning, where the goal is to estimate the true value of the Q function. The authors propose a method called Double Q-Learning, which is based on the Bellman operator. The main idea is to use the approximate Bellman operation to find non-optimal fixed points in the Q-function. The proposed method is evaluated on a variety of Atari tasks, and it is shown to outperform the baselines. ",This paper proposes a novel approach to improve the overestimation bias in double Q-learning. The main idea is to use the approximate Bellman operator to find the non-optimal fixed points in the value prediction problem. The proposed approach is based on approximate dynamic programming. The authors show that the proposed approach outperforms the state-of-the-art on Atari benchmark tasks.
63,SP:1d630b69f95392a5ef3d7d580b523e077a3555a8,"models USED-FOR high - resolution image generation. BigGAN CONJUNCTION VQVAE-2. VQVAE-2 CONJUNCTION BigGAN. compute resources USED-FOR models. VQVAE-2 HYPONYM-OF models. BigGAN HYPONYM-OF models. ESRGAN HYPONYM-OF GAN - based image super - resolution models. two - step training framework USED-FOR deep generative models ( DGMs ). high - dimensional natural images USED-FOR deep generative models ( DGMs ). wavelet domain USED-FOR sampler. wavelet super - resolution decoder network USED-FOR images. Wavelet - based down - sampling method COMPARE pixel - based methods. pixel - based methods COMPARE Wavelet - based down - sampling method. generative quality EVALUATE-FOR low - resolution sampler. Wavelet - based down - sampling method USED-FOR structural information. generative quality EVALUATE-FOR Wavelet - based down - sampling method. generative quality EVALUATE-FOR pixel - based methods. sampler CONJUNCTION decoder. decoder CONJUNCTION sampler. ImageNet EVALUATE-FOR model. model COMPARE BigGAN model. BigGAN model COMPARE model. Fréchet Inception Distance ( FID ) EVALUATE-FOR BigGAN model. Fréchet Inception Distance ( FID ) EVALUATE-FOR model. OtherScientificTerm are low - frequency bands, pixel - space, and dimensional spaces. Method is end - to - end models. Metric is training cost. ","This paper proposes a two-step training framework for deep generative models (DGMs), where the high-dimensional natural images are used to train a wavelet domain sampler and a super-resolution decoder network is used to generate images in the low-resolution domain. The proposed method is evaluated on ImageNet and achieves state-of-the-art results. ","This paper proposes a two-step training framework for deep generative models (DGMs) to generate high-resolution natural images. First, a wavelet-based down-sampling method is proposed to down-sample the high-dimensional natural images from the wavelet domain. Second, a super-resolution decoder network is used to generate the high resolution decoders from the low-dimensional domain. Experiments on ImageNet show that the proposed method outperforms the state-of-the-art pixel-based methods. "
72,SP:b943a73b1ec34867371325748dc3a91ff4011947,"self - supervised learning ( SSL ) algorithms USED-FOR Fewshot learning(FSL ). pre - trained embedding network USED-FOR downstream FSL tasks. self - supervised training USED-FOR pre - trained embedding network. SSL USED-FOR FSL. self - supervised training USED-FOR FSL. supervised training USED-FOR FSL. self - supervised loss CONJUNCTION supervised loss. supervised loss CONJUNCTION self - supervised loss. supervised training CONJUNCTION self - supervised training. self - supervised training CONJUNCTION supervised training. test accuracy EVALUATE-FOR self - supervised FSL. Material are large - scale labeled data, and labeled data. Method are embedding network, and supervised FSL methods. ",This paper studies self-supervised fewshot learning (SSL) in the presence of a pre-trained embedding network. The authors show that self-SSL can be used to improve the performance of downstream FSL tasks without the need for large-scale labeled data. They also show that the self-learning algorithm can be combined with supervised training to improve FSL performance.  ,"This paper studies the problem of fewshot learning (FSL) with self-supervised learning (SSL). In particular, the authors propose a method to learn a pre-trained embedding network for downstream FSL tasks, which can be used to improve the performance of SSL-based FSL methods. The authors show that the proposed method outperforms the state-of-the-art supervised FSL method in terms of test accuracy. The main contribution of the paper is a theoretical analysis of the relationship between SSL and FSL."
81,SP:bd552f98e6a447cefa6b1a9bbdf40bc6539fb643,"first order methods USED-FOR ultra - wide neural networks. finite width FEATURE-OF neural networks. OtherScientificTerm are global minima, initialization, teacher neurons, local minima, student neurons, and Angular Distance ( AD ) function. Method is two - layer teacher - student networks. Generic is methodology. ","This paper studies the problem of training a two-layer teacher-student neural network with a finite width. The authors show that the global minima of the network are local minima to the teacher neurons, and that the student neurons are local to them. They show that this phenomenon can be explained by the fact that the width of the teacher network is finite. They then propose to use the angular distance (AD) function to estimate the distance between the teacher and student neurons in the network, and show that it can be used to compute the AD function. ","This paper studies the problem of training a two-layer teacher-student neural network with a finite width. The authors propose a new method for training the teacher and student neural networks, where the teacher has a global minima and the student has a local minima. They show that the teacher's minima are global and that the student's are local. They also show that their method can be applied to the case where the width of the teacher network is finite."
90,SP:0f62846913ec10b44ed32845770da0565479dc75,"framework USED-FOR deep neural networks. user - provided formal knowledge USED-FOR learning from data. Deep Adaptive Semantic Logic ( DASL ) USED-FOR deep neural networks. Deep Adaptive Semantic Logic ( DASL ) HYPONYM-OF framework. knowledge representation USED-FOR first order logic. finite sampling USED-FOR truth values. infinite domains FEATURE-OF finite sampling. prior neuro - symbolic work USED-FOR DASL ’s representation. structure PART-OF image classification task. DASL USED-FOR visual relationship detection task. OtherScientificTerm are formal semantics, vanishing gradients, deeper logical structure, data requirements, commonsense knowledge, and data scarcity. ","This paper proposes a framework for learning from formal knowledge in the form of commonsense knowledge. The proposed method is based on a knowledge representation of the truth values of a set of propositions, which is then used to train a neural network to predict the truth value of a given proposition. The method is evaluated on image classification tasks and visual relationship detection tasks. ","This paper proposes Deep Adaptive Semantic Logic (DASL), a framework for deep neural networks to learn from user-provided formal knowledge. DASL learns a knowledge representation based on first-order logic, which can be used to represent the truth values in infinite domains. The authors show that the knowledge representation can be represented as a finite sampling of truth values across infinite domains with vanishing gradients. They show that this representation can also be used for visual relationship detection tasks. "
99,SP:2f19259d65fab904c1b771244da3dcb2f8aa0c26,"feedforward residual neural networks ( ResNets ) USED-FOR iterative recurrent computations. they USED-FOR neural networks. regularization approach USED-FOR learning of iterative solutions. ResNets USED-FOR iterative solutions. iteration CONJUNCTION convergence. convergence CONJUNCTION iteration. ResNets USED-FOR iterative solutions. regularizations USED-FOR iterative convergent computation. this USED-FOR inductive bias. regularizations USED-FOR inductive bias. ResNet CONJUNCTION recurrent ” ResNet. recurrent ” ResNet CONJUNCTION ResNet. method USED-FOR recurrence regularization. recurrent network USED-FOR one. one HYPONYM-OF recurrent ” ResNet. Lipschitz constraint FEATURE-OF residual functions. spectral normalization USED-FOR Lipschitz constraint. gradient coupling CONJUNCTION Lipschitz constraint. Lipschitz constraint CONJUNCTION gradient coupling. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. recurrence regularization CONJUNCTION spectral normalization. spectral normalization CONJUNCTION recurrence regularization. visual recognition tasks EVALUATE-FOR classification accuracy. Digitclutter HYPONYM-OF recognition tasks. MNIST HYPONYM-OF visual recognition tasks. classification accuracy EVALUATE-FOR spectral normalization. classification accuracy EVALUATE-FOR recurrence regularization. CIFAR-10 HYPONYM-OF visual recognition tasks. Iterative convergent computation USED-FOR tasks. inductive bias FEATURE-OF ResNets. Task are Iterative computations, and computer vision tasks. Method are Iterative methods, and soft gradient coupling. Metric is iterative convergence. Generic are them, and networks. ",This paper proposes a regularization approach for learning iterative solutions to iterative computations in neural networks. The proposed regularization is based on the Lipschitz regularization of the residual functions. The authors show that this regularization can improve the convergence of iterative methods and improve the inductive bias of the network. The experiments show the effectiveness of the proposed method on image classification tasks.,"This paper proposes a new regularization approach for learning iterative solutions of iterative ResNets. The main idea is to use the Lipschitz constraint of the residual functions of a ResNet to regularize the convergence of the ResNet. The authors show that this regularization can be used to reduce the inductive bias of the network. They also show that the proposed method can improve the performance of the model on CIFAR-10, CIFar-100, and MNIST. "
108,SP:6c14506b8b2b06043409d912e6bf877651aaa665,"Normalization techniques USED-FOR deep neural networks. they USED-FOR independent and identically distributed ( IID ) data. normalization methods USED-FOR OOD generalization. SelfNorm CONJUNCTION CrossNorm. CrossNorm CONJUNCTION SelfNorm. CrossNorm USED-FOR OOD generalization. SelfNorm USED-FOR OOD generalization. SelfNorm HYPONYM-OF normalization methods. CrossNorm HYPONYM-OF normalization methods. SelfNorm COMPARE CrossNorm. CrossNorm COMPARE SelfNorm. attention USED-FOR SelfNorm. SelfNorm USED-FOR OOD generalization. CrossNorm USED-FOR OOD generalization. SelfNorm CONJUNCTION CrossNorm. CrossNorm CONJUNCTION SelfNorm. OtherScientificTerm are channel - wise mean and variance, feature maps, and statistics usage. Task is classification and segmentation. ",This paper studies the effect of normalization methods on out-of-distribution (OOD) generalization in deep neural networks. The authors show that self-norm and cross-norm methods can be used to improve OOD generalization. They show that SelfNorm and CrossNorm are both effective in improving out of distribution generalization (OOD generalization) in classification and segmentation tasks.   ,"This paper studies the problem of out-of-distribution (OOD) generalization in deep neural networks. The authors propose a new normalization method, called SelfNorm, which is based on the idea of channel-wise mean and variance. They show that SelfNorm can improve OOD generalization over CrossNorm in terms of the number of channels and the variance of the feature maps. They also show that CrossNorm can also improve the generalization of SelfNorm. "
117,SP:2774abdc11917321dd4994af0f0da1ff824bea03,language CONJUNCTION speech. speech CONJUNCTION language. vision CONJUNCTION language. language CONJUNCTION vision. unsupervised pre - training CONJUNCTION generative modeling. generative modeling CONJUNCTION unsupervised pre - training. supervised learning CONJUNCTION unsupervised pre - training. unsupervised pre - training CONJUNCTION supervised learning. generative modeling USED-FOR multiple domains. Attention mechanisms HYPONYM-OF inductive biases. unsupervised pre - training USED-FOR multiple domains. vision HYPONYM-OF multiple domains. speech HYPONYM-OF multiple domains. language HYPONYM-OF multiple domains. neural network architectures USED-FOR reinforcement learning ( RL ). they USED-FOR neural network architectures. high dimensional inputs USED-FOR neural network architectures. pixels HYPONYM-OF high dimensional inputs. attention module PART-OF convolutional encoder. attention module PART-OF RL agent. convolutional encoder PART-OF RL agent. data augmentations CONJUNCTION contrastive losses. contrastive losses CONJUNCTION data augmentations. module USED-FOR interpretable task - relevant information. DeepMind Control Suite environments EVALUATE-FOR module. sampleefficiency EVALUATE-FOR agents. module USED-FOR agents. sampleefficiency EVALUATE-FOR module. attention mechanisms USED-FOR reinforcement learning and control. Generic is approach. ,This paper proposes an attention module for reinforcement learning. The attention module consists of a convolutional encoder that takes as input a set of high-dimensional pixels and outputs a low-dimensional representation of the input. The proposed attention module is trained using supervised learning and unsupervised pre-training. Experiments show that the proposed method outperforms baselines in the DeepMind Control Suite environments. ,This paper proposes a new attention module for reinforcement learning (RL) agents. The attention module consists of a convolutional encoder and a contrastive encoder. The encoder encodes the task-relevant information and the contrastive loss is used to augment the input data with contrastive losses. Experiments on the DeepMind Control Suite environments show that the attention module can improve the sample efficiency of RL agents.
126,SP:31a7051d08d19c01e11f1fac2f3041ed2fa28f15,"gradient - based approach USED-FOR multitask networks. GradNorm HYPONYM-OF gradient - based approach. extension USED-FOR GradNorm. game theory USED-FOR Rotograd. Rotograd COMPARE approaches. approaches COMPARE Rotograd. approaches USED-FOR multitask learning. Rotograd USED-FOR multitask learning. real - world datasets EVALUATE-FOR Rotograd. real - world datasets CONJUNCTION network architectures. network architectures CONJUNCTION real - world datasets. network architectures EVALUATE-FOR Rotograd. Task is learning. OtherScientificTerm are network parameters, gradient magnitude, gradient magnitudes, and task gradients. Generic is it. Metric is convergence. ","This paper proposes a new gradient-based approach for multi-task learning. The proposed method is based on the idea of Rotograd, which is an extension of the GradNorm method. The main idea is to compute the gradient magnitude of the task gradients in a game-theoretic manner. Theoretical analysis is provided to show the convergence of the proposed method. Empirical results are provided to demonstrate the effectiveness of the method.  ","This paper proposes a new gradient-based approach for multi-task learning, called Rotograd. The main contribution of the paper is a new extension of the GradNorm framework, which is based on the game theory of game theory. The authors show that the proposed method can converge to a state-of-the-art convergence rate in terms of the number of tasks and the magnitude of the task gradients. They also provide a theoretical analysis of the convergence rate of the method, showing that it converges to the state of the art."
135,SP:ac9ebd027b92527d9a87b13ad11d002d99a2b0f6,"geometry distortion problem FEATURE-OF methods. randomness of color transformation FEATURE-OF translation process. unwanted distortions FEATURE-OF translation. Minimal Geometry - Distortion Constraint ( MGC ) HYPONYM-OF I2I translation constraint. approximate representation of mutual information USED-FOR estimation and maximization of MGC. MGC COMPARE state - of - the - art methods. state - of - the - art methods COMPARE MGC. benchmark datasets EVALUATE-FOR state - of - the - art methods. benchmark datasets EVALUATE-FOR MGC. OtherScientificTerm are domain mapping function, mapping function, geometry structure, and consistency of geometry structures. Material are paired data, and translated images. Generic is function. ","This paper proposes a new I2I translation constraint for the geometry distortion problem. The proposed method is motivated by the randomness of color transformation in the translation process, which can lead to unwanted distortions in the original image. The main contribution of this paper is to introduce a new constraint, called Minimal Geometry-Distortion Constraint (MGC), which is based on the idea of minimizing the mutual information between the input and translation functions. Theoretical analysis is provided to show that MGC can be used to estimate and maximise the estimation and maximization of MGC. Empirical results show that the proposed method achieves state-of-the-art performance on several benchmark datasets.","This paper proposes a new I2I translation constraint for the geometry distortion problem. The proposed constraint is based on the Minimal Geometry-Distortion Constraint (MGC) framework, which is motivated by the randomness of color transformation in the translation process. The authors show that MGC can be used to estimate the mutual information between two pairs of images, and that it can also be used for the estimation and maximization of MGC. The paper also provides a theoretical analysis of the convergence of the proposed constraint. The experimental results show that the proposed method outperforms state-of-the-art methods."
144,SP:92a38d7d18f07f68b8f93c61180e2cc1dddd21de,point sampling patterns USED-FOR point cloud GANs. DGCNN CONJUNCTION PointConv. PointConv CONJUNCTION DGCNN. PointConv CONJUNCTION KPConv. KPConv CONJUNCTION PointConv. sampling - oversensitive discriminators USED-FOR valid shape generation. PointNet++ CONJUNCTION DGCNN. DGCNN CONJUNCTION PointNet++. sampling - insensitive discriminators USED-FOR shape point clouds. point clustering artifacts FEATURE-OF shape point clouds. KPConv HYPONYM-OF sampling - oversensitive discriminators. PointNet - Max HYPONYM-OF sampling - insensitive discriminators. PointNet++ HYPONYM-OF sampling - oversensitive discriminators. PointConv HYPONYM-OF sampling - oversensitive discriminators. DGCNN HYPONYM-OF sampling - oversensitive discriminators. evaluation metrics EVALUATE-FOR sampling pattern. perceptual metrics PART-OF sampling spectrum of metrics. sampling pattern COMPARE geometry. geometry COMPARE sampling pattern. sampling spectrum USED-FOR middle - point sampling - aware baseline discriminator. PointNet - Mix HYPONYM-OF point cloud generators. sampling - related metrics EVALUATE-FOR point cloud generators. PointNet - Mix HYPONYM-OF middle - point sampling - aware baseline discriminator. Task is generator design. Method is discriminator design. Generic is discriminators. ,"This paper proposes a sampling-sensitive discriminator for point cloud GANs. The proposed discriminator is based on sampling-over-sensitivity to point clustering artifacts. The authors show that PointNet++, PointNet-Max, and PointConv are sampling-oversensitive discriminators. They show that sampling-aware discriminators can generate valid shape point clouds in PointNet and DGCNN. ","This paper proposes a sampling-sensitive discriminator for shape point cloud GANs. The proposed discriminator, PointNet-Max, is based on PointNet++ and PointConv, which are sampling-oversensitive discriminators. PointNet Mix is a middle-point sampling-aware baseline discriminator that can be used to generate shape point clouds. The authors show that PointNet Max can generate point clouds with a sampling pattern that is more sensitive to point clustering artifacts than PointNet+ and PointNet+. The authors also show that the sampling pattern can be more sensitive than the geometry."
153,SP:16c4be3eb162bc81cb3343c2fc115eb8e926a5b5,"images USED-FOR Convolutional Neural Networks ( CNNs ). small quasi - imperceptible artificial perturbations USED-FOR Convolutional Neural Networks ( CNNs ). Capsule Networks ( CapsNets ) COMPARE CNNs. CNNs COMPARE Capsule Networks ( CapsNets ). CNNs COMPARE Capsule Networks ( CapsNets ). Capsule Networks ( CapsNets ) COMPARE CNNs. Capsule Networks ( CapsNets ) USED-FOR white - box attacks. attack protocols USED-FOR CNNs. CapsNets USED-FOR adversarial examples. adversarial robustness FEATURE-OF CapsNets. multi - step attack methods USED-FOR CapsNets. multi - step attack methods USED-FOR CNNs. routing process USED-FOR vote attack. vote attack PART-OF detection - aware attack paradigm. vote attack USED-FOR CapsNets. OtherScientificTerm are votes, computationally expensive routing mechanism, and votes of CapsNets. Metric is computational cost. Method is class - conditional reconstruction based detection method. ",This paper proposes Capsule Networks (CapsNets) to improve the robustness of convolutional neural networks (CNNs) against adversarial attacks. CapsNets are a class-conditional reconstruction based detection method that can detect the presence of adversarial perturbations in the input images. The proposed method is based on a novel routing mechanism that is computationally efficient and can be applied to attack CNNs.  ,This paper proposes a new attack method for white-box attacks on convolutional neural networks (CNNs). The proposed method is based on a class-conditional reconstruction-based detection method. The method is able to detect the presence of adversarial examples in the training data. The authors show that the proposed method outperforms the state-of-the-art attack methods in terms of robustness. 
162,SP:dbd093dff7a38ba8882bb8119c34623ddaaf4cc6,"Meta - reinforcement learning USED-FOR policy. recurrent neural networks USED-FOR policies. algorithm USED-FOR learning of recurrent policies. privileged information USED-FOR learning of recurrent policies. task descriptor FEATURE-OF privileged information. privileged information USED-FOR algorithm. parameters sharing CONJUNCTION auxiliary objective. auxiliary objective CONJUNCTION parameters sharing. method USED-FOR informed policy. informed policy USED-FOR task embeddings. policy HYPONYM-OF informed policy. parameters sharing USED-FOR recurrent policy. descriptors USED-FOR task embeddings. auxiliary objective USED-FOR recurrent policy. learning sample complexity EVALUATE-FOR approach. task - inference approaches USED-FOR meta - reinforcement learning. Thompson sampling CONJUNCTION task - inference approaches. task - inference approaches CONJUNCTION Thompson sampling. vanilla RNNs CONJUNCTION Thompson sampling. Thompson sampling CONJUNCTION vanilla RNNs. it COMPARE vanilla RNNs. vanilla RNNs COMPARE it. it COMPARE Thompson sampling. Thompson sampling COMPARE it. it COMPARE task - inference approaches. task - inference approaches COMPARE it. it USED-FOR meta - reinforcement learning. Thompson sampling USED-FOR meta - reinforcement learning. exploration / exploitation strategies USED-FOR algorithm. Generic are information, them, and they. Task is online adaptation setting. OtherScientificTerm is behaviour. Method is RNNs. ","This paper proposes a meta-reinforcement learning algorithm for meta-learning in the online adaptation setting, where the goal is to learn a policy that can adapt to a new task in an online setting. The proposed method is based on the idea that the task embeddings are privileged information, which can be used to learn an informed policy. Theoretical analysis is provided to show that the proposed method achieves better sample complexity compared to Thompson sampling and task-inference approaches.   ",This paper proposes a new meta-reinforcement learning algorithm for meta-regression. The key idea is to use the privileged information of the task embeddings to learn an informed policy that can be used for online adaptation. The proposed method is based on the idea of parameters sharing and auxiliary objective. The authors show that the proposed method outperforms Thompson sampling and task-inference approaches in terms of learning sample complexity.
171,SP:bd89d254fbf31db61db237d08ab42981e27c52df,"trial - and - errors USED-FOR realworld applications. trial - and - errors USED-FOR RL. simulator USED-FOR optimal policies. dataset USED-FOR simulator. offline dataset USED-FOR policy. paradigm USED-FOR RL policy. model learning technique USED-FOR paradigm. offline data USED-FOR paradigm. offline data USED-FOR RL policy. models USED-FOR policy learning. adaptive policy USED-FOR real - world environments. stochasticity FEATURE-OF dynamics. synthetic environments CONJUNCTION real - world ride - hailing platform. real - world ride - hailing platform CONJUNCTION synthetic environments. method USED-FOR robust recommendations. method USED-FOR distortion problem. Generic is approach. OtherScientificTerm are fidelity of the simulator, and online sampling. Method is learning. ",This paper proposes a method for learning from offline data to improve the fidelity of the simulator used to train an RL policy. The method is based on the observation that the fidelity to the simulator is a function of the dynamics of the environment. The authors propose to learn a model to model the dynamics in the simulator and then use this model to learn an adaptive policy in the offline data. The proposed method is evaluated on a number of synthetic and real-world environments.   ,This paper proposes a new method for learning a policy that is robust to the distortion of the simulator. The method is based on a model learning technique that learns a policy from an offline dataset and then uses the offline dataset to learn an adaptive policy that can be applied to real-world environments. The authors show that the proposed method is able to learn a robust policy that does not suffer from the distortion problem. They also show that their method can be used to improve the robustness of recommendations.
180,SP:1a166b28cf684e0d5759bd629f6a53370d2bf11c,"sparse rewards USED-FOR goal - reaching behaviors. expert demonstrations CONJUNCTION value function. value function CONJUNCTION expert demonstrations. RL algorithms USED-FOR goal reaching policies. imitation learning USED-FOR goal reaching policies. imitation learning USED-FOR RL algorithms. algorithm USED-FOR goal - reaching behaviors. goal - reaching performance CONJUNCTION robustness. robustness CONJUNCTION goal - reaching performance. robustness EVALUATE-FOR RL algorithms. goal - reaching performance EVALUATE-FOR RL algorithms. iterated supervised learning procedure USED-FOR RL objective. benchmark tasks EVALUATE-FOR RL algorithms. Method are reinforcement learning ( RL ) algorithms, and supervised imitation learning. Generic is it. OtherScientificTerm are demonstrations, policy, and performance bounds. ","This paper studies the problem of learning goal-reaching policies with sparse rewards in reinforcement learning, where the goal is to reach a goal in a goal-conditioned environment. The authors propose to use an iterated supervised learning procedure to learn a policy that maximizes the mutual information between the learned policy and the reward function. They show that this method is robust to the presence of expert demonstrations and shows that it is able to learn goal-achieving policies that are robust to adversarial attacks.  ","This paper studies the problem of goal-reaching reinforcement learning in the context of imitation learning, where the goal is to reach a goal with sparse rewards. The authors propose an iterated supervised learning procedure to learn a goal reaching policy that is robust to expert demonstrations and value function. They show that the goal reaching performance of the learned policy is bounded by the robustness of the policy. They also provide a theoretical analysis of the performance bounds of their algorithm."
189,SP:c306530164d677e670554eeba8203c66bb3d9f7a,autoregressive models USED-FOR speech. autoregressive teacher model USED-FOR duration prediction. one - to - many mapping problem PART-OF TTS. knowledge distillation USED-FOR one - to - many mapping problem. autoregressive teacher model USED-FOR FastSpeech model. teacher model USED-FOR mel - spectrograms. teacher model USED-FOR duration. information loss FEATURE-OF mel - spectrograms. pitch CONJUNCTION energy. energy CONJUNCTION pitch. energy CONJUNCTION duration. duration CONJUNCTION energy. FastSpeech 2 USED-FOR FastSpeech. FastSpeech 2 USED-FOR one - to - many mapping problem. variation information of speech USED-FOR conditional inputs. one - to - many mapping problem PART-OF TTS. duration HYPONYM-OF variation information of speech. energy HYPONYM-OF variation information of speech. pitch HYPONYM-OF variation information of speech. pitch CONJUNCTION energy. energy CONJUNCTION pitch. duration CONJUNCTION pitch. pitch CONJUNCTION duration. predicted values USED-FOR inference. conditional inputs USED-FOR training. speech waveform USED-FOR pitch. speech waveform USED-FOR energy. FastSpeech 2s USED-FOR speech waveform. end - to - end inference USED-FOR FastSpeech 2s. FastSpeech 2 COMPARE FastSpeech. FastSpeech COMPARE FastSpeech 2. FastSpeech 2 COMPARE FastSpeech. FastSpeech COMPARE FastSpeech 2. FastSpeech 2 COMPARE autoregressive models. autoregressive models COMPARE FastSpeech 2. FastSpeech 2 COMPARE FastSpeech 2s. FastSpeech 2s COMPARE FastSpeech 2. training speed - up EVALUATE-FOR FastSpeech. training speed - up EVALUATE-FOR FastSpeech 2. voice quality EVALUATE-FOR FastSpeech 2. voice quality EVALUATE-FOR FastSpeech. Method is teacher - student distillation pipeline. Task is data simplification. Generic is model. ,"This paper proposes a method to improve the performance of a teacher-student model for duration prediction in speech. The proposed method is based on distillation of the mel spectrograms extracted from the teacher model. The teacher model is trained to predict the duration and pitch using the mel-spectrograms, and the student model is used for predicting the duration using the predicted mel spectrogram. The method is evaluated on a variety of speech datasets and compared with a number of state-of-the-art methods.","This paper proposes a method to improve the performance of the teacher-student distillation (TTS) model for speech prediction. The teacher model is trained to predict the duration of a speech waveform, while the student model predicts the pitch and energy of the speech. The proposed method is based on the knowledge distillation framework, where the teacher model distills the mel-spectrograms from the teacher to the student. The student model is then trained on the teacher's mel spectrogram, and the teacher predicts the duration and the energy using the predicted mel spectrograms. The method is evaluated on a variety of speech datasets, and compared with the state-of-the-art in terms of training speed and accuracy."
198,SP:79e9fb20d383816f54738ce70d137131ebc10290,"k - dimensional subspace FEATURE-OF tempered distribution q(x ). tempered distributions USED-FOR unsupervised dimension reduction problem ( UDR ). tempered distribution q(x ) USED-FOR empirical probability density function. q CONJUNCTION pemp. pemp CONJUNCTION q. minimization of the distance USED-FOR problem. generalized functions USED-FOR minimization of the distance. sufficient dimension reduction problem ( SDR ) HYPONYM-OF data science. algorithm USED-FOR problem. algorithm USED-FOR second. algorithm USED-FOR problem. optimization problem USED-FOR optimization problem. distributions USED-FOR optimization problem. ordinary functions USED-FOR optimization problem. algorithm USED-FOR minimization of I(f ) + λR(f ). two - step iterative computation USED-FOR algorithm. two - step iterative computation USED-FOR minimization of I(f ) + λR(f ). synthetic data CONJUNCTION datasets. datasets CONJUNCTION synthetic data. examples EVALUATE-FOR method. datasets USED-FOR method. synthetic data USED-FOR method. datasets USED-FOR examples. synthetic data USED-FOR examples. UDR HYPONYM-OF examples. Method is infinite - dimensional formulation. OtherScientificTerm are nonnegative penalty function R(f ), and λR(f ). Material is real data. ","This paper studies the unsupervised dimension reduction problem (UDR) with a tempered distribution q(x) over a k-dimensional subspace, where the empirical probability density function is the empirical density function. The authors propose two algorithms to solve the problem. The first algorithm is based on the minimization of I(f) + R(f), which is a two-step iterative computation. The second algorithm uses an ordinary function to find the minimizer of the distance between the empirical distribution and the tempered distribution.   The authors show that the first algorithm can be solved in polynomial time, and that the second algorithm is equivalent to the first one. ","This paper studies the unsupervised dimension reduction problem (UDR) in data science. The authors propose a two-step algorithm for solving the problem of sufficient dimension reduction (SDR), which is an important problem in the data science community. The first step is to find the minimum distance between a k-dimensional subspace of a tempered distribution q(x) and the empirical probability density function p(x). The second step is the minimization of the distance between the two subspaces, which can be done by using a generalized function of generalized functions. The main contribution of the paper is the formulation of the problem as an infinite-dimensional formulation, which allows the authors to solve the problem in terms of a finite-dimensional optimization problem. "
207,SP:93e54522e6c2b805905d21fc968fc40866f2898b,methods USED-FOR model. methods USED-FOR robustness. rare or underrepresented patterns FEATURE-OF model. contextual feature utility CONJUNCTION contextual feature sensitivity. contextual feature sensitivity CONJUNCTION contextual feature utility. robustness CONJUNCTION sensitivity. sensitivity CONJUNCTION robustness. Feature Contrastive Learning ( FCL ) USED-FOR model. contextual utility FEATURE-OF features. robustness CONJUNCTION sensitivity. sensitivity CONJUNCTION robustness. noise FEATURE-OF generalization. sensitivity EVALUATE-FOR models. robustness EVALUATE-FOR models. generalization EVALUATE-FOR models. FCL USED-FOR models. Task is real - world applications. ,"This paper proposes a novel method to improve the robustness and sensitivity of feature contrastive learning (FCL) models. The proposed method is based on the observation that features with high contextual utility are more sensitive to noise than features with low contextual utility. To address this issue, the authors propose to use a contrastive loss to learn features with higher contextual utility and lower contextual sensitivity. Experiments show that the proposed method improves the generalization ability of FCL models. ","This paper proposes Feature Contrastive Learning (FCL) to improve the robustness and sensitivity of feature contrastive learning models. The main idea is to learn a model that maximizes the utility of features in the context of the dataset. The authors show that FCL can improve robustness, sensitivity, and generalization. They also show that it can also improve generalization in the presence of noise. "
216,SP:f03c50f15022c4f56ac2b3085354ffed38ad1145,"algorithm USED-FOR autonomous agents. latent representation PART-OF discriminator network. latent representation USED-FOR adversarial learning. adversarial learning USED-FOR algorithm. high dimensional observations USED-FOR autonomous agents. adversarial learning USED-FOR autonomous agents. mutual information constraints USED-FOR latent representation. shared feature space USED-FOR imitation. environment appearance CONJUNCTION agent embodiment. agent embodiment CONJUNCTION environment appearance. balancing CONJUNCTION manipulation and locomotive tasks. manipulation and locomotive tasks CONJUNCTION balancing. algorithm USED-FOR control problems. agent embodiment FEATURE-OF domain differences. environment appearance FEATURE-OF domain differences. manipulation and locomotive tasks HYPONYM-OF control problems. balancing HYPONYM-OF control problems. Method is Imitation learning methods. Generic are they, and constraints. OtherScientificTerm are optimal states, and features. ","This paper proposes a novel method for imitation learning in the presence of adversarial perturbations in the environment. The proposed method is based on the observation of a shared feature space, which is then used to learn a discriminator network. The discriminator is trained by minimizing mutual information constraints between the features of the environment and the latent representation of the discriminator. The method is evaluated on a variety of tasks, including balancing, manipulation and locomotive tasks.","This paper proposes a novel imitation learning method for autonomous agents. The main idea is to use a shared feature space between the agent and the discriminator network to learn a latent representation of the environment and the environment features. The discriminator is trained using adversarial learning. The authors show that the proposed method can be used for imitation learning on a variety of control problems, including manipulation and locomotive tasks."
225,SP:ef18f4188426bc01be309633b486884b0e7a81a4,"test accuracy EVALUATE-FOR unpruned network. pruned network COMPARE unpruned network. unpruned network COMPARE pruned network. computer vision CONJUNCTION natural language processing. natural language processing CONJUNCTION computer vision. deep neural network ( DNN ) USED-FOR applications. LTH PART-OF deep neural network ( DNN ). natural language processing HYPONYM-OF applications. computer vision HYPONYM-OF applications. objective function CONJUNCTION sample complexity. sample complexity CONJUNCTION objective function. guaranteed generalization EVALUATE-FOR model. algorithm USED-FOR pruned neural network. non - pruned weights PART-OF hidden layer. accelerated ) stochastic gradient descent algorithm USED-FOR algorithm. pruned neural network USED-FOR model. pruned neural network COMPARE unpruned one. unpruned one COMPARE pruned neural network. Task are lottery ticket hypothesis ( LTH ), and pruning multi - layer neural networks. Metric are generalization, and generalization of the winning ticket. OtherScientificTerm are zero generalization error, and convex region. Method is neural network model. ","This paper studies the lottery ticket hypothesis (LTH) in deep neural networks. The authors show that a pruned neural network with LTH can achieve better generalization performance than an unpruned network without LTH. In particular, the authors prove that the generalization error of the pruned network is bounded by a convex function, and that the LTH is guaranteed to be zero when the weights of the network are pruned.   ",This paper studies the lottery ticket hypothesis (LTH) for pruning multi-layer neural networks. The authors show that pruned neural networks can be more general than unpruned ones in terms of generalization error. They show that the generalization of a pruned network is bounded by the number of samples and the sample complexity of the model. They also provide an algorithm to prune the weights in the hidden layer of a neural network. 
234,SP:eed6cb2f8caed39f8295f4aeb6e044c2ac981c4d,generalization EVALUATE-FOR neural networks. accuracy CONJUNCTION generalization. generalization CONJUNCTION accuracy. data augmentation approaches USED-FOR neural networks. accuracy EVALUATE-FOR neural networks. generalization EVALUATE-FOR data augmentation approaches. accuracy EVALUATE-FOR data augmentation approaches. augmented data COMPARE clean data. clean data COMPARE augmented data. AutoLabel USED-FOR augmented data. clean distribution CONJUNCTION augmented distribution. augmented distribution CONJUNCTION clean distribution. hold - out validation set USED-FOR calibration - performance. calibration - performance USED-FOR AutoLabel. hold - out validation set USED-FOR AutoLabel. label smoothing USED-FOR AutoLabel. mixup CONJUNCTION adversarial training. adversarial training CONJUNCTION mixup. AugMix CONJUNCTION mixup. mixup CONJUNCTION AugMix. AutoLabel USED-FOR data augmentation methods. adversarial training HYPONYM-OF data augmentation methods. AugMix HYPONYM-OF data augmentation methods. mixup HYPONYM-OF data augmentation methods. CIFAR-100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-100. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. accuracy CONJUNCTION calibration. calibration CONJUNCTION accuracy. AutoLabel USED-FOR models. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. CIFAR-10 EVALUATE-FOR AutoLabel. calibration EVALUATE-FOR AutoLabel. accuracy EVALUATE-FOR AutoLabel. AutoLabel USED-FOR adversarial training. clean accuracy CONJUNCTION adversarial robustness. adversarial robustness CONJUNCTION clean accuracy. OtherScientificTerm is distributional shift. ,"This paper proposes AutoLabel, a data augmentation method that uses label smoothing to reduce the distributional shift between the augmented data and the clean data. The proposed method is based on the idea that the clean and augmented data should have the same labels. The authors show that AutoLabel is able to improve the calibration performance of the model on CIFAR-10 and ImageNet.  ","This paper proposes AutoLabel, a new data augmentation method for training neural networks. AutoLabel is based on label smoothing, which aims to improve the calibration-performance of the model. The proposed method is evaluated on CIFAR-10, ImageNet, and Cifar-100 datasets, and compared with a number of other data augmentations methods, including mixup, AugMix, and adversarial training."
243,SP:0d5017e1a405bf86e3bac40e6e59886d4bf48450,"heuristic proxy classification tasks CONJUNCTION data augmentations. data augmentations CONJUNCTION heuristic proxy classification tasks. methods USED-FOR heuristic proxy classification tasks. data augmentations USED-FOR methods. causal framework USED-FOR self - supervised representation learning. proxy classifiers USED-FOR pretraining. invariance constraints USED-FOR proxy classifiers. invariance constraints USED-FOR data augmentations. selfsupervised objective, Representation Learning USED-FOR invariant prediction of proxy targets. invariance regularizer USED-FOR generalization guarantees. invariance regularizer USED-FOR invariant prediction of proxy targets. Invariant Causal Mechanisms ( RELIC ) USED-FOR selfsupervised objective, Representation Learning. causality USED-FOR contrastive learning. contrastive learning HYPONYM-OF self - supervised method. RELIC COMPARE methods. methods COMPARE RELIC. robustness CONJUNCTION out - of - distribution generalization. out - of - distribution generalization CONJUNCTION robustness. RELIC COMPARE methods. methods COMPARE RELIC. out - of - distribution generalization FEATURE-OF ImageNet. human - level performance EVALUATE-FOR methods. out - of - distribution generalization EVALUATE-FOR RELIC. Atari EVALUATE-FOR methods. robustness EVALUATE-FOR RELIC. out - of - distribution generalization EVALUATE-FOR methods. robustness EVALUATE-FOR methods. Method is Self - supervised learning. OtherScientificTerm are supervised signals, and augmentations. Material is unlabeled data. ","This paper proposes a self-supervised representation learning method based on a causal framework for proxy classifiers. The proposed method, called Invariant Causal Mechanisms (RELIC), is based on the idea of invariance constraints on data augmentations. The authors show that the proposed method achieves better robustness and out-of-distribution generalization compared to existing methods. Experiments are conducted on ImageNet and Atari. ",This paper proposes a causal framework for self-supervised representation learning for heuristic proxy classification tasks. The main idea is to use the invariance constraints of proxy classifiers to enforce invariant prediction of data augmentations. The invariance regularizer is used to ensure the generalization guarantees of the proposed method. Experiments on Atari and ImageNet show the effectiveness of the method. 
252,SP:8f80a6f79f78c6421857f392c9a5e98061d7eb60,visual representations of the observed scene USED-FOR navigation actions. Visual Transformer Network ( VTNet ) USED-FOR informative visual representation in navigation. spatial locations of objects CONJUNCTION image regions. image regions CONJUNCTION spatial locations of objects. VTNet HYPONYM-OF structure. structure USED-FOR visual representations. pre - training scheme USED-FOR navigation policy learning. pre - training scheme USED-FOR visual representations. navigation signals USED-FOR visual representations. informative representation USED-FOR navigation. attention operations USED-FOR informative representation. descriptors USED-FOR informative representation. VTNet USED-FOR informative representation. object and region features USED-FOR spatial - aware descriptors. spatial - aware descriptors USED-FOR VTNet. object and region features USED-FOR VTNet. location cues FEATURE-OF object and region features. attention operations USED-FOR descriptors. artificial environment AI2 - Thor EVALUATE-FOR VTNet. VTNet COMPARE methods. methods COMPARE VTNet. artificial environment AI2 - Thor EVALUATE-FOR methods. Task is Object goal navigation. OtherScientificTerm is directional navigation signals. Method is visual representation. ,"This paper proposes a visual transformer architecture for object goal navigation. The architecture is based on the Transformer architecture, which is used to model the spatial locations of objects and image regions. The proposed architecture is evaluated on the task of object navigation in the artificial environment of AI2-Thor.   ","This paper proposes a new visual transformer network (VTNet) for object navigation. The proposed method is based on a pre-training scheme for navigation policy learning, where the visual representations of the observed scene are used to guide the navigation actions. The method is evaluated on a variety of environments, and the proposed method outperforms the state-of-the-art. "
261,SP:3e7cbe3dff592ef371e48dd86be7719fc5343f17,Federated learning USED-FOR neural network models. model parameters USED-FOR federated learning. solution USED-FOR privacy - preserving federated learning. secure aggregation primitive USED-FOR privacy - preserving federated learning. secure aggregation primitive USED-FOR solution. communication - computation efficient secure aggregation COMPARE secure solution. secure solution COMPARE communication - computation efficient secure aggregation. communication - computation efficient secure aggregation USED-FOR communication / computational resources. scheme USED-FOR topology. sparse random graphs COMPARE complete graph. complete graph COMPARE sparse random graphs. topology FEATURE-OF secret - sharing nodes. sparse random graphs USED-FOR topology. Erdős - Rényi graph USED-FOR G. reliability / privacy EVALUATE-FOR scheme. reliability CONJUNCTION data privacy. data privacy CONJUNCTION reliability. data privacy FEATURE-OF federated learning systems. scheme USED-FOR scheme. federated learning systems EVALUATE-FOR scheme. data privacy EVALUATE-FOR scheme. reliability EVALUATE-FOR scheme. OtherScientificTerm is local data. ,This paper proposes a new secure aggregation method for federated learning. The main idea is to use a sparse random graph to aggregate the local data in a way that preserves the privacy of the clients. Theoretical analysis is provided to show that the proposed method is efficient in terms of communication time and computation cost. Empirical results show the effectiveness of the proposed approach. ,"This paper proposes a secure aggregation primitive for privacy-preserving federated learning. The proposed method is based on the Erdős-Rényi graph, which is a sparse random graph. The key idea is to use the topology of the secret-sharing nodes as the secret topology. The authors show that the proposed method can be combined with other secure aggregation methods to achieve better performance and privacy. The experimental results demonstrate the effectiveness of the method."
270,SP:00fae41e0eca0a1575cd7b2dcfabf0dc5c9c8b8a,"incentive compatible auction PART-OF Auction Design. theoretical approaches USED-FOR problem. neural network architectures USED-FOR optimal auctions. theoretical auction design USED-FOR time - independent Lagrangian. inner maximization loop USED-FOR optimal misreports. inner maximization loop USED-FOR optimization procedure. stationary utility functions FEATURE-OF two - player game. two - player game USED-FOR Auction Design. Generic is approach. Method are hyper - parameter search, and neural network. OtherScientificTerm is auctions. ","This paper studies the problem of finding an incentive-compatible auction in a two-player game, where the goal is to design an auction that maximizes the utility of the agent and minimizes the mis-reports. The main contribution of the paper is a theoretical analysis of the problem in terms of the time-independent Lagrangian of the stationary utility function. The authors show that the optimal misreports can be obtained by maximizing the inner maximization loop of the utility function, and show that hyper-parameter search can be used to find the optimal auctions.   ",This paper studies the problem of finding the optimal auction design for a two-player game with stationary utility functions. The main contribution of the paper is a theoretical analysis of the time-independent Lagrangian of the two-party game. The authors show that the inner maximization loop of a neural network can be used to find the optimal misreports in the two player game. They also show that this inner maximisation loop can be combined with a hyper-parameter search procedure to find optimal auctions. 
279,SP:a0e8061beb5e9a6c631419861559d22b8d645cb4,pre - trained model USED-FOR downstream task. large - scale dataset USED-FOR deep neural network. supervised and unsupervised pre - training approaches USED-FOR learning representations. discriminative knowledge of labels CONJUNCTION intrinsic structure of data. intrinsic structure of data CONJUNCTION discriminative knowledge of labels. discriminative knowledge USED-FOR fine - tuning. former USED-FOR fine - tuning methods. intrinsic structure of data USED-FOR boosting fine - tuning. general learning approach USED-FOR supervised and unsupervised pre - trained representations. Bi - tuning HYPONYM-OF general learning approach. supervised and unsupervised pre - trained representations USED-FOR downstream tasks. classifier head CONJUNCTION projector head. projector head CONJUNCTION classifier head. projector head USED-FOR intrinsic structure of data. contrastive cross - entropy loss USED-FOR label information. classifier head USED-FOR label information. Bi - tuning USED-FOR vanilla fine - tuning. instancecontrast way FEATURE-OF label information. contrastive cross - entropy loss FEATURE-OF classifier head. categorical contrastive learning loss USED-FOR projector head. Bi - tuning USED-FOR fine - tuning tasks. fine - tuning tasks EVALUATE-FOR supervised and unsupervised pre - trained models. low - data regime FEATURE-OF accuracy. Generic is latter. OtherScientificTerm is pre - trained representations. , of fine-tuning a pre-trained model on a large-scale dataset. The authors propose to use a combination of self-supervised and unsupervised methods to fine-tune the model on the downstream tasks. They show that using a contrastive cross-entropy loss on the classifier heads and a categorical contrastive learning loss for the projector heads improves the performance of the model. ,This paper proposes a general learning approach for fine-tuning a pre-trained model on a large-scale dataset. The authors propose a contrastive contrastive cross-entropy loss for the classifier head and a categorical contrastive learning loss to the projector head. They show that the proposed approach outperforms vanilla fine-tuning on a variety of downstream tasks. 
288,SP:87e5b552c13d73bd85249062a152c6c140e594a9,"adversarial accuracy CONJUNCTION adversarial training. adversarial training CONJUNCTION adversarial accuracy. robustness EVALUATE-FOR classifiers. adversarial accuracy USED-FOR robustness. adversarial accuracy EVALUATE-FOR classifiers. measure USED-FOR robustness. measure USED-FOR classifiers. robustness EVALUATE-FOR classifiers. accuracy CONJUNCTION accuracy. accuracy CONJUNCTION accuracy. It USED-FOR adversarial robustness. adversarial robustness EVALUATE-FOR classifiers. It USED-FOR classifiers. accuracy FEATURE-OF adversarially perturbed samples. invariance - based adversarial examples USED-FOR model. genuine adversarial accuracy EVALUATE-FOR classifier. test accuracy CONJUNCTION lp norm - based test adversarial robustness. lp norm - based test adversarial robustness CONJUNCTION test accuracy. OtherScientificTerm are generalization, predicted classes, and perceptual classes. Material is clean data. Generic are it, and distance metrics. Method is norm - based distance metric. ","This paper studies the relationship between adversarial accuracy and robustness. The authors propose to use adversarial robustness as a metric to measure the generalization ability of a classifier. The proposed metric is based on the distance between predicted classes and perceptual classes, which can be used to measure robustness in the presence of adversarial perturbations.   The authors show that the proposed metric can be applied to both adversarial training and adversarial testing. They show that adversarial test accuracy and test robustness can be measured using the distance metric. They also show that test accuracy is correlated with adversarial performance.","This paper proposes a new metric for measuring the robustness of a classifier against adversarial perturbations. The metric is based on the invariance-based adversarial examples, where the classifier is trained on a set of invariant examples. The authors show that the distance between test accuracy and adversarial accuracy is a measure of robustness, and that it can be used to measure the generalization ability of classifiers. They also show that test accuracy is better than adversarial robustness in terms of the lp norm-based distance metric."
297,SP:2fda410b9281c5e253d385bc4382ec168bc161f3,"fairness PART-OF algorithmic designs. graph - structured data USED-FOR disparate impact. fairness concept FEATURE-OF dyadic fairness. edges PART-OF graph. graph connections USED-FOR dyadic fairness. dyadic fairness FEATURE-OF link predictive scores. algorithm USED-FOR fair adjacency matrix. fair adjacency matrix USED-FOR fair link prediction. graph structural constraints USED-FOR fair link prediction. FairAdj USED-FOR fair adjacency matrix. graph structural constraints FEATURE-OF fair adjacency matrix. method USED-FOR dyadic fairness. fairness - utility tradeoff EVALUATE-FOR method. Task are Disparate impact, machine learning applications, and mitigating discrimination. OtherScientificTerm is predictive relationship. Method is graph neural networks. Metric is predictive accuracy. ","This paper studies the problem of dyadic fairness, i.e. fair link prediction in a graph. The authors propose a new algorithm, called FairAdj, to learn a fair adjacency matrix for link predictive scores, which is a weighted average of the fair link predictions for each edge in the graph. They show that the proposed algorithm is fair under certain assumptions on the structure of the graph, e.g., the number of edges in each graph, and the distance between edges. They also show that their method is fair in terms of the fairness-utility tradeoff. ",This paper studies the problem of dyadic fairness in graph-structured data. The authors propose a fair adjacency matrix (FairAdj) for the fair link prediction problem. They show that fair link predictive scores can be computed using the FairAdj matrix. They also show that the fair adj matrix can be used to mitigate discrimination in the case of link-based data.
306,SP:b614e9fbec58e9efa7722d2ec4a60fc93d210f92,"Autoencoders HYPONYM-OF information compression framework. generative ability FEATURE-OF it. generative ability FEATURE-OF autoencoder. Gaussian prior knowledge USED-FOR synthesis. Gaussian prior knowledge USED-FOR VAE. interpolation HYPONYM-OF exploration in latent space. disentangled representation CONJUNCTION regularization. regularization CONJUNCTION disentangled representation. regularization USED-FOR exploration in latent space. Disentangled Exploration Autoencoder ( DEAE ) USED-FOR controllable synthesis. regularization USED-FOR controllable synthesis. regularization USED-FOR Disentangled Exploration Autoencoder ( DEAE ). disentangled representation USED-FOR Disentangled Exploration Autoencoder ( DEAE ). encoder USED-FOR DEAE. encoder USED-FOR latent code space. directed interpolation USED-FOR encoder. directed interpolation USED-FOR latent code space. encoder USED-FOR latent representation. disentanglement FEATURE-OF latent representation. disentanglement CONJUNCTION exploration. exploration CONJUNCTION disentanglement. positive loop USED-FOR DEAE. exploration USED-FOR positive loop. disentanglement USED-FOR positive loop. DEAE USED-FOR attribute - controllable augmented samples. DEAE USED-FOR dataset bias. DEAE USED-FOR fairness problems. Method are GAN - based adversarial training, and decoder. OtherScientificTerm are latent code, disentangled latent code, and interpolated latent code. Generic is method. ","This paper proposes a disentangled exploration autoencoder (DEAE) for generating attribute-controllable augmented samples. The proposed method is based on disentangling the latent code representation of the encoder and the decoder. The encoder is trained using directed interpolation, and the disentanglement is enforced by a regularization term that encourages the encoders to explore the latent space. The authors show that DEAE is able to generate attribute-controlled augmented samples, which can be used to reduce the dataset bias.",This paper proposes a disentangled exploration autoencoder (DEAE) for generating controllable augmented samples. The disentanglement is achieved by disentangling the latent code representation of the encoder and the decoder. The encoder is trained with Gaussian prior knowledge (VAE) and a Gaussian-based adversarial training (GAN-based). The authors show that DEAE can generate attribute-controllable augmented data samples that are more robust to dataset bias.  
315,SP:c934adb14926a00ef9c73c9773cb0b3a2669921e,"Episodic and semantic memory PART-OF human memory model. serial event ( episodic memory ) USED-FOR compressed representation. Bayesian memory allocation scheme USED-FOR episodic and semantic memory. hierarchical latent variable model USED-FOR Bayesian memory allocation scheme. locally contiguous memory USED-FOR differentiable block allocated latent memory. locally contiguous memory USED-FOR Kanerva Machine. feed forward deterministic process USED-FOR it. binarized MNIST CONJUNCTION binarized Omniglot. binarized Omniglot CONJUNCTION binarized MNIST. allocation scheme USED-FOR memory conditional image generation. binarized MNIST FEATURE-OF conditional likelihood values. DMLab Mazes CONJUNCTION Celeb - A. Celeb - A CONJUNCTION DMLab Mazes. CIFAR10 CONJUNCTION DMLab Mazes. DMLab Mazes CONJUNCTION CIFAR10. Celeb - A CONJUNCTION ImageNet32×32. ImageNet32×32 CONJUNCTION Celeb - A. Method are complementary learning systems, and heap allocation. Task is memory writing. OtherScientificTerm is read key distribution. ","This paper proposes a Bayesian memory allocation scheme for episodic and semantic memory. The proposed allocation scheme is based on a hierarchical latent variable model with a differentiable block allocated latent memory. A feed-forward deterministic process is used to allocate the memory in a deterministic manner. Experiments are conducted on ImageNet32×32, Celeb-A, DMLab Mazes, and Omniglot.","This paper proposes a new Bayesian memory allocation scheme for episodic and semantic memory. The proposed method is based on a hierarchical latent variable model and a differentiable block allocated latent memory. It is shown that the proposed allocation scheme can be used for memory conditional image generation. Experiments are conducted on CIFAR-10, Celeb-A, and ImageNet32×32. "
324,SP:e63d7d8c581019e17585fb9c0eac33d6836e187d,deep learning models USED-FOR machine learning tasks. Attention mechanisms CONJUNCTION deep learning models. deep learning models CONJUNCTION Attention mechanisms. sample complexity CONJUNCTION loss landscape. loss landscape CONJUNCTION sample complexity. loss landscape FEATURE-OF attention - based neural networks. sample complexity FEATURE-OF attention - based neural networks. attention models COMPARE models. models COMPARE attention models. local minimum PART-OF attention model. sample complexity EVALUATE-FOR models. prediction error EVALUATE-FOR local minimum. sample complexity EVALUATE-FOR attention models. OtherScientificTerm is attention. Method is self - attention. ,"This paper studies the sample complexity of attention-based deep learning models. The authors show that, in contrast to previous work, the sample cost of self-attention is bounded by the local minimum of the prediction error of the attention model. They also show that the sample costs of attention models are bounded by their sample complexity.   The main contribution of the paper is a theoretical analysis of the sample complexities of attention based models. ","This paper studies the sample complexity of attention-based neural networks. The authors consider the problem of estimating the local minimum of an attention model, which is a measure of the number of samples needed to estimate the attention. They show that the sample size of the attention model depends on the loss landscape of the network, and that it depends on how much attention is used. They also provide a theoretical analysis of sample complexity for attention models.  "
333,SP:f739d199fdee26f09994e3f9487aec1eab0f2e89,Bayesian modeling USED-FOR Active inference. biologically plausible model USED-FOR Bayesian modeling. free energy principle CONJUNCTION prior preference. prior preference CONJUNCTION free energy principle. reinforcement learning ( RL ) algorithms USED-FOR active inference. negative value function USED-FOR EFE. method USED-FOR prior preference. prior preference CONJUNCTION theoretical connection. theoretical connection CONJUNCTION prior preference. theoretical connection USED-FOR method. active inference USED-FOR inverse RL. prior preference learning USED-FOR active inference. active inference USED-FOR inverse RL problem. prior preference learning USED-FOR inverse RL problem. EFE - based rewards USED-FOR active inference. OtherScientificTerm is expected free energy ( EFE ). ,"This paper proposes a Bayesian model for active inference in Bayesian reinforcement learning. The proposed model is based on the expected free energy (EF) model, which is a biologically plausible model for Bayesian inference. The authors show that the EFE model is a natural prior over the reward function, and show that it can be used to learn a reward function that maximizes the expected EFE. They then show that this reward function can be learned using a prior over a set of reward functions, and that this prior can be leveraged to improve the performance of the agent. ","This paper proposes a Bayesian approach to active inference for inverse RL. The main idea is to use the expected free energy (EFE) as a surrogate for the prior preference of the agent, which is a biologically plausible model for active inference. The authors show that the EFE-based rewards can be used to improve the performance of active inference in the inverse RL setting. They also provide a theoretical connection between EFE and prior preference learning. "
342,SP:5592b79e49eba95c15103a3348f2bde57b60f2ab,"data augmentation method USED-FOR generalization. data augmentation method USED-FOR adversarial and standard learning. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. CIFAR-100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-100. OOD data USED-FOR learning scenario. method COMPARE data augmentation methods. data augmentation methods COMPARE method. method COMPARE adversarial training. adversarial training COMPARE method. Method is neural networks. Generic is methods. Material are UID data, and image data. OtherScientificTerm are pseudo - labels, and undesirable features. ","This paper proposes a data augmentation method to improve the generalization performance of neural networks. The proposed method is based on pseudo-labeling, where pseudo-labels are generated from a subset of unlabeled data. The authors show that the proposed method can improve the performance on CIFAR-10/100 and ImageNet.  ","This paper proposes a data augmentation method to improve the generalization performance of neural networks. The proposed method is based on the notion of pseudo-labels, which are pseudo-labeled pseudo-features that are undesirable features that can be added to the training data. The authors show that the proposed method outperforms the state-of-the-art methods on CIFAR-10/100 and ImageNet datasets."
351,SP:3cac7a2c310165ed0de46d8e5546c3bfbd639158,"Fast Linearized Adaptive Policy ( FLAP ) HYPONYM-OF metareinforcement learning ( meta - RL ) method. shared linear representation of the policy USED-FOR FLAP. adapter network USED-FOR linear weights. adapter network USED-FOR policy. MAML HYPONYM-OF prior meta - RL methods. gradient descent USED-FOR meta - policy. adaptation run - time EVALUATE-FOR separate feed - forward network. FLAP COMPARE prior methods. prior methods COMPARE FLAP. continuous - control meta - RL benchmarks EVALUATE-FOR FLAP. adaptation run - time speeds EVALUATE-FOR prior methods. average return CONJUNCTION adaptation run - time speeds. adaptation run - time speeds CONJUNCTION average return. out - of - distribution tasks EVALUATE-FOR FLAP. average return EVALUATE-FOR FLAP. adaptation run - time speeds EVALUATE-FOR FLAP. Task are outof - distribution tasks, and adaptation. Generic are task, and tasks. Method is prior MetaRL methods. ","This paper proposes Fast Linearized Adaptive Policy (FLAP), a meta-RL method for continuous control tasks with out-of-distribution tasks. The main idea is to use a shared linear representation of the policy and a linear adapter network to learn the linear weights of the meta-policy. The adaptation run-time of FLAP is faster than that of a separate feed-forward network. The proposed method is evaluated on continuous control benchmarks and achieves better performance compared to prior methods. ",This paper proposes a new meta-RL method called Fast Linearized Adaptive Policy (FLAP) for continuous control metaRL. The main idea of FLAP is to use a shared linear representation of the meta-policy and adaptively learn the policy using a feed-forward network. The authors show that FLAP achieves faster adaptation run-time compared to MAML on continuous control and out-of-distribution tasks.
360,SP:21a1bd4ada0723c96c0dbf7a142a2faf5defa4e3,"communication efficiency FEATURE-OF algorithm. kernel k - means USED-FOR optimization problem. federated settings FEATURE-OF kernel k - means. federated settings USED-FOR optimization problem. communication efficient mech anism ( CEM ) USED-FOR communication cost. feder ated kernelk - means USED-FOR privacy preservation. matrix operations USED-FOR local computational results. federated kernel k - means COMPARE kernel k - means. kernel k - means COMPARE federated kernel k - means. clustering quality EVALUATE-FOR federated kernel k - means. clustering quality EVALUATE-FOR kernel k - means. communication cost EVALUATE-FOR DSPGD. O(1 / T ) rate FEATURE-OF DSPGD. CEM USED-FOR DSPGD. CEM USED-FOR DSPGD. communication cost EVALUATE-FOR federated kerne l k - means. clustering quality EVALUATE-FOR federated kerne l k - means. Method are federated kernel k - means algorithm, and kernelk - means. OtherScientificTerm are approximate solution, and cloud server. ","-k-means is an efficient federated learning algorithm for learning kernel k means in the federated setting. In this paper, the authors propose a communication efficient mech anism (CEM) to reduce the communication cost in federated settings. The authors show that CEM can be used to improve the communication efficiency of federated kernel k-Means. The main contribution of the paper is to propose a federated federated k means algorithm that achieves O(1/T) communication cost with O(\sqrt{T}) convergence rate. ",This paper proposes a new federated kernel k-means algorithm for federated optimization problems. The main idea is to use the mech anism (CEM) to reduce the communication cost between the server and client. The authors show that the proposed algorithm achieves O(1/T) communication efficiency with O(T/1) communication cost. They also show that their algorithm is more efficient than other federated kernels k-mean algorithms. 
369,SP:be568dd3fea51ce33a6d1e4b07dda5aee6342395,"hardware & latency constraints FEATURE-OF architectures. accuracy EVALUATE-FOR architectures. approach USED-FOR models. approach USED-FOR resource - intensive tasks. deployment targets USED-FOR resource - intensive tasks. CompOFA HYPONYM-OF design space. model search / extraction time COMPARE state of the art. state of the art COMPARE model search / extraction time. heuristics COMPARE state of the art. state of the art COMPARE heuristics. design space USED-FOR models. diversity of hardware and latency targets FEATURE-OF models. Method is CNNs. Metric are constant training cost, and complexity. Generic is cost. OtherScientificTerm are combinatorial explosion of sub - optimal model configurations, search space, training budget, search, accuracy - latency Pareto frontier, model dimensions, and Pareto optimality. Material is ImageNet. ","This paper studies the problem of finding the accuracy-latency Pareto frontier of a network with hardware and latency constraints. The authors propose a method to reduce the search cost of training a network in terms of the number of training points and the search time. The main idea is to design a search space with a fixed number of search points and a fixed amount of training time, and then use the search space to find the model dimensions that are the closest to the accuracy frontier of the network. The paper shows that this method is computationally efficient and can be applied to a wide range of hardware/latency constraints. ","This paper proposes a new design space, called CompOFA, for model search and extraction. It is motivated by the fact that the cost of training a model can be very high, and that it is hard to find a model that is Pareto-optimized. The authors propose a new search space, which they call ""CompOFA"" and show that it can be used to reduce the search time of a model. They also show that the search space can be extended to include a diversity of hardware and latency targets. They show that their method can be applied to a wide range of tasks."
378,SP:04b84d26cf282dbb753cbf27f14c334f65d3f8ec,Meta - learning USED-FOR model. limited data USED-FOR model. adversarial samples USED-FOR meta - learning. ADML ( ADversarial Meta - Learner ) USED-FOR initialization of a learning model. meta - learning algorithm USED-FOR initialization of a learning model. adversarial manner USED-FOR initialization of a learning model. ADML ( ADversarial Meta - Learner ) HYPONYM-OF meta - learning algorithm. clean and adversarial samples USED-FOR ADML ( ADversarial Meta - Learner ). meta - learning algorithms COMPARE it. it COMPARE meta - learning algorithms. it USED-FOR adversarial samples. it COMPARE meta - learning algorithms. meta - learning algorithms COMPARE it. MiniImageNet CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION MiniImageNet. accuracy CONJUNCTION robustness. robustness CONJUNCTION accuracy. ADML COMPARE representative meta - learning algorithms. representative meta - learning algorithms COMPARE ADML. attack mechanisms USED-FOR adversarial samples. image datasets EVALUATE-FOR ADML. MiniImageNet HYPONYM-OF image datasets. CIFAR100 HYPONYM-OF image datasets. Method is learning model. Material is clean samples. OtherScientificTerm is limited and even contaminated samples. ,"This paper proposes a meta-learning algorithm that uses adversarial samples to improve the robustness of a learning model in the presence of limited data. The proposed method, called ADML (Adversarial Meta-Learner), is based on the idea that the initialization of the learning model should be performed in an adversarial manner. The authors propose to use a combination of clean and adversarial data to train the model. The experiments show that the proposed method outperforms the baselines in terms of accuracy and robustness.","This paper proposes a new meta-learning algorithm, ADML (Adversarial Meta-Learner), which uses clean and adversarial samples for the initialization of a learning model. The authors show that ADML can be used to improve the robustness of the learning model against adversarial attacks. They also show that it can improve the accuracy and robustness in terms of the number of clean samples."
387,SP:dfbaa6b53c4e8328d52666ad4641fc917bf0c0b3,Error correction codes PART-OF communication applications. maximum likelihood rule USED-FOR decoding of transmitted codewords. permutation USED-FOR permutation decoding. data - driven framework USED-FOR permutation selection. node embedding CONJUNCTION self - attention. self - attention CONJUNCTION node embedding. domain knowledge CONJUNCTION machine learning concepts. machine learning concepts CONJUNCTION domain knowledge. domain knowledge PART-OF data - driven framework. machine learning concepts PART-OF data - driven framework. self - attention HYPONYM-OF machine learning concepts. node embedding HYPONYM-OF machine learning concepts. simulated Bose Chaudhuri Hocquenghem ( BCH ) code COMPARE baseline decoders. baseline decoders COMPARE simulated Bose Chaudhuri Hocquenghem ( BCH ) code. bit error rate EVALUATE-FOR simulated Bose Chaudhuri Hocquenghem ( BCH ) code. bit error rate EVALUATE-FOR baseline decoders. self - attention networks USED-FOR physical layer communication systems. Method is suboptimal decoding algorithms. Generic is algorithms. ,"This paper proposes a novel method to improve the performance of decoding of transmitted codewords in physical layer communication systems. The proposed method is based on a data-driven framework, where a maximum likelihood rule is used to select the permutation to be used for decoding. The authors show that the proposed method outperforms the baselines in terms of the bit error rate. ","This paper proposes a data-driven framework to improve the performance of decoder-decoder decoders in physical layer communication systems. The proposed method is based on Bose-Chaudhuri-Hocquenghem (BCH) code, which is a variant of the BCH code. The authors show that the proposed method outperforms the baselines in terms of bit error rate and accuracy."
396,SP:c860a7b0952d708e7851c9bc4b63d246f64d1cba,"fine - tuning BERT USED-FOR text classification task. unsupervised classification task USED-FOR task. finetuning USED-FOR task. unsupervised classification task EVALUATE-FOR finetuning. unsupervised clustering USED-FOR intermediate task. labeled examples USED-FOR topical classification tasks. classification step USED-FOR topical classification tasks. classification step USED-FOR labeled examples. Material are labeled data, and data sets. Method is BERT. ", classification task is a well-studied text classification task. This paper proposes to fine-tune BERT on this task by using unsupervised clustering. The proposed method is shown to improve the performance of BERT in terms of classification accuracy.   ,"This paper proposes a novel method for fine-tuning BERT for unsupervised text classification task. The proposed method is based on the notion of ""topical classification"", where the task is a topical classification task and the data set is a set of labeled examples. The authors show that the proposed method can be used to fine-tune BERT on the task of topical classification. They also show that their method is able to improve the performance of BERT in terms of performance on a number of tasks. "
405,SP:ea37f5882fd98dd4ce233077bb3069517d4ed4ea,"fixed ( random shooting ) control agent USED-FOR generative models. mixture density nets COMPARE models. models COMPARE mixture density nets. they COMPARE probabilistic counterparts. probabilistic counterparts COMPARE they. deterministic models COMPARE probabilistic counterparts. probabilistic counterparts COMPARE deterministic models. heteroscedasticity USED-FOR regularizer. them USED-FOR control problem. sample complexity EVALUATE-FOR MBRL. framework USED-FOR MBRL. sample complexity EVALUATE-FOR framework. Acrobot EVALUATE-FOR MBRL. training schedule USED-FOR MBRL. OtherScientificTerm are multimodal posterior predictives, multimodality, and probabilistic posterior predictives. ","This paper studies the problem of learning a generative model for a random shooting robot with a fixed control agent. The authors propose to use a mixture density network to model the distribution over the environment, which they call Multi-Modal Bayesian Random Robotic Learning (MBRL). The authors show that MBRL can be viewed as an extension of Bayesian generative models with heteroscedasticity, where the distribution is modelled by a mixture of two distributions, one of which is a deterministic distribution and the other is a probabilistic distribution. They show that the distribution of the two distributions can be approximated by the same mixture of distributions, and they show that this can be used as a regularizer to regularize the training of the model. They also show that their method is able to achieve a sample complexity of $O(1/\sqrt{T})$ for a given training schedule. ",This paper studies the problem of multi-modal generative model-based random shooting (MBRL) with a fixed control agent. The authors propose a new framework for MBRL with heteroscedasticity regularization. They show that this regularization can be used to improve the sample complexity of MBRL. They also show that the proposed method can be applied to a real-world robot control problem. 
414,SP:4e25ba3714d78ba59a0d8efbb65e0ef5201702f8,"Affine Disentangled GAN ( ADIS - GAN ) HYPONYM-OF Generative Adversarial Network. affine regularizer USED-FOR inductive bias. affine transformation properties of images USED-FOR affine regularizer. transformation matrices PART-OF affine matrix. maximum likelihood estimation USED-FOR transformation parameters. horizontal and vertical zoom CONJUNCTION horizontal and vertical skew. horizontal and vertical skew CONJUNCTION horizontal and vertical zoom. horizontal and vertical skew CONJUNCTION horizontal and vertical translation. horizontal and vertical translation CONJUNCTION horizontal and vertical skew. rotation CONJUNCTION horizontal and vertical zoom. horizontal and vertical zoom CONJUNCTION rotation. rotation CONJUNCTION horizontal and vertical skew. horizontal and vertical skew CONJUNCTION rotation. disentangled representations COMPARE features. features COMPARE disentangled representations. ADIS - GAN USED-FOR features. approaches USED-FOR disentangled representations. horizontal and vertical translation HYPONYM-OF transformations. rotation HYPONYM-OF transformations. horizontal and vertical skew HYPONYM-OF transformations. horizontal and vertical zoom HYPONYM-OF transformations. ADIS - GAN USED-FOR features. MNIST, CelebA, and dSprites datasets EVALUATE-FOR ADIS - GAN. MNIST, CelebA, and dSprites datasets EVALUATE-FOR features. OtherScientificTerm is affine transformations. Method is InfoGAN. ","This paper proposes an affine disentanglement method based on the affine transformation properties of images. The proposed method is based on a GAN-based affine adversarial network (ADIS-GAN) that is trained to generate disentangled representations. The main idea of the method is to use a regularization term to encourage the transformation matrices to be close to the true affine matrix of the input image.   The method is evaluated on MNIST, CelebA, and dSprites datasets and achieves state-of-the-art performance.","This paper proposes an affine disentangled GAN (ADIS-GAN) model for disentanglement. The main idea is to use the affine transformation properties of images as a regularizer to improve the disentangling performance of the GAN. The authors propose to use maximum likelihood estimation to estimate the transformation matrices of the image. They show that the proposed model outperforms the state-of-the-art on MNIST, CelebA and dSprites datasets."
423,SP:121f8420cfb49c6d80b5ebb4051e85947182594a,"contrastive learning methods COMPARE supervised learning counterparts. supervised learning counterparts COMPARE contrastive learning methods. contrastive learning methods USED-FOR Representation learning. data augmentations USED-FOR methods. augmentations USED-FOR instance discrimination - based contrastive learning. fully supervised upper bound USED-FOR unsupervised learning. distribution divergence USED-FOR retrieval of strongly augmented queries. augmentations USED-FOR contrastive loss. ResNet-50 architecture CONJUNCTION single - layer classifier fine - tuned. single - layer classifier fine - tuned CONJUNCTION ResNet-50 architecture. ImageNet EVALUATE-FOR ResNet-50 architecture. top-1 accuracy EVALUATE-FOR method. ImageNet EVALUATE-FOR method. fully supervised ResNet-50 USED-FOR top-1 accuracy. it COMPARE self - supervised and supervised methods. self - supervised and supervised methods COMPARE it. self - supervised and supervised methods USED-FOR transfer learning and object detection tasks. transfer learning and object detection tasks EVALUATE-FOR it. Metric is generalizability. OtherScientificTerm are distortions, image structures, representation bank, overoptimistic assumption, distorted visual structures, and distributions of weakly augmented counterparts. ",This paper proposes a novel contrastive learning method for image representation learning. The proposed method is based on the observation that strongly augmented queries are more informative than weakly augmented queries. The authors then propose to use the distribution divergence between the distributions of strongly augmented and weakly-augmented data to improve the generalization performance of the proposed method. The method is evaluated on image classification tasks and transfer learning tasks.   ,This paper proposes a new contrastive learning method for representation learning with data augmentations. The main idea is to use a fully supervised upper bound for unsupervised learning to improve the generalizability of the learned representations. The proposed method is based on the distribution divergence between strongly augmented queries and weakly augmented queries. The authors show that the proposed method outperforms the state-of-the-art in terms of transfer learning and object detection accuracy on ImageNet.
432,SP:af54e542223097c315ecd677d0b968e9a0b2a1d4,"magnetic resonance imagery ( MRI ) USED-FOR De - identification. de - identification methods USED-FOR task. MRI de - identification techniques USED-FOR privacy - sensitive facial features. removal - based techniques COMPARE deep learning framework. deep learning framework COMPARE removal - based techniques. segmentation CONJUNCTION age prediction. age prediction CONJUNCTION segmentation. deep learning framework USED-FOR medical analyses. medical analyses FEATURE-OF brain. age prediction HYPONYM-OF medical analyses. segmentation HYPONYM-OF medical analyses. Material are database, and patient ’s MRI scan. Generic are they, and them. OtherScientificTerm is 3D volume. ",This paper proposes a method to perform MRI de-identification using deep neural networks. The proposed method is based on the observation of a patient’s 3D MRI scan and the 3D volume of their 3D space. The method is trained using a neural network architecture that is trained on a large database of MRI scans. The model is trained to predict the age of the patient based on their 3d volume.    The main contribution of this paper is to propose a method that uses a deep neural network to perform de-identified MRI images. ,"This paper proposes a new method for de-identifying patients from MRI scans. The proposed method is based on a deep learning framework that learns to segment a patient’s 3D volume from a 3D MRI scan. The method is applied to a variety of medical analyses, including age prediction, segmentation, and privacy-sensitive facial features. The results show that the proposed method outperforms the state-of-the-art in terms of accuracy."
441,SP:0ac3964bd2320341488476d60f57b75d2a79f92c,Graph neural networks USED-FOR modeling graph data. Graph neural networks USED-FOR node classification and link prediction tasks. representation USED-FOR graph. pooling function USED-FOR node representations. pooling function USED-FOR compact form. pooling function USED-FOR representation. task relevance CONJUNCTION structural dependencies. structural dependencies CONJUNCTION task relevance. hierarchical graph pooling methods USED-FOR representation. representation USED-FOR graphs. Weisfeiler - Lehman test FEATURE-OF graphs. Graph Multiset Transformer ( GMT ) HYPONYM-OF multi - head attention based global pooling layer. graph structure FEATURE-OF auxiliary information. auxiliary information FEATURE-OF multiset encoding problem. multiset encoding problem USED-FOR graph pooling problem. injectiveness CONJUNCTION permutation invariance. permutation invariance CONJUNCTION injectiveness. it COMPARE Weisfeiler - Lehman graph isomorphism test. Weisfeiler - Lehman graph isomorphism test COMPARE it. injectiveness FEATURE-OF GMT. permutation invariance FEATURE-OF GMT. node clustering approaches USED-FOR hierarchical graph pooling. methods USED-FOR hierarchical graph pooling. methods USED-FOR node clustering approaches. GMT COMPARE graph pooling methods. graph pooling methods COMPARE GMT. graph classification benchmarks EVALUATE-FOR graph pooling methods. memory and time efficiency EVALUATE-FOR graph pooling methods. graph classification benchmarks EVALUATE-FOR GMT. memory and time efficiency EVALUATE-FOR GMT. Material is graph data. OtherScientificTerm is node features. Generic is they. ,This paper proposes a multi-head attention-based global pooling layer for hierarchical graph pooling. The proposed method is based on the Weisfeiler-lehman test for graph isomorphism. Theoretical results show that the proposed method achieves better performance than existing methods in terms of injectiveness and permutation invariance. Experiments are conducted on several graph classification and link prediction tasks. ,"This paper proposes a multi-head attention based global pooling layer for hierarchical graph pooling. The proposed method is based on the Weisfeiler-Lehman graph isomorphism test, which is used to measure the mutual information between two graphs. The authors show that the proposed method can be used to improve the performance of existing hierarchical pooling methods. The main contribution of the paper is to propose a new multiset encoding problem for the task of pooling graphs. "
450,SP:76848e7ac3e6709e92f6a6db60269cb5177495d1,"GNNs USED-FOR prediction task. long - range interaction USED-FOR prediction task. tuning CONJUNCTION weights. weights CONJUNCTION tuning. GCN CONJUNCTION GIN. GIN CONJUNCTION GCN. over - squashing FEATURE-OF GNNs. GNNs COMPARE GAT. GAT COMPARE GNNs. GAT CONJUNCTION GGNN. GGNN CONJUNCTION GAT. GNNs USED-FOR over - squashing. GNNs COMPARE GGNN. GGNN COMPARE GNNs. bottleneck USED-FOR GNNs. GIN HYPONYM-OF GNNs. GCN HYPONYM-OF GNNs. Method are graph neural network ( GNN ), and GNN models of long - range problems. OtherScientificTerm are graph, exponentially growing information, fixed - size vectors, long - range signals, and incoming edges. ","This paper studies the problem of over-squashing in graph neural networks (GNNs) in the presence of long-range interactions. The authors show that GNNs are prone to over-smoothing in this setting, and propose a new GNN architecture called GGNN to address this issue. They also show that GGNNs can be used to improve the performance of GATs in the long range setting.","This paper studies the problem of over-squashing of graph neural networks (GNNs) in the context of long-range prediction. The authors show that GNNs can be over-represented in the long range prediction task, where the input graph is a fixed-size vector with a fixed number of edges, and the output graph is an exponentially growing graph. They also show that the GNN can be under-squashed in this setting. They show that GGNN, GCN, GAT, and GIN can all be oversquashed, and that GAT can be outperformed by GGNN."
459,SP:90d8fa381446923902e42b259392e5e975e6caa1,"cross - domain generalizable classifiers USED-FOR methods. methods USED-FOR domain - agnostic representations. annotated data USED-FOR classifier. embedding space USED-FOR domain - agnostic. data distributions USED-FOR domain - agnostic. Task are Sentiment analysis, and marketing strategies. Method are cross - domain sentiment analysis methods, and domain adaptation method. OtherScientificTerm are data annotation, and prototypical distribution. Generic is method. ","This paper studies the problem of domain generalization in sentiment analysis. The authors propose a domain adaptation method for sentiment analysis, where the goal is to learn a domain-agnostic representation that is generalizable across multiple domains. The proposed method is based on the observation that existing methods do not generalize well across different domains. To this end, the authors propose to use the prototypical distribution of the data to train a classifier that can generalize across domains.   ","This paper proposes a new method for cross-domain generalization of sentiment analysis methods. The main idea is to use the prototypical distribution of the data to learn representations that are domain-agnostic and generalizable. The proposed method is based on the notion of prototypical data distribution, which is defined as a set of data distributions that can be used to train a classifier that is generalizable across different domains. The authors show that the proposed method can be applied to a wide range of datasets, and that it can generalize across different datasets."
468,SP:893fd7440b82f5da0d4c0944928810322eaee2f0,Gender - bias stereotypes PART-OF natural language processing. genderbias FEATURE-OF natural language understanding. evaluation of genderbias PART-OF natural language understanding. inference USED-FOR natural language understanding. inference USED-FOR evaluation of genderbias. gender neutral premise COMPARE gender - specific hypothesis. gender - specific hypothesis COMPARE gender neutral premise. NLI models USED-FOR gender stereotypes. challenge task USED-FOR NLI models. occupations USED-FOR NLI models. BERT CONJUNCTION RoBERTa. RoBERTa CONJUNCTION BERT. RoBERTa CONJUNCTION BART. BART CONJUNCTION RoBERTa. models USED-FOR genderinduced prediction errors. BERT CONJUNCTION BART. BART CONJUNCTION BERT. MNLI and SNLI data - sets USED-FOR models. BART HYPONYM-OF models. BERT HYPONYM-OF models. RoBERTa HYPONYM-OF models. Generic is evaluation methodology. Method is debiasing techniques. Material is gender - balanced dataset. ,"This paper studies the impact of gender bias on the performance of NLI models on the task of predicting gender stereotypes. The authors propose a challenge task to evaluate the effect of gender biases on NLI model performance on a set of two datasets: MNLI and SNLI. They show that gender bias does not affect performance on MNLI, but on SNLI, it does affect the performance on BERT and BART. They also show that BERT is more sensitive to gender bias than BART and RoBERTa.",This paper proposes a new evaluation methodology for evaluating gender bias in NLI models. The authors propose a challenge task to evaluate the gender bias of NLI-based models on the MNLI and SNLI data-sets. The challenge is to evaluate whether a model is biased towards one gender or the other. The evaluation is based on the assumption that the data-set is gender-balanced and that the model does not make gender-induced prediction errors. The paper also proposes a debiasing technique to evaluate gender bias.
477,SP:a32ab755bd249c393b70938036ce8e810c0c439f,"variational intrinsic control ( VIC ) HYPONYM-OF unsupervised reinforcement learning method. other HYPONYM-OF VIC algorithms. one HYPONYM-OF VIC algorithms. intrinsic reward USED-FOR latter. transitional probability model CONJUNCTION Gaussian mixture model. Gaussian mixture model CONJUNCTION transitional probability model. transitional probability model USED-FOR methods. Gaussian mixture model USED-FOR methods. OtherScientificTerm are intrinsic options, and stochastic environments. ","This paper proposes a variant of Variational Intrinsistance Control (VIC), a reinforcement learning method that uses intrinsic reward as a reward function to encourage the agent to explore the environment. The paper proposes two variants of VIC methods, one based on a Gaussian mixture model and the other on a transition probability model. The main contributions of the paper are:  1. The authors show that the proposed method is equivalent to the original VIC method in the sense that the transition probability is the same as that of the original method.  2. They show how the transition probabilities of the two methods can be approximated by Gaussian Mixture Models (GMMs).  3. They provide a theoretical analysis of the performance of the proposed methods. ",This paper proposes a new variant of Variational Intrinsic Control (VIC) method for unsupervised reinforcement learning. The authors propose a new VIC algorithm that uses a Gaussian mixture model to model the intrinsic reward of the agent. They show that the proposed method outperforms the state-of-the-art VIC algorithms in terms of intrinsic reward. 
486,SP:b4df2c4627a6d46c5100133e38c4bea20b296dd8,Deep neural networks USED-FOR image classification. low data regime FEATURE-OF sample efficiency. ensemble of relatively small deep networks USED-FOR image classification problems. neural ensembling USED-FOR small data domains. technique COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE technique. deep ensembling COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE deep ensembling. deep ensembling HYPONYM-OF technique. Generic is they. Material is small datasets. Method is ensemble configurations. OtherScientificTerm is losses. ,This paper proposes to use ensemble of relatively small deep networks to solve image classification problems in the low-data regime. The proposed method is based on the observation that the performance of deep neural networks can be improved by using a large number of small networks. The authors show that the proposed method can improve the sample efficiency of image classification models on small datasets.   ,"This paper studies the problem of sample efficiency of ensemble of relatively small deep networks in the low-data regime. The authors propose a new technique called deep ensembling to improve sample efficiency in this setting. The proposed method is based on the notion of ensemble configurations, where each ensemble is composed of a small number of small neural networks, and each network is trained in a different ensemble configuration. They show that the proposed method outperforms state-of-the-art approaches in terms of the sample efficiency."
495,SP:4a0ee01f4897efa81659f37ef0468ee8195bbc4f,computational power and storage requirements CONJUNCTION processing speed. processing speed CONJUNCTION computational power and storage requirements. DNN - based applications USED-FOR InternetOf - Things ( IoT ) devices. them USED-FOR DNN - based applications. quantized networks COMPARE Binary Neural Networks ( BNNs ). Binary Neural Networks ( BNNs ) COMPARE quantized networks. speed - up EVALUATE-FOR Binary Neural Networks ( BNNs ). fixed and limited compression factor FEATURE-OF they. positive 0/1 binary weights COMPARE -1/+1 weights. -1/+1 weights COMPARE positive 0/1 binary weights. Sparse Binary Neural Networks HYPONYM-OF model and training scheme. -1/+1 weights COMPARE binary networks. binary networks COMPARE -1/+1 weights. sparsity FEATURE-OF BNNs. positive 0/1 binary weights USED-FOR sparsity. compression factor EVALUATE-FOR method. MNIST and CIFAR-10 datasets USED-FOR linear and convolutional networks. linear and convolutional networks EVALUATE-FOR method. it USED-FOR DNNs. compression rates CONJUNCTION generalization. generalization CONJUNCTION compression rates. generalization EVALUATE-FOR SBNNs. compression rates EVALUATE-FOR SBNNs. Method is Quantized neural networks. Metric is accuracy. Material is limited resources. ,"This paper proposes a method to reduce the computational cost of quantized neural networks (BNNs) by reducing the number of positive 0/1 binary weights. The proposed method is based on the observation that BNNs have a fixed and limited compression factor due to the fact that they are limited to only positive 1/2 binary weights, while quantized networks have positive 0-1/2 weights.  The authors propose a new training scheme, called Sparse Binary Neural Networks (SBNN), which uses positive-0/1 weights instead of positive 1-1-1 weights for training. The authors show that the proposed method can reduce the compression factor by a factor of up to 1.5x compared to the standard quantized BNN. They also show that SBNNs can achieve better compression rates and generalization performance compared to BNN on MNIST and CIFAR-10.","This paper proposes a new training method, Sparse Binary Neural Networks (SBNN), to improve the performance of quantized neural networks (BNNs). The main idea is to use positive 0/1 binary weights instead of negative 1/1 weights in the training of BNNs. The proposed method is evaluated on MNIST and CIFAR-10 datasets, and it outperforms the state-of-the-art in terms of compression and generalization."
504,SP:5be8539ad02595ad3c7a2d7afe8cbb3e9924467d,"OOD data USED-FOR model calibration. outlier exposure USED-FOR model probabilities. outlier exposure USED-FOR method. estimates of class membership probabilities USED-FOR model predictions. baseline method USED-FOR predictive uncertainty. softmax probabilities USED-FOR model. softmax probabilities USED-FOR baseline method. softmax probabilities USED-FOR predictive uncertainty. Stochastic Variational Bayesian Inference ( SVBI ) USED-FOR deep learning. model ensembles CONJUNCTION Stochastic Variational Bayesian Inference ( SVBI ). Stochastic Variational Bayesian Inference ( SVBI ) CONJUNCTION model ensembles. Stochastic Variational Bayesian Inference ( SVBI ) HYPONYM-OF approaches. temperature scaling HYPONYM-OF approaches. model ensembles HYPONYM-OF approaches. predicted error rates CONJUNCTION actual error rates. actual error rates CONJUNCTION predicted error rates. calibration error FEATURE-OF methods. Metric is accuracy. Task are Predictive uncertainty, and PREDICTIVE UNCERTAINTY. Generic are models, and measures. Method are post hoc calibration method, machine learning model, and Uncertainty estimates. Material is corrupted data. OtherScientificTerm are class membership probabilities, model outputs, pmax, and Brier score. ","This paper studies the problem of post hoc calibration of predictive uncertainty in machine learning models. The authors propose a baseline method to calibrate the predictive uncertainty by estimating the class membership probabilities of the model predictions from corrupted data. They show that the calibration error of the proposed method is lower than that of the baseline method, which estimates the softmax probabilities for the model. They also show that temperature scaling, model ensembles, and stochastic variational Bayesian inference (SVBBI) can be used to improve the calibration performance.  ","This paper proposes a method for post hoc calibration of deep learning models. The main idea is to estimate the softmax probabilities of the class membership probabilities of a model, and then use this estimate to calibrate the model. The authors show that the calibration error of the proposed method is lower than the predicted error rate of the model and the actual error rate. They also show that their method is more accurate than the baseline method. "
513,SP:ea503f67e38fce7dee9cc4996b55b8959911f030,Graph neural networks CONJUNCTION graph kernels. graph kernels CONJUNCTION Graph neural networks. graph kernels USED-FOR machine learning problems. Graph neural networks USED-FOR machine learning problems. graphs USED-FOR machine learning problems. graph neural networks CONJUNCTION graph kernels. graph kernels CONJUNCTION graph neural networks. approaches USED-FOR graph properties. approaches USED-FOR non - isomorphic graphs. graph representations USED-FOR similarity / distance of graphs. graph neural networks CONJUNCTION graph kernels. graph kernels CONJUNCTION graph neural networks. expressive power EVALUATE-FOR graph kernels. expressive power FEATURE-OF graph neural networks. algorithms COMPARE those. those COMPARE algorithms. graph representations and similarities COMPARE those. those COMPARE graph representations and similarities. algorithms USED-FOR graph representations and similarities. models CONJUNCTION kernels. kernels CONJUNCTION models. node attributes USED-FOR kernels. node attributes USED-FOR models. graph kernels COMPARE graph neural networks. graph neural networks COMPARE graph kernels. ,"This paper studies the expressive power of graph neural networks (GNNs) and graph kernels (GPKs) in the context of non-isomorphic graphs. In particular, the authors show that GNNs and GPKs can be viewed as a combination of GNN models and kernels, where the GNN is used to model the similarity between two graphs, and the GPK is used as a model to predict the distance between the two graphs. The authors also show that graph kernels can be seen as a special case of graph NNs, and show that they can be used to learn graph representations.   ",This paper studies the expressive power of graph neural networks (GNNs) and graph kernels (graph kernels) in the context of machine learning. The authors propose a new algorithm for learning graph representations and similarities between graphs. They show that GNNs and kernels can be expressive in terms of the similarity of graphs and the distance between them. They also show that graph kernels are expressive in the sense that they can learn the similarity between two graphs.   
522,SP:0cf7b7d929f50e0b7f4fda5e1f68e5ade2f7c29b,"inpainting USED-FOR warping artifacts. data augmentation techniques USED-FOR regularizing non - warp - based image generation. them USED-FOR image animation. difficulty of inpainting FEATURE-OF warped image. CutMix HYPONYM-OF data augmentation techniques. PriorityCut USED-FOR image animation. augmentation approach USED-FOR image animation. PriorityCut HYPONYM-OF augmentation approach. low - level similarity CONJUNCTION keypoint distance. keypoint distance CONJUNCTION low - level similarity. keypoint distance CONJUNCTION feature embedding distance. feature embedding distance CONJUNCTION keypoint distance. pixel - wise difference CONJUNCTION low - level similarity. low - level similarity CONJUNCTION pixel - wise difference. PriorityCut COMPARE vanilla CutMix. vanilla CutMix COMPARE PriorityCut. PriorityCut COMPARE image animation models. image animation models COMPARE PriorityCut. PriorityCut USED-FOR identity. vanilla CutMix COMPARE image animation models. image animation models COMPARE vanilla CutMix. low - level similarity EVALUATE-FOR image animation models. pixel - wise difference EVALUATE-FOR image animation models. inpainting USED-FOR warping artifacts. PriorityCut USED-FOR regularize discriminator predictions. occlusion information USED-FOR regularize discriminator predictions. regularize discriminator predictions USED-FOR inpainting. occlusion information USED-FOR image animation. occlusion information USED-FOR PriorityCut. Method are Image animation, Self - supervised image animation approaches, self - supervised image animation approaches, and Warp - based image animation. OtherScientificTerm are motion of a driving video, pose references, motion of the driving video, pose differences, guidance, inpainted regions, motion of the driving image, smooth transitions, and mixture of context. Task is learning. ","This paper proposes a method for self-supervised image animation based on self-inpainting. The proposed method is based on the idea that the difficulty of inpainting in a warped image can lead to poor image generation. To address this issue, the authors propose a data augmentation method called PriorityCut, which is a data-augmented version of CutMix. The main idea is to use the low-level similarity between two points in an image to improve the quality of the generated images. The method is evaluated on a driving video dataset and compared with a number of state-of-the-art methods.  ","This paper proposes a new data augmentation method for image animation, called PriorityCut, which aims to improve the performance of self-supervised image animation by reducing the inpainting of warped images. The key idea is to use the low-level similarity between two images and use the keypoint distance between the two images as a discriminator to predict which regions of the image should be inpainted and which regions should be left untouched. The proposed method is evaluated on a driving video dataset, and compared with the state-of-the-art. "
531,SP:60b535fc6cbc1a7a26ad53f706ebb17de346dc4f,"approaches USED-FOR disentangled representations. data generation process USED-FOR independent latent variables. independent causal mechanisms ( ICM ) COMPARE disentangled representations. disentangled representations COMPARE independent causal mechanisms ( ICM ). coarse granularity FEATURE-OF data generation processes ( mechanisms ). observational data USED-FOR groundtruth mechanisms. unconventional mixture prior USED-FOR self - supervised generative model. self - supervised generative model USED-FOR mechanisms. mechanisms PART-OF self - supervised scenario. intervention CONJUNCTION covariant shift. covariant shift CONJUNCTION intervention. covariant shift CONJUNCTION noise. noise CONJUNCTION covariant shift. downstream tasks EVALUATE-FOR approach. approach COMPARE disentangled representations. disentangled representations COMPARE approach. approach USED-FOR intervention. covariant shift USED-FOR approach. noise EVALUATE-FOR approach. downstream tasks EVALUATE-FOR disentangled representations. downstream tasks EVALUATE-FOR approach. Generic are model, and methods. OtherScientificTerm is disentanglement. ","This paper proposes a self-supervised generative model for learning disentangled latent variables from observational data. The authors propose to learn independent causal mechanisms (ICM), which are independent latent variables that are independent of the data generation process. They use an unconventional mixture prior to learn the mechanisms. They show that the proposed method outperforms the state-of-the-art disentanglement methods on a variety of downstream tasks.","This paper proposes a method for generating disentangled representations from data generated by a self-supervised generative model. The method is based on a mixture prior, which is used to learn the groundtruth mechanisms from observational data. The authors show that the proposed method outperforms the state-of-the-art disentanglement methods on a variety of downstream tasks. "
540,SP:44d4e24428d043a69b40013919cda0e8e7bff99c,"2D image USED-FOR molecular graph structure ( W ). graph aligning approach USED-FOR rich or detailed labels. 2D images USED-FOR chemical compound graphs. domain adaptation COMPARE pretrained model. pretrained model COMPARE domain adaptation. domain adaptation USED-FOR model. Maybridge data set EVALUATE-FOR self - labeling approach. Task are machine learning, and predicting chemical compound graphs. Method are mediating representation V, and machine learning model. OtherScientificTerm are f, normal labels W, fully mediating layer, and mediating layer. ",This paper proposes a self-labeling approach for predicting the molecular graph structure (W) from a 2D image. The proposed approach is based on a graph aligning approach that uses a graph alignment approach to generate rich or detailed labels. The authors show that the proposed approach outperforms the state-of-the-art on the Maybridge data set. ,"This paper proposes a self-labeling method for predicting chemical compound graphs from 2D images. The proposed method is based on a graph aligning approach, where the graph is represented by a graph representation f, and the label W is represented as a representation V. The authors show that the proposed method outperforms the state-of-the-art domain adaptation method on the Maybridge dataset."
549,SP:ad906dd9a176cffd283593321ff6b9ad19595528,domain knowledge based deep learning framework USED-FOR chiller plants energy optimization problems. image classification CONJUNCTION NLP. NLP CONJUNCTION image classification. deep network USED-FOR realworld physical systems. NLP HYPONYM-OF deep learning. image classification HYPONYM-OF deep learning. methods USED-FOR complex systems. methods USED-FOR linear model. linear model USED-FOR complex systems. deep network USED-FOR nonlinear model. domain knowledge USED-FOR deep network. domain knowledge USED-FOR nonlinear model. redundancy function space FEATURE-OF nonlinear model. domain knowledge USED-FOR small sample size problem. energy consumption estimation FEATURE-OF chillers. input - output monotonic problem USED-FOR energy consumption estimation. monotonic constraints FEATURE-OF Neural Network. framework COMPARE ones. ones COMPARE framework. framework USED-FOR energy optimization. method COMPARE ones. ones COMPARE method. data center FEATURE-OF cooling system. cooling system EVALUATE-FOR method. ,"This paper proposes a domain knowledge based deep learning framework for chiller plants energy optimization problems. The proposed method is based on a monotonic input-output monotonicity problem, where the goal is to estimate the energy consumption of a set of chillers in a chiller plant. To solve this problem, the authors propose to learn a nonlinear model of the system using domain knowledge. The authors show that the proposed method outperforms the state-of-the-art methods in terms of energy consumption estimation.   ","This paper proposes a domain knowledge based deep learning framework for chiller plants energy optimization problems. The authors propose to use a monotonic input-output monotonicity problem to estimate the energy consumption of a chiller plant. They use a nonlinear model to model the input and output of the chiller, and then use a linear model for the output. They show that the proposed method outperforms the state-of-the-art in terms of energy consumption estimation. "
558,SP:6cb65ee5d2926858570601eeeade24fe86c7f32f,retail forecasting CONJUNCTION urban traffic forecasting. urban traffic forecasting CONJUNCTION retail forecasting. weather forecasts CONJUNCTION retail forecasting. retail forecasting CONJUNCTION weather forecasts. spatio - temporal predictions USED-FOR real - world applications. spatio - temporal predictions USED-FOR large - scale systems. weather forecasts HYPONYM-OF real - world applications. urban traffic forecasting HYPONYM-OF real - world applications. retail forecasting HYPONYM-OF real - world applications. methods USED-FOR predicting variables. interpretability EVALUATE-FOR forecasting models. methods USED-FOR forecasting models. collaborative causal spatio - temporal fusion transformer USED-FOR collaborative causal effects of predictors. collaborative causal effects of predictors USED-FOR forecasting targets. CausalTrans HYPONYM-OF collaborative causal spatio - temporal fusion transformer. causal attention USED-FOR causal inference. nodes PART-OF graph. Taylor ’s expansion CONJUNCTION softmax. softmax CONJUNCTION Taylor ’s expansion. time complexity EVALUATE-FOR multi - head attention. softmax USED-FOR multi - head attention. Taylor ’s expansion USED-FOR multi - head attention. time efficiency EVALUATE-FOR CausalTrans. model components USED-FOR CausalTrans. time efficiency EVALUATE-FOR model components. error reduction EVALUATE-FOR baseline methods. CausalTrans framework COMPARE baseline methods. baseline methods COMPARE CausalTrans framework. error reduction EVALUATE-FOR CausalTrans framework. Material is ride - sharing platforms. Method is spatial graph fusion mechanism. ,"This paper proposes a novel method for spatio-temporal prediction in large-scale systems. The proposed method is based on a spatial graph fusion mechanism, which is able to capture the causal effects of predictors on the forecasting targets. The authors propose to use a multi-head attention mechanism for the causal inference, where each head is responsible for predicting a set of nodes in the spatial graph, and each node is associated with a prediction target.   The authors show that the proposed method achieves better performance compared to the baselines in terms of error reduction and time efficiency. ","This paper proposes a collaborative causal spatio-temporal fusion transformer (CausalTrans) to improve the interpretability of forecasting models. The proposed method is based on a spatial graph fusion mechanism, where each node in the spatial graph is represented as a set of nodes in a graph, and each node is represented by a pair of predictors. The authors propose a multi-head attention method, which is a combination of Taylor’s expansion and softmax. They show that the proposed method can reduce the time complexity of the forecasting model by a factor of two."
567,SP:223980a1954d626d90ff54d8dc61b5d85a6b349c,"unsupervised framework USED-FOR problem. coupled mixture VAE ( cpl - mixVAE ) HYPONYM-OF unsupervised framework. interacting autoencoding agents USED-FOR unsupervised framework. variational inference problem USED-FOR it. categorical assignments EVALUATE-FOR approach. MNIST and dSprites EVALUATE-FOR approach. approach USED-FOR type - specific, activity - regulated genes. type - specific, activity - regulated genes PART-OF single - cell gene expression dataset. cortical neuron types FEATURE-OF single - cell gene expression dataset. single - cell gene expression dataset EVALUATE-FOR approach. OtherScientificTerm are mixture of discrete and continuous factors of variability, and continuous factors. Method are mixture representations, and multi - agent framework. ","This paper proposes an unsupervised framework for learning a mixture of discrete and continuous factors of variability. The proposed method is based on a mixture VAE (cpl-mixVAE) with interacting autoencoding agents. The authors show that the proposed method achieves state-of-the-art performance on the MNIST and dSprites datasets. They also show that their method can identify type-specific, activity-regulated genes in the single-cell gene expression dataset.","This paper proposes a multi-agent unsupervised mixture VAE (cpl-mixVAE) framework for the problem of learning the mixture of discrete and continuous factors of variability in a single-cell gene expression dataset. The proposed method is based on the idea that the mixture is a mixture of continuous and discrete factors, and that it can be represented as a mixture representation of continuous factors. The authors propose a variational inference problem for the mixture representation, which they call the cpl-MixVAE. They show that the proposed method outperforms the state-of-the-art on MNIST and dSprites datasets. "
576,SP:c982610ad28662c3bd13132abe1f7307d1a61b68,"Group equivariant convolutional networks ( GCNNs ) USED-FOR convolutional networks. symmetry priors FEATURE-OF convolutional networks. convolutions USED-FOR models. equivariance constraint FEATURE-OF kernels. G - steerable kernels USED-FOR convolutions. G HYPONYM-OF compact group. constraints FEATURE-OF steerable kernels. constraints CONJUNCTION spherical tensor operators. spherical tensor operators CONJUNCTION constraints. steerable kernels CONJUNCTION spherical tensor operators. spherical tensor operators CONJUNCTION steerable kernels. quantum mechanics USED-FOR spherical tensor operators. generalized reduced matrix elements CONJUNCTION ClebschGordan coefficients. ClebschGordan coefficients CONJUNCTION generalized reduced matrix elements. ClebschGordan coefficients CONJUNCTION harmonic basis functions. harmonic basis functions CONJUNCTION ClebschGordan coefficients. Wigner - Eckart theorem USED-FOR spherical tensor operators. homogeneous spaces FEATURE-OF harmonic basis functions. generalized reduced matrix elements USED-FOR steerable kernel spaces. Method is GCNNs. OtherScientificTerm are G - steerability constraint, and Gsteerable kernel spaces. Generic is it. ","This paper studies the group equivariant convolutional networks (GCNNs) in the presence of steerable kernels and spherical tensor operators. In particular, the authors show that the equivariance constraint of the kernels can be expressed in terms of a steerable kernel space, which is then used to define steerable convolutions in G-steerable kernels. Theoretical results are provided to show that this kernel space can be represented as a group-equivariant function of a compact group. The authors also show that spherical tensors can be approximated by generalized reduced matrix elements and Clebsch-Gordan coefficients.","This paper studies the group equivariant convolutional networks (GCNNs) with steerable kernels. The authors show that under certain conditions, G-steerable kernels can be used to train a G-equivariant network with spherical tensor operators. They prove the Wigner-Eckart theorem for spherical tensors and generalized reduced matrix elements for steerable kernel spaces. They also show that harmonic basis functions are steerable. "
585,SP:7b2ea39069277ad0f4f79476a77ef84587a804d9,"it USED-FOR accuracy disparities. average accuracies EVALUATE-FOR selective classification. accuracy EVALUATE-FOR selective classification. selective classification USED-FOR full - coverage accuracy disparities. models EVALUATE-FOR selective classification. full - coverage accuracies EVALUATE-FOR distributionally - robust models. Method is Selective classification. OtherScientificTerm are abstentions, spurious correlations, margin distribution, symmetric margin distributions, and left - log - concavity. Material is vision and NLP datasets. Metric is accuracies. Generic is distribution. ",This paper studies the impact of abstentions in selective classification on accuracy disparities. The authors show that the margin distribution of selective classification is not symmetric. They show that selective classification can lead to accuracy disparities that are larger than the full-accuracy disparities. They further show that this phenomenon is due to the presence of spurious correlations between the margin distributions.   ,"This paper proposes a new metric to measure the accuracy of selective classification. The metric is based on the fact that the margin distribution is symmetric, i.e. symmetric with respect to the left-log-concavity. The authors show that this metric can be used to estimate the average accuracy of a classifier. They also show that it can also be used as a measure of the full coverage accuracy of the classifier, which is a measure that measures the full-covering accuracy of all classes. They show that under certain assumptions on the margin distributions, this metric is able to capture the true margin distribution."
594,SP:f1d57ee27e901daf7e4e2b84139019e945818911,multi - layer network analysis CONJUNCTION temporal document classification. temporal document classification CONJUNCTION multi - layer network analysis. temporal document classification CONJUNCTION video data analysis. video data analysis CONJUNCTION temporal document classification. topic modeling USED-FOR applications. complex multi - modal structure FEATURE-OF applications. complex multi - modal structure FEATURE-OF topic modeling. complex multi - modal structure FEATURE-OF large - scale data. latent hierarchical structure FEATURE-OF multi - modal data. large - scale data USED-FOR topic modeling. video data analysis HYPONYM-OF applications. multi - layer network analysis HYPONYM-OF applications. temporal document classification HYPONYM-OF applications. Neural NCPD USED-FOR hierarchical topic modeling. Neural NCPD HYPONYM-OF training method. multi - modal tensor data USED-FOR hierarchical topic modeling. neural network architecture CONJUNCTION backpropagation. backpropagation CONJUNCTION neural network architecture. backpropagation USED-FOR error propagation. hierarchical NCPD USED-FOR error propagation. neural network architecture USED-FOR Neural NCPD. backpropagation USED-FOR Neural NCPD. ,"This paper proposes a method for hierarchical topic modeling on multi-modal tensor data. The main idea is to use a neural network architecture and backpropagation to model the latent structure of the data, which is then used to train a topic model. The method is evaluated on two tasks: temporal document classification and video data analysis.  ","This paper proposes a method for hierarchical topic modeling for multi-modal tensor tensor data. The main contribution of the paper is the use of a neural network architecture and backpropagation to model the latent hierarchical structure of the data, which can be used to improve the performance of the topic modeling task. The method is evaluated on a variety of tasks, including multi-layer network analysis, temporal document classification, and video data analysis. "
603,SP:b6ddc3a560aa7155e7e927bf5360bedc36586597,"node classification CONJUNCTION image segmentation. image segmentation CONJUNCTION node classification. graph CONJUNCTION image. image CONJUNCTION graph. image segmentation CONJUNCTION named - entity recognition. named - entity recognition CONJUNCTION image segmentation. classifier USED-FOR tasks. named - entity recognition HYPONYM-OF tasks. node classification HYPONYM-OF tasks. image segmentation HYPONYM-OF tasks. adversarial robustness certificates USED-FOR tasks. locality property USED-FOR collective certificate. single - node certificates PART-OF collective certificate. locality property CONJUNCTION perturbations. perturbations CONJUNCTION locality property. collective certificate USED-FOR node classification. Citeseer dataset EVALUATE-FOR collective certificate. OtherScientificTerm are perturbed inputs, and perturbation. Method are collective robustness certificate, and Graph Neural Networks. ","This paper proposes a new adversarial robustness certificate for node classification and image segmentation tasks. The proposed method is based on the idea that adversarial perturbations can be represented as a collection of nodes in a graph, where each node is represented by a node classifier and the perturbation is a weighted average of all the perturbed nodes in the graph. The authors show that the proposed method can improve the performance of node classification on Citeseer dataset. ","This paper proposes a new adversarial robustness certificate for graph neural networks (GNNs). The proposed method is based on the notion of locality property, which is defined as the number of perturbations per node in the graph. The authors show that the proposed method can be used for node classification, image segmentation, and named-entity recognition tasks. They also show that it can be applied to the Citeseer dataset. "
612,SP:cc93dd2f68e415e2457166e78627865dc1b44697,"generative models USED-FOR complex real - world data. Generative Adversarial Networks ( GANs ) USED-FOR complex real - world data. Generative Adversarial Networks ( GANs ) HYPONYM-OF generative models. generative and discriminative neural networks USED-FOR Learning high - dimensional probability distributions. non - convergence problem CONJUNCTION mode collapse. mode collapse CONJUNCTION non - convergence problem. mode collapse CONJUNCTION gradient explosion or vanishing. gradient explosion or vanishing CONJUNCTION mode collapse. non - convergence problem FEATURE-OF GANs. Least Squares GAN ( LSGANs ) CONJUNCTION Wasserstein GANs ( WGAN ). Wasserstein GANs ( WGAN ) CONJUNCTION Least Squares GAN ( LSGANs ). Wasserstein GANs ( WGAN ) HYPONYM-OF GANs. Least Squares GAN ( LSGANs ) HYPONYM-OF GANs. LSGANs USED-FOR mode collapse. quantile regression USED-FOR 1 - Wasserstein distance. modification of loss functions USED-FOR GANs. approach USED-FOR modification of loss functions. approach USED-FOR GANs. quantile regression USED-FOR Quantile Regression GAN ( QRGAN ). discriminator CONJUNCTION gradients. gradients CONJUNCTION discriminator. QRGAN USED-FOR mode collapse problem. robustness EVALUATE-FOR QRGAN. QRGAN COMPARE GANs. GANs COMPARE QRGAN. generation performance assessment EVALUATE-FOR GANs. evaluation EVALUATE-FOR Frechet Inception Distance ( FID ). Frechet Inception Distance ( FID ) USED-FOR generation performance assessment. Frechet Inception Distance ( FID ) EVALUATE-FOR GANs. Frechet Inception Distance ( FID ) EVALUATE-FOR QRGAN. generation performance assessment EVALUATE-FOR QRGAN. evaluation EVALUATE-FOR QRGAN. Method are modification methodology of loss functions, WGANs, and Wasserstein distance approximation. OtherScientificTerm are local minima, inefficient computation, and real and generated data distribution. ","This paper studies the mode collapse problem in GANs, which is a non-convex optimization problem where the gradients of the discriminator and gradients are not converging to local minima. The authors propose a new method called Quantile Regression GAN (QRGAN) to address this problem. The main idea is to use quantile regression to compute the 1-Wasserstein distance between the generated distribution and the real distribution. The proposed method is shown to be robust to mode collapse and gradient explosion.   ","This paper proposes Quantile Regression GAN (QRQGAN), a novel method to tackle the mode collapse problem in GANs. QRGAN is based on quantile regression, which is a modification of the Wasserstein distance between the discriminator and the gradients. The authors show that QRGAN can be used to tackle mode collapse, which can be a non-convex non-consistency problem for GAN models. They also provide a theoretical analysis of QRGAN and show that it is more robust to mode collapse than WGANs and LSGANs. "
621,SP:4ddb47ee77c374ae6c3e419412d92ca77260692e,relevance metrics USED-FOR similarity - based explanation. cosine similarity FEATURE-OF gradients. Method is machine learning models. Generic is metrics. ,This paper proposes to use cosine similarity to measure the similarity between the gradients of two models trained on the same dataset. The authors show that the cosine distance between two gradients is a measure of the similarity of the two models. They show that cosine distances can be used as a metric to evaluate the relevance of a model to the input data. They also show that this metric is useful in the context of machine learning.,"This paper proposes a new metric, cosine similarity, to measure the similarity between the gradients of a classifier and a dataset. The authors show that cosine similarities between gradients can be used as a measure of the relevance of a model to a dataset, and that it can be combined with other metrics such as similarity-based explanation. They also provide a theoretical analysis of the effect of cosine comparisons on the performance of the model. "
630,SP:6c2cbf2bc0f6dabe974e80ec1e82d2d12189906e,"generalization power EVALUATE-FOR Graph Neural Networks ( GNNs ). algorithmic alignment USED-FOR graph isomorphism test. LRGA module PART-OF GNNs. LRGA USED-FOR it. sample complexity EVALUATE-FOR kernel ’s feature map. 2 - FWL update step USED-FOR RGNN. LRGA USED-FOR RGNN. randomly initialized two - layer MLP USED-FOR RGNN. randomly initialized two - layer MLP USED-FOR kernel ’s feature map. polynomial kernels USED-FOR RGNN. polynomial kernels USED-FOR 2 - FWL update step. LRGA USED-FOR GNN layers. LRGA USED-FOR GNN architectures. Method are dot - product attention, and expressive GNNs. OtherScientificTerm is generalization properties. Generic is kernel. Material is GNN benchmarks. ",This paper studies the generalization power of graph neural networks (GNNs) in the context of graph isomorphism test. The authors propose a new method to test the generalizability of GNNs based on the LRGA module. The proposed method is based on a two-layer MLP with polynomial kernels and a 2-FWL update step. Theoretical results show that the proposed method improves the sample complexity of the kernel’s feature map.   ,This paper studies the generalization properties of graph neural networks (GNNs). The authors propose a graph isomorphism test (LRGA) module for GNNs that can be used to test generalization performance. The authors show that the LRGA module can improve generalization power of the GNN. The main contribution of the paper is that the proposed method can be applied to any GNN architecture.   
639,SP:b4abdd28504b4c1de239eabd4e0e27d370efee71,"objectness measures USED-FOR calibration. objectness measures USED-FOR Convolutional Neural Networks ( CNNs ). calibration EVALUATE-FOR Convolutional Neural Networks ( CNNs ). loss functions USED-FOR classification CNNs. CNNs USED-FOR classifiers. transformation USED-FOR CNN. random crops USED-FOR approaches. Context dependence FEATURE-OF safety - critical applications. objectness CONJUNCTION label smoothing. label smoothing CONJUNCTION objectness. approach USED-FOR classification. label smoothing USED-FOR training. objectness USED-FOR approach. label smoothing USED-FOR approach. relative object size USED-FOR smoothing factor. approach USED-FOR confidences. adaptive label smoothing USED-FOR CNNs. approach COMPARE baselines. baselines COMPARE approach. MS COCO COMPARE hard label approach. hard label approach COMPARE MS COCO. transfer learning USED-FOR MS COCO. transfer learning COMPARE hard label approach. hard label approach COMPARE transfer learning. Generic are they, and methods. Material are ImageNet-1 K, ImageNet, and context only images. Method is class activation maps. Task is classification and transfer learning tasks. ",This paper proposes an adaptive label smoothing method to improve the calibration of convolutional neural networks (CNNs) in classification tasks. The proposed method is based on the observation that the relative object size of an image can be used as a calibration measure for CNNs. The authors propose to use this objectness measure as the calibration measure in the classification task and use it as a confidence measure for the transfer learning task. Experiments are conducted on ImageNet-1K and ImageNet with context only images and show that the proposed method outperforms previous methods. ,"This paper proposes a new calibration method for classification and transfer learning based on objectness and label smoothing. The key idea is to use the relative object size as a measure of objectness, and then use the smoothing factor to adjust the objectness of the classifier. The proposed method is evaluated on ImageNet-1 K, ImageNet, and context only images. The results show that the proposed method outperforms other calibration methods."
648,SP:5254658923e594294b69d124a8d004166852822a,"Neural networks USED-FOR inverse problems. convex dual network USED-FOR interpreting training and prediction. convex solvers USED-FOR convex dual network. neural networks USED-FOR path sparsity. weight decay regularization FEATURE-OF neural networks. piecewise linear filtering USED-FOR prediction. MNIST and fastMRI datasets EVALUATE-FOR dual network optimization problem. Task is medical imaging. Method are convex duality framework, and convex optimization. ","This paper studies the problem of learning a convex dual network for inverse problems. The authors show that the dual network can be viewed as a linear combination of two neural networks, where the first one is trained to minimize the inverse problem and the second one is learned to solve it. The main contribution of the paper is to show that this dual network is convex and that it can be used to solve inverse problems in the presence of weight decay regularization.  ",This paper studies the problem of learning a convex dual network that can be used to solve inverse problems in medical imaging. The main contribution of the paper is to study the problem in terms of the convex solvers of the dual network. The authors show that the problem can be solved by using the weight decay regularization of neural networks. They show that this regularization can be combined with piecewise linear filtering to improve the performance of the network. They also provide a theoretical analysis of the problem.
657,SP:085cad6bc143c8713580bddfaa71f06496dac314,processing stages PART-OF text - to - speech synthesis pipelines. models USED-FOR raw speech audio outputs. character or phoneme input sequences USED-FOR models. generator USED-FOR inference. generator USED-FOR training. training CONJUNCTION inference. inference CONJUNCTION training. token length prediction USED-FOR differentiable alignment scheme. differentiable alignment scheme USED-FOR generator. adversarial feedback CONJUNCTION prediction losses. prediction losses CONJUNCTION adversarial feedback. total duration CONJUNCTION mel - spectrogram. mel - spectrogram CONJUNCTION total duration. prediction losses USED-FOR It. adversarial feedback USED-FOR It. soft dynamic time warping USED-FOR spectrogram - based prediction loss. soft dynamic time warping USED-FOR model. model COMPARE models. models COMPARE model. mean opinion score EVALUATE-FOR model. multi - stage training USED-FOR models. OtherScientificTerm is normalised text or phonemes. ,This paper proposes a novel method for training text-to-speech models with a sequence of normalised text and phonemes. The proposed method is based on a differentiable alignment scheme that aligns the token length prediction and the total duration with a soft dynamic time warping. The authors show that the proposed method outperforms the state-of-the-art models in terms of mean opinion score.  ,"This paper proposes a differentiable alignment scheme for text-to-speech synthesis models. The proposed method is based on the idea of time-warping, which is a soft dynamic time warping of the time-varying prediction loss. The authors show that the proposed method outperforms state-of-the-art models in terms of mean opinion score. "
666,SP:01148cea55db606aa78d27e900818684a8bce9ab,"attributed graphs FEATURE-OF real - world graphs. non - topological features FEATURE-OF nodes. attributes PART-OF attributed graph. lower - dimensional space FEATURE-OF discrete distributions. Wasserstein metric USED-FOR lower - dimensional space. Wasserstein metric USED-FOR discrete distributions. Wasserstein graph diffusion USED-FOR distribution representations of nodes. topology structure CONJUNCTION attributes. attributes CONJUNCTION topology structure. point representations USED-FOR downstream tasks. it USED-FOR node classification. algorithms USED-FOR node classification. algorithms USED-FOR matrix completion. node classification CONJUNCTION matrix completion. matrix completion CONJUNCTION node classification. it USED-FOR matrix completion. algorithms EVALUATE-FOR representation method. missing attributes USED-FOR node classification. it USED-FOR algorithms. Method are node representation learning approaches, and non - parametric framework. OtherScientificTerm are incomplete information, decomposition of the attribute matrix, node features, Wasserstein space, and local neighborhoods. Metric is distortion. ","This paper proposes a non-parametric representation learning method for node attributes in attributed graphs. The proposed method is based on the Wasserstein graph diffusion (WGD) method, which is a generalization of GNNs. The main idea is to use WGD to learn the distribution of nodes in the attributed graph. The authors show that the proposed method can be used for matrix completion and node classification tasks.  ","This paper proposes a non-parametric representation learning framework for node representation learning based on the Wasserstein graph diffusion (WGDP) metric. The WGDP is based on a lower-dimensional space of discrete distributions, and the authors propose to use the lower dimensional space to represent the distribution representations of nodes. The authors show that their method can be applied to node classification, matrix completion, and matrix completion. They also show that it can be used to improve the performance of node classification. "
675,SP:aeeb5909f7123ef631f569b469af9715205c881f,"Adversarially Motivated Intrinsic GOals USED-FOR goal - conditioned “ student ” policy. AMIGO HYPONYM-OF agent. meta - learning USED-FOR agent. goal - generating teacher PART-OF agent. intrinsic motivation CONJUNCTION RL methods. RL methods CONJUNCTION intrinsic motivation. Task are reinforcement learning ( RL ), and procedurally - generated tasks. OtherScientificTerm are sparse extrinsic rewards, environment reward, and constructively adversarial ” objective. Generic is method. ",This paper proposes a method to learn a goal-conditioned policy that can learn from procedurally generated tasks with sparse extrinsic rewards. The method is based on a meta-learning approach where the goal-generating teacher is used to train a student policy conditioned on the goal. The proposed method is evaluated on a set of synthetic and real-world tasks.   ,This paper proposes a meta-learning method for goal-conditioned adversarial reinforcement learning (AMIGO) where the teacher is a goal-generating teacher and the student is a student. The teacher is trained to generate a student policy that is motivated by the teacher's goals. The student policy is then trained to achieve the teacher’s goals.   The authors show that AMIGO can achieve state-of-the-art performance on a variety of tasks. They also show that the teacher can be used as a teacher for the student policy. 
684,SP:3d05bc7dca97681cb582298e318b9b973841eed3,"user distortion CONJUNCTION user privacy constraint. user privacy constraint CONJUNCTION user distortion. dataset of files USED-FOR information retrieval. distortion FEATURE-OF retrieval process. private information retrieval USED-FOR model. download rate CONJUNCTION distortion. distortion CONJUNCTION download rate. distortion CONJUNCTION user privacy leakage. user privacy leakage CONJUNCTION distortion. mutual information FEATURE-OF information - theoretical formulation. download rate EVALUATE-FOR schemes. generative adversarial models USED-FOR data - driven framework. constrained minimax game USED-FOR scheme. download rate CONJUNCTION distortion. distortion CONJUNCTION download rate. synthetic Gaussian dataset CONJUNCTION MNIST and CIFAR-10 datasets. MNIST and CIFAR-10 datasets CONJUNCTION synthetic Gaussian dataset. MNIST and CIFAR-10 datasets EVALUATE-FOR scheme. synthetic Gaussian dataset EVALUATE-FOR scheme. MNIST dataset EVALUATE-FOR data - driven approach. data - driven approach COMPARE achievable scheme. achievable scheme COMPARE data - driven approach. MNIST dataset EVALUATE-FOR achievable scheme. source coding USED-FOR achievable scheme. OtherScientificTerm are privacy level, perfect privacy requirement, and distortion constraint. Metric are rate - distortion - leakage tradeoff, and rate - distortion tradeoff curve. Material is CIFAR-10. ",This paper studies the rate-distortion-leakage tradeoff between user privacy and user privacy leakage in the context of data-driven information retrieval. The authors propose to use a generative adversarial model to estimate the mutual information between the user distortion and the user privacy constraint. Theoretical analysis is provided to show that the proposed method can recover the private information with a rate of $O(1/\sqrt{T})$ times the rate of distortion. The paper also provides a theoretical analysis of the privacy-leaky tradeoff in terms of the rate and distortion.,"This paper studies the trade-off between the rate-distortion-leakage tradeoff between user privacy and user distortion. The authors propose a data-driven framework that uses a generative adversarial model to model the mutual information between user distortion and user privacy. They show that the tradeoff is bounded by a constrained minimax game, where the user distortion is constrained to be lower than the user privacy constraint. They also provide a theoretical analysis of the tradeoffs between the two trade-offs. "
693,SP:3f9e2db00fc3dcd7a40588adcb638503ec10dc09,"Graph Neural Networks ( GNNs ) USED-FOR graph - related applications. they USED-FOR large scale settings. fidelity FEATURE-OF model. fidelity EVALUATE-FOR sampling - based methods. decoupled greedy learning method USED-FOR GNNs ( DGL - GNN ). greedy auxiliary objectives USED-FOR module. method USED-FOR time or memory limited applications. efficiency EVALUATE-FOR method. sampling - based acceleration COMPARE model. model COMPARE sampling - based acceleration. efficiency CONJUNCTION accuracy. accuracy CONJUNCTION efficiency. decoupled approach COMPARE methods. methods COMPARE decoupled approach. sampling PART-OF GNN training. sampling HYPONYM-OF it. OtherScientificTerm are node embeddings, and GNN layers. Method are GNN, lazy - update scheme, and DGL - GNN model. Generic are modules, and approach. Task is parallel GNN training. ",This paper proposes a new greedy learning method for GNNs (DGL-GNN) that decouples the sampling in GNN training from the greedy auxiliary objectives. The proposed method can be used for time- and memory-limited applications. The authors show that the proposed method achieves better fidelity compared to sampling-based methods.   ,"This paper proposes a decoupled greedy learning method for graph neural networks (GNNs). The main idea is to use a greedy auxiliary objective for each GNN module, which can be used as an auxiliary objective in the training process. The authors show that the proposed method can improve the fidelity of GNNs in time- and memory-efficient settings. The proposed method is evaluated on a variety of tasks, and compared with a number of baselines."
702,SP:5ecb1b288f7fc02aead4493f81640867bc349290,Neural link predictors USED-FOR missing edges. missing edges PART-OF large scale Knowledge Graphs. logical conjunctions ( ∧ ) CONJUNCTION disjunctions. disjunctions CONJUNCTION logical conjunctions ( ∧ ). disjunctions CONJUNCTION existential quantifiers. existential quantifiers CONJUNCTION disjunctions. framework USED-FOR complex queries. incomplete Knowledge Graphs USED-FOR complex queries. solutions USED-FOR optimisation problem. gradient - based and combinatorial search HYPONYM-OF optimisation problem. gradient - based and combinatorial search HYPONYM-OF solutions. approach COMPARE state - of - the - art methods. state - of - the - art methods COMPARE approach. state - of - the - art methods COMPARE neural models. neural models COMPARE state - of - the - art methods. approach COMPARE neural models. neural models COMPARE approach. Hits@3 EVALUATE-FOR knowledge graphs. factual information FEATURE-OF knowledge graphs. intermediate solutions USED-FOR complex query atoms. intermediate solutions USED-FOR model. Generic is models. OtherScientificTerm is end - to - end differentiable objective. Method is neural link predictor. ,"This paper proposes a neural link predictor for missing edges in incomplete Knowledge Graphs. The proposed method is based on logical conjunctions and disjunctions, and uses existential quantifiers to represent the missing edges. The method is trained using gradient-based and combinatorial search. Experiments show that the proposed method outperforms the state-of-the-art methods in terms of accuracy.","This paper proposes a neural link predictor for missing edges in large-scale Knowledge Graphs (KG). The proposed method is based on an end-to-end differentiable objective, where each node in the KG is represented by a set of logical conjunctions, disjunctions, and existential quantifiers. The authors show that the proposed method outperforms state-of-the-art neural link predictors in terms of accuracy. The proposed approach is evaluated on a variety of KG datasets, and it is shown that it outperforms the state of the art."
711,SP:f04a522fd04c503754fdb8c52da68646d31271a4,"procedure USED-FOR local robustness. procedure USED-FOR feed - forward neural networks. local robustness FEATURE-OF feed - forward neural networks. piecewise - linear activation functions FEATURE-OF feed - forward neural networks. decision boundaries USED-FOR assessing robustness. highly - parallel GPU implementation USED-FOR ` 2 norm. approach COMPARE approximate verification approaches. approximate verification approaches COMPARE approach. approximate verification approaches COMPARE verifiers. verifiers COMPARE approximate verification approaches. approach COMPARE verifiers. verifiers COMPARE approach. Task is Local robustness. Generic are model, networks, network, and algorithm. OtherScientificTerm are ` p - ball consistently, adversarial inputs, convex polyhedral regions, and geometric projections. Metric is robustness. ","This paper studies the problem of evaluating the robustness of neural networks with piecewise linear activation functions in the presence of adversarial perturbations. The authors propose to use the decision boundaries as a measure of robustness. The decision boundaries are defined as a set of points along the decision boundary of the network that are close enough to the adversarial input to be robust to the perturbation. Then, the authors propose an algorithm to compute the decision bounds of the networks. The algorithm is based on the notion of local robustness, which is defined in terms of the norm of the `p-ball.   The authors show that their algorithm is computationally efficient and can be implemented on a large number of GPUs.","This paper studies the problem of verifying the robustness of feed-forward neural networks (FFNs) against adversarial inputs. The authors propose a method to measure robustness in the presence of adversarial input, where the input is generated from a set of convex polyhedral regions. They show that under certain conditions, FFNs are robust to adversarial perturbations. They also provide an algorithm for computing the robust norm of FFNs.  "
720,SP:5297651ff873f97c07b9c47ed3eff52251661844,"approach USED-FOR embedding of objects. affordance space FEATURE-OF embedding of objects. embedding COMPARE approaches. approaches COMPARE embedding. dimensions USED-FOR mental representation of objects. human judgements of object similarity USED-FOR mental representation of objects. Generic are knowledge, and they. OtherScientificTerm are object “ affordance ”, and human judgments of affordance. Material is text corpora. ","This paper proposes a new embedding method for embedding objects in text corpora. The proposed method is based on the observation that the embedding space of an object can be viewed as an affordance space, which is defined as the space in which an object is likely to be seen in a given context. The authors show that embeddings of objects in affordance spaces can be used to learn representations of objects that are similar to the objects in the context. They show that the learned representations are able to capture human judgements of object similarity.   ","This paper proposes a new embedding method for embedding objects in text corpora. The embedding is based on the notion of ""affordance space"", which is defined as the space of objects that can be represented in terms of their similarity to other objects. The authors show that the embedding of objects in affordance space can be learned from human judgements of object similarity. They also show that embedding embeddings can be used to learn representations of objects with different affordance spaces."
729,SP:72b4f3b40c6c6fa2eb53e95ed9a10a4077ffa049,"Individuality USED-FOR human society. efficiency CONJUNCTION productivity. productivity CONJUNCTION efficiency. It USED-FOR division of labor. efficiency EVALUATE-FOR It. productivity EVALUATE-FOR It. it USED-FOR multi - agent cooperation. method USED-FOR emergence of individuality ( EOI ). emergence of individuality ( EOI ) PART-OF multi - agent reinforcement learning ( MARL ). probabilistic classifier USED-FOR probability distribution. EOI USED-FOR probabilistic classifier. regularizers USED-FOR classifier. intrinsic reward USED-FOR emergence of individuality. regularizers USED-FOR emergence of individuality. MARL algorithms USED-FOR EOI. EOI COMPARE methods. methods COMPARE EOI. multi - agent cooperative scenarios EVALUATE-FOR methods. multi - agent cooperative scenarios EVALUATE-FOR EOI. OtherScientificTerm are individuality, and intrinsic reward signals. ",This paper proposes a method for multi-agent reinforcement learning (MARL) called emergence of individuality (EOI) to encourage the emergence of individualism in the agents. The proposed method is based on a probabilistic classifier that is trained to predict the probability distribution of the agents' actions. The authors show that this classifier can be used as an intrinsic reward signal to encourage agents to learn to be more individualistic. They show that the proposed method outperforms existing MARL algorithms in a variety of experiments.  ,"This paper proposes a new method for multi-agent reinforcement learning (MARL) based on emergence of individuality (EOI). The authors propose a probabilistic classifier that learns a probability distribution over the probability distribution of the agents, and a classifier with intrinsic reward signals that encourages the emergence of EOI. The authors show that the proposed method outperforms existing MARL algorithms in terms of efficiency and productivity. "
738,SP:112509d6d3573a9d495d182fdfae6ec0327cddf5,"certified robustness EVALUATE-FOR Randomized smoothing. base classifier USED-FOR randomized smoothing. Smoothed WEighted ENsembling ( SWEEN ) scheme USED-FOR randomized smoothed classifiers. SWEEN USED-FOR optimal certified robustness. adaptive prediction algorithm USED-FOR SWEEN models. adaptive prediction algorithm USED-FOR prediction and certification cost. SWEEN models COMPARE candidate models. candidate models COMPARE SWEEN models. training time EVALUATE-FOR SWEEN models. small models USED-FOR SWEEN models. OtherScientificTerm are l2 - norm adversarial attacks, and ensembling generality. Method is SWEEN model. ","This paper proposes a new randomized smoothed ensemble (SWEEN) method to improve the certified robustness of the base classifier in the face of adversarial attacks. The proposed SWEEN method is based on the randomized smoothing method, where the classifier is trained with a small number of small models. The authors show that the proposed method can achieve the best certified robust performance in terms of the l2-norm adversarial robustness. The main contributions of the paper are:  1) The proposed method is able to achieve the optimal certification cost in the presence of small classifiers.  2) The method is computationally efficient. ","This paper proposes a new method to improve the certified robustness of randomized smoothed classifiers. The proposed method is based on the Smoothed-WEIGHTed-Ensembling (SWEEN) scheme, where the base classifier is trained with randomized smoothing and the ensembled classifiers are trained with SWEEN. The main contribution of the paper is that the proposed method can be applied to a wide range of classifiers (e.g., CIFAR-10, Cifar-100, and CIFar-200) without requiring the use of a large number of small models. The method is evaluated on a variety of datasets, and it is shown that it can be used to reduce the certification cost and the training time."
747,SP:ea892e3d199ed6121279b20061a87f43afae8796,hierarchical structures USED-FOR learning process. hierarchical structures USED-FOR generalization. Ordered Memory Policy Network ( OMPN ) USED-FOR subtask hierarchy. subtask hierarchy USED-FOR task decomposition. subtask boundaries PART-OF unstructured demonstration. Craft CONJUNCTION Dial. Dial CONJUNCTION Craft. model COMPARE baselines. baselines COMPARE model. Craft EVALUATE-FOR model. Dial EVALUATE-FOR model. task decomposition EVALUATE-FOR model. unsupervised and weakly supervised settings EVALUATE-FOR model. OMPN USED-FOR partially observable environments. task decomposition EVALUATE-FOR OMPN. subtask hierarchy PART-OF model. Task is complex real - world tasks. OtherScientificTerm is inductive bias. ,"This paper proposes a method to learn a hierarchy of tasks in an unstructured demonstration setting. The proposed method is based on an ordered memory policy network (OMPN) that is trained to predict the subtask boundaries of each task in the demonstration. The method is evaluated on two tasks: Craft and Dial, where it is shown that the proposed method achieves state-of-the-art performance. ","This paper proposes a method for learning a hierarchy of tasks in an unstructured demonstration setting. The proposed method is based on the Ordered Memory Policy Network (OMPN), which is a model for learning the subtask hierarchy of a task. The method is evaluated on a variety of unsupervised and weakly supervised settings. The results show that the proposed method outperforms the state of the art. "
756,SP:cc6aa977ce561a2493ae74bb694205cd67c8d890,semantic factor CONJUNCTION variation factor. variation factor CONJUNCTION semantic factor. deep ones HYPONYM-OF supervised learning methods. methods USED-FOR OOD prediction. causal reasoning USED-FOR Causal Semantic Generative model ( CSG ). learning CONJUNCTION prediction. prediction CONJUNCTION learning. variational Bayes USED-FOR learning. variational Bayes USED-FOR prediction. causal invariance principle USED-FOR methods. CSG USED-FOR semantic factor. semantic - identification USED-FOR adaptation. OOD EVALUATE-FOR baselines. OtherScientificTerm is domain - specific correlation. Metric is OOD generalization error. , generalization error is an important problem in machine learning. This paper proposes a causal reasoning based method for OOD generalization. The proposed method is based on a causal semantic generative model (CSG) and uses a variational Bayes approach. The authors show that the proposed method outperforms baselines in terms of OOD performance. ,This paper proposes a causal semantic generative model (CSG) for over-the-odds (OOD) prediction. The CSG uses a variational Bayes-based approach to predict the OOD generalization error. The main contribution of the paper is to propose a causal reasoning framework for OOD prediction based on the causal invariance principle. The authors show that the proposed CSG can be used to improve the generalization performance of existing OOD methods.
765,SP:be3f34a59e5e61dcdbc7cb085f031ba4a5a5b758,"adversarially corrupted rewards FEATURE-OF online learning algorithms. small regret FEATURE-OF algorithms. algorithm USED-FOR corrupted rewards. uncorrupted reward distribution FEATURE-OF regret. robust estimation USED-FOR unsupervised learning problems. stochastic multi - armed bandits CONJUNCTION linear contextual bandits. linear contextual bandits CONJUNCTION stochastic multi - armed bandits. linear contextual bandits CONJUNCTION Markov Decision Processes ( MDPs ). Markov Decision Processes ( MDPs ) CONJUNCTION linear contextual bandits. robust estimation USED-FOR robust online algorithms. robust online algorithms USED-FOR scenarios. stochastic rewards and transitions FEATURE-OF Markov Decision Processes ( MDPs ). near optimal regret FEATURE-OF robust online algorithms. Markov Decision Processes ( MDPs ) HYPONYM-OF scenarios. stochastic multi - armed bandits HYPONYM-OF scenarios. linear contextual bandits HYPONYM-OF scenarios. synthetic and real datasets EVALUATE-FOR algorithms. Method are online algorithm, and online learning. OtherScientificTerm are stochastic reward, and noise rate. ","This paper studies the problem of online learning with adversarially corrupted rewards in the presence of noisy rewards. In particular, the authors propose a robust online learning algorithm that is robust to adversarial noise in the reward distribution and achieves a regret of $O(1/\sqrt{T})$ for linear contextual bandits and stochastic multi-armed bandits. The authors show that the regret is $O(\sqrt{\frac{1}{T})$, where $T$ is the number of times the reward is corrupted. They show that this regret is upper bounded by a constant factor that depends on the size of the noisy reward distribution. They also show that under certain assumptions, the regret can be bounded by an upper bound on the noise rate.   ","This paper studies the problem of online learning with adversarially corrupted rewards, where the reward distribution is corrupted and the regret is uncorrupted. The authors propose a robust online learning algorithm that can recover the corrupted reward distribution with a small regret. They show that this algorithm can be used in a variety of unsupervised learning problems, including stochastic multi-armed bandits, linear contextual bandits, Markov Decision Processes (MDPs), and linear contextual bandit. They also show that their algorithm achieves near optimal regret. "
774,SP:6d62a80aaebb2988df3953d4d7164e5a2fa1aa6d,"Encoder - decoder architecture USED-FOR neural machine translation ( NMT ). rewriter CONJUNCTION evaluator. evaluator CONJUNCTION rewriter. rewriter PART-OF It. evaluator PART-OF It. evaluator USED-FOR translation quality. rewriter CONJUNCTION evaluator. evaluator CONJUNCTION rewriter. prioritized gradient descent ( PGD ) method USED-FOR rewriter. PGD method USED-FOR Rewriter - Evaluator. framework USED-FOR NMT models. Transformer HYPONYM-OF NMT models. framework COMPARE baselines. baselines COMPARE framework. translation tasks EVALUATE-FOR framework. NMT models COMPARE baselines. baselines COMPARE NMT models. framework USED-FOR NMT models. Chinese - English and English - German HYPONYM-OF translation tasks. Generic is it. OtherScientificTerm is termination policy. Method are RewriterEvaluator, decoding, and encoder - decoder models. Task is rewriting process. ","This paper proposes a rewriter-evalator architecture for neural machine translation (NMT) models. The rewriter is an encoder-decoder architecture that consists of an evaluator that rewrites the original input text and a ""rewriter"" that evaluates the quality of the translated text. The evaluation of the original text is performed by the rewriter, which is trained using a prioritized gradient descent (PGD) method. The authors propose a termination policy that decides when to terminate the evaluation process. The proposed method is evaluated on two NMT tasks: Chinese-English and English-German. ","This paper proposes Rewriter-Evaluator, a novel approach to improve the quality of neural machine translation models. The main idea is to use a rewriter and an evaluator in the encoder-decoder architecture. The rewriter rewrites the encoders and evaluates the decoders, and the evaluators are trained using a prioritized gradient descent (PGD) method. The authors show that the proposed approach outperforms the state-of-the-art in terms of translation quality on Chinese-English and German-German translation tasks."
783,SP:9761fca8848868dfc9cacdab2537f8276ca76e0f,"images CONJUNCTION unsystematic annotation. unsystematic annotation CONJUNCTION images. Ambiguities CONJUNCTION unsystematic annotation. unsystematic annotation CONJUNCTION Ambiguities. Ambiguities CONJUNCTION images. images CONJUNCTION Ambiguities. empirical frequency FEATURE-OF sampled predictions. two - stage, cascaded strategy USED-FOR calibrated adversarial refinement. adversarial network USED-FOR coherent predictions. black - box segmentation framework USED-FOR learning of calibrated stochastic mappings. model USED-FOR learning of calibrated stochastic mappings. model PART-OF black - box segmentation framework. multigrader LIDC dataset CONJUNCTION Cityscapes dataset. Cityscapes dataset CONJUNCTION multigrader LIDC dataset. multigrader LIDC dataset EVALUATE-FOR approach. Cityscapes dataset EVALUATE-FOR approach. core design USED-FOR tasks. framework USED-FOR semantic segmentation. core design USED-FOR calibrated predictive distribution. toy regression dataset EVALUATE-FOR framework. calibrated predictive distribution USED-FOR tasks. OtherScientificTerm are distribution over predictions, empirical distribution, multimodal predictive distribution, categorical likelihood, and calibrated stochastic mappings. Method is probabilistic networks. Generic is these. ","This paper proposes a method to improve the performance of semantic segmentation by learning a calibrated distribution over predictions. The method is based on a two-stage, cascaded strategy to refine the empirical frequency of sampled predictions by an adversarial network. The proposed method is evaluated on the multigrader LIDC dataset and the Cityscapes dataset. ","This paper proposes a new black-box segmentation framework for unsupervised semantic segmentation. The proposed method is based on a two-stage, cascaded strategy for calibrated adversarial refinement. The main idea is to learn a calibrated stochastic mapping between the empirical distribution over predictions and a multimodal predictive distribution. The method is evaluated on the Cityscapes dataset and the multigrader LIDC dataset."
792,SP:ce965758f1b795a56c02f45d6a8d06cb8bdf29cb,"distributed compute systems USED-FOR stochastic optimization algorithms. stochastic optimization algorithms USED-FOR large - scale machine learning applications. distributed compute systems USED-FOR large - scale machine learning applications. error feedback ( EF ) USED-FOR compressed communication. Top - K or PowerSGD HYPONYM-OF contractive compressors. EF USED-FOR contractive compressors. alternative USED-FOR contractive compressors. alternative USED-FOR EF. construction USED-FOR contractive compressor. construction USED-FOR induced unbiased compressor. contractive compressor CONJUNCTION induced unbiased compressor. induced unbiased compressor CONJUNCTION contractive compressor. approach COMPARE EF. EF COMPARE approach. reduced memory requirements CONJUNCTION communication complexity guarantees. communication complexity guarantees CONJUNCTION reduced memory requirements. communication complexity guarantees CONJUNCTION assumptions. assumptions CONJUNCTION communication complexity guarantees. partial participation FEATURE-OF federated learning. Generic are systems, and transformation. OtherScientificTerm are communication overhead, stochastic gradients, and unbiased compressors. ","This paper proposes to use error feedback (EF) to reduce the communication cost in distributed stochastic gradient descent (SGD) algorithms. The main contribution of the paper is to show that EF can be used as a way to improve the communication efficiency in distributed gradient descent algorithms. In particular, the authors show that using error feedback as a compressed communication method can reduce the computation cost in federated learning.   ","This paper proposes a new method for computing error feedback (EF) in distributed computing systems. The main idea is to use an induced unbiased compressor (i.e., a contractive compressor) instead of the standard Top-K or PowerSGD. The proposed method is based on a construction of induced unbiased compressors, which can be applied to any unbiased compressor. The authors show that the proposed method can be used in federated learning, where it can be combined with existing error feedback methods."
801,SP:4fd702490293e481c79614852ba27dd3ce9215a4,"hyperparameter optimization ( HPO ) USED-FOR HPO. HT - AA baseline algorithms CONJUNCTION benchmarks. benchmarks CONJUNCTION HT - AA baseline algorithms. baseline COMPARE HPO algorithm. HPO algorithm COMPARE baseline. HPO PART-OF ML development. python packages USED-FOR baselines. python packages USED-FOR benchmarks. baselines CONJUNCTION benchmarks. benchmarks CONJUNCTION baselines. python packages USED-FOR HT - AA. Method are machine learning ( ML ) algorithm, ML algorithms, and neural architectures. OtherScientificTerm are hyperparameter settings, hyperparameter search space, and hyperparameter search spaces. Generic are approaches, and research framework. ","This paper studies hyperparameter optimization (HPO) in machine learning. The authors propose a new HPO framework, called Hyperparameter Optimization with AA (HT-AA), which aims to improve the performance of existing HPO algorithms. The main contribution of the paper is the introduction of a new benchmark for HPO in ML, which is based on existing benchmarks. The paper also provides a theoretical analysis of the proposed method.  ","This paper proposes a new research framework for hyperparameter optimization (HPO) for machine learning (ML) algorithms. The main contribution of the paper is to propose a new benchmark for HPO, which can be used to compare different HPO baselines and benchmarks. The benchmark is based on an existing HPO benchmark, and the authors show that the proposed benchmark outperforms the existing benchmark by a large margin.  "
810,SP:e8f99bae5853de525450fcb8facd23cf973fc161,"audio labels COMPARE categorical probabilities. categorical probabilities COMPARE audio labels. image classifier USED-FOR classification. image classifier USED-FOR audio labels. audio labels COMPARE numerical probabilities. numerical probabilities COMPARE audio labels. numerical probabilities CONJUNCTION text. text CONJUNCTION numerical probabilities. audio labels COMPARE text. text COMPARE audio labels. they USED-FOR error signal. spectrograms CONJUNCTION shuffled spectrograms. shuffled spectrograms CONJUNCTION spectrograms. shuffled spectrograms CONJUNCTION Gaussian mixtures. Gaussian mixtures CONJUNCTION shuffled spectrograms. constant matrices CONJUNCTION spectrograms. spectrograms CONJUNCTION constant matrices. Gaussian mixtures CONJUNCTION uniform random matrices. uniform random matrices CONJUNCTION Gaussian mixtures. dimensionalities FEATURE-OF uniform random matrices. uniform random matrices HYPONYM-OF label representations. constant matrices HYPONYM-OF label representations. Gaussian mixtures HYPONYM-OF label representations. shuffled spectrograms HYPONYM-OF label representations. spectrograms HYPONYM-OF label representations. high dimensional, high entropy labels COMPARE text ( categorical ) labels. text ( categorical ) labels COMPARE high dimensional, high entropy labels. robustness EVALUATE-FOR features. image classification task EVALUATE-FOR high dimensional, high entropy labels. image classification task EVALUATE-FOR text ( categorical ) labels. accuracy EVALUATE-FOR high dimensional, high entropy labels. label representations USED-FOR features. OtherScientificTerm are data labels, and adversarial attacks. Generic is models. Method are high dimensional, high entropy label representations, and label representation. ","This paper studies the problem of robustness to adversarial attacks in the presence of high-entropy labels. The authors propose to use spectrograms, shuffled spectrogram, Gaussian mixtures, and uniform random matrices as label representations to improve robustness. Theoretical analysis is provided to show that label representations are more robust than text labels. Experiments on image classification tasks show that the proposed method outperforms text-only methods.","This paper studies the problem of robustness against adversarial attacks on high-dimensional, high-entropy labels. The authors propose a new label representation for audio labels, which is based on spectrograms, shuffled spectrogram, Gaussian mixtures, constant matrices, and uniform random matrices. They show that the proposed label representations are robust to adversarial attack. They also show that they can be used to improve the robustness of image classification models."
819,SP:4e8d924cba7367af0999b30d79250b4dc40413e1,approaches USED-FOR ensemble neural networks. forward passes USED-FOR prediction. forward passes USED-FOR methods. single model ’s capacity USED-FOR subnetworks. CIFAR100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR100. accuracy CONJUNCTION calibration error. calibration error CONJUNCTION accuracy. negative log - likelihood CONJUNCTION accuracy. accuracy CONJUNCTION negative log - likelihood. CIFAR10 CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION CIFAR10. out - of - distribution variants COMPARE methods. methods COMPARE out - of - distribution variants. ImageNet CONJUNCTION out - of - distribution variants. out - of - distribution variants CONJUNCTION ImageNet. CIFAR10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR10. Generic is network. OtherScientificTerm is forward pass. Metric is model robustness. ,"This paper proposes a method to improve the robustness of ensemble neural networks (ENs) by using forward passes. The proposed method is based on the idea that the capacity of a single model can be divided into subnetworks, and each subnetwork's capacity is used to aggregate information from all the other models in the ensemble. The method is evaluated on CIFAR-10 and ImageNet, and compared with a number of baselines.   ","This paper studies the problem of model robustness in ensemble neural networks. In particular, the authors study the effect of the forward pass on the accuracy, calibration error, and negative log-likelihood of the model. They show that the accuracy is affected by the number of forward passes in the ensemble, and that the calibration error is affected as well by the negative log likelihood. They also show that out-of-distribution variants of the ensemble can be used to improve the robustness of the network."
828,SP:d2f1c23b67c6744101034dc5e1c70765a733b169,"teacher network CONJUNCTION student network. student network CONJUNCTION teacher network. method USED-FOR intermediate knowledge. Sparse Representation Matching ( SRM ) USED-FOR intermediate knowledge. Convolutional Neural Network ( CNN ) USED-FOR intermediate knowledge. sparse representation learning USED-FOR Sparse Representation Matching ( SRM ). sparse representation learning USED-FOR method. pixellevel and image - level labels USED-FOR intermediate feature maps. intermediate feature maps PART-OF student network. sparse representations of the hidden features PART-OF teacher CNN. sparse representations of the hidden features USED-FOR SRM. neural processing block USED-FOR SRM. stochastic gradient descent USED-FOR neural processing block. stochastic gradient descent USED-FOR SRM. SRM COMPARE KD techniques. KD techniques COMPARE SRM. Task is Knowledge Distillation. Method are CNN, and teacher and student networks. ",This paper proposes a method for knowledge distillation (KD) where the goal is to distill intermediate knowledge from a teacher and a student network. The proposed method is based on sparse representation learning (SRM) where intermediate feature maps from the teacher and student networks are mapped to sparse representations of the hidden features in the teacher CNN. The method is evaluated on a variety of image classification tasks and compared with other KD methods.,"This paper proposes a method for knowledge distillation (KD) where the teacher and student networks are trained with sparse representations of the hidden features of the teacher CNN and student CNN, respectively. The method is based on sparse representation learning (SRM) and uses stochastic gradient descent (SGD) to learn the intermediate feature maps of the student and teacher networks. The proposed method is evaluated on a variety of datasets and compared with a number of KD methods. "
837,SP:e8c0f43bd5debf6544f588cd3442dc3dd62d0eee,Reinforcement learning methods USED-FOR policies. sequential structure PART-OF representation learning process. sequential structure PART-OF reinforcement learning. approach COMPARE approaches. approaches COMPARE approach. theoretically motivated policy similarity metric ( PSM ) USED-FOR behavioral similarity. contrastive representation learning procedure USED-FOR state similarity metric. contrastive representation learning procedure USED-FOR policy similarity embeddings ( PSEs1 ). PSM USED-FOR policy similarity embeddings ( PSEs1 ). LQR CONJUNCTION jumping task. jumping task CONJUNCTION LQR. jumping task CONJUNCTION Distracting DM Control Suite. Distracting DM Control Suite CONJUNCTION jumping task. generalization EVALUATE-FOR benchmarks. spurious correlations FEATURE-OF LQR. benchmarks EVALUATE-FOR PSEs. generalization EVALUATE-FOR PSEs. Distracting DM Control Suite HYPONYM-OF benchmarks. jumping task HYPONYM-OF benchmarks. LQR HYPONYM-OF benchmarks. Generic is structure. OtherScientificTerm is optimal policies. ,This paper proposes a novel policy similarity metric (PSM) to measure the behavioral similarity between policies in reinforcement learning tasks. The proposed PSM is based on a contrastive representation learning procedure to learn the policy similarity embeddings (PSSEs1). Theoretical analysis is provided to show that the PSEs can be used to improve the performance of RL algorithms. Experiments are conducted on the Distracting DM Control Suite and the LQR task.,"This paper proposes a novel policy similarity metric (PSM) to measure the behavioral similarity between two policies in reinforcement learning. The PSM is motivated by the fact that the two policies can be thought of as having the same state-action similarity. The authors propose a contrastive representation learning procedure (PSE) to learn the PSEs1 embeddings for the PSM. The PSE embedding is then used to learn a policy similarity embedding (PSE) embedding for each state, which is used as a proxy for the state similarity metric. Experiments are conducted on a variety of tasks, including LQR, Distracting DM Control Suite, and jumping task. The results show that the proposed method outperforms the state-of-the-art in terms of generalization. "
846,SP:92f3b4942da9075440dda618f561a85f8fde5a5c,"approach USED-FOR disentanglement. latent representation FEATURE-OF model. approach USED-FOR disentanglement. rotations CONJUNCTION translations. translations CONJUNCTION rotations. topological defects FEATURE-OF transformations. images FEATURE-OF transformations. affine transformations HYPONYM-OF transformations. translations HYPONYM-OF affine transformations. rotations HYPONYM-OF affine transformations. approach USED-FOR disentanglement. distributed equivariant operators USED-FOR approach. approach USED-FOR disentangle affine transformations. distributed operators USED-FOR disentanglement. distributed operators USED-FOR models. Task is Machine Learning. OtherScientificTerm are object shape, encoder, and latent space. Generic is factors. Method is group representation theory. ",This paper proposes a method for disentangling affine transformations in latent representations. The key idea is to use distributed equivariant operators to disentangle the affine transformation in the latent space of the encoder and the latent representation of the model. The method is based on the notion of equivariance in group representation theory. The authors show that the disentanglement can be achieved by using a set of distributed operators that can be used to learn disentangled transformations.  ,This paper proposes a new approach to disentangle affine transformations. The key idea is to use distributed equivariant operators to learn disentanglement terms that can be used in the latent space of the encoder and decoder. The idea is inspired by group representation theory. The authors show that disentangling affine transformation can be achieved by using a set of distributed operators. They also show that the disentangled transformations can be learned by using the same number of operators.   
855,SP:ef0f58c462bc5dd1c7b78f562c42a4e17f0f252b,statistical framework USED-FOR timedependent interaction of neuronal spiking activities. Hawkes process USED-FOR timedependent interaction of neuronal spiking activities. Hawkes process USED-FOR statistical framework. Hawkes process USED-FOR inhibitory interactions. nonlinear Hawkes process USED-FOR influence pattern. excitatory or inhibitory interactions FEATURE-OF influence pattern. latent marked Poisson processes CONJUNCTION sparsity variables. sparsity variables CONJUNCTION latent marked Poisson processes. Pólya - Gamma variables CONJUNCTION latent marked Poisson processes. latent marked Poisson processes CONJUNCTION Pólya - Gamma variables. auxiliary latent variables USED-FOR functional connection weights. Gaussian form FEATURE-OF functional connection weights. analytical updates USED-FOR iterative algorithm. sparsity variables HYPONYM-OF auxiliary latent variables. latent marked Poisson processes HYPONYM-OF auxiliary latent variables. Pólya - Gamma variables HYPONYM-OF auxiliary latent variables. expectationmaximization ( EM ) algorithm USED-FOR maximum a posteriori ( MAP ) estimate. accuracy EVALUATE-FOR algorithm. synthetic and real data EVALUATE-FOR algorithm. algorithm USED-FOR temporal dynamics of interaction. algorithm USED-FOR interpretable functional connectivity. interpretable functional connectivity FEATURE-OF neural spike trains. real neural recordings EVALUATE-FOR algorithm. ,"This paper proposes a statistical framework to study the temporal dynamics of neuronal spiking activity in the presence of excitatory or inhibitory interactions. The proposed framework is based on the nonlinear Hawkes process, which is used to model the influence pattern of excitation and inhibitory activity. The authors propose an expectation-maximization (EM) algorithm to estimate the maximum a posteriori (MAP) estimate of the functional connection weights in the Gaussian form. They show that the proposed method is able to recover the functional connectivity weights in both synthetic and real data. ",This paper proposes a statistical framework for measuring the temporal dynamics of neuronal spiking and inhibitory interactions in the context of the Hawkes process. The authors propose an iterative algorithm to estimate the maximum a posteriori (MAP) estimate of the functional connection weights of a neural spike train. They also propose an expectationmaximization (EM) algorithm for estimating the maximum posteriori estimate. The empirical performance of the proposed method is demonstrated on synthetic and real neural spike trains.
864,SP:1156d3deac022829bda930ffcb081947609d972b,"gradient descent ( GD ) algorithm USED-FOR two - layer neural network models. under - parameterized regime FEATURE-OF GD dynamics. quenched ” neurons USED-FOR continued activation and deactivation process. quenching - activation process USED-FOR GD. random featurelike behavior FEATURE-OF it. quenching process USED-FOR implicit regularization ”. mean - field ” scaling FEATURE-OF GD dynamics. OtherScientificTerm are parameter regimes, neural network - like behavior, and inner - layer parameters. Method is random feature model. Generic is dynamics. ","This paper studies the gradient descent (GD) algorithm for two-layer neural networks in the under-parameterized regime. In particular, the authors show that GD dynamics in this regime can be viewed as a quenching-activation process, where the inner layer parameters are quenched. The authors also show that the inner-layer parameters of the model are independent of the outer layer parameters.    The main contribution of this paper is a theoretical analysis of the dynamics of gradient descent under the random feature model. ","This paper studies the dynamics of gradient descent (GD) in the under-parameterized regime of two-layer neural network models. In this regime, the inner-layer parameters of a neural network are randomly sampled from a random feature model, and the inner layer parameters of the outer layer parameters are also randomly sampled. The authors show that the dynamics in this regime is similar to that in the normal regime. They also show that under this regime the quenching-activation process is implicit in the dynamics, and that it can be used as an implicit regularization of the dynamics. "
873,SP:9e81401a6f30c70d870a12cce0cf600557f92b80,"constrained Markov decision process ( CMDP ) problems USED-FOR reinforcement learning problems. model USED-FOR CMDP problem. reconnaissance MDP ( R - MDP ) CONJUNCTION planning MDP ( P - MDP ). planning MDP ( P - MDP ) CONJUNCTION reconnaissance MDP ( R - MDP ). MDPs PART-OF CMDP. planning MDP ( P - MDP ) HYPONYM-OF MDPs. reconnaissance MDP ( R - MDP ) HYPONYM-OF MDPs. threat function CONJUNCTION Q - function analogue of danger. Q - function analogue of danger CONJUNCTION threat function. threat function USED-FOR R - MDP. reward - seeking policy USED-FOR P - MDP. fixed threat function USED-FOR reward - seeking policy. generative model USED-FOR threat function. reward CONJUNCTION danger - constraint. danger - constraint CONJUNCTION reward. threat function USED-FOR baseline policy. reward FEATURE-OF CMDP problems. danger - constraint FEATURE-OF CMDP problems. approximation method USED-FOR R - MDP. approximation method USED-FOR threat function. method COMPARE approaches. approaches COMPARE method. benchmark dataset CONJUNCTION complex collision - free navigation tasks. complex collision - free navigation tasks CONJUNCTION benchmark dataset. complex collision - free navigation tasks EVALUATE-FOR method. complex collision - free navigation tasks EVALUATE-FOR approaches. benchmark dataset EVALUATE-FOR method. benchmark dataset EVALUATE-FOR approaches. OtherScientificTerm are prescribed safety constraints, and state - action pair. ","This paper proposes a novel approach to solving constrained Markov decision processes (CMDPs) with safety constraints. In particular, the authors propose to use two types of MDPs: reconnaissance MDP (R-MDP) and planning MDP, where the goal is to find a state-action pair that minimizes a reward function that maximizes the Q-function of the MDP. The main contribution of the paper is the use of a generative model to model the reward-constraint and the safety function. The proposed method is evaluated on a variety of tasks and achieves state-of-the-art performance. ","This paper proposes a novel approach to solving constrained Markov decision process (CMDP) problems. The main idea is to use a generative model to model the MDPs, and then use it to learn a reward-seeking policy for each state-action pair. The proposed method is evaluated on three CMDPs: reconnaissance MDP (R-MDP), planning PDP (P-PDP), and collision-free navigation tasks. "
882,SP:f1d4ac7d5516dd0df742e224c8c09c721d0d0886,neural architectures USED-FOR classification tasks. crossentropy loss COMPARE square loss. square loss COMPARE crossentropy loss. crossentropy loss USED-FOR neural architectures. benchmark datasets USED-FOR NLP. automatic speech recognition ( ASR ) CONJUNCTION computer vision tasks. computer vision tasks CONJUNCTION automatic speech recognition ( ASR ). neural architectures CONJUNCTION benchmark datasets. benchmark datasets CONJUNCTION neural architectures. neural architectures USED-FOR NLP. benchmark datasets USED-FOR automatic speech recognition ( ASR ). NLP CONJUNCTION automatic speech recognition ( ASR ). automatic speech recognition ( ASR ) CONJUNCTION NLP. hyper - parameter settings USED-FOR architectures. square loss USED-FOR architectures. square loss USED-FOR NLP. Cross - entropy USED-FOR computer vision tasks. square loss USED-FOR classification. cross - entropy FEATURE-OF deep learning. equal footing FEATURE-OF deep learning. OtherScientificTerm is cross - entropy loss. Task is non - vision tasks. ,This paper studies the effect of cross-entropy loss on the performance of deep neural networks on classification and NLP tasks. The authors show that the cross entropy loss is equivalent to the square loss in computer vision tasks. They also show that square loss is also equivalent to cross entropy in NLP and ASR tasks. ,"This paper proposes a new cross-entropy loss for neural networks for non-vision tasks. The proposed loss is based on the square loss, which can be used for both vision and speech recognition tasks. It is shown that the proposed loss performs better than the standard cross entropy loss for vision tasks. This paper also provides a theoretical analysis of the performance of the proposed new loss.  "
891,SP:915f1f0fc4850507c28c1d609239b41775863ebe,"self - supervised objectives FEATURE-OF reward maximization. exponential moving average USED-FOR encoder. encoder USED-FOR target representations. prior methods USED-FOR sample - efficient deep RL. future prediction objective COMPARE prior methods. prior methods COMPARE future prediction objective. data augmentation USED-FOR future prediction loss. future prediction CONJUNCTION data augmentation. data augmentation CONJUNCTION future prediction. median human - normalized score EVALUATE-FOR self - supervised objective. Atari EVALUATE-FOR self - supervised objective. future prediction PART-OF self - supervised objective. data augmentation PART-OF self - supervised objective. SPR COMPARE expert human scores. expert human scores COMPARE SPR. limited data regime EVALUATE-FOR SPR. Method are deep reinforcement learning, Self - Predictive Representations ( SPR ), and transition model. OtherScientificTerm are limited interaction, latent state representations, agent ’s representations, and environment interaction. Generic are method, and state - of - the - art. ","This paper proposes a self-supervised self-prediction objective for reward maximization in reinforcement learning, where the goal is to predict the future state of the agent’s latent state representations. The authors propose to use an exponential moving average of the encoder and the transition model to learn the target representations, which are then used for future prediction and data augmentation. They show that the proposed method is sample-efficient and achieves state-of-the-art performance on Atari games.","This paper proposes a new self-supervised self-predictive representation (SPR) objective for reward maximization in reinforcement learning. The main idea is to use an exponential moving average (e.g., a moving average of the target representations of the agent’s latent state) and a transition model to predict the future state of the agents. The authors show that the proposed method outperforms the state-of-the-art in terms of the median human-normalized score on Atari. "
900,SP:983f01c170909c8c67fd3be25f121bd61bdd8307,method USED-FOR generating single - node representations. InstantEmbedding USED-FOR generating single - node representations. InstantEmbedding HYPONYM-OF method. local PageRank computations USED-FOR InstantEmbedding. local PageRank computations USED-FOR method. approach USED-FOR globally consistent representations. DeepWalk CONJUNCTION node2vec. node2vec CONJUNCTION DeepWalk. node2vec CONJUNCTION VERSE. VERSE CONJUNCTION node2vec. VERSE CONJUNCTION FastRP. FastRP CONJUNCTION VERSE. InstantEmbedding COMPARE methods. methods COMPARE InstantEmbedding. InstantEmbedding USED-FOR single node ’s embedding. computation time CONJUNCTION memory. memory CONJUNCTION computation time. FastRP HYPONYM-OF methods. DeepWalk HYPONYM-OF methods. VERSE HYPONYM-OF methods. node2vec HYPONYM-OF methods. computation time EVALUATE-FOR InstantEmbedding. node classification CONJUNCTION link prediction. link prediction CONJUNCTION node classification. unsupervised representation learning USED-FOR tasks. unsupervised representation learning USED-FOR node classification. unsupervised representation learning USED-FOR link prediction. method USED-FOR representations. method COMPARE unsupervised representation learning. unsupervised representation learning COMPARE method. link prediction HYPONYM-OF tasks. node classification HYPONYM-OF tasks. social networks CONJUNCTION chemical molecules. chemical molecules CONJUNCTION social networks. chemical molecules CONJUNCTION knowledge graphs. knowledge graphs CONJUNCTION chemical molecules. approach USED-FOR graphs. node PART-OF graph. d - dimensional embedding vector USED-FOR node. compact representations of graphs USED-FOR approach. d - dimensional embedding vector USED-FOR graph. node classification CONJUNCTION link prediction. link prediction CONJUNCTION node classification. visualization CONJUNCTION node classification. node classification CONJUNCTION visualization. Unsupervised embeddings USED-FOR machine learning tasks. visualization HYPONYM-OF machine learning tasks. link prediction HYPONYM-OF machine learning tasks. node classification HYPONYM-OF machine,"This paper proposes a method for learning embeddings of a single node in a graph. The proposed method is based on local PageRank computations. The method is able to learn a global embedding of the entire graph, which can be used for link prediction and node classification tasks. The authors show that the proposed method can achieve state-of-the-art performance on node classification and link prediction tasks. ","This paper proposes a new method for generating embeddings for single-node representations of graphs. The proposed method, InstantEmbedding, uses local PageRank computations to generate representations for each node in the graph. The method is evaluated on a variety of tasks, including node classification, link prediction, visualization, and knowledge graph generation."
909,SP:d11037b8fe2b10aee672ba82f69410b40181f0f9,"Graph coarsening USED-FOR graph. deep learning on graphs USED-FOR graph coarsening. Laplace operator CONJUNCTION projection / lift operators. projection / lift operators CONJUNCTION Laplace operator. Laplace operator USED-FOR coarse graph. framework USED-FOR coarsening algorithm. edge weight USED-FOR coarse graph. it USED-FOR coarsening quality. graph neural networks USED-FOR weight assignment map. reduction ratios CONJUNCTION graph sizes. graph sizes CONJUNCTION reduction ratios. graph sizes CONJUNCTION graph types. graph types CONJUNCTION graph sizes. method COMPARE graph coarsening methods. graph coarsening methods COMPARE method. metrics CONJUNCTION reduction ratios. reduction ratios CONJUNCTION metrics. synthetic and real networks EVALUATE-FOR method. metrics EVALUATE-FOR graph coarsening methods. metrics EVALUATE-FOR method. reduction ratios EVALUATE-FOR method. It USED-FOR graphs. OtherScientificTerm are large - scale graphs, and essential properties. Material is large graph data. Method is data - driven methods. ",This paper proposes a novel method for graph coarsening based on deep learning. The proposed method is based on the Laplace operator and the projection/lift operators. The key idea is to use the edge weights of the coarse graph to determine the quality of the coarsened graph. The method is evaluated on both synthetic and real-world graphs. ,This paper proposes a new method for graph coarsening based on the Laplace operator and the projection/lift operators. The key idea is to use the weight assignment map of a graph neural network (GNN) to determine the edge weight of a coarse graph. The proposed method is evaluated on synthetic and real-world datasets. The results show that the proposed method outperforms the state-of-the-art methods.
918,SP:0d680213339f0e2aedb0be4aeed51423706b8bf6,localization CONJUNCTION acoustic scene analysis. acoustic scene analysis CONJUNCTION localization. environmental acoustic effects CONJUNCTION localization. localization CONJUNCTION environmental acoustic effects. Acoustic properties USED-FOR 3D audio content creation. 3D audio content creation CONJUNCTION environmental acoustic effects. environmental acoustic effects CONJUNCTION 3D audio content creation. environmental acoustic effects CONJUNCTION acoustic scene analysis. acoustic scene analysis CONJUNCTION environmental acoustic effects. Acoustic properties USED-FOR environmental acoustic effects. scattering characteristics FEATURE-OF Acoustic properties. numeric solvers USED-FOR acoustic properties. numeric solvers USED-FOR interactive applications. geometric deep learning algorithm USED-FOR characteristics. characteristics USED-FOR 3D objects. interactive rates FEATURE-OF 3D objects. discrete - laplacian and implicit encoders USED-FOR geometric deep learning algorithm. multi - layer network USED-FOR acoustic properties. multi - layer network USED-FOR arbitrary topologies. arbitrary topologies FEATURE-OF acoustic properties. NVIDIA GeForce RTX 2080 Ti GPU USED-FOR multi - layer network. accuracy EVALUATE-FOR learning method. dynamic environments FEATURE-OF generating environmental acoustic effects. Method is point cloud approximation. OtherScientificTerm is high - dimensional latent space. ,This paper proposes a method for learning acoustic properties of 3D objects in dynamic environments. The proposed method is based on a geometric deep learning algorithm that learns to predict the scattering characteristics of objects in a high-dimensional latent space. The method is trained using a discrete-laplacian and implicit encoders. The authors show that the proposed method can learn the scattering properties of arbitrary topologies and achieve state-of-the-art performance on dynamic 3D audio content generation tasks.,"This paper proposes a geometric deep learning method to learn 3D acoustic properties for 3D audio content creation. The method is based on discrete-laplacian and implicit encoders, which are used to learn the characteristics of 3D objects in a high-dimensional latent space. The paper also proposes a multi-layer network for learning the acoustic properties of arbitrary topologies. The proposed method is evaluated on the NVIDIA GeForce RTX 2080 Ti GPU, and it achieves state-of-the-art performance."
927,SP:afc33a782c43e3d4c5c4fbf047d0b1108bc30bae,"model USED-FOR extreme distributional shifts. extreme distributional shifts FEATURE-OF model ’s sensitivity. model ’s sensitivity EVALUATE-FOR model. robust optimization USED-FOR Risk Extrapolation ( REx ). perturbation set of extrapolated domains ( MMREx ) USED-FOR robust optimization. REx USED-FOR causal mechanisms. causally induced distributional shifts CONJUNCTION covariate shift. covariate shift CONJUNCTION causally induced distributional shifts. REx COMPARE methods. methods COMPARE REx. robustness EVALUATE-FOR REx. covariate shift FEATURE-OF robustness. causally induced distributional shifts FEATURE-OF robustness. Invariant Risk Minimization HYPONYM-OF methods. Task is Distributional shift. Method is machine learning prediction systems. OtherScientificTerm is causal and anti - causal elements. Generic are approach, and variant. ","This paper proposes Risk Extrapolation (REx), a method for model robustness against distributional shift. The method is based on a perturbation set of extrapolated domains (MMREx), which is an extension of robust optimization (BO) to model distributional shifts. The authors show that REx is robust to both causal and anti-causal shifts, and that it is able to learn a robust model in the presence of covariate shift.  ","This paper proposes Risk Extrapolation (REx), a method for robust optimization for model robustness to extreme distributional shifts. REx is based on a perturbation set of extrapolated domains (MMREx), which is a set of perturbations that can be used to estimate the sensitivity of the model to extreme distributions. The authors show that REx can be combined with Invariant Risk Minimization (IRM) to improve the robustness of a model against distributional shift. They also show that the REx method is more robust to covariate shift than IRM. "
936,SP:411d5bcf7698d534ad60f581d479ff74849ba4de,neural networks USED-FOR mappings between finite - dimensional Euclidean spaces. this USED-FOR neural operators. neural operators USED-FOR mappings between function spaces. neural operators USED-FOR mapping. neural operators USED-FOR partial differential equations ( PDEs ). they USED-FOR PDEs. Fourier space FEATURE-OF integral kernel. integral kernel USED-FOR neural operator. Burgers ’ equation CONJUNCTION Darcy flow. Darcy flow CONJUNCTION Burgers ’ equation. Darcy flow CONJUNCTION Navier - Stokes equation. Navier - Stokes equation CONJUNCTION Darcy flow. Fourier neural operator HYPONYM-OF ML - based method. Fourier neural operator USED-FOR turbulent flows. ML - based method USED-FOR turbulent flows. zero - shot super - resolution FEATURE-OF Fourier neural operator. zero - shot super - resolution FEATURE-OF turbulent flows. It COMPARE PDE solvers. PDE solvers COMPARE It. it COMPARE learning - based solvers. learning - based solvers COMPARE it. accuracy EVALUATE-FOR learning - based solvers. fixed resolution FEATURE-OF learning - based solvers. fixed resolution FEATURE-OF it. accuracy EVALUATE-FOR it. OtherScientificTerm is functional parametric dependence. Generic is architecture. ,"This paper proposes a method for solving partial differential equations (PDEs) in finite-dimensional Euclidean spaces using neural operators. The proposed method is based on the Fourier neural operator (Fourier NN), which is a neural network-based method for approximating the integral kernel of the Burgers’ equation and the Darcy flow. The method is shown to be able to solve PDEs with zero-shot super-resolution.   ","This paper proposes a new method for solving partial differential equations (PDEs) in the Fourier space. The main idea is to use a neural network to learn a mapping between functions in the Euclidean space and the function space of the PDEs. This mapping is then used to solve the partial differential equation (PDE) using a Fourier neural operator. The proposed method is evaluated on the Burgers’ equation, the Darcy flow, and the Navier-Stokes equation. The results show that the proposed method outperforms the state of the art."
945,SP:41d268d0eac9b4c84baa156fb641aa6d3060b5a4,"gradient flow USED-FOR linear neural network training. infinitesimal step size FEATURE-OF gradient descent. gradient descent HYPONYM-OF gradient flow. convergence direction FEATURE-OF network parameters. formulation USED-FOR convergence direction. singular vectors PART-OF tensor. network USED-FOR tensor. singular vectors USED-FOR convergence direction. tensor USED-FOR convergence direction. gradient flow USED-FOR separable classification. gradient flow USED-FOR ` 2 / L max - margin problem. transformed ” input space FEATURE-OF ` 2 / L max - margin problem. network USED-FOR transformed ” input space. orthogonally decomposable FEATURE-OF L - layer linear tensor networks. gradient flow USED-FOR global minimum. transformed input space FEATURE-OF weighted ` 1 and ` 2 norms. norm - like function FEATURE-OF global minimum. transformed input space FEATURE-OF norm - like function. weighted ` 1 and ` 2 norms FEATURE-OF norm - like function. gradient flow USED-FOR underdetermined regression. Method are tensor formulation of neural networks, and linear tensor networks. OtherScientificTerm is convergence assumptions. ","This paper studies the convergence of gradient flow in the tensor formulation of linear neural networks. In particular, the authors show that gradient flow converges to a global minimum in the transformed input space of L-layer linear tensor networks when the input space is orthogonally decomposable. The authors also show that the convergence direction of the network parameters is determined by singular vectors in the singular vectors of the input tensor.  ","This paper studies the convergence of linear tensor networks with orthogonally decomposable L-layer neural networks. The main contribution of the paper is the formulation of the convergence direction of the network parameters in terms of the singular vectors of the tensor. The convergence direction is defined as a function of the number of singular vectors in the input space. The authors show that the convergence is orthogonal to that of the norm-like function in the transformed input space, where the norm is a weighted sum of the weighted `1 and `2 norms. They show that this convergence direction converges to the global minimum of the transformed space. They also show that under certain assumptions, the convergence can be obtained for underdetermined regression. "
954,SP:e27907ef4a4e6e0f5841618fcaa7e7e0db443f91,"prediction error CONJUNCTION computational cost. computational cost CONJUNCTION prediction error. storage cost EVALUATE-FOR model. FLOPs HYPONYM-OF computational cost. They USED-FOR resource - constrained settings. mobile devices HYPONYM-OF resource - constrained settings. slimmable neural networks USED-FOR sub - networks. width - multiplier USED-FOR sub - networks. performance profiles FEATURE-OF sub - networks. prediction accuracy EVALUATE-FOR network. width - multiplier USED-FOR slimmable neural networks. approach USED-FOR slimmable networks. approach USED-FOR widthmultipliers. shared weights CONJUNCTION width - multipliers. width - multipliers CONJUNCTION shared weights. algorithm USED-FOR shared weights. algorithm USED-FOR width - multipliers. width - multipliers USED-FOR sub - networks. algorithm USED-FOR sub - networks. multiobjective optimization lens USED-FOR slimmable networks. FLOPs CONJUNCTION memory footprint. memory footprint CONJUNCTION FLOPs. method COMPARE alternatives. alternatives COMPARE method. network and dataset combinations CONJUNCTION cost objectives. cost objectives CONJUNCTION network and dataset combinations. FLOPs HYPONYM-OF cost objectives. memory footprint HYPONYM-OF cost objectives. FLOPs CONJUNCTION memory footprint. memory footprint CONJUNCTION FLOPs. ImageNet dataset EVALUATE-FOR MobileNetV2. channel counts USED-FOR layers. Method is Slimmable neural networks. OtherScientificTerm are FLOP requirements, and heterogeneous width - multipliers. Task is optimizing slimmable networks. Metric is top-1 accuracy. ","This paper proposes a method to reduce the computational cost of slimmable neural networks (NNs) in resource-constrained settings. The proposed method is based on a shared weight and width-multipliers, which are shared weights and shared width- multipliers. The authors propose a multi-objective optimization lens to optimize the width-multiplyers. The method is evaluated on the ImageNet dataset and MobileNetV2.   ",This paper proposes a new method for optimizing slimmable neural networks. The main idea is to use a multi-objective optimization lens to optimize the width-multipliers of sub-networks. The proposed method is evaluated on the ImageNet dataset and MobileNetV2 dataset. The results show that the proposed method outperforms the state-of-the-art.
963,SP:cf59403abb6ca89ccee4f8e77e9a33d99e6a00f5,"Federated SemiSupervised Learning ( FSSL ) HYPONYM-OF federated learning problem. scenarios PART-OF FSSL. method USED-FOR problems. labeled and unlabeled data USED-FOR disjoint learning. inter - client consistency loss USED-FOR FedMatch. federated learning and semi - supervised learning approaches USED-FOR FedMatch. federated learning CONJUNCTION semi - supervised learning. semi - supervised learning CONJUNCTION federated learning. method COMPARE baselines. baselines COMPARE method. method COMPARE local semi - supervised learning. local semi - supervised learning COMPARE method. local semi - supervised learning CONJUNCTION baselines. baselines CONJUNCTION local semi - supervised learning. method COMPARE method. method COMPARE method. federated learning USED-FOR baselines. semi - supervised learning USED-FOR baselines. Method is federated learning approaches. Metric is labeling cost. Task are annotation, and Federated Matching ( FedMatch ). OtherScientificTerm is expert knowledge. Material are private data, and labeled data. Generic is scenario. ","This paper proposes a federated semi-supervised learning method for federated learning with unlabeled data. The proposed method is based on federated matching (FedMatch), where the goal is to match the data from different clients. The authors propose to use an inter-clients consistency loss to improve the performance of FedMatch. The method is evaluated on a variety of federated image classification tasks.   ","This paper proposes a new method for federated semi-supervised learning (FSSL) in the context of federated matching (FedMatch). FedMatch is a federated learning problem in which the goal is to match two clients with the same set of labeled and unlabeled data. In FedMatch, each client is given a set of data points, and the goal of the task is to find the best match between the two sets of data. The authors propose a method to solve FedMatch by using the inter-client consistency loss (i.e., the consistency loss between the clients) to ensure that each client matches the data points. They show that their method outperforms the baselines in terms of the cost of labeling. "
972,SP:9457b6d430a2cd864d526d7e90bf3e1ab13d6df4,real - world users FEATURE-OF discrete event sequences. discrete event sequences USED-FOR self - supervised learning. low - dimensional fixed - length vector representations USED-FOR downstream machine learning tasks. low - dimensional fixed - length vector representations USED-FOR Self - supervised learning. contrastive learning USED-FOR audio and computer vision domains. CoLES USED-FOR discrete event sequences domain. self - supervised setting USED-FOR discrete event sequences domain. contrastive learning USED-FOR CoLES. augmentation method USED-FOR discrete event sequences. augmentation method USED-FOR CoLES. CoLES USED-FOR discrete event sequences. CoLES representations COMPARE methods. methods COMPARE CoLES representations. downstream tasks EVALUATE-FOR CoLES representations. downstream tasks EVALUATE-FOR methods. public datasets EVALUATE-FOR CoLES. ,"This paper proposes a self-supervised learning method for discrete event sequences. The proposed method is based on contrastive learning to learn a low-dimensional fixed-length vector representation of the discrete event sequence, which is then used for downstream machine learning tasks. The method is evaluated on several public datasets and achieves state-of-the-art performance on downstream tasks.","This paper proposes a new low-dimensional fixed-length vector representation for self-supervised learning of discrete event sequences. The proposed method, called CoLES, is based on contrastive learning to learn a vector representation of the discrete event sequence. The authors propose a new augmentation method for CoLES that augments the vector representations of the event sequences with a contrastive loss. Experiments on a variety of downstream tasks demonstrate the effectiveness of CoLES. "
981,SP:385942a5bcee7384bb722a1669b541f2fac0cd36,constituency grammar USED-FOR assembly of one or several corresponded words. dependency grammar CONJUNCTION constituency grammar. constituency grammar CONJUNCTION dependency grammar. constituency grammar HYPONYM-OF natural language grammars. dependency grammar HYPONYM-OF natural language grammars. StructFormer USED-FOR dependency and constituency structure. model USED-FOR dependency and constituency structure. StructFormer HYPONYM-OF model. constituency tree CONJUNCTION dependency graph. dependency graph CONJUNCTION constituency tree. parsing framework USED-FOR constituency tree. parsing framework USED-FOR dependency graph. induced dependency relations PART-OF transformer. dependency - constrained self - attention mechanism USED-FOR transformer. unsupervised dependency parsing CONJUNCTION masked language modeling. masked language modeling CONJUNCTION unsupervised dependency parsing. unsupervised constituency parsing CONJUNCTION unsupervised dependency parsing. unsupervised dependency parsing CONJUNCTION unsupervised constituency parsing. model USED-FOR unsupervised constituency parsing. model USED-FOR unsupervised dependency parsing. model USED-FOR masked language modeling. Method is unsupervised parsing methods. ,This paper proposes a transformer-based method for unsupervised dependency parsing and constituency parsing in natural language. The proposed method is based on a combination of a dependency-constrained self-attention mechanism and a constituency tree-based parsing framework. The authors show that the proposed method outperforms the state-of-the-art methods in both dependency parsing tasks and masked language modeling tasks. ,"This paper proposes a transformer-based model for unsupervised dependency parsing and constituency parsing. The proposed model is based on a dependency-constrained self-attention mechanism. The model is trained on a set of natural language models, and it can be applied to both dependency parsing as well as constituency parsing tasks.   "
990,SP:078966ff62775bba6031e47d374bda95f4a7dde3,images USED-FOR structured representations. nodes of scene graphs CONJUNCTION object bounding boxes. object bounding boxes CONJUNCTION nodes of scene graphs. annotated mapping USED-FOR nodes of scene graphs. annotated mapping USED-FOR methods. scene graph nodes CONJUNCTION visual objects. visual objects CONJUNCTION scene graph nodes. object features CONJUNCTION relational features. relational features CONJUNCTION object features. visual objects CONJUNCTION scene graph nodes. scene graph nodes CONJUNCTION visual objects. Visual Genome ( VG ) CONJUNCTION Visual Relation Detection ( VRD ) datasets. Visual Relation Detection ( VRD ) datasets CONJUNCTION Visual Genome ( VG ). model COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE model. scene graph grounding task EVALUATE-FOR state - of - the - art approaches. scene graph grounding task EVALUATE-FOR model. scene graph parsing task EVALUATE-FOR method. scene graph parsing task EVALUATE-FOR model. model COMPARE method. method COMPARE model. OtherScientificTerm is weak supervision. ,"This paper proposes a method for scene graph representation learning based on object bounding boxes and scene graphs. The proposed method is based on the observation that existing methods use an annotated mapping between nodes of scene graphs and objects in the scene graph. The authors propose to use an object-centric representation learning model to learn the object-based representation. The method is evaluated on two image datasets, namely Visual Genome (VG) and VRD.   ",This paper proposes a new method for scene graph representation learning. The proposed method is based on an annotated mapping between scene graph nodes and object bounding boxes. The method is trained on two datasets: Visual Genome (VG) and Visual Relation Detection (VRD) datasets. The model is evaluated on a scene graph grounding task and the scene graph parsing task. The results show that the proposed method outperforms state-of-the-art approaches.
999,SP:4644dbf7466b6234d8abf69995fdfb357efcc119,"Relational regularized autoencoder ( RAE ) USED-FOR distribution of data. framework USED-FOR distribution of data. reconstruction loss CONJUNCTION relational regularization. relational regularization CONJUNCTION reconstruction loss. Relational regularized autoencoder ( RAE ) HYPONYM-OF framework. relational regularization FEATURE-OF latent space. sliced fused GromovWasserstein ( SFG ) USED-FOR distributions. discrepancy CONJUNCTION relational regularization. relational regularization CONJUNCTION discrepancy. relational discrepancy USED-FOR discrepancy. spherical sliced fused Gromov Wasserstein ( SSFG ) HYPONYM-OF relational discrepancy. mixture of von Mises - Fisher distributions USED-FOR vMF distribution. power spherical distribution USED-FOR sampling time. high dimension settings FEATURE-OF sampling time. power spherical distribution USED-FOR vMF distribution. discrepancies USED-FOR variants. discrepancies USED-FOR RAE framework. image generation CONJUNCTION reconstruction. reconstruction CONJUNCTION image generation. learning latent manifold structure CONJUNCTION image generation. image generation CONJUNCTION learning latent manifold structure. autoencoders USED-FOR learning latent manifold structure. autoencoders USED-FOR image generation. autoencoders USED-FOR reconstruction. OtherScientificTerm are inner discrepancy, von Mises - Fisher distribution, and latent manifold structure. Generic are approach, it, and variant. Task is discriminative task. Method is SSFG. ","This paper proposes a relational regularized autoencoder (RAE) framework for learning the distribution of data. The proposed method is based on the sliced fused Gromov Wasserstein (SFG) distribution, which is a mixture of von Mises-Fisher (vMF) distributions. The main contribution of the paper is to use the sliced SFG distribution as a regularization term to regularize the reconstruction loss and regularization of the latent space. The authors show that the proposed method can be used for image generation and reconstruction.","This paper proposes a new variant of the relational regularized autoencoder (RAE) framework, called spherical sliced fused Gromov Wasserstein (SSFG), which is based on the von Mises-Fisher (vMF) distribution. SSFG is a mixture of two distributions, one of which is a power spherical distribution, and the other is a vMF distribution. The authors show that SSFG can be used to learn the latent manifold structure of the latent space, and that it can be applied to image generation and reconstruction tasks. "
1008,SP:5ae2c0af82cac89a65f1cc38c43e2d05ea298901,"natural language processing CONJUNCTION computer vision tasks. computer vision tasks CONJUNCTION natural language processing. computational costs CONJUNCTION training time. training time CONJUNCTION computational costs. approach USED-FOR training. training USED-FOR deep networks. approach USED-FOR deep networks. repeated structures PART-OF deep networks. transformer module HYPONYM-OF repeated structures. deep linear networks USED-FOR theoretic analysis. theoretic analysis USED-FOR adaptive untying criterion. deep linear networks USED-FOR adaptive untying criterion. method USED-FOR BERT. training time EVALUATE-FOR BERT. training time EVALUATE-FOR method. OtherScientificTerm are deep learning model sizes, repeated layers, weight sharing, and monitoring gradient statistics. Method is deep network. ", and BERT. This paper proposes a method to reduce the computational cost of training deep neural networks. The proposed method is based on the observation that repeated layers in deep networks can be viewed as repeated structures in a transformer module. The authors propose to use the adaptive untying criterion to find the repeated layers that are most relevant to the training process. The method is evaluated on BERT and CIFAR-10 datasets.  ,This paper proposes an adaptive untying criterion for training deep neural networks. The key idea is to use a transformer module of a deep linear network to learn the repeated structures of the network. The proposed method is evaluated on the BERT dataset and shows that it can reduce the training time of BERT by up to 1.5x.
1017,SP:a51710551142316b67e2fccd969fea1ece35ba39,interaction inside adversarial perturbations USED-FOR adversarial transferability. adversarial transferability CONJUNCTION interaction. interaction CONJUNCTION adversarial transferability. negative correlation FEATURE-OF adversarial transferability. DNNs USED-FOR negative correlation. negative correlation USED-FOR transferability - boosting methods. methods USED-FOR transferability. interactions USED-FOR attacking process. OtherScientificTerm is adversarial perturbations. ,"This paper studies adversarial transferability in the presence of adversarial perturbations. In particular, the authors study the effect of the interaction between the perturbation and the training data. The authors show that the interaction can be seen as a way to measure the adversarial threat. They show that this interaction is correlated with adversarial risk. They then propose a transferability-boosting method based on this observation and show that it can be used to improve the transferability of adversarially perturbed data.","This paper studies the problem of adversarial transferability, i.e., the ability of an adversarial perturbation to transfer from one adversary to another. The authors study the problem in terms of negative correlation between adversarial attacks and adversarial interactions. They show that negative correlation can be used to improve transferability-boosting methods. They also provide a theoretical analysis of the relationship between negative correlation and transferability boosting methods."
1033,SP:f1565319075c1442c2cb52d96443facb492c06c2,"neural network ( hidden ) representations CONJUNCTION task semantics. task semantics CONJUNCTION neural network ( hidden ) representations. deeper layers USED-FOR forgetting. sequential training USED-FOR task representational subspaces. Methods USED-FOR forgetting. Methods USED-FOR deeper layers. maximal forgetting FEATURE-OF task sequences. forgetting CONJUNCTION task semantic similarity. task semantic similarity CONJUNCTION forgetting. intermediate similarity FEATURE-OF task sequences. Task is Catastrophic forgetting. Method are deep learning models, and neural representations. Generic are some, and others. OtherScientificTerm are feature reuse, task representations, and interference. ","This paper studies the problem of catastrophic forgetting in deep neural networks. The authors show that there is a trade-off between forgetting and task semantic similarity between tasks. They show that the forgetting is caused by interference between task representations and task semantics. They propose two methods to mitigate this interference: (1) sequential training with task representational subspaces, and (2) deeper layers. ","This paper studies the problem of catastrophic forgetting in deep neural networks. The authors study the problem in the context of feature reuse and task semantic similarity. They show that the forgetting happens when the task representations of a deep neural network are different from the representations of the original task representations. They also show that there is interference between task representations and the original representations, and that this interference can lead to catastrophic forgetting. They propose two methods to address this issue. First, they propose a method to learn task representational subspaces that are more similar to the original representation. Second, they show that deep layers can be used to improve the forgetting."
1049,SP:30d7532cdcf420bff3be6b92eea3d93bce59e6bd,"XLNet CONJUNCTION T5. T5 CONJUNCTION XLNet. BERT CONJUNCTION XLNet. XLNet CONJUNCTION BERT. Deep, heavily overparameterized language models USED-FOR natural language processing ( NLP ) tasks. T5 HYPONYM-OF Deep, heavily overparameterized language models. XLNet HYPONYM-OF Deep, heavily overparameterized language models. BERT HYPONYM-OF Deep, heavily overparameterized language models. training time USED-FOR pre - training and fine - tuning. computation resources CONJUNCTION training time. training time CONJUNCTION computation resources. computation resources USED-FOR model complexity. model compression USED-FOR large NLP models. large batch sizes USED-FOR pre - training time. training algorithm USED-FOR pre - training and fine - tuning. Early - Bird Lottery Tickets USED-FOR computer vision tasks. training algorithm USED-FOR large - scale language models. EarlyBERT HYPONYM-OF training algorithm. pre - training and fine - tuning USED-FOR large - scale language models. self - attention CONJUNCTION fully - connected sub - layers. fully - connected sub - layers CONJUNCTION self - attention. fully - connected sub - layers PART-OF transformer. structured winning tickets PART-OF BERT training. tickets USED-FOR BERT training. GLUE and SQuAD downstream tasks EVALUATE-FOR pre - training and fine - tuning. EarlyBERT COMPARE BERT. BERT COMPARE EarlyBERT. training time EVALUATE-FOR EarlyBERT. training time EVALUATE-FOR BERT. Metric is inference time. OtherScientificTerm are training process, and computational resource demands. ","This paper proposes a method for pre-training and fine-tuning large language models. The method is based on the idea of lottery tickets, which are structured winning tickets for computer vision tasks. The idea is to use the tickets as early-bird tickets in BERT training. The authors show that this method can reduce the training time and inference time of BERT by a significant margin. The proposed method is evaluated on GLUE and SQuAD tasks.","This paper proposes a new training algorithm for large-scale language models, called EarlyBERT, which pre-train and fine-tune a language model with large batch sizes. The authors propose a new method for pre-training and fine tuning a large language model. The proposed method is based on lottery tickets, which are structured winning tickets for BERT training.  The authors show that the proposed method outperforms BERT on GLUE and SQuAD downstream tasks. "
1065,SP:c547f23ff6caaf5e9f35d258490b86ae0ac8ed03,"classifier ’s predictions CONJUNCTION supervised labels. supervised labels CONJUNCTION classifier ’s predictions. f -divergence measure USED-FOR classifier ’s predictions. variational form USED-FOR decoupling property. variational difference CONJUNCTION bias term. bias term CONJUNCTION variational difference. decoupling property USED-FOR f -divergence measures. clean distribution FEATURE-OF variational difference. variational difference PART-OF divergence. bias term PART-OF divergence. robustness EVALUATE-FOR f -divergence functions. robustness FEATURE-OF f -divergence functions. f -divergence functions USED-FOR metrics. OtherScientificTerm are label noise, noise, and labels ’ noise rate. Task is learning with noisy labels. Generic is they. Material is UCSC - REAL. Metric is Robust - f - divergence - measures. ","This paper studies the robustness of f-divergence measures for learning with noisy labels. The authors propose to use the variational difference as a measure of robustness to label noise, and show that it is decoupled from the bias term of the f-Divergence measure. They also show that the bias is independent of the classifier’s predictions.   ","This paper studies the robustness of f-divergence measures for learning with noisy labels. The main contribution of the paper is a new measure of robustness to label noise, i.e., a measure that can be used to measure the classifier’s predictions and supervised labels. This measure is based on the variational difference (i.e. the difference between the true and noisy labels) and the bias term, which is a function of the true labels’ noise rate. The authors show that the f-Divergence measure can be decoupled from the true label noise by using the decoupling property of the classifiers’ predictions and the supervised labels, and show that it can also decouple from the bias terms. The paper also provides a theoretical analysis of the properties of the measure. "
1081,SP:841888179dcdac901889c8d62cb5234311fe28f1,"Q - ensemble USED-FOR uncertainty estimates. uncertainty estimates USED-FOR ensemble - based weighted Bellman backups. method USED-FOR learning. continuous and discrete control benchmarks EVALUATE-FOR method. weighted Bellman backups COMPARE Bellman backups. Bellman backups COMPARE weighted Bellman backups. weighted Bellman backups CONJUNCTION UCB Exploration. UCB Exploration CONJUNCTION weighted Bellman backups. ensemble USED-FOR weighted Bellman backups. off - policy RL algorithms USED-FOR continuous and discrete control tasks. lowdimensional and high - dimensional environments FEATURE-OF continuous and discrete control tasks. Bootstrap USED-FOR diversity. Soft Actor - Critic and Rainbow DQN HYPONYM-OF off - policy RL algorithms. Material is challenging domains. Task is Q - learning. OtherScientificTerm are Q - estimates, and noisy rewards. Metric is signal - to - noise aspect. ","This paper proposes a method for learning uncertainty estimates for Q-ensembles in continuous and discrete control tasks. The method is based on weighted Bellman backups, which is an ensemble-based method for estimating uncertainty estimates in Q-Ensembles. The proposed method is shown to outperform Bellman-based methods in continuous control tasks and UCB exploration tasks.  ","This paper proposes a new ensemble-based weighted Bellman backup method for continuous and discrete control tasks. The proposed method is based on an ensemble of Q-ensembles, where each ensemble member is trained to estimate the uncertainty of the Q-values of the reward function. The ensemble is composed of a soft actor-critic, a Rainbow DQN, and a UCB explorer. The authors show that their method outperforms Bellman backups and UCB exploration on a variety of continuous control benchmarks."
1097,SP:afc08f203562b841180811aef943bfb63a1659ea,"meta - learning algorithms USED-FOR fewshot classification problems. few - shot classification framework USED-FOR modeling uncertainty. meta - training USED-FOR model. class - wise similarities USED-FOR distributional mismatch. meta - learning models PART-OF method. training strategy USED-FOR model. training strategy USED-FOR calibrated classification. Task are prediction of uncertainty, and random sampling of tasks. OtherScientificTerm is dataset shift. Metric is accuracy. ","This paper proposes a meta-learning method for few-shot classification problems. The proposed method is based on a class-wise similarity-based meta-training approach, where the model is trained to model the distributional mismatch between the training set and the test set. The authors show that the proposed method achieves better performance than the baselines in terms of uncertainty and calibration. ","This paper proposes a meta-learning framework for few-shot classification, where the goal is to model the uncertainty in the data distribution. The authors propose a method to model uncertainty by training a class-wise similarity model, which is used to predict the distributional mismatch between the training data and the real data. They show that this method can be applied to a variety of fewshot classification problems, and show that it can be used to improve the accuracy of the model.  "
1113,SP:12ae325ea3bce1e60195afac7d85895d2d20c29c,"dominant paradigm USED-FOR video - text representations. generative model USED-FOR method. VATEX CONJUNCTION ActivityNet. ActivityNet CONJUNCTION VATEX. ActivityNet CONJUNCTION MSVD. MSVD CONJUNCTION ActivityNet. MSR - VTT CONJUNCTION VATEX. VATEX CONJUNCTION MSR - VTT. method COMPARE others. others COMPARE method. MSVD EVALUATE-FOR method. ActivityNet EVALUATE-FOR method. VATEX EVALUATE-FOR method. MSR - VTT EVALUATE-FOR others. MSR - VTT EVALUATE-FOR method. Method are noise contrastive learning, and dissimilar representations. Generic is representations. OtherScientificTerm are visually similar videos, and depicted action. ","This paper proposes a method for video-text representation learning based on contrastive learning. The proposed method is based on the observation that the representations of two videos with similar action sequences should be similar. The authors propose to use a contrastive loss to encourage the representations to be dissimilar. The method is evaluated on VATEX, ActivityNet and MSR-VTT.","This paper proposes a new method for video-text representation learning based on contrastive learning. The proposed method is based on the dominant paradigm for video representation learning, which is to learn representations that are dissimilar to the original video. The key idea is to use a generative model to learn dissimilar representations of the video, and then use a contrastive model for the video representation. The method is evaluated on a variety of datasets, including MSR-VTT, VATEX, ActivityNet, and MSVD. "
1129,SP:8a71d8fad25a126aff01431cacf348c05de75667,pre - trained language models ( PLMs ) USED-FOR Chinese natural language processing ( NLP ) tasks. single vocabulary USED-FOR masked language model pre - training. Chinese word segmentation ( CWS ) CONJUNCTION subword tokenization. subword tokenization CONJUNCTION Chinese word segmentation ( CWS ). seg tok USED-FOR Chinese BERT. Chinese word segmentation ( CWS ) USED-FOR method. subword tokenization USED-FOR method. multi - vocabulary pretraining ( MVP ) USED-FOR models expressiveness. char based vocabulary COMPARE seg tok. seg tok COMPARE char based vocabulary. MVP USED-FOR PLMs. seg tok USED-FOR Chinese PLMs. it USED-FOR seg tok. char based vocabulary USED-FOR Chinese PLMs. seg tok COMPARE it. it COMPARE seg tok. sequence labeling tasks EVALUATE-FOR it. sentence level tasks EVALUATE-FOR Chinese PLMs. sequence labeling tasks EVALUATE-FOR seg tok. sentence level tasks EVALUATE-FOR seg tok. OtherScientificTerm is Chinese characters. ,This paper proposes a method for pre-training a Chinese language model on Chinese text. The method is based on using a pre-trained BERT language model with a masked language model pretrained with a single vocabulary. The key idea is to train the language model using a large vocabulary of Chinese characters. The authors show that this method can improve the performance of the model on sentence-level tasks.,"This paper proposes a method for pre-training a Chinese language model on Chinese NLP tasks. The proposed method is based on multi-vocabulary pretraining (MVP) to improve the expressiveness of Chinese language models. The method is evaluated on a variety of tasks, including Chinese word segmentation (CWS), subword tokenization, and sentence-level tasks."
1145,SP:b93ec7bc02b48068073ffe705f71d2643e663d51,"Graph Convolutional Networks ( GCNs ) USED-FOR graph - based learning tasks. graph partition CONJUNCTION distributed training. distributed training CONJUNCTION graph partition. memory CONJUNCTION communications. communications CONJUNCTION memory. boundary nodes PART-OF partitioned subgraph. BDS - GCN USED-FOR distributed GCN training. method USED-FOR distributed GCN training. BDS - GCN HYPONYM-OF method. unbiased boundary sampling strategy USED-FOR BDS - GCN. full - graph accuracy EVALUATE-FOR method. full - graph accuracy EVALUATE-FOR BDS - GCN. unbiased boundary sampling strategy USED-FOR method. accuracy EVALUATE-FOR state - of - the - art methods. throughput EVALUATE-FOR BDS - GCN. BDS - GCN USED-FOR GCN training. Method are GCNs, and GCN architectures. Material is real - world large graphs. OtherScientificTerm is GCN structures. Metric is memory usage. ",This paper proposes a novel method for distributed graph convolutional networks (GCNs) based on partitioning the graph into subgraphs and sampling the boundary nodes in each subgraph to reduce the communication cost and memory usage. The proposed method is based on an unbiased boundary sampling strategy. The authors show that the proposed method achieves state-of-the-art performance on graph classification tasks.   ,This paper proposes a new method for graph convolutional networks (GCN) training. The main idea is to partition the graph into subgraphs and use a boundary sampling strategy to sample the subgraph boundary nodes from each subgraph. The authors show that the proposed method outperforms state-of-the-art GCN training methods in terms of full-graph accuracy and memory usage. 
1161,SP:2d4ba873d11e969ebd1fc31f9b5ab450c964d154,"Machine Learning ( ML ) USED-FOR large - scale physics - based simulations. models USED-FOR real large - scale and complex problems. quantum chemistry simulations USED-FOR catalyst discovery. model USED-FOR quantum chemistry simulations. catalyst discovery USED-FOR renewable energy applications. model USED-FOR catalyst discovery. ForceNet USED-FOR quantum chemistry simulations. ForceNet HYPONYM-OF graph neural network. graph neural network USED-FOR per - atom forces. surrounding 3D molecular structure USED-FOR per - atom forces. surrounding 3D molecular structure USED-FOR graph neural network. model scaling USED-FOR ForceNet. expressive message passing architecture USED-FOR ForceNet. basis and non - linear activation functions USED-FOR ForceNet. ForceNet COMPARE ML models. ML models COMPARE ForceNet. ForceNet USED-FOR atomic forces. ForceNet USED-FOR large - scale catalyst dataset. OC20 HYPONYM-OF large - scale catalyst dataset. ForceNet USED-FOR quantum chemistry simulations. ForceNet COMPARE ML models. ML models COMPARE ForceNet. success rate EVALUATE-FOR ML models. success rate EVALUATE-FOR ForceNet. ML - based simulations COMPARE physics - based simulations. physics - based simulations COMPARE ML - based simulations. Task is atomic simulations. OtherScientificTerm are 3D space, forces, and out - of - distribution structures. ","This paper proposes a novel graph neural network (GNN) architecture for quantum chemistry simulations. The main idea is to use GNNs to model the per-atomic forces between atoms in a 3D molecular structure, which are then used to predict the surrounding surrounding 3d molecular structure. The proposed method is evaluated on a large-scale quantum chemistry simulation dataset, and compared with state-of-the-art methods. ","This paper proposes a graph neural network (GNN) for large-scale quantum chemistry simulations. The main idea is to use graph neural networks to model the per-atom forces in the surrounding 3D molecular structure. The authors propose a message passing architecture to express the message passing of the GNNs. The model is trained on the OC20 dataset, which is a large quantum chemistry simulation dataset. The experiments show that the proposed model outperforms the state-of-the-art in terms of success rate. "
1177,SP:8bdcf4fe6abf4739d4732b7ea8538513135dcccc,regularisation USED-FOR fine - tuning. approaches USED-FOR regularisation. approaches USED-FOR fine - tuning. regularisation USED-FOR deep neural networks. Rademacher complexity USED-FOR neural network generalisation bound. bound COMPARE bounds. bounds COMPARE bound. bounds USED-FOR convolutional networks. bound USED-FOR fine - tuning. learning USED-FOR generalisation. initialisation FEATURE-OF network. transfer learning USED-FOR initialisation. transfer learning USED-FOR network. fine - tuning algorithm USED-FOR hypothesis class. generalisation EVALUATE-FOR transfer learning. It COMPARE fine - tuning competitors. fine - tuning competitors COMPARE It. It COMPARE penalty - based alternatives. penalty - based alternatives COMPARE It. Generic is algorithm. OtherScientificTerm is radius of the search space. ,"This paper studies the generalization properties of neural networks trained with regularization. The authors show that the Rademacher complexity of fine-tuning a neural network is a function of the radius of the search space and the number of parameters. They show that this generalization bound depends on the initialisation of the network, and they show that transfer learning can be used to learn the initialization of a network. They then propose an algorithm that uses transfer learning to find a hypothesis class that maximizes the transfer learning loss. ",This paper proposes a new generalization bound for fine-tuning neural networks. The generalization bounds are based on the Rademacher complexity of neural network generalization. The main contribution of the paper is that it provides a new bound for the generalization of neural networks with transfer learning. The authors show that their bound is more general than the previous bounds on the generalisation of convolutional networks. 
1193,SP:3a3249e97ef2345ea2264de5ed8287e16687838e,"model evaluation EVALUATE-FOR mask discovery. training configuration USED-FOR mask. mask discovery ( Hfind ) CONJUNCTION mask evaluation ( Heval ). mask evaluation ( Heval ) CONJUNCTION mask discovery ( Hfind ). hyperparameters USED-FOR mask evaluation ( Heval ). hyperparameters USED-FOR mask discovery ( Hfind ). unstructured magnitude pruning USED-FOR vision classification tasks. hyperparameters USED-FOR stages. unstructured magnitude pruning USED-FOR decoupled find - eval phenomenon. hyperparameters USED-FOR masks. Hfind values USED-FOR masks. layerwise pruning ratios FEATURE-OF masks. ratios USED-FOR decoupled find - eval phenomenon. Task is model pruning. Generic are model, and models. Method are lottery ticket framework, and one - shot structured pruning. OtherScientificTerm is decoupling hyperparameters. ",This paper studies the correlation between mask discovery and mask evaluation in model pruning. The authors propose a lottery ticket framework for finding the best mask for each layer in the training set. They show that the find-eval phenomenon occurs when the hyperparameters of the two stages are decoupled. They also show that one-shot structured pruning can be used to find the best masks for each stage. ,"This paper proposes a lottery ticket-based method for model pruning. The method is based on unstructured magnitude pruning, where each layer is pruned in a one-shot fashion. The authors show that layer-wise pruning results in a decoupled find-eval phenomenon. They also show that mask discovery and mask evaluation can be correlated.   "
1209,SP:2d6f5d72b21675f74ff4cde4d16bfb36abd5795f,"training USED-FOR alignment of per - example gradients. metrics COMPARE m - coherence. m - coherence COMPARE metrics. m - coherence COMPARE O(m ). O(m ) COMPARE m - coherence. memorization CONJUNCTION generalization. generalization CONJUNCTION memorization. ResNet CONJUNCTION EfficientNet models. EfficientNet models CONJUNCTION ResNet. m - coherence USED-FOR evolution of alignment of per - example gradients. label noise FEATURE-OF variants. ImageNet EVALUATE-FOR EfficientNet models. m - coherence COMPARE real labels. real labels COMPARE m - coherence. neural networks USED-FOR generalization. OtherScientificTerm are gradient, and gradient diversity. Method are Coherent Gradients ( CG ) theory, over - parameterized neural networks, and CG. ",This paper studies the alignment of per-example gradients in training. The authors propose a new metric called m-coherence to measure the coherence between the gradients of training examples. They show that this metric can be used to evaluate the generalization ability of over-parameterized neural networks. They also show that it is a better metric than O(m) in terms of generalization performance.,"This paper proposes a new metric, called m-coherence, to measure the alignment of per-example gradients during training. The authors show that m-Coherence can be used as a measure of the generalization ability of over-parameterized neural networks. They also show that it can be a good measure of memorization and generalization.  The authors also provide a theoretical analysis of the correlation between m- coherence and memorization. They show that the correlation is due to the fact that the coherence of the per-instance gradients depends on the diversity of the training data. "
1225,SP:e7c5de9a475d0ba71bc79580e8436024fb2c6f59,"summary statistics USED-FOR implicit generative models. approach USED-FOR approximate Bayesian computation. approximate Bayesian computation CONJUNCTION neural likelihood methods. neural likelihood methods CONJUNCTION approximate Bayesian computation. approach USED-FOR neural likelihood methods. tasks EVALUATE-FOR approach. Task are evaluation of the likelihood function, and constructing sufficient statistics. OtherScientificTerm are likelihood function, sufficient statistics, and density or density ratio. Generic is model. Method are deep neural networks, and infomax learning procedure. ","This paper proposes a method to learn sufficient statistics for implicit generative models. The method is based on infomax learning, where the objective is to find sufficient statistics that are close to the density or density ratio of the true likelihood function. The authors show that the proposed method can be used to approximate approximate Bayesian computation and neural likelihood methods. The proposed method is shown to outperform the state-of-the-art methods on several tasks.","This paper proposes an approach to approximate Bayesian computation for implicit generative models. The main idea is to use an infomax learning procedure to construct sufficient statistics for the likelihood function. The authors show that the proposed approach can be applied to a wide range of tasks. The proposed approach is evaluated on a variety of tasks, and it is shown to outperform the state-of-the-art."
1241,SP:c5997bf2348e94949684f45fbd418661e85220c1,"set - level supervision USED-FOR data collection. paired images CONJUNCTION domain labels. domain labels CONJUNCTION paired images. model COMPARE set - level supervised model. set - level supervised model COMPARE model. pseudo domains HYPONYM-OF hyperparameters. full labels USED-FOR set - level supervised model. TUNIT USED-FOR semi - supervised scenario. Method is image - to - image translation model. Task is image - to - image translation. OtherScientificTerm are image domains, and estimated domains. ","This paper proposes a semi-supervised image-to-image translation model, which is based on a set-level supervised model with pseudo-domains and pseudo-domain labels. The proposed model is trained with paired images and domain labels, and is trained in a supervised fashion. The authors show that the proposed model outperforms the set-wise supervised models in terms of accuracy and training time.   ","This paper proposes an image-to-image translation model for semi-supervised data collection. The proposed model is based on TUNIT, which is a set-level supervised model with pseudo-domains and pseudo-domain hyperparameters. The model is trained on a set of paired images and domain labels. It is shown that the proposed model outperforms the baseline model in terms of performance on the task of image to image translation. "
1257,SP:0cd97e64e638cabbeea0fdef3e9c5b33f4000f72,"probability distribution USED-FOR network parameters. it USED-FOR initialization procedures. probability distribution USED-FOR curvature penalty function. asymmetric initialization USED-FOR constant curvature penalty. natural cubic spline interpolation USED-FOR solution function. uniform distribution USED-FOR asymmetric initialization. multivariate regression CONJUNCTION activation functions. activation functions CONJUNCTION multivariate regression. regularization strength FEATURE-OF spatially adaptive smoothing splines. spatially adaptive smoothing splines USED-FOR training trajectories. Method are wide neural networks, and width - n shallow ReLU network. OtherScientificTerm are implicit bias in function space, and weighted second derivative. Task is 1D regression. ", sparsified splines are used to improve the performance of wide ReLU networks. The authors show that sparsifying splines can be used to reduce the implicit bias in function space. They also show that the regularization strength of the regularized splines is dependent on the width of the network. ,"This paper studies the implicit bias in function space of a wide ReLU network with width-n shallow ReLU networks. The authors show that there is a constant curvature penalty in the network parameters, which is the result of a natural cubic spline interpolation in the function space. They show that this is due to a uniform distribution of the parameters of the network. They also show that the regularization strength of spatially adaptive smoothing splines is proportional to the number of training trajectories.  "
1273,SP:8b885142facbb3b8db41ec9d83822cee81324694,Weight decay HYPONYM-OF regularization technique. regularization technique USED-FOR deep neural networks. L2 regularization USED-FOR weight decay. L2 regularization USED-FOR deep learning libraries. L2 regularization COMPARE weight decay. weight decay COMPARE L2 regularization. weight decay USED-FOR adaptive gradient methods. L2 regularization USED-FOR adaptive gradient methods. Decoupled Weight Decay ( AdamW ) USED-FOR Adam. Adaptive Momentum Estimation ( Adam ) HYPONYM-OF adaptive gradient methods. L2 regularization CONJUNCTION decoupled weight decay. decoupled weight decay CONJUNCTION L2 regularization. decoupled weight decay PART-OF deep learning libraries. decoupled weight decay HYPONYM-OF weight decay. L2 regularization HYPONYM-OF weight decay. unstable weight decay USED-FOR optimizers. stochastic gradient descent ( SGD ) HYPONYM-OF Momentum. stochastic gradient descent ( SGD ) HYPONYM-OF optimizers. Momentum USED-FOR optimizers. decoupled weight decay USED-FOR adaptive gradient methods. Stable Weight Decay ( SWD ) method USED-FOR unstable weight decay problem. L2 regularization CONJUNCTION decoupled weight decay. decoupled weight decay CONJUNCTION L2 regularization. SWD method COMPARE decoupled weight decay. decoupled weight decay COMPARE SWD method. SWD method COMPARE L2 regularization. L2 regularization COMPARE SWD method. weight decay PART-OF Adam. hyperparameters FEATURE-OF Adam variants. SWD USED-FOR Adam. SWD USED-FOR weight decay. OtherScientificTerm is hyperparameter. ,"This paper proposes a new regularization technique called stable weight decay (SWD) to improve the performance of adaptive gradient methods with L2 regularization. The main idea is to decompose the weight decay into two terms: 1) weight decay is used as a regularization term in L2-regularized gradient methods, and 2) it is used in adaptive momentum estimation (Adam). The main contribution of the paper is to show that SWD can be used in combination with adaptive momentum estimators (AdamW) and decoupled weight decay to improve their performance.","This paper proposes a new regularization technique for weight decay in adaptive gradient methods. The authors propose a new method called stable weight decay (SWD) that can be used in combination with L2 regularization and decoupled weight decay to improve the performance of Adam. They show that SWD outperforms L2 in terms of stability, and that it can be combined with decoupling weight decay and AdamW to improve performance."
1289,SP:a3206dc71e32ba1830895bf442d3840f3331a532,"Translation Memory ( TM ) USED-FOR neural machine translation ( NMT ). translation quality EVALUATE-FOR neural machine translation ( NMT ). translation quality EVALUATE-FOR Translation Memory ( TM ). encoder USED-FOR TM. TM CONJUNCTION NMT. NMT CONJUNCTION TM. method USED-FOR NMT. method USED-FOR TM. encoder USED-FOR TM information. pre - trained language model ( PLM ) USED-FOR encoder. sentence level retrieval method USED-FOR n - gram retrieval method. methods USED-FOR information flow. TM CONJUNCTION NMT decoder. NMT decoder CONJUNCTION TM. translation quality EVALUATE-FOR methods. OtherScientificTerm are semantic relationship, and similarity score. Method is sentence level retrieval approach. ","This paper proposes a method to improve the performance of neural machine translation (NMT) using Translation Memory (TM). The authors propose to use a pre-trained language model (PLM) as the encoder to extract the TM information from the NMT decoder, and then use a sentence-level retrieval method to retrieve the n-grams from the TM. The authors show that the proposed method can improve the quality of the final NMT results. ",This paper proposes a method to improve the translation quality of neural machine translation (NMT) by using a pre-trained language model (PLM) to learn the information flow between the encoder and the NMT decoder. The PLM is trained to predict the semantic relationship between the n-grams of a sentence. The authors propose a sentence level retrieval method to retrieve the information from the ngrams. They show that their method can improve the performance of NMT on a variety of tasks.
1305,SP:72b43991a242872b2ceb1861e8ffbdf26c9f4818,rate reduction CONJUNCTION ( shift ) invariant classification. ( shift ) invariant classification CONJUNCTION rate reduction. rate reduction USED-FOR deep ( convolutional ) networks. ( shift ) invariant classification USED-FOR deep ( convolutional ) networks. iterative gradient ascent scheme USED-FOR rate reduction of learned features. iterative gradient ascent scheme USED-FOR deep network. components PART-OF network. network USED-FOR discriminative deep representation. linear operators PART-OF multi - channel convolutions. spectral domain FEATURE-OF convolutional network. Generic is architectures. Method is back propagation training. Task is classification. , invariant classification is an important problem in machine learning. This paper proposes a novel method to improve the performance of deep convolutional neural networks. The proposed method is based on an iterative gradient ascent scheme. The authors show that the proposed method improves the performance on classification tasks.   ,"This paper studies the problem of rate reduction in deep convolutional neural networks (DNNs) and shift invariant classification in the spectral domain. In particular, the authors show that the rate reduction of learned features in DNNs is invariant to shift invariance. They also show that rate reduction can be achieved by using an iterative gradient ascent scheme. The authors also provide a theoretical analysis of the convergence rate of the proposed method. "
1321,SP:f8b02cf1b918b0956761829ec6ef9127596071ec,"implicit acceleration of gradient flow USED-FOR over - parameterized two - layer linear models. conservation law USED-FOR implicit acceleration. spectrum USED-FOR acceleration. matrix factorization problem CONJUNCTION Riccati type differential equations. Riccati type differential equations CONJUNCTION matrix factorization problem. small, balanced or spectral initialization FEATURE-OF weights. Method is gradient flow. OtherScientificTerm is Gramian matrices. ",This paper studies the implicit acceleration of gradient flow in over-parameterized two-layer linear models with matrix factorization. The authors prove a conservation law on the spectrum of the gradient flow and show that it converges to a stationary point in the form of a convex function. They also show that the acceleration can be expressed as a function of the number of weights in the model.  ,"This paper studies the implicit acceleration of gradient flow in over-parameterized two-layer linear models. In particular, the authors consider the case where the weights of the weights are small, balanced or spectral initialization. The authors show that under certain conditions, the acceleration of the gradient flow is bounded by a conservation law. They also show that the acceleration can be bounded by the spectrum of the spectrum.  "
1337,SP:e5f086c806be88d50e461a782b5b00124f4656fb,"approach USED-FOR opaque model ’s behavior. uniform sampling of user - defined subspaces USED-FOR framework. framework USED-FOR ML model. CLIME USED-FOR ML model. CLIME HYPONYM-OF framework. Method are machine learning techniques, LIME, surrogate interpretable model, LIME framework, and estimation algorithm. Task are explainable AI, OOD sampling problem, OOD sampling, and real - world problems. OtherScientificTerm are LIME ’s explanations, adversarial attacks, perturbation procedure, and logical constraints. Generic is model. Metric are fidelity, and accuracy. ","This paper proposes a new framework for explainable AI, called CLIME, which aims to learn a surrogate interpretable model by sampling from a uniform sampling of user-defined subspaces. The proposed method is based on the LIME framework, and the authors propose an estimation algorithm to estimate the estimation error of the surrogate explainable model. The authors show that the proposed method can be used to improve the fidelity and accuracy of the interpretable AI model. ","This paper proposes a new approach to explainable AI, called CLIME, which is based on the LIME framework. The main idea of CLIME is to use a surrogate interpretable model (i.e., a user-defined subspace) to model the opaque model’s behavior. The surrogate model is then used to estimate the fidelity of the surrogate model to the true model. This surrogate model can then be used to improve the performance of the original model. The proposed CLIME framework is evaluated on a variety of real-world datasets."
1353,SP:b1d5ef15772e192eb8c8a0e65b3c21ee7c794295,Pre - trained language models USED-FOR natural language understanding ( NLU ). BERT HYPONYM-OF Pre - trained language models. Chinese HYPONYM-OF languages. English HYPONYM-OF languages. multi - word expressions USED-FOR natural lexical units. fine - grained and coarse - grained tokenizations USED-FOR pre - trained language models. AMBERT HYPONYM-OF Multi - grained BERT. AMBERT HYPONYM-OF pre - trained language model. fine - grained and coarse - grained tokenizations USED-FOR pre - trained language model. contextualized representations of the words CONJUNCTION contextualized representations of the phrases. contextualized representations of the phrases CONJUNCTION contextualized representations of the words. encoder CONJUNCTION encoder. encoder CONJUNCTION encoder. English USED-FOR AMBERT. encoder USED-FOR AMBERT. GLUE CONJUNCTION SQuAD. SQuAD CONJUNCTION GLUE. SQuAD CONJUNCTION RACE. RACE CONJUNCTION SQuAD. CLUE CONJUNCTION GLUE. GLUE CONJUNCTION CLUE. Chinese CONJUNCTION English. English CONJUNCTION Chinese. Chinese HYPONYM-OF benchmark datasets. English HYPONYM-OF benchmark datasets. RACE HYPONYM-OF benchmark datasets. SQuAD HYPONYM-OF benchmark datasets. CLUE HYPONYM-OF benchmark datasets. GLUE HYPONYM-OF benchmark datasets. AMBERT COMPARE models. models COMPARE AMBERT. Chinese EVALUATE-FOR AMBERT. AMBERT COMPARE AMBERT. AMBERT COMPARE AMBERT. Method is coarse - grained tokenization. Metric is inference time. ,"This paper proposes a pre-trained language model, called AMBERT, that combines fine-grained tokenization and contextualized representations of the words and phrases to improve the performance of natural language understanding (NLU) in English and Chinese. The proposed model is based on a multi-word encoder and encoder-decoder architecture, where the encoder encodes both the context and the word embeddings into a single embedding space. The encoder is trained to predict the context of the tokens and the encoders are trained to generate the tokens. The model is evaluated on GLUE, CLUE, SQuAD, RACE, and CLUE. ","This paper presents AMBERT, a pre-trained language model for multi-lingual natural language understanding (NLU) that is able to learn multi-word expressions in English and Chinese. The model is built on top of BERT, which is a fine-grained language model. The authors propose a novel multi-language language model that is capable of learning both fine and coarse word representations. The proposed model is evaluated on a variety of datasets, including GLUE, CLUE, SQuAD, RACE, and CLUE.  "
1369,SP:fd1cfe80343d3789227d99d836a5674374a234f5,"task USED-FOR natural language utterance. Semantic parsing HYPONYM-OF task. natural language utterance USED-FOR machine - understandable information representation. task USED-FOR machine - understandable information representation. Transformer USED-FOR semantic parsing. PhraseTransformer architecture USED-FOR meaning representation. phrase dependencies USED-FOR PhraseTransformer architecture. phrase dependencies USED-FOR meaning representation. Long Short - Term Memory ( LSTM ) USED-FOR local context of phrases. Self - Attention mechanism USED-FOR local context of phrases. Long Short - Term Memory ( LSTM ) PART-OF Self - Attention mechanism. Self - Attention mechanism PART-OF Transformer. model COMPARE Transformer. Transformer COMPARE model. model USED-FOR detailed meaning. model USED-FOR local context awareness. Neural Network USED-FOR Atis dataset. Geo, MSParS datasets EVALUATE-FOR model. Method is Neural Machine Translation. OtherScientificTerm is long - range word dependencies. ", parsing is an important task in machine translation. This paper proposes a Transformer-based model for semantic parsing. The main idea is to use a self-attention mechanism to capture the long-range word dependencies in a sentence. The proposed method achieves state-of-the-art performance on the Atis and Geo datasets.,"This paper proposes a Transformer-based model for semantic parsing. The main idea is to use a long-term memory (LSTM) to store the long-range word dependencies between phrases. The LSTM is then used to learn the local context of each phrase. The model is evaluated on the Atis dataset, Geo dataset, and MSParS dataset."
1385,SP:2056a65a7500d79465685af883083cd706277c1f,"combinations of multiple perturbations FEATURE-OF DNN robustness. composite adversarial training ( CAT ) HYPONYM-OF training method. robustness EVALUATE-FOR individual perturbations. training method USED-FOR multiple adversarial losses. pixel perturbations CONJUNCTION spatial transformations. spatial transformations CONJUNCTION pixel perturbations. CAT COMPARE adversarial training methods. adversarial training methods COMPARE CAT. benchmark datasets EVALUATE-FOR CAT. spatial transformations HYPONYM-OF adversarial perturbation models. Method are deep neural networks ( DNNs ), and individual perturbation models. OtherScientificTerm is adversarial perturbations. ","This paper proposes a new adversarial training method that combines multiple adversarial perturbations to improve the robustness of deep neural networks. The proposed method is based on the observation that individual perturbation models are more robust than the combined ones. To this end, the authors propose to combine pixel and spatial transformations to improve robustness. Experiments show that the proposed method outperforms existing methods on several benchmark datasets. ","This paper proposes a new adversarial training method for training deep neural networks (DNNs) with multiple adversarial perturbations (e.g., pixel perturbation, spatial transformations, etc). The main contribution of the paper is to propose a new training method, which is based on the notion of composite adversarial loss (CAT), which is defined as the sum of all the adversarial losses across all the perturbed data points. The authors show that CAT can improve the robustness of DNNs against multiple perturbing data points in terms of the number of adversarial attacks. They also provide a theoretical analysis of the effect of the different types of attacks and show that it can be used to improve robustness."
1401,SP:006e5b9ac9a8eb7223843731488bfefbd8eb09bd,"Emergent Symbol Binding Network ( ESBN ) HYPONYM-OF recurrent network. external memory USED-FOR variable - binding. external memory USED-FOR recurrent network. binding mechanism USED-FOR symbol - like representations. ESBN USED-FOR rules. learning process USED-FOR symbol - like representations. architecture COMPARE competitive neural network architectures. competitive neural network architectures COMPARE architecture. Task are human intelligence, and induction of abstract rules. OtherScientificTerm are abstract rules, and symbol - processing machinery. Material are high - dimensional sensory data, and high - dimensional data. Method are Deep neural network algorithms, and symbol - processing mechanisms. ",This paper proposes a novel method for learning abstract rules from high-dimensional sensory data. The proposed method is based on a recurrent network with a variable-bounding mechanism that learns to encode abstract rules into a symbol-like representation. The method is evaluated on a set of synthetic and real-world datasets.   ,"This paper proposes a new method for learning symbolic representations from sensory data. The proposed method is based on the emergent symbol binding network (ESBN), which is a recurrent network with a variable-bounding mechanism. The binding mechanism is used to learn a symbolic representation of the sensory data, which is then used to generate a set of abstract rules. The authors show that the proposed method outperforms the state-of-the-art in terms of performance. "
1417,SP:4171ce45966ac499f51450a19fb233934c0847f0,"nested named entity recognition CONJUNCTION relation classification. relation classification CONJUNCTION nested named entity recognition. framework USED-FOR structured prediction language tasks. relation classification CONJUNCTION semantic role labeling. semantic role labeling CONJUNCTION relation classification. event extraction CONJUNCTION coreference resolution. coreference resolution CONJUNCTION event extraction. semantic role labeling CONJUNCTION event extraction. event extraction CONJUNCTION semantic role labeling. joint entity and relation extraction CONJUNCTION nested named entity recognition. nested named entity recognition CONJUNCTION joint entity and relation extraction. coreference resolution CONJUNCTION dialogue state tracking. dialogue state tracking CONJUNCTION coreference resolution. dialogue state tracking HYPONYM-OF structured prediction language tasks. coreference resolution HYPONYM-OF structured prediction language tasks. event extraction HYPONYM-OF structured prediction language tasks. semantic role labeling HYPONYM-OF structured prediction language tasks. joint entity and relation extraction HYPONYM-OF structured prediction language tasks. relation classification HYPONYM-OF structured prediction language tasks. nested named entity recognition HYPONYM-OF structured prediction language tasks. translation task USED-FOR it. augmented natural languages USED-FOR translation task. task - specific discriminative classifiers USED-FOR problem. FewRel CONJUNCTION TACRED. TACRED CONJUNCTION FewRel. approach COMPARE task - specific models. task - specific models COMPARE approach. relation classification CONJUNCTION semantic role labeling. semantic role labeling CONJUNCTION relation classification. joint entity and relation extraction CONJUNCTION relation classification. relation classification CONJUNCTION joint entity and relation extraction. relation classification CONJUNCTION FewRel. FewRel CONJUNCTION relation classification. relation classification CONJUNCTION TACRED. TACRED CONJUNCTION relation classification. approach COMPARE,., COMPARE approach. relation classification EVALUATE-FOR,. joint entity and relation extraction EVALUATE-FOR,. tasks EVALUATE-FOR task - specific models. tasks EVALUATE-FOR approach. joint entity and relation extraction EVALUATE-FOR approach. model USED-FOR tasks. hyperparameters USED-FOR tasks. architecture USED-FOR tasks. architecture CONJUNCTION","This paper proposes a framework for structured prediction language tasks such as nested named entity recognition, relation classification, coreference resolution, event extraction, and dialogue state tracking. The proposed framework is based on the idea of using a task-specific discriminative classifier for each of the tasks. The main contribution of this paper is the introduction of a translation-based model for the task of relation classification. The method is evaluated on three tasks: (1) nested entity recognition (2) relation classification (3) semantic role labeling (4) coreference state tracking (5) and the proposed method achieves state-of-the-art performance on all three tasks.","This paper proposes a new framework for structured prediction language tasks. The proposed framework is based on the idea of task-specific discriminative classifiers (task-specific classifiers) which can be applied to a variety of tasks, such as nested named entity recognition, joint entity and relation extraction, event extraction, coreference resolution, semantic role labeling, and dialogue state tracking. The model is trained using a translation task, where it is trained on a set of augmented natural languages, and the model is evaluated on three tasks: FewRel, TACRED, and FewRel."
1433,SP:8f1b2fc6829e0bdfcc981020b0dcf3e63a947910,"named entity recognition ( NER ) models USED-FOR unlabeled entity problem. approach USED-FOR misguidance. unlabeled entities USED-FOR NER models. synthetic datasets CONJUNCTION real - world datasets. real - world datasets CONJUNCTION synthetic datasets. model COMPARE prior baselines. prior baselines COMPARE model. real - world datasets EVALUATE-FOR model. synthetic datasets EVALUATE-FOR model. model USED-FOR unlabeled entity problem. well - annotated datasets EVALUATE-FOR model. OtherScientificTerm is negative instances. Method are pretraining language models, and negative sampling. ","This paper proposes a novel approach to improve the performance of NER models in the unlabeled entity recognition task. The proposed approach is based on the observation that the misguidance in the NER model is caused by the presence of negative instances in the training data. The authors propose to use negative sampling to reduce the number of negative examples in training data, and show that the proposed approach improves the performance on both synthetic and real-world datasets.","This paper proposes a new approach to the unlabeled entity recognition problem for NER models. The proposed approach is based on the notion of misguidance, which is defined as the number of negative instances of an entity in a set of labeled entities. The authors show that their approach can improve the performance of the NER model on synthetic and real-world datasets. They also show that the proposed approach outperforms the state-of-the-art in terms of performance on well-annotated datasets."
1449,SP:dd76ece8d92a8a230a8b43033d8cb2368c677a94,"stochastic neighbor embedding ( SNE ) USED-FOR sequential inputs. stochastic neighbor embedding ( SNE ) USED-FOR vector space of fixed, reduced dimensions. Acoustic Neighbor Embeddings HYPONYM-OF acoustic word embedding. Euclidean distance USED-FOR phonetic confusability. acoustic encoder CONJUNCTION text encoder. text encoder CONJUNCTION acoustic encoder. acoustic encoder USED-FOR speech signals. frame - wise subword posterior probabilities USED-FOR acoustic encoder. frame - wise subword posterior probabilities FEATURE-OF speech signals. acoustic model USED-FOR frame - wise subword posterior probabilities. acoustic encoder HYPONYM-OF encoder neural networks. text encoder HYPONYM-OF encoder neural networks. triplet loss criterion COMPARE method. method COMPARE triplet loss criterion. gradients USED-FOR neural network training. method USED-FOR neural network training. gradients EVALUATE-FOR method. text encoder network USED-FOR approximate phonetic matching task. encoder networks USED-FOR word ( name ) recognition task. low - dimensional embeddings USED-FOR it. encoder networks USED-FOR it. recognition accuracy EVALUATE-FOR finite state transducer(FST)-based decoding. test data USED-FOR finite state transducer(FST)-based decoding. Euclidean nearest - neighbor search USED-FOR isolated name recognition task. Material is speech. OtherScientificTerm are embedding space, subword transcriptions, embedding vectors, and embeddings. ","This paper proposes to use acoustic word embeddings to improve the performance of neural networks for word recognition tasks. The proposed method is based on the acoustic encoder and text encoder, which is a combination of an acoustic model and a frame-wise subword posterior probability model. The authors show that the proposed method outperforms the state-of-the-art methods in terms of accuracy and training time. ","This paper proposes a new method for learning low-dimensional word embeddings for speech. The key idea is to use the Euclidean distance between the word embedding vector and the subword embedding vectors as a surrogate for the embedding space. The proposed method is based on the idea of embedding the word vector into a vector space of fixed, reduced dimensions. The authors show that the proposed method can be used to train a neural network for the task of word recognition."
1465,SP:9142189126b8612ac0acee6fe18a0cfcb70b6545,reinforcement learning algorithm USED-FOR stationary mean - field games. mean - field state USED-FOR Nash equilibrium. mean - field state CONJUNCTION policy. policy CONJUNCTION mean - field state. gradient - descent CONJUNCTION proximal policy optimization. proximal policy optimization CONJUNCTION gradient - descent. mean - field state CONJUNCTION policy. policy CONJUNCTION mean - field state. proximal policy optimization USED-FOR fictitious play algorithm. gradient - descent USED-FOR fictitious play algorithm. gradient - descent USED-FOR policy. proximal policy optimization USED-FOR policy. algorithm USED-FOR single - agent reinforcement learning problem. fictitious play algorithm USED-FOR Nash equilibrium. OtherScientificTerm is optimum. Task is mean - field games. ,"This paper studies the problem of playing mean-field games in a single-agent setting, where the state of the game is assumed to be the mean field state and the goal is to find a Nash equilibrium. The authors propose a novel algorithm for this problem, which they call fictitious play. The algorithm is based on gradient-descent and proximal policy optimization. Theoretical analysis is provided to show that the algorithm converges to the Nash equilibrium at a rate of $O(1/\epsilon^2)$ with probability at least $O(\sqrt{1/2})$. The authors also show that this algorithm can be used for reinforcement learning.   ","This paper studies the problem of playing a stationary mean-field game with a single agent in a single-agent reinforcement learning setting. The authors show that the state of the game can be represented as a Nash equilibrium, where the agent is in a state where the state is in the mean field and the goal is to find a state in which the agent has a good chance of winning. They show that this state can be obtained by using a fictitious play algorithm, which is based on the idea of proximal policy optimization and gradient descent. They also show that their algorithm can be applied to the single agent setting."
1481,SP:c498f8a199da1818fe64ed88b0825c5aad688aec,joint distribution USED-FOR probabilistic inference. normalizing flow model USED-FOR probabilistic inference. normalizing flow model USED-FOR joint distribution. flow models USED-FOR task. framework USED-FOR approximate probabilistic inference. method USED-FOR generative model. flow model USED-FOR distribution. variational inference USED-FOR it. arbitrary differentiable transformations USED-FOR conditioning. likelihood evaluation CONJUNCTION inversion. inversion CONJUNCTION likelihood evaluation. it USED-FOR likelihood evaluation. inversion CONJUNCTION sampling. sampling CONJUNCTION inversion. it USED-FOR sampling. it USED-FOR inversion. inference tasks USED-FOR inverse problems. inference tasks EVALUATE-FOR method. approach COMPARE MCMC baselines. MCMC baselines COMPARE approach. sample quality EVALUATE-FOR MCMC baselines. sample quality EVALUATE-FOR approach. Generic is model. OtherScientificTerm is approximate posterior. ,"This paper proposes a normalizing flow model for approximate probabilistic inference, where the model is trained using variational inference to approximate the posterior distribution. The model is conditioned on an arbitrary differentiable transformation of the distribution, which is then used for conditional inference. The authors show that the model can be used for likelihood evaluation, inversion, and sampling in inverse problems.   ","This paper proposes a new framework for approximate probabilistic inference using a normalizing flow model for generative models. The framework is based on variational inference, where the posterior of the model is approximated by the joint distribution of the generative model. The model is conditioned on an arbitrary differentiable transformation of the distribution. The authors show that this conditioning can be used to improve the sample quality of the inference process. The proposed method is evaluated on a variety of inference tasks. "
1497,SP:1d0f27f61c9d32911b8bd15d6b82ef5eec644f0f,Computer vision technology USED-FOR biological and medical data analysis and understanding. Ultra - high Resolution Image Segmentation dataset USED-FOR Cell membrane. iterative annotations CONJUNCTION uncompressed high - resolution raw data. uncompressed high - resolution raw data CONJUNCTION iterative annotations. annotated Electron Microscopy ( EM ) dataset USED-FOR Cell membrane. U - RISC HYPONYM-OF annotated Electron Microscopy ( EM ) dataset. U - RISC HYPONYM-OF Ultra - high Resolution Image Segmentation dataset. segmentation evaluation criteria COMPARE human perception. human perception COMPARE segmentation evaluation criteria. evaluation criterion USED-FOR cell membrane segmentation. Perceptual Hausdorff Distance ( PHD ) HYPONYM-OF evaluation criterion. evaluation criteria CONJUNCTION PHD. PHD CONJUNCTION evaluation criteria. segmentation methods CONJUNCTION iterative manual annotation. iterative manual annotation CONJUNCTION segmentation methods. evaluation criteria USED-FOR iterative manual annotation. ,This paper proposes a new image segmentation dataset for cell membrane segmentation based on Electron Microscopy (EM) data. The proposed U-RISC dataset consists of an annotated EM dataset and an uncompressed high-resolution raw data with iterative annotations and uncompressed raw data. A novel evaluation criterion is proposed to evaluate the performance of segmentation methods.   ,"This paper proposes a new evaluation criterion for cell membrane segmentation based on the Perceptual Hausdorff Distance (PHD) criterion. The paper proposes an evaluation criterion based on a new high-resolution image segmentation dataset, U-RISC, which is an annotated Electron Microscopy (EM) dataset. The authors show that the PHD criterion can be used to evaluate the performance of cell segmentation methods, including iterative manual annotations and uncompressed high-res raw data."
1513,SP:8ca7aff87c82be69c9542550c814f52c9419ab0a,"Continual Learning ( CL ) USED-FOR catastrophic forgetting. benchmarks USED-FOR CL algorithms. benchmarks USED-FOR forgetting. short streams of tasks USED-FOR benchmarks. short streams of tasks USED-FOR forgetting. modules USED-FOR atomic skills. modules PART-OF modular architecture. learning algorithm USED-FOR learning. exponential search space FEATURE-OF task - driven prior. task - driven prior USED-FOR learning algorithm. modular architecture CONJUNCTION learning algorithm. learning algorithm CONJUNCTION modular architecture. benchmarks EVALUATE-FOR learning algorithm. CL benchmarks EVALUATE-FOR modular architecture. CL benchmarks EVALUATE-FOR learning algorithm. Method is CL system. Generic are task, and Benchmark. ","This paper proposes a method for continual learning (CL) in the presence of catastrophic forgetting. The proposed method is based on a modular architecture, where each task is composed of a set of modules, each of which is responsible for a specific task. The modules are trained using a self-supervised learning algorithm, where a task-driven prior is used to guide the learning process. The method is evaluated on a variety of continual learning benchmarks, where it is shown that the proposed method outperforms the baselines.","This paper proposes a new learning algorithm for continual learning (CL) that learns a task-driven prior that can be used to predict the forgetting rate of a given task. The learning algorithm is based on a modular architecture, where each task is represented as a set of modules, and each module can be learned in an exponential search space. The authors show that the learning algorithm can be applied to a variety of benchmarks, and show that it can learn the learning rate of the prior in a way that minimizes catastrophic forgetting.  "
1529,SP:cc819c61f408e88f247eb87946187ccec3dad32e,random selection CONJUNCTION clustering and/or augmentation. clustering and/or augmentation CONJUNCTION random selection. unsupervised meta - learning approaches USED-FOR synthetic meta - tasks. techniques USED-FOR synthetic meta - tasks. clustering and/or augmentation HYPONYM-OF techniques. random selection HYPONYM-OF techniques. approach USED-FOR metatasks. generative models USED-FOR approach. generative models USED-FOR metatasks. algorithms USED-FOR synthetic classes. synthetic classes PART-OF meta - task. approach COMPARE unsupervised learning baselines. unsupervised learning baselines COMPARE approach. benchmark datasets EVALUATE-FOR few - shot classification tasks. few - shot classification tasks EVALUATE-FOR unsupervised learning baselines. few - shot classification tasks EVALUATE-FOR approach. benchmark datasets EVALUATE-FOR unsupervised learning baselines. OtherScientificTerm is latent space. ,"This paper proposes an unsupervised meta-learning approach for generating synthetic meta-tasks for few-shot classification tasks. The proposed approach is based on a generative model that learns to generate a set of synthetic classes for each meta-task, which are then used to train a classifier on the generated data. The method is evaluated on a variety of benchmark datasets and achieves state-of-the-art performance.  ","This paper proposes a meta-learning approach for few-shot meta-tasks. The proposed approach is based on a generative model that learns a set of synthetic classes for each meta-task, which are then used to train a classifier for the task. The model is trained by using a combination of a few different methods, including random selection, clustering, and augmentation. The authors show that the proposed approach outperforms the state-of-the-art in terms of performance on a variety of tasks. "
1545,SP:b25771e5c214a352f74ba6196fbd88bca6c43c98,"inverse problems CONJUNCTION compressed sensing. compressed sensing CONJUNCTION inverse problems. it USED-FOR inference. Injectivity USED-FOR generative models. generative priors USED-FOR compressed sensing. injectivity FEATURE-OF fullyconnected and convolutional ReLU layers and networks. weight matrices USED-FOR injectivity. expansivity USED-FOR global injectivity. iid Gaussian matrices USED-FOR global injectivity. worst - case Lipschitz constants USED-FOR stability. arguments USED-FOR deep networks. differential topology USED-FOR arguments. injective ReLU network USED-FOR Lipschitz map. argument USED-FOR injectivity. random projections USED-FOR argument. neural networks USED-FOR nonlinear inverse and inference problems. OtherScientificTerm is well posedness. Method are layerwise analysis, tractable model, and injective network. ","This paper studies the injectivity of fully connected and convolutional ReLU layers and networks in the context of inverse and compressed sensing. The authors show that injectivity can be expressed as a function of the weight matrices of the network. They show that the global injectivity is defined by the iid Gaussian matrices, and that the worst-case Lipschitz constants of these matrices can be approximated by random projections of the weights. They then show that injecting a ReLU layer into a fully connected ReLU network is equivalent to injecting a weight matrix into the weight matrix of a Gaussian distribution.  ",This paper studies the injectivity of fully connected and convolutional ReLU layers and networks in the context of generative generative priors. The authors show that injectivity is well-posed in terms of the Lipschitz constant of the weight matrices of the weights of the fully connected ReLU layer. They also show that the worst-case stability of the injective network can be derived from the worst case of the parameterization of the parameters of the ReLU network. 
1561,SP:a95a153d3fe9bcf535ebf8514f51d00df483f210,"generative model USED-FOR image generation. continuous conditional generative adversarial network ( CcGAN ) HYPONYM-OF generative model. conditional GANs ( cGANs ) USED-FOR categorical conditions. class labels HYPONYM-OF categorical conditions. hidden map CONJUNCTION one - hot encoded label. one - hot encoded label CONJUNCTION hidden map. hidden map PART-OF generator / discriminator. one - hot encoded label HYPONYM-OF label input methods. hidden map HYPONYM-OF label input methods. empirical cGAN losses HYPONYM-OF cGAN losses. generator CONJUNCTION discriminator. discriminator CONJUNCTION generator. empirical cGAN losses USED-FOR continuous scenario. regression labels PART-OF discriminator. regression labels PART-OF generator. method USED-FOR regression labels. method USED-FOR generator. method USED-FOR discriminator. empirical cGAN losses USED-FOR CcGAN. hard vicinal discriminator loss ( HVDL ) CONJUNCTION soft vicinal discriminator loss ( SVDL ). soft vicinal discriminator loss ( SVDL ) CONJUNCTION hard vicinal discriminator loss ( HVDL ). empirical generator loss HYPONYM-OF empirical discriminator losses. soft vicinal discriminator loss ( SVDL ) HYPONYM-OF empirical discriminator losses. hard vicinal discriminator loss ( HVDL ) HYPONYM-OF empirical discriminator losses. HVDL CONJUNCTION SVDL. SVDL CONJUNCTION HVDL. error bounds FEATURE-OF discriminator. SVDL USED-FOR discriminator. HVDL USED-FOR discriminator. benchmark dataset USED-FOR generative image modeling. RC-49 USED-FOR generative image modeling. RC-49 HYPONYM-OF benchmark dataset. RC-49 CONJUNCTION UTKFace datasets. UTKFace datasets CONJUNCTION RC-49. Circular 2 - D Gaussians CONJUNCTION RC-49. RC-49 CONJUNCTION Circular 2 - D Gaussians. regression label FEATURE-OF image distribution. CcGAN COMPARE cGAN. cGAN COMPARE CcGAN. OtherScientificTerm are continuous, scal",This paper proposes a continuous conditional generative adversarial network (CcGAN) for image generation. The proposed method is based on the observation that the generator and discriminator in a cGAN are trained with categorical labels. The authors propose two losses to improve the performance of CcGAN: hard vicinal discriminator loss (HVDL) and soft vicinal adversarial loss (SVDL). Experiments on RC-49 and UTKFace datasets show that the proposed method outperforms existing methods. ,"This paper proposes a new continuous conditional generative adversarial network (CcGAN) for continuous image generation. The main idea of the paper is to combine the empirical cGAN losses (e.g., hard vicinal discriminator loss (HVDL) and soft vicinal loss (SVDL), empirical generator loss (Eugene) and empirical discriminator losses (Egene) to improve the performance of the CcGAN model. The authors show that the proposed method outperforms the state-of-the-art ccGAN models on RC-49 and UTKFace datasets."
1577,SP:10dd09ab315870631d1451d200f2c87a023f8226,"sample complexity EVALUATE-FOR deep learning ( DL ). Semisupervised learning ( SSL ) USED-FOR task. unlabeled instances USED-FOR Semisupervised learning ( SSL ). unlabeled instances USED-FOR task. sample complexity EVALUATE-FOR Active learning ( AL ). SSL CONJUNCTION AL. AL CONJUNCTION SSL. SSL USED-FOR fully - supervised learning ( SL ). AL USED-FOR fully - supervised learning ( SL ). labeled samples USED-FOR fully - supervised learning ( SL ). SSL USED-FOR DL - based AL algorithms. annotation efficiency EVALUATE-FOR AL algorithms. diversity FEATURE-OF AL algorithms. SSL USED-FOR AL algorithms. AL algorithm USED-FOR classification network. AL algorithm USED-FOR convergence rate. convergence rate FEATURE-OF classification network. CRC CONJUNCTION SSL algorithm. SSL algorithm CONJUNCTION CRC. deep neural network COMPARE SL. SL COMPARE deep neural network. labeled samples USED-FOR deep neural network. SSL algorithm USED-FOR deep neural network. CRC USED-FOR deep neural network. AL CONJUNCTION SSL. SSL CONJUNCTION AL. our method USED-FOR ASSL. OtherScientificTerm is human - in - the - loop. Method are pool - based AL, convergence rate control ( CRC ), and AL and SSL ( ASSL ) algorithms. Metric is rate of convergence. Generic is method. ","This paper studies the sample complexity of active learning (AL) and semi-supervised learning (SSL) in deep learning. The authors show that AL and SSL can be combined to improve the annotation efficiency of DL-based AL algorithms. The main contribution of the paper is a method to control the rate of convergence of the AL algorithm and SSL algorithm, which is based on the convergence rate control. ","This paper studies the sample complexity of active learning (AL) and semi-supervised learning (SSL) in the context of fully supervised learning (SL). The authors propose a new method, called ASSL, to control the rate of convergence of both AL and SSL algorithms. The main idea is to use the human-in-the-loop (HIL) approach to estimate the convergence rate of the two algorithms. They show that ASSL can achieve better sample complexity than AL, SSL, and AL alone. They also show that the diversity of AL algorithms can be improved by increasing the number of samples. "
1593,SP:7f3947c3fa5b09674507d8f3e10d9280376ecb94,"federated learning method USED-FOR distributively training neural network models. devices USED-FOR parallelizing gradient computation. scheme USED-FOR training. devices CONJUNCTION partial participation and unbalanced data. partial participation and unbalanced data CONJUNCTION devices. scheme USED-FOR convex and non - convex settings. Task are Federated Learning problem, and device level computations. OtherScientificTerm are local - device level empirical loss, global empirical loss, and device heterogeneity. Method are inexact minimization, and dynamic regularizer. Material is real and synthetic data. ",This paper studies federated learning with distributed training. The authors propose to use a dynamic regularizer to encourage devices to converge to the same point in the training process. Theoretical analysis is provided to show the convergence of the proposed method. Empirical results are provided to support the theoretical results. ,"This paper studies the problem of federated learning, where each device has access to a subset of the training data, and the goal is to train a neural network model across all the devices. The authors propose a new scheme for training the model, which is based on a dynamic regularizer, and show that it can be used to minimize the global empirical loss. They also show that the convergence rate of the proposed scheme is bounded by the number of devices.  "
1609,SP:a3fbb073b0e2371b20d5d9df6ab829673f90354f,"supervised counterparts USED-FOR computer vision tasks. representations COMPARE supervised counterparts. supervised counterparts COMPARE representations. representations USED-FOR computer vision tasks. self - supervised approaches USED-FOR representations. accuracy EVALUATE-FOR contrastive learning algorithms. contrastive learning USED-FOR similarity ( dissimilarity ). similarity FEATURE-OF intermediate layers. similarity USED-FOR similarity. intermediate losses USED-FOR selection. SimCLR CONJUNCTION SwAV. SwAV CONJUNCTION SimCLR. MOCO CONJUNCTION SimCLR. SimCLR CONJUNCTION MOCO. method USED-FOR MOCO. method USED-FOR SimCLR. method USED-FOR SwAV. ImageNet linear classification CONJUNCTION downstream tasks. downstream tasks CONJUNCTION ImageNet linear classification. Method are self - supervised methods, and back - propagation. Task is optimizing similarity ( dissimilarity ). OtherScientificTerm are intermediate contrastive losses, and gradient descent update. Metric is computational cost. ","This paper proposes a self-supervised contrastive learning method for image classification. The proposed method is based on the idea that the similarity between the intermediate layers of the self-trained model is a good measure of the similarity (dissimilarity) between the input images and the output images. The authors propose to use intermediate contrastive losses to select the similarity among intermediate layers, and then use gradient descent to update the weights of intermediate layers.  The authors show that the proposed method outperforms the state-of-the-art methods on ImageNet and SwAV. ",This paper proposes a new contrastive learning algorithm for self-supervised image classification. The main idea is to use intermediate contrastive losses to improve the similarity between the intermediate layers of the network. The authors show that the proposed method outperforms the state-of-the-art in terms of accuracy and computational cost. They also show that their method can be applied to a variety of downstream tasks.
1625,SP:5b5e705ea1ee1b857e17e64d560a39052804949d,actor - critic HYPONYM-OF reinforcement learning algorithms. global convergence CONJUNCTION global optimality. global optimality CONJUNCTION global convergence. global optimality FEATURE-OF actor - critic. global convergence FEATURE-OF actor - critic. bi - level or two - timescale updates USED-FOR actor - critic. policy gradient direction USED-FOR actor. critic USED-FOR policy gradient direction. Bellman evaluation operator USED-FOR critic update. linear or deep neural networks USED-FOR actor. linear or deep neural networks USED-FOR function approximation settings. rate of convergence CONJUNCTION global optimality. global optimality CONJUNCTION rate of convergence. global optimality FEATURE-OF single - timescale actor - critic. rate of convergence FEATURE-OF single - timescale actor - critic. linear function approximation USED-FOR single - timescale actor - critic. actorcritic with deep neural network USED-FOR globally optimal policy. nonlinear function approximation USED-FOR policy optimization. Task is single - timescale setting. Metric is sublinear O(K−1/2 ) rate. OtherScientificTerm is sublinear rate. ,"This paper studies the convergence of actor-critic in reinforcement learning with a single-time update. The authors show that the rate of convergence is sublinear in terms of global optimality and global gradient direction. The main contribution of the paper is to show that in the single-timescale setting, the rate is O(K-1/2) with a sublinear convergence rate.    The main contributions of this paper are as follows:  - The authors prove that the global convergence rate of the single time update of actor and critic is O(\sqrt{K}(K)^2) in the linear function approximation setting.  - In the non-linear function approximation settings, the authors show the global rate is $O(K^2/3)$ in the sublinear case. - The rate of global convergence is O($K/K)$ for the nonlinear case with a deep neural network. ","This paper studies the problem of actor-critic reinforcement learning in the multi-time-step setting. The authors show that the rate of convergence is sublinear O(K+1/2) in the single-time step setting. They also show that in the bi-level or two-timescale setting, the rate is O(1/K) times smaller than the sublinear rate in the non-linear setting.  "
1641,SP:26705a4dc305cce336f657c5937d1f5b4209548a,events CONJUNCTION messages. messages CONJUNCTION events. messages CONJUNCTION transactions. transactions CONJUNCTION messages. Log files USED-FOR events. Log files USED-FOR messages. Log files USED-FOR transactions. computer systems FEATURE-OF Log files. they USED-FOR structured textual and numerical data. natural languages CONJUNCTION temporal signals. temporal signals CONJUNCTION natural languages. logs USED-FOR sequential forms of data. natural languages HYPONYM-OF sequential forms of data. temporal signals HYPONYM-OF sequential forms of data. log level CONJUNCTION log sequence level. log sequence level CONJUNCTION log level. field level CONJUNCTION log level. log level CONJUNCTION field level. representation USED-FOR level. vector format FEATURE-OF representations. Transformer Networks ( TNs ) USED-FOR numerical and textual information. Transformer Networks ( TNs ) USED-FOR log embeddings. representation USED-FOR log processing applications. Material is Logs. ,This paper proposes to use Transformer networks to learn representations of log data in a vector format. The authors show that the representations can be used to model both numerical and textual information in log data. The proposed method is evaluated on synthetic and real-world data. ,"This paper proposes a new representation for log embeddings that can be used in computer systems to represent structured textual and numerical data. The proposed representation is based on Transformer Networks (TNs) and is applied to both numerical and textual data. It is shown that the proposed representation can be applied to a variety of applications, including log processing applications.  "
1657,SP:165c51a16f17fb8726e968f8b34742b62011d60e,"CNN kernels CONJUNCTION oriented Gabor filters. oriented Gabor filters CONJUNCTION CNN kernels. freely - trained mixture weights USED-FOR wavelet packet decompositions. AlexNet architecture USED-FOR image classification. AlexNet architecture FEATURE-OF wavelet decompositions. wavelet decompositions USED-FOR approach. directional selectivity CONJUNCTION shift invariance. shift invariance CONJUNCTION directional selectivity. feature extraction properties USED-FOR two. shift invariance HYPONYM-OF feature extraction properties. directional selectivity HYPONYM-OF feature extraction properties. separable wavelet packet transform USED-FOR variant. accuracy rate EVALUATE-FOR AlexNet. mathematical theory USED-FOR network. Task is deep convolutional neural networks ( CNNs ). Generic are formalism, and them. Method is convolutional layers. ","This paper proposes a wavelet decomposition of CNN kernels and oriented Gabor filters in deep convolutional neural networks (CNNs) based on wavelet packet decomposition (WPD). The proposed method is based on the observation that CNN kernels can be decomposed into a set of wavelets, which can then be used to compute the weight matrices of the convolution layers. The authors show that the wavelet matrices can be represented as a mixture of freely-trained mixture weights, which are then used to represent the weights of the CNN kernels.    The authors also show that wavelet matrix decomposition can be used as a special case of wavelet aggregation, which is used to model the directional selectivity and shift invariance properties of CNNs.  Finally, the authors demonstrate that the proposed method achieves state-of-the-art performance on image classification tasks.","This paper proposes a new wavelet decomposition method for deep neural networks (DNNs) that is based on the wavelet packet decomposition (WPC) framework. The main idea is to decompose the convolutional layers of DNNs into a set of wavelets, which can be used to extract features from the input image. The authors show that the wavelets can be decomposed into two parts: (1) a separable wavelet, and (2) a shift-invariant feature extractor. The paper also provides a theoretical analysis of the properties of the two wavelets. "
1673,SP:d0a284da462584724ba6a3a48c9e986d391233f6,"dynamic composition FEATURE-OF Coordinating teams. variational objective USED-FOR learning. attention mechanism USED-FOR dynamic team composition. attention mechanism USED-FOR heterogeneous agents. multi - agent particle environment FEATURE-OF resource collection tasks. resource collection tasks EVALUATE-FOR methods. zero - shot generalization USED-FOR team compositions. heterogeneous agents USED-FOR zero - shot generalization. coach USED-FOR dynamic teams. Task are real - world multi - agent teams, and real - world team sports. OtherScientificTerm is optimal team strategy. Method are coach - player framework, adaptive communication method, and adaptive communication strategy. Generic is method. ","This paper proposes a method for learning dynamic composition in multi-agent teams. The proposed method is based on a coach-player framework, where each agent is trained with a variational objective to learn the optimal team strategy. The method is evaluated on resource collection tasks, where it is shown that the proposed method outperforms baselines in terms of zero-shot generalization. ","This paper proposes a coach-player framework for zero-shot generalization in multi-agent multi-player team sports. The main idea is to use a variational objective for learning the optimal team strategy, where the goal is to find a team composition that maximizes the performance of all agents in the team. The proposed method is evaluated on two real-world resource collection tasks, where it is shown that the proposed method outperforms the state-of-the-art."
1689,SP:4eb662b527d556758aaa1a0b589495fcc337fad0,"machine learning interpretability CONJUNCTION uncertainty estimation. uncertainty estimation CONJUNCTION machine learning interpretability. Influence functions USED-FOR machine learning interpretability. Influence functions USED-FOR uncertainty estimation. gradients CONJUNCTION Hessian. Hessian CONJUNCTION gradients. Hessian FEATURE-OF model. gradients FEATURE-OF model. Hessian USED-FOR post - hoc method. gradients USED-FOR post - hoc method. influence functions USED-FOR linear models. Influence functions PART-OF deep learning. non - convex loss functions FEATURE-OF deep learning. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. Iris CONJUNCTION MNIST. MNIST CONJUNCTION Iris. influence functions PART-OF neural network models. datasets USED-FOR neural network models. Iris HYPONYM-OF datasets. ImageNet HYPONYM-OF datasets. CIFAR-10 HYPONYM-OF datasets. MNIST HYPONYM-OF datasets. model parameterization CONJUNCTION regularization techniques. regularization techniques CONJUNCTION model parameterization. regularization techniques USED-FOR influence functions. influence functions EVALUATE-FOR network architecture. accuracy EVALUATE-FOR influence estimates. influence estimates EVALUATE-FOR shallow networks. influence estimation methods USED-FOR non - convex setups. influence functions PART-OF deep learning. OtherScientificTerm are test - time predictions, convexity of the underlying loss function, and model changes. Task is estimating group influences. Metric is accuracy of influence functions. Generic is deeper networks. Method are network architectures, and weight - decay regularization. ",This paper studies the influence function estimation in deep neural networks. The authors propose a post-hoc method to estimate influence functions in linear models with non-convex loss functions. They show that the influence functions are sensitive to model parameterization and regularization techniques. They also show that deep networks with weight-decay regularization are more sensitive to influence functions.  ,This paper proposes a post-hoc method for estimating influence functions in deep neural networks. The main idea is to use the Hessian of the underlying loss function to estimate the influence function. The authors show that this method can be applied to non-convex loss functions. They also show that it can be used for deep networks with weight-decay regularization.   
1705,SP:5fea74a2031d097a99dacf613bedcb054b0c3831,Autoregressive language models USED-FOR downstream tasks. Autoregressive language models USED-FOR next word prediction. large text corpora USED-FOR next word prediction. large text corpora USED-FOR Autoregressive language models. zero - shot usage USED-FOR downstream tasks. next word prediction CONJUNCTION text classification. text classification CONJUNCTION next word prediction. language modeling HYPONYM-OF pretraining task. sentence completion tasks USED-FOR classification tasks of interest. language modeling USED-FOR downstream tasks. language models USED-FOR classification tasks. language models USED-FOR features. features USED-FOR classification tasks. crossentropy ( log - perplexity ) FEATURE-OF language models. objective function USED-FOR classification tasks. ,"This paper studies the problem of using autoregressive language models for next word prediction and text classification tasks. The authors propose to use language modeling as a pretraining task to improve the performance of language models on sentence completion tasks. They show that language models trained on the pretrained language models learn features that are useful for classification tasks, and then use these features for zero-shot usage on the downstream tasks. ","This paper proposes a new pretraining task for Autoregressive language models (ARMs) that aims to improve the performance of ARMs on downstream tasks such as next word prediction, text classification, and sentence completion tasks. The authors propose a new objective function called log-properity (log perplexity) to measure the cross-entropy between the features of the ARMs and the downstream tasks. They show that ARMs with log-perplexity outperform ARMs that do not use log perplexity by a large margin. They also provide a theoretical analysis of the effect of the log-par perplexity on the performance on the downstream task of text classification."
1721,SP:a67da438e9821010284416170c3699ae7ff96c99,"MIA approaches USED-FOR classification models. image translation HYPONYM-OF conditional image generation models. approach USED-FOR membership attacks. reconstruction error USED-FOR approach. difficulty score CONJUNCTION reconstruction error. reconstruction error CONJUNCTION difficulty score. difficulty score USED-FOR membership error. MIA accuracy EVALUATE-FOR membership error. Task is Membership inference attacks ( MIA ). Method are neural network model, machine learning, and MIA. OtherScientificTerm are overfitting, and Reconstruction error. Metric is reconstruction errors. Material is training set. ","This paper studies membership inference attacks (MIA) in machine learning, where the goal is to learn a model that is robust to membership attacks. The authors propose to use the reconstruction error as a metric to measure the difficulty of a membership attack. The reconstruction error is used as a proxy for the membership error, which is a measure of how well the model is able to learn from a given training set. The proposed method is evaluated on image classification and image generation tasks.   ",This paper studies membership inference attacks (MIA) in the context of machine learning. The main idea is to use the reconstruction error of the training set to estimate the membership error of a neural network model. The reconstruction error is defined as the difference between the reconstruction errors of a training set and the reconstruction of the test set.  The authors propose a new difficulty score to measure the difficulty of membership attacks. The difficulty score is based on the reconstruction accuracy. The authors show that the difficulty score can be used as a measure of MIA accuracy. 
1737,SP:6fe23ebe09f2a4e42a21598f8e9c79edeca99863,"it USED-FOR distribution learning problem. differentiable architecture search method USED-FOR distribution learning problem. random variables FEATURE-OF continuously relaxed architecture mixing weight. Dirichlet distribution USED-FOR random variables. pathwise derivatives USED-FOR Dirichlet parameters. gradient - based optimizer USED-FOR Dirichlet parameters. formulation USED-FOR stochasticity. generalization ability EVALUATE-FOR formulation. progressive learning scheme USED-FOR searching. searching USED-FOR large - scale tasks. progressive learning scheme USED-FOR large - scale tasks. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. Method are differentiable NAS, and neural architecture search algorithms. Generic are method, and datasets. Metric is test error. Material is NASBench-201. ",This paper proposes a differentiable architecture search method for differentiable neural architecture search (NAS) with continuous random variables. The main idea is to use the Dirichlet distribution to approximate the architecture mixing weight and then use a gradient-based optimizer to find the optimal architecture. The proposed method is shown to improve the performance of NAS on CIFAR-10 and ImageNet. ,"This paper proposes a differentiable architecture search method for the differentiable NAS problem. The main idea is to use the Dirichlet distribution as the random variables in the architecture mixing weight, and then use a gradient-based optimizer to find the optimal pathwise derivatives of the Diriclet parameters. The authors show that the proposed method can generalize well to large-scale tasks, and that it can be applied to NAS benchmark datasets."
1753,SP:c590d0ed2487b42480b53fc077546a4a0bc27a78,"signed distance functions CONJUNCTION neural radiance fields. neural radiance fields CONJUNCTION signed distance functions. function approximators USED-FOR low - dimensional - but - complex functions. neural networks USED-FOR low - dimensional - but - complex functions. neural networks USED-FOR function approximators. high dimensional inputs USED-FOR deep networks. pixel coordinates USED-FOR images. sinusoidal nonlinearities CONJUNCTION Fourier features. Fourier features CONJUNCTION sinusoidal nonlinearities. elements USED-FOR positional encodings. Fourier features USED-FOR positional encodings. elements COMPARE ReLU networks. ReLU networks COMPARE elements. positional encodings COMPARE ReLU networks. ReLU networks COMPARE positional encodings. Fourier features HYPONYM-OF elements. sinusoidal nonlinearities HYPONYM-OF elements. function approximators USED-FOR problems. multiplicative filter networks HYPONYM-OF problems. multiplicative filter networks HYPONYM-OF function approximators. Fourier or Gabor basis functions USED-FOR linear function approximator. ReLU networks CONJUNCTION sinusoidal activation networks. sinusoidal activation networks CONJUNCTION ReLU networks. Fourier features CONJUNCTION ReLU networks. ReLU networks CONJUNCTION Fourier features. multiplicative filter networks COMPARE approaches. approaches COMPARE multiplicative filter networks. Fourier features CONJUNCTION sinusoidal activation networks. sinusoidal activation networks CONJUNCTION Fourier features. sinusoidal activation networks USED-FOR approaches. ReLU networks USED-FOR approaches. Fourier features USED-FOR approaches. OtherScientificTerm are differential equations, and compositional depth. Generic are networks, and representation. ","This paper proposes to use Fourier features and sinusoidal nonlinearities to represent low-dimensional functions in neural networks. In particular, the authors propose to use the Fourier or Gabor basis functions to represent the linear function approximator as a function approximated by a ReLU network. The authors show that the proposed method outperforms ReLU networks in terms of accuracy and computational complexity.  ","This paper proposes a new way to represent low-dimensional function approximators in neural networks. The key idea is to use Fourier or Gabor basis functions as basis functions to represent a linear function approximated by a ReLU network. The authors show that the proposed method can be combined with a number of existing methods, such as ReLU networks, sinusoidal activation networks, and Fourier features. They also show that their method outperforms the state-of-the-art. "
1769,SP:f5be855300f63c185a006834302bd4b033b56258,"task - specific models CONJUNCTION meta - model. meta - model CONJUNCTION task - specific models. task - specific models PART-OF Gradient - based meta - learning. gradients USED-FOR meta - model. algorithm USED-FOR task - specific models. algorithm USED-FOR meta - model. meta - gradients USED-FOR meta - model. inner loop USED-FOR algorithm. inner loop USED-FOR task - specific models. teacherstudent scheme USED-FOR gradient - based meta - learning algorithms. student network USED-FOR task - specific models. lightweight computation graph USED-FOR meta - gradients. few - shot learning CONJUNCTION long - tailed classification. long - tailed classification CONJUNCTION few - shot learning. long - tailed classification CONJUNCTION meta - attack. meta - attack CONJUNCTION long - tailed classification. it USED-FOR meta - learning algorithms. it USED-FOR tasks. tasks EVALUATE-FOR meta - learning algorithms. meta - attack HYPONYM-OF tasks. meta - attack HYPONYM-OF meta - learning algorithms. few - shot learning HYPONYM-OF tasks. few - shot learning HYPONYM-OF meta - learning algorithms. long - tailed classification HYPONYM-OF tasks. long - tailed classification HYPONYM-OF meta - learning algorithms. Generic are loop, and approach. OtherScientificTerm are inner - loop optimization steps, high - order derivatives, and big memory footprints. ","This paper proposes a new meta-learning algorithm for few-shot learning and long-tailed classification. The main idea is to use meta-gradients from task-specific models in the inner loop of the meta-model to train a meta-classifier, which is then used as a teacher to train task-dependent models. The proposed method is evaluated on a variety of tasks and achieves state-of-the-art performance.","This paper proposes a teacher-student scheme for gradient-based meta-learning. The teacher student scheme uses a student network to learn task-specific gradients for the meta-model. The student network is trained using a lightweight computation graph, where the meta gradients are computed in an inner loop. The inner loop consists of two steps: (1) the student network learns the task specific gradients, and (2) the teacher student network trains the meta model. The paper shows that the student student scheme can be used to improve the performance of meta-learners on a variety of tasks. "
1785,SP:0361e02d56b7d121cb5ede1cb582284cc18fc599,"Offline Reinforcement Learning ( RL ) USED-FOR policies. off - policy RL algorithms USED-FOR Offline RL. Behavior regularization USED-FOR off - policy algorithms. analytical upper bound USED-FOR behavior regularizor. analytical upper bound USED-FOR KL divergence. state - dependent Lagrange multipliers USED-FOR regularization term. state - dependent Lagrange multipliers USED-FOR distributing KL divergence penalty. Lagrange multipliers USED-FOR freedom of deviation. gradient penalty term USED-FOR gradient of the Q value. gradient penalty term USED-FOR policy evaluation objective. out - of - distribution actions FEATURE-OF gradient of the Q value. gradient penalty term USED-FOR catastrophic performance degradation. out - ofdistribution actions FEATURE-OF Q values. offline RL benchmarks EVALUATE-FOR BRAC+. offline RL benchmarks EVALUATE-FOR model - free and model - based approaches. BRAC+ COMPARE model - free and model - based approaches. model - free and model - based approaches COMPARE BRAC+. Method are Reinforcement Learning agent, and behavior regularized offline reinforcement learning. OtherScientificTerm are outof - distribution ( less explored ) actions, sample based estimations, sampled batch, low probability ( less explored ) states, and rare out - of - distribution actions. ",This paper proposes a behavior regularization method for offline reinforcement learning. The main idea is to use a KL-divergence penalty to penalize the Q-values of out-of-distribution (OOD) actions. The proposed method is evaluated on a variety of offline RL benchmarks and compared with a number of baselines.  ,This paper proposes a new behavior regularization method for offline reinforcement learning. The main idea is to use a state-dependent Lagrange multipliers to control the distribution of the KL divergence between the Q value and the Q-values of the agent. The authors show that this method can be used to improve the performance of offline RL algorithms in the presence of rare out-of-distribution actions. They also provide a theoretical upper bound on the convergence of their method.
1801,SP:b2cfb380aa2a21f72f508b453cf5949257a5b4ec,"Smaller networks USED-FOR edge - devices. one - shot learning paradigm USED-FOR networks. regularization behavior FEATURE-OF adjoint training paradigm. Imagenet USED-FOR resnet-50. CIFAR-100 EVALUATE-FOR architecture. datasets EVALUATE-FOR network. network COMPARE network. network COMPARE network. datasets EVALUATE-FOR network. top-1 accuracy EVALUATE-FOR network. top-1 accuracy EVALUATE-FOR network. Task is compressing deep neural networks. Method are deep neural networks, Adjoined networks, CNN - based neural architecture, and adjoint networks. Generic is architectures. Metric are inference time, and accuracy. ","This paper proposes a simple yet effective way to compress deep neural networks. The proposed method is based on the observation that when the number of neurons in the network increases, the training time of the network decreases. The authors show that this is due to the fact that the size of the neurons increases linearly with the depth of the neural network. They show that the proposed method can be used to compress the network in a one-shot learning paradigm.  ","This paper proposes a new approach to improve the efficiency of one-shot training of deep neural networks. The proposed approach is based on the idea of ""adjoint training"", where each layer of the network is trained in a single-shot fashion. The main contribution of the paper is to study the regularization behavior of the adjoint training paradigm. The authors show that the performance of the proposed approach can be improved by reducing the number of layers in the network. They also show that their approach can improve the performance on CIFAR-100."
1817,SP:dba40073f79143e5355d194aa16db9eee0267a5d,"exploration USED-FOR reinforcement learning ( RL ). exploration methods COMPARE counterparts. counterparts COMPARE exploration methods. -greedy HYPONYM-OF counterparts. -greedy USED-FOR exploration algorithm. duration distributions USED-FOR exploration. ecological models of animal foraging behaviour USED-FOR distributions. Generic is problem. Metric are complexity, and generality. OtherScientificTerm are dithering, temporal persistence, local optima, and random duration. Method is greedy exploration. ","This paper studies the problem of exploration in reinforcement learning. The authors propose a new exploration algorithm called -greedy exploration, which is a variant of greedy exploration. The main idea is to learn a distribution over the duration of an agent's actions, and then use this distribution to estimate the probability of the agent's next action. The proposed algorithm is shown to outperform greedy exploration by a large margin.  ",This paper studies the problem of exploration in reinforcement learning (RL). The authors propose a new exploration algorithm that is greedy in the sense that it maximizes the duration of the exploration. They show that it is possible to find a greedy exploration algorithm with high complexity and high generality. They also show that this algorithm can be combined with existing exploration algorithms that are greedy in terms of the number of dithering and temporal persistence.
1833,SP:5efb581a368ace3bd085d48801a899559d6a43ef,"Matrix factorization USED-FOR implicit regularization of gradient descent. infinitesimal initialization USED-FOR Gradient Flow. gradient flow COMPARE heuristic rank minimization algorithm. heuristic rank minimization algorithm COMPARE gradient flow. infinitesimal initialization USED-FOR gradient flow. Greedy Low - Rank Learning HYPONYM-OF heuristic rank minimization algorithm. gradient flow USED-FOR depth-2 matrix factorization. OtherScientificTerm are nuclear norm, implicit regularization, and initialization magnitude. Method are norm minimization, rank minimization view, and rank minimization. Generic is convergence. ",This paper studies the implicit regularization of gradient descent in matrix factorization. The authors show that gradient flow with infinitesimal initialization is equivalent to gradient descent with infinite initialization. They show that the convergence rate of gradient flow is the same as that of the heuristic rank minimization algorithm. They also show the convergence of Greedy Low-Rank Learning with gradient flow.,"This paper studies the implicit regularization of gradient descent in the context of matrix factorization. In particular, the authors consider the nuclear norm minimization problem, where the norm minimizes the rank of the input matrix. The authors show that under certain conditions, the gradient flow can converge to a depth-2 matrix factorisation. They show that the convergence rate of gradient flow is inversely proportional to the number of initializations. They also show that gradient flow converges faster than the heuristic rank minimization algorithm."
1849,SP:7f997cf7a63a7330fc12fd525516080c91a3cb9b,"Classifiers PART-OF machine learning. two - stage framework USED-FOR robustness. data augmentations USED-FOR subgroup features. data augmentations USED-FOR classifier. CycleGAN USED-FOR intra - class, inter - subgroup augmentations. theoretically - motivated subgroup consistency regularizer CONJUNCTION robust objective. robust objective CONJUNCTION theoretically - motivated subgroup consistency regularizer. CycleGAN USED-FOR CAMEL. CAMEL USED-FOR model patching. robust error EVALUATE-FOR baseline. CAMEL COMPARE baseline. baseline COMPARE CAMEL. robust error EVALUATE-FOR CAMEL. benchmark datasets EVALUATE-FOR CAMEL. CAMEL USED-FOR model. Generic is models. Task is skin cancer classification. OtherScientificTerm are spurious bandage, subgroup differences, class information, semantic transformations, and spurious features. Method is Model patching. Material is real - world skin cancer dataset. ","This paper proposes CAMEL, a method to improve the robustness of classifiers against spurious bandages. The proposed method is based on CycleGAN, which uses intra-class and inter-subgroup augmentations to improve robustness. The method is evaluated on the skin cancer classification task and achieves better performance compared to baseline methods.","This paper proposes a two-stage framework to improve the robustness of classifiers in the context of skin cancer classification. The first step is to use CycleGAN to train a classifier that is robust to spurious bandages, i.e., the classifier is robust against spurious features. The second stage is to train the model to be robust to intra-class, inter-class and inter-subgroup augmentations. The authors show that the proposed CAMEL method can achieve better robustness compared to the baseline. "
1865,SP:de6cea1e35a0555175e17546a93422e9a96a511e,transparent inner structures CONJUNCTION model expressivity. model expressivity CONJUNCTION transparent inner structures. model interpretability FEATURE-OF transparent inner structures. decision trees HYPONYM-OF Rule - based models. large data sets EVALUATE-FOR rule - based models. Ensemble methods CONJUNCTION fuzzy / soft rules. fuzzy / soft rules CONJUNCTION Ensemble methods. interpretable nonfuzzy rules USED-FOR data representation. classifier USED-FOR interpretable nonfuzzy rules. Rulebased Representation Learner ( RRL ) HYPONYM-OF classifier. it USED-FOR continuous space. training method USED-FOR discrete model. Gradient Grafting HYPONYM-OF training method. gradient descent USED-FOR training method. gradient descent USED-FOR discrete model. logical activation functions USED-FOR RRL. scalability EVALUATE-FOR RRL. it USED-FOR continuous features. logical activation functions USED-FOR it. RRL COMPARE approaches. approaches COMPARE RRL. small and 4 large data sets EVALUATE-FOR RRL. RRL COMPARE decision trees. decision trees COMPARE RRL. complexity EVALUATE-FOR decision trees. complexity EVALUATE-FOR RRL. OtherScientificTerm is discrete parameters and structures. Method is non - differentiable RRL. ,"This paper proposes a rule-based representation learning method for learning interpretable non-fuzzy rules for data representation. The proposed method is based on gradient-grafting, which is an extension of Gradient Grafting to learn a discrete model with discrete parameters and structures. Theoretical analysis is provided to show that the proposed method can learn interpretable rules in a non-differentiable manner. Experiments are conducted on both small and large data sets.   ","This paper proposes a rule-based representation learner (RRL) model for learning interpretable non-fuzzy rules for data representation. The RRL model is trained by gradient-grafting, which is an extension of Gradient Grafting to the discrete model. The authors show that the proposed method is non-differentiable and can be used to learn interpretable rules. They also show that RRL can be applied to large data sets.   "
1881,SP:e36388a9452e557dd51bf0170bf2f9da22271a49,"molecular property prediction HYPONYM-OF biochemical applications. models USED-FOR biochemical applications. molecular scaffolds CONJUNCTION protein families. protein families CONJUNCTION molecular scaffolds. natural environments USED-FOR tasks. complex descriptors USED-FOR natural environments. protein families HYPONYM-OF complex descriptors. molecular scaffolds HYPONYM-OF complex descriptors. regret minimization ( RGM ) algorithm USED-FOR structured environments. representation USED-FOR predictor. hindsight access FEATURE-OF held - out environments. representation USED-FOR RGM. invariant risk minimization ( IRM ) USED-FOR RGM. specialized domain perturbations USED-FOR structured extension. RGM COMPARE baselines. baselines COMPARE RGM. molecular property prediction CONJUNCTION protein homology and stability prediction. protein homology and stability prediction CONJUNCTION molecular property prediction. applications EVALUATE-FOR RGM. molecular property prediction EVALUATE-FOR method. applications EVALUATE-FOR method. protein homology and stability prediction HYPONYM-OF applications. molecular property prediction HYPONYM-OF applications. OtherScientificTerm are environments, simultaneous optimality condition, and complex environments. Metric is predictive regret. ","This paper proposes a regret minimization method for molecular property prediction. The proposed method is based on a two-stage approach:  1. The first stage is to learn a predictive model that predicts a sequence of sequences from a held-out set of sequences. The second stage is a regret-minimization algorithm that learns a predictor that maximizes the regret of a sequence from a sequence. The regret of the learned model is then used to compute the regret for a sequence that is held out in the second stage.    The regret is estimated using hindsight access, which is done by computing the regret in the held out sequences from the first stage of the regret minimisation algorithm.  2. The method is evaluated on two tasks: protein homology prediction and stability prediction.  The results show that the proposed method outperforms the baselines. ","This paper proposes a regret minimization (RGM) algorithm for molecular property prediction. The main idea is to use a learned representation of the held-out environments and use hindsight access to improve the predictive regret of the predictor. The proposed method is based on the invariant risk minimisation (IRM) algorithm. The method is evaluated on a variety of molecular properties prediction tasks, including protein homology and stability prediction."
1897,SP:cad3ed2fba57faf17a3e8899dc5a744d5358aa68,BERT USED-FOR NLP tasks. BERT USED-FOR text - vision BERT models. cross - modal attentions USED-FOR text - vision BERT models. text - vision BERT models USED-FOR language - vision tasks. text - image retrieval HYPONYM-OF language - vision tasks. cross - modal attentions USED-FOR textvision BERT models. cross - modal attentions USED-FOR textvision retrieval. textvision retrieval USED-FOR large - scale search. computation cost EVALUATE-FOR cross - modal attentions. cross - probe BERT HYPONYM-OF architecture. text and vision probes USED-FOR cross - modal attentions. text and vision probes USED-FOR It. It USED-FOR crossmodal attention. Generic is method. ,This paper proposes to use cross-modal attention to improve the performance of text-vision BERT models on text-image retrieval tasks. The idea is to use a combination of text and vision probes to perform cross-attention between the two modalities. The proposed cross-probe BERT architecture is evaluated on text vision and image retrieval tasks and compared with the state-of-the-art methods.  ,"This paper proposes a new approach to perform cross-modal attentions for text-vision BERT models. The proposed approach is based on the cross-probe BERT architecture, which combines text and vision probes. The approach is evaluated on text-image retrieval and large-scale search tasks. "
1913,SP:51fd82de525fcb738fdeaeeae20fbb2cdf975f0c,Actor USED-FOR Actor - Critic algorithms. FORK HYPONYM-OF Actor. forward - looking Actor HYPONYM-OF Actor. FORK PART-OF model - free ActorCritic algorithm. continuous state and action spaces FEATURE-OF Box2D and MuJoCo environments. FORK USED-FOR BipedalWalkerHardcore. GPU USED-FOR FORK. Generic is algorithms. ,"This paper proposes a new actor-critic algorithm for reinforcement learning. The proposed method is based on the Actor-Critic algorithm, which is a model-free actor that uses a forward-looking actor. The main contribution of this paper is the introduction of a new forward-thinking actor, called FORK, that is able to learn to predict the future state of the environment.   The main contributions of the paper are:  1. A new forward looking actor, named FORK.  2. A model-based actor that is capable of learning to predict future state and action trajectories.  3. Experiments on Box2D and MuJoCo environments show the effectiveness of the proposed method. ","This paper proposes a model-free Actor-Critic algorithm for Box2D and MuJoCo environments with continuous state and action spaces. The main contribution of the paper is the introduction of FORK, a forward-looking Actor-critic algorithm that can be applied to both continuous state-action spaces and continuous action spaces in Box2d environments. The proposed FORK can be used to improve the performance of BipedalWalkerHardcore, which is an existing model-based actor critic algorithm.   "
1929,SP:6e730239e6e8b43c4988dd61dca30f15dc039ef7,"Federated learning USED-FOR global model. local models PART-OF global model. Bayesian inference perspective USED-FOR aggregation algorithm. FEDBE HYPONYM-OF aggregation algorithm. Bayesian model Ensemble USED-FOR them. Gaussian or Dirichlet distribution USED-FOR local models. Gaussian or Dirichlet distribution USED-FOR model distribution. FEDBE USED-FOR regularizing users ’ model training. Material is non - i.i.d. data. Method are global models, neural networks, aggregation method, and federated learning algorithm. Generic is it. "," in federated learning, where the goal is to train a global model on non-i.i.d. data. The authors propose an aggregation method based on Bayesian inference, where local models are aggregated using a Bayesian model ensemble. They show that the proposed method can be used to regularize users’ model training. ","This paper proposes a new aggregation method for federated learning, FedBE. FedBE is based on a Bayesian model ensemble approach, where the local models are represented by a Gaussian or Dirichlet distribution, and the global model is represented by an ensemble of local models. The authors show that FedBE can be used to regularize users’ model training. They also show that it can also be used for regularizing the training of the global models."
1945,SP:3ac5f437fc349a33810d0645664d1c448528af74,,"This paper presents a study of the relationship between the use of the term “human rights” and the concept of “fairness” in the context of online advertising. The authors argue that the idea of fair advertising is that people should be free to express their views in a way that is fair and ethical. They argue that this is not always the case, and that people are often forced to be honest about their views about human rights in order to be able to promote fair advertising. To this end, the authors introduce a new concept of fairness that is based on the notion of fair trade-off between privacy and freedom of expression. They show that this concept can be seen as a way to encourage people to be fair in advertising.",This paper presents a new study of the impact of the use of “marijuana” as a drug in the U.S. market. The authors study the effect of marijuana use in the state of the art in the United States. They find that the drug has a significant impact on the state’s ability to cope with the drug market. They also find that it has a negative effect on the local community. They argue that it is a good idea to use marijuana as an alternative to marijuana.
1961,SP:efa2343ead47263a0d09e1c17f9aa044605b9650,settling time FEATURE-OF deep neural networks. priori upper bound FEATURE-OF deep neural networks. Lyapunov based analysis USED-FOR loss function. Lyapunov based analysis USED-FOR priori upper bound. settling time FEATURE-OF priori upper bound. control theory framework USED-FOR deep learning. deterministic control theoretic setting FEATURE-OF priori guarantees of finite - time convergence. tracking problem USED-FOR learning. control problem USED-FOR supervised learning framework. analytical formula USED-FOR finite - time upper bound. analytical formula USED-FOR settling time. settling time FEATURE-OF finite - time upper bound. input perturbations FEATURE-OF loss function. Method is priori finite time convergence analysis. Generic is network. OtherScientificTerm is control inputs. ,"This paper studies the priori convergence of deep neural networks in the deterministic control theoretic setting. The authors propose a new Lyapunov-based analysis of the loss function and show that it converges to a stationary point in finite time. They also show that the convergence is guaranteed in the tracking problem, which is an extension of the control problem. ",This paper studies the priori convergence of deep neural networks in the deterministic control theory setting. The authors derive a priori upper bound on the settling time of a neural network in the setting of a tracking problem. The upper bound is based on a Lyapunov-based analysis of the loss function of the neural network. They show that the upper bound depends on the tracking problem of the network and the input perturbations. They also provide an analytical formula for the prior of the prior. 
1977,SP:7a0ded4b3b2d08d43765ff7b722da9b9863aabd6,Disentanglement of representations USED-FOR representations. incompressible - flow networks ( GIN ) USED-FOR latent variables. incompressible - flow networks ( GIN ) USED-FOR compact and disentangled representation. GIN USED-FOR informative latent variables selection. method USED-FOR informative latent variables selection. GIN USED-FOR method. mutual information USED-FOR informative latent variables. latent variables CONJUNCTION auxiliary variable. auxiliary variable CONJUNCTION latent variables. mutual information USED-FOR auxiliary variable. synthetic data EVALUATE-FOR method. outlier detection CONJUNCTION adversarial attack defence. adversarial attack defence CONJUNCTION outlier detection. classification CONJUNCTION outlier detection. outlier detection CONJUNCTION classification. downstream tasks EVALUATE-FOR method. synthetic and real data EVALUATE-FOR adversarial attack defence. classification EVALUATE-FOR method. adversarial attack defence HYPONYM-OF downstream tasks. classification HYPONYM-OF downstream tasks. outlier detection HYPONYM-OF downstream tasks. Task is machine learning. Method is nonlinear independent component analysis theory. ,This paper proposes a method to learn a disentangled representation of latent variables using a GIN-based method. The method is based on the mutual information between latent variables and auxiliary variables. The proposed method is evaluated on synthetic and real-world datasets. The experimental results show that the proposed method outperforms the baselines.,"This paper proposes a method for disentanglement of representations of latent variables in GINs. The key idea is to use a GIN to learn a compact and disentangled representation of the latent variables, which can be used for downstream tasks such as outlier detection, adversarial attack defence, and classification. The method is based on the nonlinear independent component analysis theory. The authors show that their method can be applied to both synthetic data and real data. "
1993,SP:0d9ba12bbf47b13a46c2225f9dc06878418daaea,"Pooling PART-OF convolutional neural networks. pooling operations USED-FOR feature maps. feature maps HYPONYM-OF lossy process. LiftDownPool CONJUNCTION LiftUpPool. LiftUpPool CONJUNCTION LiftDownPool. LiftPool USED-FOR bidirectional pooling layers. Lifting Scheme USED-FOR signal processing. LiftUpPool HYPONYM-OF bidirectional pooling layers. LiftDownPool HYPONYM-OF bidirectional pooling layers. LiftDownPool HYPONYM-OF LiftPool. LiftUpPool HYPONYM-OF LiftPool. LiftDownPool USED-FOR feature map. LiftDownPool USED-FOR downsized sub - bands. pooling function PART-OF LiftDownPool. image classification and semantic segmentation EVALUATE-FOR methods. backbones USED-FOR image classification and semantic segmentation. backbones USED-FOR methods. input corruptions CONJUNCTION perturbations. perturbations CONJUNCTION input corruptions. input corruptions FEATURE-OF robustness. robustness EVALUATE-FOR LiftDownPool. OtherScientificTerm are receptive fields, input variations, and downscaled feature map. Generic is they. Task are downsampling, and image - to - image translation challenges. Method is up - pooling layer LiftUpPool. ","This paper proposes a new pooling scheme for convolutional neural networks, named LiftUpPool, which is a bidirectional pooling operation that can be applied to both up- and down-sampling operations. The proposed method is based on a lifting scheme, where the weights of the up-pooling layer are updated in a lifting manner, and the down-sized sub-bands of the pooling operations are updated using a down-scaled pooling function. The method is evaluated on image classification and semantic segmentation tasks, where it is shown to outperform previous methods in terms of robustness to input corruptions.","This paper proposes a new pooling method for image-to-image translation. The proposed method, called LiftUpPool, is a bidirectional pooling layer with two sub-bands. The first sub-band is up-pooled, while the second sub-Band is down-sampled. The method is evaluated on image classification and semantic segmentation tasks.  "
2009,SP:147239edceb17bade6ea5d3dca44e3a59998aa47,"stable noise - shaping quantization scheme USED-FOR embedding method. fast linear transformation USED-FOR ` 1 norm. fast linear transformation USED-FOR Euclidean distances. ` 1 norm USED-FOR Euclidean distances. well - spread data EVALUATE-FOR method. time complexity EVALUATE-FOR method. space complexity EVALUATE-FOR method. continuous valued Johnson - Lindenstrauss embedding CONJUNCTION quantization error. quantization error CONJUNCTION continuous valued Johnson - Lindenstrauss embedding. polynomial decay FEATURE-OF quantization error. accuracy EVALUATE-FOR binary codes. natural images EVALUATE-FOR method. Material is high - dimensional dataset. OtherScientificTerm are binary sequences, T, sparse Gaussian random matrix, Hamming distance, Walsh - Hadamard matrix, and embedding dimension. Method is binary embedding methods. Task is embedding. Generic are approach, and it. ",This paper proposes to use a stable noise-shaping quantization scheme to improve the embedding performance of binary embedding methods. The proposed method is based on a continuous valued Johnson-Lindenstrauss embedding and uses a fast linear transformation to transform the Euclidean distances to the Walsh-Hadamard matrix. The authors show that the proposed method has a time complexity of $O(1/\sqrt{T})$ and space complexity of $\Omega(T)$. The authors also provide a polynomial decay of the quantization error.   ,"This paper proposes a continuous valued Johnson-Lindenstrauss embedding method for binary embedding. The proposed method is based on a stable noise-shaping quantization scheme, where the quantization error is polynomial in the embedding dimension. The authors show that the proposed method can be applied to a well-spread data set with high-dimensional data. They show that it can be used to improve the accuracy of binary codes on natural images."
2025,SP:f65e229bca3904095743e7a501b1083cc60f1e22,"robustness CONJUNCTION generalization. generalization CONJUNCTION robustness. process USED-FOR ANNs. synaptic plasticity rules USED-FOR Gradient Descent ( GD ). rule parameters USED-FOR GD. GD USED-FOR rules. plasticity rules USED-FOR recurrent neural nets ( RNNs ). GD USED-FOR plasticity rules. rules USED-FOR MNIST / Fashion MNIST. synthetic data USED-FOR rules. process USED-FOR plasticity rules. adversarial perturbations FEATURE-OF tolerance. tolerance FEATURE-OF classifiers. plasticity rules USED-FOR classifiers. perceptron algorithm CONJUNCTION multiplicative weights method. multiplicative weights method CONJUNCTION perceptron algorithm. GD USED-FOR plasticity rule. GD USED-FOR perceptron algorithm. GD USED-FOR multiplicative weights method. GD USED-FOR learning rules. evolutionary time FEATURE-OF it. Task are learning tasks, and genetic setting. Method are artificial neural nets ( ANNs ), backpropagation, and classification network. Generic is data. OtherScientificTerm is numerical parameter. ","This paper proposes to use plasticity rules to improve robustness and generalization in deep neural networks. The proposed method is based on gradient descent (GD), which is an evolutionary algorithm that learns a set of plasticity parameters that can be used to update the weights of the network during training. The authors show that the proposed method improves robustness in terms of robustness to adversarial perturbations and generalizes better on synthetic data.  ","This paper proposes a genetic plasticity rule for recurrent neural networks (RNNs) that can be used to improve robustness against adversarial perturbations. The proposed method is based on the Gradient Descent (GD) method, which is used to learn plasticity rules for RNNs. The main contribution of the paper is that the proposed method can be combined with the perceptron algorithm and the multiplicative weights method to improve the robustness of the RNN. The method is evaluated on MNIST and Fashion MNIST datasets. "
2041,SP:f435530146fa975cb27cd375a857df9bcbd87682,"visual question generation ( VQG ) USED-FOR human - like questions. image CONJUNCTION side information. side information CONJUNCTION image. image USED-FOR visual question generation ( VQG ). side information USED-FOR human - like questions. image USED-FOR human - like questions. visual objects PART-OF image. side information CONJUNCTION image. image CONJUNCTION side information. image USED-FOR generating referential and meaningful questions. learning paradigm USED-FOR visual questions. answer - awareness CONJUNCTION region - reference. region - reference CONJUNCTION answer - awareness. region - reference USED-FOR learning paradigm. answer - awareness FEATURE-OF visual questions. Double Hints textual answers CONJUNCTION visual regions of interests. visual regions of interests CONJUNCTION Double Hints textual answers. Double Hints textual answers USED-FOR visual questions. methodology USED-FOR visual hints. dynamic graph USED-FOR them. VQA2.0 CONJUNCTION COCO - QA datasets. COCO - QA datasets CONJUNCTION VQA2.0. model COMPARE baselines. baselines COMPARE model. COCO - QA datasets EVALUATE-FOR model. VQA2.0 EVALUATE-FOR model. COCO - QA datasets EVALUATE-FOR baselines. setting EVALUATE-FOR model. Task are VQG, and one - to - many mapping issue. OtherScientificTerm are human annotations, implicit topology end - to - end, and double hints. Method is graph - to - sequence model. ","This paper proposes a method for visual question generation (VQA) in the presence of human annotations. The proposed method is based on the idea of double hints, which are visual hints that can be used to generate referential and meaningful questions. The method is evaluated on VQA2.0 and COCO-QA datasets, and achieves state-of-the-art performance.","This paper proposes a method for visual question generation (VQG) that learns to generate referential and meaningful questions from a set of visual hints. The key idea is to use a graph-to-sequence model to model the relationship between the visual hints and the visual regions of interest (e.g., region-reference, answer-awareness, and region-area). The model is trained on two datasets, VQA2.0 and COCO-QA, and it is shown that it can generate more meaningful questions than the baseline model. "
2057,SP:53a26ce11647866d3f6ba8b84ca9f13106197a8d,sample size CONJUNCTION model size. model size CONJUNCTION sample size. linear regression CONJUNCTION neural networks. neural networks CONJUNCTION linear regression. linear regression HYPONYM-OF learning algorithms. neural networks HYPONYM-OF learning algorithms. optimal regularization USED-FOR double - descent phenomenon. sample size CONJUNCTION model size. model size CONJUNCTION sample size. isotropic data distribution FEATURE-OF linear regression models. optimally - tuned ` 2 regularization USED-FOR linear regression models. optimally - tuned ` 2 regularization USED-FOR double descent. test risk scalings EVALUATE-FOR algorithms. tuned regularization USED-FOR algorithms. tuned regularization USED-FOR test risk scalings. Task is generalization. ,"This paper studies the double-descent phenomenon in linear regression models with isotropic data distribution. The authors show that under certain assumptions on the model size and the number of samples, linear regression with optimal regularization suffers double descent. They show that this double descent phenomenon can be explained by the fact that the sample size and model size increase linearly with the size of the data set. They then show that the double descent can be understood as a function of the model model size, and propose two regularization strategies to mitigate this double-decay phenomenon. ","This paper studies the double-descent phenomenon in linear regression with isotropic data distribution. The authors show that the optimal regularization of linear regression models can lead to double descent in the presence of a large sample size and large model size. They show that this phenomenon is due to the fact that the sample size of the model is larger than the model size, and that the number of samples is smaller than that of the training data. They propose a new regularization method, called `2-regularization', which is a combination of two regularizations: 1) an optimally-tuned `2 regularization for linear regression, and 2) a regularization that is tuned to minimize the test risk scalings. They also show that their method can be used to improve the performance of the two regularization methods."
2073,SP:c193ccc74b987beaf8d53a29a8529a0af5e87742,spatial regularities FEATURE-OF images. spatial regularities USED-FOR generative modeling. neural network USED-FOR building image generators ( decoders ). it USED-FOR variational autoencoders ( VAEs ). sequential gating - based mechanism USED-FOR contextual information. feature maps PART-OF deep neural net. sequential gating - based mechanism USED-FOR feature maps. spatial dependency layers USED-FOR density estimation. decoder USED-FOR density estimation. decoder USED-FOR hierarchical VAE. baseline convolutional architectures USED-FOR density estimation. spatial dependency layers USED-FOR hierarchical VAE. spatial dependency layers USED-FOR decoder. SDN USED-FOR large images. SDN decoder USED-FOR learning disentangled representations. neural architectures USED-FOR task. SDN decoder USED-FOR vanilla VAE setting. spatial dependency COMPARE convolutional layers. convolutional layers COMPARE spatial dependency. Method is spatial dependency networks ( SDNs ). OtherScientificTerm is 2 - D space. Generic is models. Material is VAE settings. ,"This paper proposes a spatial dependency network (SDN) to learn disentangled representations from images. The proposed method is based on spatial dependency networks (SDNs), which is an extension of spatial dependency layers in VAE. The main contribution of this paper is to introduce spatial dependency in the decoder of a deep neural network (DNN). The authors show that spatial dependency can be used to improve the performance of DNNs on image generation tasks. ",This paper proposes a spatial dependency network (SDN) to learn disentangled representations of images. The main idea is to use spatial dependency layers to learn the feature maps of feature maps in a deep neural network (DNN). The spatial dependency layer is a sequential gating-based mechanism that maps feature maps into a spatial feature map. The spatial feature maps are then used for density estimation in a hierarchical VAE. The authors show that spatial dependency can be used to learn representations that are disentanglement-free and disentangle representations.
2089,SP:db91512a90e75675af03c2f197751c8526d6f5e9,"prior approach USED-FOR offline RL. backup operator USED-FOR algorithm. EMaQ USED-FOR sub - optimality bounds. complexity EVALUATE-FOR offline RL problems. proposal distribution USED-FOR EMaQ. offline RL setting EVALUATE-FOR EMaQ. D4RL benchmarks EVALUATE-FOR EMaQ. EMaQ COMPARE Soft Actor Critic ( SAC ). Soft Actor Critic ( SAC ) COMPARE EMaQ. online RL setting EVALUATE-FOR EMaQ. generative model design USED-FOR estimating behavior policies. complexity EVALUATE-FOR offline RL problems. Method are Off - policy reinforcement learning ( RL ), off - policy RL methods, and BCQ. Generic is methods. OtherScientificTerm are policies, dataset of interactions, heuristic design choice, behavior policy, distribution support, behavior policies, and function approximator. ","This paper proposes a new offline reinforcement learning algorithm called EMaQ, which is a variant of BCQ. The main idea is to use a generative model to estimate the proposal distribution of policies, and then use the proposed distribution as a backup operator for offline RL. Theoretical analysis is provided to show that the proposed method achieves sub-optimality bounds in the offline RL setting. Empirical results on D4RL benchmarks demonstrate the effectiveness of the proposed algorithm. ","This paper proposes a new algorithm for offline reinforcement learning (D4RL) based on a generative model design for estimating behavior policies. The proposed method, called EMaQ, uses a backup operator to estimate the proposal distribution of the proposed policies in the offline setting. Empirical results show that the proposed method outperforms the state-of-the-art in terms of sub-optimality. "
2105,SP:e2b80adeaa9208e0667a64a3f24661f77b48e487,"fair machine learning model USED-FOR demographic disparity. data preprocessing CONJUNCTION model training. model training CONJUNCTION data preprocessing. Existing techniques USED-FOR model fairness. outer optimizer USED-FOR inner problem. inner optimizer USED-FOR training algorithm. minibatch sizes USED-FOR model fairness. equal opportunity CONJUNCTION equalized odds. equalized odds CONJUNCTION equal opportunity. equalized odds CONJUNCTION demographic parity. demographic parity CONJUNCTION equalized odds. optimization USED-FOR batch selection algorithm. FairBatch HYPONYM-OF batch selection algorithm. fairness measures PART-OF batch selection algorithm. equal opportunity HYPONYM-OF fairness measures. demographic parity HYPONYM-OF fairness measures. equalized odds HYPONYM-OF fairness measures. data preprocessing CONJUNCTION model training. model training CONJUNCTION data preprocessing. PyTorch code USED-FOR batch selection. batch selection PART-OF model training. FairBatch USED-FOR fairness. fine - tuning USED-FOR FairBatch. It CONJUNCTION batch selection techniques. batch selection techniques CONJUNCTION It. faster convergence HYPONYM-OF batch selection techniques. Method are machine learning systems, and bilevel optimization. Generic are functionality, and it. Material is synthetic and benchmark real data. ","This paper proposes a batch selection algorithm for model fairness. The proposed algorithm is based on bilevel optimization, where the outer optimizer optimizes the inner problem and the inner optimizer selects the minibatch size for the training algorithm. The authors show that the proposed algorithm can be used in conjunction with existing batch selection techniques to improve model fairness in the training process. They show that their algorithm is able to improve the model fairness performance on synthetic and real data.","This paper proposes a batch selection algorithm for model fairness. The main idea is to use batch selection as a bilevel optimization problem, where the outer optimizer optimizes the inner problem, and the inner optimizer is used to optimize the training algorithm. The authors show that the batch selection method can be used to improve model fairness in terms of equal opportunity, equalized odds, and demographic parity. They also show that batch selection can be fine-tuned to improve fairness. "
2121,SP:72f26b850bb2258223c0fc71598e35ad07d690e6,"robustness guarantees CONJUNCTION generalization bounds. generalization bounds CONJUNCTION robustness guarantees. Lipschitz constants FEATURE-OF deep networks. generalization bounds CONJUNCTION smoothness of decision boundaries. smoothness of decision boundaries CONJUNCTION generalization bounds. bounds USED-FOR models. deep equilibrium ( DEQ ) model HYPONYM-OF models. monotone DEQs HYPONYM-OF DEQs. Lipschitz constants FEATURE-OF monotone DEQs. input - output mapping CONJUNCTION weight - output mapping. weight - output mapping CONJUNCTION input - output mapping. simple - yet - tight bounds USED-FOR input - output mapping. simple - yet - tight bounds USED-FOR weight - output mapping. networks USED-FOR weight - output mapping. bounds USED-FOR monotone DEQ models. multiscale convolutional structure USED-FOR monotone DEQ models. bounds USED-FOR PAC - Bayes generalization bounds. Method are infinitely - deep network, and DNNs. OtherScientificTerm are monotonicity parameter, Lipschitz constant, and exponential depth - dependence of comparable DNN bounds. Generic is they. ","This paper studies generalization bounds for deep equilibrium models with monotone weights and Lipschitz constants. The main contributions are:  1. The authors show that for monotonicity-monotone DEQ models with multiscale convolutional structure, the PAC-Bayes generalization bound can be derived from a simple yet tight bounds on the input-output mapping.  2. They show that the bounds can be extended to the case of infinitely-deep networks.  3. They also show that if the weight-outward mapping of a monotonically-weighted DEQ model is monotonic, then the generalization error can be reduced to that of a simple deep equilibrium model. ",This paper studies the generalization bounds of deep equilibrium (DEQ) models with monotone Lipschitz constants. The main contribution of the paper is to provide bounds on the generalizability of the input-output mapping and the weight- output mapping of the DEQ model. The bounds are based on the PAC-Bayes generalization bound and the smoothness of the decision boundaries. The authors also provide bounds for the monotonicity of the weights and output of the model.
2137,SP:bcfd4d7fd4590e3bc248a0a5422ce4b67db74a74,imitation learning CONJUNCTION goal - conditioned reinforcement learning. goal - conditioned reinforcement learning CONJUNCTION imitation learning. goal - conditioned reinforcement learning HYPONYM-OF settings. imitation learning HYPONYM-OF settings. probabilistic long - term dynamics CONJUNCTION desired value function. desired value function CONJUNCTION probabilistic long - term dynamics. density estimation USED-FOR approach. it USED-FOR hindsight bias. hindsight bias FEATURE-OF stochastic domains. it USED-FOR sparse rewards. expert data USED-FOR approach. Generic is solutions. Method is goalconditioned reinforcement learning. ,This paper proposes a method for goal-conditioned reinforcement learning that uses density estimation to estimate the long-term dynamics of the environment. The density estimation is based on a probabilistic model of the state-action space. The authors show that the proposed method is able to learn the desired value function in the presence of sparse rewards. The method is shown to outperform prior methods on a variety of tasks. ,"This paper proposes a new method for goal-conditioned reinforcement learning. The main idea is to use a probabilistic long-term dynamics to estimate the value function of a reward function in a stochastic setting. The authors show that the proposed method can be applied to a variety of settings, including imitation learning, goal conditioned reinforcement learning, and goal conditioned imitation learning with sparse rewards. They also show that it can be used to improve the performance of goal conditioned RL. "
2153,SP:d57550b2f323b356d7e609acc35ee33039f376b4,"probabilistic inference framework USED-FOR simultaneously learning multiple related tasks. variational multi - task learning VMTL HYPONYM-OF probabilistic inference framework. variational Bayesian inference problem USED-FOR multi - task learning. priors USED-FOR task relatedness. mixture of variational posteriors USED-FOR prior. representations CONJUNCTION classifiers. classifiers CONJUNCTION representations. VMTL USED-FOR multi - task learning. limited training data USED-FOR VMTL. limited training data USED-FOR multi - task learning. benchmark datasets EVALUATE-FOR it. Method is Multi - task learning. OtherScientificTerm are Gumbel - softmax priors, mixing weights, and shared inductive bias. Generic is tasks. ","This paper proposes a variational multi-task learning (VMTL) framework for simultaneously learning multiple related tasks. The main idea is to use a mixture of variational posteriors to model the task-relatedness of each task, and then use a Gumbel-softmax prior to learn the task representations and classifiers. The proposed VMTL is shown to achieve state-of-the-art performance on several benchmark datasets. ","This paper proposes a new probabilistic multi-task learning framework, called VMTL, for the problem of simultaneously learning multiple related tasks. The main idea is to use a mixture of variational posteriors for the task-specific priors, which are learned by mixing the weights of the two tasks. This is achieved by using a Gumbel-softmax prior for each task. The authors show that the proposed method outperforms the state-of-the-art in terms of performance on several benchmark datasets. "
2169,SP:3ccdf8322f16c8a7bef82e32fad4c03969a510d1,"Transformers USED-FOR long sequence lengths. fast Transformers USED-FOR problem. model quality EVALUATE-FOR vanilla Transformer models. systematic and unified benchmark EVALUATE-FOR model quality. Long - Range Arena EVALUATE-FOR model quality. long - context scenarios FEATURE-OF model quality. Long - Range Arena HYPONYM-OF systematic and unified benchmark. text CONJUNCTION natural, synthetic images. natural, synthetic images CONJUNCTION text. natural, synthetic images CONJUNCTION mathematical expressions. mathematical expressions CONJUNCTION natural, synthetic images. similarity USED-FOR mathematical expressions. Linear Transformers CONJUNCTION Sinkhorn Transformers. Sinkhorn Transformers CONJUNCTION Linear Transformers. Synthesizers CONJUNCTION Sparse Transformers. Sparse Transformers CONJUNCTION Synthesizers. Linformers CONJUNCTION Linear Transformers. Linear Transformers CONJUNCTION Linformers. Performers CONJUNCTION Synthesizers. Synthesizers CONJUNCTION Performers. Sparse Transformers CONJUNCTION Longformers. Longformers CONJUNCTION Sparse Transformers. Sinkhorn Transformers CONJUNCTION Performers. Performers CONJUNCTION Sinkhorn Transformers. Reformers CONJUNCTION Linformers. Linformers CONJUNCTION Reformers. Performers CONJUNCTION Sparse Transformers. Sparse Transformers CONJUNCTION Performers. Synthesizers CONJUNCTION Longformers. Longformers CONJUNCTION Synthesizers. benchmark suite EVALUATE-FOR long - range Transformer models. Longformers HYPONYM-OF long - range Transformer models. Sparse Transformers HYPONYM-OF long - range Transformer models. Sinkhorn Transformers HYPONYM-OF long - range Transformer models. Reformers HYPONYM-OF long - range Transformer models. Synthesizers HYPONYM-OF long - range Transformer models. Linformers HYPONYM-OF long - range Transformer models. Performers HYPONYM-OF long - range Transformer models. Linear Transformers HYPONYM-OF long - range Transformer models. Long - Range Arena USED-FOR Transformer models. Metric are quadratic self - attention complexity, and","This paper introduces Long-Range Arena (LRA), a systematic and unified benchmark for evaluating long-range Transformer models. The authors show that the performance of long-term transformer models is highly dependent on the number of tokens in the input sequence and the length of the sequence. They also show that long-time transformer models are more efficient than short-term models in terms of the self-attention complexity.    The authors conduct extensive experiments on text, synthetic images, and mathematical expressions to show the effectiveness of the proposed LongRangeArena. ","This paper presents a systematic and unified benchmark for evaluating the quality of long-range Transformer models. The authors propose Long-Range Arena (LRA), a systematic, unified and systematic evaluation of the model quality in long-context scenarios. The main contribution of LRA is the introduction of a new benchmark suite, Long-range Arena, which aims to evaluate model quality across a variety of long range scenarios, including text, synthetic images, and mathematical expressions.  The authors show that the performance of the models on LRA can be improved over the vanilla vanilla Transformer model. They also show that LRA outperforms the state-of-the-art in terms of model quality."
2185,SP:e12e410c3335b76133ceda4c865b244fbbab8580,"Structure CONJUNCTION Context. Context CONJUNCTION Structure. Structure USED-FOR machine learning models. Context USED-FOR machine learning models. Structure of source code USED-FOR model. source code CONJUNCTION features. features CONJUNCTION source code. language - agnostic features USED-FOR model. features HYPONYM-OF language - agnostic features. AST USED-FOR features. source code HYPONYM-OF language - agnostic features. programming languages EVALUATE-FOR monolingual code summarization. Structure CONJUNCTION Context. Context CONJUNCTION Structure. Structure USED-FOR representation learning on code. Context USED-FOR representation learning on code. OtherScientificTerm are Source code ( Context ), and computer program. Method is multilingual code summarization model. Material are non - parallel data, and low - resource languages. ","This paper proposes a multi-lingual code summarization model that learns to predict the structure and context of source code. The main idea is to learn to predict both the structure of the source code and the context of the program. The model is trained on a set of multi-language datasets in three languages: English, French, and German. The proposed model is able to perform well on the task of code summarisation.   ","This paper proposes a multi-lingual code summarization model for multi-language programming languages. The main idea of the model is to combine the structure of source code with the context of the source code. The model is trained using a monolingual language-agnostic representation learning framework. The proposed model is evaluated on CIFAR-10, COCO-100, and Python.  "
2201,SP:f46e98d48f90071831f1c0069bf74a7993be6db8,"sights USED-FOR sound source. recurrent aggregations of the audio observations USED-FOR models. reinforcement learning approach USED-FOR audio - visual navigation. elements USED-FOR reinforcement learning approach. waypoints PART-OF elements. audio and visual data USED-FOR geometry of an unmapped space. real - world 3D scenes CONJUNCTION Replica. Replica CONJUNCTION real - world 3D scenes. real - world 3D scenes EVALUATE-FOR approach. sights CONJUNCTION sounds. sounds CONJUNCTION sights. sounds CONJUNCTION space. space CONJUNCTION sounds. OtherScientificTerm are agent motion, acoustic memory, and audio_visual_waypoints. Method is navigation policy. Generic is model. ",This paper proposes a reinforcement learning approach for audio-visual navigation in 3D scenes. The proposed method is based on combining audio and visual data to learn a waypoint-based navigation policy in an unmapped space. The key idea is to use recurrent aggregations of the audio observations and visual observations to train a model that learns to predict the waypoints in the space. Experiments show that the proposed method outperforms baselines in both synthetic and real-world experiments.,This paper proposes a new approach for audio-visual navigation. The key idea is to combine audio and visual data to map the geometry of an unmapped space. The proposed approach is based on recurrent aggregations of the audio observations and visual waypoints. The approach is evaluated on three real-world 3D scenes and two replicas. 
2217,SP:23bfe317dcef00a91ea92389b3f39d9b93972454,"optimization methods COMPARE weight initializations. weight initializations COMPARE optimization methods. learning abilities FEATURE-OF neural networks. small CNN USED-FOR update rules. architecture USED-FOR task. task USED-FOR networks. architecture USED-FOR networks. initialization parameters USED-FOR gradient descent. single sign change HYPONYM-OF small perturbations. OtherScientificTerm are lottery tickets, and lottery ticket hypothesis. Method are small convolutional networks, and minimal networks. ","This paper studies the lottery ticket hypothesis, which is a well-known and well-studied idea that neural networks with a small convolutional network can learn faster than networks with large CNNs. The authors show that this is the case in the presence of small perturbations (e.g. a single sign change) to the weights of the network, and that this phenomenon can be explained by the fact that the weights in a small CNN can be optimized by gradient descent.   The authors then show that the lottery tickets hypothesis is true in the case of a very small CNN, and show that it is true for a very large CNN. They also show that if the weight initializations are large enough, then gradient descent can be used to optimize the weights.  Finally, the authors propose a new method to compute the weights for the weights, which they call ""minimal networks"".  ","This paper studies the lottery ticket hypothesis, which claims that the training of neural networks with small convolutional networks (i.e., a small CNN with a single sign change) can lead to better performance than training with a large CNN. The authors propose a novel architecture for training neural networks, where the weights of the network are fixed and the parameters of gradient descent are fixed. They show that this architecture can be used to improve the performance of the networks. They also provide a theoretical analysis of the effect of small perturbations of the weights."
2233,SP:1b5ba618d3e28d48f9205c0780f8288a08fa5392,"Semi - supervised learning ( SSL ) USED-FOR unlabeled data. consistency regularization USED-FOR SSL approaches. RankingMatch HYPONYM-OF method. computational efficiency EVALUATE-FOR objective function. BatchMean Triplet loss HYPONYM-OF objective function. accuracy EVALUATE-FOR SVHN. accuracy EVALUATE-FOR SVHN. accuracy CONJUNCTION accuracy. accuracy CONJUNCTION accuracy. accuracy EVALUATE-FOR RankingMatch. accuracy EVALUATE-FOR RankingMatch. SSL benchmarks EVALUATE-FOR RankingMatch. BatchMean Triplet loss COMPARE Triplet loss. Triplet loss COMPARE BatchMean Triplet loss. ablation study EVALUATE-FOR BatchMean Triplet loss. ablation study EVALUATE-FOR Triplet loss. Material are labeled data, CIFAR-10, and CIFAR-100. Generic is model. OtherScientificTerm is labeled data amounts. ",This paper proposes a method for semi-supervised learning with unlabeled data. The method is based on a consistency regularization term that aims to improve the consistency of the learned model. The proposed method is evaluated on CIFAR-10/100 and on ImageNet.   ,"This paper proposes a new objective function for semi-supervised learning (SSL) with consistency regularization. The objective function is based on the BatchMean Triplet loss (BMP), which is a regularization term for the consistency of the training data. The authors show that the BMP can be used to improve the accuracy of the model on CIFAR-10 and CIFar-100 datasets. They also provide ablation studies to show the effectiveness of BMP. "
2249,SP:f3abccf4a2566ffbc821aba209fab15058639ad4,"formulation USED-FOR sequential learning setting. meta - training CONJUNCTION adaptation. adaptation CONJUNCTION meta - training. sample complexity CONJUNCTION regret. regret CONJUNCTION sample complexity. sample complexity EVALUATE-FOR empirical risk minimization methods. regret EVALUATE-FOR empirical risk minimization methods. meta - learning USED-FOR online setting. meta - learning COMPARE empirical risk minimization methods. empirical risk minimization methods COMPARE meta - learning. regret EVALUATE-FOR meta - learning. sample complexity EVALUATE-FOR meta - learning. bi - level optimizations FEATURE-OF meta - learning algorithms. meta - training data USED-FOR meta - learning algorithms. meta - training data USED-FOR bi - level optimizations. meta - learning algorithms USED-FOR variable - shot settings. many - shot learning CONJUNCTION zero - shot learning. zero - shot learning CONJUNCTION many - shot learning. variable - shot settings PART-OF sequential learning. meta - learning algorithms USED-FOR sequential learning. zero - shot learning HYPONYM-OF variable - shot settings. meta - learning COMPARE supervised methods. supervised methods COMPARE meta - learning. cumulative performance EVALUATE-FOR supervised methods. sequential learning problems EVALUATE-FOR meta - learning. cumulative performance EVALUATE-FOR meta - learning. meta - learning USED-FOR learning systems. Method are Few - shot meta - learning methods, and metalearning. Generic is problem. ","This paper studies the problem of few-shot meta-learning, where the goal is to learn a classifier from meta-training and adaptation data. The authors show that meta-learners can achieve better performance than supervised learning in the online setting. The main contribution is a theoretical analysis of the sample complexity and regret of the proposed methods.   The main results are as follows:   1. The paper shows that the proposed method achieves better sample complexity than the state-of-the-art methods in online settings.  2. The proposed method is able to achieve better regret than the best supervised methods in many-shot and zero-shot settings.","This paper studies the problem of few-shot meta-learning in the variable-shot setting, which is the setting where the number of training samples is small. The authors propose a new meta-training algorithm for this setting, and show that it outperforms the state-of-the-art empirical risk minimization methods in terms of sample complexity and regret. They also show that their algorithm outperforms supervised methods in many-shot learning and zero-shot settings. "
2265,SP:95cb420d92ec42e12a4bbb0e66224f1c498a7161,"contextual representations USED-FOR NLP tasks. pretrained Transformer models USED-FOR contextual representations. representations USED-FOR sentence - level syntax. self - supervision USED-FOR Transformers networks. probes EVALUATE-FOR Transformer representations. random permutations of n - grams HYPONYM-OF perturbations. syntactic distance FEATURE-OF attention mechanism. local phrase structure FEATURE-OF sensitivity. Generic are they, network, probe, and representation. Task is computational and cognitive neuroscience. OtherScientificTerm are representational invariance, word position, syntactic phrase, global phrase structure, hierarchical phrase structure, and attention weights. Method are Transformer architecture, and Transformers. ",This paper proposes a method to improve the performance of Transformer-based models on sentence-level syntactic parsing tasks. The method is based on the observation that the attention mechanism of a pre-trained Transformer model is invariant to the syntactic distance between words and phrases. The authors show that this invariance is due to the fact that attention weights are independent of the local phrase structure of the input sentence. They also show that the model is sensitive to perturbations that change the global phrase structure.   ,"This paper proposes a new approach to learn Transformer representations for sentence-level syntactic syntax. The authors propose a new task called Transformer Contextual Representation Discovery (TRD), which aims to learn representations that are invariant to the syntactic phrase position and global phrase structure. They show that TRD can learn representations with respect to the local phrase structure and local word position. They also show that the attention mechanism is sensitive to syntactic distance between local and global phrases.   "
2281,SP:cb27b27a6fefc192ad1c2bd083d13eb9e51a5c44,"high - fidelity images USED-FOR Generative Adversarial Networks ( GAN ). large - scale GPU - clusters USED-FOR Generative Adversarial Networks ( GAN ). few - shot image synthesis task USED-FOR GAN. minimum computing cost FEATURE-OF few - shot image synthesis task. 1024 × 1024 resolution EVALUATE-FOR light - weight GAN structure. skip - layer channel - wise excitation module CONJUNCTION self - supervised discriminator. self - supervised discriminator CONJUNCTION skip - layer channel - wise excitation module. feature - encoder USED-FOR self - supervised discriminator. model COMPARE StyleGAN2. StyleGAN2 COMPARE model. datasets EVALUATE-FOR model. datasets EVALUATE-FOR StyleGAN2. image domains FEATURE-OF datasets. OtherScientificTerm are RTX-2080 GPU, and computing budget. ","-based GANs are becoming more and more popular in recent years due to their ability to generate high-quality images. This paper proposes a new GAN architecture called StyleGAN2, which is based on a skip-layer channel-wise excitation module and a self-supervised discriminator. The novelty of the proposed architecture lies in the fact that it is able to handle few-shot image synthesis tasks with a minimum computing cost. The experimental results show that the proposed model achieves state-of-the-art performance on several image datasets.",This paper proposes a new GAN architecture for the few-shot image synthesis task. The architecture is based on a skip-layer channel-wise excitation module and a self-supervised discriminator. The proposed architecture is evaluated on a variety of image datasets. 
2297,SP:c0dbeb5d94b2388595cf7ad9675c55df0bac7f8e,"neural network bounding USED-FOR neural network verification systems. specialised dual solvers USED-FOR neural network bounds. linear program HYPONYM-OF relaxation. linear relaxation USED-FOR piecewise linear activations. dual algorithm USED-FOR relaxation. tightness CONJUNCTION linear separation oracle. linear separation oracle CONJUNCTION tightness. method USED-FOR relaxation. dual space FEATURE-OF relaxation. tightness HYPONYM-OF relaxation. linear separation oracle HYPONYM-OF relaxation. dual approaches USED-FOR weaker relaxations. massive parallelism CONJUNCTION GPU implementation. GPU implementation CONJUNCTION massive parallelism. dual approaches USED-FOR it. bounds COMPARE off - the - shelf solvers. off - the - shelf solvers COMPARE bounds. speed - accuracy trade - offs EVALUATE-FOR dual solvers. running time EVALUATE-FOR off - the - shelf solvers. Generic is they. Method is customised solver. OtherScientificTerm are dual variables, computational budget, and formal verification speed - ups. ",This paper proposes a specialised dual solver for neural network bounding. The main idea is to use a linear relaxation for piecewise linear activations to reduce the computational cost of the neural network bounds. The proposed method is based on a dual algorithm that uses a linear separation oracle to compute the relaxation in the dual space. The authors show that the proposed method achieves faster running time compared to off-the-shelf solvers. ,This paper proposes a specialised dual solver for neural network bounding. The main idea is to use a linear program with piecewise linear activations and a dual algorithm for the relaxation of the neural network bounds. The authors show that the proposed method is faster than off-the-shelf solvers in terms of the computational budget and speed-up of the verification process. They also show that their method is able to achieve better accuracy and speed up the verification procedure. 
2313,SP:56e3837417dbcce0d65338dc3aac4e1a20eb0df8,"masked token prediction CONJUNCTION masked span infilling. masked span infilling CONJUNCTION masked token prediction. masked span infilling USED-FOR T5 - style PTLMs. masked token prediction USED-FOR BERT - style PTLMs. everyday concepts FEATURE-OF relational commonsense knowledge. BERT - style PTLMs HYPONYM-OF pre - training objectives. masked token prediction HYPONYM-OF pre - training objectives. masked span infilling HYPONYM-OF pre - training objectives. intermediate self - supervised learning tasks USED-FOR PTLMs. them USED-FOR intermediate self - supervised learning tasks. generative and contrastive objectives USED-FOR common sense. task - specific fine - tuning USED-FOR PTLMs. concept - centric commonsense knowledge USED-FOR PTLMs. pre - training framework USED-FOR generative and contrastive objectives. concept - aware language model ( CALM)1 HYPONYM-OF method. CALM COMPARE PTLMs. PTLMs COMPARE CALM. CALM COMPARE baseline methods. baseline methods COMPARE CALM. CALM USED-FOR PTLM. baseline methods COMPARE PTLMs. PTLMs COMPARE baseline methods. Method are Pre - trained language models ( PTLM ), and text - to - text transformer. Generic is they. OtherScientificTerm are commonsense knowledge, and external knowledge graphs. Task is NLU and NLG tasks. ",This paper proposes a concept-aware pre-trained language model (CALM) for NLU and NLG tasks. The main contribution of the paper is the introduction of concept-centric commonsense knowledge as a pre-training objective in the text-to-text transformer (T5) language model. The authors show that the proposed method can achieve state-of-the-art performance on the NLU task and comparable performance on NLG task.  ,This paper proposes a concept-aware language model (CALM) for pre-trained language models (TLMs) that can be used for intermediate self-supervised learning tasks (NLU and NLG) and fine-tune for task-specific fine-tuning of PTLMs. The key idea is to use the concept-centric commonsense knowledge from the external knowledge graph as a pre-training objective for NLU tasks. The proposed method is evaluated on a variety of tasks and compared with a number of baselines.
2329,SP:7ec69bdee021af506293c87a3b75bce1c40a03d7,"frameworks USED-FOR 2D segments. object interactions HYPONYM-OF physics. multi - scale pixel cues CONJUNCTION physical motion cues. physical motion cues CONJUNCTION multi - scale pixel cues. synthetic and real scenes EVALUATE-FOR model. object properties USED-FOR physical events. Task is unsupervised physical object discovery. OtherScientificTerm are 3D geometry, developmental psychology, and observable and partially occluded objects. Material is video. ",This paper proposes a method for unsupervised physical object discovery in video. The method is based on a multi-scale pixel-based model that learns to predict the location and motion of objects in a video frame. The model is trained to predict object properties and the object properties are used to model the physical events in the video. Experiments on synthetic and real scenes show that the proposed method outperforms the state-of-the-art.,"This paper proposes a new method for unsupervised physical object discovery in video. The proposed method is based on the idea of object interactions between two objects in a video, which is a common problem in physics. The authors propose to use a multi-scale pixel cue and physical motion cues to learn the object properties of a scene. The model is trained on synthetic and real-world scenes, and it is shown that the proposed method outperforms the state-of-the-art."
2345,SP:66997bc19a3ba6548fcf21f114e748bea95cad1c,"Deep neural networks ( DNNs ) USED-FOR adversarial attacks. convolutional neural networks PART-OF Deep neural networks ( DNNs ). attack algorithms USED-FOR Adversarial samples. training method USED-FOR DNN robustness. adversarial noises FEATURE-OF DNN robustness. Increasing Margin Adversarial ( IMA ) Training HYPONYM-OF training method. IMA method USED-FOR margins. decision boundaries USED-FOR DNN model. IMA method USED-FOR training. robustness EVALUATE-FOR IMA method. clean data EVALUATE-FOR accuracy. noisy data EVALUATE-FOR method. accuracy EVALUATE-FOR method. clean data EVALUATE-FOR method. classification accuracy EVALUATE-FOR method. approach USED-FOR robust DNN applications. COVID-19 diagnosis HYPONYM-OF robust DNN applications. CT images USED-FOR COVID-19 diagnosis. Task is life - critical applications. OtherScientificTerm is white noises. Material are COVID-19 CT image dataset, and 100 - PGD white - box adversarial attacks. ",This paper proposes a method to improve the robustness of deep neural networks against adversarial attacks. The proposed method is based on increasing the margin between the decision boundaries of the model and the input data. The method is evaluated on the COVID-19 CT image dataset and shows improved robustness against white-box attacks.,"This paper proposes a method to improve the robustness of deep neural networks (DNNs) against white-box adversarial attacks. The method is based on the idea of increasing Margin Adversarial (IMA) training, which is a training method for DNNs. The main idea is to train a DNN model on a set of samples with white noise, and then use the white noise samples to train the model. The authors show that the IMA method can improve the DNN robustness against white box attacks. They also show that their method can be applied to a variety of applications, including life-critical applications."
2361,SP:276ffd59fbf49e3ee02756da8920218102214917,"Knowledge distillation USED-FOR compact models. teacher model USED-FOR compact student network. poor local optima FEATURE-OF optimization. ProKT HYPONYM-OF model - agnostic method. supervision signals USED-FOR teacher model. student ’s parameter space FEATURE-OF supervision signals. local intermediate targets FEATURE-OF training objective. approximate mirror descent technique USED-FOR local intermediate targets. approximate mirror descent technique USED-FOR projection. quirks USED-FOR optimization. quirks USED-FOR method. ProKT COMPARE knowledge distillation methods. knowledge distillation methods COMPARE ProKT. image and text datasets EVALUATE-FOR ProKT. Method is Deep neural networks. OtherScientificTerm are computation capacity, and local optima. ","This paper proposes ProKT, a knowledge distillation method for compact deep neural networks. The main idea is to learn a compact student network by distilling the knowledge from the teacher model to the compact student model. The proposed method is model-agnostic and can be applied to any model agnostic training objective. The method is evaluated on image and text datasets and achieves good performance.","This paper proposes ProKT, a new knowledge distillation method for compact deep neural networks. The main idea is to use a teacher model to distill the student model into a compact student network. The teacher model is trained to predict the parameters of the student network, and the student is trained on the student's parameter space. The student is then trained on a set of intermediate intermediate targets, which are learned using a mirror descent technique. The paper shows that ProKT outperforms the state-of-the-art on both image and text datasets. "
2377,SP:906dc21d6988953fcf57d63bbdd12973e5818d16,"channel pruning method USED-FOR compression. compression FEATURE-OF Convolutional Neural Networks ( CNNs ). hyper - structure network USED-FOR architecture of the main network. hypernet COMPARE hyperstructure network. hyperstructure network COMPARE hypernet. regular backpropagation USED-FOR hyperstructure network. regularization term USED-FOR computational resource. regularization term USED-FOR compact network. computational resource FEATURE-OF compact network. FLOPs USED-FOR computational resource. FLOPs USED-FOR regularization. FLOPs USED-FOR it. layer - wise scaling factors USED-FOR gradients. hyper - gradient descent USED-FOR they. ImageNet EVALUATE-FOR method. method COMPARE state - of - the - art methods. state - of - the - art methods COMPARE method. CIFAR-10 EVALUATE-FOR method. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. Method is channel pruning methods. OtherScientificTerm are layers, and gates. ","This paper proposes a channel pruning method to improve the compression of convolutional neural networks (CNNs). The proposed method is based on a hyper-structure network, where the hyper-network is a combination of the architecture of the main network with a regular backpropagation. The main contribution of the paper is the regularization term, which is used to reduce the computational cost of the network. The paper shows that the proposed method achieves competitive results on CIFAR-10 and ImageNet. ","This paper proposes a new channel pruning method for compression of CNNs. The main idea is to use a hyper-structured hyper-structure network to reduce the computational complexity of the main network. The authors propose a new regularization term for the hyper- structure network, which is based on regular backpropagation. The proposed method is evaluated on CIFAR-10 and ImageNet datasets."
2393,SP:890fd9454596c051b0e9535baf73b1dd1fae67ca,"exploration mechanism USED-FOR theorem prover. imitation CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION imitation. It USED-FOR prover. imitation USED-FOR prover. reinforcement learning USED-FOR prover. Task are automated higher - order logic theorem proving, and exploration of premises. OtherScientificTerm are human proofs, deep reinforcement learning scenario, and DeepHOL Zero. Method is exploration approach. ",This paper studies the problem of learning a theorem prover that is capable of proving proofs of higher-order logic. The authors propose to use reinforcement learning to learn an exploration mechanism that allows the prover to explore new premises in the proof space. The exploration mechanism is based on imitation learning and reinforcement learning.    The main contributions of this paper are:  1. A novel exploration mechanism for theorem proving is proposed.  2. A deep reinforcement learning algorithm is proposed to learn the exploration mechanism.  3. The proposed method is evaluated on a set of synthetic and real-world problems. ,"This paper proposes an exploration mechanism for theorem prover for higher-order logic theorem proving. The exploration mechanism is based on imitation and reinforcement learning, where the prover learns to explore the premises of the proofs. The authors show that the exploration mechanism can be combined with reinforcement learning to improve the performance of theorem provers. They also show that this exploration mechanism leads to better performance than imitation."
2409,SP:88209417a8ad07e6103084e41709be900303ce5f,"models USED-FOR machine learning tasks. data augmentation USED-FOR models. augmentation methods USED-FOR data modality. image processing functions CONJUNCTION word - replacing rules. word - replacing rules CONJUNCTION image processing functions. word - replacing rules USED-FOR text data. image processing functions USED-FOR image data. MODALS USED-FOR automated data augmentation. universal data transformation operations USED-FOR transform. MODALS USED-FOR universal data transformation operations. latent space FEATURE-OF universal data transformation operations. Method are Data augmentation, and automated data augmentation approach. Material is artificial data. OtherScientificTerm are modality, and modalities. ","This paper proposes MODALS, a data augmentation method that can be applied to any data modality. The proposed method is based on the observation that data transformation operations are universal across modalities, and can be used to transform data in a latent space. The method is evaluated on image data and text data.   ","This paper proposes a new data augmentation method, MODALS, that can be applied to any data modality. The main idea is to use a set of universal data transformation operations (e.g. image processing functions, word-replacing rules, etc.) to augment the data modalities. The authors show that MODALS can be used to augment any modality in the latent space of the data, and that it can be combined with a variety of augmentation methods. The proposed method is evaluated on synthetic data and real data."
2425,SP:6d84670d321b0d584b097c630574bd748e85c9a2,"nonlinear and nontrivial dynamical limit FEATURE-OF learning dynamics. mean field limit HYPONYM-OF nonlinear and nontrivial dynamical limit. neural networks USED-FOR mean field regime. mean field limit USED-FOR large - width neural networks. analysis USED-FOR two - layer networks. mean field regime FEATURE-OF optimization efficiency. global convergence result FEATURE-OF unregularized feedforward three - layer networks. mean field regime FEATURE-OF unregularized feedforward three - layer networks. rigorous framework USED-FOR mean field limit. mean field limit FEATURE-OF three - layer networks. stochastic gradient descent training USED-FOR three - layer networks. stochastic gradient descent training USED-FOR mean field limit. probability space FEATURE-OF neural networks. neural networks PART-OF neuronal embedding. probability space PART-OF neuronal embedding. mean field limit USED-FOR global convergence guarantee. regularity CONJUNCTION convergence mode assumptions. convergence mode assumptions CONJUNCTION regularity. convergence mode assumptions FEATURE-OF global convergence guarantee. regularity USED-FOR global convergence guarantee. universal approximation property FEATURE-OF neural networks. algebraic topology argument USED-FOR universal approximation property. algebraic topology argument USED-FOR neural networks. OtherScientificTerm are global convergence guarantees, multilayer ones, and convexity. ","This paper studies the mean field limit of neural networks, which is a nonlinear and nontrivial dynamical limit of the learning dynamics. The authors prove a global convergence result for unregularized feedforward three-layer networks with a mean-field limit on the probability space of the neurons in the network. They show that the global convergence is guaranteed under the regularity assumption and convergence mode assumptions. They also show that this result holds for large-width neural networks.","This paper studies the mean field limit of neural networks, which is a nonlinear and nontrivial dynamical limit of learning dynamics. The authors prove a global convergence result for unregularized feedforward feedforward three-layer networks with mean field regime. They show that the global convergence is guaranteed under the regularity assumption and convergence mode assumptions. They also provide an algebraic topology argument to prove the universal approximation property of universal approximation. "
2441,SP:b90f893f927db9c439595fd119a565cf43c971f4,"interpretable parameterizations USED-FOR real - world decision - making. interpretable parameterizations USED-FOR introspecting and auditing policies. counterfactual reasoning USED-FOR batch inverse reinforcement learning. counterfactual reasoning USED-FOR costbenefit tradeoffs. reward functions USED-FOR expert behavior. counterfactuals USED-FOR policy evaluation. batch setting FEATURE-OF policy evaluation. real and simulated medical environments EVALUATE-FOR batch, counterfactual inverse reinforcement learning approach. OtherScientificTerm are unknown reward function, reward function, expert ’s actions, and expert policies. Task are learning explanations of expert decisions, and active experimentation. Material is healthcare. ","This paper proposes to use counterfactual reasoning to learn explanations of expert decisions in a batch-inverse reinforcement learning (BIRL) setting. In this setting, the reward function is unknown and the expert policy is assumed to be independent of the expert's actions. The authors propose to learn counterfactually the cost-benefit tradeoffs between expert and policy in the batch setting. They show that the proposed method outperforms baselines in both simulated and real-world medical environments.","This paper proposes a method for learning counterfactual explanations of expert decisions in the context of batch inverse reinforcement learning (BIRL). The authors propose a method to learn explanations of the expert's actions in a batch setting, where the reward function is unknown and the expert’s actions are unknown. They show that the proposed method outperforms the state-of-the-art in both real-world and simulated medical environments."
2457,SP:c92916780418bfa7f0796fd9766b6d28b9eea5ef,"generalisation CONJUNCTION data efficiency. data efficiency CONJUNCTION generalisation. data efficiency CONJUNCTION robustness. robustness CONJUNCTION data efficiency. Multitask Reinforcement Learning USED-FOR models. generalisation EVALUATE-FOR models. they USED-FOR graphs. physical morphology USED-FOR graph. edges USED-FOR nodes. morphological information PART-OF graph. AMORPHEUS HYPONYM-OF transformer - based approach. graph structure USED-FOR GNNs. it COMPARE GNN - based methods. GNN - based methods COMPARE it. morphological information USED-FOR message - passing scheme. GNN - based methods USED-FOR message - passing scheme. AMORPHEUS COMPARE it. it COMPARE AMORPHEUS. AMORPHEUS USED-FOR morphological information. morphological information USED-FOR GNN - based methods. OtherScientificTerm are state and action space dimensions, and limb features. Method is Graph Neural Networks ( GNN ). Generic are They, and methods. Task are graph - based continuous control, and message passing. ","This paper proposes a transformer-based approach for continuous control with graph neural networks (GNNs). The proposed method is based on the idea that the state and action space dimensions can be represented as a set of nodes and edges, and the edges are used to represent the nodes in the state space. The authors show that the proposed method, called AMORPHEUS, outperforms existing GNN-based methods on the task of continuous control. ","This paper proposes a transformer-based approach for continuous control with graph neural networks (GNNs). The main idea is to use morphological information in the graph structure to improve the generalization performance of GNNs. The proposed method, AMORPHEUS, is based on the idea of using the physical morphology of the graph as a message passing mechanism. The main contribution of the paper is that the proposed method is able to leverage the information from the physical graph structure of the GNN to improve generalization and data efficiency. "
2473,SP:2cf58f5cac20dccdc2034ef60e8e46b7988ebd7d,"modulated convolutions USED-FOR alternative. forward - pass USED-FOR inference. forward - pass USED-FOR MoVie. MoVie USED-FOR counting. module USED-FOR number ’ related questions. number ’ related questions PART-OF generic VQA models. module PART-OF generic VQA models. COCO HYPONYM-OF common object counting. module USED-FOR VQA challenge. counting - specific VQA tasks EVALUATE-FOR MoVie. common object counting HYPONYM-OF benchmarks. benchmarks EVALUATE-FOR prior - art. COCO HYPONYM-OF benchmarks. modulated convolutions USED-FOR reasoning tasks. counting HYPONYM-OF reasoning tasks. MoVie HYPONYM-OF modulated convolutions. Task is visual counting. Material is natural image. Method are explicit, symbolic models, residual bottleneck, and Modulated conVolutional bottlenecks. ","This paper proposes a method for visual counting based on modulated convolutional networks. The method is based on the idea of residual bottlenecks in symbolic models, where a residual bottleneck is used to constrain the forward pass of the convolution. The proposed method is evaluated on COCO and VQA tasks and achieves state-of-the-art results.","This paper proposes a method to solve the counting-specific VQA problem. The method is based on modulated convolutional convolutions (MoVie), which is an extension of the popular MoVie model. The key idea is to use a forward-pass to compute the number of objects in an image, and then use the forward pass to estimate the total number of items in the image. The forward pass is then used as a forward pass of the convolution. The proposed method is evaluated on COCO, a common object counting task, and on a variety of visual counting tasks."
2489,SP:c64e77507e562f236cb69361b22fb1a7951ffb22,poisoning attack USED-FOR model. online convex optimization USED-FOR poisoning attack. model - targeted poisoning attacks COMPARE attack. attack COMPARE model - targeted poisoning attacks. provable convergence FEATURE-OF target classifier. provable convergence FEATURE-OF attack. attack HYPONYM-OF model - targeted poisoning attack. it COMPARE attacks. attacks COMPARE it. attack success rate EVALUATE-FOR it. attack success rate EVALUATE-FOR attacks. attack USED-FOR online attack. Method is classifier. ,"This paper studies the problem of online poisoning attacks, where the target classifier is poisoned by an online convex optimization algorithm. The authors propose an online poisoning attack that can be used to attack the classifier in an online manner. They show that the attack is provable to converge to the true classifier with provable convergence. They also show that it is provably more effective than the existing model-targeted poisoning attacks.",This paper proposes a new model-targeted poisoning attack for online convex optimization. The main idea is to use an online attack to attack the target classifier with provable convergence of the classifier. The authors show that the attack is provable to converge to the true classifier in the online setting. They also provide a theoretical analysis of the convergence rate of the attack.
2505,SP:a526023ec4cb839b83c574d31f59a9a67bc7af00,"BiPointNet HYPONYM-OF model binarization approach. model binarization approach USED-FOR deep learning on point clouds. resource constraint USED-FOR real - time point cloud applications. edge devices USED-FOR real - time point cloud applications. binarized models USED-FOR point clouds. scale distortion USED-FOR optimization. scale distortion HYPONYM-OF challenges. aggregation - induced feature homogenization HYPONYM-OF challenges. Entropy - Maximizing Aggregation ( EMA ) USED-FOR distribution. Layer - wise Scale Recovery ( LSR ) USED-FOR feature representation capacity. Entropy - Maximizing Aggregation ( EMA ) USED-FOR BiPointNet. Layer - wise Scale Recovery ( LSR ) PART-OF BiPointNet. BiPointNet COMPARE full precision counterpart. full precision counterpart COMPARE BiPointNet. binarization methods COMPARE full precision counterpart. full precision counterpart COMPARE binarization methods. BiPointNet COMPARE binarization methods. binarization methods COMPARE BiPointNet. tasks EVALUATE-FOR techniques. speedup CONJUNCTION storage saving. storage saving CONJUNCTION speedup. storage saving EVALUATE-FOR BiPointNet. real - world resource - constrained devices EVALUATE-FOR BiPointNet. speedup EVALUATE-FOR BiPointNet. Metric are information entropy, and maximum information entropy. OtherScientificTerm is scale - sensitive structures. ","This paper proposes a novel method to improve the performance of deep learning models on point clouds. The proposed method is based on the idea of aggregation-induced feature homogenization, which aims to reduce the scale distortion in the feature representation. The authors propose a new method called Entropy-Maximizing Aggregation (EMA) to maximize the entropy of the distribution of the points in the point cloud. They also propose a layer-wise scale recovery (LSR) method to recover the scale-sensitive structures. The experimental results show that the proposed method outperforms the state-of-the-art methods on several real-time point cloud applications. ","This paper proposes a new model binarization method for real-time point cloud applications. The proposed method is based on the Entropy-Maximizing Aggregation (EMA) method, which aims to maximize the information entropy of the distribution. The authors also propose a new layer-wise scale recovery (LSR) method to recover the scale-sensitive structures of the feature representation capacity. Experimental results show that the proposed method outperforms the state-of-the-art in terms of speedup and storage saving. "
2521,SP:825b4d1db0c537a607655bb5b4bf221ec672c8af,"natural language processing tasks EVALUATE-FOR Transformer - based models. self - attention architecture USED-FOR transformer. transformer USED-FOR context - aware representations. trainable memory USED-FOR Transformer model. Memory - augmented neural networks ( MANNs ) USED-FOR representations. general - purpose memory USED-FOR representations. neural architectures USED-FOR representations. neural architectures USED-FOR Memory - augmented neural networks ( MANNs ). general - purpose memory USED-FOR neural architectures. MANNs USED-FOR algorithms. MANNs USED-FOR tasks. question answering CONJUNCTION language modeling. language modeling CONJUNCTION question answering. backpropagation USED-FOR tasks. RNNs CONJUNCTION LSTMs. LSTMs CONJUNCTION RNNs. complexity EVALUATE-FOR RNNs. language modeling HYPONYM-OF tasks. backpropagation USED-FOR MANNs. question answering HYPONYM-OF tasks. complexity EVALUATE-FOR LSTMs. Copy HYPONYM-OF algorithms. memory tokens USED-FOR non - local representations. memory bottleneck USED-FOR global information. dedicated layer USED-FOR memory update. machine translation and language modelling tasks EVALUATE-FOR model. GLUE benchmark EVALUATE-FOR tasks. memory tokens USED-FOR masked language model. it USED-FOR global context. it USED-FOR model. model USED-FOR global context. Method are element - wise representations, Transformer baseline, and memory augmented Transformers. OtherScientificTerm is memory. ",This paper proposes Memory-augmented Neural Networks (MANNs) to improve the performance of Transformer-based models on natural language processing tasks. The proposed method is based on the idea of using memory to augment the self-attention layer of a Transformer model. The authors show that the proposed method can achieve better performance than the state-of-the-art on the GLUE benchmark.  ,"This paper proposes a memory-augmented neural network (MANN) architecture to improve the performance of Transformer-based models on natural language processing tasks. The main idea is to use a self-attention layer to augment the memory of the Transformer model with a global context-aware representation. The proposed method is evaluated on a variety of tasks, including question answering, machine translation, and language modeling."
2537,SP:f0fa1b7684bc605f6edd4813c44be20988fe8b4c,Prototypical Contrastive Learning ( PCL ) HYPONYM-OF unsupervised representation learning method. contrastive learning CONJUNCTION clustering. clustering CONJUNCTION contrastive learning. it USED-FOR semantic structures. PCL USED-FOR instance discrimination. PCL USED-FOR low - level features. low - level features USED-FOR instance discrimination. PCL USED-FOR semantic structures. embedding space FEATURE-OF semantic structures. clustering USED-FOR semantic structures. prototypes USED-FOR maximum - likelihood estimation. maximum - likelihood estimation USED-FOR network parameters. prototypes USED-FOR latent variables. Expectation - Maximization framework USED-FOR maximum - likelihood estimation. E - step CONJUNCTION M - step. M - step CONJUNCTION E - step. distribution of prototypes USED-FOR E - step. clustering USED-FOR distribution of prototypes. contrastive learning USED-FOR network. InfoNCE loss USED-FOR contrastive learning. ProtoNCE loss HYPONYM-OF InfoNCE loss. PCL COMPARE instance - wise contrastive learning methods. instance - wise contrastive learning methods COMPARE PCL. low - resource transfer learning EVALUATE-FOR PCL. low - resource transfer learning EVALUATE-FOR instance - wise contrastive learning methods. Task is maximum - likelihood estimation of the network parameters. ,"This paper proposes Prototype Contrastive Learning (PCL), an unsupervised representation learning method that combines contrastive learning and clustering to improve the performance of instance discrimination. The proposed method is based on an Expectation-Maximization framework to estimate the maximum likelihood estimation of the network parameters. The authors propose to use prototypes as low-level features for instance discrimination and use the InfoNCE loss to learn the distribution of prototypes in the embedding space. Experiments show that the proposed method outperforms other contrastive methods in the low-resource transfer learning setting.","This paper proposes a novel contrastive learning method, Prototype Contrastive Contrastive Learning (PCL), which uses prototypes as low-level features for instance discrimination. The proposed method is based on the Expectation-Maximization framework (E-step, M-step and E-step-M-step) to estimate the maximum likelihood of the network parameters. The authors propose a new InfoNCE loss, ProtoNCE, to improve the performance of PCL. Experiments show that PCL outperforms the state-of-the-art in low-resource transfer learning. "
2553,SP:5342a5e1d87fd17b1a2efed967dbbfeafa440ee7,clean images USED-FOR deep neural networks. invisible perturbations FEATURE-OF clean images. block USED-FOR robust features. block USED-FOR adversarial attacks. Orthogonal Multi - Path ( OMP ) block PART-OF neural network. forward learning CONJUNCTION backward correction. backward correction CONJUNCTION forward learning. OMP block USED-FOR neural networks. forward learning USED-FOR OMP block. backward correction USED-FOR OMP block. neural networks USED-FOR features. OMP block USED-FOR features. robustness EVALUATE-FOR neural networks. variety CONJUNCTION accuracy. accuracy CONJUNCTION variety. accuracy EVALUATE-FOR vanilla neural networks. l∞ bound FEATURE-OF white - box PGD attack. accuracy EVALUATE-FOR VGG16. CIFAR10 EVALUATE-FOR vanilla neural networks. OMP block USED-FOR VGG16. OMP block USED-FOR neural networks. accuracy EVALUATE-FOR neural networks. neural networks USED-FOR black - box attacks. white - box and black - box attacks COMPARE adversarial defenders. adversarial defenders COMPARE white - box and black - box attacks. Generic is paths. OtherScientificTerm is orthogonality constraint. ,This paper proposes an orthogonal multi-path (OMP) block to improve the robustness of deep neural networks against black-box adversarial attacks. The proposed OMP block is composed of two components: forward learning and backward correction. The backward correction is used to update the weights of the forward layer with respect to the input image. The forward layer is updated with the backward correction to ensure that the weights are close to each other. The authors show that the proposed method improves the accuracy and variety of adversarial perturbations on CIFAR-10.,"This paper proposes a novel orthogonal multi-path (OMP) block to improve the robustness of neural networks against black-box adversarial attacks. The OMP block consists of two parts: forward learning and backward correction. The forward learning part is based on the orthogonality constraint, while the backward correction part is a combination of forward and backward learning. The authors show that OMP blocks can be used to improve robustness against white-box attacks. They also provide a l∞ bound on the accuracy of white- box attacks."
2569,SP:776df66274ed12449fde8dcef873a593980f397c,Attention mechanism USED-FOR graph neural networks. graph attention model USED-FOR noisy graphs. self - supervised task USED-FOR edges. attention forms USED-FOR edges. attention forms USED-FOR self - supervised task. expressive attention USED-FOR mislinked neighbors. SuperGAT USED-FOR expressive attention. edges USED-FOR SuperGAT. homophily CONJUNCTION average degree. average degree CONJUNCTION homophily. attention forms CONJUNCTION self - supervision. self - supervision CONJUNCTION attention forms. graph characteristics USED-FOR attention forms. self - supervision HYPONYM-OF graph characteristics. average degree HYPONYM-OF self - supervision. homophily HYPONYM-OF self - supervision. average degree HYPONYM-OF graph characteristics. homophily HYPONYM-OF graph characteristics. recipe USED-FOR attention design. graph characteristics USED-FOR attention design. models COMPARE baselines. baselines COMPARE models. real - world datasets EVALUATE-FOR recipe. recipe USED-FOR models. Method is graph attention. Material is graphs. ,"This paper proposes a self-supervised approach to improve the performance of GNNs on noisy graphs. The proposed method is based on the observation that existing methods such as SuperGAT are limited in their expressive power due to the noisy nature of the graph. To address this issue, the authors propose to use self-substituting the edges of a noisy graph with a set of edges that are more likely to be mislinked with each other in the original graph.    The proposed approach is to use the self-similarity and homophily properties of a graph to select the edges with the highest average degree. The authors show that the proposed method outperforms existing methods in terms of performance on a variety of datasets. ","This paper proposes a new approach to improve the expressive graph attention mechanism of graph neural networks (GNNs). In particular, the authors propose a recipe for expressive attention based on graph characteristics such as homophily, average degree, self-supervision, and self-attention. The proposed approach is evaluated on a variety of real-world datasets and compared with several baselines."
2585,SP:80a05296d6b1e4c6e9e2df01938c73029ff8487d,Dialogue system USED-FOR medical automatic diagnosis ( DSMAD ). Dialogue system USED-FOR agent. reinforcement learning methods USED-FOR it. Markov decisionmaking process USED-FOR DSMAD. medical rationality FEATURE-OF inquiring process. diagnostic accuracy EVALUATE-FOR DSMAD agents. agent USED-FOR diagnosis. agent USED-FOR diagnosing processes. agent USED-FOR medical application. inquiry module CONJUNCTION introspective module. introspective module CONJUNCTION inquiry module. inquiry module USED-FOR symptom - inquiries. cooperative modules PART-OF DSMAD agent. introspective module PART-OF DSMAD agent. introspective module HYPONYM-OF cooperative modules. inquiry module HYPONYM-OF cooperative modules. inquiry module CONJUNCTION introspective module. introspective module CONJUNCTION inquiry module. evaluation metrics EVALUATE-FOR DSMAD methods. reliability EVALUATE-FOR DSMAD methods. evaluation metrics EVALUATE-FOR reliability. INS - DS COMPARE methods. methods COMPARE INS - DS. robustness EVALUATE-FOR methods. reliability EVALUATE-FOR methods. reliability CONJUNCTION robustness. robustness CONJUNCTION reliability. robustness EVALUATE-FOR INS - DS. reliability EVALUATE-FOR INS - DS. OtherScientificTerm is inquiring symptoms. Generic is interventions. ,"This paper proposes a method for medical automatic diagnosis (DSMAD) based on a dialogue system. The proposed method is based on two modules: an inquiry module and an introspective module. In the inquiry module, the agent is given a set of symptoms and is asked to decide whether it is necessary to intervene or not to intervene based on the current state of the patient. The introspection module is used to collect information about the patient's history. The authors show that the proposed method achieves state-of-the-art results in terms of diagnostic accuracy and robustness.","This paper proposes a new method for medical automatic diagnosis (DSMAD) with a dialogue system. DSMAD is a medical system where a patient is asked a series of questions about their symptoms, and an agent is trained to answer the questions. The proposed method is based on a Markov decision-making (MDP) process, where the agent is given a set of questions to answer, and is trained using reinforcement learning. The authors show that the proposed method outperforms the state-of-the-art methods in terms of diagnostic accuracy and robustness. "
2601,SP:10ae09d90d465125433a9b4f15b1405ab017920d,"adaptive batch - wise regularization USED-FOR natural world distribution. Batch Confusion Norm ( BCN ) USED-FOR natural world distribution. fine - grained and long - tailed properties PART-OF natural world distribution. Batch Confusion Norm ( BCN ) USED-FOR adaptive batch - wise regularization. inter - class similarity CONJUNCTION intra - class variations. intra - class variations CONJUNCTION inter - class similarity. task USED-FOR FGVC classifier. attention mechanism USED-FOR discriminative parts. long - tailed distribution of visual classification USED-FOR class imbalance problem. class - balancing strategies CONJUNCTION classifier normalization. classifier normalization CONJUNCTION class - balancing strategies. classifier normalization CONJUNCTION negative gradient of tailed categories. negative gradient of tailed categories CONJUNCTION classifier normalization. adaptive confusion concept USED-FOR problems. BCN term USED-FOR overfitting. network learning USED-FOR cross - entropy loss. class predictions USED-FOR BCN loss. confusion energy - based framework USED-FOR long - tailed scenario. BCN USED-FOR distribution of confusion strength. BCN USED-FOR confusion energy - based framework. extra attention mechanism USED-FOR FGVC model. BCN technique USED-FOR FGVC model. iNaturalist2018 HYPONYM-OF natural world distribution dataset. iNaturalist2018 EVALUATE-FOR approach. natural world distribution dataset EVALUATE-FOR approach. Generic is approaches. OtherScientificTerm are image features of fine details, and tailed and head categories. Material is FGVC datasets. ","This paper proposes an adaptive batch-wise regularization method for long-tailed visual classification (FGVC) problems. The proposed method is based on the Batch Confusion Norm (BCN) concept, which is used to regularize the distribution of the confusion strength between the tailed and head categories. The BCN term is designed to mitigate the overfitting issue caused by the class imbalance problem. The method is evaluated on the iNaturalist 2018 dataset and achieves state-of-the-art performance. ",This paper proposes a novel BCN-based method for long-tailed visual classification (FGVC) that can be applied to the problem of class imbalance in natural world distribution. The main idea is to adapt the BCN term to the long-tail distribution and use it as a regularization term to improve the performance of the FGVC classifier. The proposed method is evaluated on the iNaturalist2018 dataset and compared with a number of existing methods.
2617,SP:90f1e0fe1e9678d1e9a4dcb519d4e8fd61098ce0,"Bayesian inference USED-FOR ill - posed nature. ill - posed nature FEATURE-OF inverse reinforcement learning problem. Bayesian inference USED-FOR inverse reinforcement learning problem. reward USED-FOR Bayesian inference. methods COMPARE small tabular setting. small tabular setting COMPARE methods. variational approach USED-FOR latent reward. reward FEATURE-OF approximate posterior distribution. variational approach USED-FOR policy. method USED-FOR Bayesian reward inference. real medical data CONJUNCTION control simulations. control simulations CONJUNCTION real medical data. real medical data USED-FOR method. methods USED-FOR Bayesian reward inference. Method are inner - loop MDP solver, non - Bayesian methods, Approximate Variational Reward Imitation Learning ( AVRIL ), and offline imitation learning algorithms. Task is healthcare. ","This paper proposes a new method for Bayesian reward inference in inverse reinforcement learning. The proposed method is based on a variational approach to estimate the approximate posterior distribution of the reward, which is then used to learn an imitation learning algorithm. The method is evaluated on simulated and real-world medical data. ",This paper proposes a new method for Bayesian reward inference for inverse reinforcement learning (IRL). The main idea is to use an inner-loop MDP solver to solve the problem of estimating the posterior distribution of the latent reward of a policy. The method is based on a variational approach to estimate the posterior of the reward. The authors show that the proposed method outperforms the state-of-the-art methods for IRL in a small tabular setting. 
2633,SP:ccd251d95c0a2d8dc5ad2a148ec29955e105e71e,Search USED-FOR policies. singleand multiagent environments FEATURE-OF policies. prior search approaches USED-FOR partially observable environments. computational cost EVALUATE-FOR hidden information. search procedure USED-FOR partially observable environments. Learned Belief Search ( LBS ) HYPONYM-OF search procedure. supervised task USED-FOR approximate auto - regressive counterfactual belief. supervised task USED-FOR LBS. approximate auto - regressive counterfactual belief USED-FOR LBS. public - private model architecture USED-FOR policies. LBS USED-FOR policies. LBS USED-FOR multi - agent settings. rollouts FEATURE-OF policies. public - private model architecture USED-FOR LBS. Hanabi EVALUATE-FOR LBS. exact search USED-FOR LBS. compute requirements EVALUATE-FOR LBS. OtherScientificTerm is exact belief distribution. Generic is it. Method is search methods. ,"This paper proposes a method for learning counterfactual belief distributions in partially observable environments. The method is based on the idea of learning a model of the belief distribution that can be used to estimate the true belief distribution. The model is trained using a supervised learning task, where the model is used to predict the true distribution of the true and false distribution, and then the policy is trained on this model using a public-private model architecture. The proposed method is evaluated on Hanabi and Hanabi-C, where it is shown to outperform the baselines. ","This paper proposes a method for learning the counterfactual belief of a policy in a partially observable environment. The proposed method is based on the idea of learning the approximate auto-regressive counterfactually belief of the policy. The method is applied to both single-agent and multi-agent settings, and is evaluated on Hanabi and Hanabi."
2649,SP:db408e6bfe69a9b3984f3b27ca92b802aa37af42,"MCTS CONJUNCTION random shooting. random shooting CONJUNCTION MCTS. It USED-FOR bias - variance trade - off. Task is Planning in large state spaces. Method are Shoot Tree Search ( STS ), TD(n ), and STS. Generic is algorithm. OtherScientificTerm is tree search context. ","This paper studies the problem of planning in large state spaces in a tree search setting. The authors propose a new algorithm called Shoot Tree Search (STS), which is a variant of MCTS. The main contribution of the paper is a theoretical analysis of the bias-variance trade-off of STS and random shooting in the context of TD(n). The main result shows that STS is equivalent to random shooting with TD(N) in terms of bias and variance.  ",This paper studies the problem of planning in large state spaces in the context of Shoot Tree Search (STS). The main contribution of the paper is to study the bias-variance trade-off of STS and TD(n) in the setting of MCTS and random shooting. The main result is that STS can be viewed as a variant of TD(N) with a bias-variance tradeoff. The paper also provides a theoretical analysis of the bias variance trade-offs.  
2665,SP:5efc271ccc555fd9aa542548838170bd4c98e957,"transformer networks USED-FOR inductive bias. inductive bias PART-OF neural architectures. transformer networks USED-FOR tasks. tasks USED-FOR inductive bias. datasets USED-FOR inductive bias. induction CONJUNCTION abduction. abduction CONJUNCTION induction. deduction CONJUNCTION induction. induction CONJUNCTION deduction. model USED-FOR synthetic tasks. tasks USED-FOR reasoning biases. Inductive bias USED-FOR Mathematical rEasoning. LIME HYPONYM-OF pre - training methodology. Models COMPARE vanilla transformers. vanilla transformers COMPARE Models. LIME USED-FOR Models. large mathematical reasoning benchmarks EVALUATE-FOR vanilla transformers. large mathematical reasoning benchmarks EVALUATE-FOR Models. computation cost EVALUATE-FOR pre - training approaches. computation cost FEATURE-OF downstream task. pre - training approaches USED-FOR LIME. downstream task USED-FOR LIME. computation cost USED-FOR LIME. Method is architecture engineering. OtherScientificTerm are reasoning primitives, and mathematical knowledge. Generic is they. ","This paper proposes a pre-training method for inductive bias in neural networks for mathematical reasoning tasks. The main idea is to pre-train a transformer network on a set of synthetic tasks and then use it to train a model for a downstream task, where the downstream task is a combination of the synthetic task and the original task. The authors show that the pre-trained model is able to outperform the state-of-the-art transformer models on a variety of mathematical reasoning benchmarks. ","This paper proposes a method for pre-training neural networks with inductive bias in order to improve their performance on mathematical reasoning tasks. The main idea is to train a transformer network on a set of synthetic tasks and then use the model to perform reasoning tasks on the synthetic datasets. The model is trained using a pre-trained model that is trained on a subset of the synthetic tasks, and then the model is used to solve the downstream task. The authors show that the model performs better than vanilla transformer networks on a variety of synthetic and real-world datasets."
2681,SP:bb8e0b554d3b3314fa343c902d9e60f1a141ea30,"gradient descent USED-FOR weight normalized smooth homogeneous neural nets. inductive bias FEATURE-OF weight normalized smooth homogeneous neural nets. inductive bias FEATURE-OF gradient descent. gradient flow USED-FOR networks. gradient flow path COMPARE gradient flow. gradient flow COMPARE gradient flow path. EWN USED-FOR gradient flow path. adaptive learning rate USED-FOR gradient flow. adaptive learning rate USED-FOR gradient descent. weight normalization ( SWN ) CONJUNCTION unnormalized architectures. unnormalized architectures CONJUNCTION weight normalization ( SWN ). inductive bias CONJUNCTION unnormalized architectures. unnormalized architectures CONJUNCTION inductive bias. inductive bias FEATURE-OF weight normalization ( SWN ). synthetic data sets EVALUATE-FOR unnormalized architectures. simple data sets CONJUNCTION architectures. architectures CONJUNCTION simple data sets. architectures EVALUATE-FOR sparse EWN solutions. SGD USED-FOR sparse EWN solutions. OtherScientificTerm are exponential or cross - entropy loss, radial direction, and asymptotic relative sparsity. Method is exponential weight normalization ( EWN ). Metric is asymptotic convergence rate. Task is learning prunable neural networks. ",This paper studies the gradient descent in weight normalized smooth homogeneous neural networks with weight normalization (SWN) and unnormalized architectures. The authors show that the gradient flow path of gradient descent can be viewed as an adaptive learning rate for gradient descent with a fixed learning rate. They also show that gradient flow with SWN is more efficient than gradient flow without SWN. The main contribution of this paper is to prove that gradient descent is efficient with respect to gradient flow in the radial direction.  ,"This paper studies the asymptotic convergence of weight normalized smooth homogeneous neural networks (SWN) with exponential weight normalization (EWN) and unnormalized architectures (unnormalized SWN). The main contribution of the paper is a theoretical analysis of the convergence rate of EWN with respect to the gradient flow path. The main result is that EWN converges faster than gradient flow in the radial direction, which is a result of the exponential or cross-entropy loss. The authors also show that the convergence of the EWN can be improved by using the adaptive learning rate for gradient descent."
2697,SP:c71f9d2a602516865a0b103028186e83b52e5f00,Generative Adversarial Networks ( GANs ) HYPONYM-OF generative models. mode collapse FEATURE-OF generator. Catastrophic Forgetting PART-OF continual learning. classification accuracy EVALUATE-FOR discriminator. training procedure USED-FOR discriminators. training scheme USED-FOR mode collapse. metrics EVALUATE-FOR GAN evaluation. GAN frameworks USED-FOR mode collapse. training scheme USED-FOR GAN frameworks. metrics EVALUATE-FOR training scheme. Generic is they. Task is mode collapse problem. Method is data generation procedure. ,This paper studies the catastrophic forgetting problem in GANs. The authors propose to use Catastrophic Forgetting as a regularization term to prevent mode collapse in the discriminator. They show that the mode collapse problem can be solved by training a discriminator with catastrophic forgetting. They also propose a training scheme to mitigate mode collapse.   ,"This paper studies the problem of mode collapse in generative adversarial networks (GANs). The authors propose a new training scheme for GANs that aims to mitigate the catastrophic forgetting problem. The training scheme is based on the notion of catastrophic forgetting, which is defined as the mode collapse that occurs when the discriminator discriminator fails to learn a discriminator. The authors show that the proposed training scheme can be used to improve the performance of the discriminators. The proposed training procedure is evaluated on a variety of metrics, and it is shown that it improves the performance on a number of metrics. "
2713,SP:52c48198c95826e042f9e5a512ef3265daaff882,"proxy score USED-FOR head importance. proxy score USED-FOR attention heads. AUBER HYPONYM-OF regularization method. attention heads PART-OF BERT. reinforcement learning USED-FOR regularization method. heuristics CONJUNCTION rule - based policies. rule - based policies CONJUNCTION heuristics. pruning policy USED-FOR attention heads. AUBER USED-FOR pruning policy. rule - based policies USED-FOR AUBER. heuristics USED-FOR AUBER. AUBER COMPARE pruning methods. pruning methods COMPARE AUBER. accuracy EVALUATE-FOR pruning methods. accuracy EVALUATE-FOR AUBER. Generic are it, and they. Method is heuristic - based methods. OtherScientificTerm is regularization. ",This paper proposes a new regularization method for attention heads in BERT. The proposed method is based on a proxy score for the importance of each head in the attention hierarchy. The proxy score is computed using a combination of reinforcement learning and a rule-based pruning policy. The authors show that the proposed method outperforms existing heuristic-based methods and RL-based policies in terms of accuracy.,"This paper proposes a new regularization method for BERT, called AUBER, that prunes the attention heads in BERT with reinforcement learning. The main idea is to use a proxy score to measure the importance of attention heads, and then use a pruning policy to prune attention heads. The proxy score is based on a combination of heuristic-based heuristics and rule-based policies. Experiments show that the proposed method outperforms the state-of-the-art in terms of accuracy. "
2729,SP:abcbbad146f1b0d5d579c215952c95e5499a378a,"physics parameters CONJUNCTION morphology. morphology CONJUNCTION physics parameters. representation CONJUNCTION physics parameters. physics parameters CONJUNCTION representation. unpaired and randomly collected data USED-FOR correspondences. dynamics cycles USED-FOR dynamic robot behavior. cycle - consistency constraint USED-FOR dynamics cycles. simulation CONJUNCTION real robot. real robot CONJUNCTION simulation. real robot HYPONYM-OF problem domains. simulation HYPONYM-OF problem domains. dynamic state - action trajectories FEATURE-OF simulated arm. framework USED-FOR uncalibrated monocular video. framework USED-FOR dynamic state - action trajectories. real robot arm FEATURE-OF uncalibrated monocular video. Task are robotics problems, imitation learning, sim - to - real, and transfer learning. OtherScientificTerm are physics simulators, and robotics environments. Generic are correspondence, and policy. Method is fine - tuning. Material is paired data. ",This paper proposes a method for learning from unpaired and randomly collected data. The method is based on a cycle-consistency constraint on the dynamics cycles of the robot dynamics. The authors show that the learned dynamics correspond to the dynamics of a real robot in a simulation and a real robotic arm. They show that their method is able to learn a policy that can transfer from the simulation to the real world.,"This paper proposes a new method for simulating the dynamics of a robot in a robotic environment. The method is based on the notion of correspondence, which is the correspondence between a robot’s state-action trajectories and the physics parameters of the environment. It is motivated by the observation that simulators can be used to learn a policy that can be applied to real-world robotic environments. The authors propose a new policy that learns a policy for sim-to-real transfer learning, which can be fine-tuned using the data from the simulator. They show that the policy can be learned by simulating a simulation of a real robot and a robot arm in an uncalibrated monocular video environment. They also show that their method can learn the dynamics cycles of a simulated robot."
2745,SP:006434d56992836ab9420d7d4215bc70664de304,"Explainability USED-FOR model development. Explainability USED-FOR AI. solutions USED-FOR Shapley explainability. data manifold USED-FOR solutions. data manifold USED-FOR Shapley explainability. Shapley value - function USED-FOR other. generative modelling USED-FOR solution. unintelligible explanations FEATURE-OF higher - dimensional data. OtherScientificTerm are operational nuance, model ’s features, off - manifold ” Shapley values, implicit model dependence, and sensitive attributes. Method are Shapley framework, and on - manifold explainability. Task is data imputations. ","This paper studies the problem of explainability, i.e., the ability of a model to explain a given set of observations. The authors propose to use the Shapley framework to model the on-manifold explainability of a given data manifold. The main idea is to define a Shapley value function on the data manifold, and then use a generative model to generate a solution to this function, which is then used to explain the observations on the manifold.    The authors show that the proposed method is able to recover the true Shapley values of the observed data manifold from the data. ","This paper studies the problem of explainability in the context of data imputations. The authors propose a new Shapley framework, which is based on the notion of ""on-manifold explainability"" (i.e., explainability of the data manifold). The main idea is to use the Shapley value-function as a surrogate for the true Shapley values, and then use generative modelling to find a solution to the problem. The main contribution of the paper is to provide a theoretical analysis of the problem, and to show that the problem can be formulated as a problem of ""implicit model dependence"". "
2761,SP:7cda6bccf08887c7cef66d0ac3ccefdea8f5d7c8,Existing methods USED-FOR opponent modelling. local observations USED-FOR Existing methods. chosen actions CONJUNCTION received rewards. received rewards CONJUNCTION chosen actions. observed world state CONJUNCTION chosen actions. chosen actions CONJUNCTION observed world state. variational autoencoders USED-FOR local actions. local observations USED-FOR embeddings. observed world state HYPONYM-OF local observations. chosen actions HYPONYM-OF local observations. variational autoencoders USED-FOR modelling technique. embeddings USED-FOR modelling agent ’s decision policy. deep reinforcement learning USED-FOR embeddings. deep reinforcement learning USED-FOR modelling agent ’s decision policy. method COMPARE baseline method. baseline method COMPARE method. method COMPARE baseline. baseline COMPARE method. baseline COMPARE baseline method. baseline method COMPARE baseline. opponent ’s information USED-FOR baseline. embeddings USED-FOR baseline method. Generic is policy. OtherScientificTerm is opponent observations. ,"This paper proposes a new method for opponent modelling in reinforcement learning. The proposed method is based on variational autoencoders, where the learned embeddings are used to model the agent’s decision policy. The method is evaluated on a set of synthetic and real-world experiments.   ","This paper proposes a new method for opponent modelling based on variational autoencoder (VAE) and deep reinforcement learning (DRL). The proposed method is based on the idea that the opponent can be represented as a set of embeddings, where each embedding represents a state, action, and state-action pair, and the state is represented as an embedding of the observed world state. The embedding is then used to model the agent’s decision policy, which is learned using DRL. The method is evaluated on a variety of benchmark datasets, and compared with a baseline method. "
2777,SP:c239bc531bcf7293032748af29a1b786e9d893dd,"Contrastive learning USED-FOR unsupervised visual representation learning. consistency regularization term PART-OF contrastive learning framework. consistency regularization USED-FOR semi - supervised learning. Consistent Contrast ( CO2 ) USED-FOR contrastive learning framework. unlabeled data USED-FOR consistency regularization. unlabeled data USED-FOR semi - supervised learning. consistency regularization term PART-OF Consistent Contrast ( CO2 ). CO2 USED-FOR Momentum Contrast ( MoCo ). top-1 accuracy EVALUATE-FOR Momentum Contrast ( MoCo ). ImageNet linear protocol EVALUATE-FOR CO2. top-1 accuracy EVALUATE-FOR CO2. image classification CONJUNCTION object detection. object detection CONJUNCTION image classification. object detection CONJUNCTION semantic segmentation. semantic segmentation CONJUNCTION object detection. It USED-FOR image classification. It USED-FOR semantic segmentation. It USED-FOR object detection. PASCAL VOC USED-FOR semantic segmentation. PASCAL VOC USED-FOR object detection. PASCAL VOC USED-FOR image classification. visual representations USED-FOR tasks. CO2 USED-FOR visual representations. CO2 USED-FOR tasks. OtherScientificTerm are human annotation, heterogeneous similarity, semantic class, consistency term, and labeled semi - supervised settings. Task is instance discrimination task. Generic is task. Method is label assignment strategy. Metric is top-5 accuracy. ","This paper proposes Consistent Contrast (CO2), a semi-supervised contrastive learning method for image classification, object detection and semantic segmentation. The proposed method is based on a consistency regularization term that is applied to the unlabeled data. The method is evaluated on the ImageNet linear protocol and achieves state-of-the-art performance.  ","This paper proposes Consistent Contrast (CO2), a new contrastive learning framework for semi-supervised visual representation learning. CO2 is based on the consistency regularization term (MoCo), which is used to ensure consistency between unlabeled data and labeled data. The authors show that CO2 achieves top-1 accuracy on the ImageNet linear protocol and PASCAL VOC task."
2793,SP:d18bab21790713e2facb053c47298fc9079ab783,"Optimistic Gradient Descent Ascent ( OGDA ) CONJUNCTION Optimistic Multiplicative Weights Update ( OMWU ). Optimistic Multiplicative Weights Update ( OMWU ) CONJUNCTION Optimistic Gradient Descent Ascent ( OGDA ). Optimistic Multiplicative Weights Update ( OMWU ) USED-FOR saddle - point optimization. Optimistic Gradient Descent Ascent ( OGDA ) USED-FOR saddle - point optimization. uniqueness of the optimal solution HYPONYM-OF assumptions. probability simplex FEATURE-OF bilinear games. OGDA CONJUNCTION OMWU. OMWU CONJUNCTION OGDA. OMWU USED-FOR constrained setting. last - iterate convergence FEATURE-OF OGDA. bilinear games FEATURE-OF OMWU. universal constant FEATURE-OF learning rate. simplex FEATURE-OF bilinear games. learning rate USED-FOR linear last - iterate convergence. constant learning rate FEATURE-OF last - iterate convergence rates. last - iterate convergence rates FEATURE-OF OGDA. condition FEATURE-OF bilinear games. polytope FEATURE-OF bilinear games. condition USED-FOR strongly - convex - stronglyconcave functions. Metric is convergence rates. OtherScientificTerm are exponentially small learning rate, equilibrium, smoothness of the objective function, and unique equilibrium assumption. Method is projected OGDA algorithm. ","This paper studies the convergence of Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weights Update (OMWU) in saddle-point optimization in bilinear games with probability simplex. In particular, the authors show that OGDA and OMWU converge linearly last-iterate to the optimal solution in the last iterate of the last iteration of the optimization process. The convergence rate of OGDA is shown to be exponentially small when the smoothness of the objective function is assumed to be smooth. The authors also show that the convergence rate is universal in the case of strongly-convex-strongly-concave functions. ","This paper studies the convergence rate of saddle-point optimization in the last-iterate setting of Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weights Update (OMWU) under the assumption of uniqueness of the optimal solution. In particular, the authors show that the learning rate of OGDA converges linearly to a universal constant, and that OMWU converges to a constant learning rate under the same assumption. The convergence rate is also shown to converge linearly under the condition that the smoothness of the objective function is guaranteed. The authors also show that for strongly-convex-strongly-concave functions, the convergence rates converge to the universal constant. "
2809,SP:bbc7f77308b298c332a39747f693bc396f00a89f,"federated setup USED-FOR User Verification ( UV ) models. framework USED-FOR private and secure training of UV models. Federated User Verification ( FedUV ) USED-FOR private and secure training of UV models. secret user - defined linear combination USED-FOR instance embeddings. FedUV COMPARE approaches. approaches COMPARE FedUV. voice, face, and handwriting data USED-FOR user verification. Method are loss functions, and UV models. OtherScientificTerm are user embeddings, linear combinations, error - correcting code, embedding vectors, and embeddings. Generic is model. ","This paper proposes a federated setup for user verification in federated settings. The proposed method is based on a linear combination of user-defined linear combinations of instance embeddings, which are then used to compute the embedding vectors for each instance. The method is evaluated on voice, face, and handwriting data.   ","This paper proposes Federated User Verification (FedUV), a framework for federated training of user verification (UV) models in a federated setup. FedUV uses a secret user-defined linear combination of instance embeddings to verify the user embedding vectors, which are then used to train the model. The proposed method is evaluated on voice, face, and handwriting data, and compared with several baselines."
2825,SP:40fa47cc0928e2925ef5ce6d808073f368ca2cd4,"geometry FEATURE-OF class manifolds ( CMs ). geometry FEATURE-OF model. technique USED-FOR boundaries. technique USED-FOR CMs. generalization CONJUNCTION robustness. robustness CONJUNCTION generalization. geometry of CMs CONJUNCTION generalization. generalization CONJUNCTION geometry of CMs. label randomization CONJUNCTION training set size. training set size CONJUNCTION label randomization. ensemble size CONJUNCTION label randomization. label randomization CONJUNCTION ensemble size. stage of training CONJUNCTION class. class CONJUNCTION stage of training. training set size CONJUNCTION model robustness. model robustness CONJUNCTION training set size. architecture CONJUNCTION random initialization. random initialization CONJUNCTION architecture. random initialization CONJUNCTION stage of training. stage of training CONJUNCTION random initialization. class CONJUNCTION ensemble size. ensemble size CONJUNCTION class. data corruption FEATURE-OF model robustness. dataset USED-FOR CM dimension. CMs USED-FOR ensembling. Method are Deep neural network classifiers, and real neural networks. OtherScientificTerm are margin, and random affine subspaces. Generic are method, and models. ",This paper studies the geometry of class manifolds (CMs) in deep neural networks (DNNs). The authors propose a new technique to estimate the class boundaries of DNNs based on CMs. The authors show that CMs can be used to estimate generalization error and robustness. They also show that the CMs are useful for ensembling DNN models.,"This paper proposes a method to learn class manifolds (CMs) that can be used to improve the generalization and robustness of deep neural network models. The method is based on the idea that CMs can be represented as a set of random affine subspaces, where each subspace is represented by a single class. The authors show that the size of the CMs and the number of training sets are important factors in generalization, robustness, and generalization. They also show that this method can improve the robustness against data corruption.   "
2841,SP:09bce202ac7a750c3700a8ef3cd92cfe8ed00c39,"exploration CONJUNCTION exploitation. exploitation CONJUNCTION exploration. RL methods USED-FOR problem. action noise USED-FOR policies. entropy FEATURE-OF policy. entropy temperature FEATURE-OF policy. entropy temperature USED-FOR entropy. Soft Actor - Critic ( SAC ) HYPONYM-OF policies. Curiosity - Aware entropy Temperature USED-FOR SAC ( CAT - SAC ). curiosity mechanism USED-FOR instance - level entropy temperature. curiosity mechanism USED-FOR Curiosity - Aware entropy Temperature. state prediction error USED-FOR curiosity. state prediction error USED-FOR CAT - SAC. entropy USED-FOR CAT - SAC. MuJoCo benchmark EVALUATE-FOR CAT - SAC. sample efficiency EVALUATE-FOR CAT - SAC. Task is reinforcement learning ( RL ). Generic is temperature. OtherScientificTerm are environment states, prediction error, and unfamiliar states. ","This paper studies the problem of exploration and exploitation in reinforcement learning, where the goal is to learn a policy that maximizes the entropy of the environment states. The authors propose to use a curiosity mechanism to estimate the instance-level entropy of each state using the prediction error of the policy, and use this information to compute the state-action entropy. The proposed method is evaluated on the MuJoCo benchmark and achieves state-of-the-art performance.","This paper proposes a method for learning a policy that is sensitive to the temperature of the environment. The temperature is defined as the temperature at which a policy is most likely to explore a new state. The authors propose a curiosity-aware entropy temperature (CAT-SAC) that is used to estimate the entropy of the policy. They show that the temperature can be used to predict the state prediction error of a policy, which is then used to improve the sample efficiency. The proposed method is evaluated on the MuJoCo benchmark and shows promising results."
2857,SP:dce5eb20581a21c5de0a9fc07a8a79a1fbb28c71,"Reinforcement learning algorithms USED-FOR policies. meta - reinforcement learning methods USED-FOR agents. model identification CONJUNCTION experience relabeling ( MIER ). experience relabeling ( MIER ) CONJUNCTION model identification. experience relabeling ( MIER ) HYPONYM-OF meta - reinforcement learning algorithm. meta - reinforcement learning algorithm USED-FOR out - of - distribution tasks. policies CONJUNCTION value functions. value functions CONJUNCTION policies. dynamics models COMPARE value functions. value functions COMPARE dynamics models. dynamics models COMPARE policies. policies COMPARE dynamics models. off - policy data USED-FOR dynamics models. synthetic experience USED-FOR task. dynamics models USED-FOR policies. Generic are approaches, and method. Method are on - policy meta - training, and meta - reinforcement learning. ","This paper proposes a meta-reinforcement learning method for out-of-distribution (OOD) tasks. The proposed method is based on model identification and experience relabeling (MIER), which is a combination of experience relabelling and model identification. Experiments show that the proposed method outperforms baselines in OOD tasks. ",This paper proposes a meta-reinforcement learning algorithm for out-of-distribution (OOD) tasks. The authors propose a new meta-regression method that uses experience relabeling (MIER) and model identification to learn a policy from off-policy data. Experiments show that the proposed method outperforms the baselines on a variety of datasets. 
2873,SP:34d78aa11f9d50baf75a9646a6f9128318c3389a,"meta - learning techniques USED-FOR few - shot learning ( FSL ) problem. label noise USED-FOR FSL. label noise FEATURE-OF meta - learner. gradient noise problem USED-FOR meta - overfitting problem. Eigen - Reptile ( ER ) USED-FOR gradient noise. Eigen - Reptile ( ER ) USED-FOR meta - parameters. historical taskspecific parameters USED-FOR gradient noise. historical taskspecific parameters USED-FOR Eigen - Reptile ( ER ). Introspective Self - paced Learning ( ISPL ) USED-FOR prior models. Eigen - Reptile CONJUNCTION ISPL. ISPL CONJUNCTION Eigen - Reptile. tasks EVALUATE-FOR methods. methods COMPARE state - of - the - art methods. state - of - the - art methods COMPARE methods. tasks EVALUATE-FOR state - of - the - art methods. noisy labels USED-FOR state - of - the - art methods. OtherScientificTerm are meta - overfit, sampling noise, and gradient step. Task is overfitting. ",This paper studies the few-shot learning (FSL) problem with label noise in the meta-learning setting. The authors propose to use Eigen-reptile (ER) to model the gradient noise problem in FSL. They also propose a self-paced learning (ISPL) method to improve the performance of FSL with noisy labels. The experiments show that the proposed methods outperform the state-of-the-art methods on several tasks. ,"This paper studies the problem of meta-learning in few-shot learning (FSL) with label noise. The authors propose a new method called Eigen-Reptile (ER) to mitigate the meta-overfitting problem in FSL. The main contribution of the paper is to propose a method that uses Eigenreptile as a meta-parameter for the gradient noise problem. The method is based on the idea of introspective self-paced learning (ISPL), which is an extension of self-supervised learning (SSPL). The authors show that the proposed method outperforms the state-of-the-art methods on a variety of tasks."
2889,SP:a571bff9ffe4edafd7bc064c4d10609e6b981ce3,"Adversarial training USED-FOR models. feature statistics COMPARE image pixels. image pixels COMPARE feature statistics. distributional shifts USED-FOR models. adversarially crafted distributions USED-FOR images. Stylized - ImageNet CONJUNCTION ImageNetInstagram. ImageNetInstagram CONJUNCTION Stylized - ImageNet. AdvBN USED-FOR semantic segmentation. generalization EVALUATE-FOR semantic segmentation. generalization EVALUATE-FOR AdvBN. goldfinch CONJUNCTION sulphur butterfly. sulphur butterfly CONJUNCTION goldfinch. goldfinch CONJUNCTION bulbul. bulbul CONJUNCTION goldfinch. hummingbird CONJUNCTION goldfinch. goldfinch CONJUNCTION hummingbird. house finch CONJUNCTION goldfinch. goldfinch CONJUNCTION house finch. bulbul CONJUNCTION house finch. house finch CONJUNCTION bulbul. brambling CONJUNCTION guillotine. guillotine CONJUNCTION brambling. bolete CONJUNCTION fox squirrel. fox squirrel CONJUNCTION bolete. fox squirrel CONJUNCTION hen - of - the - woods. hen - of - the - woods CONJUNCTION fox squirrel. gong CONJUNCTION bolete. bolete CONJUNCTION gong. Ibizan hound CONJUNCTION flamingo. flamingo CONJUNCTION Ibizan hound. ResNet-50 model USED-FOR classification scores. OtherScientificTerm are small adversarial perturbations, and mean and variance of deep image features. Method are adversarial training, Adversarial Batch Normalization ( AdvBN ), ResNet-50, ImageNet variants, and Adversarial Batch Normalization module. Generic is method. ","This paper proposes Adversarial Batch Normalization (AdvBN), a method to normalize the mean and variance of deep image features during adversarial training. The proposed method is based on the observation that adversarial perturbations can lead to large distributional shifts in the training data, which can negatively affect the performance of the model. To mitigate this issue, the authors propose to regularize the distribution of the image features by adding a batch normalization module. Experiments on ImageNet and Instagram show that AdvBN can improve the performance on image classification tasks. ","This paper proposes Adversarial batch normalization (Adversarial Batch Normalization (AdvBN), an approach to normalizing the mean and variance of deep image features to improve the generalization performance of image classification models. The authors show that AdvBN can improve the performance of ImageNet-50 and ImageNet variants on semantic segmentation tasks. They also show that it can also improve generalization of ResNet models. "
2905,SP:6a9c46bd3cf854299f360bff136e1d79d3edb2e4,"proxy metric USED-FOR outliers. outliers PART-OF data distribution. Variance of Gradients ( VoG ) HYPONYM-OF proxy metric. Task are machine learning, and human - in - the - loop auditing. Method are model, models, and VoG. OtherScientificTerm is VoG scores. ",This paper proposes a proxy metric called Variance of Gradients (VoG) to identify outliers in the data distribution. The proposed metric is based on the observation that a large number of outliers can be identified in the training data. The authors show that the VoG score is a good proxy metric for outliers and that it can be used as a way to detect outliers.   ,"This paper proposes a new proxy metric, Variance of Gradients (VoG), which can be used to identify outliers in the data distribution. The authors show that the VoG scores of a model trained on a set of data points can be better estimated than the true VoG score of the data points. They also provide a theoretical analysis of the impact of VoG on human-in-the-loop auditing. "
2930,SP:074bfacc75837bb19049be8a2890e10de073dd8e,"real - world data FEATURE-OF simulated samples. images HYPONYM-OF simulated samples. generation quality EVALUATE-FOR model. technique USED-FOR generated samples. non - linear Fokker - Plank equation USED-FOR gradient flow. wasteful sample rejection USED-FOR methods. DRS HYPONYM-OF methods. refinement approach USED-FOR GANs. VAEs CONJUNCTION Normalizing Flows. Normalizing Flows CONJUNCTION VAEs. GANs CONJUNCTION deep generative models. deep generative models CONJUNCTION GANs. vector - valued critics CONJUNCTION deep generative models. deep generative models CONJUNCTION vector - valued critics. vector - valued critics USED-FOR GANs. Normalizing Flows HYPONYM-OF deep generative models. VAEs HYPONYM-OF deep generative models. DGf low COMPARE Discriminator Driven Latent Sampling ( DDLS ) methods. Discriminator Driven Latent Sampling ( DDLS ) methods COMPARE DGf low. Discriminator Optimal Transport ( DOT ) CONJUNCTION Discriminator Driven Latent Sampling ( DDLS ) methods. Discriminator Driven Latent Sampling ( DDLS ) methods CONJUNCTION Discriminator Optimal Transport ( DOT ). quality of generated samples EVALUATE-FOR generative models. synthetic, image, and text datasets EVALUATE-FOR DGf low. DGf low USED-FOR generative models. DGf low COMPARE Discriminator Optimal Transport ( DOT ). Discriminator Optimal Transport ( DOT ) COMPARE DGf low. quality of generated samples EVALUATE-FOR DGf low. Method are Deep generative modeling, Discriminator Gradient f low ( DGf low ), McKean - Vlasov process, and GAN variants. OtherScientificTerm are entropy - regularized f -divergences, and real and the generated data distributions. ","This paper proposes a new method to improve the quality of generated samples in deep generative models. The proposed method is based on the non-linear Fokker-Plank equation for gradient flow, which can be viewed as a refinement approach to GANs. The authors show that the proposed method, called Discriminator Gradient f low (DGf low), can improve the generation quality in terms of entropy-regularized f-divergences. ",This paper proposes a new method to improve the quality of generated samples for deep generative models. The method is based on the non-linear Fokker-Plank equation (Fokker equation) which is used to estimate the entropy-regularized f-divergences between the real and the generated data distributions. The authors show that the proposed method can improve the generated samples quality by a factor of 1.5 on synthetic and real-world datasets. 
2955,SP:74ecbc5a6d464bfa49337da9e0dd6a0fe714d4bb,encoder - only Transformer CONJUNCTION encoder - decoder Transformer. encoder - decoder Transformer CONJUNCTION encoder - only Transformer. encoder - decoder Transformer USED-FOR generation tasks. encoder - only Transformer USED-FOR understanding tasks. tasks CONJUNCTION frameworks. frameworks CONJUNCTION tasks. encoder - decoder Transformer USED-FOR understanding tasks. model architectures CONJUNCTION pre - training tasks. pre - training tasks CONJUNCTION model architectures. sub - modules USED-FOR understanding and generation tasks. inference USED-FOR understanding and generation tasks. sub - modules PART-OF Transformer block. sub - modules PART-OF VECO. innersequence and cross - sequence masked language modeling USED-FOR sub - modules. sequence labeling CONJUNCTION question answering. question answering CONJUNCTION sequence labeling. question answering CONJUNCTION sentence retrieval. sentence retrieval CONJUNCTION question answering. text classification CONJUNCTION sequence labeling. sequence labeling CONJUNCTION text classification. XTREME benchmark EVALUATE-FOR cross - lingual understanding tasks. cross - lingual understanding tasks EVALUATE-FOR VECO. text classification HYPONYM-OF XTREME benchmark. text classification HYPONYM-OF cross - lingual understanding tasks. sentence retrieval HYPONYM-OF cross - lingual understanding tasks. sequence labeling HYPONYM-OF cross - lingual understanding tasks. XTREME benchmark EVALUATE-FOR VECO. question answering HYPONYM-OF cross - lingual understanding tasks. VECO COMPARE Transformer variants. Transformer variants COMPARE VECO. VECO COMPARE cross - lingual models. cross - lingual models COMPARE VECO. cross - lingual models CONJUNCTION Transformer variants. Transformer variants CONJUNCTION cross - lingual models. generation tasks EVALUATE-FOR Transformer variants. VECO USED-FOR generation tasks. cross - lingual models USED-FOR generation tasks. Method is multilingual representations. Task is downstream cross - lingual tasks. ,"This paper proposes a Transformer-based model for cross-lingual understanding and generation tasks. The proposed model consists of two modules: innersequence and cross-sequence masked language modeling. The innersequence is used to model the representation of the input sequence and the masked language model is used for the output of the encoder-decoder Transformer. The cross-sequences and masked language models are used to train the model on downstream tasks such as sequence labeling, question answering, sentence retrieval, and text classification. Experiments on the XTREME benchmark show that the proposed model achieves state-of-the-art performance on all tasks.","This paper proposes a Transformer-based model for cross-lingual understanding and generation tasks. The model is built on top of an encoder-decoder Transformer block, where the encoder and decoder modules are modeled as a set of sub- modules. The sub-modules are modeled using inner-sequence and cross-sequence masked language modeling. The proposed model is evaluated on the XTREME benchmark, where it is shown to outperform the state-of-the-art in terms of performance."
2980,SP:3d177ad50727d1a2619b68ab8a897b79d8652beb,"intrinsic motivation USED-FOR Reinforcement Learning ( RL ). K - means USED-FOR auditory event clusters. neural network USED-FOR auditory events. prediction errors USED-FOR intrinsic rewards. Atari games USED-FOR module. model USED-FOR audio - visual exploration. model USED-FOR active learning. Habitat simulator CONJUNCTION active learning. active learning CONJUNCTION Habitat simulator. Habitat simulator USED-FOR model. Habitat simulator USED-FOR audio - visual exploration. ThreeDWorld ( TDW ) simulator USED-FOR active learning. audio signals USED-FOR intrinsic rewards. vision - based models USED-FOR intrinsic rewards. vision - based models USED-FOR RL explorations. audio signals USED-FOR RL explorations. audio signals COMPARE vision - based models. vision - based models COMPARE audio signals. Task are causal understanding of the physical world, and RL exploration. Method is auditory event prediction. Material is acoustic data. ",This paper proposes to use auditory event clusters as an intrinsic reward for reinforcement learning. The proposed method is based on a neural network that predicts auditory events from a set of K-means and uses the prediction errors as intrinsic rewards. The model is trained using a Habitat simulator and a TDW environment. The experimental results show that the proposed method outperforms vision-based methods in terms of intrinsic reward.,"This paper proposes a method for learning a model for audio-visual exploration in the context of Reinforcement Learning (RL). The model is based on a neural network that predicts the K-means of an auditory event cluster, and a Habitat simulator that simulates a three-dimensional environment. The authors show that their model outperforms other vision-based models in terms of intrinsic rewards. "
3005,SP:014f6118ebe55ece6be23c3a10f12e4591e444b1,singleand multimodal data USED-FOR category discovery. end - to - end framework USED-FOR representation. unlabelled data USED-FOR clusters. it USED-FOR labelled and unlabelled data. noise - contrastive estimation USED-FOR self - supervised representation learning. category discrimination CONJUNCTION cross - modal discrimination. cross - modal discrimination CONJUNCTION category discrimination. instance discrimination USED-FOR contrastive learning approaches. cross - modal discrimination USED-FOR instance discrimination. category discrimination USED-FOR instance discrimination. cross - modal discrimination USED-FOR multi - modal data. category discrimination CONJUNCTION labelled data. labelled data CONJUNCTION category discrimination. pairwise pseudo labels USED-FOR unlabelled data. pairwise pseudo labels USED-FOR cluster assignments. CIFAR100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR100. CIFAR10 CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION CIFAR10. Kinetics-400 CONJUNCTION VGG - Sound. VGG - Sound CONJUNCTION Kinetics-400. image benchmarks CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION image benchmarks. CIFAR10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR10. CIFAR10 HYPONYM-OF image benchmarks. ImageNet HYPONYM-OF image benchmarks. image benchmarks EVALUATE-FOR framework. OtherScientificTerm is shared representation space. ," image classification is an important task in machine learning. This paper proposes an end-to-end framework for self-supervised representation learning on both labelled and unlabeled data. The proposed method is based on the idea of learning a shared representation space between the labelled data and unlabelled data. In addition, the proposed method uses a contrastive learning approach to learn the cluster assignments between the labeled data and pairwise pseudo labels. The experimental results show the effectiveness of the proposed methods on image classification tasks.","This paper proposes an end-to-end representation learning framework for multi-modal data. The main idea is to learn a shared representation space between labelled data and unlabelled data, and then use the shared representation to cluster the unlabeled data into clusters. The proposed method is evaluated on CIFAR-10, ImageNet, Kinetics-400, and VGG-Sound."
3030,SP:4df640f502e88ddba2d7e183625231d70b083e82,"image - level tags CONJUNCTION object bounding boxes. object bounding boxes CONJUNCTION image - level tags. image - level tags HYPONYM-OF partial annotations. object bounding boxes HYPONYM-OF partial annotations. broad region coverage FEATURE-OF sparse annotations. Class activation maps USED-FOR coarse labels. conditional random fields USED-FOR sparse labels. Class activation maps USED-FOR segmentation model. Existing methods USED-FOR weak supervision. Class activation maps CONJUNCTION conditional random fields. conditional random fields CONJUNCTION Class activation maps. semi - supervised metric learning problem USED-FOR weakly supervised segmentation. semantic annotation CONJUNCTION co - occurrence. co - occurrence CONJUNCTION semantic annotation. low - level image similarity CONJUNCTION semantic annotation. semantic annotation CONJUNCTION low - level image similarity. co - occurrence CONJUNCTION feature affinity. feature affinity CONJUNCTION co - occurrence. contrastive relationships USED-FOR low - level image similarity. contrastive relationships USED-FOR semantic annotation. partial annotations USED-FOR pixel - wise feature. data - driven grouping CONJUNCTION discriminative feature learning. discriminative feature learning CONJUNCTION data - driven grouping. Pascal VOC EVALUATE-FOR universal weakly supervised segmenter. Task are Weakly supervised segmentation, and pixel localization. Generic are task, and code. OtherScientificTerm are coarse annotations, feature space, and priors. ","This paper proposes a method for weakly supervised image segmentation. The proposed method is based on a semi-supervised metric learning problem, where the task is to learn a segmentation model that can be used for image-level tags and object bounding boxes. The main contribution of the paper is the use of conditional random fields for sparse labels and contrastive learning for low-level image similarity. Experiments show that the proposed method achieves state-of-the-art performance on Pascal VOC.","This paper proposes a new weakly supervised segmentation framework for image-level and object-level segmentation. The proposed method is based on a semi-supervised metric learning problem, where the task of segmentation is to find the best pixel-wise feature for each pixel in the feature space. The authors propose to use a class activation map for the sparse labels and conditional random fields for the coarse labels. They show that the proposed method outperforms the state-of-the-art methods on the Pascal VOC benchmark."
3055,SP:f7d6099adb40a0ce2f8a3563dbd5207cf1fdea0f,"slow convergence FEATURE-OF pretext task. small scale models COMPARE supervised counterpart. supervised counterpart COMPARE small scale models. distillation strategy USED-FOR unsupervised learning. ResNet-18 CONJUNCTION ResNet-34. ResNet-34 CONJUNCTION ResNet-18. top-1 accuracies EVALUATE-FOR linear evaluation. BINGO COMPARE baselines. baselines COMPARE BINGO. ImageNet EVALUATE-FOR linear evaluation. small scale models EVALUATE-FOR BINGO. top-1 accuracies EVALUATE-FOR baselines. top-1 accuracies EVALUATE-FOR BINGO. Method are self - supervised learning, contrastive learning based methods, and distillation. Task are optimization, and Bag of InstaNces aGgregatiOn. Generic is method. OtherScientificTerm is bag of instances. ","This paper proposes a new self-supervised learning method called BINGO, which is based on distillation. The main idea is to learn a bag of instances from a set of instances in a supervised learning task, and then use this bag as a pretext task to train a small scale model. The authors show that this distillation strategy can improve the performance of the model on ImageNet and ResNet. ","This paper proposes a distillation strategy for unsupervised learning in the context of contrastive learning based methods. The main idea is to distill a set of instances into a bag of InstaNces, where each instance is a subset of instances from the bag of instances. The bag is then used as a pretext task to train a small-scale model. The authors show that their method outperforms the state-of-the-art in terms of top-1 accuracy on ImageNet. "
3071,SP:328866aad6544c81ded8980934df31dc4472435f,"Simulation - based inference ( SBI ) HYPONYM-OF statistical inference. stochastic models USED-FOR statistical inference. SBI algorithms COMPARE generative adversarial networks ( GANs ). generative adversarial networks ( GANs ) COMPARE SBI algorithms. adversarial approach USED-FOR SBI. GATSBI HYPONYM-OF adversarial approach. SBI CONJUNCTION GANs. GANs CONJUNCTION SBI. GATSBI USED-FOR variational objective. variational objective USED-FOR implicit posterior distributions. adversarial setting FEATURE-OF variational objective. high - dimensional posterior spaces USED-FOR Inference. GATSBI USED-FOR Inference. implicit priors USED-FOR Inference. SBI benchmark problems CONJUNCTION high - dimensional simulators. high - dimensional simulators CONJUNCTION SBI benchmark problems. SBI benchmark problems EVALUATE-FOR GATSBI. high - dimensional simulators EVALUATE-FOR GATSBI. GATSBI USED-FOR well - calibrated posterior estimates. model USED-FOR wave propagation. surface of a shallow water body FEATURE-OF wave propagation. high dimensions FEATURE-OF well - calibrated posterior estimates. it USED-FOR high - dimensional posterior. it COMPARE SBI approach. SBI approach COMPARE it. model of camera optics USED-FOR it. implicit prior USED-FOR it. implicit prior USED-FOR high - dimensional posterior. GATSBI USED-FOR sequential posterior estimation. GANs USED-FOR Bayesian inference. GATSBI USED-FOR Bayesian inference. high - dimensional simulation - based models USED-FOR Bayesian inference. GANs USED-FOR GATSBI. OtherScientificTerm are likelihoods, and explicit likelihoods. ","This paper proposes GATSBI, a new method for simulation-based inference (SBI) based on adversarial training. The main idea is to use a generative adversarial network (GAN) as a variational objective to estimate the implicit posterior distributions in the adversarial setting. The proposed method is evaluated on a variety of SBI benchmark problems and high-dimensional simulators.   ",This paper proposes a new method for simulating simulation-based inference (SBI) in the context of generative adversarial networks (GANs). The main idea of the paper is to use a variational objective to estimate high-dimensional posterior distributions of implicit posterior distributions in the adversarial setting. The authors show that the proposed method is able to achieve better performance than the state-of-the-art SBI algorithms on a variety of SBI benchmark problems and high-dimension simulators.
3087,SP:2915e82097eae4eb8546dc500f32b3ec37e3766f,"latent variable USED-FOR prognostic score. prognostic score USED-FOR biostatistics. prognostic score USED-FOR TEs. model USED-FOR individualized treatment effects. latent variable USED-FOR prognostic score. method COMPARE methods. methods COMPARE method. ( semi-)synthetic datasets EVALUATE-FOR method. ( semi-)synthetic datasets EVALUATE-FOR methods. Task is causal inference. OtherScientificTerm are limited overlap, features, TE error bounds, and individualized features. Method are generative prognostic model, and variational autoencoder ( VAE ). ","This paper proposes a generative prognostic model for treatment effect estimation. The proposed method is based on a variational autoencoder (VAE) with a latent variable that is used to estimate the prognostic score for treatment effects. The VAE is trained to estimate treatment effects in the latent space, which is then used as a prognostic measure for the treatment effect. The method is evaluated on (semi-)synthetic and real-world data sets. ","This paper proposes a generative prognostic model for biostatistics. The generative model is based on a variational autoencoder (VAE) model, which is used to model individualized treatment effects (TEs). The VAE model is trained to estimate the prognostic score for each TE. The model is then used to compute the TE error bounds. The authors show that the VAE can be used to estimate TEs with limited overlap, and that it can also be used for individualized features."
3103,SP:ca358c9f36aac6e58ed1b3949c349d210c49a48e,"benchmark tasks PART-OF RL. RL algorithms USED-FOR episodic simulated environments. RL algorithms USED-FOR them. real - world platforms USED-FOR them. robots HYPONYM-OF real - world platforms. framework PART-OF simulated benchmark EARL1. simulated tasks PART-OF simulated benchmark EARL1. algorithms USED-FOR reinforcement learning. approaches USED-FOR episodic RL. approaches COMPARE approaches. approaches COMPARE approaches. autonomy FEATURE-OF algorithms. Method are Reinforcement learning ( RL ), and real - world embodied learning. OtherScientificTerm are human supervision, extrinsic intervention, and interventions. ","This paper proposes a new benchmark for episodic reinforcement learning in simulated environments. The proposed benchmark consists of a set of tasks that can be performed by an agent in an episodic fashion, where the agent is given a sequence of tasks to solve. The tasks are designed to be episodic in the sense that the agent has access to the entire environment at each time step. The authors show that the proposed framework is able to learn to solve the tasks in the simulated environment in a way that does not require intervention from the agent.  ","This paper presents a framework for training a robot that can learn to perform tasks in an episodic environment without human supervision. The goal is to train a robot on a simulated environment where it can perform tasks without human intervention. The robot is trained on a set of simulated tasks, and the robot is able to learn to solve the tasks in the simulated environment. It is shown that the robot can achieve state-of-the-art performance on the simulated tasks and can also perform well on the real-world tasks. "
3119,SP:abe51d4a9817c08f0abde5da0bb8e6ca4e02e7cf,"Question Answering ( QA ) HYPONYM-OF AI and NLP fields. human - level reasoning capability FEATURE-OF QA systems. modules USED-FOR reasoning. QA systems USED-FOR human reasoning process. modules USED-FOR QA systems. Graph Neural Networks ( GNNs ) USED-FOR modules. knowledge graphs ( KGs ) USED-FOR reasoning. pre - trained language models ( LMs ) USED-FOR QA systems. reasoning functionality FEATURE-OF GNN - based modules. GNN - based modules USED-FOR reasoning process. they USED-FOR QA. GNN modules USED-FOR QA. reasoning capability FEATURE-OF GNN modules. CommonsenseQA CONJUNCTION OpenBookQA. OpenBookQA CONJUNCTION CommonsenseQA. graph neural counter COMPARE GNN modules. GNN modules COMPARE graph neural counter. OpenBookQA HYPONYM-OF QA benchmark datasets. knowledge - aware reasoning USED-FOR QA benchmark datasets. OpenBookQA EVALUATE-FOR GNN modules. CommonsenseQA EVALUATE-FOR GNN modules. knowledge - aware GNN modules USED-FOR reasoning. counting HYPONYM-OF reasoning. reasoning modules USED-FOR knowledge - powered QA. Method are LMs, and GNN. ",This paper proposes a knowledge-based QA system that uses a knowledge graph as a counter for answering questions in question answering tasks. The paper proposes to use a graph neural network (GNN) to compute the counter for each question in a question answering task. The GNN is trained using a pre-trained language model (LMs). The paper shows that the GNNs are able to perform reasoning tasks better than the LMs.   ,"This paper proposes a method for using graph neural networks (GNNs) in question answering (QA) systems. The main idea is to use GNNs to generate knowledge graphs (KGs) for QA systems, which are then used to train a pre-trained language model (LMs) on the KGs. The GNN-based QA modules are designed to be able to perform reasoning in the QA system. The authors show that the reasoning capabilities of the GNN modules are comparable to human reasoning capabilities. They also show that their method can be used to improve the performance of QA on several benchmark datasets."
3135,SP:3ea5a38e7fcd9111dcd299ad039b634e2781685f,pruning CONJUNCTION quantization. quantization CONJUNCTION pruning. pruning HYPONYM-OF Deep Neural Networks ( DNN ) compression. quantization HYPONYM-OF Deep Neural Networks ( DNN ) compression. low - cost devices USED-FOR them. performance CONJUNCTION space consumption. space consumption CONJUNCTION performance. three - stage framework USED-FOR DNN inference. Succinct Compression HYPONYM-OF three - stage framework. Succinct Compression USED-FOR DNN inference. near - optimal compression FEATURE-OF DNN inference. Succinct Data Structures USED-FOR fast queries. compressed representation USED-FOR fast queries. Succinct Data Structures USED-FOR method. method USED-FOR DNN models. formulations USED-FOR DNN models. Element - wise or Block - wise manner FEATURE-OF formulations. method USED-FOR transformed DNN models. Succinct Data Structures USED-FOR method. Succinct Data Structures USED-FOR transformed DNN models. execution pipelines USED-FOR model formulations. method USED-FOR DNN inference. execution pipelines USED-FOR method. method COMPARE Huffman Coding. Huffman Coding COMPARE method. AlexNet / VGG-16 inference EVALUATE-FOR Huffman Coding. near - optimal compression FEATURE-OF method. AlexNet / VGG-16 inference EVALUATE-FOR method. Pruning CONJUNCTION Quantization. Quantization CONJUNCTION Pruning. method COMPARE Quantization. Quantization COMPARE method. method CONJUNCTION Pruning. Pruning CONJUNCTION method. Generic is techniques. OtherScientificTerm is inference runtime. ,"This paper proposes a new method for efficient inference in deep neural networks. The proposed method is based on a three-stage approach: (1) it compresses the representation of the model in an Element-wise or Block-wise manner, (2) it prunes the weights of the models in a Pruning-based manner, and (3) it uses a compressed representation to perform fast queries. The method is evaluated on the AlexNet and VGG-16 benchmarks.   ",This paper proposes a three-stage compression method for Deep Neural Network (DNN) inference. The main idea of the method is to compress the representation of DNN models in an Element-wise or Block-wise manner. The proposed method is evaluated on AlexNet/VGG-16 inference and outperforms Huffman Coding.
3151,SP:94c395afc794a9cc163e362078769ff83f3d20d0,"training method USED-FOR tiny neural networks. Network Augmentation ( NetAug ) HYPONYM-OF training method. data augmentation CONJUNCTION dropout. dropout CONJUNCTION data augmentation. noise USED-FOR over - fitting. regularization techniques USED-FOR large neural networks. dropout HYPONYM-OF regularization techniques. data augmentation HYPONYM-OF regularization techniques. techniques USED-FOR tiny neural networks. tiny models COMPARE large models. large models COMPARE tiny models. under - fitting CONJUNCTION over - fitting. over - fitting CONJUNCTION under - fitting. NetAug USED-FOR network ( reverse dropout ). It USED-FOR tiny model. tiny model PART-OF larger models. tiny model USED-FOR inference. image classification CONJUNCTION object detection. object detection CONJUNCTION image classification. NetAug USED-FOR image classification. NetAug USED-FOR object detection. ImageNet CONJUNCTION Cars. Cars CONJUNCTION ImageNet. NetAug USED-FOR tiny models. ImageNet EVALUATE-FOR NetAug. Pascal VOC EVALUATE-FOR NetAug. computational cost EVALUATE-FOR NetAug. Generic are model, and it. OtherScientificTerm are limited capacity, network, supervision, and inference overhead. Method is independent model. ","This paper proposes Network Augmentation (NetAug), a training method for training tiny neural networks. The main idea is to use data augmentation and reverse dropout as regularization techniques to improve the performance of large neural networks by reducing the overfitting and underfitting. The proposed method is evaluated on image classification and object detection tasks and achieves state-of-the-art performance.","This paper proposes a new training method called Network Augmentation (NetAug) to improve the performance of tiny neural networks. The main idea of the paper is to train a tiny model that is independent of the larger model, and then use the tiny model to perform inference on the smaller model. Theoretical analysis is provided to show that the proposed method can improve performance on ImageNet, CIFAR-10, and Pascal VOC. "
3167,SP:9c24549b980e415616f818acbf4cf680ef8edb52,"Point cloud sequence HYPONYM-OF data representation. flexible shape and motion information FEATURE-OF data representation. model USED-FOR temporally coherent feature spaces. real - world environments FEATURE-OF point correspondence information. generator USED-FOR temporally coherent output. point cloud sequence USED-FOR temporal coherence. learnable masking module USED-FOR upsampling ratio. point distribution USED-FOR learnable masking module. point distribution USED-FOR upsampling ratio. fluid dynamical system CONJUNCTION human action scanned data. human action scanned data CONJUNCTION fluid dynamical system. particles CONJUNCTION human action scanned data. human action scanned data CONJUNCTION particles. particles PART-OF fluid dynamical system. domains FEATURE-OF point cloud sequences. particles HYPONYM-OF point cloud sequences. fluid dynamical system HYPONYM-OF domains. human action scanned data HYPONYM-OF domains. particles HYPONYM-OF domains. quantitative and qualitative evaluation EVALUATE-FOR method. quantitative and qualitative evaluation EVALUATE-FOR upsampling task. method USED-FOR temporal coherence. quantitative and qualitative evaluation EVALUATE-FOR learning temporal coherence. upsampling task CONJUNCTION learning temporal coherence. learning temporal coherence CONJUNCTION upsampling task. irregular point cloud sequences USED-FOR temporal coherence. upsampling task EVALUATE-FOR method. OtherScientificTerm are scene flow information, and point correspondence annotation. Material is dynamic point cloud sequences. ",This paper proposes a method for learning temporal coherence in dynamic point cloud sequences. The proposed method is based on the idea of learning a masking module for upsampling from irregularly sampled points in the point cloud sequence. The method is evaluated on two types of data: (1) fluid dynamical system data and (2) human action scanned data.   ,"This paper proposes a method for learning temporal coherence in dynamic point cloud sequences. The key idea is to learn a masking module that can be used to learn the upsampling ratio of a point cloud sequence. This is achieved by using a generator to generate a temporally coherent output from the point correspondence information. The method is evaluated on a variety of domains, including a fluid dynamical system, human action scanned data, and particles. "
3183,SP:67efe60ad37807505369b7852bc0abed29ffdda8,"robustness CONJUNCTION generalization. generalization CONJUNCTION robustness. pre - training USED-FOR detection transformers. it USED-FOR object detection. task adapter USED-FOR it. textual prompts USED-FOR NLP. query positional embeddings USED-FOR model. visual prompts USED-FOR model. visual prompts USED-FOR query positional embeddings. self - attention USED-FOR task adapter. COCO dataset EVALUATE-FOR PT - DETR. it COMPARE detection transformers. detection transformers COMPARE it. robustness CONJUNCTION generalization. generalization CONJUNCTION robustness. generalization EVALUATE-FOR detection transformers. small - size datasets EVALUATE-FOR detection transformers. generalization FEATURE-OF small - size datasets. generalization EVALUATE-FOR it. robustness EVALUATE-FOR it. small - size datasets EVALUATE-FOR it. Method are Large - scale pre - training, 12 - layer transformer, FP - DETR, and encoder - only transformer. OtherScientificTerm are separated training paradigm, and common corruptions. Task is upstream and downstream tasks. Generic is method. ","This paper proposes a method for pre-training detection transformers for object detection. The main idea is to use a self-attention-based self-supervised self-encoder to generate query positional embeddings, which are then used as prompts to train the model. The proposed method is evaluated on the COCO dataset, where it is shown to achieve state-of-the-art performance. ","This paper proposes a new method for pre-training object detection transformers for object detection. The proposed method is based on a two-stage training procedure, where the first stage is to train the model on a large-scale pre-trained dataset, and the second stage is a smaller-scale training dataset. The method is evaluated on COCO dataset, where it outperforms the state-of-the-art on both robustness and generalization."
3199,SP:a1f9897496303984fc7ad469222106b14b4a6233,"Federated Averaging ( FedAvg HYPONYM-OF federated learning algorithm. FedPAGE HYPONYM-OF federated learning algorithm. communication complexity EVALUATE-FOR FedPAGE. optimal PAGE method USED-FOR federated learning algorithm. optimal PAGE method USED-FOR FedPAGE. local methods USED-FOR federated convex and nonconvex optimization. FedPAGE COMPARE local methods. local methods COMPARE FedPAGE. FedPAGE USED-FOR federated convex and nonconvex optimization. communication rounds USED-FOR FedPAGE. communication rounds FEATURE-OF FedPAGE. nonconvex setting FEATURE-OF FedPAGE. number of communication rounds EVALUATE-FOR FedPAGE. FedPAGE CONJUNCTION SCAFFOLD. SCAFFOLD CONJUNCTION FedPAGE. FedPAGE USED-FOR federated convex and nonconvex optimization. communication complexity EVALUATE-FOR FedPAGE. Method are Local - SGD ), local SGD steps, and federated learning. Task is convex setting. OtherScientificTerm are communication round, and communication cost. ","This paper studies the communication complexity of federated averaging in the convex and nonconvex setting. The authors propose a new federated aggregation method called FedPAGE, which is a federated learning algorithm with a communication complexity that scales linearly in the number of communication rounds. The main contribution of the paper is to show that the communication cost is bounded by the communication round, and the authors show that this is the case for convex federated convex problems.    The main contributions of this paper are as follows:  - The authors prove that FedAvg has a communication cost of $O(1/\sqrt{T})$ in the case of convex optimization, where $T$ is a convex function.  - They show that FedPagge is equivalent to local-SGD in the federated setting, where the communication costs are bounded by $O(\sqrt{\frac{T}{T})$.   - In the non convex setting, they show that their method is competitive with SCAFFOLD. ","This paper studies the communication complexity of FedPAGE, a federated learning algorithm for federated convex and nonconvex optimization. The main contribution of the paper is to propose a new local-SGD-based approach to reduce the number of communication rounds in the convex setting. The authors show that the communication cost of the proposed approach is lower than that of the state-of-the-art. They also show that their approach can be used in the nonconvariant setting as well."
3215,SP:81e74765abc6524edd8fdf9a3ba107d7bddaa04b,"mathematical operations USED-FOR Artificial neural networks ( ANNs ). networks USED-FOR adversarial input perturbations. decision boundary geometry FEATURE-OF ANN classifiers. adversarial perturbations USED-FOR decision boundary geometry. adversarial subspace COMPARE random subspace. random subspace COMPARE adversarial subspace. adversarial attacks PART-OF training procedure. redistribution of proximal class labels CONJUNCTION boundary curvature. boundary curvature CONJUNCTION redistribution of proximal class labels. boundary distance CONJUNCTION redistribution of proximal class labels. redistribution of proximal class labels CONJUNCTION boundary distance. Generic are network, and analysis. OtherScientificTerm are adversarial subspaces, minimal perturbation, decision boundary, boundary, and dimensionality of the subspace. Task is test - time adversarial attacks. Method is adversarial training. ","This paper studies the decision boundary geometry of neural networks in the presence of adversarial input perturbations. The authors show that the adversarial subspaces of the network are bounded by a decision boundary, which is defined as the distance between the class labels and the boundary curvature. They show that this decision boundary is independent of the number of samples and the dimensionality of the subspace. They also show that adversarial attacks can be added to the training procedure to improve the performance of the model.","This paper studies the decision boundary geometry of adversarial attacks on neural networks. The authors show that the adversarial subspaces of the network are differentiable, and that the decision boundaries of the subspace can be differentiable in terms of the dimensionality. They also provide a theoretical analysis of the boundary curvature and the redistribution of proximal class labels.   "
3231,SP:af5c25ecf38c5c3f3387720bdc80c2c54c5699fe,"hashtags USED-FOR auxiliary information. similar representations CONJUNCTION dissimilar representations. dissimilar representations CONJUNCTION similar representations. self - supervised representations COMPARE auxiliary - information - infused representations. auxiliary - information - infused representations COMPARE self - supervised representations. auxiliary - information - infused representations COMPARE supervised representations. supervised representations COMPARE auxiliary - information - infused representations. direct downstream labels USED-FOR supervision signals. self - supervised representations COMPARE supervised representations. supervised representations COMPARE self - supervised representations. direct downstream labels USED-FOR supervised representations. approach COMPARE baseline representation learning methods. baseline representation learning methods COMPARE approach. approach COMPARE approach. approach COMPARE approach. approach COMPARE baseline representation learning methods. baseline representation learning methods COMPARE approach. auxiliary data information USED-FOR approach. auxiliary data information USED-FOR baseline representation learning methods. approach USED-FOR unsupervised constructed clusters. approach USED-FOR unsupervised representation learning approach. auxiliary information HYPONYM-OF unsupervised constructed clusters. OtherScientificTerm are data clustering information, and cluster. Material is Instagram image. Method is weakly - supervised contrastive learning approach. ",This paper proposes a weakly-supervised contrastive learning approach for image representation learning. The key idea is to use auxiliary data information (hashtags) to construct clusters of similar and dissimilar images. The proposed approach is evaluated on a set of synthetic and real-world image datasets. The results show that the proposed method achieves better performance compared to self-supervision methods. ,"This paper proposes a weakly-supervised contrastive learning approach for unsupervised representation learning with auxiliary data information. The proposed approach is based on the notion of auxiliary information, i.e., the auxiliary information is a set of data clusters that are constructed from the data. The auxiliary information consists of two parts: (1) a set (2) a cluster of similar representations, and (3) a group of dissimilar representations. The authors show that the proposed approach outperforms the state-of-the-art baselines in terms of the number of auxiliary data clusters.  "
3247,SP:0a92fcc52970201de4a66b1e76c93dbea9dfd3f1,"Recovering sparse parameters PART-OF machine learning. observational data USED-FOR Recovering sparse parameters. algorithms USED-FOR problem. path - following algorithm USED-FOR PLISA. recovery accuracy EVALUATE-FOR PLISA. empirical Rademacher complexity EVALUATE-FOR PLISA. empirical Rademacher complexity EVALUATE-FOR generalization ability. generalization ability EVALUATE-FOR PLISA. PLISA USED-FOR sparse estimation problems. stability CONJUNCTION convergence. convergence CONJUNCTION stability. generalization ability CONJUNCTION algorithmic properties. algorithmic properties CONJUNCTION generalization ability. convergence FEATURE-OF unrolled algorithm. stability FEATURE-OF unrolled algorithm. convergence HYPONYM-OF algorithmic properties. stability HYPONYM-OF algorithmic properties. techniques USED-FOR learning - based algorithms. OtherScientificTerm are hyperparameters, and problem distribution of interest. Generic are they, and analysis. Method are Provable Learning - based Iterative Sparse recovery Algorithm, and generalization analysis. ","This paper studies the problem of recovering sparse parameters from observational data in sparse estimation problems. The authors propose an iterative iterative sparse estimation algorithm (PLISA) based on the Provable Learning-based Iterative Sparse Recovery Algorithm (PLISA), which is a path-following algorithm that recovers sparse parameters in the observational data.  The authors show that PLISA has a Rademacher complexity of $O(1/\sqrt{T})$ and convergence rate of $\Omega(T)$. The authors also provide a generalization analysis for the recovery accuracy and generalization ability.",This paper studies the problem of recovering sparse parameters from observational data. The authors propose an iterative iterative sparse estimation algorithm (PLISA) that is based on a path-following algorithm. They show that PLISA can be used to recover sparse parameters with good generalization performance. They also provide a theoretical analysis of the generalization properties of PLISA.
3263,SP:5064eda9ba27060af15e81b2b317b2e4558b0ac4,"robot control CONJUNCTION game AI. game AI CONJUNCTION robot control. unified homogeneous action space FEATURE-OF hybrid action space. discretization USED-FOR unified homogeneous action space. Hybrid Action Representation ( HyAR ) USED-FOR compact and decodable latent representation space. compact and decodable latent representation space USED-FOR hybrid action space. embedding table CONJUNCTION conditional Variational Auto - Encoder ( VAE ). conditional Variational Auto - Encoder ( VAE ) CONJUNCTION embedding table. HyAR USED-FOR latent space. embedding table USED-FOR HyAR. unsupervised environmental dynamics prediction USED-FOR action representation. DRL algorithms USED-FOR representation space. action space FEATURE-OF hybrid action embeddings. discrete - continuous action space USED-FOR HyAR. HyAR COMPARE baselines. baselines COMPARE HyAR. baselines USED-FOR high - dimensional action spaces. OtherScientificTerm are Discrete - continuous hybrid action space, discrete or continuous action space, approximation difficulties, discrete action, and continuous parameter. Method are Reinforcement Learning ( RL ), and RL algorithms. Task is hybrid action RL. ",This paper proposes a method for learning a compact and decodable latent representation space for discrete-continuous hybrid action spaces. The proposed method is based on an embedding table and a conditional Variational Auto-Encoder (VAE) to learn the action embeddings. The authors show that the proposed method can be used in conjunction with DRL algorithms to learn a compact representation space. The method is evaluated on a number of simulated and real-world robotic control tasks.,"This paper proposes a novel hybrid action representation for reinforcement learning (HyAR) based on a compact and decodable latent representation space. The authors propose to use a conditional Variational Auto-Encoder (VAE) and an embedding table to represent the action space. They show that HyAR can be applied to a variety of high-dimensional action spaces, including discrete-continuous and continuous action spaces. They also provide a theoretical analysis of the performance of HyAR."
3279,SP:5128bf712f6b197de113c7a371b4bec36f978eca,SGEM USED-FOR general non - convex stochastic optimization problems. Stochastic Gradient USED-FOR general non - convex stochastic optimization problems. Stochastic Gradient USED-FOR SGEM. AEGD method USED-FOR SGEM. energy CONJUNCTION momentum. momentum CONJUNCTION energy. energy USED-FOR SGEM. momentum PART-OF SGEM. energydependent convergence rates CONJUNCTION regret bound. regret bound CONJUNCTION energydependent convergence rates. regret bound USED-FOR online convex setting. energydependent convergence rates FEATURE-OF nonconvex stochastic setting. unconditional energy stability property FEATURE-OF SGEM. threshold USED-FOR energy variable. SGEM COMPARE AEGD. AEGD COMPARE SGEM. SGDM USED-FOR deep neural networks. SGEM USED-FOR deep neural networks. SGEM COMPARE SGDM. SGDM COMPARE SGEM. Method is Adaptive Gradient Descent. ,"This paper studies stochastic gradient descent in non-convex optimization problems. In particular, the authors study the convergence rate of SGEM in the online convex setting, and the regret bound is shown to be O(1/\sqrt{T}^2) in the convex case and O(T/T^2/T) in a non-Convex setting. The authors show that SGEM converges to a stationary point at a rate that depends on the energy and momentum of the problem. They also provide an unconditional energy stability property for SGEM.   ",This paper studies the problem of stochastic gradient descent (SGEM) in the nonconvex setting. The main contribution of the paper is to provide an unconditional energy stability property for SGEM. The paper also provides a regret bound for the online convex setting and an energy-dependent convergence rate for the energy dependent setting. 
3295,SP:11f49b0a975be87769be29e85d7e3924699cf2c9,"non - autoregressive ( NAR ) approaches USED-FOR inference. NAR models COMPARE AR counterparts. AR counterparts COMPARE NAR models. training CONJUNCTION inference. inference CONJUNCTION training. multiple datasets EVALUATE-FOR NAR models. NAR EVALUATE-FOR CMLMC. raw data USED-FOR CMLMC. multiple datasets EVALUATE-FOR AR. multiple datasets EVALUATE-FOR CMLMC. Metric is human - level accuracy. Method are AR framework, and distillation. OtherScientificTerm is raw data without distillation. ",This paper proposes a novel non-autoregressive (NAR) framework for training and inference of machine learning models. The proposed method is based on a distillation-based approach that distills the raw data from the training set to the inference set. The authors show that the proposed method outperforms the state-of-the-art NAR models on multiple datasets.   ,This paper proposes a novel non-autoregressive (NAR) framework for training and inference of machine learning models with human-level accuracy. The key idea is to distill the raw data without distillation and use it to train an NAR model. The authors show that their method outperforms the state-of-the-art NAR models on multiple datasets. 
3311,SP:96f8ac3c6163e56d8ae1954a162bae01e6b58a0a,Ultra - low power local signal processing USED-FOR edge applications. always - on devices USED-FOR edge applications. limited power budget FEATURE-OF domain. spiking neural networks USED-FOR Neuromorphic processors. computational power FEATURE-OF Neuromorphic processors. limited power budget FEATURE-OF Neuromorphic processors. spiking neural dynamics COMPARE dilated temporal convolutions. dilated temporal convolutions COMPARE spiking neural dynamics. WaveSense HYPONYM-OF spiking neural network. WaveNet architecture USED-FOR spiking neural network. neural dynamics CONJUNCTION fixed time - constants. fixed time - constants CONJUNCTION neural dynamics. fixed time - constants CONJUNCTION feed - forward architecture. feed - forward architecture CONJUNCTION fixed time - constants. WaveSense USED-FOR neuromorphic implementation. feed - forward architecture USED-FOR WaveSense. fixed time - constants USED-FOR WaveSense. neural dynamics USED-FOR WaveSense. datasets USED-FOR keyword - spotting. keyword - spotting EVALUATE-FOR model. datasets EVALUATE-FOR model. CNNs CONJUNCTION LSTMs. LSTMs CONJUNCTION CNNs. network COMPARE spiking neural networks. spiking neural networks COMPARE network. network COMPARE artificial neural networks. artificial neural networks COMPARE network. LSTMs HYPONYM-OF artificial neural networks. CNNs HYPONYM-OF artificial neural networks. ," for keyword spotting.  The paper proposes a novel neural network architecture called WaveSense, which uses a spiking neural network with fixed time-constant and a feed-forward architecture. The proposed method achieves state-of-the-art results on keyword spotting on two datasets. ","This paper proposes WaveSense, a new neural network architecture for low-power local signal processing for always-on devices with limited power budget. WaveSense is based on the WaveNet architecture with neural dynamics, fixed time-constant, and a feed-forward architecture. The authors show that WaveSense outperforms the state-of-the-art neural networks on keyword-spotting datasets. "
3327,SP:7f20a2e4e95f857140b87b0730360b3ff2f371f4,"machine learning USED-FOR social applications. machine learning USED-FOR injustice. algorithms USED-FOR high - confidence behavioral guarantees. Shifty algorithms HYPONYM-OF algorithms. algorithms USED-FOR demographic shift ’s challenges. Shifty HYPONYM-OF technique. real - world dataset of university entrance exams EVALUATE-FOR Shifty. algorithm USED-FOR models. high - confidence fairness guarantees FEATURE-OF algorithm. Method is machine learning algorithms. OtherScientificTerm are unfair behavior, and demographic shift. Generic are approaches, and methods. Metric is fairness assurances. ","This paper studies the problem of fairness in machine learning in the context of demographic shift. The authors propose a new algorithm, called Shifty, that is able to guarantee high-confidence fairness guarantees in the presence of demographic shifts. The algorithm is based on the idea of using the Shifty algorithm to learn a model that is robust to demographic shifts in the data distribution. The main contribution of the paper is that the authors show that the proposed algorithm can guarantee high confidence in the fairness guarantees.  ",This paper studies the problem of social fairness in machine learning. The authors propose a new algorithm for social fairness that is based on the Shifty algorithm. They show that the proposed algorithm can achieve high-confidence fairness guarantees in the presence of demographic shift. They also show that their algorithm can be applied to a real-world dataset of university entrance exams.  
3343,SP:94f097921bee5fdc10ec2e7c901b2ddb876d9d41,"Stochastic dual dynamic programming ( SDDP ) USED-FOR multi - stage stochastic optimization. Stochastic dual dynamic programming ( SDDP ) USED-FOR modeling real - world process optimization tasks. worst - case complexity EVALUATE-FOR SDDP. trainable neural model USED-FOR problem instances. trainable neural model USED-FOR piece - wise linear value function. trainable neural model USED-FOR SDDP. intrinsic low - dimension space FEATURE-OF piece - wise linear value function. SDDP CONJUNCTION reinforcement learning algorithms. reinforcement learning algorithms CONJUNCTION SDDP. solution quality EVALUATE-FOR competitors. ν - SDDP COMPARE competitors. competitors COMPARE ν - SDDP. problem solving cost EVALUATE-FOR ν - SDDP. reinforcement learning algorithms HYPONYM-OF competitors. SDDP HYPONYM-OF competitors. solution quality EVALUATE-FOR ν - SDDP. synthetic and real - world process optimization problems EVALUATE-FOR ν - SDDP. OtherScientificTerm are decision variables, and successive problems. Material is low dimensional problems. Method is SDDP solver. Task is optimization. ",This paper studies the problem of stochastic dual dynamic programming (SDDP) in the context of multi-stage optimization. The authors propose to use a neural model to model the piece-wise linear value function in the intrinsic low-dimension space of the problem. They show that this model can be used to solve the problem in a low-dimensional space and achieve the best possible performance in terms of the number of iterations and the complexity. ,"This paper proposes a new method for solving stochastic dual dynamic programming (SDDP) problems. The main idea is to use a neural network to model the low-dimensional space of the problem, and then use a linear value function to solve the problem. The authors show that the proposed method outperforms the state-of-the-art methods on synthetic and real-world problems."
3359,SP:3d9f5132f9ec3807dbca78462a459fd123a09b24,protocol USED-FOR private next - token prediction. protocol USED-FOR privacy violations. language models USED-FOR privacy violations. private corpus USED-FOR language models. relaxation of group differentially private prediction USED-FOR SUBMIX. data - dependent privacy accounting mechanism USED-FOR it. it USED-FOR data - extraction attacks. data - dependent privacy accounting mechanism USED-FOR data - extraction attacks. data - dependent privacy accounting mechanism USED-FOR SUBMIX. SUBMIX HYPONYM-OF protocol. transformer - based models USED-FOR next - token predictions. GPT-2 HYPONYM-OF transformer - based models. Generic is model. Method is language model. ,"This paper proposes a new privacy-preserving next-token prediction protocol called SUBMIX, which is based on the relaxation of group-differentially-private prediction. The authors show that the proposed method can be used to protect the privacy of a language model from data-extraction attacks. The proposed method is shown to be more robust to data-exploitation attacks than existing methods. ","This paper proposes a new privacy-aware next-token prediction protocol, called SUBMIX, which is based on the idea of group-differentially private prediction (GDP). The main idea is to use a transformer-based model (GPT-2) to predict the next token of a group, and then use a data-dependent privacy accounting mechanism (SUBMIX) to ensure that the prediction is differentially private. The authors show that the proposed method can be used to defend against data-extraction attacks."
3375,SP:7f524d186ea939309c7eeb843c62b6a4b4cfbc8a,unsupervised method USED-FOR OOD samples. classification model USED-FOR k - NN density estimate. k - NN density estimate USED-FOR unsupervised method. k - NN density estimator COMPARE OOD detection method. OOD detection method COMPARE k - NN density estimator. Label Smoothed Embedding Hypothesis HYPONYM-OF label smoothing. label smoothing USED-FOR model. proposal COMPARE OOD baselines. OOD baselines COMPARE proposal. k - NN density estimation USED-FOR OOD examples. finite - sample high - probability statistical results USED-FOR k - NN density estimation. Material is indistribution samples. ,"This paper proposes an unsupervised method to detect out-of-distribution (OOD) samples. The proposed method is based on the label smoothing hypothesis, which is an extension of the Label Smoothed Embedding Hypothesis. The main idea is to use the k-NN density estimator for OOD detection. Theoretical results are provided to show that the proposed method can detect OOD samples better than existing methods. ","This paper proposes an unsupervised method to detect out-of-distribution (OOD) samples. The proposed method is based on the label smoothing hypothesis. The authors propose a new k-NN density estimator for OOD samples, which can be used to estimate the k-NN density of an OOD sample. They show that the proposed method can outperform existing OOD detection methods in terms of the number of outliers. "
3391,SP:aafbd6ada14cc59a272fe4bf95fac71fa18e57ab,"continuous time domain FEATURE-OF stochastic differential equations. stochastic differential equations USED-FOR Diffusion - based methods. denoising score matching USED-FOR models. denoising score matching framework USED-FOR representation learning. GANs CONJUNCTION VAEs. VAEs CONJUNCTION GANs. VAEs USED-FOR representations. GANs USED-FOR representations. denoising score matching objective USED-FOR diffusion - based representation learning. approach USED-FOR infinite - dimensional latent code. infinite - dimensional latent code USED-FOR state - of - the - art models. semi - supervised image classification EVALUATE-FOR state - of - the - art models. adversarial training USED-FOR diffusionbased models. adversarial training USED-FOR sample quality. smaller noise scales FEATURE-OF approximation of the prior. sampling speed EVALUATE-FOR adversarial training. approximation of the prior USED-FOR adversarial training. Method are non - adversarial generative model, and multi - scale denoising autoencoders. OtherScientificTerm are supervised signal, and latent codes. Task is denoising. Generic is representation. ",This paper proposes a method to improve the quality of denoising score matching in representation learning for semi-supervised image classification. The proposed method is based on the idea that the denoised score matching objective can be viewed as a stochastic differential equation (SDE) in the continuous time domain. The authors propose to use a non-adversarial generative model with a denoiser to approximate the prior of the SDE. Theoretical analysis is provided to show that the proposed method can learn a representation with an infinite-dimensional latent code. Experiments show that this method can achieve state-of-the-art performance in semi-Supervised Image Classification tasks. ,"This paper proposes a novel method for denoising score matching for representation learning in the continuous time domain. The method is based on the notion of denoised score matching, which is a generalization of the denoizing score matching framework. The key idea of the method is to use a non-adversarial generative model to generate an infinite-dimensional latent code, which can be used to represent the latent code of a GAN or a VAE. The proposed method is evaluated on semi-supervised image classification tasks, and it is shown that the proposed method outperforms the state-of-the-art models."
3407,SP:8cfc837d5c10d539bbd098df7134c42e4830ba25,"Goal - conditioned reinforcement learning ( RL ) USED-FOR tasks. navigation CONJUNCTION manipulation. manipulation CONJUNCTION navigation. expert demonstrations CONJUNCTION reward shaping. reward shaping CONJUNCTION expert demonstrations. offline data CONJUNCTION expert demonstrations. expert demonstrations CONJUNCTION offline data. planning USED-FOR curriculum of intermediate states. algorithm USED-FOR distant goal - reaching task. planning USED-FOR algorithm. M - step USED-FOR goal - conditioned policy. E - step CONJUNCTION M - step. M - step CONJUNCTION E - step. expectation maximization USED-FOR goal - conditioned policies. goal - conditioned RL CONJUNCTION graph search. graph search CONJUNCTION goal - conditioned RL. prior methods COMPARE ours. ours COMPARE prior methods. planning USED-FOR ours. goal - conditioned RL USED-FOR prior methods. graph search USED-FOR prior methods. method COMPARE prior methods. prior methods COMPARE method. it USED-FOR tasks. graph search USED-FOR methods. Method are Classifier - Planning ( C - Planning ), and graph planning. Generic is policy. ","This paper proposes a novel method for goal-conditioned reinforcement learning. The main idea is to learn a curriculum of intermediate states that can be used to guide the goal-reaching process. The method is based on a classifier-planning approach, where the policy is trained to select intermediate states from a set of state-action pairs. The goal is to maximize the expected return on the intermediate states, which is achieved by maximizing the sum of the expected reward and the expected distance to the goal.   ","This paper proposes a novel method for goal-conditioned reinforcement learning. The main idea is to learn a curriculum of intermediate states that can be used to guide a policy to reach a distant goal-reaching task. The method is based on the idea of classifier-planning (C-Planning), where the goal is to find intermediate states in the curriculum that are most likely to reach the goal. The authors show that their method outperforms the state-of-the-art methods on a variety of tasks. "
3423,SP:ef3193842e06d4a6edb8a6a86ea5bc97ee5eaa4a,"regularization technique USED-FOR deep neural networks. Mixup HYPONYM-OF regularization technique. mixup USED-FOR k - mixup. Wasserstein metric FEATURE-OF interpolation. interpolation HYPONYM-OF displacement interpolation. mixup USED-FOR k - mixup case. k - mixup USED-FOR cluster and manifold structures. network architectures CONJUNCTION benchmark datasets. benchmark datasets CONJUNCTION network architectures. mixup HYPONYM-OF data augmentation approach. data augmentation approach USED-FOR models. beta distribution USED-FOR Averaging weights. mixup USED-FOR Perturbations. embedded manifold USED-FOR distributions. α USED-FOR procedure. fully - connected network USED-FOR binary classification. synthetic datasets USED-FOR binary classification. 1 - mixup CONJUNCTION 32 - mixup regularization. 32 - mixup regularization CONJUNCTION 1 - mixup. synthetic datasets USED-FOR fully - connected network. k - mixup USED-FOR local structure. blur CONJUNCTION contrast. contrast CONJUNCTION blur. displacement interpolation USED-FOR optimal transport. global cluster CONJUNCTION manifold support structure. manifold support structure CONJUNCTION global cluster. k - mixup USED-FOR perturbed training datasets. manifold support structure FEATURE-OF perturbed training datasets. global cluster FEATURE-OF perturbed training datasets. Metric are generalization, adversarial robustness, and robustness. Generic is It. OtherScientificTerm are local distributional structure, clusters, data manifold, and discrete distributions. Task is regularization. Method is mixup regularization. ","This paper proposes to use mixup regularization to improve the robustness of deep neural networks against adversarial perturbations. Mixup is a popular data augmentation technique in deep learning, where the goal is to improve generalization and robustness. The main contribution of this paper is to introduce mixup as a regularization technique for training deep networks. The authors show that mixup can be used to improve adversarial robustness by introducing a new regularization term called k-mixup, where k is the number of perturbed samples and k is a Wasserstein distance between the clusters and the data manifold. ",This paper proposes a data augmentation method for training deep neural networks with mixup regularization. The proposed method is based on the Wasserstein metric of displacement interpolation. The authors show that mixup can be used to improve generalization and robustness against adversarial perturbations. The main contribution of the paper is that the proposed method can be applied to both training and adversarial training datasets.
3439,SP:0fe6a9848026e5f6436a380199e27a9ad26cffed,lightweight network USED-FOR embeddings. nonlinear classification layer USED-FOR lightweight network. nonlinear classification layer USED-FOR embeddings. nonlinearity USED-FOR representation ( embedding ) learning. deep networks USED-FOR representation ( embedding ) learning. embeddings USED-FOR linear classifier. linear classifier USED-FOR they. nonlinearity USED-FOR deep networks. embedding vector space FEATURE-OF nonlinear classifiers. limited - capacity backbone USED-FOR network. nonlinear kernelized classification layer USED-FOR deep networks. classification layer USED-FOR nonlinear classifier. radial kernel functions USED-FOR nonlinear classifier. embeddings FEATURE-OF radial kernel functions. radial kernel functions USED-FOR classification layer. layer USED-FOR model - efficient classifiers. layer USED-FOR computer vision and natural language processing tasks. ," image embeddings in deep neural networks are usually linear in the embedding vector space. This paper proposes to use nonlinearity to improve the performance of embedding learning in deep networks. In particular, the authors propose to use a nonlinear kernelized classification layer with a limited capacity backbone. The proposed method is evaluated on image classification and natural language processing tasks. ","This paper studies the problem of learning embeddings for nonlinear classifiers. In particular, the authors propose a novel nonlinear kernelized classification layer for embedding the embedding vector space of a linear classifier. They show that the proposed layer can be used for model-efficient classifiers for computer vision and natural language processing tasks. They also show that this layer can also be used to improve the performance of a lightweight network."
3455,SP:01ee8ec81619784788eb0ce9785098e437d17a7c,Graph Neural Networks ( GNNs ) USED-FOR node representations. nodal features CONJUNCTION graph structure. graph structure CONJUNCTION nodal features. fairness - aware data augmentation frameworks USED-FOR intrinsic bias. nodal features CONJUNCTION graph structure. graph structure CONJUNCTION nodal features. nodal features USED-FOR fairness - aware data augmentation frameworks. graph structure USED-FOR fairness - aware data augmentation frameworks. schemes USED-FOR GNN - based learning mechanisms. schemes USED-FOR fairness. fairness EVALUATE-FOR GNN - based learning mechanisms. node classification CONJUNCTION link prediction. link prediction CONJUNCTION node classification. real networks USED-FOR graph contrastive learning. real networks USED-FOR node classification. real networks USED-FOR link prediction. statistical parity CONJUNCTION equal opportunity. equal opportunity CONJUNCTION statistical parity. augmentation strategies COMPARE contrastive methods. contrastive methods COMPARE augmentation strategies. augmentation strategies USED-FOR fairness. statistical parity FEATURE-OF fairness. Method is Node representation learning. Material is graphs. Generic is representations. ,"This paper studies the problem of fairness in node representation learning in graph neural networks (GNNs). The authors propose to use data augmentation to improve the fairness of GNN-based learning mechanisms. In particular, the authors propose two types of data augmentations: (1) node contrastive learning, which is based on the observation that node features are biased towards the edges, and (2) node similarity augmentation, which augments node features with edges from other nodes in the graph. Experiments are conducted on node classification and link prediction tasks, and show that the proposed methods achieve comparable or better performance than existing methods.",This paper proposes a new data augmentation framework for fairness-aware graph neural network (GNN) learning. The proposed framework is based on the observation that GNNs can be biased in the sense that they are biased in terms of intrinsic bias. The authors propose a new contrastive learning method for GNN-based learning that can be used to improve the fairness of GNN learning. They show that the proposed method outperforms the state-of-the-art contrastive methods on a variety of fairness metrics.
3471,SP:7739dc9e37f7f1384f87d2e60281e5bb27fece99,"observational data USED-FOR estimating treatment effects. instrumental variable ( IV ) USED-FOR two - stage regression. 2SLS HYPONYM-OF two - stage regression. IVs CONJUNCTION confounders. confounders CONJUNCTION IVs. nonlinear IV regression variants USED-FOR confounding bias. confounders USED-FOR nonlinear IV regression variants. balancing USED-FOR treatment effect estimation. bias - variance trade - off FEATURE-OF imbalanced treatment distributions. balanced confounders representation USED-FOR treatment effect estimation. nonlinear IV methods USED-FOR confounding. balanced representation of confounders USED-FOR confounder balancing. treatment regression PART-OF modules. modules PART-OF CB - IV algorithm. outcome regression PART-OF CB - IV algorithm. treatment regression PART-OF CB - IV algorithm. confounder balancing USED-FOR treatment effect estimation. IV regression USED-FOR treatment effect estimation. confounder balancing USED-FOR IV regression. multiplicative assumption COMPARE additive separability assumption. additive separability assumption COMPARE multiplicative assumption. multiplicative assumption FEATURE-OF CB - IV algorithm. IV regression CONJUNCTION confounder balancing methods. confounder balancing methods CONJUNCTION IV regression. CB - IV algorithm USED-FOR treatment effect estimation. CB - IV algorithm COMPARE state - of - the - art methods. state - of - the - art methods COMPARE CB - IV algorithm. state - of - the - art methods USED-FOR treatment effect estimation. confounder balancing methods USED-FOR treatment effect estimation. CB - IV algorithm COMPARE confounder balancing methods. confounder balancing methods COMPARE CB - IV algorithm. IV regression HYPONYM-OF CB - IV algorithm. confounder balancing methods HYPONYM-OF state - of - the - art methods. IV regression HYPONYM-OF state - of - the - art methods. OtherScientificTerm are unmeasured confounders, additive separability of noise, and observed confounders. Generic are they, and second stage. Material is linear setting. ","This paper proposes a two-stage regression method for the treatment effect estimation. The first stage is based on the instrumental variable (IV) and the confounders. The second stage consists of two modules: treatment regression and outcome regression. In the first stage, the IVs are used to estimate the confounding bias, and in the second stage, a balanced representation of the confoundeders is used to balance the imbalanced treatment distributions. The authors show that the proposed method outperforms the state-of-the-art methods in terms of the bias-variance trade-off.","This paper proposes a two-stage regression method for estimating the treatment effect of an instrumental variable (IV) and a confounder (i.e., the number of confounders) in a nonlinear IV regression setting. The proposed method is based on two modules: treatment regression and outcome regression. The treatment regression is done in a linear setting, and the outcome regression is performed in a 2-stage linear setting. Experiments show that the proposed method outperforms state-of-the-art methods in terms of treatment effect estimation. "
3487,SP:fdb68c39fce254b73310a3101b2fe97ba47e69fe,"stochastic gradient descent steps USED-FOR models. MAML objective COMPARE non - adaptive learning ( NAL ). non - adaptive learning ( NAL ) COMPARE MAML objective. MAML COMPARE NAL. NAL COMPARE MAML. easy and hard tasks PART-OF linear regression setting. MAML COMPARE NAL. NAL COMPARE MAML. hardness EVALUATE-FOR tasks. MAML USED-FOR hard tasks. Method are gradient descent, and two - layer neural networks. OtherScientificTerm is easy tasks optimal solutions. Task is few - shot image classification. ","This paper studies the problem of few-shot image classification in the linear regression setting, where the goal is to learn a classifier that can generalize well to unseen images. The authors propose a two-layer MAML objective, which is based on the idea that the number of steps needed to learn the classifier should be proportional to the complexity of the task. The main contribution of this paper is to show that the MAMM objective is equivalent to non-adaptive learning (NAL), which is a generalization of linear regression with stochastic gradient descent (SGD) steps. ","This paper studies the problem of few-shot image classification in the linear regression setting. The authors show that the MAML objective can be applied to both easy and hard tasks in linear regression, and that it can be used to improve the performance of two-layer neural networks. The main contribution of the paper is a theoretical analysis of the effect of gradient descent on the hardness of the task. The paper shows that the hard tasks are more difficult to solve than the easy tasks, and it is possible to find the optimal solution for the hard task."
3503,SP:e8143c7880c16ee9ce7a544e0fd80f001b1b4f9f,"astrophysics CONJUNCTION remote sensing. remote sensing CONJUNCTION astrophysics. Sparse Blind Source Separation ( BSS ) USED-FOR applications. remote sensing HYPONYM-OF applications. astrophysics HYPONYM-OF applications. Proximal Alternating Linearized Minimization ( PALM ) algorithm HYPONYM-OF sparse BSS methods. PALM hyperparameters CONJUNCTION variables. variables CONJUNCTION PALM hyperparameters. PALM hyperparameters USED-FOR Unrolling PALM. data - driven knowledge USED-FOR Unrolling PALM. Learned PALM ( LPALM ) algorithm USED-FOR semi - blind source separation. algorithm COMPARE PALM. PALM COMPARE algorithm. LPALM USED-FOR astrophysical multispectral imaging. cumbersome hyperparameter FEATURE-OF PALM. LPALM COMPARE PALM. PALM COMPARE LPALM. separation quality EVALUATE-FOR algorithm. unrolled source separation methods USED-FOR semi - blind setting. LPALM COMPARE unrolled source separation methods. unrolled source separation methods COMPARE LPALM. LPALM USED-FOR semi - blind setting. OtherScientificTerm are hyperparameter choice, and variable mixing matrices. Method are algorithm unfolding / unrolling, and unrolled algorithms. Task is real - world applications. ",This paper proposes an unrolled version of the Proximal Alternating Linearized Minimization (PALM) method for semi-blind source separation (BSS). The main idea is to learn the hyperparameters of the LPALM method and then use the learned parameters to improve the performance of the unrolled PALM method. The proposed method is evaluated on a set of semi-blind image classification tasks.   ,"This paper proposes a new method for semi-blind source separation for sparse blind source separation. The proposed method is based on the learned learned PALM algorithm, which is a variant of the Proximal Alternating Linearized Minimization (PALM) algorithm. The main difference between the learned and unrolled PALM is that the learned LPALM uses data-driven knowledge to learn the hyperparameters of the learned algorithm, and the unrolled algorithm uses the data-based knowledge to choose the parameters of the learning algorithm. Experiments are conducted to demonstrate the effectiveness of the proposed method. "
3519,SP:7716315001949ab88c8a216302fe51bae872fc87,"transformers USED-FOR language modeling. power - law relationship FEATURE-OF transformers. memory CONJUNCTION computation. computation CONJUNCTION memory. attention module USED-FOR Legendre Memory Unit based model. implicit self - attention HYPONYM-OF attention module. loss EVALUATE-FOR transformers. transformers COMPARE LSTMs. LSTMs COMPARE transformers. model COMPARE transformers. transformers COMPARE model. model COMPARE transformers. transformers COMPARE model. model COMPARE LSTMs. LSTMs COMPARE model. transformers COMPARE LSTMs. LSTMs COMPARE transformers. loss EVALUATE-FOR model. global self - attention USED-FOR architecture. OtherScientificTerm are model size, and sequence length. Metric is computational and memory requirements. ","This paper proposes a Legendre Memory Unit-based model with implicit self-attention to reduce the computational and memory requirements of transformers. The proposed model is based on the Legendre memory unit (LMU) architecture, which is an extension of Legendre-based transformers (LMT). The authors show that the proposed model can achieve comparable performance with transformers in terms of model size, computation and memory.  ",This paper proposes a Legendre Memory Unit based model (LSTM) for language modeling with implicit self-attention. The proposed model is based on the power-law relationship between transformers and LSTMs. The authors show that the proposed model can achieve better performance than transformers in terms of computational and memory efficiency. The main contribution of the paper is a new attention module for the LSTM model. The attention module consists of a global attention module and an implicit attention module. The model is evaluated on a variety of datasets.
3535,SP:832f422b3554e89702e13c8c5690ee26f2289e3b,Generative adversarial networks ( GANs ) USED-FOR image generation. photo - realistic quality EVALUATE-FOR Generative adversarial networks ( GANs ). LatentKeypointGAN HYPONYM-OF two - stage GAN. internal conditioning FEATURE-OF space keypoints. appearance embeddings PART-OF keypoints. domain knowledge CONJUNCTION supervision signals. supervision signals CONJUNCTION domain knowledge. network architectures CONJUNCTION training schemes. training schemes CONJUNCTION network architectures. LatentKeypointGAN USED-FOR interpretable latent space. re - positioning USED-FOR LatentKeypointGAN. generating portraits HYPONYM-OF keypoint embeddings. GAN - based method USED-FOR unsupervised keypoint detection. Material is image content. OtherScientificTerm is spatial and appearance factors. ,"This paper proposes a two-stage GAN-based method for unsupervised keypoint detection and image generation. The key idea is to use an encoder-decoder architecture to learn a latent representation of the keypoint embeddings. The encoder encodes the keypoints into a latent space, which is then used to train a GAN to generate the image. The proposed method is evaluated on a variety of image generation tasks.   ","This paper proposes a GAN-based method for unsupervised keypoint detection for image generation. The key idea is to use a two-stage GAN, where the first stage learns a latent space keypoint embedding, and the second stage learns the appearance embeddings of the keypoints. The appearance embedding is learned by re-positioning keypoints in the latent space. The proposed method is evaluated on a variety of datasets, and it is shown that it outperforms the state-of-the-art in terms of image quality."
3551,SP:9206ae6e31077569313838504ef6daa89ad3b59c,"layer normalization USED-FOR deep fully - connected neural networks. mean field formalism USED-FOR deep fully - connected neural networks. initialization scheme CONJUNCTION activation function. activation function CONJUNCTION initialization scheme. normalization techniques USED-FOR problems. method USED-FOR residual networks. method USED-FOR initialization variances. Task is non - perturbative analysis of signal propagation. OtherScientificTerm are depth, gradient explosion, and representation shrinkage. Method is fully - connected architecture. ","This paper studies the effect of layer normalization on the signal propagation in fully-connected neural networks. The authors propose a mean-field formalism to study the effects of depth and activation functions on the propagation of the signal. Theoretical results show that the depth of the network decreases as the depth increases, leading to a gradient explosion, and the representation shrinkage.  The authors then propose a new initialization scheme and an activation function to reduce the gradient explosion.  ",This paper proposes a new mean field formalism for fully-connected neural networks. The main contribution is a non-perturbative analysis of the signal propagation of signal propagation in the mean-field setting. The authors show that the depth of the network can be reduced to zero if the number of layers in the network is small. They show that this can be achieved by using a mean field-based initialization scheme and an activation function. They also show that their method can be applied to residual networks. 
3567,SP:2177be818b5843c580c787f1b2d725154846feb6,"optimal step sizes USED-FOR stochastic gradient descent. line searches USED-FOR step sizes. line searches PART-OF optimization. step sizes FEATURE-OF full - batch loss. line - search method USED-FOR full - batch loss. parabola USED-FOR line - search method. parabolas USED-FOR Learning rates. approach COMPARE SGD. SGD COMPARE approach. line search approaches USED-FOR Deep Learning across models. approach COMPARE line search approaches. line search approaches COMPARE approach. SGD COMPARE line search approaches. line search approaches COMPARE SGD. piece - wise constant learning rate schedule USED-FOR approach. validation and test accuracy EVALUATE-FOR batch sizes. piece - wise constant learning rate schedule USED-FOR SGD. validation and test accuracy EVALUATE-FOR approach. Task is Deep Learning. OtherScientificTerm are inherent noise, noisy update step directions, and optimal update step size. ",This paper proposes a line search method to find the optimal step sizes for stochastic gradient descent (SGD) in the presence of inherent noise in the update step directions. The proposed method is based on line search methods and uses a parabola to estimate the full-batch loss of SGD. The authors show that the optimal update step size can be computed using line searches and show that line searches can be used to find step sizes that are close to the optimal loss. The method is evaluated on a variety of image classification tasks and shows that the proposed method outperforms other line search approaches.,"This paper proposes a new line search method for stochastic gradient descent (SGD) where the goal is to find the optimal step size of the full-batch loss. The main idea is to use a parabola-based line-search method to estimate the full batch loss, and then use a constant learning rate schedule for SGD. The authors show that their method outperforms the state-of-the-art line search methods in terms of validation and test accuracy."
3583,SP:62233782f9046c85617d9ccfe8427eae7d1c9da7,"Noise - contrastive estimation ( NCE ) HYPONYM-OF statistically consistent method. statistically consistent method USED-FOR unnormalized probabilistic models. noise distribution USED-FOR NCE. noise distribution USED-FOR NCE. exponential loss USED-FOR eNCE. eNCE HYPONYM-OF NCE. exponential loss USED-FOR NCE. OtherScientificTerm are flat ) loss landscape, and exponential family. Method is normalized gradient descent. ",This paper studies the noise-contrastive estimation (NCE) method for unnormalized probabilistic models. The authors show that the exponential family of the loss landscape of normalized gradient descent converges to a flat landscape with respect to the noise distribution. They also show that eNCE is an exponential loss with the same exponential family as NCE. ,"This paper studies noise-contrastive estimation (eNCE) for unnormalized probabilistic models, where the noise distribution of the model is the same as that of the true noise distribution. The authors show that eNCE is statistically consistent with respect to the original noise distribution, and that the exponential family of the loss landscape of the exponential-based NCE is not flat. They also show that the normalized gradient descent (NDE) method is not consistent with the true loss landscape. "
3599,SP:ceba6c1421b2d03863007fdaf029b8b946519c1b,"Privacy HYPONYM-OF distributed machine learning. distributed SGD algorithm USED-FOR model. noisy information USED-FOR differential privacy ( DP ). parameter - server architecture FEATURE-OF distributed SGD algorithm. DP CONJUNCTION BR. BR CONJUNCTION DP. convergence FEATURE-OF distributed SGD. Byzantine faults FEATURE-OF distributed SGD. ( α, f)-Byzantine resilience USED-FOR those. ( α, f)-BR USED-FOR approximate convergence guarantee. hyperparameter optimization USED-FOR guarantee. approaches USED-FOR DP. approaches USED-FOR BR. DP CONJUNCTION BR. BR CONJUNCTION DP. DP CONJUNCTION BR. BR CONJUNCTION DP. Method is learning algorithm. Metric is learning accuracy. ","This paper studies the convergence of distributed SGD with differential privacy in the presence of Byzantine faults. The authors show that under certain assumptions on the parameter-server architecture, the convergence is guaranteed for DP and BR. They also provide an approximate convergence guarantee for hyperparameter optimization for DP. ","This paper studies the problem of differential privacy in distributed machine learning. The authors study the convergence of a distributed SGD algorithm with a parameter-server architecture. They show that under certain assumptions on the parameters of the algorithm, they can achieve a convergence guarantee of (alpha, f)-Byzantine resilience under Byzantine faults. They also provide an approximate convergence guarantee for (beta, f) under hyperparameter optimization.  "
3615,SP:bc783f0c829f90931535e63687d13172879631b3,code editing USED-FOR query code snippet. support exemplars USED-FOR query code snippet. editing exemplar USED-FOR editorial pattern. common pattern USED-FOR code editing. support exemplars USED-FOR common pattern. deep learning approach USED-FOR code editing problem. them USED-FOR query code snippet editing. support exemplars USED-FOR edit representations. edit representations PART-OF learning approach. multi - extent similarities ensemble USED-FOR query code snippet editing. language - specific grammar USED-FOR abstract syntax trees. similarities measurement USED-FOR collective tree representations. collective tree representations USED-FOR query and support sample matching. method COMPARE non - composition baselines. non - composition baselines COMPARE method. C # and Python datasets EVALUATE-FOR method. Task is computer source code editing. Material is support and query code snippets. Method is similarity - ranking error estimator. ,This paper proposes a method for code editing in computer code. The proposed method is based on a multi-extent similarity ensemble that learns to match query and support code snippets. The method is evaluated on C# and Python datasets and achieves state-of-the-art results. ,"This paper proposes a novel approach to code editing for query code snippet editing. The authors propose a multi-extent similarities ensemble (MECE) ensemble for code editing, where the query code snippets are represented as support exemplars, and the editing exemplars are represented by an editing exemplar. The MECE ensemble is trained using a similarity-ranking error estimator, which measures the similarity between query and support samples.  The authors show that their approach outperforms the state-of-the-art on C# and Python datasets. "
3631,SP:ca0c4bdb02f7d939fb6de38b6b446ced4b5984a0,text CONJUNCTION music. music CONJUNCTION text. deep generative models USED-FOR realistic sequence data. text HYPONYM-OF realistic sequence data. music HYPONYM-OF realistic sequence data. high - level structure USED-FOR generative process. local coherence CONJUNCTION global coherence. global coherence CONJUNCTION local coherence. global coherence EVALUATE-FOR models. local coherence EVALUATE-FOR models. approach USED-FOR global structure. relational constraints FEATURE-OF global structure. model USED-FOR realistic data. model USED-FOR relational constraints. model PART-OF generative model. model PART-OF generative model. program synthesis algorithm USED-FOR relational constraints. constraint data USED-FOR generative model. approach COMPARE state - of - the - art. state - of - the - art COMPARE approach. approach USED-FOR high - level structure. state - of - the - art USED-FOR high - level structure. capturing high - level structure EVALUATE-FOR approach. low - level structure EVALUATE-FOR approach. OtherScientificTerm is measures of music. Generic is constraints. ,This paper proposes a generative model for generating music with high-level structure. The proposed method is based on a program synthesis algorithm that generates a set of relational constraints on the data. The constraints are then used to train the model using the constraint data. Experiments show that the proposed method achieves state-of-the-art performance on both high- and low-level structures. ,"This paper proposes a generative model that can capture the high-level structure of a sequence of sequences, i.e., the global coherence of the sequence. The model is trained using a program synthesis algorithm that generates a set of relational constraints for each sequence, which are then used to generate a sequence with a high level structure. The authors show that the proposed method can capture high level coherence, while also capturing low-level coherence. The proposed method is evaluated on synthetic data and real-world data."
3647,SP:692ae0c583a1585eff1a7d9c0d3b51b7879611cc,"biological systems CONJUNCTION combinatorial optimization. combinatorial optimization CONJUNCTION biological systems. particle physics CONJUNCTION biological systems. biological systems CONJUNCTION particle physics. scaling problems FEATURE-OF set - to - hypergraph tasks. run - time complexity HYPONYM-OF scaling problems. training method USED-FOR iterative refinement. efficiency CONJUNCTION constant memory usage. constant memory usage CONJUNCTION efficiency. contributions PART-OF set - to - hypergraph model. model COMPARE state - of - the - art. state - of - the - art COMPARE model. Task is set - to - hypergraph prediction. OtherScientificTerm are hyperedges, and positive edges. Metric are memory requirements, and asymptotic memory scaling. ","This paper studies the problem of set-to-hypergraphs, where the goal is to learn a set of hypergraphs that can be used as contributions to a given set of nodes. The main contribution of the paper is to show that the memory requirements of the hypergraph generation process can scale linearly with the number of nodes in the set. This is achieved by introducing hyperedges into the set, where nodes are represented as nodes with positive edges. The authors show that this allows them to reduce the memory requirement of the set generation process, and show that their method is able to do so in a time-efficient manner.","This paper proposes a new model for set-to-hypergraph prediction. The main idea is to use hyperedges, where the hyperedge is a set of positive edges and the hypergraph is a hypergraph with negative edges. The hypergraphs are represented as sets of nodes, and each node is represented as a hyper-graph, where each node belongs to a group of nodes. Each node is then represented by a hyper graph, and the set of nodes is a collection of hyperedged nodes.  The authors propose a new training method for hypergraph prediction, which is based on an iterative refinement method. They show that the proposed method is able to achieve state-of-the-art performance in terms of asymptotic memory scaling. They also show that their method is also able to improve the efficiency of the model."
3663,SP:e3481fb6d8d1aa45d6ed4a454e781f5a2c30c57e,"biases FEATURE-OF models. post - processing method USED-FOR models. deep embeddings PART-OF pre - trained model. shallow neural network USED-FOR It. Ethical Module HYPONYM-OF shallow neural network. methodology COMPARE bias mitigation. bias mitigation COMPARE methodology. Method is deep learning algorithms. OtherScientificTerm are representation power, von Mises - Fisher loss, and latent space. Task is gender bias in facial recognition. ","This paper proposes a post-processing method to mitigate gender bias in deep neural networks. The proposed method is based on the observation that deep embeddings in a pre-trained model can be corrupted by the presence of bias in the latent space. To mitigate this problem, the authors propose a shallow neural network with an Ethical Module. The authors show that the proposed method outperforms existing bias mitigation methods in terms of accuracy.",This paper proposes a post-processing method to mitigate gender bias in deep learning models. The authors propose an approach that uses a shallow neural network with an Ethical Module (EM) module to mitigate the gender bias. Empirical results show that the proposed method outperforms the baseline bias mitigation method in terms of accuracy.
3679,SP:3fb5dcc8b8fb731e09c14b16480cada1c7ccfaa7,"new class data USED-FOR KD loss. phase model USED-FOR old class knowledge. free image stream USED-FOR placebo data. placebo data USED-FOR KD loss. Google Images HYPONYM-OF free image stream. ImageNet-1k CONJUNCTION ImageNet - Subset. ImageNet - Subset CONJUNCTION ImageNet-1k. supervision CONJUNCTION memory budget. memory budget CONJUNCTION supervision. memory budget FEATURE-OF old class exemplars. CIL methods EVALUATE-FOR method. higher - resolution benchmarks EVALUATE-FOR top - performing CIL methods. ImageNet-1k HYPONYM-OF higher - resolution benchmarks. ImageNet - Subset HYPONYM-OF higher - resolution benchmarks. Task are class - incremental learning ( CIL ), and learning of new classes. Method are knowledge distillation ( KD ), evaluation function, and reinforcement learning algorithm. Material are old - class data, and image stream. OtherScientificTerm are class overlap, placebos, and pseudo CIL tasks. Generic is function. ","This paper studies class-incremental learning (CIL), where the goal is to distill knowledge from old classes to new classes in order to improve the performance of the new classes. The authors propose to use a phase model to model the old class knowledge and a placebos model to learn the new class from the old ones. The proposed method is evaluated on ImageNet-1k and ImageNet Subset.   ","This paper proposes a new method for class-incremental learning (CIL) where the goal is to distill the old-class knowledge from the new-class data. The key idea is to use a phase model to learn the old class knowledge from a free image stream, and then use the new class data from the placebo data to distil the KD loss. The authors show that their method outperforms the state-of-the-art CIL methods on ImageNet-1k and ImageNet Subset. "
3695,SP:506e0a888c03a955b708464eed3670c04baf4912,"approach USED-FOR modeling discrete structure. Energy - based Models ( EBMs ) USED-FOR modeling discrete structure. inference CONJUNCTION learning of EBM. learning of EBM CONJUNCTION inference. Energy - based Models ( EBMs ) USED-FOR approach. inference USED-FOR EBM. sampling from discrete distributions USED-FOR it. Markov Chain Monte Carlo ( MCMC ) USED-FOR sampling. informed proposal USED-FOR Markov Chain Monte Carlo ( MCMC ). local updates FEATURE-OF informed proposal. energy changes USED-FOR it. composition of local moves USED-FOR path auxiliary algorithm. sampling CONJUNCTION inference. inference CONJUNCTION sampling. inference CONJUNCTION learning. learning CONJUNCTION inference. path auxiliary algorithms COMPARE generic samplers. generic samplers COMPARE path auxiliary algorithms. generic samplers USED-FOR sampling. generic samplers USED-FOR discrete models. path auxiliary algorithms USED-FOR discrete models. discrete models USED-FOR sampling. discrete models USED-FOR inference. generic samplers USED-FOR inference. high dimensional discrete data USED-FOR deep EBMs. OtherScientificTerm are discrete distributions, evaluation of energy function, and linearization of the energy function. Generic is algorithm. ","This paper proposes an efficient sampling method for learning energy-based models (EBMs) from discrete data. The sampling is based on sampling from a Markov chain Monte Carlo (MCMC) with local updates, where the energy changes are computed by a composition of local moves. The authors show that this sampling method can be used for inference and learning of EBMs. They also show that the sampling can be done efficiently by using path auxiliary algorithms. ","This paper proposes an efficient sampling method for learning energy-based models (EBMs) from discrete data. The main idea is to use a Markov chain Monte Carlo (MCMC) approach to sample from a discrete distribution. The sampling is done by sampling from a set of local updates of the energy function, where the local updates are composed of local moves, and the energy changes are computed by a path auxiliary algorithm. The authors show that the proposed sampling method can be used to learn a discrete model from high-dimensional data, and that it can be combined with a generic sampling method."
3711,SP:4b466277aa5561a80c48d5e72559de4ce95f228b,"Discovery and learning of an underlying spatiotemporal hierarchy PART-OF machine learning. sequential data USED-FOR Discovery and learning of an underlying spatiotemporal hierarchy. layerwise representations USED-FOR hierarchical generative models. Variational Predictive Routing ( VPR ) HYPONYM-OF neural probabilistic inference system. neural probabilistic inference system USED-FOR latent representations of video features. temporal hierarchy FEATURE-OF latent representations of video features. hierarchical renewal process USED-FOR continuous data. VPR USED-FOR organisation of representations. event detection mechanism USED-FOR VPR. organisation of representations PART-OF model. latent hierarchy USED-FOR organisation of representations. system USED-FOR event detection mechanism. latent representations USED-FOR event detection mechanism. VPR USED-FOR event boundaries. VPR USED-FOR timeagnostic rollouts. video datasets EVALUATE-FOR VPR. framework USED-FOR model - based reinforcement learning. approach USED-FOR framework. approach USED-FOR model - based reinforcement learning. neuroscience USED-FOR approach. OtherScientificTerm are spatiotemporal hierarchy, temporal dynamics, spatiotemporal features, hierarchy, and flexible and informative state - space rollouts. ",This paper proposes a method for learning a hierarchical generative model from continuous video data. The method is based on a variational inference system that learns a latent representation of the video features and uses a hierarchical renewal process to update this representation over time. The proposed method is evaluated on a variety of video datasets and is shown to achieve state-of-the-art performance. ,"This paper proposes Variational Predictive Routing (VPR), a neural probabilistic inference system that learns the latent representations of video features in a hierarchical manner. The authors propose a hierarchical renewal process to learn the representations of the video features. They also propose an event detection mechanism to detect the event boundaries of the latent features. Experiments on a variety of video datasets demonstrate the effectiveness of the proposed method."
3727,SP:459ef2e6bd7638020955dbb4d8ae1098619f7b95,global features USED-FOR methods. re - ranking process USED-FOR global features. it USED-FOR accurate and semantic local information. spatial and channel attention CONJUNCTION intermediate supervision. intermediate supervision CONJUNCTION spatial and channel attention. convolutional neural networks COMPARE RANSAC algorithm. RANSAC algorithm COMPARE convolutional neural networks. it USED-FOR UGALR. spatial and channel attention USED-FOR it. intermediate supervision USED-FOR it. RANSAC algorithm USED-FOR local feature matching. spatial and channel attention USED-FOR accurate and semantic local information. convolutional neural networks USED-FOR local feature matching. Oxford and Paris datasets EVALUATE-FOR approach. Task is Image retrieval. OtherScientificTerm is features. Method is local feature learning. Metric is memory consumption. ,This paper proposes a new local feature learning method for image retrieval. The proposed method is based on a re-ranking process that re-ranks the global features in an image and uses a convolutional neural network to learn the local features. The method is evaluated on the Oxford and Paris datasets.   ,This paper proposes a new approach to local feature matching for image retrieval. The authors propose a re-ranking process to re-rank the global features and use spatial and channel attention to match the local features. They show that the proposed approach outperforms the state-of-the-art UGALR method on Oxford and Paris datasets. 
3743,SP:487cc308a1e8ee078c54b2158bcae47e920e73f8,"computer vision CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION computer vision. Multitask learning USED-FOR applications domains. computer vision HYPONYM-OF applications domains. reinforcement learning HYPONYM-OF applications domains. algorithm USED-FOR negative transfer. it USED-FOR gradient magnitudes. RotoGrad USED-FOR negative transfer. RotoGrad HYPONYM-OF algorithm. multi - label classification CONJUNCTION computer vision tasks. computer vision tasks CONJUNCTION multi - label classification. RotoGrad COMPARE methods. methods COMPARE RotoGrad. CelebA CONJUNCTION computer vision tasks. computer vision tasks CONJUNCTION CelebA. RotoGrad USED-FOR complex problems. methods USED-FOR complex problems. NYUv2 dataset USED-FOR computer vision tasks. CelebA USED-FOR multi - label classification. computer vision tasks HYPONYM-OF complex problems. multi - label classification HYPONYM-OF complex problems. OtherScientificTerm are shared network parameters, gradient magnitude, gradient directions, and training convergence. Method is Pytorch implementation. ","This paper proposes a new multi-task learning algorithm called RotoGrad, which is based on the idea of negative transfer. The key idea is to use the gradient magnitude of the gradient directions to control the negative transfer between tasks. The authors show that the proposed method achieves better performance than the state-of-the-art methods in multi-label classification and reinforcement learning tasks.","This paper proposes a new method for multi-task learning, called RotoGrad, which is based on the idea of negative transfer. The main idea is to use gradient magnitudes of the gradient directions of the shared network parameters, which can be used to reduce the negative transfer between tasks. The authors show that the proposed method outperforms the state-of-the-art methods on the NYUv2 dataset. They also show that their method can be applied to multi-label classification tasks."
3759,SP:050cd8319d84a1bd8c2ccb930ba69b33c8fb6e60,soft neuron association USED-FOR pre - trained networks. soft neuron association USED-FOR Layer - wise model fusion. optimal transport USED-FOR Layer - wise model fusion. networks USED-FOR OTFusion. model fusion framework USED-FOR neural networks. CLAFusion HYPONYM-OF model fusion framework. cross - layer alignment USED-FOR model fusion framework. cross - layer alignment USED-FOR heterogeneous neural networks. unbalanced assignment problem USED-FOR cross - layer alignment problem. dynamic programming USED-FOR cross - layer alignment problem. cross - layer alignment USED-FOR framework. framework USED-FOR layer - wise model fusion. number of layers of neural networks USED-FOR framework. CLAFusion USED-FOR fused network. finetuning process USED-FOR it. finetuning process USED-FOR residual networks. residual networks EVALUATE-FOR it. CIFAR10 dataset EVALUATE-FOR residual networks. CIFAR10 dataset EVALUATE-FOR it. model compression CONJUNCTION knowledge distillation. knowledge distillation CONJUNCTION model compression. knowledge distillation USED-FOR teacher - student setting. Material is heterogeneous data. OtherScientificTerm is retraining. ,"This paper proposes a model fusion framework for heterogeneous neural networks. The proposed method is based on cross-layer alignment, which is an unbalanced assignment problem that can be solved by dynamic programming. The authors show that the proposed method can be used for model compression and knowledge distillation in the teacher-student setting. Experiments are conducted on CIFAR10 and ImageNet.",This paper proposes a model fusion framework CLAFusion for heterogeneous neural networks. The main idea is to use a cross-layer alignment problem to solve the unbalanced assignment problem. The authors propose a dynamic programming approach to tackle the cross-layered alignment problem. Experiments on CIFAR-10 dataset show that the proposed method outperforms the state of the art. 
3775,SP:f764eae15cd083fdb4eb2af09ac64c2d878a454f,"generalization EVALUATE-FOR deep networks. supervised learning USED-FOR deep networks. implicit regularization FEATURE-OF overparameterized deep networks. stochastic gradient descent USED-FOR implicit regularization. SGD USED-FOR supervised learning. SGD USED-FOR implicit regularization. regularizer USED-FOR degenerate solutions. implicit regularization USED-FOR temporal difference learning. regularizer COMPARE supervised learning case. supervised learning case COMPARE regularizer. representations USED-FOR state - action pairs. bootstrapping USED-FOR deep network value function. deep network value function USED-FOR feature representations. bootstrapping USED-FOR feature representations. explicit regularizer USED-FOR implicit regularizer. DR3 HYPONYM-OF explicit regularizer. D4RL domains CONJUNCTION robotic manipulation. robotic manipulation CONJUNCTION D4RL domains. Atari 2600 games CONJUNCTION D4RL domains. D4RL domains CONJUNCTION Atari 2600 games. DR3 USED-FOR unlearning. DR3 USED-FOR robotic manipulation. performance CONJUNCTION stability. stability CONJUNCTION performance. unlearning CONJUNCTION D4RL domains. D4RL domains CONJUNCTION unlearning. offline RL methods COMPARE DR3. DR3 COMPARE offline RL methods. unlearning CONJUNCTION robotic manipulation. robotic manipulation CONJUNCTION unlearning. Atari 2600 games FEATURE-OF unlearning. stability EVALUATE-FOR DR3. images USED-FOR robotic manipulation. performance EVALUATE-FOR DR3. Method are overparameterization, and deep reinforcement learning ( RL ) methods. OtherScientificTerm are parsimonious solutions, degenerate feature representations, and Bellman backup. Task is offline deep RL setting. ","This paper proposes DR3, an implicit regularizer for temporal difference learning in deep reinforcement learning. DR3 is based on bootstrapping with Bellman backup. Theoretical analysis is provided to show that DR3 can be used as a regularizer to prevent degenerate solutions. Experiments are conducted on D4RL domains and robotic manipulation tasks. ",This paper proposes a new implicit regularization method for deep reinforcement learning (DR3) based on bootstrapping. The main idea of DR3 is to use the Bellman backup as an explicit regularizer to prevent degenerate representations in the training process. The authors show that DR3 can be used to improve the generalization performance of deep neural networks in the offline RL setting. They also show that the proposed method can improve the performance of D4RL on robotic manipulation tasks.
3791,SP:6fd793b27123bf80504e2ad5957455b7ec311612,RLSVI USED-FOR posterior samples. algorithm USED-FOR deep RL. HyperDQN HYPONYM-OF algorithm. non - linear neural network USED-FOR Q - values. base model HYPONYM-OF non - linear neural network. meta model HYPONYM-OF probabilistic hypermodel. probabilistic hypermodel USED-FOR method. hypermodel USED-FOR approximate posterior samples. Q - value functions USED-FOR exploratory action sequences. RLSVI USED-FOR exploration. posterior samples USED-FOR Q - value function. Atari suite EVALUATE-FOR HyperDQN. Atari suite EVALUATE-FOR DQN. HyperDQN COMPARE DQN. DQN COMPARE HyperDQN. maximum human - normalized score EVALUATE-FOR DQN. maximum human - normalized score EVALUATE-FOR HyperDQN. HyperDQN COMPARE exploration bonus and randomized exploration methods. exploration bonus and randomized exploration methods COMPARE HyperDQN. HyperDQN USED-FOR SuperMarioBros. exploration bonus and randomized exploration methods USED-FOR SuperMarioBros. Method is exploration method. OtherScientificTerm is feature. Generic is models. Metric is efficiency. ,"This paper proposes HyperDQN, a novel exploration method for deep reinforcement learning. The main idea is to use a probabilistic hypermodel to estimate the Q-values for exploration. The hypermodel is trained using a meta-model, which is a combination of a base model and a meta model. The proposed method is shown to outperform DQN in the Atari suite.   ","This paper proposes HyperDQQN, a probabilistic hypermodel-based exploration method for deep reinforcement learning (DQN). The proposed method is based on the RLSVI model, which is a non-linear neural network with a meta model. The meta model is trained using a probababilistic model of the Q-value function. The authors show that the proposed method outperforms the state-of-the-art DQN in the Atari suite. "
3807,SP:b428383660928374c953f659ea1e05852dbdcd6e,"representation learning USED-FOR model. image classification CONJUNCTION recommender systems. recommender systems CONJUNCTION image classification. representation learning USED-FOR downstream tasks. downstream tasks EVALUATE-FOR model. recommender systems HYPONYM-OF real - world scenarios. image classification HYPONYM-OF real - world scenarios. cause, effect and spurious correlated variables PART-OF representation. hypothetical causal graph USED-FOR mutual information measures. mutual information measures USED-FOR learning procedure. learning procedure USED-FOR causal representation. hypothetical causal graph USED-FOR learning procedure. observational data USED-FOR causal representation. reduced sample complexity CONJUNCTION generalization ability. generalization ability CONJUNCTION reduced sample complexity. counterfactual loss PART-OF optimization. reduced sample complexity EVALUATE-FOR causality - inspired learning. generalization ability EVALUATE-FOR causality - inspired learning. adversarial attacks CONJUNCTION distribution shift. distribution shift CONJUNCTION adversarial attacks. causal representations USED-FOR models. adversarial attacks USED-FOR models. approach USED-FOR causal representations. approach USED-FOR models. Method is learning approaches. OtherScientificTerm is features. Metric is generalizability. ","This paper proposes a method for learning causal representations from observational data. The proposed method is based on using a hypothetical causal graph to learn the mutual information measures between cause, effect and spurious correlated variables in the representation. The method is evaluated on image classification and recommender systems. The authors show that the proposed method outperforms existing methods in terms of sample complexity and generalization ability.",This paper proposes a method for learning causal representations from observational data that can be used to improve the generalization ability of downstream tasks. The proposed method is based on the notion of mutual information measures (MIMs) that are used to measure the mutual information between the observational data and the causal representation. The authors show that MIMs can reduce the sample complexity and generalizability of the model. They also show that the proposed method can be applied to adversarial attacks. 
3823,SP:1258c05a80a17949b50e6dae13deea1d2235f456,Federated learning HYPONYM-OF distributed learning scheme. edge devices USED-FOR model. training USED-FOR edge devices. gradient compression CONJUNCTION distillation. distillation CONJUNCTION gradient compression. gradient compression HYPONYM-OF compact formats. distillation HYPONYM-OF compact formats. progressive training framework USED-FOR federated learning. ProgFed HYPONYM-OF progressive training framework. It COMPARE models. models COMPARE It. asymptotic rate EVALUATE-FOR ProgFed. ResNet CONJUNCTION ConvNets. ConvNets CONJUNCTION ResNet. VGG CONJUNCTION ResNet. ResNet CONJUNCTION VGG. ConvNets CONJUNCTION U - nets. U - nets CONJUNCTION ConvNets. computation CONJUNCTION communication costs. communication costs CONJUNCTION computation. simple classification CONJUNCTION medical image segmentation. medical image segmentation CONJUNCTION simple classification. communication costs EVALUATE-FOR converged models. VGG CONJUNCTION ConvNets. ConvNets CONJUNCTION VGG. training approach USED-FOR converged models. tasks EVALUATE-FOR training approach. tasks HYPONYM-OF architectures. medical image segmentation HYPONYM-OF tasks. simple classification HYPONYM-OF tasks. computation EVALUATE-FOR training approach. communication costs EVALUATE-FOR training approach. VGG HYPONYM-OF architectures. ConvNets HYPONYM-OF architectures. ResNet HYPONYM-OF architectures. U - nets HYPONYM-OF architectures. approach COMPARE compression. compression COMPARE approach. OtherScientificTerm is limited network bandwidth. Generic is full models. ,This paper proposes a progressive training framework for federated learning with gradient compression and distillation. The main idea is to use gradient compression to compress the gradients to a compact format and then distill the compressed gradients into a compressed version of the original gradients. The authors show that gradient compression can be used to reduce the communication cost between the edge devices. They also show that distillation can also be used as a compression method to compress gradients in the distillation stage.  ,"This paper proposes ProgFed, a progressive training framework for federated learning. The main idea is to use gradient compression and distillation to reduce the communication cost between edge devices. The authors show that the proposed method outperforms the state-of-the-art in terms of asymptotic rate and communication cost. The proposed method is evaluated on a variety of tasks. "
3839,SP:8cdaa6e0dafd750ebdb5d7a4c1987a042400662f,"adversarial attacks FEATURE-OF Deep neural networks. Adversarial training USED-FOR model. adversarial Rademacher complexity FEATURE-OF adversarial training. two - layer neural networks USED-FOR adversarial Rademacher complexity. adversarial Rademacher complexity FEATURE-OF deep neural networks. Rademacher complexity EVALUATE-FOR neural nets. product of weight norms PART-OF bound. adversarially trained weight norms COMPARE trained weight norms. trained weight norms COMPARE adversarially trained weight norms. Generic are models, and method. Task is adversarial settings. OtherScientificTerm is layer. ",This paper studies the adversarial robustness of two-layer neural networks trained with adversarial training in the presence of adversarial attacks.  The authors show that the Rademacher complexity of adversarially trained weight norms is bounded by the product of weight norms in the weight norms.   The main contribution of the paper is to show that adversarial weight norms can be approximated by a two-layered neural network with two hidden layers. The authors also show that this can be done with a simple linear combination of two layers. ,"This paper studies the adversarial Rademacher complexity of two-layer neural networks with two layers. The authors show that for any two layer neural network, the adversarially trained weight norms are more robust to adversarial attacks than the weight norms trained by the original network. They also show that the product of weight norms is more robust than the original weight norms.  "
3855,SP:925d6bb051e9b384669fb695085b678c11f7c11a,Estimation of ( differential ) entropy CONJUNCTION mutual information. mutual information CONJUNCTION Estimation of ( differential ) entropy. estimators USED-FOR differential entropy. approach USED-FOR KNIFE - based estimators. KNIFE - based estimators USED-FOR mutual information. neural networks USED-FOR real - world tasks. it USED-FOR neural networks. high - dimensional synthetic data EVALUATE-FOR method. visual domain adaptation CONJUNCTION textual fair classification. textual fair classification CONJUNCTION visual domain adaptation. textual fair classification CONJUNCTION textual fine - tuning. textual fine - tuning CONJUNCTION textual fair classification. tasks EVALUATE-FOR KNIFE - based estimation. textual fine - tuning EVALUATE-FOR KNIFE - based estimation. textual fine - tuning HYPONYM-OF tasks. visual domain adaptation HYPONYM-OF tasks. textual fair classification HYPONYM-OF tasks. Method is KNIFE. ,"This paper proposes a novel method to estimate the entropy of the mutual information between the input and output of a neural network. The proposed method is based on a novel approach to estimate differential entropy. The main idea is to use a new estimator of the entropy, which is a combination of two existing estimators, namely, the so-called ""knife"" estimator, and a new mutual information estimator. The authors show that the proposed method improves the performance of the network on several tasks, including visual domain adaptation, textual fair classification and fine-tuning.","This paper proposes a novel method for estimating the differential entropy of a neural network trained on synthetic data. The key idea is to use the mutual information between the input data and the output data to estimate the entropy of the neural network. The proposed method is based on the framework of the Knife-based estimator. The method is evaluated on a variety of tasks, including visual domain adaptation, textual fair classification, and textual fine-tuning."
3871,SP:d2f3beac855f0d72c13552fecb2bdb9d42195df3,"ε - greedy CONJUNCTION softmax. softmax CONJUNCTION ε - greedy. action - value methods USED-FOR reinforcement learning. Soft - greedy operators USED-FOR exploration. exploration USED-FOR action - value methods. ε - greedy HYPONYM-OF Soft - greedy operators. softmax HYPONYM-OF Soft - greedy operators. resmax HYPONYM-OF soft - greedy operator. It USED-FOR coverage of the state - space. It USED-FOR exploration. it COMPARE softmax. softmax COMPARE it. non - expansion USED-FOR it. exploration hyperparameter USED-FOR non - expansion. mellowmax HYPONYM-OF non - expansion. state - action specific temperature USED-FOR softmax policy. resmax COMPARE softmax. softmax COMPARE resmax. resmax COMPARE ε - greedy. ε - greedy COMPARE resmax. ε - greedy CONJUNCTION softmax. softmax CONJUNCTION ε - greedy. Generic is operators. OtherScientificTerm are suboptimality gap, and overemphasizing sub - optimal actions. Task is learning. Material is tabular and deep RL. ","This paper studies the use of soft-greedy operators for exploration in reinforcement learning. In particular, the authors propose a new soft greedy operator called resmax, which they show can be used to improve the coverage of the state-space in tabular and deep RL. They show that it is equivalent to softmax in terms of the suboptimality gap. They also propose a non-expansion method called mellowmax to improve exploration.   ","This paper proposes a new soft-greedy operator, called mellowmax, which is an extension of the popular resmax. The main idea is to use a temperature-based exploration hyperparameter to encourage exploration in the state-space. The authors show that the softmax policy can outperform the resmax policy in terms of the suboptimality gap. They also show that it can be used in combination with other soft greedy operators to improve the exploration performance."
3887,SP:792ae8808aa6902758146aef1548c975492b833c,"learnability FEATURE-OF deep learning models. concept USED-FOR model. concept USED-FOR learnability. learnability lock USED-FOR model. learnability lock USED-FOR learnability. learnability EVALUATE-FOR model. learnability FEATURE-OF dataset. universal transformation function USED-FOR class - wise perturbation. class - wise perturbation USED-FOR learnability lock. inverse transformation USED-FOR learnability. visual classification tasks EVALUATE-FOR method. Task are information technology, learnability attack, and preventing unauthorized exploitation. Method are deep learning, commercial models, adversarial invertible transformation, and machine learning models. OtherScientificTerm are digital formats, and visual features. Material is image. Generic is models. ","This paper proposes a novel adversarial invertible transformation method to improve the learnability of deep learning models. The proposed method is based on the notion of learnability lock, which is the ability of a model to prevent unauthorized exploitation of its learnability. The key idea is to use a universal transformation function to learn a class-wise perturbation to the input data, and then use inverse transformation to learn the class-specific perturbations. The method is evaluated on image classification tasks and shows improved performance compared to the baselines. ",This paper proposes an adversarial invertible transformation (AI) attack to improve the learnability of deep learning models. The key idea is to use a universal transformation function that can be applied to any class-wise perturbation of the dataset. The authors show that the proposed attack can be used in conjunction with a learnability lock. The proposed attack is tested on a variety of visual classification tasks. 
3903,SP:9af10703605e620e563241e2602a50b629f3d37a,"Graph Neural Networks ( GNNs ) USED-FOR modeling relational data. node or edge features FEATURE-OF graph. features PART-OF real - world applications. approach USED-FOR missing features. approach USED-FOR graph machine learning applications. approach USED-FOR diffusion - type differential equation. graph FEATURE-OF diffusion - type differential equation. minimization of the Dirichlet energy USED-FOR approach. Feature Propagation HYPONYM-OF algorithm. approach COMPARE methods. methods COMPARE approach. missing features EVALUATE-FOR approach. common node - classification benchmarks EVALUATE-FOR approach. common node - classification benchmarks EVALUATE-FOR methods. nodes CONJUNCTION edges. edges CONJUNCTION nodes. edges PART-OF GPU. nodes FEATURE-OF graph. edges PART-OF graph. Generic are they, and equation. Material is social networks. ","This paper proposes a new method for learning missing features in graph neural networks (GNNs). The proposed method is based on a diffusion-type differential equation (DDE), which is used to estimate the Dirichlet energy of the graph. The authors show that the DDE can be expressed as a minimax minimization of the energy of a discrete function of the nodes and edges in the graph, which is then used to learn a GNN with a feature propagation algorithm.   The authors demonstrate the effectiveness of the proposed method on node classification tasks and show that it outperforms existing methods in terms of accuracy.","This paper proposes a new method for learning graph features for graph neural networks (GNNs). The key idea is to learn a diffusion-type differential equation (DDE) between the nodes and edges of a graph, and then use the DDE to learn the missing features of the graph. The proposed method is based on the Dirichlet energy minimization (DIE) method. The authors show that the proposed method outperforms the state-of-the-art GNNs on a variety of graph classification tasks."
3919,SP:cbaa3f1379fa99159899d79ccb479c0187403aca,"limited labeled data USED-FOR model. heuristics USED-FOR sample selection strategies. integer optimization problem USED-FOR core set. discrete Wasserstein distance FEATURE-OF unlabeled pool. Generalized Benders Decomposition algorithm USED-FOR problem. unlabeled pool USED-FOR unsupervised learning. unsupervised learning USED-FOR latent features. latent features USED-FOR strategy. optimization approach COMPARE baselines. baselines COMPARE optimization approach. data sets EVALUATE-FOR optimization approach. optimization approach COMPARE them. them COMPARE optimization approach. data sets EVALUATE-FOR baselines. them USED-FOR low budget regime. optimization approach USED-FOR low budget regime. Method are Active learning, and deep learning. OtherScientificTerm is unlabeled data pool. "," is an active learning problem, where the goal is to learn a model with limited labeled data. In this setting, the authors propose to use the unlabeled data pool as a core set to select samples from. The core set is defined as the set of samples that are close enough to the true data set to be useful for training the model. The authors propose a Generalized Benders Decomposition (GBD) algorithm to solve the problem. They show that GBD can be used for unsupervised learning. ","This paper proposes a generalized Bender Decomposition algorithm for unsupervised learning with unlabeled data pool. The main idea of the paper is to use a discrete Wasserstein distance between the unlabelled data pool and the core set, which is defined as an integer optimization problem. The core set consists of a set of unlabelling samples, and the goal is to find a sample selection strategy that maximizes the Wassersteins distance between them. The authors propose a Generalized Benders Decomposable algorithm for this problem. They show that the proposed method outperforms the baselines in the low budget regime."
3935,SP:4c72923f78ca6590dc11e10d1a2403076a583718,"manual inspection USED-FOR genome reconstruction. approach USED-FOR assembling genomes. method USED-FOR approach. method USED-FOR assembling genomes. geometric deep learning USED-FOR genome assembly. geometric deep learning USED-FOR assembly graph. genomic sequence USED-FOR assembly graph. graph convolutional network USED-FOR genome. dataset USED-FOR graph convolutional network. human genomic data USED-FOR graph convolutional network. human genomic data USED-FOR dataset. greedy search algorithm COMPARE greedy search. greedy search COMPARE greedy search algorithm. greedy search algorithm USED-FOR graph topology. greedy search algorithm USED-FOR model. graph machine learning algorithms USED-FOR de novo genome assembly problem. graph machine learning algorithms COMPARE human handcrafted techniques. human handcrafted techniques COMPARE graph machine learning algorithms. Material is human DNA sequence. OtherScientificTerm are telomere, and graph. Metric is assembly speed. Generic is it. Method is de novo assemblers. ",This paper proposes a method for genome assembly based on geometric deep learning. The method is based on a graph convolutional network that is trained on a dataset of human genomic data. A greedy search algorithm is used to learn the assembly graph topology. The proposed method is evaluated on a synthetic dataset and on a real-world dataset. ,This paper proposes a method for de-novo genome assembly using graph machine learning. The method is based on a graph neural network that is trained on a dataset of human genomic data. The authors propose a greedy search algorithm to find the assembly graph topology of the genome. They show that the proposed method can achieve better assembly speed than the state of the art.
3951,SP:24de906e4289c9073b6c55c747b0913b8df5e053,"catastrophic forgetting FEATURE-OF Continual learning. meta - learning USED-FOR metacontinual learning algorithms. experience replay ( ER ) PART-OF meta - testing. ER USED-FOR meta - testing. ER USED-FOR metatraining. ER USED-FOR continual learning representations. ER PART-OF meta - training. reservoir sampling USED-FOR replay buffer. meta - learned Predictive Sample Selection USED-FOR replay buffer. meta - learned Predictive Sample Selection COMPARE reservoir sampling. reservoir sampling COMPARE meta - learned Predictive Sample Selection. method COMPARE state - of - the - art. state - of - the - art COMPARE method. clustering structures FEATURE-OF learned representations. Method are online aware meta - learning ( OML ), and OML. Generic is model. Task is online - aware nature of OML. OtherScientificTerm is randomness. ","This paper proposes an online-aware meta-learning method for continual learning in the presence of catastrophic forgetting. The proposed method is based on experience replay (ER) in meta-testing, where experience replay is used for meta-training. Theoretical analysis is provided to show that ER can be used to improve the performance of continual learning. Empirical results are provided to demonstrate the effectiveness of the proposed method.   ",This paper proposes a meta-learning method for continual learning with experience replay (ER) in meta-testing. The main idea is to use experience replay as part of the meta-training process and use the experience replay buffer as a replay buffer to sample from the replay buffer. Experiments show that the proposed method outperforms the state-of-the-art in terms of performance. 
3967,SP:3c78454f053f74930979a8054cd7c8a34b6fe63d,"Multi - agent joint Q - learning USED-FOR multi - agent cooperation. Centralized Training with Decentralized Execution ( CTDE ) USED-FOR Multi - agent joint Q - learning. methods USED-FOR multi - agent credit assignment problem. Bellman optimality equation FEATURE-OF joint Q - value. Bellman optimality FEATURE-OF joint Q - value. Q - values USED-FOR joint Q - value. gradient ascent solution USED-FOR problem. ECAQ COMPARE baselines. baselines COMPARE ECAQ. ECAQ USED-FOR credit assignment. Method are centralized training, and deep neural networks. Task are explicit credit assignment problem, and multi - agent cooperation in complex problems. OtherScientificTerm is time horizon. ","This paper proposes a method for multi-agent joint Q-learning in credit assignment problems. The proposed method is based on centralized training with decentralized execution (CTDE). The main contribution of the paper is to show that the Bellman optimality of the joint Q value of the Q-values of the agents can be expressed as a function of the time horizon, and that the gradient ascent solution of the problem can be solved by gradient ascent. The paper also shows that the proposed method can be used to solve the credit assignment problem.  ",This paper proposes a new method for multi-agent joint Q-learning with centralized training with decentralized execution (CTDE). The main idea is to use the Bellman optimality equation for the joint Q value of the Q-values of the agents. The authors prove that the joint value of Q-value is optimal under the time horizon. They also provide a gradient ascent solution for solving the problem. 
3983,SP:0d2b225ac697679d10df25f371b2a718d4949b42,"transductive learning USED-FOR adversarial robustness. defenses COMPARE defense mechanisms. defense mechanisms COMPARE defenses. defenses USED-FOR bilevel optimization problem. test - time input USED-FOR model. threat analysis perspective EVALUATE-FOR defense mechanisms. threat models USED-FOR transductive - learning based defenses. attacking model space USED-FOR bilevel attack objectives. Greedy Model Space Attack ( GMSA ) HYPONYM-OF attack framework. GMSA USED-FOR transductive - learning based defenses. weak instantiations USED-FOR GMSA. Material is NeurIPS 2020. Task are ICML 2020, and adaptive attacks. Method are transductivelearning based defenses, and transductive adversarial training. OtherScientificTerm is AutoAttack. Metric is robustness. ","This paper studies adversarial robustness of neural networks in the presence of adversarial attacks. The authors propose a new adversarial attack framework, called Greedy Model Space Attack (GMSA), which is a bilevel optimization problem with weak instantiations of the attack objective. They show that GMSA can be used to train adversarial defenses that are robust to adversarial perturbations in the test-time input. They also show that the proposed GMSA is more robust than AutoAttack.","This paper proposes a new attack framework called Greedy Model Space Attack (GMSA) to improve the robustness of transductive learning-based defenses against adversarial attacks. GMSA is a bilevel attack framework that aims to attack the model space of the attacker, which is defined as the space in which the attacker can attack the test-time input of the model. The authors show that GMSA can be used to improve robustness against weak instantiations of the attack objective. They also provide a theoretical analysis of GMSA and show that it can be applied to a variety of defense mechanisms."
3999,SP:e7024cae196fc5eb6a62d289a95d76b532b6a36c,"batch normalization USED-FOR training of neural networks. batch renormalization USED-FOR small minibatches. function class FEATURE-OF inference model. Method are neural networks, and per - example training procedure. OtherScientificTerm are gradient, and identity shortcuts. Generic are approximation, and normalization. Metric are training step computation, and model accuracy. ",This paper studies the effect of batch normalization on the training of deep neural networks. The authors show that batch renormalization can be used to reduce the number of training steps in a per-example training procedure. They show that this can lead to a reduction in the training step computation and improve the accuracy of the model.   ,"This paper studies the effect of batch renormalization (batch normalization) on the training of neural networks. The authors consider the problem of training a neural network with minibatches, where the minibatch size is small and the training procedure is per-example. They show that batch normalization can be used to reduce the training step computation, and that it can improve the accuracy of the model. The main contribution of the paper is a theoretical analysis of the impact of batch normalisation on the model accuracy."
4015,SP:4aa42984fcb0fd66936d668477b2719ef5c427d4,large - scale pretraining CONJUNCTION adaptation. adaptation CONJUNCTION large - scale pretraining. general domain data USED-FOR large - scale pretraining. large - scale pretraining PART-OF natural language processing. fine - tuning USED-FOR models. trainable parameters USED-FOR downstream tasks. trainable rank decomposition matrices PART-OF Transformer architecture. LoRA HYPONYM-OF Low - Rank Adaptation. GPT-3 175B COMPARE LoRA. LoRA COMPARE GPT-3 175B. LoRA USED-FOR trainable parameters. GPU memory requirement EVALUATE-FOR LoRA. Adam USED-FOR GPT-3 175B. LoRA COMPARE finetuning. finetuning COMPARE LoRA. GPT-2 CONJUNCTION GPT-3. GPT-3 CONJUNCTION GPT-2. RoBERTa EVALUATE-FOR finetuning. model quality EVALUATE-FOR finetuning. model quality EVALUATE-FOR LoRA. Method is fine - tuned models. Metric is training throughput. OtherScientificTerm is inference latency. Task is language model adaptation. ,This paper proposes a low-rank adaptation method for language model training and adaptation. The proposed method is based on the idea of trainable rank decomposition matrices in the Transformer architecture. Theoretical analysis is provided to show that the proposed method can reduce the number of parameters and inference latency. Experiments are conducted on GPT-3 and RoBERTa datasets.  ,This paper proposes a low-rank adaptation (LoRA) method for fine-tuning language models. The main idea is to use the rank decomposition matrices in the Transformer architecture to train the model to adapt to different downstream tasks. The authors show that LoRA can be used to reduce the memory requirement of GPT-3 175B by reducing the number of parameters required for training the model. They also show that the proposed method can improve the performance of the model compared to finetuning.
4031,SP:b77a00beb0802f47810b03d3c4aa24d92781414f,"CRFs USED-FOR distributions with nonlocal dependencies. nonlocal constraints FEATURE-OF CRFs. global arity constraints HYPONYM-OF nonlocal constraints. CRFs USED-FOR constraints. nonlocal ones HYPONYM-OF constraints. regular - constrained CRF ( RegCCRF ) COMPARE CRF. CRF COMPARE regular - constrained CRF ( RegCCRF ). RegCCRFs COMPARE models. models COMPARE RegCCRFs. constraints PART-OF training. constraints FEATURE-OF decoding. constraints PART-OF RegCCRFs. constrained training COMPARE constrained decoding. constrained decoding COMPARE constrained training. deep neural model USED-FOR semantic role labeling. RegCCRF PART-OF deep neural model. RegCCRF USED-FOR semantic role labeling. dataset EVALUATE-FOR RegCCRF. RegCCRF USED-FOR downstream tasks. Task is structured prediction. OtherScientificTerm are local dependencies, CRF ’s Markov assumption, and output structures. Generic is it. ","This paper proposes a novel constraint-based convolutional function (CRF) that is able to handle distributions with nonlocal dependencies. The proposed method is based on CRFs with global arity constraints, which are non-local constraints that can be added to the input distribution. The authors show that the proposed method can be used in conjunction with regular CRFs to achieve better performance on downstream tasks.  ","This paper proposes a new regular-constrained CRF (RegCCRF) that can be used to train deep neural models with non-local constraints. RegCCRF is based on the notion of global arity constraints, which can be applied to any CRF with local constraints. The authors show that it can be combined with other CRF-based methods, such as constrained CRF, to improve the performance of downstream tasks such as semantic role labeling."
4047,SP:74c186a96c12adff178264aa84ace8d04dc7d725,"preprocessing steps USED-FOR methods. computational budget EVALUATE-FOR core ” network. normalization CONJUNCTION color space transformation. color space transformation CONJUNCTION normalization. segmentation CONJUNCTION normalization. normalization CONJUNCTION segmentation. face detection CONJUNCTION segmentation. segmentation CONJUNCTION face detection. neural models USED-FOR camera - based physiological measurement. color space transformation CONJUNCTION preprocessing steps. preprocessing steps CONJUNCTION color space transformation. face detection USED-FOR neural models. segmentation HYPONYM-OF neural models. normalization HYPONYM-OF neural models. preprocessing steps PART-OF neural models. EfficientPhys HYPONYM-OF neural models. raw video frames USED-FOR models. accuracy EVALUATE-FOR models. latency EVALUATE-FOR networks. efficiency EVALUATE-FOR light weight network. Task are Camera - based physiological measurement, and replication. Method are endto - end ” models, and transformer or convolutional backbone. Generic is operations. ","This paper proposes a method to train end-to-end neural models for camera-based physiological measurement. The proposed method is based on a combination of normalization, color space transformation, and segmentation. The authors show that the proposed method can achieve state-of-the-art performance in terms of accuracy, latency, and computational cost.   ","This paper proposes a new method for end-to-end physiological measurement of the human body. The proposed method, EfficientPhys, is based on a combination of three preprocessing steps: segmentation, normalization, and color space transformation. The authors show that the proposed method is able to achieve state-of-the-art performance in terms of accuracy, latency, and computational budget. They also show that their method can be used in combination with other methods, such as face detection and segmentation."
4063,SP:3003bab6e3f7e2e21cd6cf27ee7d483d877d9fb3,"Structural pruning USED-FOR network architecture. inference speed EVALUATE-FOR Structural pruning. Hardware - Aware Latency Pruning ( HALP ) USED-FOR structural pruning. global resource allocation optimization problem USED-FOR structural pruning. latency reduction potential CONJUNCTION global saliency score. global saliency score CONJUNCTION latency reduction potential. latency lookup table USED-FOR latency reduction potential. latency lookup table USED-FOR global saliency score. HALP USED-FOR accuracy drop. HALP USED-FOR global saliency score. HALP USED-FOR latency reduction potential. latency lookup table USED-FOR HALP. HALP USED-FOR filter importance ranking. latency lookup table USED-FOR filter importance ranking. metrics EVALUATE-FOR pruning. metrics USED-FOR global structural pruning. reward maximization problem USED-FOR global structural pruning. pruning efficacy CONJUNCTION accuracy - efficiency trade - off. accuracy - efficiency trade - off CONJUNCTION pruning efficacy. pruning efficacy EVALUATE-FOR HALP. accuracy - efficiency trade - off EVALUATE-FOR HALP. augmented knapsack solver USED-FOR problem. HALP USED-FOR classification and detection tasks. ImageNet and VOC datasets USED-FOR classification and detection tasks. ImageNet and VOC datasets EVALUATE-FOR HALP. ImageNet USED-FOR ResNet-50/-101 pruning. HALP USED-FOR ResNet-50/-101 pruning. network throughput EVALUATE-FOR HALP. HALP USED-FOR SSD pruning. VOC EVALUATE-FOR SSD pruning. throughput EVALUATE-FOR HALP. HALP COMPARE prior art. prior art COMPARE HALP. Metric is accuracy. OtherScientificTerm are latency, and top-1 accuracy changes. ","This paper proposes Hardware-Aware Latency Pruning (HALP), a method to prune layers in a network architecture. The method is based on a global resource allocation optimization problem, where the goal is to find the most important layers in the network and prune them in a way that minimizes the latency drop. The paper proposes to use a latency-aware lookup table to compute the importance ranking of each layer and then prune the nodes that have the highest importance ranking. The proposed method is evaluated on ImageNet and VOC and achieves state-of-the-art performance. ","This paper proposes Hardware-Aware Latency Pruning (HALP), a new method for structural pruning. The main idea of HALP is to use a global resource allocation optimization problem to optimize the global saliency score, which is a weighted sum of the top-1 accuracy changes and the latency reduction potential. The paper also proposes an augmented knapsack solver to solve the problem. Experiments on ImageNet and VOC datasets show that HALP can improve the accuracy-efficiency trade-off between the accuracy drop and the performance drop."
4079,SP:c44d676c09c8e5a70d73b21b507b41a422fec809,"permutation invariance CONJUNCTION multi - objective generation. multi - objective generation CONJUNCTION permutation invariance. GraphEBM HYPONYM-OF molecular graph generation method. GraphEBM USED-FOR permutation invariant and multi - objective molecule generation. energy - based models ( EBMs ) USED-FOR molecular graph generation method. GraphEBM USED-FOR permutation invariant distribution. EBMs CONJUNCTION parameterized permutation - invariant energy function. parameterized permutation - invariant energy function CONJUNCTION EBMs. parameterized permutation - invariant energy function USED-FOR GraphEBM. molecular graphs FEATURE-OF permutation invariant distribution. contrastive divergence USED-FOR energy function. compositional generation USED-FOR drug discovery. compositional generation USED-FOR GraphEBM. Task is molecular graph generation. Method are Langevin dynamics, and learning strategy. OtherScientificTerm is flexible degrees. Generic is method. ",This paper proposes a new method for molecular graph generation based on energy-based models (EBMs). The main idea is to learn a permutation-invariant energy function that is parameterized by the permutation invariance of the underlying molecular graph. The authors show that the proposed method is able to learn the energy function in a way that allows it to be used for multi-objective generation. The method is evaluated on synthetic and real-world data sets.,This paper proposes a new method for molecular graph generation based on energy-based models (EBMs) and Langevin dynamics (Langevin dynamics). The main idea is to learn a parameterized permutation-invariant energy function (GPE) that is permutation invariant and multi-objective. The authors show that the GPE can be used to generate molecular graphs with different degrees of permutation. They also propose a contrastive divergence-based learning strategy to improve the performance of GPE.
4095,SP:70e60fa5deef3e3ba77d05d0c3e0e7fbf396aa1d,"approaches USED-FOR program synthesis. neural models USED-FOR combinatorial search algorithms. neural model USED-FOR hands - on search policy. hands - on search policy USED-FOR bottom - up synthesis. search history CONJUNCTION partial program executions. partial program executions CONJUNCTION search history. neural model USED-FOR approach. bottom - up searches USED-FOR data. - policy USED-FOR CROSSBEAM. data USED-FOR - policy. data USED-FOR CROSSBEAM. string manipulation CONJUNCTION logic programming. logic programming CONJUNCTION string manipulation. string manipulation EVALUATE-FOR CROSSBEAM. domains EVALUATE-FOR CROSSBEAM. string manipulation HYPONYM-OF domains. logic programming HYPONYM-OF domains. OtherScientificTerm are search space, search space blowup, and program space. Method is combinatorial search algorithm. Task is structured prediction. Generic is state - of - the - art. ","This paper proposes a combinatorial search algorithm called CROSSBEAM for structured prediction. The main idea is to use a neural model to model the search history and partial program executions, and then use a bottom-up search policy to generate the data. The proposed method achieves state-of-the-art performance on string manipulation and logic programming tasks. ","This paper proposes a new combinatorial search algorithm called CROSSBEAM for program synthesis. The main idea is to use a top-down search policy to generate a set of data points that can be used for the bottom-up search. The search policy is trained using a neural network to generate the data points, which are then used to train the search policy. The proposed method is evaluated on string manipulation, logic programming, and other domains."
4111,SP:daa044ffefe80bae16b014f60061d941ed8c2ba6,minimizing the squared Bellman error USED-FOR Deep Reinforcement Learning. target networks USED-FOR training. lagging parameters USED-FOR target networks. functional regularizer USED-FOR squared Bellman error. target networks COMPARE regularization. regularization COMPARE target networks. up - to - date parameters USED-FOR regularization. Atari environments EVALUATE-FOR target - network based methods. sample efficiency CONJUNCTION performance. performance CONJUNCTION sample efficiency. performance EVALUATE-FOR target - network based methods. sample efficiency EVALUATE-FOR target - network based methods. approach COMPARE squared Bellman error. squared Bellman error COMPARE approach. OtherScientificTerm is fast - changing target Q - values. Method is training method. ,"This paper studies the problem of minimizing the squared Bellman error in deep reinforcement learning (DRL) with target Q-values. The authors propose to use lagging parameters to regularize the training of the target network. They show that this regularization is equivalent to a functional regularizer, and that it can be used to improve the sample efficiency of target-network-based DRL methods. They also show that the proposed method achieves better sample efficiency compared to target-networks-based methods.",This paper proposes a new method to reduce the squared Bellman error in deep reinforcement learning. The main idea is to use a functional regularizer to improve the sample efficiency of training the target networks. The proposed method is based on the fact that the target Q-values are fast-changing and the training parameters are not always up-to-date. The authors show that the proposed method can achieve better sample efficiency and better performance compared to the state of the art.
4127,SP:dd174014d056a7d2bc86ee99119841eafa62ed52,"neighborhood subgraphs FEATURE-OF hierarchy of local isomorphism. message - passing GNNs COMPARE Weisfeiler Lehman test. Weisfeiler Lehman test COMPARE message - passing GNNs. model COMPARE Weisfeiler Lehman test. Weisfeiler Lehman test COMPARE model. model USED-FOR graph structures. Weisfeiler Lehman test USED-FOR graph structures. GraphSNN HYPONYM-OF neural model. graph learning tasks EVALUATE-FOR model. model COMPARE state - of - the - art methods. state - of - the - art methods COMPARE model. benchmark tasks EVALUATE-FOR state - of - the - art methods. benchmark tasks EVALUATE-FOR model. Method are Graph Neural Networks ( GNNs ), and GNNs. OtherScientificTerm is structural properties of graphs. ","This paper proposes a new method for learning graph structure using message-passing GNNs. The proposed method is based on the Weisfeiler Lehman test, which is used to evaluate the similarity between subgraphs in a graph. The authors show that the proposed method outperforms existing methods on several graph learning tasks. ","This paper proposes a new neural network architecture for graph learning. The main idea is to use the Weisfeiler Lehman test to learn the structure of a graph. The key idea of the paper is to learn a hierarchy of local isomorphism, where each node in the hierarchy is represented by a subgraph, and the subgraphs are represented as a set of nodes in a hierarchy. The subgraph is then used to train a graph neural network (GraphSNN) that can learn the graph structure. The proposed method is evaluated on a variety of graph learning tasks, and it outperforms state-of-the-art methods. "
4143,SP:beb9ba0261e176bfc50e9bf5bed2b6169d388285,"prediction interval ( PI ) method USED-FOR uncertainty quantification. retraining of neural networks ( NNs ) USED-FOR confidence level. retraining of neural networks ( NNs ) USED-FOR PI methods. fine tuning USED-FOR well - calibrated PI. sensitive hyperparameters FEATURE-OF customized loss functions. customized loss functions USED-FOR they. PI3NN method USED-FOR PIs. standard mean squared error loss USED-FOR NNs. root - finding algorithms USED-FOR PIs. root - finding algorithms USED-FOR linear combinations. PI3NN USED-FOR PIs. it USED-FOR crossing issue. OOD samples COMPARE in - distribution samples. in - distribution samples COMPARE OOD samples. initialization scheme USED-FOR PIs. PIs FEATURE-OF OOD samples. initialization scheme USED-FOR OOD samples. initialization scheme USED-FOR in - distribution samples. initialization scheme USED-FOR OOD identification challenge. predictive uncertainty quality CONJUNCTION robustness. robustness CONJUNCTION predictive uncertainty quality. method COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE method. robustness CONJUNCTION OOD samples identification. OOD samples identification CONJUNCTION robustness. OOD samples identification EVALUATE-FOR method. robustness EVALUATE-FOR state - of - the - art approaches. robustness EVALUATE-FOR method. predictive uncertainty quality EVALUATE-FOR state - of - the - art approaches. predictive uncertainty quality EVALUATE-FOR method. OtherScientificTerm are over - confident PIs, confidence levels, and hyperparameters. Method is retraining NNs. ",This paper proposes a new method for uncertainty quantification in prediction intervals (PI) methods. The method is based on a root-finding algorithm to find PIs that are close to the true PIs in the training set. The main idea is to use the standard mean squared error loss to estimate the confidence level of the PIs. The authors show that this method is robust to out-of-distribution (OOD) samples and can identify OOD samples better than existing methods. ,This paper proposes a new method for predicting the prediction interval (PI) of a neural network (NN) trained with a standard mean squared error (SSE) loss. The method is based on a root-finding algorithm (PI3NN) that uses a linear combination of linear combinations of the prediction intervals (PIs) to identify the OOD samples. The authors show that their method can identify the in-distribution samples more accurately than the out-of-distributions (OD) samples. They also show that the method is robust to cross-entropy.
4159,SP:4b44a834e2212bacb4c2d9408a81f1efc76a670b,"deep networks USED-FOR functions. classifiers CONJUNCTION detectors. detectors CONJUNCTION classifiers. detectors CONJUNCTION trackers. trackers CONJUNCTION detectors. deep networks USED-FOR classifiers. models USED-FOR applications. trackers HYPONYM-OF functions. classifiers HYPONYM-OF functions. detectors HYPONYM-OF functions. learning algorithms USED-FOR slow adaptation. gradient descent HYPONYM-OF learning algorithms. learning algorithms USED-FOR model. Meta - learning USED-FOR adaptation. metalearning USED-FOR online problems. meta - learning USED-FOR online setting. known ground - truth task boundaries FEATURE-OF discrete notion of tasks. discrete notion of tasks USED-FOR they. discrete boundaries PART-OF real - world settings. ground truth knowledge FEATURE-OF task boundaries. FOML COMPARE online learning methods. online learning methods COMPARE FOML. Rainbow - MNIST, and CIFAR100 datasets EVALUATE-FOR online learning methods. Rainbow - MNIST, and CIFAR100 datasets EVALUATE-FOR FOML. OtherScientificTerm are input distributions, and environmental conditions. Method are intelligent system, and Fully Online MetaLearning ( FOML ) algorithm. Task is complex and high - dimensional problems. Generic is methods. ","This paper proposes a meta-learning algorithm for online learning of classifiers and detectors. The main idea is to learn a classifier and a detector in an online setting, where the task boundaries are known and the environment is known. The proposed method is based on meta learning, which is a generalization of meta learning to online problems. The method is evaluated on the Rainbow-MNIST and CIFAR-100 datasets.   ","This paper proposes a new meta-learning method for online learning. The main idea of the paper is to use the idea of metalearning, which is a method for learning a model that can be applied to a large number of tasks in an online setting. The method is based on the notion of discrete notion of tasks, where each task is defined as a set of tasks with discrete boundaries, and the goal is to learn a classifier that can perform well at each of these tasks. The proposed method is evaluated on Rainbow-MNIST and CIFAR-100 datasets. "
4175,SP:fbae35cb171b3a3eb7c5d4bc83881ed7c4a70aae,"chemical science and engineering task USED-FOR applications. structural design of functional molecules HYPONYM-OF chemical science and engineering task. molecular optimization HYPONYM-OF structural design of functional molecules. drug discovery HYPONYM-OF applications. Deep generative models CONJUNCTION combinatorial optimization methods. combinatorial optimization methods CONJUNCTION Deep generative models. knowledge network USED-FOR discrete chemical structures. knowledge network USED-FOR differentiable scaffolding tree ( DST ). DST USED-FOR gradient - based optimization. chemical graph structure USED-FOR gradient - based optimization. Method are brute - force enumeration, graph neural network ( GNN ), and gradient - based molecular optimizations. OtherScientificTerm are molecule structures, locally differentiable ones, derivatives, graph parameters, and domain experts. ",This paper proposes a novel method for learning discrete chemical structures from graph data. The method is based on a knowledge network that learns to predict the derivatives of the graph parameters and then uses a differentiable scaffolding tree (DST) to construct a graph structure that can be used for gradient-based optimization. The proposed method is shown to outperform the state-of-the-art methods for molecular optimization.   ,This paper proposes a novel method to learn the structure of a chemical graph by using a graph neural network (GNN) and a differentiable scaffolding tree (DST). The DST can be used to perform gradient-based molecular optimization (GBO) on the chemical graph structure. The authors show that the DST is locally differentiable and can be applied to a variety of differentiable structures. They also show that their method can be combined with combinatorial optimization methods and deep generative models.
4191,SP:61b59899cf6ae442d9f8f5226e79708a4280cfb2,"diseases CONJUNCTION lab tests. lab tests CONJUNCTION diseases. patient representation USED-FOR sequential information. drug - lab interactions CONJUNCTION diagnosis - lab interactions. diagnosis - lab interactions CONJUNCTION drug - lab interactions. graphs USED-FOR drug - lab interactions. graphs USED-FOR diagnosis - lab interactions. real - world datasets EVALUATE-FOR solution. prediction errors EVALUATE-FOR solution. Method are Personalized medical systems, and knowledge - augmented approach. OtherScientificTerm is lab test responses. ","This paper proposes a knowledge-augmented approach to improve the prediction accuracy of personalized medical systems. The proposed method is based on the observation that the prediction error of a medical system can be improved by incorporating information from multiple sources, e.g. patient data and lab data. The method is evaluated on two real-world datasets.   ","This paper proposes a knowledge-augmented medical system for predicting patient responses to lab test responses. The proposed method is based on the idea that the patient representation should be able to capture the sequential information of the patient and lab interactions. The method is evaluated on a variety of real-world datasets, and it is shown that the proposed method outperforms the baselines."
4207,SP:8623cebb515c4a736427449b46ad2cdf8b806b77,"Single domain generalization ( SDG ) HYPONYM-OF domain generalization. diversity of source domain USED-FOR robust model. adversarial data augmentation strategy USED-FOR SDG methods. OS - SDG USED-FOR model. CrossMatch approach USED-FOR SDG methods. SDG methods USED-FOR identifying unknown classes. multi - binary classifier USED-FOR SDG methods. adversarial data augmentation strategy USED-FOR CrossMatch. model USED-FOR unknown class identification. multibinary classifiers CONJUNCTION model. model CONJUNCTION multibinary classifiers. consistency regularization USED-FOR auxiliary samples. consistency regularization USED-FOR model. SDG methods USED-FOR model. CrossMatch USED-FOR SDG methods. benchmark datasets EVALUATE-FOR CrossMatch. SDG methods USED-FOR OS - SDG setting. benchmark datasets EVALUATE-FOR SDG methods. OtherScientificTerm are label space, source label space, target domains, and unknown classes. Task is real - world applications. ","This paper proposes a new method for single domain generalization (SDG) based on cross-domain generalization. The proposed method is based on the CrossMatch approach, which uses adversarial data augmentation to improve the robustness of the source domain. The method is evaluated on several benchmark datasets and achieves state-of-the-art performance. ",This paper proposes a new approach for single domain generalization (SDG) based on cross-domain matching (CrossMatch). CrossMatch is an extension of the multi-binary classifier (MBC) approach for identifying unknown classes in the source domain. The authors propose a new adversarial data augmentation strategy to improve the performance of CrossMatch. They show that CrossMatch outperforms the state-of-the-art methods on a variety of benchmark datasets.
4223,SP:126f8ffb855aa22eda4d681a499953879ed3679e,Trust - region methods USED-FOR policy optimization. policy optimization USED-FOR reinforcement learning. Kullback - Leibler divergence USED-FOR Trust - region methods. Wasserstein policy optimization ( WPO ) CONJUNCTION Sinkhorn policy optimization ( SPO ). Sinkhorn policy optimization ( SPO ) CONJUNCTION Wasserstein policy optimization ( WPO ). Wasserstein and Sinkhorn trust regions FEATURE-OF policy optimization. parametric distribution class FEATURE-OF policy. Lagrangian duality USED-FOR close - form policy updates. SPO COMPARE WPO. WPO COMPARE SPO. monotonic performance improvement FEATURE-OF WPO. robotic locomotion tasks EVALUATE-FOR approaches. tabular domains CONJUNCTION robotic locomotion tasks. robotic locomotion tasks CONJUNCTION tabular domains. SPO COMPARE policy gradient methods. policy gradient methods COMPARE SPO. tabular domains EVALUATE-FOR approaches. approaches COMPARE policy gradient methods. policy gradient methods COMPARE approaches. sample insufficiency FEATURE-OF WPO. OtherScientificTerm is policy distribution. Method is entropic regularizer. ,This paper proposes two trust-region methods for policy optimization in reinforcement learning. The first method is based on the Kullback-Leibler divergence between the policy distribution and a parametric distribution class. The second method uses the Wasserstein and Sinkhorn trust regions to approximate the policy gradient. The proposed method is shown to be monotonic and monotonically better than existing trust region methods.  ,This paper proposes a new trust-region method for policy optimization in reinforcement learning. The key idea is to use the Kullback-Leibler divergence between the policy distribution and the trust region. The authors show that this divergence can be used to improve the performance of Wasserstein policy optimization and Sinkhorn policy optimization (SPO). They also show that SPO can achieve monotonic performance improvement over WPO. 
4239,SP:999eacf6500c87205584a3256d7ca45b3016fb1c,"forgetting USED-FOR learning. learning trajectories FEATURE-OF artificial neural networks. forget - and - relearn USED-FOR learning trajectories. relearning step USED-FOR features. forgetting step USED-FOR undesirable information. forget - and - relearn framework USED-FOR iterative training algorithms. image classification FEATURE-OF iterative training algorithms. forgetting operations USED-FOR algorithms. iterative training PART-OF neural networks. OtherScientificTerm are Forgetting, and disproportionate forgetting of undesirable information. Task is human and machine learning. Generic is model. ","This paper studies the problem of forgetting in the context of iterative training of deep neural networks. The authors propose a forgetting-and-reluar framework to address this problem. They show that the forgetting step is responsible for the forgetting of undesirable information, and propose a new forgetting operation to remove the undesirable information from the training data. They also show that this forgetting operation is beneficial in terms of improving the performance of the model. ","This paper proposes a new forgetting-and-relunder framework for learning trajectories for neural networks. The authors show that the forgetting of undesirable information during training can lead to disproportionate forgetting of desirable information. The forgetting step of the forgetting step is the one that is most likely to cause undesirable information to be forgotten, and the relearning step is one of the most likely ones to cause the undesirable information. They show that this forgetting step can be used to improve the performance of an iterative training algorithm for image classification.  "
4255,SP:2789859517b6624730b14a7e010444a72d3dd3ed,"sufficient coverage CONJUNCTION policy. policy CONJUNCTION sufficient coverage. RL agents COMPARE agents. agents COMPARE RL agents. offline - online manner USED-FOR RL agents. Method are Batch RL, batch RL agents, and batch RL. OtherScientificTerm are data - collection process, and agent. Task is offline - online setting. Generic is setting. ","This paper studies the offline-online batch RL setting, where the data is collected offline and the goal is to learn a policy that maximizes the coverage of the training set. The authors show that batch RL agents with sufficient coverage and sufficient policy can outperform RL agents that do not have sufficient coverage. They show that this is the case for both offline and online settings. They also show that in offline settings, batch RL can be used to improve the performance of RL agents.","This paper proposes a new offline-online setting for batch RL, where the data-collecting process is performed offline, and the agent is trained in an offline manner. The authors provide a theoretical analysis of the performance of batch RL agents in this setting. They show that the agent can outperform the state-of-the-art in terms of both coverage and policy coverage. They also provide an empirical evaluation of the effectiveness of the proposed method. "
4271,SP:76625a25e770415599a34122110d61cb3b7e614c,learning USED-FOR domain shift. episodic training procedure USED-FOR learning. episodic training procedure USED-FOR domain generalization ( DG ). learning USED-FOR domain generalization ( DG ). episodic training procedure USED-FOR domain shift. Y - discrepancy USED-FOR domain shift. source - domain samples USED-FOR Y - discrepancy. ERM CONJUNCTION domain - invariant learning. domain - invariant learning CONJUNCTION ERM. PAC - style generalization bound USED-FOR discrepancyoptimal meta - learning. PAC - style generalization bound COMPARE DG bounds. DG bounds COMPARE PAC - style generalization bound. domain - invariant learning HYPONYM-OF DG bounds. ERM HYPONYM-OF DG bounds. computational complexity EVALUATE-FOR discrepancy - optimal meta - learning. classification EVALUATE-FOR discrepancy - optimal meta - learning. classification CONJUNCTION computational complexity. computational complexity CONJUNCTION classification. bilevel optimization algorithm USED-FOR DG. DomainBed EVALUATE-FOR algorithm. DG benchmarks EVALUATE-FOR algorithm. ,"This paper studies the problem of domain generalization (DG) in the presence of domain shift. In particular, the authors propose to use the Y-divergence between source and target domains as a metric to measure the discrepancy between the two domains. The authors show that this metric can be used as a generalization bound for meta-learning, and derive a bilevel optimization algorithm to solve the problem. The proposed method is shown to be efficient in terms of computational complexity.","This paper proposes a generalization bound for discrepancy-optimal meta-learning for domain generalization (DG). The generalization bounds are based on the notion of Y-divergence, which is defined as the difference between the number of source-domain samples and the total number of samples from the source domain. The authors show that the generalization of the bound is bounded by a bilevel optimization algorithm. They also provide a theoretical analysis of the performance of the proposed bounds."
4287,SP:6421a9759c766641fd8c128a249f1a9c5699d19c,"deep reinforcement learning USED-FOR two - player games. combinatorial search methods USED-FOR NP - complete domains. SAT CONJUNCTION CSP. CSP CONJUNCTION SAT. deep reinforcement learning USED-FOR combinatorial search methods. Go HYPONYM-OF two - player games. CSP HYPONYM-OF NP - complete domains. SAT HYPONYM-OF NP - complete domains. exponential combinatorial search space FEATURE-OF hard instances. best - first search CONJUNCTION Monte Carlo tree search. Monte Carlo tree search CONJUNCTION best - first search. Monte Carlo tree search HYPONYM-OF search methods. best - first search HYPONYM-OF search methods. methods USED-FOR hard planning instances. policy and value networks USED-FOR DNN - based best - first search. Sokoban domain EVALUATE-FOR DNN - based best - first search. value network USED-FOR policy network. cost distribution FEATURE-OF search algorithms. heavy - tailed runtime distributions FEATURE-OF Sokoban planning instances. abstract tree model USED-FOR tails. policy network USED-FOR search. polynomial scaling FEATURE-OF left heavy tails. random restart strategies USED-FOR DNN - based search. random restart strategies USED-FOR combinatorial solvers. DNN - based search USED-FOR left and right heavy tails. Task is PSPACE - hard planning problems. Method is domain - specific solvers. Generic are specialized solvers, approaches, and model. OtherScientificTerm is exponentially sized sub - trees. ",This paper studies PSPACE-hard planning problems in the Sokoban domain. The authors propose to use deep reinforcement learning to learn a policy and a value network to search for hard instances in the exponential combinatorial search space for PSPACE hard planning problems with heavy tails. The main contribution of the paper is to show that DNN-based best-first search and Monte Carlo tree search can be used to solve hard instances of PSPACE problems in a polynomial time.  ,"This paper proposes a new approach for solving PSPACE-hard planning problems in the Sokoban domain. The main idea is to use a DNN-based best-first search method to find the optimal solution in the exponential combinatorial search space. The authors show that their approach outperforms the state-of-the-art methods in terms of the number of sub-trees in the search space, and the performance of their approach is comparable to the state of the art.  "
4303,SP:84c415bc0f120d1997289f91661ff74e7297d3bd,"approach USED-FOR meta - policy. adaptive loss USED-FOR meta - policy. human videos USED-FOR approach. method COMPARE baseline. baseline COMPARE method. vision - based tasks EVALUATE-FOR method. Method are Meta - Imitation Learning, and meta - imitation learning. OtherScientificTerm are human demonstrations, robot demonstrations, robot demonstration, and human imitation behavior. Generic is it. Task are meta - training phase, and data collection. ",This paper proposes a meta-imitation learning method that uses human videos to train a policy to imitate human demonstrations. The proposed method is based on the observation that human videos can be used to improve the performance of the meta-policy. The method is evaluated on a variety of vision-based tasks.   ,"This paper proposes a meta-imitation learning approach for the problem of imitation learning. The authors propose a new meta-training framework for imitation learning, where the goal is to learn a policy that is able to imitate human behavior in the presence of a dataset of human videos. The proposed method is based on the idea of meta-learning, which is to train a policy to imitate the behavior of humans in the dataset. The method is evaluated on a variety of vision-based tasks, and it is shown that the proposed method outperforms the baselines. "
4319,SP:fedf5c75e83d6ab41ef9d5daa9054ffe4e424ec2,"Over - parameterized deep networks USED-FOR classification and ranking problems. gradient - based optimizers USED-FOR Over - parameterized deep networks. weight space FEATURE-OF adaptivity. Adam HYPONYM-OF Adaptive optimizers. weight decay ( WD ) CONJUNCTION normal hyper - parameter tuning. normal hyper - parameter tuning CONJUNCTION weight decay ( WD ). adaptive optimizers COMPARE SGD. SGD COMPARE adaptive optimizers. normal hyper - parameter tuning USED-FOR adaptive optimizers. weight decay ( WD ) USED-FOR adaptive optimizers. image classification domain EVALUATE-FOR SGD. image classification domain EVALUATE-FOR adaptive optimizers. generalization performance EVALUATE-FOR SGD. generalization performance EVALUATE-FOR adaptive optimizers. OtherScientificTerm are tuned regularization, network weights, training loss, and train loss. Generic are networks, and network. ",This paper studies the adaptivity of gradient-based optimizers for over-parameterized deep networks. The authors show that adaptive optimizers are more adaptive than SGD in the weight space. They show that weight decay (WD) and normal hyper-parameters tuning (NHP) can be used to improve the performance of adaptive optimizer.   ,"This paper proposes Adam, a new adaptive optimizer for over-parameterized deep neural networks. Adam is based on weight decay (WD) and normal hyper-parameters tuning. The authors show that Adam outperforms standard gradient-based optimizers (SGD) in terms of generalization performance on image classification and ranking tasks. They also provide theoretical analysis on the generalization properties of Adam. "
4335,SP:819df8d847a99f13ed5efdcabae8b464c12b464b,"model USED-FOR equivariance. edge orientations FEATURE-OF face. edge orientations CONJUNCTION face poses. face poses CONJUNCTION edge orientations. symmetries USED-FOR low and high - level features. face poses FEATURE-OF camera. edge orientations HYPONYM-OF low and high - level features. edge orientations HYPONYM-OF symmetries. equivariant networks USED-FOR partial and full equivariances. Partial G - CNNs HYPONYM-OF equivariant networks. full equivariance FEATURE-OF Partial G - CNNs. Partial G - CNNs COMPARE G - CNNs. G - CNNs COMPARE Partial G - CNNs. discrete groups CONJUNCTION continuous groups. continuous groups CONJUNCTION discrete groups. method USED-FOR discrete groups. method USED-FOR continuous groups. Task are generalization, and natural image classification. Method is group equivariant architectures. OtherScientificTerm are distribution, and rotations. Material is rotated MNIST. Generic is them. ","This paper proposes a method for equivariant natural image classification based on partial and full equivariance. The method is based on the observation that low and high-level features can be represented by different symmetries. The authors propose to use partial equivariances to learn the low-level feature, and then use full equivariance to learn low- and high level features. The proposed method is evaluated on MNIST and CIFAR-10 datasets.","This paper proposes a new model for equivariant group equivariance for natural image classification. The model is based on a G-CNN model, which is trained on rotated MNIST images. The authors show that the proposed model can generalize to discrete groups and continuous groups. They also show that it can generalise to continuous groups as well as discrete groups. "
4351,SP:0c0ca9df96f1fa2eb8b83a47d0d5964590fef290,Markov chain Monte Carlo ( MCMC ) USED-FOR approximating intractable distributions. Langevin dynamics HYPONYM-OF Markov chain Monte Carlo ( MCMC ). datapoint - wise iterations CONJUNCTION slow convergence. slow convergence CONJUNCTION datapoint - wise iterations. its USED-FOR deep latent variable models. inference model USED-FOR latent variables. ALD USED-FOR scalable inference. large - scale datasets USED-FOR ALD. datapoint - wise iterations USED-FOR it. large - scale datasets USED-FOR scalable inference. MCMC USED-FOR it. stationary distribution USED-FOR ALD. ALD USED-FOR generative modeling. it USED-FOR prior distribution. it USED-FOR latent variable. prior distribution FEATURE-OF latent variable. ALD USED-FOR unconditional distribution. it USED-FOR generative modeling. energy - based model HYPONYM-OF unconditional distribution. ALD USED-FOR deep latent variable model. Langevin autoencoder ( LAE ) HYPONYM-OF deep latent variable model. ALD USED-FOR autoencoder - like posterior inference. LAE USED-FOR autoencoder - like posterior inference. latent space EBM USED-FOR LAE. ALD USED-FOR LAE. ALD COMPARE LD. LD COMPARE ALD. ALD USED-FOR target distributions. toy datasets EVALUATE-FOR ALD. conditional and unconditional cases FEATURE-OF target distributions. SVHN CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION SVHN. CIFAR-10 CONJUNCTION CelebA - HQ. CelebA - HQ CONJUNCTION CIFAR-10. datasets USED-FOR image generation task. image generation task EVALUATE-FOR LAE. SVHN HYPONYM-OF datasets. datasets EVALUATE-FOR LAE. CelebA - HQ HYPONYM-OF datasets. CIFAR-10 HYPONYM-OF datasets. LAE COMPARE non - amortized MCMC methods. non - amortized MCMC methods COMPARE LAE. LAE COMPARE AVI - based methods. AVI - based methods COMPARE LAE. Fréchet,This paper proposes a deep latent variable model based on Langevin autoencoder (LD) that uses a Markov chain Monte Carlo (MCMC) to approximate the prior distribution of the latent variable. The proposed method is based on the energy-based model (EBM) and can be applied to both conditional and unconditional distributions. The experiments show that the proposed method outperforms the state-of-the-art methods on image generation tasks.  ,"This paper proposes a new deep latent variable model for Langevin autoencoder (LD) based on Langevin dynamics (LD). The authors propose an energy-based model (EBM) to model the latent space of the latent variable. The authors show that the EBM can be used to learn the prior distribution of a latent variable, which can then be used as a target distribution for the target distribution. The proposed method is evaluated on a variety of datasets, including SVHN, CIFAR-10, CelebA-HQ, and Fréchet."
4367,SP:5631097031c7e599bdeae64366ffa6e4558837c6,hypergraph reasoning USED-FOR large domains. logical rules PART-OF logical reasoning. structured neural network USED-FOR hypergraph reasoning. neural networks CONJUNCTION finite - domain quantification operations. finite - domain quantification operations CONJUNCTION neural networks. SpaLoc USED-FOR grounding of relationships. sparse tensors USED-FOR SpaLoc. sparse tensors USED-FOR grounding of relationships. finite - domain quantification operations USED-FOR SpaLoc. neural networks USED-FOR SpaLoc. sparsification loss USED-FOR SpaLoc model. intermediate layers PART-OF SpaLoc model. sparsification loss USED-FOR intermediate layers. training and inference - time sub - sampling USED-FOR SpaLoc. real - world knowledge graphs HYPONYM-OF large - scale graphs. information loss FEATURE-OF sampled sub - graphs. information - theoretic measure information sufficiency USED-FOR sampling and label calibration paradigm. accuracy CONJUNCTION efficiency. efficiency CONJUNCTION accuracy. accuracy EVALUATE-FOR SpaLoc. efficiency EVALUATE-FOR SpaLoc. synthetic datasets EVALUATE-FOR SpaLoc. real - world knowledge graph reasoning benchmarks EVALUATE-FOR SpaLoc. OtherScientificTerm is grandparent relationship. Method is hypergraph neural networks. ,This paper proposes a new method for hypergraph reasoning based on sparse tensors. The proposed method is based on the idea of sparsifying intermediate layers of hypergraph neural networks. Theoretical analysis is provided to show that the sparsification loss can be used to improve the performance of the model. Experiments are conducted on synthetic and real-world knowledge graph datasets. ,"This paper proposes a new method for hypergraph reasoning, called SpaLoc, which is based on sparse tensors. The key idea of SpaLoc is to use a sparse tensor-based neural network to model the grounding of relationships in hypergraphs. The authors propose a new information-theoretic measure of information sufficiency to measure the information-sufficiency of the sampled sub-graphs, and propose a sampling and label calibration paradigm to improve the accuracy and efficiency of the model. Experiments on synthetic and real-world datasets demonstrate the effectiveness of the proposed method. "
4383,SP:9657121b01c51f78c00d06b47d3e8d678dd85d54,"top - k classification accuracy EVALUATE-FOR machine learning. probability distribution USED-FOR k. top-1 CONJUNCTION top-5 accuracy. top-5 accuracy CONJUNCTION top-1. ImageNet EVALUATE-FOR models. top-5 accuracy EVALUATE-FOR models. top-1 EVALUATE-FOR models. Task is differentiable sorting and ranking. Metric are top-5 accuracies, and top-1 accuracy. Method is ImageNet models. ","This paper studies the problem of top-k classification accuracy in differentiable sorting and ranking, where the goal is to maximize the top-1 accuracy and minimize the worst-case top-5 accuracy. The authors propose to use the probability distribution of the distribution over the probability distributions of the data points in the data distribution to estimate the probability of a given data point being in the top 5. They show that for a given set of data points, there exists a set of models that achieve the best possible top-K accuracy. They also show that these models are able to achieve the top 1 accuracy and the best top 5 accuracy. ","This paper studies the problem of differentiable sorting and ranking of top-k classification accuracy in machine learning. The authors propose a new metric that measures the difference between top-1 accuracy and top-5 accuracy, which is defined as the difference in the probability distribution of the k-th classification accuracy of a classifier. They show that the difference can be measured by the number of classes that are in the top 5 accuracy of the classifier, and that it can also be measured in terms of the top 1 accuracy. They also show that there is no difference between the top-2 accuracy and the top 3 accuracy in the classification accuracy. "
4399,SP:cb3188f435c54a365890e20e4d582c250d919833,"speed CONJUNCTION accuracy. accuracy CONJUNCTION speed. accuracy EVALUATE-FOR method. speed EVALUATE-FOR method. method USED-FOR OT problem. Douglas - Rachford splitting technique USED-FOR method. method USED-FOR approximate regularized problem. entropic regularization USED-FOR methods. algorithm COMPARE Sinkhorn method. Sinkhorn method COMPARE algorithm. method COMPARE Sinkhorn method. Sinkhorn method COMPARE method. iteration complexity EVALUATE-FOR method. linear convergence rate USED-FOR OT problem. primal - dual stopping criterion FEATURE-OF method. computation times CONJUNCTION robustness. robustness CONJUNCTION computation times. robustness EVALUATE-FOR method. computation times EVALUATE-FOR method. OtherScientificTerm are sparse transport plans, and numerical issues. ","This paper studies the sparse transport problem, where the goal is to find a sparse transport plan that is close to the true transport plan. The authors propose a method based on Douglas-Rachford splitting to solve this problem. The main idea is to split the problem into two subproblems, where one of them is an approximate regularized version of the original problem, and the other is an entropic regularization. The method is shown to converge to a solution with a linear convergence rate of $O(1/\sqrt{T})$ in the primal-dual stopping criterion.   ","This paper proposes a new method for solving the sparse transport problem with sparse transport plans. The method is based on the Douglas-Rachford splitting technique, which is used to approximate the approximate regularized problem. The authors show that the proposed method converges to the primal-dual stopping criterion with a linear convergence rate. They also show that their method is robust to numerical issues."
4415,SP:9a087cc734a3e7f3ab848bef5e2eff37fe40f303,"distribution of distributions USED-FOR Federated learning data. generalization studies USED-FOR federated learning. framework USED-FOR performance gaps. dataset synthesis strategy USED-FOR realistic simulations of generalization. OtherScientificTerm are meta - distribution, local data distributions, out - of - sample gap, and participation gap. Material are natural and synthetic federated datasets, and naturally - partitioned data. Method is semantic synthesis strategy. ","This paper studies the out-of-sample gap in federated learning. The authors propose to use the meta-distribution of distributions to measure the generalization gap between the local data distributions. They show that the performance gap can be measured by the participation gap, which is defined as the difference between the average performance of the local distribution and the global distribution. They then propose a semantic synthesis strategy to generate synthetic federated training data and show that it can be used for simulation of generalization. ","This paper proposes a new framework to study the performance gap between federated learning data and the meta-distribution of the data distribution. The main contribution of the paper is to propose a dataset synthesis strategy for realistic simulations of generalization. The proposed method is based on a semantic synthesis strategy, and it is shown that it can be used to simulate performance gaps between data distributions. The paper also provides a theoretical analysis of the performance gaps."
4431,SP:da0e8c89f343abfe500eb4c1968e418c2fb52ef6,"generalization EVALUATE-FOR LMs. self - supervision objectives USED-FOR LMs. supervised finetuning USED-FOR pre - trained language models ( PLMs ). model capacity CONJUNCTION data size. data size CONJUNCTION model capacity. pre - training techniques USED-FOR PLMs. PLMs USED-FOR few - shot setting. PLMs USED-FOR few - shot tasks. PLMs USED-FOR zero - shot setting. BERT family USED-FOR models. IMDB dataset CONJUNCTION Amazon dataset. Amazon dataset CONJUNCTION IMDB dataset. IMDB dataset HYPONYM-OF datasets. Amazon dataset HYPONYM-OF datasets. PLMs USED-FOR language understanding tasks. zero - shot setting FEATURE-OF PLMs. GLUE HYPONYM-OF language understanding tasks. Method are deep learning, language models ( LMs ), and prompt - based learning. OtherScientificTerm are manually / automatically created prompts, and manually created prompts. Metric is accuracy. ","This paper proposes a prompt-based language model (PLM) approach to improve the generalization performance of language models (LMs) on few-shot tasks. The PLM approach is based on self-supervised finetuning, where the model is trained on a large number of prompts and fine-tuned on a small amount of data. The authors show that PLMs are able to generalize better than the state-of-the-art language models on the GLUE and IMDB datasets.   ","This paper studies the problem of prompt-based language models (PLMs) in the zero-shot setting, where the goal is to improve the performance of a PLM on a few-shot task. The authors show that PLMs can improve the generalization ability of the model in this setting. They also show that the model is able to perform better than a BERT model on the GLUE and IMDB datasets."
4447,SP:9817dccb1a121058b23a2ef825ed339cf8b53674,Attention mechanism USED-FOR tasks. sharpener module USED-FOR attention mechanism. it USED-FOR representation. alignment FEATURE-OF attention. real - world scene text recognition datasets EVALUATE-FOR approach. approach COMPARE ones. ones COMPARE approach. approach COMPARE soft and hard attention. soft and hard attention COMPARE approach. soft and hard attention HYPONYM-OF ones. ,"This paper proposes a novel attention mechanism for text recognition tasks. The proposed method is based on the idea that the attention mechanism should be aligned with the representation of the input text. To this end, the authors propose to use a sharpener module to improve the alignment between the input and the output representations. The authors show that the proposed method outperforms the state-of-the-art methods on two text recognition datasets.   ","This paper proposes a novel attention mechanism for text recognition tasks. The proposed method is based on a sharpener module that is used to align the representation of the input text with the target text representation. The sharpener is used in two ways: (1) it is used for alignment between the input representation and the target representation, and (2) it can also be used to improve the performance of the attention mechanism. Experiments are conducted on two real-world scene text recognition datasets.  "
4463,SP:3913ed3b3cf6494368e3be6cacb637ff85f80ee6,"Learning USED-FOR combinatorial optimization problems. operations research solvers CONJUNCTION heuristics. heuristics CONJUNCTION operations research solvers. vehicle routing problem HYPONYM-OF combinatorial optimization problems. apriori given number of available vehicles USED-FOR complex assignment problem. bounded fleet size FEATURE-OF logistic service providers. post - processing scheme USED-FOR supervised approach. method COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE method. Method are deep reinforcement learning approaches, learning - based approaches, and supervised deep learning framework. Generic is them. OtherScientificTerm is apriori fixed number of available vehicles. ", service providers in the vehicle routing problem. The paper proposes a supervised deep learning framework to solve the problem. A post-processing scheme is used to improve the performance of the proposed method. The proposed method is evaluated on a set of real-world vehicle routing problems.  ,This paper proposes a method for solving a combinatorial routing problem where the number of available vehicles is bounded by a bounded number of logistic service providers. The authors propose a supervised deep learning framework to solve the problem. They propose a post-processing scheme to handle the task of finding the best service provider for a given number of vehicles. They show that their method outperforms state-of-the-art approaches in terms of efficiency and accuracy.
4479,SP:594a813c0d0baa66738b9c8331370f861ad3c416,"Existing methods USED-FOR variables. clustering effect HYPONYM-OF observed graph structure. observed graph structure HYPONYM-OF variables. clustering effect HYPONYM-OF variables. causal relationship FEATURE-OF variables. link prediction method USED-FOR graph learning. counterfactual inference USED-FOR link prediction method. counterfactual inference USED-FOR graph learning. It USED-FOR counterfactual links. It USED-FOR representations. observed and counterfactual links USED-FOR representations. benchmark datasets EVALUATE-FOR graph learning method. link prediction EVALUATE-FOR graph learning method. Task is graph - based applications. Generic is it. Method are causal models, and graph representations. OtherScientificTerm is global graph structural properties. ","This paper proposes a link prediction method for graph learning based on counterfactual inference. The proposed method is based on the observation of a set of ""counterfactual links"" between two nodes in a graph. The authors show that the proposed method can be used in conjunction with existing methods for link prediction. The method is evaluated on two benchmark datasets and achieves state-of-the-art performance.",This paper proposes a link prediction method for graph learning based on counterfactual link prediction. The proposed method is based on the idea of learning representations of global graph structural properties. The key idea is to learn representations of the global graph structure by comparing the observed graph structure with the predicted graph structure. The authors show that the proposed method outperforms the state-of-the-art link prediction methods on several benchmark datasets. 
4495,SP:48a7e50451b887f55be17b2662aa11ce18791cc1,"unsupervised feature selection methods USED-FOR ranking features. second - order covariance matrix CONJUNCTION first - order data matrix. first - order data matrix CONJUNCTION second - order covariance matrix. sparse attention matrix USED-FOR second - order relations between features. graph segmentation USED-FOR feature selection. attention matrix USED-FOR relational graph. SOFT COMPARE state - of - the - art methods. state - of - the - art methods COMPARE SOFT. SOFT COMPARE method. method COMPARE SOFT. Task are Unsupervised feature selection, and unsupervised feature selection. OtherScientificTerm is external guidance information. Method is knowledge contrastive disTillation ( SOFT ) model. ",This paper proposes a novel unsupervised feature selection method based on knowledge contrastive disillation (SOFT) model for feature selection. The proposed method is based on the idea of second-order relations between features in a relational graph. The authors propose a sparse attention matrix to model the second order relations between the features and the first-order covariance matrices of the data. Experiments show that the proposed method outperforms state-of-the-art methods in terms of feature selection accuracy. ,"This paper proposes a knowledge contrastive disillation (SOFT) model for feature selection in unsupervised feature selection. The SOFT model is based on a sparse attention matrix, which is used to learn the second-order relations between features in a relational graph. The attention matrix is learned by segmenting the graph using graph segmentation. The authors show that SOFT outperforms state-of-the-art methods in terms of feature selection accuracy."
4511,SP:14bcae11aeede63f28d1b80c05ed18a01d3e3f3c,"Multimodal variational autoencoders ( VAEs ) USED-FOR joint distribution. vision CONJUNCTION language. language CONJUNCTION vision. heterogeneous data USED-FOR joint distribution. language HYPONYM-OF heterogeneous data. vision HYPONYM-OF heterogeneous data. idiosyncratic representations PART-OF recognition model. mixtures CONJUNCTION factorisations. factorisations CONJUNCTION mixtures. mixtures USED-FOR idiosyncratic representations. MEME COMPARE baselines. baselines COMPARE MEME. metrics EVALUATE-FOR partial and complete observation schemes. metrics EVALUATE-FOR MEME. metrics EVALUATE-FOR baselines. representations COMPARE approaches. approaches COMPARE representations. mutual supervision USED-FOR representations. OtherScientificTerm are modalities, and relatedness between data. Generic are alternative, and formulation. Method are Mutually supErvised Multimodal VAE ( MEME ), and semisupervised VAEs. Material is partiallyobserved data. ","This paper proposes a novel multi-modal variational autoencoder (VAE) model for semi-supervised learning. The proposed model is based on the idea of mutual supervision, where the model learns to model the mutual information between the modalities. The main idea is to use a mixture of mixtures of modalities and factorization of the input data, where each modality is represented by a different mixture of features. The authors show that the proposed method is able to learn representations that are independent of the other modalities, and that are robust to changes in the data distribution.   ","This paper proposes a new multi-modal multimodal VAE (MEME) framework for learning representations from partially-observed data. The key idea is to use a mixture of mixtures of data modalities (e.g., language, vision) and factorisations to learn a representation for each data modality, which is then used as a surrogate for the full data distribution. The authors show that the proposed method outperforms the state-of-the-art on both partial and complete observation schemes.  "
4527,SP:e834a52cadebe5f125ce491273b4ad1146beae3f,"Reinforcement Learning agent USED-FOR directed behaviors. Intrinsic Motivation USED-FOR Reinforcement Learning agent. options USED-FOR simple tabular cases. Deep Explore Options USED-FOR complex visual problems. Explore Options PART-OF Deep Reinforcement Learning paradigm. unrelated intrinsic rewards USED-FOR Deep Explore Options. J - PER HYPONYM-OF transitionselection algorithm. interest of multiple agents USED-FOR transitionselection algorithm. intrinsic reward learning USED-FOR auxiliary task. architecture USED-FOR shared representation. Atari Suite FEATURE-OF hard and easy exploration games. Atari Suite EVALUATE-FOR Deep Explore Options. hard and easy exploration games EVALUATE-FOR Deep Explore Options. they COMPARE weighted sum of rewards. weighted sum of rewards COMPARE they. weighted sum of rewards COMPARE baselines. baselines COMPARE weighted sum of rewards. they COMPARE baselines. baselines COMPARE they. intrinsic rewards USED-FOR they. OtherScientificTerm are sparse or noisy rewards, intrinsic and extrinsic rewards, interesting behaviors, high dimensional spaces, and exploitative or exploratory behaviors. Method is intrinsically motivated agent. Task is exploration. ",This paper proposes to use Explore Options as an intrinsic reward for exploration in reinforcement learning. The idea is to learn a set of exploration options that are independent of the intrinsic reward and can be used as auxiliary tasks. The exploration options are learned using a transitionselection algorithm that is based on the interest of multiple agents. Experiments on Atari games show that the proposed method outperforms baselines in terms of exploration performance.  ,This paper proposes a novel approach to explore options in deep reinforcement learning (DRL). The authors propose a new exploration strategy called Deep Explore Options (DOU) that combines intrinsic and extrinsic rewards. They propose a transitionselection algorithm to select the best option for each agent based on the interest of multiple agents. Experiments show that the proposed method outperforms the state-of-the-art in Atari games.  
4543,SP:41578dd1a4bdb043b3d68afa5f9cebb3e14f3907,method USED-FOR Hamiltonian dynamical systems. stiffness FEATURE-OF dynamical system. SANN USED-FOR stiff and nonstiff portions. stiffness - aware index USED-FOR SANN. classification CONJUNCTION resampling technique. resampling technique CONJUNCTION classification. time integration strategies USED-FOR dynamical characteristics. dynamical characteristics FEATURE-OF Hamiltonian vector fields. step size adaptation USED-FOR dynamical characteristics. resampling technique USED-FOR time integration strategies. classification USED-FOR time integration strategies. step size adaptation HYPONYM-OF time integration strategies. three - body problem CONJUNCTION billiard model. billiard model CONJUNCTION three - body problem. billiard model EVALUATE-FOR SANN. complex physical systems EVALUATE-FOR SANN. three - body problem HYPONYM-OF complex physical systems. billiard model HYPONYM-OF complex physical systems. SANN COMPARE state - of - the - art methods. state - of - the - art methods COMPARE SANN. energy EVALUATE-FOR SANN. accuracy EVALUATE-FOR state - of - the - art methods. accuracy EVALUATE-FOR SANN. Method is stiffness - aware neural network ( SANN ). ,"This paper proposes a new method for learning Hamiltonian dynamical systems with stiff and non-stiff portions. The method is based on a stiffness-aware neural network (SANN), which is able to distinguish between stiff and not-so-firm portions of the dynamical system. The proposed method is evaluated on a three-body problem and a billiard model.  ","This paper proposes a new method for learning Hamiltonian dynamical systems with stiff and non-stiff parts. The authors propose a stiffness-aware neural network (SANN) that learns the stiffness of a dynamical system by learning a stiffness index for each part of the system. The stiffness index is learned by using a resampling technique and a time integration strategy to learn the dynamical characteristics of a Hamiltonian vector field. The proposed method is evaluated on a three-body problem, a billiard model, and a complex physical system."
4559,SP:bfb0a059eeb6f40a18fbd20c0eec5037a64ca09e,"language models USED-FOR tasks. generating realistic text CONJUNCTION synthesizing computer programs. synthesizing computer programs CONJUNCTION generating realistic text. generating realistic text HYPONYM-OF tasks. synthesizing computer programs HYPONYM-OF tasks. they USED-FOR tasks. unbounded multi - step computation USED-FOR tasks. models USED-FOR multistep computations. Transformers USED-FOR multi - step computations. intermediate computation steps PART-OF scratchpad ”. language models USED-FOR multi - step computations. scratchpads USED-FOR language models. OtherScientificTerm are computer programs, integers, and programs. Method is intermediate computations. ","This paper proposes to use scratchpads to train language models to perform unbounded multi-step computations on computer programs. The scratchpad consists of a set of intermediate computations, where each intermediate computation step is represented by an integer. The authors show that the scratchpad can be used to train a language model to perform multi-stage computations. The paper also shows that the model is able to learn to predict the intermediate computation steps.","This paper proposes a new scratchpad-based language model for multi-step computations. The scratchpad consists of a set of scratchpads, where each scratchpad represents a sequence of intermediate computations, and a language model is trained to predict the intermediate steps. The paper shows that the scratchpad model is able to predict intermediate steps in a multistep manner. The authors also show that the model can predict intermediate computation steps in an unbounded manner.  "
4575,SP:e6c1a8b4bba287455dc9cf145b6bd1f04e2148a9,"deep networks USED-FOR adversarial attacks. methods USED-FOR adversarial perturbations. deep image generators CONJUNCTION optimization objective. optimization objective CONJUNCTION deep image generators. deep image generators USED-FOR feature - level adversarial perturbations. optimization objective USED-FOR feature - level adversarial perturbations. them USED-FOR targeted feature - level attacks. they USED-FOR targeted feature - level attacks. ImageNet scale FEATURE-OF targeted feature - level attacks. them USED-FOR copy / paste ” adversaries. OtherScientificTerm are feature - class associations, natural objects, and targeted misclassification. Method is featurefool attacks. Generic is attacks. Material is natural image. ", is an interesting and important research topic. This paper proposes a novel adversarial attack method based on feature-fool attacks. The proposed method is based on the observation that adversarial perturbations to natural images can cause feature-class misclassification. The authors propose a novel optimization objective to improve the performance of feature-based adversarial attacks.  ,"This paper studies the problem of adversarial perturbations against deep neural networks. The authors propose a novel approach to target feature-level adversarial attacks, which they call ""featurefool"" attacks. The main idea of the paper is to use a feature-fool attack as an optimization objective to improve the performance of a deep neural network. They show that their approach can be applied to a wide range of attacks, including copy/paste attacks, copy-and-paste attacks and targeted misclassification attacks. They also show that the proposed approach is more effective than previous methods."
4591,SP:873618263dc4246a39c44d0abfecfb5f688817e3,Simulated annealing ( SA ) HYPONYM-OF stochastic global optimisation technique. stochastic global optimisation technique USED-FOR discrete and continuous variable problems. neighbour proposal distribution CONJUNCTION temperature annealing schedule. temperature annealing schedule CONJUNCTION neighbour proposal distribution. SA optimiser USED-FOR problem. temperature annealing schedule HYPONYM-OF handpicked components. handpicked components USED-FOR SA optimiser. neighbour proposal distribution HYPONYM-OF handpicked components. policy USED-FOR solution quality. policy USED-FOR proposal distribution. Knapsack problem CONJUNCTION Bin Packing problem. Bin Packing problem CONJUNCTION Knapsack problem. Bin Packing problem CONJUNCTION Travelling Salesperson problem. Travelling Salesperson problem CONJUNCTION Bin Packing problem. Rosenbrock ’s function CONJUNCTION Knapsack problem. Knapsack problem CONJUNCTION Rosenbrock ’s function. Neural SA COMPARE SA baselines. SA baselines COMPARE Neural SA. hand - selected parameters USED-FOR SA baselines. hand - selected parameters USED-FOR Neural SA. problems EVALUATE-FOR Neural SA. problems EVALUATE-FOR SA baselines. proposal distribution USED-FOR Neural SA. Travelling Salesperson problem HYPONYM-OF problems. Rosenbrock ’s function HYPONYM-OF problems. Bin Packing problem HYPONYM-OF problems. Knapsack problem HYPONYM-OF problems. solution quality CONJUNCTION wall clock time. wall clock time CONJUNCTION solution quality. Neural SA COMPARE solvers. solvers COMPARE Neural SA. wall clock time EVALUATE-FOR Neural SA. solution quality EVALUATE-FOR Neural SA. wall clock time EVALUATE-FOR solvers. solution quality EVALUATE-FOR solvers. Method is SA. OtherScientificTerm is computational budget. ,"This paper proposes a new method for simulated annealing (SA) in discrete and continuous variable problems. The proposed method is based on the idea of using a policy to estimate the neighbor proposal distribution and a temperature-annealing schedule for each problem. The method is evaluated on the Knapsack problem, the Bin Packing problem, and the Travelling Salesperson problem. ","This paper proposes a new Simulated Annealing (SA) method for solving discrete and continuous variable problems. The proposed method is based on the idea of ""neighbouring proposal distribution"" (neighbor proposal distribution) and temperature annealing schedule (temperature annealed schedule) which is a hand-picked component of the SA optimiser. The authors show that the proposed method outperforms the state-of-the-art in terms of solution quality and wall clock time."
4607,SP:cae31f7436920eb3946e3f5bca0ac88a73d7c3ec,"policy changes of agents USED-FOR learning process. measurement indicators USED-FOR non - stationarity. notion USED-FOR non - stationarity. non - stationarity FEATURE-OF policy sequence. δ - stationarity measurement HYPONYM-OF notion. trust - region constraint FEATURE-OF joint policy. policy factorization USED-FOR policy divergence. trust - region constraints FEATURE-OF factorized policies. mean - field approximation HYPONYM-OF policy factorization. computational complexity EVALUATE-FOR it. pairwise Markov random field USED-FOR joint policy. MAMT HYPONYM-OF Multi - Agent Mirror descent policy algorithm. end - to - end manner USED-FOR trust - region of the local policies. Trust region decomposition USED-FOR Multi - Agent Mirror descent policy algorithm. MAMT USED-FOR non - stationarity problem. MAMT USED-FOR joint policies ’ divergence. method COMPARE baselines. baselines COMPARE method. complexity FEATURE-OF cooperative tasks. cooperative tasks EVALUATE-FOR baselines. cooperative tasks EVALUATE-FOR method. complexity EVALUATE-FOR baselines. complexity EVALUATE-FOR method. Task is Non - stationarity. Generic is algorithms. Metric is KL - divergence. OtherScientificTerm are trust - region decomposition dilemma, joint policy divergence, and δ - stationarity. Method is message passing. ","This paper studies the non-stationarity problem in cooperative multi-agent reinforcement learning, where the goal is to learn a policy that is invariant to policy changes. The authors propose to use the trust-region decomposition dilemma to solve the joint policy divergence problem, which is an important problem in this setting. The main contribution of the paper is a new algorithm called Multi-Agent Mirror descent (MAMT), which is based on the MAMT algorithm. The key idea is to decompose the trust region of the local policies in an end-to-end manner and use it to compute the divergence of the joint policies. The proposed algorithm is shown to be computationally efficient.","This paper studies the problem of non-stationarity in multi-agent cooperative learning. The authors propose a new algorithm called MAMT to solve the trust-region decomposition dilemma, which is the joint policy divergence problem. The key idea is to use the mean-field approximation to factorize the joint policies’ divergence, and then use a pairwise Markov random field to measure the divergence between the local and joint policies. The proposed algorithm is shown to be computationally efficient in terms of KL-divergence, and it outperforms existing algorithms on cooperative tasks."
4623,SP:989b58167a15ae4fafbe27ff534d327991b6c4d7,"correlated audio and visual information PART-OF Video recordings of speech. self - supervised representation learning framework USED-FOR audio - visual speech. AV - HuBERT USED-FOR audio - visual speech representation. audio - visual speech representation USED-FOR lip - reading and automatic speech recognition. AV - HuBERT COMPARE state - of - the - art approach. state - of - the - art approach COMPARE AV - HuBERT. labeled data USED-FOR AV - HuBERT. transcribed video data USED-FOR state - of - the - art approach. WER EVALUATE-FOR AV - HuBERT. labeled data USED-FOR LRS3. audio - visual representation USED-FOR audio - only speech recognition. benchmark USED-FOR audio - only speech recognition. benchmark EVALUATE-FOR audio - visual representation. relative WER reduction EVALUATE-FOR audio - visual representation. Method are speech representation, and self - training. Material is multi - stream video input. OtherScientificTerm is multimodal hidden units. Metric is lip - reading WER. ","This paper proposes a self-supervised representation learning framework for audio-visual speech. The proposed method, called AV-HuBERT, learns a multi-modal speech representation from multi-stream video recordings of speech. In this paper, the authors propose to use self-training to learn the audio and visual speech representations from video recordings. The authors show that the proposed method achieves state-of-the-art performance on the LRS3 speech recognition benchmark. ","This paper proposes a self-supervised representation learning framework for audio-visual speech representation learning. The proposed method is based on HuBERT, which is a multi-modal representation learning method that learns the audio and visual speech representations of video recordings of speech. The authors show that the proposed method outperforms the state-of-the-art in terms of relative WER reduction on the LRS3 benchmark."
4639,SP:7c9eb8aa4a4dcb5965157d860e812d81654e3aa7,combinatorial optimisation USED-FOR real - world applications. logistics CONJUNCTION natural sciences. natural sciences CONJUNCTION logistics. natural sciences FEATURE-OF combinatorial optimisation. graphs USED-FOR combinatorial optimisation. pre - solved instances USED-FOR problems. graph neural networks ( GNNs ) USED-FOR decision step. ECORD HYPONYM-OF RL algorithm. pre - processing step USED-FOR GNN. recurrent unit USED-FOR fast - acting exploratory phase. SOTA USED-FOR RL algorithms. ECORD USED-FOR RL algorithms. RL algorithms USED-FOR Maximum Cut problem. ECORD USED-FOR SOTA. SOTA USED-FOR Maximum Cut problem. Maximum Cut problem EVALUATE-FOR ECORD. scalability EVALUATE-FOR ECORD. speed EVALUATE-FOR ECORD. nearest competitor COMPARE ECORD. ECORD COMPARE nearest competitor. optimality gap EVALUATE-FOR ECORD. Method is Reinforcement learning ( RL ). Generic is it. OtherScientificTerm is wall - clock time. ,This paper proposes a novel reinforcement learning algorithm for combinatorial optimisation. The main idea is to use graph neural networks (GNNs) for the decision step and use a recurrent unit for the fast-acting exploratory phase. The experimental results show that the proposed ECORD outperforms the state-of-the-art SOTA algorithms in the Maximum Cut problem.,This paper proposes a new algorithm for combinatorial reinforcement learning. The main idea is to use graph neural networks (GNNs) as a recurrent unit to perform a fast-acting exploratory phase during the exploration phase of reinforcement learning (SOTA). The proposed ECORD algorithm is based on a GNN with a pre-processing step. The authors show that the proposed method can achieve competitive performance on the Maximum Cut problem. 
4655,SP:f741d980c9c560a21298e947f1605dcbab7ceeac,"Discrete latent variables USED-FOR generation process of real world data. discrete latents USED-FOR Variational Autoencoders ( VAEs ). discrete VAEs COMPARE ones. ones COMPARE discrete VAEs. direct discrete optimization USED-FOR encoding model. direct discrete optimization USED-FOR discrete nature of the latents. reparameterization trick CONJUNCTION amortization. amortization CONJUNCTION reparameterization trick. sampling approximation CONJUNCTION reparameterization trick. reparameterization trick CONJUNCTION sampling approximation. amortization HYPONYM-OF VAE mechanisms. reparameterization trick HYPONYM-OF VAE mechanisms. sampling approximation HYPONYM-OF VAE mechanisms. truncated posteriors CONJUNCTION evolutionary algorithms. evolutionary algorithms CONJUNCTION truncated posteriors. variational setting USED-FOR Discrete optimization. approach USED-FOR evolutionary algorithms. evolutionary algorithms USED-FOR Discrete optimization. truncated posteriors USED-FOR variational setting. truncated posteriors USED-FOR Discrete optimization. decoder network USED-FOR latent states. gradient ascent USED-FOR network weights. gradient ascent USED-FOR discrete variational method. binary latents FEATURE-OF VAEs. discrete variational method USED-FOR VAEs. amortized training COMPARE direct discrete optimization. direct discrete optimization COMPARE amortized training. direct discrete optimization USED-FOR large neural networks. amortized training USED-FOR large neural networks. smaller networks USED-FOR direct optimization. direct optimization COMPARE zero - shot ’ learning. zero - shot ’ learning COMPARE direct optimization. large supervised neural networks COMPARE VAEs. VAEs COMPARE large supervised neural networks. sampling - based approximation CONJUNCTION reparameterization. reparameterization CONJUNCTION sampling - based approximation. approach USED-FOR training of VAEs. sampling - based approximation USED-FOR training of VAEs. direct optimization USED-FOR VAEs. direct optimization USED-FOR denoising. VAEs USED-FOR denoising. they COMPARE non - generative approaches. non - generative approaches COMPARE they. VAEs COMPARE non - generative approaches. non - generative approaches COMPARE VAEs. Method are VAE training, and small","This paper proposes to use a variational autoencoder (VAE) with discrete latents to train a generative model with discrete latent variables. The authors show that this is an efficient way to train the model, and that it is more efficient than using direct discrete optimization. They also show that the proposed method can be used to train VAEs with amortized training.   ",This paper proposes a new method for training VAEs with discrete latents. The method is based on an evolutionary algorithm that uses a variational variational model to learn the latent states of the decoder network. The authors show that their method is able to achieve better performance than the state-of-the-art VAE training method. They also show that the method can be applied to large neural networks and smaller networks.  
4671,SP:deb189d37bd51b92762ce259a106d9a9e9d81ea4,reward - based task EVALUATE-FOR approach. action - prediction USED-FOR methods. Controlled Effect Network ( CEN ) HYPONYM-OF unsupervised method. counterfactual measures of blame USED-FOR unsupervised method. it USED-FOR controlled effects. it PART-OF exploration method. CEN USED-FOR intrinsic motivator. it COMPARE action - prediction models. action - prediction models COMPARE it. CEN COMPARE action - prediction models. action - prediction models COMPARE CEN. Task is Identifying controllable aspects of the environment. Method is reinforcement learning agents. ,This paper proposes an unsupervised method to identify controllable aspects of the environment that can be used as an intrinsic motivator for exploration in reinforcement learning. The proposed method is based on counterfactual measures of blame (CMEs) that are used to measure the impact of actions on the environment. The authors show that CMEs are useful for exploration as they can help the agent to learn to identify the controlled effects of an action.  ,"This paper proposes an unsupervised method to identify controllable aspects of the environment that can be used as an intrinsic motivator for exploration in reinforcement learning. The method is based on the Controlled Effect Network (CEN), which uses counterfactual measures of blame as a motivator. The proposed method is evaluated on a reward-based task, where it is shown to outperform other state-of-the-art action prediction methods."
4687,SP:ea18d57904e25fd09ed0f6c9972029d78779a8a6,"they USED-FOR networks. neural architecture search CONJUNCTION knowledge distillation. knowledge distillation CONJUNCTION neural architecture search. neural architecture search HYPONYM-OF model compression techniques. knowledge distillation HYPONYM-OF model compression techniques. network pruning HYPONYM-OF model compression technique. it USED-FOR SR networks. filter pruning USED-FOR residual blocks. regularization FEATURE-OF pruned structure. regularization USED-FOR structure - regularized pruning ( SRP ). filters USED-FOR unimportant filters. residual USED-FOR layers. filters USED-FOR layers. SRP USED-FOR image SR networks. lightweight network SRPN - L CONJUNCTION SRPN. SRPN CONJUNCTION lightweight network SRPN - L. SRP USED-FOR lightweight network SRPN - L. SRPN - L CONJUNCTION SRPN. SRPN CONJUNCTION SRPN - L. OtherScientificTerm are moderate model size, and pruned filters. Generic is network. Method are L2 regularization, and lightweight and larger image SR networks. ","This paper proposes a regularized pruning (SRP) method for reducing the size of SR networks. The proposed method is based on the idea of residual block pruning, where the residual blocks are pruned from the top layers of the network. The main idea is to prune the filters at the bottom of the residual block to remove the most important filters, while keeping the rest of the filters close to the ground truth. The authors show that the proposed method improves the performance of SRNs on image classification tasks.   ",This paper proposes a new method for structure-regularized pruning (SRP) to reduce the size of neural networks. The main idea of SRP is to prune the structure of a neural network by pruning the residual blocks of the network. The authors show that this method can be applied to both lightweight and larger image SR networks. They also show that SRP can be used to improve the performance of lightweight networks.  
4703,SP:0dee45001ae9600f485614dfe6874a516ac01db5,"model USED-FOR few - shot learning methods. sparsely labeled novel category data USED-FOR model. abundantly labeled base category data USED-FOR model. abundantly labeled base category data USED-FOR few - shot learning methods. framework USED-FOR large domain shift. framework USED-FOR few - shot learning. contrastive loss FEATURE-OF base category data. feature extracting backbone USED-FOR framework. contrastive loss USED-FOR feature extracting backbone. masking module USED-FOR target domain classification. backbone USED-FOR features. backbone USED-FOR classifier. it EVALUATE-FOR framework. cross - domain few - shot learning benchmark EVALUATE-FOR it. cross - domain few - shot learning benchmark EVALUATE-FOR framework. framework COMPARE cross - domain methods. cross - domain methods COMPARE framework. framework COMPARE meta - learning approaches. meta - learning approaches COMPARE framework. meta - learning approaches COMPARE cross - domain methods. cross - domain methods COMPARE meta - learning approaches. Generic is methods. OtherScientificTerm are distant domain categories, and supervision. ",This paper proposes a novel meta-learning approach for few-shot learning with sparse novel category data. The proposed method is based on a feature extracting backbone and a masking module for target domain classification. The backbone extracts features from base category data and uses the extracted features to train a classifier to classify distant domain categories. The masking modules is used to improve the performance of the classifier in the target domain. The method is evaluated on the cross-domain few shot learning benchmark and achieves state-of-the-art performance.,This paper proposes a new framework for few-shot learning with sparsely labeled novel category data. The authors propose a new contrastive loss for the base category data and a masking module for the target domain classification. The proposed method is evaluated on a cross-domain few shot learning benchmark and shows promising results.
4719,SP:92aa611d71a8da597358330d84fddbb90de2cf4f,"gradient descent USED-FOR generalisation. networks USED-FOR gradient descent. infinite width networks CONJUNCTION finite width networks. finite width networks CONJUNCTION infinite width networks. Bayesian inference USED-FOR infinite width networks. gradient descent USED-FOR finite width networks. error COMPARE chance. chance COMPARE error. gradient descent USED-FOR functions. implicit biases of architecture CONJUNCTION gradient descent. gradient descent CONJUNCTION implicit biases of architecture. gradient descent PART-OF generalisation. implicit biases of architecture PART-OF generalisation. Method are neural networks, and network architecture. OtherScientificTerm are implicit bias of architecture, architecture, NNGP posterior, and minimum a posteriori functions. Metric is average test error. Generic is function. ","This paper studies the effect of gradient descent on Bayesian generalization in neural networks. The authors show that gradient descent with finite width networks is equivalent to Bayesian inference with infinite width networks. They show that this is the case for both infinite width and finite width infinite-width networks. In addition, they show that the error of the Bayesian posterior is bounded by the implicit bias of the network architecture. ","This paper studies the effect of gradient descent on the generalization ability of neural networks. In particular, the authors study the effect on generalization performance of the NNGP posterior, which is a function of the architecture of the network. The authors show that gradient descent can be used to improve the generalisation performance of a neural network. They also show that the error of the posterior of the neural network depends on the size of the training data set. They show that for finite-width networks, gradient descent improves the performance of generalization."
4735,SP:a0e3cf719a95bbc5aad2f663ba5a3169c316ee9b,"large - scale pre - trained multilingual representations USED-FOR cross - lingual transfer methods. transfer EVALUATE-FOR cross - lingual transfer methods. XTREME benchmark EVALUATE-FOR X - Mixup. X - Mixup COMPARE baselines. baselines COMPARE X - Mixup. text understanding tasks EVALUATE-FOR baselines. XTREME benchmark EVALUATE-FOR baselines. text understanding tasks EVALUATE-FOR X - Mixup. OtherScientificTerm are cross - lingual representation discrepancy, and representation discrepancy. Task is cross - lingual transfer. Generic is representations. ","This paper proposes a method for cross-lingual transfer based on Mixup, a pre-trained multilingual representation learning method. The idea is to use Mixup to improve the performance of multilingual text classification models. The proposed method is evaluated on the XTREME benchmark and shows improved performance compared to the baselines.","This paper proposes a new method for cross-lingual transfer, X-Mixup, which uses a large-scale pre-trained multilingual representations. The authors show that X-mixup can be used to improve the performance of a number of existing methods on the XTREME benchmark. They also show that the proposed method can improve performance on a variety of text understanding tasks. "
4751,SP:19f8cd8f0c274b6141ba097d2ebb6d18af0986fd,"defenses USED-FOR attacks. robust algorithms USED-FOR heterogeneous datasets. bucketing scheme USED-FOR robust algorithms. bucketing CONJUNCTION robust algorithms. robust algorithms CONJUNCTION bucketing. robust algorithms USED-FOR attacks. bucketing USED-FOR attacks. guaranteed convergence FEATURE-OF non - iid Byzantine robust problem. Method are Byzantine robust distributed or federated learning, and machine learning model. Generic are algorithm, problem, and approach. OtherScientificTerm are heterogeneous ( non - iid ), and realistic assumptions. ","This paper studies Byzantine robust distributed learning in federated learning with heterogeneous (non-iid) datasets. In this setting, the goal is to find a robust algorithm that is robust to distributed attacks in the presence of heterogeneous data. The authors propose a new Byzantine robust algorithm based on bucketing, which is a variant of bucketing in distributed learning. They show that under certain assumptions on the data distribution, the proposed algorithm is robust against distributed attacks. They also show that the proposed method is computationally efficient. ","This paper studies Byzantine robust distributed or federated learning with heterogeneous datasets. The authors study the problem of Byzantine robust robust distributed learning, where the dataset is heterogeneous (i.e., non-iid) and the attacker can attack the learner. They show that under certain assumptions, they can find a robust algorithm that is robust to attacks under the bucketing scheme. They also show that the robust algorithm is robust against attacks under realistic assumptions."
4767,SP:4d63513b9a1b9b9fc44a69b3d5679a8f48eb95e7,disentanglement CONJUNCTION multi - task learning. multi - task learning CONJUNCTION disentanglement. hard parameter sharing USED-FOR multi - task learning. hard parameter sharing USED-FOR disentanglement. automatically generated supervised tasks USED-FOR neural networks. automatically generated supervised tasks USED-FOR representations. neural networks USED-FOR representations. disentanglement PART-OF multi - task neural network training. metrics EVALUATE-FOR disentanglement. Method is disentangled representations. Generic is they. Task is multi - task learning setting. ,"This paper studies the role of disentanglement in the multi-task learning setting. The authors propose a new metric to measure the degree to which tasks are disentangled from each other in the training process. The metric is based on the notion of ""hard parameter sharing"", which is defined as the number of parameters that are shared across all tasks in a task-agnostic manner. The paper shows that hard parameter sharing is beneficial in terms of the performance of the model on tasks that are not directly supervised by the model. ","This paper studies the problem of disentanglement in the multi-task learning setting. The authors propose a new metric for disentangling the representations of a neural network trained on a set of tasks. The metric is based on the notion of hard-parameter sharing, which is defined as the number of tasks that are disentangled by the network. The paper also proposes a new way to measure the disentangleability of the representations.   "
4783,SP:9851adb72e2918780f661f83f7da06eb866787be,Certifying Robust Policies ( CROP ) USED-FOR reinforcement learning. framework USED-FOR Certifying Robust Policies ( CROP ). framework USED-FOR state level robustness certification. adversarial state perturbations FEATURE-OF reinforcement learning. robustness CONJUNCTION lower bound of cumulative rewards. lower bound of cumulative rewards CONJUNCTION robustness. per - state actions CONJUNCTION lower bound of cumulative rewards. lower bound of cumulative rewards CONJUNCTION per - state actions. robustness CONJUNCTION per - state actions. per - state actions CONJUNCTION robustness. per - state actions HYPONYM-OF robustness certification criteria. lower bound of cumulative rewards HYPONYM-OF robustness certification criteria. robustness HYPONYM-OF robustness certification criteria. policy USED-FOR robustness. Gaussian noise USED-FOR Q - functions. Q - functions USED-FOR policy. policy USED-FOR local smoothing algorithm. Gaussian noise USED-FOR policy. global smoothing algorithm USED-FOR robustness. robustness FEATURE-OF finite - horizon cumulative reward. global smoothing algorithm USED-FOR finite - horizon cumulative reward. adversarial state perturbations FEATURE-OF finite - horizon cumulative reward. tight certification bounds FEATURE-OF reward. adaptive search USED-FOR tight certification bounds. adaptive search USED-FOR local smoothing approach. adversarial training CONJUNCTION regularization. regularization CONJUNCTION adversarial training. methods USED-FOR empirically robust RL. RL robustness certification framework EVALUATE-FOR methods. regularization HYPONYM-OF methods. adversarial training HYPONYM-OF methods. adversarial training USED-FOR empirically robust RL. Atari games EVALUATE-FOR methods. RegCVX CONJUNCTION RadialRL. RadialRL CONJUNCTION RegCVX. RegPGD CONJUNCTION RegCVX. RegCVX CONJUNCTION RegPGD. certified robustness EVALUATE-FOR RadialRL. adversarial attacks EVALUATE-FOR algorithms. OtherScientificTerm is cumulative rewards. Generic is trajectory. ,"This paper proposes a framework for certifying robustness to adversarial perturbations in reinforcement learning. The main idea is to use Gaussian noise as a regularization term to improve the robustness of the policy. The proposed method is based on a local smoothing algorithm, where the policy is trained with a Gaussian-smoothed version of the Q-function. The authors also propose a global smoothing method to improve robustness against adversarial attacks. Experiments show that the proposed method outperforms baselines in terms of robustness.","This paper proposes a framework for certifying robust policies (CROP) against adversarial state perturbations in reinforcement learning. CROP is based on the notion of state-level robustness, which is defined as the lower bound of the cumulative rewards of a policy. The key idea is to use Gaussian noise to measure the robustness of the policy, and then use a local smoothing algorithm to compute the lower-bound of the reward. The authors show that CROP can be combined with adversarial training and regularization to improve the performance of robust policies. "
4799,SP:78da3c97182ec1baf6a131740bf7c91a9afb2fd2,"approach USED-FOR conformal prediction. conformal prediction USED-FOR model uncertainty. calibrated candidate set USED-FOR conformal prediction. in - silico screening USED-FOR drug discovery. coverage CONJUNCTION precision. precision CONJUNCTION coverage. natural language processing CONJUNCTION computer vision. computer vision CONJUNCTION natural language processing. computer vision CONJUNCTION computational chemistry. computational chemistry CONJUNCTION computer vision. natural language processing FEATURE-OF classification tasks. computational chemistry FEATURE-OF classification tasks. classification tasks EVALUATE-FOR approach. computer vision HYPONYM-OF classification tasks. OtherScientificTerm are coverage property, conformal sets, noisy candidates, predicted conformal sets, and user - specified tolerance. Task is large - scale settings. Generic are constraint, and algorithm. Metric is true positive rate. ","This paper proposes a method for conformal prediction, where the goal is to learn a set of conformal sets for a given set of noisy candidates. Conformal sets are defined as the set of calibrated candidate sets that are predicted to have the same coverage as the true candidate set. The proposed method is based on the idea that the true positive rate of the predicted conformal set is a function of the coverage of the candidate set and the precision of the true set.    The main contribution of this paper is to propose a method to learn conformal predictions for large-scale classification tasks. The main idea is to use a calibrated candidate set as a constraint on the model uncertainty. The method is evaluated on a variety of classification tasks, including image classification, computer vision, and in-silico screening. ","This paper proposes a new approach to conformal prediction for large-scale classification tasks. The main idea is to use a calibrated candidate set to predict conformal sets, and then use the predicted conformal set as a calibration set for the model uncertainty. The proposed method is evaluated on a variety of classification tasks, including computer vision, natural language processing and chemistry."
4815,SP:b126d2f3c397633745c8833e22ace93a2470e963,complexity EVALUATE-FOR functions. neural network USED-FOR functions. unit - length curve USED-FOR network. random initialization USED-FOR ReLU networks. higher moments of the length distortion CONJUNCTION distortion of higher - dimensional volumes. distortion of higher - dimensional volumes CONJUNCTION higher moments of the length distortion. upper bounds USED-FOR higher moments of the length distortion. upper bounds USED-FOR distortion of higher - dimensional volumes. OtherScientificTerm is curve of outputs. Metric is expected length distortion. ,This paper studies the problem of estimating the unit-length curve of a ReLU network with random initialization. The authors show that the expected length distortion of the network is bounded by the moments of the length distortion and the distortion of higher-dimensional volumes. They also provide upper bounds on the higher moment of the distortion and show that it can be bounded by higher moments. ,This paper studies the complexity of ReLU networks with unit-length curves. The authors show that the length distortion of a ReLU network is bounded by the number of moments of the distortion of higher-dimensional volumes. They also provide upper bounds for the higher moments and higher moments of higher moments in terms of the volume distortion.  The authors also provide lower bounds for higher moments.
4831,SP:b3b6d0512edfca461ea295ee8665f7f226c45d57,"methods USED-FOR policies. safety constraints CONJUNCTION sparse rewards. sparse rewards CONJUNCTION safety constraints. learning policies PART-OF reinforcement learning ( RL ) problems. Behavioral priors USED-FOR RL. Behavioral priors USED-FOR policy primitives. behavioral priors USED-FOR safe policy learning. SAFEty skill pRiors ( SAFER ) HYPONYM-OF behavioral prior learning algorithm. policy learning USED-FOR complex control tasks. behavioral prior learning algorithm USED-FOR policy learning. SAFER USED-FOR safety variable. contrastive training USED-FOR SAFER. offline data USED-FOR safety requirements. safe and unsafe data USED-FOR contrastive training. abstract actions USED-FOR safe primitive skills. offline data USED-FOR safety variable. SAFER USED-FOR safe and successful policy. safety variable CONJUNCTION abstract action. abstract action CONJUNCTION safety variable. SAFER USED-FOR inference stage. safety variable USED-FOR safety skills. safety skills USED-FOR SAFER. safety skills USED-FOR safe and successful policy. SAFER USED-FOR policies. baseline methods USED-FOR policies. SAFER COMPARE baseline methods. baseline methods COMPARE SAFER. SAFER USED-FOR safety. policies CONJUNCTION safety. safety CONJUNCTION policies. baseline methods USED-FOR safety. Material is offline datasets. OtherScientificTerm are unsafe behavior, and undesirable behaviors. Task is complex safety - critical robotic grasping tasks. ","This paper proposes a method for learning safety-critical policies in reinforcement learning problems. The proposed method is based on the idea that safety is a prior in RL, and that it is important to learn safe primitive skills that can be used to guide the learning of safe and successful policies. The paper proposes to use contrastive training to learn safety skills from offline data, and then use the learned safety skills to guide policy learning in the inference stage. Experiments show that the proposed method outperforms baselines on a variety of robotic tasks.","This paper proposes a new behavioral prior learning algorithm for safety-critical robotic grasping tasks. The proposed method is based on contrastive training, where the safety variable is learned from offline data, and the safety skills are learned from a set of abstract actions. The safety skill pRiors are learned using contrastive learning, which is an extension of contrastive contrastive prior learning. The authors show that the proposed method outperforms the state-of-the-art baselines in terms of safety and performance."
4847,SP:a5dadb3ecc3caed3b9d9a68eda0d48a53c2d1ce2,"multi - branch restoration model USED-FOR restoration tasks. Human Visual System USED-FOR multi - branch restoration model. Retinal Ganglion Cells HYPONYM-OF Human Visual System. deraindrop CONJUNCTION deblurring. deblurring CONJUNCTION deraindrop. image dehazing CONJUNCTION deraindrop. deraindrop CONJUNCTION image dehazing. datasets EVALUATE-FOR multi - branch architecture. CMFNet HYPONYM-OF multi - branch architecture. deblurring HYPONYM-OF datasets. deraindrop HYPONYM-OF datasets. image dehazing HYPONYM-OF datasets. pretrained models USED-FOR restoration tasks. Task are Image restoration, and autonomous cars. Method is learning based restoration methods. OtherScientificTerm is generalization. ","This paper proposes a multi-branch image restoration model for image restoration. The proposed model is based on the idea of retinal ganglion cells (RGCs) from Retinal Ganglion Cells, which is inspired by the human visual system. The authors show that the proposed model achieves state-of-the-art performance on deblurring and image dehazing tasks.  ","This paper proposes a multi-branch restoration model for image restoration. The proposed model is based on the Human Visual System (HVS) model, which is trained on the Retinal Ganglion Cells (RGC) dataset. The model is trained using a CMFNet-based architecture, and it is evaluated on a variety of image restoration tasks, including deblurring, deraindrop, and image dehazing. "
4863,SP:263b386beee44b0b45b6f6dc3cf80d020500be62,"FL USED-FOR data heterogeneity. Personalized federated learning ( PFL ) USED-FOR data heterogeneity. FL USED-FOR Personalized federated learning ( PFL ). FL CONJUNCTION PFL. PFL CONJUNCTION FL. unlabeled clients USED-FOR model. hypernetwork module CONJUNCTION encoder module. encoder module CONJUNCTION hypernetwork module. approach USED-FOR IT - PFLHN. encoder module USED-FOR approach. hypernetwork module USED-FOR approach. encoder module USED-FOR IT - PFLHN. hypernetwork module USED-FOR IT - PFLHN. hypernetwork USED-FOR personalized model. client representation PART-OF hypernetwork. benchmark datasets EVALUATE-FOR IT - PFL - HN. IT - PFL - HN COMPARE FL and PFL methods. FL and PFL methods COMPARE IT - PFL - HN. benchmark datasets EVALUATE-FOR FL and PFL methods. multi - task learning CONJUNCTION domain adaptation. domain adaptation CONJUNCTION multi - task learning. multi - task learning USED-FOR it. Method are Federated learning ( FL ), personalized models, prediction service, and encoder network. Material is labeled data. OtherScientificTerm are unlabeled data, large domain shift, data privacy, and differential privacy. Generic is learning setup. Metric is generalization error. ",This paper proposes a new approach for personalized federated learning (PFL) in the presence of data heterogeneity in the clients. The proposed approach is based on a hypernetwork module and an encoder module. The hypernetwork is used to model the client representation and the encoder is used for the prediction service. Experiments show that the proposed approach outperforms the state-of-the-art methods in terms of generalization error. ,"This paper proposes a new approach for personalized federated learning (PFL) with unlabeled clients. The approach is based on the hypernetwork module and the encoder module. The hypernetwork is composed of a client representation and an encoder network. The encoder is used to learn a personalized model, and the client representation is used for the prediction service. The proposed approach is evaluated on a variety of datasets. "
4879,SP:960d0a63a82593f6e72275b65f0501f0469d1924,"classification PART-OF selfsupervised learning. conditional diffusion based generative model ( RCDM ) USED-FOR representations. self - supervised models USED-FOR representations. model COMPARE generative models. generative models COMPARE model. generation quality COMPARE generative models. generative models COMPARE generation quality. generation quality EVALUATE-FOR model. tool USED-FOR self - supervised models. SSL projector embedding USED-FOR tasks. classifications HYPONYM-OF tasks. SSL model USED-FOR image manipulation. inherent structure USED-FOR image manipulation. SSL model USED-FOR inherent structure. supervised representation CONJUNCTION SSL representation. SSL representation CONJUNCTION supervised representation. Method are neural networks, SSL ( backbone ) representation, and SSL representations. Generic is representation. OtherScientificTerm is conditioning. ","This paper proposes a self-supervised learning method for image classification. The proposed method is based on a conditional diffusion-based generative model (CDDM) and uses a self supervised projection embedding (SSL) model. The SSL model is used to learn a representation of the image, which is then used to train a classifier. The method is evaluated on a variety of image classification tasks, including image classification, object detection, and image manipulation.  ","This paper proposes a new method for generating representations for self-supervised learning. The proposed method is based on a conditional diffusion based generative model (RCDM) that learns representations for a set of tasks (e.g., image classification, image manipulation, etc). The authors propose a new SSL projector embedding that can be used to learn representations for these tasks. The method is evaluated on a variety of tasks, including image classification and image manipulation. "
4895,SP:398899e6c86b4a2a17dfa5c2f4478811f4331c1d,Fp sketch HYPONYM-OF well - celebrated streaming algorithm. well - celebrated streaming algorithm USED-FOR frequency moments estimation. DP baselines COMPARE non - private baseline. non - private baseline COMPARE DP baselines. Fp sketch COMPARE DP baselines. DP baselines COMPARE Fp sketch. Fp sketch COMPARE non - private baseline. non - private baseline COMPARE Fp sketch. logarithmic factor USED-FOR non - private baseline. polylogarithmic space USED-FOR Fp sketch. differential privacy guarantee FEATURE-OF accuracy. differential privacy guarantee FEATURE-OF Fp sketch. accuracy EVALUATE-FOR Fp sketch. OtherScientificTerm is evaluation code. ,"This paper studies the problem of frequency moments estimation in the context of streaming. The authors propose a new algorithm called Fp sketch, which is based on the well-known FPN sketch. The main contribution of this paper is to show that FPN is a non-private algorithm, and that it can be used to estimate the frequency of a given signal in a private manner.   The main contributions of the paper are:  - The authors show that the FPN can be viewed as an extension of FPN in the setting where the signal is not available to the observer.  - They show that it is possible to use FPN as a streaming algorithm to estimate frequency moments in a privacy-preserving manner. - They prove that the accuracy of the algorithm is guaranteed to be non-differentially private.","This paper proposes a new streaming algorithm, Fp sketch, for frequency moments estimation. The main idea is to use a polylogarithmic space to approximate the logarithmics of the frequency moments. The authors show that the proposed algorithm is more accurate than the non-private baseline. They also provide a differential privacy guarantee for the accuracy of the algorithm."
4911,SP:3253b13851b5a3b5e3c8c6e24891db05903a4e57,"paradigm USED-FOR diverse strategies. Reward - Switching Policy Optimization ( RSPO ) USED-FOR diverse strategies. diverse strategies USED-FOR complex RL environments. Reward - Switching Policy Optimization ( RSPO ) HYPONYM-OF paradigm. trajectory - based novelty measurement USED-FOR optimization process. RSPO USED-FOR extrinsic and intrinsic rewards. trajectory - based novelty measurement USED-FOR RSPO. trajectory - based novelty measurement USED-FOR extrinsic and intrinsic rewards. policy optimization USED-FOR RSPO. extrinsic rewards USED-FOR policy optimization. extrinsic rewards USED-FOR RSPO. intrinsic diversity reward USED-FOR exploration. RSPO USED-FOR exploration. RSPO USED-FOR trajectories. policies USED-FOR trajectories. intrinsic diversity reward USED-FOR RSPO. single - agent particle - world tasks CONJUNCTION MuJoCo continuous control. MuJoCo continuous control CONJUNCTION single - agent particle - world tasks. multi - agent stag - hunt games CONJUNCTION StarCraftII challenges. StarCraftII challenges CONJUNCTION multi - agent stag - hunt games. RSPO USED-FOR strategies. MuJoCo continuous control CONJUNCTION multi - agent stag - hunt games. multi - agent stag - hunt games CONJUNCTION MuJoCo continuous control. single - agent particle - world tasks CONJUNCTION multi - agent stag - hunt games. multi - agent stag - hunt games CONJUNCTION single - agent particle - world tasks. OtherScientificTerm are learning policy, and sampled trajectory. ","This paper proposes a novel reward-switching policy optimization method for learning diverse strategies in complex RL environments. The proposed method is based on a trajectory-based novelty measurement that measures the novelty of trajectories sampled from a given policy. The novelty is computed using a combination of an extrinsic reward that encourages exploration and an intrinsic diversity reward that incentivizes the agent to explore new trajectories. The authors evaluate the proposed method on MuJoCo continuous control tasks, multi-agent stag-hunt games, and StarCraft II. ","This paper proposes a new reward-switching policy optimization (RSPO) framework for diverse strategies in RL. The authors propose a trajectory-based novelty measurement to measure the intrinsic and extrinsic rewards of RSPO. They show that the intrinsic diversity reward encourages exploration, while the extrinsics reward encourages the agent to explore the environment. They also show that RSPo can be used to learn diverse strategies for diverse environments. "
4927,SP:e3ab3aa87ab023bd9949b99a17d4b6e26c1473c0,"Diffusion models HYPONYM-OF generative models. GANs COMPARE autoregressive models. autoregressive models COMPARE GANs. sample quality CONJUNCTION autoregressive models. autoregressive models CONJUNCTION sample quality. autoregressive models USED-FOR likelihood scores. GANs HYPONYM-OF generative models. autoregressive models HYPONYM-OF generative models. sample quality EVALUATE-FOR GANs. method USED-FOR fast samplers. fast samplers USED-FOR diffusion model. flexible non - Markovian samplers USED-FOR diffusion models. Generalized Gaussian Diffusion Models ( GGDM ) HYPONYM-OF flexible non - Markovian samplers. degrees of freedom FEATURE-OF GGDM samplers. sample quality scores USED-FOR GGDM samplers. gradient descent USED-FOR sample quality scores. sample quality scores USED-FOR degrees of freedom. gradient descent USED-FOR GGDM samplers. reparametrization trick CONJUNCTION gradient rematerialization. gradient rematerialization CONJUNCTION reparametrization trick. sampling process USED-FOR optimization procedure. reparametrization trick USED-FOR optimization procedure. gradient rematerialization USED-FOR optimization procedure. DDSS USED-FOR unconditional image generation. datasets EVALUATE-FOR DDSS. FID scores HYPONYM-OF datasets. fine - tuning CONJUNCTION re - training. re - training CONJUNCTION fine - tuning. method CONJUNCTION pre - trained diffusion model. pre - trained diffusion model CONJUNCTION method. Generic is model. OtherScientificTerm are LSUN, and inference steps. Method is DDPM / DDIM baselines. ",This paper proposes a method to improve the sample quality of diffusion models. The main idea is to use non-Markovian samplers for sampling from the diffusion model. The authors show that the sampling process can be approximated by gradient descent. The proposed method is evaluated on unconditional image generation and FID scores. ,This paper proposes a method to improve the sample quality of GANs by training a fast non-Markovian diffusion model. The main idea is to use a generalized Gaussian Diffusion Model (GGDM) sampler to improve sample quality. The method is based on a reparametrization trick and a gradient rematerialization trick. The authors show that their method is able to achieve better sample quality than the state-of-the-art.
4943,SP:7a7506f2b5500a573c0cfb8b0822e5ea725c886a,Large Language Models ( LLMs ) USED-FOR factual information. embedding layer PART-OF LLMs. lightweight models HYPONYM-OF P - Adapters. embedding layer PART-OF lightweight models. continuous prompts USED-FOR LLM. LLM embeddings USED-FOR continuous prompts. LLM embeddings USED-FOR They. Mixture of Experts ( MoE ) models USED-FOR continuous prompts. classifier USED-FOR natural language prompts. classifier USED-FOR They. human - annotated data USED-FOR classifier. P - Adapters COMPARE MoE models. MoE models COMPARE P - Adapters. MoE models USED-FOR factual information. P - Adapters USED-FOR factual information. consistency EVALUATE-FOR baseline. PAdapters COMPARE baseline. baseline COMPARE PAdapters. precision CONJUNCTION consistency. consistency CONJUNCTION precision. consistency EVALUATE-FOR PAdapters. natural language queries USED-FOR baseline. precision EVALUATE-FOR PAdapters. LLM ’s embeddings USED-FOR natural language prompt. OtherScientificTerm is continuous ones. Method is P - Adapter. ,"This paper proposes a method to improve the accuracy and consistency of large language models (LLMs) using continuous prompts. The method is based on a mixture of experts (MoE) model, where a classifier is trained on continuous prompts and a human annotated data is used to annotate the classifier. The authors show that the proposed method is able to achieve better accuracy than the state-of-the-art MoE models.  ","This paper proposes a method to improve the accuracy of large language model (LLM) embeddings for continuous prompts. The proposed method is based on a mixture of experts (MoE) model, where a classifier is trained on human-annotated data and a human annotated data is used to annotate the prompt embedding. The authors show that the P-Adapters model outperforms the baseline model in terms of accuracy and consistency. "
4959,SP:35cdf71f027cc5168b55cc34c64bfb2f3087d6f5,"one - shot classification COMPARE CCTS. CCTS COMPARE one - shot classification. catastrophic forgetting CONJUNCTION over fitting. over fitting CONJUNCTION catastrophic forgetting. unclear distribution division FEATURE-OF continual learning task. continual learning task USED-FOR CCTS. Adaptive model training policy USED-FOR CCTS. Adaptive multi - distribution extraction policy PART-OF adaptability. fixed rules CONJUNCTION prior knowledge. prior knowledge CONJUNCTION fixed rules. ACCTS USED-FOR data distributions. data distributions USED-FOR time series evolution. time series evolution CONJUNCTION model change. model change CONJUNCTION time series evolution. method COMPARE baselines. baselines COMPARE method. real - world datasets EVALUATE-FOR method. Task is real - world applications. OtherScientificTerm are vital signs, features, multi - distribution form, independent identically distributed premise, and fixed division rule. Generic are concept, models, and model. Method are Continuous Classification of Time Series ( CCTS ), and Adaptive importance - based replay policy. ","This paper proposes a continual learning method for continual classification of time series. The proposed method is based on the idea of multi-distribution form, where each time series is represented as a set of independent identically distributed premise, and the goal is to learn a classifier that is able to adapt to changes in the distribution over time. The authors propose to use an Adaptive importance-based replay policy to sample from multiple data distributions and use them to update the model. The experiments show that the proposed method outperforms baselines on time series classification tasks. ",This paper proposes a method for continuous classification of time series (CCTS) with a multi-distribution form. The method is based on ACCTS (adaptive importance-based replay policy) and adaptive model training policy (Adaptive Multi-Distribution Extraction Policy). The proposed method is evaluated on a variety of real-world datasets. 
4975,SP:d9b74b749aa465496763d3a3a9bf3a53e800587e,internal representations of past inputs USED-FOR language models. approximate kNN lookup USED-FOR language modeling. benchmarks CONJUNCTION tasks. tasks CONJUNCTION benchmarks. approximate kNN lookup PART-OF memory. tasks EVALUATE-FOR language modeling. benchmarks EVALUATE-FOR language modeling. generic webtext ( C4 ) HYPONYM-OF tasks. books ( PG-19 ) HYPONYM-OF tasks. books ( PG-19 ) HYPONYM-OF benchmarks. generic webtext ( C4 ) HYPONYM-OF benchmarks. theorems USED-FOR model. Method is Language models. ,"This paper proposes to use approximate kNN to model the past representations of past inputs to improve the performance of language models. Specifically, the authors propose to use a kNN-based representation of the past input to update the current model parameters. The proposed method is evaluated on a variety of tasks, including PG-19 and generic webtext.   ","This paper proposes a new approach to approximate kNN lookup for language models. The approach is based on the idea that kNNs can be used to store the past representations of past inputs in the memory of a language model. The authors show that the approach can be applied to a variety of language modeling tasks, including C4, PG-19, and generic webtext. The paper also provides theorems for the proposed approach.  "
4991,SP:7a1bbf86c3fdb8738aa826ca330493e857d050ba,"MLMs USED-FOR probability distribution. masked language modeling ( MLM ) objective USED-FOR models. energy - based sequence models USED-FOR MLMs. MLMs USED-FOR energy parametrizations. Metropolis – Hastings Monte Carlo algorithm USED-FOR tractable sampling scheme. masked conditionals USED-FOR masked language models. energybased models USED-FOR open - ended unconditional generation. approach COMPARE undirected generation approaches. undirected generation approaches COMPARE approach. stationary distribution FEATURE-OF Markov chain. Method are parametrizations, and sampling algorithm. Task is machine translation. ","This paper proposes a masked language modeling (MLM) objective for open-ended unconditional generation in machine translation. The proposed method is based on an energy-based sequence model, where the energy parametrizations are modeled using masked conditionals. The authors propose a Metropolis-Hastings Monte Carlo (MHMC) sampling scheme to obtain tractable sampling scheme. The experimental results show that the proposed method outperforms the state-of-the-art undirected generation approaches. ","This paper proposes a method for open-ended unconditional language generation from masked language models (MLMs). The method is based on the Metropolis-Hastings Monte Carlo (MHMC) algorithm, which is a tractable sampling scheme. The authors show that the proposed method is tractable and can be applied to a variety of language models. They also show that their method can be used to generate language models that can be trained to generate a language model that is more tractable."
5007,SP:011626ba4fafee13d4a30e3f13c1df5b7071a7f1,"data augmentation USED-FOR deep neural networks. deep neural networks USED-FOR NLP tasks. data augmentation USED-FOR NLP tasks. labeled samples USED-FOR It. low - data or classimbalanced regimes HYPONYM-OF labeled samples. parameter tuning CONJUNCTION inherent randomness. inherent randomness CONJUNCTION parameter tuning. inherent randomness USED-FOR augmentation techniques. parameter tuning USED-FOR augmentation techniques. reward function USED-FOR policy. augmentation USED-FOR NLP tasks. augmentation strategy USED-FOR task. learning data augmentation policy USED-FOR augmentation strategy. reward function USED-FOR augmentation policy. learning - based augmentation COMPARE augmentation schemes. augmentation schemes COMPARE learning - based augmentation. augmentations USED-FOR task. text classification tasks EVALUATE-FOR augmentation schemes. text classification tasks EVALUATE-FOR learning - based augmentation. augmentation policy USED-FOR tasks. method USED-FOR low - data and class - imbalanced regimes. OtherScientificTerm are informative training signals, and semantic similarity. Method are data augmentation policy, and sample re - weighting scheme. Generic is model. ",This paper proposes a novel data augmentation strategy for NLP tasks. The proposed method is based on learning a policy that selects a subset of training samples that are most similar to the training samples and re-weights them with a sample re-weighting scheme to improve the performance of the model. Experiments show that the proposed method outperforms the baseline methods in both low-data and class-imbalanced settings.,"This paper proposes a new data augmentation strategy for NLP tasks. The proposed method is based on learning a data-augmentation policy, where the augmentation policy is trained with a sample re-weighting scheme. The authors show that the proposed method can be applied to both low-data and class-imbalanced regimes. They show that their method outperforms the state-of-the-art in terms of performance on text classification tasks. "
5023,SP:69d41a862ea189f72d4e8af2854e27b95a91fa41,"Meta - learning USED-FOR offline reinforcement learning ( OMRL ). augmented state USED-FOR task identity. intra - task attention mechanism CONJUNCTION inter - task contrastive learning objectives. inter - task contrastive learning objectives CONJUNCTION intra - task attention mechanism. sparse reward CONJUNCTION distribution shift. distribution shift CONJUNCTION sparse reward. sparse reward USED-FOR task representation learning. distribution shift USED-FOR task representation learning. FOCAL HYPONYM-OF SOTA OMRL algorithms. meta - RL benchmarks EVALUATE-FOR prior algorithms. Method are RL algorithms, and context - based encoder. ", is a meta-RL algorithm for offline reinforcement learning. The paper proposes a context-based encoder for meta-learning. The encoder is trained with a contrastive learning objective. The proposed method is shown to outperform the state-of-the-art methods on a variety of offline RL benchmarks.,This paper proposes a new meta-learning algorithm for offline reinforcement learning (OMRL) with augmented state and task identity. The main idea is to use a context-based encoder to learn the task identity of each task. The proposed method is evaluated on a variety of meta-RL benchmarks.  
5039,SP:ed86c60850d5c8302dcf1c2167db303e778fe681,belief state FEATURE-OF partially observable Markov system. parametric sequential generative modeling methods USED-FOR problem setting. methods USED-FOR belief state modeling. belief state modeling USED-FOR multi - agent settings. policies PART-OF belief model. inference - time improvement framework USED-FOR parametric sequential generative modeling methods. belief fine - tuning ( BFT ) HYPONYM-OF inference - time improvement framework. approximate dynamic programming USED-FOR model parameters. fine - tuning USED-FOR model parameters. BFT USED-FOR model parameters. fine - tuning FEATURE-OF approximate dynamic programming. fine - tuning USED-FOR BFT. approximate dynamic programming USED-FOR BFT. accuracy EVALUATE-FOR belief model. It USED-FOR belief model. it USED-FOR model. accuracy EVALUATE-FOR It. belief model USED-FOR BFT. BFT USED-FOR approximate public belief state search. imperfect - information games FEATURE-OF approximate public belief state search. Method is dynamics model. OtherScientificTerm is specialization. ,This paper proposes a belief fine-tuning method for belief state modeling in multi-agent Markov systems. The method is based on a belief model that is trained to predict the belief state of a partially observable Markov system. The authors show that the model parameters are learned by approximate dynamic programming (FTP) and fine-tune the parameters of the model during inference time. The proposed method is shown to be able to improve the performance of belief models in the presence of imperfect information.   ,This paper proposes a belief fine-tuning (BFT) framework for parametric sequential generative modeling of partially observable Markov systems. It is based on the idea of approximate dynamic programming (ADP) that can be used to fine-tune the model parameters. The authors show that BFT can improve the accuracy of the model in a multi-agent setting with imperfect information games. They also show that it can be applied to approximate public belief state search in imperfect-information games. 
5055,SP:6150725599c10f0e26f0d7cb1fc04b5b227a4456,"accuracy loss CONJUNCTION slow training runtime. slow training runtime CONJUNCTION accuracy loss. accuracy loss EVALUATE-FOR methods. sparse matrices USED-FOR sparsity mask. fixed structure FEATURE-OF sparse matrices. products of butterfly matrices HYPONYM-OF fixed structure. hardware USED-FOR butterfly ( block and flat ). attention CONJUNCTION MLP. MLP CONJUNCTION attention. fixed sparsity pattern USED-FOR network layers. method USED-FOR network layers. Pixelated Butterfly USED-FOR method. flat block butterfly and low - rank matrices USED-FOR fixed sparsity pattern. fixed sparsity pattern USED-FOR Pixelated Butterfly. fixed sparsity pattern USED-FOR method. flat block butterfly and low - rank matrices USED-FOR method. attention HYPONYM-OF network layers. MLP HYPONYM-OF network layers. Pixelated Butterfly COMPARE butterfly. butterfly COMPARE Pixelated Butterfly. Pixelated Butterfly USED-FOR training. ImageNet classification and WikiText-103 language modeling tasks EVALUATE-FOR sparse models. dense MLP - Mixer CONJUNCTION Vision Transformer. Vision Transformer CONJUNCTION dense MLP - Mixer. sparse models COMPARE dense MLP - Mixer. dense MLP - Mixer COMPARE sparse models. Vision Transformer CONJUNCTION GPT-2 medium. GPT-2 medium CONJUNCTION Vision Transformer. sparse models COMPARE GPT-2 medium. GPT-2 medium COMPARE sparse models. sparse models COMPARE Vision Transformer. Vision Transformer COMPARE sparse models. ImageNet classification and WikiText-103 language modeling tasks EVALUATE-FOR dense MLP - Mixer. ImageNet classification and WikiText-103 language modeling tasks EVALUATE-FOR GPT-2 medium. accuracy EVALUATE-FOR sparse models. Method are Overparameterized neural networks, Sparse model training, and model components. Metric are computational cost, and accuracy – efficiency tradeoffs. OtherScientificTerm is butterfly matrices. ","This paper proposes a method for training sparse neural networks with fixed sparsity masks. The proposed method, called Pixelated Butterfly, is based on the products of products of butterfly matrices, which are matrices with a fixed structure. The paper shows that the proposed method is computationally efficient in terms of training time and accuracy, and achieves competitive performance on ImageNet classification and WikiText-103 tasks.","This paper proposes a new method for training sparse neural networks with fixed sparsity masks. The proposed method, called Pixelated Butterfly, is based on the flat block butterfly and low-rank matrices. The paper also proposes a method for sparse models with high accuracy. The method is evaluated on ImageNet classification and WikiText-103 language modeling tasks. "
5071,SP:136e31054a55abca840f6478491972023c2296cb,"score matching USED-FOR data distribution. formulation USED-FOR controllable generation. class center PART-OF forward and reverse process. class center USED-FOR conditional diffusion probabilistic model. faster sampling USED-FOR method. inception score CONJUNCTION FID score. FID score CONJUNCTION inception score. state - of - the - art methods COMPARE conditional image generation. conditional image generation COMPARE state - of - the - art methods. framework COMPARE state - of - the - art methods. state - of - the - art methods COMPARE framework. CIFAR-10 USED-FOR conditional image generation. FID score EVALUATE-FOR conditional image generation. inception score EVALUATE-FOR conditional image generation. Method are Score - based generative models, and diffusion probabilistic models. OtherScientificTerm are Markov chain, and class clustering phenomenon. ",This paper proposes a method for conditional image generation based on score-based generative models. The proposed method is based on a conditional diffusion probabilistic model with a class center in the forward and reverse process. The class center is defined to be the center of the Markov chain of the forward process and the class clustering phenomenon. The authors show that the class center can be used to improve the sampling speed and improve the FID score of the generated images.,"This paper proposes a new method for conditional image generation based on a score-based generative model. The main idea is to use a conditional diffusion probabilistic model to predict the class center in the forward and reverse process. The method is motivated by the class clustering phenomenon, which can be attributed to the Markov chain phenomenon. The proposed method is evaluated on CIFAR-10, and it is shown that the proposed method outperforms state-of-the-art methods. "
5087,SP:fc2196f1f4ecd864398fed6640ff3f8b19870763,"fixed domain - invariant features CONJUNCTION common hypotheses. common hypotheses CONJUNCTION fixed domain - invariant features. generalization EVALUATE-FOR domain generalization ( DG ) approaches. generalization EVALUATE-FOR prediction tasks. label - informative features USED-FOR label prediction task. label - informative features USED-FOR latent sub - spaces. DG benchmarks EVALUATE-FOR it. DG benchmarks EVALUATE-FOR method. Metric is generalization capacity. Generic are assumption, and approaches. OtherScientificTerm are invariant hypothesis, and sub - spaces. Method is LASSO. ","This paper proposes a method for domain generalization (DG) based on label-in-the-loop (LISBO) and domain-invariant (DID) methods. The main idea is to learn a set of latent sub-spaces that are invariant to different domains, and then use them to train a classifier to predict the label of the target domain. The proposed method is evaluated on a variety of domain-generalization benchmarks.   ","This paper proposes a new method for domain generalization (DG) based on the LASSO framework. The key idea is to use a set of latent sub-spaces to learn the latent features of the latent space, which are then used to predict the label of the target domain. The proposed method is evaluated on a variety of tasks, and it is shown to perform better than the state-of-the-art."
5103,SP:6e8e5bdeb77e3cafe1975da8411fb65118955d14,"kernel thinning ( KT ) algorithm USED-FOR probability distribution. kernel thinning ( KT ) algorithm COMPARE independent sampling. independent sampling COMPARE kernel thinning ( KT ) algorithm. reproducing kernel Hilbert space ( RKHS ) USED-FOR independent sampling. KT USED-FOR RKHS. kernel CONJUNCTION distribution. distribution CONJUNCTION kernel. KT USED-FOR kernel. KT USED-FOR dimension - free guarantees. dimension - free guarantees FEATURE-OF kernel. inverse multiquadric CONJUNCTION sinc. sinc CONJUNCTION inverse multiquadric. Gaussian CONJUNCTION inverse multiquadric. inverse multiquadric CONJUNCTION Gaussian. target KT COMPARE square - root KT. square - root KT COMPARE target KT. maximum mean discrepancy ( MMD ) guarantees EVALUATE-FOR square - root KT. target KT HYPONYM-OF analytic kernels. maximum mean discrepancy ( MMD ) guarantees EVALUATE-FOR target KT. sinc HYPONYM-OF analytic kernels. Gaussian HYPONYM-OF analytic kernels. inverse multiquadric HYPONYM-OF analytic kernels. Laplace CONJUNCTION Matérn. Matérn CONJUNCTION Laplace. fractional power kernel USED-FOR KT. Laplace HYPONYM-OF non - smooth kernels. Matérn HYPONYM-OF non - smooth kernels. MMD guarantees CONJUNCTION individual function guarantees. individual function guarantees CONJUNCTION MMD guarantees. KT USED-FOR target and power kernels. individual function guarantees FEATURE-OF target KT. target KT CONJUNCTION KT+. KT+ CONJUNCTION target KT. integration error EVALUATE-FOR target KT. integration error EVALUATE-FOR KT+. OtherScientificTerm are square - root kernel, square - roots, and differential equation posteriors. ",This paper studies the problem of estimating the distribution of kernels in the reproducing kernel Hilbert space (RKHS) using kernel thinning (KT) algorithms. The authors show that KT is dimension-free for the target and power kernels and achieves maximum mean discrepancy (MMD) guarantees for the square-root kernel and the fractional power kernel. They also provide individual function guarantees for target KT and KT+.   ,This paper proposes a new kernel thinning (KT) algorithm for the reproducing kernel Hilbert space (RKHS) problem. The main contribution of the paper is to provide dimension-free guarantees for the target and power kernels. The authors show that the target KT can achieve maximum mean discrepancy (MMD) and individual function guarantees for both the target kernel and power kernel. They also show that KT+ can achieve better integration error than square-root KT. 
5119,SP:645c3f1864aa843d4899fc2406f694b5aab8460d,"Combinatorial optimization USED-FOR real - world problems. open - source benchmark suite USED-FOR NP - hard MAXIMUM INDEPENDENT SET problem. weighted and unweighted variants FEATURE-OF open - source benchmark suite. benchmark suite USED-FOR guided tree search algorithm. graph convolution network USED-FOR tree search. code quality CONJUNCTION extensibility. extensibility CONJUNCTION code quality. graph convolution network USED-FOR solution structure. random values USED-FOR graph convolution network. extensibility EVALUATE-FOR algorithm. code quality EVALUATE-FOR algorithm. graph kernelization HYPONYM-OF algorithmic techniques. algorithmic techniques USED-FOR tree search. tree search implementations COMPARE solvers. solvers COMPARE tree search implementations. competitive solution quality EVALUATE-FOR GNN. GNN USED-FOR solver. reinforcement learning USED-FOR solver. Method are graph neural networks ( GNNs ), machine learning - based solvers, and classical algorithmic solvers. OtherScientificTerm are NP - hard problems, and problem - specific solution structure. Generic is suite. ",This paper proposes a new benchmark suite for combinatorial optimization problems with polynomial in-distribution solutions. The benchmark consists of weighted and unweighted variants of the classic max-in-independent set problem. The authors propose to use a graph convolution network (GNN) to model the solution structure of the problem and use reinforcement learning to improve the performance of the solver. The proposed method is shown to outperform the state-of-the-art solvers on the benchmark.,"This paper presents an open-source benchmark suite for solving NP-hard problems with graph neural networks (GNNs). The benchmark suite consists of weighted and unweighted variants of the classic tree search algorithm. The authors show that the proposed algorithm is competitive in terms of code quality, extensibility, and code quality. They also show that their method is competitive with classical algorithmic solvers."
5135,SP:155ecd17d264a084b014abdfd0362146d8fb07e0,Quantization USED-FOR compressing Convolutional Neural Networks ( CNNs ). computational resources USED-FOR compressing Convolutional Neural Networks ( CNNs ). semantic segmentation CONJUNCTION depth prediction. depth prediction CONJUNCTION semantic segmentation. prediction accuracy EVALUATE-FOR networks. depth prediction HYPONYM-OF image - to - image tasks. semantic segmentation HYPONYM-OF image - to - image tasks. approach USED-FOR activation maps compression. activation maps compression USED-FOR 1 × 1 convolutions. 1 × 1 convolutions PART-OF CNNs. compression ratios CONJUNCTION computational savings. computational savings CONJUNCTION compression ratios. computational savings CONJUNCTION low bit quantization rates. low bit quantization rates CONJUNCTION computational savings. low bit quantization rates EVALUATE-FOR WCC. computational savings EVALUATE-FOR WCC. accuracy EVALUATE-FOR WCC. compression ratios EVALUATE-FOR WCC. hardware - friendly Haar - wavelet transform USED-FOR image compression. convolution PART-OF compressed activation map. 1× 1 convolution PART-OF network architecture. 1× 1 convolution USED-FOR WCC. network architecture USED-FOR WCC. WCC CONJUNCTION light quantization. light quantization CONJUNCTION WCC. Method is Convolutional Neural Networks ( CNNs ). OtherScientificTerm is quantization. Metric is compression rates. ,This paper proposes a method to compress the activation maps of convolutional neural networks (CNNs) using a hardware-friendly Haar-wavelet transform (WCC) method. The method is based on the observation that 1x1 convolutions in CNNs can be reduced to 1-1/2 convolutions with low bit quantization rates. Theoretical analysis and experimental results show that the proposed method achieves better compression ratios and computational savings.   ,This paper proposes a new compression method for 1x1 convolutional neural networks (CNNs). The main idea is to use a Haar-wavelet transform (WCC) to compress the activation maps of a CNN. The authors show that the compression ratio of WCC can be reduced by a factor of 1. They also show that their method can be combined with light quantization to reduce the quantization rate of the network.  
5151,SP:004865e6affad32403b7965493a53c8a7ffdda0a,"accelerated learning dynamics USED-FOR correlated and coarse correlated equilibria. normal - form games FEATURE-OF correlated and coarse correlated equilibria. sequential and simultaneous moves CONJUNCTION imperfect information. imperfect information CONJUNCTION sequential and simultaneous moves. imperfect information FEATURE-OF extensive - form games. sequential and simultaneous moves FEATURE-OF extensive - form games. no - regret learning dynamics USED-FOR extensive - form correlated equilibrium ( EFCE ). O(T 3/4)-approximate EFCE FEATURE-OF correlated distribution of play. structured Markov chain USED-FOR refined perturbation analysis. refined perturbation analysis USED-FOR stability of certain fixed point strategies. Task is learning in games. Method are accelerated dynamics, and framework of -regret. OtherScientificTerm is prior rate. ","This paper studies the problem of learning in games with correlated and coarse correlated equilibria. In particular, the authors consider the case of extensive-form games with sequential and simultaneous moves and imperfect information. The authors show that in this setting, the learning dynamics converge to a correlated equilibrium that is O(T 3/4)-approximate to the correlated distribution of play. They also show that the stability of certain fixed point strategies can be improved by using the accelerated dynamics. ","This paper studies correlated and coarse correlated equilibria in normal-form games. In particular, the authors consider the case of extensive-form correlated equilibrium (EFCE), which is a correlated distribution of play. The authors show that the EFCE is O(T 3/4)-approximate, which is the average of the correlated distribution and the coarse correlated distribution. They also provide a refined perturbation analysis for the stability of certain fixed point strategies."
5167,SP:ee545ff83df4d7ff256ac61fbe0eb0765f52f1d5,"discrete actions COMPARE continuous actions. continuous actions COMPARE discrete actions. maximum of the action - value function USED-FOR dynamic programmingbased methods. Action Quantization from Demonstrations ( AQuaDem ) USED-FOR discretization of continuous action spaces. method USED-FOR discretization of continuous action spaces. priors of demonstrations USED-FOR discretization of continuous action spaces. discrete action deep RL algorithm USED-FOR continuous control problem. RL CONJUNCTION RL. RL CONJUNCTION RL. demonstrations CONJUNCTION RL. RL CONJUNCTION demonstrations. play data USED-FOR RL. Imitation Learning EVALUATE-FOR method. demonstrations USED-FOR RL. setups EVALUATE-FOR method. Imitation Learning HYPONYM-OF setups. RL HYPONYM-OF setups. RL HYPONYM-OF setups. human data COMPARE synthetic data. synthetic data COMPARE human data. human data USED-FOR setups. AQuaDem COMPARE continuous control methods. continuous control methods COMPARE AQuaDem. hard manipulation tasks EVALUATE-FOR continuous control methods. hard manipulation tasks EVALUATE-FOR AQuaDem. sample efficiency EVALUATE-FOR continuous control methods. sample efficiency EVALUATE-FOR AQuaDem. Task are Reinforcement Learning ( RL ), exploration problems, and exploration problem. OtherScientificTerm is action space. ","This paper proposes a method to learn discrete action discretization from demonstrations in reinforcement learning. The main idea is to use the priors of demonstrations to learn a discretized continuous action space, which is then used to train a deep RL algorithm. The method is evaluated on simulated and real-world environments.   ",This paper proposes a method for discretizing continuous action spaces from demonstrations. The main idea is to use a discrete action deep RL algorithm to solve the continuous control problem. The proposed method is based on the Action Quantization from Demonstrations (AQuaDem) framework. The method is evaluated on simulated and real-world environments.
5183,SP:4b39279b98d6aa311bb49dd1384925f9d6f66c2d,domain generalization USED-FOR robust model. domain generalization USED-FOR semantic segmentation. labeled synthetic ( source ) data USED-FOR robust model. channelwise mean USED-FOR style features. adversarial style augmentation ( AdvStyle ) approach USED-FOR hard stylized images. style feature USED-FOR AdvStyle. adversarial training USED-FOR it. adversarial style feature USED-FOR adversarial image. adversarial image USED-FOR robust model training. AdvStyle USED-FOR models. synthetic - to - real semantic segmentation benchmarks EVALUATE-FOR AdvStyle. AdvStyle USED-FOR model. AdvStyle USED-FOR domain generalized image classification. datasets EVALUATE-FOR AdvStyle. OtherScientificTerm is image style variation. Task is overfitting. ," image classification is an important task in semantic segmentation. However, it is difficult to train a robust model in this setting due to the large amount of image style variation. This paper proposes an adversarial style augmentation (AdvStyle) approach for hard stylized images to improve the domain generalization. The proposed AdvStyle is based on adversarial training to generate a style feature from the adversarial image. The style feature is then used to train the robust model training. Experiments show that AdvStyle can improve the performance of domain generalized image classification on several datasets.","This paper proposes a new approach for domain generalization in semantic segmentation. The authors propose an adversarial style augmentation (AdvStyle) approach for hard stylized images, where the style feature is trained with adversarial training. They show that AdvStyle can improve the performance of the model on synthetic and real-world datasets. "
5199,SP:4a2e6d70b383e4941e0bc44e7e82972b22e26792,"rigid, dramatic gestures USED-FOR recognition. rigid, dramatic gestures USED-FOR systems. neuromorphic gesture analysis system USED-FOR event - based gesture data. high temporal resolution FEATURE-OF neuromorphic gesture analysis system. high temporal resolution FEATURE-OF event - based gesture data. latent space representation USED-FOR similarity of mid - air gesture data. Dynamic Vision Sensor ( DVS ) USED-FOR event - based data. it USED-FOR sparse, noisy inputs. it USED-FOR interpretable latent space representation. interpretable latent space representation USED-FOR sparse, noisy inputs. DVSGesture dataset EVALUATE-FOR Hybrid GuidedVAE. classification accuracy EVALUATE-FOR Hybrid GuidedVAE. T - SNE plots USED-FOR interpretable latent space representation. neuromorphic hardware USED-FOR model. Method is mid - air gesture recognition systems. Generic are they, approach, and algorithm. ",This paper proposes a novel method for mid-air gesture recognition based on the Dynamic Vision Sensor (DVS) data. The proposed method is based on learning a latent space representation for the similarity of mid- air gesture data from the DVS. This latent space is then used to train a neural network model that predicts the similarity between the event-based gesture data with high temporal resolution. Experiments show that the proposed method achieves state-of-the-art performance on the proposed DVSGesture dataset.,"This paper proposes a new approach for mid-air gesture recognition based on the Dynamic Vision Sensor (DVS) and a neuromorphic gesture analysis system (GuidedVAE). The proposed approach is based on an interpretable latent space representation of the event-based gesture data, which can be used to predict the similarity of mid- air gesture data. The proposed method is evaluated on the DVSGesture dataset and compared with the state of the art."
5215,SP:2e66468a6b94177e54b0052b97713ee63902c278,"accuracy EVALUATE-FOR neuron - based networks. tabular data USED-FOR Deep learning. Internet of Things ( IoT ) CONJUNCTION drone. drone CONJUNCTION Internet of Things ( IoT ). drone CONJUNCTION Natural User Interface ( NUI ) application. Natural User Interface ( NUI ) application CONJUNCTION drone. annealing mechanism USED-FOR S - HTE inference. S - HTE USED-FOR internal representations. ferns USED-FOR S - HTE. classification and regression benchmark EVALUATE-FOR accuracy. OtherScientificTerm are computational capacity, deep learning capabilities, and neurons. Method are deep learning methods, and PyTorch implementation. Generic is it. Metric is computational complexity. ","This paper proposes a method for training a neuron-based deep learning model on tabular data. The proposed method is based on annealing the weights of the neurons in the network, which is an efficient way to reduce the computational cost of training the network. The method is evaluated on classification and regression tasks.   ","This paper proposes a new method to improve the performance of neuron-based deep learning models. The proposed method is based on annealing mechanism, where the neurons are annealed using a fern-hierarchical horticulture-temperature-estimation (S-HTE) method. The authors show that the proposed method outperforms the state-of-the-art methods in terms of accuracy and computational complexity. "
5231,SP:b238db9252d83a13438bb747d70e635bb9945958,undirected stateonly experience USED-FOR learning value functions. tabular Q - learning USED-FOR value function. tabular Q - learning USED-FOR discrete Markov decision processes ( MDPs ). refinement of the action space FEATURE-OF value function. offline RL method USED-FOR value functions. Latent Action Q - learning HYPONYM-OF offline RL method. state - only experience USED-FOR value functions. Latent Action Q - learning ( LAQ ) USED-FOR value functions. discrete latent actions USED-FOR Q - learning. latent - variable future prediction model USED-FOR Q - learning. latent - variable future prediction model USED-FOR discrete latent actions. Q - learning USED-FOR Latent Action Q - learning ( LAQ ). Q - learning USED-FOR value functions. LAQ USED-FOR value functions. value functions CONJUNCTION value functions. value functions CONJUNCTION value functions. ground truth actions USED-FOR value functions. Value functions USED-FOR acquisition of goal - directed behavior. Value functions USED-FOR domain - specific low - level controllers. LAQ USED-FOR acquisition of goal - directed behavior. LAQ USED-FOR Value functions. imitation learning oracles CONJUNCTION competing methods. competing methods CONJUNCTION imitation learning oracles. alternatives CONJUNCTION imitation learning oracles. imitation learning oracles CONJUNCTION alternatives. 2D grid world CONJUNCTION 3D visual navigation. 3D visual navigation CONJUNCTION 2D grid world. LAQ COMPARE alternatives. alternatives COMPARE LAQ. environments EVALUATE-FOR LAQ. LAQ COMPARE competing methods. competing methods COMPARE LAQ. LAQ CONJUNCTION imitation learning oracles. imitation learning oracles CONJUNCTION LAQ. 3D visual navigation HYPONYM-OF environments. 2D grid world HYPONYM-OF environments. ,"This paper proposes Latent Action Q-learning (LAQ), a method for learning value functions in discrete Markov decision processes (MDPs) from state-only experience. The main idea is to use a latent variable future prediction model to predict discrete latent actions, which are then used to refine the action space and learn the value function. The proposed method is evaluated in two environments: a 2D grid world and a 3D visual navigation task.  ","This paper proposes Latent Action Q-learning (LAQ), an offline RL method for learning value functions from discrete Markov decision processes (MDPs). The method is based on tabular Q learning, where the value function is learned from the action space of a discrete MDP. The method uses a latent-variable future prediction model to predict discrete latent actions, which are then used to learn the value functions. The authors show that LAQ achieves state-of-the-art performance in 3D visual navigation and 2D grid environments."
5247,SP:108ebe9045a9e2b8b5aba8352733782462db8a81,"large models USED-FOR deep learning applications. low - latency and high - bandwidth interconnect USED-FOR distributed training algorithms. distributed training algorithms USED-FOR models. GPU clusters USED-FOR models. model parallelism USED-FOR large models. SWARM Parallelism1 HYPONYM-OF model - parallel training algorithm. model - parallel training algorithm USED-FOR swarms of poorly connected, heterogeneous unreliable devices. SWARM USED-FOR temporary randomized pipelines. compression - aware architecture modifications USED-FOR approach. network throughput FEATURE-OF swarm of preemptible T4 GPUs. shared parameters USED-FOR large Transformer language model. swarm of preemptible T4 GPUs USED-FOR large Transformer language model. OtherScientificTerm are dedicated GPU clusters, distributed training setups, and preemptible ” instances. Generic is setups. ",", this paper proposes a model-parallel training algorithm called SWARM for distributed training of large models. The main idea is to use a series of temporary randomized pipelines to train the model in a distributed fashion. The authors show that the proposed method can be used to train a large Transformer language model with shared parameters.   ","This paper proposes a new model-parallel training algorithm, SWARM, for distributed training of large models. The main idea of SWARM is to train a large model in a distributed manner, where each model is trained on a small number of GPUs. The authors show that the proposed method can be used in a variety of settings, including: (1) when the number of GPU clusters is large, and (2) when there is a large number of pre-emptible instances of the model. The proposed method is evaluated on a large Transformer language model, and it is shown that it can be applied to a number of different settings. "
5263,SP:91d2f094d5481651b554f58aecc2a6207057a47c,"Offline reinforcement learning USED-FOR policies. fixed dataset USED-FOR real - world applications. fixed dataset USED-FOR Offline reinforcement learning. fixed dataset USED-FOR policies. behavior policy COMPARE policy. policy COMPARE behavior policy. transition dynamics PART-OF offline experiences. offline training CONJUNCTION online tuning. online tuning CONJUNCTION offline training. online data USED-FOR agent policy. deployment efficiency CONJUNCTION sample efficiency. sample efficiency CONJUNCTION deployment efficiency. online transition correction ( OTC ) USED-FOR biased transition dynamics. offline and online experiences USED-FOR online transition correction ( OTC ). sampling probabilities USED-FOR online transition correction ( OTC ). distances USED-FOR similarity between transitions. transition similarity USED-FOR adaptive rank - based prioritization. embedding - based and valuebased distance HYPONYM-OF distances. OTC USED-FOR agent policies. OTC USED-FOR online tuning. agent policies USED-FOR online tuning. data efficiency EVALUATE-FOR OTC. OTC COMPARE baselines. baselines COMPARE OTC. tasks EVALUATE-FOR baselines. tasks EVALUATE-FOR OTC. Task is offline decentralized multi - agent reinforcement learning. OtherScientificTerm are online execution, value estimates, uncoordinated and suboptimal policies, and transition bias. Material is online experiences. ","This paper proposes an online transition correction (OTC) method for offline decentralized multi-agent reinforcement learning. The proposed method is based on the observation that the transition dynamics between offline and online experiences can be biased. To address this issue, the authors propose to use an embedding-based and value-based distance to measure the similarity between transitions and use this distance as a ranking function to select the policies to be used for online tuning. Experiments show that the proposed method can improve the sample efficiency and deployment efficiency of offline RL algorithms. ","This paper proposes an online transition correction (OTC) method for offline decentralized multi-agent reinforcement learning. OTC is based on the observation that the transition dynamics between offline and online experiences can be biased due to the bias in the online data. To address this bias, the authors propose to use an embedding-based and value-based distance to measure the similarity between transitions. The authors also propose an adaptive rank-based prioritization method for online tuning. Experiments show that OTC can improve the sample efficiency and deployment efficiency of the agent policies."
5279,SP:d0e650d568214481b07a0452ec606ccbf6d05410,"computational footprint EVALUATE-FOR Deep Neural Networks ( DNNs ) training. 4 - bit quantization USED-FOR methods. computational footprint EVALUATE-FOR training process. loss gradients HYPONYM-OF neural gradients. unbiased quantization USED-FOR quantized neural network training. logarithmic unbiased quantization ( LUQ ) method USED-FOR forward and backward phase. high precision fine - tuning CONJUNCTION variance reduction method. variance reduction method CONJUNCTION high precision fine - tuning. ImageNet FEATURE-OF ResNet50. method USED-FOR low precision format. OtherScientificTerm are intermediate neural layers, multiplications, and multiplier. Generic is it. Method is 4 - bit training. ",This paper proposes a new quantized neural network training method called logarithmic unbiased quantization (LUQ) for quantized deep neural networks (DNNs). The proposed method is based on the idea of quantizing the gradients of intermediate layers in the forward and backward phase of the training process. The authors show that the proposed method can be used to reduce the computational cost of quantized DNN training from $O(1/2)$ to $O(\sqrt{T})$ using only 4-bit quantization. ,"This paper proposes a new quantized neural network training method, called logarithmic unbiased quantization (LUQ), for quantized deep neural networks (DNNs). The proposed method is based on the idea of forward and backward quantization, where the forward quantization is done in the forward phase, and the backward phase is performed in the back phase. The authors show that the proposed method outperforms the state-of-the-art quantized DNN training methods in terms of accuracy, variance reduction, and fine-tuning."
5295,SP:f2862d1f987164ed6c3c375cd8962e57c369373b,"Polythetic classifications COMPARE monothetic classifications. monothetic classifications COMPARE Polythetic classifications. features USED-FOR monothetic classifications. shared patterns of features USED-FOR Polythetic classifications. threshold meta - learners USED-FOR functions. embedding dimension USED-FOR functions. embedding dimension USED-FOR threshold meta - learners. Prototypical Networks HYPONYM-OF threshold meta - learners. attentional classifiers USED-FOR problems. linear embedding dimension USED-FOR attentional classifiers. Matching Networks HYPONYM-OF attentional classifiers. linear embedding dimension USED-FOR problems. attentional models USED-FOR misclassification. selfattention feature - selection mechanism USED-FOR non - discriminative features. approach USED-FOR meta - learning Boolean functions. Material is natural world. OtherScientificTerm are task - relevant features, and task - irrelevant features. Task is meta - learning problems. ","This paper studies the problem of meta-learning Boolean functions in the presence of misclassification. In particular, the authors propose to use shared patterns of features as a meta-learner to learn a set of functions that are shared across tasks. The proposed method is based on a self-attention feature selection mechanism that selects non-discriminative features from the set of features. The authors show that the proposed method outperforms the state-of-the-art methods on the task-relevant and task-irrelevant tasks. ","This paper proposes a new meta-learning framework for polythetic classifications. The key idea is to use the shared patterns of features as a meta-learner for the task-relevant features, and the non-task-related features for the irrelevant features. The proposed method is based on a self-attention feature-selection mechanism, which selects features that are non-discriminative. The authors show that the proposed method outperforms the state-of-the-art meta-learners on a variety of tasks."
5311,SP:e1e513fef25d29e17cdadd1b36d932a8ad8897cd,"multi - agent reinforcement learning USED-FOR emergent communication. continuous acoustic channel USED-FOR Human communication. reinforcement learning USED-FOR continuous communication channel. continuous communication channel USED-FOR emergent language. channel characteristics USED-FOR emerging language. vocoder USED-FOR continuous waveform. noise FEATURE-OF communication channel. continuous signalling USED-FOR language learning. platform USED-FOR continuous signalling. deep reinforcement learning USED-FOR platform. OtherScientificTerm are discrete symbols, lossy continuous channel, continuous signal, and concept combinations. Method are environment and training methodology, and deep Q - learning. Material is messaging environment. ","This paper proposes to use reinforcement learning to learn to communicate with a continuous communication channel in a multi-agent reinforcement learning setting, where the communication channel consists of a set of discrete symbols and a lossy continuous channel. The authors propose to use a deep Q-learning framework to learn the communication protocol. The proposed method is evaluated on two tasks: 1) a reinforcement learning task where the goal is to learn a communication protocol that is able to communicate a concept to another agent, and 2) a communication task where an agent is tasked with communicating a message to a listener.  ","This paper proposes a method for learning a language that can be used for multi-agent communication in an emergent communication environment. The method is based on the idea that the communication channel can be represented as a continuous acoustic channel with a lossy continuous signal, and that it is possible to learn a language based on this communication channel. The communication channel is represented by a vocoder, which is trained using deep Q-learning. The authors show that the proposed method can be applied to a variety of communication environments, and demonstrate the effectiveness of the method.  "
5327,SP:0e6ff65ba4a3df35947d1b6f4d438612088d90a0,"backdoor attacks FEATURE-OF NLP models. NLP backdoor attacks USED-FOR tasks. NLP models CONJUNCTION tasks. tasks CONJUNCTION NLP models. attacks USED-FOR NLP models. attacks USED-FOR tasks. BadPre HYPONYM-OF task - agnostic backdoor attack. task - agnostic backdoor attack USED-FOR pre - trained NLP models. backdoor USED-FOR pre - trained model. backdoor USED-FOR downstream models. transfer learning process USED-FOR downstream models. approach USED-FOR downstream NLP tasks. Task is downstream language tasks. Method is language models. OtherScientificTerm are model misprediction, and prior information. Generic are attack, malicious model, and strategy. ","This paper proposes a novel backdoor attack on NLP models. The proposed method is based on the idea that a pre-trained NLP model can be used as a backdoor for a downstream language model, which can then be used to fool the downstream model. The authors show that the proposed method can be applied to a variety of downstream NLP tasks, including text classification, image classification, and image classification.  ","This paper proposes a novel backdoor attack for NLP models that can be applied to a variety of downstream tasks. The main idea is to use a pre-trained NLP model as a backdoor for a downstream language model, and then use it to attack the downstream model. The authors show that the backdoor can be used to attack a wide range of downstream NLP tasks. They also show that it can also be used as a transfer learning method for downstream language models."
5343,SP:58d3ecb4a1906251e79ad883aa97cc2502642658,"skill pre - training methods COMPARE RL techniques. RL techniques COMPARE skill pre - training methods. skill learning USED-FOR evolving or expanding environment. evolving environment USED-FOR skill discovery. framework USED-FOR skill discovery. incremental skills COMPARE skill discovery methods. skill discovery methods COMPARE incremental skills. evolving and static environments EVALUATE-FOR incremental skills. skill quality EVALUATE-FOR skill discovery methods. skill quality EVALUATE-FOR incremental skills. Task are Reward - free, unsupervised discovery of skills, hand - designing rewards, task supervision, and discovery - of - incremental - skill. OtherScientificTerm are stationary environments, agent dynamics, and learned skills. Generic are methods, and them. ","This paper proposes a method for skill discovery in a reward-free, unsupervised setting, where the goal is to learn a set of skills in an evolving or expanding environment. The proposed method is based on incremental skills, which are skills that are learned incrementally over time, and are then used to improve the quality of the learned skills. The method is evaluated on a variety of tasks, and compared with a number of baselines. ","This paper proposes a framework for skill discovery in a reward-free, unsupervised setting, where the goal is to discover skills in an evolving or expanding environment. The proposed method is based on the idea of skill discovery, which is to learn a set of skills that can be used to improve the performance of an agent in a stationary or evolving environment, and then use the learned skills to discover new skills in the evolving environment.  The method is evaluated in both static and evolving environments, and it is shown that the proposed method outperforms the state-of-the-art skill discovery methods in both environments. "
5359,SP:2c6595408f5ec95537eaf555e5fe3d992b58c222,"Convolutional neural networks USED-FOR features. regular quadrilateral convolution kernels USED-FOR features. regular quadrilateral convolution kernels USED-FOR Convolutional neural networks. small convolution kernels USED-FOR models. relative directions CONJUNCTION logarithmic distances. logarithmic distances CONJUNCTION relative directions. LPSC USED-FOR local spatial structures. LPSC USED-FOR single - layer receptive field. LPSC USED-FOR network architecture. convolutions PART-OF network architecture. convolution USED-FOR LPSC. log - polar space pooling USED-FOR convolution. log - polar space pooling USED-FOR LPSC. tasks EVALUATE-FOR LPSC. OtherScientificTerm are convolution kernel, small local receptive fields, and local receptive field. ",This paper proposes a new convolutional neural network architecture with small local receptive fields (LPSC). The LPSC consists of a quadrilateral convolution kernel with a small receptive field and a large receptive field with a single-layer receptive field. The authors show that this architecture is computationally efficient and can be used to improve the performance of deep neural networks on image classification tasks.,"This paper proposes a new local receptive field (LPSC) for convolutional neural networks. The LPSC is based on the quadrilateral convolution kernel, which can be used as a single-layer receptive field for a single layer neural network. The authors show that LPSc can be combined with log-polar space pooling to improve the performance of the network. They also provide a theoretical analysis of the effect of the local receptive fields. "
5375,SP:7791f96b1eef277a9133975507a750d9e7c6b8ff,"non - vacuous bounds USED-FOR NNs. PAC - Bayes theorem USED-FOR NNs generalization. IIW ’s property USED-FOR deep learning. algorithm USED-FOR approximation of IIW. information complexity EVALUATE-FOR NNs. accuracy CONJUNCTION information complexity. information complexity CONJUNCTION accuracy. accuracy EVALUATE-FOR NNs. PIB HYPONYM-OF NNs. IIW compression CONJUNCTION generalization. generalization CONJUNCTION IIW compression. IIW USED-FOR NNs. overparameterization CONJUNCTION noisy labels. noisy labels CONJUNCTION overparameterization. varying batch sizes CONJUNCTION overparameterization. overparameterization CONJUNCTION varying batch sizes. IIW USED-FOR NNs. MCMC - based algorithm USED-FOR optimal weight posterior. PIB FEATURE-OF optimal weight posterior. Task are ML research, and compressing phase transition. Method are IIW - based information bottleneck, and NNs ’ training. ","This paper studies the generalization properties of neural networks (NNs) in terms of the information bottleneck (IIW). The authors show that the IIW-based information bottleneck can be decomposed into two parts: (1) the information complexity of the posterior distribution over the training set, and (2) the number of training samples. The authors then propose a MCMC-based algorithm to find the optimal weight posterior for each sample, which is then used to compute the posterior for the next sample. They show that this algorithm is computationally efficient. ","This paper studies the generalization properties of neural networks (NNs) in the context of the PAC-Bayes theorem (i.e., generalization under the IIW-based information bottleneck). The authors provide a non-vacuous bound on the generalisation properties of NNs under the information bottleneck. They show that the information complexity of IIW can be approximated by a MCMC-based algorithm, which can be used to compute the optimal weight posterior of the posterior of an NN. The authors also provide an algorithm for computing the optimal posterior of a PIB-based posterior. "
5391,SP:a733847ade77ffbf38760fc79da17893dea8d53f,"perturbations USED-FOR attacks. linear separable FEATURE-OF perturbations. linear separability USED-FOR attacks. synthetic perturbations COMPARE deliberately crafted attacks. deliberately crafted attacks COMPARE synthetic perturbations. linear separable data USED-FOR perturbations. imperceptible scale FEATURE-OF they. shortcuts USED-FOR deep models. Method are Indiscriminate data poisoning attacks, and pre - trained feature extractors. OtherScientificTerm are imperceptible perturbations, and normal features. Task is shortcut learning problem. ","This paper studies the problem of data poisoning attacks against deep neural networks. The authors propose a new class of adversarial perturbations that are imperceptible on a linear separable set of data points, and show that they can be used as shortcuts to improve the performance of deep models. They show that this class of attacks is more effective than the standard data poisoning attack, which is based on data poisoning. ","This paper studies the problem of data poisoning attacks against deep neural networks. The authors show that the data poisoning attack is imperceptible when the data is linear separable. They show that if the data data is not linearly separable, then the attack can be used as a shortcut learning problem. They also provide a theoretical analysis of the effectiveness of the attacks."
5407,SP:7b50be406138ad01db3ee112899f622637896fe9,Offline policy optimization USED-FOR real - world decisionmaking problems. estimator USED-FOR offline policy evaluation. function approximations USED-FOR value functions. Importance sampling HYPONYM-OF estimator. value functions CONJUNCTION process models. process models CONJUNCTION value functions. function approximations USED-FOR process models. Importance sampling USED-FOR offline policy evaluation. algorithm USED-FOR overfitting. overfitting phenomenon FEATURE-OF importance weighted return. per - state - neighborhood normalization condition USED-FOR algorithm. healthcare - inspired simulator CONJUNCTION logged dataset. logged dataset CONJUNCTION healthcare - inspired simulator. healthcare - inspired simulator EVALUATE-FOR method. logged dataset EVALUATE-FOR method. method COMPARE batch reinforcement learning algorithms. batch reinforcement learning algorithms COMPARE method. overfitting EVALUATE-FOR method. Task is online learning. Generic is approach. ,This paper proposes to use importance sampling for offline policy evaluation. The main idea is to estimate the importance of each state using a function approximated by a process model. The authors show that this estimator can be used to reduce the overfitting problem in offline RL. They also show that the importance weighted return can be improved by a per-state-neighborhood normalization condition.   ,"This paper proposes a new estimator for offline policy evaluation, called Importance Sampling, which is based on the importance weighted return. Importance sampling is a well-known and well-studied estimator in the literature. However, it has been shown that it can be over-parameterized, which can lead to overfitting. The authors propose a new normalization condition to prevent overfitting, which they call per-state-neighborhood normalization. They show that this normalization can be used to reduce the overfitting of the estimator. They also show that their estimator can be applied to a healthcare-inspired simulator and a logged dataset. "
5423,SP:c976752a55b9ff47dc63c95a9fd7b51a81e8a42e,model USED-FOR continual learning. CoLLIE HYPONYM-OF model. CoLLIE USED-FOR continual learning. transformation function USED-FOR language embeddings. language CONJUNCTION images. images CONJUNCTION language. transformation function USED-FOR new language use. transformation function USED-FOR CoLLIE. semantic space FEATURE-OF images. few - shot learning COMPARE model. model COMPARE few - shot learning. it COMPARE model. model COMPARE it. zero - shot EVALUATE-FOR model. continual learning EVALUATE-FOR model. Material is vision. Method is multimodal embedding model. OtherScientificTerm is similar language use. ,"This paper proposes a method for continual learning of language embeddings from images. The method is based on a multi-modal embedding model that learns a language embedding for each image, which is then used to learn a transformation function for the image embedding. The proposed method is shown to outperform existing methods in the continual learning setting. ","This paper proposes a multi-modal language embedding model for continual learning. The model is based on CoLLIE, which is an existing model for few-shot learning, but with an additional transformation function for language embeddings. The authors show that the proposed model outperforms the state-of-the-art in continual learning on zero-shot and continual learning tasks. "
5439,SP:d3371b322acfc321ee79a2e1b438d82644872fa4,"captioning models USED-FOR visual data. fluency CONJUNCTION fidelity. fidelity CONJUNCTION fluency. fidelity CONJUNCTION adequacy. adequacy CONJUNCTION fidelity. Fidelity CONJUNCTION Fluency. Fluency CONJUNCTION Fidelity. Visual - Linguistic Adequacy CONJUNCTION Fidelity. Fidelity CONJUNCTION Visual - Linguistic Adequacy. VLAF2 USED-FOR Visual - Linguistic Adequacy. VLAF2 USED-FOR Fidelity. VLAF2 USED-FOR Fluency. linguistics USED-FOR Fluency. linguistics USED-FOR VLAF2. BERT CONJUNCTION CLIP. CLIP CONJUNCTION BERT. intrinsic language knowledge USED-FOR models. nocaps dataset EVALUATE-FOR framework. method COMPARE captioning models. captioning models COMPARE method. method COMPARE SPICE scores of human baseline. SPICE scores of human baseline COMPARE method. caption evaluation metrics EVALUATE-FOR captioning models. caption evaluation metrics EVALUATE-FOR method. fluency CONJUNCTION fidelity. fidelity CONJUNCTION fluency. fidelity CONJUNCTION adequacy. adequacy CONJUNCTION fidelity. model USED-FOR object captions. quantitative and qualitative analysis EVALUATE-FOR model. fluency EVALUATE-FOR model. adequacy EVALUATE-FOR model. fidelity EVALUATE-FOR model. Method are object captioning ( NOC ), image captioning models, object captioning models, and visual / language models. OtherScientificTerm is caption annotations. ","This paper proposes a method for improving the performance of object captioning models. The proposed method is based on the idea of Visual-Linguistic Adequacy-Fidelity-Fluency (VLAF2) framework, which aims to improve the fidelity and fluency of the captioning model. The method is evaluated on the nocaps dataset, where it achieves state-of-the-art performance on the SPICE scores of human captioners.","This paper proposes a new framework for object captioning, called VLAF2, which aims to improve the performance of captioning models on the nocaps dataset. The proposed method is based on the notion of Visual-Linguistic Adequacy (VLAF), which is a combination of fluency, fidelity, and fluency-fidelity-fluency. The key idea is to use the language knowledge of BERT and CLIP to train a model that is able to predict the fluency and fidelity of object captions. Experiments are conducted on a variety of datasets to demonstrate the effectiveness of the proposed method. "
5455,SP:9f3b6486662d80350d77a4b060d4a5b8b22a6130,"foundation models USED-FOR representations. representations USED-FOR classification. special - purpose algorithms USED-FOR problems. classifier USED-FOR representations. special - purpose algorithms USED-FOR representations. clustering property FEATURE-OF features. neural collapse HYPONYM-OF clustering property. overparameterized classification networks USED-FOR features. foundation models USED-FOR feature maps. foundation models USED-FOR transfer learning. feature maps USED-FOR transfer learning. Task are few - shot learning problems, and few - shot setting. "," in the few-shot learning setting, where the goal is to learn a set of features that can be transferred to a new task. The authors propose a method to learn the feature maps of a classifier in this setting. The proposed method is based on the idea that the features of an overparametrized classifier can be partitioned into sub-spaces that are close to each other in terms of the number of samples. The main contribution of the paper is a theoretical analysis of the clustering properties of the features. The paper shows that the feature map of the classifier has a neural collapse property, which means that it is more likely to be close to a sub-space that is close to the training data. ","This paper studies the problem of few-shot learning in the setting of transfer learning, where the goal is to learn representations that are transferable across different tasks. The authors study the problem in the context of neural collapse, which is a well-studied problem. The main contribution of this paper is to study the clustering property of the features of a classifier, which can be used to improve the transferability of the representations learned by the classifier. They show that the feature maps of overparameterized classification networks can be transferred across tasks. They also provide a theoretical analysis of the effect of the feature map on transferability."
5471,SP:624c95d9ce1ee4b66274e858e2da22bef6b052c7,"3D scanning USED-FOR Point cloud. tasks USED-FOR point cloud reconstruction. 3D sparse stacked - hourglass network USED-FOR densification and denoising. 3D sparse stacked - hourglass network HYPONYM-OF stages. refinement HYPONYM-OF stages. stages PART-OF deep point cloud reconstruction network. 3D sparse stacked - hourglass network PART-OF deep point cloud reconstruction network. transformers USED-FOR refinement. refinement PART-OF deep point cloud reconstruction network. amplified positional encoding HYPONYM-OF module. module USED-FOR transformer. points ’ distances USED-FOR adaptive refinements. module USED-FOR positional encoding vectors. points ’ distances USED-FOR module. points ’ distances USED-FOR positional encoding vectors. ScanNet CONJUNCTION ICL - NUIM. ICL - NUIM CONJUNCTION ScanNet. ICL - NUIM CONJUNCTION ShapeNetPart datasets. ShapeNetPart datasets CONJUNCTION ICL - NUIM. ICL - NUIM EVALUATE-FOR network. ShapeNetPart datasets EVALUATE-FOR network. ScanNet EVALUATE-FOR network. network USED-FOR real - world and unmet scenes. OtherScientificTerm are discrete voxels, and 3D points. ","This paper proposes a method for 3D point cloud reconstruction based on sparse stacked-hourglass networks. The method consists of two stages: densification and denoising. In densification stage, a transformer is used to map the 3D points to discrete voxels, which are then used to refine the reconstruction results. In denoizing stage, an amplified positional encoding module is proposed to encode the distances between the points. Experiments on ScanNet, ICL-NUIM, and ShapeNetPart datasets show that the proposed method achieves state-of-the-art results.","This paper proposes a method for 3D sparse stacked-hourglass-based point cloud reconstruction. The method is based on a 3D stacked hourglass network, which consists of three stages: densification, denoising, and refinement. The densification stage consists of a set of transformers that can be used to refine the 3D point cloud. The refinement stage is composed of an amplified positional encoding module, which is used to map the points’ distances to the positional encoding vectors. The proposed method is evaluated on ScanNet, ICL-NUIM, and ShapeNet."
5487,SP:34a81ca65131576d4c14332a4e9eb3a4c344cab7,"communicating node features CONJUNCTION feature gradients. feature gradients CONJUNCTION communicating node features. training efficiency CONJUNCTION model scalability. model scalability CONJUNCTION training efficiency. stale features CONJUNCTION stale feature gradients. stale feature gradients CONJUNCTION stale features. convergence rate EVALUATE-FOR GCN training. stale features USED-FOR GCN training. stale feature gradients USED-FOR GCN training. convergence rate EVALUATE-FOR PipeGCN. PipeGCN COMPARE vanilla distributed GCN training. vanilla distributed GCN training COMPARE PipeGCN. convergence rate EVALUATE-FOR vanilla distributed GCN training. smoothing method USED-FOR PipeGCN. PipeGCN COMPARE full - graph training methods. full - graph training methods COMPARE PipeGCN. accuracy EVALUATE-FOR full - graph training methods. training throughput EVALUATE-FOR PipeGCN. accuracy EVALUATE-FOR PipeGCN. Method are Graph Convolutional Networks ( GCNs ), large - scale GCNs, and distributed GCN training. Material is graph - structured data. OtherScientificTerm are partitioned subgraph, GCN layer, communication overhead, intra - partition computation, convergence, and staleness. Metric is theoretical convergence guarantee. ",This paper studies the convergence of GCN training with stale features and stale feature gradients. The authors propose a new method called PipeGCN to address the issue of stale features in distributed GCN. They show that the convergence rate of the proposed method is faster than that of the state-of-the-art methods. They also provide a theoretical convergence analysis of the method. ,"This paper studies the convergence of GCN training with stale features and stale feature gradients in distributed graph convolutional networks (GCNs). The authors propose a new method, called PipeGCN, to reduce the communication overhead between GCN layers. They show that the convergence rate of the proposed method is better than vanilla distributed GCN. They also provide theoretical convergence guarantees for the method. "
5503,SP:8302d49558ee0f16392d623d4e604e92db10d041,"robustness USED-FOR applications. in - distribution test points EVALUATE-FOR deep neural networks. accuracy EVALUATE-FOR deep neural networks. methods USED-FOR test time adaptation. ResNet-50 models CONJUNCTION robust vision transformer model. robust vision transformer model CONJUNCTION ResNet-50 models. approach COMPARE prior augmentation and adaptation strategies. prior augmentation and adaptation strategies COMPARE approach. approach COMPARE model evaluation. model evaluation COMPARE approach. baseline ResNet models CONJUNCTION ResNet-50 models. ResNet-50 models CONJUNCTION baseline ResNet models. ImageNet - C CONJUNCTION ImageNet - R. ImageNet - R CONJUNCTION ImageNet - C. ResNet-50 models CONJUNCTION ImageNet - A distribution shift benchmarks. ImageNet - A distribution shift benchmarks CONJUNCTION ResNet-50 models. OtherScientificTerm are distribution shift, model training process, data augmentations, and model parameters. Task is test time robustification. Metric is model robustness. Generic are assumptions, model, and augmentations. ","This paper studies the problem of test-time robustification, i.e., how to adapt to distribution shift in the training process. The authors propose a new method for test time adaptation based on data augmentation and show that the proposed method outperforms existing methods in terms of test time robustification. The method is evaluated on ImageNet-C, ImageNet R, and ImageNet A distribution shift benchmarks. ",This paper studies the problem of test-time robustification (test-time adaptation) of deep neural networks in the context of distribution shift. The authors propose a new metric for test time robustification based on the in-distribution test points. They show that test time adaptation can be improved by augmenting the training data during the training process with data augmentations. They also show that their method outperforms prior methods in terms of accuracy and robustness. 
5519,SP:a985de5e940ff3a4160b378201b8c02f68d1914a,"RL CONJUNCTION planning. planning CONJUNCTION RL. model USED-FOR planning. model USED-FOR RL. accuracy EVALUATE-FOR they. model CONJUNCTION policy. policy CONJUNCTION model. single objective USED-FOR policy. single objective USED-FOR model. expected return FEATURE-OF lower bound. joint optimization USED-FOR objective mismatch. global lower bound FEATURE-OF expected return. global lower bound FEATURE-OF objective. algorithm ( MnM ) COMPARE GAN. GAN COMPARE algorithm ( MnM ). classifier USED-FOR real and fake transitions. Generic are template, models, it, bound, and algorithm. Metric is MSE. Task is control. Method is RL agent. OtherScientificTerm is policies. ","This paper studies the problem of learning a model and a policy jointly in the context of planning and reinforcement learning. The authors propose a new algorithm, called MnM, which is based on the idea that the model and the policy can be jointly optimized by a single objective. The main idea is to learn a template for the model, which can then be used to train a policy that maximizes the expected return of the model. The proposed algorithm is shown to outperform the state-of-the-art GAN-based methods in terms of MSE. ",This paper proposes a new lower bound on the expected return of a model-based RL agent in the presence of a single objective. The lower bound is based on a joint optimization between the model and the policy. The authors show that the lower bound can be obtained by a GAN-based algorithm. They also show that their algorithm can be used in conjunction with GANs to improve the performance of the model.
5535,SP:a469fbcdc20b11dff4085b6fbc384e77f33cd37d,"single observations CONJUNCTION observation histories. observation histories CONJUNCTION single observations. single observations USED-FOR behavioral cloning policies. observation histories USED-FOR behavioral cloning policies. human decision making USED-FOR model combination approach. instantaneous observation USED-FOR coarse action. images CONJUNCTION MuJoCo continuous control tasks. MuJoCo continuous control tasks CONJUNCTION images. this COMPARE baselines. baselines COMPARE this. CARLA autonomous driving CONJUNCTION MuJoCo continuous control tasks. MuJoCo continuous control tasks CONJUNCTION CARLA autonomous driving. images USED-FOR CARLA autonomous driving. MuJoCo continuous control tasks EVALUATE-FOR this. CARLA autonomous driving EVALUATE-FOR this. MuJoCo continuous control tasks EVALUATE-FOR baselines. CARLA autonomous driving EVALUATE-FOR baselines. Attention maps of baseline imitation methods CONJUNCTION method. method CONJUNCTION Attention maps of baseline imitation methods. CARLA driving task EVALUATE-FOR method. single observation ( BC - SO ) USED-FOR Behavioral cloning. observation history ( BC - OH ) USED-FOR Behavioral cloning. method USED-FOR coarse - to - fine ” imitator. Method are control policy, and imitation - learned policies. Generic are approach, them, and it. OtherScientificTerm are instantaneous observations, historical information, and visual cues. ","This paper proposes a method for learning a coarse-to-fine imitator policy from a single observation. The method is based on the observation history (BC-SO), which is a combination of observation history and observation history from previous observations. The BC-SO is used to learn a policy that is able to imitate a coarse action from a coarse set of observations, and a fine action is learned from a finer set of observed observations.    The main contribution of the paper is that the method is trained to learn from multiple observations, rather than using only one observation.  The authors show that the proposed method outperforms the baselines on CARLA and MuJoCo continuous control tasks. ","This paper proposes a method for behavioral cloning, where the imitator is trained to imitate the behavior of a control policy that is learned from a single observation. The method is based on the observation history (BC-SO), which is used to model the human decision making process. The authors show that the BC-SO can be used to learn a coarse-to-fine imitation policy, which can be applied to a variety of tasks such as CARLA autonomous driving and MuJoCo continuous control tasks. They show that their method outperforms the state-of-the-art in terms of performance on CARLA driving tasks."
5551,SP:95c4533b5d1a865c4cc6a54615e7ad6357bdaad1,deep learning models USED-FOR dynamics forecasting. generalization EVALUATE-FOR deep learning models. external forces CONJUNCTION boundary conditions. boundary conditions CONJUNCTION external forces. external forces FEATURE-OF systems. them USED-FOR tasks. DyAd HYPONYM-OF model - based meta - learning method. forecaster USED-FOR shared dynamics. encoder USED-FOR time - invariant hidden features. encoder USED-FOR task. parts PART-OF DyAd. encoder HYPONYM-OF parts. forecaster HYPONYM-OF parts. weak supervision USED-FOR encoder. forecaster PART-OF DyAd. encoder PART-OF DyAd. adaptive instance normalization CONJUNCTION adaptive padding. adaptive padding CONJUNCTION adaptive instance normalization. encoder USED-FOR forecaster. forecaster USED-FOR inference. adaptive padding USED-FOR encoder. adaptive instance normalization USED-FOR encoder. adaptive instance normalization USED-FOR forecaster. generalization error EVALUATE-FOR procedure. model COMPARE approaches. approaches COMPARE model. Generic is They. ,"This paper proposes a meta-learning approach for dynamics forecasting. The authors propose a model-based meta learning method called DyAd, which consists of two parts: an encoder and a forecaster. The encoder is trained with weak supervision, while the forecaster is used for inference. The main contribution of the paper is the use of adaptive padding and adaptive instance normalization to improve the performance of the encoder. ","This paper proposes DyAd, a model-based meta-learning method for dynamics forecasting. DyAd consists of two parts: an encoder and a forecaster. The encoder encodes a time-invariant hidden feature for each task, and the forecaster predicts the shared dynamics between the two parts. The forecaster is supervised by the encoder, which is trained with weak supervision. The authors show that DyAd outperforms the state-of-the-art in terms of generalization error. "
5567,SP:ec70553cb0c27e5349c1b8cce6bcaa96a83bf050,"Monocular 3D object detection HYPONYM-OF 3D scene understanding. manually annotated 3D box labels USED-FOR LiDAR point clouds. manually annotated 3D box labels USED-FOR monocular 3D detection methods. 2D boxes PART-OF image. 2D boxes USED-FOR RoI LiDAR points. network USED-FOR 3D boxes. 3D box estimates CONJUNCTION RoI LiDAR points. RoI LiDAR points CONJUNCTION 3D box estimates. 3D alignment loss FEATURE-OF 3D box estimates. 3D alignment loss USED-FOR network. method COMPARE fully supervised methods. fully supervised methods COMPARE method. KITTI EVALUATE-FOR method. OtherScientificTerm are ill - posed nature of monocular imagery, 3D box labels, weak supervision, and 3D box label. Task are annotation process, weakly supervised monocular 3D detection, and learning problem. ","This paper proposes a method for weakly supervised monocular 3D object detection in the presence of ill-posed 3D objects. The proposed method is based on the observation that 2D boxes in the monocular image are not annotated with 3D box labels, which is a common problem in existing monocular object detection methods. To address this problem, the authors propose to use a network to predict the 3D boxes from the 2D images. The network is trained using a combination of 3D alignment loss and RoI LiDAR point clouds. Experiments on KITTI show that the proposed method achieves state-of-the-art performance.  ","This paper proposes a novel method for weakly supervised monocular 3D object detection. The proposed method is based on the idea of using 2D boxes to annotate the LiDAR point clouds of a monocular image, and then using a network to predict the 3D box labels of the image. The method is evaluated on the KITTI dataset and shows that the proposed method outperforms the state-of-the-art. "
5583,SP:34217c6a8ca43b8eeb9ddc83d6f1f0af05918984,"models USED-FOR natural language processing. rigid subword tokenization algorithms USED-FOR models. model inductive bias USED-FOR subword tokenization. characters USED-FOR latent subword representations. block scoring network USED-FOR GBST. CHARFORMER HYPONYM-OF deep Transformer model. GBST PART-OF deep Transformer model. CHARFORMER COMPARE subword - based models. subword - based models COMPARE CHARFORMER. CHARFORMER COMPARE byte - level baselines. byte - level baselines COMPARE CHARFORMER. multilingual, and noisy text datasets EVALUATE-FOR CHARFORMER. English GLUE CONJUNCTION multilingual, and noisy text datasets. multilingual, and noisy text datasets CONJUNCTION English GLUE. English GLUE EVALUATE-FOR CHARFORMER. byte - level baselines COMPARE subword - based models. subword - based models COMPARE byte - level baselines. Generic is model. OtherScientificTerm is byte level. Metric is competitive quality. ","This paper proposes a novel method to improve the performance of Transformer-based language models with rigid subword tokenization. The proposed method is based on a novel block scoring network (GBST) architecture, where each token is encoded as a set of characters, and the output of the GBST is used to train a Transformer model. The authors show that the proposed method outperforms previous methods in terms of performance on GLUE and multilingual text.","This paper proposes a novel deep Transformer-based subword tokenization model, called CHARFORMER. The proposed model is based on a block scoring network (GBST), which is used to generate a set of characters for each subword. The authors show that the proposed model outperforms the state-of-the-art subword-based models on a variety of datasets. "
5599,SP:d26d25f2ef23a89a2c139d0dd87c4c86fddcff5e,"Deep neural networks ( DNNs ) USED-FOR backdoor attacks. backdoor PART-OF DNNs. backdoor trigger USED-FOR DNNs. on - device deployed DNNs HYPONYM-OF real - world applications. adversarial objective USED-FOR backdoor detection. optimization perspective USED-FOR problem. adversarial objective USED-FOR solution. singularity FEATURE-OF adversarial map. adversarial map FEATURE-OF backdoorinfected example. skewed distribution FEATURE-OF solution. adversarial extreme value analysis ( AEVA ) USED-FOR backdoors. backdoors PART-OF black - box neural networks. extreme value analysis of the adversarial map USED-FOR AEVA. monte - carlo gradient estimation USED-FOR extreme value analysis of the adversarial map. approach USED-FOR backdoor attacks. backdoor attacks EVALUATE-FOR approach. black - box hard - label scenarios FEATURE-OF detecting backdoor attacks. Method are backdoor detection methods, and DNN. Material is poisoned training data. OtherScientificTerm are predictive confidence, and adversarial singularity phenomenon. ",This paper proposes a novel adversarial approach to detect backdoor attacks in black-box deep neural networks (DNNs). The proposed method is based on adversarial extreme value analysis (AEVA) to detect backdoors in DNNs. The main contribution of the paper is to use the singularity of the adversarial map of the backdoorinfected example to estimate the skewed distribution of the solution of AEVA. Experiments show that the proposed method outperforms the state-of-the-art methods in detecting backdoor attacks.,"This paper proposes a novel adversarial approach to detect backdoor attacks on black-box neural networks. The authors propose an adversarial singularity analysis (AEVA) approach for detecting backdoor attacks. The AEVA approach is based on a monte-carlo gradient estimation of the adversarial map, which is used to estimate the singularity of the map. They show that AEVA can be used to detect backdoors in a variety of scenarios, including on-device deployed DNNs and hard-label scenarios. They also provide a theoretical analysis of AEVA."
5615,SP:c6dbca0ed0799b7fec21777606f6f809eb2d8c48,"uncertainty estimation USED-FOR classifier. KLoS HYPONYM-OF Kullback – Leibler divergence criterion. class - probability simplex FEATURE-OF Kullback – Leibler divergence criterion. evidential models USED-FOR secondorder uncertainty representation. distributional information USED-FOR KLoS. KLoS USED-FOR class confusion. class - wise divergence measure EVALUATE-FOR KLoS. in - distribution samples USED-FOR class - wise divergence measure. auxiliary neural network USED-FOR refined criterion. KLoSNet USED-FOR refined criterion. KLoSNet HYPONYM-OF auxiliary neural network. misclassifications CONJUNCTION OOD samples. OOD samples CONJUNCTION misclassifications. KLoSNet USED-FOR OOD samples. KLoSNet USED-FOR misclassifications. measures COMPARE KLoS. KLoS COMPARE measures. Material are OOD training data, OOD data, and OOD dataset. Metric is second - order uncertainty measures. OtherScientificTerm is evidential training objective. ","This paper proposes a class-confusion-reduction method based on the Kullback-Leibler divergence criterion (KLoS). The main idea is to use the class-probability simplex to estimate the second-order uncertainty of the OOD data, which is then used to train the classifier. The main contribution of the paper is to show that KLoS can be used to improve the performance of OOD classifiers.   ","This paper proposes a new Kullback-Leibler divergence criterion (KLoS) for second-order uncertainty estimation. The proposed KLoS is based on the class-probability simplex, which is a class-wise divergence measure. The main contribution of the paper is the use of an auxiliary neural network to train the KloS-based criterion. The authors show that the proposed criterion can be used to improve the performance of the classifier on OOD training data."
5631,SP:8b4f3916dca4e627931558e14836749bd4a6792f,natural image data USED-FOR CNNs. semi - supervised algorithm USED-FOR linear classifier. datadependent features USED-FOR linear classifier. unlabeled data USED-FOR datadependent features. algorithm USED-FOR CNNs. natural distributional assumptions USED-FOR algorithm. it USED-FOR CNNs. low - dimensional structure FEATURE-OF distribution of patches. low - dimensional manifold USED-FOR patches. dimension of the patch distribution USED-FOR algorithm. Method is Convolutional networks ( CNN ). OtherScientificTerm is lower bound. ," is a semi-supervised learning algorithm for linear classifiers on natural image data. The main contribution of the paper is to provide a lower bound on the complexity of the algorithm. The lower bound is based on the assumption that the distribution of patches is low-dimensional, and that the dimension of the patch distribution is bounded by a low-dimension of the manifold.  ",This paper proposes a semi-supervised algorithm for learning a linear classifier from unlabeled data. The main idea is to use a low-dimensional structure of the distribution of patches to learn a classifier. The authors show that the proposed algorithm can be used to learn linear classifiers from natural data. They also provide a lower bound on the dimension of the patch distribution of the algorithm.
5647,SP:7f2f354d5cc1030bd97bd716aea8fe1d3af86b25,"face images USED-FOR Face clustering. representation capacity FEATURE-OF Graph Convolutional Networks ( GCN ). GCN - based methods USED-FOR face graphs. feature space FEATURE-OF kNN relations. kNN relations USED-FOR GCN - based methods. clean graphs USED-FOR GCNs. clean graphs USED-FOR algorithm. Ada - NETS HYPONYM-OF algorithm. face features USED-FOR robust features. adaptive neighbour discovery strategy USED-FOR edges. It USED-FOR graph. It USED-FOR noise edges. graph USED-FOR GCNs. clean yet rich edges FEATURE-OF graph. public clustering datasets EVALUATE-FOR Ada - NETS. Ada - NETS COMPARE state - of - the - art methods. state - of - the - art methods COMPARE Ada - NETS. public clustering datasets EVALUATE-FOR state - of - the - art methods. generalization EVALUATE-FOR Ada - NETS. OtherScientificTerm are structure space, and face image. ","This paper proposes a novel approach for face clustering based on graph convolutional networks (GCNs). The main idea is to learn a set of clean and noisy edges in the face image, which are then used to train a graph neural network (GCN) to cluster the face images. The proposed approach is based on the idea of learning a set (or set of) clean edges and then learning the clean edges from the noisy ones. The clean edges are learned by a graph-based approach, where the clean nodes are selected from the original face image and the noisy nodes are added to the original image. The noisy edges are then removed from the graph, and the clean ones are used to learn the robust features.   The proposed method is evaluated on two public face datasets and compared with several baselines. The results show that the proposed method outperforms the baselines in terms of clustering accuracy and generalization.","This paper proposes a new algorithm for face clustering based on graph convolutional networks (GCNs). The authors propose a novel algorithm called Ada-NETS, which is based on the adaptive neighbour discovery strategy. The main idea of the algorithm is to learn the kNN relations in the feature space of the graph, and then use them to construct a graph with clean and rich edges. The algorithm is evaluated on a variety of public datasets, and it outperforms state-of-the-art methods. "
5663,SP:a3bc8e26f55e78f07de081ca85865afd52b6ae4a,"crossdomain representations USED-FOR direct cross - data evaluation. domain information CONJUNCTION camera IDs. camera IDs CONJUNCTION domain information. It USED-FOR features. demographics information USED-FOR features. demographics information USED-FOR It. domain information HYPONYM-OF demographics information. camera IDs HYPONYM-OF demographics information. distributionally robust optimization ( DRO ) USED-FOR learning robust models. uncertainty set HYPONYM-OF data distributions. convex condition FEATURE-OF KL DRO. convex condition FEATURE-OF overparameterized neural networks. real scenarios FEATURE-OF distribution shifts. change - of - measure technique USED-FOR approach. Unit DRO HYPONYM-OF approach. reweighted dataset USED-FOR Unit DRO. large - scale DG ReID CONJUNCTION cross - domain ReID benchmarks. cross - domain ReID benchmarks CONJUNCTION large - scale DG ReID. Unit DRO COMPARE baselines. baselines COMPARE Unit DRO. cross - domain ReID benchmarks EVALUATE-FOR baselines. large - scale DG ReID EVALUATE-FOR baselines. cross - domain ReID benchmarks EVALUATE-FOR Unit DRO. large - scale DG ReID EVALUATE-FOR Unit DRO. OtherScientificTerm are protected demographic features, and demographics. Method are robust models, and models. ","This paper proposes a new method for learning robust models in the presence of distributional shifts in the data distribution. The proposed method is based on distributionally robust optimization (DRO), which aims to learn robust models that are robust to distribution shifts. The main idea is to use a reweighted dataset to train a robust model. The authors show that the proposed method outperforms existing methods in terms of robustness to distribution shift.  ",This paper proposes a new distributionally robust optimization (DRO) method for cross-domain data evaluation. The authors propose a change-of-measure (COM) DRO method to tackle the problem of overparameterized neural networks. The proposed method is based on a reweighted dataset and reweighting of the original dataset. Experiments show that the proposed method outperforms other DRO methods on the large-scale DG ReID and cross- domain ReID benchmarks. 
5679,SP:62c1f734b7f6c6e7d5114da6f37c9e3cdda73a23,"Graph Neural Networks ( GNNs ) USED-FOR molecular property prediction. noise correction loss USED-FOR oversmoothing. noise USED-FOR overfitting. generic architectures USED-FOR quantum chemistry. methods USED-FOR regulariser. Noisy Nodes CONJUNCTION non - spatial architectures. non - spatial architectures CONJUNCTION Noisy Nodes. Open Graph Benchmark ( OGB ) datasets EVALUATE-FOR non - spatial architectures. Open Graph Benchmark ( OGB ) datasets EVALUATE-FOR Noisy Nodes. GNN toolkit USED-FOR 3D molecular property prediction. Method is GNNs. OtherScientificTerm are noise correcting node - level loss, and node latents. ","This paper proposes a novel noise correction loss for graph neural networks (GNNs) to prevent over-smoothing in 3D molecular property prediction. The proposed method is based on the idea of noise correcting node-level loss, which is a regulariser to prevent node latents from changing too much. The authors show that the proposed method can improve the performance of GNNs on the Open Graph Benchmark (OGB) datasets. ","This paper proposes a novel noise correction loss for graph neural networks (GNNs) for molecular property prediction. The proposed method is based on the idea of noise correcting node-level loss, which can be used to reduce the overfitting of GNNs. The authors show that the proposed method can be applied to a variety of different GNN architectures, including non-spatial and spatial architectures. They also show that their method is able to improve the performance on Open Graph Benchmark (OGB) datasets. "
5695,SP:24a1b44f37f8eedbab2047fb84600a322d289f3b,"approaches USED-FOR complex interaction between set elements. ( Set)Transformers HYPONYM-OF approaches. self attention USED-FOR approaches. ( Set)Transformers HYPONYM-OF self attention. inducing - point attention CONJUNCTION optimal transport kernel embedding ( OTKE ). optimal transport kernel embedding ( OTKE ) CONJUNCTION inducing - point attention. mixture distribution FEATURE-OF i.i.d. samples. marginal likelihood maximization CONJUNCTION empirical Bayes. empirical Bayes CONJUNCTION marginal likelihood maximization. balanced assignment constraints FEATURE-OF E - step. OTKE HYPONYM-OF framework. balanced assignment constraints FEATURE-OF single - step EM. single - step EM HYPONYM-OF framework. set embedding CONJUNCTION prior - induced model regularization. prior - induced model regularization CONJUNCTION set embedding. OTKE COMPARE approach. approach COMPARE OTKE. approach USED-FOR set embedding. approach USED-FOR prior - induced model regularization. tasks EVALUATE-FOR approach. Task is set2vec problem. Method are vector representation, set embedding feed - forward network, ExpectationMaximization ( EM ) steps, MAP - EM steps, auto - diff backpropagation, and mixture set data fitting framework. OtherScientificTerm are variable number of feature vectors, mixture, and mixture parameters. Metric are computational overhead, and reduced computational cost. ","This paper studies the problem of learning the set2vec representation of a set of vectors. The authors propose to use a mixture set data fitting framework to solve the problem. The proposed method is based on the idea of optimal transport kernel embedding (OTKE), which is an extension of (Set)Transformers. Theoretical analysis is provided to show the convergence of the proposed method. Empirical results are provided to demonstrate the effectiveness of the method.","This paper proposes a new approach for learning the set2vec representation of a set of elements (i.e., a mixture of elements) from data. The authors propose a new method, called ExpectationMaximization (EM) steps, which is based on the optimal transport kernel embedding (OTKE) framework. The main idea is to use a set embedding feed-forward network to learn the vector representation of the set. The proposed method is evaluated on a variety of tasks, and it is shown to outperform the state-of-the-art."
5711,SP:b4f7b660b84fe7702fbcc8a96c192abc3a64f045,unsupervised feature selection USED-FOR informative features. informative features USED-FOR unknown downstream tasks. unsupervised feature selection USED-FOR unknown downstream tasks. CA setting USED-FOR feature selection. machine learning community USED-FOR feature selection. method USED-FOR feature selection. feature selection USED-FOR CA setting. semi - synthetic dataset CONJUNCTION real - world biomedical datasets. real - world biomedical datasets CONJUNCTION semi - synthetic dataset. state - of - the - art methods USED-FOR unsupervised feature selection scenarios. it COMPARE state - of - the - art methods. state - of - the - art methods COMPARE it. semi - synthetic dataset EVALUATE-FOR method. real - world biomedical datasets EVALUATE-FOR method. Task is contrastive analysis ( CA ) setting. Generic is background dataset. OtherScientificTerm is genes. Material is genomic data. ,"This paper proposes an unsupervised feature selection method for feature selection in contrastive analysis (CA) setting, where the goal is to select features that are informative for downstream tasks. The proposed method is based on the idea that feature selection is an important problem in the machine learning community, and the authors propose to use contrastive learning to learn feature selection. The method is evaluated on a semi-synthetic dataset and two real-world biomedical datasets.   ",This paper proposes a novel unsupervised feature selection method for contrastive analysis (CA) where the goal is to select informative features that can be used for downstream tasks. The proposed method is based on contrastive contrastive learning (CCL) and is applied to both synthetic and real-world datasets. It is shown that the proposed method outperforms state-of-the-art methods on a variety of datasets. 
5727,SP:bc4f69f23aba2034cbf14cb31bdc7a991806bbf6,"Early stopping USED-FOR over - training neural networks. optimal early stopping time CONJUNCTION model dimension. model dimension CONJUNCTION optimal early stopping time. optimal early stopping USED-FOR double descent ”. early stopping USED-FOR generalization. Method are linear regression models, linear models, and deep neural network. Task is deep learning tasks. Generic is model. OtherScientificTerm is features. ",This paper studies the effect of early stopping in linear regression models on the generalization performance. The authors show that early stopping is beneficial for over-training neural networks. They show that the optimal early stopping time depends on the model dimension and the number of features in the model. They also show that this early stopping improves the performance of the model in the presence of double descent. ,This paper studies the problem of over-training neural networks with linear regression models. The authors show that the optimal early stopping time of a linear model can be defined as a function of the model dimension and the number of features in the model. They also show that early stopping can be used to improve the generalization performance of a deep neural network. 
5743,SP:ede87b50cd9c4a6533f17e3e5ddfaaeaaac71dcf,"Policy gradient algorithms USED-FOR reinforcement learning ( RL ) problems. Regularization USED-FOR exploration. Regularization USED-FOR stability. entropy functions USED-FOR stability. entropy functions USED-FOR exploration. entropy functions FEATURE-OF Regularization. quasi - Newton method USED-FOR policy gradient algorithm. entropy regularization USED-FOR quasi - Newton method. algorithm USED-FOR natural policy gradient ( NPG ) algorithm. method USED-FOR policy gradient algorithms. method USED-FOR entropy functions. Newton - type quadratic convergence FEATURE-OF algorithms. quasi - Newton method COMPARE state - of - the - art algorithms. state - of - the - art algorithms COMPARE quasi - Newton method. synthetic and industrial - scale examples EVALUATE-FOR quasi - Newton method. single - digit iterations FEATURE-OF quasi - Newton method. OtherScientificTerm are Shannon entropy, and optimal policy. ",This paper proposes a quasi-Newton method for policy gradient algorithms in reinforcement learning. The main idea is to use entropy regularization to improve the stability of the policy gradient algorithm. The authors show that the entropy-regularized policy gradient (NPG) algorithm converges to the optimal policy with a Newton-type quadratic convergence rate. The proposed method is shown to outperform the state-of-the-art algorithms in both synthetic and industrial-scale experiments.,This paper proposes a quasi-Newton method for the natural policy gradient (NPG) algorithm for reinforcement learning (RL) problems. The main idea is to use entropy regularization to improve the stability of the policy gradient algorithm. The authors show that the entropy-regularized NPG algorithm can converge to a Newton-type quadratic convergence. They also show that their method can be applied to the case where the entropy function is Shannon. 
5759,SP:3535504f7599b1f39239f7cd8e09acd40fa8fdf0,"generalization CONJUNCTION sample efficiency. sample efficiency CONJUNCTION generalization. Text - based games ( TBG ) USED-FOR grounded language understanding. generalization HYPONYM-OF problems. sample efficiency HYPONYM-OF problems. deep reinforcement learning ( RL ) methods USED-FOR TBGs. case - based reasoning USED-FOR general method. on - policy neural agent USED-FOR TBGs. method CONJUNCTION on - policy neural agent. on - policy neural agent CONJUNCTION method. method USED-FOR TBGs. approach COMPARE methods. methods COMPARE approach. out - of - distribution generalization EVALUATE-FOR methods. out - of - distribution generalization EVALUATE-FOR approach. OtherScientificTerm is distributional shifts. Method are deep RL approaches, and case - based reasoner. ","This paper proposes a novel method for text-based games (TBG) based on case-based reasoning. The main idea is to use an on-policy neural agent to solve the problem of generalization in TBG, which is an important problem in RL. The proposed method is based on a combination of an RL agent and an off-policy RL agent. The method is evaluated on a variety of TBG tasks and compared with a number of baselines. ","This paper proposes a novel method for text-based text-to-text games (TGB) generalization and sample efficiency. The main idea is to use an on-policy neural agent to solve the problem of TBG generalization. The agent is trained using a case-based reasoning framework, where the goal is to find the best solution for a given task. The authors show that their method outperforms the state-of-the-art in terms of generalization, sample efficiency, and generalization to new tasks. "
5775,SP:9a5dd0148a15dc5b4d2bc6762dfe8a8991f8866c,Pre - trained contextual language models USED-FOR language understanding tasks. sense information USED-FOR multi - sense embeddings. multi - sense embeddings PART-OF skip - gram - like framework. pre - trained language model ( BERT ) USED-FOR two - stage method. approach USED-FOR sense disambiguation mechanism. sense disambiguation mechanism PART-OF model. output layer embeddings PART-OF BERT. distribution over word senses USED-FOR approach. BERT USED-FOR distribution over word senses. output layer embeddings USED-FOR distribution over word senses. method COMPARE multi - sense embeddings. multi - sense embeddings COMPARE method. contextual word similarity CONJUNCTION sense induction tasks. sense induction tasks CONJUNCTION contextual word similarity. sense induction tasks EVALUATE-FOR method. contextual word similarity EVALUATE-FOR method. embedding - based topic model ( ETM ) EVALUATE-FOR multi - sense embedding. multiple benchmark data sets EVALUATE-FOR method. multiple benchmark data sets EVALUATE-FOR multi - sense embeddings. Task is resource - constrained systems. Method is Noncontextual word embeddings. Generic is methods. OtherScientificTerm is polysemy. ,"This paper proposes a two-stage method for learning multi-sense embeddings in pre-trained language models. In the first stage, a skip-gram-like framework is used to learn a sense disambiguation mechanism. The second stage is to learn the distribution over word senses in the output layer of BERT. Experiments are conducted on contextual word similarity and sense induction tasks.   ","This paper proposes a skip-gram-like framework for embedding multi-sense embeddings in a pre-trained language model (BERT). The authors propose a two-stage approach to disambiguate the sense information in BERT. The first step is to use the output layer embedding of BERT to generate a distribution over word senses, and the second stage is to generate the distribution over the word senses. The authors show that the proposed method outperforms the state-of-the-art in terms of performance on a variety of tasks. "
5800,SP:e4cdba0fc7cd7f440d4436219f3959d8d5e2ad28,"3D point - clouds CONJUNCTION 2D images. 2D images CONJUNCTION 3D point - clouds. 3D point - clouds HYPONYM-OF visual representations of the physical world. 2D images HYPONYM-OF visual representations of the physical world. computer vision models USED-FOR 2D image and 3D point - cloud understanding. human vision USED-FOR representations. 2D model architectures USED-FOR 3D point - clouds. neural net model USED-FOR images. architecture USED-FOR neural net model. image - pretrained model CONJUNCTION point - cloud model. point - cloud model CONJUNCTION image - pretrained model. 2D convolutional filters CONJUNCTION 3D convolutional filters. 3D convolutional filters CONJUNCTION 2D convolutional filters. finetuning efforts FEATURE-OF models. batch normalization layers USED-FOR models. 3D point - cloud classification EVALUATE-FOR models. task - specific architectures USED-FOR point - cloud models. few - shot classification EVALUATE-FOR FIP. data efficiency EVALUATE-FOR FIP. It USED-FOR point - cloud models. Generic are transfer, and model. Method is inflated imagepretrained models ( FIP ). ","This paper proposes a method to improve the performance of point cloud and 2D image-to-point cloud models in few-shot classification tasks. The method is based on a combination of image-pretrained models and point cloud models, where the point cloud model is trained with a pre-trained 2D convolutional network and the 2D images are fed with a batch normalization layer. The authors show that the proposed method is able to achieve state-of-the-art performance on the task.   ",This paper proposes a method to improve the transferability of point-cloud models from 2D images to 3D point clouds. The authors propose a new method called inflated imagepretrained models (FIP) that can be applied to the task of few-shot point cloud classification. They show that FIP can improve the performance of point cloud models on a variety of task-specific architectures. They also show that it can be used to reduce the amount of data required to train the models.
5825,SP:dc99c307931ae9c5d4a1b998dc94cfc6ac78d11f,"Autoregressive generative models USED-FOR tasks. sequential data USED-FOR tasks. exposure bias CONJUNCTION long - range coherence. long - range coherence CONJUNCTION exposure bias. method USED-FOR autoregressive generative model. energy - based learning objective USED-FOR method. method USED-FOR exposure bias problem. constraint USED-FOR joint distributions. method USED-FOR temporal coherence. exposure bias problem CONJUNCTION temporal coherence. temporal coherence CONJUNCTION exposure bias problem. constraint USED-FOR method. energy - based models USED-FOR energy scores. autoregressive network USED-FOR energy scores. importance sampling USED-FOR model. language modeling CONJUNCTION neural machine translation. neural machine translation CONJUNCTION language modeling. neural machine translation CONJUNCTION image generation. image generation CONJUNCTION neural machine translation. benchmarks EVALUATE-FOR approach. image generation HYPONYM-OF benchmarks. language modeling HYPONYM-OF benchmarks. neural machine translation HYPONYM-OF benchmarks. Generic are They, and network. Method are chain - style conditional modeling, and MCMC process. OtherScientificTerm is distributions. ","This paper proposes an energy-based generative model to address the long-range coherence and exposure bias problem in sequential data. The proposed method is based on the idea of using an autoregressive network to estimate the energy scores of the joint distributions. The authors propose to use importance sampling to sample from the joint distribution, which is a chain-style conditional modeling. The experiments show that the proposed method achieves state-of-the-art performance on image generation and NMT tasks.","This paper proposes an energy-based learning objective for learning an autoregressive generative model for tasks with long-range coherence and exposure bias. The authors propose a method to learn an energy score for each task using a chain-style conditional model. The energy score is learned by sampling the importance of each task from the ensemble of tasks. The paper also proposes a constraint on the joint distribution of the joint distributions. The method is evaluated on image generation, language modeling, and neural machine translation tasks."
5850,SP:51e748c55bd4134047098559577fa3f37aa7433a,"adversarial attacks FEATURE-OF deep neural networks ( DNNs ). adversarial training ( AT ) method USED-FOR DNN - based classifier. adversarial training ( AT ) method USED-FOR robustness. robustness EVALUATE-FOR DNN - based classifier. adversarial examples USED-FOR adversarial training ( AT ) method. pointwise adversary USED-FOR worst - case adversarial example. PGD - AT and TRADES HYPONYM-OF AT - based methods. pointwise adversary USED-FOR AT - based methods. unified framework USED-FOR Wasserstein distributional robustness. Wasserstein distributional robustness COMPARE AT methods. AT methods COMPARE Wasserstein distributional robustness. Wasserstein cost function CONJUNCTION risk functions. risk functions CONJUNCTION Wasserstein cost function. AT methods PART-OF framework. distributional robustness AT algorithms COMPARE AT counterparts. AT counterparts COMPARE distributional robustness AT algorithms. Method are deep learning systems, classifier, and distributional robustness AT - based algorithms. OtherScientificTerm is adversarial effects. ","This paper studies the distributional robustness of adversarial training (AT) methods. The authors propose a unified framework for Wasserstein adversarial robustness (WDR) and show that it can be used to improve the robustness against adversarial attacks. The WDR framework is based on the observation that the worst-case adversarial examples are generated by a pointwise adversary, which is then used to train an adversarial model.   The authors show that the WDR method is more robust than PGD-AT and TRADES in terms of the adversarial effects. ",This paper proposes a unified framework for Wasserstein distributional robustness (WDR) for adversarial training (AT) methods. The main idea is to use WDR as a pointwise adversary for the worst-case adversarial examples. The authors show that WDR can be combined with PGD-AT and TRADES to improve the robustness of the classifier against adversarial attacks. They also provide a theoretical analysis of WDR.
5875,SP:f192046ea8ad61bfc8e05a0ddb90a8bd15b4640b,"complex dynamics CONJUNCTION sparse annotations. sparse annotations CONJUNCTION complex dynamics. Unsupervised representation learning USED-FOR multivariate time series. it COMPARE problem. problem COMPARE it. data augmentation techniques USED-FOR contrastive training. time slicing USED-FOR segmentlevel augmentation. Bilinear Temporal - Spectral Fusion ( BTSF ) HYPONYM-OF framework. dropout USED-FOR capturing long - term dependencies. dropout USED-FOR global context. instance - level augmentation USED-FOR capturing long - term dependencies. dropout USED-FOR time series. segment - level augmentation COMPARE instance - level augmentation. instance - level augmentation COMPARE segment - level augmentation. global context CONJUNCTION capturing long - term dependencies. capturing long - term dependencies CONJUNCTION global context. dropout USED-FOR instance - level augmentation. iterative bilinear temporal - spectral fusion module USED-FOR affinities. iterative bilinear temporal - spectral fusion module USED-FOR representations of time series. cross - domain interactions USED-FOR representations of time series. alignment CONJUNCTION uniformity. uniformity CONJUNCTION alignment. BTSF USED-FOR bilinear feature representations. forecasting CONJUNCTION anomaly detection. anomaly detection CONJUNCTION forecasting. classification CONJUNCTION forecasting. forecasting CONJUNCTION classification. anomaly detection HYPONYM-OF tasks. tasks USED-FOR time series. anomaly detection HYPONYM-OF time series. classification HYPONYM-OF time series. forecasting HYPONYM-OF time series. anomaly detection HYPONYM-OF tasks. classification HYPONYM-OF tasks. forecasting HYPONYM-OF tasks. BTSF COMPARE state - of - the - art methods. state - of - the - art methods COMPARE BTSF. BTSF COMPARE them. them COMPARE BTSF. Method are contrastive learning, representation learning framework, augmentation methods, and feature representation. OtherScientificTerm is sampling bias. Task is optimization. ", time series is a challenging unsupervised representation learning problem with sparse annotations and complex dynamics. This paper proposes a novel bilinear temporal-spectral fusion (BTSF) framework to address this problem. BTSF is based on time slicing and data augmentation techniques for contrastive training. The proposed method achieves state-of-the-art performance on time series classification and anomaly detection tasks. ,"This paper proposes a novel bilinear temporal-spectral fusion (BTSF) framework for unsupervised representation learning for multivariate time series. BTSF is based on the idea of time slicing, which is a popular data augmentation technique in contrastive learning. The authors propose to use dropout to capture long-term dependencies between time series, and then use instance-level augmentation to capture global context. The proposed method is evaluated on a variety of time series datasets, and compared with state-of-the-art methods. "
5900,SP:ef54840009afb095c67bbbc29a7824c20a375ee8,"model architecture CONJUNCTION batch size. batch size CONJUNCTION model architecture. learning rate USED-FOR deep neural networks. algorithm USED-FOR learning rate. gradient descent USED-FOR learning rate. learning rate CONJUNCTION model weights. model weights CONJUNCTION learning rate. approach USED-FOR learning rate. gradient descent step USED-FOR Learning rate. learning rate FEATURE-OF first and second - order gradients. scheme USED-FOR learning rates. learning rate CONJUNCTION batch size. batch size CONJUNCTION learning rate. it USED-FOR optimizing scheme. optimizing scheme EVALUATE-FOR method. Method are line - search, and neural networks. OtherScientificTerm is weight gradients. ","This paper studies the problem of learning rate optimization in deep neural networks. The authors propose a line-search-based approach to optimize the learning rate in terms of the first and second-order gradients of the model weights. They show that the first-order learning rate is a function of the batch size and learning rate of the weights, and that the second order learning rate depends on the size of the training set and the number of layers. They then show that this learning rate can be optimized using a line search method.   ","This paper studies the problem of optimizing the learning rate of deep neural networks (DNNs) in terms of the weight gradients of the weights. The authors propose a line-search-based approach to find the optimal learning rate for DNNs. The main contribution of the paper is that it proposes a new algorithm for learning rate optimization based on the first-order and second-order gradients. The proposed method is evaluated on a variety of datasets, and it is shown to be competitive with the state-of-the-art."
5925,SP:263c787361cd6d4443ce516d389c694d0fe44b28,"continual meta - learning method USED-FOR sequential multi - task learning. RL USED-FOR offline meta - learning. prior continual learning CONJUNCTION off - policy meta - reinforcement methods. off - policy meta - reinforcement methods CONJUNCTION prior continual learning. CoMPS COMPARE off - policy meta - reinforcement methods. off - policy meta - reinforcement methods COMPARE CoMPS. CoMPS COMPARE prior continual learning. prior continual learning COMPARE CoMPS. continuous control tasks EVALUATE-FOR off - policy meta - reinforcement methods. continuous control tasks EVALUATE-FOR CoMPS. Method are Prior meta - reinforcement learning algorithms, continual reinforcement learning algorithms, continual meta - policy search ( CoMPS ), and meta - training. Generic are they, and method. ",This paper proposes a continual meta-learning method for sequential multi-task learning in RL. The proposed method is based on meta-training with continual reinforcement learning (CoMPS) and meta-reinforcement learning (Reinforcement Learning (RL)). The method is evaluated on a variety of continuous control tasks and achieves state-of-the-art performance.  ,"This paper proposes a continual meta-learning method for offline meta-reinforcement learning. The method is based on continual continual learning (CoMPS), where the goal is to find the best meta-policy for each task in a sequential multi-task learning setting. CoMPS is evaluated on a variety of continuous control tasks, where it outperforms prior continual learning and off-policy meta-Reinforcement Learning (ORL). "
5950,SP:2bd729b7aa045bf74e31229c9e76e57af36e804b,"“ backdoor ” poisoning attack USED-FOR classification models. threat model USED-FOR poisoned classifier. threat model USED-FOR poisoned classifier. adversarial examples USED-FOR classifier. human interaction FEATURE-OF smoothed adversarial images. ImageNet CONJUNCTION TrojAI. TrojAI CONJUNCTION ImageNet. ImageNet HYPONYM-OF high - resolution datasets. TrojAI HYPONYM-OF high - resolution datasets. high - resolution datasets EVALUATE-FOR attack. method USED-FOR triggers. approach COMPARE method. method COMPARE approach. approach COMPARE modeling trigger distributions. modeling trigger distributions COMPARE approach. backdoors PART-OF poisoned classifiers. secret backdoor PART-OF poisoned classifiers. Method are backdoored classifiers, and Denoised Smoothing. Generic is procedure. OtherScientificTerm is trigger distributions. ","This paper proposes a new poisoning attack method that uses Denoised Smoothing to remove backdoored classifiers from adversarial examples. The proposed method is based on the observation that the smoothed adversarial images are more likely to contain backdoors than the original adversarial samples. The authors then propose a method for removing backdoors from the poisoned classifiers. The method is evaluated on ImageNet and TrojAI, and compared with other poisoning attacks. ","This paper proposes a new method for detecting backdoored classifiers. The proposed method is based on Denoised Smoothing (Dsmoothing), which is a method for smoothing adversarial images with human interaction. The authors show that Dsmoothed images are more sensitive to human interaction than the original adversarial examples. They also show that their method is able to identify the trigger distributions of the poisoned classifier.   "
5975,SP:e58ab0e3cff6b18013145a1a99cfa9da0a3d872f,"Generative adversarial networks ( GANs ) USED-FOR content generation tasks. GAN compression methods USED-FOR conditional GANs. distilling unconditional GAN USED-FOR StyleGAN2 architecture. output discrepancy issue FEATURE-OF unconditional GAN distillation. heterogeneous distillation scenario FEATURE-OF knowledge distillation losses. style module USED-FOR semantic information. initialization strategy USED-FOR student model. initialization strategy USED-FOR output consistency. semantic consistency FEATURE-OF teacher and student model. latent - direction - based distillation loss USED-FOR semantic relations. latent space FEATURE-OF semantic relations. latent - direction - based distillation loss USED-FOR semantic consistency. approach USED-FOR StyleGAN2. approach COMPARE GAN distillation methods. GAN distillation methods COMPARE approach. GAN distillation methods USED-FOR distilling StyleGAN2. OtherScientificTerm are computation, resource - constrained devices, latent code, and discrepancy issue. ","This paper proposes a new method for distilling unconditional GANs. The method is based on a style module that extracts the semantic information from the output of the teacher GAN and uses it to distill the knowledge from the student GAN. The proposed method is evaluated on StyleGAN2 and StyleGAN3, and compared with existing methods. ",This paper proposes a new method for distilling unconditional GANs. The main idea is to use a style module to extract the semantic information from the teacher and student model. The style module is then used to distill the student model to the teacher model. This is done by distilling the style module into a latent space and using a direction-based distillation loss. The authors show that their method outperforms the state-of-the-art GAN distillation methods.
6000,SP:2c2231743fa33b95828c6615263954ce1c05f95d,methodology USED-FOR offline algorithms. online settings FEATURE-OF offline algorithms. multi - task learning model USED-FOR behavioral structures. graphs USED-FOR offline algorithms. synthetic data CONJUNCTION historical stock market data. historical stock market data CONJUNCTION synthetic data. historical stock market data EVALUATE-FOR methodology. synthetic data EVALUATE-FOR methodology. ,This paper proposes a method for learning offline algorithms for stock market trading. The main idea is to use a multi-task learning model to model the behavioral structure of the stock market. The proposed method is based on the observation that the performance of offline algorithms can be improved by learning a graph representation of the data distribution. The paper shows that the proposed method outperforms existing offline algorithms on synthetic data and historical stock market data.,This paper proposes a multi-task learning model for offline algorithms. The main idea is to use a graph-based learning model to learn the behavioral structure of an offline algorithm. The model is trained on synthetic data and historical stock market data. The proposed method is evaluated on synthetic and historical data.   
6025,SP:ee3a21d2fb8a073099aa200129a53c31f3b6561d,"Gaussian Processes ( GPs ) HYPONYM-OF Bayesian models. Bayesian models USED-FOR uncertainty estimates. variational inference USED-FOR inferring q. Sparse GPs CONJUNCTION variational inference. variational inference CONJUNCTION Sparse GPs. Sparse GPs USED-FOR GPs. it COMPARE sparse variational GP approaches. sparse variational GP approaches COMPARE it. neural network USED-FOR inducing points locations. training and prediction times EVALUATE-FOR method. Generic are They, them, model, and they. Method is sparse GP approximations. OtherScientificTerm is latent function. Task are learning tasks, and prediction. ","This paper proposes a method for learning sparse Gaussian Processes (GP) models with variational inference. The main idea is to use a neural network to estimate the posterior distribution over the latent space of the GP, and then use a Gaussian process to approximate the posterior distributions of the latent variables. The paper shows that the proposed method is able to learn sparse GP models with faster training and prediction times than existing methods.","This paper proposes a method for learning sparse Gaussian Processes (GP) models with variational variational inference. The main idea is to use a neural network to learn the inducing points locations of the GP, and then use a Gaussian process to estimate the true GP. The authors show that their method outperforms the state-of-the-art in terms of training and prediction times. "
6050,SP:f20c99b441545047a16ae524cc2e317b2c3787a2,"scientific collaborations CONJUNCTION volunteer computing. volunteer computing CONJUNCTION scientific collaborations. hardest problems PART-OF deep learning. Byzantine tolerance FEATURE-OF distributed training algorithms. algorithms USED-FOR large - scale distributed deep learning. protocol USED-FOR secure ( Byzantinetolerant ) decentralized training. communication efficiency FEATURE-OF protocol. theoretical bounds USED-FOR resistance. Byzantine and Sybil attacks FEATURE-OF resistance. communication overhead FEATURE-OF it. Byzantine attackers FEATURE-OF image classification and language modeling. Generic are systems, and models. Metric is efficiency. OtherScientificTerm are redundant communication, and trusted server. ","This paper studies Byzantine tolerance in distributed deep learning. The authors propose a decentralized training algorithm that is Byzantine tolerant to Byzantine attacks. Theoretical analysis is provided to show that Byzantine attacks can be attacked in this distributed setting, and the authors provide theoretical bounds on the communication overhead. Experiments are conducted on image classification and language modeling tasks.   ",This paper studies the problem of Byzantine tolerance in distributed deep learning. The authors propose a protocol for secure decentralized training with Byzantine tolerance. They show that Byzantine tolerance can be used to reduce the communication overhead of decentralized training. They also provide theoretical bounds on the resistance of Byzantine and Sybil attacks. 
6075,SP:93894f20ab2593e5237b6972fef9fe63e96af89a,"Smoothed particle hydrodynamics ( SPH ) HYPONYM-OF mesh - free Lagrangian method. astrophysics and engineering applications FEATURE-OF weaklyand strongly compressible turbulence. physics based parameters CONJUNCTION Neural Networks. Neural Networks CONJUNCTION physics based parameters. Neural Networks USED-FOR universal function approximators. Neural Networks USED-FOR SPH informed fluid simulators. physics based parameters USED-FOR SPH informed fluid simulators. forward and adjoint based sensitivity analyses USED-FOR gradient based optimization. learning algorithm USED-FOR mixed mode approach. physics informed learning method USED-FOR inverse problems. physically interpretable parameter space FEATURE-OF inverse problems. time scales CONJUNCTION Reynolds numbers. Reynolds numbers CONJUNCTION time scales. interpretability CONJUNCTION generalizability. generalizability CONJUNCTION interpretability. hierarchy of models USED-FOR physical structure. Reynolds numbers FEATURE-OF generalizability. time scales FEATURE-OF generalizability. OtherScientificTerm are Neural Network parameters, Lagrangian statistics of turbulence, and physical symmetries. Material is training data. ","This paper proposes to use neural networks as universal function approximators for smoothed particle hydrodynamics (SPH), which is a mesh-free Lagrangian method for studying weakly-and strongly compressible turbulence. The proposed method is based on a mixed-mode approach, which uses forward and adjoint based sensitivity analyses for gradient-based optimization. Theoretical analysis is provided to show the generalization properties of the proposed method. Empirical results are also provided to demonstrate the effectiveness of the method. ","This paper proposes a new method for learning physics-informed models for smoothed particle hydrodynamics (SPH), a mesh-free Lagrangian method for turbulence simulation. The method is based on a mixture of forward and adjoint based sensitivity analyses, and a mixed mode learning algorithm for inverse problems. The authors show that the proposed method can be used to learn a physically interpretable parameter space for SPH, and that it can generalize well across time and space."
6100,SP:d11b81f9ab414fcf430a03cd70c2d3246b678474,"approach USED-FOR deterministic neural network. accuracy CONJUNCTION uncertainty estimates. uncertainty estimates CONJUNCTION accuracy. entropy maximization regularizer USED-FOR predictive distribution. entropy maximization regularizer USED-FOR approach. embedding space FEATURE-OF predictive distribution. cross - entropy loss USED-FOR approach. entropy FEATURE-OF samples. images USED-FOR convex combination. convex combination USED-FOR synthetically generating between - cluster samples. data - dependent regularization USED-FOR maximum likelihood estimation. data - dependent regularization USED-FOR solution. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. real - world datasets EVALUATE-FOR Mix - MaxEnt. calibrated probabilities USED-FOR in - distribution data. Mix - MaxEnt USED-FOR uncertainty estimates. ResNet and Wide - ResNet architectures USED-FOR real - world datasets. CIFAR-10 HYPONYM-OF real - world datasets. CIFAR-100 HYPONYM-OF real - world datasets. classification accuracy EVALUATE-FOR Mix - MaxEnt. OtherScientificTerm are class clusters, out - of - distribution samples, high entropy regions, entropy barrier, and superficial input perturbations. ","This paper proposes Mix-MaxEnt, a method to improve the prediction accuracy and uncertainty estimates of deterministic neural networks. The method is based on the observation that high-entropy regions in the training set are more likely to contain out-of-distribution samples than low-entropic regions. To address this issue, the authors propose to generate between-cluster samples in the embedding space of the predictive distribution by using a convex combination of images from different classes. The proposed method is evaluated on CIFAR-10/100 and ImageNet datasets and achieves better prediction accuracy than existing methods.  ","This paper proposes Mix-MaxEnt, a novel approach to improve the prediction accuracy of a deterministic neural network. The key idea is to use the entropy-maximization regularizer to generate between-cluster samples in the embedding space of the predictive distribution. The proposed approach is based on the cross-entropy loss, which is a convex combination of the entropy of the samples, and a data-dependent regularization. The experimental results show that the proposed approach outperforms the state-of-the-art on CIFAR-10 and Cifar-100 datasets."
6125,SP:365490b872464f00634dc7a50d024fceaf0a61ee,"Generative Adversarial Networks ( GANs ) CONJUNCTION auto - encoder animating images. auto - encoder animating images CONJUNCTION Generative Adversarial Networks ( GANs ). driving videos USED-FOR structure representation. structure representation USED-FOR animation - approaches. modules USED-FOR animation - model. modules USED-FOR extraction of structure information. Latent Image Animator ( LIA ) HYPONYM-OF self - supervised autoencoder. linear combination USED-FOR latent space. model COMPARE state - of - art methods. state - of - art methods COMPARE model. TED - talk datasets EVALUATE-FOR state - of - art methods. TED - talk datasets EVALUATE-FOR model. VoxCeleb EVALUATE-FOR state - of - art methods. VoxCeleb EVALUATE-FOR model. LIA USED-FOR motion of a driving video. landmarks CONJUNCTION region representations. region representations CONJUNCTION landmarks. LIA USED-FOR images. structure representations USED-FOR LIA. region representations HYPONYM-OF structure representations. landmarks HYPONYM-OF structure representations. Material are still images, LIA animation examples, and VoxCeleb dataset. Generic are approaches, and models. OtherScientificTerm are appearance variation, motion, and orthogonal motion directions. Metric are complexity, and generated quality. ",This paper proposes a self-supervised auto-encoder-based latent image animator (LIA) model for driving videos. The proposed method is based on a linear combination of modules that extract the structure information from the video and use this information to generate a latent representation of the motion of the vehicle in the driving video. The method is evaluated on the VoxCeleb dataset and TED-Talks dataset and achieves state-of-the-art performance.,"This paper proposes a self-supervised auto-encoder-based Latent Image Animator (LIA) model for driving videos. The model is based on a linear combination of two modules, which extract the structure information from the driving video. The proposed model is evaluated on two datasets, VoxCeleb and TED-Talks, where it outperforms state-of-the-art methods."
6150,SP:86f9f89f84e117c86478b9afaf087f65524f5472,"meta - training tasks USED-FOR meta - learning algorithms. approach USED-FOR tasks. data - adaptive meta - regularization USED-FOR MLTI. generalization EVALUATE-FOR MLTI. pose prediction CONJUNCTION molecule property prediction. molecule property prediction CONJUNCTION pose prediction. molecule property prediction CONJUNCTION medical image classification. medical image classification CONJUNCTION molecule property prediction. image recognition CONJUNCTION pose prediction. pose prediction CONJUNCTION image recognition. MLTI framework COMPARE state - of - the - art strategies. state - of - the - art strategies COMPARE MLTI framework. MLTI framework CONJUNCTION representative meta - learning algorithms. representative meta - learning algorithms CONJUNCTION MLTI framework. datasets EVALUATE-FOR MLTI framework. image recognition HYPONYM-OF datasets. medical image classification HYPONYM-OF datasets. pose prediction HYPONYM-OF datasets. molecule property prediction HYPONYM-OF datasets. Method is Meta - learning. OtherScientificTerm are real - world scenarios, and interpolation. ","This paper proposes a meta-learning approach for meta-training tasks with data-adaptive meta-regularization (MLTI) to improve the generalization performance. The proposed approach is based on the idea of data-adversarial regularization, where the meta-learner is trained with a set of samples from the training set. The authors show that the proposed approach can improve the performance on image classification, pose prediction, and molecule property prediction tasks. ","This paper proposes a data-adaptive meta-regularization (MLTI) framework for meta-learning tasks. The proposed method is based on the idea of data-based meta-training, where the training data is sampled from a set of data points and the data points are used to train the meta-learner. The authors show that the proposed method outperforms the state-of-the-art in terms of generalization on a variety of datasets. "
6175,SP:73d577e9c4f4af5e11a9e5bdb583ee0f50a315f5,"approach USED-FOR fairness of downstream predictors. encoding sensitive data USED-FOR fairness of downstream predictors. unfairness FEATURE-OF adversarial predictors. representations USED-FOR sensitive attributes. fairness guarantees FEATURE-OF learned representations. probability density USED-FOR sensitive groups. normalizing flow USED-FOR statistical distance. statistical distance FEATURE-OF latent representations. normalizing flow USED-FOR encoder. maximum unfairness EVALUATE-FOR adversarial downstream predictor. interpretability CONJUNCTION transfer learning. transfer learning CONJUNCTION interpretability. FNF USED-FOR group fairness notions. FNF USED-FOR properties. real - world datasets EVALUATE-FOR properties. transfer learning HYPONYM-OF properties. interpretability HYPONYM-OF properties. real - world datasets EVALUATE-FOR FNF. Method are Fair representation learning, Fair Normalizing Flows ( FNF ), and likelihood computation. ","This paper proposes a new approach for fair representation learning based on normalizing flows. In particular, the authors propose to use a normalizing flow to compute the statistical distance between the encoder and the latent representations of the sensitive groups. The authors show that the proposed approach can improve the fairness of downstream predictors in the presence of adversarial perturbations. The proposed approach is evaluated on two real-world datasets.",This paper proposes a new approach for fair representation learning (FFN) that uses normalizing flows to improve the fairness of downstream predictors. The main idea is to use the normalizing flow to reduce the distance between the representations learned by the encoder and the downstream predictor. The authors show that FNF can improve the interpretability and transferability of the learned representations. They also show that it can improve transfer learning and interpretability. 
6200,SP:404d5643327f60f0f06f820033a56081f9e01900,"subgraph patterns on graphs USED-FOR graph - based tasks. graph neural networks ( GNNs ) USED-FOR low - dimensional representation. node - centric message passing mechanism USED-FOR GNNs. they USED-FOR complex structure matching. complex structure matching USED-FOR isomorphism counting. COUNT - GNN USED-FOR subgraph isomorphism counting. GNN USED-FOR subgraph isomorphism counting. COUNT - GNN HYPONYM-OF GNN. edge - centric message passing scheme USED-FOR edge level. COUNT - GNN USED-FOR fine - grained structural information. edge USED-FOR encoding graph structures. graph representation USED-FOR graph level. benchmark datasets EVALUATE-FOR COUNT - GNN. COUNT - GNN COMPARE baselines. baselines COMPARE COUNT - GNN. OtherScientificTerm are graph structures, subgraph isomorphisms, nodes, graph, query graphs, structured query graphs, edges, edge adjacency, and first - class citizens. Material is graph data. Method is backtracking framework. Metric is computational cost. Task are node - oriented tasks, and matching. ","This paper proposes Count-GNN, a method for graph isomorphism counting. The method is based on a node-centric message-passing mechanism and uses edge-based message passing to encode information about the structure of the query graphs. The proposed method is evaluated on a variety of benchmark datasets and achieves state-of-the-art performance.","This paper proposes a new GNN called Count-GNN, which is based on a node-centric message passing mechanism. The key idea is to use the edge-based message-passing mechanism in GNNs to encode the structure of the graph, and then use it to perform isomorphism counting. The method is evaluated on a variety of benchmark datasets, and it is shown that the proposed method outperforms the baselines. "
6225,SP:5a94f18156ab2949c86de45fcf0de2e16977eebb,"Agnostic Personalized Federated Learning ( APFL ) HYPONYM-OF loosely constrained federated learning. Similarity Matching CONJUNCTION Kernel Factorization ( SimFed ). Kernel Factorization ( SimFed ) CONJUNCTION Similarity Matching. Kernel Factorization ( SimFed ) HYPONYM-OF method. Similarity Matching HYPONYM-OF method. ones USED-FOR personalized knowledge reflection. method USED-FOR task - level similarity. locally learned knowledge USED-FOR method. locally learned knowledge USED-FOR task - level similarity. knowledge collapse CONJUNCTION information loss. information loss CONJUNCTION knowledge collapse. dimensionlaity of parameter space USED-FOR knowledge collapse. information loss FEATURE-OF heterogeneous knowledge. basis vectors PART-OF model parameters. method COMPARE federated learning methods. federated learning methods COMPARE method. singleand multi - domain datasets EVALUATE-FOR method. singleand multi - domain datasets EVALUATE-FOR method. Task is federated learning. OtherScientificTerm are personalized labels, Label Heterogeneity, and Domain Heterogeneity. Generic is they. Method are labeling schemes, and agnostic personalized federated learning. Material is local data. ","This paper studies the problem of personalized federated learning in the presence of heterogeneous labels. The authors propose a novel approach to address the problem by leveraging the local knowledge of the clients. The proposed approach is based on the idea of label homogeneity, where each client has a set of personalized labels and the goal is to learn a model that is agnostic to each client’s personalized labels. To this end, the authors propose to use local knowledge from the clients as the basis vectors in the model parameters.   The proposed method is evaluated on a variety of datasets and achieves state-of-the-art performance.",This paper proposes a method for agnostic personalized federated learning (APFL) where the task-level similarity between two tasks is based on task-specific knowledge. The authors propose a method called Similarity Matching (similarity matching) and Kernel Factorization (SimFed) to address the problem of knowledge collapse in the domain-specific setting. The proposed method is evaluated on both single-domain and multi-domain datasets. 
6250,SP:97f30bea31eccef6c770fbce1e14fd6d2493a178,"Object Dynamics Distillation Network ( ODDN ) USED-FOR object dynamic representations. velocity HYPONYM-OF object dynamic representations. raw video input USED-FOR object dynamic representations. it USED-FOR dynamic representations of objects. relation module USED-FOR object - pair interactions. relation module USED-FOR dynamic representations of objects. video events reasoning CONJUNCTION video prediction. video prediction CONJUNCTION video events reasoning. video events reasoning EVALUATE-FOR approach. scene representation methods USED-FOR representaions. occlusion CONJUNCTION objects collision. objects collision CONJUNCTION occlusion. object dynamic clues USED-FOR model. segmentation CONJUNCTION reconstruction. reconstruction CONJUNCTION segmentation. scene decomposition quality EVALUATE-FOR reconstruction. scene decomposition quality EVALUATE-FOR segmentation. scene decomposition quality EVALUATE-FOR model. OtherScientificTerm are abstract entities, and physical events. Method are object - centric representations of scenes, and ODDN. Material is static images. Task are object dynamics, and video understanding. ","This paper proposes an object dynamics distillation network (OODDNN) to learn dynamic representations of objects in a video. The proposed method is based on the idea that objects are abstract entities, and physical events can be viewed as a set of physical events. The authors propose a relation module to model the object-pair interactions between objects, and an object-vocal velocity module to capture the dynamics of the objects. Experiments show that the proposed method achieves state-of-the-art results on video events reasoning and video prediction tasks.","This paper proposes an object dynamics distillation network (ODDN) for video understanding. The ODDN distills the video input into a dynamic representation of objects, which is then used to learn the dynamics of the scene. The model is trained using a combination of two methods: (1) object-pair interactions, and (2) a relation module. The method is evaluated on a variety of tasks, including segmentation, reconstruction, and video prediction.  "
6275,SP:ba8e50d1fa9cb824fa3f76c0c691997cd151d760,"Graph neural networks ( GNN ) USED-FOR graph - based learning tasks. nodes USED-FOR task. link / motif prediction HYPONYM-OF nodes. random node features CONJUNCTION node distance features. node distance features CONJUNCTION random node features. slow convergence CONJUNCTION inaccurate prediction. inaccurate prediction CONJUNCTION slow convergence. Laplacian Eigenmap CONJUNCTION Deepwalk. Deepwalk CONJUNCTION Laplacian Eigenmap. positional encoding ( PE ) techniques USED-FOR positional features. PE USED-FOR GNNs. positional features USED-FOR GNNs. Laplacian Eigenmap HYPONYM-OF positional encoding ( PE ) techniques. Deepwalk HYPONYM-OF positional encoding ( PE ) techniques. mathematical analysis USED-FOR PEG. PEG HYPONYM-OF GNN layers. mathematical analysis USED-FOR GNN layers. node features CONJUNCTION positional features. positional features CONJUNCTION node features. node features USED-FOR PEG. positional features USED-FOR PEG. permutation equivariance FEATURE-OF PEG. node features CONJUNCTION rotation equivariance. rotation equivariance CONJUNCTION node features. link prediction EVALUATE-FOR PEG. real - world networks USED-FOR link prediction. generalization EVALUATE-FOR PEG. Generic are they, and solution. Metric is complexity. ",This paper proposes a new positional encoding method for link prediction in GNNs. The proposed method is based on Laplacian Eigenmap and Deepwalk. Theoretical analysis is provided to show that the proposed method can be viewed as a GNN with positional encoding. Experiments are conducted on link prediction tasks on CIFAR-10 and ImageNet.  ,This paper proposes a new positional encoding (PEG) method for graph neural networks (GNNs) for link prediction. The proposed method is based on the Laplacian Eigenmap (LEM) and Deepwalk (Deepwalk-based) positional encoding techniques. The main contribution of the paper is a theoretical analysis of the PEG method. The authors show that the proposed method can be used to improve the performance of GNNs on link prediction tasks. They also show that it can improve the generalization ability of the network.
6300,SP:cf448479f68c3194c1a9e11729bf70d7cc2ae8fd,non - parallel datasets USED-FOR models. non - parallel datasets USED-FOR text style transfer models. weak supervision USED-FOR style transfer models. LaMer HYPONYM-OF text style transfer framework. large - scale language models USED-FOR text style transfer framework. MLE training CONJUNCTION imitation learning refinement. imitation learning refinement CONJUNCTION MLE training. imitation learning refinement USED-FOR intrinsic parallelism. parallel expressions PART-OF non - parallel datasets. intrinsic parallelism FEATURE-OF data. imitation learning refinement USED-FOR LaMer. MLE training USED-FOR LaMer. scene graphs USED-FOR LaMer. scene graphs USED-FOR parallel expressions. content preservation CONJUNCTION fluency. fluency CONJUNCTION content preservation. transfer accuracy CONJUNCTION content preservation. content preservation CONJUNCTION transfer accuracy. task EVALUATE-FOR model. sentiment & formality transfer EVALUATE-FOR model. sentiment & formality transfer CONJUNCTION political stance transfer. political stance transfer CONJUNCTION sentiment & formality transfer. political stance transfer EVALUATE-FOR model. political stance transfer HYPONYM-OF task. fluency EVALUATE-FOR model. content preservation EVALUATE-FOR model. transfer accuracy EVALUATE-FOR model. model COMPARE models. models COMPARE model. OtherScientificTerm is style - independent information. ,"This paper proposes a method for text style transfer from text to another language. The method is based on the idea that style-independent information can be extracted from the text in a style-dependent manner. To do this, the authors propose a method called LaMer, which is a style transfer model with weak supervision. The model is trained using MLE with imitation learning refinement and is trained on a set of non-parallel text generation datasets. The authors show that the proposed method achieves state-of-the-art performance on sentiment transfer, formality transfer, and political stance transfer.","This paper proposes LaMer, a text style transfer model for non-parallel datasets. LaMer is based on the idea that style-independent information can be extracted from a set of parallel expressions in the form of a scene graph. The model is trained with MLE training and imitation learning refinement, and it is evaluated on sentiment transfer, political stance transfer, and content preservation."
6325,SP:8f7b2d1020d9e527118b8fb816760c13b0d0bfcb,"Multi - hop logical reasoning HYPONYM-OF representation learning on knowledge graphs ( KGs ). one - hop link prediction CONJUNCTION logical queries. logical queries CONJUNCTION one - hop link prediction. one - hop link prediction PART-OF It. logical queries PART-OF It. classical, triple - based graphs USED-FOR algorithms. hyper - relational modeling paradigm USED-FOR KGs. key - value pairs FEATURE-OF typed edges. approaches USED-FOR approximate query answering ( QA ). Hyper - relational queries PART-OF real - world KG applications. qualifier pairs USED-FOR approaches. hyper - relational KGs USED-FOR complex queries. multi - hop reasoning problem USED-FOR complex queries. multi - hop reasoning problem USED-FOR hyper - relational KGs. Graph Neural Networks CONJUNCTION query embedding techniques. query embedding techniques CONJUNCTION Graph Neural Networks. method USED-FOR queries. qualifiers USED-FOR QA. query patterns USED-FOR QA. query patterns EVALUATE-FOR qualifiers. Generic is paradigm. OtherScientificTerm are fine - grained context, and hyper - relational conjunctive queries. ","This paper proposes a method for approximate query answering (QA) on hyper-relational knowledge graphs (KGs). The proposed method is based on the idea that the query embeddings of a KG can be represented as a set of key-value pairs, where the key and value pairs correspond to edges in the KG. The key idea is to use a query embedding network to embed the query into the KGs, and then use the queries as queries in the QA task. The proposed approach is evaluated on a variety of knowledge graph datasets, and compared to a number of baselines.","This paper proposes a new approach to approximate query answering (QA) for hyper-relational knowledge graphs (KGs). The proposed approach is based on the idea that the key-value pairs in a KG can be represented as a set of query pairs, and the query pairs can be used to answer queries. The key idea is to use the query embeddings of Graph Neural Networks (GNNs) and query embedding techniques (e.g., GANs) to learn the query patterns. The authors show that the proposed approach outperforms the state-of-the-art in terms of QA performance. "
6350,SP:5f8b58424a1a8eeb72217e75189d6f773a298a7a,"DYHPO HYPONYM-OF method. Bayesian optimization USED-FOR gray - box setup. Bayesian optimization USED-FOR technique. surrogate USED-FOR Gaussian Processes. multi - budget information FEATURE-OF acquisition function. acquisition function PART-OF surrogate. learning curve dynamics PART-OF surrogate. DYHPO COMPARE hyperparameter optimization baselines. hyperparameter optimization baselines COMPARE DYHPO. Method are Gray - box hyperparameter optimization techniques, and multibudget search mechanisms. OtherScientificTerm is hyperparameter configurations. ","This paper proposes a Bayesian hyperparameter optimization method for Gaussian Processes. The main idea is to use Bayesian optimization to learn the dynamics of the hyperparameters in a Gaussian process, and then use the learned dynamics as a surrogate function to estimate the acquisition function of the learning curve dynamics. The proposed method is shown to outperform existing methods in terms of performance and computational complexity.",This paper proposes a Bayesian hyperparameter optimization (DYHPO) method for Gaussian Processes with multi-budget information. The main idea is to use a surrogate function to learn the learning curve dynamics of the Gaussian processes. The acquisition function of the surrogate is then used to estimate the multi- budget information of the acquisition function. Experiments show that the proposed method outperforms the state-of-the-art in terms of hyperparameters. 
6375,SP:99d3d94e3af5d2dc7b92c00ac1345d1d2dd0d15b,"learned image compression COMPARE image coding techniques. image coding techniques COMPARE learned image compression. rate - distortion EVALUATE-FOR learned image compression. deterministic inference USED-FOR Gaussian mixture models. methods USED-FOR image compression models. cross - platform consistent manner FEATURE-OF image compression models. Method are non - deterministic calculation, and training and fine - tuning based approaches. Task is decoding. OtherScientificTerm are post - training quantization, and entropy parameters. ","This paper studies the problem of image compression in the presence of non-deterministic quantization. The authors propose to use a Gaussian mixture model to model the entropy of the image, which is then used to train an image compression model. They show that the proposed method is able to improve the rate-distortion compared to the state-of-the-art image compression methods. ","This paper studies the problem of rate-distortion in image compression. The authors propose a non-deterministic quantization method for image compression, which is based on a Gaussian mixture model. The main idea is to use a deterministic inference scheme to estimate the entropy of the image, and then fine-tune the quantization parameters. They show that their method can be applied to a variety of image compression models, and show that it can be used to improve the performance of learned image compression methods. "
6400,SP:85d0df515e9e555f3ea1c21d607304dfaeae69c0,"nanoscale resolution FEATURE-OF imaging and analysis of cellular ultrastructure. fully unsupervised Noise Reconstruction and Removal Network USED-FOR denoising scanning electron microscopy images. architecture USED-FOR noise. gated recurrent units USED-FOR architecture. sequential data USED-FOR noise. fully unsupervised training USED-FOR network. fully unsupervised training COMPARE supervised approaches. supervised approaches COMPARE fully unsupervised training. 3D electron microscopy data sets EVALUATE-FOR supervised approaches. Material is labels and/or noise - free data sets. OtherScientificTerm are time consuming manual annotations, and imaging artifacts. Metric is empirical metrics. ",This paper proposes a noise-removal network for denoising 3D electron microscopy images. The proposed method is a fully unsupervised noise removal network that is trained on a large number of noise-free data sets. The main idea is to use a gated recurrent network to remove the noise from the image. The network is trained using a combination of supervised and unsupervision.   ,This paper proposes a fully unsupervised noise reconstruction and removal network for denoising scanning electron microscopy images. The proposed method is based on a gated recurrent unit architecture and uses sequential data to train the network. Experiments show that the proposed method outperforms the state-of-the-art.
6425,SP:e6275b0b103fa90dcebcdd3d3c14c830c3402972,"Graph neural networks ( GNNs ) CONJUNCTION label propagation. label propagation CONJUNCTION Graph neural networks ( GNNs ). graph structure USED-FOR tasks. node property prediction HYPONYM-OF tasks. stacked message - passing layers USED-FOR predictive embeddings. stacked message - passing layers USED-FOR node features. neighborhood information FEATURE-OF stacked message - passing layers. stacked message - passing layers USED-FOR former. spreading label information USED-FOR unlabeled nodes. parameter - free diffusion process USED-FOR spreading label information. features CONJUNCTION labels. labels CONJUNCTION features. statistical properties PART-OF training pipeline. label trick USED-FOR training pipeline. deterministic training objective USED-FOR stochastic label trick. data - fitting term USED-FOR label leakage issues. graph structure FEATURE-OF regularization factor. Generic are latter, and two. OtherScientificTerm is GNN inputs. Material is Open Graph Benchmark ( OGB ) leaderboard. Task is label trick use cases. ","-based graph neural networks (GNNs) are well-known for their ability to predict node properties. However, they suffer from label leakage issues due to the fact that they are trained on unlabeled data. This paper aims to address this issue by introducing a stochastic label trick to improve the performance of GNNs. Theoretically, the authors show that the label trick can be used to alleviate the label leakage issue in GNN training. The proposed method is evaluated on the Open Graph Benchmark (OGB) leaderboard.   ","This paper studies the problem of label leakage in graph neural networks (GNNs). In particular, the authors propose a stochastic label trick for GNNs that can be used to improve the performance on the Open Graph Benchmark (OGB) leaderboard. The main idea is to use a parameter-free diffusion process to spread the label information between unlabeled nodes. The authors show that the proposed method can improve performance on OGB leaderboard by a factor of 1.5. "
6450,SP:b6cbc3661f9c440687c3dd01ee35a118c87db377,"Theory of mind ( ToM ) HYPONYM-OF human intelligence. machines USED-FOR theory of mind. ToM agents USED-FOR tasks. predefined roles FEATURE-OF tasks. speaker - listener scenarios HYPONYM-OF predefined roles. strategy USED-FOR SymmToM. theory of mind USED-FOR strategy. theory of mind USED-FOR SymmToM. multi - agent deep reinforcement learning models USED-FOR mental states. modeling of theory of mind USED-FOR multi - agent scenarios. OtherScientificTerm are machine theory of mind, and grid world. Method are multiagent environment SymmToM, and ToM model. ","This paper proposes a method to learn a model of the state of a multi-agent environment called SymmToM, in which each agent is given a speaker and a listener. The goal is to learn to predict the speaker’s state and the listener's state in the context of a given task. The method is based on the theory of mind (ToM) and is trained using reinforcement learning. Experiments show that the proposed method outperforms baselines on speaker-listener tasks.  ","This paper proposes a method to model the multi-agent environment SymmToM, where each agent is given a speaker and a listener, and the goal is to solve a speaker-listener task. The authors propose a model that models the state of the speaker and the listener in a grid world, where the speaker has access to the listener's state. The model is trained using a deep reinforcement learning framework, where it is trained to predict the speaker's state using a set of pre-trained agents. The proposed method is evaluated on a variety of tasks, and compared with a number of baselines."
6475,SP:f8ce83805eee46c6c196e8477bf10d8d7f7e0f46,"visual data collecting and processing technology USED-FOR robots. orientations CONJUNCTION illumination. illumination CONJUNCTION orientations. smart manufacturing CONJUNCTION high - mix - low - volume production. high - mix - low - volume production CONJUNCTION smart manufacturing. Zero - shot object detection HYPONYM-OF unsupervised learning. car CONJUNCTION people. people CONJUNCTION car. bikes CONJUNCTION car. car CONJUNCTION bikes. car HYPONYM-OF outdoor scenes. people HYPONYM-OF outdoor scenes. bikes HYPONYM-OF outdoor scenes. indoor scenes FEATURE-OF zero - shot detection of daily objects. zero - shot detection USED-FOR object size level. dataset EVALUATE-FOR zero - shot detection. Method are robot vision system, vision system, zero - shot object detection, and zero - shot object detection algorithm. Task are manufacturing environment, production process, and detection of daily objects. Generic is it. OtherScientificTerm is manufacturing setup. Material is YCB Video Dataset. ","This paper proposes a method for zero-shot object detection of daily objects in a manufacturing environment. The proposed method is based on the YCB Video Dataset, which is a collection of videos collected by a robot vision system. The method is evaluated on a variety of indoor and outdoor scenes, and it is shown that the proposed method outperforms the baselines.","This paper proposes a zero-shot object detection method for robot vision. The proposed method is based on the YCB Video Dataset (YCB) dataset, which is used to train a robot vision system that can detect daily objects in a manufacturing environment. The method is evaluated on a variety of scenarios, including high-mix-low-volume production, smart manufacturing, and smart manufacturing.   "
6500,SP:aa1dcd9217270010f16a00004facede942efea17,"generating future frames CONJUNCTION learning environment dynamics. learning environment dynamics CONJUNCTION generating future frames. Video prediction HYPONYM-OF problem. autoregressive prediction model USED-FOR image generator. autoregressive latent video models USED-FOR video prediction tool. autoregressive latent video prediction model USED-FOR high - fidelity future frames. autoregressive latent video prediction model USED-FOR high - resolution ( 256x256 ) videos. top - k sampling CONJUNCTION data augmentation. data augmentation CONJUNCTION top - k sampling. data augmentation USED-FOR video prediction quality. top - k sampling USED-FOR video prediction quality. method COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE method. method USED-FOR highresolution video prediction. complex and large - scale datasets USED-FOR highresolution video prediction. video prediction benchmarks EVALUATE-FOR state - of - the - art approaches. video prediction benchmarks EVALUATE-FOR method. Task are video prediction, and predicting high - fidelity future frames. Method are image generator model, prior models, and causal transformer model. OtherScientificTerm is latent space of the image generator. Generic is models. ",This paper proposes a video prediction model that uses an autoregressive latent video model to predict high-resolution (256x256) videos. The proposed method is based on a causal transformer model that learns to predict future frames in the latent space of the image generator model. The model is trained with top-k sampling and data augmentation to improve the video prediction quality. The experimental results show that the proposed method achieves state-of-the-art performance on video prediction benchmarks.,"This paper proposes a new video prediction method for high-resolution video prediction. The key idea is to use an autoregressive latent video prediction model to predict high-fidelity future frames. The model is based on a causal transformer model, where the latent space of the image generator is represented by a latent variable. The latent variable is then used to generate high-quality videos. The method is evaluated on a variety of video prediction benchmarks. "
6525,SP:7f57896afd63bc869d2db6ddf7abbeaa71daae11,vision - specific inductive biases USED-FOR Vision Transformers ( ViTs ). image recognition EVALUATE-FOR Vision Transformers ( ViTs ). ViT architecture PART-OF generative adversarial networks ( GANs ). regularization methods USED-FOR GANs. regularization methods USED-FOR self - attention. regularization methods USED-FOR ViT discriminators. regularization techniques USED-FOR GANs. ViTs USED-FOR GANs. CIFAR-10 CONJUNCTION CelebA. CelebA CONJUNCTION CIFAR-10. approach COMPARE CNNbased GAN models. CNNbased GAN models COMPARE approach. CelebA CONJUNCTION LSUN bedroom. LSUN bedroom CONJUNCTION CelebA. ViTGAN HYPONYM-OF approach. LSUN bedroom HYPONYM-OF datasets. datasets EVALUATE-FOR CNNbased GAN models. CIFAR-10 HYPONYM-OF datasets. datasets EVALUATE-FOR approach. CelebA HYPONYM-OF datasets. Task is image generation. Method is ViT generators. OtherScientificTerm is latent and pixel mapping layers. ," and LSUN bedroom. The paper proposes to use vision-specific inductive biases to improve the performance of ViT-based generative adversarial networks (GANs). Specifically, the paper proposes two regularization methods for self-attention and discriminators in the ViT discriminators. Experiments on CIFAR-10 and CelebA show that the proposed method outperforms the baseline methods. ","This paper proposes a novel ViT-based generative adversarial network (GAN) architecture for image recognition. The ViT architecture consists of ViT generators and discriminators, and the discriminators are trained using a series of regularization methods. The proposed method is evaluated on CIFAR-10, CelebA, and LSUN bedroom datasets. "
6550,SP:bbae3afcaea0a2e54904cb8daaed7df4fe37da6e,"sample quality EVALUATE-FOR models. variational autoencoders USED-FOR image generative modeling. entropy FEATURE-OF natural image distributions. visually imperceptible information FEATURE-OF entropy. models USED-FOR competitive likelihoods. imperceptible information PART-OF likelihood signal. good sample quality EVALUATE-FOR modeling of visually perceptible information. OtherScientificTerm are Good likelihoods, high - dimensional image data distributions, and visually perceptible bits. Task is generative modeling. ","This paper studies the problem of image generative modeling with variational autoencoders (VAEs). The authors propose to use the entropy of natural image distributions as a metric to measure the quality of the likelihood signal. The authors show that the entropy is a measure of the visual perceptibility of an image, and that it can be used as a measure for good sample quality. They also show that VAEs with good likelihoods have competitive likelihoods.   ","This paper studies the problem of good sample quality in generative models for image generative modeling. In particular, the authors focus on the question of good likelihoods for high-dimensional image data distributions. The authors show that the entropy of natural image distributions can be viewed as a measure of visually imperceptible information. They show that good samples quality can be achieved by using a variational autoencoder with a good likelihood signal. They also provide a theoretical analysis of the trade-off between good and bad sample quality."
6575,SP:bfed56018134ec66cde9a7e958df964d4cca3164,"Diffusion probabilistic models ( DPMs ) HYPONYM-OF generative models. inference USED-FOR variance. reverse process FEATURE-OF variance. optimal reverse variance CONJUNCTION optimal KL divergence. optimal KL divergence CONJUNCTION optimal reverse variance. optimal KL divergence FEATURE-OF DPM. optimal reverse variance FEATURE-OF DPM. analytic forms FEATURE-OF optimal KL divergence. analytic forms FEATURE-OF optimal reverse variance. Monte Carlo method CONJUNCTION pretrained score - based model. pretrained score - based model CONJUNCTION Monte Carlo method. Analytic - DPM HYPONYM-OF training - free inference framework. training - free inference framework USED-FOR analytic forms. Analytic - DPM USED-FOR analytic forms. Monte Carlo method USED-FOR Analytic - DPM. pretrained score - based model USED-FOR Analytic - DPM. analytic - DPM USED-FOR DPMs. analytic - DPM USED-FOR log - likelihood. log - likelihood FEATURE-OF DPMs. Task is inference of DPMs. OtherScientificTerm are score function, and lower and upper bounds. Method is score - based model. Generic is estimate. ","This paper studies the problem of inference in diffusion probabilistic models (DPMs). The authors propose a training-free inference framework, called Analytic-DPM, which uses a score-based model and a Monte Carlo method to estimate the optimal reverse variance of a DPM. The authors show that the optimal KL divergence of DPMs can be approximated by an analytic form of the reverse process, and the authors prove lower and upper bounds on the KL divergence and the log-likelihood of the model. ","This paper proposes a training-free inference framework for diffusion probabilistic models (DPMs). The main contribution of the paper is to provide lower and upper bounds for the optimal reverse variance and optimal KL divergence of DPMs. The lower bounds are based on a Monte Carlo method and a score-based model, while the upper bounds are derived from a score based model. The upper bounds can be used to estimate the log-likelihood of a DPM, and the lower bounds can also be used for the log likelihood of the DPM."
6600,SP:3f935ba5784c3e86db72421426bc479061af1a4b,"vision transformers ( ViTs ) COMPARE CNNs. CNNs COMPARE vision transformers ( ViTs ). transformer - based models USED-FOR medical image classification. CNNs CONJUNCTION transformers. transformers CONJUNCTION CNNs. medical image benchmark datasets CONJUNCTION tasks. tasks CONJUNCTION medical image benchmark datasets. vision transformers COMPARE CNNs. CNNs COMPARE vision transformers. CNNs COMPARE vision transformers. vision transformers COMPARE CNNs. ImageNet EVALUATE-FOR CNNs. Method is Convolutional Neural Networks ( CNNs ). Task are automated medical image diagnosis, classification, detection and segmentation tasks, medical imaging tasks, and supervised and self - supervised setting. Material is natural image domain. ",This paper proposes a vision transformer-based model for medical image classification. The proposed model is based on vision transformers (ViTs) and achieves state-of-the-art performance on ImageNet classification and detection and segmentation tasks in both supervised and self-supervised settings. The experiments show that the proposed model outperforms CNNs on both self- and supervised settings. ,"This paper proposes a method for medical image classification using vision transformers (ViTs) and convolutional neural networks (CNNs). The main idea is to use ViTs to train a transformer-based model that can be applied to a variety of medical imaging tasks. The authors show that ViTs can outperform CNNs on a range of tasks, both supervised and self-supervised settings. They also show that CNNs can also outperform ViTs on ImageNet."
6625,SP:a64e0535f268901e38fd51e027c612ebcdbae1a4,"pretrained NLM COMPARE it. it COMPARE pretrained NLM. NLM training heuristics USED-FOR pretraining and fine - tuning stages. NLM pretraining USED-FOR Natural Language Understanding tasks. sentence representations CONJUNCTION open domain question answering abilities. open domain question answering abilities CONJUNCTION sentence representations. Method are Pretraining Neural Language Models ( NLMs ), neural architecture, pretraining example design, and self - improving representations. OtherScientificTerm is semantically related non - neighboring sentences. ",This paper proposes a method to improve the performance of pretrained neural language models (NLM) on natural language understanding (NLP) tasks. The method is based on the observation that pretrained NLP models tend to overfit to semantically related non-neighboring sentences. The authors show that pretraining a NLP model on semantically similar sentences can improve its performance on NLP tasks. They show that this self-improving self-training strategy can be applied to pretrained NLMs.   ,This paper proposes a method to improve the performance of pre-trained neural language models (NLM) on natural language understanding (NLM) tasks. The method is based on the idea that NLM pretraining should be able to learn semantically related non-neighboring sentences that are semantically similar to neighboring sentences. The authors show that the pretrained NLM is able to achieve better performance on NLM tasks when compared to the state-of-the-art NLM training methods. They also show that their method can also be applied to the task of open-domain question answering.
6650,SP:59066956fa2e423d5f2d2ea4f91c4ddf6afd4683,"L2O models USED-FOR optimization rules. neural networks USED-FOR L2O models. neural networks USED-FOR optimization rules. meta - training USED-FOR numerical rules. optimization rule USED-FOR L2O model. neural networks USED-FOR numerical rules. holistic symbolic representation and analysis framework USED-FOR L2O. L2O model COMPARE human - designed and tuned optimizers. human - designed and tuned optimizers COMPARE L2O model. large - scale problems EVALUATE-FOR L2O model. Task are Learning to Optimize ( L2O ), optimization procedure, and L2O research. Metric are scalability, and interpretability. OtherScientificTerm is memory overhead. Method is symbolic regression. ","This paper proposes Learning to Optimize (L2O), a method for learning to solve large-scale optimization problems. The main idea is to learn a symbolic representation of the problem, which is then used to train a neural network to predict the optimal solution to the problem. The proposed method is evaluated on a variety of optimization problems and compared to a number of state-of-the-art methods.   ","This paper proposes a new symbolic representation and analysis framework for learning to optimize (L2O) models. The proposed method is based on a meta-training approach, where a neural network is trained to learn a set of numerical rules that can be used as a symbolic representation for learning the optimization rules. The authors show that the proposed method outperforms human-designed and tuned optimizers on a variety of large-scale problems. "
6675,SP:54dfeb363beee9959aecc9e0853ff06e43bd94e4,"provable adversarial robustness FEATURE-OF deep neural networks ( DNNs ). provable adversarial robustness USED-FOR static supervised learning tasks. image classification HYPONYM-OF static supervised learning tasks. DNNs USED-FOR real - world adaptive tasks. reinforcement learning ( RL ) HYPONYM-OF real - world adaptive tasks. methods USED-FOR static setting. provable robustness FEATURE-OF RL. RL adversary USED-FOR defense strategy. procedure USED-FOR adaptive RL adversary. worst - case scenario USED-FOR certificates. Pong CONJUNCTION Freeway. Freeway CONJUNCTION Pong. Freeway CONJUNCTION Mountain Car. Mountain Car CONJUNCTION Freeway. Cartpole CONJUNCTION Pong. Pong CONJUNCTION Cartpole. environments EVALUATE-FOR method. robustness guarantees EVALUATE-FOR method. Mountain Car HYPONYM-OF environments. Cartpole HYPONYM-OF environments. Freeway HYPONYM-OF environments. Pong HYPONYM-OF environments. Generic are systems, and attacks. OtherScientificTerm are adversarial attacks, non - adaptive adversary, policy, Neyman - Pearson Lemma, adversarial perturbation, Gaussian noise, policy function, and robustness certificates. Task is randomized smoothing based defenses. Method are smoothingbased certificates, and policy smoothing. ","This paper studies the problem of adversarial robustness in reinforcement learning. The authors propose a method to obtain robustness certificates for policy smoothing based on the Neyman-Pearson Lemma. The key idea is to use the adversarial perturbation of the policy function to generate adversarial certificates for the policy, and then use these certificates as a defense strategy against the adaptive RL adversary.   The authors show that the proposed method is provably robust to adversarial attacks in the worst-case scenario. They also show that their method is robust to random smoothing-based attacks. ","This paper proposes a method to improve the robustness of deep neural networks (DNNs) against adversarial attacks. The key idea is to use the Neyman-Pearson Lemma (Neyman and Pearson Lemma) as a robustness certificate for DNNs. The authors show that the worst-case scenario where the policy is not robust against the adversarial perturbation, and the policy smoothing is used as a defense strategy, can be reduced to the worst case where the smoothing-based certificates are used. They show that their method can be used in a variety of environments, including Pong, Freeway, Cartpole, and Mountain Car.  "
6700,SP:e0f9add5fde18eaab0eeb2b10b14928acc8ec5b8,"labeled source data CONJUNCTION unlabeled target data. unlabeled target data CONJUNCTION labeled source data. methods USED-FOR predicting the target domain accuracy. labeled source data USED-FOR methods. unlabeled target data USED-FOR methods. method USED-FOR threshold. BREEDS CONJUNCTION CIFAR. CIFAR CONJUNCTION BREEDS. ImageNet CONJUNCTION BREEDS. BREEDS CONJUNCTION ImageNet. WILDS CONJUNCTION ImageNet. ImageNet CONJUNCTION WILDS. CIFAR CONJUNCTION MNIST. MNIST CONJUNCTION CIFAR. ATC COMPARE methods. methods COMPARE ATC. synthetic corruptions CONJUNCTION dataset reproduction. dataset reproduction CONJUNCTION synthetic corruptions. ImageNet CONJUNCTION CIFAR. CIFAR CONJUNCTION ImageNet. WILDS CONJUNCTION BREEDS. BREEDS CONJUNCTION WILDS. model architectures EVALUATE-FOR methods. datasets EVALUATE-FOR ATC. dataset reproduction HYPONYM-OF distribution shifts. WILDS HYPONYM-OF datasets. MNIST HYPONYM-OF datasets. CIFAR HYPONYM-OF datasets. BREEDS HYPONYM-OF datasets. ImageNet HYPONYM-OF datasets. ATC COMPARE prior methods. prior methods COMPARE ATC. toy distributions USED-FOR method. Task is Real - world machine learning deployments. Method is Average Thresholded Confidence ( ATC ). OtherScientificTerm are model ’s confidence, and model confidence. Metric is predicting accuracy. Generic are problem, and it. ","This paper proposes a new method to estimate the confidence of a model in predicting the target domain accuracy from unlabeled target data. The proposed method, called Average Thresholded Confidence (ATC), is based on the idea of average thresholded confidence, which is a measure of the difference between the model’s confidence and that of the target model. The authors show that ATC can be used as a threshold to measure the uncertainty of the model on the target data, and that it can be computed by using the average of the confidence between the target and the source data.   The authors conduct extensive experiments on a variety of datasets to demonstrate the effectiveness of the proposed method. ","This paper proposes a new method for predicting the target domain accuracy from unlabeled source data. The main idea is to use the Average Thresholded Confidence (ATC) method, which is based on the idea that the confidence of the model depends on the model’s confidence in the source data, rather than the target data. To this end, the authors propose a new threshold for the model's confidence, which they call the “average thresholded confidence”. The authors show that ATC can be used to predict the accuracy of the target dataset better than the source dataset, and that it can be applied to a variety of model architectures and datasets."
6725,SP:e748bf6ee653087cae825df32a8546f9ccebfcf1,"registration USED-FOR transformation. outliers CONJUNCTION unknown non - rigid deformations. unknown non - rigid deformations CONJUNCTION outliers. outliers FEATURE-OF robustness. partial distribution matching ( PDM ) problem USED-FOR registration problem. method USED-FOR large scale PDM problem. partial Wasserstein-1 ( PW ) discrepancy USED-FOR method. Kantorovich – Rubinstein duality USED-FOR PW discrepancy. partial Wasserstein adversarial network ( PWAN ) USED-FOR PW discrepancy. neural network USED-FOR partial Wasserstein adversarial network ( PWAN ). coherence regularizer USED-FOR non - rigid transformations. coherence regularizer USED-FOR unrealistic deformations. coherence regularizer PART-OF It. PWAN COMPARE state - of - the - art methods. state - of - the - art methods COMPARE PWAN. point set registration tasks EVALUATE-FOR PWAN. point set registration tasks EVALUATE-FOR PWAN. Generic are task, and network. OtherScientificTerm are discrete distributions, and gradient. ",This paper proposes a method for partial distribution matching (PDM) based on partial Wasserstein-1 (PW) discrepancy. The proposed method is based on the Kantorovich-Rubinstein duality (KPJ) and a coherence regularization term. The method is evaluated on point set registration tasks and achieves state-of-the-art performance.,"This paper proposes a new partial Wasserstein-1 (PW) discrepancy-based partial distribution matching (PDM) method for point set registration. The proposed method is based on the Kantorovich-Rubinstein duality (KP2D) problem, which is a generalization of the partial-Wasserstein discrepancy (WSD) problem. The authors show that the proposed method can be applied to the large-scale PDM problem. They also show that their method is more robust to outliers and non-rigid deformations. "
6750,SP:f94f77696d100b2638fa2a6d82c8df47db3b6a36,"Hyperparameter optimization ( HPO ) PART-OF machine learning models. transfer learning USED-FOR HPO. approaches COMPARE Deep Kernel Gaussian Process surrogate. Deep Kernel Gaussian Process surrogate COMPARE approaches. Landmark Meta - features ( DKLM ) PART-OF Deep Kernel Gaussian Process surrogate. DKLM USED-FOR similarity between hyperparameter configurations. DKLM USED-FOR contextualized dataset - specific similarity representations. DKLM USED-FOR hyperparameter configurations. method COMPARE stateof - the - art baselines. stateof - the - art baselines COMPARE method. OpenML FEATURE-OF HPO meta - datasets. HPO meta - datasets EVALUATE-FOR DKLM. Generic is it. OtherScientificTerm are hyperparameter evaluations, and evaluated configurations. ","This paper proposes a novel method for hyperparameter optimization (HPO) in machine learning models. The proposed method is based on the concept of landmark meta-features (DKLM), which is used to measure the similarity between hyperparametrized configurations of different hyperparameters. The authors show that the DKLM can be used as a surrogate for the Deep Kernel Gaussian Process (DKGP) in HPO. The DKGP is a deep neural network that is trained on a large number of datasets and evaluated on a small number of configurations. The method is evaluated on OpenML and HPO meta-datasets.","This paper proposes a new method for hyperparameter optimization (HPO) in the context of transfer learning. The proposed method is based on the Landmark Meta-features (DKLM) framework. The DKLM is used to learn the similarity between hyperparameters between two datasets. The authors show that the DKLM can be used as a surrogate for the Deep Kernel Gaussian Process (DKGP) surrogate for HPO. The method is evaluated on two HPO meta-datasets, OpenML and CIFAR-10, where it outperforms the state of the art."
6775,SP:e3c57f3589e8ab674644d900c14b3473cd71a23f,"Generated data COMPARE real data. real data COMPARE Generated data. technology USED-FOR deep fakes. technology USED-FOR misinformation. 128 - bit fingerprint USED-FOR identifiable models. deep fake detection CONJUNCTION attribution. attribution CONJUNCTION deep fake detection. method USED-FOR deep fake detection. method USED-FOR attribution. fingerprinting mechanism USED-FOR method. Method are deep generative models, deep fake detection methods, and generative models. Generic are work, and technique. OtherScientificTerm is fingerprint. ",This paper proposes a new method for deep fake detection based on deep generative models. The key idea is to use a 128-bit fingerprinting mechanism to identify identifiable models from the generated data. The method is evaluated on synthetic and real-world datasets and compared with other deep fakes methods. ,This paper proposes a new method for deep fake detection based on deep generative models. The key idea is to use a 128-bit fingerprinting mechanism to identify identifiable models from the generated data. The authors show that the proposed method is able to detect deep fakes more accurately than other methods. They also show that their method can also be used for attribution attribution. 
6800,SP:73bffd1a0856b80d29f7a2b2b68be57882531f07,"Post - hoc explanations USED-FOR black box models. Post - hoc explanations PART-OF classification and regression settings. model agnostic local explanations USED-FOR similarity learners. tabular and text data USED-FOR model agnostic local explanations. tabular and text data USED-FOR similarity learners. method USED-FOR feature attributions. analogies USED-FOR machine learning. analogies USED-FOR explanation. explanation PART-OF machine learning. ( latent ) factors USED-FOR model. feature attributions USED-FOR analogies. submodular FEATURE-OF analogy objective function. Generic are models, and approaches. Method are black box similarity learner, and sentence encoder. OtherScientificTerm are similarity, and complementarity. Task is healthcare utilization application. ",This paper proposes a method for learning post-hoc explanations for black-box models. The main idea is to learn a model-agnostic local explanation (i.e. an explanation that is independent of the model parameters) that can be used as a complement to the original model parameters. The method is evaluated on tabular and text data and compared with a number of baselines.  ,"This paper proposes a method for learning post-hoc explanations for black-box models. The main idea is to learn a model-agnostic local explanation for black box models that can be used as a surrogate for model agnostic local explanations. The proposed method is based on the notion of complementarity, which is defined as the similarity between two features (e.g., feature attributions) and a submodular objective function. The method is applied to both tabular and text data, where it is shown that the proposed method can be applied to healthcare utilization applications."
6825,SP:6a3c4ae05d582f8896840483b08c735ced2976bc,"deep neural networks ( DNN ) USED-FOR adversarial examples. empirical and theoretical defense approaches USED-FOR single ML model. robustness EVALUATE-FOR ensemble protocols. certified robustness EVALUATE-FOR ensemble ML models. ensemble models COMPARE single model. single model COMPARE ensemble models. ensemble models COMPARE single model. single model COMPARE ensemble models. certified robustness EVALUATE-FOR ensemble models. diversified gradient CONJUNCTION confidence margin. confidence margin CONJUNCTION diversified gradient. diversified gradient USED-FOR ensemble models. Ensemble - before - Smoothing strategy USED-FOR bounded model - smoothness analysis. ensemble model COMPARE single base model. single base model COMPARE ensemble model. certified robustness EVALUATE-FOR single base model. certified robustness EVALUATE-FOR ensemble model. lightweight Diversity Regularized Training ( DRT ) USED-FOR certifiably robust ensemble ML models. DRT enhanced ensembles COMPARE single and ensemble ML models. single and ensemble ML models COMPARE DRT enhanced ensembles. certified robustness EVALUATE-FOR single and ensemble ML models. ImageNet datasets EVALUATE-FOR certified L2 - robustness. certified robustness EVALUATE-FOR DRT enhanced ensembles. Method is DNNs. OtherScientificTerm are perturbations, and model - smoothness assumption. ",This paper studies the robustness of ensemble models in the presence of adversarial perturbations. The authors show that ensemble models are more robust than single models in terms of certified L2 robustness and certified L1 robustness. The main contribution is a novel ensemble-before-smoothing strategy that improves the certified robustness by using ensemble-based diversity regularized training (DRT) to improve the diversity of the ensemble. ,"This paper studies the robustness of ensemble-based deep neural networks (DNNs) against adversarial perturbations. The authors propose a new ensemble-before-smoothing (E-Smoothing) strategy to improve the certified robustness (certified L2-robustness) of DNNs. They show that the ensemble model is more robust than the single base model, and that ensemble models are more robust to perturbation than single base models. They also show that ensemble model-based robustness is bounded by the bounded model-smoothness analysis.   "
6850,SP:3002b29c27709780238876d8c3f81bbd6a0f8112,"message passing Graph Neural Networks ( GNNs ) USED-FOR learning with graphs. expressive power EVALUATE-FOR higherorder GNNs. strategies CONJUNCTION lower bounds. lower bounds CONJUNCTION strategies. computational cost CONJUNCTION expressive power. expressive power CONJUNCTION computational cost. model USED-FOR subgraphs. recursive pooling COMPARE higher - order GNNs. higher - order GNNs COMPARE recursive pooling. computational complexity EVALUATE-FOR higher - order GNNs. sparsity USED-FOR recursive pooling. computational complexity EVALUATE-FOR recursive pooling. near ) matching information - theoretic lower bound USED-FOR counting subgraphs. graph representations USED-FOR representations of derived ( sub-)graphs. graph representations USED-FOR near ) matching information - theoretic lower bound. lower bounds FEATURE-OF time complexity. Method are recursive pooling technique of local neighborhoods, and low - order GNNs. ",wise pooling is a popular technique in graph neural networks (GNNs) to reduce the computational cost and improve the expressive power of GNNs. This paper studies the problem of pooling subgraphs in a message-passing GNN. The authors propose a new pooling technique of local neighborhoods and show that it is computationally efficient. They also provide a near-matching information-theoretic lower bound on the computational complexity. ,"This paper studies the expressive power of higher-order graph neural networks (GNNs) in the context of message passing. The authors provide a lower-bound on the computational complexity of high-order GNNs. The lower bound is based on the notion of ""near-matching"" (i.e. matching the representations of derived (sub-)graphs). The authors also provide lower bounds on the number of subgraphs that can be represented by a GNN.  "
6875,SP:5d0cbd84336caf5f31e1f98e11f6733230e4d792,Pretrained language models ( LMs ) USED-FOR factual knowledge. external knowledge PART-OF pretrained LMs. knowledge integration ( KI ) methods USED-FOR pretrained LMs. external knowledge PART-OF knowledge integration ( KI ) methods. KI methods COMPARE vanilla LMs. vanilla LMs COMPARE KI methods. integration USED-FOR catastrophic forgetting of already learned knowledge. graph convolution operation USED-FOR KI. probe model USED-FOR knowledge - enhanced LMs. Graph Convolution Simulator ( GCS ) HYPONYM-OF probe model. GCS model USED-FOR KI process. it USED-FOR knowledge - enhanced LMs. K - Adapter CONJUNCTION ERNIE. ERNIE CONJUNCTION K - Adapter. ERNIE HYPONYM-OF knowledge - enhanced LMs. K - Adapter HYPONYM-OF knowledge - enhanced LMs. models USED-FOR factual knowledge. complex relational knowledge USED-FOR ERNIE. K - Adapter USED-FOR simple relational knowledge. K - Adapter USED-FOR time - related knowledge. Generic is methods. Method is LMs. OtherScientificTerm is relations. Material is KI corpus. ,This paper proposes a knowledge integration (KI) method to improve the performance of pre-trained language models (LMs) by incorporating external knowledge. The proposed method is based on a graph convolution operation (GCS) and uses a probe model to train the KI process. The main contribution of the paper is to show that KI methods can improve the generalization performance of LMs.  ,"This paper proposes a new knowledge integration (KI) method for improving the performance of knowledge-enhanced language models. The authors propose a new probe model, GCS, which is a graph convolution-based probe model for KI. The GCS model is trained on a large KI corpus, and the authors show that the proposed method can improve the performance on a variety of KI-based methods. "
6900,SP:7e73948421e98307fceb69a316d8a4e7c4926cda,adaptation ( inner loop ) learning rate USED-FOR fast adaptation. adaptation ( inner loop ) learning rate USED-FOR MAML. adaptation learning rate USED-FOR meta - learning. adaptation learning rate FEATURE-OF mixed linear regression. mixed linear regression USED-FOR meta - learning. optimal adaptation learning rates USED-FOR MAML. optimal adaptation learning rates USED-FOR population risk. population risk FEATURE-OF MAML. empirical risk minimization ( ERM ) COMPARE MAML. MAML COMPARE empirical risk minimization ( ERM ). MAML USED-FOR initialization. average distance FEATURE-OF initialization. Metric is adaptation error. OtherScientificTerm is optimal adaptation learning rate. ,This paper studies the adaptation learning rate for meta-learning in mixed linear regression. The authors show that MAML has an optimal adaptation learning rates for fast adaptation in the inner loop and fast adaptation for the fast adaptation (adaptation (inner loop) learning rate) in the outer loop. They also show that the adaptation error is bounded by the average distance between the initialization and the meta-initialization.   The main contribution of this paper is to provide a theoretical analysis of the convergence rate of MAMRL in mixed-linear regression.,"This paper studies the adaptation learning rate of meta-learning in mixed linear regression (MAML). The authors show that MAML can be viewed as an inner-loop adaptation learning process, where the adaptation (inner-loop) learning rate is the inner loop learning rate. They show that the optimal adaptation learning rates can be computed for the population risk minimization problem. They also provide an empirical study of the performance of the adaptation in terms of empirical risk minimisation.  "
6925,SP:effbc85d89b1197d9c2abcaf5ff13864135dd6e1,"source - domain data USED-FOR adaptation. Source - free domain adaptation ( SFDA ) USED-FOR model. unlabelled data USED-FOR model. labelled data USED-FOR model. methods USED-FOR SFDA. source model USED-FOR feature - space class - separation. entropy - minimization techniques USED-FOR methods. measurement shift HYPONYM-OF domain shift. it USED-FOR features. bottom - up training scheme USED-FOR FR. accuracy CONJUNCTION calibration. calibration CONJUNCTION accuracy. BUFR COMPARE SFDA methods. SFDA methods COMPARE BUFR. calibration CONJUNCTION data efficiency. data efficiency CONJUNCTION calibration. BUFR COMPARE source model. source model COMPARE BUFR. data efficiency EVALUATE-FOR SFDA methods. calibration EVALUATE-FOR SFDA methods. real and synthetic data EVALUATE-FOR SFDA methods. data efficiency EVALUATE-FOR BUFR. calibration EVALUATE-FOR BUFR. accuracy EVALUATE-FOR SFDA methods. real and synthetic data EVALUATE-FOR BUFR. accuracy EVALUATE-FOR BUFR. Task are classification, and model calibration. OtherScientificTerm are measurement system, source features, and approximate feature distribution. Method are feature - extractor, and Feature Restoration ( FR ). Generic is network. ","This paper proposes a new method for source-free domain adaptation (SFDA) based on a bottom-up training scheme to improve the accuracy and calibration of a model trained on unlabeled data. The proposed method is called Feature Restoration (BUFR) and it is based on the idea of feature restoration (FR), which is to restore the features from the source data to the approximate feature distribution of the target domain. Theoretical analysis and empirical results show that the proposed method outperforms existing SFDA methods in terms of accuracy, calibration and data efficiency.","This paper proposes a new method for source-free domain adaptation (SFDA) where the source data is unlabeled and the source model is trained on unlabelled data. The authors propose a bottom-up training scheme to improve the accuracy and calibration of the model. They show that the proposed method outperforms existing methods in terms of accuracy, calibration, and data efficiency."
6950,SP:7d63034ec7e6a4f178681ff2a49feb485cd47116,"distributed learning schema USED-FOR model. Federated learning ( FL ) HYPONYM-OF distributed learning schema. adversarial training ( AT ) USED-FOR centralized learning. learning setting USED-FOR adversarial robustness. adversarial robustness FEATURE-OF non - iid users. batch - normalization statistics USED-FOR propagation approach. method USED-FOR FL remarkable robustness. Material is raw data. Method are FL, and FL techniques. OtherScientificTerm are FL users, highresource users, and low - resource users. Metric is model robustness. Generic is it. Task are FL process, and learning. ",This paper studies the adversarial robustness of federated learning (FL) in the presence of non-iid users. The authors propose a propagation-based adversarial training (PAT) method to improve FL robustness. The proposed method is based on batch-normalization statistics. Theoretical results show that the proposed method can improve the robustness to adversarial perturbations. ,This paper studies the problem of adversarial robustness in federated learning (FL). The authors propose a new metric to measure the robustness of a model to adversarial attacks. The metric is based on batch-normalization statistics. The authors also propose a propagation-based approach to measure robustness. The empirical results show that the proposed metric can be used to improve the performance of FL. 
6975,SP:42c7a79e58b6a9f776fa6ae928bd89c194f9303f,"utility function FEATURE-OF game. utility function USED-FOR Existing methods. transformer - like architecture USED-FOR mapping. transformer - like architecture USED-FOR symmetries. network structure inference EVALUATE-FOR methods. method COMPARE methods. methods COMPARE method. network structure inference EVALUATE-FOR method. network games EVALUATE-FOR method. synthetic and real - world data EVALUATE-FOR method. synthetic and real - world data USED-FOR network games. OtherScientificTerm are Strategic interactions, network structure, equilibrium actions, real - world scenarios, and network structure of the game. Task is economics and social sciences. ","This paper studies the problem of learning the utility function of a game between two players in which the goal is to maximize the mutual information between the two players. The authors propose to use a transformer-like architecture to learn the mapping between the equilibrium utility function and the network structure of the game. The utility function is defined as the sum of a set of symmetries of the two player's actions, and the authors show that this mapping can be expressed as a convex combination of two functions:  (1) a function that maps the equilibrium action to the utility of the other player, and (2) a mapping from the utility to the action of the current player.   The authors then propose a method for learning the mapping from this utility function to the equilibrium function of the network. The proposed method is evaluated on a synthetic and a real-world dataset, and shows that it outperforms the state-of-the-art in terms of utility estimation. ",This paper proposes a method for learning the utility function of a network game. The utility function is defined as the sum of the mutual information between the players' actions in the game and the utility of the network. The authors propose a transformer-like architecture for learning this utility function. They show that their method outperforms existing methods on synthetic and real-world data. 
7000,SP:1c7b9157cf8c06ca771da78895fc3af969b0fb85,"heterogeneous graphs USED-FOR relation prediction. ANalogy SubGraph Embedding Learning ( GraphANGEL ) HYPONYM-OF relation prediction framework. graph pattern USED-FOR logical rule. graph pattern USED-FOR explainable predictive models. inductive bias USED-FOR generalization. heterogeneous graph based recommendation CONJUNCTION knowledge graph completion. knowledge graph completion CONJUNCTION heterogeneous graph based recommendation. model COMPARE models. models COMPARE model. knowledge graph completion EVALUATE-FOR model. heterogeneous graph based recommendation EVALUATE-FOR model. knowledge graph completion EVALUATE-FOR models. heterogeneous graph based recommendation EVALUATE-FOR models. model USED-FOR explainable heat maps of attention scores. Material is knowledge graphs. OtherScientificTerm are embeddings, and subgraphs. Task is transductive setting. ","This paper proposes a method to learn relation embeddings for heterogeneous graphs. The proposed method is based on analogy subgraph embedding learning (ANalogy SubGraph Embedding Learning (GraphANGEL) framework, which learns to model the relation prediction as a logical rule on a graph pattern. The method is evaluated on heterogeneous graph based recommendation and knowledge graph completion tasks.  ","This paper proposes GraphANGEL, a framework for understanding the relationship between two graphs. The authors propose a novel approach to understand the relationship of two graphs, which is based on analogy subgraph embedding learning (ANalogy SubGraph Embedding Learning (GraphANGEL) framework). The main idea is to learn a graph pattern that can be used to explainable predictive models. The proposed method is evaluated on heterogeneous graph based recommendation and knowledge graph completion tasks. "
7025,SP:26ed25a7b42da2cf11b76a727102d8aa36d76657,"well - labeled datasets CONJUNCTION rare abnormal samples. rare abnormal samples CONJUNCTION well - labeled datasets. Few - shot learning HYPONYM-OF natural images. cross - domain tasks USED-FOR real clinics problems. contrastive learning ( CL ) USED-FOR few - shot system. label - efficient learning CONJUNCTION generalizability. generalizability CONJUNCTION label - efficient learning. contrastive learning ( CL ) CONJUNCTION latent augmentation ( LA ). latent augmentation ( LA ) CONJUNCTION contrastive learning ( CL ). latent augmentation ( LA ) USED-FOR few - shot system. LA USED-FOR semantic variations. CL COMPARE LA. LA COMPARE CL. components USED-FOR label - hungry problems. unlabeled training data USED-FOR components. LA COMPARE baselines. baselines COMPARE LA. supervised learning USED-FOR histology images. models COMPARE supervised learning. supervised learning COMPARE models. CL COMPARE supervised learning. supervised learning COMPARE CL. CL USED-FOR models. ImageNet - like images USED-FOR self - supervised learning. CL COMPARE supervised learning. supervised learning COMPARE CL. generalization EVALUATE-FOR data. generalization EVALUATE-FOR CL. generalization EVALUATE-FOR supervised learning. representation learning CONJUNCTION histological image analysis. histological image analysis CONJUNCTION representation learning. model USED-FOR representation learning. model USED-FOR histological image analysis. Task is few - shot learning in histology images. OtherScientificTerm is manual labels. Material are images, and Histology images. ","This paper proposes a few-shot learning method that combines contrastive learning (CL) and latent augmentation (LA) to improve the label-efficient learning and generalizability. The proposed method is evaluated on histology images, where it is shown that the proposed method outperforms supervised learning and self-supervised learning on ImageNet-like images. The experiments show that the method is able to learn from unlabeled training data.",This paper proposes a new few-shot learning method for few shot learning in histology images. The main idea is to combine contrastive learning (CL) and latent augmentation (LA) to improve the few-shots learning. The authors show that CL and LA can be used together to improve few shot performance. They also show that the proposed method can be applied to self-supervised learning.   
7050,SP:badbe687258cd5c282ca167b1f6fbfc6b5400dbf,"Recurrent neural networks ( RNNs ) USED-FOR irregularly - sampled time series. continuous - time hidden states USED-FOR Recurrent neural networks ( RNNs ). training FEATURE-OF gradient. memory compartment FEATURE-OF continuous - time networks. continuous - time dynamical flow PART-OF RNN. constant error propagation FEATURE-OF memory path. Mixed - MemoryRNNs COMPARE RNN - based counterparts. RNN - based counterparts COMPARE Mixed - MemoryRNNs. long - term dependencies FEATURE-OF non - uniformly sampled data. non - uniformly sampled data EVALUATE-FOR RNN - based counterparts. non - uniformly sampled data EVALUATE-FOR Mixed - MemoryRNNs. Generic are models, and it. Method are RNNs, ODE solver, and Mixed - Memory - RNNs ( mmRNNs ). OtherScientificTerm are hidden state, timecontinuous state, and time - lags. ","This paper proposes a new recurrent neural network architecture, called Mixed-Memory RNNs (mmRNNs), which uses a continuous-time dynamical flow in the memory compartment of the RNN to model the time-continuous state of the time series. Theoretical analysis is provided to show that the proposed architecture can be used to model long-term dependencies in time series with irregularly sampled data. Experiments on synthetic and real-world data demonstrate the effectiveness of the proposed method. ","This paper studies the problem of learning continuous time-continuous neural networks (RNNs) from irregularly-sampled time series. The authors propose a new RNN architecture called Mixed-Memory-RNN (mmRNN) that can learn continuous-time dynamical flow (i.e., time-lags) in the memory compartment of RNNs. They show that mmRNN outperforms the state-of-the-art RNN-based counterparts on non-uniformly sampled data. The main contribution of the paper is a new ODE solver to solve the problem."
7075,SP:4efd22f9122fa5856a9f4302eb6875fa0c414912,pre - trained BERT USED-FOR Natural Language Processing ( NLP ) tasks. 1 - bit parameters CONJUNCTION bitwise operations. bitwise operations CONJUNCTION 1 - bit parameters. binarization HYPONYM-OF compression approaches. 1 - bit parameters USED-FOR binarization. computation and memory consumption EVALUATE-FOR binarization. bitwise operations USED-FOR binarization. 1 - bit weight CONJUNCTION embedding. embedding CONJUNCTION 1 - bit weight. embedding CONJUNCTION activation. activation CONJUNCTION embedding. activation HYPONYM-OF BERT. 1 - bit weight HYPONYM-OF BERT. embedding HYPONYM-OF BERT. information degradation CONJUNCTION optimization direction mismatch. optimization direction mismatch CONJUNCTION information degradation. BiBERT USED-FOR performance bottlenecks. BiBERT HYPONYM-OF fully binarized BERT. optimization direction mismatch FEATURE-OF forward and backward propagation. DirectionMatching Distillation ( DMD ) scheme USED-FOR full binarized BERT. Bi - Attention structure USED-FOR representation information statistically. Bi - Attention structure CONJUNCTION DirectionMatching Distillation ( DMD ) scheme. DirectionMatching Distillation ( DMD ) scheme CONJUNCTION Bi - Attention structure. DirectionMatching Distillation ( DMD ) scheme USED-FOR BiBERT. Bi - Attention structure USED-FOR BiBERT. BiBERT COMPARE quantized BERTs. quantized BERTs COMPARE BiBERT. BiBERT COMPARE baseline. baseline COMPARE BiBERT. baseline COMPARE quantized BERTs. quantized BERTs COMPARE baseline. ultra - low bit activations FEATURE-OF quantized BERTs. NLP benchmark EVALUATE-FOR quantized BERTs. NLP benchmark EVALUATE-FOR BiBERT. FLOPs CONJUNCTION model size. model size CONJUNCTION FLOPs. fully binarized BERT model USED-FOR real - world resource - constrained scenarios. method USED-FOR fully binarized BERT model. fully binarized BERT EVALUATE-FOR method. model size EVALUATE-FOR method,"This paper proposes a novel method to reduce the computational cost of training BERT with 1-bit parameters. The proposed method, called BiBERT, is based on a bi-attention structure that aggregates information from the forward and backward propagation of BERT representations. The authors show that the proposed method can reduce the number of FLOPs and model size in terms of training time and memory usage. The experiments show the effectiveness of the proposed methods.","This paper proposes BiBERT, a new method for fully binarized BERT model. The main idea is to use the Bi-Attention structure of BERT to improve the performance of binarization. The proposed method is based on the DirectionMatching Distillation (DMD) scheme. The authors show that the proposed method can reduce the number of FLOPs, model size, and model size. They also show that their method can be applied to real-world resource constrained scenarios."
7100,SP:619bd742e92bea6241852f5a9d2b7bacf13b393a,"method USED-FOR keypoint detection. method USED-FOR instance association. keypoint detection CONJUNCTION instance association. instance association CONJUNCTION keypoint detection. Transformer USED-FOR method. Transformer USED-FOR problems. association information USED-FOR keypoints grouping. self - attention PART-OF Transformer. supervising self - attention USED-FOR multi - person keypoint detection. supervising self - attention USED-FOR instance association. approach USED-FOR supervising self - attention. multi - person keypoint detection CONJUNCTION instance association. instance association CONJUNCTION multi - person keypoint detection. approach USED-FOR instance association. approach USED-FOR multi - person keypoint detection. instance masks USED-FOR self - attention. pre - defined offset vector fields CONJUNCTION embedding. embedding CONJUNCTION pre - defined offset vector fields. embedding CONJUNCTION CNN - based bottom - up models. CNN - based bottom - up models CONJUNCTION embedding. supervised attention matrix USED-FOR instance segmentation. person instance segmentation task EVALUATE-FOR method. COCO multi - person keypoint detection challenge EVALUATE-FOR method. COCO multi - person keypoint detection challenge CONJUNCTION person instance segmentation task. person instance segmentation task CONJUNCTION COCO multi - person keypoint detection challenge. OtherScientificTerm are associative information, naive attention patterns, pairwise attention scores, and self - attention behavior. Method is pixel assignment pipeline. ",This paper proposes a self-attention based method for instance segmentation in keypoint detection and instance association tasks. The proposed method is based on a Transformer-based model with a supervised attention matrix. The method is evaluated on the COCO multi-person keypoints detection task and the instance association task.   ,This paper proposes a Transformer-based approach for keypoint detection and instance segmentation. The key idea is to use the self-attention of the Transformer as a supervised attention matrix to supervise the pairwise attention behavior. The proposed method is evaluated on COCO multi-person keypoint and person-instance segmentation tasks. 
7125,SP:14750819593136fc9ef4efd032ab6f94dc5f6a02,"reinforcement learning ( RL ) USED-FOR sequential decision making. Pareto efficiency FEATURE-OF MV - efficient policies. Generic are existing methods, approach, it, and methods. OtherScientificTerm are variance term, MV trade - off, expected quadratic utility function, Pareto efficient policy, computational difficulties, and gradient estimation of the variance. ","This paper studies the problem of learning a Pareto efficient policy in sequential decision-making. The authors propose a method to learn a policy that is MV-efficient, i.e. the policy is MV efficient with respect to the expected quadratic utility function. The main contribution of the paper is to show that the variance term of the expected utility function can be used to estimate the variance of the policy, which is then used to compute the gradient estimation of the variance. The method is shown to be computationally efficient.","This paper studies the problem of Pareto efficiency in reinforcement learning (RL) for sequential decision making in the context of multi-agent learning (MV). In particular, the authors consider the case where the MV trade-off is the expected quadratic utility function (e.g., the expected utility function of the agent) and the variance term of the utility function is the variance of the policy. The authors show that the variance can be computed by using gradient estimation of the variance. They also show that it is possible to compute the variance by using the gradient estimation. They show that this can be done by computing the variance in a way that is computationally efficient.  "
7150,SP:f675b564b3a9c8626ce7944d752fa3e0d868428e,"end - to - end learning USED-FOR communication system. autoencoder USED-FOR communication system. autoencoder USED-FOR end - to - end learning. test - time domain adaptation USED-FOR autoencoder system. fully - trained channel model CONJUNCTION autoencoder. autoencoder CONJUNCTION fully - trained channel model. error rate EVALUATE-FOR autoencoder. method USED-FOR autoencoder. feature transformations USED-FOR channel distribution. feature transformations USED-FOR decoder. feature transformations USED-FOR method. method USED-FOR MDN channel. simulated datasets CONJUNCTION real mmWave wireless channels. real mmWave wireless channels CONJUNCTION simulated datasets. error rate EVALUATE-FOR autoencoder. real mmWave wireless channels EVALUATE-FOR method. simulated datasets EVALUATE-FOR method. method USED-FOR autoencoder. error rate EVALUATE-FOR method. Generic is approach. Method are mixture density network ( MDN ), encoder and decoder neural networks, and MDN channel model. Material is unlabeled data. OtherScientificTerm are wireless link, and source distribution. ","This paper proposes a method for end-to-end learning in wireless communication systems. The method is based on a mixture density network (MDN) architecture, where the encoder and decoder neural networks are trained with a mixture of features from the source distribution and the distribution from the target distribution. The proposed method is evaluated on simulated and real-world wireless channels.  ","This paper proposes a method for end-to-end learning of a wireless communication system with a fully-trained channel model and an autoencoder. The method is based on a mixture density network (MDN) model, which is trained on unlabeled data. The authors show that the proposed method can achieve better error rates than the state-of-the-art in terms of test-time domain adaptation. The proposed method is evaluated on simulated and real-world datasets."
7175,SP:77dc92137ea490d3e1b4b8ee1630dbe2ee0bddfa,"joint softmax focal loss HYPONYM-OF structural loss. model USED-FOR αNLI. ACC CONJUNCTION AUC. AUC CONJUNCTION ACC. AUC EVALUATE-FOR IMSL. RoBERTa - large pretrained model EVALUATE-FOR IMSL. ACC EVALUATE-FOR IMSL. Task are abductive natural language inference task ( αNLI ), and αNLI task. Method are inference network, interactive language model, and Interactive Model with Structural Loss ( IMSL ). OtherScientificTerm is reasoning abilities. ","This paper proposes an interactive model with structural loss (IMSL) for abductive natural language inference task (alphaNLI). The proposed IMSL is based on the joint softmax focal loss (JFL) loss, which is an extension of the focal loss used in previous work on abductive language inference tasks. The main contribution of the paper is the introduction of a new loss function that is designed to improve the performance of the inference network on the αNLI task. The proposed method is evaluated on the RoBERTa-large pretrained model and achieves better ACC and AUC performance compared to previous work.","This paper proposes an interactive model with structural loss (IMSL) for abductive natural language inference task (αNLI). IMSL is based on the joint softmax focal loss (JFL) loss, which is a structural loss that is used in previous works. The authors show that the proposed model can outperform the state-of-the-art in terms of ACC and AUC on the RoBERTa-large model."
7200,SP:17cd72df5fc19398f582d27516fd742b073f79e3,"machine learning USED-FOR safety - critical systems. assessment of uncertainy USED-FOR machine learning. deep neural networks USED-FOR overconfident predictions. certifiable OOD detector CONJUNCTION classifier. classifier CONJUNCTION certifiable OOD detector. classifier PART-OF OOD aware classifier. OOD aware classifier PART-OF method. classifier PART-OF method. certifiable OOD detector PART-OF method. prediction accuracy CONJUNCTION detection. detection CONJUNCTION prediction accuracy. detection performance EVALUATE-FOR non - manipulated OOD data. classifier USED-FOR asymptotic overconfidence problem. classifier COMPARE neural networks. neural networks COMPARE classifier. asymptotic overconfidence problem FEATURE-OF neural networks. Material is OOD data. Task is certifiably adversarially robust OOD detection. OtherScientificTerm are OOD samples, and in - distribution. ","This paper proposes a method for adversarially robust out-of-distribution (OOD) detection. The proposed method is based on a classifier that is trained to detect OOD samples and a detector that detects OOD data. The classifier is trained in an adversarial fashion, where the OOD detection is performed in a way that is consistent with the classifier's predictions. The method is evaluated on synthetic and real-world datasets. ","This paper proposes a method for adversarially robust out-of-distribution (OOD) detection. The main idea is to use an OOD detector and a classifier to detect OOD samples. The classifier is trained to be OOD aware, and the detector is used to detect the OOD data. The proposed method is evaluated on a variety of datasets, and compared with other methods."
7225,SP:9c3756f13932236aff3e8104f4fa193dcc8fde2f,"Deep Neural Networks ( DNNs ) USED-FOR transfer attacks. query - free black - box setting FEATURE-OF transfer attacks. white - box surrogate models CONJUNCTION black - box victim models. black - box victim models CONJUNCTION white - box surrogate models. datasets USED-FOR surrogate models. method USED-FOR classification information. Image Classification Eraser ( ICE ) USED-FOR classification information. Image Classification Eraser ( ICE ) HYPONYM-OF method. Cifar-10 CONJUNCTION Cifar-100. Cifar-100 CONJUNCTION Cifar-10. Cifar-100 CONJUNCTION TieredImageNet. TieredImageNet CONJUNCTION Cifar-100. ICE USED-FOR GTA problem. Cifar-10 EVALUATE-FOR ICE. TieredImageNet EVALUATE-FOR ICE. transfer attack methods USED-FOR GTA problem. transfer attack methods COMPARE ICE. ICE COMPARE transfer attack methods. Task are transfer attack, and Generalized Transferable Attack ( GTA ) problem. Generic are dataset, and them. Method is victim model. ","This paper proposes Image Classification Eraser (ICE), a method to remove classification information from black-box surrogate models in the Generalized Transferable Attack (GTA) problem. The proposed method is based on Image classification eraser to remove the classification information of surrogate models. The method is evaluated on Cifar-10 and TieredImageNet and achieves state-of-the-art performance. ","This paper proposes Image Classification Eraser (ICE), a new method for transfer attacks against black-box models. The authors propose a method that erases the classification information from the victim model and the white-box surrogate model. They show that the proposed method can be applied to the Generalized Transferable Attack (GTA) problem. The method is evaluated on Cifar-10 and TieredImageNet datasets, and it is shown to outperform other transfer attack methods."
7250,SP:2e0447c741a3f09be1095633d870200355211260,discriminative PrLM USED-FOR contextualized representation. robustness EVALUATE-FOR PrLMs. pre - training methods USED-FOR false negative predictions. pre - training methods USED-FOR pre - training language models. false negative issue FEATURE-OF discriminative PrLMs. false negative predictions FEATURE-OF gradient updates. Generic is model. Material is GLUE and SQuAD benchmarks. ," pre-trained on GLUE and SQuAD shows improved robustness in terms of false negative predictions. However, the authors show that the robustness is not achieved by using discriminative pre-training methods. Instead, they propose to use a pre-train language model to improve the contextualized representation.   ",This paper studies the robustness of discriminative pre-training language models against false negative predictions in the context of contextualized representation learning. The authors study the problem of the false negative issue in the setting of pre-trained language models. They show that the true negative issue can be caused by the use of gradient updates in the training process. They also provide a theoretical analysis of the problem. 
7275,SP:281bc59d639aa76d84921b3ec4ce1ee8f1ba5b51,"semi - supervised learning USED-FOR real - world settings. ORCA HYPONYM-OF end - to - end approach. uncertainty adaptive margin USED-FOR ORCA. discriminability FEATURE-OF model. discriminability EVALUATE-FOR ORCA. image classification datasets CONJUNCTION single - cell dataset. single - cell dataset CONJUNCTION image classification datasets. single - cell dataset EVALUATE-FOR ORCA. ORCA COMPARE baselines. baselines COMPARE ORCA. seen CONJUNCTION novel classes. novel classes CONJUNCTION seen. image classification datasets EVALUATE-FOR ORCA. image classification datasets EVALUATE-FOR baselines. single - cell dataset EVALUATE-FOR baselines. ORCA USED-FOR novel classes. novel classes PART-OF ImageNet dataset. seen EVALUATE-FOR ORCA. ImageNet dataset EVALUATE-FOR ORCA. Material are unlabeled test data, labeled training data, open - world semi - supervised learning setting, and labeled and unlabeled data. Generic is assumption. Task is class distribution mismatch problem. OtherScientificTerm is prior knowledge. ","This paper proposes an end-to-end semi-supervised learning method for open-world semi supervised learning. The proposed method is based on the observation that the discriminability of the model is affected by the class distribution mismatch problem. To address this problem, the authors propose a novel uncertainty adaptive margin (ORCA) method to improve discriminable performance. The method is evaluated on image classification datasets and single-cell dataset.  ","This paper proposes an end-to-end semi-supervised learning approach for open-world semi supervised learning with unlabeled test data and labeled training data. The main contribution of the paper is a novel uncertainty adaptive margin (i.e., an uncertainty-adaptive margin) to improve the discriminability of the model. The authors show that the proposed method outperforms baselines on ImageNet dataset and single-cell dataset. "
7300,SP:6c572c4c21b01a0cf3fd9ef97fbb348ef4e405ae,"SLIM - QN HYPONYM-OF light stochastic quasi - Newton optimizer. second - order methods USED-FOR large - scale DNNs. SLIM - QN USED-FOR second - order methods. L - BFGS HYPONYM-OF stochastic training. BFGS update rule USED-FOR Hessian inverse. gradients USED-FOR BFGS update rule. BFGS update rule USED-FOR SLIM - QN. momentum CONJUNCTION adaptive damping mechanism. adaptive damping mechanism CONJUNCTION momentum. momentum USED-FOR Hessian updates. adaptive damping mechanism USED-FOR SLIM - QN. momentum USED-FOR SLIM - QN. SLIM - QN USED-FOR stable convergence. SLIM - QN USED-FOR stochastic setting. convergence EVALUATE-FOR SLIM - QN. SLIM - QN COMPARE second - order methods. second - order methods COMPARE SLIM - QN. compute and memory overhead EVALUATE-FOR second - order methods. compute and memory overhead EVALUATE-FOR SLIM - QN. SLIM - QN COMPARE SGD. SGD COMPARE SLIM - QN. large datasets EVALUATE-FOR SLIM - QN. near optimal accuracy EVALUATE-FOR SGD. wall - clock time EVALUATE-FOR SGD. near optimal accuracy EVALUATE-FOR SLIM - QN. ImageNet HYPONYM-OF large datasets. compute resources USED-FOR SGD. compute resources USED-FOR SLIM - QN. SLIM - QN USED-FOR non - convolutional architectures. Transformers HYPONYM-OF non - convolutional architectures. Metric is computational cost. OtherScientificTerm are Hessian matrix, KFAC, and convergence instability. ","This paper proposes a light-stochastic quasi-Newton optimizer (SLIM-QN) for training large-scale neural networks. The main idea is to use momentum to update the Hessian matrix of the loss function, which is then used as the update rule for the BFGS update rule. The authors show that the proposed method is able to achieve near optimal accuracy on ImageNet with less computational cost compared to SGD.  ","This paper proposes a light-stochastic quasi-Newton optimizer (SLIM-QN) for second-order stochastic training of large-scale DNNs. The main contribution of the paper is to propose a method that can be applied to the Hessian inverse (BFGS) update rule of L-BFGS, which is a second order method. The authors show that the proposed method can be used to improve the convergence of second order methods on large datasets.   "
7325,SP:4bffce00ebb02d2e676eec897647ac14c3344deb,"Graph Neural Networks ( GNNs ) USED-FOR graph - related tasks. GNNs USED-FOR problems. redundant components PART-OF large graphs. pre - processing step USED-FOR graph. node or edge removals USED-FOR inference. LocalitySensitive Pruning ( LSP ) USED-FOR graph pruning. systematic method USED-FOR graph pruning. Locality - Sensitive Hashing USED-FOR LocalitySensitive Pruning ( LSP ). Locality - Sensitive Hashing USED-FOR systematic method. pruning COMPARE pruning strategies. pruning strategies COMPARE pruning. locality properties USED-FOR pruning. local graph properties USED-FOR pruning. synthetic and real - world datasets EVALUATE-FOR LSP. LSP USED-FOR edges. edges PART-OF large graphs. LSP USED-FOR large graphs. Task is real - world problems. OtherScientificTerm are real - world graphs, and sparsified graph. Method is GNNs layers. ","This paper proposes a method for graph pruning based on locality sensitive hashing (LSP) to prune redundant nodes and edges in large graphs. LSP is based on the idea of locality-sensitive hashing, which is a technique used in graph neural networks (GNNs). Theoretical and empirical results show that LSP outperforms existing pruning methods in terms of accuracy and computational cost. ","This paper proposes a systematic method for graph pruning based on locality sensitive pruning (LSP). LSP prunes nodes and edges in a sparsified graph by using a pre-processing step that prunes the node or edge removals. The method is based on the idea of locality sensitive hashing (LHS), which prunes edges that are sensitive to local properties of the graph. The authors show that LHS can be applied to both synthetic and real-world datasets, and it can be used to prune large graphs."
7350,SP:c5e024f4e2079586298519ca868630efd7579eca,"Data augmentation USED-FOR contrastive self - supervised learning. identitydistinctive information FEATURE-OF R(x ). it PART-OF R(x ). VAE ’s bottleneck space FEATURE-OF G(x ). identity - disentangled adversarial augmentation ( IDAA ) USED-FOR self - supervised learning methods. benchmark datasets EVALUATE-FOR IDAA. efficiency CONJUNCTION generalization performance. generalization performance CONJUNCTION efficiency. efficiency EVALUATE-FOR IDAA. generalization performance EVALUATE-FOR IDAA. dataset USED-FOR IDAA. Generic is augmentations. Task is ineffective learning. Method are adversarial augmentation method, and contrastive learning. OtherScientificTerm are hard positives / negatives, variational auto - encoder ( VAE ) reconstruction, VAE objective, augmentation, and sample identity. ","This paper proposes IDAA, a self-supervised data augmentation method for contrastive learning. IDAA is motivated by the observation that data augmentations can lead to ineffective learning. The authors show that IDAA can improve the efficiency of contrastive self-learning in the presence of hard positives/negative samples. The proposed IDAA improves the efficiency and generalization performance of the proposed method.   ",This paper proposes an identity-disentangled adversarial augmentation (IDAA) method for contrastive self-supervised learning. IDAA is a data augmentation method that uses the identity-distinctive information of the data to improve the performance of contrastive learning. The key idea is to use the VAE’s bottleneck space of G(x) as the objective of IDAA. The authors show that IDAA can improve the efficiency of contrastively supervised learning and generalization performance. 
7375,SP:0991bc5f213bd8ab7572e2fed309e1b57a35835b,"tests USED-FOR distribution shifts. these USED-FOR arbitrary shifts. non - sequential methods USED-FOR these. method USED-FOR harmful shifts. accuracy CONJUNCTION calibration. calibration CONJUNCTION accuracy. sequential tools USED-FOR risk function of interest. calibration HYPONYM-OF risk function of interest. accuracy HYPONYM-OF risk function of interest. aggregation of statistical evidence USED-FOR tracking process. constructing time - uniform confidence sequences USED-FOR aggregation of statistical evidence. simulated and real datasets EVALUATE-FOR framework. Method is machine learning models. OtherScientificTerm are data distribution, and benign shifts. Generic is model. Metric is false alarm rate. ",This paper proposes a method to detect distribution shifts in the training data. The method is based on the aggregation of statistical evidence from the tracking process. The authors propose to construct time-uniform confidence sequences to aggregate statistical evidence for tracking distribution shifts. The proposed method is evaluated on simulated and real-world datasets.,This paper proposes a method to detect harmful shifts in the data distribution. The main idea is to use time-uniform confidence sequences (TUCS) to track changes in the distribution. TUCS is based on the idea of aggregation of statistical evidence. The authors show that the proposed method is able to identify harmful shifts more quickly than other non-sequential methods. They also show that their method is more accurate than other methods.
7400,SP:1c7b954273e3a9cda333385b15a3e8ed3bf8178a,Neural networks USED-FOR dynamics of diverse physical systems. neural implicit representations USED-FOR appearance modeling. neural implicit representations CONJUNCTION neural ordinary differential equations ( ODEs ). neural ordinary differential equations ( ODEs ) CONJUNCTION neural implicit representations. neural ordinary differential equations ( ODEs ) USED-FOR interpretable physical models. visual observations USED-FOR interpretable physical models. neural implicit representations USED-FOR processing of high - resolution videos. neural implicit representations USED-FOR synthesis of photo - realistic imagery. processing of high - resolution videos CONJUNCTION synthesis of photo - realistic imagery. synthesis of photo - realistic imagery CONJUNCTION processing of high - resolution videos. model COMPARE approaches. approaches COMPARE model. model USED-FOR physical parameters. large training datasets USED-FOR approaches. embedded neural ODE USED-FOR identification of interpretable physical parameters. known parametric form FEATURE-OF embedded neural ODE. scenes USED-FOR photo - realistic rendering. physical parameters USED-FOR scenes. physical parameters USED-FOR photo - realistic rendering. method USED-FOR physical parameters. real - world videos USED-FOR method. pendulum motion HYPONYM-OF real - world videos. real - world videos USED-FOR physical parameters. physical parameters USED-FOR reconstruction. model USED-FOR metric length of the pendulum. monocular video USED-FOR model. monocular video USED-FOR metric length of the pendulum. Generic is they. Material is high - resolution videos. Task is long - term prediction in state space. OtherScientificTerm is state space. Metric is relative error. ,This paper proposes a method for learning the dynamics of physical systems from high-resolution videos. The method is based on neural ODEs and uses a neural implicit representation to model the appearance modeling. The model is trained using monocular videos and is able to predict the long-term prediction in state space. The proposed method is evaluated on pendulum motion from real-world videos.  ,"This paper proposes a method for generating high-resolution videos from a set of high-res images. The method is based on a neural ODE model that is trained on a large dataset of high resolution videos. The model is trained to predict the physical parameters of the scene from the high resolution video. It is shown that the model is able to identify the true physical parameters in the video, and that it can predict the true parameters of a real-world pendulum motion.  "
7425,SP:51efd1451343f4994d857daa5490e299b812bc2d,"abrupt ( discontinuous ) context changes CONJUNCTION Markovian context evolution. Markovian context evolution CONJUNCTION abrupt ( discontinuous ) context changes. Bayesian approach CONJUNCTION variational inference. variational inference CONJUNCTION Bayesian approach. Bayesian approach USED-FOR it. variational inference USED-FOR it. sticky Hierarchical Dirichlet Process ( HDP ) prior USED-FOR model learning. sticky Hierarchical Dirichlet Process ( HDP ) prior USED-FOR Markov process modeling. context distillation procedure USED-FOR spurious contexts. optimal policy USED-FOR policy learning. RL algorithms USED-FOR policy learning. state - of - the - art methods USED-FOR frameworks. Generic are case, components, and approach. OtherScientificTerm are context cardinality assumption, and drone. ",This paper proposes a context distillation-based framework for learning policies in the presence of discontinuous contexts. The framework is based on the notion of context cardinality and is able to handle discontinuities and Markovian context evolution in the context space. The main contributions are:  1. The authors propose to use a sticky Hierarchical Dirichlet Process (HDP) prior to model the Markov process.  2. They show that this prior can be used to learn the optimal policy by distilling spurious contexts.  3. They demonstrate that the proposed method outperforms the state-of-the-art methods in terms of policy learning performance. ,This paper proposes a new framework for model learning for Markov process modeling. The key idea is to use the sticky Hierarchical Dirichlet Process (HDP) prior to model the dynamics of the Markov processes. The authors propose a context distillation procedure to remove spurious contexts from the context cardinality assumption. They show that this distillation can be used to find the optimal policy for the given context. They also show that it can be applied to a variety of RL algorithms.
7450,SP:ea167b126212b2092bc1190d7f8376bf7c54a888,Knowledge enriched language representation learning USED-FOR knowledge - intensive NLP tasks. monolingual knowledge graph data USED-FOR knowledge based language models. framework USED-FOR knowledge based multilingual language models ( KMLMs ). pretraining tasks USED-FOR knowledge learning. language models USED-FOR logical patterns. intraand inter - sentence structures USED-FOR pretraining tasks. intraand inter - sentence structures FEATURE-OF data. language models USED-FOR factual knowledge. factual knowledge retrieval CONJUNCTION relation classification. relation classification CONJUNCTION factual knowledge retrieval. named entity recognition CONJUNCTION factual knowledge retrieval. factual knowledge retrieval CONJUNCTION named entity recognition. pretrained KMLMs USED-FOR knowledge - intensive cross - lingual NLP tasks. relation classification CONJUNCTION task. task CONJUNCTION relation classification. task HYPONYM-OF knowledge - intensive cross - lingual NLP tasks. logic reasoning HYPONYM-OF task. named entity recognition HYPONYM-OF knowledge - intensive cross - lingual NLP tasks. relation classification HYPONYM-OF knowledge - intensive cross - lingual NLP tasks. factual knowledge retrieval HYPONYM-OF knowledge - intensive cross - lingual NLP tasks. Material is Wikidata knowledge graphs. Method is pretrained language models. ,"This paper proposes a framework to train knowledge-based multilingual language models (KMLMs) on knowledge-intensive cross-lingual NLP tasks. The proposed framework is based on the idea that knowledge-rich language representation learning can be used to improve the performance of knowledge-driven language models. To this end, the authors propose to use knowledge-enhanced language representations learned from monolingual knowledge graph data as input to pretrained language models to learn logical patterns. The authors conduct experiments on knowledge retrieval, relation classification, named entity recognition, and logic reasoning tasks.","This paper proposes a framework for knowledge-based multilingual language models (KMLMs) that can be applied to knowledge-intensive cross-lingual NLP tasks. The proposed framework is based on the idea that KMLMs can be used to learn the logical patterns of a knowledge graph, which can then be used as a pre-trained language model for knowledge learning. The framework is applied to a variety of tasks, such as factual knowledge retrieval, relation classification, named entity recognition, and logical reasoning. "
7475,SP:6c11cf29c90f923346372ba6f11452c36e69ad6d,"agents USED-FOR altruistic behaviour. task - agnostic manner USED-FOR altruistic behaviour. multi - agent environments EVALUATE-FOR approach. unsupervised agents COMPARE them. them COMPARE unsupervised agents. Method are artificial agents, reinforcement learning agents, altruistic agent, and human agents. OtherScientificTerm are external supervision, and altruistic agent ’s behaviour. Generic is concept. ","This paper proposes a new concept of ""altruism"" in reinforcement learning, which is a generalization of the idea of ""self-supervised learning"" (i.e. self-supervision) in which an agent is trained to learn to behave in an altruistic manner in a task-agnostic manner. The authors show that this new concept can be used to motivate agents to act in a self-interested manner in multi-agent environments. They show that the proposed concept is applicable to reinforcement learning agents that are trained in an unsupervised manner. They also show that it is possible to train an agent in such a way that it behaves in a manner similar to a human agent. ","This paper proposes a new concept of “self-supervised altruism”, where an agent’s self-supervision is defined as the difference between its own behaviour and that of another agent in a multi-agent environment. The authors show that this concept can be applied to the task-agnostic setting of reinforcement learning, and that it can be used as a tool to improve the performance of unsupervised agents. They show that their approach can improve the overall performance of an agent in the multi-agents environment. They also show that the proposed method can outperform the state of the art."
7500,SP:5dbc54201ba184266c5054f0d2944bd197bc307a,"Neural Tangent Kernel USED-FOR neural networks. finite - width neural networks USED-FOR double descent. interpolation threshold FEATURE-OF double descent behaviour. optimum FEATURE-OF Hessian. loss function USED-FOR double descent. neural networks CONJUNCTION Hessian spectra. Hessian spectra CONJUNCTION neural networks. OtherScientificTerm are Double descent, and population loss. Generic is models. Method are linear and kernel regression models, influence functions, and parametric model. ","This paper studies the double descent in linear and kernel regression models with influence functions. The authors show that under certain assumptions on the model parameters, the Hessian of the regression model converges to the optimum at a constant rate. They show that this is the case for finite-width neural networks and show that it is a special case of the Neural Tangent Kernel (NTK) model.   ","This paper studies the double descent of neural networks with finite-width neural networks in the context of linear and kernel regression models. The authors show that under certain assumptions on the Hessian spectra of the neural network and the influence function, double descent can be approximated by a population loss. They show that the population loss depends on the number of neurons in the network. They also provide an upper bound on the interpolation threshold of double descent.   "
7525,SP:b485114712055f39a7afb951dbc3db482ff523fd,"Graph convolutional networks ( GCNs ) USED-FOR graph - structured data. over - smoothing problem FEATURE-OF deep GCNs. expressive power CONJUNCTION trainability. trainability CONJUNCTION expressive power. trainability CONJUNCTION optimization perspective. optimization perspective CONJUNCTION trainability. expressive power CONJUNCTION optimization perspective. optimization perspective CONJUNCTION expressive power. expressive power EVALUATE-FOR deep GCNs. expressivity COMPARE trainability. trainability COMPARE expressivity. Graph Neural Tangent Kernel ( GNTK ) USED-FOR optimization trajectory. Graph Neural Tangent Kernel ( GNTK ) USED-FOR wide GCNs. gradient descent USED-FOR wide GCNs. Graph Neural Tangent Kernel ( GNTK ) USED-FOR gradient descent. gradient descent USED-FOR optimization trajectory. asymptotic behaviors USED-FOR dropping trainability. dropping trainability FEATURE-OF wide and deep GCNs. exponential rate FEATURE-OF optimization process. asymptotic behaviors FEATURE-OF GNTK. large depth FEATURE-OF asymptotic behaviors. exponential rate FEATURE-OF dropping trainability. large depth FEATURE-OF GNTK. exponential rate FEATURE-OF wide and deep GCNs. theoretical framework USED-FOR residual connection - based techniques. residual connection - based techniques USED-FOR exponential decay of trainability. method COMPARE counterparts. counterparts COMPARE method. infinite - width and finite - width FEATURE-OF counterparts. Method are node representations, gradient descentbased optimizer, and DropEdge. OtherScientificTerm are expressive space, and exponential decay problem. ",This paper studies the over-smoothing problem in graph convolutional networks (GCNs) and proposes a novel method to reduce the expressive power of deep GCNs. The main idea is to use the graph neural tangent kernel (GNTK) as a gradient-descent-based optimizer for wide GCNs and drop the expressivity of the deep ones. Theoretical analysis is provided to show the asymptotic behavior of GNTK in terms of the exponential decay of expressivity and the drop in expressivity. Empirical results show that the proposed method is able to reduce expressivity in both the infinite-width and finite-width settings. ,"This paper studies the over-smoothing problem of graph convolutional networks (GCNs) in the context of graph-structured data. In particular, the authors propose a new method, DropEdge, which is based on the graph neural tangent kernel (GNTK) and a gradient descent-based optimizer. They show that DropEdge can be used to improve the expressive power of wide and deep GCNs at the expense of the trainability. They also provide a theoretical analysis of the asymptotic behavior of DropEdge in terms of the exponential decay of trainability in the expressive space."
7550,SP:25a92b3583afdc6892e59f1e769125d52c8011af,Computer vision methods USED-FOR first - order dynamics. optical flow HYPONYM-OF first - order dynamics. acceleration HYPONYM-OF higher - order changes. blood pressure CONJUNCTION arterial disease. arterial disease CONJUNCTION blood pressure. second derivative USED-FOR blood pressure. second derivative USED-FOR arterial disease. heart rate HYPONYM-OF summary statistics. videos USED-FOR cardiac measurements. waveform morphology USED-FOR clinically impactful scenarios. accuracy EVALUATE-FOR waveform morphology. loss function USED-FOR neural models. neural models USED-FOR higher - order dynamics. second - derivative inputs USED-FOR second - order dynamics. model USED-FOR left ventricle ejection time ( LVET ) intervals. second derivative PART-OF training procedure. second derivative USED-FOR model. second derivative FEATURE-OF vital sign signals. OtherScientificTerm is cardiac pulse. Task is camera - based vital sign measurement. ,This paper proposes a method for learning second-order dynamics from video recordings of the heart. The method is based on a neural network architecture that is trained to predict the second derivative of the left ventricle ejection time (LVET) intervals. The second derivative is used to model the second order dynamics of the LVET. The authors show that the model is able to learn the second derivatives of LVET intervals with a loss function that minimizes the loss of the first-order model.   ,"This paper proposes a novel method to learn the second-order dynamics of a cardiac pulse using a camera-based vital sign measurement. The method is based on a neural network model that is trained to predict the left ventricle ejection time (LVET) intervals of the heart. The model is trained using the second derivative of the cardiac pulse, which is used as a surrogate for the second order dynamics. The authors show that the model is able to predict LVET intervals with high accuracy.   "
7575,SP:0a88d2fcbdfab3e196bf6b9c75adb1006ab87536,"simple interactions USED-FOR human language. architecture USED-FOR communication system. symbolic mapping PART-OF communication system. symbolic mapping HYPONYM-OF architecture. symbolic mapping USED-FOR language learning. referential games USED-FOR symbolic mapping. symbolic mapping USED-FOR language learning. process USED-FOR multi - agent language learning. simplicity CONJUNCTION complexity. complexity CONJUNCTION simplicity. Task are emergent communication, and vocabulary expansion. OtherScientificTerm are language, and compositional and symmetric language. Material is dialog games. ","This paper studies the problem of multi-agent communication in referential games, where the goal is to learn to communicate with multiple agents in a dialog game. The authors propose to learn a symbolic mapping between the referents in the dialog game and the agents in the game. They show that this symbolic mapping can be used to improve the performance of language learning in dialog games. They also show that it is possible to learn language that is compositional and symmetric.","This paper proposes a new language learning framework for multi-agent language learning in dialog games. The proposed framework is based on a symbolic mapping architecture, where each agent is given a set of referential games, and the goal is to learn a language that is compositional and symmetric. The authors show that this symbolic mapping can be used to improve the performance of the language learning process. They also show that the proposed method can be applied to a variety of language learning tasks, and demonstrate that it can achieve better performance than the state of the art. "
7600,SP:89575be04cb33b41d7a0a7b62f9496c2838a1317,Robotic agents USED-FOR domestic chores. natural language directives USED-FOR Robotic agents. hierarchical modular approach USED-FOR agents. hierarchy FEATURE-OF policy. language instructions USED-FOR subgoals. navigation policy CONJUNCTION independent interaction policies. independent interaction policies CONJUNCTION navigation policy. master policy USED-FOR agent ’s navigation. interaction policy USED-FOR object masks. object masks USED-FOR manipulation actions. interaction policy USED-FOR manipulation actions. ALFRED benchmark EVALUATE-FOR hierarchical agent. OtherScientificTerm is divide - andconquer manner. Method is Compositional Reasoning. ,"This paper proposes a hierarchical modular approach to training robotic agents with natural language instructions. The main idea is to learn a hierarchy of subgoals that is composed of a navigation policy, an interaction policy, and a manipulation policy. The interaction policy is responsible for masking the environment and the navigation policy for navigation. The navigation policy is used to guide the agent to the desired goal, while the interaction policy acts as a guide for the manipulation actions. Experiments on the ALFRED benchmark show that the proposed method outperforms the baselines. ","This paper proposes a hierarchical modular approach for robotic agents to solve the task of domestic chores. The main idea is to learn a hierarchy of subgoals, where each subgoal is composed of a navigation policy, an interaction policy, and a manipulation policy. The navigation policy is the one that is responsible for navigation, and the interaction policy is a combination of the two. The interaction policy learns to manipulate the objects in the environment, while the manipulation policy learns how to control the environment. The authors show that their approach outperforms the state-of-the-art on the ALFRED benchmark. "
7625,SP:e2c8efe00db7baba2368f4f6a37815809b9e235e,"relationship USED-FOR model. Nuisance - Randomized Distillation ( NURD ) USED-FOR predictive models. nuisance - label relationship FEATURE-OF distributions. nuisance - randomized distribution HYPONYM-OF distribution. representations COMPARE representations. representations COMPARE representations. NURD USED-FOR representation. distribution PART-OF nuisance - varying family. NURD USED-FOR models. models USED-FOR pneumonia. NURD USED-FOR pneumonia. non - lung patches USED-FOR NURD. tasks EVALUATE-FOR NURD. non - lung patches USED-FOR nuisance. spurious correlations USED-FOR models. tasks EVALUATE-FOR NURD. chest X - ray classification EVALUATE-FOR NURD. chest X - ray classification HYPONYM-OF tasks. Task is prediction problems. OtherScientificTerm are nuisance variable, covariates, and background. Material is natural images. ",This paper proposes Nuisance-randomized distillation (NURD) to improve the nuisance-label relationship in predictive models. NURD is based on the observation that the distribution of covariates in the nuisance variable can vary over time. The authors propose a new distribution called nuisance- randomized distribution (NUDR) which is a family of distributions that is more robust to spurious correlations. NUDR is shown to improve performance on chest X-ray classification and pneumonia classification tasks.,"This paper proposes a novel method for training nuisance-randomized distributions (NURD) for predicting the nuisance-label relationship between two distributions. NURD is based on the notion of nuisance-varied distribution, which is a family of distributions that can be divided into two groups, one of which is the nuisance variable and the other is the covariate. The nuisance variable is defined as the one that is most likely to be correlated with the covariates of the other group. The covariates are defined as those that are not correlated with any of the two groups. The proposed method is evaluated on chest X-ray classification and pneumonia prediction tasks."
7650,SP:c75998b76f4e0510fc719d25959a10fc07db1c40,"computer vision models USED-FOR predefined categories. natural language USED-FOR supervision. InfoNCE loss USED-FOR model. model USED-FOR text captions. images CONJUNCTION text captions. text captions CONJUNCTION images. image - text pairs USED-FOR CLIP. Optimal TransporT distillation USED-FOR zero - shot Recognition. online entropic optimal transport USED-FOR soft image - text match. online entropic optimal transport USED-FOR contrastive learning. soft image - text match USED-FOR contrastive learning. Optimal TransporT distillation USED-FOR OTTER. online entropic optimal transport USED-FOR OTTER. pretrained image and text encoders USED-FOR models. image text pairs USED-FOR models. OTTER USED-FOR models. label smoothing CONJUNCTION knowledge distillation. knowledge distillation CONJUNCTION label smoothing. InfoNCE loss CONJUNCTION label smoothing. label smoothing CONJUNCTION InfoNCE loss. InfoNCE loss COMPARE OTTER. OTTER COMPARE InfoNCE loss. OTTER COMPARE baselines. baselines COMPARE OTTER. Google Open Images CONJUNCTION multi - labeled ImageNet. multi - labeled ImageNet CONJUNCTION Google Open Images. label smoothing COMPARE OTTER. OTTER COMPARE label smoothing. knowledge distillation COMPARE OTTER. OTTER COMPARE knowledge distillation. Google Open Images EVALUATE-FOR zero - shot evaluation. zero - shot evaluation EVALUATE-FOR baselines. zero - shot evaluation EVALUATE-FOR OTTER. multi - labeled ImageNet EVALUATE-FOR OTTER. OTTER COMPARE baselines. baselines COMPARE OTTER. dataset / architecture settings EVALUATE-FOR OTTER. OtherScientificTerm are visual concepts, and supervised "" gold "" labels. ","This paper proposes a novel method for zero-shot image classification. The proposed method is based on the CLIP framework, where the image-text pairs are generated from a pre-trained image and text encoder and decoder model. The authors propose to use the online entropic optimal transport (OTTER) method for contrastive learning. The method is evaluated on ImageNet and CIFAR-10.  ","This paper proposes a novel zero-shot recognition method called Optimal TransporT distillation (OTTER) for image-text matching. OTTER is based on the idea of online entropic optimal transport (OTT), which is a technique for contrastive learning. OTT distills the input image and text pairs into a single image and a single text pair, which is then used to train a model to match the image and the text pairs. The authors show that OTTER outperforms the state-of-the-art in terms of performance on Google Open Images, ImageNet, and multi-label ImageNet."
7675,SP:e83cd70377542b5d187998e2e4a7ac070f453ed6,Pix2Seq USED-FOR object detection. prior knowledge USED-FOR task. prior knowledge USED-FOR approaches. language modeling task USED-FOR object detection. bounding boxes CONJUNCTION class labels. class labels CONJUNCTION bounding boxes. neural net USED-FOR image. class labels HYPONYM-OF Object descriptions. bounding boxes HYPONYM-OF Object descriptions. it COMPARE detection algorithms. detection algorithms COMPARE it. task - specific data augmentations USED-FOR approach. COCO dataset EVALUATE-FOR detection algorithms. COCO dataset EVALUATE-FOR approach. COCO dataset EVALUATE-FOR it. Pix2Seq framework USED-FOR object detection. bounding boxes CONJUNCTION class labels. class labels CONJUNCTION bounding boxes. neural net USED-FOR image. ,"This paper proposes a new object detection framework called Pix2Seq, which uses bounding boxes and class labels for object detection. The proposed method is based on the idea of using bounding box as a language model to model the object detection task. The method is evaluated on the COCO dataset and achieves state-of-the-art performance.  ","This paper proposes Pix2Seq, a new object detection framework that leverages language modeling to improve the performance of object detection. The proposed method is based on a neural network-based approach, where a neural net is trained to generate a set of bounding boxes, class labels, and class descriptions. The authors show that the proposed method outperforms the state-of-the-art methods on the COCO dataset. "
7700,SP:abc9315f61929cc1c54dfef8ff83d7eac56ec2f2,"Deep vision models PART-OF visual reinforcement learning ( RL ). visual reinforcement learning ( RL ) USED-FOR policy networks. Deep vision models USED-FOR policy networks. hierarchical reasoning PART-OF stage - wise approach. geometric and numerical symbols CONJUNCTION operators. operators CONJUNCTION geometric and numerical symbols. approach USED-FOR policy network. approach USED-FOR interpretable symbolic policy. policy network USED-FOR interpretable symbolic policy. geometric and numerical symbols PART-OF interpretable symbolic policy. operators PART-OF interpretable symbolic policy. policy regression algorithm USED-FOR symbolic rules. RoundTourMix HYPONYM-OF policy regression algorithm. distilled symbolic policy COMPARE CNN based RL agents. CNN based RL agents COMPARE distilled symbolic policy. symbolic distillation approach USED-FOR CNN policy. policy distillation USED-FOR geometric relations. numerical state USED-FOR Detected bounding box Velocity. CNN policy network knowledge USED-FOR symbolic policy. Pong CONJUNCTION CircusCharlie. CircusCharlie CONJUNCTION Pong. Airstriker - Genesis CONJUNCTION Pong. Pong CONJUNCTION Airstriker - Genesis. CircusCharlie CONJUNCTION Seaquest. Seaquest CONJUNCTION CircusCharlie. Generic is policies. OtherScientificTerm are interpretability, input distribution shifts, and teacher - student. Method are end - to - end learning pipeline, and symbolic distillation. ","This paper proposes a symbolic distillation method for visual reinforcement learning. The main idea is to distill a symbolic policy into a set of geometric and numerical operators, which are then used to train an interpretable policy network. The proposed method is evaluated on a variety of visual tasks, including Airstriker-Genesis, Pong, Seaquest, and CircusCharlie.  ",This paper proposes a symbolic distillation method for visual reinforcement learning. The key idea is to distill a symbolic policy into a set of geometric and numerical symbols that can be interpreted by a teacher and student. The symbolic policy is then used to guide the teacher-student in the end-to-end learning pipeline. Experiments show that the proposed method outperforms the state-of-the-art in terms of interpretability. 
7725,SP:04e7e181aeb1244ae1c4837ad416aef93ea3ea32,"coarse - level object arrangements ( posture ) CONJUNCTION fine - grained level styling ( identity ). fine - grained level styling ( identity ) CONJUNCTION coarse - level object arrangements ( posture ). exemplar sources USED-FOR fine - grained level styling ( identity ). techniques PART-OF StyleGAN2. techniques USED-FOR PIVQGAN. generator USED-FOR pose - identity disentanglement. VQSN module USED-FOR shaping and composition information. GANInversion encoder CONJUNCTION generator. generator CONJUNCTION GANInversion encoder. self - supervision methods USED-FOR GANInversion encoder. joint - training scheme USED-FOR generator. joint - training scheme CONJUNCTION self - supervision methods. self - supervision methods CONJUNCTION joint - training scheme. joint - training scheme USED-FOR GANInversion encoder. self - supervision methods USED-FOR generator. one CONJUNCTION other. other CONJUNCTION one. other USED-FOR identity. one USED-FOR pose. one HYPONYM-OF ones. training scheme USED-FOR VQSN module. VQSN module USED-FOR pose - related representations. VQSN module CONJUNCTION training scheme. training scheme CONJUNCTION VQSN module. synthesis image quality EVALUATE-FOR model. disentangling scores EVALUATE-FOR model. synthesis image quality CONJUNCTION disentangling scores. disentangling scores CONJUNCTION synthesis image quality. latent - space reducing feature USED-FOR VQSN module. posture - identity disentangling FEATURE-OF model applications. VQSN module USED-FOR model applications. latent - space reducing feature USED-FOR model applications. PIVQGAN USED-FOR Unsupervised image - to - image translation. disentangled posture and identity control USED-FOR PIVQGAN. PIVQGAN USED-FOR segmentation - like ” masks. Task is image - to - image translation task. Material are training - set images, pose images, and referential identity images. ","This paper proposes a method for unsupervised image-to-image translation based on StyleGAN2. The main idea is to use a VQSN module for shaping and composition information, and a self-supervised GAN for pose-identity disentanglement. Experiments show that the proposed method is able to achieve state-of-the-art performance on the task.","This paper presents a method for unsupervised image-to-image translation (i.e., pose-identity disentanglement) using StyleGAN2. The proposed method is based on the VQSN module, which is a combination of a GAN-Inversion encoder and GAN generator, and a self-supervised GAN encoder. The model is trained using a joint-training scheme, where the GAN is trained to disentangle two images, one representing the identity and the other representing the posture. The VQSNN module is trained with a latent-space reducing feature, which can be used for segmentation-like masks. Experiments show that the proposed method outperforms the state-of-the-art in terms of synthesis image quality and disentangling scores."
7750,SP:e51a7f45493064972585109f203a867e9828eb15,"speech synthesis CONJUNCTION speech enhancement. speech enhancement CONJUNCTION speech synthesis. speech recognition CONJUNCTION speech synthesis. speech synthesis CONJUNCTION speech recognition. Transformers USED-FOR speech processing tasks. speech enhancement HYPONYM-OF speech processing tasks. speech recognition HYPONYM-OF speech processing tasks. speech synthesis HYPONYM-OF speech processing tasks. models USED-FOR speech related tasks. speech - MLP HYPONYM-OF multi - layer perceptron ( MLP ) architecture. speech - MLP USED-FOR multiscale local temporal dependency. keyword spotting CONJUNCTION speech enhancement. speech enhancement CONJUNCTION keyword spotting. keyword spotting EVALUATE-FOR model. speech enhancement EVALUATE-FOR model. tasks EVALUATE-FOR model. speech enhancement HYPONYM-OF tasks. keyword spotting HYPONYM-OF tasks. Google speech command V2 - 35 CONJUNCTION LibriWords. LibriWords CONJUNCTION Google speech command V2 - 35. dataset ( VoiceBank ) USED-FOR speech enhancement. benchmark datasets USED-FOR keyword spotting. benchmark datasets EVALUATE-FOR speech enhancement. benchmark datasets CONJUNCTION dataset ( VoiceBank ). dataset ( VoiceBank ) CONJUNCTION benchmark datasets. Google speech command V2 - 35 HYPONYM-OF benchmark datasets. LibriWords HYPONYM-OF benchmark datasets. speech - MLP COMPARE transformer - based solutions. transformer - based solutions COMPARE speech - MLP. Material is speech signals. OtherScientificTerm are feature channels, contextual window sizes, and resource - constrained scenarios. Generic is chunks. Metric is GFLOPS. Method is transformers. ","This paper proposes a multi-layer MLP architecture for speech processing tasks. The proposed architecture is based on the idea of multi-scale local temporal dependency, where each layer is responsible for local temporal dependencies. The authors show that the proposed model is able to achieve state-of-the-art performance on two tasks: keyword spotting and speech enhancement.   ","This paper proposes a multi-layer perceptron (MLP) architecture for speech recognition and speech enhancement tasks. The proposed model is based on the multi-layered MLP architecture, where each layer has a local temporal dependency. The model is trained on the VoiceBank dataset (V2-35) and the LibriWords dataset (VoiceBank). The model achieves state-of-the-art performance on both speech enhancement and speech recognition tasks. "
7775,SP:d708d3886f4abd4552d8ccb2096df7361c803b13,"labeled data USED-FOR massive models. data collection CONJUNCTION labeling. labeling CONJUNCTION data collection. generalization error EVALUATE-FOR transfer learning algorithm. transfer learning algorithm USED-FOR lower bound. generalization error EVALUATE-FOR lower bound. computational complexity EVALUATE-FOR transfer learning algorithm. it USED-FOR source / target data distributions. source domains USED-FOR knowledge transfer. bounds USED-FOR setting. bounds USED-FOR generalization error. real image classification CONJUNCTION action recognition data sets. action recognition data sets CONJUNCTION real image classification. lower bounds COMPARE upper - bounds. upper - bounds COMPARE lower bounds. lower bounds COMPARE transfer learning base - lines. transfer learning base - lines COMPARE lower bounds. transfer learning base - lines USED-FOR upper - bounds. source(s ) and target data sets USED-FOR weighted empirical risk minimization. weighted empirical risk minimization USED-FOR upper - bounds. weighted empirical risk minimization USED-FOR transfer learning base - lines. Task are machine learning, and binary classification problems. Method is Transfer learning. Material are labeled training data, and real world data sets. ","This paper studies the problem of transfer learning in the binary classification setting, where the goal is to transfer knowledge from source to target domains. The authors propose a new lower bound for the generalization error between source and target data distributions, which is based on a transfer learning algorithm. The proposed lower bound is shown to be computationally efficient, and the authors show that it can be used to compute the upper bound on the transfer learning error. The upper bound is obtained by minimizing the weighted empirical risk minimization between the target and source data sets.","This paper proposes a lower bound for the generalization error of a transfer learning algorithm for binary classification problems. The lower bound is based on a weighted empirical risk minimization (WERM) algorithm, which minimizes the risk between source and target data distributions. The upper bound is also based on the WERM algorithm. The authors show that the lower bound can be used to compute the transfer learning base-lines, which are then used to derive the upper bound.   "
7800,SP:f7511ba9ccad03233b34b1bf41bbac7361d20a57,"probabilistic shape completion method USED-FOR continuous geometry of large - scale 3D scenes. Generative Cellular Automata USED-FOR multi - modal distribution. formulation USED-FOR large - scale continuous geometry. latent code PART-OF sparse voxel embedding. sparse voxel embedding USED-FOR local continuous shape. progressive generation USED-FOR generative model. training objective USED-FOR sparse voxel embedding. variational lower bound FEATURE-OF complete shape distribution. variational lower bound USED-FOR training objective. probabilistic formulation USED-FOR geometry completion. approach COMPARE deterministic models. deterministic models COMPARE approach. Material are Real - world scans of 3D scenes, and missing data. Task is shape completion. Generic is model. ",This paper proposes a probabilistic shape completion method for large-scale continuous geometry of 3D scenes. The proposed method is based on a generative model with a latent code and a sparse voxel embedding. A progressive generation objective is used to generate the local continuous shape and a variational lower bound is used for the complete shape distribution. Experiments show that the proposed method achieves state-of-the-art performance on the task of shape completion.,This paper proposes a probabilistic shape completion method for large-scale 3D scenes. The proposed method is based on generative cellular automata (GCA) with a sparse voxel embedding and progressive generation. The training objective is a variational lower bound on the complete shape distribution. The experimental results show that the proposed method outperforms deterministic models.
7825,SP:d22d8f074adbe8fb0f25fb8f8d96201b3159bf6b,"exploration PART-OF deep reinforcement learning. Behavioral priors USED-FOR problem. reduced generality CONJUNCTION restricted transferability. restricted transferability CONJUNCTION reduced generality. exploration USED-FOR reinforcement learning. temporal consistency USED-FOR state - independent temporal priors. probabilistic mixture of policy and temporal prior USED-FOR off - policy reinforcement learning. approach COMPARE baselines. baselines COMPARE approach. long - horizon continuous control tasks EVALUATE-FOR baselines. sparse reward settings USED-FOR baselines. sparse reward settings USED-FOR long - horizon continuous control tasks. long - horizon continuous control tasks EVALUATE-FOR approach. sparse reward settings USED-FOR approach. OtherScientificTerm are temporal priors, and behavioral priors. ","This paper proposes a probabilistic mixture of policy and temporal prior for off-policy reinforcement learning in continuous control tasks. The proposed method is motivated by the observation that behavioral priors are not sufficient for efficient exploration in RL, and proposes to use temporal consistency to learn state-independent temporal priors. The method is evaluated on a variety of long-horizon continuous control problems and achieves state-of-the-art performance.",This paper proposes a probabilistic mixture of policy and temporal prior for off-policy reinforcement learning. The authors show that the proposed method outperforms the state-of-the-art on long-horizon continuous control tasks with sparse reward settings. The proposed method is evaluated on a variety of tasks.
7850,SP:25e06c022ae8b3cbbb8db413d7b534a1a5c92391,stochastic optimization USED-FOR deep neural networks. Learning rate scheduling HYPONYM-OF stochastic optimizers. Adam HYPONYM-OF stochastic optimizers. pre - defined rules USED-FOR scheduling. GNS USED-FOR dynamics. directed graph USED-FOR neural network. agent USED-FOR learning rate. directed graph USED-FOR GNS. GNS USED-FOR agent. reinforcement learning USED-FOR GNS. graph message passing network USED-FOR GNS. reinforcement learning USED-FOR agent. reinforcement learning USED-FOR learning rate. graph message passing network USED-FOR dynamics. scheduler USED-FOR intermediate layer information. reward collection procedure USED-FOR training. Fashion - MNIST CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION Fashion - MNIST. CIFAR10 CONJUNCTION GLUE. GLUE CONJUNCTION CIFAR10. Fashion - MNIST CONJUNCTION GLUE. GLUE CONJUNCTION Fashion - MNIST. GLUE USED-FOR language understanding. Fashion - MNIST CONJUNCTION image classification. image classification CONJUNCTION Fashion - MNIST. CIFAR10 USED-FOR image classification. image classification CONJUNCTION GLUE. GLUE CONJUNCTION image classification. Fashion - MNIST HYPONYM-OF benchmarking datasets. CIFAR10 HYPONYM-OF benchmarking datasets. GLUE HYPONYM-OF benchmarking datasets. Fashion - MNIST EVALUATE-FOR framework. GLUE EVALUATE-FOR framework. benchmarking datasets EVALUATE-FOR framework. GNS COMPARE baselines. baselines COMPARE GNS. GNS USED-FOR CNN and Transformer models. baselines USED-FOR CNN and Transformer models. network structures USED-FOR GNS. Method is scheduling mechanism. ,"This paper proposes a method for learning rate scheduling in deep neural networks. The main idea is to use a directed graph message passing network (GNN) to model the dynamics of the network and use reinforcement learning to control the learning rate. The proposed method is evaluated on Fashion-MNIST, CIFAR-10, and GLUE datasets.  ","This paper proposes a method for learning rate scheduling (GNS) for deep neural networks (DNNs). GNS is an extension of Adam, which is a stochastic optimization method for DNNs. In contrast to Adam, GNS uses a graph message passing network (GNN) to control the learning rate of a DNN. GNNs are trained using reinforcement learning to learn the dynamics of the network. The authors propose a reward-based reward collection procedure for training the GNN. The proposed method is evaluated on Fashion-MNIST, CIFAR-10, GLUE, and GLUE datasets."
7875,SP:d73cb0471c1770607ad3e4621cfc5f170683dd8e,"high - level relational reasoning CONJUNCTION scalable machine intelligence. scalable machine intelligence CONJUNCTION high - level relational reasoning. point cloud USED-FOR high - level relational reasoning. point cloud USED-FOR scalable machine intelligence. point cloud USED-FOR deep object - centric learning. framework USED-FOR 3D point cloud. framework USED-FOR spatial mixture model. Chamfer Mixture Loss PART-OF variational training pipeline. point clouds USED-FOR spatial mixture model. scheme USED-FOR SPAIR3D. unsupervised scene decomposition EVALUATE-FOR method. Method are object - specification scheme, and unsupervised manner. OtherScientificTerm is local voxel grid cell. "," is a novel method for object-centric 3D point cloud modeling. The method is based on the Chamfer Mixture Loss (MML), which is used to train a spatial mixture model on a set of point clouds. The main contribution of the paper is that the method is able to learn object-specific representations in an unsupervised manner. The experimental results show that the proposed method achieves state-of-the-art results.","This paper proposes a method to learn a 3D point cloud for object-centric relational reasoning. The proposed method is based on the Chamfer Mixture Loss (MML) loss, which is used to train a spatial mixture model for the point cloud. The authors show that the proposed method outperforms the state-of-the-art in unsupervised scene decomposition. "
7900,SP:3c57e921c1bf23e482551ceb71702931a7f07439,"world knowledge USED-FOR interactive environments. large language models ( LLMs ) USED-FOR interactive environments. large language models ( LLMs ) USED-FOR world knowledge. natural language USED-FOR high - level tasks. they USED-FOR high - level tasks. LMs USED-FOR high - level tasks. LMs USED-FOR they. VirtualHome environment EVALUATE-FOR method. method COMPARE LLM baseline. LLM baseline COMPARE method. executability EVALUATE-FOR LLM baseline. VirtualHome environment EVALUATE-FOR LLM baseline. executability EVALUATE-FOR method. executability CONJUNCTION correctness. correctness CONJUNCTION executability. language models1 USED-FOR actionable knowledge. OtherScientificTerm are actionable steps, low - level plans, and admissible actions. Method is LLMs. Generic is procedure. Metric is human evaluation. ","This paper proposes a method for evaluating the correctness of actions taken by large language models (LLMs) in an interactive environment, where the agent is given a set of actionable steps to take. The method is based on the idea that the agent should be able to identify actions that are admissible to take in the environment. The authors show that this is possible by training a language model to predict the actions that should be taken in an environment. They evaluate their method on the VirtualHome environment and show that their method is able to outperform the state-of-the-art on the task.","This paper proposes a method for evaluating the correctness of actions taken by large language models (LLMs) in an interactive environment. The method is based on the notion of actionable knowledge, which is defined as a set of actions that can be performed by a language model (e.g., a high-level task or a low-level one). The authors show that the method can be applied to a variety of environments, including the VirtualHome environment. They also show that their method can outperform a baseline LLM baseline in terms of executability and correctness. "
7925,SP:e0159d1c9df2e657892a3a0c77549df4698d9a1a,geometrical interpretation USED-FOR Variational Autoencoder framework. VAEs USED-FOR Riemannian structure of the learned latent space. vanilla VAE COMPARE VAE models. VAE models COMPARE vanilla VAE. geometrical considerations USED-FOR vanilla VAE. benchmark datasets EVALUATE-FOR VAE models. VAE USED-FOR Riemannian manifold. Riemannian manifold USED-FOR uniform distribution. method USED-FOR deep generative models. low data regime EVALUATE-FOR method. high dimensional data CONJUNCTION low sample sizes. low sample sizes CONJUNCTION high dimensional data. high dimensional data FEATURE-OF complex neuroimaging dataset. low sample sizes FEATURE-OF complex neuroimaging dataset. complex neuroimaging dataset EVALUATE-FOR method. ,"This paper proposes a method to model the Riemannian structure of the learned latent space of a generative model using a variational autoencoder (VAE) framework. The proposed method is based on the observation that the distribution over the latent space can be viewed as a function of the underlying geometrical structure. The authors then propose to use a VAE to learn the distribution of the latent variables in the learned space, which is then used as the basis for the distribution on the ground-truth data.   The authors show that the proposed method can be used in a variety of settings, including the low-data regime and the high-dimensional regime, and achieves state-of-the-art performance on several benchmark datasets.",This paper proposes a Variational Autoencoder (VAE) framework for learning the Riemannian structure of the learned latent space of a VAE. The proposed method is based on the idea that the latent space can be represented as a Riemanian manifold. The authors show that their method can be applied to both high dimensional data and low dimensional data. They show that the proposed method outperforms the state-of-the-art VAE models on a variety of datasets.
7950,SP:b4b8e1727f8617894f10f20365cb68de79f0e650,"natural language processing ( NLP ) CONJUNCTION computer vision tasks. computer vision tasks CONJUNCTION natural language processing ( NLP ). computer vision tasks EVALUATE-FOR transformers. natural language processing ( NLP ) EVALUATE-FOR transformers. attention heads USED-FOR applications. redundant heads PART-OF transformers. mixture of keys PART-OF transformer architecture. redundant heads PART-OF transformer architecture. Gaussian mixture model USED-FOR mixtures of keys. Transformer - MGK USED-FOR training. transformer counterpart COMPARE Transformer - MGK. Transformer - MGK COMPARE transformer counterpart. FLOPs USED-FOR Transformer - MGK. accuracy EVALUATE-FOR Transformer - MGK. linear attentions USED-FOR Transformer - MGK. language modeling CONJUNCTION tasks. tasks CONJUNCTION language modeling. applications EVALUATE-FOR Transformer - MGK. tasks HYPONYM-OF applications. tasks EVALUATE-FOR Transformer - MGK. language modeling HYPONYM-OF applications. Wikitext-103 and Long Range Arena benchmark EVALUATE-FOR Transformer - MGKs. Transformer - MGKs COMPARE baseline transformers. baseline transformers COMPARE Transformer - MGKs. Wikitext-103 and Long Range Arena benchmark EVALUATE-FOR baseline transformers. Method are Multi - head attention, and Transformer. OtherScientificTerm are redundant embedding, and attention head. Generic is model. ",This paper proposes a method to remove redundant heads in the Transformer architecture. The proposed method is based on a Gaussian mixture model (MGK) to remove the redundant embedding in the attention heads. The method is evaluated on language modeling and image classification tasks and achieves state-of-the-art performance.,"This paper proposes a new approach for multi-head attention in transformers. The key idea is to use a Gaussian mixture model to model the mixtures of keys in the attention head. The proposed approach is evaluated on a variety of tasks, including Wikitext-103 and Long Range Arena benchmark. The experimental results show that the proposed approach outperforms the state of the art."
7975,SP:82731dcce233e748f63382e09b6224a513fe9689,"biological agents USED-FOR Spatial navigation. proprioception CONJUNCTION linear and angular velocity. linear and angular velocity CONJUNCTION proprioception. direct - inverse model of environment dynamics USED-FOR image and action related signals. direct - inverse model of environment dynamics USED-FOR reconstruction of the action. direct - inverse model of environment dynamics USED-FOR two – dimensional continuous environment. Resetting Path Integrator ( RPI ) HYPONYM-OF minimalistic recurrent architecture. RPI USED-FOR internal state. it USED-FOR cognitive map. internal state FEATURE-OF minimal model. architecture COMPARE LSTM networks. LSTM networks COMPARE architecture. architecture USED-FOR internal dynamics. tasks EVALUATE-FOR LSTM networks. tasks EVALUATE-FOR architecture. Generic is models. OtherScientificTerm are image signal, resetting, and integration of past movement. Method is direct - inverse models. ","This paper proposes a method to learn a two-dimensional continuous state representation of the environment using a direct-inverse model. The proposed method is based on the resetting path integralrator (RPI) architecture, which is an extension of the RPI architecture proposed in [1]. The main contribution of this paper is to propose a method for learning the state representation in the two-dimensions of the continuous environment. The method is evaluated on a variety of spatial navigation tasks.   ","This paper proposes a minimalistic recurrent architecture for spatial navigation in a two-dimensional continuous environment. The proposed method is based on the resetting path integralrator (RPI) model, which is used to model the internal state of the agent. The authors show that the proposed method outperforms the state-of-the-art LSTM networks on a variety of tasks. "
8000,SP:1a27c397d1e73def5e724c5c6f25548975ba50fa,"features USED-FOR prediction. feature learning USED-FOR neural networks. practical data USED-FOR learning problems. neural networks USED-FOR problems. gradient descent USED-FOR neural networks. linear models USED-FOR data - independent features. polynomial sizes FEATURE-OF linear models. polynomial sizes FEATURE-OF data - independent features. polynomial algorithm PART-OF Statistical Query model. neural networks USED-FOR feature learning. OtherScientificTerm are class relevant patterns, background patterns, and structure of the input distribution. Material is synthetic and real data. ","This paper studies the problem of feature learning in linear regression, where the goal is to learn features that are independent of the input distribution. The authors propose to use a linear model with polynomial size to model the data-independent features and show that gradient descent can be used to learn such features. They also show that the polynomials of the features can be learned by a linear network trained with gradient descent. ","This paper studies the problem of feature learning in the context of linear models. The authors propose a new polynomial-based algorithm for feature learning for linear models, which can be used to learn data-independent features from synthetic and real-world data. The main contribution of the paper is that the authors show that the size of the data-dependent features can be reduced to polynomials in the presence of a linear model. They show that this can be done by using a polynome of the class-relevant patterns of the input distribution. They also show that their method can be applied to the case where the data is synthetic data. "
8025,SP:8ada73ed7eade9ebdeef376485e849c42575bc5f,robustness EVALUATE-FOR machine learning models. machine learning models USED-FOR adversarial examples. robustness FEATURE-OF adversarial examples. test - time adversaries FEATURE-OF adversarial examples. data distribution CONJUNCTION attacker constraints. attacker constraints CONJUNCTION data distribution. lower bounds USED-FOR model. bounds USED-FOR arbitrary classification functions. architectures CONJUNCTION models. models CONJUNCTION architectures. neural networks HYPONYM-OF architectures. neural networks HYPONYM-OF models. methodology USED-FOR robustness. robustness EVALUATE-FOR classifier. methodology USED-FOR fixed feature extractors. robustness EVALUATE-FOR fixed feature extractors. bounds USED-FOR classifier. bounds FEATURE-OF robustness. it USED-FOR classifier. method USED-FOR collisions. closed - form expressions USED-FOR collision finding. bespoke algorithm USED-FOR arbitrary feature extractors. closed - form expressions USED-FOR linear feature extractors. convex program USED-FOR bespoke algorithm. Method is training methods. ,"This paper studies the problem of robustness against adversarial examples in the presence of test-time adversaries. The authors propose a new method to estimate adversarial collisions in the training data, which is based on a closed-form expression for adversarial collision. They show that their method is computationally efficient and can be applied to arbitrary feature extractors.   ",This paper studies the problem of robustness against adversarial examples. The authors propose a new lower bound on the robustness of a classifier trained with a fixed feature extractor. The upper bounds are based on the fact that the classifier can be trained with arbitrary classification functions. The lower bounds are then used to derive a bespoke algorithm to find the robust classifier. The method is evaluated on a variety of datasets and models.
8050,SP:874b5fa51924cbcceed490d98a0ea80f74586b32,RL USED-FOR real - world problems. Offline reinforcement learning ( RL ) USED-FOR RL. regularization or constraints USED-FOR extrapolation error. regularization or constraints USED-FOR offline RL algorithms. framework USED-FOR V -function. learning procedure PART-OF offline dataset. optimal value learning CONJUNCTION behavior cloning. behavior cloning CONJUNCTION optimal value learning. conservatism FEATURE-OF offline learning. Expectile V -Learning ( EVL ) USED-FOR generalization. implicit planning USED-FOR V -values. offline trajectories USED-FOR implicit planning. Value - based Episodic Memory ( VEM ) HYPONYM-OF offline method. D4RL benchmark EVALUATE-FOR method. sparse - reward tasks HYPONYM-OF tasks. tasks EVALUATE-FOR method. sparse - reward tasks EVALUATE-FOR method. D4RL benchmark EVALUATE-FOR VEM method. OtherScientificTerm is Q - function. ,"This paper proposes Value-based Episodic Memory (VEM), a value-based offline reinforcement learning algorithm. The main idea of VEM is to learn a set of offline trajectories from which the Q-function can be estimated and used to estimate the expected value of a state-action pair in the offline setting. The method is evaluated on the D4RL benchmark and achieves state-of-the-art performance. ",This paper proposes a value-based Episodic Memory (VEM) method for offline reinforcement learning (RL) where the goal is to learn a V-function that maximizes the Q-function of an offline dataset. The main idea is to use the Expectile V-Learning (EVL) method to learn the V-values of the dataset and then use implicit planning to plan the offline trajectories. The method is evaluated on the D4RL benchmark and on a variety of tasks. 
8086,SP:34f08d92681504490c2f739b0d08f79f9764b2f5,"robust accuracy CONJUNCTION standard accuracy. standard accuracy CONJUNCTION robust accuracy. neural network classifiers USED-FOR adversarial perturbations. robustness FEATURE-OF neural network classifiers. adversarial training USED-FOR adversarial perturbations. adversarial training framework USED-FOR robust generalization. importance weight USED-FOR parametric function. bilevel optimization problem USED-FOR weighted adversarial training. approach COMPARE state - of - the - art baselines. state - of - the - art baselines COMPARE approach. approach COMPARE techniques. techniques COMPARE approach. clean and robust accuracy EVALUATE-FOR techniques. techniques CONJUNCTION state - of - the - art baselines. state - of - the - art baselines CONJUNCTION techniques. clean and robust accuracy EVALUATE-FOR state - of - the - art baselines. clean and robust accuracy EVALUATE-FOR approach. OtherScientificTerm are class - conditioned margin, and sample ’s multi - class margin. Method are MAML - based approaches, and robust classifier. Generic is upper - level task. ",This paper proposes a new adversarial training framework to improve the robustness of neural network classifiers against adversarial perturbations. The proposed method is based on a bilevel optimization problem to optimize the importance weight of the parametric function and the class-conditioned margin. The authors show that the proposed method can improve the clean and robust accuracy of the classifier. ,This paper proposes a method for improving the robustness of neural network classifiers against adversarial perturbations. The main idea is to use the importance weight of the parametric function of the classifier as a bilevel optimization problem to optimize the class-conditioned margin. The proposed method is evaluated on a variety of datasets and compared with a number of baselines. 
8122,SP:3ad36be6b6900aabe43da043461cf178ce977082,"force CONJUNCTION velocity. velocity CONJUNCTION force. position CONJUNCTION force. force CONJUNCTION position. velocity CONJUNCTION spin. spin CONJUNCTION velocity. computational physics CONJUNCTION chemistry. chemistry CONJUNCTION computational physics. spin HYPONYM-OF covariant information. velocity HYPONYM-OF covariant information. position HYPONYM-OF covariant information. force HYPONYM-OF covariant information. vectors HYPONYM-OF covariant information. geometric and physical information USED-FOR message and update functions. steerable MLPs PART-OF model. geometric and physical information PART-OF model. MLPs USED-FOR activation functions. activation functions USED-FOR steerable feature fields. MLPs USED-FOR steerable feature fields. components PART-OF SEGNNs. non - linear message aggregation CONJUNCTION linear ( steerable ) point convolutions. linear ( steerable ) point convolutions CONJUNCTION non - linear message aggregation. invariant messages FEATURE-OF equivariant graph networks. equivariant graph networks USED-FOR steerable messages. non - linear message aggregation HYPONYM-OF components. non - linear message aggregation PART-OF SEGNNs. computational physics CONJUNCTION chemistry. chemistry CONJUNCTION computational physics. chemistry EVALUATE-FOR method. computational physics EVALUATE-FOR method. OtherScientificTerm are node and edge attributes, invariant scalars, and steerable node attributes. Method is equivariant non - linear convolutions. ",This paper proposes a steerable message and update model for graph neural networks (GNNs). The main idea is to use steerable MLPs to model both geometric and physical information in the model. Theoretical results show that the proposed model is equivariant and non-linear. Experiments are conducted on synthetic and real-world data. ,"This paper proposes a steerable model for equivariant graph neural networks (SEGNNs) that can handle steerable features (e.g., force, position, velocity, spin, etc.) and invariant scalars. The model is built on top of a linear (steerable) point convolution and non-linear (non-linear) message aggregation. It is shown that the model is able to handle both steerable feature fields (force and position) as well as covariant information (velocity and spin). The authors also show that their model can be used for nonlinear message aggregation and nonlinear point convolutions."
8158,SP:8928aa83f7ebd4e310f4fe1d01ff0eb0c96e4d2b,"physics models CONJUNCTION gradient - based learning. gradient - based learning CONJUNCTION physics models. model explicability CONJUNCTION data efficiency. data efficiency CONJUNCTION model explicability. Differentiable physics modeling USED-FOR model explicability. gradient - based learning PART-OF Differentiable physics modeling. physics models PART-OF Differentiable physics modeling. It USED-FOR dynamics. It USED-FOR inverse problems. It USED-FOR design. inverse problems CONJUNCTION design. design CONJUNCTION inverse problems. dynamics CONJUNCTION inverse problems. inverse problems CONJUNCTION dynamics. rigid bodies CONJUNCTION deformable sheets. deformable sheets CONJUNCTION rigid bodies. rigid bodies HYPONYM-OF physics models. deformable sheets HYPONYM-OF physics models. material structures CONJUNCTION force interactions. force interactions CONJUNCTION material structures. Fine - grained models USED-FOR material structures. gradient - based learning USED-FOR Fine - grained models. Fine - grained models USED-FOR force interactions. gradient - based learning USED-FOR force interactions. individual yarn physics CONJUNCTION yarn - to - yarn interactions. yarn - to - yarn interactions CONJUNCTION individual yarn physics. differentiable fabrics model USED-FOR composite materials. cloths HYPONYM-OF composite materials. differentiable forces USED-FOR gradient - based learning. differentiable forces PART-OF empirical physics. forces USED-FOR cloths. complex physical structures CONJUNCTION heterogeneous materials. heterogeneous materials CONJUNCTION complex physical structures. data - efficiency CONJUNCTION high - fidelity. high - fidelity CONJUNCTION data - efficiency. model USED-FOR physical parameters. high - fidelity USED-FOR subtle dynamics. model USED-FOR subtle dynamics. high - fidelity EVALUATE-FOR model. data - efficiency EVALUATE-FOR model. OtherScientificTerm are complex physical phenomena, and granularity of yarns. Material is physical systems. ","This paper proposes to use gradient-based learning to learn differentiable physics models to model complex physical systems. The main idea is to model the dynamics of a system as a set of inverse problems, and then learn a model that can be used to solve the inverse problems. The model is trained using gradient descent. The authors show that the model is able to model force interactions between particles in a complex physical system, and that it can be trained with gradient descent to learn the dynamics. ","This paper proposes a differentiable physics model for differentiable materials. The model is based on the idea of fine-grained physics models, which can be used to model material structures and force interactions. The main contribution of this paper is to propose a new model for fine-rigid materials, which is able to capture the dynamics of material structures, force interactions, and material structures. The paper also proposes a new method for designing materials that can be applied to different types of materials."
8194,SP:2c8358c095b10981d3015b9f6c75765419a9480d,"logical composition USED-FOR framework. logical composition USED-FOR reinforcement learning. OtherScientificTerm are task - specific skill, optimal policy, Boolean expression, unknown distribution, and task distribution. Generic are algorithm, distribution, approach, and tasks. Method are transferred policy, transfer learning, and transfer learning approach. ","This paper studies the problem of transfer learning from one task to another, where the goal is to learn a new task-specific skill that is transferable across tasks. The authors propose a transfer learning approach that learns a new policy that is a combination of the optimal policy and a logical composition of the two tasks. They show that this approach is computationally efficient and can be applied to a wide range of tasks.  ","This paper proposes a new method for transfer learning in reinforcement learning, where the goal is to transfer a skill from one task to another task. The key idea is to use the notion of ""logical composition"" (i.e., a set of tasks that can be represented as a sequence of logical expressions) to represent the task-specific skill. The authors show that this can be applied to the problem of transfer learning, and propose a new approach to do so. The method is evaluated on a variety of tasks."
8230,SP:c85d71d05164d019cc32bf423e4c4fe20c169f41,"machine and deep learning solutions USED-FOR multivariate time series classification ( MTSC ). prediction accuracy EVALUATE-FOR complex models. accuracy EVALUATE-FOR solutions. ROCKET HYPONYM-OF MTSC solution. random convolutional kernels USED-FOR ROCKET. random convolutional kernels USED-FOR MTSC solution. distributed solution USED-FOR MTSC. LightWaveS HYPONYM-OF distributed solution. solution COMPARE deep learning solutions. deep learning solutions COMPARE solution. wavelet scattering transformation CONJUNCTION distributed feature selection. distributed feature selection CONJUNCTION wavelet scattering transformation. distributed feature selection USED-FOR solution. accuracy EVALUATE-FOR deep learning solutions. wavelet scattering transformation USED-FOR solution. wavelet scattering transformation USED-FOR time series. ROCKET features USED-FOR solution. accuracy EVALUATE-FOR solution. nodes CONJUNCTION channels. channels CONJUNCTION nodes. nodes USED-FOR LightWaveS. channels USED-FOR LightWaveS. it USED-FOR MTSC problem. inference speedup CONJUNCTION scalability. scalability CONJUNCTION inference speedup. accuracy CONJUNCTION inference speedup. inference speedup CONJUNCTION accuracy. training time CONJUNCTION accuracy. accuracy CONJUNCTION training time. training time EVALUATE-FOR algorithm. ROCKET USED-FOR inference. speedup EVALUATE-FOR ROCKET. speedup EVALUATE-FOR inference. edge device USED-FOR inference. datasets EVALUATE-FOR speedup. datasets EVALUATE-FOR inference. OtherScientificTerm are real - world environments, and features. Metric are prediction speed, and inference time. Task is training. ","This paper proposes a distributed multi-variate time series classification (MTSC) solution, named as LightWaveS, which is a distributed MTSC solution with random convolutional kernels. The proposed method is based on a wavelet scattering transformation and distributed feature selection. Experiments show that the proposed method achieves state-of-the-art performance in terms of accuracy and inference speed.  ","This paper proposes a new method for multivariate time series classification (MTSC) based on a wavelet scattering transformation and distributed feature selection. The proposed method, called LightWaveS, is a distributed solution for MTSC, where each time series is represented as a set of time series, and each channel is represented by a random convolutional kernel. The authors show that the proposed method outperforms the state-of-the-art in terms of prediction accuracy and inference speedup."
8266,SP:db43614ca016280a79448f44a97c81c8ff5ba981,"AMOS USED-FOR text encoders. Mixture Of Signals USED-FOR auxiliary generators. Mixture Of Signals USED-FOR Adversarial learning curriculum. Adversarial learning curriculum USED-FOR text encoders. discriminator USED-FOR replaced tokens. discriminator USED-FOR encoder. encoder USED-FOR replaced tokens. auxiliary masked language models ( MLMs ) USED-FOR replaced tokens. MLMs USED-FOR training signals. mixture weights USED-FOR discriminator loss. gradient PART-OF discriminator. mixture weights USED-FOR auxiliary MLMs ’ outputs. Gumbel - Softmax USED-FOR gradient. MLMs PART-OF unified auxiliary model. AMOS COMPARE pretrained models. pretrained models COMPARE AMOS. AMOS COMPARE ELECTRA. ELECTRA COMPARE AMOS. GLUE and SQuAD benchmarks EVALUATE-FOR BERT base - sized models. ELECTRA COMPARE pretrained models. pretrained models COMPARE ELECTRA. BERT base - sized models EVALUATE-FOR pretrained models. GLUE and SQuAD benchmarks EVALUATE-FOR pretrained models. GLUE and SQuAD benchmarks EVALUATE-FOR AMOS. Method are ELECTRA - style pretraining, and MLM. Metric is pretraining efficiency. ",This paper proposes a novel adversarial training method for text encoders. The proposed method is based on a mixture of masked language models (MLMs) to replace the replaced tokens in the encoder with a discriminator that is trained with Gumbel-Softmax. The authors show that the proposed method outperforms ELECTRA on GLUE and SQuAD benchmarks. ,"This paper proposes a novel adversarial learning curriculum for text encoders. The proposed method, AMOS, uses a mixture of auxiliary masked language models (MLMs) to replace the replaced tokens in the training signal. The discriminator is trained with a Gumbel-Softmax-based discriminator, and the encoder is trained using a mixture-of-signals-based encoder. AMOS is evaluated on GLUE and SQuAD datasets, and compared with ELECTRA-style pretraining. "
8302,SP:db3825633ab5d0671340390b23ab655838cc38b2,"pre - trained language models USED-FOR relational knowledge. clozestyle sentence USED-FOR pre - trained language models. clozestyle sentence USED-FOR relational knowledge. language models COMPARE knowledge graphs. knowledge graphs COMPARE language models. precision EVALUATE-FOR language models. adaptive fine - tuning USED-FOR fill - mask task. pre - trained language model USED-FOR fill - mask task. pre - trained language model USED-FOR adaptive fine - tuning. complex prompting techniques CONJUNCTION adaptive fine - tuning. adaptive fine - tuning CONJUNCTION complex prompting techniques. adaptive fine - tuning COMPARE baselines. baselines COMPARE adaptive fine - tuning. transfer learning capabilities FEATURE-OF language model. Task is relational fact extraction task. OtherScientificTerm are knowledge graph facts, and knowledge graph. Generic are model, and approach. Metric is knowledge extraction quality. ","This paper proposes a method to improve the performance of pre-trained language models on the relational fact extraction task. The method is based on the observation that language models trained on knowledge graph facts are better at extracting relational knowledge than knowledge graphs. The authors propose to use a clozestyle sentence to extract relational knowledge from the knowledge graph, where the knowledge is represented as a set of clozestories. The model is trained using a combination of two methods: (1) using a pre-training language model and (2) adaptive fine-tuning.   ","This paper proposes a method to improve the knowledge extraction quality of pre-trained language models for relational knowledge extraction. The method is based on adaptive fine-tuning, where the model is trained to extract a set of facts from a knowledge graph, and then fine-tune the model to extract the correct facts from the knowledge graph. The authors show that their method outperforms baselines in terms of accuracy and transfer learning capabilities."
8311,SP:ae25d32714b2b9f7e02cc20f4a36252e20e78e4f,"multi - relations FEATURE-OF Knowledge bases. symmetry CONJUNCTION inversion. inversion CONJUNCTION symmetry. inversion CONJUNCTION composition. composition CONJUNCTION inversion. symmetry HYPONYM-OF properties. composition HYPONYM-OF properties. inversion HYPONYM-OF properties. Euclidean embedding models USED-FOR properties. hyperbolic space USED-FOR transitivity. representation learning framework USED-FOR relation properties. geometric spaces FEATURE-OF knowledge base embeddings. out - of - taxonomy entity typing task EVALUATE-FOR aligned embeddings. knowledge graph USED-FOR entities. datasets EVALUATE-FOR approach. low dimensions CONJUNCTION small training rates. small training rates CONJUNCTION low dimensions. low dimensions EVALUATE-FOR approach. YAGO3 USED-FOR datasets. small training rates EVALUATE-FOR approach. OtherScientificTerm are Euclidean space, and tree - like properties. Method is manifold alignment. ",This paper proposes to use hyperbolic embeddings to learn the relations between entities in a knowledge base. The proposed method is based on the observation that the relations in Euclidean space have two properties: inversion and inversion + inversion = inversion. The authors propose to learn these two properties using a representation learning framework that learns the relation properties from the embedding space. The method is evaluated on the out-of-taxonomy entity typing task and on the YAGO3 dataset.,"This paper proposes a representation learning framework for embedding knowledge base embeddings in a hyperbolic Euclidean space. The key idea is to learn a set of properties of the embedding space, such as symmetry, inversion, composition, and inversion-inversion inversion. These properties are then used to train an embedding model that can be used to align the embedded embedding to the hyper-dimensional space. Experiments show that the proposed method outperforms the state-of-the-art in terms of accuracy and training speed.   "
8320,SP:9ab3bc525ee4a9c96518c43e4c43082655a7674f,frequency distribution FEATURE-OF real - world knowledge graphs. approaches USED-FOR static knowledge graphs. one - shot learning framework USED-FOR link prediction. temporal knowledge graphs USED-FOR one - shot learning framework. temporal knowledge graphs USED-FOR link prediction. self - attention mechanism USED-FOR temporal interactions between entities. network USED-FOR similarity score. self - attention mechanism CONJUNCTION network. network CONJUNCTION self - attention mechanism. network USED-FOR method. self - attention mechanism USED-FOR method. algorithm COMPARE baselines. baselines COMPARE algorithm. algorithm USED-FOR sparse relations. Method is low - shot learning methods. Task is temporal settings. OtherScientificTerm is data scarcity. ,"This paper proposes a one-shot learning framework for link prediction in temporal knowledge graphs. The proposed method uses a self-attention mechanism to capture temporal interactions between entities in the knowledge graph, and a network to predict the similarity score between entities. The authors show that the proposed method outperforms existing methods in terms of link prediction accuracy.   ","This paper proposes a new one-shot learning framework for link prediction in temporal knowledge graphs. The authors propose a self-attention mechanism to capture the temporal interactions between entities in a temporal knowledge graph, and a network to measure the similarity score between entities. They show that the proposed method outperforms baselines in terms of link prediction accuracy. "
8336,SP:91f92a40e12afd0702f07ae7f4175ecce57b7007,"solver USED-FOR task. neural module USED-FOR solver. module USED-FOR task. module USED-FOR modules. visual reasoning tasks EVALUATE-FOR model. model COMPARE attention - based baseline. attention - based baseline COMPARE model. human judges USED-FOR reasoning process. Generic is tasks. OtherScientificTerm are Lower modules, and forgetting. ",This paper proposes a method to improve the performance of visual reasoning models on visual reasoning tasks. The proposed method is based on the idea that a lower module is trained to predict the state of the task and then a higher module is used to solve the task. The paper shows that the proposed method outperforms the baselines in terms of accuracy on the visual reasoning task.  ,"This paper proposes a new model for visual reasoning tasks. The main idea is to use a lower-level module for each task, and a higher-level one for the upper-level modules. The lower-layer module is used to solve the task, while the higher-layer is used as a solver. The upper-layer solver is trained using an attention-based approach, and the lower-learner is trained on the task-specific module. The model is evaluated on a variety of tasks, and it is shown to outperform the state-of-the-art in terms of accuracy."
8345,SP:de33b02e7f2faec5bcae9a5516721aa1ef190572,"channel - selectivity FEATURE-OF convolutional layer. Selective Convolutional Unit ( SCU ) HYPONYM-OF architectural unit. parameter efficiency EVALUATE-FOR CNNs. architectural unit USED-FOR CNNs. Selective Convolutional Unit ( SCU ) HYPONYM-OF CNNs. bottlenecks FEATURE-OF CNNs. parameter efficiency EVALUATE-FOR architectural unit. SCU USED-FOR channel - selectivity. SCU USED-FOR training. pruning unimportant channels USED-FOR SCU. SCU - based models COMPARE baselines. baselines COMPARE SCU - based models. model compression EVALUATE-FOR baselines. postprocessing USED-FOR SCU - based models. model compression EVALUATE-FOR SCU - based models. OtherScientificTerm are identity ( e.g., residual ) connection, identity connection, pruned parameters, rewired parameters, and convolutional kernels. Method is deep convolutional neural networks ( CNN ). ",This paper proposes a new architectural unit called Selective Convolutional Unit (SCU) to improve the channel-selectivity of convolutional layers. The proposed SCU is based on pruning unimportant channels and re-wiring parameters to improve channel selectivity. Experiments show that the proposed method can improve the efficiency of CNNs. ,"This paper proposes a new architectural unit, called Selective Convolutional Unit (SCU), to improve the channel-selectivity of deep convolutional neural networks (CNNs) by pruning unimportant channels and rewiring parameters. SCU-based models are shown to be more efficient than baselines in terms of model compression and post-processing. "
8354,SP:2d80fa4bc440061be2234b5070503d3fa056baed,positive data CONJUNCTION unlabeled data. unlabeled data CONJUNCTION positive data. positive data USED-FOR binary classifier. unlabeled data USED-FOR binary classifier. labeled positive data COMPARE unlabeled positive data. unlabeled positive data COMPARE labeled positive data. selection bias FEATURE-OF labeling process. it USED-FOR selection bias. PU learning USED-FOR Bayes optimal classifier. method USED-FOR classifier. algorithm USED-FOR scoring function. algorithm USED-FOR classifier. threshold USED-FOR classifier. method COMPARE methods. methods COMPARE method. methods USED-FOR PU learning. method USED-FOR PU learning. real - world datasets EVALUATE-FOR PU learning. real - world datasets EVALUATE-FOR method. real - world datasets EVALUATE-FOR methods. OtherScientificTerm is class posterior. ,"This paper studies the problem of positive-unlabeled PU learning, where the goal is to learn a Bayesian classifier from unlabeled positive data. The authors propose a method to learn the Bayesian posterior of a positive classifier using PU learning. The method is based on the observation that the labeling process has a selection bias towards positive data, which can be explained by the fact that the labeled positive data is more likely to have a higher probability of being labeled than the unlabelled data. To address this issue, the authors propose to use a scoring function for the posterior of the positive class, which is then used to train a classifier. The proposed method is evaluated on a variety of real-world datasets and achieves state-of-the-art performance. ","This paper proposes a method for learning a Bayesian classifier from positive and unlabeled data. The proposed method is based on the notion of PU learning, which is the problem of learning a classifier that maximizes the performance of the class posterior. The method is motivated by the observation that the bias in the labeling process is due to the selection bias between positive and negative data. To overcome this bias, the authors propose a new scoring function for the classifier. The scoring function is a weighted sum of the number of positive samples and the amount of negative samples. The authors show that this scoring function can be used to learn a Bayes optimal classifier, and they also show that their method outperforms the state-of-the-art methods in terms of performance."
8363,SP:5f312626b0613d2e07c59214c5f00db208a98717,"auxiliary losses USED-FOR representations. approach USED-FOR statistical inefficiency. statistical inefficiency FEATURE-OF neural networks. auxiliary losses USED-FOR approach. auxiliary loss USED-FOR main loss. reinforcement learning CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION reinforcement learning. multi - task supervised learning CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION multi - task supervised learning. Atari games USED-FOR reinforcement learning. gridworld USED-FOR reinforcement learning. domains EVALUATE-FOR algorithm. ImageNet USED-FOR multi - task supervised learning. multi - task supervised learning HYPONYM-OF domains. reinforcement learning HYPONYM-OF domains. reinforcement learning HYPONYM-OF domains. OtherScientificTerm are auxiliary task, and adaptive weight. ","This paper proposes to use auxiliary losses to improve the performance of neural networks in multi-task learning. The main idea is to use an auxiliary task as a regularization term to the main loss, and then use the auxiliary task to update the weights of the main network. The authors show that using auxiliary losses improves the performance on image classification tasks and reinforcement learning tasks.  ","This paper proposes a novel approach to reduce the statistical inefficiency of neural networks. The main idea is to use auxiliary losses as auxiliary losses, which can be used as auxiliary weights for the main loss. The auxiliary losses are used to improve the performance of the main losses. The proposed approach is evaluated on a variety of domains, including reinforcement learning, multi-task supervised learning, and Atari games."
8372,SP:e270ae3eeb7ab4fa91ba37d4d68ce10f2fa0a3b5,"Adversarial examples HYPONYM-OF machine learning models. geometric framework USED-FOR high - dimensional geometry of adversarial examples. manifold reconstruction literature USED-FOR geometric framework. low - dimensional data manifolds PART-OF high - dimensional space. decision boundary USED-FOR low - dimensional data manifold. decision boundary USED-FOR Adversarial examples. nearest neighbor classifiers CONJUNCTION ball - based adversarial training. ball - based adversarial training CONJUNCTION nearest neighbor classifiers. robustness EVALUATE-FOR norms. sufficient sampling conditions USED-FOR nearest neighbor classifiers. sufficient sampling conditions USED-FOR ball - based adversarial training. OtherScientificTerm are misclassifications, codimension, adversarial examples, and manifold. Method is adversarial training. ",This paper studies the geometry of adversarial examples in the presence of misclassification. The authors propose a new geometric framework to study the high-dimensional geometry of the misclassified data manifold. They show that the low-dimensional data manifold can be represented as a low dimensional data manifold with a decision boundary. The decision boundary can then be used to train adversarially robust models.   The main contribution of this paper is to provide a geometric framework for analyzing the geometry and properties of the data manifold in the context of the adversarial training. ,"This paper proposes a geometric framework for reconstructing the high-dimensional geometry of adversarial examples from a low-dimensional data manifold. The proposed framework is based on the manifold reconstruction literature. The main contribution of the paper is the formulation of a new decision boundary between the low dimensional data manifold and the high dimensional space. The decision boundary is defined as the distance between the data and the data manifold, which is defined in terms of a low dimensional manifold. This decision boundary can be used to define a high dimensional high dimensional low dimensional space, which can then be used as a lowdimensional high dimensional manifold to reconstruct the data.  "
8381,SP:e07d948a79d478ecd23a0a4406d4ddd3ac5e3be3,"human cognition USED-FOR high - dimensional spaces. interpretable low - dimensional representations USED-FOR areas. representation learning algorithms USED-FOR time series data. interpretable discrete dimensionality reduction CONJUNCTION deep generative modeling. deep generative modeling CONJUNCTION interpretable discrete dimensionality reduction. deep generative modeling USED-FOR representation learning framework. interpretable discrete dimensionality reduction USED-FOR representation learning framework. framework USED-FOR discrete representations of time series. discrete representations of time series USED-FOR smooth and interpretable embeddings. non - differentiability FEATURE-OF discrete representation learning. way USED-FOR non - differentiability. self - organizing map algorithm COMPARE original. original COMPARE self - organizing map algorithm. representation space FEATURE-OF Markov model. Markov model USED-FOR probabilistic interpretation of our method. model USED-FOR natural representation of uncertainty. model USED-FOR temporal transition structure. model USED-FOR clustering. static ( Fashion-)MNIST data CONJUNCTION time series of linearly interpolated ( Fashion-)MNIST images. time series of linearly interpolated ( Fashion-)MNIST images CONJUNCTION static ( Fashion-)MNIST data. clustering CONJUNCTION interpretability. interpretability CONJUNCTION clustering. eICU data set FEATURE-OF real world medical time series application. macro states FEATURE-OF chaotic Lorenz attractor system. clustering EVALUATE-FOR model. real world medical time series application EVALUATE-FOR model. interpretability EVALUATE-FOR model. static ( Fashion-)MNIST data EVALUATE-FOR model. Material are High - dimensional time series, and real world data. OtherScientificTerm is data features. Generic are representation, method, and representations. "," time series is an important topic in machine learning. This paper proposes to learn interpretable embeddings of time series using discrete dimensionality reduction and deep generative modeling. The proposed method is based on a self-organizing map algorithm that learns a Markov model in the representation space, which is then used to learn a probabilistic interpretation of the time series. The method is evaluated on time series of linearly interpolated (Fashion-)MNIST images and eICU data. ","This paper proposes a novel representation learning framework for time series data. The authors propose a new representation learning algorithm that uses a Markov model to learn a smooth and interpretable embeddings of time series. The proposed method is motivated by the notion of non-differentiability in discrete representation learning, which can be used to improve the interpretability of the embedding space. The method is evaluated on the Fashion-MNIST dataset and the eICU medical time series dataset. "
8390,SP:5915ee71ea58dbdbafa31c1ad291d1e5940a0cf4,"multidimensional probability distributions USED-FOR latent space prior distributions. latent space prior distributions USED-FOR implicit generative models. linear interpolations USED-FOR latent space. random latent vectors USED-FOR decoding linear interpolations. non - linear interpolations USED-FOR distribution mismatch. latent probability distribution USED-FOR distribution mismatch. multidimensional Cauchy distribution USED-FOR prior distribution. OtherScientificTerm are latent distribution, finite mean, and latent distributions. ",This paper studies the problem of distribution mismatch in implicit generative models with multidimensional probability distributions. The authors show that the distribution mismatch is caused by a distribution mismatch between the latent distribution and the prior distribution. They show that this distribution mismatch can be explained by the fact that the latent space prior distribution is a multi-dimensional Cauchy distribution with a finite mean. They then show that linear interpolations in the latent spaces of the prior distributions can be decomposed into a set of random latent vectors that can be used for decoding linear interpolation.   ,"This paper studies the problem of distribution mismatch in latent space prior distributions for implicit generative models. In particular, the authors consider the case where the latent distribution is a multidimensional Cauchy distribution with a finite mean, and the latent space is a latent space with random latent vectors. The authors show that the distribution mismatch is due to a non-linear interpolation of the latent vectors, which is a result of a distribution mismatch between the latent probability distribution and the prior distribution. They provide a theoretical analysis of this distribution mismatch, and show that it can be decomposed into two parts: (1) a linear interpolation between the two latent distributions, and (2) a multi-dimensional probability distribution.  "
8399,SP:19b63ca635712f1509ca6e0141303c192f2709e0,"hyperbolic space FEATURE-OF shallow networks. embeddings USED-FOR ubiquitous attention mechanisms. ubiquitous attention mechanisms USED-FOR neural networks architectures. hyperbolic geometry FEATURE-OF embeddings. hyperbolic geometry COMPARE Euclidean geometry. Euclidean geometry COMPARE hyperbolic geometry. generalization EVALUATE-FOR neural machine translation. WMT’14 FEATURE-OF neural machine translation. learning on graphs EVALUATE-FOR method. synthetic and real - world graph tasks EVALUATE-FOR learning on graphs. visual question answering ( CLEVR ) tasks EVALUATE-FOR method. neural machine translation EVALUATE-FOR method. generalization EVALUATE-FOR method. Generic are approaches, and model. Method are geometry of embedding of object representations, and neural representations. OtherScientificTerm are embedding space, and semantic distance. Material is graphs. ",This paper proposes to use hyperbolic embeddings to improve the generalization performance of neural networks on graph tasks. The proposed method is based on the observation that shallow networks learn representations in the embedding space of a shallow network in a way that depends on the geometry of the object embedding. The authors show that embedding the object representations in a hyper-bolic space is more efficient than embedding them in the Euclidean space. They show that this is the case for neural networks with shallow networks with attention mechanisms. They also show that the proposed method can be used to improve generalization on synthetic and real-world graph tasks in terms of semantic distance. ,"This paper proposes a method for learning hyperbolic embeddings for deep neural networks. The key idea is to learn a hyper-parameterized embedding of the embedding space of the neural network, and then use this embedding to learn the semantic distance between the neural representations and the object representations. The proposed method is evaluated on synthetic and real-world graph tasks, and it is shown to outperform the state-of-the-art."
8408,SP:f6049e9f80a63c9306c1cebcb6b229aa6da44ddc,"attacks USED-FOR architecture information. attacks USED-FOR deep neural networks ( DNN ). architecture information FEATURE-OF deep neural networks ( DNN ). cache side - channels FEATURE-OF DNN fingerprinting attacks. threat model USED-FOR attacks. attack USED-FOR architecture. Flush+Reload HYPONYM-OF cache side - channel technique. DeepRecon HYPONYM-OF attack. cache side - channel technique USED-FOR internal information. Flush+Reload USED-FOR internal information. internal information USED-FOR attack. VGG19 CONJUNCTION ResNet50. ResNet50 CONJUNCTION VGG19. forward propagation USED-FOR complex networks. ResNet50 HYPONYM-OF complex networks. VGG19 HYPONYM-OF complex networks. meta - model USED-FOR pretrained model. transfer learning setting FEATURE-OF pretrained model. empirical security analysis USED-FOR DNNs ’ vulnerability. cache side - channel attacks FEATURE-OF DNNs ’ vulnerability. OtherScientificTerm are black - box networks, shared framework, network architecture, and architecture attributes. Method are victim model, co - located process, deep learning ( DL ) system, framework - level defense techniques, and DNNs. Task is fingerprinting process. ","This paper proposes a new method to attack deep neural networks (DNNs) using cache side-channel attacks. The proposed method, called DeepRecon, is based on the idea of co-located process, which is a variant of the Flush+Reload technique. The authors show that the proposed method is able to attack the architecture of DNNs in a transfer learning setting, where a meta-model is used to train a pretrained model.   ",This paper proposes a new attack technique called Flush+Reload for deep neural network (DNN) fingerprinting attacks. The proposed method is based on a meta-model that is trained on top of a pre-trained model. The authors show that the proposed method can be used to attack DNNs in a transfer learning setting. They also provide empirical evidence of the effectiveness of the proposed technique. 
8417,SP:6a3dd89db6c24a1f98e8866ef0a4c1c2c1ec6635,"representational hierarchy USED-FOR predicting future video frames. spatiotemporal memories PART-OF representational hierarchy. hierarchical network model USED-FOR spatiotemporal memories. Hierarchical Prediction Network ( HPNet ) HYPONYM-OF hierarchical network model. feedforward, feedback and lateral recurrent circuits PART-OF mammalian hierarchical visual system. feedforward, feedback and lateral recurrent circuits USED-FOR model. recurrent connections USED-FOR spatiotemporal memories. feed - forward path CONJUNCTION feedback path. feedback path CONJUNCTION feed - forward path. feed - forward path USED-FOR spatiotemporal features. feed - forward path PART-OF model. feedback path PART-OF model. feed - forward path CONJUNCTION feedback path. feedback path CONJUNCTION feed - forward path. feedback path PART-OF recurrent gated circuit. benchmark datasets EVALUATE-FOR long range video sequence predictions. hierarchical interaction PART-OF network. predictive self - supervised learning USED-FOR representational learning. visual cortex FEATURE-OF representational learning. OtherScientificTerm are hierarchy, internal memory states, prediction errors, frame - to - frame basis, memories of global movement patterns, and early visual cortex. Generic is level. ","This paper proposes a new model for predicting future video frames. The proposed model is based on a hierarchical network model with feedforward, feedback and lateral recurrent circuits. The model is trained using self-supervised learning and self-attention. Experiments show that the proposed model achieves state-of-the-art performance on long-range video sequence predictions. ","This paper proposes a hierarchical network model for predicting future video frames. The model is based on feedforward, feedback and lateral recurrent circuits in a mammalian hierarchical visual system. The proposed model is trained using predictive self-supervised learning. Experiments show that the proposed model outperforms state-of-the-art models on a variety of datasets."
8426,SP:fb74e57f35666742caf651e6da33b5defcf259a8,continuous embeddings USED-FOR kmers. method USED-FOR continuous embeddings. raw RNA - seq data USED-FOR continuous embeddings. raw RNA - seq data USED-FOR kmers. DNA sequence similarity CONJUNCTION DNA sequence abundance. DNA sequence abundance CONJUNCTION DNA sequence similarity. model USED-FOR DNA sequence similarity. model USED-FOR DNA sequence abundance. DNA sequence abundance FEATURE-OF embedding latent space. latent space USED-FOR exon information. them COMPARE known gene sub - structures. known gene sub - structures COMPARE them. acute myeloid leukemia patients FEATURE-OF raw RNA - Seq data. raw RNA - Seq data USED-FOR exon information. latent space USED-FOR detection of genomic abnormalities. visualization CONJUNCTION analysis. analysis CONJUNCTION visualization. representation space USED-FOR visualization. representation space USED-FOR analysis. translocations CONJUNCTION patient - specific mutations. patient - specific mutations CONJUNCTION translocations. patient - specific mutations HYPONYM-OF detection of genomic abnormalities. translocations HYPONYM-OF detection of genomic abnormalities. Generic is vectors. OtherScientificTerm is genomic abnormalities. ,"This paper proposes a method to learn continuous embeddings of exons from raw RNA-seq data. The method is based on the idea of learning a continuous embedding of a set of ""kmers"", which are sequences that are close to the original exons in the sequence space. The embedding space is then used to learn a model of DNA sequence similarity and DNA sequence abundance. The model is trained to predict the distribution of DNA sequences in the latent space, which is used to predict genomic abnormalities.   ","This paper proposes a method for generating continuous embeddings of raw RNA-Seq data from the raw data. The method is based on a model of DNA sequence similarity and DNA sequence abundance. The model is trained to predict the number of exons in the data, which is then used to generate a continuous embedding of the data. It is shown that the model is able to detect genomic abnormalities such as translocations and patient-specific mutations.   "
8435,SP:03aca6ff6a7f0ad2d5ccbcb15ed9536e305a9880,"approach USED-FOR model compression. weight or filter space FEATURE-OF network. architecture space USED-FOR approach. 1 - D CNN encoder / decoder USED-FOR mapping. continuous embedding CONJUNCTION back. back CONJUNCTION continuous embedding. embedding USED-FOR parameter count. dataset EVALUATE-FOR architecture. gradient descent USED-FOR compression objective function. accuracy CONJUNCTION parameter count. parameter count CONJUNCTION accuracy. accuracy EVALUATE-FOR compression objective function. parameter count FEATURE-OF compression objective function. continuous space FEATURE-OF gradient descent. gradient descent USED-FOR compression phase. continuous feature USED-FOR discrete architecture. decoder USED-FOR discrete architecture. FMNIST CONJUNCTION SVHN. SVHN CONJUNCTION FMNIST. CIFAR-10/100 CONJUNCTION FMNIST. FMNIST CONJUNCTION CIFAR-10/100. CIFAR-10 EVALUATE-FOR compression. visual recognition tasks EVALUATE-FOR approach. compression EVALUATE-FOR approach. SVHN HYPONYM-OF visual recognition tasks. CIFAR-10/100 HYPONYM-OF visual recognition tasks. FMNIST HYPONYM-OF visual recognition tasks. Method are Architecture Compression, and model compression methods. OtherScientificTerm is discrete architecture space. ","This paper proposes a new method for model compression based on architecture compression. The main idea is to use a 1-D CNN encoder/decoder to map the weight space of the network to a discrete architecture space, and then use gradient descent to compute the compression objective function in the architecture space. The proposed method is evaluated on CIFAR-10/100, FMNIST, SVHN, and Cifar-10. ","This paper proposes a new approach for model compression. The main idea is to use a 1-D CNN encoder/decoder to map the parameters of the network to a discrete architecture space, and then use gradient descent to compress the parameters in the architecture space. The authors show that the proposed approach can achieve better compression performance than existing methods on CIFAR-10/100 and SVHN datasets."
8444,SP:0511b5d10a90e3fe814e2d35208b4a987894ea62,"local model - based control CONJUNCTION global value function learning. global value function learning CONJUNCTION local model - based control. global value function learning CONJUNCTION exploration. exploration CONJUNCTION global value function learning. local trajectory optimization USED-FOR value function learning. approximate value functions USED-FOR policies. approximate value functions USED-FOR planning horizon. trajectory optimization USED-FOR temporally coordinated exploration. estimating uncertainty USED-FOR value function approximation. trajectory optimization CONJUNCTION estimating uncertainty. estimating uncertainty CONJUNCTION trajectory optimization. temporally coordinated exploration CONJUNCTION estimating uncertainty. estimating uncertainty CONJUNCTION temporally coordinated exploration. humanoid locomotion CONJUNCTION dexterous in - hand manipulation. dexterous in - hand manipulation CONJUNCTION humanoid locomotion. components USED-FOR control tasks. humanoid locomotion HYPONYM-OF control tasks. dexterous in - hand manipulation HYPONYM-OF control tasks. Method are plan online and learn offline ” framework, and internal model. OtherScientificTerm are value function, and local solutions. ","This paper proposes a method for learning value function approximations of trajectories in an online and offline setting. The method is based on a combination of value function approximation and exploration, where the value function is approximated by an approximate value function, and the exploration is done by planning over the horizon. The authors show that the method is able to learn a value function that is close to the true value function in the planning horizon, and that it can be used to estimate the uncertainty in the future trajectories, which can then be used for value function learning.   The authors evaluate the method on a variety of control tasks, including humanoid locomotion and dexterous in-hand manipulation. ","This paper proposes a new framework for planning online and learning offline, where the goal is to learn a global value function that can be used to guide the exploration of the environment. The method is based on the idea of value function approximation, which is to estimate the uncertainty of the value function and use it as a planning horizon for the planning horizon. The authors show that the method can be applied to a variety of control tasks, including humanoid locomotion, dexterous in-hand manipulation, and humanoid control tasks."
8453,SP:771494fda4702cd8c7efbf225b19028f91b449b9,"parallel data USED-FOR Neural Machine Translation ( NMT ) systems. zero - shot and dual learning PART-OF approach. reinforcement learning USED-FOR duality of the machine translation task. reinforcement learning USED-FOR latter. UN corpus EVALUATE-FOR zero - shot dual system. zero - shot dual system COMPARE NMT system. NMT system COMPARE zero - shot dual system. NMT system USED-FOR zero - shot translation. English - French and English - Spanish USED-FOR zero - shot dual system. SpanishFrench EVALUATE-FOR NMT system. zero - shot dual method COMPARE LSTM - based unsupervised NMT system. LSTM - based unsupervised NMT system COMPARE zero - shot dual method. en− →fr task EVALUATE-FOR LSTM - based unsupervised NMT system. en− →fr task EVALUATE-FOR zero - shot dual method. Material are low - resource languages, monolingual data, and newstest2014. Task are unsupervised and semi - supervised methods, machine translation task, and fr− →en task. ",This paper proposes a novel approach for zero-shot and dual learning in machine translation. The proposed approach is based on the duality of the machine translation task and uses reinforcement learning to learn a duality model for each language. The authors show that the proposed approach achieves state-of-the-art results on two tasks: English-French and English-Spanish.  ,"This paper proposes a zero-shot and dual learning approach for unsupervised and semi-supervised neural machine translation (NMT) systems. The proposed approach is based on the notion of duality of the machine translation task, where the goal is to learn the duality between English-French and Spanish-Spanish. The authors propose a new corpus of monolingual data from the UN corpus, which is used to train the NMT system. They show that the proposed approach outperforms the state-of-the-art NMT systems on the en-to-en task and the en+to-fr task. "
8462,SP:1558dc03f99670f9ddccdca9c223a2baf962d438,"framework USED-FOR Information - Retrieval ( IR ). IRGAN USED-FOR Information - Retrieval ( IR ). framework USED-FOR IRGAN. generator USED-FOR distribution. minimax loss function USED-FOR generator. adversarial fashion USED-FOR models. Method is Generative Adversarial Networks. Material is multiple domains. Generic are task, and model. OtherScientificTerm are conditional probability distribution, adversarial formulation, loss curves, loss functions, and co - training like setup. ","This paper proposes a framework for information-retrieval (IR) based generative adversarial networks (IRGAN) that uses a conditional probability distribution to train a generator and a discriminator. The generator is trained in a co-training like setup, where the discriminator is trained to maximize the mutual information between the generator and discriminator, and the discriminators are trained in an adversarial fashion. The authors show that the proposed IRGAN achieves competitive performance on image classification tasks.   ","This paper proposes a new framework IRGAN for information-retrieval (IR) in the context of generative adversarial networks (GANs) where the generator is trained in a co-training-like setup. The main contribution of the paper is a new adversarial formulation of IRGAN, which is based on the minimax loss function of the generator. The authors show that the generator can be trained in an adversarial fashion in the same way as a generative model. They also show that it is possible for the generator to be trained with the same loss function as the discriminator.   "
8471,SP:6a13dda852ab075a3c0fb691476d6dc57919c729,"Variational auto - encoders ( VAEs ) USED-FOR approximate inference. approximate inference USED-FOR intractable generative models. representations USED-FOR auxiliary tasks. VAEs USED-FOR latent codes. human interpretation HYPONYM-OF auxiliary tasks. classification HYPONYM-OF auxiliary tasks. variational auto - encoders CONJUNCTION sparse coding. sparse coding CONJUNCTION variational auto - encoders. sparsity FEATURE-OF latent space. latent space FEATURE-OF VAE. Spike and Slab prior distribution USED-FOR latent space. Spike and Slab prior distribution USED-FOR sparsity. evidence lower bound USED-FOR approximate posterior inference. approximate posterior inference COMPARE VAE case. VAE case COMPARE approximate posterior inference. discrete mixture recognition function USED-FOR approximate posterior inference. discrete mixture recognition function USED-FOR evidence lower bound. approach USED-FOR sparse representations. intractable non - linear probabilistic models USED-FOR sparse representations. sparse representations COMPARE VAE representations. VAE representations COMPARE sparse representations. classification accuracy CONJUNCTION robustness. robustness CONJUNCTION classification accuracy. robustness EVALUATE-FOR sparse representations. classification accuracy EVALUATE-FOR sparse representations. sparse elements USED-FOR subjectively understandable sources of variation. OtherScientificTerm are interpretability, and latent dimensions. Material is MNIST. ","This paper proposes a method to improve the interpretability of variational auto-encoders (VAEs) by using sparse coding in the latent space. The method is based on the Spike and Slab prior distribution, which is used to model the sparsity of the latent distribution. The authors show that the proposed method is able to learn sparse representations that are subjectively more interpretable and robust to variations in the data. ","This paper proposes a new approach to approximate posterior inference for variational auto-encoders (VAEs) with sparse coding. The key idea is to use the Spike and Slab prior distribution to estimate the sparsity of the latent space of a VAE, and then use a discrete mixture recognition function to approximate the posterior of the VAE. The authors show that the proposed approach outperforms the state-of-the-art in terms of accuracy and robustness. "
8480,SP:06a22143186fa2948fbe324ccae96a62ff12064e,non - adversarial feature matching - based approach USED-FOR generative models. pretrained neural networks USED-FOR feature extraction. autoencoders CONJUNCTION ConvNet classifiers. ConvNet classifiers CONJUNCTION autoencoders. pretrained neural networks USED-FOR Generative Feature Matching Networks ( GFMN ). pretrained neural networks USED-FOR approach. ConvNet classifiers HYPONYM-OF pretrained neural networks. autoencoders HYPONYM-OF pretrained neural networks. ImageNet HYPONYM-OF challenging datasets. CIFAR10 CONJUNCTION STL10. STL10 CONJUNCTION CIFAR10. first order statistics USED-FOR approach. pretrained ImageNet classifiers USED-FOR features. challenging benchmarks EVALUATE-FOR approach. CIFAR10 HYPONYM-OF challenging benchmarks. STL10 HYPONYM-OF challenging benchmarks. ,-based generative models have shown good performance on ImageNet and STL10. This paper proposes a novel non-adversarial feature matching-based approach for feature extraction. The proposed method is based on a generative feature matching network (GFMN) that uses a pre-trained convolutional neural network (CNN) to extract features from the input image. The authors show that GFMN outperforms the state-of-the-art autoencoders and ConvNet classifiers.  ,"This paper proposes a novel non-adversarial feature matching-based approach for generative models. The authors propose a generative feature matching network (GFMN) that learns to match the features extracted by a pre-trained generative model with the input data. The GFMN is trained by using a combination of two methods: (1) a GAN that matches the input features with the output data, and (2) an autoencoder-based method that learns the features from the input. The proposed method is evaluated on CIFAR10, STL10, and ImageNet datasets."
8489,SP:2d7cf2f07a27d6c8e304a1b47c25387ad2e4432d,Graph Neural Networks ( GNNs ) USED-FOR representation learning of graphs. representation vector USED-FOR node. neighborhood aggregation scheme USED-FOR GNNs. node and graph classification tasks EVALUATE-FOR GNN variants. GNNs USED-FOR graph representation learning. GNNs USED-FOR graph structures. theoretical framework USED-FOR graph structures. theoretical framework USED-FOR GNNs. Graph Convolutional Networks CONJUNCTION GraphSAGE. GraphSAGE CONJUNCTION Graph Convolutional Networks. Graph Convolutional Networks HYPONYM-OF GNN variants. GraphSAGE HYPONYM-OF GNN variants. architecture COMPARE WeisfeilerLehman graph isomorphism test. WeisfeilerLehman graph isomorphism test COMPARE architecture. architecture COMPARE GNNs. GNNs COMPARE architecture. graph classification benchmarks EVALUATE-FOR model. Generic is they. ,This paper proposes a novel neighborhood aggregation scheme for graph convolutional neural networks (GNNs). The proposed method is based on the observation that GNNs learn a representation vector for each node in a graph by aggregating its neighbors in a neighborhood. The authors show that this aggregation scheme can be used to improve the performance of GNN on node and graph classification tasks. ,"This paper proposes a novel neighborhood aggregation scheme for graph neural networks (GNNs). The proposed method is based on the Weisfeiler-Lehman graph isomorphism test, which is a theoretical framework for graph representation learning. The authors show that the proposed method outperforms existing GNNs on both node and graph classification tasks. "
8498,SP:51126f2dd37ce57d2614c9044ede1e43627f0829,framework USED-FOR interpretable continual learning ( ICL ). this USED-FOR ICL. ICL idea USED-FOR continual learning approaches. saliency maps USED-FOR metric. average classification accuracy EVALUATE-FOR overall continual learning performance. metric EVALUATE-FOR ICL. overall continual learning performance EVALUATE-FOR ICL. average classification accuracy EVALUATE-FOR ICL. Method is variational continual learning framework. OtherScientificTerm is catastrophic forgetting. ,"This paper proposes an interpretable continual learning (ICL) framework for continual learning. The main idea is to use saliency maps as a metric to measure the importance of each task in the continual learning process. The proposed ICL framework is based on a variational continual learning framework, where the saliency map is computed using a loss function. The authors show that the proposed method outperforms the state-of-the-art continual learning methods in terms of average classification accuracy. ","This paper proposes a new continual learning framework for interpretable continual learning (ICL). The main idea is to use saliency maps to measure the saliency of the data, which can be used as a new metric for continual learning. The proposed method is based on variational continual learning, which is an extension of previous work on interpretable continuous learning (ICL).  The main contribution of the paper is to propose a new ICL framework that can be applied to a variety of continual learning methods. The main contributions of this work are:  1. A new metric to measure saliency is proposed to measure how well the data is interpretable.  2. The authors show that the proposed ICL method is able to improve the overall continual learning performance.  "
8507,SP:27a565b3e5442b93d208652784051e640b0c1bfe,"perturbations USED-FOR model. evaluation framework USED-FOR adversarial attacks. adversarial attacks FEATURE-OF seq2seq models. constraints USED-FOR word - based MT systems. human and automatic evaluation EVALUATE-FOR they. adversarial training USED-FOR model. meaning - preserving attacks FEATURE-OF adversarial training. adversarial robustness EVALUATE-FOR model. Material is Adversarial examples. Metric is robustness. OtherScientificTerm are semantics, and meaning preservation. Task is machine translation ( MT ). Generic is methods. ","This paper proposes a new evaluation framework for evaluating the robustness of machine translation models in the face of adversarial attacks. The proposed evaluation framework is based on the observation that adversarial examples can be used to improve the performance of a machine translation model in terms of robustness to adversarial perturbations. The evaluation is performed on a word-based machine translation task, where the goal is to evaluate how well the model is able to preserve the meaning of the input words.    The paper proposes two evaluation methods: (1) mean-preserving adversarial training and (2) meaning preserving attacks. In the former, the authors show that the proposed method can improve adversarial robustness in the presence of meaning preservation attacks. ","This paper proposes a new evaluation framework for evaluating the robustness of machine translation (MT) models against adversarial attacks. The evaluation framework is based on the notion of meaning-preserving attacks (i.e., attacks that preserve the meaning of the target language). The authors show that the proposed evaluation framework can be applied to both human and automatic evaluation of MT models. They also show that adversarial training can be used to improve the performance of MT systems. "
8516,SP:54ddd8132bf9e4259d2c2d72b348d2bb5f9e227c,rewards CONJUNCTION inverse ( negative ) rewards. inverse ( negative ) rewards CONJUNCTION rewards. rewards USED-FOR policies. inverse ( negative ) rewards USED-FOR policies. policies COMPARE policies. policies COMPARE policies. policies USED-FOR mis - actions. inverse rewards USED-FOR policies. deep Q - learning CONJUNCTION double Q - learning. double Q - learning CONJUNCTION deep Q - learning. double Q - learning CONJUNCTION on - policy actor - critic. on - policy actor - critic CONJUNCTION double Q - learning. hybrid polices COMPARE algorithms. algorithms COMPARE hybrid polices. rewards FEATURE-OF hybrid polices. on - policy actor - critic USED-FOR hybrid polices. deep Q - learning USED-FOR hybrid polices. double Q - learning USED-FOR hybrid polices. polices COMPARE policies. policies COMPARE polices. Method is reinforcement learning algorithms. OtherScientificTerm is inverse policies. Material is OpenAI gym. ,"This paper proposes to use inverse policies to improve the performance of reinforcement learning agents in the OpenAI gym. In particular, the authors propose to use a combination of Q-learning and double-Q-learning to learn policies with inverse rewards. The authors show that the proposed hybrid policies outperform the state-of-the-art policies in terms of performance on OpenAI Gym. ","This paper studies the problem of hybrid policies, where a policy is trained with inverse (negative) rewards and a policy with positive (inverse) rewards. The authors show that under certain conditions, a policy can be trained with both inverse and positive rewards. They also show that a policy trained with negative rewards can outperform its inverse counterpart in terms of performance. They show that this is the case under some conditions. "
8525,SP:89a732b57934d08b937c93560f391b7758e54f8a,"object parts CONJUNCTION hierarchical structure. hierarchical structure CONJUNCTION object parts. dynamics model USED-FOR object parts. hierarchical, disentangled object representation CONJUNCTION dynamics model. dynamics model CONJUNCTION hierarchical, disentangled object representation. formulation USED-FOR hierarchical, disentangled object representation. formulation USED-FOR dynamics model. unlabeled videos USED-FOR dynamics model. structural descriptor USED-FOR low - level concepts. structural descriptor USED-FOR hierarchical structure. layered image representation USED-FOR object parts. structural descriptor USED-FOR hierarchy. PSD model USED-FOR segmenting object parts. real and synthetic datasets EVALUATE-FOR PSD model. PSD model USED-FOR tasks. PSD model USED-FOR motion distributions. motion distributions HYPONYM-OF tasks. segmenting object parts HYPONYM-OF tasks. OtherScientificTerm is system dynamics. ","This paper proposes a method for learning object parts from unlabeled videos. The proposed method is based on a hierarchical, disentangled object representation and a dynamics model, which is trained to predict low-level concepts. The method is evaluated on a variety of tasks, including object segmentation, object disentanglement, and object detection.  ","This paper proposes a new model for disentanglementing object parts in a video. The proposed model is based on a hierarchical, disentangled object representation and a dynamics model. The model is trained on unlabeled videos and is able to segment the video into low-level concepts, which are then used to generate a hierarchical representation of the object parts. Experiments are conducted on both synthetic and real-world datasets. "
8534,SP:bb2a655d67bed9da43f0b8ec7d888b89c217d12e,noisy training datasets USED-FOR deep neural networks. noisy ( incorrect ) class labels PART-OF Large - scale datasets. noisy datasets USED-FOR softmax neural classifier. Deep Determinantal Generative Classifier ( DDGC ) HYPONYM-OF inference method. softmax neural classifier USED-FOR decision boundary. hidden feature spaces PART-OF discriminative deep model. discriminative deep model USED-FOR generative classifier. hidden feature spaces USED-FOR generative classifier. minimum covariance determinant estimator USED-FOR generative classifier. DDGC USED-FOR adversarial perturbations. noisy labels USED-FOR DDGC. noisy labels CONJUNCTION adversarial samples. adversarial samples CONJUNCTION noisy labels. training techniques USED-FOR noisy labels. training techniques USED-FOR adversarial samples. learning models USED-FOR noisy labels. learning models USED-FOR adversarial samples. learning models USED-FOR DDGC. training techniques USED-FOR DDGC. training techniques USED-FOR learning models. test accuracy EVALUATE-FOR deep model. CIFAR10 dataset EVALUATE-FOR deep model. noisy training labels FEATURE-OF CIFAR10 dataset. noise - handling training method USED-FOR deep model. Metric is classification accuracy. OtherScientificTerm is large margin property. ,This paper proposes a method for training deep neural networks with noisy labels in the presence of adversarial perturbations. The main idea is to use a discriminative deep model to learn a generative classifier in the hidden feature spaces of the noisy labels. The proposed method is evaluated on CIFAR-10 with noisy training labels and adversarial samples.,"This paper proposes a new deep generative classifier, Deep Determinantal Generative Classifier (DDGC), which uses a discriminative deep model to learn the decision boundary of a classifier with noisy labels. The proposed method is motivated by the fact that the classification accuracy of the classifier depends on the number of noisy labels, which is not always the case in practice. The main contribution of the paper is that DDGC is able to handle noisy labels and adversarial perturbations. The method is evaluated on the CIFAR-10 dataset, where it outperforms the state-of-the-art in terms of test accuracy."
8543,SP:0fa525cc708470b757a60117cb608bb2feaa2c50,approaches USED-FOR Reinforcement Learning ( RL ). huge state spaces CONJUNCTION sparse delayed reward feedback. sparse delayed reward feedback CONJUNCTION huge state spaces. approaches USED-FOR large - scale applications. huge state spaces FEATURE-OF large - scale applications. sparse delayed reward feedback USED-FOR large - scale applications. action selection policies USED-FOR Hierarchical Reinforcement Learning ( HRL ) methods. temporal abstraction FEATURE-OF action selection policies. skill policies USED-FOR subgoals. approaches USED-FOR subgoal discovery. subgoal discovery USED-FOR HRL. approaches USED-FOR HRL. internal reward signal USED-FOR subgoal attainment. internal reward signal USED-FOR skills. intrinsic motivation USED-FOR skills. model - free method USED-FOR subgoal discovery. incremental unsupervised learning USED-FOR model - free method. method USED-FOR subgoals. intrinsic motivation learning mechanism CONJUNCTION method. method CONJUNCTION intrinsic motivation learning mechanism. approach USED-FOR HRL. rooms environment CONJUNCTION ATARI 2600 game. ATARI 2600 game CONJUNCTION rooms environment. sparse delayed feedback FEATURE-OF RL problems. Montezuma ’s Revenge HYPONYM-OF ATARI 2600 game. RL problems EVALUATE-FOR method. ATARI 2600 game HYPONYM-OF RL problems. Montezuma ’s Revenge HYPONYM-OF RL problems. rooms environment HYPONYM-OF RL problems. OtherScientificTerm is Abstraction. Generic is model. ,"This paper proposes an incremental unsupervised learning method for Hierarchical Reinforcement Learning (HRL) that learns to discover subgoals in the state space using an intrinsic motivation learning mechanism. The method is based on the idea of temporal abstraction, where the state is represented as a sequence of states, and the goal is a set of skills that can be used to achieve the goal. The proposed method is evaluated on the ATARI 2600 game and Montezuma's Revenge. ","This paper proposes a Hierarchical Reinforcement Learning (HRL) method that uses a model-free approach to discover subgoals in a hierarchical setting. The method is based on incremental unsupervised learning (i.e., learning a set of skills that can be used to achieve a subgoal) and an intrinsic motivation learning mechanism. The proposed method is evaluated on the ATARI 2600 game and the Montezuma’s Revenge game."
8552,SP:e5861538bc8bb9165cb33299bbf12dd875abf976,Representation Learning CONJUNCTION Formal Methods. Formal Methods CONJUNCTION Representation Learning. Neuro - Symbolic Methods HYPONYM-OF Formal Methods. neural framework USED-FOR Circuit Satisfiability problem. model USED-FOR SAT problem. rich embedding architecture USED-FOR problem structure. end - to - end differentiable training procedure USED-FOR Reinforcement Learning. rich embedding architecture CONJUNCTION end - to - end differentiable training procedure. end - to - end differentiable training procedure CONJUNCTION rich embedding architecture. rich embedding architecture USED-FOR framework. end - to - end differentiable training procedure USED-FOR framework. framework COMPARE NeuroSAT method. NeuroSAT method COMPARE framework. out - of - sample generalization EVALUATE-FOR framework. out - of - sample generalization EVALUATE-FOR NeuroSAT method. Method is rich neural architectures. ,"This paper proposes a neural framework for solving the Circuit Satisfiability problem in reinforcement learning. The proposed method is based on Neuro-Symbolic Methods (NeuroSAT), which is an extension of NeuroSAT to the circuit satisfaction problem. The main contribution of this paper is the introduction of an end-to-end differentiable training procedure for Reinforcement Learning. Experiments show that the proposed method achieves state-of-the-art performance in terms of generalization performance.   ","This paper proposes a neural framework for solving the Circuit Satisfiability Problem (SAT). The proposed method is based on the Neuro-Symbolic Methods (NSM) framework, which is an extension of the NeuroSAT framework. The main idea is to use a neural network to solve the SAT problem, and then use a differentiable training procedure for Reinforcement Learning. The authors show that the proposed method outperforms the state-of-the-art neuro-symbolic method (NeuroSAT) in terms of generalization performance."
8561,SP:ff3e5d44619df3825632b0b1a943add081364861,"Deep neuroevolution CONJUNCTION deep reinforcement learning ( deep RL ) algorithms. deep reinforcement learning ( deep RL ) algorithms CONJUNCTION Deep neuroevolution. approaches USED-FOR policy search. deep reinforcement learning ( deep RL ) algorithms USED-FOR policy search. deep reinforcement learning ( deep RL ) algorithms HYPONYM-OF approaches. Deep neuroevolution HYPONYM-OF approaches. Deep neuroevolution USED-FOR policy search. them PART-OF approach. ad hoc evolutionary algorithm CONJUNCTION goal exploration process. goal exploration process CONJUNCTION ad hoc evolutionary algorithm. Deep Deterministic Policy Gradient ( DDPG ) algorithm HYPONYM-OF sample efficient off - policy deep RL algorithm. goal exploration process CONJUNCTION Deep Deterministic Policy Gradient ( DDPG ) algorithm. Deep Deterministic Policy Gradient ( DDPG ) algorithm CONJUNCTION goal exploration process. ad hoc evolutionary algorithm CONJUNCTION Deep Deterministic Policy Gradient ( DDPG ) algorithm. Deep Deterministic Policy Gradient ( DDPG ) algorithm CONJUNCTION ad hoc evolutionary algorithm. Deep Deterministic Policy Gradient ( DDPG ) algorithm USED-FOR combinations. ad hoc evolutionary algorithm USED-FOR combinations. goal exploration process USED-FOR combinations. cross - entropy method ( CEM ) USED-FOR combination scheme. CEM - RL HYPONYM-OF method. sample efficiency EVALUATE-FOR CEM - RL. Generic are former, latter, and methods. OtherScientificTerm is hyper - parameter setting. Method are off - policy deep RL algorithm, and DDPG. Task is deep RL. ","This paper proposes a sample efficient off-policy deep RL algorithm that combines an evolutionary algorithm and a goal exploration process to improve sample efficiency in policy search. The proposed method is based on a cross-entropy method (CEM), which is a combination of DDPG and an ad hoc evolutionary algorithm. The experimental results show that the proposed method achieves better sample efficiency compared to the baselines. ","This paper proposes a sample efficient off-policy deep RL algorithm for policy search in the hyper-parameter setting. The authors propose a cross-entropy method (CEM-RL) to find the best combination of the three components of the DDPG algorithm (ad hoc evolutionary algorithm, goal exploration process, and Deep Deterministic Policy Gradient (DDPG) algorithm). The authors show that the proposed method is more sample efficient than the state-of-the-art in terms of sample efficiency. "
8570,SP:78b2eb326695da0b0cc4ba39a9206d11644a5e32,forecasting EVALUATE-FOR model. multivariate time series USED-FOR predictive model. forecasting CONJUNCTION temporal and variable level importance interpretation. temporal and variable level importance interpretation CONJUNCTION forecasting. hidden state matrix CONJUNCTION update process. update process CONJUNCTION hidden state matrix. IMV - LSTM USED-FOR variableswise hidden states. hidden state matrix USED-FOR IMV - LSTM. update process USED-FOR IMV - LSTM. summarization methods USED-FOR temporal and variable importance. mixture attention mechanism CONJUNCTION summarization methods. summarization methods CONJUNCTION mixture attention mechanism. mixture attention mechanism USED-FOR temporal and variable importance. real datasets EVALUATE-FOR IMV - LSTM. IMV - LSTM COMPARE baselines. baselines COMPARE IMV - LSTM. real datasets EVALUATE-FOR baselines. end - to - end framework USED-FOR forecasting. end - to - end framework USED-FOR knowledge extraction. forecasting CONJUNCTION knowledge extraction. knowledge extraction CONJUNCTION forecasting. It USED-FOR knowledge extraction. It USED-FOR forecasting. It USED-FOR end - to - end framework. multi - variate data USED-FOR knowledge extraction. OtherScientificTerm is target and exogenous variables. Generic is it. ,"This paper proposes a model for multi-variate time series forecasting. The proposed model is based on a LSTM-based model, where the hidden state matrix is represented by a mixture of target and exogenous variables. The model is trained using a mixture attention mechanism and a summarization method to capture the temporal and variable importance of the time series. Experiments show that the proposed model achieves state-of-the-art performance on several real-world datasets. ","This paper proposes an end-to-end model for multi-variate time series forecasting. The model is based on a multivariate time series model, where the time series is represented by a hidden state matrix, and the model is trained using a mixture attention mechanism. The proposed model is evaluated on a variety of datasets, and it is shown to outperform the state-of-the-art."
8579,SP:1c26660569b579f060f7b4a31e321c6d2356b928,"adversarial examples USED-FOR defenses. defenses USED-FOR adversarial attacks. feature smoothing HYPONYM-OF data augmentation method. feature smoothing USED-FOR neural network. virtual training data USED-FOR neural network. interpolation of features USED-FOR neural network. feature smoothing USED-FOR virtual data points. logit squeezing USED-FOR feature smoothing. adversarial and clean accuracy EVALUATE-FOR feature smoothing. weight decay CONJUNCTION mix up. mix up CONJUNCTION weight decay. logit squeezing CONJUNCTION weight decay. weight decay CONJUNCTION logit squeezing. label smoothing CONJUNCTION logit squeezing. logit squeezing CONJUNCTION label smoothing. mix up CONJUNCTION feature smoothing. feature smoothing CONJUNCTION mix up. weight decay CONJUNCTION feature smoothing. feature smoothing CONJUNCTION weight decay. feature smoothing USED-FOR unbiased estimation of the decision boundary. label smoothing CONJUNCTION weight decay. weight decay CONJUNCTION label smoothing. symmetrical assumptions CONJUNCTION label smoothing. label smoothing CONJUNCTION symmetrical assumptions. estimated variance FEATURE-OF unbiased estimation of the decision boundary. weight decay HYPONYM-OF methods. OtherScientificTerm are computational overhead, computational burden, and decision boundary. Material is MNIST and CIFAR10 datasets. Method is data augmentation methods. Generic is unified framework. ","This paper proposes a data augmentation method called feature smoothing to improve robustness against adversarial attacks. The proposed method is based on the observation that feature interpolation can be used as a way to improve the robustness to adversarial examples. The authors show that the proposed method can be combined with weight decay, label smoothing, and logit squeeze to improve adversarial robustness.  ","This paper proposes a new data augmentation method, called feature smoothing, to improve the robustness of neural networks against adversarial attacks. The proposed method is based on the idea of feature interpolation, where features are interpolated from the training data to the virtual training data. The authors show that the proposed method outperforms the state-of-the-art on MNIST and CIFAR-10 datasets. "
8588,SP:88d652f9e411dd3a2e9ad651d9011e579653c6aa,"deep convolutional neural network ( DCNN ) HYPONYM-OF deep and locally connected nonlinear network. theoretical framework USED-FOR networks. ReLU nonlinearity FEATURE-OF networks. framework USED-FOR disentangled representations. framework USED-FOR data distribution. gradient descent rules USED-FOR data distribution. Batch Norm HYPONYM-OF regularization techniques. teacher - student setting USED-FOR framework. Gaussian inputs CONJUNCTION independence of activation. independence of activation CONJUNCTION Gaussian inputs. independence of activation HYPONYM-OF unrealistic assumptions. Gaussian inputs HYPONYM-OF unrealistic assumptions. disentangled representations PART-OF deep networks. OtherScientificTerm are projection nature, and teacher ’s computational graph. ","This paper proposes a theoretical framework for learning disentangled representations in deep convolutional neural networks (DCNNs) with ReLU nonlinearity. The proposed framework is based on the teacher-student setting, where the teacher's computational graph is represented as a graph and the student's input is a Gaussian distribution over the input space. The authors show that under certain assumptions on the data distribution and the teacher’s computational graph, the proposed framework can learn disentanglement representations in DCNNs. ","This paper proposes a theoretical framework for disentangling representations of deep neural networks with ReLU nonlinearity. The main idea is to use a teacher-student setting where the teacher is a teacher and the student is a student. The teacher is given a set of data points, and a student is given the teacher’s computational graph, and the teacher uses the teacher's graph to learn the data distribution of the data. The student is then trained to learn a disentangled representation of the input data distribution.  Theoretical results are provided for the case of Gaussian inputs and independence of activation.  "
8597,SP:7842bbe0e2324cfd732db8745550733ccc3dfcdc,"Prefrontal cortex ( PFC ) USED-FOR behavior repertoire. connectivity CONJUNCTION human behavior formation process. human behavior formation process CONJUNCTION connectivity. Behavioral Module ( BM ) CONJUNCTION end - to - end training strategy. end - to - end training strategy CONJUNCTION Behavioral Module ( BM ). Behavioral Module ( BM ) PART-OF modular architecture of neural networks. end - to - end training strategy PART-OF modular architecture of neural networks. approach USED-FOR learning of behaviors. learning of behaviors CONJUNCTION preferences representation. preferences representation CONJUNCTION learning of behaviors. approach USED-FOR preferences representation. property USED-FOR user modeling. property USED-FOR recommendation tasks. user modeling CONJUNCTION recommendation tasks. recommendation tasks CONJUNCTION user modeling. video games playing EVALUATE-FOR method. independent learning of new behavior patterns USED-FOR network extendability. strategy USED-FOR transfer of newly learned BMs. Task are dialog agents, and personalized representations of different user states. OtherScientificTerm is BMs. ","This paper proposes a method to learn a behavioral module (BM) for dialog agents. The proposed method is based on the observation that BMs can be used to learn new behavior patterns, which are then used to improve the performance of the agent. The method is evaluated on a variety of tasks, including recommendation tasks and video games playing. ","This paper proposes a method to learn a behavioral module (BM) for dialog agents. The BM consists of two parts: (1) a pre-trained behavioral module and (2) an end-to-end training strategy. The first part learns the preferences representation of different user states, and the second part trains the BM to learn the preferences representations of different users states. The method is evaluated on a variety of tasks, including video games, recommendation tasks, and user modeling."
8606,SP:300c391ff644b6889cd9ae27cf0d162dfcdd4451,plastic changes in synaptic connectivity USED-FOR lifelong learning. neuromodulation USED-FOR changes. learning CONJUNCTION adaptation. adaptation CONJUNCTION learning. self - modifying abilities USED-FOR biological reinforcement learning. self - modifying abilities USED-FOR learning. self - modifying abilities USED-FOR adaptation. self - modifying abilities FEATURE-OF brain. brain USED-FOR learning. brain USED-FOR adaptation. neuromodulated plasticity USED-FOR artificial neural networks. gradient descent USED-FOR artificial neural networks. differentiable formulation USED-FOR neuromodulation of plasticity. neuromodulated plasticity USED-FOR neural networks. neuromodulated plasticity USED-FOR reinforcement learning and supervised learning tasks. neural networks USED-FOR reinforcement learning and supervised learning tasks. neuromodulated plastic LSTMs COMPARE LSTMs. LSTMs COMPARE neuromodulated plastic LSTMs. task EVALUATE-FOR LSTMs. task EVALUATE-FOR neuromodulated plastic LSTMs. task EVALUATE-FOR benchmark language modeling task. benchmark language modeling task EVALUATE-FOR neuromodulated plastic LSTMs. benchmark language modeling task EVALUATE-FOR LSTMs. differentiable neuromodulation of plasticity USED-FOR neural networks. Method is differentiable Hebbian plasticity. ,"This paper proposes to use differentiable Hebbian plasticity to improve the performance of neural networks in reinforcement learning tasks. The proposed method is based on a differentiable formulation of neuromodulation of plasticity, which can be used to train neural networks without gradient descent. Experiments show that the proposed method outperforms the state-of-the-art LSTMs on a language modeling task.","This paper proposes a differentiable formulation for neuromodulation of plasticity in neural networks. The authors show that the differentiable Hebbian plasticity of neural networks is differentiable and can be used to train neural networks for reinforcement learning and supervised learning tasks. They also show that this differentiable plasticity can be applied to a variety of tasks, including language modeling and reinforcement learning. "
8615,SP:1ab5d94d31e99351433436c026799c8aa597bf73,"quantization techniques USED-FOR inference latency / memory consumption. full precision model USED-FOR non - intrusive quantization technique. quantization training process COMPARE training process. training process COMPARE quantization training process. loss function USED-FOR reduced quantization error. binary quantization USED-FOR full precision accuracy. 2 bit quantization USED-FOR full precision accuracy. 1.5 bits hybrid model COMPARE TWN LSTM model. TWN LSTM model COMPARE 1.5 bits hybrid model. WikiText-2 EVALUATE-FOR TWN LSTM model. Method are Deep Neural Networks, and binary model. Generic is techniques. Material are CIFAR dataset, and ImageNet. ","This paper proposes a non-intrusive quantization technique to reduce the inference latency and memory consumption of deep neural networks. The proposed method is based on a binary quantization method, where the quantization error is computed using the loss function of the full precision model. The authors show that the proposed method outperforms previous methods on ImageNet and WikiText-2.  ","This paper proposes a new quantization technique for training a full precision model for image quantization. The proposed method is based on a new loss function, which can be used to reduce the quantization error. The authors show that the proposed method outperforms the state-of-the-art on WikiText-2 and ImageNet datasets. "
8624,SP:0876b1d9a6d664808ca1ab15865679fbf638267e,"class - irrelevant properties USED-FOR style. method USED-FOR content embedding. deep metric - learning technique USED-FOR method. deep metric - learning technique USED-FOR content embedding. content encoder PART-OF variational autoencoder ( VAE ). content encoder CONJUNCTION to - be - trained style encoder. to - be - trained style encoder CONJUNCTION content encoder. to - be - trained style encoder PART-OF variational autoencoder ( VAE ). auxiliary loss CONJUNCTION leakage filtering. leakage filtering CONJUNCTION auxiliary loss. style information USED-FOR reconstruction. style information PART-OF content representation. auxiliary loss PART-OF method. leakage filtering PART-OF method. content representation USED-FOR style representation. method USED-FOR data - set augmentation. pose CONJUNCTION expression. expression CONJUNCTION pose. expression CONJUNCTION hairstyle. hairstyle CONJUNCTION expression. lighting CONJUNCTION pose. pose CONJUNCTION lighting. decompositions USED-FOR classification. Recombinations USED-FOR creative exercise. Recombinations USED-FOR data set augmentation. approach USED-FOR content - style decomposition and recombination. specific domain knowledge USED-FOR approaches. human body pose HYPONYM-OF specific domain knowledge. leakage filtering USED-FOR STOC. objective PART-OF STOC. leakage filtering HYPONYM-OF objective. supervised training USED-FOR style and content representations. STOC USED-FOR content - style recombination. Material are visual domains, masterworks of art, and Open - Ended Content. Method are domain - independent method, and Decompositions. OtherScientificTerm are VAE reconstruction loss, content, within - class variation, betweenand within - class variation, and musical composition. Task are few - shot learning tasks, face - recognition task, and emotion - recognition task. ","This paper proposes Open-ended content-style decomposition (STOC), a method for learning style representations for few-shot image classification tasks. The method is based on the idea that style information in the content representation is irrelevant to class-irrelevant properties in the style encoder, and that the style representation should be learned independently of the content. To this end, the authors propose to use a combination of style and content encoders in a VAE to learn the style representations. The authors show that the proposed method achieves state-of-the-art performance on the face recognition task and emotion-recognition task.","This paper proposes a method for content-style decomposition and recombination in few-shot learning tasks. The method is based on the Open-ended content (STOC) framework, which is a deep metric-learning technique for content embedding. The main idea is to learn a style encoder and a content encoder, and then combine the style and content encoders in a variational autoencoder (VAE) to reconstruct the content. The style information is extracted from the content and the content information from the VAE, and the style information and content information are combined in a style-aware way. The authors show that the method can be applied to a variety of tasks, such as face recognition, emotion recognition, and music composition."
8633,SP:d37e15cde7765fca87595a242f0a4511b3346d46,method USED-FOR deep reinforcement learning ( deep RL ) training. deep reinforcement learning ( deep RL ) training USED-FOR problems. state - action permissibility ( SAP ) FEATURE-OF problems. permissibility PART-OF SAP. deep RL algorithms USED-FOR state - action exploration. SAP property PART-OF deep RL algorithms. SAP property USED-FOR state - action exploration. SAP guidance USED-FOR training. ,"This paper studies the problem of state-action permissibility (SAP) in deep reinforcement learning. The authors propose a new notion of ""permissibility"", which they term ""state-action flexibility"", and show that it is an important property of deep RL algorithms. They show that this property can be used to improve the performance of RL algorithms by allowing for more exploration in the state space. They also show how to use this property to improve deep RL training.","This paper studies the problem of state-action permissibility (SAP) in deep reinforcement learning (DRL) training. The authors propose a new method for training deep RL algorithms that can be applied to a variety of DRL problems. The main contribution of the paper is to provide a theoretical analysis of the properties of SAP in DRL training. In particular, the authors show that under certain conditions, DRL algorithms can be trained with SAP guidance. They also provide an empirical evaluation of the effectiveness of the proposed method."
8642,SP:20015d8b60e13300586b67c281858cbe28825c48,"random weights USED-FOR weight - tied multilayer vanilla autoencoders. random deep weight - tied autoencoder model USED-FOR approximate inference. deep autoencoders COMPARE shallow counterparts. shallow counterparts COMPARE deep autoencoders. layer - wise pre - training CONJUNCTION batch normalization. batch normalization CONJUNCTION layer - wise pre - training. batch normalization HYPONYM-OF techniques. layer - wise pre - training HYPONYM-OF techniques. tanh activation USED-FOR deep autoencoder. OtherScientificTerm are large dimensions, phase transition phenomena, reversibility, and Lipschitz activations. Task is training initialization practice. Method is analytical techniques. ","This paper studies the phase transition phenomena of deep autoencoders in the presence of weight-tied multilayer autoencoder models. The authors show that when the number of layers is large enough, the training process converges to a phase transition. They show that this phenomenon can be explained in terms of the Lipschitz constant of the activations of the layers. They also show that the phase transitions can be reversed by adding random weights to the weights of the layer weights.  The authors then show that layer-wise pre-training and batch normalization can be used to improve the performance of the model.","This paper proposes a method for training deep autoencoder models with weight-tied multilayer neural networks. The main idea is to use a tanh activation for training the model, which is based on the Lipschitz activation. The authors show that the tanh activations can be used to improve the performance of the model in terms of the phase transition phenomena. They also provide a theoretical analysis of the reversibility of tanh-based activations."
8651,SP:91764f80dbe2401ade38b35a8253ba05f0f86386,"search problem USED-FOR construction of adversarial images. model evaluations USED-FOR sporadic feedback. low frequency component PART-OF discrete cosine transform ( DCT ). iterative principle USED-FOR search strategy. iterative principle USED-FOR algorithm. method USED-FOR targeted and untargeted attacks. query efficiency EVALUATE-FOR method. median queries USED-FOR Google Cloud Vision. algorithm USED-FOR adversarial black - box attacks. PyTorch code USED-FOR it. Generic is model. Task is Model evaluations. Metric is adversarial loss. Material are ResNet-50, and adversarial ImageNet image. ","This paper proposes to use discrete cosine transform (DCT) to construct adversarial images for black-box image generation. The proposed method is based on the observation that the low-frequency component of DCT is responsible for the sporadic feedback in the adversarial image generation process. The authors propose to use this low frequency component as a low-rank estimator of the loss function of the DCT, and use this estimator to estimate the probability that the input image will be adversarially perturbed. This estimator is then used to compute the median query for the target image.  The authors show that the proposed method can be used for both targeted and untargeted adversarial attacks. They also show that their method can improve the query efficiency of median queries for Google Cloud Vision. ","This paper proposes a new adversarial black-box attack method for the problem of generating adversarial images. The key idea is to use the discrete cosine transform (DCT) as the low-frequency component of the adversarial input image. The authors propose an iterative search strategy for the search problem, which is based on the iterative principle. They show that the proposed method can be applied to both targeted and untargeted attacks. The proposed method is tested on two datasets: ResNet-50 and ImageNet-ImageNet."
8660,SP:fc20ae0fbf57a1ce489c04b85c7c2f4c93dc2450,"temporal abstractions USED-FOR curse of dimensionality. Hierarchical Reinforcement Learning USED-FOR temporal abstractions. method USED-FOR temporal abstractions. options framework HYPONYM-OF hierarchical framework. heuristics USED-FOR Option discovery. method COMPARE discovering bottlenecks. discovering bottlenecks COMPARE method. method USED-FOR bottlenecks. Successor options HYPONYM-OF model. Successor representations USED-FOR Successor options. Successor representations USED-FOR model. pseudo - reward USED-FOR intra - option policies. primitive actions USED-FOR Successor representations. Incremental Successor options model USED-FOR options. grid worlds CONJUNCTION complex high dimensional environments. complex high dimensional environments CONJUNCTION grid worlds. complex high dimensional environments EVALUATE-FOR approach. Deepmind - Lab HYPONYM-OF complex high dimensional environments. grid worlds EVALUATE-FOR approach. OtherScientificTerm are task - agnostic transferable skills, bottleneck states, landmark ” sub - goals, well connected regions, and sub - goals. Task is discovering bottleneck states. ","This paper proposes a method for learning to discover sub-goal sub-goals in a hierarchical reinforcement learning setting. The idea is to learn a hierarchical model of options, where each option represents a sequence of primitive actions that can be used as a pseudo-reward to guide the exploration of new options. The method is evaluated on grid worlds and high-dimensional environments.   ","This paper proposes a Hierarchical Reinforcement Learning (HRL) approach for finding sub-goal sub-goals that are transferable across tasks. The key idea is to use a hierarchical options framework to discover sub goals that are well connected and well connected regions. The approach is evaluated on a variety of environments, including grid worlds and high dimensional environments."
8669,SP:12a172c1e2892d016b37932acfc48dcb56874a89,"probabilistic distributions USED-FOR domain division. problem USED-FOR recognition tasks. Open Set Learning ( OSL ) HYPONYM-OF recognition tasks. probabilistic way USED-FOR decision boundary. domain division algorithm USED-FOR recognition tasks. domain USED-FOR recognition tasks. bootstrapping CONJUNCTION KolmogorovSmirnov ( K - S ) Test. KolmogorovSmirnov ( K - S ) Test CONJUNCTION bootstrapping. statistical tools USED-FOR decision boundary. KolmogorovSmirnov ( K - S ) Test USED-FOR decision boundary. KolmogorovSmirnov ( K - S ) Test HYPONYM-OF statistical tools. bootstrapping HYPONYM-OF statistical tools. uncertain domain PART-OF framework. OSL and G - ZSL benchmarks EVALUATE-FOR approach. Method are classifiers, and WSVM. OtherScientificTerm is known, unknown and uncertain domains. ","This paper proposes a probabilistic approach for domain division in Open Set Learning (OSL) and G-ZSL, where the goal is to partition the training set into known, unknown and uncertain domains. The approach is based on bootstrapping and Kolmogorov-Smirnov (K-S) test, which is used to estimate the decision boundary between the known and unknown domains. Experiments show that the proposed approach achieves state-of-the-art performance on OSL and GZSL tasks.",This paper proposes a probabilistic approach for domain division in Open Set Learning (OSL) and G-ZSL. The authors propose a new approach to solve the problem of domain division. The proposed approach is based on the Kolmogorov-Smirnov (K-S) Test and bootstrapping. They show that the proposed approach outperforms the state-of-the-art on both OSL and GZSL benchmarks. 
8678,SP:28bcf7c6a4673e9ec2b4ebed09839d85188e0b2a,"neural network USED-FOR classification and regression. softmax cross - entropy CONJUNCTION mean squared error. mean squared error CONJUNCTION softmax cross - entropy. maximum margin separation CONJUNCTION simplicity ( Occam ’s Razor ). simplicity ( Occam ’s Razor ) CONJUNCTION maximum margin separation. mean squared error HYPONYM-OF solutions. softmax cross - entropy HYPONYM-OF solutions. simplicity ( Occam ’s Razor ) HYPONYM-OF inductive structures. maximum margin separation HYPONYM-OF inductive structures. polar prototype networks HYPONYM-OF networks. polar prototypes USED-FOR structure. maximal separation FEATURE-OF they. angular distances USED-FOR training. training USED-FOR regression. higher - dimensional outputs USED-FOR regression. polar interpolation USED-FOR training. large margin separation CONJUNCTION semantic class structure. semantic class structure CONJUNCTION large margin separation. semantic class structure USED-FOR polar prototype networks. large margin separation USED-FOR polar prototype networks. classification COMPARE network methods. network methods COMPARE classification. regression CONJUNCTION classification. classification CONJUNCTION regression. OtherScientificTerm are layout structures, layout, polar prototype, hypersphere, semantic priors, class prototypes, prototypes, and output dimensions. Task is minimizing angular distances. ","This paper proposes a novel method for training neural networks for classification and regression. The proposed method is based on the idea of polar prototypes, which is an extension of the polar interpolation method. The main idea is to learn a set of prototypes for each class in the network, which are then used to compute the angular distances between the class prototypes and the output dimensions. The angular distances are used to train the network to minimize the mean squared error. The method is evaluated on regression and classification tasks. ","This paper proposes a new class of inductive networks based on polar prototypes. The proposed network is based on the polar prototype network, which is an extension of the polar interpolation network. The main contribution of the paper is a theoretical analysis of the properties of polar prototypes in terms of the angular distance between the input and output dimensions. The authors show that polar prototypes can be viewed as a hypersphere, and that they can be used as an inductive structure for class prototypes. They also show that the proposed network can be applied to classification and regression tasks."
8687,SP:d1034342785d133cf8372b8624897963cc2ee83a,"Reinforcement learning ( RL ) agents USED-FOR features. reward function USED-FOR features. it EVALUATE-FOR idea. it USED-FOR proof - of - concept environments. proof - of - concept environments EVALUATE-FOR idea. Maximum Causal Entropy IRL USED-FOR algorithm. Generic are preferences, and robot. OtherScientificTerm are implicit preference information, and side effects. ","This paper proposes a method to learn a reward function that is independent of the reward function. The reward function is defined as the sum of a set of features extracted from the environment. The authors show that this reward function can be used to train an RL agent that learns to select features that are close to the current reward function, and that the agent can then use this learned reward function to select the next state to explore. They show that the proposed method is able to learn such features in an efficient way, and achieves state-of-the-art performance in a number of experiments. ","This paper proposes a method to learn a reward function that maximizes the maximum entropy of the reward function. The reward function is defined as the sum of a set of features that are learned by the agent, and the agent is trained to maximize the entropy of these features. The main idea is that the reward should be maximized in a way that minimizes the total entropy of all features in the environment. The authors show that this maximization of entropy can be achieved by minimizing the Maximum Causal Entropy IRL (MCEI) algorithm. They also show that the MCEI algorithm can be used in a proof-of-concept setting. "
8696,SP:417a4e0acee699b3e004ad30d0ecf533a9ed987e,method USED-FOR dependency structure between latent variables. deep generative models CONJUNCTION probabilistic graphical models. probabilistic graphical models CONJUNCTION deep generative models. deep generative models PART-OF modeling and inference framework. probabilistic graphical models PART-OF modeling and inference framework. latent variable space FEATURE-OF variational autoencoder ( VAE ). flexible dependency structure FEATURE-OF Bayesian network. Bayesian network USED-FOR variational autoencoder ( VAE ). Bayesian network USED-FOR latent variable space. network parameters CONJUNCTION variational parameters. variational parameters CONJUNCTION network parameters. variational parameters CONJUNCTION latent topology. latent topology CONJUNCTION variational parameters. single objective USED-FOR latent topology. single objective USED-FOR variational parameters. single objective USED-FOR network parameters. latent variable values FEATURE-OF top - down and bottom - up reasoning. top - down and bottom - up reasoning USED-FOR Inference. sampling procedure USED-FOR Inference. MNIST CONJUNCTION Omniglot. Omniglot CONJUNCTION MNIST. Omniglot CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION Omniglot. Omniglot EVALUATE-FOR framework. CIFAR-10 EVALUATE-FOR framework. MNIST EVALUATE-FOR framework. structured variational autoencoder baselines COMPARE model. model COMPARE structured variational autoencoder baselines. Method is deep latent variable models. OtherScientificTerm is latent variable structures. ,"This paper proposes a Bayesian variational autoencoder (VAE) model for deep latent variable models. The proposed model is based on the Bayesian neural network (BN) model, which is an extension of Bayesian generative models (BPGM). The main idea of the model is to model the dependency structure between latent variables in the latent variable space as a variational distribution over the variational parameters of the BPGM. The model is trained using a sampling procedure based on top-down and bottom-up reasoning. Experiments on MNIST, Omniglot and CIFAR-10 show the effectiveness of the proposed model.  ","This paper proposes a method to model the dependency structure between latent variables in a variational autoencoder (VAE) model. The proposed method is based on a Bayesian network, where the latent variable space is represented by a Bayes-based variational network. The authors propose a sampling procedure for top-down and bottom-up reasoning, and show that the proposed method outperforms other VAE baselines on MNIST and Omniglot datasets. "
8705,SP:976dedab53e69610692a563382ada1dbb82c1e9d,"interconnected neurons PART-OF dynamical neural network. numerical solutions USED-FOR mathematical optimization or learning problems. computational properties FEATURE-OF It. it CONJUNCTION massively parallel computer architecture. massively parallel computer architecture CONJUNCTION it. massively parallel computer architecture USED-FOR power and throughput efficiency. local memory HYPONYM-OF local information. dynamical network USED-FOR gradients. top - down feedback CONJUNCTION contrastive learning. contrastive learning CONJUNCTION top - down feedback. dynamical network USED-FOR ` 1 - minimizing dictionary learning problem. top - down feedback USED-FOR dynamical network. contrastive learning USED-FOR dynamical network. gradients USED-FOR learning. spiking neurons USED-FOR dynamical network. OtherScientificTerm is state space. Method are computational system, and learning process. Task is dictionary learning problems. ",This paper proposes to use a dynamical neural network to solve dictionary learning problems. The proposed method is based on the idea that the gradients of the weights of the neurons in the dynamical network can be used to estimate the gradient of the solution of the dictionary learning problem. The authors show that the proposed method achieves state-of-the-art performance on a dictionary learning task.   ,"This paper proposes a new dynamical neural network (DNN) architecture for solving dictionary learning problems. The main idea is to use a dynamical network to solve a dictionary learning problem, where the goal is to find a solution that minimizes the number of entries in the dictionary. The proposed method is based on top-down feedback, contrastive learning, and spiking neurons. The authors show that the proposed method can achieve state-of-the-art performance in terms of computational efficiency and speed. "
8714,SP:f45117a6beaeb86a70b1380b4fac3cfba37fb892,"spatial pyramid structure CONJUNCTION encoder - decoder structure. encoder - decoder structure CONJUNCTION spatial pyramid structure. semantic image segmentation CONJUNCTION lane detection. lane detection CONJUNCTION semantic image segmentation. spatial pyramid structure USED-FOR nets. encoder - decoder structure FEATURE-OF nets. nets USED-FOR lane detection. nets USED-FOR semantic image segmentation. weak visual appearance CONJUNCTION prior information. prior information CONJUNCTION weak visual appearance. multi - scale context CONJUNCTION pixel - level accuracy. pixel - level accuracy CONJUNCTION multi - scale context. network USED-FOR lane detection. encoder - decoders module USED-FOR lane detection. evaluation methods EVALUATE-FOR lane detection. Method are Convolutional neural networks ( CNNs ), and encoder - decoders nets. Task are lane detection task, and model - based lane detection. Generic is methods. ",This paper studies the problem of model-based lane detection in semantic image segmentation. The authors propose to use an encoder-decoder architecture with a spatial pyramid structure and a multi-scale context to improve the performance of lane detection. The proposed method achieves state-of-the-art performance on the semantic segmentation task.  ,This paper proposes a new method for lane detection based on the spatial pyramid structure of CNNs. The proposed method is based on an encoder-decoder structure that is similar to the one used for semantic image segmentation. The authors show that the proposed method outperforms the state-of-the-art in terms of pixel-level accuracy and multi-scale context. They also show that their method is able to outperform the state of the art on the task of lane detection. 
8723,SP:68b0a10ca06df74612d0753cc3f3ddddde806035,policy COMPARE off - policy training data. off - policy training data COMPARE policy. supervised learning and online learning settings COMPARE batch contextual bandit learning. batch contextual bandit learning COMPARE supervised learning and online learning settings. ad platforms CONJUNCTION recommendation systems. recommendation systems CONJUNCTION ad platforms. batch learning setting USED-FOR online and interactive systems. ad platforms HYPONYM-OF online and interactive systems. recommendation systems HYPONYM-OF online and interactive systems. Policy Optimizer USED-FOR Exponential Models ( POEM ). Inverse Propensity Scoring ( IPS ) CONJUNCTION Policy Optimizer. Policy Optimizer CONJUNCTION Inverse Propensity Scoring ( IPS ). Policy Optimizer HYPONYM-OF approaches. Inverse Propensity Scoring ( IPS ) HYPONYM-OF approaches. inverse propensity weights USED-FOR approaches. Maximum Likelihood Inverse Propensity Scoring ( MLIPS ) USED-FOR batch learning. approach USED-FOR batch learning. logged bandit feedback USED-FOR Maximum Likelihood Inverse Propensity Scoring ( MLIPS ). logged bandit feedback USED-FOR batch learning. historical policy USED-FOR inverse propensity weights. logged action - context pairs USED-FOR maximum likelihood surrogate policy. MLIPS COMPARE IPS. IPS COMPARE MLIPS. nonasymptotic mean squared error EVALUATE-FOR IPS. nonasymptotic mean squared error EVALUATE-FOR MLIPS. surrogate policy COMPARE historical policy. historical policy COMPARE surrogate policy. large - scale ad placement dataset EVALUATE-FOR MLIPS. multi - label classification problems CONJUNCTION large - scale ad placement dataset. large - scale ad placement dataset CONJUNCTION multi - label classification problems. multi - label classification problems EVALUATE-FOR MLIPS. surrogate policy technique COMPARE error reduction techniques. error reduction techniques COMPARE surrogate policy technique. surrogate policy technique USED-FOR approaches. OtherScientificTerm is logged feedback. Metric is mean squared error. ,"This paper proposes a new method for batch contextual bandit learning in online and interactive systems. The proposed method is based on Maximum Likelihood Inverse Propensity Scoring (MLIPS), which uses logged bandit feedback to learn a maximum likelihood surrogate policy from logged action-context pairs and inverse propensity weights from historical policy. Theoretical analysis shows that the proposed method outperforms previous methods in terms of non-asymptotic mean squared error. Experiments show that MLIPS achieves better performance than previous methods.","This paper proposes a new method for batch contextual bandit learning based on logits. The proposed method is based on Maximum Likelihood Inverse Propensity Scoring (MLIPS), which is an extension of the inverse propensity scoring (IPS) framework. The main idea of MLIPS is to use the logits as a surrogate policy for the historical policy, which is then used to train a maximum likelihood surrogate policy based on the logged action-context pairs. The authors show that MLIPS outperforms IPS in terms of non-asymptotic mean squared error. "
8732,SP:8e0ed65c5dded23b34798499b2436b24422fd729,learning framework USED-FOR few - shot classification tasks. Meta - learning USED-FOR learning framework. Meta - learning USED-FOR few - shot classification tasks. meta - learner USED-FOR model optimization. parameter initialization CONJUNCTION similarity metric. similarity metric CONJUNCTION parameter initialization. model optimization CONJUNCTION parameter initialization. parameter initialization CONJUNCTION model optimization. meta - learner PART-OF meta - learning methods. individualized feature embedding USED-FOR classifying. feature embedding USED-FOR individualized feature space. kernel generator USED-FOR feature embedding. feature embedding USED-FOR query images. kernel generator USED-FOR meta - learner. meta - knowledge USED-FOR convolutional kernels. kernel generator USED-FOR convolutional kernels. training USED-FOR convolutional kernels. meta - knowledge USED-FOR kernel generator. few - shot classification data sets EVALUATE-FOR method. Omniglot CONJUNCTION miniImageNet. miniImageNet CONJUNCTION Omniglot. miniImageNet EVALUATE-FOR method. miniImageNet HYPONYM-OF few - shot classification data sets. Omniglot HYPONYM-OF few - shot classification data sets. Method is fine - tuning. , in few-shot classification tasks. This paper proposes a meta-learning approach to improve the performance of meta-learners. The proposed approach is based on the idea of using meta-knowledge to learn the convolutional kernels for each query image. The method is evaluated on Omniglot and mini-ImageNet. ,"This paper proposes a meta-learning framework for few-shot classification. The proposed method is based on the idea of individualized feature embedding, which is used to learn the individualized features for each query image. The feature embeddings are learned by using a kernel generator that is trained with meta-knowledge. The method is evaluated on Omniglot, miniImageNet, and mini-ImageNet."
8741,SP:faa3f7ffdcfb6e3b8ec0421193dae3d9987b015c,backpropagation HYPONYM-OF gradient - based learning algorithms. gradient - based learning algorithms USED-FOR Deep artificial neural networks ( DNNs ). Q - learning CONJUNCTION policy gradients. policy gradients CONJUNCTION Q - learning. Evolution strategies ( ES ) COMPARE backprop - based algorithms. backprop - based algorithms COMPARE Evolution strategies ( ES ). policy gradients USED-FOR deep reinforcement learning ( RL ) problems. backprop - based algorithms USED-FOR deep reinforcement learning ( RL ) problems. policy gradients HYPONYM-OF backprop - based algorithms. Q - learning HYPONYM-OF backprop - based algorithms. it USED-FOR stochastic gradient descent. ES HYPONYM-OF gradient - based algorithm. finite - difference approximation of the gradient HYPONYM-OF operation. operation USED-FOR it. operation USED-FOR stochastic gradient descent. non - gradient - based evolutionary algorithms USED-FOR DNN scales. Atari CONJUNCTION humanoid locomotion. humanoid locomotion CONJUNCTION Atari. it USED-FOR hard deep RL problems. humanoid locomotion HYPONYM-OF hard deep RL problems. Atari HYPONYM-OF hard deep RL problems. Deep GA USED-FOR networks. free parameters FEATURE-OF networks. evolutionary algorithm USED-FOR neural networks. ES CONJUNCTION GA. GA CONJUNCTION ES. DNNs CONJUNCTION novelty search. novelty search CONJUNCTION DNNs. A3C CONJUNCTION ES. ES CONJUNCTION A3C. novelty search USED-FOR exploration. DQN CONJUNCTION A3C. A3C CONJUNCTION DQN. DNNs USED-FOR high - dimensional problem. reward - maximizing algorithms USED-FOR high - dimensional problem. GA HYPONYM-OF reward - maximizing algorithms. ES HYPONYM-OF reward - maximizing algorithms. DQN HYPONYM-OF reward - maximizing algorithms. A3C HYPONYM-OF reward - maximizing algorithms. A3C CONJUNCTION DQN. DQN CONJUNCTION A3C. ES CONJUNCTION A3C. A3C CONJUNCTION ES. Deep GA,"This paper proposes an evolutionary algorithm for deep reinforcement learning (DRL) problems. The main idea is to use a finite difference approximation of the gradient of the reward function to learn a policy gradient, which is then used to update the parameters of the network. The authors show that the proposed method is able to learn faster than gradient-based methods such as Q-learning and policy gradient descent (PGD). The authors also show that it is possible to scale up the number of parameters of DNNs in terms of the size of the training set.",This paper proposes a new evolutionary algorithm for deep neural networks (DNNs) based on stochastic gradient descent. The main idea is to use a finite-difference approximation of the gradient of a DNN to learn a reward-maximizing algorithm. The authors show that the proposed algorithm outperforms the state-of-the-art gradient-based evolutionary algorithms on Atari and humanoid locomotion.
8750,SP:dfdbe3267a8160f24746884cdf5297993e424231,"rewards USED-FOR learning. episodic memory USED-FOR novelty bonus. episodic memory USED-FOR curiosity method. VizDoom CONJUNCTION DMLab. DMLab CONJUNCTION VizDoom. DMLab CONJUNCTION MuJoCo. MuJoCo CONJUNCTION DMLab. VizDoom FEATURE-OF visually rich 3D environments. visually rich 3D environments EVALUATE-FOR approach. agent COMPARE curiosity method. curiosity method COMPARE agent. agent COMPARE ICM. ICM COMPARE agent. navigational tasks EVALUATE-FOR agent. curiosity method COMPARE ICM. ICM COMPARE curiosity method. VizDoom CONJUNCTION DMLab. DMLab CONJUNCTION VizDoom. ant USED-FOR MuJoCo. curiosity module PART-OF ant. OtherScientificTerm are Rewards, sparsity, curious behaviour, real task reward, environment dynamics, and first - person - view curiosity. Method are reinforcement learning algorithms, and RL algorithms. ","This paper proposes a novel novelty bonus for reinforcement learning based on episodic memory. Specifically, the novelty bonus is computed based on the observation of the agent’s first-person view of the environment. The novelty bonus can be computed using an episodic replay buffer that stores information from past experiences. The paper also proposes a curiosity module that can be added to existing reinforcement learning algorithms. Experiments on VizDoom, DMLab and MuJoCo show that the proposed novelty bonus improves the performance of RL algorithms.","This paper proposes a new method for learning a novelty bonus reward in reinforcement learning. The novelty bonus is an episodic memory that is stored in the learner's episodic history, which is then used to improve the performance of the agent on a variety of tasks. This novelty bonus can be used as an additional reward for the agent to improve its performance on a number of tasks, such as VizDoom, DMLab, and MuJoCo. Experiments show that the novelty bonus improves the agent's performance on all three tasks. "
8759,SP:1e58a1c5344d1b5b7c8a40210a243700bd933d65,representation USED-FOR transition models. complex uncertain domains FEATURE-OF transition models. relational rules USED-FOR representation. iterative greedy algorithm USED-FOR deictic references. Feed - forward neural networks USED-FOR transition distribution. strategy COMPARE monolithic transition model. monolithic transition model COMPARE strategy. simulated domain EVALUATE-FOR monolithic transition model. OtherScientificTerm is rule. ,"This paper studies the problem of learning representations for transition models in complex uncertain domains. The authors propose to use relational rules to represent the transition distribution in the domain. The main idea is to use deictic references to model the deformation of the transition model. The proposed method is based on a feed-forward neural network, which is trained with an iterative greedy algorithm. Experiments on simulated data show that the proposed method outperforms the state-of-the-art methods.",This paper proposes an iterative greedy algorithm for representing the transition distribution of a transition model in a complex uncertain domain. The main idea is to use the deictic references of the transition model to represent the distribution of transition rules in a relational way. The authors propose a feed-forward neural network (FNN) architecture to learn the transition representation. They show that the proposed method outperforms the state-of-the-art monolithic transition model. 
8768,SP:8ce00a3fedbf54a7f2c1ff414511cbb7d59b4597,"INVASE HYPONYM-OF instance - wise feature selection method. selector network CONJUNCTION predictor network. predictor network CONJUNCTION selector network. predictor network CONJUNCTION baseline network. baseline network CONJUNCTION predictor network. neural networks CONJUNCTION selector network. selector network CONJUNCTION neural networks. baseline network USED-FOR selector network. actor - critic methodology USED-FOR baseline network. actor - critic methodology USED-FOR INVASE. neural networks PART-OF INVASE. predictor network PART-OF INVASE. selector network PART-OF INVASE. baseline network PART-OF INVASE. methodology USED-FOR INVASE. INVASE COMPARE state - of - the - art benchmarks. state - of - the - art benchmarks COMPARE INVASE. synthetic and real data experiments EVALUATE-FOR INVASE. Material is big data. OtherScientificTerm is features. Task are global feature selection, and instance - wise feature selection. Generic is state - of - the - art methods. ","This paper proposes a novel instance-wise feature selection method called INVASE, which combines a selector network, a predictor network, and a baseline network. The selector network is trained using an actor-critic approach, and the predictor network is used to select features from a set of instances. The proposed method is evaluated on synthetic and real data experiments. ","This paper proposes a new feature selection method for instance-wise feature selection. The proposed method is based on the actor-critic approach, where the selector and the predictor are trained using a combination of neural networks. The method is evaluated on both synthetic and real data experiments. "
8777,SP:b91d6c33349df0bb6cb7e1c5e9433f0d4744b4da,"per - pixel annotations USED-FOR supervised models. semantic segmentation HYPONYM-OF Predicting structured outputs. convolutional neural networks HYPONYM-OF supervised models. per - pixel annotations USED-FOR Predicting structured outputs. annotations USED-FOR model finetuning. disentangled space USED-FOR discriminative feature representations of patches. label histograms USED-FOR discriminative feature representations of patches. adversarial learning scheme USED-FOR feature representations. representations USED-FOR guidance. global alignment process CONJUNCTION patch - level alignment. patch - level alignment CONJUNCTION global alignment process. global alignment process USED-FOR framework. semantic segmentation EVALUATE-FOR framework. patch - level alignment USED-FOR framework. Generic are models, and benchmark datasets. Task is annotation. Method is domain adaptation method. ",This paper proposes a method to improve the performance of semantic segmentation models using per-pixel annotations. The proposed method is based on a disentangled space and an adversarial learning scheme to learn the discriminative feature representations of patches. A global alignment process and patch-level alignment are used to improve model performance. Experiments show that the proposed method achieves state-of-the-art performance on a variety of benchmark datasets.,This paper proposes a framework for learning per-pixel annotations for semantic segmentation. The authors propose a disentangled space for learning the discriminative feature representations of patches. They use a global alignment process and patch-level alignment to learn the feature representations. They also propose an adversarial learning scheme to improve the accuracy of the representations. The proposed method is evaluated on a variety of benchmark datasets.
8786,SP:00922af13a21464cbc4cd7b34c196dd4f86c9247,"optimistic algorithms USED-FOR AMSGrad. AMSGrad CONJUNCTION Adam. Adam CONJUNCTION AMSGrad. optimistic algorithms USED-FOR Adam. predictability of gradients USED-FOR optimistic algorithms. momentum method CONJUNCTION adaptive gradient method. adaptive gradient method CONJUNCTION momentum method. algorithms USED-FOR OPTIMISTIC ONLINE LEARNING. adaptive gradient method CONJUNCTION algorithms. algorithms CONJUNCTION adaptive gradient method. algorithms USED-FOR algorithms. adaptive gradient method USED-FOR algorithms. momentum method USED-FOR algorithms. Method are optimization algorithms, and deep neural nets. OtherScientificTerm is mini - batch of stochastic gradients. Task is online learning literature. ","This paper studies online learning with stochastic gradients in deep neural networks. In particular, the authors propose two algorithms: AMSGrad and Adam. The main contribution of the paper is a theoretical analysis of the predictability of the gradients. The authors show that the predictable gradients can be used to improve the performance of optimistic online learning algorithms.  ",This paper studies the problem of online online learning with optimistic algorithms. The main contribution of the paper is a theoretical analysis of the predictability of gradients in online learning. The authors show that optimistic online learning algorithms with optimistic gradients are more likely to be accurate than pessimistic algorithms with pessimistic gradients. They also show that adaptive gradient method and momentum method can be used to improve the performance of the optimistic algorithms in the online learning literature. 
8795,SP:52228b48f2776d57dd422edb33b82e247f056b75,benchmarks USED-FOR image classifier robustness. classifiers USED-FOR safety - critical applications. benchmark USED-FOR corruption robustness topic. IMAGENET - C USED-FOR corruption robustness topic. IMAGENET - C HYPONYM-OF benchmark. dataset EVALUATE-FOR classifier. common perturbations FEATURE-OF classifier ’s robustness. IMAGENET - P HYPONYM-OF dataset. common corruptions CONJUNCTION perturbations. perturbations CONJUNCTION common corruptions. benchmark USED-FOR perturbations. perturbations COMPARE worst - case adversarial perturbations. worst - case adversarial perturbations COMPARE perturbations. common corruptions FEATURE-OF benchmark. AlexNet classifiers CONJUNCTION ResNet classifiers. ResNet classifiers CONJUNCTION AlexNet classifiers. relative corruption robustness EVALUATE-FOR ResNet classifiers. relative corruption robustness EVALUATE-FOR AlexNet classifiers. common perturbation robustness EVALUATE-FOR bypassed adversarial defense. Generic is networks. ,"This paper proposes a new benchmark for image classifier robustness. The proposed benchmark is based on ImageNet-C, which is an image classification dataset with common corruptions and perturbations. The authors show that common perturbation robustness is a measure of the classifier’s robustness against adversarial attacks. They also show that the adversarial robustness of a classifier is related to its robustness to common perturbs. ","This paper proposes a new benchmark for improving the robustness of image classifiers against common adversarial perturbations. The proposed benchmark is based on the IMAGENET-C dataset, which consists of a set of common corruptions and adversarial attacks. The authors show that the common perturbation robustness is better than the worst-case adversarial robustness. They also show that a bypassed adversarial defense can be used to improve the classifier's robustness against common perturbs. "
8804,SP:20358ea0f769e6ea9222d8e35159d711ee1b20b2,"MAP estimation USED-FOR dropout training. model PART-OF family. models USED-FOR power mean. lower bounds FEATURE-OF stochastic subvariants. sampled dropout masks USED-FOR power mean. models PART-OF family. deterministic dropout USED-FOR MC averaging. Task is dropout. Method are conditional models, and regularisation - heavy language modelling. OtherScientificTerm are dropout objective, and deterministic subvariant ’s bound. Generic is It. ",This paper studies the problem of estimating the power mean of a conditional model trained with dropout. The authors show that the dropout objective is deterministic in the sense that it does not depend on the distribution of the model parameters. They also show that this deterministic dropout is equivalent to sampling from a distribution of sampled dropout masks.    The authors also provide a lower bound on the power-mean of stochastic subvariants of the conditional model. The lower bound is based on the deterministic subvariant’s bound and shows that it is a function of the number of samples in the distribution. ,This paper studies the problem of estimating the power mean of a conditional model trained with dropout. The authors provide a lower bound on the subvariant of the dropout objective of the conditional model. The lower bound is based on the deterministic subvariants of the stochastic model. They show that the lower bound depends on the number of sampled dropout masks. They also provide an upper bound for the power-mean of the model.   
8840,SP:ac1b950ad29429ae045bb5e53279014a6a0b9d2b,"redundant filters PART-OF Convolutional Neural Networks ( CNNs ). robust pruning method USED-FOR GSFP. soft pruning strategy USED-FOR GSFP. cumulative saliency strategy USED-FOR pruning. accuracy EVALUATE-FOR pruning. cumulative saliency strategy USED-FOR accuracy. pruning USED-FOR model recovery process. saliency FEATURE-OF filter. saliency FEATURE-OF filter. saliency USED-FOR pruning. pruning COMPARE local pruning. local pruning COMPARE pruning. normalization formula USED-FOR layers of filters. layers of filters PART-OF network. CNN architectures CONJUNCTION data sets. data sets CONJUNCTION CNN architectures. CNN architectures EVALUATE-FOR GSFP. data sets EVALUATE-FOR GSFP. GSFP USED-FOR global and soft pruning strategies. MNIST CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION MNIST. CIFAR10 EVALUATE-FOR it. MNIST EVALUATE-FOR it. test accuracy EVALUATE-FOR it. compression ratio EVALUATE-FOR it. OtherScientificTerm are global redundancy, and excessive pruning rate. Generic is model. Task is pruning guidance. Method is pre - trained CNN model. ","This paper proposes a robust pruning method called GSFP, which prunes layers of filters in a convolutional neural network. The main idea of GSFP is to use the saliency of each filter in the network as a proxy for the importance of each layer in the pruning process. The saliency is determined by the normalization of the filter weights, which is based on the weight normalization formula. The authors show that the proposed method can be used for both global pruning and soft pruning. Experiments on MNIST and CIFAR-10 show the effectiveness of the method.",This paper proposes a robust pruning method for CNNs that prunes layers of filters in the training process. The main idea is to prune the layers of the network that are most sensitive to the saliency of the input data. This is done by pruning the layers that have the highest saliency. The authors show that this method can be applied to both global and soft pruning strategies. They also show that it can be used to improve the compression ratio.
8849,SP:621e41d4199e333ec7f9d0936d4e34c918f39c11,character - based embedder CONJUNCTION word - based classifier. word - based classifier CONJUNCTION character - based embedder. transfer learning scheme USED-FOR cross - lingual subword similarity. limited training data USED-FOR transfer learning scheme. character - based embedder USED-FOR transfer learning scheme. embedder USED-FOR vector representations. written forms USED-FOR vector representations. word vectors USED-FOR classifier. multi - task objective USED-FOR model. CACO models COMPARE cross - lingual word embedding models. cross - lingual word embedding models COMPARE CACO models. low - resource settings USED-FOR CACO models. related language pairs USED-FOR cross - lingual word embedding models. high - resource settings USED-FOR cross - lingual word embedding models. Task is Text classification. Method is joint character representation. Material is cross - lingual or monolingual resources. ,-based cross-lingual word embedding models (CACO) are proposed for text classification. CACO models are based on character-based embeddings and word-based classifiers. The authors propose a novel transfer learning scheme to learn the cross-language subword similarity. The proposed method is evaluated on text classification tasks.   ,This paper proposes a cross-lingual word embedding (CACO) model for text classification. CACO is based on a character-based embedding and a word-based classifier. The authors propose a transfer learning scheme to learn the cross-language subword similarity. They also propose a multi-task objective to improve the performance of the model. The proposed method is evaluated on a variety of text classification tasks. 
8858,SP:544e421f9c747640d949f433e3091763508b7237,"marginalized average aggregation ( MAA ) module USED-FOR MAAN. latent discriminative probabilities USED-FOR MAA. latent discriminative probabilities USED-FOR MAA module. MAAN USED-FOR dense and integral action regions. MAAN USED-FOR class activation sequences. algorithm USED-FOR MAA. algorithm USED-FOR complexity. large - scale video datasets EVALUATE-FOR MAAN. MAAN USED-FOR weakly - supervised temporal action localization. large - scale video datasets EVALUATE-FOR weakly - supervised temporal action localization. OtherScientificTerm are dense and integral regions, overestimation of the most salient regions, video snippet features, averaged subset features, and O(T ). Method is marginalized average attentional network ( MAAN ). ",This paper proposes a new method for video localization based on marginalized average aggregation (MAAN). The proposed method is based on the marginalized average attentional network. The authors show that the proposed method can be used for weakly supervised video localization. The main contributions of the paper are:  1. The proposed MAAN is able to learn dense and integral regions of the video.  2. The method is shown to be computationally efficient.   ,"This paper proposes a marginalized average attentional network (MAAN) for video localization. MAAN is based on the marginalized average aggregation (MAAA) module, which is an extension of MAA. The MAA module consists of two parts: (1) a latent discriminative probability module that estimates the most salient regions, and (2) an average of the averaged subset features. The proposed MAAN can be used for weakly-supervised temporal action localization. The authors show that MAAN outperforms the state-of-the-art in terms of O(T) complexity. "
8867,SP:9f98c9bac99003741dd14e093b54d692c0b0e8d8,"neural models USED-FOR Natural Language Processing. structureless distributed representations USED-FOR neural models. models COMPARE representational form. representational form COMPARE models. structures PART-OF wordlevel and chunk - level representations. HRR USED-FOR models. models USED-FOR crude linguistic roles. HRR USED-FOR structured compositional representation. OtherScientificTerm are linguistic structures, and syntax. Method are language models, and Holographic Reduced Representation ( HRR ). ","This paper proposes a method to reduce the size of word-level and chunk-level representations in language models. The method is based on the idea of holographic reduced representation (HRR), which reduces the number of words in a sentence to a single image. The authors show that this reduced representation can be used as a way to improve the performance of language models on tasks such as classification and sentence generation.  ","This paper proposes a new representation for natural language processing, called Holographic Reduced Representation (HRR), which is based on the notion of ""holographic reduced representation"" (HRL). HRR aims to reduce the number of word-level and chunk-level representations in a language model. The authors show that HRR can be used to improve the performance of language models on a variety of tasks, and that it can be applied to both word level and chunk level representations. In particular, HRR is shown to improve performance on a number of tasks such as sentence completion, sentence generation, and sentence completion."
8876,SP:5908b6acfed0e7c51e203c72eba907e6635e6c60,"Partially observable Markov decision processes ( POMDPs ) USED-FOR decision - making. perception decision CONJUNCTION planning decision. planning decision CONJUNCTION perception decision. greedy strategy USED-FOR observation selection. point - based value iteration algorithm USED-FOR near - optimal uncertainty reduction. greedy strategy USED-FOR near - optimal uncertainty reduction. sampled belief points USED-FOR near - optimal uncertainty reduction. greedy strategy PART-OF point - based value iteration algorithm. solver USED-FOR reachable subspace of belief simplex. computations USED-FOR perception. planning HYPONYM-OF computations. active perception CONJUNCTION planning. planning CONJUNCTION active perception. OtherScientificTerm are stochastic outcome, known distribution, real - world scenarios, and action space. Method are POMDP models, and selection process. Material is robotic scenarios. ","This paper studies the problem of learning partially observable Markov decision processes (POMDPs) from observation data. The authors propose a greedy strategy for observation selection in the POMDP model, and propose a point-based value iteration algorithm for uncertainty reduction. The proposed method is shown to achieve near-optimal uncertainty reduction, and is able to learn a reachable subspace of belief simplex. The method is tested on simulated and real-world scenarios.","This paper studies partially observable Markov Decision Processes (POMDPs) in which the agent has access to a set of observations that are partially observable, but not fully observable. The authors propose a greedy strategy for observation selection in the POMDP model, which is based on a point-based value iteration algorithm. They show that the greedy strategy is near-optimal in terms of uncertainty reduction, and show that it can be combined with a solver to find a reachable subspace of belief simplex. They also show that their approach can be applied to real-world scenarios, where the agent is in control of the environment."
8885,SP:0adec4abec17b3aab0c6eb69d11925dc20544950,"Curriculum learning USED-FOR network. data complexity CONJUNCTION network training. network training CONJUNCTION data complexity. internal covariate shift PART-OF network forward pass. representation loss USED-FOR low weighted samples. adaptive weight CONJUNCTION representation loss. representation loss CONJUNCTION adaptive weight. adaptive weight PART-OF curriculum loss. representation loss PART-OF curriculum loss. random sampling USED-FOR curriculum learning. curriculum loss CONJUNCTION stochastic algorithms. stochastic algorithms CONJUNCTION curriculum loss. curriculum loss COMPARE SGD. SGD COMPARE curriculum loss. SGD HYPONYM-OF stochastic algorithms. Method are Deep neural networks, top layers, and learning of top layers. OtherScientificTerm are distribution changes in weight of top layers, backward pass, hard examples, noisy gradients, embedding space, fluctuation of top layers, and hard samples. Task are distribution shifting, and training. Material are Low - weighted data, and benchmark datasets. ","This paper studies the problem of distribution shift in the weight of top layers in deep neural networks. The authors propose a curriculum loss for low-weighted data, which they call ""curriculum learning"". They show that this loss can be used to improve the performance of the network in the forward pass, and in the backward pass, where the weights of the top layers change over time.  The authors show that the curriculum loss is equivalent to random sampling, and that it can be combined with adaptive weight and representation loss to improve performance.  ","This paper studies the problem of curriculum learning in deep neural networks, where the weights of the top layers of the network are shifted during training. The authors propose a new curriculum loss for the forward pass, which is based on the notion of internal covariate shift. They show that the proposed curriculum loss is more robust to distribution shift than standard stochastic gradient descent (SGD) in terms of the number of samples per layer. They also show that it is more sensitive to hard examples than SGD.   "
8894,SP:8b555b9f24044bc68c204169d6a37e262361d706,"heuristics USED-FOR combinatorial optimization problems. REINFORCE USED-FOR baseline. value function USED-FOR baseline. deterministic greedy rollout USED-FOR baseline. attention layers USED-FOR model. REINFORCE USED-FOR model. heuristics USED-FOR Travelling Salesman Problem ( TSP ). heuristics USED-FOR Vehicle Routing Problem ( VRP ). hyperparameters USED-FOR heuristics. Orienteering Problem ( OP ) HYPONYM-OF Vehicle Routing Problem ( VRP ). Generic are it, models, problems, and baselines. Method are Pointer Network, and Prize Collecting TSP ( PCTSP ). ",This paper studies reinforcement learning in combinatorial optimization problems. The authors propose to use deterministic greedy rollout to learn a baseline for solving the Travelling Salesman Problem (TSP) and the Vehicle Routing Problem (VRP). The authors show that the deterministic rollout can be used to train a model that can solve the TSP and VRP in a deterministic manner. They also show that this can be applied to solve the PCTSP and Prize Collecting TSP. ,"This paper proposes a novel approach to combinatorial optimization problems with a deterministic greedy rollout. The approach is based on REINFORCE, which is an extension of the Pointer Network (PNN) framework. The authors show that this approach can be applied to the Travelling Salesman Problem (TSP) and the Vehicle Routing Problem (VRP) problems. They also show that the approach can also be used to solve the Prize Collecting TSP (PCTSP) problem."
8903,SP:efb76bcf1dbd9a9cf6b5db74b5d4256a9f9e9e73,"time and space complexity EVALUATE-FOR neural network inference. network quantization USED-FOR neural network inference. limited computational and memory resources FEATURE-OF embedded and mobile devices. differentiable neural architecture search ( DNAS ) framework USED-FOR exponential search space. gradient - based optimization USED-FOR differentiable neural architecture search ( DNAS ) framework. neural architecture search problem USED-FOR problem. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. CIFAR-10 EVALUATE-FOR ResNet. ImageNet EVALUATE-FOR ResNet. quantized models COMPARE full precision models. full precision models COMPARE quantized models. model size CONJUNCTION computational cost. computational cost CONJUNCTION model size. computational cost EVALUATE-FOR full precision models. model size EVALUATE-FOR quantized models. computational cost EVALUATE-FOR quantized models. Method is quantization methods. OtherScientificTerm are design space, and bit - widths. ","This paper proposes a novel quantization method for neural networks. The proposed method is based on a differentiable neural architecture search (DNAS) framework, which uses gradient-based optimization to find the optimal bit-widths for each layer in the network. The method is evaluated on CIFAR-10 and ImageNet, where it achieves state-of-the-art performance on both model size and computational cost.","This paper proposes a differentiable neural architecture search (DNAS) framework for neural network quantization. DNAS is based on gradient-based optimization, where the search space is an exponential search space. The authors show that DNAS can be used to reduce the computational and memory cost of quantized neural networks. They also show that quantized models can be competitive with full precision models in terms of model size and computational cost. "
8912,SP:ea4173f8265bc50296de51c4ee7ecb6b8f78bec0,"attention USED-FOR neural architectures. attention USED-FOR decoding stage. posterior attention distribution USED-FOR attention. posterior attention models COMPARE attention models. attention models COMPARE posterior attention models. morphological inflection tasks EVALUATE-FOR posterior attention models. alignment accuracy EVALUATE-FOR attention models. translation EVALUATE-FOR posterior attention models. translation CONJUNCTION morphological inflection tasks. morphological inflection tasks CONJUNCTION translation. BLEU score EVALUATE-FOR attention models. BLEU score CONJUNCTION alignment accuracy. alignment accuracy CONJUNCTION BLEU score. morphological inflection tasks EVALUATE-FOR attention models. translation EVALUATE-FOR attention models. alignment accuracy EVALUATE-FOR posterior attention models. BLEU score EVALUATE-FOR posterior attention models. Method are attention architectures, and Posterior Attention Models. Generic is architecture. ","This paper proposes a new attention architecture, called posterior attention models (PAMs), which is based on the posterior attention distribution (PUD). The authors show that PUD can be viewed as a special case of attention in the decoding stage, where the posterior distribution is used to model the importance of each input feature. The authors evaluate the performance of PUD on translation and morphological inflection tasks.   ","This paper proposes a new attention architecture, called Posterior attention models, which is based on the posterior attention distribution (PAD). The authors show that PADs are able to perform better than other attention models in terms of BLEU score, alignment accuracy, and translation accuracy on morphological inflection tasks. They also show that they can achieve better alignment accuracy than the state of the art. "
8921,SP:987e2c14abc091d4d3ef9b48fb2046408eb1f59e,"artifacts CONJUNCTION degenerated transformations. degenerated transformations CONJUNCTION artifacts. smoothness term USED-FOR harmonic functions. harmonic functions USED-FOR consistent mappings. smoothness term PART-OF sample graph. HarmonicGAN USED-FOR bi - directional translations. similarity - consistency USED-FOR inherent selfconsistency property. histogram CONJUNCTION CNN. CNN CONJUNCTION histogram. features FEATURE-OF Distance metrics. CNN HYPONYM-OF Distance metrics. histogram HYPONYM-OF Distance metrics. CNN HYPONYM-OF features. histogram HYPONYM-OF features. HarmonicGAN COMPARE state of the art. state of the art COMPARE HarmonicGAN. CycleGAN COMPARE HarmonicGAN. HarmonicGAN COMPARE CycleGAN. interpretability EVALUATE-FOR HarmonicGAN. object transfiguration CONJUNCTION semantic labeling. semantic labeling CONJUNCTION object transfiguration. medical imaging CONJUNCTION object transfiguration. object transfiguration CONJUNCTION medical imaging. medical imaging HYPONYM-OF applications. semantic labeling HYPONYM-OF applications. object transfiguration HYPONYM-OF applications. tasks EVALUATE-FOR methods. method USED-FOR medical imaging task. Task are unpaired image - to - image translation, manifold view of the problem, and translation. Generic is it. OtherScientificTerm are pixel - to - pixel supervision, manual inputs, and mean - squared error. Metric is training - time cost. ","This paper proposes a method for unpaired image-to-image translation, where the goal is to learn a map from one image to the other. The proposed method is based on the idea of using a smoothness term in the embedding space of a graph, which is used as a regularization term to ensure consistency between the two images. The method is evaluated on object transfiguration, semantic labeling and medical imaging tasks.","This paper proposes a method for bi-directional image-to-image translation. The main idea is to use a smoothness term in the sample graph to ensure consistency between the two images. The authors show that the smoothness of the samples can be used to improve the interpretability of their method. The method is evaluated on a variety of tasks, including medical imaging, object transfiguration, and semantic labeling.  "
8930,SP:885a69003bad0e79cb2872a4e5c772191ad7e34f,"EVGP USED-FOR gradient components. stochastic algorithm ( h - detach ) USED-FOR LSTM optimization. stochastic algorithm ( h - detach ) USED-FOR problem. linear path ( cell state ) PART-OF LSTM computational graph. long term dependencies FEATURE-OF components. LSTM USED-FOR dependencies. seed CONJUNCTION learning rate. learning rate CONJUNCTION seed. convergence speed CONJUNCTION robustness. robustness CONJUNCTION convergence speed. robustness CONJUNCTION learning rate. learning rate CONJUNCTION robustness. robustness CONJUNCTION seed. seed CONJUNCTION robustness. benchmark datasets EVALUATE-FOR generalization. benchmark datasets EVALUATE-FOR LSTM gradient. convergence speed EVALUATE-FOR vanilla LSTM gradient based training. LSTM gradient USED-FOR generalization. Method are Recurrent neural networks, and LSTMs. Task is exploding and vanishing gradient problem ( EVGP ). OtherScientificTerm are LSTM weights, and gradients. Generic is path. ",This paper studies the exploding and vanishing gradient problem (EVGP) in recurrent neural networks. The authors propose to use LSTMs to compute the gradient components of an LSTM and then use a stochastic algorithm (h-divergent) to compute its gradient. Theoretical analysis is provided to show the convergence and robustness properties of the proposed method. Empirical results are provided to support the theoretical results. ,This paper studies the exploding and vanishing gradient problem (EVGP) in recurrent neural networks (LSTMs). The main contribution of the paper is a theoretical analysis of the convergence of the EVGP. The main result is that EVGP converges faster than vanilla LSTM gradient-based training. The authors show that the convergence rate of EVGP is bounded by the learning rate and the robustness of the LSTMs. They also provide an empirical study of the generalization performance of the proposed method. 
8939,SP:9aaff3777321347d1194884af5690b0b5185eff9,posterior distribution FEATURE-OF binary weights. Bayesian deep learning perspective USED-FOR real binary weight networks. reinforcement learning scheme USED-FOR policy network. policy network USED-FOR posterior distribution. binary weights USED-FOR burn - after - reading style. binary weight instances USED-FOR recognition architecture. policy network USED-FOR binary weight instances. policy network USED-FOR recognition architecture. policy network USED-FOR neural network architecture. nested parameter structure FEATURE-OF policy network. nested parameterization USED-FOR joint posterior distribution of binary weights. ImageNet HYPONYM-OF visual recognition tasks. visual recognition tasks EVALUATE-FOR SnapQuant. ImageNet EVALUATE-FOR SnapQuant. Task is point estimation. Generic is method. , image recognition is a popular image classification task. This paper proposes a Bayesian deep learning approach to estimate the posterior distribution of binary weights in a binary weight network. The proposed method is based on a reinforcement learning approach where a policy network is trained to predict the posterior of the binary weights. The method is evaluated on ImageNet and achieves state-of-the-art performance. ,"This paper proposes SnapQuant, a novel method for learning binary weight networks from scratch. SnapQuant is based on a reinforcement learning approach to learn the posterior distribution of the binary weights of a neural network. The posterior distribution is learned by training a policy network that learns the joint posterior distribution between the binary weight instances of the neural network and the recognition model. The authors show that SnapQuant outperforms the state-of-the-art in terms of performance on ImageNet and CIFAR-10."
8948,SP:29d1f6d0661a51e56c59bbb106da56700fc22d9a,Bayesian nonparametric framework USED-FOR federated learning with neural networks. inference approach USED-FOR global network. supervision CONJUNCTION data pooling. data pooling CONJUNCTION supervision. federated learning problems EVALUATE-FOR approach. image classification datasets USED-FOR federated learning problems. OtherScientificTerm is local neural network weights. Generic is framework. ,"This paper proposes a Bayesian nonparametric framework for federated learning with neural networks. The proposed framework is based on Bayesian inference, where the weights of the local neural network weights are aggregated to form a global network. The authors show that the proposed method is able to achieve state-of-the-art performance on image classification tasks. ","This paper proposes a Bayesian nonparametric framework for federated learning with neural networks. The main idea of the paper is to use a local neural network weights as the weights of a global network. The local weights are learned by using a supervised learning approach, where the local weights of the global network are learned using a data pooling strategy. The proposed approach is evaluated on a variety of datasets and is shown to outperform the baselines. "
8957,SP:ab1f2bd216635d63450688866c729a501bd7e9d0,"GANs CONJUNCTION intrinsic curiosity. intrinsic curiosity CONJUNCTION GANs. GANs CONJUNCTION multi - agent RL. multi - agent RL CONJUNCTION GANs. intrinsic curiosity CONJUNCTION multi - agent RL. multi - agent RL CONJUNCTION intrinsic curiosity. differentiable games USED-FOR learning methods. approach USED-FOR learning dynamics. Opponent shaping USED-FOR learning dynamics. Opponent shaping HYPONYM-OF approach. learning dynamics FEATURE-OF games. approach USED-FOR games. theoretical guarantees FEATURE-OF algorithms. LOLA CONJUNCTION stable variant. stable variant CONJUNCTION LOLA. method USED-FOR LOLA. Stable Opponent Shaping ( SOS ) HYPONYM-OF method. LookAhead HYPONYM-OF stable variant. LookAhead USED-FOR equilibria. strict saddles PART-OF differentiable games. Method are Opponent - Learning Awareness ( LOLA ), LOLA agents, and SOS. Generic is algorithm. OtherScientificTerm are cooperation, Iterated Prisoner ’s Dilemma, ‘ arrogant ’ behaviour, and learning of opponents. ","This paper studies the problem of learning from opponents in differentiable games. In this setting, the goal is to learn from the actions of opponents in order to improve the performance of the learner. To this end, the authors propose a new learning algorithm called Opponent-Learning Awareness (LOLA) that learns from the behavior of the opponents. The authors show that the learning dynamics of LOLA can be approximated by opponent shaping, which is a variant of stochastic opponent shaping (SOS). Theoretical analysis is provided to show the convergence of the proposed method to the equilibria of the game.",This paper studies the problem of learning from opponents in differentiable games. The authors propose a novel method to learn the dynamics of a differentiable game from the perspective of opponent-learning awareness (LOLA) agents. They show that the learning dynamics of LOLA agents can be approximated by a stable opponent shaping (SOS) algorithm. They also show that SOS can be used to find equilibria in the differentiable setting.  
8966,SP:bdafb5fca09a775a8c92d2826d5dc977d28091c2,"learning system USED-FOR rare events. feature space FEATURE-OF classifiers / regressors. shape feature HYPONYM-OF prior information. segmentation algorithms USED-FOR it. shape feature USED-FOR feature space. Variational Auto - Encoder(VAE ) USED-FOR segmentation result. loss function USED-FOR shape feature. ground truth masks USED-FOR VAE. VAE USED-FOR shapes. representation USED-FOR qualities of segmentation results. one - dimensional feature space FEATURE-OF representation. segmentation algorithms USED-FOR medical segmentation task. medical segmentation task EVALUATE-FOR alarm system. segmentation algorithms EVALUATE-FOR alarm system. OtherScientificTerm are low dimensional feature space, bad shapes, and loss value. Generic is system. ","This paper proposes to use shape features to improve the segmentation performance of a medical segmentation system. The proposed method is based on the idea that shape features are useful for segmentation in the feature space, but are not useful in the training process. The authors propose to use a VAE to learn the shape features from the ground truth masks, and then use the learned shape features as a loss function to train the classifier and regressors. The paper shows that the proposed method outperforms the state-of-the-art segmentation methods on a medical classification task.  ","This paper proposes a VAE-based segmentation method for medical segmentation. The proposed method is based on the idea that the shape feature of a segmentation result can be represented as a representation of the ground truth mask. The shape feature is represented in a one-dimensional feature space, and the VAE encodes the shape into a low dimensional feature space. The VAE is then used as a loss function for the segmentation results. The method is evaluated on a variety of segmentation tasks, and it is shown that the proposed method outperforms the state of the art."
8975,SP:60738395d9efe2b3fe3a00c542ebb4261e54386c,"denoising CONJUNCTION inpainting. inpainting CONJUNCTION denoising. inpainting CONJUNCTION reconstruction. reconstruction CONJUNCTION inpainting. Deep neural networks USED-FOR compressing images. Deep neural networks USED-FOR inverse problems. compressing images USED-FOR inverse problems. reconstruction HYPONYM-OF inverse problems. convolutional neural networks HYPONYM-OF Deep neural networks. denoising HYPONYM-OF inverse problems. few and noisy measurements USED-FOR reconstruction. inpainting HYPONYM-OF inverse problems. tools COMPARE imagegenerating deep neural networks. imagegenerating deep neural networks COMPARE tools. wavelets HYPONYM-OF tools. deep neural network USED-FOR natural images. deep decoder HYPONYM-OF untrained simple image model. deep decoder USED-FOR images. network weights COMPARE wavelet - based thresholding. wavelet - based thresholding COMPARE network weights. underparameterization USED-FOR deep decoder. deep decoder USED-FOR denoising. underparameterization USED-FOR overfitting. ReLU activation CONJUNCTION channelwise normalization. channelwise normalization CONJUNCTION ReLU activation. pixel - wise linear combination of channels CONJUNCTION ReLU activation. ReLU activation CONJUNCTION pixel - wise linear combination of channels. upsampling unit CONJUNCTION pixel - wise linear combination of channels. pixel - wise linear combination of channels CONJUNCTION upsampling unit. them USED-FOR signal representations. neural networks USED-FOR signal representations. it USED-FOR neural networks. neural networks USED-FOR them. theoretical analysis USED-FOR network. OtherScientificTerm are output dimension, weight parameters, convolutions, and output dimensionality. Material is large datasets. ",This paper studies the problem of image denoising and inpainting with deep convolutional neural networks (DNNs). The authors show that DNNs can be used to solve inverse problems with few and noisy measurements. They show that under-parameterization of the network weights can lead to overfitting and over-parametrizing of the output dimensionality. They also show that channel-wise normalization and ReLU activation can help improve the performance of the DNN. ,"This paper presents a theoretical analysis of the performance of deep convolutional neural networks (DNNs) for inverse inverse problems, i.e., denoising, reconstruction, and inpainting. The authors show that DNNs can outperform wavelet-based thresholding and deep decoders in terms of performance on these inverse problems. They also show that underparameterization can lead to overfitting and overfitting can be caused by overfitting of the network weights. "
8984,SP:1c9bad3bd4d670172f65aa0304e9837ecafc6b3d,"natural language ( NL ) USED-FOR Program synthesis. SAPS HYPONYM-OF end - to - end neural network. end - to - end neural network USED-FOR multi - sentence NL specifications. pretrained word embedding CONJUNCTION bi - directional multi - layer LSTM. bi - directional multi - layer LSTM CONJUNCTION pretrained word embedding. bi - directional multi - layer LSTM USED-FOR processing of word sequences. abstract syntax trees CONJUNCTION pretrained word embedding. pretrained word embedding CONJUNCTION abstract syntax trees. pretrained word embedding USED-FOR processing of word sequences. pretrained word embedding USED-FOR architecture. bi - directional multi - layer LSTM USED-FOR architecture. abstract syntax trees USED-FOR architecture. neural components USED-FOR architecture. signal propagation schemes CONJUNCTION soft attention mechanism. soft attention mechanism CONJUNCTION signal propagation schemes. doubly - recurrent LSTM CONJUNCTION signal propagation schemes. signal propagation schemes CONJUNCTION doubly - recurrent LSTM. signal propagation schemes USED-FOR decoder. soft attention mechanism USED-FOR decoder. doubly - recurrent LSTM USED-FOR decoder. SAPS COMPARE method. method COMPARE SAPS. NL analyzer CONJUNCTION source code generator. source code generator CONJUNCTION NL analyzer. methods COMPARE it. it COMPARE methods. fixed - dimensional latent representation USED-FOR NL analyzer. post - processing USED-FOR it. fixed - dimensional latent representation USED-FOR it. Task are software development, and end - user programming. ",This paper proposes an end-to-end neural network for program synthesis in natural language (NL) specifications. The main idea is to use a pretrained word embedding and a bi-directional multi-layer LSTM to learn to predict the program specifications from a set of abstract syntax trees. The paper also proposes a soft attention mechanism to improve the performance of the decoder. Experiments show that the proposed method outperforms the state-of-the-art in terms of accuracy and efficiency. ,"This paper proposes a novel end-to-end neural network architecture for natural language (NL) program synthesis. The proposed architecture is based on a bi-directional multi-layer LSTM, which is trained with a pre-trained word embedding. The paper also proposes a soft-attention mechanism for the decoder and a signal propagation scheme for the signal propagation schemes. The experimental results show that the proposed method outperforms the state-of-the-art in terms of program synthesis performance."
8993,SP:d2ec231bb6153a303e5110e671dea14c2721e636,"deep neural networks USED-FOR tiny input perturbations. MNIST HYPONYM-OF computer vision. deep neural networks USED-FOR MNIST. L0 robustness EVALUATE-FOR undefended networks. adversarial robustness EVALUATE-FOR MNIST. class - conditional data distributions USED-FOR robust classification model. adversarial attacks USED-FOR model. robustness EVALUATE-FOR MNIST. MNIST EVALUATE-FOR approach. robustness EVALUATE-FOR approach. Method are neural network model, L∞ defense, input binarization, and decision - based attack. OtherScientificTerm are adversarial perturbations, L2 perturbations, Lp norms, and L0, L2 and L∞ perturbations. Material are unrecognizable images, and adversarial examples. Generic is attack. ","This paper studies the problem of adversarial robustness of deep neural networks against adversarial examples. The authors propose to use class-conditional data distributions to improve the robust classification model. The proposed method is based on the observation that the L0 robustness is lower than the L2 robustness in the presence of Lp norms, and L0, L2 and L∞ perturbations.   The authors then propose a new adversarial attack strategy that uses binarization to reduce the Lp norm of the adversarial example. The method is evaluated on MNIST and CIFAR-10 datasets.","This paper proposes a method to improve the robustness of deep neural networks against adversarial attacks. The main idea is to use class-conditional data distributions to train a robust classification model, where the class-conditioned data distributions are class-dependent. The proposed method is evaluated on MNIST and CIFAR-10 datasets, where it is shown that the proposed method improves the L0 robustness."
9002,SP:91a24e7f4b952c37441feab4a7e8555014c856a4,"spectra of weight matrices PART-OF discriminator. spectra of weight matrices USED-FOR GANs. framework USED-FOR GANs. weight matrices PART-OF discriminator. slow singular value decays FEATURE-OF weight matrices. regularizers CONJUNCTION constraints. constraints CONJUNCTION regularizers. reparameterization approach USED-FOR GANs. reparameterization approach USED-FOR weight matrices. regularizers USED-FOR spectra of the weight matrices. constraints USED-FOR spectra of the weight matrices. spectrum control USED-FOR GANs. methods COMPARE method. method COMPARE methods. CIFAR-10, STL-10, and ImgaeNet datasets EVALUATE-FOR method. spectral normalization USED-FOR method. Method are Generative Adversarial Networks ( GANs ), and singular value decompositions. OtherScientificTerm is slow singular value decay. ","This paper proposes to use spectral normalization to control the spectrum of weight matrices in GANs. The main idea is to use a reparameterization approach to reparametrize the spectra of the weight matrix in the discriminator. The authors propose to use regularization and constraints to regularize the spectral distribution of the weights. The proposed method is evaluated on CIFAR-10, STL-10 and ImgaeNet datasets.  ",This paper proposes a method to control the spectra of weight matrices in GANs. The main idea is to use a reparameterization approach to reparametrize the spectral normalization of the weight matrix. This is achieved by using a combination of regularizers and constraints. The authors show that the proposed method can control the spectrum control of the spectral matrices of the discriminator. They also show that their method can be applied to CIFAR-10 and STL-10 datasets.
9011,SP:8115fd9b681198d62100c36794926fb57dc0a4f5,Acceleration USED-FOR reinforcement learning methods. Anderson acceleration technique USED-FOR value iteration. Anderson Accelerated Value Iteration ( A2VI ) HYPONYM-OF accelerated value iteration algorithm. method USED-FOR Deep Q - learning algorithm. approach USED-FOR approximation of the policy evaluation. approximate method USED-FOR policy evaluation. A2VI COMPARE policy iteration. policy iteration COMPARE A2VI. policy iteration HYPONYM-OF approximate method. toy problems CONJUNCTION Atari games. Atari games CONJUNCTION toy problems. Material is historical data. Generic is algorithm. ,"This paper proposes an accelerated value iteration method based on the Anderson acceleration technique for value iteration in reinforcement learning. The proposed method, called Anderson Accelerated Value Iteration (A2VI), is a variant of value iteration with Anderson acceleration. The authors show that A2VI can be used to accelerate the policy evaluation in deep Q-learning. They also show that the proposed method is equivalent to policy iteration in toy problems and Atari games. ",This paper proposes a new accelerated value iteration method for value iteration in reinforcement learning. The main idea is to use the Anderson acceleration technique to speed up the value iteration process. The authors show that the proposed method can be applied to the Deep Q-learning algorithm. They also show that it can be used to improve the performance of the policy evaluation.
9020,SP:bd79b0c0af778a36008a0c0cf2fb6393fd2789d4,method USED-FOR catastrophic forgetting problem. SupportNet USED-FOR catastrophic forgetting problem. class incremental learning scenario FEATURE-OF catastrophic forgetting problem. deep learning CONJUNCTION support vector machine ( SVM ). support vector machine ( SVM ) CONJUNCTION deep learning. support vector machine ( SVM ) PART-OF SupportNet. deep learning PART-OF SupportNet. SVM PART-OF SupportNet. consolidation regularizers USED-FOR model. SupportNet COMPARE deep learning model. deep learning model COMPARE SupportNet. SupportNet COMPARE incremental learning methods. incremental learning methods COMPARE SupportNet. tasks EVALUATE-FOR SupportNet. tasks EVALUATE-FOR method. OtherScientificTerm is catastrophic forgetting. ,"This paper studies the catastrophic forgetting problem in the class incremental learning scenario. The authors propose to use deep learning and support vector machine (SVM) to solve the catastrophic learning problem. The proposed method, called SupportNet, is a combination of deep learning with SVM and consolidation regularizers. Experiments show that the proposed method achieves state-of-the-art performance on several tasks.",This paper proposes a method to address the catastrophic forgetting problem in the class incremental learning scenario. The proposed method is based on a combination of deep learning and support vector machine (SVM) with consolidation regularizers. The method is evaluated on a variety of tasks and compared with other incremental learning methods. 
9029,SP:d228d213f79716774043cea253305fecece659ec,"methods USED-FOR representations. methods USED-FOR unit selectivity. unit selectivity USED-FOR representations. neural networks ( NNs ) USED-FOR representations. measures PART-OF AlexNet. localist selectivity HYPONYM-OF measures. precision CONJUNCTION class - conditional mean activity selectivity CCMAS. class - conditional mean activity selectivity CCMAS CONJUNCTION precision. precision and CCMAS measures USED-FOR selectivity. fc6 CONJUNCTION conv5. conv5 CONJUNCTION fc6. units PART-OF conv5. units PART-OF fc6. RNNs COMPARE AlexNet. AlexNet COMPARE RNNs. AlexNet USED-FOR localist representations. RNNs USED-FOR localist representations. Generic is measure. Metric are top - class selectivity, and selectivity measures. Method are recurrent neural networks ( RNNs ), activation maximization ( AM ) images, fc8, and NNs. OtherScientificTerm are hidden layers, and selective units. ","This paper proposes two measures of unit selectivity in neural networks (NNs) based on precision and class-conditional mean activity selectivity (CCMAS). The precision and CCMAS measures measure the top-class selectivity of units in the network. The authors show that fc8 and fc6 units are the most selective units in conv5 and conv5, respectively.   The authors also show that AlexNet achieves the state-of-the-art performance on activation maximization (AM) images. ",This paper proposes a new measure of unit selectivity for neural networks (NNs) called localist selectivity. The measure is based on two existing measures: precision and class-conditional mean activity selectivity (CCMAS). The authors show that the proposed measure can be used to measure the top-class selectivity of RNNs. They also show that AlexNet is able to achieve state-of-the-art performance on fc8 and fc6 datasets.
9038,SP:b9deae0392e0160b400d76c549d382e235196f8c,"spectral methods CONJUNCTION posterior inference. posterior inference CONJUNCTION spectral methods. probabilistic graphical models USED-FOR posterior inference. graphs USED-FOR Community detection. spectral methods USED-FOR Community detection. posterior inference USED-FOR Community detection. signal - to - noise ratio FEATURE-OF statistical and computational detection thresholds. stochastic block model HYPONYM-OF random graph families. graphs USED-FOR node - wise classification problem. node - wise classification problem USED-FOR community detection. learning perspective USED-FOR it. Graph Neural Networks ( GNNs ) USED-FOR community detection problems. supervised learning setting USED-FOR community detection problems. belief propagation algorithm USED-FOR binary and multiclass stochastic block models. they COMPARE belief propagation algorithm. belief propagation algorithm COMPARE they. line graph of edge adjacencies FEATURE-OF non - backtracking operator. non - backtracking operator USED-FOR GNNs. real - world datasets EVALUATE-FOR GNNs. linear ) GNNs USED-FOR community detection problems. linear ) GNNs USED-FOR optimization landscape. Generic is approaches. Method is generative models. OtherScientificTerm are computational threshold, local minimum, and global minimum / minima. ","This paper proposes a new community detection method based on GNNs for stochastic block models. The proposed method is based on the line graph of edge adjacency operator, which is a non-backtracking operator. The authors show that the proposed method outperforms existing community detection methods in terms of the signal-to-noise ratio and the computational detection threshold. ",This paper proposes a new method for community detection using graph neural networks (GNNs). The main idea is to use a stochastic block model to model the node-wise classification problem of the community detection problem. The main contribution of the paper is to propose a belief propagation algorithm for binary and multiclass block models. The authors show that their method outperforms the state-of-the-art baselines in terms of both statistical and computational detection thresholds. 
9047,SP:a9ed31090e55f6152fc31c7512af5d634cc7225a,"sparse weights PART-OF linear combination. provable algorithms USED-FOR dictionary learning. provable dictionary learning methods USED-FOR coefficient recovery. linear and non - linear operations PART-OF it. algorithm COMPARE state - of - the - art techniques. state - of - the - art techniques COMPARE algorithm. Task are dictionary learning problem, optimization, and recovery of the dictionary. OtherScientificTerm are dictionary, coefficients, and geometric rate. Method are linear model, NOODL, and neural architectures. ","This paper studies the problem of recovering the coefficients of a linear combination of sparse weights in a dictionary learning problem. The authors propose a new algorithm that recovers the coefficients in the dictionary with the help of linear and non-linear operations. The proposed algorithm is based on the idea of coefficient recovery, which is to recover the weights of the linear combination in a linear model with sparse weights. The main contribution of the paper is to show that the recovery of the dictionary can be done with a provable geometric rate.   ","This paper proposes a provable dictionary learning algorithm that recovers the coefficients of a linear combination of sparse weights in a linear model. The authors show that the recovery of the coefficients is provable under a variety of assumptions, including that the weights are sparse and that the coefficients are non-linear. They show that their algorithm can recover the coefficients in terms of both linear and non-linearly operations. They also show that it can recover coefficients with a geometric rate that is close to the geometric rate of the dictionary."
9056,SP:85232b72a2643d6dc81cf952ccbb95192032b7c5,"differentiable model CONJUNCTION similarity function. similarity function CONJUNCTION differentiable model. loss function USED-FOR binary hash codes. differentiable model USED-FOR binary hash codes. loss function COMPARE prior methods. prior methods COMPARE loss function. log likelihood loss USED-FOR prior methods. log likelihood loss USED-FOR loss function. multi - indexing USED-FOR hashes. techniques USED-FOR similarity search tasks. ImageNet CONJUNCTION SIFT 1 M. SIFT 1 M CONJUNCTION ImageNet. information retrieval tasks EVALUATE-FOR SIFT 1 M. ImageNet USED-FOR information retrieval tasks. OtherScientificTerm are Hamming distance target, loss terms, and minibatch. Method is training scheme. Metric are MAP, and query cost. ",This paper proposes a new loss function for binary hash codes for similarity search. The proposed loss function is based on a differentiable model and a similarity function. The authors show that the proposed loss can be used to improve the performance of similarity search on ImageNet and SIFT-1M tasks.  ,This paper proposes a new loss function for binary hash-based similarity search. The proposed loss function is based on a differentiable model and a similarity function. The authors show that the proposed loss can be used to improve the performance of similarity search on ImageNet and SIFT-1M tasks. They also show that it can be combined with multi-indexing to reduce the query cost. 
9065,SP:3bd4ccff7f48380d2db8dff2c4ca515894a7f1db,Neural architecture search ( NAS ) USED-FOR task - specific neural network topology. networks USED-FOR search. Graph HyperNetwork ( GHN ) USED-FOR search cost. graph neural network USED-FOR inference. regular hypernetworks CONJUNCTION premature early stopping. premature early stopping CONJUNCTION regular hypernetworks. GHNs USED-FOR architecture. GHNs USED-FOR network. GHNs COMPARE regular hypernetworks. regular hypernetworks COMPARE GHNs. GHNs COMPARE premature early stopping. premature early stopping COMPARE GHNs. validation accuracy EVALUATE-FOR networks. validation accuracy EVALUATE-FOR surrogate search signal. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. they COMPARE random search methods. random search methods COMPARE they. GHNs COMPARE random search methods. random search methods COMPARE GHNs. ImageNet EVALUATE-FOR random search methods. CIFAR-10 EVALUATE-FOR random search methods. networks COMPARE manual designs. manual designs COMPARE networks. GHNs USED-FOR anytime prediction setting. speed - accuracy tradeoff EVALUATE-FOR networks. Method is manual architecture designs. Generic is it. Task is NAS. ,This paper proposes a novel architecture search method called Graph HyperNetwork (GHN) for neural architecture search (NAS). The main idea is to use a graph neural network (GNN) as a surrogate search signal to reduce the search cost. The proposed method is evaluated on CIFAR-10 and ImageNet and achieves better performance than existing methods. ,"This paper proposes a novel approach to neural architecture search (NAS) by using graph hypernetworks (GHNs) to reduce the search cost of neural networks. The authors propose a novel method to use GHNs as a surrogate search signal, which can be used to improve the accuracy of the search signal. They show that GHNs can improve the performance of NAS on CIFAR-10, ImageNet, and ImageNet.  "
9074,SP:65ccf43cd4e033d22239069057f5200d49f33724,Imitation learning USED-FOR optimal policy. expert demonstrations USED-FOR Imitation learning. expert demonstrations USED-FOR optimal policy. expert demonstrations USED-FOR deep learning. method USED-FOR generative adversarial imitation learning. multiclass classification USED-FOR discriminator functions. method USED-FOR multiclass classification. method USED-FOR discriminator functions. method COMPARE generative adversarial imitation learning baseline. generative adversarial imitation learning baseline COMPARE method. continuous control tasks EVALUATE-FOR method. method USED-FOR policies. continuous control tasks EVALUATE-FOR generative adversarial imitation learning baseline. OtherScientificTerm is non - expert demonstrations. ,"This paper proposes a generative adversarial imitation learning (GAN) method for imitation learning in the presence of non-expert demonstrations. The main idea is to learn a discriminator function that can be used as a surrogate for the discriminator of the expert policy. The discriminator is trained using a combination of two losses: (1) a regularization term that penalizes the difference between the true discriminator and the learned discriminator, and (2) an adversarial loss that penalises the similarity between the discriminators of the two discriminators. The proposed method is evaluated on a variety of continuous control tasks and achieves state-of-the-art performance.   ","This paper proposes a generative adversarial imitation learning (GAN) method for imitation learning with expert demonstrations. The main idea is to learn a discriminator function that can be used to predict the optimal policy from expert demonstrations, and then use this discriminator to learn the discriminator. The method is evaluated on a variety of continuous control tasks, where it is shown to outperform the state-of-the-art baseline."
9083,SP:e8427949a98effbd37ce7604fa11f240e2342196,"natural science HYPONYM-OF applications. neural networks USED-FOR task. Invertible Neural Networks ( INNs ) HYPONYM-OF neural networks. neural networks USED-FOR ambiguous inverse problem. INNs USED-FOR forward process. neural networks COMPARE INNs. INNs COMPARE neural networks. latent output variables USED-FOR INNs. model USED-FOR inverse process. invertibility USED-FOR model. medicine CONJUNCTION astrophysics. astrophysics CONJUNCTION medicine. INNs USED-FOR unrecoverable parameters. INNs USED-FOR multi - modalities. INNs USED-FOR parameter correlations. parameter space FEATURE-OF multi - modalities. OtherScientificTerm are hidden system parameters, posterior parameter distribution, observed measurement, and distribution of the latent variables. Task is inverse problem. Generic is ambiguity. Method is INN. Material is artificial data. ","This paper studies the problem of invertible neural networks (INNs) for the inverse problem, which is an important problem in machine learning. In this problem, the parameters of the system are unknown to the model, and the model is trained to predict the posterior distribution of the observed parameters. The authors propose an INN model that can be viewed as a forward process in which the latent variables of the forward process are assumed to be the same as the latent variable of the inverse process. The INN is trained by minimizing a loss function that minimizes the distance between the true parameters and the posterior distributions of the hidden parameters.   The authors show that the INN can be used to learn a model with invertibility, and that INNs can be trained to recover the parameters from the unknown posterior distribution.  The INNs are shown to perform well on synthetic and real-world data. ","This paper studies the problem of invertible neural networks (INNs) for the ambiguous inverse problem. In the inverse problem, the hidden system parameters are assumed to be different from the observed system parameters, and the posterior distribution of the latent variables is unknown. The authors propose an INN model that can be used to model the inverse process in the latent space. They show that INNs can be applied to a wide range of applications, including medicine, astrophysics, and astrophysics.  "
9092,SP:75c9bb53bac29bdb390f9ba5707caee4ab1f5925,"ensemble of NNs COMPARE Bayesian NNs. Bayesian NNs COMPARE ensemble of NNs. scoring rule USED-FOR ensemble of NNs. finite mixture model USED-FOR ensemble method. uniform mixing weights USED-FOR finite mixture model. adaptive, input - dependent distribution USED-FOR fixed mixing weights. NN USED-FOR adaptive, input - dependent distribution. model COMPARE approaches. approaches COMPARE model. uncertainty estimates EVALUATE-FOR model. Method are deep neural networks ( NNs ), mixture model approach, mixture density networks, and compound density networks. OtherScientificTerm are prediction uncertainty, and mixture components. Material is adversarial examples. ","This paper proposes an ensemble method for deep neural networks (NNs) that uses a mixture model to estimate the uncertainty in the prediction uncertainty. The proposed method is based on a scoring rule for the ensemble of NNs, where the mixing weights are fixed. The authors show that the ensemble method is more efficient than Bayesian NNs in terms of the uncertainty estimates.  ","This paper proposes an ensemble of neural networks (NNs) with a finite mixture model, where the ensemble consists of a mixture of NNs with uniform mixing weights and a fixed mixing weights. The ensemble is trained by using a scoring rule to estimate the uncertainty of the ensemble. The authors show that the ensemble can outperform Bayesian NNs in terms of uncertainty estimates. "
9101,SP:e1e38289285c1b8fdb318e4f6d37a198a08787a2,"energy consumption CONJUNCTION communication bandwidth. communication bandwidth CONJUNCTION energy consumption. communication bandwidth CONJUNCTION storage requirements. storage requirements CONJUNCTION communication bandwidth. deep neural networks HYPONYM-OF model class. model size reduction PART-OF deep learning. pruning CONJUNCTION quantization. quantization CONJUNCTION pruning. techniques USED-FOR Shannon - style coding schemes. Shannon - style coding schemes USED-FOR empirical weight distribution. quantization HYPONYM-OF techniques. pruning HYPONYM-OF techniques. full variational distribution USED-FOR coding schemes. compression rates EVALUATE-FOR coding schemes. KullbackLeibler divergence FEATURE-OF sampled variational distribution. random sample USED-FOR network weights. constraint USED-FOR compression rate. constraint FEATURE-OF Kullback - Leibler divergence. encoding scheme COMPARE information - theoretical lower bound. information - theoretical lower bound COMPARE encoding scheme. variational family USED-FOR information - theoretical lower bound. variational family USED-FOR encoding scheme. it COMPARE approaches. approaches COMPARE it. method USED-FOR neural network compression. VGG-16 / CIFAR-10 EVALUATE-FOR approach. fixed memory budget EVALUATE-FOR approach. compression rates EVALUATE-FOR approach. compression rates EVALUATE-FOR it. OtherScientificTerm are memory footprint, deterministic weights, weight determinism, encoding distribution, and expected loss. Method is bits - back argument. ",This paper proposes a method for neural network compression based on sampling random weights from a variational distribution. The method is based on the Kullback-Leibler divergence (KLD) between the weights and the expected loss. The KLD is used to constrain the compression rate of the network weights. The authors show that the KLD can be used to reduce the memory footprint of neural networks.   ,This paper proposes a new compression method for neural network compression. The proposed method is based on the Kullback-Leibler divergence (KL divergence) between the weights of a neural network and a random sample from a variational distribution. The KL divergence is defined as the difference between the expected loss of the weights and the expected compression rate of the network weights. The authors show that the proposed method can achieve better compression rates than existing methods. 
9110,SP:ad70d8cf3a4558aab0d3b7155594464a3debd912,neural network architectures USED-FOR Neural architecture search ( NAS ). architectures USED-FOR large - scale tasks. ImageNet HYPONYM-OF large - scale tasks. GPU hours EVALUATE-FOR Differentiable NAS. continuous representation of network architecture USED-FOR Differentiable NAS. continuous representation of network architecture USED-FOR GPU hours. proxy tasks USED-FOR they. architectures USED-FOR proxy tasks. large - scale target tasks CONJUNCTION hardware platforms. hardware platforms CONJUNCTION large - scale target tasks. architectures USED-FOR large - scale target tasks. ProxylessNAS USED-FOR architectures. ProxylessNAS USED-FOR large - scale target tasks. GPU hours CONJUNCTION GPU memory. GPU memory CONJUNCTION GPU hours. computational cost EVALUATE-FOR regular training. GPU hours HYPONYM-OF computational cost. GPU memory HYPONYM-OF computational cost. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. CIFAR-10 EVALUATE-FOR model. test error EVALUATE-FOR model. ImageNet EVALUATE-FOR model. model COMPARE MobileNetV2. MobileNetV2 COMPARE model. ImageNet EVALUATE-FOR MobileNetV2. top-1 accuracy EVALUATE-FOR MobileNetV2. GPU latency EVALUATE-FOR model. top-1 accuracy EVALUATE-FOR model. ProxylessNAS USED-FOR neural architectures. Neural architecture search ( NAS ) USED-FOR neural network architecture design. neural network architecture design USED-FOR deep learning tasks. Neural architecture search ( NAS ) USED-FOR deep learning tasks. image recognition CONJUNCTION language modeling. language modeling CONJUNCTION image recognition. image recognition HYPONYM-OF deep learning tasks. language modeling HYPONYM-OF deep learning tasks. models USED-FOR task. NAS USED-FOR large - scale task. ImageNet HYPONYM-OF large - scale task. building blocks USED-FOR proxy tasks. top - performing blocks USED-FOR large - scale target task. paradigm USED-FOR NAS algorithms. blocks USED-FOR proxy tasks. latency HYPONYM-OF hardware metrics. methods USED-FOR transferability. Proxy,"This paper proposes ProxylessNAS, a method to reduce the computational cost of Neural Architecture Search (NAS) for large-scale image classification tasks. The main idea is to use a continuous representation of the architecture of the target task as a proxy task, which is then used to compute the top-performing blocks for the proxy task. The method is evaluated on ImageNet on CIFAR-10 and ImageNet with MobileNetV2 and achieves the state-of-the-art results. ","This paper proposes a new approach to perform neural architecture search (NAS) for large-scale tasks. The proposed method, called ProxylessNAS, is based on a continuous representation of network architecture, which can be used as a proxy task for NAS. The method is evaluated on CIFAR-10, ImageNet, and MobileNetV2 datasets, where it outperforms the state-of-the-art on ImageNet. "
9119,SP:e5b70d43d301d1980fae02623ea711976b429c14,"Lagrangian dual FEATURE-OF problem. additive linear penalties USED-FOR Lagrangian dual. non - convex settings FEATURE-OF problem. training procedure USED-FOR non - convex, large - data settings. second - order ones FEATURE-OF linear penalties. secondorder penalties USED-FOR penalized objective. penalty coefficient USED-FOR penalized objective. method USED-FOR gradients. second - order penalties FEATURE-OF gradients. algorithm USED-FOR classifier. Metric is fairness. OtherScientificTerm are linear constraints, constrained objective, Lagrangian, deterministic saddle - point equilibrium, instability, and stochastic mini - batch settings. Method is two - player min - max games. ","This paper studies the problem of penalizing the Lagrangian of a classifier in a two-player min-max game with linear constraints, where the objective function is constrained by a second-order penalty. The main contribution of the paper is to show that the penalized objective can be approximated by an additive linear penalty, which is equivalent to the second order penalty of the Lagrange dual of the problem. The authors then show that this penalization can be used to compute the gradients of the classifier, which can then be used as a regularization term to improve the performance of the model.   ","This paper studies the problem of two-player min-max games with two players, where the objective is to find a Lagrangian-constrained classifier that maximizes the maximum score of the two players. The main contribution of the paper is to study the problem in a non-convex setting, and to show that the penalty of the second-order linear penalty can be penalized by a Lagrangeian dual. The authors show that this penalization can be used to improve the performance of the classifier. They also provide a theoretical analysis of the problem, showing that the penalized objective can be approximated by a second order linear penalty."
9128,SP:e4720b8e4efdb222c45eafd47fd8a7fbf15d881d,"Sampling discrete latent variables USED-FOR highvariance gradient estimators. continuous - relaxation methods USED-FOR latter. control - variate schemes USED-FOR former. branch paths PART-OF model. control - variate schemes CONJUNCTION continuous - relaxation methods. continuous - relaxation methods CONJUNCTION control - variate schemes. control - variate schemes USED-FOR state - of - the - art methods. state - of - the - art methods USED-FOR discrete latent - variable models. it COMPARE state - of - the - art methods. state - of - the - art methods COMPARE it. models CONJUNCTION inference networks. inference networks CONJUNCTION models. importance weighted autoencoder COMPARE RWS. RWS COMPARE importance weighted autoencoder. RWS USED-FOR inference networks. RWS USED-FOR models. RWS USED-FOR deep generative models. Method are Discrete latent - variable models, and continuous latentvariable models. OtherScientificTerm is pathwise derivative. ","This paper proposes a method for sampling discrete latent variable models with high-variance gradient estimators. The main idea is to use a pathwise derivative (RWS) estimator to estimate the gradients of the model. The RWS estimator is based on the fact that the pathwise derivatives of the discrete latent variables can be approximated by a set of branch paths in the model, which can be computed using a control-variate scheme. The authors show that RWS can be used to compute the gradient estimator of a continuous latent variable model, and that it can be combined with continuous-relaxation methods.  The authors also show that the RWS method can be applied to deep generative models. ","This paper proposes a novel method for sampling discrete latent variables from a continuous latent variable model. The main idea is to use a high-variance gradient estimator to estimate the gradient of a discrete latent variable, and then use a control-variate scheme to sample the latent variable. The authors show that this method can be combined with other control-vacuum-based methods, such as continuous-relaxation methods, to achieve state-of-the-art results. They also show that their method outperforms the state of the art."
9137,SP:7459ae5b1d886e68930c4c9e21df508bc8ab3c9a,"human knowledge CONJUNCTION non - differentiable pipelines. non - differentiable pipelines CONJUNCTION human knowledge. non - differentiable pipelines USED-FOR scalar reward function. human knowledge USED-FOR scalar reward function. scalar reward function USED-FOR tasks. truncated randomized search USED-FOR structured prediction energy networks ( SPENs ). truncated randomized search USED-FOR reward function. structured prediction energy networks ( SPENs ) USED-FOR test - time inference. gradient - based search USED-FOR structured prediction energy networks ( SPENs ). gradient - based search USED-FOR test - time inference. truncated randomized search USED-FOR unknown local improvements. supervision USED-FOR SPENs. truncated randomized search USED-FOR supervision. truncated randomized search USED-FOR reward function. Task are structured output prediction tasks, and structured prediction. OtherScientificTerm are output space, and score landscape. Material is labeled training data. ","This paper proposes a novel method for learning a reward function for structured output prediction tasks. The proposed method is based on the idea of structured prediction energy networks (SPENs), which is a non-differentiable pipeline that learns a scalar reward function from a set of labeled training data. The reward function is learned using a truncated randomized search (TRS) algorithm. The authors show that the proposed method outperforms the state-of-the-art methods in terms of test-time inference.  ","This paper proposes a novel method for training structured prediction energy networks (SPENs) for structured output prediction tasks. SPENs are trained using truncated randomized search (TRS), where the goal is to find a scalar reward function that maximizes the performance of the SPEN. The authors show that TRS can be used in conjunction with supervised supervision to improve performance on the task. They also provide a theoretical analysis of the effect of TRS on the score landscape."
9146,SP:638c1bc09992029b78bd83f0127594dcccb96c06,"It USED-FOR transferring policies. simulation environment FEATURE-OF transferring policies. these USED-FOR robust policies. active learning based framework USED-FOR model parameters. EffAcTS HYPONYM-OF active learning based framework. framework USED-FOR method. sample efficiency EVALUATE-FOR approach. EPOpt HYPONYM-OF method. continuous control tasks EVALUATE-FOR approach. Multi - Task Learning perspective USED-FOR Robust Policy Search. framework COMPARE Multi - Task Learning. Multi - Task Learning COMPARE framework. Task is learning policies. OtherScientificTerm are environment model parameters, and policies. Generic is approaches. ","This paper proposes a method for learning robust policies in a multi-task learning setting, where the goal is to transfer robust policies from one task to another. The main idea is to learn a set of robust policies that can transfer well across multiple tasks. The proposed method is based on an active learning approach where the model parameters are updated during the training process. The method is evaluated on a variety of continuous control tasks.   ",This paper proposes an active learning-based framework for multi-task learning for robust policy search. The main idea is to learn a set of robust policies that can be used to transfer from one task to another in a simulation environment. The proposed method is evaluated on a variety of continuous control tasks and shows promising results. 
9155,SP:491c239713a6489f0b1790ca26db54a1813c67ae,"policy evaluation CONJUNCTION control. control CONJUNCTION policy evaluation. value function USED-FOR policy evaluation. value function USED-FOR control. fixed basis CONJUNCTION fixed representation. fixed representation CONJUNCTION fixed basis. algorithms USED-FOR linear function approximation. fixed basis USED-FOR linear function approximation. fixed representation USED-FOR linear function approximation. temporal difference learning CONJUNCTION Q - learning. Q - learning CONJUNCTION temporal difference learning. extensions USED-FOR nonlinear function approximation. Q - learning HYPONYM-OF methods. temporal difference learning HYPONYM-OF methods. nonlinear gradient temporal difference learning HYPONYM-OF nonlinear function approximation. two - timescale network ( TTN ) architecture USED-FOR linear methods. algorithms USED-FOR nonlinear value estimates. algorithms USED-FOR linear setting. data - efficient least - squares methods CONJUNCTION eligibility traces. eligibility traces CONJUNCTION data - efficient least - squares methods. linear policy evaluation algorithms USED-FOR nonlinear value estimates. eligibility traces CONJUNCTION linear policy evaluation algorithms. linear policy evaluation algorithms CONJUNCTION eligibility traces. algorithms USED-FOR approach. linear policy evaluation algorithms HYPONYM-OF algorithms. data - efficient least - squares methods HYPONYM-OF algorithms. eligibility traces HYPONYM-OF algorithms. dependent features FEATURE-OF linear component. nonlinear value function approximation algorithms USED-FOR policy evaluation and control. TTNs COMPARE nonlinear value function approximation algorithms. nonlinear value function approximation algorithms COMPARE TTNs. TTNs USED-FOR policy evaluation and control. Method are reinforcement learning agents, and nonlinear representation. ",This paper proposes a two-timescale network (TTN) architecture for nonlinear value function approximation in reinforcement learning. The main contribution of this paper is to extend the nonlinear gradient temporal difference learning (TBDL) framework to the linear setting. The authors show that the TTN architecture can be used for non-linear value approximation in policy evaluation and control. The proposed method is shown to be more data-efficient than existing least-squares methods. ,"This paper proposes a two-timescale network (TTN) architecture for nonlinear value function approximation in reinforcement learning. The main idea of the paper is to use the two-time-scale network (2TNN) architecture to learn a nonlinear representation of the value function. The authors show that the proposed method is more data-efficient than existing least-squares methods (e.g., Q-learning) in terms of the number of data points. They also show that their method can be used in combination with other non-linear value approximation algorithms. "
9164,SP:327d606cf3813b00a009a7785e08ef9e11f89493,"intrinsic semantic regularities PART-OF man - made environments. multi - target sub - policy CONJUNCTION Bayesian model. Bayesian model CONJUNCTION multi - target sub - policy. visual inputs USED-FOR multi - target sub - policy. semantic structures USED-FOR Bayesian model. Bayesian model PART-OF LEArning and Planning with Semantics ( LEAPS ). multi - target sub - policy PART-OF LEArning and Planning with Semantics ( LEAPS ). House3D HYPONYM-OF 3D environment. real - world objects FEATURE-OF human - designed indoor scenes. human - designed indoor scenes PART-OF 3D environment. House3D USED-FOR visual navigation tasks. LEAPS COMPARE baselines. baselines COMPARE LEAPS. semantic content USED-FOR baselines. Method are deep reinforcement learning agents, semantic model, and sub - policy. Task is AI. ",This paper proposes a method for planning in 3D environments with semantic regularities. The method is based on a multi-target sub-policy and a Bayesian model. The proposed method is evaluated on visual navigation tasks in House3D. The results show that the proposed method outperforms baselines on the visual navigation task. ,"This paper proposes LEArning and Planning with Semantics (LEAPS), a new method for planning and navigation in a 3D environment. The main idea of LEAPS is to use a multi-target sub-policy, which is trained with a Bayesian model, to predict the semantic structure of visual inputs. The proposed method is evaluated on a House3D environment, where it is shown to outperform other baselines on visual navigation tasks."
9173,SP:d7c26f43bc68d160095b1f50447528843d79edbd,"multi - task perception - related basic knowledge CONJUNCTION driving knowledge. driving knowledge CONJUNCTION multi - task perception - related basic knowledge. perception module CONJUNCTION driving module. driving module CONJUNCTION perception module. perception module PART-OF driving model. driving module PART-OF driving model. driving knowledge USED-FOR it. multi - task perception - related basic knowledge USED-FOR it. segmentation map CONJUNCTION depth map. depth map CONJUNCTION segmentation map. control commands USED-FOR difficult driving task. depth map USED-FOR easier drivingrelated perception problems. depth map CONJUNCTION pixel level understanding of images. pixel level understanding of images CONJUNCTION depth map. generalization CONJUNCTION accident explanation ability. accident explanation ability CONJUNCTION generalization. multitask perception knowledge USED-FOR accident explanation ability. multitask perception knowledge USED-FOR generalization. method COMPARE benchmark method. benchmark method COMPARE method. average sucess rate EVALUATE-FOR navigation tasks. average sucess rate EVALUATE-FOR benchmark method. trained weather CONJUNCTION untrained weathers. untrained weathers CONJUNCTION trained weather. method USED-FOR navigation tasks. average sucess rate EVALUATE-FOR method. Method are deep learning driving models, and driving models. OtherScientificTerm is unobserved driving environment. Material is diversity of training driving dataset. ","This paper proposes to combine vision and driving knowledge to improve the performance of vision-based driving models. In particular, the authors propose to use a combination of a vision module and a driving module to jointly learn to navigate in an unobserved driving environment. The vision module is trained to predict the location of the object in the unobserved environment, while the driving module is used to map the object to the predicted location using a depth map. The proposed method is evaluated on a variety of driving tasks, and achieves better performance compared to baselines.  ","This paper proposes a method for training a multi-task perception-related driving model that can be combined with a driving model. The proposed method is based on a combination of two existing methods: (1) learning a depth map and a segmentation map, and (2) using these two maps to learn a driving-related perception problem. The method is evaluated on a variety of driving tasks, and it is shown to outperform the state-of-the-art in terms of generalization."
9182,SP:b6bd98cc70fab97e1245cbb63a42ef89ab7e7ed5,adversarial robustness CONJUNCTION generalization. generalization CONJUNCTION adversarial robustness. accuracy EVALUATE-FOR model. adversarial perturbations FEATURE-OF robustness. robustness EVALUATE-FOR model. robust classifiers COMPARE classifiers. classifiers COMPARE robust classifiers. robust classifiers USED-FOR feature representations. feature representations COMPARE classifiers. classifiers COMPARE feature representations. salient data characteristics CONJUNCTION human perception. human perception CONJUNCTION salient data characteristics. robust models USED-FOR features. ,This paper studies the relationship between adversarial robustness and generalization in the presence of adversarial perturbations. The authors show that robust classifiers are able to learn feature representations that are more robust to adversarial attacks than classifiers without adversarial training. They further show that feature representations learned by robust models are similar to features learned by humans in terms of salient data characteristics.  ,This paper studies the relationship between adversarial robustness and generalization in the presence of salient data characteristics and human perception. The authors show that robust classifiers can generalize better than classifiers that do not generalize robustly. They also show that the robustness of a robust classifier is related to the generalization ability of the classifier. They show that a robust model that generalizes robustly is more robust than a classifier that is not robust. 
9191,SP:9c9275d75cd95b1b82e0cbb1421e3d3ade1ce33a,"backpropagation HYPONYM-OF reverse - mode automatic differentiation. reverse - mode automatic differentiation USED-FOR Deep neural networks. method USED-FOR gradient - based training of neural networks. Equilibrium Propagation USED-FOR gradient - based training of neural networks. local learning rules USED-FOR gradient - based training of neural networks. local learning rules USED-FOR method. iterative optimization of neural activations USED-FOR inference. iterative inference procedure USED-FOR Equilibrium propagation. feedforward network USED-FOR iterative inference procedure. feedforward network USED-FOR Initialized Equilibrium Propagation. local learning rule USED-FOR feed - forward network. initializing network USED-FOR inference. initializing network USED-FOR feedforward network. network COMPARE Equilibrium propagation. Equilibrium propagation COMPARE network. backpropagation USED-FOR deep networks. Method is Biological networks. OtherScientificTerm are gradients, neurons, and error gradient. ","This paper proposes a new method for gradient-based training of neural networks. The proposed method is based on equilibrium propagation, which is a local learning rule to learn the weights of the network. The main idea is to use a feedforward network to compute the gradient of the weights and then use an initializing network to update the weights. The method is shown to outperform backpropagation in terms of accuracy and training time.  ","This paper proposes a new method for gradient-based training of neural networks. The main idea is to use a feedforward network to learn a local learning rule for the initializing network, which is then used to learn the local learning rules for the final network. The authors show that the proposed method outperforms the state-of-the-art backpropagation method in terms of accuracy. "
9200,SP:ac9ea91eb465517de495477cf67bc94d5ed1b0cb,gradient - free operations CONJUNCTION signSGD. signSGD CONJUNCTION gradient - free operations. gradient - free operations PART-OF ZO - signSGD. signSGD PART-OF ZO - signSGD. latter COMPARE SGD - type algorithms. SGD - type algorithms COMPARE latter. convergence speed EVALUATE-FOR SGD - type algorithms. convergence speed EVALUATE-FOR latter. sign information of gradient estimates USED-FOR latter. ZO - signSGD COMPARE signSGD. signSGD COMPARE ZO - signSGD. gradient estimators USED-FOR ZO - signSGD. gradient estimators USED-FOR convergence. ZO - signSGD CONJUNCTION black - box adversarial attacks. black - box adversarial attacks CONJUNCTION ZO - signSGD. ZO - signSGD USED-FOR robust deep learning. black - box adversarial attacks USED-FOR robust deep learning. ZO - signSGD USED-FOR generation of adversarial examples. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. image classification datasets EVALUATE-FOR ZO - signSGD. CIFAR-10 HYPONYM-OF image classification datasets. black - box neural networks USED-FOR generation of adversarial examples. Metric is convergence rate. OtherScientificTerm is optimization variables. ,"This paper proposes a new gradient-based SGD algorithm called ZO-signSGD, which uses sign-based gradient estimators to estimate the gradient of the gradient with respect to the optimization variables. The authors show that the sign information of gradient estimates can be used to improve the convergence rate of the proposed method. The proposed method is evaluated on image classification tasks on MNIST and CIFAR-10.   ",This paper proposes a new algorithm ZO-signSGD for black-box adversarial training. The main idea is to use the sign information of gradient estimates of gradient estimators to improve the convergence of signSGD. The authors show that the convergence rate of the proposed method is faster than the convergence speed of sign SGD. They also show that it can be used to improve robustness against black box adversarial attacks.
9209,SP:5f79b11777f6ef1d70c85418bfc2e4616dd7d960,"Deep learning USED-FOR artificial intelligence applications. energy - limited edge device USED-FOR complex neural network model. optimization method USED-FOR convolutional neural networks. multiply - accumulate ( MAC ) operations PART-OF convolutional filter. checkpoint USED-FOR MAC process. fine - tuning process USED-FOR accuracy drop. CIFAR-10 example model CONJUNCTION Network in Network. Network in Network CONJUNCTION CIFAR-10 example model. MAC operations EVALUATE-FOR method. CIFAR-10 and CIFAR-100 datasets EVALUATE-FOR Network in Network. accuracy drop EVALUATE-FOR method. method COMPARE method. method COMPARE method. CIFAR-100 dataset EVALUATE-FOR method. CIFAR-10 dataset EVALUATE-FOR method. CIFAR-10 dataset EVALUATE-FOR method. Method is convolutional operations. OtherScientificTerm are activation or pooling layers, filter, and checkpoints. ","This paper proposes a method for training convolutional neural networks with multiply-accommodate (MAC) operations in the filter layers. The method is based on the observation that the accuracy of the network can drop when the number of filters in the network increases too much. To address this issue, the authors propose to add a checkpoint to each filter in each layer to ensure that it is close to the output of the previous layer. The proposed method is evaluated on CIFAR-10 and Cifar-100 datasets.   ",This paper proposes a method for fine-tuning a convolutional neural network model with energy-limited edge devices. The proposed method is based on the multiply-accumulate (MAC) operations in the filter of a neural network. The authors show that the accuracy drop of their method is proportional to the number of MAC operations. They also show that their method can improve the accuracy of the model on CIFAR-10/100 datasets.
9218,SP:7801e9c854ad7d960c0d24fda15597af6994c23f,"adversarial examples USED-FOR neural network models. unique data properties USED-FOR learning principles. temporal dependency USED-FOR adversarial examples. temporal dependency USED-FOR discriminate power. temporal dependency FEATURE-OF audio data. automatic speech recognition ( ASR ) tasks CONJUNCTION audio adversarial attacks. audio adversarial attacks CONJUNCTION automatic speech recognition ( ASR ) tasks. temporal dependency USED-FOR discriminative power. image adversarial defense USED-FOR input transformation. robustness EVALUATE-FOR ASR systems. domain - specific data properties USED-FOR adversarial examples. OtherScientificTerm are adversarial inputs, audio adversarial examples, and adaptive attacks. ",This paper studies the problem of robustness to adversarial attacks in the context of audio adversarial examples. The authors propose to use temporal dependency as a measure of the discriminative power of an adversarial attack. They show that the temporal dependency is a property of audio data that is responsible for improving the robustness of the model. They further show that this temporal dependency can be used as a way to improve robustness in the face of domain-specific data properties. ,This paper studies the problem of robustness against audio adversarial attacks. The authors propose a new adversarial defense strategy based on the notion of temporal dependency. They show that the temporal dependency of audio data can be used to improve the discriminative power of adversarial examples. They also propose an adaptive defense strategy that can be applied to the input transformation of the audio data.  
9227,SP:51830b811a8e39b4f0a5b7609df719e026fac6a1,"They USED-FOR representations. images USED-FOR approaches. core inductive biases USED-FOR approaches. generator USED-FOR GAN. composition USED-FOR images. real - world images USED-FOR generative model. multi - object image datasets EVALUATE-FOR approach. generative model USED-FOR images. reference distribution FEATURE-OF images. Method are Deep generative models, and object representations. OtherScientificTerm is representational level. ","This paper proposes a method to improve the performance of generative models on multi-object image datasets. The main idea is to use a generative model to generate images from a reference distribution of images from the reference distribution, and then use the generated images as input to a GAN to generate new images. The proposed method is evaluated on a variety of image datasets, and compared to several baselines. ","This paper proposes a novel approach to learn representations for multi-object image datasets. The key idea is to use a generative model to generate a set of images from a reference distribution of images, and then use a GAN to generate images from the reference distribution. The model is trained to generate the images from this reference distribution, and it is shown that the generated images are more similar to the reference images than the original images. The proposed method is evaluated on a variety of datasets, and compared with a number of baselines."
9236,SP:fb59990b8da0e95d8202383478a456667de60449,"Learning disentangled representations USED-FOR computer vision tasks. visual data USED-FOR Learning disentangled representations. referencebased disentangling HYPONYM-OF learning setting. deep generative model USED-FOR weak supervisory signal. reference - based variational autoencoders HYPONYM-OF deep generative model. reference set USED-FOR weak supervisory signal. adversarial learning USED-FOR objective function. adversarial learning USED-FOR variational inference framework. variational inference framework USED-FOR training. model USED-FOR disentangled representations. feature learning CONJUNCTION conditional image generation. conditional image generation CONJUNCTION feature learning. conditional image generation CONJUNCTION attribute transfer. attribute transfer CONJUNCTION conditional image generation. tasks EVALUATE-FOR model. minimal supervision USED-FOR model. attribute transfer HYPONYM-OF tasks. feature learning HYPONYM-OF tasks. conditional image generation HYPONYM-OF tasks. minimal supervision USED-FOR disentangled representations. OtherScientificTerm are high - level generative factors, target factors, supervision, and factors of interest. Method is Supervised approaches. Generic is representation. ",This paper proposes a novel method for learning disentangled representations from visual data. The proposed method is based on a reference-based variational autoencoder with a weak supervisory signal. The weak supervision is provided by a reference set generated by a generative model that is trained with adversarial training. The method is evaluated on feature learning and conditional image generation tasks. ,"This paper proposes a generative model for learning disentangled representations from visual data. The model is based on a reference-based variational autoencoder with a weak supervisory signal, which is used to generate a reference set for disentanglement. The proposed model is trained using a variational inference framework with adversarial learning. Experiments show that the proposed model outperforms the state-of-the-art on a variety of tasks. "
9245,SP:dbc1983d9b9d72aa14f8e8515d793d2bbde26c9c,"Deep neural network models USED-FOR rapid online adaptation. method USED-FOR continual online learning. deep neural network models USED-FOR method. mixture of models USED-FOR non - stationary task distributions. expectation maximization algorithm USED-FOR mixture of models. stochastic gradient descent USED-FOR model parameters. expectation maximization algorithm USED-FOR non - stationary task distributions. Chinese restaurant process USED-FOR expectation maximization algorithm. stochastic gradient descent USED-FOR online learning procedure. models COMPARE models. models COMPARE models. meta - learning USED-FOR model. SGD USED-FOR online adaptation. meta - learning USED-FOR online learning ( MOLe ) approach. motor failures CONJUNCTION unexpected disturbances. unexpected disturbances CONJUNCTION motor failures. varying terrains CONJUNCTION motor failures. motor failures CONJUNCTION varying terrains. MOLe COMPARE prior methods. prior methods COMPARE MOLe. MOLe USED-FOR continuous adaptation. continuous adaptation USED-FOR non - stationary task distributions. predictive model USED-FOR control. meta - learning USED-FOR model - based reinforcement learning. online learning ( MOLe ) approach USED-FOR model - based reinforcement learning. unexpected disturbances HYPONYM-OF non - stationary task distributions. varying terrains HYPONYM-OF non - stationary task distributions. motor failures HYPONYM-OF non - stationary task distributions. Method are predictive models, and large function approximators. OtherScientificTerm is real - world phenomena. ","This paper proposes a meta-learning approach for continual online learning with a mixture of models for non-stationary task distributions. The main idea is to use a large-function approximation to estimate the model parameters and use this approximation to train a new model for each new task. The method is evaluated on a variety of tasks, including continuous control tasks with varying terrains and motor failures. The results show that the proposed method outperforms the baselines on these tasks. ","This paper proposes a meta-learning method for continual online learning (MOLe) for non-stationary task distributions. The method is based on a Chinese restaurant process, where a mixture of models is used to learn a model for each task, and a large function approximator is used for large function approximation. The authors show that the method can be applied to a variety of tasks, including motor failures, disturbances, and varying terrains. They also show that it can be used for continuous adaptation, where it is possible to adapt a model to a new task in a continuous manner. "
9254,SP:5665e5f006f84927beb0440e145f476e02538077,"distributed prioritized experience replay USED-FOR RNN - based RL agents. representational drift CONJUNCTION recurrent state staleness. recurrent state staleness CONJUNCTION representational drift. parameter lag USED-FOR representational drift. single network architecture CONJUNCTION hyperparameters. hyperparameters CONJUNCTION single network architecture. single network architecture USED-FOR agent. Recurrent Replay Distributed DQN HYPONYM-OF agent. It HYPONYM-OF agent. human - level performance EVALUATE-FOR It. human - level performance EVALUATE-FOR agent. Atari games EVALUATE-FOR It. Method are distributed training of RL agents, and training strategy. Material are Atari-57, and DMLab-30. ","This paper proposes a distributed prioritized experience replay (DQN) method for distributed reinforcement learning. The proposed method is based on the idea of replay replay, which is used in prior work to improve the performance of reinforcement learning agents on Atari games. The main contribution of this paper is to propose a method to address the representational drift and recurrent state staleness in distributed RL. The authors show that the proposed method can be used in conjunction with a single network architecture and hyperparameters to improve performance on Atari-57 and Atari-30.","This paper proposes a new distributed prioritized experience replay (DQN) strategy for distributed RL agents. The proposed method is based on the notion of representational drift, which is defined as the difference between the representation of a state and the state of the agent. The authors propose to use a single network architecture and a single hyperparameter to control the representation drift. They show that the proposed method outperforms the state-of-the-art on Atari-57 and DMLab-30. "
9263,SP:47ace37f31a46d5ee85c283e62ddb71a12f2c5c4,sequential generative models USED-FOR coordinated multi - agent trajectory behavior. offensive basketball gameplay HYPONYM-OF coordinated multi - agent trajectory behavior. hierarchical models USED-FOR long - term coordination. hierarchical models USED-FOR settings. intermediate variables USED-FOR hierarchical models. intermediate variables USED-FOR high - level behavioral semantics. hierarchical framework USED-FOR sequential generative models. programmatically produced weak labels USED-FOR spatiotemporal regime. programmatically produced weak labels USED-FOR approach. framework USED-FOR complex interactions between basketball players. framework USED-FOR realistic multi - agent trajectories of basketball gameplay. quantitative and qualitative evaluations EVALUATE-FOR approach. OtherScientificTerm is synthetic settings. ,This paper proposes a hierarchical generative model for long-term multi-agent trajectory modeling in offensive basketball. The proposed model is based on a hierarchical model that learns a sequence of intermediate variables that encode the high-level behavioral semantics. The intermediate variables are programmatically produced weak labels for the spatiotemporal regime. The authors show that the proposed model can learn complex interactions between basketball players and achieve state-of-the-art performance in both quantitative and qualitative evaluations.,This paper proposes a hierarchical generative model to model long-term coordination between multi-agent trajectories of basketball players. The model is based on a hierarchical model that learns to model the high-level behavioral semantics of each agent. The authors show that the model can be applied to both synthetic and real-world settings. They also show that their model is able to generate realistic trajectories in the spatiotemporal regime. 
9272,SP:1a90cdf028068528b0559e7d44bf26dda20310bd,vision model USED-FOR interacting agents. method USED-FOR temporal information. ambiguous visual information USED-FOR dynamics model. dynamics model USED-FOR method. method COMPARE baselines. baselines COMPARE method. one CONJUNCTION one. one CONJUNCTION one. one EVALUATE-FOR method. one EVALUATE-FOR method. soccer game engine USED-FOR one. real basketball trajectories USED-FOR one. one HYPONYM-OF sports datasets. one HYPONYM-OF sports datasets. sports datasets EVALUATE-FOR method. sports datasets EVALUATE-FOR baselines. ,"This paper proposes a method for learning to predict future trajectories in video games. The proposed method is based on a vision model and a dynamics model. The vision model is used to predict the future trajectory of an agent in a video game. The dynamics model is then used to train a game engine to predict trajectories. The game engine is trained using a set of trajectories generated by the dynamics model, which is trained in an end-to-end fashion.   The method is evaluated on two real-world basketball datasets and one synthetic basketball dataset. The results show that the proposed method outperforms baselines on both datasets.","This paper proposes a method to train a vision model for multi-agent multi-player video games. The vision model is trained using a dynamic dynamics model, which is used to predict the trajectories of the agents. The dynamics model is then used to generate trajectories for the agents, which are then used as input to the game engine. The authors show that their method outperforms the state-of-the-art baselines on two sports datasets. "
9281,SP:8392f04b7265f665ba6d44d297bca245d44b4708,"Deep neural networks USED-FOR approximating complicated functions. gradient descent methods USED-FOR Deep neural networks. neural network USED-FOR functionality. method USED-FOR end - to - end training. base neural network USED-FOR end - to - end training. method USED-FOR base neural network. differentiable neural network USED-FOR black - box functionality. differentiable estimator CONJUNCTION external blackbox non - differentiable counterpart. external blackbox non - differentiable counterpart CONJUNCTION differentiable estimator. neural network USED-FOR input to blackbox functionality. Estimate and Replace ” paradigm USED-FOR neural network. black - box function USED-FOR integrated model. integrated model COMPARE fully differentiable model. fully differentiable model COMPARE integrated model. black - box function USED-FOR inference. black - box function USED-FOR fully differentiable model. integrated model COMPARE RL - based methods. RL - based methods COMPARE integrated model. Task are training, and end - to - end optimization process. Generic is task. OtherScientificTerm are black - box functions, blackbox functions, black - box function interface, and intermediate labels. Method is base network. ",This paper proposes a method for training deep neural networks to approximate black-box functions. The main idea is to use a differentiable estimator and an external blackbox non-differentiable counterpart to estimate and replace the input to the blackbox function interface. The proposed method is evaluated on image classification tasks and compared with the state-of-the-art methods.,"This paper proposes a new method for training deep neural networks with black-box functions. The main idea is to use the Estimate and Replace (ER) paradigm, where the base neural network is trained with a differentiable estimator and the black box function interface is replaced with a non-differentiable counterpart. The authors show that the integrated model can outperform a fully differentiable model in terms of training accuracy and inference accuracy. "
9290,SP:13fb86de763a0b34ac6fa34ea9dfbd1c476ce43e,learning USED-FOR task. data - driven inductive bias USED-FOR learning. gradient - based meta - learning CONJUNCTION hierarchical Bayes. hierarchical Bayes CONJUNCTION gradient - based meta - learning. function approximator USED-FOR mixture of hierarchical Bayesian models. neural network HYPONYM-OF function approximator. stochastic expectation maximization procedure USED-FOR parameter initializations. parameter initializations USED-FOR gradient descent. stochastic expectation maximization procedure USED-FOR gradient descent. stochastic expectation maximization procedure USED-FOR latent assignment of tasks. initializations USED-FOR latent assignment of tasks. approach USED-FOR diversity of training tasks. inductive biases PART-OF hyperparameters. miniImageNet benchmark EVALUATE-FOR 1 - shot classification. miniImageNet benchmark EVALUATE-FOR generalization. method USED-FOR task distribution. non - parametric variant USED-FOR task distribution. method USED-FOR non - parametric variant. few - shot regression tasks EVALUATE-FOR non - parametric variant. OtherScientificTerm is transfer. ,"This paper proposes a meta-learning approach for learning tasks with data-driven inductive bias. The proposed method is based on a mixture of hierarchical Bayesian models and a function approximator. The authors show that this mixture of Bayes models can be used to train a neural network to approximate the task distribution, which is then used as the initialization for gradient descent. The method is evaluated on mini-ImageNet classification and few-shot regression tasks. ",This paper proposes a new method for meta-learning with data-driven inductive bias. The main idea is to use a function approximator to learn a mixture of hierarchical Bayesian models. The authors propose a stochastic expectation maximization procedure to maximize the latent assignment of tasks. They also propose a non-parametric variant of their method for few-shot regression tasks. 
9299,SP:a410144dbe19713a06c63da87d9fb58b999a7492,Auxiliary learning USED-FOR principal task. domain knowledge USED-FOR manually - defined auxiliary tasks. auxiliary tasks USED-FOR auxiliary tasks. Meta Auxiliary Learning ( MAXL ) USED-FOR image classification. hierarchical sub - class image classification HYPONYM-OF auxiliary task. meta learner USED-FOR sub - class target labels. meta learner USED-FOR multi - task evaluator. MAXL COMPARE baseline auxiliary learning methods. baseline auxiliary learning methods COMPARE MAXL. CIFAR datasets EVALUATE-FOR MAXL. MAXL COMPARE method. method COMPARE MAXL. CIFAR datasets EVALUATE-FOR baseline auxiliary learning methods. human - defined sub - class hierarchies USED-FOR method. MAXL USED-FOR automated generalisation. OtherScientificTerm is human knowledge. ,"This paper proposes a meta-augmentation method for image classification. The proposed method is based on meta-learning, where a meta learner is trained to predict the sub-class target labels for each auxiliary task, and a multi-task evaluator is used to evaluate the performance of the meta-learner. The experiments show that the proposed method achieves state-of-the-art performance on CIFAR-10 and ImageNet.   ","This paper proposes a meta auxiliary learning method for image classification. The proposed method is based on a meta learner that learns a hierarchical sub-class image classification task, and then uses a multi-task evaluator to evaluate the performance of the auxiliary tasks. The method is evaluated on CIFAR-10 and ImageNet datasets, and outperforms the state-of-the-art."
9308,SP:76248e1c914c60ce69de244fe7ec62488d01e161,neural network based representation USED-FOR open set recognition problem. datasets EVALUATE-FOR approaches. Generic is representation. ,"This paper proposes a neural network based representation learning method for open set recognition. The proposed method is based on the idea that open sets can be represented as a set of data points, and the goal is to learn a representation of the data points that can be used to classify the data in an open set. The method is evaluated on a variety of open set datasets.   ",This paper proposes a novel neural network based representation for the open set recognition problem. The main idea is to use a neural network-based representation to represent the set of open sets. The proposed method is based on the idea that each set is represented as a set of neurons in a network. The authors show that the proposed method can be applied to a wide range of datasets. They also show that it can be used in conjunction with a variety of existing methods.  
9317,SP:d4ee856bbf2dfb6390e5247086fec2e52dcb6858,"ResNet-34 CONJUNCTION ResNet-50. ResNet-50 CONJUNCTION ResNet-34. ResNet-50 CONJUNCTION ResNet-152. ResNet-152 CONJUNCTION ResNet-50. Inception - v3 CONJUNCTION densenet161. densenet161 CONJUNCTION Inception - v3. densenet161 CONJUNCTION VGG-16bn networks. VGG-16bn networks CONJUNCTION densenet161. ResNet-152 CONJUNCTION Inception - v3. Inception - v3 CONJUNCTION ResNet-152. accuracy EVALUATE-FOR full - precision baseline networks. finetuning USED-FOR full - precision baseline networks. ImageNet classification benchmark EVALUATE-FOR VGG-16bn networks. ResNet-18 CONJUNCTION ResNet-34. ResNet-34 CONJUNCTION ResNet-18. accuracy EVALUATE-FOR full - precision baseline networks. stochastic gradient descent USED-FOR training error. pretrained fp32 precision baseline networks CONJUNCTION fine - tuning. fine - tuning CONJUNCTION pretrained fp32 precision baseline networks. pretrained fp32 precision baseline networks USED-FOR solution distance. matched learning rate annealing USED-FOR combat noise. techniques USED-FOR low - precision networks. techniques CONJUNCTION activation function range calibration. activation function range calibration CONJUNCTION techniques. activation function range calibration USED-FOR low - precision networks. Task is embedded deep network inference. Metric are energy and area efficiency, and energy. Method are pretrained models, and fp32 precision baseline networks. Generic are 4 - bit models, baseline networks, noise, and they. OtherScientificTerm are cosine similarity, gradient noise, quantization, and maximum variance of the gradient estimates. ","This paper studies the effect of cosine similarity on the accuracy of deep neural networks trained with fp32 precision on ImageNet classification tasks. The authors show that the cosine distance between the solution and the gradient of the training set is a function of the gradient noise, and that this distance can be computed using a matched learning rate annealing method. The paper also shows that the energy and area efficiency of the model is affected by cosine distances between the solutions.   ","This paper studies the problem of energy efficiency in embedded deep network inference. The authors show that the energy efficiency of fp32-precision baseline networks can be improved by fine-tuning the accuracy of the training error. They show that this can be done by quantizing the cosine similarity of the gradient noise. They also show that it is possible to fine-tune the accuracy by matching the learning rate annealed with a matched learning rate. Finally, they show that low-probability networks with low precision can be finetuned with low-accuracy baseline networks."
9326,SP:6bfdc37b346e6ddfa049e0414647f4beda8ede3f,approach USED-FOR surface properties. model USED-FOR post - bounce trajectories. bouncing restitution CONJUNCTION effective collision normals. effective collision normals CONJUNCTION bouncing restitution. model USED-FOR physical properties. sensor inputs USED-FOR post - bounce trajectories. physical properties USED-FOR bouncing restitution. physical properties USED-FOR effective collision normals. sensor inputs USED-FOR model. Physics Inference Module ( PIM ) CONJUNCTION Visual Inference Module ( VIM ). Visual Inference Module ( VIM ) CONJUNCTION Physics Inference Module ( PIM ). modules PART-OF model. Bounce PART-OF model. Visual Inference Module ( VIM ) HYPONYM-OF modules. Physics Inference Module ( PIM ) HYPONYM-OF modules. Visual Inference Module ( VIM ) PART-OF model. PIM USED-FOR physical interactions. PIM USED-FOR prediction task. VIM USED-FOR physical parameters. physical parameters CONJUNCTION observed pre - collision 3D trajectories. observed pre - collision 3D trajectories CONJUNCTION physical parameters. physical interactions USED-FOR prediction task. physical parameters USED-FOR PIM. observed pre - collision 3D trajectories USED-FOR PIM. dataset EVALUATE-FOR model. dataset EVALUATE-FOR baselines. predicting post - bounce trajectories CONJUNCTION physical properties. physical properties CONJUNCTION predicting post - bounce trajectories. model COMPARE baselines. baselines COMPARE model. model USED-FOR predicting post - bounce trajectories. model USED-FOR physical properties. trajectory fitting USED-FOR post - bounce trajectories. Newtonian physics USED-FOR trajectory fitting. trajectory fitting HYPONYM-OF baselines. Material is Bounce Dataset. OtherScientificTerm is physics simulations. ,"This paper proposes a method for predicting post-collision 3D trajectories based on the bouncing restitution and effective collision normals. The method is based on a combination of physics and visual information from the bounce dataset. The bouncing restitution is defined as the sum of the bouncing normals of a set of trajectories that have been bouncing in the same direction as the target trajectory. The effective collision norms are defined as a function of the physical properties of the surface.    The proposed method is evaluated on the Bounce dataset, where it is shown to outperform the baselines in terms of trajectory prediction accuracy. ","This paper proposes a new model for predicting post-bounce trajectories based on physical properties of the surface. The model is built on top of a physics-inference module (PIM) and a Visual Inference Module (VIM), which are used to predict post- bounce trajectories. Theoretical analysis is provided to show that the model is able to predict bouncing restitution and effective collision normals. Experiments are conducted on a dataset of 3D trajectories from the Bounce Dataset. "
9335,SP:010bd055310c363d3cb0fbe0e11546de58220e15,"neural networks USED-FOR adversarial images. gradients USED-FOR adversarial vulnerability. ` 1 - norm FEATURE-OF gradients. OtherScientificTerm are targeted but imperceptible image perturbations, image size, and network ’s weight distribution. Method is network architectures. Generic is nets. ","This paper studies the adversarial vulnerability of deep neural networks in the presence of small perturbations to the input image. The authors show that the gradients of the weights of the network are highly correlated with the size of the perturbation. They show that this is due to the fact that the image size is dependent on the network's weight distribution, and that the gradient of the weight distribution is proportional to the number of perturbed images. They also show that if the image is large enough, then the gradient will be close to the norm of the original image.  ","This paper studies the vulnerability of neural networks to image perturbations. The authors show that the gradients of a neural network can be seen as a function of the size of the image and the weight distribution of the network. They show that if the image size is larger than the weight of the neural network, then the network is vulnerable to the perturbation. They also show that gradients can be used as a measure of the vulnerability.  "
9351,SP:5fa3ae057e55be6b71cc94a7dbfe31e54e1c536f,"agent modeling USED-FOR mind model. probing USED-FOR agent modeling. pure curiosity - driven reinforcement learning USED-FOR probing policy. imitation learning USED-FOR approximated agent model. pure curiosity - driven reinforcement learning HYPONYM-OF learning processes. learning processes PART-OF framework. pure curiosity - driven reinforcement learning PART-OF framework. imitation learning PART-OF framework. imitation learning HYPONYM-OF learning processes. tasks EVALUATE-FOR approach. collaboration CONJUNCTION competition. competition CONJUNCTION collaboration. passive observation CONJUNCTION random probing. random probing CONJUNCTION passive observation. agent model COMPARE ones. ones COMPARE agent model. distilling optimal planning CONJUNCTION collaboration. collaboration CONJUNCTION distilling optimal planning. random probing CONJUNCTION curiositydriven approaches. curiositydriven approaches CONJUNCTION random probing. distilling optimal planning CONJUNCTION policy net. policy net CONJUNCTION distilling optimal planning. curiositydriven approaches USED-FOR ones. random probing USED-FOR agent model. approach USED-FOR agent model. competition HYPONYM-OF applications. distilling optimal planning HYPONYM-OF applications. passive observation USED-FOR agent model. passive observation USED-FOR ones. random probing USED-FOR ones. collaboration HYPONYM-OF applications. Method are interactive agent modeling scheme, and probing agent. ","This paper proposes a curiosity-driven reinforcement learning approach for interactive agent modeling. The proposed approach is based on the observation-based approach, where a probing policy is trained using imitation learning to learn a probabilistic model of the agent's state and action space. The method is evaluated on a variety of tasks, including distilling optimal planning, collaboration, competition, and collaboration with a probing agent.  ","This paper proposes a curiosity-driven reinforcement learning approach for interactive agent modeling. The approach is based on imitation learning, where the agent model is approximated by a probabilistic probing policy. The probing policy is trained using a combination of two learning processes: (1) imitation learning and (2) random probing. Experiments are conducted on a variety of tasks, including distilling optimal planning, collaboration, competition, and collaboration. "
9367,SP:3af184a5529d6ec2a0862efd1af80ef5b50d2952,"modification USED-FOR Artificial Neural Networks ( ANNs ). Artificial Neural Networks ( ANNs ) USED-FOR ANNs. firing modes FEATURE-OF biological neuron. peripheral factors FEATURE-OF biological neuron. neuromodulators HYPONYM-OF peripheral factors. modification USED-FOR ANN nodes. ANN nodes USED-FOR activation sensitivities. Convolutional Neural Networks CONJUNCTION Long Short - Term Memory networks. Long Short - Term Memory networks CONJUNCTION Convolutional Neural Networks. modification COMPARE ANN nodes. ANN nodes COMPARE modification. ANN nodes USED-FOR Convolutional Neural Networks. ANN nodes USED-FOR Long Short - Term Memory networks. OtherScientificTerm are biological neurons, Biological neurons, biological neuromodulators, modulators, and slope of the activation function. ","This paper proposes to modify the activation function of neural networks in order to improve their sensitivity to activation modulators. Specifically, the authors propose to use a modification to the activation of a neural network to control the slope of its activation function. They show that this modification can be used to improve the performance of the network in terms of accuracy and efficiency. They also show that the proposed modification can improve the accuracy of the networks.  ",This paper studies the effect of neural modulators on the activation of biological neurons. The authors show that biological neuromodulators can be used to modify the activation function of a biological neuron. They also show that the effect can be extended to neural networks. They show that neural networks can be modified by modifying the modulators of the neural network.  
9383,SP:287a577834fd2820a939a1113b39146a22727491,voice CONJUNCTION pitch. pitch CONJUNCTION voice. neural analysis and synthesis ( NANSY ) framework USED-FOR voice. neural analysis and synthesis ( NANSY ) framework USED-FOR pitch. information bottleneck USED-FOR analysis features. analysis features USED-FOR controllable synthesis. information perturbation USED-FOR training strategy. formant CONJUNCTION pitch. pitch CONJUNCTION formant. pitch CONJUNCTION frequency response. frequency response CONJUNCTION pitch. reconstruction quality CONJUNCTION controllability. controllability CONJUNCTION reconstruction quality. controllability EVALUATE-FOR it. reconstruction quality EVALUATE-FOR it. wav2vec feature CONJUNCTION pitch feature. pitch feature CONJUNCTION wav2vec feature. Yingram USED-FOR self - supervised training. pitch feature CONJUNCTION Yingram. Yingram CONJUNCTION pitch feature. analysis features USED-FOR NANSY. pitch feature HYPONYM-OF analysis features. wav2vec feature HYPONYM-OF analysis features. Yingram HYPONYM-OF analysis features. selfsupervised training USED-FOR NANSY. NANSY USED-FOR multilingual setting. multilingual dataset USED-FOR NANSY. multilingual dataset USED-FOR it. zero - shot voice conversion CONJUNCTION pitch shift. pitch shift CONJUNCTION zero - shot voice conversion. pitch shift CONJUNCTION time - scale modification. time - scale modification CONJUNCTION pitch shift. NANSY USED-FOR zero - shot voice conversion. NANSY USED-FOR applications. NANSY USED-FOR pitch shift. NANSY USED-FOR time - scale modification. time - scale modification HYPONYM-OF applications. zero - shot voice conversion HYPONYM-OF applications. pitch shift HYPONYM-OF applications. Method is synthesis networks. OtherScientificTerm is bottleneck structures. Material is speech data. ,This paper proposes a method for voice synthesis based on neural analysis and synthesis (NANSY) framework. NANSY uses information perturbation to improve the controllability of voice synthesis. The proposed method is based on self-supervised training with wav2vec and Yingram analysis features. Experiments are conducted on a multilingual dataset and show that the proposed method can achieve good performance on voice synthesis tasks. ,"This paper proposes a new neural analysis and synthesis (NANSY) framework for voice synthesis. NANSY is based on the idea of information bottleneck, which is an information perturbation strategy to improve the controllability of the voice synthesis network. The key idea is to use the information bottleneck as an information bottleneck for the analysis features, which can be used for controllable synthesis. The proposed method is evaluated on a multilingual dataset, and it is shown that it can improve the reconstruction quality of the reconstructed voice. "
9408,SP:90f35ad1ec0c38b0817f5678ee2a5c4f0e08fb38,"gradient - based ) bilevel programming framework USED-FOR hyperparameter optimization. overfitting FEATURE-OF validation set. expectation bound USED-FOR cross - validation algorithm. gradient - based algorithms COMPARE cross - validation. cross - validation COMPARE gradient - based algorithms. regularization terms USED-FOR overfitting problem. regularization terms USED-FOR gradient - based algorithms. overfitting problem PART-OF gradient - based algorithms. outer and inner levels FEATURE-OF regularization terms. feature learning CONJUNCTION data reweighting. data reweighting CONJUNCTION feature learning. data reweighting USED-FOR noisy labels. OtherScientificTerm are optimization properties, and uniform stability. Task is generalization. Method is bilevel programming. ","This paper proposes a bilevel programming framework for hyperparameter optimization in the presence of overfitting. The authors show that the overfitting problem can be solved by a cross-validation algorithm, where the validation set is composed of a set of samples from the training set and a validation set from the test set. The main contribution of the paper is to show that regularization terms can be added to the training data to improve the performance of the cross validation algorithm. ",This paper proposes a bilevel programming framework for hyperparameter optimization for cross-validation. The main contribution of the paper is to provide an expectation bound for the overfitting of the validation set. The expectation bound is based on the assumption of uniform stability and uniform stability of the training data. The authors also provide a theoretical analysis of the generalization properties of gradient-based algorithms.   
9433,SP:42f52aec3a776d87daa5fd72b8e6325d12c88d63,"knowledge distillation approach USED-FOR transfer of dark knowledge. student models USED-FOR methods. algorithm USED-FOR student - friendly representations. algorithm USED-FOR student branches. knowledge distillation methods USED-FOR student models. accuracy CONJUNCTION convergence speed. convergence speed CONJUNCTION accuracy. approach USED-FOR teacher models. technique USED-FOR student models. technique USED-FOR knowledge distillation methods. convergence speed EVALUATE-FOR student models. knowledge distillation techniques EVALUATE-FOR algorithm. teacher and student models USED-FOR knowledge distillation techniques. accuracy EVALUATE-FOR algorithm. Task are knowledge transfer, and knowledge distillation procedure. Method are teacher model, and teacher networks. ","This paper proposes a knowledge distillation approach for transfer of dark knowledge from teacher networks to student networks. The main idea is to learn a student-friendly representation of the teacher network by distilling the knowledge from the student model to the teacher model. The proposed method is based on a two-stage process: first, the student network is trained by distillation from the original teacher network, and second, a student model is trained from the learned representations of the original and the teacher networks.   The main contribution of the paper is to show that the proposed method achieves better accuracy and convergence speed compared to the state-of-the-art methods.","This paper proposes a new knowledge distillation method for the transfer of dark knowledge from teacher models to student models. The key idea is to use a student-friendly representation of the teacher model, which can be used to improve the performance of student distillation methods. The method is based on the idea that the student model should be able to learn representations that are more similar to the teacher representations. The student model is trained with a teacher model and a student model trained with the teacher network. The proposed method is evaluated on a variety of datasets, and it is shown that it can achieve better performance than the state-of-the-art."
9458,SP:e15a1c21229233fd97dc1dfa0a4ef48b69dc9f95,"Generalization PART-OF machine learning. invariant features USED-FOR algorithms. invariance USED-FOR OOD generalization. generalization USED-FOR out - of - distribution. expansion function USED-FOR OOD generalization. model selection module PART-OF OOD learning algorithm. model selection criterion FEATURE-OF theory. model selection criterion COMPARE baselines. baselines COMPARE model selection criterion. benchmark OOD datasets EVALUATE-FOR model selection criterion. benchmark OOD datasets EVALUATE-FOR baselines. Task are extracting invariant features, OOD, and OOD problem. OtherScientificTerm is OOD generalization error bounds. ","This paper studies the problem of out-of-distribution (OOD) generalization in machine learning. In particular, the authors show that the OOD generalization error bound is upper bounded by an expansion function. The authors then propose a model selection criterion to select OOD models that are invariant to OOD data. Theoretical analysis is provided to show the convergence of the proposed method. Empirical results are provided to support the theoretical results. ",This paper studies the problem of out-of-distribution (OOD) generalization in machine learning. The main contribution of the paper is a theoretical analysis of the generalization error bounds for OOD generalization. The generalization bounds are based on the theory of model selection criterion. The authors show that model selection criteria can be used to improve generalization performance of OOD learning algorithms. They also provide a theoretical justification for their results.
9483,SP:37b04b9068d39bcf0a581eb8181d13cf1a8926bf,"stationary distribution USED-FOR meta - learning. Dynamic Gaussian Mixture Model USED-FOR meta - parameters. Dynamic Gaussian Mixture Model USED-FOR VC - BML. Chinese Restaurant Process USED-FOR number of component distributions. Dynamic mixtures USED-FOR meta - parameter level. Dynamic mixtures USED-FOR negative knowledge transfer problem. Dynamic mixtures USED-FOR diverse and dissimilar tasks. structured variational inference USED-FOR avoiding forgetting knowledge. posterior approximation method USED-FOR avoiding forgetting knowledge. point estimation method COMPARE posterior approximation method. posterior approximation method COMPARE point estimation method. structured variational inference HYPONYM-OF posterior approximation method. point estimation method USED-FOR posteriors of model parameters. VC - BML USED-FOR catastrophic forgetting. tasks EVALUATE-FOR VC - BML. non - stationary distributions FEATURE-OF tasks. Task is online setting. OtherScientificTerm are non - stationary distribution, and parameter space. ","This paper proposes a method for meta-learning in the online setting where the meta-parameters are non-stationary. The proposed method is based on Dynamic Gaussian Mixture Model (DGM), which is a Chinese Restaurant Process (CPR) based method to model the number of component distributions. The authors show that the proposed method can avoid catastrophic forgetting and improve the performance of the model on a variety of tasks. ","This paper proposes a new meta-learning framework called Dynamic Gaussian Mixture Model (VC-BML), which is based on the Chinese Restaurant Process (CPR) framework. The main idea is to use a dynamic mixture model to model the meta-parameters at meta-level, and then use structured variational inference to estimate the posterior of the model parameters. The proposed method is evaluated on a variety of tasks, and it is shown that the proposed method can avoid catastrophic forgetting. "
9508,SP:776d5b02b8d3a8bbcc1f52706f3887c384cb149e,"boundary conditions FEATURE-OF ordinary differential equations. it USED-FOR BVPs. Gauss – Markov prior CONJUNCTION it. it CONJUNCTION Gauss – Markov prior. linear time FEATURE-OF posterior distribution. mesh refinement CONJUNCTION hyperparameter adaptation. hyperparameter adaptation CONJUNCTION mesh refinement. uncertainty quantification CONJUNCTION mesh refinement. mesh refinement CONJUNCTION uncertainty quantification. model USED-FOR uncertainty quantification. model USED-FOR mesh refinement. model USED-FOR hyperparameter adaptation. probabilistic BVP solver COMPARE non - probabilistic algorithms. non - probabilistic algorithms COMPARE probabilistic BVP solver. algorithms USED-FOR ODE boundary value problems. first - order problems USED-FOR higher - order problems. manifold learning USED-FOR BVPs. numerical simulation CONJUNCTION probabilistic inference. probabilistic inference CONJUNCTION numerical simulation. lineartime complexity CONJUNCTION adaptive step - size selection. adaptive step - size selection CONJUNCTION lineartime complexity. adaptive step - size selection CONJUNCTION polynomial convergence rates. polynomial convergence rates CONJUNCTION adaptive step - size selection. adaptive step - size selection FEATURE-OF probabilistic solvers. lineartime complexity FEATURE-OF probabilistic solvers. probabilistic solvers USED-FOR initial value problems. Generic are algorithm, and scheme. Method are non - probabilistic methods, statistical modelling tool - chain, and Probabilistic numerical algorithms. Task are Boundary value problems, first - order boundary value problem, machine learning, and Neural Information Processing Systems. OtherScientificTerm are computational pipelines, leftand right - hand side boundary conditions, vector field, ODE knowledge, infectious disease, integration domain, and structured output uncertainty. ","This paper proposes a probabilistic method for solving ODE boundary value problems. The main idea is to use a Gauss-Markov prior to model the posterior distribution of the ODE, which is then used to compute the uncertainty quantification and mesh refinement in the integration domain. The authors show that the proposed method is computationally tractable in linear time. They also show that their method has polynomial convergence rates.","This paper proposes a probabilistic solver for ODE boundary value problems. The main contribution of the paper is a new method for solving the first-order boundary value problem. The method is based on the Gauss-Markov prior, which is a well-known and well-studied technique for solving ODEs. The authors show that the proposed method outperforms the state-of-the-art non-probabilistic methods in terms of lineartime complexity and polynomial convergence rates."
9533,SP:86aac0c6b75fdc12f84bba342934865616f866d4,"partially observable system FEATURE-OF near optimal policy. episodic reinforcement learning USED-FOR reward - mixingMarkov decision process ( MDP ). reward models USED-FOR reward function. near optimal policy USED-FOR reward - mixing MDPs. algorithmic and analysis techniques USED-FOR problem. polynomial - time algorithm USED-FOR -optimal policy. observation space COMPARE latent state space. latent state space COMPARE observation space. Task is reinforcement learning. Method are reward model, and switching reward - models. OtherScientificTerm are dynamics, time - horizon, and partially observed environments. Generic are approaches, assumptions, and algorithm. ","This paper studies the problem of reward-mixing in episodic reinforcement learning. In this setting, the goal is to learn a near-optimal policy in a partially observed MDP with reward mixing. The authors propose a polynomial-time algorithm for learning a near optimal policy in this setting. The algorithm is based on the observation-based approach, where the policy is learned in an episodic fashion, and the reward model is learned by sampling from a set of reward models. ","This paper studies the problem of reward-mixing Markov decision process (MDP) in episodic reinforcement learning, where the goal is to find a near optimal policy for a partially observed system in a partially observable environment. The authors propose a polynomial-time algorithm for finding a near-optimal policy in the partially observed state space. They show that the optimal policy can be found in the observation space and the latent state space, and they show that it can be done in polynomially time. "
9558,SP:1a3c70ae9cf2a806d603f4b9e7ca6e10b720a956,"methods USED-FOR conditional average treatment effect estimation. two - step procedure USED-FOR multi - cause treatment effect. Single - cause Perturbation ( SCP ) USED-FOR multi - cause treatment effect. Single - cause Perturbation ( SCP ) HYPONYM-OF two - step procedure. covariate adjustment USED-FOR estimator. covariate adjustment USED-FOR augmented dataset. It USED-FOR estimator. covariate adjustment USED-FOR It. synthetic and semi - synthetic experiments EVALUATE-FOR SCP. Generic are applications, problem, and procedure. Task are multi - cause treatment effect problems, multi - cause problem, and causal inference. OtherScientificTerm are confounding bias, cause combination, and single - cause interventions. Material is observational dataset. ","This paper studies the conditional average treatment effect estimation in the multi-causal setting. The authors propose a new method called single cause perturbation (SCP) to estimate the treatment effect in this setting. In particular, the authors show that the covariate adjustment of the augmented dataset can be used to improve the estimator of the estimated treatment effect. The proposed method is evaluated on synthetic and semi-synthetic experiments. ","This paper proposes a two-step procedure for conditional average treatment effect estimation for multi-cause treatment effect problem. The main idea is to use single-cause perturbation (SCP) to estimate the multi-causal treatment effect, and then use a covariate adjustment for the augmented dataset to improve the estimator. The empirical results on synthetic and semi-synthetic experiments demonstrate the effectiveness of the proposed method."
9583,SP:247bc6675cce89d51558537daf63dadb0c4307f8,"multiwavelet - based neural operator learning scheme USED-FOR operator ’s kernel. fine - grained wavelets USED-FOR multiwavelet - based neural operator learning scheme. multiwavelet polynomial bases USED-FOR projection of the kernel. Burgers ’ equation CONJUNCTION Darcy Flow. Darcy Flow CONJUNCTION Burgers ’ equation. Darcy Flow CONJUNCTION Navier - Stokes equation. Navier - Stokes equation CONJUNCTION Darcy Flow. Korteweg - de Vries ( KdV ) equation CONJUNCTION Burgers ’ equation. Burgers ’ equation CONJUNCTION Korteweg - de Vries ( KdV ) equation. neural operator approaches COMPARE model. model COMPARE neural operator approaches. state - of - the - art EVALUATE-FOR model. accuracy EVALUATE-FOR neural operator approaches. accuracy EVALUATE-FOR model. relative L2 error EVALUATE-FOR Burgers ’ ( KdV ) equation. method USED-FOR time - varying equations. relative L2 error EVALUATE-FOR method. mappings between function spaces USED-FOR method. lower - resolution data USED-FOR method. OtherScientificTerm are inverse operator map, and complex dependencies. Method are inverse multiwavelet filters, projected kernel, and resolution - independent scheme. ","This paper proposes a multi-wavelet-based neural operator learning scheme to learn the operator’s kernel. The proposed method is based on the projection of the operator's kernel to a polynomial-based polynomials, which are then used to train a neural network to predict the inverse operator map. The method is shown to achieve state-of-the-art performance on time-varying equations.  ","This paper proposes a multi-wavelet-based neural operator learning scheme for time-varying equations (e.g., Burgers’ equation, Darcy flow, Navier-Stokes equation, and Korteweg-de Vries (KdV) equation). The proposed method is based on a multiwavelet polynomial-based projection of the kernel of the operator’s kernel. The authors show that the proposed method outperforms the state-of-the-art in terms of accuracy and relative L2 error. "
9608,SP:1153785e6a016cfee2644952a772aa08927299b6,"Binary neural networks ( BNNs ) USED-FOR full - precision weights. 1 - bit with sign function USED-FOR Binary neural networks ( BNNs ). gradient FEATURE-OF sign function. approximate gradient USED-FOR optimization difficulty. sine functions USED-FOR BNNs. frequency domain approximation ( FDA ) HYPONYM-OF BNNs. Fourier frequency domain FEATURE-OF gradient of sign function. frequency domain approximation ( FDA ) HYPONYM-OF sine functions. low - frequency information FEATURE-OF sign function. noise adaptation module USED-FOR approximation error. benchmark datasets CONJUNCTION neural architectures. neural architectures CONJUNCTION benchmark datasets. method USED-FOR binary network. Method is back - propagation. Generic are approximations, and approach. OtherScientificTerm are factual gradient, and high - frequency coefficients. ",This paper proposes a novel method for training binary neural networks (BNNs) with 1-bit with sign function. The method is based on approximating the gradient of the sign function in the Fourier frequency domain with a sine function.  The main contribution of the paper is to show that the error of the approximation error is bounded by the low-frequency information. The authors then propose to use a noise adaptation module to mitigate the effect of the low frequency information. ,This paper proposes a novel method for training a binary neural network with 1-bit with sign function. The authors propose a novel way to estimate the gradient of the sign function in the Fourier frequency domain of the binary network. The main idea is to use a noise adaptation module to adapt the noise of the input signal to the input noise. The proposed method is evaluated on a variety of datasets and neural architectures.
9633,SP:33b95ea8da4d30b8e8f9d3fe3acca023d4b8d831,cortical areas USED-FOR tasks. Recurrent neural networks ( RNNs ) USED-FOR cortical areas. Recurrent neural networks ( RNNs ) USED-FOR neuroscience - based tasks. cortical area USED-FOR tasks. multi - area RNNs USED-FOR multi - area computation. neuroscience - inspired architecture constraints FEATURE-OF multi - area RNNs. Dale ’s Law USED-FOR networks. full observability FEATURE-OF RNNs. full observability USED-FOR output - relevant information. modular computation USED-FOR minimal sufficient representations of task information. cortex USED-FOR minimal sufficient representations of task information. modular computation USED-FOR cortex. constrained multi - area RNNs USED-FOR computations. distributed computation PART-OF neural systems. OtherScientificTerm is coordination of multiple brain areas. Generic is computation. ,This paper proposes to use multi-area RNNs to perform multi-task computation in the brain. The main idea is to constrain the number of input-output connections in the RNN to be limited to a fixed number of connections in each area. This is motivated by the observation that a large number of neurons in a single area may not be sufficient to perform the task efficiently. The authors show that this is the case in a number of experiments. They also show that the proposed method can be used in conjunction with existing multi-attentional networks.,"This paper proposes a new architecture for multi-area RNNs, which is based on the idea of multi-task coordination in the brain. The main contribution of the paper is that the proposed architecture can be applied to a variety of neuroscientific tasks. The proposed architecture is motivated by the observation of Dale’s Law, which states that the output-relevant information of a multi-attentional network should be fully observable. The authors show that this observation can be used to construct a network that is able to perform a modular computation of tasks in the cortex. They also show that such a network can be combined with other RNN-based architectures, and show that it can be trained to perform modular computation in a way that minimizes the computational complexity of the network."
9658,SP:db3ced65d67e3373fb3936ec50f41c8ef010bbbe,Saliency maps USED-FOR convolutional neural networks ( CNNs ). convolutional neural networks ( CNNs ) USED-FOR image classification. saliency map USED-FOR image of interest. maps USED-FOR classification. confidence EVALUATE-FOR classifier. structured attention graphs ( SAGs ) USED-FOR attention maps. compact and representative SAG USED-FOR visualization. approach USED-FOR compact and representative SAG. diverse sampling USED-FOR approach. diverse sampling USED-FOR compact and representative SAG. diverse sampling USED-FOR visualization. SAGs COMPARE saliency maps. saliency maps COMPARE SAGs. SAGs USED-FOR comparative counterfactual questions. saliency maps USED-FOR comparative counterfactual questions. user study USED-FOR comparative counterfactual questions. user study EVALUATE-FOR SAGs. image classifications FEATURE-OF comparative counterfactual questions. SAGs COMPARE saliency map baselines. saliency map baselines COMPARE SAGs. user accuracy EVALUATE-FOR SAGs. user accuracy EVALUATE-FOR saliency map baselines. Method is beam search algorithm. OtherScientificTerm is image regions. ,"This paper proposes to use structured attention graphs (SAGs) to improve the visualization of saliency maps for image classification. The proposed method is based on the idea that the saliency map is a representation of the image of interest, which can be used as a confidence measure for the classifier. The authors propose to use SAGs to learn a compact and representative SAG, which is then used to generate diverse sampling for the visualization. The method is evaluated on image classification tasks and counterfactual questions.   ","This paper proposes a novel approach to visualize the saliency map of a classifier using structured attention graphs (SAGs). The proposed approach is based on the idea of diverse sampling, which allows for compact and representative SAGs. The proposed method is evaluated on a variety of image classification tasks, and it is shown to improve the accuracy of the classifier. "
9683,SP:f2b385bfd9ada0e26aa8829214b424f58582d9f7,loss functions CONJUNCTION regularizers. regularizers CONJUNCTION loss functions. loss functions USED-FOR image classification tasks. image classification tasks EVALUATE-FOR regularizers. test accuracy EVALUATE-FOR loss functions. test accuracy EVALUATE-FOR regularizers. loss functions USED-FOR representations. representations USED-FOR downstream tasks. loss functions USED-FOR downstream tasks. transferability EVALUATE-FOR hidden representations of convolutional neural networks. ImageNet USED-FOR hidden representations of convolutional neural networks. fixed feature extractors USED-FOR downstream tasks. networks USED-FOR tasks. objectives COMPARE vanilla softmax cross - entropy. vanilla softmax cross - entropy COMPARE objectives. ImageNet accuracy EVALUATE-FOR vanilla softmax cross - entropy. ImageNet accuracy EVALUATE-FOR objectives. centered kernel alignment USED-FOR hidden representations of networks. objectives CONJUNCTION hyperparameter combinations. hyperparameter combinations CONJUNCTION objectives. hyperparameter combinations USED-FOR class separation. objectives USED-FOR class separation. features USED-FOR downstream tasks. accuracy EVALUATE-FOR task. class separation FEATURE-OF Representations. task EVALUATE-FOR Representations. accuracy EVALUATE-FOR Representations. learning invariant features CONJUNCTION features. features CONJUNCTION learning invariant features. features USED-FOR transfer tasks. learning invariant features USED-FOR task. OtherScientificTerm is loss. Generic is network. ,"This paper proposes to use centered kernel alignment to improve the transferability of convolutional neural networks on image classification tasks. Specifically, the authors propose two objectives: (1) to learn invariant features for downstream tasks, and (2) to improve class separation. The proposed methods are evaluated on ImageNet classification tasks, where they show that the proposed methods outperform the existing methods.   ","This paper studies the transferability of representations of convolutional neural networks (CNNs) for image classification tasks. The authors propose two objectives to improve the performance of CNNs on ImageNet. The first objective, centered kernel alignment, is based on the idea of centred kernel alignment. The second objective, cross-entropy, aims to improve performance on downstream tasks by learning invariant features that are invariant to different downstream tasks.  The authors show that the two objectives can be combined to achieve better transferability. They also show that they can achieve better performance than vanilla softmax cross entropy."
9708,SP:b66b5e24f68563e2e200eda660f0dbaff53efeff,"spatial sampling CONJUNCTION temporal frequency of sampling. temporal frequency of sampling CONJUNCTION spatial sampling. neural network training strategy USED-FOR deep generative models of latent dynamics. selective backpropagation through time ( SBTT ) HYPONYM-OF neural network training strategy. SBTT USED-FOR sequential autoencoders. electrophysiological and calcium imaging data USED-FOR neural population dynamics. SBTT USED-FOR inference of neuronal population dynamics. electrophysiology USED-FOR inference of neuronal population dynamics. SBTT USED-FOR electrophysiology. interface bandwidths FEATURE-OF inference of neuronal population dynamics. SBTT USED-FOR high - frequency temporal structure. high - frequency temporal structure FEATURE-OF neural population activity. SBTT USED-FOR neural population activity. SBTT USED-FOR two - photon calcium imaging. limited, highbandwidth sampling USED-FOR pretrain dynamics models. SBTT USED-FOR models. models USED-FOR sparsely - sampled data. OtherScientificTerm are neural interfaces, brain circuits, bandwidth limits, latent low - dimensional population dynamics, latent dynamics, neuronal population dynamics, and implanted neuroelectronic interfaces. Task is Neural Information Processing Systems. ","This paper proposes a new training strategy for generative models of latent dynamics based on selective backpropagation through time (SBTT). The proposed SBTT is based on a sequential autoencoder model that is trained with limited, limited, high-bandwidth sampling. The authors show that SBTT improves the performance of neural population dynamics models on two-photon calcium imaging data from implanted neuroelectronic interfaces.  ",This paper proposes selective backpropagation through time (SBTT) as a neural network training strategy for deep generative models of latent dynamics. The authors show that SBTT can be used to improve the performance of neural networks trained with sparsely-sampled data. They also show that it can improve the accuracy of neural network models trained with high-frequency temporal structure of neural population dynamics. 
9733,SP:3513a83806e71006b86d60b779d8bd6bb87c3546,"Sequence - to - sequence learning USED-FOR sequence prediction tasks. neural networks USED-FOR Sequence - to - sequence learning. approach USED-FOR local distribution. neural network USED-FOR approach. neural network USED-FOR local distribution. hierarchical approach USED-FOR sequence - to - sequence learning. quasi - synchronous grammars USED-FOR hierarchical approach. style transfer CONJUNCTION small - scale machine translation. small - scale machine translation CONJUNCTION style transfer. compositional generalization ( SCAN ) CONJUNCTION style transfer. style transfer CONJUNCTION compositional generalization ( SCAN ). it COMPARE baselines. baselines COMPARE it. latent neural grammar USED-FOR domains. latent neural grammar USED-FOR diagnostic language navigation task. diagnostic language navigation task EVALUATE-FOR compositional generalization ( SCAN ). diagnostic language navigation task EVALUATE-FOR small - scale machine translation. diagnostic language navigation task HYPONYM-OF domains. style transfer HYPONYM-OF diagnostic language navigation task. compositional generalization ( SCAN ) HYPONYM-OF domains. small - scale machine translation HYPONYM-OF domains. style transfer HYPONYM-OF domains. Generic is models. Task is compositional generalization. Method are neural parameterization of the grammar, and manual feature engineering. OtherScientificTerm is combinatorial space of derivation rules. ","This paper proposes a hierarchical approach for sequence-to-sequence learning with quasi-synchronous grammars for style transfer and compositional generalization in machine translation. The proposed approach is based on a hierarchical model that learns to predict the local distribution of a set of derivation rules in the combinatorial space of the grammar. The method is evaluated on a variety of domains, including style transfer, style transfer with style transfer (SCAN) and small-scale machine translation, where the proposed method outperforms baselines. ","This paper proposes a hierarchical approach for sequence-to-sequence learning. The authors propose a quasi-synchronous grammars-based approach for compositional generalization (SCAN) and style transfer (style transfer) tasks. The approach is based on a latent neural grammar, which is used to model the local distribution of the input data. The proposed approach is evaluated on a diagnostic language navigation task and a small-scale machine translation task. "
9769,SP:d06fc251f2a9287f7a2236a188349628d8f39d9a,Feature Selection CONJUNCTION Functional Data Analysis. Functional Data Analysis CONJUNCTION Feature Selection. algorithm USED-FOR function - on - scalar feature selection. algorithm USED-FOR Group Elastic Net. Group Elastic Net USED-FOR function - on - scalar feature selection. scalar predictors USED-FOR functional response. algorithm USED-FOR Group Elastic Net. sparsity structure FEATURE-OF Augmented Lagrangian. algorithm USED-FOR ultrahigh dimensional settings. algorithm USED-FOR sparsity structure. ultrahigh dimensional settings FEATURE-OF Group Elastic Net. algorithm USED-FOR function - on - scalar regression framework. Functional Principal Components USED-FOR algorithm. approach COMPARE competitors. competitors COMPARE approach. simulations EVALUATE-FOR approach. Genome Wide Association Study USED-FOR application. Task is analysis of large and complex data sets. OtherScientificTerm is computational burden. ,"This paper proposes a novel algorithm for feature selection in functional data analysis. The main idea is to use a group of scalar predictors to predict the functional response of the input data, and then use an augmented Lagrangian to estimate the function-on-scalar regression. The authors show that the proposed algorithm is computationally efficient in the presence of sparsity in the Augmented Lagrangians. ",This paper proposes a new algorithm for feature selection in the context of functional data analysis. The proposed algorithm is based on the Group Elastic Net (GNN) framework. The main idea is to use the Augmented Lagrangian (AGL) as a scalar predictor to predict the functional response of the input data. The authors show that the proposed algorithm can be applied to a large number of data sets. They also show that it can be used in ultra-high dimensional settings. 
9805,SP:e0b53f76f3a6b756fedd09926f9cf034f89f4a5a,functional principal component analysis ( FPCA ) USED-FOR model estimation. real data analyses EVALUATE-FOR framework. Material is Structured point process data. Generic is matrix. OtherScientificTerm is log - Gaussian Cox processes. ,This paper studies the problem of estimating the log-Gaussian Cox processes from structured point process data. The authors propose to use functional principal component analysis (FPCA) to estimate the model parameters of the point process. They show that FPCA can be used to estimate model parameters in terms of the functional principal components of the data matrix. They also show that the proposed method can be applied to real data analyses. ,This paper proposes a new framework for the functional principal component analysis (FPCA) of point process data. The main contribution of the paper is to introduce the notion of log-Gaussian Cox process (LGP) in the FPCA framework. The LGP is defined to be a log-gaussian process where the input is a set of points and the output is a matrix. The authors show that the LGP can be used to estimate the model parameters of a point process. They also provide an empirical evaluation of the effectiveness of the proposed method. 
9841,SP:3aa213076f3e9f9838ac654517df2fe1fca33499,"online multi - task learning approach USED-FOR adaptive nonlinear control. adversarial disturbance CONJUNCTION unknown environmentdependent nonlinear dynamics. unknown environmentdependent nonlinear dynamics CONJUNCTION adversarial disturbance. unknown environmentdependent nonlinear dynamics FEATURE-OF nonlinear system. adversarial disturbance FEATURE-OF nonlinear system. shared representation USED-FOR environmentdependent dynamics. approach USED-FOR robot control. unified framework USED-FOR control - theoretic and learning - theoretic guarantees. non - asymptotic endto - end convergence guarantee USED-FOR multi - task nonlinear control. OMAC CONJUNCTION deep representation learning. deep representation learning CONJUNCTION OMAC. OMAC COMPARE adaptive control approaches. adaptive control approaches COMPARE OMAC. Method are Online Meta - Adaptive Control ( OMAC ), online representation learning, and control theory. Task is robotic system. ","This paper proposes an online multi-task learning approach for adaptive nonlinear control in the presence of adversarial perturbations and unknown environment-dependent nonlinear dynamics. The proposed method, called Online Meta-Adaptive Control (OMAC), is based on online representation learning and online meta-adaptive control. The main contribution of the paper is to provide a unified framework for control theory and learning-theoretic guarantees for the proposed method.  ",This paper proposes an online multi-task learning approach for adaptive nonlinear control. The main contribution of the paper is a unified framework for control-theoretic and learning- theoretic guarantees for online meta-adaptive control (OMAC) and deep representation learning. The authors provide a convergence guarantee for OMAC with a non-asymptotic end-to-end convergence guarantee. They show that OMAC outperforms the state-of-the-art in terms of the number of tasks and the performance of the system.
9877,SP:cb274c93a169b199ea09120ca02105a3f16b31c5,"bound propagation based certified robust training methods USED-FOR neural networks. certifiable robustness guarantees FEATURE-OF neural networks. interval bound propagation ( IBP ) CONJUNCTION CROWN - IBP. CROWN - IBP CONJUNCTION interval bound propagation ( IBP ). interval bound propagation ( IBP ) HYPONYM-OF SOTA ) methods. CROWN - IBP PART-OF SOTA ) methods. weight initialization method USED-FOR IBP training. regularization USED-FOR ReLU activation states. regularization USED-FOR certified bounds. BN USED-FOR ReLU activation states. Batch Normalization ( BN ) USED-FOR model. regularization USED-FOR certified training. verified error CONJUNCTION verified error. verified error CONJUNCTION verified error. verified error FEATURE-OF TinyImageNet. network architecture USED-FOR SOTA. Metric is per - batch training complexity. Method are neural network training, and Fast - Certified - Robust - Training. Generic are they, and methods. OtherScientificTerm are exploded bounds, long warmup schedules, and training schedules. Task is wamrup. Material is CIFAR-10. ",This paper studies the problem of training neural networks with certified robustness guarantees. The authors propose to use interval bound propagation (IBP) and CROWN-IBP to improve the robustness of deep neural networks. The main contribution of the paper is to show that IBP training with Batch Normalization (BN) can improve the certified bounds on the verified error and verified error. ,This paper proposes a method for fast certified robust training of neural networks. The method is based on the interval bound propagation (IBP) and CROWN-IBP methods. The main idea is to use batch normalization (BN) to improve the robustness of the model. The authors show that BN can be used to reduce the complexity of the training process. They also show that this method can be combined with other SOTA methods.
9913,SP:18ffeb199a670fb2b1f4417b8653479001944dab,"change point detection method USED-FOR adversaries. Huber ε - contamination framework USED-FOR adversarial attacks. phase transition phenomenon FEATURE-OF change point detection. minimax lower bound USED-FOR computationally - feasible method. Task are Change point detection, and univariate mean change point detection problem. Method are theoretically - justified methods, and robust change point detection methods. OtherScientificTerm are model violations, heavy - tailed noise distribution, isolate outliers, systematic contamination, spurious change points, contamination distributions, detection boundary, contamination proportion ε, contamination proportion, and logarithmic factors. Metric is minimax - rate optimal localisation error rate. ","This paper studies the change point detection problem in the presence of adversarial perturbations. The authors propose a Huber-Contradiction framework for the detection of change points in the heavy-tailed noise distribution. They show that the detection boundary of a change point is determined by the contamination proportion of the detection distributions. They then show that under certain assumptions on the contamination distributions, they are able to find a detection boundary with a minimax rate optimal localisation error rate.  ","This paper studies the problem of change point detection for adversarial attacks. The authors propose a Huber-Contradiction framework for the problem, which is motivated by the phase transition phenomenon of change points. They prove a minimax lower bound on the localisation error rate of the proposed method, and show that it is computationally feasible. They also provide a theoretical justification for their method."
9949,SP:d03617b5fc446768809cf015c9234b0c9386a690,"differentiable model CONJUNCTION neural network. neural network CONJUNCTION differentiable model. batch Gradient Descent ( GD ) USED-FOR empirical loss. batch Gradient Descent ( GD ) USED-FOR learning. paradigms USED-FOR learning problems. GD USED-FOR learning. SGD USED-FOR learning. SGD CONJUNCTION GD. GD CONJUNCTION SGD. precision ρ FEATURE-OF gradient calculations. statistical queries ( SQ ) USED-FOR learning. SGD USED-FOR sample - based learning algorithm. learning power EVALUATE-FOR PAC learning. SGD USED-FOR SQ learning. fine enough precision COMPARE minibatch size. minibatch size COMPARE fine enough precision. GD USED-FOR sample - based learning algorithm. fine enough precision USED-FOR GD. GD USED-FOR PAC learning. SGD USED-FOR PAC learning. SGD CONJUNCTION GD. GD CONJUNCTION SGD. SGD COMPARE SQ learning. SQ learning COMPARE SGD. OtherScientificTerm are population loss, bρ, ρ, and mini - batch size. ",This paper studies batch Gradient Descent (GD) and statistical queries (SQ) for learning with differentiable models and neural networks. The authors show that the empirical loss of SGD is a function of the precision of the minibatch size and the number of samples in the mini-batch. They show that GD can be used to improve the performance of sample-based learning algorithms in PAC learning. They also show that SGD and GD are equivalent to each other in terms of learning power.,"This paper proposes batch Gradient Descent (SGD) as a sample-based learning algorithm for PAC learning. The main contribution of the paper is a theoretical analysis of the empirical performance of SGD on PAC learning, where it is shown that SGD can be used to improve the performance of PAC learning when the minibatch size is smaller than the precision of the gradient calculations. The paper also provides a theoretical justification for SGD and GD. "
9985,SP:1de2864fe2f53e25596a9bd2c61e2048e79296f6,"machine learning CONJUNCTION inverse problems. inverse problems CONJUNCTION machine learning. model probability distribution USED-FOR discrete data. discrete data USED-FOR inverse problems. discrete data USED-FOR machine learning. Wasserstein distance FEATURE-OF model distribution. uniform probability distribution USED-FOR Wasserstein distance. convergence FEATURE-OF Lloyd - type algorithm. ambient space FEATURE-OF point cloud. point cloud USED-FOR algorithm. Poliak - Łojasiewicz inequality USED-FOR Wasserstein distance cost. Task is minimization problem. Method are Lloyd ’s algorithm, and gradient descent. OtherScientificTerm are Voronoi cells, Power cells, spurious critical points, error term, and discrete distribution. Metric is Wasserstein error. Generic are problem, and bounds. ","This paper studies the problem of estimating the Wasserstein distance between a point cloud and a distribution over a discrete set of points. The authors consider the case where the distribution over the point cloud is assumed to be a uniform distribution over Voronoi cells. They show that under certain assumptions on the distribution of the Voroni cells, they can find a solution to the problem with a convergence rate of $O(1/\epsilon^2)$ that is $O(\sqrt{1/2})$ faster than the standard gradient descent algorithm. They also show that the error term of the problem is bounded by a power cell.  ","This paper studies the problem of minimizing the Wasserstein distance between two points in a point cloud, where the point cloud consists of Voronoi cells, Power cells, and spurious critical points. The main contribution of the paper is to provide a convergence result for a Lloyd-type algorithm for this problem. The convergence result is based on the Poliak-Łojasiewicz inequality, which shows that under certain assumptions on the distribution of the points, the convergence rate of the algorithm converges to zero. The paper also provides a proof of the convergence of the method."
10021,SP:c3d364aeee55230a436c3ce4e8dc8310ee73959e,"Convolution HYPONYM-OF feature transform. Convolution HYPONYM-OF neural networks. feature transform PART-OF neural networks. convolution layers CONJUNCTION self - attention blocks. self - attention blocks CONJUNCTION convolution layers. convolution layers PART-OF Transformer networks. dynamic transforms USED-FOR video understanding. correspondence relations USED-FOR representation. motion information HYPONYM-OF correspondence relations. self - attention HYPONYM-OF dynamic transforms. relational kernels CONJUNCTION relational contexts. relational contexts CONJUNCTION relational kernels. rich structures of spatio - temporal relations USED-FOR relational feature transform. relational kernels USED-FOR rich structures of spatio - temporal relations. relational self - attention ( RSA ) HYPONYM-OF relational feature transform. Diving48 CONJUNCTION FineGym. FineGym CONJUNCTION Diving48. Something - Something - V1&V2 CONJUNCTION Diving48. Diving48 CONJUNCTION Something - Something - V1&V2. RSA network COMPARE convolution and self - attention counterparts. convolution and self - attention counterparts COMPARE RSA network. motion - centric benchmarks USED-FOR video action recognition. Something - Something - V1&V2 HYPONYM-OF video action recognition. Diving48 HYPONYM-OF video action recognition. motion - centric benchmarks EVALUATE-FOR RSA network. FineGym HYPONYM-OF motion - centric benchmarks. Something - Something - V1&V2 HYPONYM-OF motion - centric benchmarks. Diving48 HYPONYM-OF motion - centric benchmarks. Method are deep learning, stationary convolution kernels, and dynamic feature transforms. ","This paper proposes a novel self-attention method for video recognition. The proposed method is based on a relational feature transform (RSA), which is an extension of the traditional convolutional feature transform. The key idea is to use the correspondence relations between motion information and the representation of the input frames as a way to represent the motion information. The authors show that the proposed method outperforms the state-of-the-art methods on several motion-centric benchmarks.   ",This paper proposes a new feature transform called relational self-attention (RSA) for video recognition. The key idea of RSA is to use the correspondence relations between motion information and spatio-temporal relations to represent the representation of the video. The authors show that RSA can be used in combination with convolution layers and self attention blocks to improve the performance of video recognition tasks. The proposed RSA network outperforms the state-of-the-art on a variety of motion-centric benchmarks. 
10057,SP:2c2530069d5cab485629090243da464d107feadd,"mean field theory FEATURE-OF multilayer neural networks. mean field limit USED-FOR learning dynamics. infinite - width limit FEATURE-OF random fluctuation. large - width expansion USED-FOR random fluctuation. formulation USED-FOR stochastic dependency. fluctuation FEATURE-OF multilayer networks. system of dynamical equations USED-FOR limiting fluctuation distribution. second - order mean field limit HYPONYM-OF system of dynamical equations. stochasticity CONJUNCTION nonlinear time evolution. nonlinear time evolution CONJUNCTION stochasticity. nonlinear time evolution FEATURE-OF limiting fluctuation. cross - layer dependency CONJUNCTION nonlinear time evolution. nonlinear time evolution CONJUNCTION cross - layer dependency. cross - layer dependency FEATURE-OF stochasticity. large - width networks USED-FOR fluctuation. vanishing fluctuation FEATURE-OF output function. squared loss FEATURE-OF empirical risk minimization setting. empirical risk minimization setting FEATURE-OF shallow networks. loss function FEATURE-OF multilayer networks. squared loss FEATURE-OF shallow networks. OtherScientificTerm are infinite - width scaling, network depth, complex interaction, limit theorem, large - width regime, training trajectory, and global optimum. Material is multilayer case. Method are neuronal embedding framework, and gradient descent mean field training. Generic are it, and network. ","This paper studies the mean field theory of learning dynamics in multilayer neural networks. The authors show that in the large-width regime, the training trajectory of the network converges to a global optimum. They show that the limiting fluctuation distribution is a system of dynamical equations with second-order mean field limit. They also show that this system is nonlinear with respect to the number of layers in the network.","This paper studies the mean field theory of multilayer neural networks in the large-width regime. The authors derive a second-order mean field limit for the dynamics of the training trajectory of a multilayers neural network. They show that the limiting fluctuation distribution of the network is a system of dynamical equations. They prove that the limit theorem holds for the large width regime. They also show that in the case of shallow networks, the loss function is a squared loss, which is the same as the squared loss of a shallow network."
10093,SP:a3d927854d9d7fd39b8d05a79666810d585d5062,inductive biases USED-FOR predictive extrapolation. Hamiltonian / Lagrangian form USED-FOR structure. inductive biases USED-FOR Forecasting of time - series data. dissipative brackets PART-OF metriplectic dynamical systems. metriplectic dynamical systems USED-FOR parameterization of dissipative brackets. process USED-FOR generalized Casimirs. generalized Casimirs USED-FOR entropy. dynamics COMPARE penalty - based approaches. penalty - based approaches COMPARE dynamics. time - series data USED-FOR dynamical system. data - driven modeling USED-FOR physical systems. data - driven modeling CONJUNCTION machine learning ( ML ) tasks. machine learning ( ML ) tasks CONJUNCTION data - driven modeling. learnable dynamics FEATURE-OF dynamical system. physics - based structure USED-FOR architectures. minimal bias FEATURE-OF black - box model form. approaches USED-FOR structure preserving models of reversible dynamics. structure preserving models of reversible dynamics USED-FOR inductive bias. approaches USED-FOR inductive bias. algebraic structure of Hamiltonian / Lagrangian dynamics USED-FOR flow map. energy FEATURE-OF flow map. symplectic structure FEATURE-OF flow map. approaches USED-FOR reversible systems. entropy HYPONYM-OF generalized Casimirs. framework USED-FOR Physical systems. thermodynamic consistency FEATURE-OF mimetic properties. first and second laws of thermodynamics HYPONYM-OF mimetic properties. fluctuation dissipation theorem ( FDT ) USED-FOR closed stochastic systems. model USED-FOR metriplectic systems. algebraic structure FEATURE-OF system. first principles modeling USED-FOR system. system USED-FOR multiscale problems. time history USED-FOR multiscale problems. training strategy USED-FOR NODEs. metriplectic system USED-FOR time - series data. training strategy USED-FOR algebraic objects. internal entropy CONJUNCTION temperature. temperature CONJUNCTION internal entropy. non - observable states FEATURE-OF dissipative systems. internal entropy HYPONYM-OF non - observable states. temperature HYPONYM-OF non - observable states. null - spaces USED-FOR reversible and irreversible components of the dynamics. dissipative chaotic systems USED-FOR science and engineering problems. latent dimension FEATURE-OF,"This paper proposes a method for learning a metriplectic dynamical system with a Hamiltonian/Lagrangian form that preserves the structure of the system in terms of dissipative brackets. The method is based on the generalized Casimir process, which is a generalization of the entropy of the first and second laws of thermodynamics. The authors show that the proposed method is able to learn the parameters of the metrizlectic system in a black-box model form with minimal bias. The proposed method can be applied to time-series data and is shown to outperform existing methods for learning time-varying dynamics.","This paper proposes a new framework for learning the dynamics of metriplectic dynamical systems. The main idea is to use a generalized Casimirs-based approach to learn the dynamics in a black-box model form, which is based on the Hamiltonian/Lagrangian form of a metrizlectic system. The authors show that the learned dynamics can be used to predict the time-series data of a time-varying dynamical system. They also show that their method can be applied to a closed stochastic system, where the dynamics is modeled by the fluctuation dissipation theorem (FDT), which is a well-known property of the system."
10129,SP:32e8e83e06b1e9a4dad761334d5947c91bfd1853,"Fairness CONJUNCTION robustness. robustness CONJUNCTION Fairness. robustness PART-OF Trustworthy AI. Fairness PART-OF Trustworthy AI. Fairness CONJUNCTION robustness. robustness CONJUNCTION Fairness. sample selection - based algorithm USED-FOR fair and robust training. combinatorial optimization problem USED-FOR unbiased selection of samples. greedy algorithm USED-FOR optimization problem. algorithm COMPARE state - of - the - art technique. state - of - the - art technique COMPARE algorithm. fairness CONJUNCTION robustness. robustness CONJUNCTION fairness. robustness EVALUATE-FOR state - of - the - art technique. fairness EVALUATE-FOR state - of - the - art technique. robustness EVALUATE-FOR algorithm. fairness EVALUATE-FOR algorithm. synthetic and benchmark real datasets EVALUATE-FOR algorithm. synthetic and benchmark real datasets EVALUATE-FOR state - of - the - art technique. fair and robust training baselines COMPARE algorithm. algorithm COMPARE fair and robust training baselines. sampling step USED-FOR batch selection. sampling step USED-FOR algorithm. clean data USED-FOR algorithm. Method are unbiased model, and training algorithm. OtherScientificTerm is data corruption. ",This paper proposes a sample selection-based algorithm for fair and robust training in Trustworthy AI. The proposed algorithm is based on a combinatorial optimization problem for unbiased selection of samples from a set of clean data. The authors show that the proposed algorithm outperforms the state-of-the-art fair training methods in terms of fairness and robustness.   ,"This paper proposes a sample selection-based algorithm for fair and robust training for trustworthy AI. The authors propose a combinatorial optimization problem for unbiased selection of samples from a set of samples, and propose a greedy algorithm to solve the optimization problem. They show that their algorithm outperforms the state-of-the-art in terms of fairness and robustness on synthetic and real datasets. "
10165,SP:991127729bf067fe27fdd7ed360aab39e4df5921,"Neural network models USED-FOR hidden data biases. function space FEATURE-OF inductive biases. inductive biases USED-FOR models. periodic activation functions USED-FOR Bayesian neural networks. triangular wave CONJUNCTION periodic ReLU activation functions. periodic ReLU activation functions CONJUNCTION triangular wave. deep neural networks USED-FOR out - of - domain detection. periodic activation functions USED-FOR deep neural networks. in - domain data EVALUATE-FOR periodic activation functions. Generic is them. OtherScientificTerm are network weights, translation - invariant, stationary Gaussian process priors, sinusoidal ( Fourier ) activations, and perturbed inputs. ","This paper studies the use of periodic activation functions in Bayesian deep neural networks (BNNs) for out-of-domain detection. In particular, the authors propose to use a triangular wave and periodic ReLU activation functions for Bayesian BNNs. The authors show that these activation functions are translation-invariant, stationary Gaussian process priors, and sinusoidal (Fourier) activations, and perturbed inputs. They also show that they can be used for in-domain data detection.  ","This paper studies the inductive bias of Bayesian neural networks with periodic activation functions in the function space. The authors show that the weights of the network weights are translation-invariant, stationary Gaussian process priors, sinusoidal (Fourier) activations, and perturbed inputs. They also provide a theoretical analysis of the effect of the weights on out-of-domain detection. "
10201,SP:d61a2aecfea4612c473b4e6fd41f3dc2fcbb04a1,user interaction CONJUNCTION complex dynamic systems. complex dynamic systems CONJUNCTION user interaction. complex dynamic systems FEATURE-OF programs. user interaction FEATURE-OF programs. mouse based games HYPONYM-OF complex dynamic systems. autonomous methods USED-FOR feedback. unit tests USED-FOR interactive programs. feedback USED-FOR interactive programs. classifying Markov Decision Processes ( MDPs ) USED-FOR feedback. dynamics and reward model USED-FOR MDP. agent USED-FOR differential trajectories. agent CONJUNCTION autoregressive model. autoregressive model CONJUNCTION agent. differential trajectories PART-OF MDP. agent USED-FOR cooperative objective. autoregressive model USED-FOR cooperative objective. method USED-FOR automatic feedback system. automatic feedback system USED-FOR interactive code assignments. anonymized student submissions FEATURE-OF dataset. Task is coding education. Method is classifier. Material is hand - coded bug labels. ,This paper proposes a method for automatic feedback for interactive code assignments in interactive programs. The method is based on a classifying Markov Decision Processes (MDPs) and an autoregressive model with a dynamics and reward model to model the dynamics and dynamics of the MDPs. The proposed method is evaluated on a dataset of student submissions and shows that the proposed method outperforms baselines in terms of bug detection accuracy. ,"This paper proposes a method to train a classifier to classify Markov Decision Processes (MDPs) for interactive code assignments. The proposed method is based on an autoregressive model and a dynamic dynamics and reward model for MDPs. The agent is trained to generate a set of differential trajectories for the MDP, and the agent and the reward model are trained in a cooperative manner. Experiments show that the proposed method outperforms the state-of-the-art in terms of accuracy.   "
10237,SP:daf99ad91613d6e11b13315ccbd1bbe25094ae4b,"superpixels CONJUNCTION attentions. attentions CONJUNCTION superpixels. attentions CONJUNCTION saliency maps. saliency maps CONJUNCTION attentions. superpixels USED-FOR low - level input features. high - level latent object features USED-FOR approach. disentangled representation USED-FOR high - level latent object features. identifiable latent representation USED-FOR independent factors of variation. mimic tree USED-FOR DRL action values. identifiable latent representation USED-FOR Represent And Mimic ( RAMi ) framework. fidelity EVALUATE-FOR mimic tree. Minimum Description Length ( MDL ) objective EVALUATE-FOR mimic tree. Information Bottleneck ( IB ) principle USED-FOR Minimum Description Length ( MDL ) objective. mimic tree COMPARE baseline models. baseline models COMPARE mimic tree. decision rules CONJUNCTION causal impacts. causal impacts CONJUNCTION decision rules. latent traversals CONJUNCTION decision rules. decision rules CONJUNCTION latent traversals. causal impacts CONJUNCTION human evaluation results. human evaluation results CONJUNCTION causal impacts. latent traversals FEATURE-OF mimic tree. decision rules PART-OF mimic tree. Task is Interpreting Deep Reinforcement Learning ( DRL ) models. OtherScientificTerm are transparency regulations, latent features, IB - optimal mimic tree, and nodes. Method is DRL model. ","This paper proposes a method to improve the fidelity of deep reinforcement learning (DRL) models by learning a disentangled representation of high-level latent object features from low-level input features. The proposed method, called Represent and Mimic (RAMi), uses an identifiable latent representation to capture independent factors of variation in DRL action values. A mimic tree is trained to predict the action values of DRL agents. The method is evaluated on a variety of benchmark tasks and compared with state-of-the-art baselines.","This paper proposes Represent And Mimic (RAMi) framework for Deep Reinforcement Learning (DRL) with a disentangled representation of high-level latent object features. The proposed method is based on the Information Bottleneck (IB) principle, which is used to estimate the minimum description length (MDL) objective of a mimic tree. The paper shows that the mimic tree can be used to improve the fidelity of DRL models with respect to human evaluation results."
10273,SP:84560de78af979354fff83d1370d8675c1e9191f,"weather forecasts CONJUNCTION political prognostications. political prognostications CONJUNCTION weather forecasts. political prognostications CONJUNCTION financial projections. financial projections CONJUNCTION political prognostications. Bayesian framework USED-FOR structure of dynamic predictions. GLIM HYPONYM-OF Bayesian framework. Gaussian latent information martingale HYPONYM-OF Bayesian framework. historical data USED-FOR latent process of information flow. martingale structure CONJUNCTION volatility. volatility CONJUNCTION martingale structure. approach USED-FOR probability paths. volatility HYPONYM-OF probability paths. martingale structure HYPONYM-OF probability paths. GLIM COMPARE baseline methods. baseline methods COMPARE GLIM. metrics USED-FOR estimated posterior probability path distributions. estimated posterior probability path distributions EVALUATE-FOR GLIM. estimated posterior probability path distributions EVALUATE-FOR baseline methods. metrics EVALUATE-FOR baseline methods. metrics EVALUATE-FOR GLIM. Task are probability estimates of future binary outcomes, and time series analysis. Generic are first, second, former, and trajectories. OtherScientificTerm is dynamic structure of predictions. ","This paper proposes a Bayesian approach to estimate the probability estimates of future binary outcomes from time series data. The approach is based on the Gaussian latent information martingale (GLIM) framework, which uses a Gaussian process of information flow to model the information flow in the latent space of historical data.   The proposed method is evaluated on a variety of time series datasets, including weather forecasts, political prognostications, and financial projections. The results show that the proposed method outperforms existing Bayesian methods in terms of the estimated posterior probability path distributions. ","This paper proposes a new Bayesian framework for predicting the structure of dynamic predictions of future binary outcomes. The authors propose a Gaussian latent information martingale (GLIM) framework to model the information flow of historical data. The proposed method is based on the idea of a latent process of information flow, where the latent process is represented as a latent distribution over the historical data, and the latent distribution is represented by a Gaussian latent distribution.  The authors show that GLIM can be used to estimate the probability paths of the future binary outcome, and that it can be applied to time series analysis.  "
10309,SP:0c4bfb44e0a353256692d5e5ae96f65c1a14363d,"fixed confidence FEATURE-OF active pure exploration. generic stochastic bandit environments USED-FOR active pure exploration. instance - specific lower bounds FEATURE-OF expected sample complexity. instance - specific lower bounds USED-FOR problem. proportions USED-FOR optimization problem. tractability FEATURE-OF optimization problem. algorithm USED-FOR pure exploration problems. Frank - Wolfe algorithm USED-FOR lower - bound optimization problem. Frank - Wolfe algorithm USED-FOR it. FWS USED-FOR pure exploration tasks. arm identification HYPONYM-OF pure exploration tasks. FWS COMPARE state - of - art algorithms. state - of - art algorithms COMPARE FWS. OtherScientificTerm are sampling budget, structural properties of the environment, and lower bounds. Method are Oracle algorithm, and learning algorithms. Metric is sample complexity. ","This paper studies the problem of active pure exploration in stochastic bandit environments with fixed confidence. In this setting, the goal is to find a set of arms that maximize the probability that the agent will return to the starting point of the current state. The main contribution of this paper is to provide an instance-specific lower bound on the expected sample complexity of this problem. The proposed method is based on the Frank-Wolfe algorithm, which is a generalization of the Oracle algorithm. The authors show that this method is tractable in the sense that the sample complexity is bounded by the proportions of the optimization problem. ","This paper studies the problem of active pure exploration in a stochastic bandit environment with fixed confidence. The authors provide a lower bound on the expected sample complexity of the problem, which is based on the Frank-Wolfe algorithm (FWS). They also provide an instance-specific lower bound for the sample complexity. They show that FWS is tractable for a wide range of environments, and show that it can be used to solve the Oracle algorithm for pure exploration problems."
10345,SP:0947a0f08fba53d3c8af9b78dd64e6e10fc73e32,"sequences CONJUNCTION trees. trees CONJUNCTION sequences. trees CONJUNCTION graphs. graphs CONJUNCTION trees. graphs HYPONYM-OF optimizing combinatorial spaces. sequences HYPONYM-OF optimizing combinatorial spaces. trees HYPONYM-OF optimizing combinatorial spaces. black - box function evaluations USED-FOR combinatorial spaces. Bayesian optimization ( BO ) USED-FOR problems. framework USED-FOR problems. BO approach USED-FOR combinatorial spaces. deep generative models ( DGMs ) USED-FOR latent representation of structures. discrete structure USED-FOR function evaluation. latent space USED-FOR surrogate model. DGM USED-FOR surrogate model. LADDER HYPONYM-OF approach. latent space representation USED-FOR surrogate modeling. structural information PART-OF decoded structures. structural information PART-OF structure - coupled kernel. real - world benchmarks EVALUATE-FOR LADDER. LADDER COMPARE BO. BO COMPARE LADDER. LADDER COMPARE state - of - the - art methods. state - of - the - art methods COMPARE LADDER. LADDER COMPARE latent space method. latent space method COMPARE LADDER. real - world benchmarks EVALUATE-FOR BO. BO COMPARE latent space method. latent space method COMPARE BO. real - world benchmarks EVALUATE-FOR state - of - the - art methods. Task is drug design. OtherScientificTerm are physical lab experiments, continuous spaces, continuous space, inductive bias, and black - box function. ","This paper proposes a method for Bayesian optimization in combinatorial spaces, where the goal is to optimize a discrete function in a continuous space. The main idea is to use deep generative models (DGMs) to learn the latent representation of structures, which can then be used as a surrogate model for function evaluation. The proposed method is evaluated on synthetic and real-world data sets. ",This paper proposes a new method for Bayesian optimization (BO) for combinatorial spaces. The main idea is to use deep generative models (DGMs) to model the structure-coupled kernel of a discrete structure as a surrogate model for the black-box function evaluation. The proposed method is evaluated on synthetic and real-world datasets. 
10381,SP:37adabdc6615c5199a481553c8ccc06d57363614,"representation of state - action value functions USED-FOR regret minimization. constant regret FEATURE-OF MDP. linear reward function FEATURE-OF MDP. low - rank MDPs CONJUNCTION zero inherent Bellman error. zero inherent Bellman error CONJUNCTION low - rank MDPs. condition USED-FOR problems. LSVI - UCB CONJUNCTION ELEANOR. ELEANOR CONJUNCTION LSVI - UCB. constant regret bound USED-FOR optimistic algorithms. LSVI - UCB HYPONYM-OF optimistic algorithms. ELEANOR HYPONYM-OF optimistic algorithms. algorithm USED-FOR representation selection. representations CONJUNCTION them. them CONJUNCTION representations. constant regret EVALUATE-FOR it. representations USED-FOR it. OtherScientificTerm are linear structure, universally spanning optimal features ( UNISOFT ), Bellman closure assumption, and UNISOFT condition. Generic is representation. ","This paper studies the regret minimization problem in linear MDPs with linear reward function with linear structure. In this setting, the reward function is assumed to have a linear structure, and the goal is to minimize the regret of the state-action value function. The main contribution of this paper is to show that the Bellman closure assumption of the MDP is violated in this setting.   The main result of the paper is a constant regret bound for LSVI-UCB and ELEANOR. The regret bound is obtained by using the UNISOFT condition. ",This paper studies the problem of regret minimization in linear MDPs with linear reward function. The main contribution of the paper is to provide a constant regret bound for optimistic algorithms. The regret bound is based on the universal spanning optimal features (UNISOFT) condition and the Bellman closure assumption. The authors also provide an algorithm for representation selection.   
10417,SP:92566b664ab2f6ee9b73f29327aeef85d14ecf60,"energy conservation FEATURE-OF dynamics. Lagrangian or Hamiltonian dynamics PART-OF neural network architecture. differential equations USED-FOR approaches. legged robots CONJUNCTION robotic manipulators. robotic manipulators CONJUNCTION legged robots. contacts CONJUNCTION collisions. collisions CONJUNCTION contacts. contacts PART-OF physical systems. collisions PART-OF physical systems. robotic manipulators HYPONYM-OF physical systems. legged robots HYPONYM-OF physical systems. differentiable contact model USED-FOR contact mechanics. frictionless / frictional CONJUNCTION elastic / inelastic. elastic / inelastic CONJUNCTION frictionless / frictional. frictionless / frictional HYPONYM-OF differentiable contact model. elastic / inelastic HYPONYM-OF differentiable contact model. elastic / inelastic HYPONYM-OF contact mechanics. frictionless / frictional HYPONYM-OF contact mechanics. model USED-FOR inequality constraints. contact model USED-FOR Lagrangian and Hamiltonian neural networks. simultaneous learning of contact and system properties USED-FOR contact model. coefficients of restitution FEATURE-OF 2D and 3D physical systems. 2D and 3D physical systems EVALUATE-FOR framework. differentiable physics simulator USED-FOR downstream gradient - based optimization tasks. dynamics USED-FOR differentiable physics simulator. dynamics USED-FOR downstream gradient - based optimization tasks. planning and control HYPONYM-OF downstream gradient - based optimization tasks. OtherScientificTerm are inductive bias, and joint angles. ","This paper proposes to learn a differentiable contact model for physical systems. The contact model is based on a Lagrangian and Hamiltonian neural network, which is used to learn the contact and system properties simultaneously. The proposed method is evaluated on two physical systems: a 2D and a 3D physical system.   ","This paper proposes a differentiable contact model for learning the dynamics of physical systems. The model is based on a Lagrangian and Hamiltonian neural network architecture, which is used to learn the dynamics and the properties of the system. The authors show that the model can be applied to a variety of differentiable physics simulation tasks, including planning and control, and downstream gradient-based optimization tasks. "
10453,SP:82d59a3609dfd458f90f23d4e477c8b497e9dc18,"Lipschitz constant USED-FOR parameter trajectory. 1st layer bias FEATURE-OF NNs. bounded complexity EVALUATE-FOR NNs. Task is Benevolent Training Hypothesis ( BTH ). Metric is complexity. Method are deep neural network ( NN ), and stochastic training procedure. OtherScientificTerm are training dynamics, BTH, NN ’s Lipschitz constant, input space, Dropout, and trainingand datadependent generalization bound. ",This paper studies the Lipschitz constant of the training dynamics of deep neural networks (NNs) in the presence of dropout and stochastic gradient descent. The authors show that the parameter trajectory of a deep neural network (NN) with 1st-layer bias is bounded by a bounded complexity. They also provide a generalization bound for the training and datadependency generalization bounds.  ,"This paper studies the Benevolent training hypothesis (BTH) of deep neural networks (NNs). The authors show that the Lipschitz constant of a neural network is bounded by the training dynamics of a stochastic training procedure. They show that under certain assumptions on the input space and the dropout, the BTH can be used to derive a generalization bound for NNs with bounded complexity. They also show that BTH is applicable to the 1st layer bias of NNs. "
10489,SP:9b329c915fa8d4045c167c9df37a49ee314d190e,"operation USED-FOR distribution. Forster transform HYPONYM-OF operation. disjoint mixture of few distributions USED-FOR distribution. polynomial - time algorithm USED-FOR distribution - independent PAC learning of halfspaces. distribution - independent PAC learning of halfspaces PART-OF Massart noise model. polynomial sample complexity FEATURE-OF polynomial - time algorithm. algorithms USED-FOR learning problem. sample complexity EVALUATE-FOR algorithms. OtherScientificTerm are anticoncentration properties, and bit complexity. ","This paper studies the problem of distribution-independent PAC learning of half-spaces in the Massart noise model. The authors show that the sample complexity of the learning problem is polynomial in the number of samples. The main contribution of the paper is to show that there exists a distribution independent PAC learning algorithm for the half-space learning problem.   The main contributions of this paper are as follows:  - The authors prove that the Forster transform operation is anticoncentratic, which means that the distribution is a disjoint mixture of few distributions.  - They prove that there exist two algorithms for the learning of the distribution with sample complexity in the order of polynomials. ",This paper studies the problem of distribution-independent PAC learning of half-spaces in a Massart noise model. The authors study the problem in terms of the anticoncentration properties of the distribution. They show that the sample complexity of the learning problem is polynomial in the number of samples. They also provide an algorithm for learning the distribution of halfspaces.   
10525,SP:e5229305af00067ae2dbabd903e585964aec8928,"models USED-FOR graph - based learning tasks. Graph neural networks USED-FOR graph - based learning tasks. Graph neural networks HYPONYM-OF models. adversarial attacks USED-FOR graph - level classification. biochemistry and social network analysis HYPONYM-OF real - life applications. Bayesian optimisation - based attack method USED-FOR graph classification models. graph properties CONJUNCTION constraints. constraints CONJUNCTION graph properties. constraints CONJUNCTION modes of attack. modes of attack CONJUNCTION constraints. graph properties FEATURE-OF graph classification tasks. graph classification tasks EVALUATE-FOR method. adversarial robustness EVALUATE-FOR graph classification models. Task is node - level classification tasks. OtherScientificTerm are unrealistic setups, perturbation, and adversarial samples. ","This paper proposes a Bayesian optimisation-based adversarial attack method to improve the robustness of graph neural networks against node-level adversarial attacks. The proposed method is based on Bayesian Optimization-based Adversarial Attacks (BOA), which is a novel Bayesian optimization-based attack method for graph classification models. The main contribution of the paper is the introduction of a new adversarial perturbation method that can be applied to any graph classification model. The method is evaluated on several graph classification tasks and shows improved performance compared to the baselines.","This paper proposes a Bayesian optimisation-based attack method to improve the robustness of graph-level classification models against adversarial attacks. The proposed method is based on Bayesian Optimisation (BIN) and is motivated by the observation that adversarial robustness can be improved by improving the generalizability of the model. The method is evaluated on a variety of graph classification tasks, and it is shown that the proposed method can improve robustness to adversarial perturbations. "
10561,SP:4999e5664383066fdacd14be6242c7b83f85f3dd,"distribution shifts FEATURE-OF Machine learning models. adaptation USED-FOR label distribution shift. adaptation USED-FOR online setting. online learning USED-FOR online label shift adaptation. Follow The Leader ( FTL ) CONJUNCTION Online Gradient Descent ( OGD ). Online Gradient Descent ( OGD ) CONJUNCTION Follow The Leader ( FTL ). Online Gradient Descent ( OGD ) USED-FOR adaptation algorithms. online learning techniques USED-FOR adaptation algorithms. Online Gradient Descent ( OGD ) HYPONYM-OF online learning techniques. Follow The Leader ( FTL ) HYPONYM-OF online learning techniques. OGD USED-FOR label shift scenarios. OtherScientificTerm are test - time label distribution, regret bounds, and simulated and real world label distribution shifts. Generic is model. Task is estimation of the expected test loss. ","This paper studies online label shift adaptation in the online setting, where the test-time label distribution is different from the test distribution in the offline setting. The authors propose two online learning algorithms: Follow The Leader (FTL) and Online Gradient Descent (OGD). The authors show that FTL and OGD can be used to adapt the online model to label distribution shifts. The regret bounds are provided for both simulated and real-world label distribution shift scenarios.",This paper studies the problem of online label shift adaptation in the online learning setting. The authors consider the case where the test-time label distribution is different from the real-world label distribution. They provide a regret bound for the expected test loss of an online learning algorithm based on the online gradient descent (OGD) algorithm. The regret bounds are derived for both simulated and real world label distribution shifts.   
10597,SP:806515ae07fb1c9d02773592005d53d4158ef102,distribution FEATURE-OF detection and localization of gradual changes. time - ordered observations USED-FOR distribution. time - ordered observations USED-FOR detection and localization of gradual changes. discontinuity jump in distribution USED-FOR abrupt setting. method USED-FOR detecting and localizing gradual changes. features FEATURE-OF distribution. prior knowledge FEATURE-OF distribution. prior knowledge FEATURE-OF features. detection CONJUNCTION localization. localization CONJUNCTION detection. method USED-FOR detection. method USED-FOR localization. Method is data generating model. ,"This paper proposes a method for detecting and localizing gradual changes in the distribution of time-varying observations. The proposed method is based on the observation-based method of GANs. The method is applied to detect and localize abrupt discontinuities in the data generating model, and is shown to be effective in detecting and localization of gradual changes. ","This paper proposes a method for detecting and localizing gradual changes in the distribution of time-series data. The authors propose a method to detect and localize gradual changes of the distribution by using the time-ordered observations of a data generating model. The method is based on the notion of discontinuity jump in distribution, which is defined as a discontinuity in distribution that occurs when there is an abrupt jump in the data distribution.  The authors show that their method can detect gradual changes and localization of gradual changes. They also show that the method can also detect and localization abrupt changes."
10633,SP:7a3c8a7b17ecab19361d36e1d3d73fa35b71214c,"brain USED-FOR blind source separation ( BSS ) problems. linear BSS problems PART-OF signal processing. Independent Component Analysis ( ICA ) USED-FOR linear BSS problems. neural architecture CONJUNCTION synaptic learning rules. synaptic learning rules CONJUNCTION neural architecture. objective function USED-FOR biologically plausible NN. objective function USED-FOR ICA. neural architecture PART-OF biologically plausible NN. synaptic learning rules PART-OF biologically plausible NN. synaptic plasticity USED-FOR algorithm. extracellular calcium CONJUNCTION local field potential. local field potential CONJUNCTION extracellular calcium. local field potential CONJUNCTION nitric oxide. nitric oxide CONJUNCTION local field potential. neuromodulators CONJUNCTION extracellular calcium. extracellular calcium CONJUNCTION neuromodulators. OtherScientificTerm are biological circuit, biophysical variables, and synapse. Method are ICA neural network ( NN ), and NN. Task is synaptic weight update. ","This paper studies blind source separation (BSS) problems in the brain, where the goal is to identify the source of a given input signal. The authors propose to use ICA neural networks (ICA) to solve the problem, which is based on independent component analysis (ICA). The authors show that ICA can be used to learn biologically plausible neural networks that are biologically plausible in terms of learning rules and parameters. They also show that the proposed method is able to learn a biologically plausible ICA network that is biologically plausible. ",This paper proposes a biologically plausible neural network (NCA) for blind source separation (BSS) problems. The main contribution of the paper is to propose an ICA neural network that is biologically plausible. The proposed method is based on a neural network with a neural architecture and a set of learning rules. The authors show that biologically plausible NN is able to learn biologically plausible parameters of the ICA network. They also show that the proposed method can be used to solve linear BSS problems. 
10669,SP:22f8b517a3df65144412938f5891c463d7bae0ab,"neural activity USED-FOR task - related behavior. Recurrent Neural Networks ( RNNs ) USED-FOR neural activity. Recurrent Neural Networks ( RNNs ) USED-FOR task - related behavior. neuroscience CONJUNCTION machine learning. machine learning CONJUNCTION neuroscience. space of solutions FEATURE-OF task. RNNs COMPARE neural data. neural data COMPARE RNNs. space of solutions USED-FOR tasks. two - neuron network USED-FOR task. discrete dynamical regimes USED-FOR diversity. Delayed discrimination CONJUNCTION Interval discrimination. Interval discrimination CONJUNCTION Delayed discrimination. Interval discrimination CONJUNCTION Time reproduction. Time reproduction CONJUNCTION Interval discrimination. Delayed discrimination HYPONYM-OF neuroscience - inspired tasks. Time reproduction HYPONYM-OF neuroscience - inspired tasks. Interval discrimination HYPONYM-OF neuroscience - inspired tasks. neural activity FEATURE-OF networks. extrapolation patterns USED-FOR dynamical objects. tool USED-FOR reduced dynamics of networks. compact directed graph USED-FOR tool. compact directed graph USED-FOR reduced dynamics of networks. Machine learning CONJUNCTION Neuroscience. Neuroscience CONJUNCTION Machine learning. Method is machine learning algorithms. OtherScientificTerm are underspecification, hidden structure, and neural features. Generic is representation. ","This paper proposes a method to analyze the dynamics of recurrent neural networks (RNNs) in time-reconstruction tasks. The proposed method is based on a two-neuron network that is trained to solve a set of tasks in discrete dynamical regimes. The authors show that RNNs are able to learn to solve the tasks in the space of solutions, and that the learned dynamics can be used as a proxy for the underlying dynamics of the network.   The authors also propose a new method to estimate the extrapolation patterns of the dynamics, which are used to infer the hidden structure of the networks. ","This paper proposes a new method to study the dynamics of recurrent neural networks (RNNs) in the space of discrete dynamical regimes (i.e., task-specific tasks). The proposed method is based on a two-neuron network that is trained on a set of tasks, and the authors show that RNNs perform better than neural networks on a subset of these tasks. The authors also show that the diversity of RNN dynamics can be reduced by using a compact directed graph.   "
10705,SP:9b08a0f547ead3b59077a43b1052c6d46a0730f6,"Modeling distributions of covariates PART-OF unsupervised learning. density estimation PART-OF unsupervised learning. density estimation HYPONYM-OF Modeling distributions of covariates. arbitrary conditional density estimation USED-FOR conditional distribution. covariates FEATURE-OF conditional distribution. arbitrary conditional density estimation HYPONYM-OF problem. prior knowledge USED-FOR inference. unobserved features CONJUNCTION observed features xo. observed features xo CONJUNCTION unobserved features. ACE USED-FOR complexity. learning one - dimensional conditionals USED-FOR problem. energy function USED-FOR densities. approach COMPARE prior methods. prior methods COMPARE approach. arbitrary conditional likelihood estimation CONJUNCTION data imputation. data imputation CONJUNCTION arbitrary conditional likelihood estimation. ACE USED-FOR arbitrary conditional likelihood estimation. ACE USED-FOR data imputation. state - of - the - art USED-FOR arbitrary conditional likelihood estimation. state - of - the - art EVALUATE-FOR ACE. benchmarks EVALUATE-FOR state - of - the - art. benchmarks EVALUATE-FOR data imputation. benchmarks EVALUATE-FOR ACE. OtherScientificTerm are distributions of covariates, joint distribution, and one - dimensional conditionals. Generic is method. Method is Arbitrary Conditioning with Energy ( ACE ). ",This paper proposes a new method for estimating the conditional density of covariates in an unsupervised learning setting. The proposed method is based on the idea of learning one-dimensional conditionals to estimate the conditional distribution of the covariates. The main contribution of the paper is the use of an energy-based method to approximate the density of the joint distribution. Theoretical analysis is provided to show that the proposed method has a lower computational complexity compared to existing methods. Empirical results are provided to demonstrate the effectiveness of the method. ,"This paper proposes a new method for arbitrary conditional density estimation of covariates in unsupervised learning. The main idea is to learn one-dimensional conditionals for the conditional distribution of the covariates, and then use the energy function to estimate the density of the conditional distributions. The authors show that the proposed method achieves state-of-the-art performance on a variety of benchmarks. "
10741,SP:f2b14f5854e6aa6922795d1d2051b7402486cef6,"MSE or L1 loss function USED-FOR low - level vision. single image super - resolution ( SISR ) HYPONYM-OF low - level vision. texture and edge areas COMPARE smooth areas. smooth areas COMPARE texture and edge areas. smooth areas PART-OF photographic images. adaptive weighted loss USED-FOR deep networks. adaptive weighted loss USED-FOR SISR. adaptive weighted loss USED-FOR situations. SISR USED-FOR deep networks. textured and edge pixels HYPONYM-OF situations. variance estimation USED-FOR SISR solutions. sparsity prior USED-FOR regularizing SISR solutions. uncertainty estimation USED-FOR regularizing SISR solutions. visual quality EVALUATE-FOR SISR. uncertainty - driven loss COMPARE MSE or L1 loss. MSE or L1 loss COMPARE uncertainty - driven loss. uncertainty - driven loss COMPARE loss functions. loss functions COMPARE uncertainty - driven loss. SISR networks EVALUATE-FOR uncertainty - driven loss. computation EVALUATE-FOR loss functions. OtherScientificTerm are visual information, pixel - by - pixel basis, high - resolution image ( mean ), and uncertainty ( variance ). Task is spatial adaptation. Method is network architectures. ","This paper proposes a new loss function for the single image super-resolution (SISR) problem. The proposed loss function is based on the variance estimation of the high-resolution image (mean) and the low-resolution images (variance). The variance estimation is used to estimate the uncertainty of the SISR solutions, which is then used as a regularization term to improve the performance of the network. Experiments show that the proposed method outperforms the MSE or L1 loss in terms of visual quality. ","This paper proposes an adaptive weighted loss for single image super-resolution (SISR) for low-level vision. The proposed loss is based on variance estimation and sparsity prior, which is used to regularize SISR solutions. Experiments show that the proposed loss performs better than the MSE or L1 loss in terms of visual quality. "
10777,SP:9997583f40fa648adf57bb4fc34228f357be0cf1,"PAC - Bayesian generalization bounds USED-FOR adversarial robustness. PACBayesian framework USED-FOR averaged risk. perturbations FEATURE-OF averaged risk. majority votes FEATURE-OF perturbations. robust model USED-FOR attacks. adversarial attacks HYPONYM-OF attacks. Generic is model. Method are worst - case analysis, theoretically founded analysis, and PAC - Bayesian framework. Task is learning phase. ","This paper proposes a PAC-Bayesian generalization bound for adversarial robustness in the presence of adversarial perturbations. The generalization bounds are based on the PACBayesian framework, where the averaged risk is computed using the majority votes of the perturbation. The authors show that the robust model is robust to adversarial attacks in the worst-case. ","This paper studies the generalization of PAC-Bayesian generalization bounds for adversarial robustness. The main contribution of the paper is a theoretically-founded analysis of the robustness of the model against adversarial attacks. The authors provide a generalization bound for the PACBayesian framework, which is based on the notion of averaged risk. They show that the robust model is robust against the majority of the perturbations. They also provide a worst-case analysis for the worst case analysis. "
10813,SP:90b72e8dc41584e38f25dff9fb2853f5b11dc8fa,Logical reasoning USED-FOR querying mechanism. large and incomplete databases USED-FOR querying mechanism. Knowledge Graphs ( KGs ) USED-FOR Logical reasoning. spatial geometries USED-FOR query representations. spatial geometries USED-FOR approaches. boxes HYPONYM-OF spatial geometries. transformation tricks USED-FOR unions. Probabilistic Entity Representation Model ( PERM ) USED-FOR entities. semantic position CONJUNCTION smooth decision boundary. smooth decision boundary CONJUNCTION semantic position. Multivariate Gaussian density USED-FOR semantic position. Multivariate Gaussian density USED-FOR smooth decision boundary. mean and covariance parameters USED-FOR semantic position. Multivariate Gaussian density USED-FOR Probabilistic Entity Representation Model ( PERM ). mean and covariance parameters FEATURE-OF Multivariate Gaussian density. Multivariate Gaussian density USED-FOR entities. intersection CONJUNCTION union. union CONJUNCTION intersection. projection CONJUNCTION intersection. intersection CONJUNCTION projection. projection HYPONYM-OF closed logical operations. union HYPONYM-OF closed logical operations. intersection HYPONYM-OF closed logical operations. end - to - end objective function USED-FOR union. end - to - end objective function USED-FOR closed logical operations. PERM COMPARE state - of - the - art methods. state - of - the - art methods COMPARE PERM. logical query reasoning problem EVALUATE-FOR PERM. logical query reasoning problem EVALUATE-FOR state - of - the - art methods. evaluation metrics EVALUATE-FOR public benchmark KG datasets. public benchmark KG datasets EVALUATE-FOR state - of - the - art methods. evaluation metrics EVALUATE-FOR state - of - the - art methods. public benchmark KG datasets EVALUATE-FOR PERM. evaluation metrics EVALUATE-FOR PERM. work COMPARE methods. methods COMPARE work. F1 EVALUATE-FOR methods. F1 EVALUATE-FOR work. COVID-19 drugrepurposing case study EVALUATE-FOR PERM ’s competence. low - dimensional visualization of the Gaussian representations USED-FOR query answering process. Task is logical operations of projection and intersection. OtherScientific,"This paper proposes a probabilistic entity representation model (PERM) for logical reasoning in Knowledge Graphs (KG). The proposed model is based on a Gaussian distribution over the entities in the KG, which is used to model the logical operations of projection and intersection. The proposed method is evaluated on several public benchmark KG datasets and achieves state-of-the-art results.","This paper proposes a probabilistic entity representation model (PERM) for logical query reasoning in Knowledge Graphs (KG). PERM is based on the Probabilistic Entity Representation Model (BERM) framework. PERM consists of two components: (1) a Gaussian density for the semantic position, and (2) a low-dimensional visualization of the Gaussian representations for the query answering process. PerM is evaluated on two public benchmark KG datasets (F1 and COVID-19) and compared with a number of state-of-the-art methods."
10849,SP:b6184c9732dbb7eba7c20cae8869d975c428efe4,"memory scaling CONJUNCTION gradient degradation issues. gradient degradation issues CONJUNCTION memory scaling. Gradient - based hyperparameter optimization USED-FOR few - shot meta - learning. algorithm USED-FOR memory scaling issues. forward - mode differentiation USED-FOR memory scaling issues. noise reduction properties EVALUATE-FOR algorithm. theoretical guarantees FEATURE-OF algorithm. noise reduction properties FEATURE-OF theoretical guarantees. greedy gradientbased alternatives COMPARE black - box methods. black - box methods COMPARE greedy gradientbased alternatives. hyperparameter search ranges FEATURE-OF CIFAR-10. Generic is tasks. OtherScientificTerm are hyperparameters, and greediness. Method is unrolled optimization. ","This paper studies hyperparameter optimization for few-shot meta-learning. The authors propose a greedy gradient-based method that uses forward-mode differentiation to address the memory scaling issue. Theoretical guarantees are provided for the proposed algorithm, and experiments are conducted on CIFAR-10 and ImageNet.",This paper studies the problem of few-shot meta-learning with greedy hyperparameter optimization in the context of memory scaling and gradient degradation. The main contribution of the paper is to provide theoretical guarantees on the noise reduction properties of the proposed algorithm. Theoretical guarantees are based on the forward-mode differentiation of the hyperparameters. The paper also provides empirical results on CIFAR-10 to demonstrate the effectiveness of the algorithm.
10885,SP:9c3a326e5ee4e862923d3bf9415f32a077db8534,"systems PART-OF Human reasoning. Neural sequence models USED-FOR structured tasks. neural sequence model USED-FOR candidate generations. symbolic reasoning module USED-FOR logical consistency. neural System 1 CONJUNCTION logical System 2. logical System 2 CONJUNCTION neural System 1. neural inference USED-FOR neural System 1. neural inference USED-FOR approach. story generation CONJUNCTION grounded instruction - following. grounded instruction - following CONJUNCTION story generation. accuracy EVALUATE-FOR neurally - based generations. coherence CONJUNCTION accuracy. accuracy CONJUNCTION coherence. coherence EVALUATE-FOR neurally - based generations. approach USED-FOR neurally - based generations. coherence EVALUATE-FOR approach. accuracy EVALUATE-FOR approach. Method are System 1, System 1 - like sequence models, and System 2 - inspired logical reasoning. Generic is they. ",This paper proposes a method for learning logical reasoning in neural sequence models. The main idea is to use a symbolic reasoning module to model the logical consistency between candidate generations of a neural sequence model. The proposed method is evaluated on two tasks: story generation and grounded instruction-following. The results show that the proposed method achieves state-of-the-art performance on both tasks.,"This paper proposes a neural system-based method for learning symbolic reasoning in a sequence-based manner. The proposed method is based on a symbolic reasoning module, which is used to model the logical consistency of a sequence of candidate generations. The method is evaluated on three tasks: story generation, grounded instruction-following, and logical system 2-inspired logical reasoning. "
10921,SP:d77d046095e4c8336c0c76ac48cb046923230753,"off - policy evaluation ( OPE ) USED-FOR continuous treatment settings. personalized dose - finding HYPONYM-OF continuous treatment settings. decision rule USED-FOR historical data. discrete treatment settings FEATURE-OF OPE. estimation method USED-FOR OPE. estimation method USED-FOR continuous treatments. deep jump learning USED-FOR estimation method. deep learning CONJUNCTION multiscale change point detection. multiscale change point detection CONJUNCTION deep learning. OPE methods USED-FOR continuous treatments. OPE methods USED-FOR discrete treatments. OtherScientificTerm are treatment decision rule, and treatment space. Generic is method. Method is deep discretization. Task is Warfarin Dosing. ",This paper proposes a method for off-policy evaluation (OPE) for personalized dose-finding in continuous treatment settings. The proposed method is based on a deep learning-based estimation method for estimation of the treatment decision rule in the treatment space. The method is evaluated on the Warfarin Dosing task.   ,"This paper proposes a new method for off-policy evaluation (OPE) in continuous treatment settings. The proposed method is based on deep learning and multiscale change point detection. The method is applied to the Warfarin Dosing task, where the goal is to find the optimal dose for a patient to be given. The main contribution of the paper is to propose a new estimation method for OPE in the discrete treatment setting.   "
10957,SP:4d085e57286fdd36143108a002d16914222c239a,natural sciences CONJUNCTION engineering applications. engineering applications CONJUNCTION natural sciences. modeling framework USED-FOR inference in time - series data. Switching dynamical systems USED-FOR modeling framework. inference in time - series data USED-FOR engineering applications. inference in time - series data USED-FOR natural sciences. biology CONJUNCTION discrete - event systems. discrete - event systems CONJUNCTION biology. subordinated diffusion process FEATURE-OF Markov jump process. continuous time FEATURE-OF areas. Markov jump process USED-FOR model. biology HYPONYM-OF areas. discrete - event systems HYPONYM-OF areas. evolution equations USED-FOR prior and posterior marginal densities. Gaussian process approximation CONJUNCTION posterior inference. posterior inference CONJUNCTION Gaussian process approximation. posterior inference USED-FOR Markov jump processes. Gaussian process approximation USED-FOR diffusion level. posterior inference PART-OF continuous - time variational inference algorithm. Gaussian process approximation PART-OF continuous - time variational inference algorithm. path - wise Kullback - Leibler divergence USED-FOR Bayesian latent state estimates. variational expectation maximization USED-FOR point estimates of unknown system parameters. real - world examples EVALUATE-FOR algorithm. Material is time - series data. OtherScientificTerm is real axis. ,This paper proposes a variational inference framework for inference in time-series data with switching dynamical systems. The proposed model is based on a Markov jump process with a submodular diffusion process. The authors show that this model can be used to estimate the posterior distributions of unknown system parameters in time series data.    The main contributions of the paper are:  - The authors propose to use a continuous-time variational approximation of the Markov Jump Process with a Gaussian process approximation for the diffusion level.  - A path-wise Kullback-Leibler divergence is used to compute the Bayesian latent state estimates of the system parameters. ,"This paper proposes a new model for time-series inference from time-varying dynamical systems. The model is based on a subordinated diffusion process, where the prior and posterior marginal densities of the system are modeled by a Gaussian process approximation. The authors propose a variational variational inference algorithm for the Markov jump process, which uses a path-wise Kullback-Leibler divergence for Bayesian latent state estimates. They show that the proposed method outperforms the state-of-the-art in a variety of real-world applications."
10993,SP:d1f396e691f9d331adfb7b694a99c50e8004331f,"A HYPONYM-OF linear mapping. compressed sensing CONJUNCTION phase retrieval. phase retrieval CONJUNCTION compressed sensing. model USED-FOR signal processing problems. nonlinear processing function USED-FOR model. phase retrieval HYPONYM-OF signal processing problems. compressed sensing HYPONYM-OF signal processing problems. spectrum of sensing matrices HYPONYM-OF sensing matrices. expectation propagation algorithm ( EP ) HYPONYM-OF recovery methods. spikiness FEATURE-OF spectrum. measure USED-FOR EP. EP USED-FOR recovery. spikiness of the spectrum USED-FOR EP recovery. Task are nonlinear inverse problem, phase - retrieval problems, and 1 - bit compressed sensing problems. Method are componentwise nonlinear transformation, and optimal sensing systems. OtherScientificTerm are f, spikier spectrums, and sub - Gaussian and orthogonal matrices. Generic is framework. ","This paper studies the problem of compressed sensing and phase retrieval in the presence of a nonlinear inverse problem. The authors show that the spectrum of sensing matrices can be represented as a non-linear mapping from a linear function f to a sub-Gaussian and orthogonal matrices. They show that this mapping can be recovered using expectation propagation algorithm (EP) and show that EP recovers the spikiness of the spectrum. They also show that if the spectrum is spikier than f, then EP can recover the optimal sensing systems.","This paper proposes a new framework for recovering the spikiness of the spectrum of sensing matrices of a 1-bit compressed sensing problem. The proposed framework is based on the expectation propagation algorithm (EP) framework. The authors show that EP recovers spikier spectrums of the spectral matrices, which is an important property of EP. They also propose a new measure of the spiking of the spectrums, which can be used to improve the recovery of EP recovery. "
11029,SP:ee66604d4da9fd04826e90ccbb94f0499eba4c63,auxiliary semantic information USED-FOR Generalized Zero - Shot Learning ( GZSL ). category attributes HYPONYM-OF auxiliary semantic information. cross - domain transferability CONJUNCTION category discriminability. category discriminability CONJUNCTION cross - domain transferability. category discriminability EVALUATE-FOR visual representations. cross - domain transferability EVALUATE-FOR visual representations. prototypes USED-FOR prototypical visual patterns. attribute prototypes USED-FOR DPPN. DPPN USED-FOR attribute - related local regions. attribute prototypes USED-FOR attribute - region correspondence. DPPN USED-FOR attribute - region correspondence. attribute prototypes USED-FOR DPPN. DPPN USED-FOR visual representations. semantic - visual alignment CONJUNCTION representation transferability. representation transferability CONJUNCTION semantic - visual alignment. attribute localization ability FEATURE-OF visual representations. DPPN USED-FOR visual representations. progressive attribute localization CONJUNCTION DPPN. DPPN CONJUNCTION progressive attribute localization. category prototypes USED-FOR DPPN. unifed framework USED-FOR visual representations. DPPN USED-FOR visual representations. unifed framework USED-FOR attribute and category prototypes. DPPN USED-FOR domain shift problem. DPPN USED-FOR GZSL. domain shift problem FEATURE-OF GZSL. Generic is approach. Method is Dual Progressive Prototype Network ( DPPN ). ,"This paper proposes a novel method for zero-shot learning in the presence of auxiliary semantic information. The proposed method is based on the idea of prototype learning, which is an extension of the original prototype learning method. The key idea is to use prototype learning to learn a set of prototypical visual patterns, which are then used to learn the attribute-related local regions. The experimental results show that the proposed method achieves state-of-the-art performance in GZSL.   ","This paper proposes a new method for zero-shot learning for GZSL, called Dual Progressive Prototype Network (DPPN). The proposed method is based on the idea of combining attribute prototypes and category prototypes, which can be used as prototypes for prototypical visual patterns. The proposed DPPN is trained to learn the attribute-region correspondence between prototypes and the category prototypes. Experiments show that the proposed method outperforms the state-of-the-art methods in terms of cross-domain transferability and category discriminability. "
11065,SP:61eb6297568c3f6869fbb03eaf6a21260de5466c,"Defocus blur HYPONYM-OF blur effects. blur effects PART-OF images. end - to - end deep learning approach USED-FOR removing defocus blur. all - in - focus image USED-FOR consequent vision tasks. end - to - end deep learning approach USED-FOR all - in - focus image. accuracy EVALUATE-FOR models. linear parametric form FEATURE-OF spatially variant defocus blur kernels. fixed - point iteration USED-FOR GKM - based deblurring. fixed - point iteration USED-FOR deep neural network. GKMNet HYPONYM-OF deep neural network. scale - recurrent attention module USED-FOR mixing coefficients. GKM USED-FOR defocus deblurring. mixing coefficients PART-OF GKM. lightweight scale - recurrent architecture CONJUNCTION scale - recurrent attention module. scale - recurrent attention module CONJUNCTION lightweight scale - recurrent architecture. mixing coefficients USED-FOR defocus deblurring. scale - recurrent attention module USED-FOR GKMNet. lightweight scale - recurrent architecture USED-FOR GKMNet. model complexity CONJUNCTION computational efficiency. computational efficiency CONJUNCTION model complexity. GKMNet COMPARE defocus deblurring methods. defocus deblurring methods COMPARE GKMNet. computational efficiency EVALUATE-FOR GKMNet. model complexity EVALUATE-FOR GKMNet. OtherScientificTerm are spatially variant amount, and defocus blur. ","This paper proposes a method for removing defocus blur from an image. The proposed method is based on a GKM-based deblurring method, where the mixing coefficients of the defocus kernels are computed in a linear parametric form. The mixing coefficients are computed using a scale-recurrent attention module. The authors show that the proposed method achieves state-of-the-art performance on image classification tasks. ","This paper proposes a deep neural network-based method for removing defocus blur from an all-in-focus image. The proposed method is based on GKM-based deblurring, which is an end-to-end deep learning approach to remove the blur effects from an image.  The authors show that the mixing coefficients of the GKKM kernel are a linear parametric form of a spatially variant defocus kernel. The authors propose a lightweight scale-recurrent attention module to control the mixing of the mixing coefficient of the defocus kernels. They show that their method can achieve better accuracy and computational efficiency compared to existing methods. "
11101,SP:18bf447c90935c373e5ec4cdfbbf8f2a273d2edb,"models USED-FOR SSVRL. visual content PART-OF videos. RGB frames CONJUNCTION motion vectors. motion vectors CONJUNCTION RGB frames. motion vectors USED-FOR low - resolution optical flows. compressed videos USED-FOR motion vectors. supervision signals FEATURE-OF motion vectors. multi - instance InfoNCE loss USED-FOR cross guidance contrastive learning algorithm. downstream tasks EVALUATE-FOR MVCGC. MVCGC COMPARE competitors. competitors COMPARE MVCGC. Generic are methods, and method. OtherScientificTerm is mutual information. Metric is storage and computation efficiency. ",This paper proposes a new method to learn SSVRL models for low-resolution optical flows. The proposed method is based on a cross-guided contrastive learning (CGL) approach. The method is evaluated on a variety of downstream tasks and achieves state-of-the-art performance. ,"This paper proposes a cross guidance contrastive learning (CGC) method for SSVRL. The main idea is to use a multi-instance InfoNCE loss to learn the mutual information between the motion vectors and the supervision signals. The proposed method is evaluated on a variety of downstream tasks, and it is shown to be competitive with other methods. "
11137,SP:8c7b1d976d9758cd534c565ec31a23f97892e503,"Bayesian treatment USED-FOR overconfidence. Bayesian treatment USED-FOR ReLU nets. overconfidence FEATURE-OF ReLU nets. features FEATURE-OF BNN. ReLU features USED-FOR Bayesian linear models. it USED-FOR BNNs. model COMPARE BNNs. BNNs COMPARE model. infinite ReLU features FEATURE-OF finite ReLU BNNs. GP USED-FOR finite ReLU BNNs. model USED-FOR GP posterior. it USED-FOR ReLU BNN. Method are ReLU Bayesian neural networks ( BNNs ), and Gaussian process ( GP ). OtherScientificTerm are infinite - width limit, and asymptotic overconfidence. ","This paper studies the problem of overconfidence in Bayesian Bayesian neural networks (BNNs) with ReLU features. The authors show that the overconfidence of BNNs is due to the over-confidence of the Gaussian process (GP), which is a Bayesian linear model with a Gaussian posterior. The main contribution of this paper is to prove that the GP posterior is a function of the number of ReLU parameters and the width of the input space.   The authors then show that under certain assumptions, the GP can be approximated by a ReLU BNN with infinite width and infinite ReLU feature space. They show that this GP posterior can be used to estimate the posterior of a finite-width BNN in the infinite-width limit. They also show that if the width is bounded, then the GP is equivalent to the true GP. Finally, the authors prove that under some assumptions on the width and the GP, the BNN has asymptotic overconfidence. ","This paper studies the problem of overconfidence in ReLU Bayesian neural networks (BNNs) with finite ReLU features. In particular, the authors study the asymptotic overconfidence of BNNs with infinite-width features. The main contribution of the paper is to show that the Gaussian process (GP) can be used to estimate the posterior of a finite-width ReLU BNN. The authors show that under certain conditions, the GP posterior can be approximated by a Bayesian linear model, and that it can be applied to a finite set of ReLU neural networks with finite width. They also show that this GP posterior is equivalent to a Gaussian Bayesian model."
11173,SP:e77276f61626e896f6a985296f1d832129242cdf,"tools USED-FOR finite - sample confidence bounds. LUCB CONJUNCTION Successive Elimination. Successive Elimination CONJUNCTION LUCB. tools USED-FOR estimation of potentially complex nuisance functions. finite - sample confidence bounds USED-FOR estimation of potentially complex nuisance functions. finite - sample confidence bounds USED-FOR asymptotic variance. Successive Elimination USED-FOR best - arm - identification algorithms. LUCB USED-FOR best - arm - identification algorithms. bounds USED-FOR best - arm - identification algorithms. sample complexity EVALUATE-FOR upper bounds. upper bounds EVALUATE-FOR method. sample complexity EVALUATE-FOR method. OtherScientificTerm are data collection mechanism, and arm. Method is bestarm - identification bandit framework. Material is artificially generated data. ","This paper studies the best-arm identification bandit problem, where the goal is to identify the best arm from a set of arms that are generated by a data collection mechanism. The main contribution of this paper is to provide a finite-sample confidence bounds for the asymptotic variance of the best arms in the bandit setting. The authors first show that the confidence bounds of LUCB and Successive Elimination can be used to obtain finite-sampled confidence bounds on the variance of best arms. Then, the authors propose a new algorithm for best arms identification based on the new confidence bounds. The proposed algorithm is shown to have a sample complexity of $O(1/\epsilon^2)$ with a probability of $\Omega(\sqrt{T})$ in terms of the number of arms.","This paper studies the problem of best-arm-identification bandit, where the goal is to identify the best arm from a set of samples generated by a bandit. The main contribution of this paper is to provide a new confidence bound for the best-armed identification bandit problem. The upper bounds are based on the LUCB and Successive Elimination (SED) methods. The authors show that the upper bounds can be used to estimate the asymptotic variance of the worst-arm identification problem.   "
11209,SP:471361588bfc6c6033631509d1e43e77fd9721ce,"scalability EVALUATE-FOR distributed learning. communication FEATURE-OF gradient. algorithm USED-FOR biased compression. variance FEATURE-OF stochastic gradient. moving average USED-FOR history gradients. moving average USED-FOR variance. compression error USED-FOR ErrorCompensatedX. asymptotic convergence rate EVALUATE-FOR ErrorCompensatedX. unified theoretical analysis framework USED-FOR variance reduced algorithms. Metric are Communication cost, communication cost, and convergence speed. Method are stochastic gradient descent, training without compression, and error compensation. ","This paper studies the variance reduction problem in distributed stochastic gradient descent (DSGD) with biased compression. In particular, the authors show that the variance of the history gradients can be reduced to a moving average of the average of past gradients. They show that this can be done by computing the compression error for the history of the gradient, which is then used to compute the error compensation. The authors show the convergence rate of the proposed algorithm is O(log(1/\sqrt{T}) where T is the communication cost. They also provide an asymptotic convergence rate for their algorithm.","This paper studies the variance-reduced stochastic gradient descent with biased compression. The authors show that the variance of the gradient can be reduced by using a moving average of the history gradients. They show that this can be done by computing the error of the compression error, which is a function of the communication cost. They also show the convergence rate of the proposed algorithm. "
11245,SP:3b7ff0dc668cac2191d95fcc4dc6e0335dec3206,"graph USED-FOR model. local explainability CONJUNCTION global explainability. global explainability CONJUNCTION local explainability. performant paradigm USED-FOR multi - grained explainability. pre - training and fine - tuning idea USED-FOR explainer. pre - training and fine - tuning idea USED-FOR multi - grained explanations. explainer USED-FOR multi - grained explanations. synthetic and real - world datasets EVALUATE-FOR explainer. AUC EVALUATE-FOR baselines. explainer COMPARE baselines. baselines COMPARE explainer. explaining graph classification EVALUATE-FOR baselines. AUC EVALUATE-FOR explaining graph classification. synthetic and real - world datasets EVALUATE-FOR baselines. explaining graph classification EVALUATE-FOR explainer. AUC EVALUATE-FOR explainer. Method are graph neural network ( GNN ), explainers, pre - training phase, and fine - tuning phase. Metric is explainability. Generic is approaches. OtherScientificTerm are class - wise patterns, local context, and class - wise characteristics. ",This paper proposes a method to improve the explainability of GNNs. The main idea is to use a pre-training and fine-tuning approach to learn a set of explainers that can be used during the training phase and then fine-tune during the test phase. The proposed method is evaluated on a variety of synthetic and real-world datasets and achieves state-of-the-art performance.  ,"This paper proposes a performant paradigm for multi-grained explainability for graph neural networks (GNNs). The main idea is to use a pre-training and fine-tuning approach to improve the explainability of GNNs. The main contribution of the paper is to propose a new measure of explainability that can be used to measure the performance of a GNN. This measure is based on the AUC, which is a measure of how well the model is able to explain the class-wise patterns in a graph. The paper also proposes a new metric that measures the performance in terms of AUC of the explainer. The authors show that the new metric is more performant than AUC for explaining graph classification."
11281,SP:9b5a62d3a2b27bc60da28980e9fb0ecdff1215c0,"subgraph USED-FOR methods. method USED-FOR counterfactual explanations. GNNs USED-FOR counterfactual explanations. GNNs USED-FOR common decision logic. common decision boundaries USED-FOR GNN. GNN USED-FOR they. common decision boundaries USED-FOR they. Method is Graph Neural Networks ( GNNs ). OtherScientificTerm are noise, human intuition, explanations, and edges. ","This paper proposes a method for generating counterfactual explanations for graphs. The main idea is to use GNNs to model the decision boundaries between nodes in a graph. The authors show that the edges of a graph can be represented as subgraphs, which can be used to represent the decision boundary between nodes and edges of the original graph. They show that a GNN can be trained to learn to predict the edges in a subgraph. They also show that this method can be applied to the problem of counterfactuality.","This paper proposes a method for generating counterfactual explanations for graph neural networks (GNNs). The main idea is that GNNs can be used to learn a graph that contains a set of subgraphs (e.g., edges) that represent the decision boundaries of a given GNN. The subgraph is then used to generate counterfactually explanations for a GNN that is trained to predict the decision boundary of a subgraph. The method is tested on a variety of datasets, and it is shown that the proposed method outperforms existing methods on a number of tasks. "
11317,SP:4edb870786c9cea2c6075359cb4e79b02a8e2f5f,"information bottleneck CONJUNCTION adversarial feedback. adversarial feedback CONJUNCTION information bottleneck. information bottleneck USED-FOR VoiceMixer. adversarial feedback USED-FOR VoiceMixer. self - supervised representation learning USED-FOR information bottleneck. self - supervision USED-FOR model. adversarial feedback USED-FOR discriminator. voice style FEATURE-OF generalization. content and style discriminator PART-OF discriminator. generalization EVALUATE-FOR model. self - supervision USED-FOR content and style discriminator. transfer EVALUATE-FOR model. content information USED-FOR audio quality. audio quality EVALUATE-FOR model. Task is voice conversion. Material is converted voice. OtherScientificTerm are converted speech containing source speech style, and source speech content. ","This paper proposes a self-supervised voice-to-text model for voice conversion. The proposed method is based on two components: a content-style discriminator and an adversarial style discriminator. The content discriminator is used to learn the content and style representations of the source speech, and the style discriminators are used to train the discriminator to discriminate between the source content and the target style. The method is evaluated on two tasks: (1) audio quality transfer and (2) generalization.  ","This paper proposes a self-supervised voice-to-speech model for voice conversion. The model learns a discriminator that learns the content and style of the converted speech. The discriminator is trained by using adversarial feedback on the source speech and the target speech. Experiments show that the discriminator discriminates between the content content and the style content, and that the model generalizes well to different speech styles."
11353,SP:9fbb0c6beb3f8f88972f13dcf0e1fe7db03233c7,"Siamese voxel - to - BEV tracker USED-FOR tracking. sparse 3D point clouds FEATURE-OF tracking. Siamese shape - aware feature learning network CONJUNCTION voxel - to - BEV target localization network. voxel - to - BEV target localization network CONJUNCTION Siamese shape - aware feature learning network. Siamese shape - aware feature learning network PART-OF it. voxel - to - BEV target localization network PART-OF it. Siamese shape - aware feature learning network USED-FOR discriminative features. Siamese shape - aware feature learning network USED-FOR 3D shape information. dense 3D shape USED-FOR shape information. 2D center CONJUNCTION z - axis center. z - axis center CONJUNCTION 2D center. 2D center CONJUNCTION z - axis center. z - axis center CONJUNCTION 2D center. voxelized point cloud USED-FOR dense BEV feature map. max pooling USED-FOR dense BEV feature map. max pooling USED-FOR voxelized point cloud. KITTI and nuScenes datasets EVALUATE-FOR method. method COMPARE state - of - the - art methods. state - of - the - art methods COMPARE method. Task is 3D object tracking in point clouds. Material is point clouds. OtherScientificTerm are dynamic environments, sparse point clouds, and template ’s feature. Method are template feature embedding, and voxel - toBEV target localization network. ","This paper proposes a Siamese voxel-to-BEV tracker for 3D object tracking in sparse 3D point clouds. The proposed method consists of a shape-aware feature learning network, a voxels-based target localization network, and a discriminative features network. The method is evaluated on KITTI and nuScenes datasets and achieves state-of-the-art performance. ","This paper proposes a Siamese voxel-to-bev tracking method for tracking sparse 3D point clouds. The method is based on a shape-aware feature learning network, which learns to predict the 3D shape of the point cloud, and a target localization network that maps the voxels to the target object. The paper also proposes a max pooling method to improve the performance of the method. "
11389,SP:8b788c78680a54c453a04f4551436763ee57585e,Positional encoding USED-FOR attention - based deep model architectures. Transformer HYPONYM-OF attention - based deep model architectures. learnable Fourier features USED-FOR positional encoding method. multi - layer perceptron USED-FOR trainable encoding. learnable Fourier feature mapping USED-FOR trainable encoding. representation USED-FOR spatial multi - dimensional position. L2 distances CONJUNCTION positional relationships. positional relationships CONJUNCTION L2 distances. image FEATURE-OF pixel positions. image HYPONYM-OF spatial multi - dimensional position. pixel positions HYPONYM-OF spatial multi - dimensional position. learnable Fourier feature representation USED-FOR multi - dimensional positional encoding. learnable Fourier feature representation COMPARE methods. methods COMPARE learnable Fourier feature representation. faster convergence EVALUATE-FOR methods. accuracy EVALUATE-FOR learnable Fourier feature representation. accuracy EVALUATE-FOR methods. Method is Attentional mechanisms. ,"This paper proposes a new positional encoding method for attention-based deep models. The proposed method is based on a learnable Fourier feature mapping, which maps the L2 distance between pixels to a spatial multi-dimensional position, and then uses a multi-layer perceptron to encode the position into a trainable encoding. The method is evaluated on image classification tasks, and compared with a number of baselines. ",This paper proposes a new positional encoding method for attention-based deep neural networks. The proposed method is based on a learnable Fourier feature mapping that maps the L2 position of a pixel to a spatial multi-dimensional position. The authors propose a multi-layer perceptron that can be used to train the positional encoding. They show that the proposed method outperforms the state-of-the-art in terms of accuracy and convergence. 
11425,SP:d2ac1b6381315bce4449f09bd519f33a2a42d714,"latent variables CONJUNCTION selection bias. selection bias CONJUNCTION latent variables. causal MAG FEATURE-OF system. observational data USED-FOR system. observational data USED-FOR causal MAG. Constraint - based methods USED-FOR problem. latter USED-FOR CI tests. computational complexity EVALUATE-FOR former. lower bound USED-FOR constraint - based method. lower bound USED-FOR CI tests. CI tests USED-FOR constraint - based method. upper bound CONJUNCTION approach. approach CONJUNCTION upper bound. approach COMPARE state of the art. state of the art COMPARE approach. synthetic and real - world structures EVALUATE-FOR state of the art. synthetic and real - world structures EVALUATE-FOR approach. Generic are methods, and technique. OtherScientificTerm are large graphs, completeness guarantees, structure, and conditional independence ( CI ) tests. Method is recursive constraint - based method. ","This paper studies the problem of conditional independence (CI) tests for large-scale causal graphs, where the goal is to determine the causal MAG of a system from observational data. The authors propose a new method based on constraint-based methods to perform conditional independence tests. The main contribution of the paper is to provide a lower bound on the computational complexity of the proposed method. The lower bound is based on the fact that the upper bound of the existing methods depends on the completeness guarantees of the graph structure, and the authors show that the new method is computationally efficient. ","This paper proposes a new method to perform conditional independence (CI) tests on large graphs. The authors propose a new lower bound on the computational complexity of CI tests. The upper bound is based on a new constraint-based method, and the lower bound is derived from a previous lower bound. The method is evaluated on synthetic and real-world graphs, and it outperforms the state of the art."
11461,SP:49a4912ce457f5f5ec62c44fa10444af8075fabf,information parallelism USED-FOR online decision making problems. stochastic multi - arm bandit CONJUNCTION linear contextual bandit. linear contextual bandit CONJUNCTION stochastic multi - arm bandit. batch Thompson Sampling framework USED-FOR canonical online decision making problems. linear contextual bandit HYPONYM-OF canonical online decision making problems. stochastic multi - arm bandit HYPONYM-OF canonical online decision making problems. asymptotic ) regret bound EVALUATE-FOR batch Thompson Sampling policy. batch policy USED-FOR exploration - exploitation trade - off. batch policy USED-FOR exponential reduction. dynamic batch allocation COMPARE natural baselines. natural baselines COMPARE dynamic batch allocation. static batch allocations HYPONYM-OF natural baselines. ,"This paper studies online decision-making problems with information parallelism in the context of stochastic multi-arm bandit and linear contextual bandit. The authors propose a batch Thompson Sampling (BTS) framework for online decision making problems with Thompson sampling. The proposed method is based on dynamic batch allocation, where each batch is allocated according to a dynamic Thompson sampling policy.  The authors show that the proposed method achieves a regret bound of $O(1/\sqrt{T})$ for both linear and multi-armed bandit problems. The regret bound is shown to be exponential in the number of arms and $O(\sqrt{\log T})$ in the time horizon. ","This paper proposes a batch Thompson Sampling framework for online decision-making problems with information parallelism. The main idea is to use a dynamic batch allocation policy to sample from a set of agents, and then use the batch policy to explore the exploration-exploitation trade-off between the agent and the agent. The authors show that the batch Thompson sampling policy can achieve a regret bound of $O(1/\sqrt{T})$ with an asymptotic regret bound. They also show that this regret bound can be extended to the case of linear contextual bandit. "
11497,SP:653a519e3c799c25e0d0b4240322642040b121a3,"multiple source DA CONJUNCTION domain generalization ( DG ) settings. domain generalization ( DG ) settings CONJUNCTION multiple source DA. upper - bounds USED-FOR domain - invariant representations. upper - bounds USED-FOR target general loss. Task is Domain adaptation ( DA ). Method is domain - invariant representation. Generic are representations, them, and theory. ",This paper studies the problem of domain adaptation (DA) and domain generalization (DG) in the presence of multiple source data sources. The authors show that the target generalization error can be reduced to a target general loss for domain-invariant representations. The main contribution of the paper is a theoretical analysis of the problem in the multiple source DA setting. ,"This paper studies the problem of domain adaptation (DA) in the context of domain generalization (DG), where the goal is to learn a domain-invariant representation of the target domain. The main contribution of the paper is a theoretical analysis of the generalization properties of domain invariant representations in the setting of multiple source DA. The authors provide upper bounds on the target generalization of the representation, and show that the representation is invariant to the target target general loss. They also provide upper-bounds for multiple source and multiple source DG settings.   "
11533,SP:2a7bee950cd07494d59dfee60ac2e86cc0e481b1,"lightweight architectures USED-FOR SR methods. neural architecture search CONJUNCTION knowledge distillation. knowledge distillation CONJUNCTION neural architecture search. memory and computation resources USED-FOR model compression techniques. knowledge distillation HYPONYM-OF model compression techniques. neural architecture search HYPONYM-OF model compression techniques. network pruning HYPONYM-OF model compression technique. it USED-FOR SR networks. filter pruning USED-FOR residual blocks. L2 regularization USED-FOR sparsity. L2 regularization USED-FOR scale parameters. L2 regularization USED-FOR aligned structured sparsity learning ( ASSL ). weight normalization layer PART-OF aligned structured sparsity learning ( ASSL ). sparsity structure alignment penalty term USED-FOR norm of soft mask gram matrix. layers FEATURE-OF pruned filter locations. sparsity structure alignment penalty term USED-FOR pruned filter locations. aligned structured sparsity learning strategy USED-FOR image SR network. model size CONJUNCTION computation. computation CONJUNCTION model size. ASSLN HYPONYM-OF image SR network. ASSLN COMPARE methods. methods COMPARE ASSLN. OtherScientificTerm are moderate model size, and network parameters. Generic is state - of - the - art methods. Method is lightweight SR networks. "," pruning is a popular method for reducing the computational cost of neural network architectures. This paper proposes a new pruning method called aligned structured sparsity learning (ASSL), which prunes filter locations in residual blocks by aligning the norm of the soft mask gram matrix with the weight normalization layer. The proposed method is evaluated on image classification tasks and achieves state-of-the-art performance. ",This paper proposes an aligned structured structured sparsity learning (ASSL) method for weight normalization-based weight-normalization based model compression. The main idea of ASSL is to align the soft mask gram matrix of the weights of the weight normalisation layer with the norm of soft mask Gram matrix. The authors show that the alignment of the weighted normalization layer can be achieved by pruning the filter locations of the pruned filter locations. They also show that this alignment can be used to improve the performance of the model. 
11569,SP:e9830bb9e7d3ddc3bd1c2994590fdb5d8f3668be,"exploration USED-FOR complex coordination problems. EMC HYPONYM-OF Episodic Multi - agent reinforcement learning. reward backpropagation USED-FOR centralized training. individual utility functions USED-FOR local execution. individual utility functions HYPONYM-OF induced "" individual Q - values. episodic memory USED-FOR policy training. episodic memory USED-FOR explored informative experience. intrinsic rewards USED-FOR coordinated exploration. intrinsic reward USED-FOR coordinated exploration. method COMPARE MARL baselines. MARL baselines COMPARE method. StarCraft II micromanagement benchmark FEATURE-OF tasks. didactic examples USED-FOR method. tasks EVALUATE-FOR MARL baselines. tasks EVALUATE-FOR method. StarCraft II micromanagement benchmark EVALUATE-FOR MARL baselines. Method is factorized MARL algorithms. OtherScientificTerm are embeddings of local actionobservation histories, and individual Q - value function. ","This paper proposes a method for multi-agent reinforcement learning with episodic episodic memory. The proposed method is based on the idea of learning an episodic Q-value function for each agent, which is used as a reward function to encourage agents to explore the environment. The method is evaluated on the StarCraft II micromanagement task and achieves state-of-the-art performance on the task.","This paper proposes a new method for multi-agent exploration in multi-player reinforcement learning (MARL). The proposed method is based on the EMC framework, where the agents are given an episodic memory of their actions, and the goal is to learn a policy that maximizes the utility function of each agent. The authors show that the proposed method outperforms the state-of-the-art MARL algorithms on the StarCraft II micromanagement benchmark."
11605,SP:c7e33d479575c88e22282ee6fd4f978bcd3c06ed,"Gaussian covariates USED-FOR linear regression model. Statistical Query ( SQ ) lower bound USED-FOR problem. upper bounds USED-FOR task. SQ lower bound COMPARE algorithms. algorithms COMPARE SQ lower bound. Task is list - decodable linear regression. OtherScientificTerm are noise distribution, hypothesis vectors, and regression vector. ","This paper studies the problem of list-decodable linear regression with Gaussian covariates. The authors propose a statistical query (SQ) lower bound for the problem, which is based on the fact that the noise distribution is Gaussian and the covariates are Gaussian. They show that the SQ lower bound can be used to derive an efficient algorithm to solve the problem. They also provide an upper bound on the complexity of the problem in terms of the number of data points.","This paper studies the problem of list-decodable linear regression with Gaussian covariates. The main contribution of the paper is to provide a lower bound for the statistical query (SQ) lower bound of the problem. The lower bound is based on the assumption that the noise distribution is Gaussian and that the covariates are Gaussian. The authors show that under certain assumptions, the lower bound can be derived for the case where the Gaussian distribution has Gaussian covariances. The upper bound is derived by considering the case when the covariate distribution is non-Gaussian.  "
11641,SP:7b258252a9063514348f5fa8d9c85afd85748747,"expert domain knowledge USED-FOR ML models. patient health status CONJUNCTION disease progression. disease progression CONJUNCTION patient health status. pharmacology USED-FOR domain knowledge. systems of Ordinary Differential Equations ( ODEs ) USED-FOR Pharmacological models. expert - designed ODEs CONJUNCTION machine - learned Neural ODEs. machine - learned Neural ODEs CONJUNCTION expert - designed ODEs. expert and latent variables USED-FOR observable quantities. synthetic data EVALUATE-FOR LHM. Task is Modeling a system ’s temporal behaviour. Method are Machine Learning ( ML ) approaches, and latent hybridisation model ( LHM ). OtherScientificTerm is small sample regime. Generic are application, models, variables, and system. ","This paper proposes a method for learning systems of differential equations (ODEs) based on pharmacological data. The method is based on a combination of two approaches: (1) an expert-designed ODE and (2) a machine-learned neural ODE. The expert ODE is trained using a mixture of expert and latent variables, and the machine-trained ODE uses the expert and the latent variables to model the observable quantities. The experiments on synthetic data show that the proposed method outperforms the baselines. ",This paper proposes a latent hybridisation model (LHM) that combines expert and latent variables to model a system’s temporal behaviour. The LHM is based on the idea that the expert and the latent variables can be combined in a way that they can be used to model the system's temporal behaviour in a small sample regime. The authors show that the LHM outperforms the state-of-the-art in terms of performance on synthetic data.
11677,SP:3ea9e86e5755ef84d28e3163c60531ace5d62e3a,Representation learning USED-FOR meta - learning. Representation learning USED-FOR rapid learning of new tasks. meta - learning USED-FOR rapid learning of new tasks. works USED-FOR task - specific representations. MAML USED-FOR task - specific representations. MAML HYPONYM-OF works. fine - tuning - based objective HYPONYM-OF per - task adaptation. per - task adaptation USED-FOR representation. representation USED-FOR task - specific representations. theoretical framework USED-FOR MAML - like algorithm. risk bounds FEATURE-OF predictors. shared structure USED-FOR method. finetuning USED-FOR risk bounds. finetuning USED-FOR predictors. gradient descent USED-FOR finetuning. gradient descent USED-FOR predictors. logistic regression and neural network settings EVALUATE-FOR bounds. OtherScientificTerm is frozen representation ” objective. Generic is algorithm. Method is few - shot learning. ,"This paper studies the problem of few-shot representation learning in meta-learning, where the goal is to learn a task-specific representation that can be used for future tasks. The authors propose to use a ""frozen representation"" objective, which is defined as a function of the number of tasks in the meta-training set, and a ""fine-tuning-based"" objective that is based on the shared structure of the learned representations. They show that the frozen objective is equivalent to the ""fine tuning-based objective"", which is the one that is used in MAML. They also provide a theoretical framework to show the convergence of the proposed method.   ","This paper studies the problem of few-shot representation learning in meta-learning, where the goal is to learn a representation of a task-specific representation that can be used to guide the learning of a new task. The authors propose a new method for this problem, called MAML-like, which is based on a fine-tuning-based objective. The main contribution of the paper is a theoretical analysis of the risk bounds of the proposed method. The risk bounds are derived by considering a set of predictors of the task, and the authors show that the risk of the predictors can be reduced to zero by finetuning. They also show that their method can be applied to a variety of tasks."
11713,SP:8ba5a2ac80f7c53f81ad008e96c033ecad14ac0d,"paired images CONJUNCTION texts. texts CONJUNCTION paired images. lexicalist approach USED-FOR compositional and grounded meaning representation of language. grounded data USED-FOR compositional and grounded meaning representation of language. paired images HYPONYM-OF grounded data. texts HYPONYM-OF grounded data. neural network embedding USED-FOR shiny objects. symbolic form FEATURE-OF neuro - symbolic semantic program. lexical meanings USED-FOR neuro - symbolic program. syntax USED-FOR lexical meanings. joint parsing CONJUNCTION expected execution algorithm. expected execution algorithm CONJUNCTION joint parsing. exponentiallygrowing compositional space FEATURE-OF learning. expected execution algorithm USED-FOR learning. joint parsing USED-FOR learning. visual reasoning CONJUNCTION language - driven navigation. language - driven navigation CONJUNCTION visual reasoning. language - driven navigation EVALUATE-FOR G2L2. visual reasoning EVALUATE-FOR G2L2. domains EVALUATE-FOR G2L2. language - driven navigation HYPONYM-OF domains. visual reasoning HYPONYM-OF domains. G2L2 USED-FOR compositions of words. OtherScientificTerm are syntactic type, syntactic type of adjective, and local marginalization. Method is meaning programs. Metric is training time. ","This paper proposes a method to learn compositional and grounded meaning representation of language. The method is based on a lexicalist approach to learn a neuro-symbolic semantic program in the symbolic form, which is then used to represent the lexical meanings of words in a compositional space. The proposed method is evaluated on two tasks: visual reasoning and language-driven navigation.","This paper proposes a neuro-symbolic semantic program (G2L2) that learns the compositional and grounded meaning representation of language. The method is based on a lexicalist approach, where the lexical program is composed of two parts: (1) a syntactic program that represents the syntactic type of an adjective, and (2) an expected execution algorithm that learns a compositional space for each word in the program. The authors show that the learned compositional program is exponentially growing in size, and that it can be used to learn a symbolic program that is compositional in nature. Experiments are conducted on visual reasoning, language-driven navigation, and visual reasoning tasks. "
11749,SP:16c458651815813efdcbe8ba1205bbddbe3e4e68,stochastic Newton algorithm USED-FOR homogeneous distributed stochastic convex optimization. stochastic gradients CONJUNCTION stochastic Hessian - vector products. stochastic Hessian - vector products CONJUNCTION stochastic gradients. stochastic gradients FEATURE-OF population objective. method COMPARE methods. methods COMPARE method. convergence guarantees FEATURE-OF quasi - self - concordant objectives. method USED-FOR communication rounds. communication rounds COMPARE methods. methods COMPARE communication rounds. logistic regression HYPONYM-OF quasi - self - concordant objectives. OtherScientificTerm is stochastic computations. ,This paper proposes a new stochastic Newton algorithm for distributed stochastically convex optimization. The main idea is to use a population objective that is self-concordant with respect to the population distribution. The population objective is defined as a weighted sum of the gradient and the Hessian vector products. The authors prove convergence guarantees for the population objective and show that the proposed method can converge to the true objective with probability at least $O(1/\sqrt{n})$. The authors also show that their method can be used to reduce the number of communication rounds in distributed stoching.,This paper proposes a stochastic Newton algorithm for homogeneous distributed stochastically convex optimization. The main contribution of the paper is to provide convergence guarantees for quasi-self-concordant objectives. The convergence guarantees are based on the assumption that the population is homogeneous. The authors also provide convergence results for the communication rounds of the algorithm.
11785,SP:d7e479d59f82d4c55372a68ca7b4516f2871f346,"Chamfer Distance ( CD ) CONJUNCTION Earth Mover ’s Distance ( EMD ). Earth Mover ’s Distance ( EMD ) CONJUNCTION Chamfer Distance ( CD ). Chamfer Distance ( CD ) HYPONYM-OF metrics. Earth Mover ’s Distance ( EMD ) HYPONYM-OF metrics. global distribution USED-FOR EMD. them USED-FOR consistent evaluation. Density - aware Chamfer Distance ( DCD ) HYPONYM-OF similarity measure. it USED-FOR disparity of density distributions. it COMPARE EMD. EMD COMPARE it. it COMPARE CD. CD COMPARE it. it USED-FOR detailed structures. CD USED-FOR It. DCD USED-FOR point cloud completion task. CD CONJUNCTION EMD. EMD CONJUNCTION CD. DCD USED-FOR local geometric details. DCD USED-FOR training loss. metrics EVALUATE-FOR model. CD loss USED-FOR model. CD CONJUNCTION EMD. EMD CONJUNCTION CD. DCD USED-FOR it. OtherScientificTerm are mismatched local density, fidelity of detailed structures, unbounded value range, outliers, and bounded value range. Method is point discriminator module. Task are guided downsampling step, and point cloud similarity evaluation. ","This paper proposes a new metric called density-aware Chamfer distance (DCD) to measure the disparity of density distributions between points in a point cloud. The proposed metric is based on the Chamfer Distance (CD) and Earth Mover’s Distance (EMD), which are popular metrics for point cloud similarity evaluation. The authors show that the proposed metric can be used as a training loss to improve the accuracy of point cloud completion tasks. The paper also shows that it can improve the performance of the point cloud classification task.","This paper proposes a new Chamfer Distance (CD) and Earth Mover’s Distance (EMD) metrics for point cloud completion task. The proposed metrics are based on the density-aware Chamfer distance (DDCD), which is a measure of the disparity of density distributions. The authors show that the proposed metrics can be used to improve the performance of the model on the task. They also show that DCD can improve the accuracy of the training loss."
11821,SP:e4b302009520770814ff2c096020b779a9fc38fe,Knowledge distillation USED-FOR small student network. small student network USED-FOR teacher model. ensemble of networks HYPONYM-OF teacher model. knowledge distillation USED-FOR student generalization. dataset USED-FOR distillation. Generic is it. OtherScientificTerm is predictive distributions. Task is optimization. ,"This paper studies the problem of knowledge distillation, i.e. how to train a teacher model with a small student network to improve the generalization performance of the teacher model. The authors propose to use an ensemble of networks to model the student generalization. The proposed method is based on the idea that the student model is trained by distilling the knowledge of the large teacher model to the small student model, where the student network is trained to maximize the mutual information between the two models.   The main contribution of the paper is a theoretical analysis of the effect of the size of the student and teacher networks on the performance of distillation. The main results are:  1) The authors show that when the number of student networks is small, the teacher network is able to generalize better. 2) When the student networks are large, the distillation performance is not as good. 3) In the experiments, it is shown that the teacher and student networks have different predictive distributions.","This paper studies the problem of knowledge distillation, where a teacher model is trained with a small student network, and the student network is trained using a large teacher model. The teacher model consists of an ensemble of networks and a student model, where the student model is a small ensemble of small student networks. The student is trained by distilling the teacher model into a small set of student networks, and then the teacher is trained to maximize the student's knowledge of the teacher's predictions.  The authors show that the student generalization of a small teacher model can be improved by distillation. They show that this distillation can be achieved by using a dataset with a large number of student and teacher networks.  "
11857,SP:895c7e03f9e4dadb94be1f39d61bf0b5e1533f4f,"algorithm USED-FOR ( k, ε)-coreset. decision trees PART-OF machine learning. decision trees CONJUNCTION partition trees. partition trees CONJUNCTION decision trees. computational geometry FEATURE-OF partition trees. sklearn CONJUNCTION lightGBM. lightGBM CONJUNCTION sklearn. coresets USED-FOR random forests. computation time EVALUATE-FOR random forests. random forests CONJUNCTION parameter tuning. parameter tuning CONJUNCTION random forests. lightGBM EVALUATE-FOR coresets. sklearn EVALUATE-FOR coresets. computation time EVALUATE-FOR parameter tuning. real - world data - sets EVALUATE-FOR random forests. coresets USED-FOR parameter tuning. computation time EVALUATE-FOR coresets. accuracy EVALUATE-FOR parameter tuning. real - world data - sets EVALUATE-FOR coresets. accuracy EVALUATE-FOR coresets. Method is k - tree. OtherScientificTerm are axis - parallel rectangles, error parameter, tree, optimal k - tree, and coreset. Metric is regression or classification loss. Generic is loss. ","This paper studies the problem of computing the coreset of a decision tree. The coreset consists of a set of axis-parallel rectangles, each of which is represented by a partition tree.  The authors show that computing the coresets is computationally efficient in terms of the number of iterations needed to compute the error parameter.  They also show that the optimal k-tree can be computed in a single iteration.  Finally, the authors provide theoretical results on the computational complexity of computing coresets.","This paper studies the problem of computing the coreset of a k-tree for a class of decision trees. The coreset consists of a set of axis-parallel rectangles, each of which is composed of a (k-tree) decision tree and a (K-coreset) partition tree. The authors propose a new algorithm for computing the coresets, which is based on the notion of a ""coreset"" (i.e., a collection of k-trees) that is a subset of the decision tree. They show that this coreset can be computed in a time-efficient way, and that it can be used for parameter tuning. They also show that it is possible to compute coresets in a way that is computationally efficient."
11893,SP:f3ece96b15ec06d703925df2061ed9694ec3bca5,"δ - correct algorithm USED-FOR Top - m identification problem. sample complexity EVALUATE-FOR δ - correct algorithm. tractable lower bound USED-FOR δ - correct algorithm. sample complexity EVALUATE-FOR tractable lower bound. algorithm USED-FOR setting. sample complexity FEATURE-OF upper bound. algorithm COMPARE baselines. baselines COMPARE algorithm. synthetic and real - world data EVALUATE-FOR algorithm. Method are fixed - confidence Top - m identification ), misspecified linear bandit models, and linear models. Generic are problem, and algorithms. Task is medicine and recommendation systems. OtherScientificTerm are linearity, structure of the problem, misspecification, and lower bound. ","This paper studies the fixed-confidence Top-m identification problem with misspecified linear bandit models. The authors show that the sample complexity of the correct algorithm for this problem is O(1/\delta^2) with probability O(\log(d)^2), where $d$ is the number of samples needed to solve the problem. The main contribution of this paper is to provide a tractable lower bound for the correct upper bound of $O(\log \delta)$ in the case of misspecification. The lower bound is shown to be tractable in the setting where the linearity of the problem is not known. The upper bound is proved to be O(log \log (1/d) for the case where the model is known. ","This paper studies the problem of Top-m identification in the setting of misspecified linear bandit models. The authors provide a tractable lower bound on the sample complexity of the correct algorithm for this problem. The lower bound is based on the assumption that the bandit model is linear, and the upper bound is on the linearity of the problem. They show that the lower bound does not depend on linearity, and that it is tractable for the case where the model is missingpecified. They also show that their lower bound can be used to derive a new upper bound for the problem in the context of medicine and recommendation systems. "
11929,SP:e71c5e39b8d8d1640d6de2352ac51ddd52eea89d,"self - supervised learning USED-FOR graph neural networks ( GNNs ). self - supervised learning USED-FOR representation of graph - structure data. self - supervised learning methods USED-FOR GNNs. self - supervised learning USED-FOR disentangled graph representations. Disentangled Graph Contrastive Learning ( DGCL ) method USED-FOR disentangled graph - level representations. self - supervision USED-FOR disentangled graph - level representations. factorized representations USED-FOR latent and disentangled aspect. latent factor FEATURE-OF graph. factorized representations USED-FOR expressive information. contrastive learning manner FEATURE-OF factor - wise discrimination objective. latent factors USED-FOR expressive information. synthetic and real - world datasets EVALUATE-FOR method. method COMPARE state - of - the - art baselines. state - of - the - art baselines COMPARE method. synthetic and real - world datasets EVALUATE-FOR state - of - the - art baselines. OtherScientificTerm are real - world graph, and entanglement of the latent factors. Generic is learned representations. Task is Learning disentangled graph representations. ",This paper proposes a self-supervised learning method for learning disentangled graph representations. The proposed method is based on a contrastive learning approach where the learned representations are conditioned on the latent representation of the graph and the contrastive loss is used to improve the disentanglement. The method is evaluated on both synthetic and real-world datasets.   ,This paper proposes a method for learning disentangled graph representations using self-supervised learning for graph neural networks (GNNs). The key idea is to use contrastive learning to disentangle the latent and disentanglement aspects of the graph. The authors show that the learned representations can be used to improve the expressive power of GNNs. The proposed method is evaluated on both synthetic and real-world datasets.
11965,SP:0a7edbbdabab11273689c40c517001eb46491113,"robustness EVALUATE-FOR network. Statistical Reliability Engineering FEATURE-OF stochastic simulation. stochastic simulation USED-FOR network. stochastic simulation USED-FOR robustness. statistical hypothesis test USED-FOR robustness assessment. Importance Splitting simulation USED-FOR procedure. OtherScientificTerm are theoretical guarantees, sample size, and network function. Method is large scale networks. Generic is method. ","This paper studies the problem of estimating the robustness of a large network to perturbations. The authors propose to use stochastic simulation to estimate the robustity of the network. Theoretical guarantees are provided for the proposed method, and experiments are conducted on synthetic and real-world data.",This paper studies the problem of estimating the robustness of a large-scale neural network under the assumption that the sample size of the network is small. The main contribution of the paper is to provide theoretical guarantees on the sample-size and the network function. Theoretical guarantees are given for the case where the sample is small and the function is large. The paper also provides a statistical hypothesis test for robustness assessment.   
12001,SP:c1db485ff1ff9573daa421e167225654babb55ac,Generative modeling USED-FOR machine learning. Deep polynomial neural networks ( PNNs ) USED-FOR unsupervised image generation. PNNs USED-FOR conditional generation tasks. super - resolution HYPONYM-OF conditional generation tasks. noise variable CONJUNCTION conditional variable. conditional variable CONJUNCTION noise variable. single - variable polynomial expansions USED-FOR PNNs. conditional variable HYPONYM-OF two - variable inputs. noise variable HYPONYM-OF two - variable inputs. framework USED-FOR autoand cross - correlations. framework USED-FOR polynomial expansion. input variables USED-FOR CoPE. edges - to - image translation CONJUNCTION image - to - image translation. image - to - image translation CONJUNCTION edges - to - image translation. inverse problems CONJUNCTION edges - to - image translation. edges - to - image translation CONJUNCTION inverse problems. class - conditional generation CONJUNCTION inverse problems. inverse problems CONJUNCTION class - conditional generation. image - to - image translation CONJUNCTION attributeguided generation. attributeguided generation CONJUNCTION image - to - image translation. class - conditional generation CONJUNCTION edges - to - image translation. edges - to - image translation CONJUNCTION class - conditional generation. tasks EVALUATE-FOR CoPE. class - conditional generation EVALUATE-FOR CoPE. attributeguided generation HYPONYM-OF tasks. class - conditional generation HYPONYM-OF tasks. inverse problems HYPONYM-OF tasks. image - to - image translation HYPONYM-OF tasks. edges - to - image translation HYPONYM-OF tasks. CoPE USED-FOR conditional generation tasks. CoPE USED-FOR polynomial_nets_for_conditional_generation. OtherScientificTerm is noise. Material is synthesized image. ,"This paper proposes a method for conditional image generation using polynomial neural networks (PNNs). The main idea is to use a two-variance polynomials to model the two-variable inputs: the noise variable and the conditional variable. The main contribution of the paper is to introduce a polynomic expansion of the input variables to the PNNs. The proposed method is evaluated on image-to-image translation, class-conditional generation, inverse problems, and attribute-guided generation. ","This paper proposes a new framework for unsupervised image generation using polynomial neural networks (PNNs) for conditional generation tasks. The main idea is to combine the auto- and cross-correlation of two-variable inputs (i.e., the noise variable and the conditional variable) into a single-variable polynomially expansion of the PNNs. The proposed method is evaluated on a variety of tasks, including image-to-image translation, class-conditional generation, inverse problems, and attributeguided generation."
12037,SP:5a75bc7a3ea0ce971cfceebbc1c2434e3aa2584d,neural tangent kernel ( NTK ) CONJUNCTION MMD. MMD CONJUNCTION neural tangent kernel ( NTK ). approach USED-FOR MMD statistic. connection USED-FOR approach. memory and computational complexity EVALUATE-FOR MMD statistic. MMD statistic USED-FOR online implementation. approach USED-FOR NTK based two - sample tests. connection USED-FOR NTK based two - sample tests. theories USED-FOR kernel MMD. connection USED-FOR NTK test statistic properties. Type - I error CONJUNCTION testing power. testing power CONJUNCTION Type - I error. testing power HYPONYM-OF NTK test statistic properties. Type - I error HYPONYM-OF NTK test statistic properties. synthetic and real - world datasets EVALUATE-FOR theory. synthetic and real - world datasets EVALUATE-FOR NTK - MMD statistic. OtherScientificTerm is two - sample test. ,"This paper studies the connection between neural tangent kernels (NTKs) and MMDs. Theoretical results show that the NTK-MMD connection can be used to perform two-sample tests for NTKs. This is achieved by computing the MMD statistic using the connection of NTK and kernel MMD, which is computationally efficient and memory-efficient. Experiments on synthetic and real-world datasets show the effectiveness of the proposed method.",This paper proposes a new two-sample test for neural tangent kernel (NTK) and multi-modal (MMD) tests. The main idea is to use the connection between NTK and MMD to improve the memory and computational complexity of NTK-MMD test. Theoretical results on synthetic and real-world datasets demonstrate the effectiveness of the proposed method. 
12073,SP:1df2ffbbe56b8018067820980b93af2a8b57f891,"minimum necessary information USED-FOR neural net D ( · ). class - disentanglement USED-FOR variational autoencoder G ( · ). former COMPARE latter. latter COMPARE former. variational autoencoder G ( · ) USED-FOR class - dependent information. class - disentanglement USED-FOR class - dependent information. classification PART-OF x − G(x ). latter USED-FOR classification. clean images CONJUNCTION adversarial images. adversarial images CONJUNCTION clean images. it USED-FOR adversarial images. it USED-FOR clean images. adversarial attacks USED-FOR perturbations. class - dependent part USED-FOR perturbations. adversarial detection CONJUNCTION adversarial defense. adversarial defense CONJUNCTION adversarial detection. adversarial defense USED-FOR G(x ). detection CONJUNCTION defense. defense CONJUNCTION detection. approach USED-FOR adversarial attacks. defense USED-FOR adversarial attacks. approach USED-FOR defense. detection EVALUATE-FOR approach. Task are detection and defense of adversarial attacks, and classification and attack models. ",This paper proposes a class-dependent variational autoencoder for adversarial detection and defense of adversarial attacks. The main idea is to use the class-disentanglement between clean and adversarial images to improve the detection performance of the adversarial model. The proposed method is based on a variational auto-encoder (VAE) and a classifier (classifier + classifier + adversarial defense). The proposed approach is evaluated on image classification and defense tasks.   ,"This paper proposes a new approach to detect and defend against adversarial attacks. The proposed method is based on the class-dependent information of a variational autoencoder G (G(x) = G(x), where x is the class of the classifier and the class is the target of the adversarial attack. The method is applied to both clean and adversarial images, and it is shown that it is able to detect clean images as well as adversarial ones.   "
12109,SP:2789874561620ba7894c4672f935056bb911e919,Bayesian optimization ( BO ) USED-FOR federated learning ( FL ) setting. federated Thompson sampling ( FTS ) algorithm USED-FOR applications. federated hyperparameter tuning HYPONYM-OF applications. federated Thompson sampling ( FTS ) algorithm USED-FOR Bayesian optimization ( BO ). federated Thompson sampling ( FTS ) algorithm USED-FOR federated learning ( FL ) setting. privacy guarantee FEATURE-OF FL. privacy guarantee FEATURE-OF FTS. differential privacy ( DP ) USED-FOR deep neural networks. DP USED-FOR iterative algorithms. DP USED-FOR user - level privacy. FTS USED-FOR user - level privacy. DP USED-FOR FTS. local modeling USED-FOR BO. DP framework USED-FOR parameter vectors. utility EVALUATE-FOR algorithm. local modeling USED-FOR algorithm. distributed exploration ( DE ) USED-FOR utility. distributed exploration ( DE ) USED-FOR algorithm. privacy CONJUNCTION utility. utility CONJUNCTION privacy. theoretical guarantees FEATURE-OF privacy. theoretical guarantees FEATURE-OF utility. theoretical guarantees FEATURE-OF differentially private FTS. privacy CONJUNCTION utility. utility CONJUNCTION privacy. utility CONJUNCTION privacy guarantee. privacy guarantee CONJUNCTION utility. real - world experiments EVALUATE-FOR DP - FTS - DE. utility EVALUATE-FOR DP - FTS - DE. OtherScientificTerm is privacy - utility trade - off. ,"This paper proposes a federated Thompson sampling (FTS) algorithm for Bayesian optimization in the federated learning setting. The proposed algorithm is based on the Thompson Sampling (TS) algorithm, which is differentially private in the FL setting.  Theoretical analysis is provided for the privacy-utility trade-off of the proposed FTS algorithm. The main contribution of the paper is a theoretical analysis of the privacy and utility trade-offs of FTS.    The main contributions of this paper are as follows:   1. The authors propose a new algorithm for federated hyperparameter tuning based on differential privacy (DP).  2. They show that the proposed algorithm can be viewed as an extension of the FTS with DP. ",This paper proposes a federated Thompson sampling (FTS) algorithm for Bayesian optimization (BO) in the federated learning (FL) setting. The main idea is to use differential privacy (DP) to improve the utility trade-off between privacy and utility of the FTS algorithm. The authors show that the utility of FTS is better than the privacy of the algorithm. They also provide theoretical guarantees for the utility and privacy trade-offs of the proposed algorithm. 
12145,SP:be7d6b81736a2c3f89abd8771b41b18802e88832,"it USED-FOR real - world problems. data annotation USED-FOR MLC models. informative samples USED-FOR cost - effective annotation. BM USED-FOR label correlations. mixture component USED-FOR global pattern of label correlations. Bayesian Bernoulli mixture of label clusters USED-FOR BM. Bayesian Bernoulli mixture of label clusters USED-FOR label correlations. predictive GP USED-FOR feature - component - label mapping. BM CONJUNCTION predictive GP. predictive GP CONJUNCTION BM. BM USED-FOR feature - component - label mapping. predictive GP USED-FOR data features. AL USED-FOR sparse labels. BM USED-FOR sparse labels. GP USED-FOR mixture components. auxiliary variable based variational inference algorithm USED-FOR non - conjugacy. mapping process USED-FOR end - to - end posterior inference. predictive distribution USED-FOR label prediction. model USED-FOR predictive distribution. feature uncertainty CONJUNCTION label covariance. label covariance CONJUNCTION feature uncertainty. label covariance USED-FOR data sampling. BM ) USED-FOR data sampling. label covariance CONJUNCTION BM ). BM ) CONJUNCTION label covariance. GP USED-FOR feature uncertainty. real - world multi - label datasets EVALUATE-FOR model. AL EVALUATE-FOR model. real - world multi - label datasets EVALUATE-FOR AL. Task is Multi - label classification ( MLC ). OtherScientificTerm are correlated ( hence non - exclusive ) labels, sparse label space, correlated label space, inductive bias, and label covariance matrix. ","This paper proposes a Bayesian Bernoulli mixture of label clusters (BM) method for multi-label classification (MLC) with sparse labels. The proposed method is based on Bayesian variational inference (BVI) and Bayesian Monte Carlo (BCM) to learn a mixture component of the label space, which is then used to learn the global pattern of label correlations. The method is evaluated on three real-world MLC datasets and achieves state-of-the-art performance. ",This paper proposes a Bayesian Bernoulli mixture of label clusters (BM) model for multi-label classification (MLC). The main idea is to use an auxiliary variable-based variational inference algorithm for non-conjugacy of the feature- component-label mapping. The auxiliary variable based variational algorithm is based on a variational posterior inference (VAE) algorithm. The proposed method is evaluated on a variety of real-world datasets. 
12181,SP:2b7270b0370c193300bcbbb5fb0a4101b3329d99,"wedge - shaped point cloud sectors COMPARE point cloud. point cloud COMPARE wedge - shaped point cloud sectors. end - to - end latency EVALUATE-FOR lidar perception models. lidars HYPONYM-OF streaming data source. cartesian coordinate systems USED-FOR methods. multi - scale padding USED-FOR spatial context. feature undistortion CONJUNCTION range stratified convolutions. range stratified convolutions CONJUNCTION feature undistortion. feature undistortion USED-FOR core polar convolutional architecture. range stratified convolutions USED-FOR core polar convolutional architecture. nuScenes dataset EVALUATE-FOR streaming based methods. OtherScientificTerm are sectors, and rectangular regions. Method are polar coordinate system, and non - streaming methods. ", and nuScenes datasets. The paper proposes a method to learn a set of point cloud sectors from a point cloud. The proposed method is based on the observation that point clouds are more complex than point clouds. The authors propose a method for learning the point cloud by partitioning the point clouds into rectangular and wedge-shaped regions.    The method is evaluated on the nuSCenes dataset and compared with a number of baselines. ,"This paper proposes a new method to improve the performance of lidar perception models. The proposed method is based on a polar coordinate system, where each point cloud is composed of a set of rectangular regions, and each region is represented by a point cloud with a wedge-shaped point cloud. The key idea of the method is to use multi-scale padding to add spatial context to the data. The method is evaluated on the nuScenes dataset, where it outperforms other streaming-based methods."
12217,SP:7ae2c5b7d9c8a6c8f4a353606aa419929c47f31b,"Structured latent variables USED-FOR deep learning models. prior knowledge PART-OF deep learning models. variables USED-FOR learning. differentiable surrogate USED-FOR training. learning approach USED-FOR latent variable. Gumbel - Max trick USED-FOR distributions. structured domains FEATURE-OF distributions. score function estimators USED-FOR optimization. score function estimators USED-FOR differentiable surrogates. stochastic invariant HYPONYM-OF recursive algorithms. gradient estimates CONJUNCTION control variates. control variates CONJUNCTION gradient estimates. feature USED-FOR gradient estimates. feature USED-FOR control variates. structured latent variable models COMPARE relaxation - based counterparts. relaxation - based counterparts COMPARE structured latent variable models. OtherScientificTerm are surrogate, and biased gradients. Generic is model. ","This paper proposes a method to learn a differentiable surrogate for the training of deep learning models with structured latent variables. The method is based on the Gumbel-Max trick, which is used to estimate the score function of a distribution over a set of distributions in a structured domain. The score function estimators are then used to compute the gradient of the surrogate, which can then be used to train the model. The paper shows that the proposed method can be used in conjunction with a relaxation-based method to improve the performance of the model in terms of accuracy. ","This paper studies the problem of learning a differentiable surrogate for structured latent variables in deep learning models. The key idea is to use the Gumbel-Max trick to learn a surrogate that is differentiable and differentiable, and then use this surrogate to train a deep learning model. The authors show that this surrogate can be used to train deep models with differentiable surrogates. They also show that the surrogate can also be used as a control variable in the training process. "
12253,SP:415d363c66a6967c1daca9dc02001b85bf7f0752,"Deep convolutional neural networks ( CNNs ) USED-FOR image denoising. large datasets USED-FOR Deep convolutional neural networks ( CNNs ). noisy image USED-FOR denoisers. models USED-FOR features. large datasets USED-FOR CNN models. convolutional layers PART-OF CNN. multiplicative scaling parameter USED-FOR GainTuning. GainTuning COMPARE CNNs. CNNs COMPARE GainTuning. denoising EVALUATE-FOR GainTuning. image - denoising benchmarks EVALUATE-FOR CNNs. image - denoising benchmarks EVALUATE-FOR GainTuning. noise level CONJUNCTION image type. image type CONJUNCTION noise level. adaptive GainTuning USED-FOR transmission - electronmicroscope images. synthetic data USED-FOR CNN. CNN USED-FOR adaptive GainTuning. GainTuning USED-FOR structure of catalytic nanoparticles. methodology COMPARE GainTuning. GainTuning COMPARE methodology. low signal - to - noise ratios FEATURE-OF data. data USED-FOR GainTuning. data USED-FOR structure of catalytic nanoparticles. Generic are they, and them. OtherScientificTerm is overfitting. "," images are often used for image denoising. However, the noisy images can cause overfitting in the denoisers. To address this issue, this paper proposes to add a multiplicative scaling parameter to the convolutional layers in CNNs. The proposed method, called GainTuning, is shown to improve the performance of CNNs on image-denoising benchmarks.  ","This paper proposes GainTuning, a method to improve the performance of CNNs on image denoising tasks. The main idea is to adaptively fine-tune the weights of the convolutional layers of the CNNs during the training process. The proposed method is based on a multiplicative scaling parameter, which is used to adjust the weighting of the weights during training. The method is evaluated on a variety of image-denoising benchmarks, and it is shown that it outperforms the state of the art."
12289,SP:90afa1102683b456bc72a54abef466326827546a,convolutional neural network CONJUNCTION asymmetric multiway cut problem solver. asymmetric multiway cut problem solver CONJUNCTION convolutional neural network. fully differentiable architecture USED-FOR simultaneous semantic and instance segmentation. panoptic segmentation PART-OF fully differentiable architecture. convolutional neural network PART-OF fully differentiable architecture. asymmetric multiway cut problem solver PART-OF fully differentiable architecture. latter USED-FOR combinatorial optimization problem. combinatorial optimization problem USED-FOR panoptic labeling. semantic and boundary predictions USED-FOR panoptic labeling. semantic and boundary predictions PART-OF combinatorial optimization problem. formulation USED-FOR smooth surrogate of the panoptic quality metric. gradient USED-FOR optimization problem. Cityscapes and COCO datasets EVALUATE-FOR approaches. combinatorial optimization USED-FOR panoptic segmentation ( COPS ). optimization USED-FOR large scale real - world problem. optimization CONJUNCTION deep learning. deep learning CONJUNCTION optimization. deep learning USED-FOR large scale real - world problem. approach USED-FOR combinatorial optimization. optimization USED-FOR approach. Generic is architecture. ,"This paper proposes a method for panoptic segmentation (COPS) based on a convolutional neural network and an asymmetric multi-way cut (AMC) problem solver. The main idea is to learn a smooth surrogate of the panoptics quality metric, which is then used to compute the gradient of the optimization problem. Experiments on Cityscapes and COCO datasets show the effectiveness of the proposed method. ","This paper proposes a fully differentiable architecture for panoptic segmentation (COPS) for semantic and instance segmentation. COPS is a real-world problem where the semantic and boundary predictions are made for each instance. The authors propose a combinatorial optimization problem for COPS, where the objective is to find a smooth surrogate of the quality metric for the semantic/instance segmentation objective. They show that the proposed method is able to achieve state-of-the-art performance on Cityscapes and COCO datasets."
12325,SP:1952e174d9ec7b83ad1d394ece7fe77ea1f6d78d,"Probabilistic context - free grammars ( PCFGs ) CONJUNCTION dynamic Bayesian networks ( DBNs ). dynamic Bayesian networks ( DBNs ) CONJUNCTION Probabilistic context - free grammars ( PCFGs ). dynamic Bayesian networks ( DBNs ) HYPONYM-OF sequence models. Probabilistic context - free grammars ( PCFGs ) HYPONYM-OF sequence models. PCFGs USED-FOR nested hierarchical dependencies ( tree structures ). continuous latent variables USED-FOR DBNs. PCFGs CONJUNCTION DBNs. DBNs CONJUNCTION PCFGs. PCFGs USED-FOR Recursive Bayesian Networks ( RBNs ). RBNs USED-FOR joint distribution. discrete or continuous latent variables FEATURE-OF tree - structured Bayesian networks. tree - structured Bayesian networks USED-FOR joint distribution. exponential number of possible structures CONJUNCTION continuous variables. continuous variables CONJUNCTION exponential number of possible structures. exponential number of possible structures USED-FOR joint inference. maximum posterior estimates USED-FOR continuous latent variables. PCFGs USED-FOR inside and outside probabilities. inside and outside probabilities USED-FOR RBNs. gradient descent USED-FOR maximum posterior estimates. robust parameter optimisation CONJUNCTION Bayesian inference. Bayesian inference CONJUNCTION robust parameter optimisation. marginal data likelihood ( evidence ) CONJUNCTION marginal posterior distribution. marginal posterior distribution CONJUNCTION marginal data likelihood ( evidence ). change point detection CONJUNCTION hierarchical clustering. hierarchical clustering CONJUNCTION change point detection. RBNs USED-FOR segmentation. RBNs COMPARE change point detection. change point detection COMPARE RBNs. RBNs COMPARE hierarchical clustering. hierarchical clustering COMPARE RBNs. noisy sequences USED-FOR RBNs. examples EVALUATE-FOR RBNs. musical data USED-FOR hierarchical music analysis. raw note level USED-FOR hierarchical music analysis. OtherScientificTerm are dependencies, latent variables, nested hierarchical dependency structure, mixed discrete - continuous case, network structures, and expert annotations. Generic is neither. Method is Gaussian RBNs. Material is synthetic data. ","This paper proposes Recursive Bayesian Networks (RBNs), which is an extension of Probabilistic Context-Free Grammars (PCFGs) and dynamic Bayesian networks (DBNs). The main idea of RBNs is to use a tree-structured Bayesian network to model the joint distribution of discrete and continuous latent variables. The authors show that the RBN can be viewed as a combination of PCFGs and DBNs, and that it can be used to model both inside and outside probabilities of the data. The RBN is shown to be robust to noisy sequences, and it is shown that it is computationally efficient to compute the maximum posterior estimates of the continuous latent variable using gradient descent. ","This paper proposes a new model for hierarchical Bayesian networks, called Recursive Bayesian Networks (RBNs). RBNs are a tree-structured Bayesian network with continuous latent variables, where the latent variables can be discrete or continuous. The authors show that the RBN can be used to learn the joint distribution of continuous and discrete latent variables. They also show that it is possible to estimate the maximum posterior of the continuous latent variable by gradient descent. They show that this can be done by using a Gaussian RBN, which can be combined with PCFGs and DBNs.   The authors also provide a theoretical analysis of the performance of the proposed model."
12361,SP:5f29b169d3e4bbaeeec85e1aeebe2094fae4be6e,"objective functions FEATURE-OF deep neural networks. Backward propagation of errors ( backpropagation ) USED-FOR objective functions. loss functions HYPONYM-OF objective functions. pseudo - Lagrange multiplier method USED-FOR constrained backpropagation ( CBP ) algorithm. two - bit shift weight constraints HYPONYM-OF constraints. binary HYPONYM-OF constraints. ResNet-18 CONJUNCTION ResNet-50. ResNet-50 CONJUNCTION ResNet-18. ResNet-50 CONJUNCTION GoogLeNet. GoogLeNet CONJUNCTION ResNet-50. AlexNet CONJUNCTION ResNet-18. ResNet-18 CONJUNCTION AlexNet. CBP USED-FOR AlexNet. CBP USED-FOR GoogLeNet. CBP USED-FOR ResNet-18. posttraining method USED-FOR CBP. GoogLeNet PART-OF ImageNet. ImageNet USED-FOR CBP. backpropagation USED-FOR CBP. ResNet-18 CONJUNCTION ResNet50. ResNet50 CONJUNCTION ResNet-18. ResNet50 CONJUNCTION GoogLeNet. GoogLeNet CONJUNCTION ResNet50. algorithm COMPARE methods. methods COMPARE algorithm. top-1 accuracy EVALUATE-FOR ResNet-18. binary weights USED-FOR GoogLeNet. ImageNet EVALUATE-FOR methods. ImageNet EVALUATE-FOR algorithm. top-1 accuracy EVALUATE-FOR algorithm. CBP USED-FOR learning algorithm. constraint functions USED-FOR learning algorithm. constraint functions USED-FOR CBP. OtherScientificTerm are weight precision, objective function, and minimal performance loss. Generic is algorithms. Method is CBP algorithm. ",This paper proposes a novel constrained backpropagation (CBP) algorithm for training deep neural networks with weight-constrained constraints. The main idea is to use a pseudo-Lagrange multiplier method to compute the pseudo-lange multiplier (PL) method to reduce the loss of the objective function. The proposed method is shown to be computationally efficient and achieves state-of-the-art performance on ImageNet. ,"This paper proposes a new constrained backpropagation (CBP) algorithm for training deep neural networks. The proposed method is based on a pseudo-Lagrange multiplier method, where the objective function is constrained by two-bit shift weight constraints. The paper also proposes a post-training method to improve the performance of CBP. Experiments show that the proposed method outperforms the state-of-the-art in terms of accuracy. "
12397,SP:3ddf8e2e108fb261bb23aec8a27a25aba7523dc1,"acquisition function USED-FOR data / label efficiency. acquisition function USED-FOR Active learning. discrete instance set ( pool - based scenario CONJUNCTION continuous instance space ( query synthesis scenario ). continuous instance space ( query synthesis scenario ) CONJUNCTION discrete instance set ( pool - based scenario. active learning scenarios USED-FOR Gaussian Process Classification ( GPC ). active learning strategies USED-FOR classification error. active learning strategies USED-FOR Estimated Error Reduction ( EER ). gradient - based optimization techniques USED-FOR continuous instance space. continuous instance space USED-FOR query synthesis. it COMPARE gradient - based optimization techniques. gradient - based optimization techniques COMPARE it. gradient - based optimization techniques USED-FOR query synthesis. algorithms USED-FOR EER - based active learning. GPC USED-FOR EER - based active learning. one - dimensional integral USED-FOR joint predictive distribution of label pairs. gradient chain rule USED-FOR gradient of the acquisition function. query synthesis active learning algorithm USED-FOR EER - based strategies. algorithms COMPARE state - of - the - art algorithms. state - of - the - art algorithms COMPARE algorithms. sampling efficiency EVALUATE-FOR state - of - the - art algorithms. synthetic and real - world datasets EVALUATE-FOR state - of - the - art algorithms. sampling efficiency EVALUATE-FOR algorithms. synthetic and real - world datasets EVALUATE-FOR algorithms. Task is labeling. Method is onestep - look - ahead manner. OtherScientificTerm are EER - based acquisition functions, and EER. Metric is computational overhead. ","This paper studies active learning in Gaussian Process Classification (GPC) and proposes to use estimated error reduction (ER) as the acquisition function to improve the data/label efficiency. The proposed method is based on the estimation of the joint predictive distribution of label pairs in the discrete instance set and continuous instance space. The authors propose a query synthesis active learning algorithm for EER-based active learning, which uses a gradient chain rule to optimize the gradient of acquisition function. The experiments on synthetic and real-world datasets show the effectiveness of the proposed method.","This paper proposes a new active learning strategy for Gaussian process classification (GPC) based on estimated error reduction (EER) to improve the data/label efficiency of active learning. The main idea is to use a one-dimensional integral to estimate the joint predictive distribution of label pairs, and then use a gradient chain rule to optimize the gradient of the acquisition function. The proposed method is evaluated on synthetic and real-world datasets. "
12433,SP:fa1fac04cd4ccb1f3eaf80807db09f9683ce6b50,"energy functions USED-FOR bounded gradients. gradients USED-FOR numerical instabilities. regularization FEATURE-OF autoencoder - based architectures. low - dimensional manifold FEATURE-OF data. natural images HYPONYM-OF data. natural images HYPONYM-OF low - dimensional manifold. VAE models HYPONYM-OF autoencoder - based architectures. over - regularization CONJUNCTION underregularization. underregularization CONJUNCTION over - regularization. infinite gradients FEATURE-OF autoencoder - based energy function. Method are continuous variational autoencoder ( VAE ) models, and VAE energy function. OtherScientificTerm are parameter gradients, unbounded gradients, posterior collapse, overand under - regularization, and large gradients. Generic is model. Task is suboptimal feature selection. ","This paper studies the effect of over- and under-regularization in continuous variational autoencoder (VAE) models. In particular, the authors show that unbounded gradients can lead to posterior collapse, where the posterior of the model converges to the ground truth distribution. The authors also show that the gradients are unbounded in the case of infinite gradients.   The main contribution of the paper is a theoretical analysis of the effects of under- and over-regularized VAE models. ","This paper studies the effect of over- and under-regularization in continuous variational autoencoder (VAE) models. The authors show that under- and over-regularized VAE models suffer from the posterior collapse of unbounded gradients, which is a result of the over-parameterization of the energy function and the under-parametrizing of the parameter gradients. They also show that unbounded energy functions have infinite gradients and show that the overparameterized energy function has bounded gradients in the case of finite data.   The authors also provide a theoretical analysis of the effects of under-and over-normalized energy functions."
12469,SP:2611cfd6e0696a57d061687993cef1fe5c95999d,"graph feedback USED-FOR bandit problem. directed graph USED-FOR bandit problem. min - max regret EVALUATE-FOR graph. fractional weak domination number CONJUNCTION k - packing independence number. k - packing independence number CONJUNCTION fractional weak domination number. linear program USED-FOR them. fractional vertex packing set HYPONYM-OF linear program. strong duality theorem USED-FOR regret upper bound. integrality gap FEATURE-OF dual linear program. trees CONJUNCTION graphs. graphs CONJUNCTION trees. bounded integrality gap FEATURE-OF vertex packing problem. graphs FEATURE-OF vertex packing problem. trees FEATURE-OF vertex packing problem. bounded degree FEATURE-OF graphs. OtherScientificTerm are bandit arms, lower bound, and optimal regret. Metric is regret. Generic are notions, and bounds. ","This paper considers the problem of learning a directed graph with feedback in a bandit setting, where the goal is to maximize the regret of a given set of arms. The main contribution of this paper is to provide a regret upper bound for the directed bandit problem. The upper bound is based on the strong duality theorem, which shows that the regret is bounded by a bounded integrality gap. The regret bound is then extended to the case of directed trees and graphs with bounded degree. ","This paper studies the regret upper bound for the bandit problem in directed graphs. The main result is based on the strong duality theorem, which shows that the regret of a directed graph is bounded by the fractional weak domination number and the k-packing independence number. The upper bound is also proved for the bounded integrality gap of a dual linear program. The lower bound is proved for a bounded degree of graphs with bounded degree."
12505,SP:e50dec57af337839cbde4b65fb7b431785fda44d,"Shapley values USED-FOR model agnostic feature attributions. global population distribution USED-FOR feature absence. neighbourhood reference distributions USED-FOR Shapley values. Nadaraya - Watson estimator HYPONYM-OF kernel regressor. self - normalised importance sampling estimator USED-FOR Nadaraya - Watson estimator. Neighbourhood Shapley values USED-FOR sparse feature relevance attributions. on - manifold explainability CONJUNCTION robustness. robustness CONJUNCTION on - manifold explainability. robustness EVALUATE-FOR adversarial classifiers. They USED-FOR adversarial classifiers. robustness EVALUATE-FOR They. on - manifold explainability EVALUATE-FOR They. OtherScientificTerm are global population, and local model behaviour. Method is Shapley analysis. ",This paper proposes to use neighbourhood Shapley values for model agnostic feature attributions in sparse feature relevance attributions. The proposed method is based on the Shapley analysis of the global population distribution and the neighbourhood reference distributions. The main contribution of the paper is to use the Nadaraya-Watson estimator as a kernel regressor for the kernel of the model. The authors show that this estimator is a self-normalized importance sampling estimator and can be used to estimate the neighbourhood Shapleys values. The results show that the proposed method improves the on-manifold explainability and robustness against adversarial examples.,"This paper proposes a new Shapley analysis method for model agnostic feature attributions for sparse feature relevance attributions. The key idea is to use the global population distribution of the neighbourhood reference distributions to estimate the Shapley values for feature absence. The method is based on the Nadaraya-Watson estimator, which is a kernel regressor with a self-normalised importance sampling estimator. The authors show that the proposed method is robust against adversarial classifiers."
12541,SP:35bdeb78f9fe74e754177fb54b48e7399dc8590d,"feature representations USED-FOR deep reinforcement learning ( RL ). them USED-FOR feature learning. state - action sequences HYPONYM-OF un - experienced or less - experienced trajectories. data efficiency EVALUATE-FOR RL feature representation learning. backward dynamics model USED-FOR trajectory cycle. dynamics model USED-FOR PlayVirtual. actions USED-FOR virtual state - action trajectories. cycle consistency constraint FEATURE-OF trajectory. Atari and DeepMind Control Suite benchmarks EVALUATE-FOR designs. benchmarks EVALUATE-FOR method. Method is RL. OtherScientificTerm are data inefficiency, cycle - consistent virtual trajectories, latent space, and groudtruth state supervision. ","This paper proposes PlayVirtual, a novel method for learning cycle-consistent virtual state-action trajectories in deep reinforcement learning. The proposed method is based on the observation that most of the trajectories learned in deep RL are un-experienced or less-experience trajectories, which can lead to data inefficiency. To address this issue, the authors propose to learn a cycle-constrained virtual trajectory in the latent space, which is then used to guide the learning process. The authors show that the proposed method achieves state-of-the-art performance on Atari and DeepMind Control Suite benchmarks.","This paper proposes PlayVirtual, a method for learning a virtual state-action sequence that is cycle-consistency-constrained. The key idea is to use a backward dynamics model to model a trajectory cycle, which is then used to learn the dynamics of a trajectory in the latent space. The dynamics model is modeled by a dynamics model that is used to predict the trajectory of the agent. The authors show that the dynamics model can be used to train a virtual trajectory that is consistent with the environment. The proposed method is evaluated on Atari and DeepMind Control Suite benchmarks. "
12577,SP:ca09e472cbcf2ac8c8c9b192a87df2ed59218210,Noisy labels FEATURE-OF large real - world datasets. noisy labels FEATURE-OF robustness. robustness EVALUATE-FOR network ’s architecture. framework USED-FOR robustness. robustness EVALUATE-FOR network. architecture CONJUNCTION target / noise functions. target / noise functions CONJUNCTION architecture. predictive power EVALUATE-FOR representations. predictive power EVALUATE-FOR network ’s robustness. representations USED-FOR linear model. clean labels USED-FOR linear model. architecture COMPARE noise. noise COMPARE architecture. architecture USED-FOR network. predictive power COMPARE methods. methods COMPARE predictive power. predictive power USED-FOR representations. predictive power COMPARE noisy - label - training methods. noisy - label - training methods COMPARE predictive power. representations COMPARE noisy - label - training methods. noisy - label - training methods COMPARE representations. test accuracy EVALUATE-FOR noisy - label - training methods. test accuracy EVALUATE-FOR predictive power. clean labels USED-FOR methods. Method is neural network architectures. ,"This paper studies the problem of robustness to noisy labels in deep neural networks. The authors propose a framework to evaluate the robustness of a neural network architecture in terms of its predictive power in the presence of noisy labels. The proposed framework is based on the observation that a network's robustness depends on the architecture and the target/noise functions, and that the network's predictive power can be measured by the predictive power of the networks' representations.  The authors show that the proposed framework can be used to train a linear model with clean labels on a large number of datasets with noisy labels, and show that it is able to achieve state-of-the-art performance on these datasets.  ","This paper proposes a new framework for measuring the robustness of neural networks against noisy labels. The authors propose a new metric that measures the predictive power of a network’s representations in the presence of noisy labels, which they call “robustness of representations”. They show that a network trained with noisy labels is more robust than one trained with clean labels. They also show that training with a linear model can be as robust as training with noise, and show that it can be even more robust if the network is trained with a clean model."
12613,SP:903727fe028684623a8ccadec210e641ecffc685,"RL algorithm USED-FOR reward function. method USED-FOR value function. transitions USED-FOR value function. hyperparameters USED-FOR method. data - driven Bellman equation USED-FOR method. approach COMPARE prior methods. prior methods COMPARE approach. Method are Reinforcement learning ( RL ) algorithms, RL algorithms, and control algorithm. Generic are process, and two - stage process. OtherScientificTerm are intermediate reward function, and reward function term. ","This paper proposes a two-stage process to learn the reward function in reinforcement learning. The main idea is to use the Bellman process to estimate the value function of the state-action pairs, and then use this value function as the intermediate reward function. The authors show that this two stage process can be used to train an RL algorithm with hyperparameters that are close to the true value function. They show that the proposed method is computationally efficient.","This paper proposes a two-stage control algorithm for reinforcement learning (RL) where the goal is to learn a value function that maximizes the reward function. The authors propose a data-driven Bellman equation to solve this problem. The main idea is to use a two stage process, where the first stage is to control the value function, and the second stage is the control of the intermediate reward function, which is the transition between the two stages.  The authors show that their method can be applied to a variety of hyperparameters, and show that it can be used in conjunction with other control algorithms. "
12649,SP:39ccbd5909a1d7ed212fe92d8d6843c2c70dfe1f,"convex and non - convex settings FEATURE-OF differentially private stochastic optimization. algorithm USED-FOR l2 setting. differentially private algorithms USED-FOR general convex losses. algorithm USED-FOR optimal excess population risk. near - linear time FEATURE-OF algorithm. super - linear time FEATURE-OF differentially private algorithms. algorithm USED-FOR l1 setting. algorithm USED-FOR dimension dependent lower bound. nearly - optimal excess population risk FEATURE-OF algorithm. algorithms USED-FOR differentially private non - convex setting. smooth losses CONJUNCTION polyhedral constraint. polyhedral constraint CONJUNCTION smooth losses. smooth losses FEATURE-OF l1 - case. polyhedral constraint FEATURE-OF l1 - case. linear time FEATURE-OF nearly dimension independent rate. smooth losses FEATURE-OF constrained l2 - case. linear - time algorithm USED-FOR constrained l2 - case. method USED-FOR non - smooth weakly convex stochastic optimization. method COMPARE non - private algorithm. non - private algorithm COMPARE method. method USED-FOR l2 - case. non - convex l2 setting CONJUNCTION lp setting. lp setting CONJUNCTION non - convex l2 setting. Material is convex case. OtherScientificTerm are general non - smooth convex losses, and polylogarithmic ( in the dimension ) overhead. ","This paper studies the problem of differentially private stochastic optimization in the convex and non-convex setting. The main contributions of this paper are:  1. The authors prove a near-linear time algorithm in the l1 and l2 setting for differentially-private convex loss functions.  2. In the l2 and lp setting, the authors provide a nearly-dimension-independent rate in the polylogarithmic overhead.  3. Theorem 3.1 shows that the optimal excess population risk of the proposed algorithm is near-optimal.   ","This paper studies the problem of differentially private stochastic optimization in the convex and non-convex setting. The main contribution of the paper is a new lower bound on the excess population risk in the non-smooth l2 setting. This lower bound is based on the fact that the polyhedral constraint in the l1-case is polylogarithmic (in the dimension) and the smooth case is polyhedral. The lower bound of the l2-case can be obtained in linear time. The upper bound is obtained for the lp-case, and the lower bound can be derived for the constrained l2 case.   "
12685,SP:99a476f71e6901aefe281f11fb72ff78265a5b6e,"cooperative bandit problem USED-FOR large - scale decision - making. arbitrary corruptions CONJUNCTION delays. delays CONJUNCTION arbitrary corruptions. arbitrary corruptions FEATURE-OF stochastic networks. stochastic networks USED-FOR communication. cooperative bandit learning HYPONYM-OF real - world communication scenarios. adversarially corrupted rewards FEATURE-OF message - passing. random delays FEATURE-OF network. byzantine communication PART-OF message - passing. network USED-FOR instantaneous rewardsharing. stochastic time - varying networks USED-FOR message - passing. message - passing HYPONYM-OF real - world communication scenarios. instantaneous rewardsharing HYPONYM-OF real - world communication scenarios. message - passing HYPONYM-OF real - world communication scenarios. near - optimal guarantees FEATURE-OF incurred group regret. decentralized algorithms CONJUNCTION near - optimal guarantees. near - optimal guarantees CONJUNCTION decentralized algorithms. decentralized algorithms USED-FOR environments. delayed - update algorithm COMPARE state - of - the - art. state - of - the - art COMPARE delayed - update algorithm. network topologies EVALUATE-FOR delayed - update algorithm. network topologies EVALUATE-FOR state - of - the - art. tight network - dependent minimax lower bounds FEATURE-OF group regret. Generic are problem, and algorithms. OtherScientificTerm is perfect communication. Task is real - world distributed settings. ","This paper studies the cooperative bandit problem with adversarially corrupted rewards in message-passing with stochastic time-varying networks. In this setting, the goal is to maximize the mutual information between the agents. The authors propose a decentralized algorithm with near-optimal regret guarantees. They also provide a tight network-dependent minimax lower bound for the group regret.  ","This paper studies cooperative bandit learning with stochastic time-varying time-divergence networks. In particular, the authors study the problem of group regret under adversarially corrupted rewards in a message-passing setting. The authors provide a tight network-dependent minimax lower bounds for the group regret. They show that the proposed method can achieve near-optimal group regret guarantees. "
12721,SP:d3e896a65470f2439bc7753b4f66e152306b2d6f,"transformer USED-FOR computer vision applications. architectures USED-FOR feature representations. vision transformers USED-FOR feature representations. convolutional neural networks COMPARE vision transformers. vision transformers COMPARE convolutional neural networks. architectures FEATURE-OF vision transformers. mobile devices USED-FOR feature representations. post - training quantization algorithm USED-FOR vision transformers. optimal low - bit quantization intervals USED-FOR quantization task. ranking loss USED-FOR quantization objective. quantization loss CONJUNCTION feature diversity. feature diversity CONJUNCTION quantization loss. nuclear norm FEATURE-OF attention map. nuclear norm USED-FOR mixedprecision quantization scheme. method COMPARE posttraining quantization algorithms. posttraining quantization algorithms COMPARE method. benchmark models EVALUATE-FOR method. top-1 accuracy EVALUATE-FOR DeiT - B model. ImageNet dataset EVALUATE-FOR DeiT - B model. Method is attention mechanism. OtherScientificTerm are self - attention, and quantization. "," transformers are a popular architecture in computer vision. However, they are computationally expensive to train. This paper proposes a post-training quantization algorithm for vision transformers. The main idea is to use a low-bit quantization intervals to improve the performance of the quantization task.   ",This paper proposes a post-training quantization algorithm for vision transformers. The main idea is to use the nuclear norm of the attention map as a quantization objective to improve the performance of the quantization task. The proposed method is evaluated on the DeiT-B model on the ImageNet dataset.
12757,SP:aa6b1328585b5916267a3ff4f9119e7aa4ce2bb5,"Double Q - learning USED-FOR overestimation issue of Q - learning. polynomial learning rate USED-FOR analysis. polynomial learning rate USED-FOR slower convergence rate. analytical tools USED-FOR convergence rate. sampling strategy USED-FOR asynchronous double Q - learning. synchronous double Q - learning USED-FOR global optimum. time complexity EVALUATE-FOR asynchronous algorithm. fast convergence FEATURE-OF double - Q learning. Method are Q - learning, double Q - learning, and finite - time analysis. OtherScientificTerm are constant learning rate, state - action space, and discount factor. ","This paper studies the overestimation issue of Q-learning from the perspective of double-Q-learning. In particular, the authors show that the global optimum of double Q learning has a global convergence rate of $O(1/\epsilon^2)$ with a polynomial learning rate. The authors also provide a finite-time analysis of the convergence rate.   The main contributions of the paper are:  - The authors provide a new analysis for double-q-learning in the setting where the learning rate is constant and the state-action space is finite-horizon.  - A new sampling strategy is proposed to improve the convergence of the synchronous double Q-Learning. ","This paper studies the overestimation issue of double-Q-learning, where the learning rate is polynomial in the state-action space. The authors show that the global optimum of synchronous double Q-learning is a global optimum with a finite-time analysis. They also provide a sampling strategy for asynchronous double-q-learning and show that it can converge to the global optimal."
12793,SP:04fd4d83717c4f7e1a4b5651a59200151f33411d,"unlabeled and test data COMPARE labeled data. labeled data COMPARE unlabeled and test data. SSL algorithms USED-FOR real - world applications. semi - supervised OOD detection HYPONYM-OF setting. labeled data CONJUNCTION in - distribution data. in - distribution data CONJUNCTION labeled data. approach STEP USED-FOR OOD detection. technique USED-FOR approach STEP. Structure - Keep Unzipping HYPONYM-OF technique. It USED-FOR representation space. representation space USED-FOR OOD samples. STEP approach COMPARE methods. methods COMPARE STEP approach. OOD detection benchmarks EVALUATE-FOR STEP approach. benchmarks EVALUATE-FOR detection. detection EVALUATE-FOR STEP approach. benchmarks EVALUATE-FOR STEP approach. Task are semi - supervised learning ( SSL ) studies, OOD detection settings, and training. OtherScientificTerm are distribution of labeled data, and unknown distribution. Method is optimization algorithm. ",This paper studies semi-supervised out-of-distribution (OOD) detection in the presence of unlabeled and test data. The authors propose a new method called STEP for OOD detection based on the structure-keep-unzipping (STU) technique. The main idea is to use STU to learn the representation space of the OOD samples and then use it to estimate the distribution of the labeled data from the unknown distribution. Experiments show that the proposed method outperforms the existing methods in terms of detection accuracy. ,"This paper proposes a method for semi-supervised out-of-distribution (OOD) detection in SSL settings. The main idea is to use the structure-keep-unzipping (STU) technique to learn a representation space for OOD samples, and then use this representation space to learn an OOD detection algorithm. The proposed method is evaluated on a variety of benchmarks, and it is shown to outperform the state of the art. "
12829,SP:6bf8b94483b26033795b0eda9649518027f5e1c2,phrase localization CONJUNCTION referring expression comprehension / segmentation. referring expression comprehension / segmentation CONJUNCTION phrase localization. visual grounding USED-FOR visual reasoning. referring expression comprehension / segmentation HYPONYM-OF visual grounding. phrase localization HYPONYM-OF visual grounding. approaches USED-FOR referring expression comprehension ( REC ). approaches USED-FOR segmentation ( RES ). referring expression comprehension ( REC ) CONJUNCTION segmentation ( RES ). segmentation ( RES ) CONJUNCTION referring expression comprehension ( REC ). one - stage multi - task framework USED-FOR visual grounding tasks. modalities PART-OF visual - lingual encoder. modalities PART-OF transformer architecture. model USED-FOR contextualized lingual queries. segmentation mask USED-FOR referred regions. model USED-FOR decoder. contextualized model COMPARE state - of - the - art methods. state - of - the - art methods COMPARE contextualized model. REC and RES tasks EVALUATE-FOR contextualized model. REC and RES tasks EVALUATE-FOR state - of - the - art methods. external dataset EVALUATE-FOR pre - training schedule. contextualized information CONJUNCTION multi - task training. multi - task training CONJUNCTION contextualized information. contextualized information USED-FOR model. multi - task training USED-FOR model. Generic is two - stage setup. Method is complex task - specific one - stage architectures. OtherScientificTerm is bounding box. ,"This paper proposes a one-stage multi-task framework for visual grounding tasks. The proposed framework consists of a visual-lingual encoder, a transformer architecture, and a segmentation mask. The model is trained using contextualized lingual queries and a bounding box. Experiments show that the proposed method achieves state-of-the-art results on the two grounding tasks, REC and RES.",This paper proposes a multi-task framework for visual grounding tasks. The proposed framework is based on a transformer architecture with a visual-lingual encoder and a segmentation-based decoder. The model is trained using a pre-trained pre-training schedule on an external dataset. Experiments show that the proposed method outperforms state-of-the-art methods on both REC and RES tasks. 
12865,SP:29b552b36696c9bda72f3ab4f31605d98880fd6b,"Boosting HYPONYM-OF algorithmic approach. weak learner HYPONYM-OF agnostic PAC learner. classification loss EVALUATE-FOR agnostic PAC learner. boosting algorithm USED-FOR weak hypotheses. booster CONJUNCTION weak learner. weak learner CONJUNCTION booster. OtherScientificTerm are weak and moderately inaccurate hypotheses, and weak - learner calls. Method are multiclass boosting, Multiclass boosting, boosting, and AdaBoost. Metric is weak learner ’s accuracy parameter. ","This paper studies the problem of learning weak and moderately inaccurate hypotheses in multi-class boosting, where the weak learner is an agnostic PAC learner. The authors propose a boosting algorithm for weak hypotheses and show that it is possible to learn weak hypotheses in this setting. They also show that the classification loss of the agnostic learner can be improved by a factor of at least 1.  ","This paper studies the problem of agnostic PAC learning, where the learner is agnostic to the weak and moderately inaccurate hypotheses. The authors propose a new algorithm, AdaBoost, which is based on multiclass boosting. They show that the weak learner’s accuracy parameter is the same as the agnostic learner's accuracy parameter. They also show that AdaBoost can be used to improve the performance of weak learners.  "
12901,SP:f63b050773871338c48b778c362172e4b72477a4,"methods USED-FOR unsupervised object segmentation. methods USED-FOR interpretable object - centric scene generation. unsupervised object segmentation CONJUNCTION interpretable object - centric scene generation. interpretable object - centric scene generation CONJUNCTION unsupervised object segmentation. limited visual complexity FEATURE-OF simulated and real - world datasets. simulated and real - world datasets EVALUATE-FOR methods. RNNs USED-FOR object representations. paradigms COMPARE embedding - based approach. embedding - based approach COMPARE paradigms. clustering procedure USED-FOR randomly ordered object representations. iterative refinement COMPARE clustering procedure. clustering procedure COMPARE iterative refinement. RNNs CONJUNCTION iterative refinement. iterative refinement CONJUNCTION RNNs. GENESIS - V2 USED-FOR variable number of object representations. GENESIS - V2 HYPONYM-OF model. RNNs USED-FOR variable number of object representations. iterative refinement USED-FOR variable number of object representations. GENESIS - V2 COMPARE baselines. baselines COMPARE GENESIS - V2. unsupervised image segmentation CONJUNCTION object - centric scene generation. object - centric scene generation CONJUNCTION unsupervised image segmentation. synthetic datasets CONJUNCTION real - world datasets. real - world datasets CONJUNCTION synthetic datasets. GENESIS - V2 USED-FOR unsupervised image segmentation. GENESIS - V2 USED-FOR object - centric scene generation. baselines USED-FOR unsupervised image segmentation. synthetic datasets USED-FOR unsupervised image segmentation. object - centric scene generation EVALUATE-FOR baselines. real - world datasets USED-FOR object - centric scene generation. synthetic datasets USED-FOR object - centric scene generation. synthetic datasets EVALUATE-FOR GENESIS - V2. real - world datasets EVALUATE-FOR GENESIS - V2. real - world datasets EVALUATE-FOR baselines. Method are unsupervised learning of object - representations, and stochastic stick - breaking process. OtherScientificTerm is unnatural ordering. "," image segmentation and object-centric scene generation is an important problem in the unsupervised learning of object representations. This paper proposes a novel method for object segmentation based on a clustering-based approach. The proposed method, called GENESIS-V2, is able to learn a variable number of objects by iteratively refining the object representations by a stochastic stick-breaking process. Experiments on simulated and real-world datasets demonstrate the effectiveness of the proposed method.  ","This paper presents a novel method for unsupervised object segmentation and object-centric scene generation. The method is based on a clustering-based approach, where the object representations are randomly ordered. The authors propose a new model, called GENESIS-V2, which is trained with a stochastic stick-breaking process. The proposed method is evaluated on synthetic and real-world datasets. "
12937,SP:408deb9e5577ee7118b836fee77135df641fe545,"black box method USED-FOR point predictions. conformal inference USED-FOR framework. conformal inference methods COMPARE adaptive approach. adaptive approach COMPARE conformal inference methods. coverage frequency EVALUATE-FOR adaptive approach. learning problem USED-FOR distribution shift. adaptive conformal inference HYPONYM-OF method. real world datasets EVALUATE-FOR adaptive conformal inference. real world datasets EVALUATE-FOR method. Generic is methods. OtherScientificTerm are data generating distribution, data generating process, and distribution shifts. ","This paper proposes an adaptive conformal inference method for point prediction in the presence of distribution shifts in the data generating distribution. The proposed method is based on the observation that the coverage frequency of point predictions can be affected by distribution shifts. To address the distribution shifts, the authors propose to use a conformal model to estimate the distribution shift in the generating process. The authors show that the proposed method outperforms the state-of-the-art conformal methods in terms of coverage frequency.  ","This paper proposes an adaptive conformal inference framework for point prediction. The main idea is to use a black-box method to learn the distribution shift in the data generating process, and then use the learned distribution shift as a learning problem to improve the performance of the model. The proposed method is evaluated on a variety of real-world datasets, and it outperforms the state-of-the-art. "
12973,SP:e6e5b1e2428abcf1a163ec1cce15cd299f9a544f,"crowded scenes USED-FOR Multi - person pose estimation. bounding box detection CONJUNCTION keypoint grouping. keypoint grouping CONJUNCTION bounding box detection. bounding box detection PART-OF direct pose - level inference strategy. keypoint grouping PART-OF direct pose - level inference strategy. Pose - level Inference Network ( PINet ) USED-FOR complete pose cues. visible body parts USED-FOR complete pose cues. Part - based Pose Generation ( PPG ) USED-FOR coarse poses. PINet USED-FOR coarse poses. Part - based Pose Generation ( PPG ) USED-FOR PINet. pose priors USED-FOR Pose Refinement module. Pose Refinement module USED-FOR coarse poses. visual body cues USED-FOR global pose cues. visual body cues USED-FOR PINet. discriminative body parts USED-FOR PINet. crowded scenes pose estimation benchmarks EVALUATE-FOR PINet. AP EVALUATE-FOR it. OCHuman dataset EVALUATE-FOR it. OtherScientificTerm are overlapping and occlusions, person bounding boxes, and pose cues. Method is Pose Fusion module. ",This paper proposes a novel method for multi-person pose estimation in crowded scenes. The proposed method is based on a Pose-level Inference Network (PINet) that learns to infer the complete pose cues from visible body parts. A Pose Refinement module is used to refine the pose priors for coarse poses and a Pose Fusion module uses visual body cues to generate global pose cues. Experiments show that the proposed method achieves state-of-the-art results on several crowded scenes pose estimation benchmarks. ,"This paper proposes a Pose-level Inference Network (PINet) for multi-person pose estimation in crowded scenes. PINet is based on a part-based Pose Generation (PPG) and Pose Refinement (Pose Fusion) module. The PINet uses bounding box detection and keypoint grouping to detect overlapping and occlusions, and pose-level inference strategy to infer the pose cues. The proposed PINet achieves state-of-the-art results on the OCHuman dataset. "
13022,SP:e76f048c3dccffcb8bcc6a66f6165fc19d175610,Robust Markov decision processes ( RMDPs ) PART-OF robust reinforcement learning algorithms. algorithm USED-FOR Bellman operator. Bellman operator USED-FOR S - rectangular robust Markov decision processes. L∞-constrained rectangular ambiguity sets FEATURE-OF S - rectangular robust Markov decision processes. homotopy continuation method CONJUNCTION bisection method. bisection method CONJUNCTION homotopy continuation method. algorithm USED-FOR S - rectangular ambiguity. bisection method USED-FOR S - rectangular ambiguity. homotopy continuation method USED-FOR S - rectangular ambiguity. quasi - linear time FEATURE-OF S - rectangular ambiguity. bisection method PART-OF algorithm. homotopy continuation method PART-OF algorithm. cubic time FEATURE-OF leading general linear programming methods. leading general linear programming methods USED-FOR algorithm. cubic time EVALUATE-FOR algorithm. it COMPARE leading commercial optimization package. leading commercial optimization package COMPARE it. Generic is method. ,"This paper studies the problem of learning robust RMDPs in the presence of S-rectangular ambiguity. In particular, the authors consider the case where the Bellman operator is restricted to L-constrained rectangular ambiguity sets. The main contribution of this paper is to provide a quasi-linear time algorithm for solving the S- rectangular RMDP problem.    The main contributions of the paper are as follows:  1. The authors propose a new algorithm for learning robust MDPs.  2. The algorithm is based on the homotopy continuation method and the bisection method.  3. The proposed algorithm is shown to converge in cubic time. ","This paper studies the problem of solving robust Markov decision processes (RMDPs) with L-constrained rectangular ambiguity sets. The main contribution of this paper is to propose a new algorithm for solving RMDPs with rectangular ambiguity set. The proposed algorithm is based on the Bellman operator, which is a well-known and well-studied operator. The authors show that the proposed algorithm can solve RMDP with quasi-linear time in the case of S-rectangular ambiguity set, and in cubic time for S- rectangular ambiguity."
13071,SP:c4af66a64a5c2bd58ca2e29dbc4b27d5bf4b63b8,machine - learned predictions USED-FOR online algorithms. generalized one - way trading CONJUNCTION two - stage online knapsack. two - stage online knapsack CONJUNCTION generalized one - way trading. competitive ratio EVALUATE-FOR online algorithms. Task is online knapsack problem. OtherScientificTerm is upper and lower bound. ,"This paper studies the problem of generalized one-way trading in the online knapsack setting, where the goal is to maximize the competitive ratio between the buyer and seller. The main contribution of this paper is to study the online algorithms for this problem. The authors show that the competitive ratios between the buyers and sellers can be computed using machine-learned predictions. They also show that there exists a two-stage online algorithm for the problem. ",This paper studies the problem of generalized one-way trading in a two-stage online knapsack setting. The paper provides a lower bound on the competitive ratio between two online trading algorithms. The upper and lower bounds are derived for generalized one way trading and two stage online knapack. The lower bound is based on the fact that the upper bound is lower than the lower bound. 
13120,SP:1d478d4fa3f5df0ded963ef164325667fd744dbb,"Episodic control USED-FOR reinforcement learning. model - based episodic memory of trajectories USED-FOR episodic control. memory USED-FOR agent. memory USED-FOR complementary learning model. model - based, episodic and habitual learning PART-OF architecture. dynamic hybrid control CONJUNCTION model - based, episodic and habitual learning. model - based, episodic and habitual learning CONJUNCTION dynamic hybrid control. dynamic hybrid control USED-FOR complementary learning model. model - based, episodic and habitual learning USED-FOR complementary learning model. model COMPARE reinforcement learning agents. reinforcement learning agents COMPARE model. OtherScientificTerm are episodic memory, and stochastic and non - Markovian settings. ","This paper proposes a model-based episodic control method that uses episodic memory of trajectories to improve the performance of reinforcement learning agents in non-Markovian and stochastic settings. The proposed method is based on a combination of episodic and habitual learning, where the former is used to learn a model of the past and the latter is used as an episodic representation of the current state. The authors show that the proposed method outperforms baselines in a variety of tasks.   ","This paper proposes a model-based, episodic and habitual learning architecture for episodic control in reinforcement learning. The proposed method is based on a combination of two existing approaches: dynamic hybrid control and a complementary learning model. The authors show that the proposed method outperforms the state-of-the-art in both stochastic and non-Markovian settings. "
13169,SP:551174c1266b5f4b6aaf5432a4c713386f90898c,labeled data USED-FOR deep learning. Semi - supervised learning ( SSL ) USED-FOR unlabeled data. pseudo labels USED-FOR unlabeled data. data programming ( DP ) scheme USED-FOR probabilistic labels. probabilistic labels USED-FOR unlabeled data. DP - SSL HYPONYM-OF SSL method. data programming ( DP ) scheme USED-FOR SSL method. LFs PART-OF SSL style. DP methods USED-FOR initial labeling functions ( LFs ). human experts USED-FOR DP methods. noisy labels USED-FOR label model. probabilistic labels USED-FOR unlabeled samples. LFs USED-FOR noisy labels. DP - SSL COMPARE SSL methods. SSL methods COMPARE DP - SSL. classification EVALUATE-FOR SSL methods. SSL benchmarks EVALUATE-FOR DP - SSL. test sets EVALUATE-FOR SSL methods. DP - SSL USED-FOR unlabeled data. test sets EVALUATE-FOR classification. classification EVALUATE-FOR DP - SSL. test sets EVALUATE-FOR DP - SSL. CIFAR-10 EVALUATE-FOR DP - SSL. classification accuracy EVALUATE-FOR test data. unlabeled data EVALUATE-FOR DP - SSL. test data EVALUATE-FOR DP - SSL. classification accuracy EVALUATE-FOR DP - SSL. annotation accuracy EVALUATE-FOR DP - SSL. Method is SSL. Material is labeled samples. ,This paper proposes a data programming-based semi-supervised learning (SSL) method that uses pseudo labels for unlabeled data. The proposed method is based on the data programming (DP) scheme and uses probabilistic labels to generate pseudo labels. The DP-SSL method is evaluated on several SSL benchmarks and test sets and achieves state-of-the-art performance. ,"This paper proposes a data programming-based semi-supervised learning (SSL) method for unlabeled data. The proposed method is based on the data programming (DP) scheme, which uses probabilistic labels as initial labeling functions (LFs) in the SSL style. The authors show that DP-SSL outperforms other SSL methods on CIFAR-10 and other SSL benchmarks. "
13218,SP:d1d6a40a8bde62a21da4fc18a076e344c84ab0d0,"Multi - view Pose transformer ( MvP ) USED-FOR estimating multi - person 3D poses. multi - view images USED-FOR Multi - view Pose transformer ( MvP ). multi - view images USED-FOR estimating multi - person 3D poses. MvP USED-FOR multi - person 3D poses. volumetric representation USED-FOR per - person 3D pose. volumetric representation USED-FOR estimating 3D joint locations. detected 2D poses USED-FOR per - person 3D pose. query embeddings USED-FOR MvP. query embeddings USED-FOR skeleton joints. accuracy EVALUATE-FOR pipeline. hierarchical scheme USED-FOR query embeddings of multi - person skeleton joints. accuracy EVALUATE-FOR MvP. hierarchical scheme USED-FOR MvP. geometrically guided attention mechanism USED-FOR cross - view information. MvP USED-FOR geometrically guided attention mechanism. projective attention HYPONYM-OF geometrically guided attention mechanism. feature representations USED-FOR projective attention. view - dependent camera geometry PART-OF feature representations. RayConv operation USED-FOR MvP. MvP model COMPARE state - of - the - art methods. state - of - the - art methods COMPARE MvP model. it COMPARE approach. approach COMPARE it. Panoptic dataset EVALUATE-FOR AP25. AP25 EVALUATE-FOR it. Panoptic dataset EVALUATE-FOR it. MvP USED-FOR recovering human mesh. MvP USED-FOR modeling multi - person body shapes. SMPL model USED-FOR modeling multi - person body shapes. SMPL model USED-FOR recovering human mesh. Generic is intermediate tasks. OtherScientificTerm are multi - view information, 3D joint locations, human mesh, and multi - person body shapes. Method is inputdependent query adaptation approach. ",This paper proposes a multi-view pose transformer (MVPT) model for estimating multi-person 3D poses from multiple views. The proposed model is based on an attention mechanism that takes into account the cross-view information and uses it to estimate the 3D joint locations. The method is evaluated on the Panoptic dataset and achieves state-of-the-art performance. ,This paper proposes a multi-view pose transformer (MVPT) model for estimating multi-person 3D poses. The proposed model is based on a volumetric representation for estimating 3D joint locations and a geometrically guided attention mechanism for cross-view information. The model is evaluated on the Panoptic dataset AP25 and the human mesh dataset.
13267,SP:2e147bd5321e25bb27d2531fd58c46460a1e5320,"sparse vectors PART-OF family. error - free responses USED-FOR sparse vectors. support recovery CONJUNCTION approximate recovery problems. approximate recovery problems CONJUNCTION support recovery. approximate recovery problems USED-FOR problems. support recovery USED-FOR problems. 1 - bit compressed sensing USED-FOR approximate recovery problems. 1 - bit compressed sensing USED-FOR problems. learning algorithms USED-FOR problem. learning algorithms USED-FOR problem. Task is learning problems. OtherScientificTerm are noisy responses, and unknown vectors. Method is learning model. Metric is query complexity. ","This paper studies the problem of sparse support recovery, where the goal is to recover a sparse vector from a set of noisy responses to a query. In this setting, the input is a sparse set of vectors, and the query is to find the sparse vectors from the set of responses that are error-free. The authors show that the query complexity of this problem is bounded by the number of queries. They show that this problem can be formulated as a 1-bit compressed sensing problem, and they provide a learning algorithm for this problem.   ","This paper studies the problem of learning sparse vectors with noisy responses from sparse vectors, where the sparse vectors are part of a family of sparse vectors. The authors consider the case where the input is a sparse vector and the output is a set of noisy responses. They show that the query complexity of the sparse vector learning problem is bounded by the number of query queries. They then propose a new learning algorithm for this problem, which is based on 1-bit compressed sensing. "
13316,SP:e3388e479a825be429f3a878e2c4d8b05903ff10,"sensors USED-FOR detecting abrupt changes in temporal behavior patterns. detecting abrupt changes in temporal behavior patterns FEATURE-OF industrial and security applications. sensors USED-FOR industrial and security applications. information - theoretic lower bound USED-FOR finitely parameterized probability distributions. information - theoretic lower bound FEATURE-OF detection delay. bounds COMPARE information - theoretic lower bounds. information - theoretic lower bounds COMPARE bounds. expected delay bounds USED-FOR scheme. synthetic and real datasets EVALUATE-FOR method. OtherScientificTerm are abrupt changes in temporal behavior patterns, abrupt changes, sensing actions, and exploitation of querying informative actions. Task is bandit quickest changepoint detection problem. Method is online sensing scheme. Metric is false alarm rates. ",This paper studies the problem of detecting abrupt changes in temporal behavior patterns in the presence of a bandit. The authors propose a novel online sensing scheme to detect abrupt changes. The detection delay is estimated using an information-theoretic lower bound on the detection delay of a probability distribution over a set of discrete probability distributions. The proposed method is shown to outperform the state-of-the-art methods on both synthetic and real datasets.,This paper studies the problem of detecting abrupt changes in temporal behavior patterns in an online sensing scheme. The authors provide an information-theoretic lower bound for the detection delay of a bandit bandit under a finitely parameterized probability distribution. The lower bound is based on the fact that the bandit can exploit the information of querying informative actions in order to evade detection. The upper bound is also based on a lower bound on the expected delay of the detection.  The authors show that their lower bound can be used to estimate the false alarm rate of an online bandit detection scheme.
13365,SP:268260e9452ba2bc57e50a6b7b3328233137ac9b,"stochastic bilevel CONJUNCTION min - max. min - max CONJUNCTION stochastic bilevel. min - max CONJUNCTION compositional optimization. compositional optimization CONJUNCTION min - max. Stochastic nested optimization USED-FOR machine learning applications. compositional optimization HYPONYM-OF Stochastic nested optimization. stochastic bilevel HYPONYM-OF Stochastic nested optimization. min - max HYPONYM-OF Stochastic nested optimization. nested structure FEATURE-OF problems. SGD - type updates USED-FOR nested problems. they COMPARE non - nested problems. non - nested problems COMPARE they. convergence rate EVALUATE-FOR they. SGD - type updates USED-FOR stochastic nested problems. ALternating Stochastic gradient dEscenT ( ALSET ) method HYPONYM-OF SGD approach. ALSET USED-FOR stochastic nested problems. hidden smoothness FEATURE-OF problem. SGD - type algorithms USED-FOR stochastic nested problems. Method is problem - specific algorithms and analyses. Generic are analysis, and it. OtherScientificTerm are nested problem, and regularity conditions. Metric is sample complexity. ","This paper studies the problem of stochastic nested optimization, which is a variant of compositional optimization where the objective function is composed of a bilevel function and a min-max function. In this setting, the goal is to find the optimal solution to a set of optimization problems that satisfy certain regularity conditions. The main contribution of this paper is to study the convergence of SGD-based algorithms in this setting. In particular, the authors show that the ALternating Stochastic Gradient dEscenT (ALSET) method can be used to solve this problem with a sample complexity of $O(1/\sqrt{T})$ for any $T \in \mathbb{R}^d$ and $T$ with $T=1$. ","This paper studies the problem of stochastic nested optimization, where the objective is to solve a set of nested problems, and the objective function is a function of the structure of the nested structure. The main contribution of this paper is a theoretical analysis of the convergence rate of SGD-type updates for stochastically nested problems. The authors show that under certain regularity conditions, the convergence of the proposed algorithm is faster than the convergence for non-nested problems. They also show that the proposed method can be applied to non-stochastic nested problems as well. "
13414,SP:82ad52361bc5b2c421f1dc6b76e1a5520570fc6c,"supervised learning USED-FOR transformer - based model. siamese sampling mechanism USED-FOR sparse and similar clips. interdependent knowledge PART-OF network. reasoning strategy USED-FOR interdependent knowledge. interdependent knowledge PART-OF network inference. siamese clips HYPONYM-OF sparse and similar clips. Siamese Sampling and Reasoning ( SiaSamRea ) approach USED-FOR interdependent knowledge. reasoning strategy PART-OF Siamese Sampling and Reasoning ( SiaSamRea ) approach. reasoning strategy USED-FOR network inference. siamese sampling mechanism PART-OF Siamese Sampling and Reasoning ( SiaSamRea ) approach. siamese knowledge reasoning USED-FOR soft label. siamese knowledge reasoning HYPONYM-OF modules. siamese knowledge generation HYPONYM-OF modules. modules PART-OF reasoning strategy. siamese knowledge reasoning PART-OF reasoning strategy. siamese knowledge generation PART-OF reasoning strategy. SiaSamRea USED-FOR multimodal reasoning paradigm. ActivityNet - QA CONJUNCTION How2QA. How2QA CONJUNCTION ActivityNet - QA. How2QA CONJUNCTION TGIF - QA. TGIF - QA CONJUNCTION How2QA. MSVD - QA CONJUNCTION ActivityNet - QA. ActivityNet - QA CONJUNCTION MSVD - QA. MSRVTT - QA CONJUNCTION MSVD - QA. MSVD - QA CONJUNCTION MSRVTT - QA. VideoQA benchmarks EVALUATE-FOR SiaSamRea. Task is VideoQA ) task. OtherScientificTerm are inter - relationship, and soft labels. ",This paper proposes a method for multi-modal reasoning in a transformer-based model. The main idea is to use a siamese sampling mechanism to sample sparse and similar clips from the input data and use them as soft labels in the reasoning process. The method is evaluated on a variety of video QA tasks.   ,"This paper proposes a Siamese Sampling and Reasoning (SiaSamRea) approach for multi-modal reasoning in supervised learning. The main idea is to use the siamese sampling mechanism to sample sparse and similar clips from a video, and then use the reasoning strategy to generate a soft label for the soft labels. The method is evaluated on a variety of video-augmented QA tasks. "
13463,SP:160022e2cd61159da92f92e85520b7062a337a8d,"Structured distributions USED-FOR latent probabilistic representations. observed data USED-FOR latent probabilistic representations. computational and memory complexity EVALUATE-FOR latent representations. Hidden Markov Models ( HMMs ) CONJUNCTION Probabilistic Context - Free Grammars ( PCFGs ). Probabilistic Context - Free Grammars ( PCFGs ) CONJUNCTION Hidden Markov Models ( HMMs ). Probabilistic Context - Free Grammars ( PCFGs ) HYPONYM-OF models. Hidden Markov Models ( HMMs ) HYPONYM-OF models. computational and memory complexity EVALUATE-FOR structured models. approach USED-FOR structured models. computational and memory complexity EVALUATE-FOR approach. rank USED-FOR speed. matrix - vector product USED-FOR central inference step. polyphonic music modeling CONJUNCTION unsupervised grammar induction. unsupervised grammar induction CONJUNCTION polyphonic music modeling. language modeling CONJUNCTION polyphonic music modeling. polyphonic music modeling CONJUNCTION language modeling. neural parameterized structured models USED-FOR language modeling. unsupervised grammar induction CONJUNCTION video modeling. video modeling CONJUNCTION unsupervised grammar induction. neural parameterized structured models USED-FOR polyphonic music modeling. accuracy EVALUATE-FOR models. approach COMPARE models. models COMPARE approach. neural parameterized structured models EVALUATE-FOR approach. neural parameterized structured models USED-FOR unsupervised grammar induction. unsupervised grammar induction EVALUATE-FOR approach. large state spaces FEATURE-OF models. accuracy EVALUATE-FOR approach. OtherScientificTerm are combinatorial spaces, hidden states, and low - rank constraint. ",This paper proposes a method for learning latent probabilistic representations from structured data. The proposed method is based on a low-rank approximation of the matrix-vector product of the hidden states in the latent space. The authors show that the proposed method can reduce the computational and memory complexity of learning the latent representations of the observed data.   ,"This paper proposes a new method for learning latent probabilistic representations for structured models. The proposed method is based on a matrix-vector product (i.e., a low-rank constraint) that is used to estimate the latent representations of the model. The authors show that the proposed method outperforms the state-of-the-art in terms of both computational and memory complexity. They also show that their method can be used for unsupervised grammar induction and video modeling."
13512,SP:238592ad73927194cdf0c0cf9ae2e48ca86e182c,"exploration USED-FOR Reinforcement Learning. Thompson Sampling HYPONYM-OF Bayesian exploration strategies. technique USED-FOR complex environments. computational intractability FEATURE-OF probability distributions. deep neural network models CONJUNCTION approximate posterior methods. approximate posterior methods CONJUNCTION deep neural network models. approximation techniques USED-FOR exploration - exploitation trade - offs. Sample Average Uncertainty ( SAU ) HYPONYM-OF uncertainty measure. uncertainty measure USED-FOR contextual bandits. SAU HYPONYM-OF frequentist approach. Bayesian approaches USED-FOR outcomes uncertainty. Thompson Sampling HYPONYM-OF Bayesian approaches. SAU USED-FOR uncertainty measure. SAU USED-FOR deep contextual bandits. drop - in replacement USED-FOR epsilongreedy exploration. drop - in replacement USED-FOR deep contextual bandits. SAU - based exploration COMPARE deep Bayesian bandit methods. deep Bayesian bandit methods COMPARE SAU - based exploration. modest computation cost EVALUATE-FOR deep Bayesian bandit methods. real - world datasets EVALUATE-FOR deep Bayesian bandit methods. real - world datasets EVALUATE-FOR SAU - based exploration. Task is exploration - exploitation dilemma. OtherScientificTerm are action - value function, value predictions, and regret bounds. Method are outcome models, and outcome model. Metric is complexity. Material is deep bandit scenario. ","This paper studies the exploration-exploitation dilemma in Bayesian reinforcement learning. The authors propose a new uncertainty measure called Sample Average Uncertainty (SAU) for contextual bandits, which is a frequentist approach to measure the uncertainty in the outcome model. They show that this uncertainty measure is a drop-in replacement for epsilongreedy exploration in the context of deep contextual bandits. They also show that the proposed SAU-based exploration can be computed with modest computation cost.  ",This paper proposes Sample Average Uncertainty (SAU) as a new uncertainty measure for deep contextual bandits. The authors show that SAU can be used as a drop-in replacement for Thompson Sampling in the exploration-exploitation trade-off between exploration and exploitation. They also show that the proposed SAU-based exploration can be combined with other Bayesian bandit methods to reduce the computational complexity of exploration. 
13561,SP:ffc5b18f7e18607b2934e5aa199e7542005d79f4,"behavior CONJUNCTION neural activity. neural activity CONJUNCTION behavior. deep learning USED-FOR automated analysis of behavior. computer vision CONJUNCTION deep learning. deep learning CONJUNCTION computer vision. computer vision USED-FOR automated analysis of behavior. Disentangled Behavior Embedding ( DBE ) USED-FOR robust behavioral embeddings. DBE CONJUNCTION stochastic temporal model. stochastic temporal model CONJUNCTION DBE. end - to - end approach USED-FOR discrete behavior representations. models USED-FOR consistent behavior representations. dynamic behavioral factors ( pose ) PART-OF deep autoencoder. temporal structures of pose dynamics USED-FOR models. fine - grained behavioral motif generation CONJUNCTION behavior decoding. behavior decoding CONJUNCTION fine - grained behavioral motif generation. approaches COMPARE DBE. DBE COMPARE approaches. approaches COMPARE VDBE. VDBE COMPARE approaches. DBE CONJUNCTION VDBE. VDBE CONJUNCTION DBE. DBE USED-FOR tasks. tasks EVALUATE-FOR VDBE. tasks EVALUATE-FOR approaches. behavior decoding HYPONYM-OF tasks. fine - grained behavioral motif generation HYPONYM-OF tasks. Material are neuroscience, large and high - quality video datasets, and interpretable behavioral videos. Task is motor task. ","This paper proposes a method for learning behavior embeddings from video data. The method is based on disentangling the dynamics of a video into a set of discrete components, which can then be used to learn representations of the observed dynamics. The proposed method is evaluated on two tasks: fine-grained behavioral motif generation and behavior decoding.   ","This paper proposes a method for generating robust behavioral embeddings from video videos. The method is based on the Disentangled Behavior Embedding (DBE) framework, which is an end-to-end approach for generating discrete behavior representations. The authors propose a stochastic temporal model and a deep autoencoder to encode dynamic behavioral factors (positions) into the video. The proposed method is evaluated on a variety of tasks, including fine-grained behavioral motif generation, behavior decoding, and fine- grained behavior motif generation. "
13610,SP:bf78a450e4aad6b87fdeb8ec0d68adaaff7b595b,"DMTET HYPONYM-OF deep 3D conditional generative model. user guides USED-FOR DMTET. coarse voxels HYPONYM-OF user guides. hybrid 3D representation USED-FOR implicit and explicit 3D representations. DMTET USED-FOR reconstructed surface. implicit approaches COMPARE DMTET. DMTET COMPARE implicit approaches. deep 3D generative models USED-FOR explicit representations. deep 3D generative models COMPARE model. model COMPARE deep 3D generative models. meshes HYPONYM-OF explicit representations. deformable tetrahedral grid USED-FOR discretized signed distance function. implicit signed distance representation CONJUNCTION explicit surface mesh representation. explicit surface mesh representation CONJUNCTION implicit signed distance representation. discretized signed distance function CONJUNCTION differentiable marching tetrahedra layer. differentiable marching tetrahedra layer CONJUNCTION discretized signed distance function. differentiable marching tetrahedra layer USED-FOR implicit signed distance representation. deformable tetrahedral grid CONJUNCTION differentiable marching tetrahedra layer. differentiable marching tetrahedra layer CONJUNCTION deformable tetrahedral grid. deformable tetrahedral grid PART-OF DMTET. differentiable marching tetrahedra layer PART-OF DMTET. reconstruction CONJUNCTION adversarial losses. adversarial losses CONJUNCTION reconstruction. surface mesh USED-FOR adversarial losses. adversarial losses USED-FOR generation of the hierarchy of subdivisions. reconstruction USED-FOR generation of the hierarchy of subdivisions. approach USED-FOR conditional shape synthesis. coarse voxel inputs USED-FOR conditional shape synthesis. OtherScientificTerm are signed distance values, finer geometric details, arbitrary topology, and hierarchy of subdivisions. Material is complex 3D animal shapes. ","This paper proposes a deep 3D conditional generative model that combines implicit and explicit 3D representations. The implicit representation is composed of a discretized signed distance function, a differentiable marching tetrahedra layer, and a deformable tetrahedral grid. The explicit surface mesh representation is a mixture of explicit and implicit surface mesh representations.  The proposed method achieves state-of-the-art results on a set of synthetic and real-world datasets. ","This paper proposes a deep 3D conditional generative model (DMTET) that combines implicit and explicit 3D representations. The implicit representation is a discretized signed distance function, and the explicit representation is an explicit surface mesh representation. The explicit representation consists of a differentiable marching tetrahedra layer and a deformable tetrahedral grid, which is used to reconstruct the surface of the generated surface. The surface mesh is then used to generate the hierarchy of subdivisions, which can be used for conditional shape synthesis. "
13659,SP:2bc0bd6aa2a12691b16145f0d23542c4c86e3a44,"information theory CONJUNCTION statistics. statistics CONJUNCTION information theory. statistics CONJUNCTION machine learning. machine learning CONJUNCTION statistics. statistical dependence FEATURE-OF Mutual information ( MI ). structural properties FEATURE-OF it. sliced MI ( SMI ) USED-FOR surrogate measure of dependence. it USED-FOR structural properties. scalable computation CONJUNCTION estimation. estimation CONJUNCTION scalable computation. structural properties FEATURE-OF MI. estimation EVALUATE-FOR it. scalable computation EVALUATE-FOR it. MI COMPARE SMI. SMI COMPARE MI. deterministic transformations USED-FOR SMI. SMI USED-FOR feature extraction. processing functions of raw data USED-FOR it. independence testing CONJUNCTION feature extraction. feature extraction CONJUNCTION independence testing. MI USED-FOR high - dimensional inference. SMI COMPARE MI. MI COMPARE SMI. SMI USED-FOR high - dimensional inference. independence testing USED-FOR theory. feature extraction USED-FOR theory. Task is estimation of highdimensional MI. OtherScientificTerm are statistical scalability, and one - dimensional random projections. ","This paper proposes sliced mutual information (SMI) as a surrogate measure of statistical dependence between two data points. The authors show that sliced MI can be used to estimate high-dimensional mutual information. They show that SMI can be computed with deterministic transformations, and that it is more scalable than the original Mutual Information (MI) which is based on one-dimensional random projections. They also show that the sliced MI is able to be used for feature extraction from raw data.  ","This paper proposes sliced mutual information (SMI) as a surrogate measure of statistical dependence between two data points. SMI can be used to measure the structural properties of mutual information, and it is shown that sliced MI is more scalable than sliced MI. The paper also shows that SMI is able to be used for high-dimensional estimation of MI, and that it can be applied to feature extraction and independence testing."
13708,SP:e220b348901b476c2afd95f97630fb5400582f40,"query efficiency EVALUATE-FOR myopic methods. non - myopic Bayesian optimization COMPARE myopic methods. myopic methods COMPARE non - myopic Bayesian optimization. expected improvement HYPONYM-OF myopic methods. query efficiency EVALUATE-FOR non - myopic Bayesian optimization. unreliable bruteforce derivative - free optimization USED-FOR Monte Carlo rollout acquisition function. unreliable bruteforce derivative - free optimization USED-FOR multi - step lookahead constrained BO method. sample average approximation CONJUNCTION infinitesimal perturbation analysis. infinitesimal perturbation analysis CONJUNCTION sample average approximation. reparameterization trick USED-FOR Methods. likelihoodratio - based unbiased estimator USED-FOR acquisition function optimization. 2 - OPT - C COMPARE methods. methods COMPARE 2 - OPT - C. query efficiency EVALUATE-FOR methods. query efficiency EVALUATE-FOR 2 - OPT - C. Metric is computational cost. Method is unconstrained BO methods. Material is unconstrained setting. OtherScientificTerm are constraints, sampled acquisition function surface, feasible and infeasible regions, and tight constraints. Task are constrained problems, and sequential and batch settings. ","This paper studies Bayesian optimization in the constrained setting, where there are constraints on the sampled acquisition function surface and the feasible and infeasible regions. The authors propose a multi-step lookahead constrained BO method with unreliable derivative-free optimization to solve the Monte Carlo rollout acquisition function. They also propose a likelihood-ratio-based unbiased estimator to estimate the acquisition function optimization. The proposed method is shown to be computationally efficient. ",This paper proposes a new method for non-myopic Bayesian Bayesian optimization with constrained constraints. The authors propose a multi-step lookahead constrained constrained BO method with a Monte Carlo rollout acquisition function and a likelihoodratio-based unbiased estimator for acquisition function optimization. They also propose a reparameterization trick to improve the query efficiency of the proposed method. The proposed method is evaluated on both sequential and batch settings.
13757,SP:51fbd861422647912f275b48861ea3c4812afdc8,scalar value functions PART-OF value network. distributional RL USED-FOR return distribution. return distribution COMPARE scalar value. scalar value COMPARE return distribution. hybrid reward architectures ( HRA ) USED-FOR source - specific value functions. hybrid reward architectures ( HRA ) USED-FOR RL. source - specific value functions USED-FOR reward. distributional RL CONJUNCTION hybrid reward architectures. hybrid reward architectures CONJUNCTION distributional RL. Multi - Dimensional Distributional DQN ( MD3QN ) USED-FOR joint return distribution. distributional RL USED-FOR joint return distribution. distributional RL USED-FOR Multi - Dimensional Distributional DQN ( MD3QN ). MD3QN USED-FOR randomness in returns. MD3QN USED-FOR rich reward correlation. joint return distribution CONJUNCTION Bellman target. Bellman target CONJUNCTION joint return distribution. Maximum Mean Discrepancy FEATURE-OF joint return distribution. Maximum Mean Discrepancy USED-FOR empirical algorithm. method USED-FOR joint return distribution. method COMPARE RL methods. RL methods COMPARE method. multi - dimensional reward functions USED-FOR control setting. richly correlated reward functions FEATURE-OF joint return distribution. control setting EVALUATE-FOR RL methods. multi - dimensional reward functions USED-FOR method. multi - dimensional reward functions USED-FOR RL methods. Method is joint distribution modeling. OtherScientificTerm is joint distributional Bellman operator. ,"This paper proposes a new distributional reinforcement learning algorithm for multi-dimensional reward functions. The main idea is to use distributional RL to model the joint return distribution and Bellman operator of the reward function. The proposed method, called Multi-Dimensional Distributional DQN (MD3QN), is based on the distributional approximation of the distribution of the Bellman operators. The authors show that the proposed method is able to learn the joint distribution of reward and distribution of distributional Bellman. The empirical results show the effectiveness of the proposed approach.",This paper proposes a new multi-dimensional distributional distributional DQN (MD3QN) to model the joint return distribution between the source-specific value function and the joint distributional Bellman operator. The main idea is to use the distributional RL to model a joint distribution of the return distribution and the Bellman target. The authors show that the proposed method can be used in a control setting where the reward function is multi-divergence between the target and the source function. They also provide an empirical analysis of the empirical performance of the method.
13806,SP:1f85c93d6bbfd65bf497c92c9cd534d799753097,"CorticalFlow HYPONYM-OF geometric deep - learning model. diffeomorphic transformations USED-FOR model. numeric conditions USED-FOR manifoldness. discrete resolution USED-FOR topological errors. numeric conditions USED-FOR topological errors. CorticalFlow USED-FOR brain cortical surface reconstruction. its USED-FOR brain cortical surface reconstruction. its EVALUATE-FOR CorticalFlow. computation time EVALUATE-FOR CorticalFlow. CorticalFlow USED-FOR generation of anatomically plausible surfaces. Material is 3 - dimensional image. OtherScientificTerm are template mesh ’s topological properties, and GPU memory footprint. Method are flow Ordinary Differential Equation ( ODE ) framework, and surface reconstruction methods. Task is generation of surfaces. ","This paper proposes a geometric deep learning model, called CorticalFlow, for 3D surface reconstruction in the brain. The main idea is to use a flow-based model to model the topological properties of a 3D template mesh. Theoretical analysis is performed to show that the model is able to recover the topology of the template mesh, which is then used to train a neural network to reconstruct the surface. Experiments are conducted on a brain surface reconstruction task.   ","This paper proposes a new geometric deep learning model, called CorticalFlow, for the task of brain cortical surface reconstruction. The model is based on the flow Ordinary Differential Equation (ODE) framework, which is used to model the topological properties of the template mesh. Theoretical analysis is provided to show that the model is able to generate anatomically plausible surfaces, and that it can be used for brain surface reconstruction in the presence of topological errors. Experiments are conducted to demonstrate the effectiveness of the model."
13855,SP:2f31d9cf4ad17ad08344439ca0aef7ec91944545,"computational cost EVALUATE-FOR models. differential privacy CONJUNCTION max information. max information CONJUNCTION differential privacy. deletion guarantees CONJUNCTION deletion guarantees. deletion guarantees CONJUNCTION deletion guarantees. deletion guarantees FEATURE-OF non - adaptive sequences. deletion guarantees FEATURE-OF adaptive sequences. provable deletion guarantees FEATURE-OF adaptive deletion sequences. attack USED-FOR SISA algorithm. non - convex models USED-FOR adaptive deletion sequences. MNIST CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION MNIST. CIFAR-10 CONJUNCTION MNIST. MNIST CONJUNCTION CIFAR-10. Method is Data deletion algorithms. Task is non - convex setting. OtherScientificTerm are update sequence, non - adaptive deletion sequences, and training methodologies. ","This paper studies the problem of data deletion in the non-convex setting, where the update sequence is updated in a non-adaptive manner. The authors show that the SISA algorithm can be attacked in this setting, and provide a deletion algorithm with provable deletion guarantees. They also show that adaptive deletion sequences can be used to attack the SISIA algorithm. ","This paper studies the problem of data deletion in the non-convex setting where the update sequence is non-adaptive and the deletion sequence is adaptive. The authors show that the SISA algorithm can be attacked in this setting. They also show that adaptive deletion sequences are provably provable deletion guarantees and that adaptive sequences can be provably non-probabilistic. Finally, the authors provide a theoretical analysis of the effect of the attack on the performance of SISA."
13904,SP:7150006590e268ab732c9be6c9048f67a377f956,epistemic uncertainty CONJUNCTION aleatoric uncertainty. aleatoric uncertainty CONJUNCTION epistemic uncertainty. prior distribution FEATURE-OF MDPs. policy optimising CVaR USED-FOR setting. aleatoric uncertainty CONJUNCTION inherent stochasticity of MDPs. inherent stochasticity of MDPs CONJUNCTION aleatoric uncertainty. prior distribution FEATURE-OF epistemic uncertainty. Monte Carlo tree search CONJUNCTION Bayesian optimisation. Bayesian optimisation CONJUNCTION Monte Carlo tree search. two - player stochastic game USED-FOR problem. Monte Carlo tree search USED-FOR approximate algorithm. Bayesian optimisation USED-FOR approximate algorithm. approach COMPARE baseline approaches. baseline approaches COMPARE approach. baseline approaches USED-FOR problem. approach USED-FOR problem. Task is risk - averse Bayes - adaptive reinforcement learning. OtherScientificTerm is conditional value at risk ( CVaR ). ,"This paper studies risk-averse Bayes-adaptive reinforcement learning in the setting of conditional value at risk (CVaR), where the goal is to learn a policy that maximizes the CVaR in a two-player stochastic game. The main contribution of the paper is a Bayesian optimisation algorithm based on Monte Carlo tree search and Bayesian Optimization with Monte Carlo Tree Search (MCTS). The authors show that MCTS can be used to estimate the epistemic uncertainty and aleatoric uncertainty of the MDPs. The authors also show that the prior distribution of the prior distributions can be approximated by Bayes optimisation. ","This paper studies risk-averse Bayes-adaptive reinforcement learning (RARL) in a two-player stochastic game setting, where each player has a conditional value at risk (CVaR), and the goal is to find a policy that maximizes the CVaR of the other player. The authors propose a Bayesian algorithm for this problem, which is based on Monte Carlo tree search and Bayesian optimisation. They show that the proposed algorithm can outperform the state-of-the-art baselines in terms of performance. "
13953,SP:a94f39406f73d7483ddd744ed2f03c78b8bc5d44,"binary classification data USED-FOR shallow ReLU networks. logistic loss USED-FOR shallow ReLU networks. gradient descent USED-FOR logistic loss. gradient descent USED-FOR shallow ReLU networks. sigmoid mapping USED-FOR conditional distribution. gradient descent USED-FOR population risk. early stopping USED-FOR gradient descent. complexity measure EVALUATE-FOR conditional model. local interpolation property FEATURE-OF univariate classifier. Deep networks USED-FOR arbitrary prediction problems. gradient descent USED-FOR Deep networks. constant step size FEATURE-OF vanilla gradient descent. vanilla gradient descent USED-FOR inner ( inputfacing ) weights. shallow ReLU networks HYPONYM-OF networks. induced conditional model COMPARE model. model COMPARE induced conditional model. optimality FEATURE-OF population misclassification rate. sigmoid mapping USED-FOR induced conditional model. gradient descent USED-FOR restricted conditional models. network nodes CONJUNCTION gradient descent iterations. gradient descent iterations CONJUNCTION network nodes. optimal test error FEATURE-OF noisy distributions. optimal test error EVALUATE-FOR univariate predictor. local interpolation property FEATURE-OF univariate predictor. multiplicative error property FEATURE-OF logistic loss. technique USED-FOR large network width. empirical logistic risk CONJUNCTION logistic risk. logistic risk CONJUNCTION empirical logistic risk. logistic loss CONJUNCTION empirical logistic risk. empirical logistic risk CONJUNCTION logistic loss. compactly - supported marginal FEATURE-OF Borel measure. gradient descent USED-FOR optimal test error. it USED-FOR learning task. universal approximation properties FEATURE-OF neural networks. Bayes ( convex ) risk USED-FOR conditional model. agnostic learning setting HYPONYM-OF predictors. shallow ReLU networks USED-FOR predictors. gradient descent USED-FOR shallow ReLU networks. OtherScientificTerm are data distribution, joint distribution, training time, measurable functions, training risk, data simplicity, distribution, finite sample, sphere, function f, R, and computational and statistical obstructions. Metric are Bayes risk, logistic and misclassification losses, calibration, Bayes risk R, and misclassification loss. Method are stalwart methods, infinite - width random feature model, classification calibration, and universal",This paper studies the Bayes risk of deep ReLU networks for binary classification problems. The authors show that gradient descent can be used to estimate the risk of a classifier in a restricted conditional model with a sigmoid mapping to the conditional distribution. They show that this method is equivalent to using gradient descent with constant step size in the inner (input-facing) weights and early stopping in the outer (output) weights. They also show that the logistic and misclassification losses can be approximated by gradient descent.   ,This paper studies the problem of estimating the population misclassification rate of deep ReLU networks. The authors show that the optimal test error of a shallow ReLU network is bounded by the logistic and logistic losses. They show that this is a function of the size of the data distribution and the number of samples. They also show that a sigmoid mapping of the conditional distribution can be used to estimate the test error. The paper also shows that this method can be applied to the agnostic learning setting. 
14002,SP:a9c786cbb61e1f10f3542161b13e43a1a68ab34d,"misinformation campaigns USED-FOR social outcomes. misinformation campaigns USED-FOR coordinated accounts. social media FEATURE-OF coordinated accounts. coordinated group detection USED-FOR misinformation. methodology USED-FOR misinformation. methodology USED-FOR coordinated group detection. social media USED-FOR misinformation. social media FEATURE-OF sparsity of account activities. limited expressive power EVALUATE-FOR detectors. prior knowledge USED-FOR detectors. temporal logic CONJUNCTION pre - defined filtering functions. pre - defined filtering functions CONJUNCTION temporal logic. prior knowledge FEATURE-OF neural temporal point process. neural temporal point process PART-OF coordination detection framework. pre - defined filtering functions HYPONYM-OF prior knowledge. temporal logic HYPONYM-OF prior knowledge. account embedding space CONJUNCTION prior knowledge. prior knowledge CONJUNCTION account embedding space. theoretically guaranteed variational inference approach USED-FOR mean - field approximation. mean - field approximation USED-FOR it. theoretically guaranteed variational inference approach USED-FOR it. real - world dataset EVALUATE-FOR method. method COMPARE model. model COMPARE method. real - world dataset EVALUATE-FOR model. unsupervised and semi - supervised settings EVALUATE-FOR model. unsupervised and semi - supervised settings EVALUATE-FOR method. COVID-19 Vaccine Tweets dataset EVALUATE-FOR model. COVID-19 vaccines FEATURE-OF spreading misinformation. Method is deep learning based coordination detectors. Generic are they, and distribution. OtherScientificTerm is Gibbs distribution of group assignment. Task is detection. ",This paper proposes a method for coordinated group detection of misinformation in social media. The proposed method is based on a neural temporal point process (NTPGP) that uses temporal logic and pre-defined filtering functions to detect coordinated accounts. The method is evaluated on the COVID-19 Vaccine Tweets dataset and achieves state-of-the-art performance.,"This paper proposes a method for coordinated group detection of misinformation on social media. The proposed method is based on a neural temporal point process (NTP) framework, which is a combination of temporal logic and pre-defined filtering functions. The authors show that the proposed method outperforms state-of-the-art methods in both unsupervised and semi-supervised settings. "
14051,SP:b5c6e967a26a02861db2ecd620e9061db0c03e59,"binary classification task HYPONYM-OF model problem. unit sphere FEATURE-OF smooth curves. structure USED-FOR model problem. deep fully - connected neural network USED-FOR binary classification task. network depth COMPARE geometric properties. geometric properties COMPARE network depth. generalization guarantee EVALUATE-FOR deep networks. nonlinear data USED-FOR deep networks. fitting resource USED-FOR classification problem. network depth HYPONYM-OF fitting resource. neural tangent kernel ( NTK ) regime FEATURE-OF reduction to dynamics. convergence CONJUNCTION generalization. generalization CONJUNCTION convergence. decay properties FEATURE-OF NTK. fine - grained control USED-FOR decay properties. fine - grained control USED-FOR NTK. manifolds FEATURE-OF translationally invariant operator. smooth functions USED-FOR NTK. translationally invariant operator USED-FOR NTK. OtherScientificTerm are low - dimensional nonlinear structure, mild regularity conditions, network width, intrinsic data properties, and network. Task is engineering and scientific problems. Method is randomly - initialized gradient descent. ","This paper studies the neural tangent kernel (NTK) for binary classification problems with low-dimensional nonlinear structure. The authors show that under mild regularity conditions, the NTK converges to a unit sphere on the unit sphere, which is equivalent to a smooth function on a smooth unit sphere. The NTK is then shown to be translationally invariant, which means that it can be used to define a translationally-invariant operator for NTKs on smooth manifolds. Theoretical results on the generalization properties of NTK are also provided. ","This paper studies the generalization properties of neural tangent kernels (NTKs) in the low-dimensional nonlinear setting. The authors show that NTKs can be used as a generalization guarantee for deep neural networks for binary classification problems. They show that under mild regularity conditions, NTK can generalize well to low dimensional nonlinear data. They also provide a fine-grained control over the decay properties of NTK. "
14100,SP:8f6bee3be43df6b6e80804974014caaafe08c49e,"class information PART-OF GAN. auxiliary classifier GAN HYPONYM-OF cGANs. softmax cross - entropy loss ( ACGAN ) FEATURE-OF auxiliary classifier GAN. relational information FEATURE-OF class - labeled dataset. Tiny - ImageNet CONJUNCTION CUB200. CUB200 CONJUNCTION Tiny - ImageNet. CIFAR10 CONJUNCTION Tiny - ImageNet. Tiny - ImageNet CONJUNCTION CIFAR10. CUB200 CONJUNCTION ImageNet datasets. ImageNet datasets CONJUNCTION CUB200. Tiny - ImageNet EVALUATE-FOR ReACGAN. CUB200 EVALUATE-FOR ReACGAN. CIFAR10 EVALUATE-FOR ReACGAN. ImageNet datasets EVALUATE-FOR ReACGAN. D2D - CE CONJUNCTION StyleGAN2 architecture. StyleGAN2 architecture CONJUNCTION D2D - CE. differentiable augmentations USED-FOR ReACGAN. software package USED-FOR representative cGANs. Model weights CONJUNCTION software package. software package CONJUNCTION Model weights. Method are Conditional Generative Adversarial Networks ( cGAN ), ACGAN, and classifier. OtherScientificTerm are diversity, and unit hypersphere. ","This paper proposes ReACGAN, an extension of Conditional Generative Adversarial Networks (cGANs) with a softmax cross-entropy loss (ACGAN) to the auxiliary classifier GAN. The proposed method is based on the relational information of the class-labeled dataset and uses a differentiable augmentations to improve the diversity of the unit hypersphere. Experiments on CIFAR-10, Tiny-ImageNet, and CUB200 show that the proposed method achieves state-of-the-art performance.","This paper proposes ReACGAN, an auxiliary classifier GAN for Conditional Generative Adversarial Networks (cGANs). The main idea of the paper is to use a softmax cross-entropy loss (ACGAN) to improve the diversity of the auxiliary classifiers in cGANs. The authors show that ACGAN can be used to improve diversity in the classifier and model weights. They also propose differentiable augmentations for differentiable augmentation that can be applied to differentiable classifiers. The experiments show that the proposed method outperforms the state-of-the-art in terms of diversity."
14149,SP:080e80746a87228b156408ff649ab7a17f44e92d,Policy Space Response Oracles ( PSRO ) HYPONYM-OF reinforcement learning ( RL ) algorithm. reinforcement learning ( RL ) algorithm USED-FOR two - player zero - sum games. large games FEATURE-OF approximate Nash equilibria. PSRO USED-FOR continuous actions. PSRO USED-FOR approximate Nash equilibrium. Extensive - Form Double Oracle ( XDO ) HYPONYM-OF extensive - form double oracle algorithm. extensive - form double oracle algorithm USED-FOR two - player zero - sum games. PSRO COMPARE XDO. XDO COMPARE PSRO. best responses USED-FOR XDO. deep RL USED-FOR Neural XDO ( NXDO ). deep RL USED-FOR best response. XDO COMPARE PSRO. PSRO COMPARE XDO. XDO USED-FOR approximate Nash equilibrium. XDO COMPARE CFR. CFR COMPARE XDO. Leduc poker game CONJUNCTION Oshi - Zumo. Oshi - Zumo CONJUNCTION Leduc poker game. exploitability EVALUATE-FOR CFR. exploitability EVALUATE-FOR XDO. NXDO COMPARE PSRO. PSRO COMPARE NXDO. NXDO COMPARE NFSP. NFSP COMPARE NXDO. PSRO CONJUNCTION NFSP. NFSP CONJUNCTION PSRO. sequential multidimensional continuous - action game EVALUATE-FOR NFSP. sequential multidimensional continuous - action game EVALUATE-FOR PSRO. sequential multidimensional continuous - action game EVALUATE-FOR NXDO. NXDO HYPONYM-OF deep RL method. deep RL method USED-FOR approximate Nash equilibrium. NXDO USED-FOR approximate Nash equilibrium. high - dimensional continuous - action sequential games FEATURE-OF approximate Nash equilibrium. OtherScientificTerm is infostates. Material is Leduc poker. ,"This paper proposes an extensive-form double oracle algorithm for two-player zero-sum games. The proposed method is based on the policy space response oracles (PSRO) algorithm, which is a reinforcement learning algorithm for finding approximate Nash equilibria in large games. In this paper, the authors show that PSRO can be used to find the approximate Nash equilibrium in continuous action sequential games. They also show that the Neural XDO (NXDO) is a deep RL method that uses deep RL to learn the best response to the best action in a continuous action game.  ","This paper proposes an extensive-form double oracle algorithm (XDO) for two-player zero-sum games. The main idea of XDO is to use the policy space response oracles (PSRO) to learn the Nash equilibrium between the two players. The authors show that PSRO can be used to learn Nash equilibria for large games with high-dimensional games. They also propose a deep RL method (NXDO), which uses deep RL to learn an approximate Nash equilibrium for the best response. They show that XDO outperforms PSRO in terms of exploitability.   "
14198,SP:bda04facef4f34679fc4e17b8ea1aae74c3d649f,"graph structured data USED-FOR deep neural networks. node - level unsupervised learning HYPONYM-OF nodeor graph - level supervised learning. node clustering HYPONYM-OF node - level unsupervised learning. representation complexity EVALUATE-FOR graphs. adjacency matrices USED-FOR graphs. permutation - invariant variational autoencoder USED-FOR graph structured data. model USED-FOR node order. model USED-FOR graph reconstruction. extracted representations USED-FOR downstream graph - level classification and regression. Method are graph - level unsupervised representation learning, and graph matching. ",This paper proposes a graph-level unsupervised representation learning method for graph structured data. The proposed method is based on a permutation-invariant variational autoencoder (VAE) that learns the adjacency matrices between nodes in a graph. The authors show that the proposed method can learn node-level representations that can be used for downstream graph classification and regression tasks. ,This paper proposes a graph-level unsupervised representation learning method for graph structured data. The proposed method is based on a permutation-invariant variational autoencoder (VAE) model that learns the adjacency matrices of the graph structure. The authors show that the proposed method outperforms the state-of-the-art in terms of graph reconstruction and graph matching. 
14247,SP:e17ea6aeba78c9dfc25596d8b35a2a4f1f1f6763,"limited scalability EVALUATE-FOR Graph Neural Networks ( GNNs ). subgraph USED-FOR GNN. bounded - size scope FEATURE-OF localized subgraph. critical neighbors PART-OF subgraph. GNN USED-FOR informative representation. GNN USED-FOR local neighborhood. function approximation ( GraphSAGE ) CONJUNCTION topological learning ( GIN ). topological learning ( GIN ) CONJUNCTION function approximation ( GraphSAGE ). graph signal processing ( GCN ) CONJUNCTION function approximation ( GraphSAGE ). function approximation ( GraphSAGE ) CONJUNCTION graph signal processing ( GCN ). decoupling USED-FOR GNN expressive power. graphs CONJUNCTION backbone GNN architectures. backbone GNN architectures CONJUNCTION graphs. backbone GNN architectures EVALUATE-FOR design. graphs EVALUATE-FOR design. OtherScientificTerm are graph and model sizes, model depth, receptive field, degraded expressivity, oversmoothing, neighborhood explosion, node, and global graph. Task is expensive computation. Method is GNNs. ","This paper proposes a novel method to improve the expressivity of GNNs. The proposed method is based on the observation that when the number of nodes in a subgraph is too large, the expressiveness of a GNN can degrade due to oversmoothing and neighborhood explosion. To address this issue, the authors propose to use the local neighborhood of each node in the subgraph to learn the critical neighbors of the local node. The critical neighbors are the nodes that are close to the critical nodes in the local neighbourhood. The authors show that this method can be used to improve expressivity and reduce the computational cost. The experiments show that the proposed method outperforms the baselines in terms of expressiveness and computation cost.","This paper proposes a novel approach to improve the expressive power of graph neural networks (GNNs) by decoupling local subgraphs from the global graph. The proposed approach is based on topological learning (GIN), graph signal processing (GCN), and function approximation (GraphSAGE). The authors show that the proposed approach can improve the expressivity of GNNs in the face of oversmoothing and neighborhood explosion. They also show that their approach can be applied to backbone GNN architectures."
14296,SP:4890f251db559a0a572afc66e0c1f899b577d9ff,"Normalizing flows HYPONYM-OF latent - variable generative models. tractable likelihood FEATURE-OF latent - variable generative models. Jacobian FEATURE-OF latent - to - observable - variable transformation. linear time FEATURE-OF likelihood. nearly - singular Jacobian FEATURE-OF networks. affine couplings USED-FOR regular distributions. well - conditioned affine - coupling flows USED-FOR log - concave distribution. underdamped Langevin dynamics CONJUNCTION Hénon maps. Hénon maps CONJUNCTION underdamped Langevin dynamics. Hénon maps HYPONYM-OF structured dynamical system. underdamped Langevin dynamics HYPONYM-OF stochastic differential equation. affine coupling architectures CONJUNCTION underdamped Langevin dynamics. underdamped Langevin dynamics CONJUNCTION affine coupling architectures. symplectic diffeomorphisms FEATURE-OF structured dynamical system. iid Gaussians USED-FOR padded version of the input distribution. Gaussian padding USED-FOR normalizing flows. Method is Affine - coupling models. Generic is architecture. OtherScientificTerm are representational power, ill - conditioned Jacobians, well - conditioned affine coupling flows, and Gibbs measures. Task are universal approximation, likelihood - based training, and training affine couplings. ",This paper proposes to use affine coupling to train normalizing flows for log-concave distributions. The authors show that affine-coupling is a well-conditioned way to train a logistic regression model with tractable likelihoods. They show that this is equivalent to using a normalizing flow with Gaussian padding. They also show that the affine couplings can be used to train underdamped Langevin dynamics.  ,"This paper proposes a new normalizing flow architecture for generative models. The authors show that the well-conditioned affine-coupling flows can be used to approximate the log-concave distribution of the input distribution. They also show that underdamped Langevin dynamics, Hénon maps, and symplectic diffeomorphisms can be approximated by the affine coupling flows. Finally, they show that Gaussian padding can be added to the normalizing flows to make them more tractable."
14345,SP:5ffa81488ed1092deb89bd5e150fa146325057ce,"Lagrangian problem USED-FOR coupons allocation. method USED-FOR coupons allocation policy. λ - generalization method USED-FOR policy learning process. λ USED-FOR policy learning process. offline reinforcement learning method CONJUNCTION off - policy evaluation algorithm. off - policy evaluation algorithm CONJUNCTION offline reinforcement learning method. policy learning CONJUNCTION policy evaluation. policy evaluation CONJUNCTION policy learning. offline reinforcement learning method USED-FOR policy learning. off - policy evaluation algorithm USED-FOR policy learning. off - policy evaluation algorithm USED-FOR policy evaluation. offline reinforcement learning method USED-FOR policy evaluation. simulation platform CONJUNCTION real - world e - commerce market. real - world e - commerce market CONJUNCTION simulation platform. real - world e - commerce market EVALUATE-FOR approach. simulation platform EVALUATE-FOR approach. Task are Coupons allocation, and online e - commerce environment. Material are e - commerce market, and e - commerce platform. OtherScientificTerm are coupons, Lagrangian multiplier variable λ, and policy space. Method are coupons allocation policy learning, and λ - generalization ( BCORLE(λ ) ) framework. Metric are computation overhead, and users ’ retention rate. Generic are policy, and problem. ","This paper studies the problem of coupons allocation in an online e-commerce market, where the goal is to allocate a set of coupons to each user. The authors propose to use the Lagrangian multiplier variable $L_\theta$ in the coupons allocation problem to learn a policy that maximizes the generalization of the policy in the policy space. The main contribution of the paper is to propose a generalization method based on the $L_{\phi}$-generalization (BCORLE) framework, which is based on off-policy evaluation and offline reinforcement learning. The proposed method is evaluated on a simulated and a real-world e- commerce market. ",This paper studies the problem of coupons allocation in an online e-commerce market. The authors propose a new generalization framework based on the Lagrangian multiplier variable (Lagrangian) for the coupons allocation problem. The main idea of the paper is to use an off-policy evaluation algorithm and offline reinforcement learning method to learn a policy in the policy space. The proposed method is evaluated on a simulation platform and a real-world e- commerce market.
14394,SP:6b04cc7b4e45b9e65a1d34c15e3f75a2ef27d601,"local affinity FEATURE-OF label consistency. local affinity USED-FOR intrinsic structure. self regularization loss USED-FOR noisy neighbors. inherent structure USED-FOR domain adaptation. local neighbors CONJUNCTION reciprocal neighbors. reciprocal neighbors CONJUNCTION local neighbors. reciprocal neighbors CONJUNCTION expanded neighborhood. expanded neighborhood CONJUNCTION reciprocal neighbors. reciprocal neighbors USED-FOR local structure. local neighbors USED-FOR local structure. Task is Domain adaptation ( DA ). Method are DA methods, source pretrained model, and source domain classifier. Generic is method. OtherScientificTerm are affinity, and expanded neighborhoods. Material is 2D image and 3D point cloud recognition datasets. ","This paper proposes a method for domain adaptation (DA) in the presence of noisy neighbors in the source domain. The proposed method is based on the notion of local affinity, which is a measure of label consistency between the source and target domains. The authors show that the self-regularization loss can be used to reduce the distance between the target and source domains. They also propose to use the local affinity as a regularization term to improve the performance of domain adaptation.  ","This paper proposes a new method for domain adaptation (DA) for point cloud recognition. The proposed method is based on the notion of ""local affinity"", which is defined as the affinity between the source domain classifier and its neighbors. The authors show that this affinity can be used to improve the performance of DA methods. They also show that the proposed method can be applied to both 2D image and 3D point cloud datasets. "
14443,SP:ac1bf04ff782e5892a0bc5fe5949848ca8e731c2,graph learning CONJUNCTION image / video recognition. image / video recognition CONJUNCTION graph learning. image / video recognition CONJUNCTION object detection. object detection CONJUNCTION image / video recognition. point cloud processing CONJUNCTION graph learning. graph learning CONJUNCTION point cloud processing. Learning representations from sets USED-FOR point cloud processing. point cloud processing CONJUNCTION image / video recognition. image / video recognition CONJUNCTION point cloud processing. geometrically - interpretable and generic pooling mechanism USED-FOR fixed - dimensional representation. geometrically - interpretable and generic pooling mechanism USED-FOR features. end - to - end trainable Euclidean embedding USED-FOR sliced - Wasserstein distance. end - to - end trainable Euclidean embedding USED-FOR set - structured data. method COMPARE set representation learning approaches. set representation learning approaches COMPARE method. pooling method COMPARE method. method COMPARE pooling method. point - cloud HYPONYM-OF set - structured data. set - structured data EVALUATE-FOR pooling method. OtherScientificTerm is probability distribution. ,"This paper proposes a method for learning representations from sets for point cloud processing. The proposed method is based on a set-based pooling method, where each set is represented by a fixed-dimensional embedding of a set of points, and the goal is to learn a set representation from the set. The method is motivated by the fact that the Wasserstein distance between a set and a point cloud can be expressed as a function of the distance between the set and the probability distribution of the points in the set, which can be used as a way to learn representations from set-structured data.    The main contributions of the paper are:  1. The paper proposes an end-to-end trainable Euclidean embedding for set- structured data, which is used to learn the set representation.  2. The authors show that the proposed method outperforms the existing methods in terms of accuracy on point cloud classification tasks. ","This paper proposes a new pooling method for learning representations from set-structured data. The proposed method is based on end-to-end trainable Euclidean embedding for set- structured data, which is used to learn the sliced-Wasserstein distance between the data points. The authors show that the proposed method outperforms the state-of-the-art pooling methods on a variety of tasks, including graph learning and point cloud processing."
14492,SP:6cb2f0cbc076f8680cb00411790629f8e1478053,"training stability FEATURE-OF recurrent neural networks ( RNNs ). SBO - RNN HYPONYM-OF RNNs. stochastic bilevel optimization ( SBO ) USED-FOR RNNs. feedforward and backpropagation USED-FOR lower and upper - level optimization. stochastic gradient descent ( SGD ) USED-FOR SBO problem. stochastic gradient descent ( SGD ) USED-FOR RNN. RNN USED-FOR SBO problem. benchmark datasets EVALUATE-FOR approach. OtherScientificTerm are hidden states, hyperparameters, and vanishing or exploding gradient. Material is training data. ",This paper studies the stability of RNNs trained with stochastic bilevel optimization (SBO) with feedforward and backpropagation. The authors show that the training stability of SBO-RNNs can be improved by using SGD to solve the lower and upper-level optimization of the SBO problem. They show that SGD can be used in conjunction with RNN to improve training stability. They also show that RNN can be trained with SGD and SBO to improve the stability.,"This paper studies the stability of RNNs trained with stochastic bilevel optimization (SBO) with feedforward and backpropagation. In particular, the authors show that the training stability of SBO-RNNs is bounded by the number of hidden states and hyperparameters, and that the vanishing or exploding gradient can occur when the training data is too small. The authors propose a novel approach to solve the SBO problem, which is based on SGD. The proposed approach is evaluated on a variety of benchmark datasets. "
14541,SP:d3a4300e21ca215334f256f0467a428470548fe4,"online problem USED-FOR minimizing power consumption. algorithm USED-FOR power - saving states. energy consumption CONJUNCTION wake - up costs. wake - up costs CONJUNCTION energy consumption. wake - up costs FEATURE-OF power - saving states. energy consumption FEATURE-OF power - saving states. predicted lengths of the idle periods USED-FOR learning - augmented online algorithm. worst - case guarantee EVALUATE-FOR algorithm. algorithm USED-FOR online ski rental problem. learning augmented setting USED-FOR online ski rental problem. OtherScientificTerm is prediction error. Generic are problem, and approach. ","This paper studies the online ski rental problem, where the goal is to minimize the energy consumption while maximizing the wake-up cost. The paper proposes a learning-augmented online algorithm for this problem, which uses the predicted lengths of the idle periods to estimate the prediction error of the power-saving states. The proposed algorithm is shown to outperform the state-of-the-art worst-case algorithm in terms of energy consumption and wake up costs.","This paper proposes a learning-augmented online algorithm for the online ski rental problem, where the goal is to minimize the energy consumption and wake-up cost of the user. The main contribution of the paper is to propose an algorithm that learns the predicted lengths of the idle periods of the users. The algorithm is based on a learning augmented setting where the user can learn the predicted length of idle periods. The proposed algorithm is shown to be competitive with the state-of-the-art in terms of worst-case performance. "
14590,SP:22aba6284123af0ecd6605ee4e89b351bd7e10a3,sample sizes FEATURE-OF tasks. task similarities CONJUNCTION sample complexity. sample complexity CONJUNCTION task similarities. mathematical framework USED-FOR transferability. sample complexity EVALUATE-FOR learning models. transferability FEATURE-OF multi - source transfer learning problems. optimal combining coefficients USED-FOR transferability. models USED-FOR tasks. models USED-FOR task. model complexity CONJUNCTION similarities. similarities CONJUNCTION model complexity. sample sizes CONJUNCTION model complexity. model complexity CONJUNCTION sample sizes. analytical expression USED-FOR transferability measure. sample sizes FEATURE-OF analytical expression. model complexity FEATURE-OF analytical expression. sample sizes FEATURE-OF transferability measure. model complexity FEATURE-OF transferability measure. analyses USED-FOR practical learning tasks. parameterized model USED-FOR quantifiable transferability measure. deep neural networks USED-FOR multi - source transfer learning tasks. alternating iterative algorithm USED-FOR deep neural networks. approach COMPARE transfer learning algorithms. transfer learning algorithms COMPARE approach. image classification tasks EVALUATE-FOR approach. image classification tasks EVALUATE-FOR transfer learning algorithms. transfer learning algorithms USED-FOR multi - source and few - shot scenarios. multi - source and few - shot scenarios EVALUATE-FOR approach. Method is transfer learning algorithm designs. Task is knowledge transferring mechanism. ,"This paper studies the problem of transfer learning in multi-source transfer learning, where the goal is to learn a model that can transfer knowledge from one source to another. The authors propose a new quantifiable measure of transferability based on the sample complexity of the model and the similarity between the source and target tasks. They show that the transferability measure is a function of the sample size and the number of samples. They then propose an alternating iterative algorithm to learn the model with the proposed quantifiable transferability metric. They evaluate their method on image classification tasks and show that their method outperforms other transfer learning methods in terms of transfer performance.",This paper proposes a quantifiable transferability measure for multi-source transfer learning problems. The transferability is defined as the sum of the sample complexity of a model and the similarity between tasks. The authors propose an alternating iterative algorithm for learning the transferability of deep neural networks. They show that their method outperforms the state-of-the-art transfer learning algorithms on a variety of tasks. 
14639,SP:0fb8dcf15e0d43547d566fdba7bc70b3bb600005,"asymmetry FEATURE-OF search tasks. search image USED-FOR computational model. eccentricity - dependent visual recognition CONJUNCTION target - dependent top - down cues. target - dependent top - down cues CONJUNCTION eccentricity - dependent visual recognition. eccentricity - dependent visual recognition USED-FOR model. model COMPARE human behavior. human behavior COMPARE model. human behavior USED-FOR paradigmatic search tasks. asymmetry FEATURE-OF paradigmatic search tasks. paradigmatic search tasks EVALUATE-FOR model. model USED-FOR search asymmetry. ImageNet USED-FOR model. developmental diet USED-FOR model. classical perceptual properties FEATURE-OF neural network models. Task are Visual search, and visual search. OtherScientificTerm are eye movements, polarity of search asymmetry, and VisualSearchAsymmetry. Method is task - specific training. Material is natural images. ","This paper proposes VisualSearchAsymmetry, a method for visual search based on eccentricity-dependent visual recognition and top-down cues. The method is based on the observation that the search image asymmetry affects the computational model’s ability to identify the target image from a search image. The authors show that the model is able to learn to predict the search asymmetry of a given search image in a way that is consistent with human behavior. They also show that their method is capable of identifying the target images from a set of natural images.","This paper proposes VisualSearchAsymmetry (VSA), a method to model the search asymmetry of visual search tasks. The proposed method is based on the observation that the search image of a search image is not always the same as that of a target image. The authors propose a method for training a search model that is able to capture the search-asymmetry of the search images. The model is trained on the ImageNet dataset, where it is shown to be able to identify the search search image from the target image, and it is also shown that the model can identify the target search images from the eccentricity-dependent visual recognition and target-dependent top-down cues. The experimental results show that the proposed method outperforms human search models in terms of search performance."
14688,SP:f0cc968ea9da4884dcdaf6d0c75ea9f1511bdfc3,"adversarial examples USED-FOR certifiably robust models. tightness of the upper bound USED-FOR certifiably robust models. Interval Bound Propagation ( IBP ) training COMPARE models. models COMPARE Interval Bound Propagation ( IBP ) training. looser bounds USED-FOR Interval Bound Propagation ( IBP ) training. tighter bounds USED-FOR models. loss landscapes FEATURE-OF linear relaxation - based methods. tightness CONJUNCTION smoothness. smoothness CONJUNCTION tightness. tightness USED-FOR method. smoothness USED-FOR method. Method are Certifiable training, certifiable training, and certifiable training method. OtherScientificTerm are worst - case loss, and loss landscape. Generic is state - of - the - arts method. ","This paper studies the problem of certifiable robustness in the presence of adversarial examples. In particular, the authors propose a new method called Interval Bound Propagation (IBP) training, which is based on the notion of interval bound propagation. The main idea is to use the worst-case loss as the upper bound on the loss landscape of the model. The authors show that the proposed method is more robust than existing linear relaxation-based methods in terms of the worst case loss. ","This paper studies the problem of certifiable training, i.e., training a model that is robust to adversarial examples in the worst-case setting. The authors show that the upper bounds of the upper bound of the robustness of the model are tighter than those of the best-case upper bounds. They show that this tightness is due to the fact that the worst case upper bounds can be much tighter than the best case upper bound. They also show that such tightness can be achieved by using a linear relaxation-based method.   "
14737,SP:a158f8772a9dada059ffd1d6d7838ed40d8483da,"stochastic setting FEATURE-OF online linear regression. online ridge regression CONJUNCTION forward algorithm. forward algorithm CONJUNCTION online ridge regression. high probability regret bounds USED-FOR online ridge regression. high probability regret bounds USED-FOR forward algorithm. robustness FEATURE-OF regularization parameter. ridge USED-FOR forward algorithm. it PART-OF algorithms. linear function approximation PART-OF algorithms. it USED-FOR regret bounds. modification USED-FOR linear bandit settings. Method is online regression algorithms. OtherScientificTerm are bounded observations, boundedness assumption, and theoretical bounds. ","This paper studies online linear regression in the stochastic setting with bounded observations. In this setting, the authors show that online ridge regression with a regularization parameter is equivalent to a forward algorithm with a linear function approximation. The authors also show that the regret of the proposed algorithm is bounded by a constant factor.  ","This paper studies the problem of online ridge regression in the stochastic setting. The main contribution of the paper is to provide a high-probability regret bound for the online ridge-regression algorithm. The bounds are based on the assumption of bounded observations, and the authors show that the bounds are robust to the robustness of the regularization parameter. They also provide a bound on the regret of the forward algorithm. "
14786,SP:17ff9a2133aebf2d1b1787e8efc49d709389c0e7,"generative adversarial network CONJUNCTION adversarial training. adversarial training CONJUNCTION generative adversarial network. method USED-FOR setting. nonconvex - nonconcave setting FEATURE-OF minimax problems. adversarial training HYPONYM-OF minimax problems. generative adversarial network HYPONYM-OF minimax problems. two - time - scale variant PART-OF EG. slowO(1 / k ) rate FEATURE-OF squared gradient norm. smooth structured nonconvexnonconcave setting FEATURE-OF two - time - scale variant. EG+ HYPONYM-OF EG. EG+ HYPONYM-OF two - time - scale variant. slowO(1 / k ) rate EVALUATE-FOR two - time - scale variant. O(1 / k ) rate FEATURE-OF squared gradient norm. anchoring technique USED-FOR EG. EG+ CONJUNCTION EAG. EAG CONJUNCTION EG+. fast O(1 / k ) rate FEATURE-OF squared gradient norm. squared gradient norm FEATURE-OF smooth structured nonconvex - nonconcave problems. EG+ USED-FOR two - time - scale EG. negative comonotonicity condition FEATURE-OF saddle - gradient operator. fast extragradient ( FEG ) HYPONYM-OF two - time - scale EG. FEG - A HYPONYM-OF backtracking line - search version. Method are extragradient ( EG ) method, extra anchored gradient ( EAG ), and FEG. OtherScientificTerm are smooth convex - concave setting, and problem parameters. ","This paper studies the extragradient (EG) method for nonconvex-nonconcave minimax problems. The authors propose a two-time-scale variant of EG, called EG+ and EAG, which is based on the anchoring technique. The main contribution of the paper is to show that the squared gradient norm of EG+ converges to O(1/k) at a rate of O(\sqrt{k}) in the convex-concavity setting, and O(k/1) in the non-convolutional setting.  ","This paper proposes a two-time-scale variant of the extragradient (EG) method for nonconvex-nonconcave minimax problems. The main contribution of the paper is a new anchoring technique, called extra anchored gradient (EAG), which can be used to anchor the two time-scale variants of EG+ and EAG. The authors show that EAG can improve the O(1/k) rate of the squared gradient norm in the smooth structured structured nonconformal setting. They also show that FEG-A, a backtracking line-search version of EG, can also be used in this setting. "
14835,SP:4e38973033de24fc183c6112e1146f8eef0ddaea,"statistical data USED-FOR uniformity testing. rankings FEATURE-OF statistical data. uniform distribution COMPARE Mallows model. Mallows model COMPARE uniform distribution. pairwise statistics USED-FOR uniform distribution. pairwise statistics USED-FOR Mallows model. uniformity testing algorithm USED-FOR local DP scenario. ranking data USED-FOR binary statistics. binary statistics USED-FOR it. OtherScientificTerm are alternative class, large domain, uniformity, ✏ 0, and privacy budget parameter. Method are Mallows models, central DP algorithm, and uniformity testing algorithms. Task is Testing ranking data. ","This paper studies uniformity testing for ranking data in the presence of an alternative class. In particular, the authors consider the case where the alternative class is a uniform distribution over a large domain. They show that the uniformity of the distribution over the ranking data depends on the privacy budget parameter. The authors show that uniformity can be measured using pairwise statistics. They also provide a uniformity test for the local DP scenario.","This paper studies the problem of uniformity testing for ranking data. The authors propose a new uniformity-testing algorithm based on the Mallows model. The main idea is to test the uniformity of the data in a local setting, where the data comes from an alternative class and the data is from a large domain, and the privacy budget parameter is $0$. The authors show that the proposed uniformity test can be applied to the local setting and the large domain setting, and show that it can outperform the standard uniformity tests. "
14884,SP:99a835191a3ba8372e391b6d3316e9b68e543295,Greedy algorithms USED-FOR learning graphical models. worst - case exponential runtime EVALUATE-FOR greedy algorithms. greedy algorithms USED-FOR learning directed acyclic graphs. greedy scorebased algorithm USED-FOR learning DAGs. edge - greedy algorithms COMPARE approach. approach COMPARE edge - greedy algorithms. vertex - greedy USED-FOR approach. GES and hill - climbing algorithms HYPONYM-OF edge - greedy algorithms. score evaluations USED-FOR approach. polynomial - time algorithms USED-FOR learning DAG models. polynomial - time algorithms PART-OF algorithm. score - based algorithms USED-FOR order - based algorithms. Bregman divergences CONJUNCTION exponential families. exponential families CONJUNCTION Bregman divergences. score functions CONJUNCTION optimality conditions. optimality conditions CONJUNCTION score functions. algorithm USED-FOR score. Task is learning statistical models with sparse structure. Generic is they. OtherScientificTerm is DAGs. Method is DAG models. Metric is sample and computational complexity bounds. ,"This paper proposes a score-based greedy algorithm for learning directed acyclic graphs (DAGs) with sparse structure. The main idea is to use score evaluations to estimate the score of a DAG, which is then used to compute the order of the DAGs. The authors show that this algorithm is polynomial in the number of edges and in the time complexity. They also provide sample complexity bounds for this algorithm.   ","This paper proposes a score-based algorithm for learning directed acyclic graphs (DAGs) with sparse structure. The main contribution of the paper is a new score-driven algorithm for DAGs, which is based on edge-greedy algorithms. The authors show that their algorithm is polynomial-time in terms of sample and computational complexity, and that it is competitive with edge-based greedy algorithms. "
14933,SP:b60989706296b963b6671c01f22384978a334be1,"accuracy CONJUNCTION adversarial robustness. adversarial robustness CONJUNCTION accuracy. adversarial robustness EVALUATE-FOR CNNs. adversarial training USED-FOR CNNs. adversarial training USED-FOR adversarial robustness. adversarial robustness FEATURE-OF backbone CNNs. dilation architecture USED-FOR backbone CNN. accuracy CONJUNCTION adversarial robustness. adversarial robustness CONJUNCTION accuracy. real - world datasets CONJUNCTION benchmark neural networks. benchmark neural networks CONJUNCTION real - world datasets. real - world datasets EVALUATE-FOR algorithm. benchmark neural networks EVALUATE-FOR algorithm. adversarial robustness EVALUATE-FOR algorithm. accuracy EVALUATE-FOR algorithm. Method are convolutional neural networks ( CNNs ), and neural architecture dilation algorithm. Generic is they. OtherScientificTerm are adversarial attacks, and minimal computational overhead. ",This paper proposes a neural architecture dilation (NAD) method to improve the robustness of convolutional neural networks (CNNs) against adversarial attacks. The proposed method is based on the observation that adversarial training can improve the adversarial robustness in the backbone CNNs. The main contribution of the paper is to propose a novel neural architecture Dilation (ND) algorithm to improve adversarial accuracy and robustness. The method is evaluated on a variety of real-world datasets.   ,This paper proposes a novel neural architecture dilation algorithm to improve the robustness of backbone CNNs against adversarial attacks. The main contribution of the paper is to propose a novel dilation method for the backbone neural network architecture. The authors show that the proposed method can improve robustness and accuracy of backbone neural networks. The proposed method is evaluated on a variety of real-world datasets and benchmark networks.
14982,SP:77ed765e911a4e5f2bfba13cbd2403500a5d05e6,"linear function approximation USED-FOR episodic Markov decision processes ( MDPs ). model - based reward - free reinforcement learning USED-FOR episodic Markov decision processes ( MDPs ). linear function approximation USED-FOR model - based reward - free reinforcement learning. transition probability kernel PART-OF MDP. feature mappings FEATURE-OF linear function. Linear Mixture MDP assumption USED-FOR algorithm. linear function USED-FOR transition probability kernel. reward function USED-FOR ε - optimal policy. UCRL - RFE USED-FOR ε - optimal policy. Bernstein - type bonus USED-FOR UCRL - RFE. upper bound COMPARE lower bound. lower bound COMPARE upper bound. Task are planning phase, and exploration phase. Generic is policy. OtherScientificTerm is feature mapping. Method are linear Mixture MDPs, and reward - free algorithm. ","This paper studies model-based reward-free reinforcement learning in episodic Markov decision processes (MDPs) with linear Mixture MDPs. The authors propose UCRL-RFE, which is a reward-based RL algorithm that uses a linear function approximation to estimate the transition probability kernel in the MDP. The main contribution is a UCRL algorithm that can learn a reward function that maximizes the probability of finding the optimal policy with probability $\epsilon^2$ in the exploration phase.   The main contributions of the paper are as follows:  - The authors show that the reward function can be approximated by a linear mixture MDP with probability $p(y|x) = p(y | x)$.  - They show that this linear MDP can be represented as a set of feature mappings of the linear function.  - In the planning phase, the authors propose to use a Bernstein-type bonus to maximize the reward of the policy. ","This paper proposes UCRL-RFE, a reward-free algorithm for linear Mixture MDPs. The main idea is to use a linear function approximation to estimate the transition probability kernel of the MDP, which is a feature mapping of the feature mappings of the linear function. The authors provide a lower bound for UCRL with a Bernstein-type bonus. The upper bound is also provided.   "
15031,SP:28563ba0975f56ddb662cd46e85de78bb6024d36,seasonal patterns FEATURE-OF data stream of events. Shifting Seasonal Matrix Factorization approach USED-FOR seasonal patterns. SSMF HYPONYM-OF Shifting Seasonal Matrix Factorization approach. it USED-FOR regime shifts. regime shifts PART-OF seasonal patterns. regime shifts USED-FOR it. lossless data compression scheme USED-FOR it. lossless data compression scheme USED-FOR method. algorithm COMPARE baseline methods. baseline methods COMPARE algorithm. real - world data streams EVALUATE-FOR baseline methods. real - world data streams EVALUATE-FOR algorithm. OtherScientificTerm is human intervention. ,This paper proposes a new method for seasonal matrix factorization (SSMF) based on Shifting Seasonal Matrix Factorization. The proposed method is based on the observation that seasonal patterns in the data stream of events are not uniform across regimes. The authors propose to use a lossless data compression scheme to improve the performance of SSMF. Experiments on real-world data streams demonstrate the effectiveness of the proposed method. ,This paper proposes a Shifting Seasonal Matrix Factorization (SSMF) approach to shift the seasonal patterns in a data stream of events. The main idea of SSMF is to use a lossless data compression scheme to reduce the number of regime shifts in the data stream. The authors show that SSMF can be applied to a wide range of data streams and show that it can be used to improve the performance of existing methods. 
15080,SP:e4bb07033001be4d04695ef058f426d49fe440be,"Assignment HYPONYM-OF informatics. exact solvers USED-FOR assignment problems. objective functions CONJUNCTION prior assumptions. prior assumptions CONJUNCTION objective functions. algorithms USED-FOR real problems. WeaveNet HYPONYM-OF neural network architecture. feature weaving layer PART-OF core module. strongly NP - hard settings USED-FOR stable matching. stable matching HYPONYM-OF non - linear assignment problems. OtherScientificTerm is NP - hardness or incomplete input. Method are approximation algorithms, learning - based 7 method, learning - based baselines, and algorithmic method. Task are real - world assignment problems, and combinatorial problem of assignment. Generic is model. ","This paper studies the assignment problem in the combinatorial setting, where the objective function is a function of the number of possible assignments. The authors show that in this setting, it is NP-hard to find an exact solution to the assignment problems. They then propose a learning-based 7-step approximation algorithm for this problem, which is shown to be stable in strongly-NP-hard settings. They also show that this algorithm can be extended to non-linear assignment problems, where it is shown that it is possible to find a stable solution to this problem.","This paper studies the problem of stable matching in a combinatorial setting, where the objective function is non-linear and the input is incomplete. The authors propose a method for solving this problem, which they call WeaveNet, which is a neural network architecture with a feature weaving layer in the core module. The main contribution of the paper is to provide a theoretical analysis of the problem in the setting of strongly NP-hard settings. They show that under certain assumptions, the proposed method can be used to solve the stable matching problem in a learning-based 7 method."
15129,SP:8a559e21d45661eef427b310e5fe8488d5749137,3D point cloud data USED-FOR safety - critical applications. autonomous driving HYPONYM-OF safety - critical applications. robustness EVALUATE-FOR 3D deep learning models. adversarial attacks FEATURE-OF 3D deep learning models. threat models USED-FOR 3D point clouds. self - supervised learning proxy tasks USED-FOR threat models. adversarial training USED-FOR self - supervised learning proxy tasks. adversarial training USED-FOR threat models. MLP - based ( PointNet ) CONJUNCTION convolution - based ( DGCNN ). convolution - based ( DGCNN ) CONJUNCTION MLP - based ( PointNet ). convolution - based ( DGCNN ) CONJUNCTION transformer - based ( PCT ) 3D architectures. transformer - based ( PCT ) 3D architectures CONJUNCTION convolution - based ( DGCNN ). self - supervision USED-FOR 3D point cloud recognition. self - supervision COMPARE adversarial training baseline. adversarial training baseline COMPARE self - supervision. robustness EVALUATE-FOR 3D point cloud recognition. robustness EVALUATE-FOR self - supervision. it USED-FOR adversarial propagation. local feature learning USED-FOR adversarial robustness. local feature learning USED-FOR point clouds. adversarial robustness FEATURE-OF point clouds. DGCNN CONJUNCTION jigsaw proxy task. jigsaw proxy task CONJUNCTION DGCNN. jigsaw proxy task USED-FOR 3D adversarial robustness. 3D adversarial robustness EVALUATE-FOR DGCNN. OtherScientificTerm is point - level input perturbations. ,"This paper studies the robustness of 3D deep learning models against adversarial attacks on 3D point cloud data. The authors propose to use self-supervised learning proxy tasks to train 3D models with adversarial training. The proposed method is based on the observation that point clouds are more vulnerable to adversarial perturbations than point-level inputs. To address this issue, the authors proposed to use a combination of MLP-based (PointNet) and convolution-based(DGCNN) 3D architectures. The experiments show that the proposed method improves the adversarial robustness against 3D adversarial attack.","This paper studies the robustness of 3D deep learning models against 3D point cloud attacks. The authors show that self-supervised learning proxy tasks (e.g., PointNet, DGCNN, MLP-based) and transformer-based (PCT) 3D architectures are more robust to 3D adversarial attacks than adversarial training (i.e., adversarial perturbations at the point-level). They also show that local feature learning (local feature learning) can be used to improve the adversarial robustness. "
15178,SP:657c5a1114c0d054b9e767d85990bbbb0492912d,"FISTA CONJUNCTION mirror descent. mirror descent CONJUNCTION FISTA. projected Newton ’s method CONJUNCTION FISTA. FISTA CONJUNCTION projected Newton ’s method. near - optimal regret bounds CONJUNCTION convergence rates. convergence rates CONJUNCTION near - optimal regret bounds. projected Newton ’s method CONJUNCTION mirror descent. mirror descent CONJUNCTION projected Newton ’s method. O(T ) regret EVALUATE-FOR online mirror descent. near - optimal regret bounds FEATURE-OF Optimization algorithms. mirror descent HYPONYM-OF Optimization algorithms. projected Newton ’s method HYPONYM-OF Optimization algorithms. FISTA HYPONYM-OF Optimization algorithms. conditional gradient variants USED-FOR linear optimization. O(T ) regret HYPONYM-OF suboptimal rates. toolkit USED-FOR projections. discrete and continuous perspectives USED-FOR toolkit. discrete and continuous perspectives USED-FOR projections. early termination USED-FOR away - step Frank - Wolfe algorithm. runtime EVALUATE-FOR Bregman projections. OtherScientificTerm are computational bottleneck, iterative projections, and cardinality based submodular polytopes. Metric is runtime v / s convergence rates. ","This paper studies the convergence of online mirror descent and projected Newton's method in linear optimization. The main contributions of the paper are:  1. The regret bound for online FISTA is improved to O(T^2) from O(1/\sqrt{T}) for mirror descent.  2. For projected Newton’s method, the regret bound is improved from $O(T)$ to $O(\log T)$ in the online case.  3. The authors show that online Frank-Wolfe with early termination can achieve O(V^2/s) regret at a time v/s.  4. For online Bregman projections, the authors show the regret can be improved from O(\sqrt{\log T}) to O(\varepsilon) at a cost of O(v/s). ",This paper studies the regret bounds of online mirror descent and FISTA for linear optimization. The regret bounds are based on the convergence rate v/s of the Bregman projections. The authors show that the convergence rates of the two algorithms converge to near-optimal regret bounds. They also show that early termination of the Frank-Wolfe algorithm can be used to improve the performance of the algorithms.
15227,SP:8dae43d6b5cebb7ef6c39437d997b390c2380536,"natural parameters PART-OF k - parameter minimal exponential family. i.i.d. samples USED-FOR natural parameters. maximum likelihood estimator USED-FOR exponential family. finite sample guarantees USED-FOR parameter estimation. maximum likelihood estimation USED-FOR re - parameterized distribution. exponential family FEATURE-OF re - parameterized distribution. maximum likelihood estimation USED-FOR method. re - parameterized distribution USED-FOR method. Generic are it, and estimator. Metric are sample complexity, and computational complexity. ","This paper studies the problem of parameter estimation in the k-parameter minimal exponential family, where the natural parameters are assumed to be in the i.i.d. range. The authors propose a method to estimate the parameters of this exponential family using a re-parametrized version of the maximum likelihood estimator. The main contribution of this paper is to show that the estimator can be used to estimate natural parameters in the exponential family with finite sample complexity.   ","This paper studies the sample complexity of parameter estimation in the k-parameter minimal exponential family. The authors consider the problem of estimating the parameters of a re-parametrized exponential family, where the natural parameters are assumed to be in the form of a set of i.i.d. samples. The main contribution of the paper is to provide a sample complexity bound for the exponential family for parameter estimation, which is based on the maximum likelihood estimator of the family.   The main contributions of this paper are as follows:  1. The sample complexity bounds are derived for the k parameters in the family of exponential families. 2. The bounds are obtained by using a maximum likelihood estimation of the re- parameterized family. 3. The upper bound is obtained by assuming that the parameters are in an exponential family with finite sample guarantees. 4. The lower bound is derived by assuming a finite sample guarantee."
15276,SP:4f9ddb697e86356fb293ef34a69ca3702c4e8164,"differentiable renderers USED-FOR predicting intrinsic object properties. learning - based approaches USED-FOR inverse graphics. rasterization - based renderers USED-FOR learning - based approaches. rasterization CONJUNCTION ray - tracing. ray - tracing CONJUNCTION rasterization. DIBR++ HYPONYM-OF hybrid differentiable renderer. speed CONJUNCTION realism. realism CONJUNCTION speed. hybrid differentiable renderer USED-FOR photorealistic effects. ray - tracing PART-OF hybrid differentiable renderer. direct estimation CONJUNCTION spherical basis functions. spherical basis functions CONJUNCTION direct estimation. renderer USED-FOR light transport. environmental lighting and spatially - varying material models PART-OF renderer. direct estimation USED-FOR light transport. spherical basis functions USED-FOR light transport. learning frameworks USED-FOR geometry, reflectance and lighting prediction. physics - based differentiable renderers COMPARE DIB - R++. DIB - R++ COMPARE physics - based differentiable renderers. compact and expressive shading model USED-FOR DIB - R++. path tracing USED-FOR physics - based differentiable renderers. approach COMPARE rasterization - based approaches. rasterization - based approaches COMPARE approach. material editing CONJUNCTION relighting. relighting CONJUNCTION material editing. approach USED-FOR artistic applications. material and lighting disentanglement FEATURE-OF synthetic and real data. rasterization - based approaches USED-FOR artistic applications. material and lighting disentanglement EVALUATE-FOR rasterization - based approaches. relighting HYPONYM-OF artistic applications. material editing HYPONYM-OF artistic applications. material and lighting disentanglement EVALUATE-FOR approach. synthetic and real data EVALUATE-FOR approach. Method is naive lighting and material models. OtherScientificTerm are non - Lambertian, specular reflections, and ground - truth. ","This paper proposes a new renderer architecture for inverse rendering that combines the advantages of ray-tracing and physics-based renderers. The main idea is to use a differentiable renderer to predict the geometry, reflectance and lighting prediction, which is then used to train a learning framework for geometry and reflectance prediction. The method is evaluated on synthetic and real-world data.   ","This paper proposes a hybrid differentiable renderer (DIB-R++) for inverse graphics. The main idea is to combine the physics-based renderer with a ray-tracing based renderer to achieve a more realistic renderer. The renderer is based on a compact and expressive shading model, which can be applied to both synthetic and real data. Experiments show that the proposed renderer can achieve state-of-the-art results in terms of speed and realism. "
15325,SP:6ac1c8556e7131939cc582f513bc9921470e1b09,"Soft - argmax operation USED-FOR detection - based methods. soft - argmax USED-FOR neural network. sampling - argmax HYPONYM-OF differentiable training method. continuous formulation USED-FOR output distribution. continuous formulation USED-FOR differentiable sampling process. continuous formulation USED-FOR expectation. sampling - argmax USED-FOR localization tasks. soft - argmax operation USED-FOR localization tasks. sampling - argmax COMPARE soft - argmax operation. soft - argmax operation COMPARE sampling - argmax. OtherScientificTerm are differentiable manner, probability map, pixel - wise supervision, map, implicit constraints, and expectation of the localization error. Generic are model, and method. Metric is average error. ","This paper proposes a differentiable training method for localization based on sampling-argmax. The main idea is to use a continuous formulation of the output distribution of the model to estimate the expectation of the localization error, which is then used to train the model. The method is shown to improve the performance of the proposed method on a variety of localization tasks. ","This paper proposes a new differentiable training method for differentiable neural networks. The main idea is to use a continuous formulation of the output distribution of the neural network as a differentiable sampling process, where the expectation of the localization error is computed as a function of the input distribution. The authors show that the sampling process is differentiable in the sense that it can be used to estimate the average error of the model. They also show that their method can be applied to a variety of localization tasks."
15374,SP:478c05c90090f9d80b72ac352c488073b45a5d8b,"Graph Contrastive Learning ( GCL ) USED-FOR generalizable representations. contrastive views USED-FOR generalizable representations. data augmentation USED-FOR contrastive views. incomplete structure information USED-FOR models learning. directional structure FEATURE-OF directed graphs. hand - picking parameters FEATURE-OF predefined contrastive views. data augmentation USED-FOR contrastive information. directional structure HYPONYM-OF intrinsic graph structural information. data augmentation USED-FOR graph structure. predefined contrastive views USED-FOR GCL. hand - picking parameters USED-FOR GCL. it USED-FOR contrastive information. Laplacian perturbation HYPONYM-OF directed graph data augmentation method. contrastive views USED-FOR directed graph contrastive learning framework. Laplacian perturbation USED-FOR contrastive views. multi - task curriculum learning USED-FOR it. model COMPARE GCL models. GCL models COMPARE model. structural features of directed graphs EVALUATE-FOR model. benchmarks EVALUATE-FOR state - of - the - art approaches. Method is message passing scheme. OtherScientificTerm are graph changing action, directed graph structure, and easy - to - difficult contrastive views. ","This paper proposes a directed graph contrastive learning (GCL) framework that uses contrastive views to improve the generalization performance of directed graph models. The proposed method is based on the observation that the contrastive information in directed graphs is incomplete due to the incomplete structure information. To address this issue, the authors propose to use data augmentation to improve contrastive representation learning in directed graph data. The main idea is to use Laplacian perturbation as a contrastive augmentation method. The authors also propose a multi-task curriculum learning approach to learn contrastive representations. The experimental results show that the proposed method achieves state-of-the-art performance on several benchmarks.  ","This paper proposes a directed graph contrastive learning (GCL) framework that augments the contrastive views of directed graphs with a Laplacian perturbation to improve the generalization ability of GCL models. The proposed method is based on the idea of contrastive view augmentation, which is a data augmentation method that is used to augment the graph structure information. The authors show that the proposed method outperforms the state-of-the-art in terms of performance on a variety of benchmark datasets."
15423,SP:85b383d2f722f7bff438840e423f5cb4c67d5980,common interface FEATURE-OF grounded language learning environments. RTFM CONJUNCTION Messenger. Messenger CONJUNCTION RTFM. grid - world environments CONJUNCTION symbolic counterparts of visual worlds. symbolic counterparts of visual worlds CONJUNCTION grid - world environments. interpreting rich natural language USED-FOR complex scenes. interpreting rich natural language USED-FOR symbolic counterparts of visual worlds. RTFM HYPONYM-OF grid - world environments. ALFWorld HYPONYM-OF complex scenes. Messenger HYPONYM-OF grid - world environments. grid - world environments PART-OF SILG. symbolic counterparts of visual worlds PART-OF SILG. action space CONJUNCTION language specification. language specification CONJUNCTION action space. richness of observation space CONJUNCTION action space. action space CONJUNCTION richness of observation space. language specification CONJUNCTION plan complexity. plan complexity CONJUNCTION language specification. shared model architecture USED-FOR RL. recurrent state - tracking CONJUNCTION entity - centric attention. entity - centric attention CONJUNCTION recurrent state - tracking. egocentric local convolution CONJUNCTION recurrent state - tracking. recurrent state - tracking CONJUNCTION egocentric local convolution. entity - centric attention CONJUNCTION pretrained LM. pretrained LM CONJUNCTION entity - centric attention. shared model architecture USED-FOR environments. SILG USED-FOR pretrained LM. shared architecture COMPARE environment - specific architectures. environment - specific architectures COMPARE shared architecture. SILG EVALUATE-FOR models. SILG USED-FOR language grounding. Method is unified models. OtherScientificTerm is entities. Material is multi - environment benchmark. ,"This paper proposes a new multi-environment benchmark called SILG, which is a set of environments that are grounded in natural language. The goal of the benchmark is to test the ability of language-based RL models to learn in environments with rich natural language representations. The authors propose to use a shared model architecture for learning in the environments. The proposed model is based on a combination of a pre-trained language model (LM) and an entity-centric attention model (E2A). The proposed method is evaluated on ALFWorld, RTFM, and Messenger.  ","This paper proposes a unified language learning framework for grounded language learning environments (SILG). The framework is built on top of the popular RTFM, Messenger, and ALFWorld environments. The authors propose a shared model architecture for RL in the SILG framework, which is based on an egocentric local convolution, entity-centric attention, and recurrent state-tracking. The proposed framework is evaluated on a variety of environments, and compared with state-of-the-art baselines."
15472,SP:23c8db56f59f778fe812a5dd161f7a1f21c3cdba,"Vision MoE ( V - MoE ) COMPARE dense networks. dense networks COMPARE Vision MoE ( V - MoE ). Vision MoE ( V - MoE ) HYPONYM-OF Vision Transformer. V - MoE COMPARE networks. networks COMPARE V - MoE. V - MoE USED-FOR image recognition. extension USED-FOR adaptive per - image compute. routing algorithm USED-FOR extension. V - MoE USED-FOR scale vision models. V - MoE USED-FOR 15B parameter model. ImageNet EVALUATE-FOR 15B parameter model. Task are Natural Language Processing, and Computer Vision. Method is vision models. ","This paper proposes Vision MoE (V-MoE), a transformer-based vision model that is able to learn to process multiple images at the same time. The proposed method is based on the idea of self-attention, which is an extension of the MoE architecture. The authors show that the proposed method outperforms the state-of-the-art vision models on ImageNet.","This paper proposes Vision MoE (V-MoE), an extension of Vision Transformer (MoE) that can be applied to the task of image recognition. The main idea is to adaptively compute per-image compute for each image. The proposed extension is based on a routing algorithm. The authors demonstrate the effectiveness of the proposed extension on the ImageNet dataset."
15521,SP:c5235f41dfb8b5cc478f11c5d5e0ab0b8676871e,"benign optimization landscape FEATURE-OF loss function. n ( sample size ) neurons FEATURE-OF 1 - hidden - layer networks. zero training loss FEATURE-OF global minimizer. local - min or saddle points FEATURE-OF nice local region. global minimizer FEATURE-OF KKT point. projected gradient methods USED-FOR KKT points. SGD USED-FOR narrow neural nets. projected gradient methods USED-FOR narrow neural nets. projected gradient methods COMPARE SGD. SGD COMPARE projected gradient methods. projected gradient methods USED-FOR constrained formulation. Method are neural networks, narrow networks, and gradient descent. Generic is network. OtherScientificTerm are activation, expressivity, and feasible region. Task is constrained optimization formulation. ",This paper studies the optimization landscape of a 1-hidden layer neural network with n neurons and zero training loss. The authors show that there exists a set of KKT points in the landscape of the loss function that are feasible for a given n-neuron network. They show that the KKT point is a global minimizer of a feasible region of the landscape. They then show that projected gradient methods can be used to find the feasible region.   ,"This paper studies the problem of constrained optimization of a 1-hidden-layer neural network with n-neighboring neurons. The authors consider the setting where the number of neurons in the network is n and the size of the training set is n. They show that the local-min and saddle points of the network can be represented as a set of KKT points, where KKT is the global minimizer of the KKT point. They provide a constrained formulation of the problem, and show that this formulation can be applied to the constrained optimization formulation of SGD.  "
15570,SP:0be529f5254afd59dcfa6b34a359c7037e7a8323,"risk measures USED-FOR risk - aware multi - armed bandit models. variance HYPONYM-OF risk measures. correlated options FEATURE-OF real - world online decision making problems. learner PART-OF CMCB. full - bandit feedback HYPONYM-OF feedback settings. full - information HYPONYM-OF feedback settings. logarithmic factors FEATURE-OF optimal regrets. matching lower bounds USED-FOR algorithms. optimal regrets FEATURE-OF algorithms. option correlation FEATURE-OF risk - aware bandits. analytical techniques USED-FOR bandit analysis. analytical techniques USED-FOR concentration. estimated covariance USED-FOR concentration. analytical techniques USED-FOR risk of selected actions. analytical techniques USED-FOR estimated covariance. sampling strategy properties USED-FOR bandit analysis. sampling strategy properties USED-FOR analytical techniques. Generic is they. OtherScientificTerm are weight vectors, random feedback, option covariance, reward observation scenarios, and covariance structures. ","This paper studies the problem of risk-aware multi-armed bandit (CMCB) in which the learner has access to a set of correlated options. The authors propose a novel risk measure, called option correlation, to measure the risk of selected actions in the CMCB setting. They show that this measure is a good measure of the risk in the full-information feedback setting, and provide a matching lower bound on the optimal regret of the proposed algorithms. They also provide an upper bound for the regret of their algorithms.","This paper studies the problem of risk-aware multi-armed bandits (CMCB) in which the learner has access to a large number of options and can choose which of these options to take. The authors propose a new risk measure, called option correlation, which is a measure of the correlation between the options and the risk of selected actions. They show that the variance of option correlation can be used as a risk measure for CMCB. They also provide a lower bound on the optimal regret of an algorithm based on this measure."
15619,SP:472a90bb175b0286765c5a47b040e1a58f594a05,"r × r - dimensional PSD matrices PART-OF Positive Semidefinite ( PSD ) factorization. PSD factorizations USED-FOR semidefinite programs. quantum resources USED-FOR information theory. Nonnegative Matrix Factorization ( NMF ) problem USED-FOR PSD factorization task. algorithm USED-FOR NMFs. Multiplicative Update algorithm HYPONYM-OF algorithm. positive diagonal matrices USED-FOR non - negativity. non - commutative extension USED-FOR PSD factorizations. Lee - Seung ’s algorithm USED-FOR non - commutative extension. Matrix Multiplicative Update ( MMU ) algorithm HYPONYM-OF non - commutative extension. multiplicative update algorithm USED-FOR NMF. squared loss objective EVALUATE-FOR update scheme. blockdiagonal PSD factorizations CONJUNCTION tensor PSD factorizations. tensor PSD factorizations CONJUNCTION blockdiagonal PSD factorizations. MMU algorithm USED-FOR blockdiagonal PSD factorizations. MMU algorithm USED-FOR tensor PSD factorizations. MMU algorithm USED-FOR primitive. primitive USED-FOR blockdiagonal PSD factorizations. primitive USED-FOR tensor PSD factorizations. real and synthetic data EVALUATE-FOR method. OtherScientificTerm are r - dimensional non - negative vectors, and matrix geometric mean of appropriate PSD matrices. Generic are problem, and it. Method are PSD factorization, MajorizationMinimization framework, and Lieb ’s Concavity Theorem. ","This paper studies the nonnegative matrix factorization (NMF) problem in the positive semidefinite (PSD) setting. The authors propose a non-commutative extension of Lee-Seung's Multiplicative Update (MMU) algorithm for PSD factorization, which is based on the matrix geometric mean of appropriate PSD matrices. Theoretical results on the squared loss objective of the update scheme are provided, and experiments on real and synthetic data demonstrate the effectiveness of the proposed method.",This paper studies the non-negative matrix factorization (NMF) problem for positive semidefinite (PSD) factorization. The authors propose a non-commutative extension of Lee-Seung’s algorithm (MMU) to the PSD factorization problem. The main contribution of the paper is the introduction of the matrix geometric mean of appropriate PSD matrices. The proposed method is evaluated on both real and synthetic data.
15668,SP:83abd6d149d88cc6e96cbc4d488e4fe9dc2a4fcb,"Domain Generalization ( DG ) USED-FOR model. DG approaches USED-FOR domaininvariant information. DG approaches USED-FOR generalization capability. features PART-OF latent space. domain - specific representation USED-FOR generalization. meta - learning framework USED-FOR domain - specific representation. mDSDI COMPARE state - of - the - art techniques. state - of - the - art techniques COMPARE mDSDI. state - of - the - art techniques USED-FOR DG. mDSDI USED-FOR DG. domain - specific COMPARE domain - invariant. domain - invariant COMPARE domain - specific. Background - Colored - MNIST HYPONYM-OF dataset. OtherScientificTerm are domain - specific information, invariance view, and domain - invariant and domainspecific features. Generic is framework. Method is unified framework. ","This paper proposes a meta-learning framework for domain generalization (DG) based on domain-invariant and domain-specific features. The proposed method, called mDSDI, aims to learn a domain-agnostic representation of features in the latent space, which is then used to improve the generalization performance of the model.   The main contribution of this paper is to propose a unified framework to learn domain-dependent and domain invariant features simultaneously.  The experiments show that the proposed method achieves state-of-the-art performance on the Background-Colored-MNIST dataset and achieves better performance than the state of the art methods. ","This paper proposes a meta-learning framework for domain generalization (DG) based on domain-invariant and domain-specific features. The key idea is to learn a representation of the latent space that is invariant to different domains, and then use this representation to improve the generalization performance of the model. The proposed method is evaluated on a dataset called Background-Colored-MNIST, and compared with state-of-the-art techniques."
15717,SP:4191474c75e2fedf514f0f3001a67a047eb74c30,"diffusion models COMPARE generative models. generative models COMPARE diffusion models. image sample quality EVALUATE-FOR generative models. image sample quality EVALUATE-FOR diffusion models. architecture USED-FOR unconditional image synthesis. diversity CONJUNCTION fidelity. fidelity CONJUNCTION diversity. method USED-FOR diversity. gradients USED-FOR classifier. sample quality USED-FOR conditional image synthesis. classifier guidance USED-FOR sample quality. classifier USED-FOR method. gradients USED-FOR method. classifier guidance USED-FOR conditional image synthesis. classifier guidance CONJUNCTION upsampling diffusion models. upsampling diffusion models CONJUNCTION classifier guidance. FID EVALUATE-FOR classifier guidance. Material are ImageNet 128 ⇥ 128, ImageNet 256 ⇥ 256, and ImageNet 512 ⇥ 512. Method is BigGAN - deep. ",This paper proposes a new method for unconditional image synthesis. The method is based on a classifier that is trained to predict the gradients of the input image. The gradients are then used to update the output of the classifier to improve the quality of the generated images. The proposed method is evaluated on ImageNet and ImageNet-512.   ,"This paper proposes a new method for unconditional image synthesis based on a classifier-based approach. The method is based on the idea of classifier guidance, which aims to improve the sample quality of conditional image synthesis. The main contribution of the paper is to propose a method to improve diversity and fidelity of the generated images. The proposed method is evaluated on three datasets: ImageNet 128, ImageNet 256, and ImageNet 512."
15766,SP:fe3cab08596cde4c14ecf6fca8d0f95b02bab229,"out - of - distribution samples USED-FOR few - shot learning. unlabeled samples HYPONYM-OF out - of - distribution samples. out - of - distribution samples USED-FOR classifier. query data HYPONYM-OF in - distribution samples. approach USED-FOR inductive and transductive settings. method COMPARE pretrained networks. pretrained networks COMPARE method. architectures FEATURE-OF pretrained networks. OtherScientificTerm are irrelevant features, prototypes, and feature extractors. Task is pre - training. ",This paper proposes a novel approach for few-shot learning with out-of-distribution (OOD) samples. The main idea is to train a classifier on a set of query data and then use an out of distribution set of unlabeled samples to improve the performance of the classifier. The proposed method is shown to outperform existing methods in both inductive and transductive settings.,This paper proposes a method for few-shot learning with out-of-distribution samples. The main idea of the paper is to train a classifier on unlabeled samples and use them as prototypes for the query data. The proposed method is applied to both inductive and transductive settings. The authors show that the proposed method outperforms the state of the art in terms of accuracy and accuracy on a variety of tasks. 
15815,SP:b1f65724926f136979829b7a6c870bc31f38f591,experience replay USED-FOR reinforcement learning. Prioritized sampling USED-FOR samples. recentness CONJUNCTION corrective feedback. corrective feedback CONJUNCTION recentness. TD error CONJUNCTION recentness. recentness CONJUNCTION TD error. criteria PART-OF prioritization. TD error HYPONYM-OF criteria. recentness HYPONYM-OF prioritization. corrective feedback HYPONYM-OF criteria. corrective feedback PART-OF prioritization. recentness HYPONYM-OF criteria. optimal prioritization strategy USED-FOR Bellman update. on - policiness CONJUNCTION Q value. Q value CONJUNCTION on - policiness. methods USED-FOR prioritization weight. ReMERN CONJUNCTION ReMERT. ReMERT CONJUNCTION ReMERN. ReMERT HYPONYM-OF methods. ReMERN HYPONYM-OF methods. ReMERT USED-FOR temporal ordering of states. ReMERN HYPONYM-OF error network. MuJoCo CONJUNCTION Atari. Atari CONJUNCTION MuJoCo. Atari CONJUNCTION Meta - World. Meta - World CONJUNCTION Atari. methods COMPARE prioritized sampling algorithms. prioritized sampling algorithms COMPARE methods. RL benchmarks EVALUATE-FOR prioritized sampling algorithms. RL benchmarks EVALUATE-FOR methods. Meta - World HYPONYM-OF RL benchmarks. MuJoCo HYPONYM-OF RL benchmarks. Atari HYPONYM-OF RL benchmarks. Metric is regret minimization objective. OtherScientificTerm is hindsight TD error. Task is sampling. Generic is strategy. ,"This paper studies the problem of prioritized sampling in reinforcement learning, where the goal is to maximize the regret minimization of the learned policy. The authors propose to use hindsight TD error and recentness as two criteria for prioritization, and propose two methods to compute the prioritization weight. The first method, ReMERN, is based on the Bellman update, and the second method is ReMERT, which uses the error network as a prioritized version of the error function. The proposed methods are evaluated on MuJoCo, Meta-World, and Atari.   ","This paper proposes a new prioritized sampling strategy for experience replay in reinforcement learning. The main idea is to use recentness and corrective feedback as criteria for prioritizing samples in experience replay. The authors propose two methods, ReMERN and ReMERT, which are based on the Bellman update strategy. They show that the proposed method outperforms the state-of-the-art on MuJoCo, Atari, and Meta-World."
15864,SP:601ebf30b3c6aa35fcef49633aa8eb0acd0f2c66,nonstationary environment FEATURE-OF expert advice. nonstationary environment FEATURE-OF sequential prediction. expert advice USED-FOR sequential prediction. regret bounds USED-FOR linear - time algorithm. relative entropy projection step PART-OF algorithm. projection COMPARE weight - sharing approaches. weight - sharing approaches COMPARE projection. implicit costs FEATURE-OF weight updates. algorithm USED-FOR projection step. linear time FEATURE-OF projection step. OtherScientificTerm is long - term memory guarantees. Task is portfolio optimization. ,"This paper studies the problem of portfolio optimization in the non-stationary setting, where the goal is to find a good value function that maximizes the expected return on the current portfolio. The authors propose a linear-time algorithm with regret bounds of $O(1/\sqrt{T})$ where $T$ is the number of expert advice points and $\tilde{O}(T)$ is a random variable. They show that the regret of the proposed algorithm is $O(\sqrt{\Omega(T))$ with respect to the true value function.    The main contribution of the paper is a novel projection step to reduce the implicit costs of weight updates in the projection step.  The authors show that this projection step can be computed in linear time, and that it is equivalent to a weight-sharing approach. ",This paper studies the regret bounds of a linear-time algorithm for weight-sharing in a nonstationary environment where expert advice is provided. The authors provide a regret bound for a linear time algorithm with a relative entropy projection step. The regret bounds are based on the fact that the projection step is linear in time. They also provide a long-term memory guarantee for their algorithm.
15913,SP:b2439973063e827b3cbe92306a2fdee3286b6b44,navigational engines CONJUNCTION recommendation systems. recommendation systems CONJUNCTION navigational engines. routing applications USED-FOR recommendation systems. routing applications USED-FOR navigational engines. contextual linear bandits USED-FOR routing applications. routing applications USED-FOR variant. contextual linear bandits USED-FOR variant. algorithms USED-FOR problem. O(d log d ) regret CONJUNCTION list size poly(d ). list size poly(d ) CONJUNCTION O(d log d ) regret. O(d log d ) regret FEATURE-OF algorithm. list size poly(d ) FEATURE-OF algorithm. nearly tight algorithms USED-FOR problem. Steiner ’s formula USED-FOR centroid of a convex set. algorithmic techniques USED-FOR convex geometry. Steiner ’s formula HYPONYM-OF algorithmic techniques. OtherScientificTerm is hidden d - dimensional value. Method is cutting - plane algorithms. Metric is regret. ,"This paper considers the problem of finding the centroid of a convex set of size $d$ of a set of $k$ points. The goal is to minimize the regret of a linear bandit algorithm with $k=1,2,3,4$ points, where $K$ is the number of points in the set, and $D$ is a list size of $d$. The main result is a regret bound of $O(d log d)$ for any $k = 1, 2,3$. The regret bound depends on the size of the set of points, and the complexity of the algorithm.   ","This paper studies the problem of finding the centroid of a convex set of a set of d-dimensional values, where d is the number of items in the set, and the value of each item is the size of the list of items. The authors show that this problem can be solved by a linear bandit algorithm with O(d log d) regret. They also provide a nearly tight algorithm for solving this problem.   "
15962,SP:abe83c7e0bcf4829742609d709637e2f84d8a4d9,"Automated machine learning ( AutoML ) USED-FOR data scientists. combinators USED-FOR compositional code. orthogonal combinators USED-FOR machinelearning operators. orthogonal combinators USED-FOR pipelines. machinelearning operators PART-OF pipelines. search spaces USED-FOR AutoML optimizers. hyperparameter schemas CONJUNCTION search spaces. search spaces CONJUNCTION hyperparameter schemas. translation scheme USED-FOR search spaces. hyperparameter schemas USED-FOR translation scheme. pipelines USED-FOR translation scheme. Lale HYPONYM-OF sklearn - compatible AutoML library. user study EVALUATE-FOR it. Method are machine learning, AutoML, and functional programming. OtherScientificTerm is non - compositional code changes. ","This paper proposes a method for composing compositional code in AutoML. The method is based on orthogonal combinators, which can be used to construct compositional pipelines of machine learning operators. The main idea is to translate the hyperparameter schemas of AutoML optimizers into a set of search spaces, which are then used to map hyperparameters of the optimizers to the search spaces of the compositional codes. The authors show that the proposed method can be combined with existing AutoML libraries and achieves state-of-the-art performance on several benchmarks.","This paper proposes a new way to construct compositional code for AutoML. The idea is to use orthogonal combinators (e.g., orthonormal combinators) to construct a compositional pipeline of machine learning operators. The authors propose a translation scheme to translate the hyperparameter schemas into search spaces, which are then used to construct the search spaces. The proposed method is evaluated on synthetic data and real-world datasets."
16011,SP:0d7f1cae577ed598048b64617e85ca6bd5c6d7fa,"small datasets USED-FOR neural network weights. problem - byproblem basis FEATURE-OF pattern of sparsity. generalization CONJUNCTION interference. interference CONJUNCTION generalization. interference FEATURE-OF few - shot and continual learning problems. generalization EVALUATE-FOR selective sparsity. meta - learning USED-FOR adaptable features. inductive bias USED-FOR meta - learning systems. Method are weight initialization, learning algorithm, sparse learning, and sparse gradient descent. Metric is generalization error. OtherScientificTerm are patterned sparsity, and learning rates. ","This paper studies the generalization error of meta-learning with sparse gradient descent in few-shot and continual learning settings. The authors show that the generalisation error depends on the pattern of sparsity in the training data. They show that this pattern is determined on the problem-by-problem basis, and show that it is a function of the number of training samples and the learning rate. They also show that in the case of sparse gradients, this pattern can be controlled by the learning rates.   ","This paper studies the generalization error of meta-learning with sparse gradient descent. The authors consider the problem of few-shot and continual learning, where the number of training samples is small and the training data is sparse. They show that the generalisation error is bounded by the pattern of sparsity in the training dataset. They also show that there is interference between the generalized generalization and the interference of the interference. The interference is due to the fact that the interference depends on the size of the dataset. The paper also provides a theoretical analysis for the interference and generalization of sparse gradients. "
16060,SP:05037e1850003a725a466b64d3e32aa2aed458fb,"shared response modeling HYPONYM-OF multi - view learning problem. multi - set canonical correlation analysis USED-FOR unmixing matrices. sampling noise USED-FOR Multiset CCA. joint diagonalization USED-FOR approach. joint diagonalization USED-FOR Multiset CCA. ShICA - ML HYPONYM-OF maximum - likelihood method. non - Gaussianity USED-FOR ShICA - J. maximum - likelihood method USED-FOR non - Gaussianity. maximum - likelihood method USED-FOR ShICA - J. second - order statistics USED-FOR ShICA - J. method USED-FOR shared components estimation. ShICA USED-FOR shared components estimation. ShICA USED-FOR method. ShICA COMPARE alternatives. alternatives COMPARE ShICA. fMRI and MEG datasets EVALUATE-FOR ShICA. OtherScientificTerm are common components, shared independent components, additive Gaussian noise, and noise variances. Method is Shared Independent Component Analysis ( ShICA ). Generic is model. ","This paper proposes a new method for multi-view learning in shared response modeling. The proposed method, called ShICA, is based on the canonical correlation analysis (CCA) framework, which is used to unmix the unmixing matrices in a multi-set CCA. The authors propose to use additive Gaussian noise to estimate the shared independent components, and then use a maximum likelihood method based on second-order statistics to estimate shared components estimation. The experimental results on fMRI and MEG datasets demonstrate the effectiveness of the proposed method.","This paper proposes a new method for multi-view learning in shared response modeling. The proposed method is based on a multi-set canonical correlation analysis for unmixing matrices. The authors propose to use additive Gaussian noise as a sampling noise to improve the performance of the model. They also propose a new maximum likelihood method for ShICA-ML, which is a variant of ShICA. The method is evaluated on fMRI and MEG datasets. "
16109,SP:44dd1faa1813c433fd7581d05cae3df440bfb93e,"self - play ( SP ) CONJUNCTION population play ( PP ). population play ( PP ) CONJUNCTION self - play ( SP ). multi - agent reinforcement learning techniques USED-FOR agents. population play ( PP ) HYPONYM-OF multi - agent reinforcement learning techniques. self - play ( SP ) HYPONYM-OF multi - agent reinforcement learning techniques. model USED-FOR human - aware ” agents. behavioral cloning play ” CONJUNCTION BCP. BCP CONJUNCTION behavioral cloning play ”. behavioral cloning play ” HYPONYM-OF human - aware ” agents. BCP HYPONYM-OF human - aware ” agents. behavioral cloning USED-FOR human model. generalization EVALUATE-FOR agents. agents USED-FOR human co - players. approach USED-FOR agents. generalization EVALUATE-FOR approach. multi - agent approaches USED-FOR competitive domains. self - play agents USED-FOR agent partner. FCP agents COMPARE SP. SP COMPARE FCP agents. SP CONJUNCTION PP. PP CONJUNCTION SP. PP CONJUNCTION BCP. BCP CONJUNCTION PP. FCP agents COMPARE BCP. BCP COMPARE FCP agents. FCP agents COMPARE PP. PP COMPARE FCP agents. Material is human data. Generic are it, and method. Method are Fictitious Co - Play ( FCP ), and two - player collaborative cooking simulator. OtherScientificTerm is subjective preference. ","This paper proposes a novel method for learning human-aware agents in cooperative multi-agent reinforcement learning. The proposed method, called Fictitious Co-Play (FCP), is based on the idea that agents learn a model of human preferences that can be used to select the best partner to play with. The method is evaluated on a two-player collaborative cooking simulator, where it is shown to outperform self-play and population play.  ","This paper proposes a new method for multi-agent cooperative multi-player reinforcement learning (MCRL) called Fictitious Co-Play (FCP). FCP is an extension of population play (PP) and self-play (SP) that aims to improve the generalization ability of agents in competitive domains. In particular, the authors propose to use a human-aware model to predict the preferences of the agent partner. The human model is trained by cloning the behavior of the human partner to the agent. The agents are then trained to play with the human model. Experiments are conducted on a two-player collaborative cooking simulator and show that FCP outperforms SP and population play. "
16158,SP:21c84bd720b1e90ea0f88fbf8fd24dbcb49b547c,"method USED-FOR cooperative multi - agent reinforcement learning. discrete and continuous action spaces FEATURE-OF cooperative multi - agent reinforcement learning. deep deterministic policy gradients USED-FOR policies. approach USED-FOR policies. MADDPG HYPONYM-OF multi - agent actor - critic method. deep deterministic policy gradients USED-FOR approach. QMIX HYPONYM-OF multi - agent Q - learning algorithm. FACMAC USED-FOR centralised but factored critic. per - agent utilities CONJUNCTION joint action - value function. joint action - value function CONJUNCTION per - agent utilities. joint action - value function FEATURE-OF centralised but factored critic. per - agent utilities PART-OF centralised but factored critic. non - linear monotonic function USED-FOR centralised but factored critic. non - linear monotonic function USED-FOR joint action - value function. it USED-FOR tasks. representational capacity USED-FOR it. monolithic, or monotonically factored critics USED-FOR tasks. joint action space FEATURE-OF centralised policy gradient estimator. centralised policy gradient estimator USED-FOR FACMAC. multi - agent MuJoCo benchmark CONJUNCTION StarCraft II micromanagement tasks. StarCraft II micromanagement tasks CONJUNCTION multi - agent MuJoCo benchmark. multi - agent particle environments CONJUNCTION multi - agent MuJoCo benchmark. multi - agent MuJoCo benchmark CONJUNCTION multi - agent particle environments. multi - agent MuJoCo benchmark EVALUATE-FOR FACMAC. multi - agent particle environments EVALUATE-FOR FACMAC. StarCraft II micromanagement tasks EVALUATE-FOR FACMAC. FACMAC COMPARE baselines. baselines COMPARE FACMAC. FACMAC COMPARE MADDPG. MADDPG COMPARE FACMAC. MADDPG COMPARE baselines. baselines COMPARE MADDPG. OtherScientificTerm are critic, and coordinated policy changes. Method are nonmonotonic factorisation, and centralised critic. ","This paper proposes a novel multi-agent actor-critic method for cooperative multi-agents reinforcement learning. The proposed method, called FACMAC, is based on a centralised but factored critic, which is able to factorize the per-agent utilities and the joint action-value function. The centralised policy gradient estimator is used to estimate the policy gradient for each agent. The authors show that the proposed method achieves state-of-the-art performance on MuJoCo and StarCraft II tasks. ",This paper proposes a novel multi-agent actor-critic method for cooperative multi-player reinforcement learning. The centralised but factored critic is a non-linear monotonic factorisation of the joint action-value function between the agents. The proposed method is evaluated on the MuJoCo benchmark and StarCraft II micromanagement tasks.
16207,SP:1c8351b8a6cdf1212840388e19a596729b3bfda4,Hebbian plasticity USED-FOR storage. Hebbian plasticity CONJUNCTION attractor dynamics. attractor dynamics CONJUNCTION Hebbian plasticity. storage CONJUNCTION attractor dynamics. attractor dynamics CONJUNCTION storage. Hopfield networks PART-OF neuroscience. memory - augmented neural networks USED-FOR machine learning. key - value mechanism USED-FOR memory - augmented neural networks. augmented networks COMPARE variants. variants COMPARE augmented networks. three - factor plasticity rules USED-FOR basic key - value memory. network parameters USED-FOR rules. heteroassociative memory CONJUNCTION sequence learning. sequence learning CONJUNCTION heteroassociative memory. continual recall CONJUNCTION heteroassociative memory. heteroassociative memory CONJUNCTION continual recall. network COMPARE Hopfield networks. Hopfield networks COMPARE network. network USED-FOR continual recall. network USED-FOR heteroassociative memory. Hopfield networks USED-FOR autoassociative memory tasks. network USED-FOR sequence learning. autoassociative memory tasks EVALUATE-FOR network. Hopfield network USED-FOR model of biological long - term memory. ,"This paper proposes a new memory-augmented neural network architecture based on Hopfield networks. The key idea is to use Hebbian plasticity and attractor dynamics to learn a key-value mechanism for memory. The proposed method is evaluated on continual recall, sequence learning and hetero-associative memory tasks.  ","This paper proposes a new memory-augmented neural network architecture for biological long-term memory. The key idea is to use Hebbian plasticity rules to learn a key-value mechanism to store the key-values of a sequence of neurons. The proposed network is tested on a variety of tasks, including continual recall, hetero-associative memory, sequence learning, and continual recall. The experimental results show that the proposed network outperforms the state-of-the-art in all three tasks. "
16256,SP:7ad6da2c63859d64970e9b35326e9ceab48add47,"Pairwise learning HYPONYM-OF learning tasks. bipartite ranking CONJUNCTION metric learning. metric learning CONJUNCTION bipartite ranking. It USED-FOR machine learning tasks. metric learning HYPONYM-OF machine learning tasks. bipartite ranking HYPONYM-OF machine learning tasks. approach USED-FOR streaming data. streaming data USED-FOR pairwise learning. online gradient descent ( OGD ) algorithm USED-FOR approach. stochastic and online gradient descent methods USED-FOR pairwise learning. optimization CONJUNCTION generalization error bounds. generalization error bounds CONJUNCTION optimization. stability results CONJUNCTION optimization. optimization CONJUNCTION stability results. generalization error bounds USED-FOR smooth and nonsmooth problems. generalization error bounds USED-FOR convex and nonconvex. convex and nonconvex CONJUNCTION smooth and nonsmooth problems. smooth and nonsmooth problems CONJUNCTION convex and nonconvex. optimization CONJUNCTION generalization analysis. generalization analysis CONJUNCTION optimization. techniques USED-FOR optimization. techniques USED-FOR generalization analysis. generalization bounds USED-FOR OGD. buffering set USED-FOR OGD. buffering set USED-FOR generalization bounds. algorithms USED-FOR differentially private SGD algorithms. differentially private SGD algorithms USED-FOR pairwise learning. stability analysis USED-FOR differentially private SGD algorithms. algorithms CONJUNCTION stability analysis. stability analysis CONJUNCTION algorithms. OtherScientificTerm are loss function, scalability issue, and gradient direction. Metric is storage and computational complexity. ",This paper studies the problem of pairwise learning with streaming data. The authors propose an online gradient descent (OGD) algorithm for this problem. The main contribution of this paper is to provide generalization bounds for the OGD algorithm. The generalization error bounds are obtained for convex and non-convex problems and smooth and nonsmooth problems.,"This paper studies the generalization of online gradient descent (OGD) for pairwise learning with streaming data. The authors provide generalization bounds for convex and nonconvex problems, smooth and nonsmooth problems, and smooth and non-smooth problems. The generalization error bounds are based on stability analysis, optimization, and generalization analysis. They also provide a generalization bound for differentially private SGD algorithms. "
16305,SP:cb11dacc930d71a616ee2fbe4acfae030f9dca59,"REDO HYPONYM-OF class - agnostic framework. class - agnostic framework USED-FOR Dynamic Objects. RGBD or calibrated videos USED-FOR REDO. RGBD or calibrated videos USED-FOR class - agnostic framework. rigid motion CONJUNCTION non - rigid motion. non - rigid motion CONJUNCTION rigid motion. non - rigid motion CONJUNCTION articulation. articulation CONJUNCTION non - rigid motion. occlusion CONJUNCTION camera settings. camera settings CONJUNCTION occlusion. unified framework USED-FOR problem setting. articulation HYPONYM-OF object dynamics. rigid motion HYPONYM-OF object dynamics. non - rigid motion HYPONYM-OF object dynamics. aggregated temporal visual cues USED-FOR canonical 4D implicit function. 4D transformation module USED-FOR object dynamics. REDO COMPARE dynamic reconstruction methods. dynamic reconstruction methods COMPARE REDO. Generic are modules, and component. Material is real - world video data 3DPW. ","This paper proposes REDO, a method for dynamic object reconstruction from 3D video data. The method is based on a unified framework that can handle both rigid motion and non-rigid motion. The main idea is to learn a canonical 4D implicit function that can be used to estimate the 4D transformation of the object dynamics. The proposed method is evaluated on real-world video data 3DPW.   ","This paper proposes a new framework for dynamic object reconstruction based on 3D video data. The core idea is to use a 4D implicit function to model the dynamics of an object, which can be either rigid motion, non-rigid motion, or non-rigid motion. The implicit function is composed of two components: 1) an implicit function that maps the object dynamics to its position in the scene, and 2) a transformation module that reconstructs the 4D dynamics of the object. The authors show that the proposed method outperforms the state-of-the-art in terms of accuracy and accuracy of the reconstruction results."
16354,SP:8ae97752e74b4395774575009031abcb6ba5cea7,"fixed stepsize FEATURE-OF linear stochastic approximation ( LSA ) algorithms. methods USED-FOR machine learning tasks. high probability bounds USED-FOR LSA. covariance matrices PART-OF central limit theorems. Method are random estimates, polynomial concentration bounds, and Gaussian or exponential high probability bounds. OtherScientificTerm are products of matrices, stepsize, and random matrices. Generic is bounds. ","This paper studies the high probability bounds for linear stochastic approximation (LSA) algorithms with fixed stepsize in the presence of a fixed product of products of matrices. In particular, the authors provide polynomial concentration bounds on the product of the covariance matrices, and show that the polynomials of the products of the matrices are Gaussian or exponential in the number of samples. The authors also show that under certain assumptions on the products, they can obtain Gaussian/exponential high-probability bounds.  ","This paper studies high-probability bounds for linear stochastic approximation (LSA) algorithms with fixed stepsize. The main contribution of the paper is to study the central limit of theorems for LSA with fixed step size. The central limit is the product of the covariance matrices of a set of random matrices. The authors show that the product is polynomial in terms of the concentration of the products of matrices, which is a result of the fact that the stepsize of LSA is fixed. They also show that polynomials of the product can be used to obtain Gaussian or exponential high probability bounds.  "
16403,SP:86c1e937755e35efafecc09dfe2606ffb1653a41,"options framework USED-FOR temporal abstraction. options framework USED-FOR reinforcement learning. temporal abstraction USED-FOR reinforcement learning. discounted Markov decision processes ( MDPs ) CONJUNCTION average - reward MDPs. average - reward MDPs CONJUNCTION discounted Markov decision processes ( MDPs ). samplebased planning variants PART-OF learning algorithms. intra - option algorithms CONJUNCTION samplebased planning variants. samplebased planning variants CONJUNCTION intra - option algorithms. algorithms CONJUNCTION convergence proofs. convergence proofs CONJUNCTION algorithms. those USED-FOR algorithms. those USED-FOR convergence proofs. Four - Room domain EVALUATE-FOR algorithms. OtherScientificTerm is option - interrupting behavior. Method are discounted, and average - reward formulation. ",This paper studies the options framework for reinforcement learning in the four-room setting. The authors propose to use discounted MDPs and average-reward MDP to model the dynamics of the MDP. They show how to use options to learn a policy in this setting. They also provide convergence results for the proposed algorithms. ,"This paper proposes a new options framework for reinforcement learning in the context of discounted Markov decision processes (MDPs) and average-reward MDPs. In particular, the authors introduce a new option-interrupting behavior (i.e., option-injecting behavior) that can be used to learn a sample-based planning algorithm. The authors also provide convergence results for the proposed algorithm. "
16452,SP:7e4e1e20e7c253d02c6ae58457fb30029f130f0c,"Visual Transformers ( VTs ) COMPARE Convolutional networks ( CNNs ). Convolutional networks ( CNNs ) COMPARE Visual Transformers ( VTs ). CNNs COMPARE VTs. VTs COMPARE CNNs. VTs USED-FOR global relations between image elements. representation capacity FEATURE-OF they. models COMPARE common CNNs. common CNNs COMPARE models. local properties FEATURE-OF visual domain. local properties USED-FOR VTs. local properties PART-OF CNN architectural design. small training set regime FEATURE-OF robustness. robustness EVALUATE-FOR VTs. images USED-FOR auxiliary selfsupervised task. VTs USED-FOR spatial relations. task USED-FOR VT training. task USED-FOR VTs. task CONJUNCTION ( supervised ) training. ( supervised ) training CONJUNCTION task. it PART-OF VTs. accuracy EVALUATE-FOR VTs. VTs EVALUATE-FOR method. accuracy EVALUATE-FOR method. OtherScientificTerm are convolutional inductive bias, and computational overhead. Material is ImageNet. Method is VTs - Drloc. ",This paper proposes to use visual transformers (VTs) to improve the robustness of convolutional neural networks (CNNs) in image classification tasks. The authors propose to use an auxiliary self-supervised task called Drloc to learn spatial relations between image elements. The proposed method is evaluated on ImageNet and CIFAR-10 datasets.   ,"This paper proposes a method to improve the robustness of visual transformers (VTs) in the small training set regime. The authors propose a new task called Drloc, which is an auxiliary self-supervised task for training VTs. The proposed method is evaluated on ImageNet and CIFAR-10 datasets, and it outperforms the state-of-the-art. "
16501,SP:0132ef17585e293b23e9dc45189c0989d829b52a,datasets USED-FOR Label - free alignment. Hyperbolic spaces USED-FOR informative representations of hierarchical data. geometric approach USED-FOR label - free alignment of hierarchical datasets. translation CONJUNCTION scaling. scaling CONJUNCTION translation. scaling CONJUNCTION rotation. rotation CONJUNCTION scaling. Riemannian geometry USED-FOR Lorentz model of hyperbolic space. Riemannian geometry USED-FOR HPA. theoretical properties CONJUNCTION stability. stability CONJUNCTION theoretical properties. stability CONJUNCTION computational efficiency. computational efficiency CONJUNCTION stability. theoretical properties EVALUATE-FOR HPA. gene expression and mass cytometry data FEATURE-OF batch correction tasks. batch correction tasks EVALUATE-FOR its. methods USED-FOR label - free alignment in hyperbolic spaces. data USED-FOR unsupervised batch effect removal. Method is hyperbolic Procrustes analysis ( HPA ). Generic is components. Task is alignment. OtherScientificTerm is hyperbolic spaces. ,"This paper proposes a new method for label-free alignment in hyperbolic spaces. The proposed method is based on a new geometric approach for label free alignment of hierarchical datasets. The main contribution of the paper is to introduce a new Riemannian geometry to model the Lorentz model of hyperboloid space. Theoretical properties of the proposed method are proved, and experiments on batch correction tasks on gene expression and mass cytometry data show its effectiveness. ","This paper proposes a new method for label-free alignment of hierarchical datasets in hyperbolic spaces. The proposed method is based on the Lorentz model of hyperbolics, which is a Riemannian geometry. Theoretical properties of the proposed method include stability, computational efficiency, and computational efficiency. Experiments are conducted on two datasets: gene expression and mass cytometry data."
16550,SP:3580ac64f09e3021de5d4c92411bcc0f3c5d10f3,"Privacy - protected microdata USED-FOR differentially private algorithm. logarithmic factor FEATURE-OF accuracy. accuracy EVALUATE-FOR differentially private query answering systems. sum query CONJUNCTION point queries. point queries CONJUNCTION sum query. noisy answers USED-FOR sum query. noisy answers USED-FOR point queries. log(d ) factor COMPARE O(d ) factor. O(d ) factor COMPARE log(d ) factor. log(d ) factor USED-FOR point queries. O(d ) factor USED-FOR sum query. lower bounds USED-FOR pure, approximate, and concentrated differential privacy. Material are microdata, and benchmark datasets. Method are uncertainty principle, pure differential privacy, and mitigation strategies. OtherScientificTerm is microdata requirement. ","This paper studies the problem of differentially private query answering in the presence of privacy-protected microdata. The authors show that under certain assumptions on the uncertainty principle, the accuracy of the query answering system can be improved by a logarithmic factor in terms of the accuracy with respect to the number of queries. The main contribution of the paper is a lower bound on the error of the sum query and the point query, which shows that the error is bounded by a factor of log(d) in the case of private microdata, and log(log(d)) for point queries.   ","This paper studies the problem of differentially private query answering in the context of privacy-protected microdata. The authors provide a lower bound on the logarithmic factor of the accuracy of a query answering system under the uncertainty principle. The lower bounds are based on the assumption that the data is collected from a differentially protected dataset. They show that under certain assumptions on the uncertainty of the data, the upper bounds are lower bounds for pure, approximate and concentrated differential privacy."
16599,SP:c0e64dc8acfaed3e4d7745af12fd34003d0e5017,"sparse reward CONJUNCTION inefficient exploration. inefficient exploration CONJUNCTION sparse reward. inefficient exploration FEATURE-OF long - horizon tasks. RL CONJUNCTION planning. planning CONJUNCTION RL. path - planner CONJUNCTION RL agent. RL agent CONJUNCTION path - planner. dense feedback USED-FOR curriculum of tree - structured sub - tasks. RL agent USED-FOR dense feedback. dense feedback USED-FOR path - planner. planner USED-FOR long - horizon task. easy - to - hard curriculum USED-FOR planner. bottom - up traversal of the tree USED-FOR RL agent. RL agent CONJUNCTION planner. planner CONJUNCTION RL agent. mutual training USED-FOR CO - PILOT. SAC CONJUNCTION PPO. PPO CONJUNCTION SAC. RL CONJUNCTION planning ( RRT *. planning ( RRT * CONJUNCTION RL. CO - PILOT COMPARE RL. RL COMPARE CO - PILOT. CO - PILOT COMPARE combination ( SoRB ). combination ( SoRB ) COMPARE CO - PILOT. navigation and continuous control tasks EVALUATE-FOR combination ( SoRB ). SAC HYPONYM-OF RL. navigation and continuous control tasks EVALUATE-FOR CO - PILOT. PPO HYPONYM-OF RL. success rate CONJUNCTION sample efficiency. sample efficiency CONJUNCTION success rate. sample efficiency EVALUATE-FOR CO - PILOT. success rate EVALUATE-FOR CO - PILOT. Method are Goal - conditioned reinforcement learning ( RL ), Planning, environment model, and planning policy. OtherScientificTerm are dense reward / guidance, tree of sub - tasks, sub - tasks, and RRT *. Generic is task. ",This paper proposes a method to combine goal-conditioned reinforcement learning (RL) and planning to solve long-horizon tasks with sparse rewards. The main idea is to learn a curriculum of tree-structured sub-tasks that are easy-to-hard for the RL agent to solve. The curriculum is then used to guide the planning policy to reach the goal. The proposed method is evaluated on a variety of navigation and continuous control tasks.   ,"This paper proposes a curriculum of tree-structured sub-tasks for goal-conditioned reinforcement learning (RL) and planning (RRT). The curriculum consists of an easy-to-hard curriculum of task-specific sub-task-specific tasks, and a planner-RL agent. The curriculum is built on top of a bottom-up traversal of the tree of tasks, which is done by the planner and the RL agent in a top-down manner. The RL agent is trained to navigate the tree, and the planner is trained on the bottom-down traversal. The proposed curriculum is evaluated on SAC, PPO, and continuous control tasks. "
16648,SP:9911693a04a300b5a93634fb0267ef83e5489d77,"black box explanations USED-FOR model credibility. techniques USED-FOR explanations. hyper - parameter tuning USED-FOR methods. Bayesian framework USED-FOR generating local explanations. LIME CONJUNCTION KernelSHAP. KernelSHAP CONJUNCTION LIME. credible intervals USED-FOR feature importances. real world datasets CONJUNCTION user studies. user studies CONJUNCTION real world datasets. OtherScientificTerm are local explanations, and feature importance. Generic are framework, and uncertainty. ","This paper proposes a Bayesian framework for generating local explanations for black-box models. The proposed framework is based on the idea that feature importance is a function of the uncertainty of the model, and the uncertainty is estimated using a Bayes framework. The authors show that the proposed framework can be used to estimate the importance of each feature in the black box model.   The authors also show that their framework is able to generate credible intervals for feature importances. ","This paper proposes a Bayesian framework for generating local explanations for black-box black box explanations. The main idea is to use the fact that there is uncertainty about the importance of each feature in the black box explanation, and to use this uncertainty to estimate the feature importance of the model. The authors show that this uncertainty can be used to generate credible intervals for feature importances, which are then used to estimate feature importance. They also show that these intervals can also be used for hyper-parameter tuning, which can help to improve model credibility."
16697,SP:5efb4b81bd37c70640e8768e9dfb5bba14a0cfb8,"energy - efficient neural networks CONJUNCTION hardware accelerations. hardware accelerations CONJUNCTION energy - efficient neural networks. multiplications PART-OF convolutional neural networks ( CNNs ). Adder neural networks ( ANNs ) USED-FOR low energy cost. ANNs COMPARE CNNs. CNNs COMPARE ANNs. accuracy EVALUATE-FOR ANNs. accuracy EVALUATE-FOR CNNs. ANNs CONJUNCTION CNNs. CNNs CONJUNCTION ANNs. knowledge distillation HYPONYM-OF training tricks. filters CONJUNCTION features. features CONJUNCTION filters. similarity measurement FEATURE-OF features. similarity measurement FEATURE-OF filters. similarity measurement USED-FOR property difference. unordered heavy tails PART-OF ANNs. classification EVALUATE-FOR ANNs. feature distributions PART-OF loss function. method USED-FOR heavy tails. angle - based constraint USED-FOR diversity of tails. method USED-FOR ANNs. heavy tails PART-OF ANNs. angle - based constraint USED-FOR distribution parameters. classifier USED-FOR method. approach USED-FOR ANNs. benchmarks EVALUATE-FOR approach. benchmarks EVALUATE-FOR distributions. OtherScientificTerm are fatter tails, feature space, Multivariate Skew Laplace distributions, and ANN features. ",This paper proposes a method to improve the performance of deep neural networks (DNNs) by using unordered heavy tails in the feature space. The method is based on an angle-based constraint on the number of heavy tails that can be added to the loss function. The authors show that the proposed method can improve the accuracy of DNNs on ImageNet and CIFAR-10/100. ,This paper proposes a new method to improve the energy efficiency of deep neural networks (DNNs) by reducing the number of heavy tails in the feature space. The main idea is to use an angle-based constraint on the distribution parameters of the classifier to encourage diversity in the distribution of the weights. The method is evaluated on a variety of datasets and shows that it can improve the performance of DNNs. 
16746,SP:cbccb65457564992d534504c0d060da44cafce8c,"gradient descent phenomenon USED-FOR learning proclivity. learning proclivity FEATURE-OF over - parameterized neural networks. features USED-FOR task. feature imbalances FEATURE-OF neural networks. learning dynamics USED-FOR imbalance. learning dynamics USED-FOR gradient descent. guarantees USED-FOR regularization method. formalism USED-FOR regularization method. regularization method USED-FOR feature learning dynamics. formalism USED-FOR guarantees. accuracy EVALUATE-FOR regularization method. robustness EVALUATE-FOR regularization method. OtherScientificTerm are Gradient Starvation, predictive features, statistical structure, and gradient starvation. Metric is cross - entropy loss. Method is Dynamical Systems theory. ",This paper studies the gradient descent phenomenon in over-parameterized neural networks with feature imbalances. The authors propose a regularization method based on Dynamical Systems theory to regularize the learning dynamics of feature learning dynamics. Theoretical guarantees are given for gradient starvation and cross-entropy loss. Empirical results show that the proposed method improves the accuracy and robustness of the model. ,"This paper studies the phenomenon of gradient starvation in over-parameterized neural networks. In particular, the authors show that the learning proclivity of a neural network is dependent on the number of features in the training set, and propose a regularization method for this phenomenon. The authors provide a formalization of the gradient starvation phenomenon in Dynamical Systems theory, and show that it can be reduced to a cross-entropy loss, which can be used to improve the robustness of the neural network.  "
16795,SP:8f6fe37cb0a332b66e10cc00261a44622841c8c6,"Deep reinforcement learning USED-FOR superhuman AI. superhuman AI USED-FOR competitive games. Go and StarCraft HYPONYM-OF competitive games. learning techniques USED-FOR AI teammate. AI teammate USED-FOR human - machine collaborative games. AI teammates COMPARE those. those COMPARE AI teammates. subjective metrics of trust EVALUATE-FOR those. objective team performance EVALUATE-FOR AI teammates. AI agents PART-OF cooperative card game Hanabi. interpretability CONJUNCTION trust. trust CONJUNCTION interpretability. teamwork CONJUNCTION interpretability. interpretability CONJUNCTION teamwork. interpretability HYPONYM-OF subjective measures. trust HYPONYM-OF subjective measures. teamwork HYPONYM-OF subjective measures. AI design CONJUNCTION reinforcement learning benchmarking. reinforcement learning benchmarking CONJUNCTION AI design. subjective metrics of human - AI teaming CONJUNCTION objective task performance. objective task performance CONJUNCTION subjective metrics of human - AI teaming. Method are rule - based and learning - based agents, rule - based AI teammate ( SmartBot ), and learning - based agent. OtherScientificTerm are game score, and human - AI team performance. Metric is subjective metrics. ","This paper presents an experiment on cooperative cooperative card game Hanabi, where humans and AI agents are playing against each other in a cooperative game. The authors evaluate the performance of AI agents in terms of trust, teamwork, interpretability, and interpretability-based trust metrics. They show that agents trained with reinforcement learning outperform humans in Hanabi by a large margin. They also show that learning-based and rule-based AI agents outperform human-AI teammates in terms both of these metrics.","This paper presents a study of human-AI teaming in cooperative cooperative card game Hanabi, where humans and AI agents are competing against each other in a cooperative game. The authors show that humans outperform their AI teammates in terms of trust, teamwork, and interpretability. They also show that the human team outperforms the AI team by a large margin. The main contribution of the paper is that it proposes a new way to measure trust and teamwork in cooperative games, which is based on the game score. They show that human team performance in Hanabi is better than the performance of AI teammates on the objective team performance."
16844,SP:2a05e333fc1a14057515ef3addde9a40152373db,"visual question generation ( VQG ) USED-FOR human - like neural questions. image CONJUNCTION side information. side information CONJUNCTION image. side information USED-FOR human - like neural questions. image USED-FOR human - like neural questions. double visual and answer hints USED-FOR model. rule - based similarity matching method USED-FOR candidate visual hints. learning approach USED-FOR double - hints based VQG. weakly supervised learning problem USED-FOR learning approach. benchmark datasets EVALUATE-FOR method. automatic machine metrics CONJUNCTION human evaluation. human evaluation CONJUNCTION automatic machine metrics. method COMPARE approaches. approaches COMPARE method. benchmark datasets EVALUATE-FOR approaches. automatic machine metrics HYPONYM-OF metrics. human evaluation HYPONYM-OF metrics. human evaluation EVALUATE-FOR method. metrics EVALUATE-FOR approaches. metrics EVALUATE-FOR method. automatic machine metrics EVALUATE-FOR method. OtherScientificTerm are uninformative and non - referential questions, visual hints, salient visual regions of interest, predicted salient visual regions of interest, and quality of predicted visual hints. Generic is they. Method is generation procedure. ",This paper proposes a method for visual question generation (VQG) based on double visual and answer hints. The method is based on a rule-based similarity matching method to match candidate visual hints with the answer. The proposed method is evaluated on three benchmark datasets and compared with several baselines.   ,This paper proposes a method for visual question generation (VQG) where the goal is to generate human-like neural questions that are uninformative and non-referential. The proposed method is based on a rule-based similarity matching method that matches candidate visual hints with candidate answer hints. The method is evaluated on a variety of benchmark datasets. 
16908,SP:15756d6ef47b39ded404acea2135c93bd5ee1062,"Label noise CONJUNCTION class imbalance. class imbalance CONJUNCTION Label noise. label noise CONJUNCTION class imbalance. class imbalance CONJUNCTION label noise. Generalized Data Weighting ( GDW ) USED-FOR class imbalance. Generalized Data Weighting ( GDW ) USED-FOR label noise. class level FEATURE-OF gradients. gradients USED-FOR Generalized Data Weighting ( GDW ). GDW USED-FOR loss gradient. chain rule USED-FOR GDW. GDW COMPARE instance weighting methods. instance weighting methods COMPARE GDW. GDW USED-FOR class - level weights. computational cost EVALUATE-FOR instance weighting methods. gradient descent step USED-FOR class - level weights. class - level weights USED-FOR GDW. gradient descent step USED-FOR GDW. GDW COMPARE state - of - the - art methods. state - of - the - art methods COMPARE GDW. uniform noise setting FEATURE-OF CIFAR10. uniform noise setting EVALUATE-FOR GDW. CIFAR10 EVALUATE-FOR GDW. Material are real - world datasets, and clean and unbiased data. Generic is methods. OtherScientificTerm are class - level information, class - level gradients, and intermediate gradients. ","This paper proposes a generalized data weighting (GDW) method to deal with class imbalance and label noise in image classification. The proposed method is motivated by the observation that the class-level gradients of gradients at the class level can be biased in the presence of label noise. To deal with this issue, the authors propose to use a chain rule to learn the class weights at the intermediate layers of the loss gradient. The authors show that the proposed GDW method is computationally efficient and achieves state-of-the-art performance on CIFAR-10 with uniform noise.",This paper proposes a generalized data weighting (GDW) method to address the problem of class imbalance and label noise. GDW is based on a chain rule to learn the class-level gradients of the data. The authors show that GDW outperforms the state-of-the-art methods on CIFAR-10 in the uniform noise setting. 
16972,SP:7a8f56a01bec51ebf70d9ff689005a62cccfe5c6,"it USED-FOR embodied agents. language USED-FOR embodied agents. sensorimotor modalities USED-FOR language. embodied agent FEATURE-OF spatio - temporal descriptions of behavioral traces. time - extended predicates CONJUNCTION spatio - temporal references. spatio - temporal references CONJUNCTION time - extended predicates. time - extended predicates PART-OF descriptions. spatio - temporal references PART-OF descriptions. architectural biases USED-FOR task. attention computations USED-FOR latter. multimodal Transformer architectures HYPONYM-OF models. generalization CONJUNCTION generalization. generalization CONJUNCTION generalization. generalization EVALUATE-FOR models. randomly held - out sentences USED-FOR generalization. generalization EVALUATE-FOR models. grammar primitives USED-FOR generalization. generalization HYPONYM-OF generalization. generalization HYPONYM-OF generalization. object identity FEATURE-OF attention computation. attention computation PART-OF Transformers. object identity USED-FOR generalization. code CONJUNCTION pretrained models. pretrained models CONJUNCTION code. OtherScientificTerm are Language, grounded language, spatio - temporal linguistic concepts, and truth function. Task are spatio - temporal language grounding task, and language - guided autonomous embodied agents. ","This paper proposes a spatio-temporal language grounding task to test the generalization ability of language-guided embodied agents on behavioral traces. The task is to generate a set of spatially-grounded descriptions of behavioral traces, which are then used to train a Transformer-based language model. The proposed method is evaluated on two tasks: (1) generating spatially grounded sentences and (2) generating time-extended predicates. The results show that the proposed method outperforms baselines on both tasks. ","This paper proposes a new language grounding task for embodied agents. The goal is to learn a language grounded on spatio-temporal descriptions of behavioral traces. The language grounding is based on the idea that the agent should be able to identify objects in the spatiotemporal description. The task is formulated as a multi-modal Transformer task, where the agent is asked to describe a set of objects (e.g., a person, a dog, a car, a building, etc.) in a sequence of sentences. The agent is then asked to identify the objects in this sequence. The model is trained using a Transformer architecture, and it is shown that the model is able to generalize well across different types of language grounding tasks."
17036,SP:3d4a9d439bc84c3b0e6600f6985a23bdf95cd67f,"detecting CONJUNCTION tracking. tracking CONJUNCTION detecting. tracking USED-FOR Multiple object tracking and segmentation. detecting USED-FOR Multiple object tracking and segmentation. single frame predictions USED-FOR segmentation mask. temporal dimension USED-FOR association problem. approaches USED-FOR association problem. temporal dimension USED-FOR approaches. Prototypical Cross - Attention Network ( PCAN ) USED-FOR online multiple object tracking and segmentation. rich spatio - temporal information USED-FOR online multiple object tracking and segmentation. rich spatio - temporal information USED-FOR Prototypical Cross - Attention Network ( PCAN ). cross - attention USED-FOR rich information. cross - attention USED-FOR PCAN. prototypical appearance module USED-FOR contrastive foreground and background prototypes. PCAN USED-FOR contrastive foreground and background prototypes. prototypical appearance module USED-FOR PCAN. PCAN COMPARE video instance tracking and segmentation competition winners. video instance tracking and segmentation competition winners COMPARE PCAN. Youtube - VIS and BDD100 K datasets EVALUATE-FOR video instance tracking and segmentation competition winners. Youtube - VIS and BDD100 K datasets EVALUATE-FOR PCAN. OtherScientificTerm are space - time memory, and prototypes. ",This paper proposes Prototypical Cross-Attention Network (PCAN) for online multiple object tracking and segmentation. The proposed method is based on a cross-attention network with a contrastive foreground and background prototype module.  The proposed approach is evaluated on Youtube-Viv and BDD100K datasets.  ,This paper proposes Prototypical Cross-Attention Network (PCAN) for online multiple object tracking and segmentation. PCAN is a cross-attention network that uses contrastive foreground and background prototypes for contrastive object tracking. The proposed approach is evaluated on Youtube-VIS and BDD100K datasets. 
17100,SP:1175ad16382b349ab1a39895150172d266abe571,optimization USED-FOR deep learning. it USED-FOR gradient descent. approximate numerical solution USED-FOR initial value problem of gradient flow. curvature FEATURE-OF gradient flow trajectory. gradient descent USED-FOR initial value problem of gradient flow. gradient descent USED-FOR approximate numerical solution. homogeneous activations FEATURE-OF deep neural networks. favorable curvature FEATURE-OF gradient flow trajectories. gradient descent USED-FOR they. gradient descent USED-FOR global minimum. deep linear neural networks USED-FOR gradient flow. random initialization USED-FOR gradient descent. gradient descent COMPARE gradient flow. gradient flow COMPARE gradient descent. deep neural networks USED-FOR gradient descent. step size FEATURE-OF gradient descent. OtherScientificTerm is Gradient flow. Metric is computational efficiency. Method is gradient flows. ,"This paper studies the gradient descent for the initial value problem of gradient flow in deep neural networks. The authors show that gradient descent can be used to approximate the approximate numerical solution of the gradient flow problem in deep linear neural networks with homogeneous activations. In particular, the authors prove that gradient flow trajectories with favorable curvature can be approximated by gradient descent with a small step size. They also show that the convergence rate of gradient descent to the global minimum is faster than gradient flow.","This paper studies the problem of gradient flow in deep neural networks with homogeneous activations. In particular, the authors study the convergence of the gradient flow to the global minimum of the initial value of a gradient flow. They show that gradient flow converges to a global minimum with a small step size. They also provide a theoretical analysis of the generalization of gradient descent to gradient flows."
17164,SP:b8412e9ce82ce92125fe7cd3aff7bea8b906d16e,"multi - armed bandits USED-FOR delayed and longterm impact of actions. action history USED-FOR learning. regret EVALUATE-FOR algorithm. OtherScientificTerm are delayed impact of actions, arm rewards, feedback loop, and delayed impacts of historical actions. Generic are setting, and techniques. Task is bandit setting. Metric is matching regret lower bound. Method are bandit literature, and fair algorithms. ","This paper considers the multi-armed bandit setting with delayed and long-term impact of actions in the feedback loop. In this setting, the delayed impact is measured in terms of the cumulative regret over the history of actions. The authors propose a regret lower bound of $O(1/\sqrt{T})$ where $T$ is the number of arms, and $T$, is the time horizon. They show that this lower bound matches the lower bound for fair algorithms. They also show that the regret of their algorithm matches the upper bound.","This paper studies the problem of multi-armed bandits with delayed and long-term impact of actions. The authors consider the setting of a multi-arm bandit setting, where each arm has two arms, and each arm receives a reward for each action. They show that the regret of a fair bandit algorithm can be reduced to a lower bound of $O(\sqrt{T})$, where $T$ is the number of actions taken by the bandit. They also show that this lower bound can be improved to $O(T)$ by a fair algorithm.   "
17228,SP:9c1d678dff5f609197dc3cfb67b841827f4a439a,"end - to - end solution USED-FOR video instance segmentation ( VIS ). transformers USED-FOR end - to - end solution. per - clip pipeline COMPARE per - frame methods. per - frame methods COMPARE per - clip pipeline. per - clip models USED-FOR frame - to - frame communications. benchmark sets EVALUATE-FOR method. method USED-FOR near - online inference. Method are Inter - frame Communication Transformers ( IFC ), and offline inference. OtherScientificTerm are overhead, concise memory tokens, and features. Material is YouTube - VIS 2019 val set. ","This paper proposes an end-to-end solution for video instance segmentation (VIS) using transformers. The proposed method is based on the Inter-frame Communication Transformers (IFC) architecture, which is a transformer-based architecture for video segmentation. The main contribution of the paper is the proposed IFC architecture.   The proposed model is evaluated on the YouTube-VIS 2019 val set, and the proposed method achieves state-of-the-art results. ","This paper proposes an end-to-end video segmentation method for video instance segmentation. The proposed method is based on the Inter-frame Communication Transformers (IFC) framework. The main idea of the method is to use a per-clip model to model the per-clip model, and then use the IFC model to communicate between the frames. The model is trained using the YouTube-VIS 2019 val set. The method is evaluated on three benchmark datasets, and the proposed method outperforms the state-of-the-art."
17292,SP:6c922eaa358f6fb9771690b1240e4f6f08a35b69,"vector - space representation USED-FOR machine learning applications. vector - space representation USED-FOR graph analysis. graph analysis CONJUNCTION machine learning applications. machine learning applications CONJUNCTION graph analysis. Graph embedding USED-FOR vector - space representation. Graph embedding USED-FOR graph. sampling of context nodes USED-FOR graph embedding methods. random walks USED-FOR sampling of context nodes. random walks HYPONYM-OF biased sampler. degree FEATURE-OF random walks. residual2vec HYPONYM-OF graph embedding method. random walks ’ bias USED-FOR graph embedding. random graphs USED-FOR residual2vec. link prediction CONJUNCTION clustering. clustering CONJUNCTION link prediction. debiasing USED-FOR structural properties. structural properties PART-OF graph embedding. clustering EVALUATE-FOR debiasing. link prediction EVALUATE-FOR debiasing. OtherScientificTerm are structural properties of graphs, node, and structural biases in graphs. Task is graph representation learning. ","This paper proposes a novel sampling strategy for graph embedding based on random walks. The proposed sampling strategy is based on the observation that the degree of random walks depends on the structure of the context nodes in the graph. The authors show that random walks are biased in terms of their degree, and propose a sampling strategy based on this biased sampling strategy. They show that the sampling strategy can be used for link prediction and clustering tasks.   ","This paper proposes a new graph embedding method, called residual2vec, which is based on the idea of ""debiasing"" (i.e., learning a vector-space representation of a graph by sampling from a random set of context nodes). The main idea is to use a biased sampler to sample from a set of random graphs, where the degree of each node is determined by the distance between it and the context node. The authors show that the bias is due to the fact that random walks are biased in the direction of the degree. They also show that this bias can be overcome by using a debiasing strategy, which can be used to learn a graph embeddings that is unbiased in the sense that it does not rely on the bias of the random walks. They show that their method can be applied to a variety of applications, including link prediction, clustering, and clustering."
17356,SP:851eac96135b577a5014166edcb43db6a190cf4b,"local differential privacy FEATURE-OF estimating non - linear functionals of discrete distributions. quadratic risk USED-FOR power sum functional. plug - in type estimators COMPARE MLE. MLE COMPARE plug - in type estimators. two - step procedure USED-FOR sequentially interactive case. α - LDP mechanisms CONJUNCTION estimators. estimators CONJUNCTION α - LDP mechanisms. private samples USED-FOR estimators. OtherScientificTerm are discrete distribution, non - interactive case, and privacy constraint. Method are privacy mechanisms ( PM ), multinomial model, and Gaussian model. ","This paper studies the problem of estimating non-linear functionals of discrete distributions under local differential privacy constraints. The main contribution is to show that the quadratic risk of estimating the power sum functional of the discrete distribution is bounded by a quadratically-robust function of the private samples.   The main contributions of the paper are:  1. The authors show that under certain assumptions on the privacy mechanisms (PM) and the multinomial model, the power-sum functional of a Gaussian distribution can be estimated in the non-interactive case, and in the interactive case, a two-step procedure is proposed.  2. They show that for the Gaussian model, they can estimate the power function of a discrete distribution with private samples with probability at least $1/\epsilon$.  3. They also show that if the privacy constraints are satisfied, then they can recover the power of the power. ","This paper studies the problem of estimating non-linear functionals of discrete distributions under local differential privacy. The main contribution of the paper is a two-step procedure for estimating the power sum functional of a discrete distribution under the privacy constraint of a multinomial model and a Gaussian model. In the non-interactive case, the paper proposes a two step procedure to estimate the nonlinear power sum of the discrete distribution. The paper also provides a theoretical analysis of the privacy constraints of the proposed method. "
17420,SP:a0408b54f88a26479f33f36bb27e0a675f637ccd,"directed graph USED-FOR learner ’s feedback. filtering CONJUNCTION label efficient classification. label efficient classification CONJUNCTION filtering. feedback graphs USED-FOR applications. label efficient classification HYPONYM-OF applications. filtering HYPONYM-OF applications. GAPPLETRON HYPONYM-OF online multiclass algorithm. arbitrary feedback graphs USED-FOR online multiclass algorithm. surrogate regret bounds USED-FOR algorithm. domination number HYPONYM-OF graph - theoretic parameter. full information case FEATURE-OF GAPPLETRON. surrogate regret EVALUATE-FOR GAPPLETRON. synthetic data EVALUATE-FOR algorithm. algorithm COMPARE baselines. baselines COMPARE algorithm. synthetic data EVALUATE-FOR baselines. feedback graphs EVALUATE-FOR algorithm. Task is online multiclass classification. OtherScientificTerm are bandit feedback, surrogate losses, prediction space, and time horizon. Generic are bounds, lower bound, and upper bounds. ","This paper studies the problem of online multiclass classification in which the learner is given a set of classes and the goal is to learn a classifier that minimizes the expected regret. The paper proposes a new algorithm called GAPPLETRON, which uses a directed graph as the feedback function to estimate the loss function. The main contribution is a surrogate regret bound of $O(1/\sqrt{T})$ where $T$ is the number of classes in the set. The regret bound is based on the domination number of the directed graph, which is an upper bound on the regret of the algorithm in the information-theoretic setting.   ","This paper studies the problem of online multiclass classification in which the learner is given a directed graph and the target class is a set of classes. The goal of the paper is to find a surrogate regret bound for the algorithm. The paper provides a lower and upper bound on the regret of GAPPLETRON. The upper bound is based on the domination number of the directed graph, which is a graph-theoretic parameter for the full information case. The lower bounds are based on a lower bound of the total regret of the algorithm, while the upper bounds depend on the number of classes and the time horizon of the prediction space."
17484,SP:490262589efce6fb10b913431ec6db8d4e5b2dec,"threshold cut USED-FOR single dimension ( feature ). decision tree USED-FOR it. decision tree USED-FOR k - clustering. algorithm USED-FOR explainable clustering. O(k ) CONJUNCTION O(k ). O(k ) CONJUNCTION O(k ). Ω(k ) lower bound USED-FOR k - means. Ω(log k ) lower bound USED-FOR k - medians. Ω(log k ) lower bound CONJUNCTION Ω(k ) lower bound. Ω(k ) lower bound CONJUNCTION Ω(log k ) lower bound. upper bounds COMPARE O(k ). O(k ) COMPARE upper bounds. O(k ) HYPONYM-OF upper bounds. OtherScientificTerm are cluster, k - means objective, upper and lower bounds, and ` p - norms. Metric is k - medians objective. ","This paper studies the problem of clustering k-means in the presence of a threshold cut, where the threshold cut is defined as a decision tree over a set of features, and the goal is to cluster the features in such a way that the distance between the clusters is bounded by a function of the number of samples. The main result is a lower bound on the k-medians objective, which is shown to be upper-bounded by $O(k)$ and $O(\sqrt{log k})$. The lower bound is also proved to be lower-bounds on the $k$-median objective. The upper bound is proved for the $p$-mean objective. ","This paper studies the problem of explainable clustering, i.e., k-means clustering for a set of k-medians, where the objective is to find a cluster of the k-Medians of a given set of features. The main contribution of the paper is a lower bound on the upper bound of the upper bounds of the lower bound for k-Means objective, which is based on the fact that the lower bounds are lower than the upper ones. The upper bounds are based on a decision tree, and the lower ones on the decision tree. The authors show that the upper and lower bounds can be obtained for the case when the objective of the clustering objective is k-mean objective. The lower bound is also proved for the setting where the goal is to cluster the k mean objective. "
17548,SP:6a9e47be710ddaf386bffc54d003d7dc2b67fdc3,"pre - trained language model ( PrLM ) USED-FOR downstream natural language processing tasks. multilingual PrLM USED-FOR limited resources. language universality USED-FOR limited resources. limited resources USED-FOR low - resource languages. multilingual PrLM USED-FOR downstream natural language processing tasks. language universality USED-FOR multilingual PrLM. plain text USED-FOR multilingual PrLMs. monolingual linguistic structure knowledge USED-FOR PrLMs. explicit universal dependency parsing CONJUNCTION implicit language modeling. implicit language modeling CONJUNCTION explicit universal dependency parsing. multilingual PrLM USED-FOR explicit universal dependency parsing. multilingual PrLM USED-FOR implicit language modeling. learned representation USED-FOR model. universal dependency parse FEATURE-OF Syntax. model COMPARE multilingual PrLM. multilingual PrLM COMPARE model. model COMPARE multilingual - BERT. multilingual - BERT COMPARE model. model COMPARE approach. approach COMPARE model. linguistic structure parsing datasets EVALUATE-FOR multilingual PrLM. linguistic structure parsing datasets EVALUATE-FOR model. multilingual - BERT HYPONYM-OF multilingual PrLM. OtherScientificTerm are universal linguistic structure clues, and PrLM interpretability. ",This paper proposes a multilingual pre-trained language model (PrLM) that uses language universality to improve performance on downstream tasks with limited resources in low-resource languages. The main idea is to use multilingual PrLM to learn universal linguistic structure clues for downstream tasks in languages with monolingual linguistic structure knowledge. The authors propose to use explicit universal dependency parsing and implicit language modeling to improve the performance of the model. The experiments show that the proposed method outperforms the multilingual-BERT model in terms of performance on language parsing tasks. ,"This paper proposes a multilingual pre-trained language model (PrLM) for multilingual language modeling. The main idea is to use monolingual linguistic structure knowledge to improve the interpretability of multilingual PrLMs. The model is trained using a combination of explicit universal dependency parsing and implicit language modeling, and it is shown to perform better than the multilingual-BERT model on a variety of downstream tasks."
17612,SP:94f4b65214a648cbc84f13beba45a825e2e9901a,"deep architecture USED-FOR vehicle routing problems ( VRPs ). Transformer HYPONYM-OF deep architecture. Transformer USED-FOR vehicle routing problems ( VRPs ). positional encoding ( PE ) method USED-FOR representing VRP solutions. learning improvement models USED-FOR VRP. it USED-FOR learning improvement models. Dual - Aspect Collaborative Transformer ( DACT ) USED-FOR embeddings. embeddings USED-FOR node and positional features. Transformer USED-FOR symmetry of VRP solutions. cyclic positional encoding ( CPE ) method USED-FOR Transformer. cyclic sequences HYPONYM-OF symmetry of VRP solutions. cyclic positional encoding ( CPE ) method USED-FOR positional features. curriculum learning strategy USED-FOR sample efficiency. Proximal Policy Optimization USED-FOR DACT. traveling salesman problem ( TSP ) CONJUNCTION capacitated vehicle routing problem ( CVRP ). capacitated vehicle routing problem ( CVRP ) CONJUNCTION traveling salesman problem ( TSP ). DACT USED-FOR capacitated vehicle routing problem ( CVRP ). DACT USED-FOR traveling salesman problem ( TSP ). DACT COMPARE Transformer based improvement models. Transformer based improvement models COMPARE DACT. synthetic and benchmark instances EVALUATE-FOR DACT. OtherScientificTerm are VRP solutions, and incompatible correlations. Generic are them, and ones. ","This paper proposes a novel transformer-based method for solving vehicle routing problems (VRPs). The proposed method is based on the Dual-Aspect Collaborative Transformer (DACT) architecture, which is an extension of the positional encoding (PE) method to represent VRP solutions. The authors propose a curriculum learning strategy to improve the sample efficiency of the proposed method. Experiments on TSP and CVRP show that DACT achieves state-of-the-art performance on both synthetic and benchmark instances.",This paper proposes a new transformer architecture for vehicle routing problems (VRPs). The proposed method is based on the Dual-Aspect Collaborative Transformer (DACT) architecture. DACT uses a cyclic positional encoding (CPE) method to encode the positional features of VRP solutions. The authors also propose a curriculum learning strategy to improve the sample efficiency of DACT. Experimental results show that DACT outperforms the state-of-the-art transformer-based VRP improvement models. 
17676,SP:e5c8680d8da9e7548fcb9bb5c073848eb80e1dd0,"Bayes error FEATURE-OF generative models. normalizing flows USED-FOR generative models. invertible transformation USED-FOR Bayes error. it USED-FOR Gaussian base distributions. Bayes error EVALUATE-FOR flow models. Holmes - Diaconis - Ross integration USED-FOR it. it USED-FOR Bayes error. synthetic datasets COMPARE benchmark datasets. benchmark datasets COMPARE synthetic datasets. Bayes error FEATURE-OF synthetic datasets. approach USED-FOR classification models. method USED-FOR benchmark datasets. Task is data - driven classification problem. Metric is classification error. OtherScientificTerm are data distribution, and intractable quantity. Generic are technique, and models. ",This paper studies the problem of estimating the Bayes error of generative models with normalizing flows. The authors propose to use an invertible transformation of the data distribution to estimate the error of the generative model. They show that the invertibility of the distribution can be expressed as a function of the parameters of the normalizing flow. They then show that this transformation can be used to estimate Bayes errors for the Gaussian base distributions. They also show that it is possible to use this transformation to approximate the Bayesian distribution of the training data.,This paper proposes a new way to estimate the Bayes error of a generative model for data-driven classification problems. The main idea is to use the Holmes-Diaconis-Ross integration (HDSR) to compute the invertible transformation of a Gaussian base distribution. The authors show that this transformation can be used to estimate an intractable quantity of the data distribution. They also show that their method can be applied to a variety of synthetic datasets.
17740,SP:2896679f0472522bc3334178cd7574494cf12b7b,"language modeling CONJUNCTION computer vision. computer vision CONJUNCTION language modeling. neural architectures USED-FOR language modeling. neural architectures USED-FOR computer vision. hyper - parameter choices CONJUNCTION training instability. training instability CONJUNCTION hyper - parameter choices. automated and architecture agnostic method USED-FOR initializing neural networks. GradInit HYPONYM-OF automated and architecture agnostic method. GradInit USED-FOR initializing neural networks. SGD CONJUNCTION Adam. Adam CONJUNCTION SGD. norm FEATURE-OF network layer. heuristic USED-FOR GradInit. numerical scheme USED-FOR variables. GradInit USED-FOR convolutional architectures. skip connections USED-FOR convolutional architectures. learning rates CONJUNCTION momentum coefficients. momentum coefficients CONJUNCTION learning rates. Adam CONJUNCTION SGD. SGD CONJUNCTION Adam. Transformer architecture USED-FOR machine translation. It USED-FOR Transformer architecture. learning rate warmup FEATURE-OF it. Adam USED-FOR learning rate warmup. SGD USED-FOR learning rate warmup. SGD USED-FOR it. Adam USED-FOR it. Generic are architectures, and schemes. OtherScientificTerm are network parameters, hyperparameters, scalar multiplier variable, and normalization layers. Method are architecture - specific initialization schemes, and neural networks. ","This paper proposes a new method for initialization of neural networks based on the scalar multiplier variable. The method is based on a heuristic heuristic called GradInit, which is able to estimate the norm of the network layer and the hyperparameters. Theoretical analysis is provided to show that the heuristic can be used to compute the hyper-parameters of a network layer. Experiments are conducted on the Transformer architecture for machine translation.  ","This paper proposes a new method for initializing neural networks, called GradInit, which is an automated and architecture agnostic method for training neural networks. The method is based on a heuristic that uses the norm of the network layer to determine the hyperparameters of the neural network. The authors show that GradInit can be used to improve the performance of neural networks in terms of learning rate warmup, momentum coefficients, and learning rates. They also show that the method can be applied to the Transformer architecture. "
17804,SP:f69731403592fa5bdd4ca327708582d615aa131c,"Linear mixed - effect models USED-FOR disease progression. interpretable parameters USED-FOR subject trajectories. diffeomorphism USED-FOR Euclidean metric. diffeomorphism USED-FOR metric. reproducible kernel Hilbert space FEATURE-OF radial basis functions. radial basis functions USED-FOR diffeomorphism. metric update USED-FOR forecasting of imaging and clinical biomarkers. TADPOLE challenge EVALUATE-FOR methods. Material is longitudinal data. Method are interpretable models, and ADNI. OtherScientificTerm are progression profiles, Riemannian manifold, patient - specific trajectories, and central geodesic. Generic is approach. Metric is interpretability. Task is Neural Information Processing Systems. ",This paper proposes a new approach to model disease progression in longitudinal data. The main idea is to use a diffeomorphism of the Euclidean metric of the Riemannian manifold to model the subject trajectories. The authors show that the proposed approach can be used in conjunction with ADNI to improve the interpretability of the model. The proposed approach is evaluated on the TADPOLE challenge.  ,"This paper proposes a new approach to interpretable linear mixed-effect models for disease progression. The proposed approach is based on the notion of a central geodesic, which is defined as a Riemannian manifold over a set of patient-specific trajectories. The authors show that the proposed approach can be used to update the Euclidean metric of a patient's trajectory in a reproducible kernel Hilbert space. The approach is evaluated on a TADPOLE challenge, where it is shown to be competitive with existing methods."
17868,SP:438e906f52c4c0538956b51a2270b3ac498b27a8,"routing - by - memory mechanism USED-FOR CNN architectures. memory head CONJUNCTION procedure. procedure CONJUNCTION memory head. memory head PART-OF PU. procedure PART-OF PU. procedures USED-FOR features. mechanism USED-FOR Networks. four - step training strategy USED-FOR mechanism. four - step training strategy USED-FOR Networks. VGGNet CONJUNCTION ResNet. ResNet CONJUNCTION VGGNet. ResNet CONJUNCTION EfficientNet ’s accuracies. EfficientNet ’s accuracies CONJUNCTION ResNet. Tiny ImageNet CONJUNCTION ImageNet. ImageNet CONJUNCTION Tiny ImageNet. ImageNet CONJUNCTION CIFAR-100 benchmarks. CIFAR-100 benchmarks CONJUNCTION ImageNet. Tiny ImageNet EVALUATE-FOR EfficientNet ’s accuracies. VGGNet EVALUATE-FOR method. ResNet EVALUATE-FOR method. EfficientNet ’s accuracies EVALUATE-FOR method. ImageNet EVALUATE-FOR method. CIFAR-100 benchmarks EVALUATE-FOR method. Tiny ImageNet EVALUATE-FOR method. Method are Convolutional Neural Networks ( CNNs ), parallel procedures, and parallel Procedural Units ( PUs ). OtherScientificTerm are semantic features, procedure sequence, intermediate features, and intermediate feature. Generic are specialized procedures, It, and network. ","This paper proposes a new routing-by-memory mechanism to improve the efficiency of CNN architectures. The proposed method is based on the idea of parallel procedures, where each procedure is a sequence of procedures, and each procedure consists of a memory head and a procedure procedure. The procedure procedure sequence is composed of a set of intermediate features, which are then used to update the intermediate features in the final layer of the network.    The method is evaluated on VGGNet, ResNet, and EfficientNet and achieves state-of-the-art results on ImageNet and CIFAR-100.","This paper proposes a new routing-by-memory mechanism to improve the performance of CNNs. The proposed method is based on a four-step training strategy, where each procedure is trained in a parallel manner. The method is evaluated on VGGNet, ResNet, Tiny ImageNet, and CIFAR-100."
17932,SP:d240173080cd3647dbaa5173a6422396f226775b,"fundamental symmetries CONJUNCTION coordinate freedoms of physical law. coordinate freedoms of physical law CONJUNCTION fundamental symmetries. coordinate freedoms of physical law FEATURE-OF neural networks. fundamental symmetries FEATURE-OF neural networks. irreducible representations USED-FOR frameworks. rotation CONJUNCTION reflection ( parity ). reflection ( parity ) CONJUNCTION rotation. translation CONJUNCTION rotation. rotation CONJUNCTION translation. fundamental symmetries USED-FOR physical laws. scalar products CONJUNCTION scalar contractions. scalar contractions CONJUNCTION scalar products. scalar contractions HYPONYM-OF scalars. scalar products HYPONYM-OF scalars. OtherScientificTerm are high - order tensor objects, symmetry - enforcing constraints, classical physics, permutations, symmetries, and Euclidean, Lorentz, and Poincaré groups. Method are polynomial functions, and scalar - based method. Generic is theory. ",This paper studies the problem of learning high-order tensor representations with symmetry-enforcing constraints in the context of classical physics. The authors propose a scalar-based method based on scalar products and scalar contractions to learn such representations. Theoretical results show that such representations can be represented as a set of polynomial functions that satisfy certain symmetry-preserving constraints. Theorems are also provided to show that the representations are irreducible.,"This paper studies the problem of learning irreducible representations for high-order tensor objects, i.e., high-dimensional tensors. The authors propose a scalar-based method that can be used to learn representations for such high-ordered tensors, which can be applied to neural networks. The main contributions of the paper are: (1) The authors provide a theoretical analysis of the representation of high-orders tensors in terms of symmetry-enforcing constraints in classical physics. (2) They show that the representation can be represented as a polynomial function that is invariant to permutation. (3) They also provide an analysis of how the representations can be encoded in neural networks and show that it is possible to use the representation as a neural network. "
17996,SP:72c0f47566904deb27d8157da30807ec1d6b5685,Bounding box ( bbox ) regression HYPONYM-OF computer vision. loss functions USED-FOR bbox regression. Intersection over Union ( IoU ) loss HYPONYM-OF loss functions. IoUbased losses USED-FOR power IoU losses. power IoU term CONJUNCTION power regularization term. power regularization term CONJUNCTION power IoU term. power parameter α FEATURE-OF power regularization term. power regularization term FEATURE-OF power IoU losses. power IoU term FEATURE-OF power IoU losses. order preservingness CONJUNCTION loss / gradient reweighting. loss / gradient reweighting CONJUNCTION order preservingness. α - IoU losses HYPONYM-OF losses. loss / gradient reweighting HYPONYM-OF properties. order preservingness HYPONYM-OF properties. α - IoU losses COMPARE IoU - based losses. IoU - based losses COMPARE α - IoU losses. small datasets CONJUNCTION noisy bboxes. noisy bboxes CONJUNCTION small datasets. object detection benchmarks CONJUNCTION models. models CONJUNCTION object detection benchmarks. bbox regression accuracy EVALUATE-FOR detectors. performance margin EVALUATE-FOR α - IoU losses. performance margin EVALUATE-FOR IoU - based losses. OtherScientificTerm is α. ,This paper proposes a new loss function for bounding box regression. The proposed loss function is based on the Intersection over Union (IoU) loss. The main idea is to use the power of the IoU loss as a regularization term to improve the performance of the bbox regression model.   The main contribution of this paper is to show that the power IoU losses are order-preserving and can be used in conjunction with the power regularization terms to improve bbox performance. ,"This paper proposes a new loss function for bounding box regression, which is based on the Intersection over Union (IoU) loss. The main contribution of the paper is to propose a new IoU-based loss function, which can be used to improve the performance of bbox regression. The proposed IoU loss can be combined with the power IoU term and the power regularization term to improve performance. The paper also provides a theoretical analysis of the properties of the IoU losses. "
18060,SP:397125177d7007316d67194ec00d5dc57b44ac79,"imitation learning problem USED-FOR policy. Markov Decision Process ( MDP ) setting FEATURE-OF policy. imitation learning USED-FOR policy. policy USED-FOR problem. adversarial construction USED-FOR policy. DROIL CONJUNCTION Maximum Entropy Inverse Reinforcement Learning. Maximum Entropy Inverse Reinforcement Learning CONJUNCTION DROIL. framework USED-FOR generalized concept of entropy. generalized concept of entropy USED-FOR DROIL. framework USED-FOR DROIL. approach USED-FOR objective function. approach USED-FOR convex optimization problem. state and action spaces FEATURE-OF loss functions. convex optimization problem USED-FOR objective function. polynomial number of variables FEATURE-OF convex optimization problem. approach USED-FOR stationary and non - stationary policies. methods COMPARE it. it COMPARE methods. inner reinforcement learning problem USED-FOR it. synthetic data CONJUNCTION highway driving environment. highway driving environment CONJUNCTION synthetic data. optimization method USED-FOR DROIL. synthetic data EVALUATE-FOR DROIL. highway driving environment EVALUATE-FOR DROIL. synthetic data EVALUATE-FOR optimization method. highway driving environment EVALUATE-FOR optimization method. OtherScientificTerm are reward function, demonstrated behaviors, noisy demonstrations, and optimistic generalizations. Generic is task. Method is Distributionally Robust Imitation Learning ( DROIL ). ","This paper proposes Distributionally Robust Imitation Learning (DROIL), a method for imitation learning in the Markov Decision Process (MDP) setting, where the goal is to learn a policy that is robust to noisy demonstrations. The main idea is to use a generalized concept of entropy in the MDP to learn an imitation learning policy that maximizes the entropy of the state and action spaces. The proposed method is based on the adversarial construction of the policy, which is a convex optimization problem with a polynomial number of variables. DROIL is shown to outperform existing methods in both stationary and non-stationary MDPs.","This paper proposes Distributionally Robust Imitation Learning (DROIL), a method for imitation learning in the Markov Decision Process (MDP) setting, where the goal is to learn a policy that maximizes the entropy of the state and action spaces in the MDP. DROIL is motivated by the notion of entropy, which is a generalized concept of entropy that can be applied to the problem of imitation learning. The authors propose to use a convex optimization problem with a polynomial number of variables to solve the objective function. They show that DROIL can be used for stationary and non-stationary MDPs, and show that it can outperform existing methods on synthetic data and highway driving environments. "
18124,SP:58f220bbbed8d3e0633b408fca3b6838c4ad323d,"Post - processing USED-FOR algorithmic fairness. approach USED-FOR ML systems. Post - processing HYPONYM-OF approach. retraining USED-FOR it. post - processing algorithms USED-FOR individual fairness ( IF ). similarity graph USED-FOR fairness constraints. graph Laplacian regularization USED-FOR graph smoothing problem. graph smoothing problem USED-FOR IF post - processing problem. post - processing algorithms USED-FOR individual biases. post - processing algorithms USED-FOR large - scale NLP models. individual biases FEATURE-OF large - scale NLP models. accuracy EVALUATE-FOR post - processing algorithms. BERT HYPONYM-OF large - scale NLP models. Method is postprocessing. Generic is model. OtherScientificTerm are objective function, and individual fairness. ","This paper proposes a post-processing approach for algorithmic fairness in ML systems. In particular, the authors propose to use individual fairness (IF) as a regularization term to enforce fairness constraints on the similarity graph. The authors show that post-processing with individual fairness is equivalent to post-smoothing with a graph Laplacian regularization. The proposed method is shown to improve the performance of large-scale NLP models on individual biases.","This paper studies the problem of post-processing for algorithmic fairness in ML systems. In particular, the authors propose a graph smoothing method for individual fairness (IF) based on graph Laplacian regularization. They show that the proposed method can be applied to a large-scale NLP model (BERT) and show that it outperforms the state-of-the-art in terms of fairness. "
18188,SP:ef791aa29decd839e7e583c9d1f71e8309ca87ef,"Text - to - SQL task USED-FOR SQL queries. model USED-FOR database schemas. graph structure USED-FOR unified encoding model. unified encoding model USED-FOR natural language question and database schema. graph structure USED-FOR SADGA. question - graph CONJUNCTION schema - graph. schema - graph CONJUNCTION question - graph. unified modeling USED-FOR structure - aware aggregation method. Global Graph Linking CONJUNCTION Local Graph Linking. Local Graph Linking CONJUNCTION Global Graph Linking. Local Graph Linking CONJUNCTION DualGraph Aggregation Mechanism. DualGraph Aggregation Mechanism CONJUNCTION Local Graph Linking. Global Graph Linking USED-FOR structure - aware aggregation method. Local Graph Linking PART-OF structure - aware aggregation method. DualGraph Aggregation Mechanism PART-OF structure - aware aggregation method. Text - to - SQL benchmark Spider EVALUATE-FOR proposal. Task are Text - to - SQL, and cross - domain Text - to - SQL. Method are encoding method, and question - schema linking method. OtherScientificTerm is database schema. ","This paper proposes a new model for text-to-symbolic-semantic-semantics (SADGA) task. The proposed model is based on a unified encoding model for natural language question and database schema, where the question-graph is a graph structure and the database schema is a set of database schemas. The authors propose a global graph linking and local graph linking methods to improve the performance of SADGA on the Spider benchmark.","This paper proposes a unified encoding model for the text-to-symbolic query generation task. The proposed model is based on the question-schema-graph (SADGA) model, which encodes the question graph and the query graph into a single graph. The question graph is represented as a graph-graph, and query graph is modeled as a query graph. A global graph-linker is used to link query graph and query query graph, and a local graph linker is applied to link the query and query graphs. The model is evaluated on the Spider benchmark, and the proposed model outperforms the state-of-the-art. "
18252,SP:a2fa25a4539a38af61a0993f65ecc14339f26c2e,"models USED-FOR supervised and reinforcement learning. discrete and continuous model components USED-FOR models. approach USED-FOR discrete - continuous computation graphs. discrete probability distributions USED-FOR neural networks. stochastic softmax tricks USED-FOR neural networks. discrete component USED-FOR graph ’s execution paths. discrete component USED-FOR computation graphs. sequential discrete components USED-FOR stochastic computations graphs. small gradients CONJUNCTION local minima. local minima CONJUNCTION small gradients. scale parameter FEATURE-OF Gumbel noise perturbations. scale parameter USED-FOR learning behavior. dropout residual connections USED-FOR stochastic, discrete - continuous computation graphs. complex discrete - stochastic models COMPARE continuous counterparts. continuous counterparts COMPARE complex discrete - stochastic models. benchmark datasets EVALUATE-FOR complex discrete - stochastic models. benchmark datasets EVALUATE-FOR continuous counterparts. Method are discrete - continuous models, and complex discrete - continuous models. Generic is strategies. ","This paper proposes a novel approach to learning stochastic computation graphs with discrete probability distributions. The main idea is to use a discrete component to model the computation graph’s execution paths, and a continuous component to compute the gradients and local minima of the computation graphs. The authors propose to use dropout residual connections to connect the discrete and continuous components. The proposed method is shown to outperform the state-of-the-art methods on several benchmark datasets.","This paper proposes a new approach for learning stochastic computation graphs for discrete-continuous models. The authors propose a new stochastically-constrained computation graph that is composed of a discrete component and a continuous component. The discrete component consists of a sequence of small gradients and local minima, and the continuous component is a dropout residual connection between the two components. They show that this graph can be used to learn complex discrete models that outperform their continuous counterparts on a variety of benchmarks."
18316,SP:bb3ec363e90269db4a2ba99d8107cb56f86e68f0,"Approximate Bayesian inference USED-FOR neural networks. Approximate Bayesian inference COMPARE training. training COMPARE Approximate Bayesian inference. high - fidelity approximate inference FEATURE-OF Bayesian neural networks ( BNNs ). full - batch Hamiltonian Monte Carlo USED-FOR high - fidelity approximate inference. covariate shift FEATURE-OF Bayesian model average. approximate inference procedures CONJUNCTION maximum a - posteriori ( MAP ) training. maximum a - posteriori ( MAP ) training CONJUNCTION approximate inference procedures. priors USED-FOR BNNs. robustness EVALUATE-FOR BNNs. robustness EVALUATE-FOR priors. Material is out - of - distribution data. Method is classical estimation. OtherScientificTerm are linear dependencies, features, and posterior contraction. ",This paper studies approximate Bayesian inference in Bayesian neural networks (BNNs). The authors propose a method for approximate inference based on Hamiltonian Monte Carlo (HMC) with Bayesian model average (BMA). The main contribution of the paper is to show that the Bayesian posterior of a BMA trained with HMC can be approximated by a Bayesian network trained with full-batch HMC. The authors also show that Bayesian networks trained with BMA are more robust to covariate shift and out-of-distribution data.  ,This paper studies the problem of high-fidelity approximate inference for Bayesian neural networks (BNNs). The authors propose a new Hamiltonian Monte Carlo (HMC) approach to approximate Bayesian inference for BNNs. They show that HMC can be used to improve the robustness of Bayesian models to out-of-distribution data. They also show that the HMC approach can be combined with maximum a-priori (MAP) training to improve robustness. 
18380,SP:f86ec7042e9b73ae071704a6d3ed17d7e3da1b75,"settings PART-OF meta - learning evaluation. in - distribution [ ID ] HYPONYM-OF settings. out - of - distribution [ OOD ] HYPONYM-OF settings. task distribution USED-FOR train and test tasks. metalearning theory CONJUNCTION FSL applications. FSL applications CONJUNCTION metalearning theory. they USED-FOR task generation. few - shot classification benchmarks EVALUATE-FOR OOD evaluation. ID setting USED-FOR metalearning theory. meta - learning methods USED-FOR ID setting. OOD datasets EVALUATE-FOR meta - learning methods. meta - learning method USED-FOR model selection. ID evaluation CONJUNCTION OOD evaluation. OOD evaluation CONJUNCTION ID evaluation. FSL benchmarks USED-FOR ID evaluation. FSL benchmarks USED-FOR OOD evaluation. benchmarks USED-FOR OOD evaluation. Task are OOD setting, and ID vs. OOD evaluation. Generic is methods. ","This paper studies the problem of meta-learning evaluation in out-of-distribution (OOD) settings. In this setting, the goal is to evaluate the performance of a meta-learner on a set of few-shot classification tasks, where the task distribution is unknown to the learner. The authors propose two settings for OOD evaluation: (1) ID evaluation, in which the distribution of the training set is known, and (2) FSL evaluation, which is a generalization of the ID setting.   The authors show that ID evaluation is beneficial for meta-learners, and that it is beneficial to use ID evaluation to select the best model for a given task. They show that using ID evaluation improves the performance on FSL tasks. They also show that FSL benchmarks are beneficial for ID evaluation. ","This paper proposes a meta-learning method for out-of-distribution (OOD) evaluation, where the goal is to evaluate the performance of a few-shot classifier on a set of tasks that are out of the training distribution (ID). The method is based on the metalearning theory of FSL applications, which is a well-studied area of research. The authors show that ID evaluation can be better than OOD evaluation, and propose a method for ID evaluation that can be applied to both ID and OOD settings. They also show that FSL-based methods can be used to evaluate OOD performance. "
18444,SP:371f77148b4f00a929f7c118b1bb7c5a6238d264,"rules PART-OF knowledge base ( KB ). language model ( LM)-based rule generation USED-FOR rules. KB - based rule induction CONJUNCTION LM - based rule generation. LM - based rule generation CONJUNCTION KB - based rule induction. data commonalities USED-FOR KB - based methods. LMs USED-FOR free text. rich expressive power FEATURE-OF LMs. methods USED-FOR canned ” rules. open rule induction problem USED-FOR open rules. Orion ( open rule induction ) system USED-FOR open rules. LMs USED-FOR open rules. automatically inducted rules COMPARE manually annotated rules. manually annotated rules COMPARE automatically inducted rules. open rules USED-FOR relation extraction. OtherScientificTerm are Rules, annotated rules, and supervision of annotated rules. Method are inference systems, rule induction systems, and LM - based methods. Generic is they. ","This paper proposes an open rule induction method that uses language models to generate rules from a knowledge base of rules. The proposed method is based on the idea that open rules are rules that are not annotated in the knowledge base, but can be generated by a language model. The authors show that the proposed method outperforms existing methods in terms of inference time and accuracy. ",This paper proposes a new open rule induction method for rule induction in the context of language model (LM) based rule generation. The proposed method is based on the idea that open rules are open rules that are not annotated in the language model. The authors show that the open rules can be easily inducted into the LM-based rule generation system. They also show that their method can be used to extract open rules from the knowledge base. 
18508,SP:8be2e0ea4a83fe32a4859f456007a829e5e9270a,Reinforcement Learning ( RL ) algorithms USED-FOR real - world scenarios. single - agent counterpart COMPARE offline multiagent RL. offline multiagent RL COMPARE single - agent counterpart. state and action space FEATURE-OF agents. agents PART-OF offline multiagent RL. offline RL algorithms USED-FOR multi - agent systems. offline RL algorithm USED-FOR extrapolation error. state - action pairs USED-FOR value estimation. Implicit Constraint Q - learning ( ICQ ) HYPONYM-OF offline RL algorithm. ICQ USED-FOR multi - agent tasks. implicit constraint USED-FOR joint - policy. OtherScientificTerm is accumulated extrapolation error. ,This paper studies the problem of offline multi-agent reinforcement learning in the presence of multiple agents in the state and action space. The authors propose an implicit constraint Q-learning (ICQ) algorithm to learn a joint policy for the joint-policy and state-action pairs. They show that the accumulated extrapolation error is bounded by the sum of the value estimation error of the state-actions and the value of the joint policy. The proposed ICQ algorithm is shown to outperform the single-agent counterpart in a variety of experiments.,"This paper proposes Implicit Constraint Q-learning (ICQ) for multi-agent reinforcement learning (MRL). ICQ is based on the idea of implicit constraint Q-Learning, which is an extension of the implicit constraint learning (ICL) framework. The main idea is to use an implicit constraint to enforce a joint-policy between the agents. The authors show that the accumulated extrapolation error of ICQ can be used to estimate the value of each agent's action in a multi-state multi-action setting. "
18572,SP:1939b24b68970c33ca16ce238deed257f76d009e,"machine learning models USED-FOR security related applications. real - world adversaries USED-FOR neural network based detectors. uniform norm - bounded perturbations USED-FOR adversarial examples ( AEs ). finance CONJUNCTION social networks. social networks CONJUNCTION finance. malware CONJUNCTION finance. finance CONJUNCTION malware. AEs USED-FOR domains. social networks HYPONYM-OF domains. malware HYPONYM-OF domains. finance HYPONYM-OF domains. semantically meaningful dependencies FEATURE-OF features. features USED-FOR applications. non - uniform perturbations USED-FOR feature dependencies. non - uniform perturbations USED-FOR adversarial training. malware classification CONJUNCTION credit risk prediction. credit risk prediction CONJUNCTION malware classification. credit risk prediction CONJUNCTION spam detection. spam detection CONJUNCTION credit risk prediction. approach USED-FOR real - world attacks. certification EVALUATE-FOR non - uniform bounds. non - uniform perturbation bounds USED-FOR robustness certification. Metric is imperceptibility. OtherScientificTerm are uniform perturbations, and empirical data distribution. ","This paper proposes to use non-uniform norm-bounded perturbations to improve the robustness of adversarial examples (AEs) against real-world adversaries. The proposed method is based on the observation that uniform perturbation is not sufficient to guarantee robustness to real-life adversarial attacks. To this end, the authors propose to use uniform norm-bound perturbed examples (UEs) for adversarial training. Theoretical analysis is provided to show that the proposed method can be used for robustness certification. Empirical results are provided to demonstrate the effectiveness of the proposed methods.","This paper studies the robustness of adversarial examples (AEs) against uniform norm-bounded perturbations (i.e., non-uniform perturbation) in the presence of semantically meaningful dependencies (e.g., feature dependencies) between features in the data distribution. The authors propose a new robustness certification metric, which measures the imperceptibility of AEs against uniform perturbed examples. They show that non-Uniform perturbing AEs are more robust than uniform perturbs in terms of imperceptible features. They also provide a new metric for robustness, which is based on the empirical data distribution of the data.  "
18636,SP:417b30930b245667d777e5d90ee80dd41546760e,spectral filtering USED-FOR statistical properties. spectral filtering USED-FOR learning with kernels. learning with kernels USED-FOR statistical properties. regularization schemes COMPARE Tikhonov regularization. Tikhonov regularization COMPARE regularization schemes. faster convergence rates FEATURE-OF excess risk. regularization schemes USED-FOR excess risk. regularization schemes USED-FOR least squares. faster convergence rates EVALUATE-FOR regularization schemes. loss functions USED-FOR estimators. Tikhonov regularization USED-FOR generalized self concordant loss functions ( GSC ). logistic loss HYPONYM-OF generalized self concordant loss functions ( GSC ). proximal point method USED-FOR optimization. iterated Tikhonov regularization scheme CONJUNCTION proximal point method. proximal point method CONJUNCTION iterated Tikhonov regularization scheme. fast and optimal rates USED-FOR GSC. iterated Tikhonov regularization scheme USED-FOR fast and optimal rates. iterated Tikhonov regularization scheme USED-FOR GSC. OtherScientificTerm is source and capacity conditions. Task is learning task. ,"This paper studies the convergence of generalized self-concordant loss functions (GSC) with Tikhonov regularization. In particular, the authors show that the convergence rate of GSC is bounded by the excess risk of the least squares. The authors then propose a proximal point method (PPM) to improve the convergence rates of the GSC.   ","This paper studies the convergence of generalized self-concordant loss functions (GSC) with Tikhonov regularization. The authors show that under certain conditions, the convergence rate of the GSC is faster than the convergence rates of other regularization schemes. The main contribution of the paper is that the authors provide a theoretical analysis of the convergence speedup of GSC with the help of a proximal point method. They show that the proximal method can converge to the optimal rate of convergence faster than existing methods."
18700,SP:1caeee4f00b52fe356ff4e5dd004d0203e838370,"linear transform USED-FOR input - output dimensions. butterfly matrices USED-FOR linear transform. Deformable Butterfly ( DeBut ) HYPONYM-OF linear transform. It USED-FOR neural networks. sparsity FEATURE-OF DeBut layer. sparsity USED-FOR network compression. light weight CONJUNCTION inference complexity. inference complexity CONJUNCTION light weight. DeBut COMPARE fully connected and convolutional layers. fully connected and convolutional layers COMPARE DeBut. OtherScientificTerm are butterflies, and natural complexity - accuracy tradeoff. Method is neural network. Generic is it. Metric is accuracy. ","This paper proposes a new linear transform called Deformable butterfly matrices (DeBut) to reduce the computational complexity of neural networks. The proposed method is based on the observation that the input-output dimensions of a neural network can be represented as a set of ""butterfly matrices"", where the weights of each butterfly matrix are independent of the input dimension. The authors show that the DeBut layer can be decomposed into two parts: (1) a ""lightweight"" layer that is composed of a fixed number of ""deformable"" weights, and (2) a larger ""deformed"" weight that is added to the input layer. The weights of the deformable layer are chosen to be sparse, and the authors propose to use sparsity to encourage the weights to be close to each other in the deformation matrix. Experiments on synthetic and real-world datasets show the effectiveness of the proposed method.  ","This paper proposes a new layer of neural networks, called Deformable Butterfly (DeBut), which can be used to reduce the computational complexity of a neural network. The proposed DeBut layer is based on the butterfly matrices of the input-output dimensions. The paper shows that DeBut can reduce the complexity-accuracy trade-off between fully connected and convolutional layers. The authors also show that the sparsity of DeBut layers can be reduced by using network compression."
18764,SP:d345ce1d7afc367ee1a9fb68d50ff1b2219f02cb,"weights PART-OF network. method USED-FOR weight reusability. shared weights USED-FOR MARK. common Knowledge Base ( KB ) USED-FOR shared weights. metalearning approach USED-FOR weight reusability. metalearning approach USED-FOR KB. benchmarks EVALUATE-FOR MARK. average accuracy EVALUATE-FOR methods. MARK USED-FOR reusable knowledge. Method are artificial neural networks, and MetA Reusable Knowledge. Task are Catastrophic Forgetting ( CF ), and overwriting. OtherScientificTerm are forgetting of old information, trainable masks, and KB relevant weights. Generic are task, and model. Material is 20 - Split - MiniImageNet dataset. Metric is forgetfulness. ","This paper proposes a method to address catastrophic forgetting in deep neural networks. The proposed method is based on the MetA Reusable Knowledge (MARK) method, which is a re-useful knowledge base (KB) that is shared among all the weights in the network. The idea is to use the KB as a meta-learning objective and use it to train the network to re-learn the relevant weights. The experiments show that the proposed method outperforms the baselines on the 20-Split-MiniImageNet dataset.","This paper proposes a method for reusing the weights in a neural network. The key idea is to use a common Knowledge Base (KB) to store the weights of the network, and then use the KB to re-train the network. This is done by using a metalearning approach to find the relevant weights in the KB. The proposed method is evaluated on the 20-Split-MiniImageNet dataset, and outperforms the baselines."
18828,SP:722c52467e384058f8fdffa254d0e8db47440a64,"exact solvers USED-FOR Mixed Integer Programming ( MIP ). Primal heuristics USED-FOR exact solvers. MIP heuristics PART-OF solver. hard - coded rules USED-FOR solvers. rules USED-FOR problem. rules USED-FOR heuristics. data - driven framework USED-FOR scheduling heuristics. scheduling heuristics PART-OF exact MIP solver. data - driven framework USED-FOR exact MIP solver. algorithm USED-FOR schedule. Task are real - world applications, and learning task. Method are primal heuristics, problem - specific schedule of heuristics, and academic MIP solver. Metric is average primal integral. ",This paper studies the problem of exact MIP solver with hard-coded primal heuristics. The authors propose a data-driven framework to learn the optimal schedule of primal heuristic based on the data. The proposed method is shown to outperform the state-of-the-art exact solver in terms of the average primal integral. ,"This paper proposes a data-driven framework for exact MIP solvers that learns primal heuristics for mixed-integer programming (MIP) solvers. The core idea is to learn a set of hard-coded rules for solving the problem, and then use these rules to schedule the solvers to solve the MIP problem. The authors show that the average primal integral of the solver is the average of the primal integral over the set of rules. They show that this is a good metric to measure the performance of the exact solver. They also provide an algorithm to learn the optimal primal integral for the problem.   "
18892,SP:5a21f0a49731dcb1d68deb06a75138e8e9d514d5,"it COMPARE real - world applications. real - world applications COMPARE it. self - driving cars CONJUNCTION robotics. robotics CONJUNCTION self - driving cars. real - world applications PART-OF reinforcement learning. robotics HYPONYM-OF reinforcement learning. self - driving cars HYPONYM-OF reinforcement learning. robotics HYPONYM-OF real - world applications. self - driving cars HYPONYM-OF real - world applications. sublinear regret EVALUATE-FOR algorithm. unknown parametric model USED-FOR trajectory labels. Task are reinforcement learning ( RL ), RL practice, and learning. OtherScientificTerm are binary feedback, and reward signal. Generic is this. ","This paper studies the problem of reinforcement learning in the setting where the reward signal is binary and the trajectory labels are unknown. In this setting, the goal is to learn a policy that maximizes the expected reward while minimizing the sublinear regret. The authors propose an algorithm that uses an unknown parametric model to model the reward and trajectory labels. They show that the regret of their algorithm is sublinear with respect to the number of trajectories. They also show that their algorithm can be used in a self-driving car setting.  ","This paper proposes a new algorithm for sublinear regret reduction in reinforcement learning. The main idea is to use a parametric model to model the trajectory labels of the agent, and then use this model to estimate the regret. The authors show that their algorithm can be applied to a variety of real-world applications, such as self-driving cars, robotics, and robotics with real-time control."
18956,SP:e66bd9582058ba0f6091bb1042ce2ecfdaae1515,"node embedding CONJUNCTION graph pooling methods. graph pooling methods CONJUNCTION node embedding. Graph neural networks USED-FOR representing graph - structured data. edges PART-OF graph. edges PART-OF graph. edges USED-FOR discrimination. graph reconstruction and generation CONJUNCTION graph classification tasks. graph classification tasks CONJUNCTION graph reconstruction and generation. graph classification tasks HYPONYM-OF tasks. graph reconstruction and generation HYPONYM-OF tasks. nodes PART-OF hypergraph. edges PART-OF graph. Dual Hypergraph Transformation ( DHT ) USED-FOR edge representation learning framework. message - passing techniques USED-FOR node representations. message - passing techniques USED-FOR edges. dual hypergraph construction USED-FOR message - passing techniques. hypergraphs USED-FOR edge representations. method COMPARE graph representation learning methods. graph representation learning methods COMPARE method. edge representation learning method USED-FOR graph representation and generation. graph datasets USED-FOR graph representation and generation. hypergraphs USED-FOR edge representation learning method. graph datasets EVALUATE-FOR hypergraphs. graph datasets EVALUATE-FOR edge representation learning method. lossless compression of the nodes CONJUNCTION removal of irrelevant edges. removal of irrelevant edges CONJUNCTION lossless compression of the nodes. edge representation learning and pooling method COMPARE graph pooling methods. graph pooling methods COMPARE edge representation learning and pooling method. graph pooling methods USED-FOR graph classification. edge representation learning and pooling method USED-FOR graph classification. Material is graph - structured data. Generic is they. OtherScientificTerm is connectivity. Method are graph representation learning, holistic graph - level edge representations, and edge representation learning. ",This paper proposes a novel edge representation learning framework based on the dual hypergraph transformation (DHT) to learn edge representations for graph reconstruction and generation tasks. The main idea is to construct a hypergraph of nodes and edges in the graph using message-passing techniques. The proposed method is evaluated on several graph datasets and achieves state-of-the-art performance. ,This paper proposes a novel edge representation learning framework for graph representation learning. The authors propose a dual hypergraph transformation (DHT) method to learn the edge representation of a hypergraph. They use a message-passing technique to represent the edges in the hypergraphs. They show that the proposed method outperforms existing graph pooling and node embedding methods. They also show that their method can be applied to graph classification tasks. 
19020,SP:e398873e29b05176e1d52dc6f86a59a4f405e6fd,"Mutual information ( MI ) maximization USED-FOR learning representations of data. representations USED-FOR learning. representations USED-FOR reinforcement learning ( RL ). representations USED-FOR RL. MI objectives USED-FOR representations. samples of high - dimensional observations USED-FOR MI. state representation USED-FOR optimal policy. objectives USED-FOR insufficient representations. visual observations FEATURE-OF simulated game environment. OtherScientificTerm are irrelevant and redundant information, MI based objectives, and structure of the MDP. Task is control. Generic is methods. ",This paper studies the problem of learning representations of data in reinforcement learning. The authors propose to use mutual information maximization (MI) to learn representations of high-dimensional observations. The main idea is to maximize the mutual information between the state representation and the optimal policy. The proposed method is evaluated on a variety of simulated environments and compared to a number of baselines.,"This paper studies the problem of learning representations of data in reinforcement learning (RL). The authors propose a new objective called mutual information maximization (MI) maximization, which is based on the notion of ""mutual information"" (MI). MI maximizes the mutual information between the two parties in a MDP (i.e., the MDP is represented as a set of high-dimensional observations). They show that this objective can be used to learn representations of high dimensional observations, which are then used to improve the performance of a policy. The authors also show that MI can be applied to a variety of MDPs (e.g., a toy environment, a real-world environment, and a simulated environment).  "
19084,SP:50181f740910195d3a50dd7d7f8cbb1c476d730b,"steerable convolution USED-FOR 3D semantic analysis. SS - Conv USED-FOR steerable convolution. sparse tensors USED-FOR steerable convolution. pipeline USED-FOR precise estimation of object poses. SS - Conv USED-FOR pipeline. Feature - Steering module USED-FOR pose refinement. SE(3)-equivariance USED-FOR Feature - Steering module. instance - level 6D pose estimation CONJUNCTION category - level 6D pose and size estimation. category - level 6D pose and size estimation CONJUNCTION instance - level 6D pose estimation. category - level 6D pose and size estimation CONJUNCTION categorylevel 6D pose tracking. categorylevel 6D pose tracking CONJUNCTION category - level 6D pose and size estimation. categorylevel 6D pose tracking HYPONYM-OF 3D object semantic analysis. instance - level 6D pose estimation HYPONYM-OF 3D object semantic analysis. category - level 6D pose and size estimation HYPONYM-OF 3D object semantic analysis. pipeline COMPARE methods. methods COMPARE pipeline. metrics EVALUATE-FOR tasks. tasks EVALUATE-FOR pipeline. tasks EVALUATE-FOR methods. metrics EVALUATE-FOR pipeline. metrics EVALUATE-FOR methods. SS - Conv USED-FOR pipeline. SS - Conv COMPARE convolutions. convolutions COMPARE SS - Conv. accuracy CONJUNCTION efficiency. efficiency CONJUNCTION accuracy. efficiency EVALUATE-FOR SS - Conv. accuracy EVALUATE-FOR SS - Conv. efficiency EVALUATE-FOR convolutions. accuracy EVALUATE-FOR convolutions. Method are SE(3)-equivariant deep feature learning, and Sparse Steerable Convolution ( SS - Conv ). Material is dense, volumetric data. Task is processing of 3D data. Generic are designs, and code. ","This paper proposes a novel steerable convolution method for 3D semantic analysis. The proposed method is based on a feature-steering module that is trained with sparse tensors. The main idea is to learn a feature extractor that is SE(3)-equivariant to the volumetric data, which is then used to train a steerable neural network. The method is evaluated on instance-level pose estimation, category-level 6D pose and size estimation, and category level semantic analysis tasks.   ","This paper proposes a novel steerable convolution for 3D object semantic analysis. The main idea is to use a feature-steering module to improve the accuracy and efficiency of the task. The proposed method is based on the SE(3)-equivariant deep feature learning framework. The method is evaluated on three tasks: instance-level 6D pose estimation, category-level pose and size estimation, and categorylevel 6d pose tracking."
19148,SP:d746bfb200577c980d92727bb0b1a3c23e7bfdc5,"Attention USED-FOR vision transformers. informative tokens USED-FOR image recognition. dynamic token sparsification framework USED-FOR redundant tokens. lightweight prediction module USED-FOR importance score. features USED-FOR importance score. module USED-FOR redundant tokens. module PART-OF layers. layers USED-FOR redundant tokens. attention masking strategy USED-FOR prediction module. hierarchically pruning USED-FOR method. accuracy EVALUATE-FOR vision transformers. FLOPs EVALUATE-FOR method. accuracy EVALUATE-FOR method. throughput EVALUATE-FOR method. DynamicViT models COMPARE CNNs. CNNs COMPARE DynamicViT models. CNNs CONJUNCTION vision transformers. vision transformers CONJUNCTION CNNs. DynamicViT models COMPARE vision transformers. vision transformers COMPARE DynamicViT models. dynamic token sparsification framework USED-FOR DynamicViT models. ImageNet EVALUATE-FOR CNNs. ImageNet EVALUATE-FOR vision transformers. complexity / accuracy trade - offs EVALUATE-FOR DynamicViT models. Method are self - attention, and raoyongming. OtherScientificTerm is unstructured sparse tokens. Generic is framework. ",This paper proposes a dynamic token sparsification framework to remove redundant tokens in vision transformers. A lightweight prediction module is added to the self-attention module to reduce the importance score. A masking strategy is used to improve the performance of the prediction module. Experiments on ImageNet show that the proposed DynamicViT model achieves state-of-the-art accuracy and FLOPs. ,"This paper proposes a dynamic token sparsification framework for vision transformers. The key idea is to use a lightweight prediction module to predict the importance score for each token, which is then used to prune the redundant tokens. The proposed method is evaluated on ImageNet and CIFAR-10 datasets. "
19212,SP:d0b6cde42b1cba5e6e3c7c5131426fd84adbd3d7,"cross - validation methods CONJUNCTION conformal prediction. conformal prediction CONJUNCTION cross - validation methods. holdout methods CONJUNCTION cross - validation methods. cross - validation methods CONJUNCTION holdout methods. inference USED-FOR regression function. methods USED-FOR predictive inference. distribution - free guarantees USED-FOR predictive inference. methods USED-FOR distribution - free guarantees. inference USED-FOR inference. inference USED-FOR conditional mean. inference HYPONYM-OF regression function. conformal prediction HYPONYM-OF methods. holdout methods HYPONYM-OF methods. cross - validation methods HYPONYM-OF methods. non - vanishing width FEATURE-OF confidence interval. inference USED-FOR E [ Y |X ]. finite setting CONJUNCTION continuous setting. continuous setting CONJUNCTION finite setting. Task is data analysis problems. OtherScientificTerm are distributional assumptions, inference guarantees, sample size, and vanishing - width confidence intervals. ","This paper studies the problem of predictive inference for regression under distributional assumptions. In particular, the authors consider the case where the conditional mean of the regression function is assumed to be a function of the data distribution. The authors show that under certain assumptions on the distribution, the confidence intervals of the posterior distribution can be approximated by the confidence interval of the true posterior distribution. They show that this is the case for both finite and continuous data sets.   ",This paper studies the problem of distribution-free predictive inference. The main contribution of the paper is a theoretical analysis of the vanishing-width confidence intervals. The authors show that the confidence intervals are non-vanishing in the finite and continuous setting. They also provide theoretical guarantees of the confidence interval in the continuous and finite setting. 
19276,SP:123952325765c040c3078fc7dca2b6d370e55590,bias mitigation methods USED-FOR DNN models. learning debiased encoders USED-FOR bias mitigation methods. instance - level annotations USED-FOR sensitive attributes. fairness sensitive information PART-OF encoder. discrimination EVALUATE-FOR DNN models. task - specific classification head PART-OF DNN models. Representation Neutralization for Fairness ( RNF ) HYPONYM-OF mitigation technique. neutralized representations USED-FOR classification head. ground - truth label CONJUNCTION sensitive attributes. sensitive attributes CONJUNCTION ground - truth label. classification head PART-OF DNN model. neutralized representations USED-FOR DNN model. RNF USED-FOR classification head. fairness sensitive information PART-OF encoder representations. bias - amplified model USED-FOR proxy annotations. proxy annotations USED-FOR sensitive attributes. bias - amplified model USED-FOR low - resource settings. benchmark datasets EVALUATE-FOR RNF framework. RNF framework USED-FOR DNN models. benchmark datasets EVALUATE-FOR discrimination of DNN models. task - specific performance EVALUATE-FOR discrimination of DNN models. task - specific performance EVALUATE-FOR RNF framework. Method is biased representations. Task is fairness. OtherScientificTerm is sensitive attribute annotations. ,"This paper proposes Representation Neutralization for Fairness (RNF), a method for learning debiased encoders to improve fairness in classification tasks. The proposed method is based on the observation that sensitive attribute annotations can be biased by instance-level annotations. To mitigate this problem, the authors propose to use a bias-amplified model to amplify the sensitive attributes in the ground-truth label. Experiments show that the proposed method improves the performance of DNN models on several benchmark datasets.","This paper proposes Representation Neutralization for Fairness (RNF), a new bias mitigation method for learning debiased encoders. The proposed method is based on a bias-amplified model, which is used to amplify the sensitive attribute annotations in the encoder representations. The authors show that the proposed method can improve the performance of DNN models on a variety of tasks. "
19340,SP:210eb2c811f966bb1ac53932cacabbad9bb608fe,translations CONJUNCTION rotations. rotations CONJUNCTION translations. rotations FEATURE-OF learning models. translations FEATURE-OF learning models. learning models USED-FOR image analysis. Convolutional Neural Networks ( CNN ) USED-FOR image analysis. convolutions USED-FOR They. physics FEATURE-OF Bessel functions. Bessel functions USED-FOR convolutional layer. Task is medical imaging. Method is Bessel - CNNs ( B - CNNs ). OtherScientificTerm is rotation angles. ,This paper proposes to use Bessel functions to learn convolutional layers in CNNs. The main idea is to use a Bessel function to model the rotation angle of the input image. The authors show that the Bessel-CNNs can be used to learn the rotation angles of input images. The proposed method is evaluated on a variety of image classification tasks.   ,This paper proposes a new Bessel-CNN (B-CNN) architecture for medical imaging. The main contribution of the paper is a theoretical analysis of the Bessel functions of the convolutional layer of B-CNNs. The authors show that the convolutions of a Bessel function are invariant to rotation angles. They also provide a theoretical explanation of the effect of the rotation angles on the Busses. The paper also provides a theoretical justification for the proposed Bessel CNN.
19404,SP:ee51ecbd476d5b65903c942a62be89ff5d91698b,large - scale solver USED-FOR kernel ridge regression. ParK HYPONYM-OF large - scale solver. ParK HYPONYM-OF kernel ridge regression. random projections CONJUNCTION iterative optimization. iterative optimization CONJUNCTION random projections. partitioning CONJUNCTION random projections. random projections CONJUNCTION partitioning. partitioning CONJUNCTION iterative optimization. iterative optimization CONJUNCTION partitioning. partitioning PART-OF approach. iterative optimization PART-OF approach. space and time complexity EVALUATE-FOR approach. random projections PART-OF approach. statistical accuracy EVALUATE-FOR approach. local effective dimension CONJUNCTION bias. bias CONJUNCTION local effective dimension. orthogonality FEATURE-OF local estimators. feature space COMPARE input space. input space COMPARE feature space. feature space FEATURE-OF partitions. statistical - computational tradeoff EVALUATE-FOR model. large - scale datasets EVALUATE-FOR method. ,"This paper proposes a new method for solving kernel ridge regression. The proposed method is based on partitioning the feature space into subspaces, which are then used to compute the KL-divergence between the features of the input and the partitioned feature space. The authors show that the proposed method achieves better performance than the state-of-the-art methods in terms of accuracy and computational complexity. ","This paper proposes a new method for large-scale kernel ridge regression. The main idea is to partition the feature space into two parts, one for the input space and the other for the output space. The first partition is based on the orthogonality of local estimators, and the second partitioning involves the use of random projections and iterative optimization. The authors show that the proposed method achieves better performance than the state-of-the-art in terms of both space and time complexity. "
19468,SP:1f096d6fabd5b1fde43d06c552d46d87cd35cb4a,"reinforcement learning settings USED-FOR Neural agents. discrete tokens USED-FOR Neural agents. one - hot vectors USED-FOR discrete communication tokens. zero - shot understanding HYPONYM-OF communication. natural language processing USED-FOR word embedding techniques. discrete tokens USED-FOR neural agent architectures. continuous space USED-FOR discrete tokens. technique USED-FOR communication. technique COMPARE one - hot tokens. one - hot tokens COMPARE technique. decision theoretic framework EVALUATE-FOR technique. Generic are techniques, and method. OtherScientificTerm are human communication, unlabeled emergent agent communication, and one - hot communication. ","This paper proposes a method for learning discrete communication tokens for reinforcement learning agents. The idea is to use a one-hot vector to represent the communication between agents in a continuous space, which is then used to train a neural network. The authors show that the proposed method is able to learn discrete communication in reinforcement learning tasks, where the goal is to learn a zero-shot understanding of the agent’s actions. ","This paper proposes a method for learning discrete communication tokens for zero-shot understanding in reinforcement learning. The key idea is to use a continuous space of discrete tokens that can be represented as a one-hot vector. The authors show that the proposed method can be used to learn discrete tokens in the presence of unlabeled emergent agent communication. The proposed method is evaluated on a variety of tasks, and it is shown that it can achieve better performance than the state-of-the-art."
19532,SP:8630ccc627534f9033bced04e2137a897ffef701,they COMPARE convolutional networks. convolutional networks COMPARE they. Transformers USED-FOR computer vision. generalization COMPARE convolutional networks. convolutional networks COMPARE generalization. model capacity FEATURE-OF Transformers. depthwise Convolution CONJUNCTION self - Attention. self - Attention CONJUNCTION depthwise Convolution. convolution layers CONJUNCTION attention layers. attention layers CONJUNCTION convolution layers. CoAtNets HYPONYM-OF hybrid models. capacity CONJUNCTION efficiency. efficiency CONJUNCTION capacity. generalization CONJUNCTION capacity. capacity CONJUNCTION generalization. coat ” nets HYPONYM-OF CoAtNets. relative attention USED-FOR hybrid models. relative attention USED-FOR depthwise Convolution. relative attention USED-FOR self - Attention. CoAtNets COMPARE CoAtNet. CoAtNet COMPARE CoAtNets. resource constraints FEATURE-OF CoAtNets. JFT-3B USED-FOR CoAtNet. top-1 accuracy EVALUATE-FOR CoAtNet. top-1 accuracy EVALUATE-FOR it. ImageNet EVALUATE-FOR it. OtherScientificTerm is inductive bias. Generic is architectures. Metric is ImageNet top-1 accuracy. Material is ImageNet-21 K. ,"This paper proposes to combine depthwise convolution and self-attention to improve the efficiency and generalization of convolutional neural networks. The authors propose a hybrid model called CoAtNets, which combines depthwise Convolution with relative attention in the attention layer. Theoretical analysis is provided to show that the proposed method is more efficient than the state-of-the-art. Experiments on ImageNet-21K and JFT-3B demonstrate the effectiveness of the proposed methods. ","This paper proposes CoAtNets, a hybrid model that combines depthwise convolution, self-attention, and attention layers. The main idea is to combine depthwise Convolution and self-Attention in the same layer. The authors show that this hybrid model can achieve better generalization and capacity than the state-of-the-art convolutional networks. They also show that CoAtNet can achieve top-1 accuracy on ImageNet-21K. "
19596,SP:d3ecbeeffa5ab365743ba8653c6739f24742ee31,second - order oracle bound USED-FOR expected risk. expected risk FEATURE-OF weighted majority vote. second - order oracle bound USED-FOR weighted majority vote. one - sided Chebyshev ’s ) HYPONYM-OF parametric form of the ChebyshevCantelli inequality. parametric form of the ChebyshevCantelli inequality USED-FOR bound. form USED-FOR optimization challenge. Chebyshev - Cantelli inequality CONJUNCTION C - bounds. C - bounds CONJUNCTION Chebyshev - Cantelli inequality. optimization challenge USED-FOR prior oracle bounds. Chebyshev - Cantelli inequality USED-FOR prior oracle bounds. it USED-FOR oracle bound. second order Markov ’s inequality USED-FOR it. second order Markov ’s inequality USED-FOR oracle bound. PAC - Bayesian bounding CONJUNCTION Bennett ’s inequality. Bennett ’s inequality CONJUNCTION PAC - Bayesian bounding. PAC - Bayes - Bennett HYPONYM-OF concentration of measure inequality. PAC - Bayesian bounding PART-OF it. Bennett ’s inequality PART-OF it. it USED-FOR empirical estimation of the oracle bound. PAC - Bayes - Bennett inequality COMPARE PAC - Bayes - Bernstein inequality. PAC - Bayes - Bernstein inequality COMPARE PAC - Bayes - Bennett inequality. ChebyshevCantelli inequality CONJUNCTION PAC - Bayes - Bennett inequality. PAC - Bayes - Bennett inequality CONJUNCTION ChebyshevCantelli inequality. parametric form USED-FOR concentration of measure. PAC - Bayes - Bennett inequality USED-FOR concentration of measure. parametric form CONJUNCTION PAC - Bayes - Bennett inequality. PAC - Bayes - Bennett inequality CONJUNCTION parametric form. parametric form FEATURE-OF ChebyshevCantelli inequality. Task is minimization. Generic is bounds. ,"This paper proposes a parametric form of the Chebyshev-cantelli inequality for the weighted majority vote, which is a second-order oracle bound for the expected risk. The authors show that this inequality is equivalent to the one-sided Chebyashev’s inequality, which can be used as an optimization challenge to find the oracle bounds. They also show that it is equivalent with the PAC-Bayes-Bennett inequality, and that it can be approximated by the second order Markov inequality.","This paper proposes a parametric form of the Chebyshev-cantelli inequality (C-Cantelli) for the weighted majority vote, which is a second-order oracle bound for the expected risk of a weighted majority. The authors show that the parametric version of the inequality can be used for the optimization challenge of the PAC-Bayes-Bennett inequality, which they call the ""PAC-Bayesian bounding"" of the concentration of measure inequality. They also provide an empirical estimation of the upper bound of the oracle bounds. "
19660,SP:5bac542a6532d43cf100e085398b4a4783719814,"audio - visual video parsing task USED-FOR audio or visual event categories. method USED-FOR audio or visual events. common and diverse event semantics USED-FOR audio or visual events. common and diverse event semantics USED-FOR method. method USED-FOR event co - occurrence. method COMPARE methods. methods COMPARE method. weakly - supervised audio - visual video parsing EVALUATE-FOR method. weakly - supervised audio - visual video parsing EVALUATE-FOR methods. OtherScientificTerm are audio and visual events, cross - modality co - occurrence, supervisory signals, and video - level annotations. Method is parsing model. ","This paper proposes a method for weakly supervised audio-visual video parsing, where the goal is to identify audio and visual events in a video sequence. The paper proposes to use a video-level parsing model to learn event semantics for audio and video events. The method is based on the idea that events in audio and videos should have a common semantic structure. The proposed method is evaluated on weakly-supervised video parsing tasks and achieves state-of-the-art performance. ","This paper proposes a method for weakly-supervised audio-visual video parsing, where the goal is to extract audio and visual event categories from a video. The method is based on the idea of cross-modality co-occurrence, where audio and video events occur in the same time frame, and video-level annotations are used to annotate the event categories. The proposed method is evaluated on a variety of weakly supervised video parsing tasks, and it is shown to be competitive with other methods. "
19724,SP:8fd6a03c1794afa524328d45f4232eacf6f86693,"federated learning ( FL ) USED-FOR global model. quantized and personalized FL algorithm USED-FOR collective ( personalized model compression ) training. knowledge distillation ( KD ) USED-FOR collective ( personalized model compression ) training. quantization parameters CONJUNCTION model dimensions / structures. model dimensions / structures CONJUNCTION quantization parameters. model dimensions / structures FEATURE-OF compressed personalized models. quantization parameters FEATURE-OF compressed personalized models. algorithm USED-FOR quantized models. relaxed optimization problem USED-FOR algorithm. knowledge distillation loss USED-FOR local client objectives. global model USED-FOR knowledge distillation loss. model dimension EVALUATE-FOR compressed model. knowledge distillation loss USED-FOR compressed personalization framework. alternating proximal gradient update USED-FOR compressed personalization problem. FedAvg CONJUNCTION local training of clients. local training of clients CONJUNCTION FedAvg. QuPeD COMPARE personalized FL methods. personalized FL methods COMPARE QuPeD. personalized FL methods CONJUNCTION FedAvg. FedAvg CONJUNCTION personalized FL methods. QuPeD COMPARE FedAvg. FedAvg COMPARE QuPeD. QuPeD COMPARE local training of clients. local training of clients COMPARE QuPeD. local training of clients HYPONYM-OF personalized FL methods. Method are FL algorithms, and ( federated ) learning process. Material is heterogeneous data. Task is personalization. OtherScientificTerm is quantization values. ","This paper proposes a new personalized federated learning algorithm named QuPeD for quantized and personalized model compression. The authors propose to use knowledge distillation (KD) as a local client objective to compress the global model, and then use alternating proximal gradient update for the compressed personalization problem. Experiments show that the proposed method outperforms FedAvg and local training of clients. ","This paper proposes a new approach to quantized and personalized federated learning (FL) training. The approach is based on the knowledge distillation (KD) loss, which is used to compress the global model and the local model. The authors propose a relaxed optimization problem for the quantized model compression problem. They show that their approach outperforms FedAvg and QuPeD in terms of local training of clients. "
19788,SP:fca8b4f1e765cf1724a37f0ae9a7dac1cb79c8b1,"Constrained clustering PART-OF machine learning. partially labeled data USED-FOR prior information. prior information USED-FOR it. partially labeled data USED-FOR it. framework USED-FOR constrained clustering. deep generative models USED-FOR framework. stochastic gradient variational inference USED-FOR framework. domain knowledge USED-FOR model ( DC - GMM ). probabilistic relations FEATURE-OF domain knowledge. DC - GMM COMPARE deep constrained clustering methods. deep constrained clustering methods COMPARE DC - GMM. robustness EVALUATE-FOR deep constrained clustering methods. data sets EVALUATE-FOR DC - GMM. data sets EVALUATE-FOR deep constrained clustering methods. robustness EVALUATE-FOR DC - GMM. real - world applications EVALUATE-FOR approach. Generic are model, and constraints. OtherScientificTerm are prior clustering preferences, and pairwise constraints. Task is clustering process. ",This paper proposes a novel framework for constrained clustering based on deep generative models. The proposed method is based on a generative model that is trained with a set of constraints on the clustering process. The authors show that the proposed method outperforms existing methods in terms of robustness and robustness to perturbations in the data. ,"This paper proposes a new framework for constrained clustering based on deep generative models (DC-GMM). DC-GMM is based on a stochastic gradient variational inference (SGVI) framework, where the goal is to learn a probabilistic relation between the data and the domain knowledge. The authors show that DC-GMI can be used to learn the prior clustering preferences of the data. They also provide a theoretical analysis of the relationship between the prior and the pairwise constraints. The proposed method is evaluated on several real-world datasets. "
19852,SP:84379c0c881b7390ecc22fb398edfaf66d1af1ff,Neural Tangent Kernel ( NTK ) USED-FOR infinitely - wide neural networks. least squares loss USED-FOR infinitely - wide neural networks. gradient descent USED-FOR least squares loss. gradient descent USED-FOR infinitely - wide neural networks. NTK regression COMPARE finitely - wide neural networks. finitely - wide neural networks COMPARE NTK regression. small - scale datasets USED-FOR finitely - wide neural networks. kernel methods USED-FOR large - scale learning tasks. computational complexity EVALUATE-FOR kernel methods. near input - sparsity time approximation algorithm USED-FOR NTK. near input - sparsity time approximation algorithm USED-FOR learning. polynomial expansions of arc - cosine kernels USED-FOR near input - sparsity time approximation algorithm. NTK USED-FOR learning. spectral approximation guarantee USED-FOR NTK matrix. random features CONJUNCTION sketching algorithm. sketching algorithm CONJUNCTION random features. random features PART-OF arc - cosine kernels. leverage score sampling USED-FOR random features. accuracy EVALUATE-FOR CNTK. linear regressor COMPARE CNTK. CNTK COMPARE linear regressor. accuracy EVALUATE-FOR linear regressor. speedup EVALUATE-FOR linear regressor. CNTK features USED-FOR linear regressor. speedup EVALUATE-FOR CNTK. large - scale regression and classification tasks EVALUATE-FOR methods. CIFAR-10 dataset EVALUATE-FOR CNTK. Method is convolutional counterpart of NTK ( CNTK ). OtherScientificTerm is linear runtime. ,"This paper proposes a convolutional version of the Neural Tangent Kernel (NTK) that can be used to train infinitely-wide neural networks with least-squares loss in a near-input-sparsity time. The proposed method is based on polynomial expansion of arc-cosine kernels, which can be approximated by a near input-sparse time approximation algorithm. The authors also propose a sketching algorithm to sample random features from the NTK matrix and leverage score sampling to improve the performance of the proposed method. ",This paper proposes a novel method for learning neural networks with neural tangent kernels (NTK). The authors propose a near input-sparsity time approximation algorithm for learning the NTK matrix. They also propose a convolutional counterpart of NTK (CNTK) that can be used for large-scale regression and classification tasks. The proposed method is evaluated on CIFAR-10 dataset and outperforms the linear regressor. 
19916,SP:fa2668083ff3bb592c29a4c6822ae96ff54d0dbe,"framework USED-FOR multi - person 3D motion trajectory prediction. local - range encoder CONJUNCTION global - range encoder. global - range encoder CONJUNCTION local - range encoder. global - range encoder USED-FOR social interactions. local - range encoder USED-FOR individual motion. local - range encoder PART-OF Multi - Range Transformers model. global - range encoder PART-OF Multi - Range Transformers model. Transformer decoder USED-FOR prediction. model COMPARE methods. methods COMPARE model. long - term 3D motion prediction EVALUATE-FOR methods. long - term 3D motion prediction EVALUATE-FOR model. OtherScientificTerm are human pose trajectory, and local and global - range encoder features. ",This paper proposes a multi-person 3D motion trajectory prediction model that combines a local-range encoder and a global-range decoder for predicting the trajectory of each individual person. The proposed model is based on the Transformer architecture and consists of two modules: a local encoder that predicts the trajectory for each person and a local and global encoder for each individual motion. The local encoders are used to predict the trajectory and the global decoder is used to model the global trajectory. The model is evaluated on a variety of datasets and compared with a number of baselines.,"This paper proposes a framework for multi-person 3D motion trajectory prediction based on a Transformer-based model. The model consists of two parts: a local-range encoder and a global-range decoder. The local encoder encodes the human pose trajectory, and the global encoder is used to predict the global trajectory. The global range encoder predicts the individual motion of each person in the group. The proposed model is evaluated on a variety of datasets, and it is shown to outperform the state-of-the-art in terms of accuracy."
19997,SP:0a0e07af37c8fe8580639b1df62d27b6f63f8dee,"reinforcement learning USED-FOR long - horizon planning problems. programs USED-FOR reinforcement learning. programs USED-FOR settings. strategy USED-FOR program. program synthesis USED-FOR guiding programs. generative model USED-FOR It. It USED-FOR program. model USED-FOR program. approach COMPARE non - program - guided approaches. non - program - guided approaches COMPARE approach. benchmarks EVALUATE-FOR non - program - guided approaches. 2D Minecraft - inspired environment HYPONYM-OF benchmarks. benchmarks EVALUATE-FOR approach. program - guided reinforcement learning USED-FOR approach. Generic is approaches. Method are guiding program, model predictive program synthesis ( MPPS ), and handcrafted programs. Task is programming task. ","This paper proposes a method for learning to generate programs for long-horizon planning problems. The main idea is to use a generative model to generate a set of programs that can be used to guide the RL agent to solve the problem. The method is evaluated on a variety of long-term planning tasks, including a 2D Minecraft-like environment.   ","This paper proposes a novel approach to program-guided reinforcement learning for long-horizon planning problems. The approach is based on a generative model that learns a program synthesis strategy to guide the user to generate a guiding program. The method is evaluated on a variety of environments, including a Minecraft-inspired environment and a 2D Minecraft-like environment.  "
20078,SP:5bb42b178b0d27da271bfa60e633fdac718638c4,"causal imitation learning USED-FOR Imitation learning. sequential settings USED-FOR causal imitation learning. graphical criterion USED-FOR causal imitation. algorithm USED-FOR imitability. Task are naïve imitation, and single - stage decision - making. OtherScientificTerm are sensors, imitator, demonstrator ’s behavior ( DO ), and demonstrator. Generic is theory. ","This paper studies the problem of causal imitation learning in sequential settings. In this setting, the goal is to learn to imitate the demonstrator’s behavior in order to improve the performance of the imitator. The main contribution of the paper is to provide a graphical criterion for causal imitation, which can be used to evaluate the quality of the imitation.    The main contributions of this paper are as follows:  1. The authors provide a theoretical analysis of the causal imitation problem in the context of sequential decision-making.  2. They provide an algorithm for learning to imitate a single-stage decision-maker in a sequential setting.  3. They show that the proposed algorithm achieves better performance than naive imitation in the sequential setting, and outperforms the baselines.","This paper studies the problem of causal imitation learning in the context of imitation learning, where the imitator imitates the demonstrator’s behavior (DO) in a single-stage decision-making setting. The main contribution of the paper is a theoretical analysis of the causal imitation problem, which is motivated by the observation that the imitability of a demonstrator can be defined as the difference between the two demonstrators’ actions. Theoretical results are provided for a variety of settings, including the setting in which the demonstrators and imitators interact in a sequential fashion. The paper also provides a graphical criterion for measuring imitability, which can be used as a surrogate for the true imitability."
20159,SP:85bd81f0c5b6ccbc421ebbaf6f5c72164bc70b7f,transition losses USED-FOR object - structured representation. object - structured representation COMPARE pixels. pixels COMPARE object - structured representation. transition losses COMPARE pixels. pixels COMPARE transition losses. transition losses USED-FOR model. object persistence CONJUNCTION object identity. object identity CONJUNCTION object persistence. alignment module USED-FOR model. object identity HYPONYM-OF transition models. object persistence HYPONYM-OF transition models. objectlevel loss CONJUNCTION object alignment. object alignment CONJUNCTION objectlevel loss. object occlusion CONJUNCTION re - appearance. re - appearance CONJUNCTION object occlusion. model COMPARE baseline. baseline COMPARE model. it USED-FOR object occlusion. it USED-FOR re - appearance. partially observable environments FEATURE-OF re - appearance. partially observable environments FEATURE-OF object occlusion. OtherScientificTerm is slot - wise object memory. ,"This paper proposes to use object alignment and object persistence to improve the performance of object recognition models. The alignment module is based on the idea of object alignment, and the object persistence module is a combination of object persistence and object identity. The authors show that object alignment improves the performance on object detection tasks, and that object persistence improves object detection on object occlusion tasks.  ","This paper proposes a new transition model for object-structured representation learning. The proposed model is based on object-level loss, object alignment, object level loss, and object occlusion loss. The authors show that the proposed model outperforms the state-of-the-art model in terms of object persistence, object identity, and re-appearance. "
20240,SP:f32eddbb5c33a8422c075579ff08aa9833338d44,"classification CONJUNCTION regression. regression CONJUNCTION classification. regression CONJUNCTION off - policy policy learning. off - policy policy learning CONJUNCTION regression. classification CONJUNCTION off - policy policy learning. off - policy policy learning CONJUNCTION classification. Empirical risk minimization ( ERM ) PART-OF machine learning. classification HYPONYM-OF machine learning. adaptively collected data USED-FOR generic importance sampling weighted ERM algorithm. maximal inequality USED-FOR rates. exploration rate FEATURE-OF rates. fast rates USED-FOR regression. convexity of squared - error loss USED-FOR fast rates. regret guarantees USED-FOR policy learning. OtherScientificTerm are modelagnostic guarantees, hypothesis class, importance sampling structure, and exploration. Method is contextual bandit algorithm. Metric is fast convergence rates. Material is bandit - collected data. Generic is theory. ","This paper studies empirical risk minimization (ERM) with adaptively collected data in the context of contextual bandit algorithms. The authors show that under certain assumptions on the hypothesis class and importance sampling structure, the proposed algorithm converges with fast convergence rates. They also provide regret guarantees for off-policy policy learning and regression. ","This paper studies the problem of empirical risk minimization (ERM) in the context of adaptively collected data. In particular, the authors study the general importance sampling weighted ERM algorithm with respect to the contextual bandit algorithm. They show that the rate of convergence of the ERM rate is bounded by the maximal inequality of the exploration rate. They also provide regret guarantees for the generalization of the algorithm."
20321,SP:f549a0c231b71bae0acbed6e3afb41890ee89cd9,weighted regression model USED-FOR machine learning tasks. predictive power EVALUATE-FOR models. low sample sizes CONJUNCTION covariate perturbations. covariate perturbations CONJUNCTION low sample sizes. mitigation strategy USED-FOR problems. doubly non - negative matrix USED-FOR sample weights. log - determinant divergence CONJUNCTION Bures - Wasserstein distance. Bures - Wasserstein distance CONJUNCTION log - determinant divergence. uncertainty set FEATURE-OF weighting matrix. Bures - Wasserstein distance USED-FOR weighting matrix. log - determinant divergence USED-FOR weighting matrix. first - order methods USED-FOR adversarially reweighted estimate. Task is kernel - reweighted regression. Method is reweighting strategy. ,-reweighted regression is a well-studied problem in machine learning. This paper proposes a novel reweighting strategy for this problem. The main idea is to use a doubly non-negative matrix as the weighting matrix for the sample weights. The authors show that the reweighted estimator is robust to covariate perturbations.   ,This paper proposes a reweighting strategy for kernel-reweighted regression. The main idea is to use a doubly non-negative matrix as the weighting matrix for the sample weights. The authors show that the Bures-Wasserstein distance between the weights and the uncertainty set can be used to estimate the reweighted matrix. They also provide a first-order method for adversarially reweightsing the weights. 
20402,SP:fe12e13602925b9400fd596a987755beb10aa3d1,"discrete latent variables USED-FOR models. continuous relaxation USED-FOR low - variance reparameterization gradients. binary random variables USED-FOR it. importance sampling CONJUNCTION statistical couplings. statistical couplings CONJUNCTION importance sampling. importance sampling USED-FOR estimator. statistical couplings USED-FOR estimator. sequences of binary variables CONJUNCTION Rao - Blackwellization. Rao - Blackwellization CONJUNCTION sequences of binary variables. Rao - Blackwellization USED-FOR reparameterizing categorical variables. sequences of binary variables USED-FOR reparameterizing categorical variables. reparameterizing categorical variables USED-FOR gradient estimators. Rao - Blackwellization USED-FOR gradient estimators. estimators COMPARE REINFORCE. REINFORCE COMPARE estimators. leave - one - out - baseline estimator USED-FOR estimators. leave - one - out - baseline estimator USED-FOR REINFORCE. Method are unbiased gradient estimators, performant estimator, continuous relaxations, and categorical gradient estimators. Material is categorical setting. OtherScientificTerm is stick - breaking coupling. ",This paper proposes a continuous relaxation for reparameterization of discrete latent variables with binary random variables. The main idea is to use a low-variance estimator that is a combination of importance sampling and statistical couplings. The authors show that the proposed estimator is performant in the categorical setting. They also show that it is a performant estimator in the continuous relaxation setting.,This paper proposes a performant estimator for low-variance reparameterization gradients. The main idea is to use a continuous relaxation of the gradient estimator to reduce the variance of the estimator. The authors show that the proposed estimator REINFORCE can be used in the categorical setting. They also show that it is more performant than other estimators that do not use continuous relaxations.
20483,SP:e16fdf963ec2f9c0d79fa404e47e7862a5d6e922,"predictors USED-FOR top architectures. search path USED-FOR high - performance sub - space. weaker predictors USED-FOR search path. strong predictor USED-FOR architecture space. predictor USED-FOR well - performed architectures. coarse - to - fine iteration USED-FOR ranking of sampling space. NAS - Bench-101 CONJUNCTION NAS - Bench-201. NAS - Bench-201 CONJUNCTION NAS - Bench-101. WeakNAS USED-FOR top - performance architectures. ImageNet MobileNet Search Space EVALUATE-FOR SOTA. SOTA EVALUATE-FOR WeakNAS. ImageNet MobileNet Search Space EVALUATE-FOR WeakNAS. Method are Neural Architecture Search ( NAS ), predictor - based NAS approaches, proxy accuracy predictor, and weak predictor. OtherScientificTerm are architecture - performance pairs, and weak predictors. Generic are architecture, and framework. ",This paper proposes to use weak predictors to improve the performance of neural architecture search (NAS) methods. The weak predictor is a proxy accuracy predictor that predicts the accuracy of the architecture-performance pairs. The paper proposes a coarse-to-fine iterative method to find the high-performing sub-space in the architecture space. The method is evaluated on NAS-Bench-101 and NAS-bench-201 and shows that the proposed method outperforms the baseline methods. ,"This paper proposes a new framework for Neural Architecture Search (NAS) based on proxy-accuracy predictor. The proposed framework is based on the idea of weak predictors, which are predictors that can be used to find the high-performing sub-space in the architecture space. The weak predictor is a predictor that predicts the performance of a well-performing architecture, while the strong predictor is the predictor for the well-performed architecture. The paper also proposes a coarse-to-fine iteration method for ranking the sampling space, which can be applied to both the strong and weak predictor. Experiments on NAS-bench-101 and NAS-Bench-201 show that the proposed method outperforms the state-of-the-art in terms of performance. "
20564,SP:8f74abb04037ba2e59dcf8320dc555b149f68ed8,"latent codes PART-OF globally consistent coordinate system. Entropic Desired Dynamics USED-FOR Intrinsic ConTrol ( EDDICT ). tractable learning CONJUNCTION interpretable latent space. interpretable latent space CONJUNCTION tractable learning. EDDICT ’s globally consistent codes USED-FOR it. prior methods COMPARE EDDICT ’s globally consistent codes. EDDICT ’s globally consistent codes COMPARE prior methods. state coverage CONJUNCTION unsupervised. unsupervised CONJUNCTION state coverage. hard exploration games EVALUATE-FOR unsupervised. Montezuma ’s Revenge HYPONYM-OF hard exploration games. OtherScientificTerm are local objective, and fixed additive latent dynamics. ","This paper proposes a method for learning a global coordinate system that is globally consistent and interpretable. The method is based on the Entropic Desired Dynamics (EDD) framework, which is a generalization of the classical entropic dynamical system. The key idea is to learn a global latent code that is invariant to perturbations in the coordinate system, and then use this code to learn an interpretable latent space that can be used for exploration. The proposed method is shown to be tractable in terms of learning a tractable latent code, and it is shown that the learned latent codes are globally consistent.   ",This paper proposes a new method for learning a globally consistent coordinate system based on the Entropic Desired Dynamics (EDDICT) framework. The key idea is to learn a global coordinate system that is globally consistent with respect to a fixed additive latent dynamics. The method is based on tractable learning and interpretable latent space. Experiments on Montezuma’s Revenge show that the proposed method outperforms the state-of-the-art in terms of state coverage and unsupervised exploration.
20645,SP:c731a78c3e7f98ccd0253b51a0d42bf8deeb71f9,"reinforcement learning ( RL ) USED-FOR drug design. reward scoring function USED-FOR RL. molecular docking program USED-FOR RL. molecular docking program HYPONYM-OF physical simulation. molecular docking program HYPONYM-OF reward scoring function. physical simulation USED-FOR protein - small molecule binding affinity. models USED-FOR chemically realistic and pharmacochemically acceptable molecules. local optima CONJUNCTION smooth surfaces. smooth surfaces CONJUNCTION local optima. docking score optimization HYPONYM-OF exploration problem. RL framework USED-FOR pharmacochemically acceptable molecules. docking scores FEATURE-OF pharmacochemically acceptable molecules. fragment - based generation method CONJUNCTION error - prioritized experience replay ( PER ). error - prioritized experience replay ( PER ) CONJUNCTION fragment - based generation method. Explorative Experience replay USED-FOR Drug design ( FREED ). Explorative Experience replay USED-FOR Fragment - based generative RL. de novo and scaffold - based schemes EVALUATE-FOR model. model COMPARE methods. methods COMPARE model. method USED-FOR model. predictive error - PER ( FREED(PE ) ) USED-FOR model. predictive error - PER ( FREED(PE ) ) HYPONYM-OF method. OtherScientificTerm are molecular structure, realistic and qualified chemical space, and drugs. ",This paper proposes a new reinforcement learning framework for drug design. The proposed method is based on a fragment-based generation method and error-prioritized experience replay (PER). The authors show that the proposed method can generate molecules that are chemically realistic and pharmacochemically acceptable. Experiments are conducted on both de novo and scaffold-based schemes.  ,"This paper proposes a new framework for drug design based on a molecular docking program (MDP). The authors propose a new reward function, which is based on protein-small molecule binding affinity, and a new exploration problem, where the goal is to find molecules that are pharmacochemically acceptable. The authors also propose a fragment-based generation method (FREED) and an error-prioritized experience replay (PER) method. Experiments on de novo and scaffold-based schemes demonstrate the effectiveness of the proposed method. "
20726,SP:b938bca513e7de1231212064caf8877a78d8b612,"complexity EVALUATE-FOR directed acyclic graphical models. observational data USED-FOR directed acyclic graphical models. local Markov boundary search procedure USED-FOR ancestral sets. ancestral sets PART-OF graphical model. local Markov boundary search procedure USED-FOR approach. forward greedy search algorithm USED-FOR Markov boundary. backward pruning phase PART-OF forward greedy search algorithm. forward greedy search algorithm USED-FOR graph ensembles. identifiability condition FEATURE-OF graph. finite - sample guarantees USED-FOR recovering Markov boundaries. results USED-FOR polytrees. polynomial time FEATURE-OF polytrees. minimal assumptions USED-FOR structure of directed graphical models. approach USED-FOR discrete or continuous distributions. OtherScientificTerm are distributional assumptions, nodes, and Markov boundaries. Metric is sample complexity. Generic is algorithm. ","This paper studies the problem of recovering Markov boundaries in directed acyclic graphical models (DAGMs) from observational data. The authors propose a local Markov boundary search procedure to recover ancestral sets in DAGMs. The main idea is to use a forward greedy search algorithm to prune the nodes in a backward pruning phase, and then use an identifiability condition on the graph to recover the edges of the ancestral sets. Theoretical guarantees are given for the construction of graph ensembles, and finite-sample guarantees are also given.   ",This paper studies the problem of recovering Markov boundaries in directed acyclic graphical models (DAGs) from observational data. The authors propose a local Markov boundary search procedure to recover the ancestral sets of DAGs. The main contribution of the paper is a forward greedy search algorithm that recovers the ancestral set in polynomial time. The paper also provides a finite-sample guarantee for recovering the Markov bounds. 
20807,SP:af08109d4c45dc9401efb0e63c22167e9da28adb,"( "", ) DP algorithm USED-FOR privately learnable class. public randomness USED-FOR global stability. Task is learning with differential privacy ( DP ). OtherScientificTerm are privacy protection, probabilistic representation dimension, and nearly - matching lower bound. Method are "" -DP algorithms, local model, and correlated sampling strategy. ","This paper studies the problem of learning with differential privacy (DP), where the goal is to learn a privately learnable class. The main contribution is a new DP algorithm for learning with DP, which is based on the notion of ""probabilistic representation dimension"". The authors show that under certain assumptions on the local model, the proposed algorithm can learn a private class with DP. The authors also provide a nearly-matching lower bound on the complexity of the algorithm.   ","This paper studies the problem of learning with differential privacy (DP) in the setting where the privacy protection of the learner depends on the probabilistic representation dimension of the learned class. The authors provide a nearly-matching lower bound for the generalization of the ""-DP"" algorithm to the ""private learning"" setting. The lower bound is based on the assumption that the representation dimension is bounded by a local model, and a correlated sampling strategy is used to ensure global stability.  "
20888,SP:da4f21d107a7f442c4d3e3ec13bdb44b041e07cf,"Estimating the per - state expected cumulative rewards PART-OF reinforcement learning approaches. latent Markov decision process USED-FOR transition and reward models. gradient descent USED-FOR global optima. gradient descent USED-FOR linear parametrization. convergence rates USED-FOR cases. stochastic gradient descent ( SGD ) COMPARE explicit counterpart. explicit counterpart COMPARE stochastic gradient descent ( SGD ). implicit representation COMPARE explicit counterpart. explicit counterpart COMPARE implicit representation. implicit representation USED-FOR stochastic gradient descent ( SGD ). OtherScientificTerm are per - state expected cumulative rewards, and value predictions. Method are deep neural - network function - approximation methods, value iteration networks, and implicit representations of value functions. Generic is approach. ","This paper studies the problem of estimating the per-state expected cumulative rewards in reinforcement learning. The authors propose to use the implicit representation of the transition and reward models as a latent Markov decision process, and show that gradient descent converges to global optima with linear parametrization. They also show that the convergence rates of the proposed method are faster than the explicit counterpart. ","This paper studies the problem of estimating the per-state expected cumulative rewards in reinforcement learning. In particular, the authors consider the case where the transition and reward models are modeled as a latent Markov decision process. The authors show that the implicit representation of the value function can be better than the explicit representation, and that the convergence rate of SGD can be improved by using the implicit representations. They also provide a theoretical analysis of the convergence rates of the implicit and explicit representations."
20988,SP:992aa07d4f815d1c81f967374590eece933833b1,text sources USED-FOR Knowledge Graphs ( KGs ). embeddings USED-FOR inferring new facts. KG refinement task USED-FOR KGs. embeddings USED-FOR KGs. techniques USED-FOR KG refinement. inference rules USED-FOR techniques. ontological information USED-FOR embeddings. ontological information CONJUNCTION inferences rules. inferences rules CONJUNCTION ontological information. IterefinE HYPONYM-OF KG refinement framework. inferences rules USED-FOR one. ontological information USED-FOR one. ComplEx HYPONYM-OF KG embeddings. KG embeddings USED-FOR IterefinE. ontological information USED-FOR IterefinE. type - supervised embeddings USED-FOR KG. KG benchmarks EVALUATE-FOR embeddings. embeddings USED-FOR KG. PSL - KGI USED-FOR KG. overall weighted F1 score EVALUATE-FOR embeddings. Task is KG - based question answering. OtherScientificTerm is ontologies. Method is IterefinE framework. ,"This paper proposes a novel method for KG refinement based on type-supervised embeddings. The proposed method is based on the idea that KG-based question answering can be formulated as a knowledge graph refinement task, where the goal is to refine the KG by learning a set of inference rules that can be used to infer new facts from the original KG. The method is evaluated on the PSL-KGI benchmark and shows improved performance compared to existing methods.","This paper proposes a method to improve the performance of knowledge graph (KG) embeddings for the task of KG-based question answering. The method is based on the IterefinE framework, which proposes a new type-supervised embedding for KG. The proposed method is evaluated on PSL-KGI and PSL KGI-KG datasets."
20997,SP:676fc4a3041af22e8f20ccba7daa2a0b1f5d6af5,"binary predictions USED-FOR KBC quality. evaluation paradigm USED-FOR model selection criteria. real - world entities PART-OF KB. model COMPARE KB. KB COMPARE model. benchmark EVALUATE-FOR KB embeddings models. completion task EVALUATE-FOR ranking task. prediction separability FEATURE-OF KB embedding models. thresholding PART-OF TransE. classification F1 score EVALUATE-FOR TransE. Method is Knowledge base completion ( KBC ) methods. Material are knowledge base ( KB ), and FB14k - QAQ. Generic are method, and models. OtherScientificTerm are likelihood ranking, evaluation data structure, and KB queries. Task are ranking setting, and ranking - based and classification - based evaluation. ","This paper proposes a new evaluation framework for knowledge base completion (KBC) methods. The evaluation framework is based on ranking-based and classification-based evaluation, where the goal is to evaluate the quality of the knowledge base. The authors propose a new benchmark, FB14k-QAQ, to evaluate KBC methods on a ranking task and a classification task. The proposed method, called TransE, is evaluated on both ranking and classification tasks, and it is shown that the proposed method outperforms existing methods on both tasks. ","This paper proposes a new evaluation framework for knowledge base completion (KBC) methods. The evaluation framework is based on a ranking-based and classification-based evaluation, where the goal is to evaluate the quality of the knowledge base (KB) based on the likelihood ranking, and the evaluation data structure, and KB queries. The authors propose a new benchmark, FB14k-QAQ, which is a benchmark for KB embeddings models. They show that the proposed method outperforms the state-of-the-art KB embedding models on the ranking task and the classification task."
21006,SP:83fe0a496a79bcf97ccba1c6d34b7d11e7d5c330,dialog system models USED-FOR tasks. human annotations USED-FOR dialog system models. language priors USED-FOR down - stream NLP tasks. BERT CONJUNCTION GPT-2. GPT-2 CONJUNCTION BERT. GPT-2 HYPONYM-OF large pre - trained language models. BERT HYPONYM-OF large pre - trained language models. pre - trained language models USED-FOR dialog response generation. Alternating Roles Dialog Model ( ARDM ) HYPONYM-OF framework. large pretrained language model USED-FOR ARDM. belief states CONJUNCTION dialog acts. dialog acts CONJUNCTION belief states. It USED-FOR conversations. supervision USED-FOR It. human annotations USED-FOR It. belief states HYPONYM-OF human annotations. dialog acts HYPONYM-OF human annotations. ARDM COMPARE state - of - the - art methods. state - of - the - art methods COMPARE ARDM. CamRest676 CONJUNCTION MultiWOZ. MultiWOZ CONJUNCTION CamRest676. task - oriented dialog datasets EVALUATE-FOR state - of - the - art methods. task - oriented dialog datasets EVALUATE-FOR ARDM. MultiWOZ HYPONYM-OF task - oriented dialog datasets. CamRest676 HYPONYM-OF task - oriented dialog datasets. ARDM USED-FOR non - collaborative tasks. persuasion HYPONYM-OF non - collaborative tasks. ARDM USED-FOR human - like responses. ARDM USED-FOR persuasion tasks. , models are used to generate human-like responses in dialogs. This paper proposes a new framework called Alternating Roles Dialog Model (ARDM) that uses a large pretrained language model to generate dialog responses. ARDM is trained with two tasks: 1) belief state generation and 2) persuasion. Experiments show that ARDM outperforms state-of-the-art methods on task-oriented dialog datasets.,"This paper proposes a new dialog model called Alternating Roles Dialog Model (ARDM) that combines a pre-trained language model with a large pretrained language model to generate human-like dialog responses. ARDM is trained on a set of task-oriented dialog datasets, where it is shown to outperform state-of-the-art methods on task-specific dialog datasets. It is also shown that ARDM can be used for non-collaborative tasks, such as persuasion."
21015,SP:b11c06b7c4ef1aa43c59f808a679425e302d158e,deep neural network USED-FOR classification. softmax values FEATURE-OF network. implied loss FEATURE-OF uncertainty measure. confidence measures USED-FOR Top k. networks USED-FOR method. binning values USED-FOR confidence measures. Generic is values. ,"This paper studies the problem of estimating the softmax values of deep neural networks in the presence of uncertainty. The authors propose a new uncertainty measure, which they call the Top k-uncertainty measure, that measures the uncertainty of the confidence measures of the top k neural networks. They show that this uncertainty measure can be computed using the binning values of the weights of the network. They also show that the uncertainty measures can be used to estimate the confidence of top k networks.   ","This paper considers the problem of estimating confidence measures for a class of softmax values of a deep neural network. The authors propose a new confidence measure, the Top k-confidence measure, that can be used to estimate the confidence of a classifier with respect to the softmax value of the classifier. The confidence measure is defined as the sum of two confidence measures, the binning value and the uncertainty measure. The paper shows that the confidence measure can be computed for any classifier, and that it can be combined with the confidence measures of the top k-network. "
21024,SP:ab9666e15f2a0113d96cb4b47b1cbb30fa1f7982,"architecture CONJUNCTION hyperparameters. hyperparameters CONJUNCTION architecture. generalization FEATURE-OF neural networks. wide neural networks USED-FOR gradient descent. spectrum FEATURE-OF NNGP kernel. kernel USED-FOR gradient descent. kernel USED-FOR Gaussian Processes. Neural Network Gaussian Process ( NNGP ) kernel HYPONYM-OF kernel. Neural Tangent Kernel ( NTK ) HYPONYM-OF kernel. NTK COMPARE NNGP kernel. NNGP kernel COMPARE NTK. spectrum COMPARE NNGP kernel. NNGP kernel COMPARE spectrum. spectrum FEATURE-OF NTK. Fully Connected Networks ( FCNs ) CONJUNCTION Convolutional Neural Networks ( CNNs ). Convolutional Neural Networks ( CNNs ) CONJUNCTION Fully Connected Networks ( FCNs ). Convolutional Neural Networks ( CNNs ) HYPONYM-OF architectures. Fully Connected Networks ( FCNs ) HYPONYM-OF architectures. CNNs COMPARE FCNs. FCNs COMPARE CNNs. learning dynamics FEATURE-OF CNNs. average pooling FEATURE-OF CNNs. pooling FEATURE-OF CNNs. Task is deep learning. OtherScientificTerm are wide network limit, large depth limit, and hyperparameter space. Method are random networks, and NNGP. Metric are trainability, and training accuracy. Material is real datasets. ","This paper studies the generalization performance of wide neural networks in the presence of a large depth limit. The authors propose a new kernel called Neural Network Gaussian Process (NNGP) kernel, which is based on the Neural Tangent Kernel (NTK) kernel. They show that the NNGP kernel is equivalent to the spectrum of the NTK kernel in terms of gradient descent, and that it can be used as a generalization kernel for deep neural networks. They also show that this kernel can be applied to fully connected networks (FCNs) and convolutional neural networks (CNNs).  ","This paper studies the generalization properties of neural networks with wide neural networks. The authors propose a new neural network Gaussian process kernel (NNGP) kernel, which they call the Neural Tangent Kernel (NTK) kernel. They show that the NTK kernel is more general than the NNGP kernel in terms of generalization generalization. They also show that NTK can be used as a generalization kernel for wide networks. Finally, they show that FCNs and convolutional neural networks (CNNs) can generalize better than FCNs."
21033,SP:d3470c35aae48bf92439a55fdb98ccf07100e567,"computational methods USED-FOR protein folding. geometric invariance CONJUNCTION computational efficiency. computational efficiency CONJUNCTION geometric invariance. graph - based method USED-FOR protein models. GRAPHQA HYPONYM-OF graph - based method. GRAPHQA USED-FOR protein models. geometric invariance HYPONYM-OF favorable properties. representation learning HYPONYM-OF favorable properties. state - ofthe - art EVALUATE-FOR hand - engineered and representation - learning approaches. Material is Proteins. Task is biological processes. OtherScientificTerm are 3D structure, protein ’s structure, and sequential and 3D structure. Method is GRAPHQA components. ","This paper proposes a graph-based representation learning method for protein folding. The proposed method is based on GraphQA, which is a graph neural network architecture that learns to predict sequential and 3D structures of a protein. The authors show that the proposed method achieves state-of-the-art results in terms of computational efficiency and geometric invariance. ","This paper proposes a graph-based method for protein folding. The main idea of the paper is to learn a graph representation of a protein’s 3D structure, which can be used to predict the sequence of protein folds. The proposed method, GRAPHQA, is based on Graph-based Representation Learning (GRAPH). The authors show that the proposed method outperforms state-of-the-art methods in terms of geometric invariance, computational efficiency, and representation learning."
21042,SP:5188280131b58a35d3deda126a0754aea8fa6e58,"loss function FEATURE-OF neural network. geometry of the functional space CONJUNCTION parameterization of this space. parameterization of this space CONJUNCTION geometry of the functional space. network ’s weights USED-FOR parameterization of this space. pure critical points COMPARE spurious critical points. spurious critical points COMPARE pure critical points. loss function FEATURE-OF linear neural networks. determinantal variety HYPONYM-OF functional space. linear maps HYPONYM-OF determinantal variety. bounded rank FEATURE-OF linear maps. functional space USED-FOR network. loss functions CONJUNCTION parameterizations. parameterizations CONJUNCTION loss functions. loss functions FEATURE-OF linear networks. architectures USED-FOR linear maps. loss landscape FEATURE-OF linear networks. determinantal variety FEATURE-OF functional space. Generic is space. OtherScientificTerm are parameterization, determinantal varieties, smooth convex losses, filling architectures, quadratic loss, non - filling architectures, and architecture. Task is landscape of linear networks. ",This paper studies the loss landscape of linear neural networks. The authors show that the functional space of a linear network can be viewed as a determinantal variety of the weights of the network. They show that this space can be seen as a function of the parameterization of this space and the network’s weights. They also show that pure critical points in this space are not spurious critical points.   ,"This paper studies the landscape of linear neural networks in terms of the determinantal variety of the functional space of the network's parameters. The authors show that for any smooth convex loss function with bounded rank, there exists a set of critical points in the space of linear maps that are pure critical points and spurious critical points. They also show that the number of such critical points is bounded by a factor of the dimension of the space. Finally, the authors provide a theoretical analysis of the properties of the function space of a linear network. "
21051,SP:ee71597ceab23eb4db1d6608f15f80ad51f7ff6d,"Inductive and unsupervised graph learning USED-FOR predictive or information retrieval tasks. graph similarity evaluation USED-FOR learning processes. reconstruction error based loss functions USED-FOR learning processes. embedding of the subgraph vector distribution USED-FOR output vector representation. output vector representation USED-FOR graph. SEED framework USED-FOR subgraphs. reconstruction errors FEATURE-OF subgraphs. SEED CONJUNCTION graph isomorphism. graph isomorphism CONJUNCTION SEED. SEED framework COMPARE competitive baseline methods. competitive baseline methods COMPARE SEED framework. public benchmark datasets EVALUATE-FOR SEED framework. OtherScientificTerm are label information, subgraph samples, subgraph vectors, and subgraph vector distribution. Method is graph learning. Material is graph structured objects. ",This paper proposes a method for graph learning based on the reconstruction error based loss functions. The main idea is to use the reconstruction errors of the subgraphs as the loss function to learn the representation of the graph. The method is evaluated on several public benchmark datasets and shows competitive performance. ,"This paper proposes a new method for unsupervised graph learning, called SEED, which is based on graph similarity evaluation. The main idea of SEED is to use a graph isomorphism-invariant embedding of the subgraph vector distribution to learn the output vector representation of the graph. The authors show that SEED achieves competitive performance on a variety of public benchmark datasets."
21060,SP:d9406fdf0a180a5efc6f15ba8739524665f0f9d2,Counterfactual regret minimization ( CFR ) methods USED-FOR twoplayer zero - sum extensive games. imperfect information FEATURE-OF twoplayer zero - sum extensive games. vanilla CFR USED-FOR large - scale games. game tree USED-FOR vanilla CFR. Lazy - CFR HYPONYM-OF CFR algorithm. lazy update strategy USED-FOR CFR algorithm. Lazy - CFR COMPARE vanilla CFR. vanilla CFR COMPARE Lazy - CFR. regret EVALUATE-FOR Lazy - CFR. regret COMPARE regret. regret COMPARE regret. Lazy - CFR COMPARE regret. regret COMPARE Lazy - CFR. regret EVALUATE-FOR vanilla CFR. regret EVALUATE-FOR vanilla CFR. Lazy - CFR COMPARE CFR. CFR COMPARE Lazy - CFR. ,"This paper studies counterfactual regret minimization (CFRM) methods for two-player extensive games with imperfect information. The authors propose a new method called Lazy-FRM, which uses a lazy update strategy to reduce the computational cost of the original CFR algorithm. Theoretical results show that the proposed method achieves a regret of $O(1/\sqrt{T})$ for large-scale games. ",This paper proposes a new counterfactual regret minimization (CFR) algorithm for zero-sum extensive games with imperfect information. The main idea is to use the game tree as a game tree and use a lazy update strategy to reduce the number of iterations of the CFR algorithm. The authors show that the proposed algorithm outperforms the vanilla CFR algorithm in terms of regret. 
21069,SP:023aa3dca1cf7992b22993a7088e8a74c92bb47e,"Unsupervised Domain Adaptation ( UDA ) methods USED-FOR transferable features. explicit feature distribution modeling USED-FOR UDA. Distribution Matching Prototypical Network ( DMPN ) USED-FOR deep features. Gaussian mixture distributions USED-FOR Distribution Matching Prototypical Network ( DMPN ). Gaussian mixture distributions USED-FOR deep features. domain discrepancy losses PART-OF DMPN. probabilistic interpretations FEATURE-OF domain discrepancy losses. one USED-FOR pseudo negative log likelihood. classification loss CONJUNCTION domain discrepancy losses. domain discrepancy losses CONJUNCTION classification loss. labeled source data CONJUNCTION domain discrepancy losses. domain discrepancy losses CONJUNCTION labeled source data. classification loss USED-FOR DMPN. labeled source data USED-FOR classification loss. DMPN USED-FOR discriminative and domain invariant features. domain discrepancy losses USED-FOR DMPN. Digits Image transfer task EVALUATE-FOR state - of - the - art approaches. Digits Image transfer task EVALUATE-FOR approach. mean accuracy EVALUATE-FOR DMPN. VisDA 2017 dataset EVALUATE-FOR DMPN. hyper - parameter sensitivity analysis EVALUATE-FOR approach. hyper - parameter changes FEATURE-OF approach. OtherScientificTerm are feature distribution discrepancy, feature distributions, Gaussian component means, and source feature distribution. Generic is methods. Task is UDA tasks. ","This paper proposes a method for unsupervised domain adaptation (UDA) based on Gaussian mixture distribution modeling. The proposed method is based on the Distribution Matching Prototypical Network (DMPN) framework, which uses Gaussian component means to model the source feature distribution. The authors propose to use domain discrepancy losses to learn discriminative and domain invariant features. The method is evaluated on the Digits Image Transfer task and VisDA 2017 dataset.","This paper proposes a method for unsupervised domain adaptation (UDA) where the target domain is the source domain and the target feature distribution is the target distribution. The proposed method is based on the Distribution Matching Prototypical Network (DMPN), which uses Gaussian mixture distributions to model the source and target feature distributions. The method is evaluated on the Digits Image transfer task and the VisDA 2017 dataset."
21078,SP:40be996e8bb86e887077b762b87c7c34a786ac98,"deep generative models USED-FOR tasks. Continuous Normalizing Flows ( CNFs ) HYPONYM-OF deep generative models. conditional image generation CONJUNCTION downstream predictive tasks. downstream predictive tasks CONJUNCTION conditional image generation. CNFs USED-FOR conditional image generation. CNFs USED-FOR downstream predictive tasks. model USED-FOR highdimensional latent code. class - specific supervised code CONJUNCTION unsupervised code. unsupervised code CONJUNCTION class - specific supervised code. InfoCNF HYPONYM-OF conditional CNF. unsupervised code PART-OF conditional CNF. class - specific supervised code PART-OF conditional CNF. gating networks USED-FOR error tolerances. gating networks USED-FOR ordinary differential equation ( ODE ) solvers. partitioning strategy USED-FOR InfoCNF. error tolerances FEATURE-OF ordinary differential equation ( ODE ) solvers. InfoCNF USED-FOR error tolerances. gating networks USED-FOR InfoCNF. test accuracy EVALUATE-FOR baseline. InfoCNF COMPARE baseline. baseline COMPARE InfoCNF. NFEs FEATURE-OF CIFAR10. likelihood scores EVALUATE-FOR InfoCNF. CIFAR10 EVALUATE-FOR InfoCNF. test accuracy EVALUATE-FOR InfoCNF. partitioning strategy USED-FOR extrapolation. partitioning strategy USED-FOR InfoCNF. time - series data USED-FOR InfoCNF. time - series data EVALUATE-FOR partitioning strategy. Task is exact likelihood estimation. OtherScientificTerm are latent space, and labeled information. ","This paper proposes a new continuous normalizing flow (CNF) model for conditional image generation. The proposed method is based on a class-specific supervised code and an unsupervised code. The supervised code is composed of a set of classes and a classifier that outputs a latent code for each class. The classifier is then used to predict the output of the classifier, which is then fed into the ODE to compute the likelihood of the image. The method is evaluated on CIFAR-10 and ImageNet and achieves state-of-the-art results.","This paper proposes a conditional continuous normalizing flow (CNF) model for conditional conditional image generation. The proposed method is based on a GAT-based approach to partitioning the latent space into two parts: a class-specific supervised code and an unsupervised code. The supervised code is composed of two parts, one for each class, and the other for the class of the data. The class-based code is learned by using a gating network to estimate the error tolerance of the class-dependent supervised code, which is then used as a partitioning strategy for the time-series data. Experiments on CIFAR-10 show that the proposed method outperforms the state-of-the-art. "
21087,SP:97764e3393216106ff2ac3f550845acf4636119f,"nonlinear functions USED-FOR approximation of the value function. Temporal - Difference ( TD ) learning algorithm USED-FOR nonlinear functions. lazy training regime FEATURE-OF algorithm. non - lazy TD learning USED-FOR models. Generic are problem, regime, model, and convergence. OtherScientificTerm are approximating function, learning process, scaling, and exponential convergence. Method are lazy training, neural networks, and underand over - parametrized frameworks. ","This paper studies the problem of temporal-difference (TD) learning with nonlinear functions, where the goal is to learn an approximation of the value function. The authors show that in the lazy training regime, the learning process scales linearly with the number of epochs. They also show that under and over-parameterized frameworks, under-and-over-parametrized frameworks can converge to the optimal solution. ","This paper studies the problem of temporal-difference (TD) learning for nonlinear functions, where the goal is to learn the approximation of the value function. The authors study the problem in the lazy training regime, and show that the convergence rate of the algorithm is exponential in this regime. They also show that in the under-parametrized regime, the algorithm converges faster than in the over-parameterized regime. "
21096,SP:c518e4030f12b0f59ad1d7c0fc0ebd313c68ef95,"reinforcement learning problem USED-FOR hypothesis verification. agents USED-FOR problem. action sequence CONJUNCTION post - condition. post - condition CONJUNCTION action sequence. pre - condition CONJUNCTION action sequence. action sequence CONJUNCTION pre - condition. Generic are agent, and they. ","This paper studies the problem of hypothesis verification in reinforcement learning, where the goal is to find a sequence of actions that satisfies a set of conditions on the state and action space. The authors propose to use a pre-condition on the action sequence and a post-condition to train an agent that is able to verify the existence of a hypothesis. The main contribution of the paper is to show that this problem can be solved using reinforcement learning.   ","This paper studies the problem of hypothesis verification in reinforcement learning. The main idea is to train an agent with a pre-condition and a post-condition on a sequence of actions. The agent is given an action sequence and a set of post-conditions, and the goal is to verify the hypothesis that the action sequence is the one that leads to the action. The authors show that the agent can be trained to verify a hypothesis that is the same as the pre-conditional action sequence, and that it can also be verified that the post-Conditional action sequences are the same. They also show that it is possible for the agent to verify that the hypothesis is true."
21105,SP:6fa2f842b1bc993ed8024a3ce13dbd91529c61be,"neural networks USED-FOR approximate reasoning. fixed dimensional latent space USED-FOR approximate reasoning. latent space FEATURE-OF approximate reasoning. formula space CONJUNCTION latent space. latent space CONJUNCTION formula space. embeddings COMPARE predicted embeddings. predicted embeddings COMPARE embeddings. formula space FEATURE-OF rewrite steps. latent space FEATURE-OF rewrite steps. graph neural networks USED-FOR rewrite - success of statements. mathematical disciplines PART-OF corpus of mathematical formulas. OtherScientificTerm are transformations, semantic features, vector space, rewrite rule, and predicted latent representations. Generic are reasoning, and they. ",This paper studies the problem of approximate reasoning in the latent space of mathematical formulas. The authors propose to use a graph neural network to predict the rewrite success of statements in the formula space. They show that the predicted embeddings are better than the true embedding of the true formula space in terms of the number of rewrite steps. They also show how the predicted latent representations can be used to estimate the rewrite-success of statements. ,"This paper studies the problem of approximate reasoning in a fixed dimensional latent space. The authors propose a new embedding for the formula space and a rewrite rule for the latent space, which they call the rewrite-success rule. The rewrite rule is based on a graph neural network, which is trained to predict the embeddings of the formula and the latent representations. They show that the rewrite rule can be used to improve the performance of the proposed embedding. "
21114,SP:a77ab500a5e7d4ea8430871d1e603941e92974fd,"multi - view geometry USED-FOR methods. approach USED-FOR depth. images CONJUNCTION sparse depth measurements. sparse depth measurements CONJUNCTION images. images USED-FOR approach. sparse depth measurements USED-FOR depth. images USED-FOR depth. global - local network architecture USED-FOR inductive bias. model USED-FOR monocular dense depth estimation. sparse ground truth USED-FOR model. sparse ground truth USED-FOR monocular dense depth estimation. global parameters USED-FOR metric agent motion. network USED-FOR global parameters. Method are Natural intelligent agents, artificial systems, and natural agents. OtherScientificTerm are equations of projective geometry, visual and haptic feedback, and sparse supervision. ","This paper proposes a method to estimate the depth of an agent from a set of images. The proposed method is based on a global-local network architecture, which is able to estimate depth from multiple views of the same image. The method is evaluated on a variety of vision and haptic tasks.   ","This paper proposes a method for learning depth estimation for natural agents. The method is based on a global-local network architecture, where the global parameters are learned using a sparse ground truth, and the local parameters are modeled as a global network. The proposed method is evaluated on a variety of natural and artificial environments.   "
21123,SP:2afba5e24478da4e9d493887c7cf00e288cc0deb,word pieces USED-FOR machine learning tasks. word pieces USED-FOR natural language models. natural language models USED-FOR machine learning tasks. machine learning tasks USED-FOR opaque ids. hash functions USED-FOR hash tokens. multi - layer Transformer USED-FOR Bloom filter digests. multi - layer Transformer USED-FOR models. accuracy EVALUATE-FOR models. They COMPARE models. models COMPARE They. computational budget FEATURE-OF sampled softmax. sampled softmax USED-FOR models. multi - layer Transformer USED-FOR Bloom filter digests. method USED-FOR problems. this USED-FOR problems. this USED-FOR method. large vocabulary size FEATURE-OF problems. Method is Bloom filter. OtherScientificTerm is hashing. ,"This paper proposes to use a Transformer-based model to solve the Bloom filter problem. The proposed method is based on the idea of using a hash function to hash the word tokens into a set of hash tokens, which are then used as input to a neural network to predict the output of a Bloom filter. The authors show that the proposed method outperforms the state-of-the-art methods in terms of accuracy and computational cost. ","This paper proposes a new method for learning Bloom filter digests. The proposed method is based on a multi-layer Transformer, where each layer is composed of a set of tokens, and each token is represented by a hash function. The authors show that the proposed method can be applied to a wide range of tasks, and that it can be used to learn Bloom filters for a variety of tasks. They also show that their method is more efficient than existing methods."
21132,SP:745dd86d7f7bba79a02d27922003b764b620f83e,"grouping policy USED-FOR small part proposals. grouping policy USED-FOR learningbased agglomerative clustering framework. local context USED-FOR part - level features. largescale fine - grained 3D part dataset EVALUATE-FOR method. method USED-FOR knowledge of parts. PartNet HYPONYM-OF largescale fine - grained 3D part dataset. shape segmentation baselines COMPARE approach. approach COMPARE shape segmentation baselines. Task are discovering 3D parts, and contextual bandit problem. Generic is prior. Method is data - driven shape segmentation approaches. "," is a novel approach to learning 3D parts from a large part dataset. The proposed method is based on a learning-based agglomerative clustering framework, where the part-level features are learned from the local context. The method is evaluated on a large-scale fine-grained 3D part dataset and achieves state-of-the-art results. ","This paper proposes a learning-based agglomerative clustering framework for 3D shape segmentation. The proposed method is based on the idea of contextual bandit, which is to learn the local context of the 3D part-level features. The method is evaluated on the PartNet dataset, a large-scale fine-grained part dataset of 3D shapes. "
21141,SP:868fc6df740b04963442d5abcfe2f4845585cfc8,"input / output datasets USED-FOR they. gender - related characteristics CONJUNCTION hair color. hair color CONJUNCTION gender - related characteristics. generative adversarial network ( GAN ) USED-FOR images of black - haired men. edit USED-FOR transformation. latent space FEATURE-OF transformation. autoencoder USED-FOR transformed data. editing transformation USED-FOR transformed data. transformation USED-FOR complex and non - linear transformations. latent trained space USED-FOR transformation. data domains CONJUNCTION modalities. modalities CONJUNCTION data domains. modalities CONJUNCTION applications. applications CONJUNCTION modalities. technique USED-FOR data domains. applications PART-OF biology. image transformations USED-FOR it. removal of batch artifacts HYPONYM-OF biology. removal of batch artifacts HYPONYM-OF applications. Method are generative neural networks, discriminator, generative models, and neuron editing. OtherScientificTerm are blond - haired men, source distribution, target distribution, manifold, distribution shifts, neuron ’s activations, unwanted noise, and drug treatments. Material is images of black - haired women. ","This paper proposes a method to edit neurons in GANs to remove unwanted noise from the input data. The method is based on the idea that the source distribution of a GAN can be transformed into the target distribution, which is then fed to an autoencoder to generate a transformed image. The authors show that the resulting transformed image can be used to edit the activations of neurons in the discriminator to remove the unwanted noise. They show that this method can be applied to image transformations in a variety of data modalities. ","This paper proposes a method for generating images of black-haired men from images of blond-haired women using a GAN-based discriminator. The method is based on an autoencoder-based model, where the data is first transformed into a latent space, and then the latent space is transformed into the target distribution. The model is trained using a discriminator-based GAN. The authors show that the model is able to generate images of the source distribution and target distribution from the same latent space. They also show that their method can be applied to a wide range of data domains and modalities."
21150,SP:6dee6932e64fe47bb44dd42fc242fa9d89b8d89c,"training algorithms CONJUNCTION model architectures. model architectures CONJUNCTION training algorithms. reinforcement learning CONJUNCTION image semantic segmentation. image semantic segmentation CONJUNCTION reinforcement learning. few - shot image classification CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION few - shot image classification. model architectures USED-FOR few - shot domain. meta - learning approaches USED-FOR few - shot image classification. meta - learning approaches USED-FOR reinforcement learning. training algorithms USED-FOR few - shot domain. neural network representations USED-FOR meta - learning approaches. learning systems USED-FOR few - shot to many - shot settings. first - order meta - learning of initializations USED-FOR deep neural networks. first - order meta - learning of initializations USED-FOR dense, structured predictions. FOMAML CONJUNCTION Reptile. Reptile CONJUNCTION FOMAML. neural network architecture USED-FOR fast learning. generalization error EVALUATE-FOR meta - learning algorithms. small benchmark dataset EVALUATE-FOR meta - learning systems. meta - learning systems USED-FOR fewand many - shot settings. EfficientLab HYPONYM-OF neural network architecture. FP - k HYPONYM-OF small benchmark dataset. meta - learned initializations USED-FOR image segmentation. meta - learned initializations USED-FOR canonical few - shot learning problems. meta - learned initializations COMPARE random and ImageNet - trained initializations. random and ImageNet - trained initializations COMPARE meta - learned initializations. update routine USED-FOR tasks. FSS-1000 dataset EVALUATE-FOR network. Method are ensembling many models, relation networks, and MAML - type algorithms. OtherScientificTerm is initialization. Generic are task, and model. ","This paper proposes a meta-learning approach for few-shot image classification and image semantic segmentation. The idea is to use MAML-based initializations to learn dense, structured predictions for dense and structured predictions. The proposed method is evaluated on the FP-k and FSS-1000 datasets. The results show that the proposed method outperforms the baseline methods in terms of accuracy.","This paper presents a meta-learning approach for few-shot image classification and reinforcement learning. The authors propose a method for first-order meta-learning of initializations of deep neural networks. The method is based on a MAML-type algorithm, where the model is ensembled with a relation network, and the initializations are structured into dense, structured predictions. The proposed method is evaluated on the FSS-1000 dataset and the FP-k dataset, where it is shown that the proposed method outperforms random and ImageNet-trained initializations. "
21159,SP:ec6f390f6d45fb79c33ae5d9c8a24cadb96fbd60,"unlabelled data USED-FOR few - shot learning. Prototypical Random Walk Networks(PRWN ) HYPONYM-OF SS - FSL approach. Prototypical Networks ( PN ) USED-FOR SS - FSL approach. Prototypical Networks ( PN ) USED-FOR Prototypical Random Walk Networks(PRWN ). random walk semi - supervised loss USED-FOR representations. random walk semi - supervised loss USED-FOR network. network USED-FOR representations. graph - based approaches USED-FOR few - shot learning. prototypical random walk notion USED-FOR compact and well - separated class representations. model COMPARE art. art COMPARE model. model COMPARE fully supervised prototypical networks. fully supervised prototypical networks COMPARE model. 1 - shot mini - Imagenet case EVALUATE-FOR it. accuracy EVALUATE-FOR it. robustness FEATURE-OF labelled / unlabelled class distribution mismatch. discriminative power test EVALUATE-FOR baseline. Task are human intelligence, transductive setting, and mini - Imagenet 5 - shot classification task. Method is AI models. OtherScientificTerm are graph - NN parameters, distractors, and unlabeled data. Material are collective test set, mini - Imagenet, and Omniglot. ",This paper proposes Prototypical Random Walk Networks (PRWN) for few-shot learning in the transductive setting. PRWN is based on the prototypical random walk notion and uses a graph-NN based semi-supervised loss to learn compact and well-separated class representations. The proposed method achieves state-of-the-art performance in the 1-shot mini-Imagenet and Omniglot tasks. ,This paper proposes a new method for few-shot learning from unlabeled data in the transductive setting. The proposed method is based on the Prototypical Random Walk Networks (PRWN) framework. PRWN is a semi-supervised method that uses the prototypical random walk notion to learn compact and well-separated class representations. The method is evaluated on the mini-Imagenet 5-shot classification task and Omniglot dataset. 
21168,SP:d12e687bd2ee9fa60554312e644bb0a6487974f1,"machine learning USED-FOR remote sensing. labeled data USED-FOR machine learning. deep convolutional neural networks HYPONYM-OF models. selfsupervised learning approaches USED-FOR remote sensing domain. multi - sensor, multi - channel information USED-FOR remote sensing applications. Contrastive Sensor Fusion USED-FOR representations. Contrastive Sensor Fusion HYPONYM-OF self - supervised training objective. model USED-FOR representation. encoder USED-FOR representations. dataset USED-FOR encoder. unlabeled coterminous image triplets PART-OF dataset. remote sensing classification task EVALUATE-FOR representations. Material are unlabeled data, and coterminous data. Method is fused multi - sensor representations. Generic is method. "," sensors are used in remote sensing applications. In this paper, the authors propose a self-supervised training objective for multi-sensor image fusion. The proposed method is based on contrastive sensor fusion, which is a technique for learning multi-channel representations from unlabeled data. The method is evaluated on a remote sensing classification task.   ","This paper proposes Contrastive Sensor Fusion (CSF), a self-supervised training objective for multi-channel remote sensing applications. CSF is based on contrastive sensor fusion, which is a technique for learning multi-sensor representations from unlabeled coterminous image triplets. The proposed method is evaluated on a remote sensing classification task, where it achieves state-of-the-art performance."
21177,SP:4d8e054f07006b4f896721b5c24da805727d2c22,"fine - tuning HYPONYM-OF retraining technique. fine - tuning COMPARE retraining techniques. retraining techniques COMPARE fine - tuning. Learning rate rewinding USED-FOR unpruned weights. learning rate schedule USED-FOR weight rewinding. Learning rate rewinding COMPARE weight rewinding. weight rewinding COMPARE Learning rate rewinding. learning rate schedule USED-FOR Learning rate rewinding. rewinding techniques COMPARE fine - tuning. fine - tuning COMPARE rewinding techniques. accuracy CONJUNCTION compression ratios. compression ratios CONJUNCTION accuracy. rewinding techniques USED-FOR network - agnostic pruning algorithm. compression ratios EVALUATE-FOR network - agnostic pruning algorithm. accuracy EVALUATE-FOR network - agnostic pruning algorithm. Method are neural network pruning algorithms, and Weight rewinding. Generic is network. OtherScientificTerm are learning rate, and training schedule. ",This paper studies the effect of weight rewinding on the performance of neural network pruning methods. The authors show that weight re-weighting is beneficial in terms of accuracy and compression ratios. They show that re-weighing the weights at the beginning and end of the training process can improve the accuracy of the network. They also show that the re-winding can be used as an alternative to fine-tuning.   ,"This paper proposes a new method for weight rewinding for neural network pruning. The main idea is to re-weight the weights of the weights that are unpruned from the training set, and then re-train the weights with a learning rate that is similar to the learning rate of the original weights. The authors show that this method is more efficient than fine-tuning, and that it can be applied to a wide range of network-agnostic pruning algorithms. They also show that the proposed method can be used to improve the accuracy and compression ratio of the network."
21186,SP:3bb1c79f9482e09828eda45fbb2e654f37219365,normalized ) output margin CONJUNCTION generalization. generalization CONJUNCTION normalized ) output margin. output margin FEATURE-OF generalization. all - layer margin HYPONYM-OF margin. generalization EVALUATE-FOR deep models. theoretically inspired training algorithm USED-FOR all - layer margin. neural net USED-FOR adversarially robust setting. robust test error FEATURE-OF deep networks. Jacobian and hidden layer norms USED-FOR neural nets. Method is linear classifiers. Generic is algorithm. ,"This paper studies the relationship between the output margin and generalization in deep neural networks. The authors show that the all-layer margin is the best way to measure the generalization performance of deep networks. They show that this is the case for both Jacobian and hidden layer norms. They also show that for adversarially robust networks, this is also the case. ","This paper studies the generalization properties of neural networks in the adversarially robust setting. The authors show that the all-layer margin (i.e., the output margin of a neural network) can be used as a measure of generalization. They show that it can be a function of the Jacobian and hidden layer norms of the network. They also provide a theoretically inspired training algorithm that can be applied to the case where the network is robust to adversarial attacks."
21195,SP:3d44f27468087280e85dfb1fc7291db05179fe6d,"ungrounded dialogues CONJUNCTION unstructured documents. unstructured documents CONJUNCTION ungrounded dialogues. unstructured documents USED-FOR model. ungrounded dialogues USED-FOR model. limited training examples USED-FOR small parameters. benchmarks EVALUATE-FOR model. out - of - domain knowledge EVALUATE-FOR model. Task are intelligent conversational agent, and knowledge - grounded dialogue generation. Material is knowledge - grounded dialogues. Method are response generation model, disentangled response decoder, and generation model. ","This paper proposes a method for knowledge-ground dialogue generation from ungrounded dialogs. The method is based on a disentangled response decoder, which is used to generate a set of disentanglement terms for each dialogue. The decoder is then used to predict the response of the dialogue generation model. The proposed method is evaluated on a variety of tasks, including out-of-domain knowledge generation, knowledge-based dialogue generation, and knowledge-driven dialogue generation. ","This paper proposes a method for knowledge-grounded dialogue generation from unstructured dialogues. The method is based on a disentangled response decoder, which is used to generate dialogues that are disentanglement-free. The model is trained with limited training examples and is evaluated on a variety of tasks, including out-of-domain knowledge generation, ungrounded dialogues, and un-structured documents."
21204,SP:9b555f7fe743f5effdbdc8701ed519ce3159c4b0,"parallel corpus USED-FOR neural machine translation models ( NMT ). non - parallel bilingual data USED-FOR decoding. non - parallel bilingual data USED-FOR training. training CONJUNCTION decoding. decoding CONJUNCTION training. non - parallel bilingual data USED-FOR Existing approaches. source to target translation model CONJUNCTION target to source translation model. target to source translation model CONJUNCTION source to target translation model. target to source translation model CONJUNCTION language models. language models CONJUNCTION target to source translation model. mirror - generative NMT ( MGNMT ) HYPONYM-OF single unified architecture. source to target translation model PART-OF single unified architecture. target to source translation model PART-OF single unified architecture. language models PART-OF single unified architecture. translation models CONJUNCTION language models. language models CONJUNCTION translation models. latent semantic space FEATURE-OF language models. non - parallel data USED-FOR translation directions. translation models CONJUNCTION language models. language models CONJUNCTION translation models. language models USED-FOR decoding. translation models USED-FOR decoding. MGNMT COMPARE approaches. approaches COMPARE MGNMT. Material are non - parallel corpora, and resource - rich and low - resource situations. ", translation models are trained on non-parallel bilingual data. This paper proposes a mirror-generative neural machine translation model (MGNMT) that combines a source-to-target translation model and a language model to learn the translation directions. The proposed method is evaluated on two tasks: training and decoding.   ,"This paper proposes a new model for training neural machine translation models (NMT) on non-parallel bilingual data. The proposed model is based on mirror-generative NMT (MGNMT), which combines a source-to-target translation model and a language model with a target to source translation model. The model is trained using a single unified architecture, where the source and target translation models are trained on the same dataset, and the language model is used to encode the translation directions in the latent semantic space of the language models. Experiments show that the proposed model outperforms the state-of-the-art in terms of accuracy and training time."
21213,SP:d7a530a0ec4112095a58cef4cda9646f8ca6449d,maximum entropy reinforcement learning algorithms USED-FOR Deep Reinforcement Learning ( DRL ). benchmarks EVALUATE-FOR sample efficiency. entropy term USED-FOR maximum entropy algorithms. entropy term USED-FOR bounded nature of the action spaces. entropy term PART-OF Soft Actor Critic ( SAC ). entropy term USED-FOR Mujoco benchmark. streamlined algorithms USED-FOR SAC. entropy maximization USED-FOR streamlined algorithms. non - uniform sampling method USED-FOR transitions. transitions PART-OF replay buffer. streamlined algorithm COMPARE SAC. SAC COMPARE streamlined algorithm. continuous control tasks EVALUATE-FOR streamlined algorithm. non - uniform sampling scheme USED-FOR streamlined algorithm. OtherScientificTerm is maximum entropy objective. Task is training. ,This paper proposes a method to improve the sample efficiency of maximum entropy reinforcement learning (MRL) algorithms. The main idea is to use a non-uniform sampling scheme to sample from the replay buffer during training. The proposed method is evaluated on Mujoco benchmark and achieves state-of-the-art performance. ,This paper proposes a new algorithm for maximum entropy reinforcement learning. The main contribution of the paper is to propose a non-uniform sampling scheme for the soft actor critic (SAC) algorithm. The proposed method is based on the Mujoco benchmark. The authors show that the proposed method outperforms the state-of-the-art in terms of sample efficiency. 
21222,SP:545e8da553fcb47d84eaa044d8a4947d3cd3230e,"machine learning models USED-FOR adversarial attacks. industrial copyright detection tools USED-FOR web. industrial copyright detection tools USED-FOR adversarial attacks. neural net USED-FOR system. gradient methods USED-FOR system. AudioTag copyright detector CONJUNCTION YouTube ’s Content ID system. YouTube ’s Content ID system CONJUNCTION AudioTag copyright detector. Adversarial music USED-FOR industrial systems. YouTube ’s Content ID system HYPONYM-OF industrial systems. AudioTag copyright detector HYPONYM-OF industrial systems. Method are classifier, copyright detection systems, neural network based systems, and music identification method. Generic is they. OtherScientificTerm is attacks. Material is adversarial examples. ","This paper studies the problem of adversarial attacks on industrial copyright detection systems. The authors propose to use adversarial music as an input to a neural network-based system, which is trained using gradient methods. They show that adversarial examples can be generated by perturbing the weights of the classifier. They also show that the proposed method can be used to improve the performance of the system. ","This paper studies the problem of adversarial attacks on industrial copyright detection systems. The authors propose a new method for identifying the source of the adversarial examples. The method is based on a neural network-based system, and the authors show that it can be used to identify the source and target of the attacks. They also provide a theoretical analysis of the effectiveness of the method."
21231,SP:b511822850da3bf1079a36ed6f5ad4db80fbc424,"visual explanation USED-FOR deep metric learning. model COMPARE classification. classification COMPARE model. framework USED-FOR metric learning applications. framework USED-FOR model. cross - view pattern discovery CONJUNCTION interactive retrieval. interactive retrieval CONJUNCTION cross - view pattern discovery. interactive retrieval HYPONYM-OF applications. cross - view pattern discovery HYPONYM-OF applications. Method are learning representation, and metric learning. OtherScientificTerm are overall activation map, and point - to - point activation intensity. ","This paper proposes a method for visual explanation in deep metric learning. The method is based on the observation that the overall activation map is more informative than the point-to-point activation intensity. The authors propose to use this observation to learn a representation of the activation map, which is then used to train a metric learning model. The proposed method is evaluated on image classification and retrieval tasks.   ","This paper proposes a new metric learning framework for visual explanations for deep metric learning. The key idea is to use a point-to-point activation map, where each point in the activation map is represented as a set of points with different activation intensity. The authors show that this mapping can be used to learn representations for both the overall activation map and the point to point activation map. They also show that their method can be applied to a variety of applications such as cross-view pattern discovery and interactive retrieval."
21240,SP:67bf71219fe6bedec5f5525200e734638e4a6ca2,"learning control USED-FOR online lifelong learning scenario. they USED-FOR failure modes. computational resources USED-FOR model - based planning methods. model - based planning CONJUNCTION model - free learning. model - free learning CONJUNCTION model - based planning. planner CONJUNCTION model - free components. model - free components CONJUNCTION planner. Method are model - free policy learning methods, Adaptive Online Planning ( AOP ), AOP, continual learning agent, and reinforcement learning methods. OtherScientificTerm are compact networks, performance degradation, dynamics, constrained computation limits, and unpredictable changes in the world. Generic are setting, and algorithm. Task is planning. ","This paper studies the problem of continual learning in the online lifelong learning setting, where the goal is to learn a compact network that can be used for planning in the future. The authors propose a method called Adaptive Online Planning (AOP) that combines model-based planning and model-free learning. They show that AOP is able to achieve better performance than previous methods in this setting. They also provide a theoretical analysis of the performance degradation of AOP in terms of dynamics and constrained computation.","This paper studies the problem of adaptive online planning (AOP) in the lifelong learning setting, where the goal is to learn a continuous learning agent that can adaptively plan for the future. The main contribution of the paper is to provide a theoretical analysis of the performance degradation of AOP in the context of model-based planning, model-free learning, and continual learning. The authors show that AOP can suffer from performance degradation due to constrained computation limits, and unpredictable changes in the world. They also provide an algorithm for AOP, which is based on a combination of a planner and a planner component."
21249,SP:11159cb878a436a5d4fc6edb4132f2cc3c1b3f72,"Visual attention mechanisms USED-FOR image captioning models. sparsemax CONJUNCTION Total - Variation Sparse Attention ( TVMAX ). Total - Variation Sparse Attention ( TVMAX ) CONJUNCTION sparsemax. sparsity - promoting transformations USED-FOR softmax attention mechanism. Total - Variation Sparse Attention ( TVMAX ) HYPONYM-OF sparsity - promoting transformations. sparsemax HYPONYM-OF sparsity - promoting transformations. sparsemax USED-FOR sparse attention weights. interpretability EVALUATE-FOR TVMAX transformation. humanrated caption quality CONJUNCTION attention relevance. attention relevance CONJUNCTION humanrated caption quality. TVMAX COMPARE attention mechanisms. attention mechanisms COMPARE TVMAX. attention relevance EVALUATE-FOR attention mechanisms. attention relevance EVALUATE-FOR TVMAX. humanrated caption quality EVALUATE-FOR attention mechanisms. humanrated caption quality EVALUATE-FOR TVMAX. OtherScientificTerm are image structure, relevant features, and sparsity. Material is Microsoft COCO and Flickr30k datasets. Method is softmax. ",This paper proposes a novel attention mechanism for image captioning models. The proposed attention mechanism is based on the idea that sparse attention weights are more informative than the softmax attention weights. The authors propose a new attention mechanism called Total-variation Sparse Attention (TVMAX) to improve the interpretability of the attention mechanism. TVMAX is a combination of sparse-max and total-variance sparse attention. Theoretical analysis is provided to show that the proposed TVMAX can improve the human-rated caption quality. Experiments are conducted on COCO and Flickr30k datasets to show the effectiveness of the proposed method.,This paper proposes a new visual attention mechanism for image captioning called TVMAX. TVMAX is based on sparsemax and total-variation sparse attention (TVMAX). The authors show that TVMAX improves human-rated caption quality and attention relevance compared to other visual attention mechanisms. They also show that the TVMAX attention mechanism is more interpretable compared to softmax. 
21258,SP:fb0c3ce3db6ad674ddc615bdc6203cdcbe42c804,Neural networks USED-FOR structured data. Neural networks USED-FOR graphs. graphs HYPONYM-OF structured data. Predicting the evolution of dynamic graphs PART-OF graph mining. model USED-FOR evolution of dynamic graphs. graph neural network USED-FOR temporal evolution patterns of dynamic graphs. graph neural network CONJUNCTION recurrent architecture. recurrent architecture CONJUNCTION graph neural network. recurrent architecture USED-FOR temporal evolution patterns of dynamic graphs. generative model USED-FOR graph instance. graph instance USED-FOR topology. common network evolving dynamics FEATURE-OF artificial datasets. real - world datasets EVALUATE-FOR model. artificial datasets EVALUATE-FOR model. OtherScientificTerm is static graphs. Material is real - world networks. Generic is task. ,This paper proposes a method for predicting the evolution of dynamic graphs in graph mining. The method is based on a graph neural network and a recurrent architecture. The model is trained to predict the temporal evolution patterns of the dynamic graphs. The proposed method is evaluated on two synthetic and two real-world datasets.,This paper proposes a novel approach to predict the evolution of dynamic graphs in graph mining. The authors propose a generative model that predicts the topology of a dynamic graph by using a graph neural network and a recurrent architecture. They show that their approach outperforms the state-of-the-art on several real-world datasets. 
21267,SP:ff722957a1765c0568426ed88dd910a6b74054ef,"incomplete datasets USED-FOR machine learning applications. missing data imputation techniques USED-FOR filling missing values. method USED-FOR imputing missing features. method USED-FOR distribution of target assignments. incomplete data USED-FOR distribution of target assignments. generator network USED-FOR imputations. generator network USED-FOR imputations. predictor network USED-FOR classification uncertainties. generator network USED-FOR predictor network. imputed samples USED-FOR predictor network. CIFAR-10 image dataset CONJUNCTION real - world tabular classification datasets. real - world tabular classification datasets CONJUNCTION CIFAR-10 image dataset. real - world tabular classification datasets EVALUATE-FOR method. CIFAR-10 image dataset EVALUATE-FOR method. method USED-FOR generating imputations. class uncertainties FEATURE-OF classification task. OtherScientificTerm are missing values, distribution of missing values, missing features, and missingness rates. Method is discriminator network. ","This paper proposes a method for imputing missing features from incomplete data. The method is based on imputing the distribution of target assignments from the incomplete data, which are generated by a generator network and a predictor network. The generator network is trained to generate imputations from the missing data, while the predictor network is used to predict the class uncertainties. The imputation samples are then used to train the discriminator network. Experiments on CIFAR-10 image dataset and tabular classification tasks show that the proposed method outperforms previous methods.","This paper proposes a method for imputing missing features from incomplete datasets. The method is based on a discriminator network that is trained to predict the distribution of missing features, and a generator network that generates imputations from the imputations. The generator network is trained with a predictor network that predicts the uncertainty of the missing features. The imputations are then used to train the discriminator. The proposed method is evaluated on CIFAR-10 image dataset and tabular classification datasets."
21276,SP:c051b0fe779d9e4131016970b7ba469b596f3009,"Off - policy estimation USED-FOR long - horizon problems. healthcare CONJUNCTION robotics. robotics CONJUNCTION healthcare. Off - policy estimation USED-FOR real - life applications. robotics HYPONYM-OF real - life applications. healthcare HYPONYM-OF real - life applications. curse of horizon FEATURE-OF importance - sampling - based methods. stationary distribution FEATURE-OF known behavior policy. estimator USED-FOR importance ratios of stationary distributions. Reproducing Kernel Hilbert Spaces ( RKHSs ) USED-FOR estimator. asymptotic consistency CONJUNCTION finite - sample generalization. finite - sample generalization CONJUNCTION asymptotic consistency. Method is high - fidelity simulators. Task is on - policy evaluation. Generic are approach, it, problem, and operator. Material is off - policy data. ","This paper proposes an off-policy estimation method for long-horizon optimization problems, where the goal is to estimate the importance ratio of a known behavior policy with respect to a known stationary distribution. The proposed method is based on an estimator of the importance ratios of stationary distributions in RKHSs. The authors show that the estimator is consistent and generalizable to finite-sample generalization. The main contribution of the paper is a new estimator for importance sampling-based methods, which can be applied to long-term optimization problems.",This paper proposes a new method for off-policy estimation for long-horizon problems. The main idea is to estimate the importance of a stationary distribution over a known behavior policy in a high-fidelity simulator. The authors propose a new estimator based on the Reproducing Kernel Hilbert Spaces (RKHSs) that can be used to compute the importance ratio of the stationary distribution. They show that their estimator is consistent with the asymptotic consistency and finite-sample generalization of their method. 
21285,SP:065c900843011a71b70ed35357a2f71fe83872a7,"probabilistic framework USED-FOR dataset. Mixture Model ( MM ) HYPONYM-OF probabilistic framework. modes PART-OF dataset. Gaussian distribution FEATURE-OF modes. paintings dataset CONJUNCTION fashion images. fashion images CONJUNCTION paintings dataset. unlabelled modes PART-OF large datasets. fashion images HYPONYM-OF large datasets. paintings dataset HYPONYM-OF large datasets. plausible method USED-FOR probabilities. Generative Adversarial Network ( GAN ) framework USED-FOR plausible method. GAN USED-FOR distribution. GAN USED-FOR classification network. techniques USED-FOR unsupervised dataset. smooth linear interpolation USED-FOR outdistribution ” data. Method are Gaussian MM, GMM, and GMM paradigm. OtherScientificTerm are conditional likelihood, distribution index, Euclidean distances, responsibility distribution, latent representation of x, and dataset segments. Generic is responsibility. ","This paper proposes a new probabilistic framework for unsupervised learning of unlabeled modes in images. The proposed framework is based on the Gaussian mixture model (GMM) framework, where the mode distribution is modeled by a Gaussian distribution. The authors show that the proposed framework can be used to estimate the probability distribution of the modes in the dataset, which is then used to train a generative adversarial network (GAN) to predict the probabilities of each mode in the data.  ","This paper proposes a new probabilistic framework for unsupervised data classification using a Gaussian Mixture Model (MM) framework. The proposed framework is based on a GMM-based framework, where the modes of the data are modeled as Gaussian distributions. The authors propose a GAN-based approach to generate a plausible method to estimate the probabilities of the modes. The method is based upon a GNN-based method, which is a variant of the GAN framework.   "
21294,SP:2da1608209058d214f8671062cc9eb0833ba4831,method USED-FOR large capacity neural networks. accuracy CONJUNCTION dynamic computational cost. dynamic computational cost CONJUNCTION accuracy. accuracy EVALUATE-FOR method. fine - grained - level FEATURE-OF deep - learning architecture. residual block architecture USED-FOR convolutional channels. fine - grained manner FEATURE-OF residual block architecture. fine - grained manner FEATURE-OF convolutional channels. marginal aggregate posteriors of features PART-OF neural network. pre - specified prior distribution FEATURE-OF marginal aggregate posteriors of features. technique USED-FOR gates. CIFAR-10 and ImageNet datasets USED-FOR image classification. Cityscapes USED-FOR semantic segmentation. image classification CONJUNCTION Cityscapes. Cityscapes CONJUNCTION image classification. average computational cost COMPARE architecture. architecture COMPARE average computational cost. method USED-FOR architectures. method COMPARE architecture. architecture COMPARE method. ImageNet EVALUATE-FOR ResNet34 gated networks. accuracy EVALUATE-FOR ResNet18 model. top-1 accuracy EVALUATE-FOR ResNet34 gated networks. complexity EVALUATE-FOR ResNet18 model. features CONJUNCTION features. features CONJUNCTION features. features USED-FOR networks. OtherScientificTerm is convolutional maps. Generic is network. Method is batch - shaping. ,"This paper proposes a method to reduce the computational cost of convolutional layers in deep neural networks. The proposed method is based on the marginal aggregate posteriors of features in a neural network, which are aggregated from a pre-specified prior distribution. The method is evaluated on CIFAR-10 and ImageNet, where it achieves state-of-the-art accuracy on both image classification and semantic segmentation.","This paper proposes a new method for batch-shaping convolutional layers of deep neural networks. The proposed method is based on the notion of marginal aggregate posteriors of features, which is used as a pre-specified prior distribution. The authors show that the proposed method can achieve better accuracy and lower computational cost compared to existing methods. The method is evaluated on CIFAR-10 and ImageNet datasets, and compared to a number of baselines."
21303,SP:f90e9f0eb53f92601bdfa3f7bf86f71d037aad30,"probabilistic importance inference approach USED-FOR pruning DNNs. approach COMPARE techniques. techniques COMPARE approach. lossless compression rates EVALUATE-FOR techniques. lossless compression rates EVALUATE-FOR approach. Method are Deep neural networks ( DNNs ), DNNs, DNN, and nonparemetric scoring test. OtherScientificTerm are energy and computational resources, and DNN ’s outputs. ",This paper proposes a probabilistic importance inference approach for pruning DNNs. The main idea is to use a non-paremetric scoring test to evaluate the importance of each input to the pruned DNN. The proposed method is based on the idea that the importance is proportional to the number of pruned outputs. The authors show that the proposed method can be used to perform pruning in a way that does not require too much computational resources.,This paper proposes a probabilistic importance inference approach for pruning DNNs. The main idea is to use a non-paremetric scoring test to determine the importance of a DNN's outputs. The authors show that their approach outperforms the state-of-the-art in terms of energy and computational resources. 
21312,SP:64cbbb6a2f6847ef71cd5a23ba3e4cc5c815a56e,"approaches USED-FOR hierarchical reinforcement learning. approaches USED-FOR sub - goal structure. method USED-FOR iteratively compressing action trajectories. iteratively compressing action trajectories USED-FOR nested behavioral hierarchies. method USED-FOR nested behavioral hierarchies. action primitives USED-FOR deeper hierarchies. approach USED-FOR learning. transfer USED-FOR approach. Generic is perspective. OtherScientificTerm are compact code of action trajectories, and non - trivial hierarchical structure. "," is a method for hierarchical reinforcement learning that compresses action trajectories into a compact code of sub-goals. The method is based on the observation that the sub-goal space can be decomposed into sub-primitives, which can then be used as action primitives to learn deeper hierarchies. The proposed method is evaluated on a variety of tasks and compared with a number of baselines. ","This paper proposes a new approach for hierarchical reinforcement learning. The key idea is to learn a compact code of action trajectories that can be easily transferred across different sub-goal hierarchies. This is achieved by iteratively compressing the action trajectory into a set of action primitives that are then used to learn the hierarchical structure of the sub-goals. The authors show that their approach can be applied to a variety of reinforcement learning tasks, and that it can be used to improve the transferability of the learned actions."
21321,SP:e1ccfb3a684aef8a0fb36194eb16af1667811e81,"generative models USED-FOR complex data. Autoencoders HYPONYM-OF generative models. images HYPONYM-OF complex data. variational autoencoder ( VAE ) HYPONYM-OF models. unimodal Gaussian decoders USED-FOR models. Hierarchical Bayes Autoencoder ( HBAE ) HYPONYM-OF probabilistic generative model. energybased model ( EBM ) USED-FOR multimodal decoder. multimodal decoder PART-OF HBAE. variational inference USED-FOR VAE. variational inference USED-FOR HBAE. conditional generator USED-FOR EBM distribution. adversarial approximation USED-FOR decoder. conditional generator USED-FOR stochastic reconstruction. code USED-FOR stochastic reconstruction. sampling steps PART-OF HBAE. HBAE USED-FOR sets. latent code USED-FOR HBAE. decoder USED-FOR realistic unconditional samples. single image and set cases EVALUATE-FOR decoder. model USED-FOR complex image sets. Set - HBAE USED-FOR complex image sets. Set - HBAE HYPONYM-OF model. OtherScientificTerm are semantic variations, and latent codes. Method is unimodal Gaussian distribution. ","This paper proposes a Hierarchical Bayes Autoencoder (HBAE), a probabilistic generative model that uses an energy-based model (EMB) for the multimodal decoder. The proposed method is based on variational inference with a conditional generator and an adversarial approximation to the decoder, which is used for stochastic reconstruction. Experiments show that the proposed method achieves state-of-the-art performance on image classification tasks. ","This paper proposes a Hierarchical Bayes Autoencoder (HBAE) model for multi-modal Gaussian decoders. The proposed model is based on an energy-based model (EBM) and a variational autoencoding (VAE) framework. The model is evaluated on a variety of datasets, and it is shown that the proposed model outperforms the state-of-the-art VAE on both single image and set cases. "
21330,SP:1130a391afa30d1e0fddadedd2a3aaa70a4cb751,"deep off - policy TD algorithms CONJUNCTION feature normalization techniques. feature normalization techniques CONJUNCTION deep off - policy TD algorithms. normalization USED-FOR target networks. normalization USED-FOR optimization stability. mixture of onand off - policy transitions USED-FOR normalization. batch normalization USED-FOR It. DDPG CONJUNCTION TD3. TD3 CONJUNCTION DDPG. cross - normalization USED-FOR TD3. cross - normalization USED-FOR DDPG. MuJoCo benchmark tasks EVALUATE-FOR cross - normalization. Method are reinforcement learning ( RL ) algorithms, normalization techniques, and off - policy learning. ",This paper studies the effect of normalization on off-policy training in reinforcement learning. The authors propose to use batch normalization to improve the stability of the target network. They show that the normalization improves the performance of TD3 and DDPG on MuJoCo tasks.   ,This paper proposes a new normalization technique called cross-normalization for off-policy reinforcement learning (OTL). The idea is to use a mixture of on-and-off-policy transitions to improve the stability of the target network. The authors show that cross-normation improves the performance of TD3 and DDPG on MuJoCo benchmark tasks. They also show that it can be used to improve performance on TD3.
21339,SP:f9cafaa5131176290fa069e6d24046c079cd9eea,"bias CONJUNCTION confounding effects. confounding effects CONJUNCTION bias. spurious associations of confounding variables HYPONYM-OF challenges. residualization CONJUNCTION stratification. stratification CONJUNCTION residualization. precomputed features USED-FOR confounding variables. techniques USED-FOR precomputed features. stratification HYPONYM-OF techniques. residualization HYPONYM-OF techniques. techniques USED-FOR statistical methods. techniques USED-FOR end - to - end deep learning methods. method USED-FOR discriminative features. adversarial training strategy USED-FOR discriminative features. adversarial training strategy USED-FOR method. synthetic data CONJUNCTION medical images. medical images CONJUNCTION synthetic data. method USED-FOR synthetic data. medical images EVALUATE-FOR method. Task are machine learning applications, and face recognition systems. Material is medical studies. Generic are datasets, and models. OtherScientificTerm are biases, confounder(s ), and bias or confounder variables. Method is adversarial loss function. ",This paper proposes a method to train deep neural networks with adversarial perturbations to remove confounding variables from the training set. The proposed method is based on an adversarial training strategy where the confounder(s) are modeled using a precomputed set of features. The authors demonstrate the effectiveness of the proposed method on synthetic data and medical images.,"This paper proposes a novel approach to tackle the problem of end-to-end data augmentation in machine learning applications. The authors propose a novel adversarial training strategy to learn the discriminative features of the data, which can be used to reduce the confounding effects of confounding variables. The proposed method is evaluated on synthetic data and medical images, and it is shown that the proposed method outperforms the state of the art."
21348,SP:783049ff463edd1283c058c6106a3e1f9a033df4,Transformer USED-FOR Character - level language modeling. limited resources USED-FOR character - level language models. computational resources USED-FOR Transformer - based models. lightweight model USED-FOR calculation paths. GroupTransformer HYPONYM-OF lightweight model. grouped embedding operators USED-FOR lightweight model. grouped embedding operators USED-FOR calculation paths. inter - group linear operators USED-FOR Group - Transformer. enwik8 CONJUNCTION text8. text8 CONJUNCTION enwik8. LSTM - based models COMPARE Transformer - based models. Transformer - based models COMPARE LSTM - based models. benchmark tasks EVALUATE-FOR GroupTransformer. text8 EVALUATE-FOR GroupTransformer. enwik8 EVALUATE-FOR GroupTransformer. enwik8 HYPONYM-OF benchmark tasks. text8 HYPONYM-OF benchmark tasks. OtherScientificTerm is limitation of recursive operation. Method is group strategy. Task is qualitative analysis. ,"This paper proposes a new transformer architecture for character-level language modeling. The proposed architecture is based on the idea of group-transformer, where each group is represented as a set of linear operators, and the goal is to minimize the number of iterations needed to compute the final output of the model.  The proposed method is evaluated on enwik8 and text8 tasks, where it is shown to achieve state-of-the-art performance.  ","This paper proposes a lightweight Transformer-based model for character-level language modeling. The model is based on the idea of grouped embedding operators, where each group is represented as a set of linear operators, and the goal is to find the shortest path between each of these linear operators. The proposed model is evaluated on enwik8, text8, and text-to-text tasks, where it outperforms the state-of-the-art models."
21357,SP:946c26d371297c88d0ac246257104099b4585edc,hierarchical - latent - variable structures FEATURE-OF Probabilistic models. approach USED-FOR models. Variational Autoencoders USED-FOR approach. Variational Autoencoders USED-FOR models. inference and optimisation schemes USED-FOR approaches. non - likelihood - based framework USED-FOR generative models. bespoke models CONJUNCTION inference networks. inference networks CONJUNCTION bespoke models. approach USED-FOR models. deep - latent hierarchies USED-FOR models. Optimal Transport USED-FOR approach. Optimal Transport USED-FOR models. it COMPARE Wasserstein Autoencoder. Wasserstein Autoencoder COMPARE it. method USED-FOR generative model. deep - latent hierarchy USED-FOR generative model. Maximum Mean Discrepancy divergence FEATURE-OF Wasserstein Autoencoder. ,"This paper proposes a generative model with deep latent hierarchies for variational autoencoders (VAEs). The model is based on a non-optimal transport-based framework, where the latent variables are represented by a deep latent hierarchy. The authors show that the proposed model is able to achieve better performance than the state-of-the-art Wasserstein autoencoder (WAE) model.","This paper proposes a framework for generative models with hierarchical latent structures. The authors propose a new generative model with a hierarchical structure that can be combined with Variational Autoencoders (VAE). The proposed method is based on the maximization of the Maximum Mean Discrepancy divergence (MMD) between the model parameters and the latent variables. The method is evaluated on a variety of datasets, and compared with the Wasserstein Autoencoder. "
21366,SP:309b47441d227ffa33f96f9f16f2addc607e5bb0,"latent variable models CONJUNCTION adversarial training. adversarial training CONJUNCTION latent variable models. video - specific neural network architectures CONJUNCTION latent variable models. latent variable models CONJUNCTION video - specific neural network architectures. adversarial training CONJUNCTION methods. methods CONJUNCTION adversarial training. methods PART-OF video generation models. they USED-FOR continuations. realism FEATURE-OF continuations. benchmark datasets EVALUATE-FOR autoregressive video generation models. three - dimensional self - attention mechanism USED-FOR autoregressive video generation models. camera movement CONJUNCTION complex object interactions. complex object interactions CONJUNCTION camera movement. complex object interactions CONJUNCTION human movement. human movement CONJUNCTION complex object interactions. Kinetics HYPONYM-OF large scale action recognition dataset. Kinetics HYPONYM-OF phenomena. YouTube videos FEATURE-OF large scale action recognition dataset. Kinetics USED-FOR models. human movement HYPONYM-OF phenomena. camera movement HYPONYM-OF phenomena. complex object interactions HYPONYM-OF phenomena. Metric are statistical complexity, complexity, and fidelity. OtherScientificTerm are inherent stochasticity, and narrow domains. Task is generating natural video. Material is natural video. Generic is approaches. ","This paper proposes a three-dimensional self-attention mechanism for video generation models. The proposed method is based on the observation that current video-generating models suffer from the inherent stochasticity of natural video. To address this issue, the authors propose to use three-dimensions of self attention to model the dynamics of the generated video. The authors show that the proposed method can improve the quality of generated videos in terms of the number of continuations.  ",This paper proposes a three-dimensional self-attention mechanism for video generation models. The proposed method is based on the three-dimensionality of the self attention mechanism. The authors show that the proposed method can be applied to video-specific neural network architectures and latent variable models. They also show that their method is able to generate high-quality continuations of natural video.   
21375,SP:ad8fcdbc47a50dd2bf58aba2bc6cfe199e84dd4d,"International Classification of Diseases ( ICD ) HYPONYM-OF classification codes. noisy clinical document inputs CONJUNCTION long - tailed label distribution. long - tailed label distribution CONJUNCTION noisy clinical document inputs. Automatic ICD coding HYPONYM-OF multi - label text classification task. frequent and zeroshot codes USED-FOR fine - grained classification. long - tailed label distribution FEATURE-OF multi - label text classification task. noisy clinical document inputs FEATURE-OF multi - label text classification task. latent feature generation framework USED-FOR generalized zero - shot ICD coding. codes USED-FOR prediction. ICD code hierarchical structure CONJUNCTION cycle architecture. cycle architecture CONJUNCTION ICD code hierarchical structure. cycle architecture USED-FOR keywords. framework USED-FOR semantically meaningful features. semantically meaningful features USED-FOR zero - shot codes. cycle architecture USED-FOR framework. ICD code hierarchical structure USED-FOR framework. adversarial generative model USED-FOR generalized zero - shot learning. generalized zero - shot learning USED-FOR multi - label text classification. public MIMIC - III dataset EVALUATE-FOR methods. methods USED-FOR zero - shot codes. AUC score EVALUATE-FOR methods. F1 score EVALUATE-FOR methods. OtherScientificTerm are labeled data, and seen codes. Generic is approach. ",This paper proposes a method for zero-shot ICD coding for multi-label text classification in the presence of long-tailed label distribution. The proposed method is based on a latent feature generation framework that uses a cycle architecture to generate ICD codes and a generative adversarial model to learn the ICD code. Experiments on the public MIMIC-III dataset show that the proposed method outperforms the baselines in terms of AUC and F1 score.,"This paper proposes a generalized zero-shot ICD coding method for the multi-label text classification task. The proposed method is based on a latent feature generation framework to generate zero shot ICD codes, which are then used to train a generative model for zero shot text classification. The method is evaluated on the public MIMIC-III dataset. "
21384,SP:3ce82ae297e5759ab957babe9927062e7a71b0ba,self - supervised representation learning USED-FOR reinforcement learning ( RL ). self - supervised representation learning USED-FOR sample efficiency. sample efficiency EVALUATE-FOR reinforcement learning ( RL ). forward prediction objective USED-FOR embeddings of states and action sequences. embeddings USED-FOR policy learning. embeddings USED-FOR structure of the environment ’s dynamics. action embeddings USED-FOR model - free RL. sample efficiency EVALUATE-FOR model - free RL. low - dimensional states USED-FOR model - free RL. sample efficiency EVALUATE-FOR action embeddings. state and action embeddings USED-FOR learning of high - quality policies. goal - conditioned continuous control USED-FOR learning of high - quality policies. pixel observations USED-FOR learning of high - quality policies. ,This paper proposes to use self-supervised representation learning to improve the sample efficiency in reinforcement learning. The authors propose to use a forward prediction objective to learn the embeddings of states and action sequences. The proposed method is based on the idea that the embedding of the state and the action sequences can capture the structure of the environment’s dynamics and thus improve the performance of policy learning. Experiments are conducted on goal-conditioned continuous control tasks and show that the proposed method achieves better sample efficiency compared to model-free RL.,"This paper proposes a method for learning state and action embeddings for self-supervised reinforcement learning (SRL). The key idea is to use a forward prediction objective to learn the embedding of states and action sequences, which can be used to improve the sample efficiency of SRL. The proposed method is based on a goal-conditioned continuous control framework, where the goal is to learn high-quality policies. The authors show that the proposed method can improve sample efficiency in a model-free RL setting. "
21393,SP:11ce1616e721340eea9e80dad7460c77355ac7d1,meta - learning USED-FOR tasks. hand - crafted structure design USED-FOR task - specific meta - learning methods. knowledge bases USED-FOR knowledge organization. structure knowledge USED-FOR meta - learner. framework USED-FOR task heterogeneity. model interpretability EVALUATE-FOR framework. meta - knowledge graph USED-FOR framework. meta - knowledge graph USED-FOR task heterogeneity. 2D toy regression CONJUNCTION few - shot image classification. few - shot image classification CONJUNCTION 2D toy regression. ARML COMPARE baselines. baselines COMPARE ARML. Generic is ones. Method is globally shared meta - learning methods. OtherScientificTerm is cross - task relations. ,This paper proposes a meta-learning framework for task-specific meta learning. The proposed framework is based on a knowledge graph that is used to model the cross-task relations between tasks. The meta-knowledge graph is then used to train a meta learner that is able to generalize to new tasks. Experiments on toy regression and few-shot image classification show the effectiveness of the proposed method. ,"This paper proposes a new meta-learning framework for task-specific meta-learners. The proposed framework is based on a meta-knowledge graph, which is composed of knowledge bases for each task. The knowledge base consists of a set of tasks, and the meta-learner is given a knowledge base for each of the tasks. The meta- learner is trained on the knowledge base, and a model is trained to predict the performance of the meta learner on the task.  The proposed method is evaluated on toy regression and few-shot image classification tasks, where it is shown that the proposed method outperforms the baselines."
21402,SP:37c209cd1c628b5c2f2b282fbeaf4bbf437c7670,"model architecture CONJUNCTION fine - tuning. fine - tuning CONJUNCTION model architecture. attribute - specific data USED-FOR fine - tuning. Plug and Play Language Model ( PPLM ) USED-FOR controllable language generation. pretrained LM CONJUNCTION attribute classifiers. attribute classifiers CONJUNCTION pretrained LM. attribute classifiers USED-FOR text generation. pretrained LM PART-OF Plug and Play Language Model ( PPLM ). attribute models HYPONYM-OF classifiers. attribute models COMPARE LM. LM COMPARE attribute models. attribute alignment CONJUNCTION fluency. fluency CONJUNCTION attribute alignment. automated and human annotated evaluations EVALUATE-FOR attribute alignment. automated and human annotated evaluations EVALUATE-FOR fluency. differentiable attribute models USED-FOR text generation. Material are huge text corpora, and Model samples. Method are retraining, attribute model, and PPLMs. Task are Sampling, and generation. OtherScientificTerm are gradients, and hidden activations. ","This paper proposes a method to train a language model for text generation from attribute-specific data. The proposed method is based on a Plug and Play Language Model (PPLM) architecture, which is trained on attribute data and fine-tuned on attribute specific data. It is shown that the proposed method outperforms baselines on both automated and human evaluation. ","This paper proposes a new approach to control controllable language generation using the Plug and Play Language Model (PPLM) framework. The authors propose to use a differentiable attribute model for the task of language generation. The attribute model is trained with a pre-trained language model (LM) and fine-tuned using attribute-specific data. They show that the attribute model outperforms the LM in terms of fluency, alignment, and accuracy. They also show that their approach can be used to improve the performance of the PPLM."
21411,SP:12d0980bfea2de880905a0b87b40856969bb1c58,"deep neural networks USED-FOR machine learning tasks. unlabeled data USED-FOR learning robust representations. unsupervised and self - supervised learning approaches USED-FOR visual data. domain knowledge USED-FOR unsupervised and self - supervised learning approaches. gradient domain FEATURE-OF clean data. clean data USED-FOR noisy input data. denoising autoencoder USED-FOR data representations. visual benchmarks EVALUATE-FOR representations. approach USED-FOR representations. representations USED-FOR vision tasks. Material is supervised data. Method is unsupervised learning framework. Generic is agent. OtherScientificTerm are data structures, and single - scale corruption. ","This paper proposes an unsupervised self-supervised learning framework for learning robust representations from unlabeled data. The proposed method is based on a denoising autoencoder, which is used to learn representations from noisy input data.  The authors show that the proposed method can learn robust representations in the presence of single-scale corruption in the data structure. The authors also show that their method is able to learn a robust representation in the gradient domain.",This paper proposes a novel unsupervised learning framework for learning robust representations from unlabeled visual data. The authors propose a denoising autoencoder to learn representations from a set of clean data and noisy input data. They show that the learned representations are robust to single-scale corruption. They also show that their method can learn robust representations even when the input data is noisy. 
21420,SP:12afc1b259e51a31cbeb72366d2b93fbee1aafaa,"Neural networks USED-FOR Natural Language Processing. under - sensitivity FEATURE-OF natural language inference. technique USED-FOR formal verification of this specification. technique USED-FOR models. interval bound propagation ( IBP ) approach USED-FOR technique. training methods USED-FOR under - sensitivity. SNLI and MNLI datasets EVALUATE-FOR IBP training. verified accuracy EVALUATE-FOR IBP training. Generic are they, specification, method, model, and metrics. Method are decomposable attention mechanism, and training. OtherScientificTerm is under - sensitivity problem. Material is SNLI test set. ","This paper studies the problem of under-sensitivity in natural language inference. The authors propose an interval bound propagation (IBP) approach to address this problem. The IBP approach is based on a decomposable attention mechanism, which is a decomposition of the original attention mechanism. The proposed method is evaluated on the SNLI and MNLI datasets.   ","This paper studies the problem of under-sensitivity in natural language inference (NLI). The authors propose an interval bound propagation (IBP) approach for training neural networks to improve the performance of NLI models. The IBP approach is based on a decomposable attention mechanism, where the attention mechanism consists of two parts: (1) an attention mechanism that is trained on the SNLI test set, and (2) a training method that is used to train the model on the MNLI dataset. The authors show that the IBP training method can improve the accuracy of the NLI model on both SNLI and MNLI datasets."
21429,SP:14257af9fe83522c6e5b5d6b0d68945b944e30fb,"replay memory USED-FOR network updates. soft divergence FEATURE-OF structure. data graph USED-FOR transitions. Q - values FEATURE-OF Markov Decision Process ( MDP ). favorable structure FEATURE-OF subgraph. transition PART-OF MDP. transition PART-OF continuous Q - learning problem. Q - value FEATURE-OF transition. Q - value USED-FOR continuous Q - learning problem. Q - value FEATURE-OF transition. lower bounds USED-FOR method. lower bounds USED-FOR TD learning. TD learning USED-FOR method. soft divergence FEATURE-OF method. sample efficiency EVALUATE-FOR method. replay memory capacity FEATURE-OF algorithm. OtherScientificTerm are state and action spaces, QGRAPH, hyperparameters, and QGRAPHs. ","This paper studies the problem of continuous Q-learning in a Markov Decision Process (MDP), where the goal is to learn the Q-values of transitions in the MDP. The authors propose to use a data graph to represent the state and action spaces, and use replay memory to store the updates of the network updates. The main contribution is to show that the soft divergence of the data graph can be used to estimate the soft Q-value of a transition in an MDP, which is then used to compute the transition value of the next state.    The main contributions of the paper are as follows:  - The authors show that their method is efficient in terms of sample complexity and sample efficiency. - The proposed method is shown to be more efficient than the state-of-the-art TD learning method. ","This paper studies the problem of continuous Q-learning in the Markov Decision Process (MDP) setting, where the data graph is composed of subgraphs with favorable structure. The main contribution of the paper is to provide lower bounds for the sample efficiency of the proposed method. The lower bounds are based on the assumption that the data graphs have a favorable structure, and that the transition between states and actions is a transition in the Q-value space. The authors show that the proposed algorithm can achieve sample efficiency in terms of replay memory, and show that it can be used for TD learning. "
21438,SP:c92c97e47d8b218dfd009bbf61f5b3547b395f91,approach USED-FOR problem. domain - invariant embeddings USED-FOR approach. domain - invariant embeddings USED-FOR problem. embedding complexity FEATURE-OF generalization. theoretical framework USED-FOR multilayer neural networks. strategy COMPARE layer - dependent complexity tradeoff. layer - dependent complexity tradeoff COMPARE strategy. Task is Unsupervised domain adaptation. Generic is complexity. ,"This paper studies the problem of unsupervised domain adaptation, where the goal is to learn a domain-invariant representation of the target domain. The authors propose to use a multilayer neural network with domain invariant embeddings to solve the problem. They show that the complexity of the problem is bounded by the embedding complexity, and show that this is the case for any embedding-based approach. They also provide a theoretical analysis of the generalization properties of the proposed approach.",This paper studies the problem of unsupervised domain adaptation in the multilayer neural network setting. The authors propose a new approach to solve the problem by using domain-invariant embeddings. The main contribution of the paper is a theoretical analysis of the complexity trade-off between layer-dependent and layer-independent complexity tradeoff. They show that the complexity of the problem is bounded by the number of layers in the network. They also provide a theoretical framework to study the generalization performance of the proposed approach.
21447,SP:f3f3c6fbae757836551b3f1ee54a7d1e040132b8,"framework USED-FOR algorithm - dependent generalization error bounds. PAC - Bayesian theory CONJUNCTION algorithmic stability. algorithmic stability CONJUNCTION PAC - Bayesian theory. PAC - Bayesian theory USED-FOR framework. mini - batch and acceleration CONJUNCTION Entropy - SGD. Entropy - SGD CONJUNCTION mini - batch and acceleration. momentum CONJUNCTION mini - batch and acceleration. mini - batch and acceleration CONJUNCTION momentum. Bayes - Stability method USED-FOR data - dependent generalization bounds. data - dependent generalization bounds USED-FOR stochastic gradient Langevin dynamics ( SGLD ). momentum HYPONYM-OF noisy gradient methods. Entropy - SGD HYPONYM-OF noisy gradient methods. mini - batch and acceleration HYPONYM-OF noisy gradient methods. data - dependent bounds USED-FOR randomly labelled data. randomly labelled data COMPARE normal data. normal data COMPARE randomly labelled data. bounded loss CONJUNCTION ` 2 regularization term. ` 2 regularization term CONJUNCTION bounded loss. bounded loss PART-OF total loss. ` 2 regularization term PART-OF total loss. Log - Sobolev inequality USED-FOR parameter distribution. generalization bounds USED-FOR continuous Langevin dynamic. Log - Sobolev inequality USED-FOR generalization bounds. Metric is Generalization error. OtherScientificTerm are out - of - sample error, tight generalization error bounds, generalization error bounds, and noise level. Task is statistical learning theory. Method is Bayes - Stability. Generic is bounds. ","This paper proposes a generalization error bound for stochastic gradient Langevin dynamics (SGLD) based on PAC-Bayesian theory and algorithmic stability. The generalization bounds are based on the Bayes-Stability method, which is a general framework that combines PAC theory and algorithm stability. In particular, the authors show that the generalization bound is data-dependent, i.e., it depends on the number of samples and the noise level of the data.    The main contributions of the paper are:  1. The authors provide a general algorithm-dependent bound for SGLD with bounded loss and a regularization term.  2. They show that for the continuous Langevin dynamic, the bound is tight.  3. They also show that under certain assumptions on the parameter distribution, they can obtain tight generalization errors for random labelled data.",This paper studies generalization bounds for stochastic gradient Langevin dynamics (SGLD) under the PAC-Bayesian theory framework. The main contribution of the paper is a new generalization error bound for SGLD based on the Bayes-Stability method. The generalization bound is derived from the Log-Sobolev inequality of the parameter distribution of the Langevin dynamic. The authors show that the bound is tight under the bounded loss and the `2-regularization term of the total loss.
21456,SP:a82fcd1d3196ddf078cfe8f4bc6f445d9d2bdc11,"spatial memory CONJUNCTION goal - directed spatial navigation. goal - directed spatial navigation CONJUNCTION spatial memory. hippocampus USED-FOR goal - directed spatial navigation. hippocampus USED-FOR spatial memory. hippocampal CA1 neurons USED-FOR continual learning. populationlevel activity FEATURE-OF hippocampal CA1 neurons. populationlevel activity USED-FOR continual learning. continual learning USED-FOR spatial navigation strategies. navigational strategies CONJUNCTION reward location. reward location CONJUNCTION navigational strategies. hippocampal neurons USED-FOR task variables. decisions CONJUNCTION navigational strategies. navigational strategies CONJUNCTION decisions. firing activity USED-FOR dPCA. decisions HYPONYM-OF task variables. reward location HYPONYM-OF task variables. navigational strategies HYPONYM-OF task variables. dPCA USED-FOR components. hippocampal features COMPARE reinforcement learning algorithms. reinforcement learning algorithms COMPARE hippocampal features. deep reinforcement learning model COMPARE animal learning. animal learning COMPARE deep reinforcement learning model. hippocampus USED-FOR reinforced spatial continual learning. biological and machine learning USED-FOR spatial continual learning. Task are continual learning of navigational strategies, and allocentric and egocentric spatial tasks. Method is Demixed Principal Component Analysis ( dPCA ). OtherScientificTerm is task switching. ","This paper studies the role of the hippocampus in continual learning in the continual learning of spatial navigation tasks. The authors show that the hippocampus plays a key role in continual continual learning and proposes a new method to measure the importance of spatial memory and goal-directed spatial navigation. The method is based on the Demixed Principal Component Analysis (dPCA) method and is able to capture the spatial memory, navigation strategies, and reward location components of the task. The experiments show that using dPCA, the authors are able to identify spatial memory as well as navigation strategies in the hippocampus.","This paper proposes a method for continual continual learning of spatial navigation strategies in the hippocampus. The method is based on the Demixed Principal Component Analysis (dPCA) method. The authors show that dPCA is able to capture the spatial memory of the hippocampus, which is a key component of spatial continual learning. They also show that the hippocampus can be used as a model for continual learning in the presence of a task switching task. They show that their method outperforms state-of-the-art methods in terms of performance."
21465,SP:51acf1f8108683dce543a1fb4a61fbd593f9b4cc,"Monte Carlo Tree Search ( MCTS ) USED-FOR discrete environments. it USED-FOR continuous domains. Go HYPONYM-OF discrete environments. tree search based policy optimization method USED-FOR continuous environments. TPO HYPONYM-OF tree search based policy optimization method. hybrid approach USED-FOR policy optimization. hybrid approach USED-FOR TPO. continuous action space FEATURE-OF MCTS tree. off - policy MCTS trajectories USED-FOR policy gradient. pre - trained policy USED-FOR bootstrapping tree search. branching factor CONJUNCTION simulation count. simulation count CONJUNCTION branching factor. policy bootstrapping USED-FOR continuous MCTS. branching factor USED-FOR continuous MCTS. simulation count USED-FOR continuous MCTS. PPO USED-FOR baseline policy optimization algorithm. TPO USED-FOR policy. Humanoid HYPONYM-OF complex environments. Method are limiting tree search branching factor, and MCTS training. OtherScientificTerm are tree search branching factor, policy distribution, and loss function. Generic are approach, and baseline algorithm. Metric is MCTS branching factor. ","This paper proposes a method for continuous Monte Carlo Tree Search (MCTS) based on bootstrapping tree search. The main idea is to use off-policy MCTS trajectories from a pre-trained policy to compute the policy gradient from the policy distribution, and then use this gradient to compute a policy gradient for bootstrapped tree search, which is then used for policy optimization. Theoretical analysis is provided to show the convergence of the proposed method. Experiments show that the proposed approach outperforms the baseline Monte Carlo Policy Optimization (PPO) algorithm in several continuous action space settings.",This paper proposes a new approach to bootstrapping Monte Carlo Monte Carlo Tree Search (MCTS) in continuous environments. The main idea of the paper is to use a bootstrapped bootstrap-based policy to bootstrap the MCTS tree in a continuous action space. The bootstrap policy is trained with a pre-trained policy and the policy is then used to optimize the policy gradient. The paper shows that the bootstrap strategy can be used to improve the simulation count of the policy.   
21474,SP:1ce3bc4d31712886f7dcada5b5ae67c3c376819a,accuracy EVALUATE-FOR network. sparse subnetworks PART-OF neural networks. supervision USED-FOR generating process. winning tickets COMPARE winning tickets. winning tickets COMPARE winning tickets. ImageNet dataset EVALUATE-FOR winning tickets. ImageNet classification task EVALUATE-FOR winning tickets. ImageNet classification task EVALUATE-FOR winning tickets. Material is lottery ticket hypothesis. Method is neural network optimization. OtherScientificTerm is initializations. ,This paper studies the lottery ticket hypothesis for neural network training. The authors propose to use sparse subnetworks in the training process to generate winning tickets. They show that winning tickets are more likely to be found in the sparse subnetwork than random initializations. They also show that lottery tickets can be found more efficiently than random initialization.  ,This paper studies the lottery ticket hypothesis for neural network training. The authors show that winning tickets for training neural networks with sparse subnetworks outperform winning tickets without supervision. They also show that the winning tickets are more accurate than winning tickets. The paper also provides a theoretical analysis of the effect of winning tickets on the accuracy of the network.
21483,SP:dbcebe5b73486885d9f4478b258047c02f8481a2,"excessive prediction undersensitivity HYPONYM-OF complementary problem. spurious surface patterns USED-FOR models. data augmentation CONJUNCTION adversarial training. adversarial training CONJUNCTION data augmentation. adversarial training USED-FOR defence strategies. data augmentation USED-FOR defence strategies. undersensitivity attacks FEATURE-OF model. held out evaluation data EVALUATE-FOR undersensitivity attacks. held out evaluation data EVALUATE-FOR model. they COMPARE model. model COMPARE they. train / evaluation distribution mismatch FEATURE-OF biased data setting. biased data setting EVALUATE-FOR model. predictive cues USED-FOR they. biased data setting EVALUATE-FOR adversarially robust models. F1 EVALUATE-FOR they. F1 EVALUATE-FOR model. Method are Neural reading comprehension models, noisy adversarial attack, and NewsQA models. OtherScientificTerm are adversarially selected input, semantically invariant text perturbations, and semantic variations of comprehension questions. Generic is attack. Material is adversarially generated questions. ",This paper studies the problem of excessive prediction undersensitivity in neural reading comprehension models. The authors propose to use noisy adversarial perturbations to generate adversarial examples that are semantically invariant to semantic variations of comprehension questions. They show that adversarial training and data augmentation can help to improve the robustness of the model. They also show that the proposed methods can improve the performance of adversarially robust models.,"This paper studies the problem of excessive prediction undersensitivity (excessive prediction under-performances) in the context of adversarial training and adversarial defense. The main contribution of the paper is a theoretical analysis of the effect of data augmentation on the performance of adversarially trained models. The authors show that under a biased data setting, the model is more sensitive to underperformers than the model trained on the same data set. They also show that the model underperforms under the same biased data set when the training data distribution mismatch is high.  "
21492,SP:5da870060778de460c1abe91562d6f3e707efef4,"reinforcement learning ( RL ) agents USED-FOR real - world tasks. reinforcement learning ( RL ) agents USED-FOR safety. approaches USED-FOR problem. safety penalty PART-OF reward function. complex domains EVALUATE-FOR approaches. model - based approach USED-FOR safety. imaginative module HYPONYM-OF directed graph. it CONJUNCTION RL algorithm. RL algorithm CONJUNCTION it. gridworld environments CONJUNCTION self - driving car simulator. self - driving car simulator CONJUNCTION gridworld environments. approach COMPARE baseline. baseline COMPARE approach. self - driving car simulator EVALUATE-FOR approach. gridworld environments EVALUATE-FOR approach. self - driving car simulator EVALUATE-FOR proposal. gridworld environments EVALUATE-FOR proposal. OtherScientificTerm are bad incentives, unsafe scenarios, transition dynamics of the environment, baseline state, and discrete action space. Generic are they, graph, method, and task. ",This paper proposes a novel approach to improve safety in reinforcement learning by adding a safety penalty to the reward function. The proposed approach is based on a directed graph that is used to model the transition dynamics of the environment. The authors show that the proposed approach can be combined with existing reinforcement learning algorithms to improve the performance of the agent. Experiments on simulated and real-world environments demonstrate the effectiveness of the proposed method.,"This paper proposes a directed graph-based approach to improve the safety of reinforcement learning agents. The proposed method is based on the idea of an imaginative module, where the agent is encouraged to explore the environment in order to improve its safety. The authors show that the proposed method outperforms the state-of-the-art baselines in a variety of environments. "
21501,SP:c2796f28fb067138303df8d424d646f4ada31558,"numerical error FEATURE-OF finite differences. deep learning models USED-FOR physics - governing observations. unstructured grid USED-FOR physics - governing observations. neighboring information USED-FOR finite differences. physics equations USED-FOR finite differences. PA - DGN USED-FOR dynamical relations. synthetic data CONJUNCTION real - world climate observations. real - world climate observations CONJUNCTION synthetic data. PA - DGN USED-FOR approximation of directional derivatives. PA - DGN USED-FOR prediction of graph signals. approximation of directional derivatives CONJUNCTION prediction of graph signals. prediction of graph signals CONJUNCTION approximation of directional derivatives. real - world climate observations USED-FOR prediction of graph signals. synthetic data USED-FOR prediction of graph signals. weather stations USED-FOR real - world climate observations. Task is dynamics of physical systems. OtherScientificTerm are discretization error, spatial and temporal differences, and sequential observations. Material is sparse data. Generic is architecture. ","This paper proposes a deep learning model for predicting the dynamics of physical systems. The model is based on an unstructured grid, which is used to model the spatial and temporal differences in the physics equations. The authors show that the discretization error of the model is bounded by the distance between neighboring information and the finite differences of the physical system. They show that this discretized error can be used to estimate the directional derivatives of the system.   ","This paper proposes a new deep learning model for physics-governed observations. The model is based on an unstructured grid, which is used to model the dynamics of physical systems. In particular, the model is trained to predict the directional derivatives of the physics equations. The authors show that the model can be used to learn the dynamical relations between the physical system and its neighbors. They also show that it can be applied to the prediction of graph signals. "
21510,SP:db8ed4f4fc3967f5dd4d208d5d029730eb99e840,"nonsmooth regularization CONJUNCTION constraints. constraints CONJUNCTION nonsmooth regularization. nonsmooth regularization FEATURE-OF structured neural networks ( NN ). constraints FEATURE-OF structured neural networks ( NN ). interval constraints HYPONYM-OF constraints. ` 1 - norm HYPONYM-OF nonsmooth regularization. constrained nonsmooth nonconvex optimization problem USED-FOR training. ProxSGD USED-FOR sparse or binary neural networks. regularization function CONJUNCTION constraint set. constraint set CONJUNCTION regularization function. regularization function USED-FOR ProxSGD. constraint set USED-FOR ProxSGD. OtherScientificTerm are learning rates, and stationary point. Method is ProxSGD algorithm. ","This paper studies the problem of training sparse and binary neural networks with nonsmooth regularization and interval constraints. The authors propose a new regularization called ProxSGD, which is a nonconvex optimization problem where the regularization function is a function of the constraint set and the interval constraints are a set of non-concave intervals. They show that this problem can be solved by a linear combination of Prox-SGD and ProxSgd, and show that under certain assumptions on the learning rate and the number of training epochs, the authors show that the algorithm converges to a stationary point at a rate of $O(1/\sqrt{T})$ with probability $1/T$.  ","This paper studies the problem of nonsmooth nonconvex optimization for sparse or binary neural networks. The authors propose a new method called ProxSGD, which is based on the notion of a stationary point. They show that the stationary point is a function of the learning rate of the neural network and the number of training epochs. They also show that this stationary point can be used as a regularization function for the regularization of the training process.   "
21519,SP:2ca1f4da9faee79768764cda5d09d949cc942acc,"encoder USED-FOR compression algorithms. approximate methods USED-FOR encoder. approximate methods USED-FOR algorithms. framework USED-FOR lossy image compression. non - deterministic compression codec USED-FOR framework. expected code length CONJUNCTION relative entropy. relative entropy CONJUNCTION expected code length. gradient - based optimizers USED-FOR end - to - end differentiable compression framework. it USED-FOR lossy image compression. rate - distortion curves COMPARE state - of - the - art. state - of - the - art COMPARE rate - distortion curves. low bitrates FEATURE-OF rate - distortion curves. it USED-FOR method. Kodak dataset EVALUATE-FOR rate - distortion curves. Probabilistic Ladder Networks ( PLNs ) USED-FOR it. low bitrates EVALUATE-FOR state - of - the - art. CLIC 2018 dataset EVALUATE-FOR Probabilistic Ladder Networks ( PLNs ). Material is image. OtherScientificTerm are discrete code, quantization step, continuous space, and encoding distribution. Method is decoder. Generic is process. ",This paper proposes an end-to-end differentiable compression framework for lossy image compression. The main idea is to use a non-deterministic compression codec with gradient-based optimizers to optimize the expected code length and the relative entropy. The proposed method achieves state-of-the-art rate-distortion curves on the Kodak and CLIC 2018 datasets. ,This paper proposes an end-to-end differentiable compression framework for lossy image compression. The authors propose a non-deterministic compression codec for image compression that is differentiable in terms of the expected code length and the relative entropy of the encoder and decoder. They also propose a gradient-based optimizer to improve the rate-distortion of the compression algorithm. The proposed method is evaluated on the CLIC 2018 dataset and the Kodak dataset. 
21528,SP:788fd2b6956dd69bf7752d39ea21883947128c8a,"compressed JPG ( C - JPG ) image USED-FOR SR. SR HYPONYM-OF image processing operations. components CONJUNCTION cycle loss. cycle loss CONJUNCTION components. components PART-OF SR structure. cycle loss PART-OF SR structure. high - qualified SR images USED-FOR prevalent C - JPG images. hybrid loss function USED-FOR SR generation. cycle loss PART-OF SR solver. cycle loss USED-FOR hybrid loss function. SR solver USED-FOR hybrid loss function. approach COMPARE state - of - the - art methods. state - of - the - art methods COMPARE approach. Task are Super Resolution ( SR ), and SR issue. Method are SR models, C - JPG, functional sub - model, and SR approaches. OtherScientificTerm is storage space. Material is C - JPG images. ","This paper proposes a method to address the Super Resolution (SR) problem in compressed JPG images. The authors propose to use a functional sub-model to solve the SR problem, which is based on a combination of components and a cycle loss. The main contribution of the paper is to propose a hybrid loss function for SR generation. The proposed method is evaluated on C-JPG images and achieves state-of-the-art results.","This paper proposes a new approach to tackle the Super Resolution (SR) problem. The authors propose a hybrid loss function for SR generation, which is a combination of two existing approaches. The main idea is to use a functional sub-model to generate high-quality SR images from C-JPG images. The hybrid loss is then used to solve the SR solver problem. "
21537,SP:18dd92f2f55020be4f5a089b3b251327e47886f4,"fully convolutional network architecture USED-FOR surface of pass probabilities. professional soccer matches USED-FOR single - location labels. single - location labels USED-FOR fully convolutional network architecture. single - location labels USED-FOR surface of pass probabilities. feature hierarchy USED-FOR network. low - level inputs USED-FOR network. approach USED-FOR weakly supervised learning. approach USED-FOR spatiotemporal decision - making analysis. spatiotemporal decision - making analysis HYPONYM-OF sports. network USED-FOR pass - selection likelihood. deep learning architecture USED-FOR sports analytics. OtherScientificTerm are sampling levels, coarse and fine detail, single pixel correspondence, and predicted probability map. ",This paper proposes to learn the surface of pass probabilities in soccer matches by using a fully convolutional network architecture with single-location labels. The proposed method is based on a feature hierarchy with low-level inputs and a high-level feature hierarchy. The network is trained with weakly supervised learning and is able to predict the probability map of each pass. The method is evaluated on a set of soccer matches and achieves state-of-the-art performance.   ,"This paper proposes a novel approach to analyze the surface of pass probabilities in soccer matches. The proposed method is based on a fully convolutional network architecture with a feature hierarchy and a low-level input. The network is trained to predict the probability map of a set of high-level inputs. The method is applied to a spatiotemporal decision-making analysis of professional soccer matches, where it is shown to be able to identify the top-10 players with the best pass-selection likelihood."
21546,SP:1ae31baf383fc520687b255d9cac14c3b040e253,"side information USED-FOR inductive matrix completion model. user ’s age CONJUNCTION movie ’s genre. movie ’s genre CONJUNCTION user ’s age. movie ’s genre HYPONYM-OF content ( side information ). user ’s age HYPONYM-OF content ( side information ). IGMC USED-FOR graph neural network ( GNN ). It COMPARE transductive baselines. transductive baselines COMPARE It. model USED-FOR Douban movie ratings. MovieLens dataset USED-FOR model. Long - range dependencies USED-FOR modeling recommender systems. side information USED-FOR inductive matrix completion models. OtherScientificTerm are ( rating ) matrix, low - dimensional latent embeddings, embeddings, rating matrix, subgraphs, and local graph patterns. Method are matrix completion methods, and transductive methods. Task is matrix completion. Generic is it. ","This paper proposes an inductive matrix completion model (IGMC) based on side information. The idea is to learn a low-dimensional latent embedding of the rating matrix, which is then used to model the relationship between the user's age and the movie's genre. The model is trained using a graph neural network (GNN) and is evaluated on the MovieLens dataset.  ","This paper proposes a novel inductive matrix completion model (IGMC) based on a graph neural network (GNN) architecture. The main idea is to use a low-dimensional latent embedding of the rating matrix (e.g., the user’s age, the genre, and the age of the user) as the input to a GNN. The model is trained on the Douban movie ratings dataset, where it is shown to be competitive with other matrix completion methods. "
21555,SP:c5cb1b50e17a69e88d5ae28848e265215162da1e,smooth objective function USED-FOR unconstrained minimization. heavy ball momentum USED-FOR stochastic zeroth - order method. learning to continuous control tasks EVALUATE-FOR method. STP COMPARE policy gradient methods. policy gradient methods COMPARE STP. STP COMPARE derivative - free optimization algorithms. derivative - free optimization algorithms COMPARE STP. derivative - free optimization algorithms CONJUNCTION policy gradient methods. policy gradient methods CONJUNCTION derivative - free optimization algorithms. SMTP COMPARE STP. STP COMPARE SMTP. SMTP COMPARE methods. methods COMPARE SMTP. STP CONJUNCTION methods. methods CONJUNCTION STP. importance sampling USED-FOR SMTP. OtherScientificTerm is function evaluations. Metric is complexity. Method is SMTP_IS. ,This paper proposes a method for unconstrained minimization of a smooth objective function in continuous control problems. The main idea is to use a stochastic zeroth-order method with heavy ball momentum (STP) to approximate the objective function. The authors show that this method can be used to learn a smooth continuous control problem with a continuous control task. The method is shown to be computationally efficient.  ,This paper proposes a stochastic zeroth-order method (STP) for unconstrained minimization with smooth objective function. The main idea is to use heavy ball momentum to improve the convergence rate of STP by using importance sampling. The authors show that SMTP_IS outperforms the state-of-the-art in terms of convergence rate and convergence rate.
21564,SP:a216cfc29937eb398ea98cb1aea3481c9aed8240,"environmental stochasticity CONJUNCTION uncertainties. uncertainties CONJUNCTION environmental stochasticity. deep learning architecture USED-FOR multiagent coordination. multiagent coordination mechanisms USED-FOR deep learning architecture. Action Semantics Network ( ASN ) HYPONYM-OF network architecture. action semantics USED-FOR neural networks. neural networks USED-FOR ASN. action semantics USED-FOR ASN. ASN CONJUNCTION deep reinforcement learning ( DRL ) algorithms. deep reinforcement learning ( DRL ) algorithms CONJUNCTION ASN. StarCraft II micromanagement CONJUNCTION Neural MMO. Neural MMO CONJUNCTION StarCraft II micromanagement. DRL approaches COMPARE network architectures. network architectures COMPARE DRL approaches. Neural MMO EVALUATE-FOR ASN. StarCraft II micromanagement EVALUATE-FOR ASN. ASN COMPARE DRL approaches. DRL approaches COMPARE ASN. ASN COMPARE network architectures. network architectures COMPARE ASN. Task are multiagent systems ( MASs ), and system evolution. Material is MASs. OtherScientificTerm is co - learning agents. ","This paper proposes an action semantics network (ASN) for multi-agent reinforcement learning. The main idea is to use a neural network to learn the action semantics of a set of agents, and then use the learned action semantics to guide the learning of the other agents. The proposed method is evaluated on a variety of environments, including StarCraft II micromanagement and Neural MOMA. The results show that the proposed method outperforms state-of-the-art deep RL algorithms on these environments.","This paper proposes an action semantics network (ASN) architecture for multi-agent coordination. The proposed ASN is based on the action-semantic network (as opposed to action-decision network) architecture. The ASN can be applied to a variety of deep RL algorithms, including deep reinforcement learning (DRL) algorithms and Neural Multiplayer Online Game (NMM) algorithms. The authors show that the ASN outperforms the state-of-the-art DRL algorithms on StarCraft II and Neural MMO environments. "
21573,SP:efaf3a440dc17e05177832083ffbc23760ed7c97,Value - based methods USED-FOR planning and deep reinforcement learning ( RL ). Q function HYPONYM-OF state - action value function. system dynamics USED-FOR global structures of the Q function. lowrank structure USED-FOR big data matrices. low - rank Q functions USED-FOR control and deep RL tasks. Matrix Estimation ( ME ) techniques USED-FOR framework. low - rank structure FEATURE-OF Q functions. low - rank structure USED-FOR framework. planning procedure USED-FOR classical control. scheme USED-FOR value - based RL techniques. scheme USED-FOR “ low - rank ” tasks. control tasks CONJUNCTION Atari games. Atari games CONJUNCTION control tasks. control tasks EVALUATE-FOR approach. Atari games EVALUATE-FOR approach. Task is planning and deep RL. Generic is structures. ,This paper proposes a method for learning a low-rank Q function for planning and reinforcement learning. The method is based on the observation that the state-action value function can be viewed as a matrix of matrices with a low rank structure. The authors propose to use matrix estimation techniques to estimate the low rank matrices of the Q function and use this information to learn a planning procedure for classical control. The proposed method is evaluated on a variety of control tasks and Atari games.,"This paper proposes a new framework for planning and deep reinforcement learning (DRL) based on matrix estimation (ME) techniques. The main idea is to use a low-rank structure of the Q function to model the dynamics of the state-action value function, which can be used for planning. The proposed method is evaluated on a variety of control tasks and Atari games.  "
21582,SP:430336893b247b7bd45687d78b0d0511a7369e87,"batch reinforcement learning USED-FOR sample - efficient learning. batch reinforcement learning USED-FOR Deep Reinforcement Learning ( DRL ). off - policy DRL algorithms USED-FOR batch DRL setting. action space FEATURE-OF maximizing Q functions. state - action pairs USED-FOR policy network. it USED-FOR policy network. state - action pairs USED-FOR it. imitation learning USED-FOR it. imitation learning USED-FOR policy network. Mujoco benchmark EVALUATE-FOR BAIL. Generic is algorithm. Method are Best - Action Imitation Learning ( BAIL ), and offpolicy DRL algorithms. OtherScientificTerm is Q functions. "," is an off-policy reinforcement learning (RL) algorithm. The main contribution of this paper is to propose a new algorithm called Best-Action Imitation Learning (BAIL) for batch RL. BAIL is based on the observation that in the action space, maximizing the Q-function over the state-action pairs is equivalent to maximizing the best-action-pair between the current state-actions and the current policy. To this end, the authors propose to learn the best action-pair from each state using imitation learning. The proposed algorithm is evaluated on the Mujoco benchmark and achieves state-of-the-art performance.","This paper proposes a new algorithm for off-policy deep reinforcement learning (DRL) in the Mujoco setting. The main idea is to use imitation learning to improve the sample-efficiency of the policy network in the action space. The algorithm is based on the idea of best-action imitation learning (BAIL), where the goal is to maximize the Q-function of the state-action pairs. The authors show that BAIL can be used to improve sample efficiency in a batch DRL setting. They also provide a theoretical analysis of their algorithm."
21591,SP:94078964876667e8a5d9ae7728d779d5b91a576e,"feature representations CONJUNCTION classifiers. classifiers CONJUNCTION feature representations. classifiers PART-OF deep extreme multi - label learning. feature representations PART-OF deep extreme multi - label learning. deep extreme classifiers USED-FOR short text documents. word embeddings USED-FOR DeepXML. negative sub - sampling techniques USED-FOR negative training data. accuracy EVALUATE-FOR DeepXML. residual connection USED-FOR them. Slice algorithm USED-FOR DeepXML architecture. Slice algorithm USED-FOR pretrained embeddings. pretrained embeddings USED-FOR DeepXML architecture. it COMPARE XML - CNN. XML - CNN COMPARE it. XML - CNN CONJUNCTION AttentionXML. AttentionXML CONJUNCTION XML - CNN. it COMPARE AttentionXML. AttentionXML COMPARE it. DeepXML COMPARE leading techniques. leading techniques COMPARE DeepXML. leading techniques USED-FOR search engine queries. search engine queries CONJUNCTION advertiser bid phrases. advertiser bid phrases CONJUNCTION search engine queries. DeepXML USED-FOR search engine queries. Method are DeepXML algorithm, and classifier. Generic is architecture. ","This paper proposes a novel approach to learn word embeddings and classifiers for short text documents. The proposed method is based on the Slice algorithm, which uses a pre-trained embedding and classifier to learn a residual connection between the embedding features and the classifier. The authors show that the proposed method outperforms the state-of-the-art methods in terms of accuracy and performance on search engine queries.  ","This paper proposes a novel architecture for deep extreme multi-label learning for short text documents. The key idea is to use the word embeddings from a pre-trained word embedding as a negative sub-sampling technique for negative training data, and then use the residual connection between the embedding and the classifier as a residual connection. The authors show that the proposed architecture outperforms the state-of-the-art in terms of performance on search engine queries. "
21600,SP:b1b1252d82fa1bea18309e0b0b894e0f28f48bc9,"binary vector representations ( hash codes ) USED-FOR Hashing - based collaborative filtering. Hamming distance USED-FOR recommendations. Hamming distance USED-FOR hashing - based collaborative filtering. user hash code USED-FOR mask. Boolean AND operation USED-FOR user hash code. Boolean AND operation USED-FOR mask. approach COMPARE baselines. baselines COMPARE approach. NDCG EVALUATE-FOR approach. runtime overhead EVALUATE-FOR Hamming distance. self - masking COMPARE Hamming distance. Hamming distance COMPARE self - masking. OtherScientificTerm are hash codes, and binary user - level importance weighting. Task is distance computation. Generic is it. ","This paper proposes a new method for hashing-based collaborative filtering based on Hamming distance. The proposed method is based on the use of user-level importance weighting, where each user is represented by a binary vector representation and the goal is to minimize the distance between the user hash code and the hash code of the recommendation recommendation. The paper proposes to use a binary AND operation to compute the hash codes for each user. The method is evaluated on the NDCG benchmark and shows that the proposed method outperforms baselines in terms of accuracy and computation time.","This paper proposes a new method for hashing-based collaborative filtering, where the user hash code is represented as a binary vector representation. The key idea is to use the Hamming distance between user hash codes as a surrogate for the user-level importance weighting. The paper proposes to use a simple and efficient method to compute the distance between users' hash codes. The method is evaluated on the NDCG dataset, and it is shown to be competitive with other baselines. "
21609,SP:80898d0f2b2c8dc3388fa9164e529eae36aa1b21,"Generative adversarial networks ( GANs ) USED-FOR images. mode collapse FEATURE-OF GAN ’s learned distribution. evaluation metrics EVALUATE-FOR image synthesis. low - level perceptual quality EVALUATE-FOR evaluation metrics. statistical tools USED-FOR mode collapse. mode collapse FEATURE-OF GANs. mode collapse FEATURE-OF GANs. toolset USED-FOR GANs. toolset USED-FOR mode collapse. OtherScientificTerm are GAN learned distribution, and model parameters. ","This paper studies the problem of mode collapse in GANs. The authors show that mode collapse is a phenomenon that occurs when the distribution of the model parameters changes over time. They show that the mode collapse can be explained as a function of the number of parameters in the GAN, and that it is related to the amount of training data that is used to train the model. They then propose a new metric to evaluate the quality of the generated images, and show that it can be used to measure mode collapse.","This paper studies the phenomenon of mode collapse in GANs, where the learned distribution of a GAN’s learned distribution can collapse due to low-level perceptual quality. The authors propose a new metric to measure the mode collapse, which they call “image synthesis”. They show that mode collapse can be quantified using a set of statistical tools, and show that it can be used to evaluate the quality of the generated images. They also provide a theoretical analysis of how mode collapse affects the performance of the GAN."
21618,SP:e5b5dda2f024cfda10526e744aa035e0165af58a,"over - parametrized neural networks CONJUNCTION linearized models. linearized models CONJUNCTION over - parametrized neural networks. Neural Tangent Kernels ( NTKs ) USED-FOR linearized models. neural networks COMPARE linearized models. linearized models COMPARE neural networks. Taylor expansion FEATURE-OF network. optimization landscape FEATURE-OF randomized two - layer networks. escaping - saddle algorithms USED-FOR optimization landscape. mild distributional assumptions FEATURE-OF dimension factor. it USED-FOR networks. higher - order terms PART-OF Taylor series. networks CONJUNCTION higher - order terms. higher - order terms CONJUNCTION networks. it USED-FOR randomization technique. Method are NTK theory, Taylor expansion of the network, quadratic models, and randomized networks. Generic are theory, and them. OtherScientificTerm are NTK regime, NTK, and sample complexity bounds. Material is quadratic case. ","This paper studies the optimization landscape of randomized two-layer neural networks in the NTK regime. The authors show that under mild distributional assumptions, the Taylor expansion of the network can be approximated by an escaping-saddle algorithm. They show that this algorithm is equivalent to a randomization technique. They also provide sample complexity bounds for this setting.  ","This paper studies the Taylor expansion of neural networks in randomized two-layer networks. The authors show that under some mild distributional assumptions, the sample complexity of such networks is bounded by the Taylor series of the dimension factor. They also show that this is the case in the quadratic case, and that it is also true in the over-parameterized regime.  The authors also provide a theoretical analysis of the optimization landscape of randomized two layer networks. "
21627,SP:cef7ea513eb3e42be4edf40e4ee1701a969bcbea,"graph data USED-FOR downstream tasks. Graph Neural Networks ( GNNs ) USED-FOR graph data. graph filter design PART-OF GNN models. filter USED-FOR graph data. graph properties USED-FOR graph filter. filters USED-FOR graph. assessment tool EVALUATE-FOR graph convolutional filters. graph convolutional filters USED-FOR graph. node classification EVALUATE-FOR graph convolutional filters. graphs USED-FOR graph convolutional filters. model USED-FOR data - specific filters. Adaptive Filter Graph Neural Network ( AFGNN ) HYPONYM-OF model. AFGNN USED-FOR graph. base filters PART-OF AFGNN. graph filter assessment USED-FOR AFGNN. synthetic and real - world benchmark datasets EVALUATE-FOR model. model USED-FOR filter. Method are optimal filter, and Graph Filter Discriminant Score ( GFD ). Task is semi - supervised node classification task. OtherScientificTerm is loss term. ","This paper proposes an adaptive filter graph neural network (AFGNN) for semi-supervised node classification. The main idea is to learn a set of filters for each node in a graph, which are then used to train a graph convolutional network (GNN). The proposed method is evaluated on a variety of node classification tasks, including node classification, node classification and node clustering.   ","This paper proposes an adaptive filter graph neural network (AFGNN) for graph classification. The main idea is to design a graph filter for each node in a graph, and then use the graph filter to assess the performance of the filter on the graph. This is done by using the GFD (Graph Filter Discriminant Score) as an assessment tool. The authors show that the AFGNN is able to improve the performance on a variety of graph classification tasks. "
21636,SP:3c5ec9dbcf914c8901e4e35f3c2a7df4707422ab,"i.i.d. test set EVALUATE-FOR Overparameterized neural networks. Distributionally robust optimization ( DRO ) USED-FOR models. group DRO USED-FOR overparameterized neural networks. average training loss FEATURE-OF model. vanishing worst - case training loss FEATURE-OF model. natural language inference task CONJUNCTION image tasks. image tasks CONJUNCTION natural language inference task. early stopping HYPONYM-OF regularization. regularization USED-FOR group DRO models. regularization USED-FOR worst - group generalization. it USED-FOR average generalization. overparameterized regime FEATURE-OF worst - group generalization. stochastic optimization algorithm USED-FOR group DRO models. convergence guarantees FEATURE-OF stochastic optimization algorithm. OtherScientificTerm is spurious correlations. Metric are worst - case training loss, worst - group accuracies, and average accuracies. ","This paper studies distributionally robust optimization (DRO) for overparameterized neural networks. The authors show that the worst-case training loss is vanishing in the overparametrized regime, and show that worst-group generalization can be improved by regularization. They also show that early stopping is a regularization technique that improves the average generalization.   ","This paper studies the problem of distributionally robust optimization (DRO) for overparameterized neural networks. The authors show that the worst-case training loss is vanishing in the overparametrized regime, and that the average training loss of the model is vanishing as well. They also show that group DRO models with early stopping (e.g., early stopping) are more robust to spurious correlations in the worst group generalization regime. Finally, they provide convergence guarantees for a stochastic optimization algorithm that can be used to improve the generalization performance of group DRo models."
21645,SP:eb1ee2e0f7d8466a04b58508ecb3da7b667eecdf,"local explanation methods USED-FOR decision of black - box classifiers. ad hoc constraints FEATURE-OF classification loss. ad hoc constraints USED-FOR relevance scores. neural network USED-FOR distribution of relevance scores. it CONJUNCTION neural network. neural network CONJUNCTION it. it USED-FOR distribution of relevance scores. classification loss USED-FOR predictor. strategy USED-FOR discriminative scores. features USED-FOR discriminative scores. faithfulness CONJUNCTION explainability. explainability CONJUNCTION faithfulness. method COMPARE others. others COMPARE method. faithfulness EVALUATE-FOR others. explainability EVALUATE-FOR others. faithfulness EVALUATE-FOR method. explainability EVALUATE-FOR method. Generic is methods. Method are mask predictor, and distribution controllers. OtherScientificTerm is hyperparameters. ","This paper proposes a method for improving the faithfulness of black-box classifiers. The proposed method is based on the idea of using a mask predictor to predict the distribution of relevance scores, which are then used to train a neural network that predicts the discriminative scores. The method is evaluated on a variety of image classification tasks.   ","This paper proposes a method to improve the faithfulness of black-box classifiers. The proposed method is based on the notion of local explanation, which is an extension of the local explanation method. The main idea of the paper is to use a neural network to predict the distribution of relevance scores, which are then used to train a classifier. The method is evaluated on a variety of datasets, and it is shown that the proposed method outperforms other local explanation methods. "
21654,SP:32ea7cbc47cbdb1f703f4e07c31ce90abe083424,"deep network USED-FOR image reconstruction and classification problems. task - specific network USED-FOR domain specific problem. patches USED-FOR task - specific network. auto - encoder or classifier HYPONYM-OF task - specific network. slack variable USED-FOR top - K selection. method USED-FOR recurring structures. it COMPARE state - of - the - art. state - of - the - art COMPARE it. Task are detection of multiple object instances, and training optimization problem. OtherScientificTerm is supervision. Generic are network, and It. Method are non - differentiable top - K selection process, and multi - stage training. ","This paper proposes a method for image reconstruction and classification problems with multiple object instances. The proposed method is based on top-k selection, which is a non-differentiable top-K selection process. The method is evaluated on ImageNet and CIFAR-10 datasets.   ","This paper proposes a method for training a task-specific deep neural network for image reconstruction and classification. The proposed method is based on a non-differentiable top-K selection process, where the top-k selection process is a slack variable. The key idea of the method is to use a fixed number of patches to train the network. The method is evaluated on a variety of image reconstruction tasks and classification tasks."
21663,SP:da1c5f6351d531482e90b86c3cceb52850c520de,assembly code USED-FOR state change. CPU FEATURE-OF state change. RAM FEATURE-OF state change. self - learning reinforcement learning USED-FOR large code space. AutoAssemblet HYPONYM-OF neural program synthesis algorithm. self - learning reinforcement learning USED-FOR AutoAssemblet. Policy networks CONJUNCTION value networks. value networks CONJUNCTION Policy networks. value networks USED-FOR Monte Carlo Tree Search. Policy networks USED-FOR synthesis. value networks USED-FOR synthesis. multi - entropy policy sampling technique USED-FOR online update correlations. AutoAssemblet USED-FOR basic programming tasks. success rates EVALUATE-FOR baselines. AutoAssemblet COMPARE baselines. baselines COMPARE AutoAssemblet. success rates EVALUATE-FOR AutoAssemblet. Method is Neural inductive program synthesis. Task is task generating instructions. ,"This paper proposes a method for program synthesis using reinforcement learning. The main idea is to use reinforcement learning to learn a policy to generate assembly code that can be used to solve a given task. The method is based on a multi-entropy policy sampling technique, where the policy is sampled from a set of state-action pairs, and the value network is trained to predict the next state. The proposed method is evaluated on a variety of tasks and achieves state-of-the-art performance. ","This paper proposes AutoAssemblet, a neural program synthesis algorithm for large code space. The authors propose a multi-entropy policy sampling technique to improve the performance of the synthesis process. They also propose a Monte Carlo Tree Search method for Monte Carlo tree search. The proposed method is evaluated on a variety of tasks and compared with several baselines."
21672,SP:0d4687fc36c02e27d1b95d532a3947589f92b1da,"speed of training CONJUNCTION resource requirements. resource requirements CONJUNCTION speed of training. accuracy CONJUNCTION speed of training. speed of training CONJUNCTION accuracy. accuracy EVALUATE-FOR model architecture. model architecture USED-FOR gradient descent optimization. speed of training EVALUATE-FOR gradient descent optimization. speed of training EVALUATE-FOR model architecture. ODE ’s coefficient matrix H USED-FOR convergence rate. first - order ODE USED-FOR gradient descent. analysis technique USED-FOR H. analysis technique USED-FOR model architecture modifications. OtherScientificTerm are neural network architecture design space, network, and model architecture parameters. Generic is architecture. Metric is speed of convergence. ",This paper studies the convergence rate of gradient descent on neural networks with different architectures in terms of the ODE’s coefficient matrix H. The authors show that the speed of convergence depends on the number of parameters in the network and the number and dimension of the training set. They also provide an analysis technique to analyze the effect of model architecture modifications on the convergence rates. ,"This paper studies the convergence rate of a neural network architecture design space in terms of the first-order ODE. The authors show that the speed of convergence depends on the number of parameters in the network, and the model architecture parameters. They also provide an analysis technique to study the effect of model architecture modifications."
21681,SP:3e3bc8f617df742a395e7d315ec3810a42071294,"overparametrized neural networks ( NNs ) CONJUNCTION kernel methods. kernel methods CONJUNCTION overparametrized neural networks ( NNs ). initialization USED-FOR overparametrized NNs. gradient descent USED-FOR overparametrized NNs. minimum complexity solution USED-FOR interpolating kernel method. squared loss USED-FOR fully - connected wide ReLU - NNs. test error EVALUATE-FOR wide NNs. minimum complexity interpolating kernel methods CONJUNCTION NNs. NNs CONJUNCTION minimum complexity interpolating kernel methods. generalization EVALUATE-FOR initialization scheme. Generic are first, and second. OtherScientificTerm are initialization variance, and generalization bounds. Method is initialization strategies. ","This paper studies the generalization performance of overparametrized neural networks (NNs) and interpolating kernel methods (KMs) in the presence of initialization variance variance. The authors show that the initialization variance is a function of the number of parameters in the network. They show that for wide ReLU-NNs, they can obtain a test error of $O(1/\sqrt{T})$ for any initialization strategy. They also provide a generalization bound for the squared loss of a fully connected ReLU network.   ","This paper studies the generalization properties of overparametrized neural networks (NNs) and interpolating kernel methods (i.e., interpolating kernels). The authors provide a generalization bound for wide ReLU-NNs, and show that generalization is bounded by the variance of the initialization variance. They also provide generalization bounds for overparameterized kernel methods and NNs.   "
21690,SP:b15ea009a36a0a76728dfc103d668d6781a8a99a,"cars CONJUNCTION pedestrians. pedestrians CONJUNCTION cars. Detecting objects USED-FOR autonomous driving. 3D FEATURE-OF pedestrians. 3D FEATURE-OF cars. 3D FEATURE-OF Detecting objects. pedestrians HYPONYM-OF Detecting objects. cars HYPONYM-OF Detecting objects. LiDAR sensors USED-FOR accurate depth information. LiDAR sensors USED-FOR approaches. stereo images USED-FOR pseudo - LiDAR. stereo depth estimation USED-FOR pseudo - LiDAR framework. loss function USED-FOR depth estimation of faraway objects. stereo network architecture CONJUNCTION loss function. loss function CONJUNCTION stereo network architecture. depth estimates USED-FOR depthpropagation algorithm. depth estimation CONJUNCTION stereo - based 3D object detection. stereo - based 3D object detection CONJUNCTION depth estimation. KITTI object detection benchmark EVALUATE-FOR approach. approach USED-FOR depth estimation. detection accuracy USED-FOR faraway objects. approach USED-FOR stereo - based 3D object detection. approach COMPARE detection accuracy. detection accuracy COMPARE approach. OtherScientificTerm are insufficient information, and depth map. Task is 3D detection. ",This paper proposes a method for depth-based 3D object detection using pseudo-LiDAR. The proposed method is based on the idea that depth information from faraway objects can be used to improve the performance of the depth-propagation algorithm. The method is evaluated on the KITTI object detection benchmark.   ,This paper proposes a pseudo-LiDAR framework for stereo-based 3D object detection. The authors propose a depth-propagation-based method for depth estimation of faraway objects. The proposed method is based on a stereo network architecture and a loss function. The method is evaluated on the KITTI object detection benchmark and shows promising results.
21699,SP:983d84502264633f3385d426c1d4601a0744ea9a,"models USED-FOR sensitive domains. deep neural networks USED-FOR adversarial examples. defense USED-FOR attacks. detecting adversarial samples USED-FOR methods. adversarial example detection method USED-FOR norm - constrained white - box attacks. detector USED-FOR natural data. base detectors USED-FOR K class classification problem. generative approach USED-FOR detecting / classifying adversarial examples. classconditional data USED-FOR unnormalized density model. unnormalized density model USED-FOR base detector. classconditional data USED-FOR base detector. Method are detection mechanism, one - versus - the - rest classification, k - th detector, adversarial example detection / classification methods, and GAT - Generative - Adversarial - Training. OtherScientificTerm is adversarial example. ",This paper proposes a generative adversarial training method to detect and classify adversarial examples. The proposed method is based on an unnormalized density model with a class-conditional classifier. The method is shown to outperform the state-of-the-art methods in terms of adversarial detection and classification accuracy. ,"This paper proposes a generative adversarial example detection method for white-box attacks. The proposed method is based on the GAT-Generative-Adversarial-Training (GAT) framework, where the detector is trained with a class-conditional data, and the class-conditioned data is used to train the detector. The method is evaluated on a variety of adversarial detection and classification tasks, and it is shown that the proposed method outperforms the baselines."
21708,SP:461e9308d050bc3dc7b35233452668bb31f5d491,"Exploration PART-OF model - free reinforcement learning. sparse reward environments FEATURE-OF Exploration. intrinsic rewards USED-FOR exploration. MiniGrid FEATURE-OF procedurally - generated tasks. high - dimensional observations USED-FOR tasks. tasks EVALUATE-FOR method. procedurally - generated tasks EVALUATE-FOR method. approach COMPARE exploration methods. exploration methods COMPARE approach. exploration methods USED-FOR procedurally - generated MiniGrid environments. intrinsic reward FEATURE-OF agent. approaches COMPARE intrinsic reward. intrinsic reward COMPARE approaches. OtherScientificTerm are extrinsic rewards, and procedurally - generated environments. Method is learned state representation. Generic is it. ","This paper proposes a method for model-free reinforcement learning in sparse reward environments that uses intrinsic rewards to encourage exploration. The proposed method is based on the observation-based exploration approach, where the agent is given a set of high-dimensional observations and a learned representation of the state space. The agent is encouraged to explore the environment in a way that maximizes the intrinsic reward. The method is evaluated on MiniGrid tasks and achieves state-of-the-art performance.   ","This paper proposes a method for model-free exploration in sparse reward environments, where the agent is given a set of high-dimensional observations of the environment. The agent is encouraged to explore the environment in a way that maximizes the intrinsic reward (i.e., the state representation of the agent) and minimizes the extrinsic reward (e.g., the number of tasks in the environment). The authors show that their method outperforms the state-of-the-art exploration methods on a variety of MiniGrid tasks. "
21717,SP:c002c20b5e8696588e029c0f65e88860418826c4,"recall EVALUATE-FOR retrieval algorithm. scoring phase COMPARE retrieval phase. retrieval phase COMPARE scoring phase. cross - attention models USED-FOR BERT - style pre - training tasks. sparse handcrafted features USED-FOR models. pre - training tasks USED-FOR embedding - based Transformer model. Transformer models COMPARE BM-25. BM-25 COMPARE Transformer models. Transformer models COMPARE embedding models. embedding models COMPARE Transformer models. BM-25 CONJUNCTION embedding models. embedding models CONJUNCTION BM-25. paragraph - level pre - training tasks EVALUATE-FOR Transformer models. Body First Selection ( BFS ) CONJUNCTION Wiki Link Prediction ( WLP ). Wiki Link Prediction ( WLP ) CONJUNCTION Body First Selection ( BFS ). Inverse Cloze Task ( ICT ) CONJUNCTION Body First Selection ( BFS ). Body First Selection ( BFS ) CONJUNCTION Inverse Cloze Task ( ICT ). Inverse Cloze Task ( ICT ) HYPONYM-OF paragraph - level pre - training tasks. Wiki Link Prediction ( WLP ) HYPONYM-OF paragraph - level pre - training tasks. Body First Selection ( BFS ) HYPONYM-OF paragraph - level pre - training tasks. Task is large - scale query - document retrieval problem. Material is large document corpus. Generic is problem. OtherScientificTerm are solution space, and TF - IDF weights. Method are Information Retrieval ( IR ) methods, and embedding - based retrieval models. ","This paper proposes a method for large-scale query-document retrieval, where the goal is to retrieve documents from a large document corpus. The authors propose to use a Transformer-based retrieval model, which is based on an embedding-based Transformer model. The model is trained using BERT-style pre-training tasks, where sparse handcrafted features are used to improve the performance of the model.   The authors show that the proposed method outperforms the baselines in terms of accuracy and recall on three retrieval tasks. ","This paper proposes a new approach for large-scale query-document retrieval. The approach is based on embedding-based Transformer models, which are trained using BERT-style pre-training tasks with sparse handcrafted features. The authors show that the proposed approach outperforms BM-25 and embedding models in terms of performance on a variety of retrieval tasks, including Inverse Cloze Task (ICT), Wiki Link Prediction (WLP), and Body First Selection (BFS)."
21726,SP:4e161e08a624f87633dfb49dfd46bd1665e15189,social graphs CONJUNCTION molecular structures. molecular structures CONJUNCTION social graphs. point clouds CONJUNCTION social graphs. social graphs CONJUNCTION point clouds. Graph neural networks USED-FOR applications. Graph neural networks USED-FOR learning relational representations. learning relational representations CONJUNCTION modeling data on irregular domains. modeling data on irregular domains CONJUNCTION learning relational representations. modeling data on irregular domains HYPONYM-OF applications. molecular structures HYPONYM-OF modeling data on irregular domains. point clouds HYPONYM-OF modeling data on irregular domains. social graphs HYPONYM-OF modeling data on irregular domains. learning relational representations HYPONYM-OF applications. graph convolution operator USED-FOR graph neural network architectures. graph convolution operations CONJUNCTION non - parameterized pooling or expansion layers. non - parameterized pooling or expansion layers CONJUNCTION graph convolution operations. non - parameterized pooling or expansion layers USED-FOR representational hierarchy. graph convolution operations USED-FOR representational hierarchy. parameterized strided and transpose convolution operations CONJUNCTION skip connections. skip connections CONJUNCTION parameterized strided and transpose convolution operations. convolutional network architectures CONJUNCTION parameterized strided and transpose convolution operations. parameterized strided and transpose convolution operations CONJUNCTION convolutional network architectures. bipartite graph convolution operation HYPONYM-OF parameterized transformation. framework USED-FOR multi - graph aggregation. framework COMPARE graph convolution and pooling. graph convolution and pooling COMPARE framework. framework USED-FOR flexible and adaptable network architectures. BiGraphNet HYPONYM-OF flexible and adaptable network architectures. memory requirements FEATURE-OF hierarchical networks. graph convolution USED-FOR hierarchical architectures. graph convolution CONJUNCTION single parametric bipartite graph convolution. single parametric bipartite graph convolution CONJUNCTION graph convolution. graph skip connections CONJUNCTION graph autoencoders. graph autoencoders CONJUNCTION graph skip connections. BiGraphNet formalism ( iii ) USED-FOR architectures. modeling flexibility USED-FOR architectures. BiGraphNet formalism ( iii ) USED-FOR modeling flexibility. graph autoen,"This paper proposes a novel graph convolution operator, BiGraphNet, for graph neural networks. The main idea is to combine graph convolutions with non-parameterized pooling or expansion layers. Theoretical analysis and experimental results show that the proposed method is able to achieve state-of-the-art performance on a variety of graph classification tasks.","This paper proposes a new graph neural network architecture, BiGraphNet, which can be applied to a variety of graph neural networks, including graph convolution, graph pooling, skip connections, graph autoencoder, and graph skip connections. The main contribution of the paper is the introduction of a new biGraphNet formalism (ii) that allows for flexible and adaptable network architectures that can be used for a wide range of applications. The paper also introduces a new parameterized transformation (Bipartite Graph Convolution) that is used to model the representational hierarchy of the graph. The proposed method is evaluated on a range of datasets, including social graphs, molecular structures, point clouds, and social graphs."
21735,SP:9b9b6ee9014e5538442ba76d6059ed01f59ec8fb,"metric function USED-FOR metric - based few - shot classification algorithms. metric function USED-FOR feature embeddings. metric - based methods USED-FOR few - shot classification. domain shifts FEATURE-OF few - shot classification. feature - wise transformation layers USED-FOR image features. affine transforms USED-FOR feature distributions. feature - wise transformation layers USED-FOR feature distributions. affine transforms USED-FOR feature - wise transformation layers. learning - to - learn approach USED-FOR hyper - parameters. hyper - parameters FEATURE-OF feature - wise transformation layers. learning - to - learn approach USED-FOR feature - wise transformation layers. learning - to - learn approach USED-FOR feature distributions. Cars CONJUNCTION Places. Places CONJUNCTION Cars. CUB CONJUNCTION Cars. Cars CONJUNCTION CUB. Places CONJUNCTION Plantae. Plantae CONJUNCTION Places. mini - ImageNet CONJUNCTION CUB. CUB CONJUNCTION mini - ImageNet. Plantae HYPONYM-OF few - shot classification datasets. mini - ImageNet HYPONYM-OF few - shot classification datasets. CUB HYPONYM-OF few - shot classification datasets. Places HYPONYM-OF few - shot classification datasets. Cars HYPONYM-OF few - shot classification datasets. feature - wise transformation layer USED-FOR metric - based models. feature - wise transformation layer USED-FOR few - shot classification. domain shift FEATURE-OF few - shot classification. Task are Few - shot classification, and domain generalization setting. Generic is methods. OtherScientificTerm is feature distribution. ",This paper proposes a method for few-shot classification with domain generalization. The method is based on the affine transformation of the feature embeddings. The main idea is to use a learning-to-learn approach to learn the feature-wise transformation layers. Experiments show that the proposed method outperforms the state-of-the-art methods on a variety of image classification tasks. ,"This paper proposes a new method for few-shot classification based on feature-wise transformation layers. The key idea is to use affine transforms to learn the feature distributions of the feature embeddings, and then use a learning-to-learn approach to learn hyper-parameters for the feature distribution. The proposed method is tested on a variety of datasets, including mini-ImageNet, CUB, Places, Cars, and Plantae. "
21744,SP:df46627cb984a56bba36d510bfc52e00751e9107,approach USED-FOR Lagrangian fluid simulation. convolutional network USED-FOR approach. moving particles USED-FOR fluids. networks USED-FOR moving particles. graph structure USED-FOR particles. N - D convolutions USED-FOR continuous domain. network architecture USED-FOR inverse problems. network architecture USED-FOR arbitrary collision geometries. continuous convolutions COMPARE prior formulations. prior formulations COMPARE continuous convolutions. accuracy CONJUNCTION speed. speed CONJUNCTION accuracy. speed EVALUATE-FOR prior formulations. speed EVALUATE-FOR continuous convolutions. accuracy EVALUATE-FOR continuous convolutions. accuracy EVALUATE-FOR prior formulations. Generic is approaches. Method is spatial convolutions. ,This paper proposes a continuous convolutional network for Lagrangian fluid simulation. The proposed method is based on convolution of moving particles in a graph structure. The authors propose to use N-D convolutions in the continuous domain to solve the inverse problems. They show that the proposed method outperforms prior methods in terms of accuracy and speed.  ,"This paper proposes a new convolutional network for Lagrangian fluid simulation. The proposed method is based on the idea of N-D convolution, which is a continuous convolution with a graph structure. The authors show that the proposed method outperforms the state-of-the-art in terms of accuracy and speed.  "
21753,SP:3e17f333cf07183969c02bb66afdd3ccbf25bb19,"accuracy CONJUNCTION predictive uncertainty. predictive uncertainty CONJUNCTION accuracy. predictive uncertainty EVALUATE-FOR single neural networks. accuracy EVALUATE-FOR single neural networks. BatchEnsemble1 HYPONYM-OF ensemble method. Hadamard product USED-FOR weight matrix. ensembles COMPARE BatchEnsemble. BatchEnsemble COMPARE ensembles. BatchEnsemble COMPARE ensembles. ensembles COMPARE BatchEnsemble. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. speedup CONJUNCTION memory reduction. memory reduction CONJUNCTION speedup. CIFAR-10 EVALUATE-FOR BatchEnsemble. out - of - distribution tasks EVALUATE-FOR BatchEnsemble. BatchEnsemble USED-FOR lifelong learning. Split - CIFAR-100 EVALUATE-FOR BatchEnsemble. BatchEnsemble COMPARE progressive neural networks. progressive neural networks COMPARE BatchEnsemble. computational and memory costs EVALUATE-FOR BatchEnsemble. BatchEnsemble USED-FOR lifelong learning. Split - ImageNet USED-FOR lifelong learning. sequential learning tasks PART-OF lifelong learning. Method are Ensembles, and neural networks. Metric is ensemble ’s cost. OtherScientificTerm are mini - batch, and ensemble. ",This paper proposes an ensemble method to reduce the computational and memory cost of training neural networks. The proposed method is based on the Hadamard product of the weight matrices of mini-batch and mini-batch. The method is evaluated on out-of-distribution tasks on CIFAR-10 and Split-ImageNet. The results show that the proposed method outperforms existing methods in terms of accuracy and predictive uncertainty.  ,"This paper proposes a new ensemble method called BatchEnsemble, which is based on the Hadamard product of the weight matrix of the ensemble. The authors show that the proposed method outperforms the state-of-the-art ensembles on CIFAR-10 and Split-ImageNet datasets. They also show that it can be used for lifelong learning. "
21762,SP:a123a425ef3eb6188833d5a42e851bc3fa59df65,"neural network - based partial differential equations solver USED-FOR forward and inverse problems. mesh free and shape free FEATURE-OF solver. neural network USED-FOR solution. explicit smooth differentiable function USED-FOR solution. analytical form FEATURE-OF explicit smooth differentiable function. finite differences CONJUNCTION finite elements. finite elements CONJUNCTION finite differences. finite differences HYPONYM-OF numerical methods. finite elements HYPONYM-OF numerical methods. algorithm USED-FOR forward and inverse problems. Robust boundary conditions constraints CONJUNCTION regularizers. regularizers CONJUNCTION Robust boundary conditions constraints. Electrical Impedance Tomography ( EIT ) CONJUNCTION diffusion and wave equations. diffusion and wave equations CONJUNCTION Electrical Impedance Tomography ( EIT ). free shape 2D second order systems USED-FOR Electrical Impedance Tomography ( EIT ). method USED-FOR diffusion and wave equations. method USED-FOR Electrical Impedance Tomography ( EIT ). free shape 2D second order systems USED-FOR diffusion and wave equations. method USED-FOR free shape 2D second order systems. Method is unsupervised approach. Generic are network, framework, and methods. OtherScientificTerm are strong PDE solution, boundary conditions, derivatives of the desired function, and loss function. ","This paper proposes a neural network-based method for solving the forward and inverse partial differential equations (PDEs) in a mesh-free and shape-free manner. The main idea is to use a network to learn the solution to the PDE, which is then used to compute the loss function. The proposed method is shown to be able to solve PDEs with strong PDE solutions, and is able to find a solution that satisfies the boundary conditions of the desired function.",This paper proposes a neural network-based partial differential equations solver for mesh-free and shape-free PDE solvers. The main contribution of the paper is to propose an explicit smooth differentiable function that can be used to solve the solver. The authors show that the proposed method can solve the forward and inverse problems with strong boundary conditions. They also show that their method can also be used for free shape 2D second order systems.
21771,SP:973d0ad0faadcf7298300f2758de9154205e7113,neural networks PART-OF deep learning. Binarized Neural Networks HYPONYM-OF networks. SAT solvers HYPONYM-OF logic - based reasoning tools. tools USED-FOR existential and probabilistic queries. tools USED-FOR explanation generation. existential and probabilistic queries FEATURE-OF network. they USED-FOR logic - based reasoners. training procedure USED-FOR network. network USED-FOR SAT solvers. BNN architecture CONJUNCTION training procedure. training procedure CONJUNCTION BNN architecture. approach COMPARE work. work COMPARE approach. work USED-FOR existential and probabilistic queries. approach USED-FOR existential and probabilistic queries. deep neural networks COMPARE work. work COMPARE deep neural networks. approach USED-FOR deep neural networks. deep neural networks USED-FOR existential and probabilistic queries. OtherScientificTerm is Boolean logic. Generic is methods. Method is BNNs. Metric is accuracy. ,"This paper proposes a method to improve the performance of SAT solvers, a class of logic-based reasoning tools. The proposed method is based on a BNN architecture and a training procedure. The main idea is to train the BNNs to be able to answer both existential and probabilistic queries. The method is evaluated on a set of SAT problems and compared with a number of baselines.","This paper proposes a new method for solving SAT solvers. The main idea is to train a neural network to solve the SAT solver using a BNN architecture and a training procedure. The training procedure consists of two steps: first, the network is trained to solve a set of existential and probabilistic queries, and second, the training procedure is used to train the BNN. The proposed method is evaluated on a variety of SAT problems, and it is shown that the proposed method outperforms existing methods."
21780,SP:ca985e758f195bd04fb9f24b290a83974d6d308b,"expressive power FEATURE-OF graph neural networks. message - passing framework ( GNNmp ) FEATURE-OF graph neural networks. node attributes CONJUNCTION layer expressiveness. layer expressiveness CONJUNCTION node attributes. technique USED-FOR impossibility statements. approximation USED-FOR tasks. Method are GNNmp, and distributed computing. OtherScientificTerm is lower bounds. Material is graphs. Generic is problems. ","This paper studies the expressive power of GNNs in the message-passing framework (GNNs). The authors show that the expressiveness of a GNN is a function of the number of nodes in the graph, the node attributes, and the layer expressiveness. The authors prove that for a given GNN, there exists a set of impossibility statements that can be proved to be true for any GNN. They then show that this set of statements can be proven to be false in certain cases. Finally, the authors provide a lower bound on the expressivity of the GNN and show that it is upper bounded.",This paper proposes a new lower bound on the number of impossibility statements in the message passing framework (GNNmp) of graph neural networks. The lower bound is based on the fact that GNNmp is a distributed computing problem. The authors show that the upper bound is lower than the lower bound of the original paper. They also provide an approximation of the lower bounds. 
21789,SP:a98ae70a91850bbe624c307ba61d3daeb2494b82,"flow - based density models USED-FOR target distributions. complicated topologies FEATURE-OF target distributions. continuous bijections USED-FOR flow - based density models. stacked continuous mixtures of bijections PART-OF LGFs. flow - based methods USED-FOR LGF model. method COMPARE flow - based methods. flow - based methods COMPARE method. normalising flows COMPARE LGFs. LGFs COMPARE normalising flows. density estimation tasks EVALUATE-FOR LGFs. Method are localised generative flows ( LGFs ), and variational scheme. OtherScientificTerm are bijection, and log likelihoods. ","This paper proposes a localised generative flow (LGF) method for density estimation. The proposed method is based on the idea of localised bijections, where the target distribution is represented by a continuous mixture of continuous bijection. The authors show that the proposed method can be viewed as a variant of normalising flows, and that it can be used in conjunction with a variational scheme. Theoretical analysis is provided to show the convergence of the method. Empirical results are provided to demonstrate the effectiveness of the proposed approach.","This paper proposes a new localised generative flow (LGF) model for density estimation. The main idea is to use continuous bijections in LGF to model the target distributions. The bijection is composed of stacked continuous mixtures of bijection, and the log likelihoods are computed by using a variational scheme. The method is evaluated on a variety of density estimation tasks, and it is shown that the proposed LGF model outperforms the state-of-the-art."
21798,SP:3adc341dece170f428195e4dccadfb5f5daddf2d,"environment re - splitting CONJUNCTION feature replacement. feature replacement CONJUNCTION environment re - splitting. environment re - splitting USED-FOR diagnosis experiments. feature replacement USED-FOR diagnosis experiments. language CONJUNCTION navigational graph. navigational graph CONJUNCTION language. ResNet features USED-FOR low - level visual appearance. low - level visual information FEATURE-OF semantic representations. features USED-FOR agent. baseline agent model CONJUNCTION training method. training method CONJUNCTION baseline agent model. R4R CONJUNCTION CVDN. CVDN CONJUNCTION R4R. R2R CONJUNCTION R4R. R4R CONJUNCTION R2R. R2R HYPONYM-OF datasets. OtherScientificTerm are naturallanguage instructions, step - by - step navigational instructions, and semantic features. Task is VLN. Method are neural agent models, and agent model. Generic is state - of - the - art models. ","This paper studies the problem of visual navigation in natural language navigation (VLN) tasks, where the agent is given a set of instructions in a natural language and must navigate through the environment to reach the goal. The authors propose to use a combination of ResNet features to learn a low-level visual appearance representation of the environment, which is then used to guide the agent to the goal in the natural language instructions. The proposed method is evaluated on R2R, R4R, and CVDN datasets.   ","This paper studies the problem of visual navigation in natural language navigation (VLN), where the agent is given a set of instructions to navigate through an environment. The agent is trained with a ResNet-based model, and the goal is to find the best way to navigate the environment. To this end, the authors propose a new dataset, called R2R, which is composed of two datasets, R4R and CVDN. The dataset consists of a language, a navigational graph, and a visual representation of the environment, where the environment is represented as a visual image. The authors show that the visual representation can be used as a surrogate for the language and the navigational graphs. They also show that this representation can help the agent to identify the environment and the environment re-splits.  "
21807,SP:298e0043e99f586d314fbd9d16fdc6ae885e1ebb,"implicit human feedback USED-FOR DRL algorithm. expert labeling CONJUNCTION demonstrations. demonstrations CONJUNCTION expert labeling. agent ’s learning USED-FOR RL tasks. implicit feedback USED-FOR agent ’s learning. system USED-FOR implicit human feedback. implicit human feedback USED-FOR state - action pairs. Atari - type environment FEATURE-OF state - action pairs. error - related event potentials HYPONYM-OF implicit human feedback. auxiliary reward function USED-FOR DRL algorithm. them USED-FOR DRL algorithm. DRL algorithm USED-FOR learning of the game. them USED-FOR auxiliary reward function. electroencephalogram ( EEG ) cap USED-FOR error - potentials. definition USED-FOR game. frameworks USED-FOR error - potential based feedback system. DRL USED-FOR error - potential based feedback system. implicit human feedback USED-FOR complex environments ( games ). synthetic and real user experiments EVALUATE-FOR approach. OtherScientificTerm are human feedback, non - expert humans, human ’s intrinsic reactions, event - related electric potentials, and Atari - games. Generic is paradigm. Method is RL agent. ",This paper proposes to use implicit human feedback as an auxiliary reward function in Deep RL (DRL) to improve the performance of an RL agent in Atari-type environments. The authors propose to use an electroencephalogram (EEG) to capture the human’s intrinsic reactions to the state-action pairs in an Atari-like environment and use this information as a reward function to guide the learning of the game. The proposed method is evaluated on a variety of Atari games and real-world environments. ,"This paper proposes a new approach for learning Atari-type environments with implicit human feedback. The authors propose a new reward function for learning the state-action pairs in an Atari-like environment. The reward function is based on an electroencephalogram (EEG) cap that captures the human’s intrinsic reactions to the state and action pairs. The paper also proposes an auxiliary reward function, which is a combination of the expected reward function and the expected error-potential function. Experiments on synthetic and real-world Atari environments demonstrate the effectiveness of the proposed method. "
21816,SP:a8395f8b877e1eebaef9ff2e8b4e488d55a74ef4,"laconic classification USED-FOR diverse image classifiers. classifier USED-FOR approximate minimal - entropy positive image. classifier USED-FOR approximate minimal - entropy positive image. colour reduction CONJUNCTION resolution reduction. resolution reduction CONJUNCTION colour reduction. reductions USED-FOR classification. crop CONJUNCTION colour reduction. colour reduction CONJUNCTION crop. crop HYPONYM-OF reductions. resolution reduction HYPONYM-OF reductions. colour reduction HYPONYM-OF reductions. complementary frameworks USED-FOR minimal - entropy positive images. cropping CONJUNCTION reduced colour. reduced colour CONJUNCTION cropping. texture bias FEATURE-OF ILSVRC - trained models. minimal - entropy positive images USED-FOR human and machine classifiers. complementary frameworks USED-FOR human and machine classifiers. reduced resolution FEATURE-OF humans. machines CONJUNCTION reduced resolution. reduced resolution CONJUNCTION machines. cropping USED-FOR machines. reduced colour USED-FOR machines. Metric are entropy, and precision. Material is ILSVRC test - set. Method are machine classifiers, and machine models. ","This paper studies the problem of laconic classification, i.e. learning from images with minimal entropy positive images. The authors show that reducing the number of pixels and reducing the resolution of the image can improve the performance of a classifier. The main contribution of the paper is that the proposed method is able to learn from minimal-entropy positive images in a way that is similar to human and machine learning.   ","This paper studies the problem of laconic classification in the context of minimal-entropy positive image classification. The authors propose two complementary frameworks for reducing the entropy of positive images: cropping and reduced colour reduction. They show that reducing the colour and reducing the resolution of negative images can improve the performance of human and machine classifiers. They also show that cropping reduces the accuracy of human classifiers, while reduced colour and reduced resolution of machines.  "
21825,SP:81cec8f907d8fa0653b5bc08af1f59bfefd49619,"defenses USED-FOR Convolutional Neural Networks. instability assumption USED-FOR defense techniques. deterministic lossy compression algorithms CONJUNCTION randomized perturbations. randomized perturbations CONJUNCTION deterministic lossy compression algorithms. robustness EVALUATE-FOR randomized perturbations. deterministic lossy compression algorithms PART-OF defenses. randomized perturbations PART-OF defenses. Material is adversarial examples. OtherScientificTerm are small perturbations, and decision space. Method is perturbation defenses. ",This paper studies the robustness of neural networks against adversarial perturbations. The authors show that the instability assumption is necessary for robustness to adversarial examples. They show that deterministic lossy compression algorithms and randomized perturbation can be used to improve robustness. They also provide a theoretical analysis to show that a deterministic compression algorithm can be combined with randomized perturbed examples. ,"This paper studies the robustness of perturbation defenses against adversarial examples in the presence of small perturbations in the decision space. In particular, the authors consider the instability assumption, which is an important assumption in the defense literature. The authors show that the stability assumption holds for deterministic lossy compression algorithms and randomized perturbs. They also show that deterministic compression algorithms can be robust against randomized perturbed examples.   "
21834,SP:a136b98e0ed478144ce9dd26e2b6d611543124e8,"view prediction HYPONYM-OF prediction tasks. view prediction USED-FOR 3D visual recognition. moving camera USED-FOR 2.5D ( color and depth ) video streams. 2.5D ( color and depth ) video streams USED-FOR neural 3D mapping networks. contrastive prediction losses COMPARE color regression loss. color regression loss COMPARE contrastive prediction losses. visual representations USED-FOR semi - supervised learning of 3D object detectors. model USED-FOR visual representations. visual representations USED-FOR unsupervised learning of 3D moving object detectors. model USED-FOR unsupervised learning of 3D moving object detectors. semi - supervised learning of 3D object detectors CONJUNCTION unsupervised learning of 3D moving object detectors. unsupervised learning of 3D moving object detectors CONJUNCTION semi - supervised learning of 3D object detectors. videos of dynamic scenes FEATURE-OF motion of the inferred 3D feature maps. scalable self - supervised task USED-FOR 3D object detection. view prediction USED-FOR 3D object detection. view prediction HYPONYM-OF scalable self - supervised task. Method is Predictive coding theories. Generic are task, and them. OtherScientificTerm are perception, retinas, and 3D feature maps. Material is complex photorealistic data. ",This paper proposes a method for self-supervised 3D object detection in dynamic scenes. The method is based on a neural 3D mapping network that predicts the 3D feature maps from a 2.5D (color and depth) video stream and uses a contrastive prediction loss. The proposed method is shown to outperform the state-of-the-art methods for 3D vision and 3D motion detection.   ,"This paper proposes a new self-supervised view prediction task for 3D object detection. The proposed method is based on contrastive learning, where the model is trained on a 2.5D (color and depth) video stream and a 2D (depth and color) video streams. The authors show that the proposed method outperforms the state-of-the-art contrastive prediction loss on a variety of datasets. They also show that their method can be applied to unsupervised learning of 3D moving object detectors. "
21843,SP:6fd61604a2eeb8a2cbbda6c40807cebef6d40f2f,"model USED-FOR applications. Optimal Transport ( OT ) framework USED-FOR UDT. low energy transformations FEATURE-OF mappings. methods USED-FOR mappings. theoretical guarantees EVALUATE-FOR methods. approach USED-FOR UDT. dynamic formulation of OT CONJUNCTION CycleGAN. CycleGAN CONJUNCTION dynamic formulation of OT. mapping USED-FOR domain translation. translation USED-FOR problems. image captioning CONJUNCTION natural language translation. natural language translation CONJUNCTION image captioning. neural network USED-FOR mapping. hidden layers PART-OF neural network. photographs CONJUNCTION paintings. paintings CONJUNCTION photographs. model USED-FOR task. dynamical formulation USED-FOR model. Optimal Transport theory USED-FOR UDT models. model COMPARE CycleGAN - like models. CycleGAN - like models COMPARE model. Task are Unsupervised Domain Translation ( UDT ), UDT problems, and UDT problem. OtherScientificTerm are implicit biases, implicit bias, map, unwanted pairings, objective function, and well - behaved mappings. Generic are approaches, and models. Method are CycleGAN model, and networks of minimal complexity. Metric is complexity. ","This paper studies the problem of unsupervised domain translation (UDT), where the goal is to find a mapping from one domain to another. The authors propose to use the Optimal Transport (OT) framework to solve the problem. The proposed method is based on a dynamic formulation of OT and CycleGAN. Theoretical results are provided to show that the proposed method can find the mapping with low energy transformations. Experiments are conducted on image captioning and painting tasks.  ","This paper studies the problem of unsupervised domain translation (UDT). The authors propose a dynamic formulation of the Optimal Transport (OT) framework for UDT, which is based on the notion of low energy transformations. The authors show that the proposed method can be applied to a variety of UDT problems, including image captioning, natural language translation, and image painting. They also provide theoretical guarantees for their method. "
21852,SP:8bb3ce11ad773685f6e41d90db3e7a5481e5ba47,regularization method USED-FOR neural networks. RotationOut USED-FOR neural networks. RotationOut HYPONYM-OF regularization method. Dropout USED-FOR neuron / channel. Dropout COMPARE RotationOut. RotationOut COMPARE Dropout. convolutional layers CONJUNCTION recurrent layers. recurrent layers CONJUNCTION convolutional layers. RotationOut USED-FOR recurrent layers. RotationOut USED-FOR convolutional layers. RotationOut USED-FOR co - adaptation reduction. Dropout USED-FOR co - adaptation reduction. RotationOut CONJUNCTION Dropout. Dropout CONJUNCTION RotationOut. noise analysis method USED-FOR co - adaptation reduction. RotationOut / Dropout CONJUNCTION Batch Normalization. Batch Normalization CONJUNCTION RotationOut / Dropout. vision and language tasks EVALUATE-FOR method. RotationOut CONJUNCTION RotationOut. RotationOut CONJUNCTION RotationOut. Method is regularization. ,This paper proposes a new regularization method called RotationOut to improve the performance of convolutional layers and recurrent layers in deep neural networks. The proposed method is based on the observation that regularization methods such as Dropout and Batch Normalization can be used to reduce the number of neurons and channels in a network. The authors show that the proposed method can achieve better performance than Dropout on vision and language tasks. ,This paper proposes a new regularization method called RotationOut for neural networks. The main idea of rotation out is to use a dropout-based regularization to reduce the number of neurons and channels in a neural network. The authors show that rotation out can be combined with dropout to improve the performance of the network. They also propose a new noise analysis method for co-adaptation reduction. The experiments on vision and language tasks demonstrate the effectiveness of the method.
21861,SP:37620ae8dc5683eb2843792e0aa4cbe6cba366f7,"method USED-FOR Universal Adversarial Perturbations ( UAP ). Universal Adversarial Perturbations ( UAP ) USED-FOR CNN. sequential optimization USED-FOR adversarial perturbation. dilate loss USED-FOR sequential optimization. dilate loss USED-FOR adversarial perturbation. Euclidean norm EVALUATE-FOR Dilate loss. method COMPARE data - free work. data - free work COMPARE method. fooling rate EVALUATE-FOR data - free work. fooling rate EVALUATE-FOR method. Method is Data - free approaches. Task are crafting adversaries, adversary generation, and crafting UAPs. OtherScientificTerm are nonlinearity, perturbation, ReLU activation function, and limited data cases. ",This paper proposes a novel method for generating adversarial perturbations for CNNs. The proposed method is based on the observation that adversarial examples can be generated from a small number of data points. The authors propose to use a dilate loss to generate adversarial samples from the input data. The method is evaluated on CNNs trained on ImageNet and CIFAR-10.  ,This paper proposes a novel method for generating adversarial perturbations (UAPs) for CNNs. The main idea is to use a dilate loss to generate UAPs that are non-linear in the Euclidean norm of the input data. The authors show that the proposed method can achieve fooling rates of up to 1/3 of the fooling rate of data-free work. 
21870,SP:2fd7d5507a8727db743dc89379a6f021d31ed39a,"Neural Architecture Search ( NAS ) COMPARE hand - designed networks. hand - designed networks COMPARE Neural Architecture Search ( NAS ). Neural Architecture Search ( NAS ) USED-FOR artificial intelligence areas. architecture USED-FOR task. NAS CONJUNCTION fast adaptation of neural architectures. fast adaptation of neural architectures CONJUNCTION NAS. T - NAS HYPONYM-OF Transferable Neural Architecture Search method. meta - learning USED-FOR Transferable Neural Architecture Search method. architecture USED-FOR task. T - NAS USED-FOR meta - architecture. few - shot learning CONJUNCTION supervised learning. supervised learning CONJUNCTION few - shot learning. supervised learning EVALUATE-FOR T - NAS. few - shot learning EVALUATE-FOR T - NAS. Method are NAS methods, and neural architectures. Metric is searching cost. Generic is method. ","This paper proposes a transferable neural architecture search method called T-NAS, which uses meta-learning to learn a meta-architecture for a given task. The proposed method is based on the idea of meta-adaptation, i.e. learning a meta architecture for each task, which is then used to train a new architecture for the new task. This meta architecture is trained using a combination of meta learning and gradient descent. The method is evaluated on few-shot learning and supervised learning tasks. ","This paper proposes a Transferable Neural Architecture Search (T-NAS) method for transferable neural architecture search. T-NAS is based on meta-learning to learn a meta-architecture for each task, which is then used to find the best architecture for the task at hand. The method is evaluated on few-shot learning, supervised learning, and supervised learning. "
21879,SP:1314a79ba12474adb33ff31b3cb22bed25b94fb7,"variational information bottleneck ( VIB ) CONJUNCTION noise regularized learning. noise regularized learning CONJUNCTION variational information bottleneck ( VIB ). dropout CONJUNCTION Bayesian neural networks. Bayesian neural networks CONJUNCTION dropout. Bayesian neural networks CONJUNCTION variational information bottleneck ( VIB ). variational information bottleneck ( VIB ) CONJUNCTION Bayesian neural networks. noise regularized learning HYPONYM-OF paradigms. dropout HYPONYM-OF paradigms. Bayesian neural networks HYPONYM-OF paradigms. variational information bottleneck ( VIB ) HYPONYM-OF paradigms. network compression CONJUNCTION robustness. robustness CONJUNCTION network compression. generalization CONJUNCTION network compression. network compression CONJUNCTION generalization. adversarial attack CONJUNCTION label noise. label noise CONJUNCTION adversarial attack. robustness FEATURE-OF adversarial attack. activation uncertainty CONJUNCTION activation variability. activation variability CONJUNCTION activation uncertainty. pruning CONJUNCTION adversarial defense. adversarial defense CONJUNCTION pruning. SNNs COMPARE SE - SNN. SE - SNN COMPARE SNNs. adversarial defense CONJUNCTION learning with label noise. learning with label noise CONJUNCTION adversarial defense. network compression EVALUATE-FOR SE - SNN. adversarial defense USED-FOR network compression. pruning USED-FOR network compression. Method are Stochastic neural networks ( SNNs ), and neural network variants. Generic is networks. Task is discriminative learning. ",This paper proposes a new variant of stochastic neural networks (SNNs) based on variational information bottleneck (VIB) and noise regularized learning (NLL) to improve the robustness and compression properties of SNNs. The authors show that VIB and NLL can be combined with dropout and Bayesian neural networks to improve robustness to adversarial attacks and label noise. They also show that pruning and adversarial defense can help improve the compression properties. ,This paper proposes a new method to improve the robustness of stochastic neural networks (SNNs) against adversarial attacks and label noise. The method is based on the variational information bottleneck (VIB) and noise regularized learning (SE-SNN) paradigms. The main contribution of the paper is a theoretical analysis of the effect of the VIB on the generalization of SNNs. The authors show that SNN can be more robust to label noise and adversarial attack than SNN with dropout and Bayesian neural networks. They also show that pruning can help improve the performance of the SNN.
21888,SP:bd4935d4fcf33f60f22e0f2fd9f7dc8ddfab6d17,"inner loop USED-FOR reinforcement learning. curiosity mechanisms USED-FOR agent ’s reward signal. meta - learning USED-FOR generating curious behavior. reward signal USED-FOR reinforcement learning. reward signal USED-FOR inner loop. transferring neural network weights USED-FOR meta - RL methods. nearest - neighbor modules CONJUNCTION custom loss functions. custom loss functions CONJUNCTION nearest - neighbor modules. buffers CONJUNCTION nearest - neighbor modules. nearest - neighbor modules CONJUNCTION buffers. neural networks PART-OF rich language of programs. custom loss functions PART-OF rich language of programs. image inputs CONJUNCTION acrobot. acrobot CONJUNCTION image inputs. curiosity algorithms COMPARE human - designed published curiosity algorithms. human - designed published curiosity algorithms COMPARE curiosity algorithms. acrobot CONJUNCTION ant. ant CONJUNCTION acrobot. grid navigation CONJUNCTION acrobot. acrobot CONJUNCTION grid navigation. image inputs USED-FOR grid navigation. OtherScientificTerm are curiosity, and outer loop. Method are evolution, and meta - learn algorithms. Material is ML papers. Generic is approach. ","This paper proposes a meta-learning approach for meta-reinforcement learning in which the inner loop learns a curiosity mechanism to generate curious behavior in the outer loop. The novelty of the approach is that it does not need to transfer the weights of the inner-loop to the outer-loop in order to learn the curiosity mechanism. The motivation for this is that the reward signal in the innerloop is generated by a reward signal generated by the curiosity mechanisms. The paper proposes to use a combination of two meta-RL methods: (1) meta-regret, which is a meta RL method that learns to transfer neural network weights from the inner to outer loops, and (2) nearest-neighbor modules, which are a set of buffers and loss functions that are designed to capture the rich language of programs. The experiments show that the proposed approach outperforms the baselines on a variety of tasks. ","This paper proposes a meta-learning approach for meta-regressive reinforcement learning, where the inner loop is motivated by curiosity, and the outer loop is driven by the agent's reward signal. The inner loop consists of two parts: (1) the reward signal, and (2) a reward function, which is learned by transferring neural network weights between the inner and outer loops. The reward function is learned in a way that maximizes the mutual information between the reward function and the agent’s inner loop. The outer loop, in contrast, is learned using a neural network that is trained on a rich language of programs. The authors show that their approach outperforms the state-of-the-art meta-RL methods on a variety of tasks. "
21897,SP:6dff0f3a84809ae0ba9f58f36303597f1ba6dcc5,"approach USED-FOR AnyC2C. approach USED-FOR code snippet. strict syntax of programming languages USED-FOR code snippet. tree – structural language modeling ( SLM ) USED-FOR code snippet. strict syntax of programming languages USED-FOR approach. neural model USED-FOR conditional probabilities. AST paths USED-FOR neural model. structural techniques COMPARE approach. approach COMPARE structural techniques. structural techniques USED-FOR expressions. structured approaches USED-FOR Java and C # code. model COMPARE seq2seq. seq2seq COMPARE model. model COMPARE structured approaches. structured approaches COMPARE model. model USED-FOR Java and C # code. seq2seq CONJUNCTION structured approaches. structured approaches CONJUNCTION seq2seq. Generic are problem, it, and task. OtherScientificTerm are structural information, and programming language. Method is SLM. ","This paper proposes a tree-structured language modeling (SLM) approach for AnyC2C, where the goal is to predict the conditional probabilities of a sequence of expressions in a programming language. The main idea is to use a neural model to model the AST paths of the code, which are then used to train a neural network that predicts the conditional probability of each expression. The proposed approach is evaluated on both Java and C code.   ","This paper proposes a tree-structured language modeling (SLM) approach for AnyC2C code generation. The main idea is to use a neural model to model the conditional probabilities of the AST paths of a code snippet. The model is trained using a neural network to predict the AST path of a given code snippet, which is then used to predict conditional probabilities for the next AST path. The proposed method is evaluated on both Java and C code generation tasks. "
21906,SP:7fc60d6fd1cfcc135c34f9664d172d3fd1c0ae0a,"gradient descent methods USED-FOR non - convex optimization problems. objective functions USED-FOR NNs. NN model space CONJUNCTION canonical space. canonical space CONJUNCTION NN model space. disparity matrix HYPONYM-OF pointwise linear transformation. pointwise linear transformation USED-FOR gradients. gradient descent methods USED-FOR global minimum of zero loss. full rank FEATURE-OF disparity matrices. learning of NNs COMPARE normal convex optimization. normal convex optimization COMPARE learning of NNs. gradient decent algorithms USED-FOR global minimum of zero loss. Method are large - scale neural networks ( NN ), large NNs, and over - parameterized NNs. OtherScientificTerm are canonical model space, full - rank condition, and singular disparity matrices. ","This paper studies gradient descent methods for non-convex optimization problems in large-scale neural networks (NNs) with over-parameterized NNs. The authors show that the gradient descent method can converge to a global minimum of zero loss in the canonical model space and the NN model space with full-rank condition, and singular disparity matrices. They also show that gradient decent algorithms can be used to converge to the global minimum.  ","This paper studies the problem of learning large-scale neural networks (NNs) with over-parameterized NNs. The authors study the problem in the setting of non-convex optimization, where the objective function is a pointwise linear transformation of the NN model space and the canonical model space. The main contribution of the paper is to study the full-rank condition of the disparity matrix, which is a special case of the singular disparity matrix. They show that under this condition, the global minimum of zero loss can be obtained for any gradient descent method. They also provide an algorithm that achieves this global minimum. "
21915,SP:78a536138570fe9b5d88350e4b16d598a7db1fe0,"Large - scale ground truth data sets USED-FOR deep learning based segmentation models. interactive graph - based segmentation algorithms USED-FOR connectivity. instanceaware heuristic USED-FOR discrete Potts model. feature maps USED-FOR DCNN. feature maps USED-FOR algorithms. RGB USED-FOR algorithms. PASCAL VOC 2012 CONJUNCTION Cityscapes dataset. Cityscapes dataset CONJUNCTION PASCAL VOC 2012. Cityscapes dataset EVALUATE-FOR semantic ( and panoptic ) segmentation. PASCAL VOC 2012 EVALUATE-FOR semantic ( and panoptic ) segmentation. VOC validation set EVALUATE-FOR mIoU. mIoU EVALUATE-FOR interactive approach. VOC validation set EVALUATE-FOR interactive approach. They USED-FOR interactive annotation. They USED-FOR weakly supervised learning framework. OtherScientificTerm are global optimum, and scribbles. ",This paper proposes a novel interactive graph-based segmentation algorithm for large-scale ground truth data sets. The proposed method is based on a discrete Potts model and uses instance-aware heuristics to learn a discrete feature map for each instance using a DCNN. Experiments on PASCAL VOC 2012 and Cityscapes dataset show that the proposed method achieves state-of-the-art results. ,"This paper proposes a novel interactive graph-based segmentation algorithm for large-scale ground truth data sets. The proposed method is based on a discrete Potts model, where the graph is represented as a discrete feature map, and the feature maps are represented by a DCNN. The authors propose a novel instance-aware heuristic, which they call instanceaware heuristics. They also propose a weakly supervised learning framework, which can be applied to both semantic (and panoptic) segmentation and annotation tasks."
21924,SP:2eb90879ddbc39b6b5c05152784d6044d1940513,"salient features FEATURE-OF image. saliency tools USED-FOR adversarial examples. salient features USED-FOR defense. it COMPARE baseline. baseline COMPARE it. gradient - based saliency tools USED-FOR adversarial defense. model USED-FOR baseline. saliency map USED-FOR baseline. learnt saliency models USED-FOR saliency. computational cost EVALUATE-FOR learnt saliency models. saliency models USED-FOR real - time defense. learnt saliency model USED-FOR defense. adversarial images CONJUNCTION natural images. natural images CONJUNCTION adversarial images. CNN USED-FOR adversarial images. CNN USED-FOR natural images. CNN HYPONYM-OF defense. salient pixels USED-FOR CNN. CIFAR-10 CONJUNCTION ASSIRA. ASSIRA CONJUNCTION CIFAR-10. defense USED-FOR adversarial attacks. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. MNIST EVALUATE-FOR defense. ASSIRA EVALUATE-FOR defense. CIFAR-10 EVALUATE-FOR defense. saliency map USED-FOR defense. C&W CONJUNCTION DeepFool. DeepFool CONJUNCTION C&W. weak defenses USED-FOR adversarial images. attacks USED-FOR adversarial images. DeepFool USED-FOR adversarial images. DeepFool HYPONYM-OF attacks. C&W HYPONYM-OF attacks. OtherScientificTerm are Adversarial perturbations, misclassification, and adversarial perturbations. ","This paper proposes a method to improve the robustness of image classifiers against adversarial attacks. The proposed method is based on the observation that adversarial perturbations can lead to misclassification. The authors propose to use a learned saliency model to estimate the saliency map of an image, which is then used to train an adversarial defense. The method is evaluated on MNIST, CIFAR-10, and ASSIRA. ","This paper proposes a new adversarial defense method for real-time defense against adversarial attacks. The main idea is to use a learned saliency model to map the salient features of the image to the adversarial examples. The proposed method is evaluated on MNIST, CIFAR-10, and ASSIRA datasets. It is shown that the proposed method outperforms the baseline defense. "
21933,SP:fe5510d05ff091a5f133f2dbcd1b23d8d58d2c3e,"global adversarial robustness guarantees FEATURE-OF machine learning models. measurability FEATURE-OF local robustness properties. concentration inequalities USED-FOR global robustness. Fashion - MNIST CONJUNCTION CIFAR. CIFAR CONJUNCTION Fashion - MNIST. MNIST CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION MNIST. neural networks architectures CONJUNCTION training methods. training methods CONJUNCTION neural networks architectures. robustness / accuracy trade - off EVALUATE-FOR neural networks architectures. training methods USED-FOR MNIST. training methods USED-FOR Fashion - MNIST. robustness EVALUATE-FOR networks. accuracy EVALUATE-FOR networks. robustness CONJUNCTION accuracy. accuracy CONJUNCTION robustness. stochastic gradient descent CONJUNCTION iterative pruning techniques. iterative pruning techniques CONJUNCTION stochastic gradient descent. Bayesian settings FEATURE-OF them. iterative pruning techniques USED-FOR networks. stochastic gradient descent USED-FOR networks. Generic are model, and methods. OtherScientificTerm are adversarial attacks, and estimation error. ","This paper studies the problem of adversarial robustness in machine learning models. The authors show that the robustness of a model to adversarial attacks depends on its local robustness, i.e., it depends on the concentration inequalities of the adversarial perturbations in the input space. They show that if the concentration inequality holds, then the global robustness can be expressed as the sum of the local and global perturbation losses.   The authors then propose to use this concentration inequality to derive the global and global adversarial guarantees of a neural network. They then show that under certain assumptions on the input distribution, the global adversarially robustness is bounded by the global loss and the local loss.  Finally, the authors propose a method to prune the weights of the model in order to improve the global accuracy. ",This paper studies the trade-off between robustness and accuracy of neural networks in the context of global adversarial robustness. The main contribution of the paper is a theoretical analysis of the global robustness properties of neural network architectures and training methods. The authors show that the robustness of a neural network depends on the concentration inequalities of the training data. They also provide an empirical evaluation of the tradeoff between the robust and accuracy trade-offs.
21942,SP:8f5616a1480b68c04b496ed498d237d5a7e87794,"robustness FEATURE-OF environmental dynamics. learning algorithms USED-FOR robustness. robustness FEATURE-OF system dynamics. transition probability HYPONYM-OF system dynamics. Wasserstein distance USED-FOR disturbance. infinite - dimensional optimization problem CONJUNCTION finite - dimensional risk - aware problem. finite - dimensional risk - aware problem CONJUNCTION infinite - dimensional optimization problem. risk - aware optimal Bellman equation USED-FOR optimal robust policies. sensitivity analysis USED-FOR perturbations. Wasserstein Robust HYPONYM-OF robust learning algorithm. Cart - Pole environment EVALUATE-FOR algorithm. OtherScientificTerm are optimal policy, simulated environmental parameters, reference transition kernel, transition kernel disturbance, and state disturbance. ","This paper studies the problem of robustness to perturbations in the system dynamics. In particular, the authors propose a new robust learning algorithm based on the Wasserstein distance between the reference transition kernel and the state disturbance. The authors show that the optimal robust policy can be learned by minimizing the WASSERSTIN distance between state disturbance and the reference kernel. The proposed method is shown to be robust in the Cart-Pole environment.",This paper studies the problem of learning robust policies that are robust to perturbations in the environment. The authors propose a new robust learning algorithm based on the Wasserstein distance between the state disturbance and the transition probability of the system dynamics. They show that the optimal robust policy can be learned in a finite-dimensional optimization problem with a risk-aware optimal Bellman equation. They also provide an empirical evaluation of the proposed algorithm in the Cart-Pole environment.
21951,SP:d85963f5f0f6b20cf08f2a7c169ae33a45db7de2,"Nash equilibrium PART-OF multi - player games. deep learning based approaches USED-FOR pure strategy Nash equilibrium. method USED-FOR mixed strategy Nash equilibria. multi - player continuous games FEATURE-OF mixed strategy Nash equilibria. pure ones PART-OF method. pushforward measure technique USED-FOR mixed strategy. continuous spaces FEATURE-OF mixed strategy. joint strategy profile CONJUNCTION Nash equilibrium. Nash equilibrium CONJUNCTION joint strategy profile. gradient descent algorithm USED-FOR approach. convexity assumption FEATURE-OF payoff functions. approach USED-FOR stationary Nash equilibrium. blotto games CONJUNCTION GAMUT games. GAMUT games CONJUNCTION blotto games. method COMPARE works. works COMPARE method. quadratic games CONJUNCTION blotto games. blotto games CONJUNCTION quadratic games. works USED-FOR Nash equilibrium. method USED-FOR Nash equilibrium. approximating Nash equilibrium USED-FOR quadratic games. approximating Nash equilibrium CONJUNCTION blotto games. blotto games CONJUNCTION approximating Nash equilibrium. GAMUT games EVALUATE-FOR method. OtherScientificTerm are continuous strategy spaces, and pure strategy weakness. Task is generative adversarial networks. Generic is equilibrium. ","This paper studies the problem of finding Nash equilibria in multi-player continuous games with mixed strategy. The authors propose a method for finding mixed strategy Nash equilibrium in multi player continuous games. The main idea is to use a pushforward measure technique to estimate the mixed strategy in a continuous space and then use a gradient descent algorithm to find the Nash equilibrium. The method is shown to work well in quadratic games, blotto games and GAMUT games.","This paper proposes a method for solving mixed strategy Nash equilibria in multi-player continuous games. The main idea is to use a pushforward measure technique to measure the mixed strategy in a continuous space, and then use a gradient descent algorithm to solve the Nash equilibrium. The authors show that their method can achieve a stationary Nash equilibrium with a convexity assumption on the payoff function. They also show that the method can be applied to quadratic games and blotto games. "
21960,SP:280d85cd8164a268f9d496ae5f17189c50f30dc1,"deep neural networks USED-FOR NLP tasks. labeled data USED-FOR data - hungry models. sufficient domain knowledge USED-FOR labeled data. supervision USED-FOR sufficient domain knowledge. supervision FEATURE-OF Natural language ( NL ) explanations. them USED-FOR augmenting model learning. modularized model USED-FOR semantics. linguistic variants FEATURE-OF NL explanations. Neural Execution Tree ( NExT ) framework1 USED-FOR text classification. NL explanations USED-FOR Neural Execution Tree ( NExT ) framework1. NExT USED-FOR actions. NL explanations USED-FOR executable logical forms. logical forms USED-FOR actions. semantic parsing USED-FOR NL explanations. semantic parsing USED-FOR executable logical forms. relation extraction CONJUNCTION sentiment analysis. sentiment analysis CONJUNCTION relation extraction. NLP tasks EVALUATE-FOR baseline methods. sentiment analysis HYPONYM-OF NLP tasks. relation extraction HYPONYM-OF NLP tasks. Task are data annotation, and multi - hop question answering. Metric is annotation time. Method is model learning. OtherScientificTerm is NL explanation. ","This paper proposes to use natural language explanations (NL explanations) to augment model learning in NLP tasks. In particular, the authors propose to use NL explanations for multi-hop question answering and relation extraction tasks. The proposed method is based on the Neural Execution Tree (NExT) framework for text classification, which is a modularized model that learns to model the semantics from NL explanations. The authors show that NL explanations can be used to improve the performance of the model on relation extraction and sentiment analysis tasks.  ","This paper proposes a method for augmenting model learning with natural language explanations (NL explanations) for NLP tasks. The method is based on the Neural Execution Tree (NExT) framework1, which is a modularized model for NL explanations. NExT is built on top of the NNML framework, and it is used to augment the model learning process with NL explanations to improve the performance of model learning. The proposed method is evaluated on a variety of tasks, including relation extraction, sentiment analysis, and multi-hop question answering."
21969,SP:a9b5f7257dedd719cfe341fca275776734af1d98,"robustness FEATURE-OF misclassification. machine learning models USED-FOR Formal verification. robustness HYPONYM-OF properties. it USED-FOR complex specifications. it USED-FOR recurrent neural network architectures. recurrent neural network architectures CONJUNCTION complex specifications. complex specifications CONJUNCTION recurrent neural network architectures. specifications USED-FOR temporal properties. adversarial robustness FEATURE-OF complex specifications. it USED-FOR verified training. specifications HYPONYM-OF complex specifications. verified training method USED-FOR models. verified training method USED-FOR models. training USED-FOR models. OtherScientificTerm are perturbations of the input features, and desired specifications. Method are verification procedure, verifiably robust models, and language model. ","This paper studies the problem of verifying the robustness of machine learning models in the presence of adversarial perturbations of input features. The authors propose a verification procedure based on the verification procedure of a language model, where a set of desired specifications are given and the model is trained to produce the desired specifications. The paper shows that the proposed verification procedure can be used to train robust models in an adversarial robust manner. The proposed verification method is shown to be robust to adversarial attacks.","This paper studies the problem of verifying the robustness of machine learning models against misclassification in the presence of perturbations of the input features. The main contribution of the paper is to propose a verification procedure that can be applied to a variety of complex specifications (e.g., a language model, a recurrent neural network, and a complex neural network architecture). The authors show that this verification procedure can be used to train robust models, and that it can be combined with a verified training method to improve the performance of the model."
21978,SP:3903680e07b676409e3cf6a1044b67291fe38630,"learned state representations USED-FOR constant. technique COMPARE domain randomization. domain randomization COMPARE technique. generalization scores EVALUATE-FOR domain randomization. generalization scores EVALUATE-FOR technique. Task are reinforcement learning, and visual domain randomization problem. Generic is method. Method are visual domain randomization, and regularization method. OtherScientificTerm are policies, and randomization parameters. ","This paper studies the problem of visual domain randomization in reinforcement learning. In this setting, the goal is to ensure that the learned state representations are close to the true state representation of the agent. The authors propose a regularization method that uses the learned representations as a regularizer for the randomization parameters. The proposed method is shown to improve the generalization performance of the learned policies.   ","This paper studies the problem of visual domain randomization in reinforcement learning, where the goal is to improve the generalization performance of the learned state representations. The authors propose a new regularization method for the problem, which is based on the idea of learning a constant that is the sum of the state representations learned by the learned policies. They show that this constant can be used as a regularization term for the learned representations. They also show that their method can achieve better generalization scores than the state-of-the-art. "
21987,SP:c79046dc56b9ee9c926f87386046422ea134ae8d,"deep learning USED-FOR Deep metric learning ( DML ). complicated losses CONJUNCTION hard example mining methods. hard example mining methods CONJUNCTION complicated losses. pairwise binary classification problem USED-FOR DML. framework USED-FOR model. distributionally robust optimization USED-FOR robust loss. uncertainty decision set FEATURE-OF dual variable. uncertainty decision set USED-FOR complicated losses. benchmark data sets EVALUATE-FOR method. method COMPARE state. state COMPARE method. Task is computer vision. Generic are problem, and variants. OtherScientificTerm is imbalanced data pairs. ","This paper proposes a method for deep metric learning (DML) in the presence of imbalanced data pairs. The proposed method is based on a pairwise binary classification problem, where each data pair is represented by a pair of binary labels. The authors propose a distributionally robust optimization (DRO) framework to solve the problem. The main idea is to use the uncertainty decision set (UDS) to model the uncertainty of the dual variable, which is then used to optimize the loss function. Experiments show that the proposed method achieves state-of-the-art performance on several benchmark data sets.",This paper proposes a new framework for deep metric learning (DML) based on the pairwise binary classification problem. The proposed framework is based on a distributionally robust optimization (DRO) framework. The main idea is to use the uncertainty decision set of the dual variable to learn a robust loss for complicated losses. The authors show that the proposed method outperforms the state-of-the-art hard example mining methods on two benchmark datasets.
21996,SP:38420928e40ef80c0136ad607b9275f9ab1e0769,"local minimum PART-OF non - convex finite - sum minimization. inexact gradient and Hessian estimation USED-FOR trust region method. stochastic trust region ( STR ) algorithm USED-FOR (, √ ) -approximate local minimum. runtime complexity EVALUATE-FOR Hessian - free STR algorithms. Metric is convergence rate. Method are differential estimations, and Hessian estimator. OtherScientificTerm is stochastic Hessian oracle queries. Generic is algorithms. ","This paper studies the trust region method for non-convex finite-sum minimization in the presence of Hessian uncertainty. In particular, the authors consider the case where the gradient and Hessian are inexact and the Hessian estimator is unknown. The authors propose a trust region (STR) algorithm that achieves an approximate local minimum of $O(1/\sqrt{K}^2)$ with $O(\sqrt{\frac{K}{\epsilon})$ time complexity. They show that this algorithm is faster than the existing Hessian-free STR algorithms.  ","This paper studies the problem of estimating the local minimum of a non-convex finite-sum minimization problem with a stochastic trust region (STR) algorithm. In particular, the authors consider the case where the Hessian estimator of the trust region is inexact, and they show that the optimal estimator is a Hessian-free algorithm. They also provide a convergence analysis of the algorithm.   "
22005,SP:28a35b70b5e6915af28cacebc4ea50690c9534af,"batch normalization CONJUNCTION weight initialization. weight initialization CONJUNCTION batch normalization. linear programming USED-FOR Farkas layers. benchmark datasets EVALUATE-FOR network sizes. ReLU activation USED-FOR residual networks. Method are deep neural networks, and geometrically motivated method. Generic is method. Metric is training capacity. OtherScientificTerm is initialization. ",This paper proposes a method to reduce the number of layers in deep neural networks. The proposed method is based on batch normalization and weight initialization. Theoretical results show that the proposed method can reduce the size of the network by a factor of at least 1.5. Experimental results show the effectiveness of the method.  ,"This paper proposes a geometrically motivated method to reduce the training capacity of deep neural networks (DNNs) in terms of the number of training epochs. The main idea is to use a linear programming-based linear programming method for the Farkas layers of a DNN. The method is motivated by the fact that the training of DNNs can be very expensive due to batch normalization, weight initialization, and linear programming. The authors show that the size of the training epoch can be reduced by a factor of 1/\sqrt{1/2} by using the geometrical method. They also show that their method can be applied to the ReLU activation of residual networks. "
22014,SP:1d325b148e3efe407241c1f1cbe8d17400499741,"decision boundary PART-OF classifier. adversarial examples FEATURE-OF robustness certificate. minimum distance FEATURE-OF robustness certificate. robustness certificates USED-FOR deep classifiers. nonconvex optimization USED-FOR it. computationally - efficient robustness certificates USED-FOR deep classifiers. differentiable activation functions USED-FOR computationally - efficient robustness certificates. eigenvalues FEATURE-OF Hessian. Hessian FEATURE-OF network. l2 norm FEATURE-OF robustness certificate. convex optimization USED-FOR robustness certificate. curvature FEATURE-OF deep network. computationallyefficient differentiable upper bound USED-FOR deep network. curvature FEATURE-OF computationallyefficient differentiable upper bound. curvature bound USED-FOR regularization term. regularization term USED-FOR network. curvature bound USED-FOR network. adversarial examples FEATURE-OF certified robustness. Curvature - based Robustness Certificate ( CRC ) CONJUNCTION Curvature - based Robust Training ( CRT ). Curvature - based Robust Training ( CRT ) CONJUNCTION Curvature - based Robustness Certificate ( CRC ). CRT COMPARE adversarial training. adversarial training COMPARE CRT. CRC COMPARE CROWN ’s certificate. CROWN ’s certificate COMPARE CRC. certified accuracy EVALUATE-FOR adversarial training. CRC COMPARE CRT. CRT COMPARE CRC. CRC COMPARE adversarial training. adversarial training COMPARE CRC. regularizer USED-FOR CRC. regularizer USED-FOR CROWN ’s certificate. certified accuracy EVALUATE-FOR CRC. certified accuracy EVALUATE-FOR CRT. OtherScientificTerm are lower bound, and classification output. ",This paper proposes a new method for computing robustness certificates for deep neural networks. The main idea is to use the curvature of the decision boundary of the network as a regularization term to improve the robustness certificate. The method is based on the observation that the eigenvalues of the Hessian of the neural network can be approximated by a convex function. The authors show that this convex optimization is computationally efficient and can be used to compute differentiable activation functions for differentiable networks.   The main contributions of the paper are:  1. A new method to compute a robustness certification for deep networks based on curvature-based optimization.  2. A method to improve robustness of deep networks to adversarial examples.,"This paper proposes a new robustness certificate based on the curvature of the Hessian of the decision boundary of a classifier. The authors prove a differentiable differentiable upper bound for the l2 norm of the classifier’s eigenvalues. The upper bound is based on a nonconvex optimization, which is computationally efficient and computationally-efficient differentiable.  The authors also propose a new regularization term to improve the robustness of the network.   "
22023,SP:33f6f5aa0d4655e5d75fe612e0eff05e579d45c5,"method USED-FOR compressed sensing recovery. untrained deep generative models USED-FOR method. convolutional weights PART-OF network. Deep Image Prior ( DIP ) USED-FOR method. approach USED-FOR differentiable linear inverse problem. approaches COMPARE method. method COMPARE approaches. generative models USED-FOR approaches. prior information FEATURE-OF network weights. prior information PART-OF learned regularization technique. DIP optimization approach USED-FOR overparameterized single - layer networks. Method are unlearned methods, and early stopping. OtherScientificTerm are pre - training, and noisy measurements. Metric is reconstruction error. Task is fitting problem. ","This paper proposes a method for compressed sensing recovery with deep generative models. The proposed method is based on the Deep Image Prior (DIP) method, which is a differentiable linear inverse problem. The main contribution of this paper is to use DIP to learn the weights of the convolutional weights in the network. The authors show that the proposed method can be used to improve the performance of compressed sensing methods.  ","This paper proposes a method for compressed sensing recovery for untrained deep generative models. The proposed method is based on the Deep Image Prior (DIP) method, which is a learned regularization technique that learns the prior information of the weights of the network weights. The authors show that their method is differentiable and differentiable for differentiable linear inverse problem. They also show that the proposed method can be applied to overparameterized single-layer networks. "
22032,SP:23c0f621e6041003b59bf0532130760694cf6a4a,"reinforcement learning ( RL ) USED-FOR real - world problems. long time horizons FEATURE-OF action - reward correlation. hand - tuned network structure CONJUNCTION pre - defined subgoals. pre - defined subgoals CONJUNCTION hand - tuned network structure. hand - tuned network structure PART-OF hierarchies. pre - defined subgoals PART-OF hierarchies. HRL framework USED-FOR temporal abstraction. TAIC USED-FOR temporal abstraction. approach USED-FOR latent space. information - theoretic constraints USED-FOR approach. latent representations of action sequences USED-FOR temporal abstraction problem. latent variables CONJUNCTION state changes. state changes CONJUNCTION latent variables. algorithm USED-FOR abstraction of the long action sequences. abstraction USED-FOR tasks. convergence rate CONJUNCTION sample efficiency. sample efficiency CONJUNCTION convergence rate. sample efficiency EVALUATE-FOR RL algorithms. convergence rate EVALUATE-FOR RL algorithms. convergence rate EVALUATE-FOR technique. sample efficiency EVALUATE-FOR technique. Method are Hierarchical reinforcement learning ( HRL ) methods, and temporal abstractions. OtherScientificTerm are task - specific knowledge, and visualization of the latent space. Metric is mutual information. Material is benchmark learning problems. ","This paper studies the problem of temporal abstraction in Hierarchical reinforcement learning (HRL), where the goal is to learn a hierarchy of subgoals that can be used for long time horizons. The authors propose a new method called TAIC to learn temporal abstractions in HRL. TAIC is based on an information-theoretic approach to learn the latent representations of action sequences in a latent space, which is then used to compute the mutual information between latent variables and state changes. Theoretical analysis is provided to show that TAIC converges to a state-of-the-art algorithm with a convergence rate of $O(1/\sqrt{T})$ and sample efficiency of $\Omega(T)$. Empirical results are provided to demonstrate the effectiveness of TAIC. ","This paper proposes a hierarchical reinforcement learning (HRL) framework for temporal abstraction of long time-horizon action-reward correlation. The main idea is to use a latent representation of action sequences as a latent space, which can be used to learn a task-specific knowledge of the long action sequences. The authors show that this latent space can be easily visualized by using the information-theoretic constraints on the latent space. They also show that their method can be applied to a variety of tasks, and show that it can achieve better sample efficiency and convergence rates than existing methods."
22041,SP:4e54c9196ba1eb2b6a0b0eee41e4a6f3a9de72dd,"Graph Convolutional Network ( GCN ) USED-FOR graph representation learning. small graphs USED-FOR shallow models. acceleration methods USED-FOR GCNs. larger graphs CONJUNCTION deeper layers. deeper layers CONJUNCTION larger graphs. GCN - like models USED-FOR deeper layers. GCN - like models USED-FOR larger graphs. local bi - directional influence ( correlation ) FEATURE-OF mini - batch of nodes. recursive propagation CONJUNCTION skip connection. skip connection CONJUNCTION recursive propagation. first - order and higher - order proximities FEATURE-OF single layer propagation process. first - order and higher - order proximities PART-OF model. large benchmark graphs EVALUATE-FOR model. Task is graph - based applications. Method are layer - wise sampling strategy, and self - attention mechanism. Metric is time complexity. OtherScientificTerm is sampled nodes. ",This paper proposes a new sampling strategy for graph convolutional networks (GCNs). The proposed sampling strategy is based on local bi-directional influence (i.e. local correlation) of mini-batches of nodes in the mini-batch of nodes. The authors show that this sampling strategy can improve the performance of GCNs on large graphs. The proposed method is evaluated on several graph classification tasks.,"This paper proposes a method to speed up the training of graph convolutional networks (GCNs) by using local bi-directional influence (i.e., local correlation) in the mini-batch of nodes. The proposed method is based on a self-attention mechanism, where each node is sampled from a set of mini-batch of nodes, and the attention is applied to each node individually. The authors show that the proposed method outperforms the state-of-the-art GCN-based methods on a variety of benchmark datasets. "
22050,SP:bb0af9c011ef982c34fcadb545f6b5771818e7fa,velocities CONJUNCTION interactions. interactions CONJUNCTION velocities. STOVE HYPONYM-OF state - space model. state - space model USED-FOR videos. image model CONJUNCTION dynamics model. dynamics model CONJUNCTION image model. dynamics model USED-FOR inference. image model USED-FOR It. dynamics model USED-FOR It. STOVE COMPARE unsupervised models. unsupervised models COMPARE STOVE. STOVE COMPARE supervised baselines. supervised baselines COMPARE STOVE. unsupervised models COMPARE supervised baselines. supervised baselines COMPARE unsupervised models. model USED-FOR model - based control. Method is physical system. Generic is models. Task is regularizing training. OtherScientificTerm is physical behavior. ,"This paper proposes a state-space model to model the dynamics of a physical system. The model is trained using a combination of an image-based model and a dynamics model, where the dynamics model is used to predict the state of the system, and the state-action model is learned from the video. The authors show that the proposed model achieves state-of-the-art performance on a variety of simulated control tasks. ","This paper proposes a new state-space model for video-based control, called STOVE. The model is based on a combination of an image model and a dynamics model, where the dynamics model is used to model the dynamics of the video, and the image model is trained on the video. The authors show that the model outperforms other state-of-the-art unsupervised models on a variety of tasks. The main contribution of the paper is that it proposes a model that can be used to train a model-based model to control a physical system."
22059,SP:e67b463bc0aec2345925d609fa521ea49df57fd9,"variational autoencoders ( VAE ) CONJUNCTION generative adversarial networks ( GAN ). generative adversarial networks ( GAN ) CONJUNCTION variational autoencoders ( VAE ). variational autoencoders ( VAE ) PART-OF autoencoding model. model USED-FOR λ - Jeffreys divergence. Gaussian CONJUNCTION Laplace. Laplace CONJUNCTION Gaussian. Laplace HYPONYM-OF explicit likelihood. Gaussian HYPONYM-OF explicit likelihood. approach USED-FOR VAE model. implicit likelihood USED-FOR approach. adversarially trained discriminator USED-FOR implicit likelihood. adversarially trained discriminator USED-FOR approach. implicit likelihood USED-FOR VAE model. mode - seeking CONJUNCTION mass - covering behaviour. mass - covering behaviour CONJUNCTION mode - seeking. mode - seeking FEATURE-OF model. CIFAR-10 and TinyImagent datasets EVALUATE-FOR model. mass - covering behaviour FEATURE-OF model. Method are GAN, VAE, and adversarial training. OtherScientificTerm are mode collapsing problem, model distribution, and VAE loss. Generic are it, It, and objective. ","This paper proposes a method to train a generative model with variational autoencoders (VAE) and adversarial networks (GANs) in a mode-collapsing manner. The proposed method is based on the observation that the mode-concavity of VAE and GANs is a mode collapsing problem, where the distribution of the model distribution converges to the same distribution as the training distribution. To address this problem, the authors propose to use an adversarial training objective to train the VAE model. The main idea is to use a discriminator to estimate the likelihood of the target distribution, which is then used as the discriminator in the training of the GAN. The authors show that the proposed method outperforms the baselines on CIFAR-10 and TinyImagent.","This paper proposes a new approach to model the mode collapsing problem in variational autoencoding (VAE) models. The main contribution of the paper is to propose an implicit likelihood for the VAE model, which is based on the Laplace-Laplace divergence between the model distribution and the target distribution. The implicit likelihood is learned by using an adversarially trained discriminator to estimate the implicit likelihood. The proposed method is evaluated on CIFAR-10 and TinyImagent datasets. "
22068,SP:87056d0147ddcaf5d78f6888b05161fbdbb3346c,"Adversarial attacks FEATURE-OF CNN classifiers. unreasonably linear extrapolation USED-FOR CNNs. attacks USED-FOR Bayes - Optimal classifier. Bayes - Optimal classifier USED-FOR class distributions. optimal classifier USED-FOR attacks. smooth decision surface FEATURE-OF classifier. datasets EVALUATE-FOR optimal classifier. datasets USED-FOR Bayes - Optimal classifier. adversarial examples USED-FOR it. large - margin methods USED-FOR classifier. machine learning USED-FOR adversarial vulnerability. Task is classification. OtherScientificTerm are geometry of high dimensions, data distribution, optimal decision boundary, and low dimensions. Material is digits. Method are CNN training, and suboptimal training methods. ","This paper studies the problem of adversarial attacks on CNN classifiers. The authors propose a Bayes-optimal classifier, which is a classifier with a smooth decision surface that is robust to adversarial examples. They show that the optimal classifier is a Bayesian classifier that is invariant to the geometry of high dimensions of the data distribution, and the optimal decision boundary is close to the low-dimensional decision boundary of the class distributions. They also show that under certain assumptions on the data distributions, the Bayesian optimal classifiers can be trained with large margin.   ","This paper proposes a Bayes-optimal classifier (BOP) for adversarial attacks on CNNs. The proposed method is based on the idea that the optimal classifier is a smooth decision surface with smooth decision boundary. The authors show that under certain assumptions on the data distribution, the optimal BOP classifier can be trained to be robust to adversarial examples. They also provide a theoretical analysis of the performance of the proposed method. "
22077,SP:a7b3a35e6a79084bdfd1e4a963dfa081279cd8bb,"top-1 accuracy EVALUATE-FOR sparse and non - sparse models. Method are Neural network pruning techniques, network, pruning, abstract representations, and fine - grained classification. Metric are top1 test set accuracy, and image quality. OtherScientificTerm are pruning identified exemplars ( PIEs ), and sparsity. Material are PIE images, and hard - to - generalize - to images. ",This paper studies the effect of pruning identified exemplars (PIEs) on the top-1 accuracy of sparse and non-sparse models. The authors show that PIEs are hard-to-generalize-to images that are hard to generalize to and hard to prune. They also show that sparse models are more sensitive to PIE images than non-sparse models.   ,"This paper studies the problem of fine-grained classification in sparse and non-sparsity settings. The authors show that the top-1 accuracy of a sparse model can be improved by pruning identified exemplars (PIEs), which are hard-to-generalize-to images that are hard to generalize to. They also show that sparse models can be more general than non-sparse models in terms of test set accuracy and image quality. "
22086,SP:4b17edaa7ec6201891433320d85f9a415656b763,"Interactive Fiction games HYPONYM-OF text - based simulations. natural language understanding CONJUNCTION partial observability. partial observability CONJUNCTION natural language understanding. reinforcement learning agents USED-FOR natural language understanding. partial observability CONJUNCTION action generation. action generation CONJUNCTION partial observability. combinatorially - large text - based action spaces FEATURE-OF action generation. template - based action space USED-FOR KG - A2C1. knowledge graph USED-FOR game state. knowledge graph USED-FOR natural language generation. KG - A2C COMPARE IF agents. IF agents COMPARE KG - A2C. IF games EVALUATE-FOR KG - A2C. OtherScientificTerm are natural language, dynamic knowledge graph, combinatorially large natural language actions, and action space size. Generic is They. ","This paper proposes a novel method for text-based reinforcement learning in interactive fiction games. The method is based on a knowledge graph that is used to model the dynamics of the game state, which is then used to train an RL agent. The proposed method is evaluated on a variety of games from the Interactive Fiction Games (IF) suite. The results show that the proposed method outperforms the baselines in terms of performance and exploration.",This paper proposes a novel approach to text-based interactive fiction games. The authors propose a novel action space-based reinforcement learning agent (KG-A2C1) that learns a dynamic knowledge graph for natural language understanding and partial observability. They also propose a template-based action space for action generation. Experiments show that the proposed approach outperforms the state-of-the-art IF agents in a variety of simulated games.
22095,SP:b1784ecbb8f36eef9cae33d61ce60d80c2f9c38d,"language generation HYPONYM-OF sequence prediction problems. maximum likelihood estimation ( MLE ) USED-FOR sequence prediction problems. data - dependent Gaussian prior CONJUNCTION detailed training prediction. detailed training prediction CONJUNCTION data - dependent Gaussian prior. data - dependent Gaussian prior USED-FOR Kullback – Leibler divergence term. text summarization CONJUNCTION storytelling. storytelling CONJUNCTION text summarization. supervised and unsupervised machine translation CONJUNCTION text summarization. text summarization CONJUNCTION supervised and unsupervised machine translation. storytelling CONJUNCTION image captioning. image captioning CONJUNCTION storytelling. language generation tasks EVALUATE-FOR method. image captioning HYPONYM-OF language generation tasks. supervised and unsupervised machine translation HYPONYM-OF language generation tasks. text summarization HYPONYM-OF language generation tasks. storytelling HYPONYM-OF language generation tasks. Generic is it. Method is MLE. OtherScientificTerm are negative diversity ignorance, and prior topological order of tokens. Metric is MLE loss. ","This paper proposes to use a data-dependent Gaussian prior and a detailed training prediction to improve the performance of the maximum likelihood estimation (MLE) for sequence prediction problems. The proposed method is based on the Kullback-Leibler divergence (KL) divergence between the posterior distribution and the true posterior distribution. The KL divergence term is defined as the difference between the KL and the KL-divergence between the two distributions. The authors show that this KL-Divergence term is independent of the prior topological order of tokens in the training set.   The authors also show that the proposed method can be used for language generation tasks such as text summarization, text captioning, and image captioning. ","This paper proposes a new method for maximum likelihood estimation (MLE) for language generation. The main idea is to use the Kullback-Leibler divergence (KLD) divergence term for the data-dependent Gaussian prior (i.e., the prior topological order of tokens). The authors show that the KLD divergence term can be used to improve the performance of MLE on a variety of tasks, including text summarization, machine translation, and image captioning. The authors also provide a theoretical analysis of the MLE loss."
22104,SP:7c29cb5a32b14e1392408dc5daba4cd35848bea9,"Temperature scaling USED-FOR DNN. Temperature scaling HYPONYM-OF calibration approach. cross - entropy loss CONJUNCTION focal loss. focal loss CONJUNCTION cross - entropy loss. focal loss USED-FOR models. temperature scaling CONJUNCTION focal loss. focal loss CONJUNCTION temperature scaling. confidence FEATURE-OF model. focal loss USED-FOR calibrated models. accuracy EVALUATE-FOR focal loss. network architectures USED-FOR approach. accuracy CONJUNCTION calibration. calibration CONJUNCTION accuracy. accuracy EVALUATE-FOR approach. calibration EVALUATE-FOR approach. Task are Miscalibration, downstream tasks, and miscalibration. Method is Deep Neural Networks ( DNNs ). Generic is networks. Material is NLP ( SST, 20 Newsgroup ) datasets. ",This paper proposes a method to calibrate deep neural networks (DNNs) in the presence of miscalibration. The proposed method is based on temperature scaling and focal loss. The main idea is to use temperature scaling as a calibration method and use focal loss as a regularization term to improve the confidence of the model. Experiments show that the proposed method can improve the accuracy and calibration of DNNs.,This paper proposes a new calibration method for deep neural networks (DNNs). The main idea is to use temperature scaling to calibrate the accuracy of DNNs. The proposed method is based on the cross-entropy loss and the focal loss. The authors show that temperature scaling and focal loss can be used to improve the confidence of the trained model. They also show that the proposed method can be applied to a wide range of network architectures.
22113,SP:cd6b8417ec8bcb773c78cff677bb0a76d6b3f6f3,Lipschitz constant FEATURE-OF neural networks. LiPopt HYPONYM-OF polynomial optimization framework. sparse connectivity USED-FOR network. sparse connectivity USED-FOR complexity of computation. approach COMPARE baselines. baselines COMPARE approach. networks CONJUNCTION networks. networks CONJUNCTION networks. ` ∞-Lipschitz constant EVALUATE-FOR approach. random weights CONJUNCTION networks. networks CONJUNCTION random weights. random weights USED-FOR networks. MNIST USED-FOR networks. Task is optimization problems. Method is convolutional as well as pruned neural networks. ,"This paper proposes a new polynomial optimization framework, LiPopt, for neural networks with sparse connectivity. The main idea is to use sparse connectivity to reduce the computational cost of training neural networks in terms of the Lipschitz constant of the weights. The authors show that the proposed method is computationally efficient and achieves better performance than existing methods.   ",This paper proposes a new polynomial optimization framework LiPopt for neural networks with sparse connectivity. The main idea is to use sparse connectivity to reduce the complexity of computation. The authors show that the proposed method can be used for both convolutional and pruned neural networks. The proposed method is evaluated on the MNIST dataset. 
22122,SP:31c9dc0dd8806daddc9cb48c56ec819577fe46cd,self - supervised learning approach USED-FOR video features. video classification CONJUNCTION captioning and segmentation. captioning and segmentation CONJUNCTION video classification. tasks EVALUATE-FOR methods. self - supervised learning approach USED-FOR tasks. self - supervised learning approach COMPARE methods. methods COMPARE self - supervised learning approach. captioning and segmentation HYPONYM-OF tasks. video classification HYPONYM-OF tasks. BERT model USED-FOR text sequences. softmax loss CONJUNCTION noise contrastive estimation ( NCE ). noise contrastive estimation ( NCE ) CONJUNCTION softmax loss. sequences of real - valued feature vectors USED-FOR method. BERT model USED-FOR method. sequences of visual features CONJUNCTION sequences of words. sequences of words CONJUNCTION sequences of visual features. automatic speech recognition USED-FOR sequences of words. sequences of words USED-FOR representations. sequences of visual features USED-FOR representations. Method is cross - modal training. ,This paper proposes a self-supervised learning approach for video classification and captioning tasks. The proposed method is based on a BERT model that is trained on video features and text sequences. The key idea is to use a sequence of real-valued feature vectors as input to BERT and use a softmax loss and noise contrastive estimation (NCE) to improve the performance of the model. Experiments show that the proposed method achieves state-of-the-art performance on video classification tasks.   ,"This paper proposes a self-supervised learning approach for video classification, video captioning and segmentation tasks. The proposed method is based on a BERT model trained on a sequence of sequences of real-valued feature vectors. The method is trained using a softmax loss and a noise contrastive estimation (NCE) loss. The authors show that the proposed method outperforms the state-of-the-art methods on a variety of video classification tasks. "
22131,SP:0f24424d10f1201dd25e8c56354e10afc9b2b11c,"selection masks CONJUNCTION neural network. neural network CONJUNCTION selection masks. Task are inference phase, and data transfer. OtherScientificTerm is public storage server. Method is machine learning models. Generic are framework, model, and masks. ",This paper studies the problem of data transfer in the inference phase of machine learning models. The authors propose a new framework for data transfer based on selection masks and neural networks. They show that selection masks can be used to improve the performance of the model in inference phase. They also provide a theoretical analysis of the effect of the selection masks on the model performance. ,"This paper proposes a new framework for data transfer in the inference phase of machine learning models. The main idea is to use a public storage server to store the training data and transfer the data to a public server. The model is trained on the public server, and the data is stored on the server. In the inference stage, the model uses a selection mask and a neural network to select the selection masks. The data is then transferred to the server using the public storage. The authors show that the model can be used to transfer data from one server to another. "
22140,SP:aa4fcf5b2cae05c5c6a903c24e4992b56655dee2,"Deep neural networks USED-FOR classification tasks. methodology USED-FOR neural network. it USED-FOR outof - distribution ( OOD ) examples. Outlier Exposure ( OE ) technique USED-FOR loss function. loss function USED-FOR out - of - distribution detection. image and text classification tasks EVALUATE-FOR out - of - distribution detection. OE USED-FOR loss function. image and text classification tasks EVALUATE-FOR loss function. OE USED-FOR out - of - distribution detection. method CONJUNCTION Mahalanobis distance - based classifier. Mahalanobis distance - based classifier CONJUNCTION method. OOD detection task EVALUATE-FOR method. Task is artificial intelligence. Method are neural networks, and classification algorithms. OtherScientificTerm is novel class distributions. Metric is classification accuracy. ","This paper proposes a novel loss function for out-of-distribution detection. The proposed loss function is based on the Outlier Exposure (OE) technique, which is an extension of the outlier exposure (OPE) method. The authors show that the proposed OE loss function can be used for OOD detection on image and text classification tasks. ","This paper proposes a novel method for out-of-distribution (OOD) detection. The method is based on the Outlier Exposure (OE) technique, which is an extension of the Mahalanobis distance-based classifier. The authors show that the OE technique can be applied to both image and text classification tasks. They show that OE can be used to improve the OOD detection performance on both image classification and text detection tasks."
22149,SP:89bc528ef801182365ac279e8963803afccb391d,end - to - end deep learning model USED-FOR RNA secondary structure prediction. E2Efold HYPONYM-OF end - to - end deep learning model. unrolled algorithm USED-FOR constrained programming. E2Efold USED-FOR RNA base - pairing matrix. unrolled algorithm USED-FOR deep architectures. deep architectures USED-FOR constraints. it COMPARE SOTA. SOTA COMPARE it. it COMPARE algorithms. algorithms COMPARE it. benchmark datasets EVALUATE-FOR E2Efold. E2Efold COMPARE it. it COMPARE E2Efold. SOTA USED-FOR pseudoknotted structures. E2Efold COMPARE SOTA. SOTA COMPARE E2Efold. inference time EVALUATE-FOR algorithms. ,"This paper proposes an end-to-end deep learning model for RNA secondary structure prediction. The proposed method is based on the unrolled algorithm for constrained programming, where the base-pairing matrix is represented by an unrolled matrix. The authors show that the proposed method can achieve state-of-the-art performance on several benchmark datasets.","This paper proposes an end-to-end deep learning model for RNA secondary structure prediction. The authors propose a novel unrolled algorithm E2Efold, which can be used to learn a base-pairing matrix for the RNA base pairing matrix. The proposed method is based on constrained programming, and the authors show that it can be applied to a variety of deep architectures. They also show that the proposed method outperforms SOTA in terms of inference time. "
22158,SP:b68560cce8c64ebe0ca5e6534b3732c775d36452,"collective policies COMPARE individually trained policies. individually trained policies COMPARE collective policies. OtherScientificTerm are biases, virtual simulation, agents ’ simulations, biased representations, and internal simulations. Method is collective policy. Material is real - world environment. ","This paper studies the effect of internal simulation biases on the performance of agents in a virtual simulation. The authors show that agents’ simulations are biased representations of the real world, and propose a method to mitigate this issue by training a collective policy. They show that this collective policy is more effective than individual policies in terms of performance on simulated tasks. They also show that their method can be used in conjunction with other methods to improve performance on real-world tasks. ",This paper studies the effect of biases in simulation on the performance of a group of agents in a virtual simulation. The authors show that the simulation performance of the agents is affected by their internal representations of the environment. They also show that individual agents' simulations can be biased in a way that the collective simulation is not. They provide a theoretical analysis of the impact of the biases in the simulation and show that it can be attributed to the agents' internal representations.
22167,SP:bd1dc08b4fd9a5cc78d26d7eb7f05dbb4a629ab1,"Generic responses HYPONYM-OF open - domain dialog generation. one - to - one task USED-FOR one - to - many task. dialog generation model USED-FOR semantic latent space. prompt USED-FOR features. model USED-FOR semantically related responses. regression task USED-FOR pair relationship. model COMPARE baselines. baselines COMPARE model. coherence EVALUATE-FOR baselines. model USED-FOR generic response problem. coherence EVALUATE-FOR model. OtherScientificTerm are latent space, and MLE loss. Method is autoencoder. ",This paper proposes a dialog generation model for generic responses in open-domain dialog generation. The proposed model is based on an autoencoder-based model that learns a semantic latent space for the prompt and uses it to generate semantically related responses. The model is trained using a one-to-many task and a regression task to evaluate the pair relationship between the prompts and the responses generated by the model. Experiments show that the proposed model achieves state-of-the-art performance on the generic response problem.,"This paper proposes a dialog generation model for one-to-many dialog generation. The proposed model is based on an autoencoder-based model that learns the semantic latent space of the dialog prompt. The model is trained to generate semantically related responses to the prompt, which are then used to predict the pair relationship between the prompt and the response. The authors show that the proposed model outperforms the state-of-the-art baselines in terms of coherence. "
22176,SP:ef0d5fd333ed60feb3946d24002e9a90642aea66,"it USED-FOR life - affecting decisions. Gaussian light and shadow ( GLAS ) HYPONYM-OF salient explanation method. feature perturbation USED-FOR deep models. GLAS USED-FOR coarseto - fine control. scalability of Gaussian mask USED-FOR GLAS. scalability of Gaussian mask USED-FOR coarseto - fine control. GLAS USED-FOR fine - grained classification. fine - grained classification dataset EVALUATE-FOR GLAS. high speed EVALUATE-FOR GLAS. ImageNet Large Scale Visual Recognition Challenge EVALUATE-FOR GLAS. OtherScientificTerm are discriminative features, salient explanation, and 224×224 image. Task is fine - grained classification task. Method are Gaussian mask, and recursive GLAS. ",This paper proposes Gaussian light and shadow (GLAS) as a salient explanation method for fine-grained classification tasks. The proposed method is based on the observation that Gaussian masks can be used to explain the discriminative features. The authors show that the scalability of the Gaussian mask is the key to improve the performance of the model.   ,"This paper proposes Gaussian light and shadow (GLAS) as a salient explanation method for fine-grained classification. GLAS is based on the idea of coarseto-fine control, which is a feature perturbation method for deep models. The authors show that the scalability of Gaussian mask can be used to improve the performance of GLAS. They also show that GLAS can be applied to the ImageNet Large Scale Visual Recognition Challenge (ISVRC)."
22185,SP:d17ca20cc527c28ab7358cb5b14954e5fb56409f,"Convolution PART-OF Convolutional Neural Networks ( CNNs ). kernel USED-FOR Convolutional Neural Networks ( CNNs ). convolutional kernels USED-FOR redundant data. procedure USED-FOR pixel - wise and channel - wise correlations. computational cost EVALUATE-FOR convolution layer. deconvolution filters PART-OF network. center - surround structure FEATURE-OF biological neurons. center - surround structure HYPONYM-OF deconvolution filters. visual regions of the brain FEATURE-OF biological neurons. Filtering USED-FOR sparse representation. kernels USED-FOR sparse representation. kernels USED-FOR Filtering. network deconvolution operation USED-FOR neural network models. MNIST CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION MNIST. Fashion - MNIST CONJUNCTION Cityscapes. Cityscapes CONJUNCTION Fashion - MNIST. CIFAR-100 CONJUNCTION MNIST. MNIST CONJUNCTION CIFAR-100. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. CIFAR-100 CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION CIFAR-100. Cityscapes CONJUNCTION ImageNet datasets. ImageNet datasets CONJUNCTION Cityscapes. CIFAR-10 CONJUNCTION MNIST. MNIST CONJUNCTION CIFAR-10. MNIST CONJUNCTION Cityscapes. Cityscapes CONJUNCTION MNIST. Fashion - MNIST CONJUNCTION ImageNet datasets. ImageNet datasets CONJUNCTION Fashion - MNIST. MNIST EVALUATE-FOR network deconvolution operation. Fashion - MNIST EVALUATE-FOR network deconvolution operation. CIFAR-100 EVALUATE-FOR network deconvolution operation. ImageNet datasets EVALUATE-FOR network deconvolution operation. CIFAR-10 EVALUATE-FOR network deconvolution operation. Material is real - world image data. Task is neural network training. Method are network deconvolution, Network deconvolution, neural networks, and batch normalization. Metric is faster convergence. ","This paper proposes a method to reduce the computational cost of convolutional layers by removing redundant data. The method is based on the idea of deconvolution filters, which are a type of center-surround structure found in neurons in the brain. The proposed method is evaluated on CIFAR-10, MNIST, Fashion-MNIST, and Cityscapes datasets. ","This paper proposes a method to reduce the computational cost of convolutional neural networks (CNNs) by removing redundant data from the training data. The method is based on the center-surround structure of biological neurons, which is similar to center-center structure in the brain. The authors show that the proposed method can be applied to the training of CNNs. The proposed method is evaluated on CIFAR-10, MNIST, Fashion-MNIST, and Cityscapes datasets."
22194,SP:e1b0de9a36bf8359df368b7a55a7f23e99d88db7,"smartphones HYPONYM-OF edge devices. neural network quantization methods USED-FOR GANs. CNN quantization methods USED-FOR GAN models. CNN quantization methods USED-FOR extreme low bits. quantization method USED-FOR GANs. QGAN HYPONYM-OF quantization method. EM algorithms USED-FOR quantization method. multi - precision algorithm USED-FOR quantization precision. multi - precision algorithm USED-FOR GANs. quantization precision FEATURE-OF GANs. CIFAR-10 CONJUNCTION CelebA. CelebA CONJUNCTION CIFAR-10. QGAN USED-FOR GANs. QGAN COMPARE models. models COMPARE QGAN. CIFAR-10 EVALUATE-FOR QGAN. CelebA EVALUATE-FOR QGAN. QGAN USED-FOR 1 - bit or 2 - bit representations. Method are generative adversarial neural networks ( GANs ), convolutional neural networks ( CNNs ), quantization algorithms, and generator and discriminator networks. Generic is them. OtherScientificTerm is image qualities requirements. ","This paper proposes a novel quantization method for GANs. The proposed method, called QGAN, is based on EM-based quantization. The authors show that the proposed method can achieve 1-bit or 2-bit quantization precision in GAN models. In addition, the authors propose a multi-precision quantization algorithm to improve the precision of the quantized representations. The experimental results show that QGAN achieves better performance than the state-of-the-art GAN quantization methods.",This paper proposes a new quantization method for GANs. The main idea is to use a multi-precision quantization algorithm to improve the quantization precision of GAN models. The proposed method is based on EM-based quantization methods. The method is evaluated on CIFAR-10 and CelebA datasets. The results show that the proposed method outperforms the state of the art.
22203,SP:58c4905f59f04a50b30d27c99521126a6455d38a,"Generative Adversarial Networks HYPONYM-OF nonconvex applications. bilinear and convex - strongly concave settings FEATURE-OF global last - iterate convergence rates. Simultaneous Gradient Descent / Ascent HYPONYM-OF natural algorithms. linear convergence FEATURE-OF HAMILTONIAN GRADIENT DESCENT ( HGD ) algorithm. “ sufficiently bilinear ” condition FEATURE-OF convex - concave problems. convergence rates FEATURE-OF stochastic HGD. Task are convex - concave min - max optimization, and training Generative Adversarial Networks. OtherScientificTerm are averageiterate convergence results, last - iterate convergence guarantees, last - iterate convergence, and convex - concave min - max settings. Method is Consensus Optimization algorithm. ","This paper studies the last-iterate convergence of Hamiltonian gradient descent (HGD) in convex-concave min-max problems. The main result is that the last iterate convergence rate of HGD converges linearly in the convex case, and exponentially in the strongly concave case. The authors show convergence rates for both bilinear and convex strongly convex problems.    The main contribution of the paper is to show that the convergence rate for HGD is global in both the strongly and strongly convecave cases. ","This paper studies the last-iterate convergence of the Hamiltonian Gradient Descent (HGD) algorithm in the bilinear and convex-strongly concave settings. In particular, the authors study the convergence rate of the HGD in the setting where the objective function is a convex convex function. They show that the last iterate convergence rate converges linearly to a global rate, and that it converges to the optimal rate in the strongly concave setting. They also show that in the non-convex setting, the convergence rates converge to the global rate.  "
22212,SP:d8556b52272321a1415ac2d85bb12e88b51ee73a,"stability FEATURE-OF ResNet. stability FEATURE-OF ResNet. gradient descent USED-FOR global minima. over - parameterization requirement FEATURE-OF ResNet. ResNet COMPARE vanilla feedforward network. vanilla feedforward network COMPARE ResNet. normalization layer USED-FOR deep ResNet. normalization layer USED-FOR ResNet. Method is ResNet structure. OtherScientificTerm are ResNet block hl, ReLU activation, initialization, residual blocks, and global convergence. Metric is τ. Generic is forward process. ","This paper studies the stability of deep neural networks with ReLU activations. The authors show that under certain conditions, gradient descent converges to global minima at the rate of $tau$ with respect to the ReLU activation. They show that the stability is due to the over-parameterization of the network. They also show that deep networks with normalization layers are more stable than deep networks without normalization.  ","This paper studies the stability of deep ResNet with ReLU activation in the context of global convergence. The authors show that under certain conditions, the global minima of the ResNet can be reached by gradient descent. They also show that the global convergence can be achieved with over-parameterization of the ReLU activations. The main contribution of the paper is a theoretical analysis of the stability properties of ResNet. "
22221,SP:cf70dc496825ece2f28fdf4f1a6f4316c69e0e48,Sparse neural networks COMPARE dense networks. dense networks COMPARE Sparse neural networks. they USED-FOR wall clock inference times. sparse networks USED-FOR inference. dense networks USED-FOR sparse networks. method USED-FOR sparse neural networks. fixed parameter count CONJUNCTION fixed computational cost. fixed computational cost CONJUNCTION fixed parameter count. accuracy EVALUATE-FOR dense - to - sparse training methods. fixed computational cost USED-FOR method. fixed parameter count USED-FOR method. parameter magnitudes CONJUNCTION infrequent gradient calculations. infrequent gradient calculations CONJUNCTION parameter magnitudes. method USED-FOR topology. topology FEATURE-OF network. parameter magnitudes USED-FOR topology. parameter magnitudes USED-FOR method. infrequent gradient calculations USED-FOR method. approach COMPARE prior techniques. prior techniques COMPARE approach. accuracy EVALUATE-FOR prior techniques. floating - point operations ( FLOPs ) USED-FOR approach. accuracy EVALUATE-FOR approach. MobileNet v1 CONJUNCTION MobileNet v2. MobileNet v2 CONJUNCTION MobileNet v1. ResNet-50 CONJUNCTION MobileNet v1. MobileNet v1 CONJUNCTION ResNet-50. WideResNets CONJUNCTION RNNs. RNNs CONJUNCTION WideResNets. ImageNet-2012 dataset CONJUNCTION WideResNets. WideResNets CONJUNCTION ImageNet-2012 dataset. WideResNets CONJUNCTION CIFAR-10 dataset. CIFAR-10 dataset CONJUNCTION WideResNets. MobileNet v2 CONJUNCTION WideResNets. WideResNets CONJUNCTION MobileNet v2. WikiText-103 dataset EVALUATE-FOR RNNs. ResNet-50 USED-FOR sparse training. ImageNet-2012 dataset EVALUATE-FOR MobileNet v2. Method is trainable sparse model. Task is optimization. OtherScientificTerm is local minima. ,"This paper proposes a method for training sparse neural networks with fixed parameter count and fixed FLOPs. The method is based on the observation that dense networks are more efficient than sparse networks in terms of wall clock inference times. The authors propose a method to compute the parameter magnitudes and infrequent gradient calculations in sparse networks. The proposed method is evaluated on MobileNet v1, MobileNetv2, WideResNets, and CIFAR-10. ",This paper proposes a method for training sparse neural networks with fixed parameter count and fixed computational cost. The method is based on the idea that the topology of a sparse network is a function of the parameter magnitudes of the network. The authors propose a method to learn a sparse model that can be trained with a fixed number of parameters. Theoretical analysis is provided to show that this method can be used to improve the accuracy of sparse networks. Empirical results show that the proposed method outperforms the state-of-the-art in terms of accuracy.
22230,SP:d2d2b892518d54d0e63e26a056f2298be3be2610,"deep generative models USED-FOR photo - realistic images. computer vision CONJUNCTION natural language processing. natural language processing CONJUNCTION computer vision. deep generative models USED-FOR visual or textual content embeddings. photo - realistic images CONJUNCTION visual or textual content embeddings. visual or textual content embeddings CONJUNCTION photo - realistic images. deep generative models USED-FOR natural language processing. deep generative models USED-FOR computer vision. translation CONJUNCTION zoom. zoom CONJUNCTION translation. zoom CONJUNCTION color variations. color variations CONJUNCTION zoom. color variations HYPONYM-OF transformations. translation HYPONYM-OF transformations. zoom HYPONYM-OF transformations. GANs CONJUNCTION variational auto - encoders. variational auto - encoders CONJUNCTION GANs. variational auto - encoders EVALUATE-FOR method. GANs EVALUATE-FOR method. approach CONJUNCTION BigGAN model. BigGAN model CONJUNCTION approach. Method are generative process, generative models, and generative model. OtherScientificTerm are latent space, and human annotations. ","This paper proposes a method for training generative models for image generation. The method is based on the idea that the latent space of a generative model can be represented as a set of transformations (e.g. translation, zoom, color variations, etc.) and the output of the model can then be used as a human-annotated version of the original image. The proposed method is evaluated on a variety of image generation tasks, including image generation, text generation, and image classification. ","This paper presents a method for generating a set of embeddings from a latent space of a generative model. The latent space is represented by a GAN model, which is trained to predict the embedding embedding of a given image. The embedding is then used to train a GNN model to generate the latent embedding. The model is trained using a combination of GANs, variational auto-encoders, and BigGANs. The proposed method is evaluated on a variety of datasets, and it is shown that the proposed method outperforms the baselines."
22239,SP:1c63389e972d4652fac831e9d11609cd3c3c371a,"model USED-FOR unsupervised physical parameter estimation of systems. differential equations USED-FOR scene dynamics. video USED-FOR unsupervised physical parameter estimation of systems. object state supervision USED-FOR physical scene understanding methods. objects CONJUNCTION state and velocity representations. state and velocity representations CONJUNCTION objects. framework USED-FOR long term extrapolative video prediction. framework USED-FOR vision - based model - predictive control. long term extrapolative video prediction CONJUNCTION vision - based model - predictive control. vision - based model - predictive control CONJUNCTION long term extrapolative video prediction. approach COMPARE unsupervised methods. unsupervised methods COMPARE approach. unsupervised methods USED-FOR long - term future frame prediction of systems. approach USED-FOR long - term future frame prediction of systems. dynamics PART-OF model. interacting objects FEATURE-OF long - term future frame prediction of systems. inductive bias USED-FOR dynamics. vision - actuated model - based control USED-FOR pendulum system. goal - driven control CONJUNCTION physical reasoning. physical reasoning CONJUNCTION goal - driven control. controller ’s interpretability USED-FOR goal - driven control. physical reasoning USED-FOR zero - data adaptation. controller ’s interpretability USED-FOR physical reasoning. controller ’s interpretability USED-FOR zero - data adaptation. goal - driven control USED-FOR zero - data adaptation. OtherScientificTerm is labeled states. Method are differentiable physics, physics - as - inverse - graphics approach, and vision - physics integration. ","This paper proposes a vision-based model-based predictive control for long-term extrapolative video prediction of systems. The proposed method is based on differential equations that model the dynamics of a pendulum system in a video. The model is trained using vision-physiologically-inverse graphics (PIGS) and is able to predict the long term future frame of the system in the video. In addition, the model is also able to learn to control the system with vision-predictive control.   ","This paper proposes a vision-based model-predictive control framework for unsupervised physical parameter estimation of systems. The model is built on top of a physics-as-inverse-graphic (PIG) model, which predicts the dynamics of a pendulum system from a video frame. The dynamics of the system is modeled using a differential equation, and the model is trained with a vision model-based predictive control. Experiments show that the proposed method outperforms the state-of-the-art in terms of long-term future frame prediction and zero-data adaptation. "
22248,SP:c6b8b682bf3087a65cb2379700b8a0183853c2af,"Graph Convolutional Networks ( GCN ) USED-FOR class relevance. Graph Convolutional Networks ( GCN ) USED-FOR structure of clean and noisy data. graph per class USED-FOR structure of clean and noisy data. GCN - inferred “ clean ” probability USED-FOR relevance measure. GCN USED-FOR binary classifier. weighted binary cross - entropy loss function USED-FOR GCN. weighted binary cross - entropy loss function USED-FOR binary classifier. few - shot learning problem EVALUATE-FOR method. classification accuracy EVALUATE-FOR few - shot classification. few - shot classification EVALUATE-FOR GCN - based cleaning process. classification accuracy EVALUATE-FOR GCN - based cleaning process. GCN - based method COMPARE transductive approach. transductive approach COMPARE GCN - based method. Method is classifier. OtherScientificTerm are clean from noisy examples, and relevance. Material is noisy data. ",This paper proposes to use graph convolutional networks (GCNs) to learn the structure of clean and noisy data in the few-shot learning problem. The proposed method is based on the idea that a GCN-implicitly inferred “clean” probability can be used to measure the relevance of a given class to a given set of clean examples. The authors then propose to use a weighted binary cross-entropy loss function to train a binary classifier. Experiments show that the proposed method achieves state-of-the-art performance on the few shot learning task. ,"This paper proposes a novel GCN-based method for few-shot learning. The proposed method is based on the notion of class relevance, which is a measure of the similarity between clean and noisy data. The authors propose to use a weighted binary cross-entropy loss function for the classifier. They show that the proposed method outperforms the state-of-the-art transductive method in the few shot learning setting. "
22257,SP:dd9c9a5dccbba5dd15b03ca6b314a9e153e95548,"edge features CONJUNCTION message passing channels. message passing channels CONJUNCTION edge features. differentiable objective USED-FOR MI. variational approach USED-FOR differentiable objective. variational approach USED-FOR MI. objective USED-FOR model. MI - maximized models USED-FOR learning tasks. objective USED-FOR edge information. edge information FEATURE-OF model. molecular graphs USED-FOR regression. relation prediction in knowledge graphs HYPONYM-OF learning tasks. regression HYPONYM-OF learning tasks. Method is Graph Neural Networks ( GNNs ). OtherScientificTerm are representation vector, and parameterized transform matrix. Metric is Mutual Information ( MI ). Material is knowledge graphs. ","This paper proposes to use mutual information (MI) to improve the performance of graph neural networks (GNNs) in relation prediction tasks. In particular, the authors propose a variational approach to maximize the mutual information between edge features and message passing channels in GNNs. The proposed approach is based on the idea of using a differentiable objective to maximize mutual information. Experiments show that the proposed approach outperforms existing GNN models on a variety of tasks.  ","This paper studies the problem of mutual information maximization (MI) in graph neural networks (GNNs). The authors propose a differentiable objective that maximizes the mutual information between the input graph features and the message passing channels of a GNN. They show that the MI-maximized GNNs can outperform the state-of-the-art GNN models on a variety of tasks, including regression and relation prediction. They also show that their approach can be applied to molecular graphs."
22266,SP:f1cf63d728da51b4f83eb50ef69e3788b3a5ed74,"models USED-FOR specifying visual transformations. Generative networks USED-FOR specifying visual transformations. Generative networks HYPONYM-OF models. verification methods USED-FOR generative networks. verifier USED-FOR generative networks. APPROXLINE HYPONYM-OF verifier. deterministic and probabilistic abstract interpretation USED-FOR APPROXLINE. Task is certification of generative models. Method is generative models. OtherScientificTerm are sufficient non - convexity, non - convexity, and network ’s latent space. ","This paper studies the verification of generative models in the presence of non-convexity in the latent space of a generative model. In particular, the authors propose a method to certify the existence of a given generative network in a deterministic and probabilistic abstract interpretation. The main contribution is to show that under certain conditions, a verifier can be trained to certify that a given latent space is not convex.    The main contributions of the paper are as follows:  1. The authors prove that the latent spaces of a certain class of GNNs are non-consistently non-uniform in the sense that the convexity of the network is not guaranteed. 2. They prove that under some conditions, the distribution of the latent variables of a GNN can be guaranteed to be a convex function of the underlying latent variables.  3. They show that the distribution over latent variables can be verified by a probabilistically abstract interpretation of the distribution. ","This paper proposes APPROXLINE, a probabilistic verifier for verifying generative networks. The main contribution of the paper is a deterministic interpretation of the latent space of a generative model. The authors show that the latent spaces of generative models are non-convex if they are sufficiently convex. They also show that if the latent latent space is not convex, the generative network’s latent space does not have sufficient convexity. They show that APPROxLINE can be used to certify generative neural networks. "
22275,SP:2b0887dcf09249e8cee30d38163aeb9ef1e92b27,"deep architectures USED-FOR models. spectral graph convolutional operator USED-FOR graph neural networks ( GNNs ). GNNs USED-FOR suspended animation problem. GNNs USED-FOR suspended animation problem. nodes ’ raw features CONJUNCTION intermediate representations. intermediate representations CONJUNCTION nodes ’ raw features. intermediate representations PART-OF graph. intermediate representations USED-FOR model layers. GRESNET ( Graph Residual Network ) framework USED-FOR connected highways. norm preservation perspective USED-FOR graph residual terms. GCN CONJUNCTION GAT. GAT CONJUNCTION GCN. GAT CONJUNCTION LOOPYNET. LOOPYNET CONJUNCTION GAT. GRESNET framework USED-FOR GNNs. LOOPYNET HYPONYM-OF GNNs. GCN HYPONYM-OF GNNs. GAT HYPONYM-OF GNNs. Generic are problem, model, and learning settings. OtherScientificTerm are model depth, suspended animation limit, and node ’s representations. Material are graph data, and real - world benchmark datasets. Method is residual learning methods. ",This paper proposes a new method for learning graph residualsolutions. The method is based on the spectral graph convolutional operator (GPC) and uses a graph residual network (GRESNET) to learn the graph residual terms. Theoretical analysis is provided to show that the proposed method is able to recover the true graph representation of the original graph. Experiments are conducted on synthetic and real-world datasets. ,"This paper proposes a new framework for graph residual learning, GRESNET (Graph Residual Network), which is based on the spectral graph convolutional operator (GCN) operator. The main idea of the paper is to use the graph residual terms to preserve the norm of the graph. The authors show that the model depth and the number of nodes in the graph can be preserved by using the GRESnet framework. They also provide a theoretical analysis of the relationship between model depth, number of node representations, and graph residuals."
22284,SP:dc436ade4d04072de35a90e5e4a1bfebfddb04e9,"face prior knowledge USED-FOR reconstruction process. limited scan data USED-FOR face prior knowledge. linear 3D morphable models ( 3DMM ) HYPONYM-OF face prior knowledge. limited scan data USED-FOR linear 3D morphable models ( 3DMM ). face prior knowledge USED-FOR ambiguity. expressions CONJUNCTION poses. poses CONJUNCTION expressions. poses CONJUNCTION lightings. lightings CONJUNCTION poses. expressions FEATURE-OF facial images. poses FEATURE-OF facial images. linear parametric models USED-FOR methods. convolutional neural networks ( CNN ) USED-FOR nonlinear parametric model. linear 3DMM USED-FOR dataset. dataset USED-FOR models. identity and expression representations PART-OF models. semi - supervised manner FEATURE-OF adversarial loss. unconstrained photo collections USED-FOR unlabeled face images. adversarial loss USED-FOR model. semi - supervised manner USED-FOR model. hybrid batches of unlabeled and labeled face images USED-FOR model. identity shape FEATURE-OF facial images. expression CONJUNCTION pose. pose CONJUNCTION expression. identity CONJUNCTION expression. expression CONJUNCTION identity. pose CONJUNCTION lighting representations. lighting representations CONJUNCTION pose. model USED-FOR facial editing applications. model USED-FOR expression. model USED-FOR identity. model USED-FOR lighting representations. expression transfer HYPONYM-OF facial editing applications. model COMPARE state - of - the - art methods. state - of - the - art methods COMPARE model. model USED-FOR expression. Task are Recovering 3D geometry shape, ill - posed problem, and reconstruction. OtherScientificTerm is center loss. ","This paper proposes a new method for face reconstruction based on linear 3D morphable models (3DMM). The proposed method is based on a convolutional neural network (CNN) and a nonlinear parametric model (LPM). The main idea is to use an unconstrained photo collection of unlabeled face images and a hybrid batch of unlabelled and labeled face images to learn identity shape, expression, pose, and lighting representations. A semi-supervised adversarial loss is used to improve the performance of the model. Experiments show that the proposed method outperforms state-of-the-art methods.","This paper proposes a new method for face reconstruction based on 3D morphable models (3DMM). The proposed method is based on a semi-supervised approach, where the model is trained with a hybrid batch of unlabeled and labeled face images. The model learns the identity shape, expression, pose, and lighting representations of the unlabelled and labeled images, and then reconstructs the original face from the original images.  The method is evaluated on three different datasets, and compared with state-of-the-art methods. "
22293,SP:f7bc06697b09e2d59ec06b2cbcf3c0828ece32ae,"transition kernel USED-FOR Model - based imitation learning methods. partial knowledge FEATURE-OF transition kernel. Reinforcement Learning ( RL ) USED-FOR imitation problems. unknown transition kernel CONJUNCTION synthetic kernel. synthetic kernel CONJUNCTION unknown transition kernel. synthetic kernel USED-FOR transition of state components. kernel USED-FOR state components. transition kernel USED-FOR transition of state components. multiplayer games FEATURE-OF imitation tasks. policy gradient algorithm CONJUNCTION model. model CONJUNCTION policy gradient algorithm. policy gradient algorithm COMPARE simulation - free alternative. simulation - free alternative COMPARE policy gradient algorithm. model COMPARE simulation - free alternative. simulation - free alternative COMPARE model. Task is policy evaluation. Method are eMDP, and transition model. Generic is components. OtherScientificTerm is sr. ","This paper proposes a model-based imitation learning method for multi-player multi-agent reinforcement learning. The main idea is to learn a model of the eMDP with a transition model, where the transition model consists of a set of state components. The transition model is then used to train a policy gradient algorithm. The authors show that the proposed method outperforms the state-of-the-art simulation-free imitation learning methods in a variety of multiplayer games. ",This paper proposes a model-based imitation learning method for multiplayer multiplayer games. The main idea is to use a synthetic transition kernel to model the transition of state components of the game. The authors show that the transition kernel can be used as a surrogate for the true transition kernel. They also show that their method is able to learn a policy gradient algorithm that is more efficient than a simulation-free method. 
22302,SP:82cce92821e8168ab4a6fd67573b66c1d17673b8,"manually - designed reward function USED-FOR Learning useful skills. OpenAI Gym CONJUNCTION navigation task. navigation task CONJUNCTION OpenAI Gym. OpenAI Gym FEATURE-OF simulated robotic manipulation tasks. navigation task HYPONYM-OF simulated robotic manipulation tasks. Gazebo simulator USED-FOR navigation task. navigation task EVALUATE-FOR approach. simulated robotic manipulation tasks EVALUATE-FOR approach. intrinsic mutual information rewards USED-FOR method. mutual information discriminator USED-FOR learning. pre - trained policy USED-FOR learning. pre - trained policy CONJUNCTION mutual information discriminator. mutual information discriminator CONJUNCTION pre - trained policy. learning USED-FOR task rewards. mutual information discriminator USED-FOR task rewards. pre - trained policy USED-FOR task rewards. sparse rewards USED-FOR robotic manipulation tasks. Method are reinforcement learning, and selfsupervised Reinforcement Learning approach. OtherScientificTerm are external reward function, intrinsic objective, context states, robot states, states of interest, and mutual information. ",This paper proposes a self-supervised reinforcement learning approach for robotic manipulation tasks where the goal is to learn a state-action pair that maximizes mutual information between the current state and the next state in the state space. The proposed method is based on the idea of using a discriminator to predict the state and action pairs that are most relevant to the current task. The discriminator is trained using a pre-trained policy and is trained to predict which states are relevant and which ones are not relevant. The method is evaluated on a variety of simulated robotic tasks and achieves state-of-the-art performance. ,This paper proposes a self-supervised reinforcement learning method for robotic manipulation tasks. The method is based on a mutual information discriminator (MIDC) that learns the mutual information between the robot states and the state of interest. The MIDC discriminator is trained using a pre-trained policy and a discriminator that is trained to discriminate between the states of interest and the task rewards. Experiments are conducted on a Gazebo simulator and a simulated robotic manipulation task.
22311,SP:5db63d39cfd8132bec832ab64b8fbd403b3b8df0,"Neural network ( NN ) trojaning attack HYPONYM-OF attack. adversarial attacks COMPARE it. it COMPARE adversarial attacks. it USED-FOR malicious functionality. small datasets USED-FOR NN trojaning attacks. generality CONJUNCTION stealthiness. stealthiness CONJUNCTION generality. trojaning attack method USED-FOR large models. capability CONJUNCTION generality. generality CONJUNCTION capability. trojaning attack method COMPARE studies. studies COMPARE trojaning attack method. generality EVALUATE-FOR studies. capability EVALUATE-FOR studies. stealthiness EVALUATE-FOR studies. trojaning attack USED-FOR small domain. large - scale dataset USED-FOR trojaned model. biased behavior FEATURE-OF trojan. Method is NN models. OtherScientificTerm are weight parameters, fixed target classes, and malicious misclassification target. ",This paper proposes a novel adversarial attack for neural networks. The proposed attack is based on the idea that adversarial attacks on neural networks can be viewed as a form of adversarial training. The authors show that the proposed attack can be used to generate adversarial examples that can be misclassified by the target class. The main contribution of the paper is that the authors propose a new attack method that is capable of generating adversarial images that are malicious in nature.   ,"This paper proposes a novel adversarial attack for neural network (NN) trojaning attacks. The main idea is to use a large-scale dataset as a training dataset to train a trojaned model, and then use it as a test dataset to test the effectiveness of the trojan. The authors show that the proposed attack can be used in combination with adversarial attacks to improve the generality and stealthiness of NN trojans. "
22320,SP:35ea626ee4dd1a7a368a660eb852192924966b7f,prediction tasks PART-OF drug 1 discovery. few - shot regression ( FSR ) problems USED-FOR prediction tasks. modelling of biological assays HYPONYM-OF few - shot regression ( FSR ) problems. reinforcement learning methods USED-FOR applications. few - shot classification USED-FOR applications. few - shot classification CONJUNCTION reinforcement learning methods. reinforcement learning methods CONJUNCTION few - shot classification. FSR methods USED-FOR tasks. real - world constraints FEATURE-OF tasks. deep kernel learning USED-FOR FSR 6 algorithm. kernel function CONJUNCTION differentiable kernel 8 algorithm. differentiable kernel 8 algorithm CONJUNCTION kernel function. deep network CONJUNCTION kernel function. kernel function CONJUNCTION deep network. deep network CONJUNCTION differentiable kernel 8 algorithm. differentiable kernel 8 algorithm CONJUNCTION deep network. deep network PART-OF algorithm. differentiable kernel 8 algorithm PART-OF algorithm. kernel function PART-OF algorithm. kernel USED-FOR task. algorithm USED-FOR kernel. kernel USED-FOR inference. task PART-OF inference. algorithm USED-FOR task. It COMPARE state - of - the - art algorithms. state - of - the - art algorithms COMPARE It. It USED-FOR complex task distributions. real - world benchmarks EVALUATE-FOR state - of - the - art algorithms. real - world benchmarks EVALUATE-FOR It. FSR algorithms USED-FOR noisy and uncertain environments. biological assays USED-FOR benchmarks. drug discovery HYPONYM-OF noisy and uncertain environments. Task is data generation. ,This paper proposes a new few-shot regression (FSR) algorithm based on deep kernel learning for FSR. The main idea is to use a differentiable kernel function and a deep network to learn a kernel function for each task. The proposed method is evaluated on synthetic and real-world FSR tasks.   ,"This paper proposes a new few-shot regression (FSR) algorithm for drug discovery. The main idea of the paper is to use deep kernel learning to learn a kernel function for FSR 6, which can be used to learn the task distribution of the task. The proposed algorithm is based on a differentiable kernel 8 algorithm and a deep network. The authors show that the proposed algorithm outperforms state-of-the-art FSR methods on a variety of drug discovery tasks. "
22329,SP:91ca4c3ee07617356250bae9f4ef9799b3b134ff,"Neural networks USED-FOR reasoning tasks. Graph Neural Networks ( GNNs ) USED-FOR tasks. Graph Neural Networks ( GNNs ) HYPONYM-OF network structures. network structures USED-FOR tasks. expressive power FEATURE-OF they. framework USED-FOR reasoning tasks. computation structure COMPARE algorithmic structure. algorithmic structure COMPARE computation structure. algorithmic structure FEATURE-OF reasoning process. network USED-FOR reasoning tasks. sample complexity bound EVALUATE-FOR alignment. framework EVALUATE-FOR reasoning models. visual question answering CONJUNCTION shortest paths. shortest paths CONJUNCTION visual question answering. intuitive physics CONJUNCTION visual question answering. visual question answering CONJUNCTION intuitive physics. dynamic programming ( DP ) HYPONYM-OF algorithmic paradigm. dynamic programming ( DP ) USED-FOR reasoning tasks. algorithmic paradigm USED-FOR reasoning tasks. intuitive physics HYPONYM-OF reasoning tasks. shortest paths HYPONYM-OF reasoning tasks. visual question answering HYPONYM-OF reasoning tasks. GNNs USED-FOR tasks. GNNs COMPARE DP. DP COMPARE GNNs. reasoning tasks EVALUATE-FOR theory. Method is structured networks. OtherScientificTerm are network structure, and algorithmic alignment. ","This paper studies the problem of learning reasoning models with graph neural networks (GNNs) in the context of dynamic programming (DPs). The authors show that GNNs can be viewed as a special case of DPs, and show that they can be used to learn reasoning models that perform better than DPs on reasoning tasks. They show that DPs are able to learn better models for reasoning tasks with a sample complexity bound of $O(1/\sqrt{n})$. They also provide a theoretical analysis of the sample complexity of GNN models trained with DPs.","This paper proposes a new framework for understanding the expressive power of graph neural networks (GNNs) for reasoning tasks. The main idea is to use dynamic programming (DP) as a way to model the reasoning process of GNNs. The authors propose a new sample complexity bound for the task of reasoning, which is based on the sample complexity of a GNN. The sample complexity is defined as the difference between the complexity of the GNN and the computation structure of the task. They show that the complexity is bounded by the difference in the number of tasks that GNN can solve. They also show that DP can be used to learn GNN-based reasoning models. "
22338,SP:a52aee8da5cf5acd2baf3c2a62cb679e13b18bd5,conditional consistency CONJUNCTION intra - conditioning diversity. intra - conditioning diversity CONJUNCTION conditional consistency. image quality CONJUNCTION conditional consistency. conditional consistency CONJUNCTION image quality. conditional consistency HYPONYM-OF metrics. metrics EVALUATE-FOR models. it USED-FOR properties. Fréchet distance FEATURE-OF joint distributions. metric USED-FOR it. metric USED-FOR properties. controllable synthetic dataset EVALUATE-FOR FJD. FJD COMPARE metrics. metrics COMPARE FJD. bounding boxes CONJUNCTION images. images CONJUNCTION bounding boxes. object masks CONJUNCTION bounding boxes. bounding boxes CONJUNCTION object masks. images CONJUNCTION text captions. text captions CONJUNCTION images. class labels CONJUNCTION object masks. object masks CONJUNCTION class labels. metric EVALUATE-FOR cGAN - based models. cGAN - based models USED-FOR conditioning modalities. text captions HYPONYM-OF conditioning modalities. images HYPONYM-OF conditioning modalities. class labels HYPONYM-OF conditioning modalities. bounding boxes HYPONYM-OF conditioning modalities. object masks HYPONYM-OF conditioning modalities. cGAN benchmarking CONJUNCTION model selection. model selection CONJUNCTION cGAN benchmarking. metric USED-FOR cGAN benchmarking. metric USED-FOR model selection. FJD USED-FOR metric. FJD USED-FOR cGAN benchmarking. Method is Conditional Generative Adversarial Networks ( cGANs ). Task is model benchmarking. Metric is Fréchet Joint Distance ( FJD ). OtherScientificTerm is conditioning. ,"This paper proposes a new metric for evaluating conditional generative adversarial networks (cGANs) based on the Fréchet Joint Distance (FJD) metric. The proposed FJD measures the distance between the joint distributions of the conditioned and unconditioned distributions. The authors show that the FJD can be used as a metric to evaluate the quality of conditional consistency, intra-conditioning diversity, and conditional consistency in image quality.  The authors also show that FJD is useful for model selection and model selection in model selection.","This paper proposes a new metric, Fréchet Joint Distance (FJD), which measures the distance between the joint distributions of two conditional generative adversarial networks (cGANs) trained on a controllable synthetic dataset. The authors show that FJD can be used as a metric for model selection in the context of cGAN-based benchmarking. They show that the FJD is better than other metrics in terms of the quality of the data and the diversity of the training data. They also show that it can be applied to model selection. "
22347,SP:fa822e8472efae17c7dfde8258057898383ecbbb,"decision states USED-FOR exploration. exploration USED-FOR downstream goal - driven tasks. partially observable environments FEATURE-OF downstream goal - driven tasks. Method is VIC framework. OtherScientificTerm are empowerment objective, and extrinsic rewards. Task is identification of decision states. ","This paper proposes a novel exploration framework for goal-driven tasks with partially observable environments. The proposed method is based on the VIC framework, where the goal is to identify decision states that can be used for exploration. The authors show that the proposed method outperforms other exploration methods in a variety of tasks, including goal-conditioned and goal-free tasks.  ","This paper proposes a new VIC framework for the exploration of partially observable environments, where the goal is to identify decision states that can be used for downstream goal-driven tasks. The main idea of the paper is to use the empowerment objective as an extrinsic reward to encourage exploration in a partially observable environment. The paper shows that this empowerment objective can be combined with a goal-based exploration objective, and that it can be applied to a variety of environments. The experiments show that the proposed method is able to achieve state-of-the-art performance on a number of tasks."
22356,SP:a19a51df7e28a5d3380be4fba13842efbfe3efec,"architectures USED-FOR irregularly - sampled and asynchronous time series. real - world datasets FEATURE-OF irregularly - sampled and asynchronous time series. healthcare applications HYPONYM-OF real - world datasets. framework USED-FOR classifying irregularly sampled time series. data efficiency EVALUATE-FOR framework. unaligned measurements USED-FOR irregularly sampled time series. method COMPARE competitors. competitors COMPARE method. runtime EVALUATE-FOR it. healthcare time series datasets EVALUATE-FOR competitors. healthcare time series datasets EVALUATE-FOR method. Method are deep neural networks, and differentiable set function learning. Task is online monitoring scenarios. ","This paper proposes a framework for irregularly sampled and asynchronous time series classification. The proposed method is based on a differentiable set function learning approach, which uses the unaligned measurements of the time series to learn the set function. The method is evaluated on two real-world healthcare time series datasets and achieves state-of-the-art performance.  ","This paper proposes a framework for classifying irregularly sampled and asynchronous time series. The proposed method is based on a differentiable set function learning approach. The method is evaluated on a variety of real-world datasets, including healthcare time series, and it is shown to be competitive with existing methods."
22365,SP:4ae89d64460b08749acc192004545c1fa8b7553b,Convolutional neural networks ( CNNs ) USED-FOR image recognition. inductive biases USED-FOR natural image priors. inductive biases FEATURE-OF CNNs. deep networks USED-FOR audio signals. inductive biases FEATURE-OF audio signals. inductive biases FEATURE-OF deep networks. network architectures USED-FOR audio processing. local neighborhoods USED-FOR convolutional kernels. harmonic series USED-FOR kernels. networks USED-FOR audio priors. networks USED-FOR unsupervised audio restoration. Harmonic Convolution USED-FOR networks. Harmonic Convolution USED-FOR they. they USED-FOR supervised musical source separation. Harmonic Convolution USED-FOR supervised musical source separation. generalization EVALUATE-FOR supervised musical source separation. generalization EVALUATE-FOR they. Generic is priors. OtherScientificTerm is harmonic structure. ,"This paper proposes to use harmonic convolutional kernels for audio processing. Specifically, the authors use harmonic series to model the harmonic structure of the input audio signal. The authors show that harmonic convolutions can be used to model audio priors. The proposed method is evaluated on unsupervised audio restoration tasks and supervised musical source separation tasks. ","This paper proposes a new method for unsupervised audio restoration based on harmonic convolutional networks (Harmonic Convolutional Networks). The main idea is to use a harmonic series of local neighborhoods in the audio signal to train the network. The harmonic series is composed of a series of harmonic series, where each harmonic series represents a different harmonic structure. The authors show that the harmonic series can be used to train a neural network to learn audio priors. They also show that their method can be applied to supervised musical source separation.  "
22374,SP:c81a2b3fd1c56b9b18e4a358e3ff8b40aea5256a,"specialized hardware accelerators USED-FOR neural network training. GPUs USED-FOR neural network training. GPUs CONJUNCTION specialized hardware accelerators. specialized hardware accelerators CONJUNCTION GPUs. disk I / O CONJUNCTION data preprocessing. data preprocessing CONJUNCTION disk I / O. workloads EVALUATE-FOR data echoing algorithms. data echoing algorithm COMPARE baseline. baseline COMPARE data echoing algorithm. upstream computation USED-FOR data echoing algorithm. upstream computation USED-FOR baseline. wall - clock time FEATURE-OF ResNet-50. ImageNet FEATURE-OF ResNet-50. Task are training pipeline, and training. OtherScientificTerm are accelerators, echoing, and batch sizes. Method are data echoing, pipeline stages, Data echoing, and network. Metric is training time. ","This paper proposes a data-augmented version of data echoing, which is a popular technique to speed up the training of deep neural networks. The authors show that data echoing can be used to reduce the number of training epochs in the training pipeline. They show that this technique can be applied to a wide range of architectures and datasets, and that it can be combined with other techniques such as data preprocessing to improve the efficiency of the training process.  ","This paper studies the problem of data echoing in neural network training. The authors propose a new data echoing algorithm for training neural networks. The main idea is to use a data-augmented version of the training pipeline, where the data is fed to the network at different stages of training. They show that this method can reduce the training time in terms of the number of training epochs and batch sizes. They also show that their method is faster than the baseline data echoing method.  "
22383,SP:b4cf56d3fa7d65cacde33f17cd04bd5bbc52dd71,"policy COMPARE policies. policies COMPARE policy. controllable subspace FEATURE-OF Markov decision process. Markov decision process FEATURE-OF behaviors. controllable subspace FEATURE-OF behaviors. Successor features USED-FOR generalization problem. grounded feature space FEATURE-OF reward function. generalization CONJUNCTION task inference. task inference CONJUNCTION generalization. algorithm USED-FOR generalization. algorithm USED-FOR task inference. Variational Intrinsic Successor FeatuRes ( VISR ) USED-FOR task inference. Variational Intrinsic Successor FeatuRes ( VISR ) HYPONYM-OF algorithm. successor features framework USED-FOR task inference. Atari suite EVALUATE-FOR VISR. human - level performance EVALUATE-FOR VISR. Generic are formulation, tasks, techniques, and method. OtherScientificTerm are controllable features, and limited feedback. ","This paper studies the problem of generalization in Markov decision processes (MDPs), where the goal is to learn a policy that can generalize well to unseen tasks. The authors propose a successor features framework to solve this problem. Theoretical results show that the proposed method is able to generalize better than the state-of-the-art in the Atari suite. Experiments are conducted to demonstrate the effectiveness of the method. ","This paper proposes a successor features framework for the generalization problem of a Markov decision process with controllable subspaces. The key idea is to use the successor features as a grounded feature space in the feature space of the reward function, and to use this feature space to learn a generalization algorithm for the task inference problem. The main contribution of the paper is to propose a new algorithm for task inference. The proposed method is evaluated on the Atari suite, and it achieves state-of-the-art performance.  "
22392,SP:83500230586a9134f910ad067b7233dc563dc1ba,"functional view USED-FOR networks. functional view USED-FOR them. smoothness of the functional approximation CONJUNCTION flat initial approximation. flat initial approximation CONJUNCTION smoothness of the functional approximation. smoothness of the functional approximation USED-FOR generalization. flat initial approximation USED-FOR generalization. Method are deep neural networks, functional view of these networks, and massively overparamaterized networks. OtherScientificTerm are initializations, loss surface, and smoothness. ",This paper studies the generalization properties of deep neural networks from a functional point of view. The authors show that the smoothness of the functional approximation of the loss surface and the initialization of the network is important for generalization. They show that this is the case for over-parameterized networks and for massively overparametrized networks. They also show that a smooth initialization is necessary and sufficient for good generalization performance.  ,"This paper studies the smoothness of the functional approximation of neural networks. The authors show that the initialization of a neural network is smooth if the loss surface is smooth. They show that this smoothness is related to the generalization ability of the network. They also show that if the initializations are smooth, then the network generalizes well. "
22401,SP:7225825e353b711a7d023f706fafe5e17e4e2fb2,"Generative Adversarial Network ( GAN ) USED-FOR image - to - image translation problem. imbalance problem FEATURE-OF GAN - based methods. mode collapse CONJUNCTION diminished gradients. diminished gradients CONJUNCTION mode collapse. generator CONJUNCTION discriminator. discriminator CONJUNCTION generator. relative model capacities FEATURE-OF generator. relative model capacities FEATURE-OF discriminator. attention mechanism USED-FOR GuideGAN. it USED-FOR attention map. attention mechanism USED-FOR discriminator. attention map USED-FOR generator. image transfer tasks EVALUATE-FOR GuideGAN framework. Task are supervised and unsupervised manner, and prediction. Generic is approach. "," image-to-image translation is an important problem in machine translation. This paper proposes a method to address the imbalance problem of GAN-based methods. The proposed method is based on the observation that GANs suffer from mode collapse and diminished gradients. To address this problem, the authors propose to use the attention mechanism of the discriminator to map the input image to the attention map of the generator. Experiments are conducted on image transfer tasks to show the effectiveness of the proposed method. ","This paper proposes a new GAN-based approach for image-to-image translation. The proposed method is based on the idea that the attention map of the discriminator can be used to learn the relative model capacity of the generator and discriminator. The attention map is learned in a supervised manner and unsupervised manner, and prediction is made using the generator. Experiments are conducted on a variety of image transfer tasks to demonstrate the effectiveness of the proposed method."
22410,SP:41c089ba65393174dae1dc136f79030a0a4fc532,"attention layers CONJUNCTION hypernetworks. hypernetworks CONJUNCTION attention layers. hypernetworks CONJUNCTION dynamic convolutions. dynamic convolutions CONJUNCTION hypernetworks. multiplicative interaction USED-FOR neural network architectural motifs. gating CONJUNCTION attention layers. attention layers CONJUNCTION gating. dynamic convolutions HYPONYM-OF neural network architectural motifs. hypernetworks HYPONYM-OF neural network architectural motifs. attention layers HYPONYM-OF neural network architectural motifs. gating HYPONYM-OF neural network architectural motifs. Multiplicative interaction layers HYPONYM-OF primitive operations. multiplicative interactions USED-FOR inductive bias. multiplicative interactions USED-FOR neural network architectures. them USED-FOR multiplicative interactions. Generic are layers, and they. Method is neural networks. OtherScientificTerm are conditional computation, and concatenation operation. ",This paper studies the effect of multiplicative interaction layers on the inductive bias of neural networks. The authors show that multiplicative interactions can be viewed as primitive operations that can be used in conjunction with other primitive operations such as concatenation and gating. They show that the multiplicative layers can be seen as a special case of concatenations of primitive operations. They also show that adding a multiplicative layer can improve the performance of the network.   ,"This paper studies the effect of multiplicative interaction layers on the inductive bias of neural networks. The authors show that the multiplicative interactions can be used to improve the performance of neural network architectures. They also show that these layers can be combined with other primitive operations such as gating, hypernetworks, and dynamic convolutions. They provide a theoretical analysis of the effects of these layers and show that they can lead to better performance."
22419,SP:5144391584e6d3825e12684b7c053e4e282cff2b,"algorithm USED-FOR batch active learning. deep neural network models USED-FOR algorithm. deep neural network models USED-FOR batch active learning. predictive uncertainty CONJUNCTION sample diversity. sample diversity CONJUNCTION predictive uncertainty. algorithm USED-FOR Batch Active learning. predictive uncertainty PART-OF strategy. Diverse Gradient Embeddings ( BADGE ) USED-FOR Batch Active learning. uncertainty CONJUNCTION diversity. diversity CONJUNCTION uncertainty. diversity EVALUATE-FOR BADGE. uncertainty EVALUATE-FOR BADGE. it USED-FOR real world active learning problems. approaches COMPARE BADGE. BADGE COMPARE approaches. approaches USED-FOR batch sizes. BADGE USED-FOR real world active learning problems. OtherScientificTerm are hallucinated gradient space, and hand - tuned hyperparameters. ","This paper proposes a new method for batch active learning. The proposed method is based on the idea of diverse gradient embeddings (DGE), which aims to improve the uncertainty and diversity in the training process. The main idea is to use a mixture of gradient estimators to estimate the uncertainty in the gradient space, which is then used as a regularization term to encourage the gradient to be sampled from a diverse set of samples. Experiments on synthetic and real-world datasets show the effectiveness of the proposed method.   ","This paper proposes a new method for batch active learning, called Diverse Gradient Embeddings (BADGE), which aims to improve the diversity and uncertainty of the training data. The main idea of BADGE is to use a hallucinated gradient space, where the data points in the gradient space are hallucinated. The authors show that BADGE can improve the uncertainty and sample diversity of the data. They also show that the proposed method can be applied to real-world active learning problems.  "
22428,SP:ce6023b1e6bf45b071a6f5457b2575425ae03366,"models USED-FOR decision making parameters. obscure feature extraction CONJUNCTION transformation process. transformation process CONJUNCTION obscure feature extraction. complex architectures CONJUNCTION obscure feature extraction. obscure feature extraction CONJUNCTION complex architectures. non - linearity FEATURE-OF activation functions. low level features CONJUNCTION high level features. high level features CONJUNCTION low level features. low level features FEATURE-OF hidden layer. high level features FEATURE-OF hidden layer. feature leveling architecture USED-FOR low level features. low level features CONJUNCTION high level features. high level features CONJUNCTION low level features. GLM layer PART-OF architecture. GLM layer USED-FOR feature leveling architecture. per - layer basis USED-FOR high level features. models COMPARE main - stream architectures. main - stream architectures COMPARE models. Method are Self - explaining models, General Linear Models ( GLMs ), deep neural networks ( DNNs ), and deep architectures. Task is model reasoning process. OtherScientificTerm is model weights. ","This paper proposes a new self-explaining model for deep neural networks (DNNs) based on General Linear Models (GLMs). The main contribution of the paper is to introduce a feature-leveling architecture for low-level features, which is based on a mixture of low level features and high level features extracted from the hidden layer of the DNN. The authors show that the proposed method outperforms the state-of-the-art DNNs on ImageNet and CIFAR-10.","This paper studies the problem of self-explaining deep neural networks (DNNs) and general linear models (GLMs) with non-linearity. The authors propose a feature-leveling architecture for the GLM layer, which uses a per-layer basis for high level features and per-level features for low level features. They show that the feature leveling can be used to improve the performance of DNNs and GLMs. They also provide a theoretical analysis of the effect of the feature leveling architecture."
22437,SP:b70ceead1bf6c7dc684c74501716e7012b891022,"gradient cost FEATURE-OF softmax regression. uniform negative sampling USED-FOR scalable softmax approximation. training method USED-FOR gradient signal. adversarial model USED-FOR data distribution. adversarial model USED-FOR negative samples. negative samples USED-FOR training method. adversarial sampling mechanism USED-FOR negative samples. adversarial sampling USED-FOR gradient variance. large scale data sets EVALUATE-FOR competitive baselines. adversarial sampling mechanism USED-FOR gradient updates. training time EVALUATE-FOR competitive baselines. non - uniform sampling USED-FOR bias. Method are classifier, and extreme classification. Task is technology. OtherScientificTerm is slow convergence. Metric is signal - to - noise ratio. ","This paper proposes a new adversarial training method for softmax regression with uniform negative sampling. The proposed method is based on adversarial sampling, where the adversarial model is trained to predict the data distribution and the negative samples are used to train the classifier. The authors show that the proposed method can achieve competitive performance on large scale data sets. ","This paper proposes a method to reduce the gradient cost of softmax regression with uniform negative sampling. The method is based on an adversarial model that is trained on the data distribution, where the negative samples are sampled from the training data distribution. The authors show that the proposed method is competitive with the state-of-the-art in terms of training time and convergence rate. "
22446,SP:29b52fee83309268d9864f3b1fc3617948577d41,approach USED-FOR efficient exploration. lowdimensional encoding of the environment USED-FOR approach. modelbased and model - free objectives USED-FOR lowdimensional encoding of the environment. weighted distance of nearest neighbors USED-FOR novelty. intrinsic rewards USED-FOR novelty. low dimensional representational space FEATURE-OF weighted distance of nearest neighbors. weighted distance of nearest neighbors USED-FOR intrinsic rewards. intrinsic rewards USED-FOR approach. intrinsic rewards USED-FOR sample - efficient exploration. intrinsic rewards USED-FOR planning routines. representational space FEATURE-OF planning routines. planning routines USED-FOR sample - efficient exploration. exploration approach COMPARE baselines. baselines COMPARE exploration approach. maze tasks CONJUNCTION control problem. control problem CONJUNCTION maze tasks. control problem EVALUATE-FOR approach. maze tasks EVALUATE-FOR approach. Metric is model accuracy. ,"This paper proposes a method for efficient exploration in low-dimensional environments. The proposed method is based on a low-dimension encoding of the environment using a weighted distance of nearest neighbors, which is used as an intrinsic reward to encourage exploration in the low dimensional space. The method is evaluated on a number of maze tasks and a control problem.   ",This paper proposes a low-dimensional encoding of the environment for efficient exploration in a model-based and model-free setting. The authors propose to use a weighted distance of nearest neighbors as a low dimensional representational space to encode the environment. They show that this representation space can be used for sample-efficient exploration. They also show that their method can be applied to the control problem in a maze task.
22455,SP:257c98dc1a9f3efcbf9544d9ee2ff524b000543d,learning model USED-FOR few - shot classification. it USED-FOR out - of - distribution inputs. few - shot classification CONJUNCTION out - of - distribution detection. out - of - distribution detection CONJUNCTION few - shot classification. tasks USED-FOR outof - distribution detection. few - shot setting FEATURE-OF outof - distribution detection. few - shot classification datasets USED-FOR tasks. few - shot classification datasets USED-FOR benchmark datasets. methods USED-FOR task. benchmark datasets EVALUATE-FOR metrics. benchmark datasets EVALUATE-FOR baseline out - of - distribution detection. metrics USED-FOR baseline out - of - distribution detection. ,"This paper proposes to use few-shot classification as an out-of-distribution detection task. The authors propose two metrics to evaluate the performance of the out of distribution detection. The first metric is based on the number of samples in the training set, while the second is a weighted sum of the average distance between the training and test samples. The proposed metrics are evaluated on three benchmark datasets and compared with several baselines.","This paper proposes a new few-shot classification task, out-of-distribution detection, which is an important problem in the few shot classification setting. The authors propose a new benchmark dataset for out of distribution detection and show that the proposed method outperforms the state of the art in terms of performance on the benchmark datasets. They also provide a set of metrics to evaluate the performance of the proposed methods."
22464,SP:a3632b773143dfb3a8f104c6b658dfa1167d155b,question - answering CONJUNCTION natural language inference. natural language inference CONJUNCTION question - answering. Undirected neural sequence models USED-FOR discriminative natural language understanding tasks. BERT HYPONYM-OF discriminative natural language understanding tasks. BERT HYPONYM-OF Undirected neural sequence models. natural language inference HYPONYM-OF discriminative natural language understanding tasks. question - answering HYPONYM-OF discriminative natural language understanding tasks. monotonic generation USED-FOR directed sequence models. models USED-FOR generating sequences. decoding PART-OF directed and undirected models. decoding PART-OF generalized model of sequence generation. framework USED-FOR generation. framework USED-FOR neural sequence models. refinement - based non - autoregressive models HYPONYM-OF neural sequence models. autoregressive HYPONYM-OF neural sequence models. decoding algorithms USED-FOR directed sequence models. decoding algorithms USED-FOR undirected models. decoding strategies USED-FOR cross - lingual masked translation model. framework USED-FOR undirected sequence models. approach USED-FOR constant - time translation. approach COMPARE linear - time translation. linear - time translation COMPARE approach. constant - time translation COMPARE linear - time translation. linear - time translation COMPARE constant - time translation. model USED-FOR linear - time translation. Material is WMT’14 English - German translation. Method is autoregressive model. ,"This paper proposes a method to improve the translation performance of directed and undirected neural sequence models on WMT’14 English-German translation tasks. The proposed method is based on a cross-lingual masked translation model, which is used to model the translation from one language to the other. The method is evaluated on the WMT14 German-English translation task, where it is shown that the proposed method outperforms the state-of-the-art methods.  ",This paper proposes a new method for translation from English-German translation to German-English translation. The main idea is to use a cross-lingual masked translation model to translate from English to German. The proposed method is based on a generalized model of sequence generation. The method is evaluated on the WMT’14 English- German translation task.  
22473,SP:eca5e2be9831dfb79c4f5e633cbfadcfd2e00eb1,"real scenes USED-FOR MEs recognition. two - stage approach USED-FOR LaTeX sequence. printed mathematical expression image USED-FOR two - stage approach. method USED-FOR math symbols. object detection algorithm USED-FOR method. object detection algorithm USED-FOR math symbols. seq2seq model USED-FOR LaTeX sequences. attention mechanism USED-FOR seq2seq model. position information USED-FOR math symbols. two - stage method COMPARE end - to - end method. end - to - end method COMPARE two - stage method. ExpRate(expression recognition rate ) EVALUATE-FOR model. model COMPARE end - to - end model. end - to - end model COMPARE model. ExpRate(expression recognition rate ) EVALUATE-FOR end - to - end model. Task are mathematical expressions ( MEs ) recognition, and detection of mathematical symbols. Method is neutral network. OtherScientificTerm are mathematical symbols, and mathematical formulas. Metric are recognition accuracy, and generalization ability. ","This paper proposes a two-stage approach for the detection of mathematical symbols in LaTeX sequences. The first stage uses a seq2seq model to predict the location of the symbols in the LaTeX sequence, and the second stage uses an object detection algorithm to detect the objects in the sequence. The proposed method achieves state-of-the-art performance on the task of mathematical expression recognition. ","This paper proposes a two-stage approach for LaTeX-based mathematical expression recognition. The main idea is to use a sequence-to-sequence model (seq2seq) to generate a sequence of LaTeX sequences, which is then used to generate an image of the LaTeX sequence, and then use an object detection algorithm to identify the symbols in the sequence. The proposed method is evaluated on a variety of real-world datasets, and it is shown to be competitive with the state-of-the-art. "
22482,SP:923fee8623da1569a7f54a57b4b326f29440b4c0,"memory footprint FEATURE-OF convolutional network architectures. loss reconstruction error FEATURE-OF in - domain inputs. it USED-FOR in - domain inputs. it USED-FOR loss reconstruction error. bytealigned codebooks USED-FOR compressed weights. method USED-FOR inference. unlabelled data USED-FOR quantization time. CPU USED-FOR inference. bytealigned codebooks USED-FOR method. unlabelled data USED-FOR method. bytealigned codebooks USED-FOR inference. top-1 accuracy EVALUATE-FOR approach. Method are vector quantization method, ResNet-50 model, and Mask R - CNN. OtherScientificTerm is memory size. Metric is compression factor. Task is ImageNet object classification. ", in ImageNet object classification task. The paper proposes a vector quantization method to reduce the memory footprint of convolutional network architectures. The proposed method is based on the idea that the loss reconstruction error of in-domain inputs can be used to estimate the in- domain inputs. The method is evaluated on ResNet-50 model and Mask R-CNN and achieves the top-1 accuracy.,"This paper proposes a method to reduce the memory footprint of convolutional neural networks by quantizing the in-domain inputs of a ResNet-50 model. The key idea is to use byte-aligned codebooks to compress the weights of the weights in the encoder-decoder network, and then use the compressed weights to estimate the loss reconstruction error of the network. The proposed method is evaluated on ImageNet object classification tasks, and it achieves state-of-the-art performance. "
22491,SP:74850ad70241948f93fed95ba1f0ac11360437c1,Tensor - Product Representations USED-FOR explicit representation of relation structure. Tensor - Product Representations PART-OF Transformer. free - form math wordproblems FEATURE-OF Mathematics Dataset. Mathematics Dataset EVALUATE-FOR TensorProduct Transformer ( TP - Transformer ). TP - Attention HYPONYM-OF attention mechanism. attention USED-FOR ambiguities. TP - Transformer ’s attention maps USED-FOR it. Generic is model. Task is representation - building. Method is Pretrained models. ,"This paper proposes to use Tensor-Product Representations (TPR) to learn the representation of relation structure in a Transformer-based model. The authors show that TPR can be used to learn representations for free-form math wordproblems in the Mathematics Dataset. They also propose a novel attention mechanism, called TP-Attention, to capture the ambiguities in relation structure. The experiments show that the proposed method outperforms the state-of-the-art Transformer models on the mathematics dataset.","This paper proposes a Transformer-based model for the problem of representation-building in mathematics. The model is based on the TensorProduct Transformer (TP-Transformer) framework. The authors propose a new attention mechanism, called TP-Attention, which maps the attention maps of the Transformer’s attention maps to the original attention maps. The attention maps are then used to represent the relation structure of a word problem. The proposed method is evaluated on the Mathematics Dataset.   "
22500,SP:d319df820c6630c409fab32097652a083e8f53ea,"identically distributed training and test sets EVALUATE-FOR Deep artificial neural networks. training and test accuracies EVALUATE-FOR Deep artificial neural networks. training and test sets COMPARE empirical sample set. empirical sample set COMPARE training and test sets. real - world input samples PART-OF empirical sample set. procedure USED-FOR source code. learning algorithm USED-FOR source code. procedure USED-FOR learning algorithm. Kolmogorov complexity USED-FOR universal cognitive similarity metric. information distance HYPONYM-OF universal cognitive similarity metric. optimization problem USED-FOR classification function. condition USED-FOR optimization problem. features USED-FOR classifier. empirical sample set FEATURE-OF training and test sets. model COMPARE model. model COMPARE model. model USED-FOR corruptions. corruptions CONJUNCTION adversarial perturbations. adversarial perturbations CONJUNCTION corruptions. corrupted or perturbed input features PART-OF empirical sample set. model USED-FOR adversarial perturbations. uncoded input features USED-FOR model. uncoded input features USED-FOR model. projected gradient descent USED-FOR adversarial perturbations. encoded input features USED-FOR model. Gaussian and shot noise HYPONYM-OF corruptions. Task are generalization, inference, and image classification. Metric is training and inference accuracies. OtherScientificTerm is channel codes. ",This paper studies the problem of training and inference of deep neural networks with corrupted or perturbed input features from source code. The authors propose to use the information distance as a universal cognitive similarity metric to measure the similarity between the training and test sets. They show that the proposed method is able to learn from corrupted and perturbed features in the empirical sample set. They also show that adversarial perturbations can be computed using projected gradient descent.,This paper studies the problem of identifying the source code of a classifier trained on a training and test set. The main contribution of the paper is to propose a universal cognitive similarity metric to measure the similarity between the source and test sets. This metric is based on the Kolmogorov complexity of the information distance between the input features and the classifier. The paper also proposes a learning algorithm to find the source codes of the test and training sets. The empirical results show that the proposed method outperforms the state of the art. 
22509,SP:b8e86f5e89330d81ba4967a7ed2dbfb56375d8a0,Deep Graph Neural Networks ( GNNs ) USED-FOR graph - based regression tasks. Deep Graph Neural Networks ( GNNs ) USED-FOR graph classification. graph classification CONJUNCTION graph - based regression tasks. graph - based regression tasks CONJUNCTION graph classification. graph pooling USED-FOR GNNs. GNNs USED-FOR graphs. HaarPooling HYPONYM-OF graph pooling operation. compressive Haar transforms USED-FOR graph pooling operation. sequential clusterings USED-FOR HaarPooling. compressive Haar basis USED-FOR clustering. compressive Haar basis USED-FOR pooling layer. HaarPooling USED-FOR fine detail information. compressive Haar transforms USED-FOR HaarPooling. synthesis of nodes USED-FOR HaarPooling. compressive Haar transforms USED-FOR fine detail information. transforms USED-FOR structure information. sparsity of the Haar basis USED-FOR HaarPooling. linear complexity FEATURE-OF HaarPooling. HaarPooling CONJUNCTION graph convolution layers. graph convolution layers CONJUNCTION HaarPooling. diverse graph classification problems EVALUATE-FOR GNN. graph convolution layers USED-FOR GNN. HaarPooling USED-FOR GNN. Generic is tasks. OtherScientificTerm is cluster. ,"ar transforms are used to compute the Haar basis for the pooling layer in GNNs. The authors show that the compressive Haar transforms can be used to generate clusterings of nodes in the graph, which are then used for aggregation. The proposed method is shown to be computationally efficient with a linear complexity. Experiments on several graph classification tasks show the effectiveness of the proposed method.  ","This paper proposes a new graph pooling method, called HaarPooling, for graph classification and regression tasks. The main idea is to use a compressive Haar basis for the pooling layer, which is then used to cluster the nodes in the graph. The authors show that the proposed method can be applied to a variety of graph classification tasks, including graph classification, graph-based regression, and graph classification. They also show that it can be used to improve the performance of graph convolutional layers. "
22518,SP:17bea301d6718ef5f28864dd2445552b3cf65eeb,"Point clouds USED-FOR 3D objects. encoder networks USED-FOR semantics of their input point clouds. fully - connected networks USED-FOR shape representations. fully - connected networks USED-FOR point - cloud decoders. decoder architectures USED-FOR semantics of variable sized point clouds. sample - based point - cloud decoders USED-FOR shape representation. Metric is precision. OtherScientificTerm are point feature distribution, and sampled features. Method are sample - based decoder architectures, and feedforward architectures. ","This paper proposes a novel point cloud decoder architecture for 3D object detection. The proposed method is based on sampling from the point cloud distribution, where each point in the distribution is represented by a set of points sampled from the distribution of the input point clouds. The authors show that the proposed method can learn the semantics of the point clouds in a fully-connected network, which can be used to learn the shape representation of the object. ",This paper proposes a new way to learn the shape representation of 3D point clouds. The key idea is to use a sample-based point-cloud decoder architecture that learns the shape representations of variable-sized point clouds by sampling from the feature distribution of the input point cloud. The authors show that this can be achieved by using a fully-connected network and a feedforward architecture. The paper also provides a theoretical analysis of the performance of the proposed method.
22527,SP:51d826ead5d1d9cb89d493ce4c39728651bbc57b,"controlled synthetic noise USED-FOR deep learning. controlled noise levels FEATURE-OF realworld noisy labels. Deep Neural Networks ( DNNs ) USED-FOR real - world noise. noisy data EVALUATE-FOR ImageNet architectures. Robust learning methods USED-FOR synthetic noise. Robust learning methods USED-FOR real - world noise. OtherScientificTerm are noise levels, networks, and Real - world noise. Material are benchmark of realworld noisy labels, and real - world noisy data. Method are DNNs, and robust DNN methods. ",This paper studies the problem of robustness of deep neural networks to real-world noisy labels. The authors propose a new benchmark for real world noisy labels and show that robust neural networks are more sensitive to real world noise compared to synthetic labels. They also show that deep networks trained with ImageNet architectures are less sensitive to noisy labels compared to networks trained without ImageNet. They further show that networks trained on ImageNet with noisy labels are more robust to synthetic and real world labels.,This paper studies the problem of robustness of deep neural networks to real-world noisy labels. The authors show that robust neural networks are more sensitive to controlled noise levels than robust ones. They also show that a robust network is more sensitive than a robust one to synthetic noise levels. They further show that the robustness can be improved by training a robust neural network that is sensitive to synthetic data.
22536,SP:9873f78fb2821afdbb5551700e6ab6a0e8bcb9f0,"pain - staking human supervision USED-FOR labeled data. rule - exemplar method USED-FOR collecting human supervision. it USED-FOR learning. training algorithm USED-FOR rules. coverage and label variables FEATURE-OF soft implication loss. latent coverage variables USED-FOR training algorithm. soft implication loss USED-FOR model. latent coverage variables USED-FOR rules. model USED-FOR inference. denoised rules CONJUNCTION model. model CONJUNCTION denoised rules. denoised rules USED-FOR inference. coupled rule - exemplar supervision USED-FOR denoising rules. algorithm COMPARE methods. methods COMPARE algorithm. tasks EVALUATE-FOR algorithm. tasks EVALUATE-FOR methods. clean and noisy supervision USED-FOR algorithm. clean and noisy supervision USED-FOR methods. OtherScientificTerm are human supervision, and supervision. ","This paper proposes a rule-exemplar method for collecting human supervision for learning from labeled data. The proposed method is based on the idea of learning a set of denoised rules that can be used as exemplars for training and inference. The main contribution of this paper is to use a soft-implicit loss to learn the coverage and label variables of the labeled data, and then use the learned rules as examples to train the model.   ","This paper proposes a rule-exemplar method to collect human supervision for pain-staking human supervision on labeled data. The training algorithm is based on a soft-implicit loss, where the coverage and label variables of the data are modeled as a set of latent coverage variables, and the model is trained with a soft implication loss. The proposed method is evaluated on a variety of tasks, and it is shown that the proposed method outperforms the state-of-the-art on both clean and noisy supervision."
22545,SP:6f2c656dbb7629f652a4291d6971625184d8118b,"Graph neural networks ( GNNs ) HYPONYM-OF deep models. memory layer USED-FOR GNNs. GNNs USED-FOR node representations. memory layer USED-FOR node representations. graph memory network ( GMN ) USED-FOR hierarchical graph representations. memory - based GNN ( MemGNN ) CONJUNCTION graph memory network ( GMN ). graph memory network ( GMN ) CONJUNCTION memory - based GNN ( MemGNN ). networks USED-FOR hierarchical graph representations. graph memory network ( GMN ) HYPONYM-OF networks. memory - based GNN ( MemGNN ) HYPONYM-OF networks. layer USED-FOR networks. graph classification and regression benchmarks EVALUATE-FOR models. chemical features PART-OF molecule data. chemical features FEATURE-OF representations. OtherScientificTerm are graphs, and graph. ",This paper proposes a novel memory-based graph memory network (MemGNN) and graph memory networks (GMN) for graph classification and regression tasks. The proposed MemGNN and GMN are based on the idea that the memory layer is used to store the node representations in the graph. The authors show that the proposed models outperform the state-of-the-art GNNs on several graph classification tasks.  ,"This paper proposes a new memory-based graph memory network (MemGNN) for graph classification and regression tasks. The authors propose a new graph memory layer for graph representation learning, which can be used to store the graph representation of a graph. The proposed MemGNN can be combined with graph memory networks (GNNs) to learn a hierarchical graph representation. Experiments are conducted on synthetic and real-world datasets.  "
22554,SP:81bc52d734c86975d741b6482d65ca71a9d81620,"initial parameter values USED-FOR gradient - based optimization. convergence times CONJUNCTION model. model CONJUNCTION convergence times. gradient - based optimization USED-FOR deep neural networks. initialization USED-FOR deep linear networks. convergence COMPARE Gaussian initialization. Gaussian initialization COMPARE convergence. iid weights USED-FOR Gaussian initialization. Gaussian initializations USED-FOR convergence. initialization USED-FOR learning. dynamical isometry USED-FOR deep non - linear networks. Method are deep learning systems, initialization schemes, deep networks, and orthogonal initializations. OtherScientificTerm are orthogonal group, and global minimum. ","This paper studies the convergence of deep neural networks with Gaussian initializations. The authors show that for deep linear networks with iid weights, the convergence is faster than for deep non-linear networks with random initialization. They show that this is due to the existence of a global minimum of the network, which is a function of the number of initializations, and that this global minimum is the product of the convergence rate of the networks with and without random initialization, and the rate of initialization with randomness.    The authors also show that the convergence rates of deep networks with non-Gaussian initialization are faster than those of linear networks when the weights are iid. ","This paper studies the convergence of deep neural networks with orthogonal initializations (i.e., the number of initializations per layer of a neural network). The authors show that the convergence rate of deep linear networks with Gaussian initializations is bounded by the global minimum of the group of initializers. They also show that for deep non-linear networks, they can converge to a global minimum in terms of the global convergence rate. The authors also provide a theoretical analysis of the convergence rates of deep networks with iid weights. "
22563,SP:9f5d95fc89c2f0d59d04838aa180f3db67997dfa,"quantization methods USED-FOR deep neural networks. coarse quantization USED-FOR layers. 2 - bit quantization HYPONYM-OF high rate compression. additivity property FEATURE-OF deep neural networks. method USED-FOR optimization problem. Lagrangian Formulation USED-FOR method. joint framework USED-FOR optimal bit allocation problem. Lagrangian Formulation USED-FOR optimization problem. deep neural networks EVALUATE-FOR method. It USED-FOR deep CNN ResNet-50. accuracy loss EVALUATE-FOR It. Metric is accuracy. Generic is methods. OtherScientificTerm are equal bit rate, quantization, and additivity of output error. Task is deep CNNs compression. Method is deep CNNs. ",This paper proposes a new quantization method for deep neural networks. The proposed method is based on a Lagrangian formulation of the quantization problem. Theoretical analysis is provided to show that the proposed method has the additivity property. Experiments are conducted to show the effectiveness of the method.  ,"This paper proposes a new quantization method for high-rate compression of deep neural networks. The key idea is to use a Lagrangian formulation of the bit allocation problem to find the optimal bit allocation for each layer of the network. The authors show that the quantization of a deep neural network is additivity-dependent, which is a property of the output error of a neural network. They propose a joint framework to solve the optimization problem of quantization and show that their method achieves the best bit allocation in terms of accuracy. "
22572,SP:7191d7b217a12b1bf9c47d790896a8227c14cc3d,"metric USED-FOR convergence. Wasserstein distance USED-FOR Wasserstein GAN ( WGAN ). auto - encoders CONJUNCTION WGANs. WGANs CONJUNCTION auto - encoders. framework USED-FOR auto - encoders. framework USED-FOR WGANs. encoder network CONJUNCTION generative network. generative network CONJUNCTION encoder network. encoder network PART-OF iWGAN. generative network PART-OF iWGAN. iterative primal dual optimization process USED-FOR generative network. iterative primal dual optimization process USED-FOR iWGAN. generalization error bound FEATURE-OF iWGANs. maximum likelihood estimation USED-FOR model. iWGAN COMPARE autoencoder GANs. autoencoder GANs COMPARE iWGAN. stopping criteria FEATURE-OF iWGAN. model USED-FOR convergence. model USED-FOR mode collapse. measurement of quality check EVALUATE-FOR model. benchmark datasets EVALUATE-FOR state - of - the - art. benchmark datasets EVALUATE-FOR iWGANs. Method are Generative Adversarial Networks ( GANs ), minmax two - player training of GANs, and inference WGAN ( iWGAN ) model. OtherScientificTerm is unstable training. ","This paper proposes a new method for training generative adversarial networks (GANs) with auto-encoders and WGANs. The main idea is to use the Wasserstein distance between the encoder and the generative network as a metric to measure the convergence of the two networks. The proposed method is based on a two-player training of GANs, where the autoencoder is used to train the WGAN and the generator is used for inference. The method is evaluated on a variety of benchmark datasets and achieves state-of-the-art performance.",This paper proposes a new method for training a generative adversarial network (GAN) with auto-encoders and WGANs. The main idea is to use the Wasserstein distance between the encoder and the generative network as a metric to measure the generalization error of a GAN. The authors show that the proposed method can achieve better generalization performance than the state-of-the-art GANs on a variety of benchmark datasets.
22581,SP:cca6ae14fd0dd12352855e594acf7f3263bb1f24,coreference CONJUNCTION NLP. NLP CONJUNCTION coreference. Crowdsourcing COMPARE expert annotation. expert annotation COMPARE Crowdsourcing. classification tasks USED-FOR annotation. adjudication USED-FOR crowdsourcing. MPA USED-FOR sparsity. sparsity FEATURE-OF crowdsourcing environments. stick breaking process USED-FOR nonparametric partially pooled structure. large - scale crowdsourced anaphora dataset EVALUATE-FOR model. crowdsourcing setups FEATURE-OF model. annotation tasks USED-FOR classification. model USED-FOR annotation tasks. Task is anaphoric annotation. OtherScientificTerm is coreference chains. Generic is it. ,". The paper proposes a method for crowdsourcing anaphora. The method is based on the idea of MPA, which is a nonparametric partially pooled structure. The proposed method is evaluated on a large-scale crowdsourcing dataset. The results show that the proposed method outperforms the state-of-the-art crowdsourcing methods.  ","This paper proposes a method for crowdsourcing anaphoric annotation. The authors propose a nonparametric partially pooled structure for the coreference chains, which they call ""stick breaking"" (MPA). They show that MPA can be used to reduce sparsity in crowdsourcing environments. They also show that their method can be applied to a large-scale crowdsourced anaphora dataset. "
22597,SP:4295cae4a56a02eb21c486408c1bf37a7483cb49,"Exploration FEATURE-OF sparse reward reinforcement learning. intrinsic motivation USED-FOR sparse extrinsic reward signal. drives USED-FOR stabilize learning. exploration CONJUNCTION stabilize learning. stabilize learning CONJUNCTION exploration. successor feature control ( SFC ) HYPONYM-OF intrinsic reward. It COMPARE methods. methods COMPARE It. local information USED-FOR intrinsic motivation. statistics over complete trajectories USED-FOR It. local information USED-FOR methods. DeepMind Lab CONJUNCTION DeepMind Control Suite. DeepMind Control Suite CONJUNCTION DeepMind Lab. VizDoom CONJUNCTION DeepMind Lab. DeepMind Lab CONJUNCTION VizDoom. environments EVALUATE-FOR scheduled intrinsic drive ( SID ) agent. pure visual inputs USED-FOR environments. pure visual inputs USED-FOR scheduled intrinsic drive ( SID ) agent. DeepMind Lab HYPONYM-OF pure visual inputs. VizDoom HYPONYM-OF environments. DeepMind Control Suite HYPONYM-OF environments. DeepMind Lab HYPONYM-OF environments. exploration efficiency EVALUATE-FOR SFC. Generic is signals. OtherScientificTerm are bonus rewards, and intrinsic drives. Method are mixture policy, and intrinsic and extrinsic task policies. ","This paper proposes successor feature control (SFC), an intrinsic reward for sparse reward reinforcement learning. The intrinsic reward is a mixture of rewards from the intrinsic and extrinsic task policies. The authors show that the intrinsic reward can be used to improve exploration efficiency and stabilize learning in sparse reward environments. The proposed method is evaluated on the DeepMind Control Suite and VizDoom environments. ","This paper proposes successor feature control (SFC), a new method for sparse reward reinforcement learning with intrinsic and extrinsic reward signals. The intrinsic reward is learned using a mixture policy, where the intrinsic reward can be used to guide exploration and stabilize learning. The authors show that the proposed method outperforms the state-of-the-art in terms of exploration efficiency, stability, and exploration efficiency. "
22613,SP:9fa22eb03a79bce0fc1c8e84ae8640e010701eca,"video - sentence pairs USED-FOR model. visual and language representations USED-FOR latent correspondence. multi - level co - attention mechanism USED-FOR multimodal representations. Frame - By - Word interaction module CONJUNCTION Word - Conditioned Visual Graph ( WCVG ). Word - Conditioned Visual Graph ( WCVG ) CONJUNCTION Frame - By - Word interaction module. Word - Conditioned Visual Graph ( WCVG ) PART-OF mechanism. Frame - By - Word interaction module PART-OF mechanism. positional encodings USED-FOR visual - semantic representations. positional encodings USED-FOR Transformers. iterative message - passing USED-FOR positional encodings. positional encodings PART-OF approach. iterative message - passing USED-FOR visual - semantic representations. wMAN model COMPARE weakly - supervised method. weakly - supervised method COMPARE wMAN model. DiDeMo and Charades - STA datasets EVALUATE-FOR representations. Recall@1 accuracy metric EVALUATE-FOR wMAN model. Task is weakly - supervised video moment retrieval. OtherScientificTerm are temporal annotations, and temporal sequence. Material is DiDeMo dataset. ",This paper proposes a method for weakly supervised video moment retrieval from video-sentence pairs. The proposed method is based on a frame-by-word interaction module and a word-conditioned visual graph (WCVG) to learn a latent representation of the video and language representations.   The method is evaluated on the DiDeMo and Charades-STA datasets and achieves state-of-the-art performance. ,"This paper proposes a method for weakly-supervised video moment retrieval from video-sentence pairs. The proposed method is based on a multi-level co-attention mechanism, which is used to capture the latent correspondence between visual and language representations. The model is trained with a frame-by-word interaction module and a Word-conditioned visual graph (WCVG).  The proposed model is evaluated on the DiDeMo dataset and Charades-STA dataset."
22629,SP:27ac670353f34ee7a23bb7622f80c1dfbc0985e0,"image - based rendering CONJUNCTION GAN - based image synthesis. GAN - based image synthesis CONJUNCTION image - based rendering. image - based rendering USED-FOR learned image - guided rendering technique. GAN - based image synthesis USED-FOR learned image - guided rendering technique. virtual showrooms CONJUNCTION virtual tours. virtual tours CONJUNCTION virtual showrooms. virtual tours CONJUNCTION digital inspection of historical artifacts. digital inspection of historical artifacts CONJUNCTION virtual tours. digital inspection of historical artifacts HYPONYM-OF virtual and augmented reality applications. virtual tours HYPONYM-OF virtual and augmented reality applications. virtual showrooms HYPONYM-OF virtual and augmented reality applications. handling of view - dependent effects PART-OF work. object - specific deep neural network USED-FOR view - dependent appearance. video USED-FOR proxy geometry. multi - view stereo USED-FOR proxy geometry. diffuse surfaces USED-FOR warping. specular highlights HYPONYM-OF view - dependent effects. deep neural network USED-FOR view - dependent effects. EffectsNet HYPONYM-OF deep neural network. pipeline USED-FOR view - dependent effects. composition network USED-FOR photo - realistic results. image - guided approach USED-FOR network. it USED-FOR appearance of captured images. real data EVALUATE-FOR approach. Generic are method, and estimations. OtherScientificTerm are 3D proxy, appearance, and diffuse images. ",This paper proposes a method for image-guided rendering based on image-based rendering and GAN-based image synthesis. The main idea is to use a 3D proxy to capture the appearance of captured images and then use a composition network to produce photo-realistic results. The proposed method is evaluated on synthetic and real-world data. ,"This paper proposes an image-guided rendering method for image-based rendering. The method is based on GAN-based image synthesis and a deep neural network. The key idea is to use a 3D proxy to capture the 3D appearance of captured images, which is then used to estimate the appearance of the rendered image. The proxy is generated by a multi-view stereo stereo, and the proxy geometry is learned by a combination of video and a composition network. Experiments show that the proposed method achieves better results than the state-of-the-art."
22645,SP:257d124367b1da9a595dc11a9df750d6bade298e,"sparse representation of model uncertainty USED-FOR deep neural networks ( DNNs ). diagonal correction of the Kronecker - factored eigenbasis PART-OF scalable Laplace Approximation scheme. scalable Laplace Approximation scheme USED-FOR model uncertainty. operation USED-FOR full Bayesian analysis. low - rank approximation USED-FOR spectral sparsity. spectral sparsity FEATURE-OF DNNs. low - rank approximation USED-FOR eigenbasis. Methods USED-FOR sparsification. Methods USED-FOR memory - wise tractable sampling computations. approach COMPARE methods. methods COMPARE approach. OtherScientificTerm are information form, Kronecker - factored eigenbasis, and information matrix. Task is inversion of the information matrix. ",This paper proposes a diagonal correction of the Kronecker-factored eigenbasis of the information matrix of a deep neural network (DNN) to improve the representation of model uncertainty in Bayesian analysis. The proposed diagonal correction is based on a scalable Laplace Approximation scheme. The authors propose a low-rank approximation of the eigenbases of DNNs with spectral sparsity. They show that the proposed method is memory-wise tractable and can be used for memory-efficient sampling computations.,This paper proposes a diagonal correction of the Kronecker-factored eigenbasis in the Laplace Approximation scheme for sparse representation of model uncertainty in deep neural networks (DNNs). The diagonal correction is based on a low-rank approximation of the information matrix. The authors show that the diagonal correction can be applied to the full Bayesian analysis of the model uncertainty. They also show that it can be used for spectral sparsity of DNNs. 
22661,SP:2e03ceba4004b82f86f8349352a8ee4520e9c35d,"Minwise Hashing ( MinHash ) USED-FOR set similarities. compact high - dimensional data USED-FOR learning and searching. set similarities CONJUNCTION compact high - dimensional data. compact high - dimensional data CONJUNCTION set similarities. Minwise Hashing ( MinHash ) USED-FOR compact high - dimensional data. MinHash USED-FOR MinHash values. permutation ( hash function ) USED-FOR MinHash values. permutation ( hash function ) USED-FOR Permutation Hashing ( OPH ). strategies USED-FOR densification. densification HYPONYM-OF remedial strategy. Amortization Hashing ( AHash ) USED-FOR empty bins. Amortization Hashing ( AHash ) HYPONYM-OF load - balanced hashing. AHash COMPARE densification strategies. densification strategies COMPARE AHash. AHash COMPARE OPH. OPH COMPARE AHash. OPH CONJUNCTION densification strategies. densification strategies CONJUNCTION OPH. runtime efficiency EVALUATE-FOR densification strategies. runtime efficiency EVALUATE-FOR AHash. Material are high - dimensional data, and real datasets. OtherScientificTerm are bins, and unbalanced load. Task is false similarity computation. ","This paper proposes a novel method to improve the performance of MinHash, a popular hashing algorithm for compact high-dimensional data. The main idea is to use a permutation-based hash function to reduce the number of bins in MinHash. The proposed method, called Amortization Hashing (AHash), is a load-balanced hashing algorithm, which is able to efficiently reduce the amount of false similarity computation. Theoretical analysis is provided to show that AHash is more efficient than MinHash in terms of accuracy and runtime.","This paper proposes a new method to improve the performance of Minwise Hashing (MinHash) for compact high-dimensional data. MinHash is a popular method for learning and searching compact high dimensional data with set similarities. The authors propose a new approach called Amortization Hashing, which is a load-balanced hashing method for MinHash. The main idea of the method is to use a permutation-based hash function (Permutation Hashing) to reduce the number of bins in MinHash, so that it can be applied to the full set of MinHash values. The proposed method is evaluated on a variety of datasets, and compared with MinHash and OPH. "
22677,SP:d73827ab98b0ff6bd92abfefea43a5f88ea40de2,"feature extraction USED-FOR periodic signals. power generation CONJUNCTION industrial machine. industrial machine CONJUNCTION power generation. industrial machine CONJUNCTION robotic system. robotic system CONJUNCTION industrial machine. mechanized transportation vehicle CONJUNCTION power generation. power generation CONJUNCTION mechanized transportation vehicle. rotating shafts PART-OF robotic system. rotating shafts PART-OF mechanized transportation vehicle. multi - layer perceptron HYPONYM-OF methods. robust method USED-FOR features. machine learning architecture USED-FOR graph data. robust method USED-FOR phase shift data. cyclic permutation USED-FOR machine learning architecture. phase shift data USED-FOR features. cyclic permutation FEATURE-OF graph data. graph structure USED-FOR robust method. OtherScientificTerm are periodicity, shaft ’s rotation, Imprecise timing, phase shifts, and phase shift. ","This paper proposes a method to extract periodic signals from a set of periodic graphs. The proposed method is based on the observation that periodic graphs have periodicity, which is a property that is important for power generation and machine translation. The authors propose to extract the periodic features from the periodic graphs using a multi-layer perceptron architecture. The method is evaluated on a series of real-world experiments.   ","This paper proposes a method to extract features from periodic signals from a graph. The authors propose a robust method for extracting features from the graph, which is based on a cyclic permutation of the graph structure. The proposed method is evaluated on a variety of tasks, including power generation, industrial machine, and robotic system. "
22693,SP:0df5ad333eb4ff9cca7f2d117909e2ce533a65d8,them USED-FOR real world systems. controllability FEATURE-OF systems. precision USED-FOR real world systems. confidence score FEATURE-OF confidence oriented decoder. calibration technique USED-FOR faithful generation. calibration technique USED-FOR inference time. approach COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE approach. automatic metrics CONJUNCTION human evaluation. human evaluation CONJUNCTION automatic metrics. structured data - to - text dataset EVALUATE-FOR approach. structured data - to - text dataset EVALUATE-FOR state - of - the - art approaches. WikiBio HYPONYM-OF structured data - to - text dataset. human evaluation EVALUATE-FOR approach. human evaluation EVALUATE-FOR state - of - the - art approaches. automatic metrics EVALUATE-FOR state - of - the - art approaches. automatic metrics EVALUATE-FOR approach. Method is Neural conditional text generation systems. OtherScientificTerm is variational Bayes objective. ,"This paper proposes a method to improve the accuracy of text generation systems. The main idea is to use a confidence-based decoder to predict the output of the decoder, which is then used to train a decoder network to generate the text. The decoder is trained with a variational Bayes objective, where the objective is to maximize the probability that the predicted output is close to the true output. The proposed method is evaluated on a text-to-image dataset and on a WikiBio dataset.   ",This paper proposes a new method for conditional text generation from structured data-to-text datasets. The method is based on a confidence-oriented decoder that uses a calibration technique to ensure faithful generation. The proposed method is evaluated on the WikiBio dataset and compared with other state-of-the-art approaches. 
22709,SP:03307deac29173b2968fbd08f95fc77eb1f82410,Magnitude - based pruning USED-FOR pruning neural networks. magnitude - based pruning USED-FOR pruning modern architectures. Frobenius distortion FEATURE-OF linear operator. magnitude - based pruning USED-FOR Frobenius distortion. single layer optimization USED-FOR multi - layer optimization. single layer FEATURE-OF linear operator. lookahead pruning HYPONYM-OF pruning method. single layer optimization USED-FOR pruning method. method COMPARE magnitude - based pruning. magnitude - based pruning COMPARE method. VGG CONJUNCTION ResNet. ResNet CONJUNCTION VGG. networks EVALUATE-FOR magnitude - based pruning. networks EVALUATE-FOR method. VGG HYPONYM-OF networks. ResNet HYPONYM-OF networks. Method is neural networks. OtherScientificTerm is high - sparsity regime. , pruning is a popular method for pruning neural networks. This paper proposes a new pruning method based on Frobenius distortion. The main idea is to prune a linear operator at each layer of the network. The authors show that this pruning can be done in a single-layer optimization. The proposed method is evaluated on VGG and ResNet.,This paper proposes a method for pruning neural networks with Frobenius distortion. The main idea is to use a single layer optimization for multi-layer optimization of a linear operator. The authors show that their method is more efficient than the lookahead pruning method in the high-sparsity regime. The method is evaluated on VGG and ResNet networks.
22725,SP:dc80fdc75bc14ae19fe4ba9b85c35ce00b12856f,"parallel workers PART-OF graph. technique USED-FOR decentralized SGD. quantized communication USED-FOR technique. quantized communication USED-FOR decentralized SGD. asymptotic rate EVALUATE-FOR algorithm. Moniqua COMPARE algorithm. algorithm COMPARE Moniqua. full - precision communication USED-FOR algorithm. Moniqua COMPARE quantized decentralized algorithms. quantized decentralized algorithms COMPARE Moniqua. wall clock time EVALUATE-FOR Moniqua. bit - budgets FEATURE-OF Moniqua. 4 - bits - per - parameter communication USED-FOR Moniqua. CIFAR10 EVALUATE-FOR VGG16. Method is Decentralized stochastic gradient descent ( SGD ). OtherScientificTerm are memory, biased or linear quantizers, and convergence. Task is non - convex objectives. ","This paper studies decentralized stochastic gradient descent (SGD) with parallel workers in a graph. The authors propose Moniqua, a new decentralized SGD algorithm that uses quantized communication to reduce the memory footprint of SGD. Theoretical analysis is provided to show the convergence rate of the proposed algorithm. Empirical results show that the proposed method outperforms the state-of-the-art decentralized algorithms in terms of wall clock time.","This paper proposes a new method for decentralized stochastic gradient descent (SGD) in the context of decentralized communication between workers in a graph. The authors propose Moniqua, which is a new algorithm for decentralized SGD. The main contribution of the paper is to propose a new quantized communication method that can be applied to the problem of decentralized gradient descent. The proposed method is based on the idea of quantized decentralized communication, where the communication is done by a set of workers in the graph.  The authors show that the proposed method outperforms the state-of-the-art in terms of the asymptotic rate of convergence."
22741,SP:86c61a658d07ab86e2d84cef7e480bf7a06e4ddb,"Method are reinforcement learning, and partial models. Generic are it, and they. Task is jointly modeling future observations. Material is images. ","This paper studies the problem of jointly modeling future observations in reinforcement learning with partial models. The authors show that partial models can be seen as a set of partial models that are trained to jointly model future observations. They show that this is the case in the case that the partial models are trained on images, and that they can be used to train partial models in the presence of adversarial perturbations. They also show that when training partial models, they are able to learn to predict future observations that are independent of each other.","This paper studies the problem of jointly modeling future observations in reinforcement learning with partial models. The authors propose a novel approach to jointly modeling the future observations of a pair of partial models, where the partial model is trained to predict the observations of the other partial model. They show that this approach can be applied to a variety of tasks, and show that it can be used to improve the performance of the joint model. "
22757,SP:c70479b2096a52584b242de58272ca8d8565feea,"succinct common representation of two correlated data variables USED-FOR conditional and joint generation tasks. variational autoencoder ( VAE ) model USED-FOR succinct common representation of two correlated data variables. distributed simulation CONJUNCTION channel synthesis. channel synthesis CONJUNCTION distributed simulation. information theoretic problems USED-FOR Wyner VAE model. channel synthesis HYPONYM-OF information theoretic problems. distributed simulation HYPONYM-OF information theoretic problems. common representation CONJUNCTION local representations. local representations CONJUNCTION common representation. Wyner VAE USED-FOR correlated data variables. mutual information FEATURE-OF regularization term. common representation PART-OF correlated data variables. local representations PART-OF Wyner VAE. shared concept HYPONYM-OF common representation. synthetic data CONJUNCTION real images. real images CONJUNCTION synthetic data. approach USED-FOR joint and conditional generation. real images USED-FOR style control. synthetic data USED-FOR style control. style control USED-FOR joint and conditional generation. VAE variants CONJUNCTION variational information bottleneck method. variational information bottleneck method CONJUNCTION VAE variants. model COMPARE VAE variants. VAE variants COMPARE model. model COMPARE variational information bottleneck method. variational information bottleneck method COMPARE model. succinct common representation USED-FOR generative. OtherScientificTerm are Wyner ’s common information, and data variables. ","This paper proposes a new variational autoencoder (VAE) model that learns a succinct common representation of two correlated data variables in a joint and conditional generation task. The proposed model is based on the idea of a shared concept between the two data variables, which is then used to regularize the mutual information between the data variables. The authors show that the proposed model achieves state-of-the-art performance on both conditional and joint generation tasks. ","This paper proposes a new VAE model, called Wyner VAE, for conditional and joint generation tasks. The main idea is to use the common representation of two correlated data variables to generate a succinct representation of the data variables. This is achieved by using a shared concept between the two data variables, which is based on the mutual information between the local representations of the two variables. The authors show that their model can achieve state-of-the-art performance on both synthetic data and real-world data."
