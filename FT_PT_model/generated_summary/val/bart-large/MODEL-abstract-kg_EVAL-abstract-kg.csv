,paper_id,input,output_bestRouge1,output_bestRougeAvg
0,SP:7f4b788b00a2a10bcd60351c3e04c8f597101e96,Federated learning ( FL ) USED-FOR machine learning models. decentralized data sources USED-FOR machine learning models. local differential privacy constraints USED-FOR FL. communication efficiency CONJUNCTION highdimensional compatibility. highdimensional compatibility CONJUNCTION communication efficiency. sqSGD ( selective quantized stochastic gradient descent ) HYPONYM-OF gradient - based learning algorithm. privacy - preserving quantization scheme USED-FOR algorithm. training performance CONJUNCTION communication costs. communication costs CONJUNCTION training performance. fixed privacy budget USED-FOR gradient subsampling strategy. communication costs EVALUATE-FOR gradient subsampling strategy. training performance EVALUATE-FOR gradient subsampling strategy. randomized rotation USED-FOR quantization error. quantization CONJUNCTION perturbation. perturbation CONJUNCTION quantization. perturbation USED-FOR FL algorithm. quantization USED-FOR FL algorithm. privacy and communication constraints FEATURE-OF FL algorithm. benchmark datasets EVALUATE-FOR framework. LeNet CONJUNCTION ResNet. ResNet CONJUNCTION LeNet. sqSGD USED-FOR large models. local privacy constraints FEATURE-OF large models. ResNet HYPONYM-OF large models. LeNet HYPONYM-OF large models. sqSGD COMPARE baseline algorithms. baseline algorithms COMPARE sqSGD. fixed privacy and communication level FEATURE-OF sqSGD. OtherScientificTerm is sensitive data disclosures. Method is privacy - preserving FL algorithms. Generic is base algorithm. ,"This paper proposes a privacy-preserving gradient-based learning algorithm for federated learning. The proposed algorithm is based on the selective quantized stochastic gradient descent (sqSGD) algorithm. The authors show that the proposed algorithm can be used to improve the privacy and communication efficiency of FL algorithms while maintaining high-dimensional compatibility. In addition, the authors also propose a gradient subsampling strategy to reduce the communication costs.","This paper proposes a privacy-preserving gradient-based learning algorithm for federated learning. The proposed algorithm is based on the selective quantized stochastic gradient descent (sqSGD) algorithm. The authors show that the proposed algorithm can be used to improve the privacy and communication efficiency of FL algorithms while maintaining high-dimensional compatibility. In addition, the authors also propose a gradient subsampling strategy to reduce the communication costs."
9,SP:632666b52c7c551d67fbbe70c06ed589c3a5e187,Self - attention networks ( SANs ) USED-FOR natural language processing tasks. it USED-FOR language representation. language knowledge USED-FOR it. prior knowledge USED-FOR language representation. prior knowledge USED-FOR general representation method. method USED-FOR SANs. prior knowledge USED-FOR SANs. it USED-FOR language representation. it USED-FOR prior word frequency knowledge. prior word frequency knowledge CONJUNCTION prior translation lexicon knowledge. prior translation lexicon knowledge CONJUNCTION prior word frequency knowledge. it USED-FOR prior translation lexicon knowledge. prior translation lexicon knowledge USED-FOR bilingual data. prior word frequency knowledge CONJUNCTION monolingual data. monolingual data CONJUNCTION prior word frequency knowledge. method COMPARE Transformer - based baseline. Transformer - based baseline COMPARE method. Method is neural networks. ,This paper proposes a general representation method for self-attention networks (SANs) that leverages prior knowledge for language processing tasks. The method is based on the prior knowledge of bilingual data and monolingual data. The authors show that the proposed method is able to achieve better performance than the Transformer-based baseline in terms of generalization. ,This paper proposes a general representation method for self-attention networks (SANs) that leverages prior knowledge for language processing tasks. The method is based on the prior knowledge of bilingual data and monolingual data. The authors show that the proposed method is able to achieve better performance than the Transformer-based baseline in terms of generalization. 
18,SP:e3e728837f26acb9da283a42c219b6c3b3e131cb,"cat - and - mouse game USED-FOR cybersecurity. Moving Target Defense ( MTD ) HYPONYM-OF proactive defense methods. leader - follower games USED-FOR MTD. models USED-FOR sequential settings. incomplete information FEATURE-OF rational adversary. learning defense policies USED-FOR cyber - security. learning defense policies USED-FOR sequential settings. sequential settings USED-FOR cyber - security. optimal movement policy USED-FOR BSMGs. interaction USED-FOR optimal movement policy. Bayesian Stackelberg Markov Games ( BSMGs ) HYPONYM-OF game - theoretic model. BSMGs PART-OF landscape of incomplete - information Markov games. Strong Stackelberg Equilibrium ( SSE ) FEATURE-OF them. learning approach USED-FOR SSE. learning approach USED-FOR BSMG. MTD USED-FOR web - application security. optimal policy USED-FOR MTD domains. movement policy USED-FOR optimal policy. SSE FEATURE-OF BSMG. incomplete information FEATURE-OF MTD domains. MTD EVALUATE-FOR movement policy. OtherScientificTerm are reconnaissance, sub - optimal movement strategies, incomplete - information Markov games, and prior information. Method are movement strategies, defense policies, single - agent reinforcement learning techniques, and MTD system. Generic is they. ",This paper proposes a Bayesian Stackelberg Markov Games (BSMG) model for moving target defense (MTD). BSMGs are a variant of incomplete-information Markov games. The authors show that the optimal movement policy of BSMG can be learned from prior information. They also show that SSE can be used to learn the optimal MDP.,This paper proposes a Bayesian Stackelberg Markov Games (BSMG) model for moving target defense (MTD). BSMGs are a variant of incomplete-information Markov games. The authors show that the optimal movement policy of BSMG can be learned from prior information. They also show that SSE can be used to learn the optimal MDP.
27,SP:97911e02bf06b34d022e7548beb5169a1d825903,"unsupervised disentangled representation learning EVALUATE-FOR Variational Autoencoder ( VAE ) based frameworks. VAE im3 plementation choices USED-FOR PCA - like behavior. PCA - like behavior FEATURE-OF data sam4 ples. local orthogonality CONJUNCTION data re6 construction. data re6 construction CONJUNCTION local orthogonality. models USED-FOR entangled representations. architecture CONJUNCTION hyperparameter 7 setting. hyperparameter 7 setting CONJUNCTION architecture. architecture USED-FOR models. hyperparameter 7 setting FEATURE-OF models. multi9 ple VAEs PART-OF VAE ensemble framework. VAE ensemble 15 objective USED-FOR linear transformations. approach COMPARE unsupervised disen18 tangled representation learning approaches. unsupervised disen18 tangled representation learning approaches COMPARE approach. OtherScientificTerm are model identifiability, disentangled representations, signed permutation transformation, pair - wise linear transformations, VAEs, triv16 ial transformations, and latent representations. Method are VAE based disentanglement 5 frameworks, and VAE ensemble. Generic is It. ",This paper proposes a VAE-based unsupervised disentangled representation learning framework. The proposed framework is based on the VAE im3 plementation choices. The authors show that the proposed method outperforms the baselines in terms of disentanglement performance. ,This paper proposes a VAE-based unsupervised disentangled representation learning framework. The proposed framework is based on the VAE im3 plementation choices. The authors show that the proposed method outperforms the baselines in terms of disentanglement performance. 
36,SP:59f9de3ebe4a04d2fc8778d8e3415bf85efb7822,zero - shot approach USED-FOR automated machine learning ( AutoML ). model USED-FOR supervised learning task. zero - shot approach USED-FOR model. approach USED-FOR AutoML. meta - feature extractor USED-FOR data. free - text descriptions CONJUNCTION meta - feature extractor. meta - feature extractor CONJUNCTION free - text descriptions. transformer - based language embedding USED-FOR algorithms. meta - feature extractor USED-FOR method. free - text descriptions USED-FOR algorithms. transformer - based language embedding USED-FOR method. meta - feature extractor USED-FOR algorithms. graph neural network USED-FOR machine learning pipeline. approach USED-FOR AutoML. unsupervised representation learning USED-FOR AutoML. unsupervised representation learning USED-FOR natural language processing. unsupervised representation learning USED-FOR approach. Method is AutoML systems. Metric is running time. OtherScientificTerm is prediction time. ,"This paper proposes a zero-shot approach for automated machine learning (AutoML). The proposed method is based on transformer-based language embedding and meta-feature extractor. The proposed approach is evaluated on a variety of tasks, including unsupervised representation learning and natural language processing. The paper shows that the proposed method outperforms baselines in terms of running time and prediction accuracy. ","This paper proposes a zero-shot approach for automated machine learning (AutoML). The proposed method is based on transformer-based language embedding and meta-feature extractor. The proposed approach is evaluated on a variety of tasks, including unsupervised representation learning and natural language processing. The paper shows that the proposed method outperforms baselines in terms of running time and prediction accuracy. "
45,SP:0f74dff929a4908405ebfa8e60fe1860eec6364f,optimization process USED-FOR non - compositional solutions. compositionality learning approaches USED-FOR compositionality. model architecture design USED-FOR compositionality learning approaches. compositional learning CONJUNCTION gradient descent. gradient descent CONJUNCTION compositional learning. machine learning models USED-FOR human - level intelligence. Method is neural network optimization. Task is compositional generalization. ,This paper studies the problem of compositional generalization in neural network optimization. The authors propose two approaches to learn compositional solutions: compositionality learning and gradient descent. The compositional learning approach is based on the idea that the compositionality of a solution can be learned by optimizing the gradient of the solution. The gradient descent approach uses the idea of compositionality to learn a compositional solution. Experiments show that the proposed approach outperforms the baselines in terms of generalization.,This paper studies the problem of compositional generalization in neural network optimization. The authors propose two approaches to learn compositional solutions: compositionality learning and gradient descent. The compositional learning approach is based on the idea that the compositionality of a solution can be learned by optimizing the gradient of the solution. The gradient descent approach uses the idea of compositionality to learn a compositional solution. Experiments show that the proposed approach outperforms the baselines in terms of generalization.
54,SP:f99a1b2dbcb7a7b30dbfcfc60668e94b4ad53410,"Knowledge graph ( KG ) representation learning USED-FOR entity alignment. machine translation CONJUNCTION feature extraction. feature extraction CONJUNCTION machine translation. methods COMPARE embeddingbased ones. embeddingbased ones COMPARE methods. embedding spaces PART-OF KGs. pre - aligned entities USED-FOR embedding spaces. scoring function USED-FOR embedding learning. margin FEATURE-OF scoring function. margin USED-FOR representation discrepancy. approach USED-FOR KG - invariant and principled entity representations. feature distribution CONJUNCTION ontology knowledge. ontology knowledge CONJUNCTION feature distribution. neural ontologies PART-OF KGs. state - of - the - art ones HYPONYM-OF embedding - based entity alignment methods. Generic are they, paradigm, and model. Method is alignment learning. OtherScientificTerm is geometric distance. ",This paper proposes a new approach for entity alignment based on knowledge graph (KG) representation learning. The key idea is to use pre-aligned entities as a scoring function to measure the distance between two entities in a KG. The authors show that the proposed method is able to achieve state-of-the-art entity alignment performance on a number of datasets. The paper also provides a theoretical analysis of the proposed approach. ,This paper proposes a new approach for entity alignment based on knowledge graph (KG) representation learning. The key idea is to use pre-aligned entities as a scoring function to measure the distance between two entities in a KG. The authors show that the proposed method is able to achieve state-of-the-art entity alignment performance on a number of datasets. The paper also provides a theoretical analysis of the proposed approach. 
63,SP:0e42de72d10040289283516ec1bd324788f7d371,"Convolutional Neural Networks ( CNNs ) powered functionalities USED-FOR ubiquitous intelligent “ IoT cameras ”. medicineand wearable - related ones HYPONYM-OF applications. CNNs COMPARE IoT devices. IoT devices COMPARE CNNs. limited resources FEATURE-OF IoT devices. storage and energy cost USED-FOR CNNs. form factor FEATURE-OF PhlatCam. compression techniques USED-FOR storage and energy reduction. Sensor Algorithm Co - Design framework USED-FOR CNN - powered PhlatCam. SACoD USED-FOR CNN - powered PhlatCam. SACoD HYPONYM-OF Sensor Algorithm Co - Design framework. mask CONJUNCTION backend CNN model. backend CNN model CONJUNCTION mask. PhlatCam sensor CONJUNCTION backend CNN model. backend CNN model CONJUNCTION PhlatCam sensor. mask PART-OF PhlatCam sensor. differential neural architecture search USED-FOR mask. model compression CONJUNCTION energy savings. energy savings CONJUNCTION model compression. energy savings EVALUATE-FOR SACoD framework. model compression EVALUATE-FOR SACoD framework. task accuracy EVALUATE-FOR SACoD framework. PhlatCam imaging system EVALUATE-FOR SACoD. Method are IoT systems, CNN algorithm, and SOTA ) designs. Metric is camera form factor. OtherScientificTerm is model parameters. Generic is tasks. ","This paper proposes a sensor-based method to reduce the computational cost of a CNN-based imaging system. The proposed method, called SACoD, is based on differential neural architecture search (DNNS) to find the best mask for a given sensor. The method is evaluated on the PhlatCam imaging system and shows that the proposed method outperforms the baselines in terms of accuracy and energy savings.","This paper proposes a sensor-based method to reduce the computational cost of a CNN-based imaging system. The proposed method, called SACoD, is based on differential neural architecture search (DNNS) to find the best mask for a given sensor. The method is evaluated on the PhlatCam imaging system and shows that the proposed method outperforms the baselines in terms of accuracy and energy savings."
72,SP:493afcfa3fd64967785928ba3acecf3ffa6ce579,"Honey bees USED-FOR complex social systems. dataset USED-FOR temporal matrix factorization model. temporal matrix factorization model USED-FOR average developmental path. lifetime trajectories PART-OF dataset. social sciences CONJUNCTION neuroscience. neuroscience CONJUNCTION social sciences. behavioral biology CONJUNCTION social sciences. social sciences CONJUNCTION behavioral biology. neuroscience CONJUNCTION information science. information science CONJUNCTION neuroscience. method USED-FOR behavioral heterogeneity. behavioral heterogeneity FEATURE-OF complex social systems. information science HYPONYM-OF fields. behavioral biology HYPONYM-OF fields. social sciences HYPONYM-OF fields. neuroscience HYPONYM-OF fields. OtherScientificTerm are global behavior, and social network. Material is honey bee colonies. ",This paper proposes a temporal matrix factorization model for estimating the average developmental path of a population of honey bees. The model is based on the observation that the average trajectory of the population of a given population of bees follows a trajectory that is similar to that of a single individual. The authors propose to use this model to estimate the average trajectories of all individuals in a population. The proposed model is tested on a large dataset of honey bee colonies and shows that the model is able to capture behavioral heterogeneity in the population.,This paper proposes a temporal matrix factorization model for estimating the average developmental path of a population of honey bees. The model is based on the observation that the average trajectory of the population of a given population of bees follows a trajectory that is similar to that of a single individual. The authors propose to use this model to estimate the average trajectories of all individuals in a population. The proposed model is tested on a large dataset of honey bee colonies and shows that the model is able to capture behavioral heterogeneity in the population.
81,SP:08ae056f269c731b92b5a3d59e18f9ccfc0b703c,"Deep neural networks USED-FOR image restoration and reconstruction tasks. noisy or corrupted measurement USED-FOR networks. pipeline USED-FOR data augmentation. Data Augmentation ( DA ) USED-FOR classification problems. data augmentation USED-FOR image reconstruction tasks. medical imaging FEATURE-OF image reconstruction tasks. invariances FEATURE-OF medical imaging measurements. naive DA strategies USED-FOR DA pipeline. invariances USED-FOR DA pipeline. problem regimes EVALUATE-FOR DA. fastMRI dataset EVALUATE-FOR DA. training data USED-FOR single - coil reconstruction. training data USED-FOR multi - coil reconstruction. multi - coil reconstruction CONJUNCTION single - coil reconstruction. single - coil reconstruction CONJUNCTION multi - coil reconstruction. training data CONJUNCTION training data. training data CONJUNCTION training data. Task is accelerated magnetic resonance imaging. OtherScientificTerm are under - sampled linear measurements, and high - data regime. Method is data augmentation pipeline. ",This paper proposes a data augmentation pipeline for image restoration and reconstruction tasks. The proposed pipeline is based on the idea of data augmentations. The authors show that the proposed pipeline can be applied to both low-data and high-data settings. They also show that it can be used to improve the performance of existing methods. ,This paper proposes a data augmentation pipeline for image restoration and reconstruction tasks. The proposed pipeline is based on the idea of data augmentations. The authors show that the proposed pipeline can be applied to both low-data and high-data settings. They also show that it can be used to improve the performance of existing methods. 
90,SP:3fdaae674a2b9d437a43d32778437dc7df9c1686,"deep repulsive clustering ( DRC ) algorithm USED-FOR order learning. ordered data USED-FOR deep repulsive clustering ( DRC ) algorithm. order - related feature CONJUNCTION identity feature. identity feature CONJUNCTION order - related feature. facial age estimation CONJUNCTION aesthetic score regression. aesthetic score regression CONJUNCTION facial age estimation. aesthetic score regression CONJUNCTION historical color image classification. historical color image classification CONJUNCTION aesthetic score regression. algorithm USED-FOR ordered data. historical color image classification EVALUATE-FOR algorithm. facial age estimation EVALUATE-FOR algorithm. rank estimation EVALUATE-FOR algorithm. Method is order - identity decomposition ( ORID ) network. OtherScientificTerm are identity features, repulsive term, and rank. ","This paper proposes a deep repulsive clustering (DRC) algorithm for order learning. The proposed algorithm is based on order-identity decomposition (ORID) network, which decomposes the order-related features into identity features and rank features. The authors show that the proposed algorithm outperforms existing DRC algorithms for rank estimation and aesthetic score regression. ","This paper proposes a deep repulsive clustering (DRC) algorithm for order learning. The proposed algorithm is based on order-identity decomposition (ORID) network, which decomposes the order-related features into identity features and rank features. The authors show that the proposed algorithm outperforms existing DRC algorithms for rank estimation and aesthetic score regression. "
99,SP:a5775441639529d61b7fee4b4298fd82a0c93bb5,"Exploration PART-OF model - free reinforcement learning. sparse reward USED-FOR Exploration. intrinsic rewards USED-FOR state - of - the - art methods. methods USED-FOR procedurally - generated environments. intrinsic rewards USED-FOR methods. episode - level exploration method USED-FOR procedurally - generated environments. RAPID HYPONYM-OF episode - level exploration method. per - episode and long - term views FEATURE-OF episodic exploration score. episodic exploration score EVALUATE-FOR RAPID. sparse MuJoCo tasks EVALUATE-FOR method. procedurally - generated MiniGrid environments EVALUATE-FOR method. RAPID COMPARE intrinsic reward strategies. intrinsic reward strategies COMPARE RAPID. sample efficiency EVALUATE-FOR intrinsic reward strategies. sample efficiency EVALUATE-FOR RAPID. OtherScientificTerm are uncertain environment dynamics, and ranking buffer. Material is MiniWorld. ","This paper proposes an episodic exploration method for model-free reinforcement learning. The proposed method is based on the idea of intrinsic reward, which is an extension of the intrinsic reward framework for MuJoCo tasks. The authors show that the proposed method outperforms existing methods in terms of sample efficiency and exploration efficiency. They also show that their method can be applied to procedurally generated MiniGrid environments.","This paper proposes an episodic exploration method for model-free reinforcement learning. The proposed method is based on the idea of intrinsic reward, which is an extension of the intrinsic reward framework for MuJoCo tasks. The authors show that the proposed method outperforms existing methods in terms of sample efficiency and exploration efficiency. They also show that their method can be applied to procedurally generated MiniGrid environments."
108,SP:30024ac5aef153ae24c893a53bad93ead2526476,"semantic space of class attributes CONJUNCTION visual space of images. visual space of images CONJUNCTION semantic space of class attributes. Isometric Propagation Network ( IPN ) USED-FOR class dependency. IPN USED-FOR class representations. auto - generated graph USED-FOR class representations. ZSL benchmarks EVALUATE-FOR IPN. them USED-FOR IPN. Method are Zero - shot learning ( ZSL ), static representation, and dynamic propagation procedures. OtherScientificTerm are class attributes, imbalanced supervision, and semantic and the visual space. Generic are representations, and mapping. Task is ZSL settings. Material is seen - class data. Metric is consistency loss. ","This paper proposes a method for zero-shot learning (ZSL) based on the Isometric Propagation Network (IPN) framework. The proposed method is based on an auto-generated graph, which is used to generate class representations for each class. The method is evaluated on a number of benchmark datasets and shows that the proposed method outperforms state-of-the-art methods.","This paper proposes a method for zero-shot learning (ZSL) based on the Isometric Propagation Network (IPN) framework. The proposed method is based on an auto-generated graph, which is used to generate class representations for each class. The method is evaluated on a number of benchmark datasets and shows that the proposed method outperforms state-of-the-art methods."
117,SP:1d7c174f4f7a0eb26edceecc117f9af1528802e5,"multi - task model USED-FOR tasks. HyperGrid Transformers HYPONYM-OF Transformer architecture. task - conditioned hyper networks USED-FOR feed - forward layers. task - conditioned hyper networks USED-FOR Transformer architecture. decomposable hypernetwork USED-FOR grid - wise projections. global ( task - agnostic ) state CONJUNCTION local task - specific state. local task - specific state CONJUNCTION global ( task - agnostic ) state. method USED-FOR hypernetwork. SuperGLUE test set EVALUATE-FOR state - of - the - art. fine - tuning CONJUNCTION multi - task learning approaches. multi - task learning approaches CONJUNCTION fine - tuning. method USED-FOR fine - tuning. method USED-FOR multi - task learning approaches. Task is natural language understanding tasks. Generic are model, and approach. OtherScientificTerm is weight matrices. Material is GLUE / SuperGLUE. ",This paper proposes a method for learning multi-task models for natural language understanding tasks. The proposed method is based on task-conditioned hyper-networks. The authors propose a decomposable hyper-network that is able to learn a global (task-agnostic) state and a local task-specific state for each task. The method is evaluated on the SuperGLUE and GLUE/SuperGLUE test set. ,This paper proposes a method for learning multi-task models for natural language understanding tasks. The proposed method is based on task-conditioned hyper-networks. The authors propose a decomposable hyper-network that is able to learn a global (task-agnostic) state and a local task-specific state for each task. The method is evaluated on the SuperGLUE and GLUE/SuperGLUE test set. 
126,SP:d957241c02163c1c5bc03a688aa4a2eb486fb9f1,"lighting CONJUNCTION weather. weather CONJUNCTION lighting. weather CONJUNCTION visibility conditions. visibility conditions CONJUNCTION weather. image input USED-FOR autonomous driving. image input USED-FOR learning algorithm. algorithm USED-FOR task. sensitivity analysis USED-FOR algorithm. sensitivity analysis USED-FOR task. approach USED-FOR learning outcomes. data augmentation CONJUNCTION adversarial training. adversarial training CONJUNCTION data augmentation. neural network training USED-FOR self - driving cars. approach COMPARE techniques. techniques COMPARE approach. algorithm USED-FOR neural network training. robustness EVALUATE-FOR algorithm. adversarial training HYPONYM-OF techniques. data augmentation HYPONYM-OF techniques. OtherScientificTerm are external and environmental factors, and sensors. Task is perceptual data processing. ","This paper proposes a method for self-driving cars that learns a learning algorithm that is robust to external and environmental factors. The proposed method is based on the idea of sensitivity analysis, which is an extension of the sensitivity analysis framework that is used for adversarial training and data augmentation. The authors show that the proposed method outperforms the baselines in terms of robustness and robustness to adversarial perturbations. ","This paper proposes a method for self-driving cars that learns a learning algorithm that is robust to external and environmental factors. The proposed method is based on the idea of sensitivity analysis, which is an extension of the sensitivity analysis framework that is used for adversarial training and data augmentation. The authors show that the proposed method outperforms the baselines in terms of robustness and robustness to adversarial perturbations. "
135,SP:2df9ba21f72e041f80c7bc9ecfe89353f172b058,"deep networks USED-FOR approximate solvers. hard constraints FEATURE-OF Large optimization problems. hard constraints FEATURE-OF problems. method USED-FOR feasibility. gradient - based corrections USED-FOR inequality constraints. differentiable procedure USED-FOR method. differentiable procedure USED-FOR feasibility. DC3 USED-FOR AC optimal power flow. DC3 USED-FOR synthetic optimization tasks. synthetic optimization tasks CONJUNCTION AC optimal power flow. AC optimal power flow CONJUNCTION synthetic optimization tasks. hard constraints FEATURE-OF physics of the electrical grid. DC3 USED-FOR near - optimal objective values. feasibility FEATURE-OF DC3. Method are classical solvers, deep learning approaches, and Deep Constraint Completion and Correction ( DC3 ). OtherScientificTerm are infeasible solutions, and equality constraints. Generic is algorithm. ","This paper proposes Deep Constraint Completion and Correction (DC3), a method for solving hard constraints. DC3 is based on gradient-based corrections for inequality constraints. The authors show that DC3 can achieve near-optimal objective values in a variety of optimization problems. The proposed method is evaluated on a number of synthetic and real-world problems.","This paper proposes Deep Constraint Completion and Correction (DC3), a method for solving hard constraints. DC3 is based on gradient-based corrections for inequality constraints. The authors show that DC3 can achieve near-optimal objective values in a variety of optimization problems. The proposed method is evaluated on a number of synthetic and real-world problems."
144,SP:61a0163b21dc8f92dd699c1e154f53d30c80b2fe,"Regularization USED-FOR sparsity. Regularization USED-FOR deep neural network pruning. pruning schedule CONJUNCTION weight importance scoring. weight importance scoring CONJUNCTION pruning schedule. problems PART-OF pruning. weight importance scoring HYPONYM-OF pruning. pruning schedule HYPONYM-OF pruning. weight importance scoring HYPONYM-OF problems. pruning schedule HYPONYM-OF problems. it COMPARE one - shot counterpart. one - shot counterpart COMPARE it. L2 regularization variant COMPARE one - shot counterpart. one - shot counterpart COMPARE L2 regularization variant. rising penalty factors FEATURE-OF L2 regularization variant. Hessian information USED-FOR pruning. growing penalty scheme USED-FOR approach. approach USED-FOR Hessian information. networks USED-FOR structured and unstructured pruning. algorithms USED-FOR large datasets. large datasets CONJUNCTION networks. networks CONJUNCTION large datasets. networks EVALUATE-FOR algorithms. CIFAR and ImageNet datasets EVALUATE-FOR deep neural networks. OtherScientificTerm are small penalty strength regime, and regularization. Task is Hessian approximation problems. Generic is state - of - the - art algorithms. ","This paper proposes a new regularization method for deep neural network pruning. The proposed method, called L2 regularization, is based on a growing penalty scheme. The authors show that the proposed method outperforms existing methods on CIFAR-10, ImageNet, and ImageNet-100 datasets. ","This paper proposes a new regularization method for deep neural network pruning. The proposed method, called L2 regularization, is based on a growing penalty scheme. The authors show that the proposed method outperforms existing methods on CIFAR-10, ImageNet, and ImageNet-100 datasets. "
153,SP:7b2bf0e36c926d1ed5ab9593a11e4ebce49df6ba,"Model - based planning USED-FOR deep, careful reasoning. deep, careful reasoning CONJUNCTION generalization. generalization CONJUNCTION deep, careful reasoning. Model - based planning USED-FOR generalization. generalization USED-FOR artificial agents. deep, careful reasoning USED-FOR artificial agents. deep function approximation USED-FOR model - based reinforcement learning ( MBRL ). planning USED-FOR MBRL agents. planning USED-FOR generalization. MuZero HYPONYM-OF MBRL algorithm. MuZero COMPARE MBRL algorithms. MBRL algorithms COMPARE MuZero. overlapping components CONJUNCTION MBRL algorithms. MBRL algorithms CONJUNCTION overlapping components. MBRL algorithm COMPARE MBRL algorithms. MBRL algorithms COMPARE MBRL algorithm. overlapping components PART-OF MuZero. overlapping components PART-OF MBRL algorithm. control tasks CONJUNCTION Atari. Atari CONJUNCTION control tasks. Atari CONJUNCTION 9x9 Go. 9x9 Go CONJUNCTION Atari. Planning USED-FOR learning process. Planning USED-FOR policy updates. Planning USED-FOR data distribution. Monte - Carlo rollouts USED-FOR shallow trees. Planning USED-FOR generalization. planning USED-FOR reinforcement learning settings. zeroand few - shot learning CONJUNCTION strategic thinking. strategic thinking CONJUNCTION zeroand few - shot learning. Model - based reinforcement learning ( MBRL ) COMPARE model - free methods. model - free methods COMPARE Model - based reinforcement learning ( MBRL ). data efficiency CONJUNCTION zeroand few - shot learning. zeroand few - shot learning CONJUNCTION data efficiency. zeroand few - shot learning EVALUATE-FOR model - free methods. data efficiency EVALUATE-FOR model - free methods. planning CONJUNCTION learning. learning CONJUNCTION planning. learning PART-OF methods. planning PART-OF methods. models USED-FOR intelligent artificial agents. models USED-FOR discrete search. planning PART-OF MBRL algorithm. MuZero HYPONYM-OF MBRL algorithm. value estimation CONJUNCTION policy optimization. policy optimization CONJUNCTION value estimation. learned model CONJUNCTION value estimation. value estimation CONJUNCTION learned model. search - based planning CONJUNCTION","This paper proposes MuZero, a model-based reinforcement learning algorithm that uses Monte-Carlo rollouts to learn a deep function approximation for planning in reinforcement learning (MBRL). The authors show that MuZero is able to achieve state-of-the-art performance on zero-shot learning and zero-and-few-shot reinforcement learning tasks. The authors also show that the proposed MuZero algorithm outperforms existing MBRL algorithms in terms of data efficiency and generalization. ","This paper proposes MuZero, a model-based reinforcement learning algorithm that uses Monte-Carlo rollouts to learn a deep function approximation for planning in reinforcement learning (MBRL). The authors show that MuZero is able to achieve state-of-the-art performance on zero-shot learning and zero-and-few-shot reinforcement learning tasks. The authors also show that the proposed MuZero algorithm outperforms existing MBRL algorithms in terms of data efficiency and generalization. "
162,SP:96afc34acb196af0b37f66ca9c89ae22ee7b6521,"long - range reasoning CONJUNCTION understanding of environment dynamics. understanding of environment dynamics CONJUNCTION long - range reasoning. Value Iteration Networks ( VINs ) USED-FOR implicit planning. long - range reasoning USED-FOR tasks. deep reinforcement learning USED-FOR implicit planning. understanding of environment dynamics USED-FOR tasks. graph representation learning CONJUNCTION neural algorithmic reasoning. neural algorithmic reasoning CONJUNCTION graph representation learning. contrastive self - supervised learning CONJUNCTION graph representation learning. graph representation learning CONJUNCTION contrastive self - supervised learning. generic environments USED-FOR VIN - style models. XLVINs COMPARE VIN - like models. VIN - like models COMPARE XLVINs. XLVINs COMPARE model - free baselines. model - free baselines COMPARE XLVINs. MDP USED-FOR VIN - like models. Generic is model. Task is planning computations. OtherScientificTerm are state space, and Markov decision process ( MDP ). Method is Latent Value Iteration Networks ( XLVINs ). ",This paper proposes a new model for implicit planning based on value iteration networks (VINs). The proposed model is based on the Latent Value Iteration Networks (XLVIN) framework. The authors show that the proposed model outperforms the baselines on a variety of tasks. The main contribution of the paper is that the model is able to learn a Markov decision process (MDP) that can be used for planning. ,This paper proposes a new model for implicit planning based on value iteration networks (VINs). The proposed model is based on the Latent Value Iteration Networks (XLVIN) framework. The authors show that the proposed model outperforms the baselines on a variety of tasks. The main contribution of the paper is that the model is able to learn a Markov decision process (MDP) that can be used for planning. 
171,SP:b0fa24ad48e7e60d6899bd799adcd03473cadd6e,"Learning functions PART-OF machine learning. Boolean variables FEATURE-OF Learning functions. neural networks USED-FOR functions. distribution free setting FEATURE-OF functions. networks USED-FOR they. read - once DNFs HYPONYM-OF functions. convex neural network CONJUNCTION gradient descent. gradient descent CONJUNCTION convex neural network. convex neural network USED-FOR functions. gradient descent USED-FOR functions. inductive bias FEATURE-OF learning process. ones HYPONYM-OF networks. networks USED-FOR risk. gradient descent USED-FOR compact representation. process USED-FOR DNF. it USED-FOR process. computer assisted proof USED-FOR inductive bias. inductive bias FEATURE-OF DNFs. computer assisted proof USED-FOR DNFs. network USED-FOR process. network USED-FOR DNF. optimization USED-FOR inductive bias. learning process CONJUNCTION optimization. optimization CONJUNCTION learning process. network USED-FOR l2 norm. network USED-FOR DNF terms. margin constraints FEATURE-OF l2 norm. OtherScientificTerm are uniform distribution, neurons, logical formulas, and high dimensional DNFs. Method is zero - error networks. Metric is population risk. Material is tabular datasets. ","This paper studies the problem of zero-error neural networks (DNFs) in the distribution-free setting. The authors consider the case of read-once DNFs, which can be seen as a special case of convex neural networks. They show that the learning process of a DNF can be viewed as a convex optimization problem, and that the gradient descent of the DNF is an inductive bias. They also provide a computer assisted proof that shows that the l2 norm of the L2 norm is a function of the number of neurons in the neural network. ","This paper studies the problem of zero-error neural networks (DNFs) in the distribution-free setting. The authors consider the case of read-once DNFs, which can be seen as a special case of convex neural networks. They show that the learning process of a DNF can be viewed as a convex optimization problem, and that the gradient descent of the DNF is an inductive bias. They also provide a computer assisted proof that shows that the l2 norm of the L2 norm is a function of the number of neurons in the neural network. "
180,SP:6e600bedbf995375fd41cc0b517ddefb918318af,exploration CONJUNCTION exploitation. exploitation CONJUNCTION exploration. sparse environment FEATURE-OF map. graph structure USED-FOR exploration directions. Graph Structured Reinforcement Learning ( GSRL ) USED-FOR value function estimation. graph structure USED-FOR value function estimation. graph structure PART-OF historical trajectories. graph structure USED-FOR Graph Structured Reinforcement Learning ( GSRL ). state transitions PART-OF replay buffer. GSRL USED-FOR dynamic graph. attention strategy USED-FOR map. state transitions USED-FOR dynamic graph. historical trajectories USED-FOR dynamic graph. graph structure USED-FOR value learning. GSRL COMPARE state - of - the - art algorithms. state - of - the - art algorithms COMPARE GSRL. sample efficiency EVALUATE-FOR state - of - the - art algorithms. sample efficiency EVALUATE-FOR GSRL. Task is reinforcement learning. OtherScientificTerm is sparse reward functions. ,"This paper proposes a graph structured reinforcement learning (GSRL) algorithm for sparse reward learning. The authors propose to use graph structure to learn the dynamics of the reward function in the sparse environment. The key idea is to learn a graph structure of historical trajectories, which is then used for value learning and exploration. The proposed algorithm is evaluated on a variety of tasks, including reinforcement learning, exploration, and value estimation. ","This paper proposes a graph structured reinforcement learning (GSRL) algorithm for sparse reward learning. The authors propose to use graph structure to learn the dynamics of the reward function in the sparse environment. The key idea is to learn a graph structure of historical trajectories, which is then used for value learning and exploration. The proposed algorithm is evaluated on a variety of tasks, including reinforcement learning, exploration, and value estimation. "
189,SP:80c62de18a6a7433c9728fe0d731f733bb89e898,"Simulated environments EVALUATE-FOR systematic generalization of reinforcement learning agents. procedurally generated content FEATURE-OF Simulated environments. positions of entities CONJUNCTION asset appearances. asset appearances CONJUNCTION positions of entities. layout CONJUNCTION positions of entities. positions of entities CONJUNCTION layout. asset appearances CONJUNCTION rules. rules CONJUNCTION asset appearances. generalization CONJUNCTION robustness. robustness CONJUNCTION generalization. generalization EVALUATE-FOR test levels. robustness EVALUATE-FOR test levels. levels USED-FOR learning progress. framework USED-FOR future learning potential. Prioritized Level Replay HYPONYM-OF framework. Prioritized Level Replay USED-FOR future learning potential. sample - efficiency CONJUNCTION generalization. generalization CONJUNCTION sample - efficiency. Procgen Benchmark environments CONJUNCTION MiniGrid environments. MiniGrid environments CONJUNCTION Procgen Benchmark environments. Prioritized Level Replay USED-FOR implicit curriculum. Generic is environment. OtherScientificTerm are environment transitions, training levels, agent, and temporal - difference ( TD ) errors. ","This paper proposes a method for learning to generalize to new environments. The method is based on a priori learning of a curriculum of tasks, which is then used to train an agent on a set of tasks at different training levels. The curriculum is used to evaluate the generalization ability of the agent to new tasks at a given level. The authors show that the curriculum is able to learn a good generalization performance on a variety of environments, including Procgen, MiniGrid, and MiniGrid. ","This paper proposes a method for learning to generalize to new environments. The method is based on a priori learning of a curriculum of tasks, which is then used to train an agent on a set of tasks at different training levels. The curriculum is used to evaluate the generalization ability of the agent to new tasks at a given level. The authors show that the curriculum is able to learn a good generalization performance on a variety of environments, including Procgen, MiniGrid, and MiniGrid. "
198,SP:fd92d766a7721a411ff8c422bec18391d028fa78,"pre - training CONJUNCTION multitask learning. multitask learning CONJUNCTION pre - training. deep learning USED-FOR data - rich settings. pre - training USED-FOR tasks. multitask learning USED-FOR tasks. automatic differentiation procedures CONJUNCTION randomized singular value decomposition. randomized singular value decomposition CONJUNCTION automatic differentiation procedures. randomized singular value decomposition USED-FOR scalability. scalability EVALUATE-FOR method. automatic differentiation procedures USED-FOR method. randomized singular value decomposition USED-FOR method. approach COMPARE baselines. baselines COMPARE approach. out - of - distribution data USED-FOR Text and Image classification tasks. Text and Image classification tasks EVALUATE-FOR approach. out - of - distribution data EVALUATE-FOR baselines. out - of - distribution data EVALUATE-FOR approach. OtherScientificTerm are model parameterizations, auxiliary tasks, auxiliary task gradients, auxiliary updates, and primary task loss. Method is modelagnostic framework. Generic are algorithm, and framework. ","This paper proposes a model-agnostic framework for multi-task learning. The proposed method is based on the idea of auxiliary task gradients, which is an extension of the dual task loss framework. The main idea is to learn a set of auxiliary tasks that are independent of the primary task. The auxiliary tasks are then used to train the model. The authors show that the proposed method outperforms the baselines on out-of-distribution (OOD) classification and text classification tasks. ","This paper proposes a model-agnostic framework for multi-task learning. The proposed method is based on the idea of auxiliary task gradients, which is an extension of the dual task loss framework. The main idea is to learn a set of auxiliary tasks that are independent of the primary task. The auxiliary tasks are then used to train the model. The authors show that the proposed method outperforms the baselines on out-of-distribution (OOD) classification and text classification tasks. "
207,SP:8eb8c34e56de137bfc32ea0fd8cd94e4bff5907d,"large text - based neural language models USED-FOR one - shot learning. RL algorithms USED-FOR one - shot word learning. short - term, within - episode knowledge CONJUNCTION long - term lexical and motor knowledge. long - term lexical and motor knowledge CONJUNCTION short - term, within - episode knowledge. memory writing mechanism USED-FOR one - shot word - object binding. dual - coding memory USED-FOR intrinsic motivation. deep neural networks USED-FOR fast - mapping. episodic memory CONJUNCTION multi - modal environment. multi - modal environment CONJUNCTION episodic memory. meta - learning CONJUNCTION episodic memory. episodic memory CONJUNCTION meta - learning. multi - modal environment USED-FOR fast - mapping. transformative capacity FEATURE-OF artificial agents. human cognitive development CONJUNCTION transformative capacity. transformative capacity CONJUNCTION human cognitive development. fast - mapping HYPONYM-OF human cognitive development. meta - learning USED-FOR deep neural networks. multi - modal environment USED-FOR deep neural networks. episodic memory USED-FOR deep neural networks. Material is simulated 3D world. OtherScientificTerm are dual - coding external memory, visual perception and language, and ShapeNet category. ","This paper proposes a novel method for one-shot word learning based on a dual-coding memory. The proposed method is based on meta-learning and episodic memory, and is able to achieve state-of-the-art performance in the ShapeNet category. The authors show that the proposed method can be applied to both visual perception and language tasks. ","This paper proposes a novel method for one-shot word learning based on a dual-coding memory. The proposed method is based on meta-learning and episodic memory, and is able to achieve state-of-the-art performance in the ShapeNet category. The authors show that the proposed method can be applied to both visual perception and language tasks. "
216,SP:9bd3d99bce743d356eb18692ef93365c78e5fcec,"Few - shot learning USED-FOR models. support set USED-FOR setup. class - imbalance HYPONYM-OF dynamic nature of the real world. backbones USED-FOR few - shot learning methods. strategies USED-FOR imbalance. feature - transfer CONJUNCTION metric - based methods. metric - based methods CONJUNCTION feature - transfer. strategies USED-FOR few - shot case. balanced task COMPARE class - imbalance counterparts. class - imbalance counterparts COMPARE balanced task. imbalance FEATURE-OF supervised learning. strategies USED-FOR supervised learning. imbalance COMPARE support set level. support set level COMPARE imbalance. imbalance FEATURE-OF dataset level. dataset level COMPARE support set level. support set level COMPARE dataset level. class - imbalance counterparts COMPARE optimization - based methods. optimization - based methods COMPARE class - imbalance counterparts. OtherScientificTerm are few - shot class - imbalance, dataset vs. support set imbalance, and imbalance distributions. Method is rebalancing techniques. ","This paper studies the problem of class-imbalance in few-shot learning. The authors propose two strategies to deal with the problem. First, the authors propose a new class-balanced learning method, which is based on the idea that the class imbalance is a function of the support set imbalance. Second, they propose a class-unbalanced learning strategy, which uses the class-over-class imbalance as a regularizer to improve the performance of the model. ","This paper studies the problem of class-imbalance in few-shot learning. The authors propose two strategies to deal with the problem. First, the authors propose a new class-balanced learning method, which is based on the idea that the class imbalance is a function of the support set imbalance. Second, they propose a class-unbalanced learning strategy, which uses the class-over-class imbalance as a regularizer to improve the performance of the model. "
225,SP:2a9cbbe3661d2f02f71472d0111f22a739412226,"convolution operators USED-FOR representations of graphs. Graph Convolutional Neural Networks ( GCNs ) USED-FOR representations of graphs. convolution operators USED-FOR Graph Convolutional Neural Networks ( GCNs ). neighborhood aggregating scheme USED-FOR convolution operators. local topological information USED-FOR convolution operators. decoupled representations USED-FOR them. graph convolution layer USED-FOR neighbouring nodes. topological distances FEATURE-OF neighbouring nodes. readout layers USED-FOR representations. convolution operators CONJUNCTION linear stacking. linear stacking CONJUNCTION convolution operators. Polynomial Graph Convolution ( PGC ) layer COMPARE convolution operators. convolution operators COMPARE Polynomial Graph Convolution ( PGC ) layer. Polynomial Graph Convolution ( PGC ) layer USED-FOR strategy. receptive field FEATURE-OF convolution operator. single PGC layer USED-FOR Graph Neural Network architecture. graph classification benchmarks EVALUATE-FOR Graph Neural Network architecture. OtherScientificTerm are wider topological receptive fields, and GC parameters. Method is non - linear graph convolutions. Metric is neural network expressiveness. ","This paper proposes a novel graph convolutional neural network (GCN) architecture based on the Polynomial Graph Convolution (PGC) layer. In particular, the authors propose a novel neighborhood aggregating scheme to improve the expressiveness of convolution operators. The proposed method is evaluated on several graph classification benchmarks. ","This paper proposes a novel graph convolutional neural network (GCN) architecture based on the Polynomial Graph Convolution (PGC) layer. In particular, the authors propose a novel neighborhood aggregating scheme to improve the expressiveness of convolution operators. The proposed method is evaluated on several graph classification benchmarks. "
234,SP:b0a6873eb4bbf5cdc4a5dfa08782225ae91fc589,labeled datasets USED-FOR SG generation techniques. neural network models COMPARE real data. real data COMPARE neural network models. synthetic data USED-FOR neural network models. scalable technique USED-FOR sim - to - real transfer. sim - to - real transfer USED-FOR scene graph generation. Sim2SG HYPONYM-OF scalable technique. scalable technique USED-FOR scene graph generation. Sim2SG USED-FOR domain gap. supervision PART-OF real - world dataset. supervision USED-FOR Sim2SG. baselines USED-FOR domain gap. toy simulators CONJUNCTION realistic simulators. realistic simulators CONJUNCTION toy simulators. real - world data USED-FOR realistic simulators. realistic simulators EVALUATE-FOR approach. toy simulators EVALUATE-FOR approach. Task is Scene graph ( SG ) generation. Material is Synthetic data. OtherScientificTerm is appearance. Generic is discrepancies. ,This paper proposes a new method for scene graph generation based on sim-to-real transfer (Sim2SG). Sim2SG is a scalable technique that can be applied to both synthetic and real-world datasets. The authors show that the proposed method is able to achieve state-of-the-art results on toy and realistic simulators. ,This paper proposes a new method for scene graph generation based on sim-to-real transfer (Sim2SG). Sim2SG is a scalable technique that can be applied to both synthetic and real-world datasets. The authors show that the proposed method is able to achieve state-of-the-art results on toy and realistic simulators. 
243,SP:ccc72f26d0637476d01671c147b5cb5d30fa8c2d,model - based methods COMPARE model - free methods. model - free methods COMPARE model - based methods. sample efficiency EVALUATE-FOR model - free methods. model - free methods USED-FOR continuous - action DRL benchmarks. continuous - action DRL benchmarks EVALUATE-FOR model - based methods. sample efficiency EVALUATE-FOR model - based methods. modelbased algorithm USED-FOR MuJoCo benchmark. REDQ COMPARE model - based method. model - based method COMPARE REDQ. parameters USED-FOR model - based method. wall - clock run time EVALUATE-FOR REDQ. parameters USED-FOR REDQ. random subset of Q functions PART-OF ensemble. REDQ USED-FOR it. ensemble of Q functions CONJUNCTION in - target minimization. in - target minimization CONJUNCTION ensemble of Q functions. random subset of Q functions USED-FOR in - target minimization. REDQ CONJUNCTION model - free algorithms. model - free algorithms CONJUNCTION REDQ. model - free DRL algorithm USED-FOR continuous - action spaces. REDQ HYPONYM-OF model - free DRL algorithm. REDQ USED-FOR continuous - action spaces. UTD ratio 1 USED-FOR model - free DRL algorithm. UTD ratio 1 USED-FOR REDQ. Method is modelfree algorithm. OtherScientificTerm is UTD ratio. ,"This paper proposes a model-free DRL algorithm for continuous-action reinforcement learning (DRL). The proposed algorithm is based on the idea of ensemble of Q functions, which is an extension of model-based DRL. The proposed method is evaluated on MuJoCo benchmark and is shown to outperform the model based DRL algorithms in terms of wall-clock run time and sample efficiency.","This paper proposes a model-free DRL algorithm for continuous-action reinforcement learning (DRL). The proposed algorithm is based on the idea of ensemble of Q functions, which is an extension of model-based DRL. The proposed method is evaluated on MuJoCo benchmark and is shown to outperform the model based DRL algorithms in terms of wall-clock run time and sample efficiency."
252,SP:c424d050996a7f383d2f12418dfdcea90d94ea65,"deep learning USED-FOR classification or regression. probability distributions USED-FOR deep learning. distribution samples USED-FOR classification or regression. Lipschitz - bounded transformations of the input distribution FEATURE-OF robustness. tasks EVALUATE-FOR approach. DIDA USED-FOR meta - features. DIDA USED-FOR labelled ) dataset. meta - features USED-FOR labelled ) dataset. DIDA USED-FOR tasks. SVM CONJUNCTION logistic regression. logistic regression CONJUNCTION SVM. logistic regression CONJUNCTION linear SGD. linear SGD CONJUNCTION logistic regression. k - NN CONJUNCTION SVM. SVM CONJUNCTION k - NN. hyper - parameter configuration COMPARE configuration. configuration COMPARE hyper - parameter configuration. fixed algorithm USED-FOR hyper - parameter configuration. hyper - parameter configuration USED-FOR learning. dataset EVALUATE-FOR configuration. OpenML benchmarking suite USED-FOR dataset. SVM HYPONYM-OF fixed algorithm. DSS CONJUNCTION DATASET2VEC architectures. DATASET2VEC architectures CONJUNCTION DSS. tasks EVALUATE-FOR DIDA. tasks EVALUATE-FOR models. DIDA COMPARE models. models COMPARE DIDA. DIDA COMPARE DATASET2VEC architectures. DATASET2VEC architectures COMPARE DIDA. tasks EVALUATE-FOR DATASET2VEC architectures. DIDA COMPARE DSS. DSS COMPARE DIDA. DATASET2VEC architectures CONJUNCTION models. models CONJUNCTION DATASET2VEC architectures. hand - crafted meta - features USED-FOR models. OtherScientificTerm are permutation of the samples, and permutation of the features. Method are neural architectures, and universal approximation. Generic are architecture, and task. ","This paper proposes a meta-feature learning method for classification and regression tasks. The proposed method is based on Lipschitz-bounded transformations of the input distribution. The authors show that the proposed method can be applied to a wide range of deep learning tasks, including logistic regression, SVM, linear SGD, and k-NN. The method is evaluated on OpenML benchmarking suite and compared to DSS, DATASET2VEC, and other baselines.","This paper proposes a meta-feature learning method for classification and regression tasks. The proposed method is based on Lipschitz-bounded transformations of the input distribution. The authors show that the proposed method can be applied to a wide range of deep learning tasks, including logistic regression, SVM, linear SGD, and k-NN. The method is evaluated on OpenML benchmarking suite and compared to DSS, DATASET2VEC, and other baselines."
261,SP:3e5d5b61dceca85c444b3d0d06577229c3146664,data clustering CONJUNCTION visualization. visualization CONJUNCTION data clustering. dimensionality reduction CONJUNCTION data clustering. data clustering CONJUNCTION dimensionality reduction. data representation and analysis CONJUNCTION dimensionality reduction. dimensionality reduction CONJUNCTION data representation and analysis. manifold learning CONJUNCTION data representation and analysis. data representation and analysis CONJUNCTION manifold learning. Graph learning USED-FOR data mining and machine learning tasks. visualization HYPONYM-OF data mining and machine learning tasks. manifold learning HYPONYM-OF data mining and machine learning tasks. data clustering HYPONYM-OF data mining and machine learning tasks. data representation and analysis HYPONYM-OF data mining and machine learning tasks. dimensionality reduction HYPONYM-OF data mining and machine learning tasks. approach USED-FOR ultra - sparse undirected graphs. graphLaplacian - like matrix PART-OF graphical Lasso. graphLaplacian - like matrix FEATURE-OF precision matrix. high - dimensional input data USED-FOR ultra - sparse undirected graphs. GRASPEL USED-FOR graphs. spectrally - critical edges PART-OF graph. nearly - linear time spectral methods USED-FOR ultrasparse yet spectrally - robust graphs. spectral clustering ( SC ) CONJUNCTION dimensionality reduction. dimensionality reduction CONJUNCTION spectral clustering ( SC ). graph learning approaches COMPARE GRASPEL. GRASPEL COMPARE graph learning approaches. manifold learning CONJUNCTION spectral clustering ( SC ). spectral clustering ( SC ) CONJUNCTION manifold learning. computing efficiency CONJUNCTION solution quality. solution quality CONJUNCTION computing efficiency. GRASPEL USED-FOR data mining and machine learning applications. solution quality EVALUATE-FOR data mining and machine learning applications. computing efficiency EVALUATE-FOR data mining and machine learning applications. solution quality EVALUATE-FOR GRASPEL. computing efficiency EVALUATE-FOR GRASPEL. dimensionality reduction HYPONYM-OF data mining and machine learning applications. manifold learning HYPONYM-OF data mining and machine learning applications. spectral clustering ( SC ) HYPONYM-OF data mining and machine learning applications. Method is spectral graph densification approach,"This paper proposes a new graph learning method for ultra-sparse undirected graphs. The proposed method is based on the graphLaplacian-like matrix, which can be used to reduce the dimensionality of a graph to a precision matrix. The authors show that the proposed method outperforms existing graph learning methods in terms of computing efficiency and solution quality. They also show that their method can be applied to data mining and machine learning applications.","This paper proposes a new graph learning method for ultra-sparse undirected graphs. The proposed method is based on the graphLaplacian-like matrix, which can be used to reduce the dimensionality of a graph to a precision matrix. The authors show that the proposed method outperforms existing graph learning methods in terms of computing efficiency and solution quality. They also show that their method can be applied to data mining and machine learning applications."
270,SP:7e6c73a642a8b3d64156c1d0ecf11f84e7222a22,"images CONJUNCTION text descriptions. text descriptions CONJUNCTION images. deep reinforcement learning USED-FOR goal - conditioned policy. explicit embedding space USED-FOR nonparametric distance. abstract - level policy CONJUNCTION goal - conditioned policy. goal - conditioned policy CONJUNCTION abstract - level policy. unsupervised learning approach USED-FOR goal - conditioned policy. unsupervised learning approach USED-FOR abstract - level policy. intrinsic motivation ( GPIM ) FEATURE-OF goal - conditioned policy. goal - conditioned policy HYPONYM-OF unsupervised learning approach. discriminator USED-FOR abstract - level policy. latent variable USED-FOR abstract - level policy. discriminator USED-FOR intrinsic reward function. intrinsic reward function USED-FOR goal - conditioned policy. intrinsic reward function USED-FOR trajectory. discriminator USED-FOR goal - conditioned policy. discriminator USED-FOR trajectory. abstract - level policy USED-FOR trajectory. robotic tasks EVALUATE-FOR GPIM method. GPIM method COMPARE prior techniques. prior techniques COMPARE GPIM method. robotic tasks EVALUATE-FOR prior techniques. OtherScientificTerm are perceptually - specific goals, and hand - crafted rewards. Generic is policy. ","This paper proposes an unsupervised learning approach for goal-conditioned reinforcement learning. The key idea is to learn an abstract-level policy and a goal-conditional policy using a discriminator. The discriminator is trained to predict the intrinsic reward function of the goal-constrained policy, and the abstract level policy is used to learn a trajectory. The paper shows that the proposed method outperforms the state-of-the-art methods on a number of tasks.","This paper proposes an unsupervised learning approach for goal-conditioned reinforcement learning. The key idea is to learn an abstract-level policy and a goal-conditional policy using a discriminator. The discriminator is trained to predict the intrinsic reward function of the goal-constrained policy, and the abstract level policy is used to learn a trajectory. The paper shows that the proposed method outperforms the state-of-the-art methods on a number of tasks."
279,SP:bdf293bf2118a927cbec6b96be03bfcad0243640,"policy switches HYPONYM-OF low switching cost. low switching cost USED-FOR deep reinforcement learning problems. robotics CONJUNCTION dialogue agents. dialogue agents CONJUNCTION robotics. recommendation systems CONJUNCTION education. education CONJUNCTION recommendation systems. education CONJUNCTION robotics. robotics CONJUNCTION education. medical domains CONJUNCTION recommendation systems. recommendation systems CONJUNCTION medical domains. recommendation systems CONJUNCTION robotics. robotics CONJUNCTION recommendation systems. dialogue agents HYPONYM-OF applications. medical domains HYPONYM-OF applications. robotics HYPONYM-OF applications. education HYPONYM-OF applications. recommendation systems HYPONYM-OF applications. Q - network CONJUNCTION learning Q - network. learning Q - network CONJUNCTION Q - network. deep Q - networks USED-FOR policy switching criteria. feature distance USED-FOR adaptive approach. medical treatment environment CONJUNCTION Atari games. Atari games CONJUNCTION medical treatment environment. switching cost EVALUATE-FOR feature - switching criterion. sample efficiency EVALUATE-FOR feature - switching criterion. OtherScientificTerm are low - switching - cost constraint, and representation learning perspective. ",This paper studies the problem of policy switching in reinforcement learning. The authors propose a novel feature-switching criterion for policy switching based on the representation learning perspective. The proposed criterion is based on an adaptive adaptive approach to the feature switching criteria. The paper also proposes a learning Q-network to learn the feature distance between the policy switching criteria and the feature learning criteria. Experiments show that the proposed criterion improves sample efficiency and sample complexity. ,This paper studies the problem of policy switching in reinforcement learning. The authors propose a novel feature-switching criterion for policy switching based on the representation learning perspective. The proposed criterion is based on an adaptive adaptive approach to the feature switching criteria. The paper also proposes a learning Q-network to learn the feature distance between the policy switching criteria and the feature learning criteria. Experiments show that the proposed criterion improves sample efficiency and sample complexity. 
288,SP:d06bef9ee5e9bdda1571478b6a8a7a2d3ab42f1b,First - order stochastic methods USED-FOR large - scale non - convex optimization problems. First - order stochastic methods USED-FOR big - data applications. deep neural networks HYPONYM-OF big - data applications. homotopy methods CONJUNCTION SGD. SGD CONJUNCTION homotopy methods. diffusion CONJUNCTION mollifying networks. mollifying networks CONJUNCTION diffusion. Gaussian continuation USED-FOR optimization. diffusion HYPONYM-OF heuristics. mollifying networks HYPONYM-OF heuristics. optimization HYPONYM-OF heuristics. homotopy methods USED-FOR first - order stochastic algorithm. SGD USED-FOR first - order stochastic algorithm. scheme USED-FOR H - SGD. scheme USED-FOR homotopy parameter. fast and inexpensive iterations FEATURE-OF H - SGD. global linear rate of convergence EVALUATE-FOR H - SGD. H - SGD COMPARE SGD. SGD COMPARE H - SGD. Metric is slow global convergence rate. OtherScientificTerm is neighborhood of a minimizer. Generic is algorithm. ,"This paper studies the problem of solving non-convex optimization problems with homotopy. The authors propose a new algorithm called H-SGD, which is a homotopic version of SGD. The main idea is to use a Gaussian continuation as the homotopical parameter for the first-order stochastic optimization problem. They show that the proposed algorithm is faster and cheaper than SGD in terms of global convergence rate. ","This paper studies the problem of solving non-convex optimization problems with homotopy. The authors propose a new algorithm called H-SGD, which is a homotopic version of SGD. The main idea is to use a Gaussian continuation as the homotopical parameter for the first-order stochastic optimization problem. They show that the proposed algorithm is faster and cheaper than SGD in terms of global convergence rate. "
297,SP:195d090d9df0bda33103edcbbaf300e43f4562be,Bayesian meta - learning problem USED-FOR shape completion. encoder USED-FOR posterior distribution. encoder USED-FOR latent representation. posterior distribution FEATURE-OF latent representation. sparse cloud USED-FOR latent representation. encoder USED-FOR learning of object shapes. sparse point clouds USED-FOR learning of object shapes. meta - learning algorithm USED-FOR shape completion of newly - encountered objects. object - specific properties CONJUNCTION object - agnostic properties. object - agnostic properties CONJUNCTION object - specific properties. sparse observations USED-FOR shape completion of newly - encountered objects. ICL - NUIM benchmarks EVALUATE-FOR method. Method is shape representations. ,"This paper proposes a meta-learning algorithm for shape completion of newly-discovered objects. The proposed method is based on the idea of Bayesian meta learning. The paper proposes to learn the posterior distribution of the latent representation of a point cloud from a set of sparse observations, and then use this posterior distribution to learn a latent representation for learning the shape of an object. The method is evaluated on the ICL-NUIM benchmark and shows that the proposed method outperforms baselines.","This paper proposes a meta-learning algorithm for shape completion of newly-discovered objects. The proposed method is based on the idea of Bayesian meta learning. The paper proposes to learn the posterior distribution of the latent representation of a point cloud from a set of sparse observations, and then use this posterior distribution to learn a latent representation for learning the shape of an object. The method is evaluated on the ICL-NUIM benchmark and shows that the proposed method outperforms baselines."
306,SP:ca637a2692cf2424d1ec5c7d2051c7881a5816f4,"adversarial examples COMPARE natural examples. natural examples COMPARE adversarial examples. adversarial examples COMPARE natural examples. natural examples COMPARE adversarial examples. activation magnitudes FEATURE-OF adversarial examples. channels COMPARE natural examples. natural examples COMPARE channels. channel - wise activation perspective FEATURE-OF adversarial examples. defense adversarial training USED-FOR activation magnitudes. CAS USED-FOR model. model USED-FOR adversarial activation. robustness EVALUATE-FOR defense methods. OtherScientificTerm are uniform activation, redundant activation, and adversarial perturbations. Method is intermediate layer activation of DNNs. ",This paper studies the problem of adversarial training of DNNs. The authors propose a method to improve the robustness of the model to adversarial perturbations. The proposed method is based on the idea that the activation magnitudes of the adversarial examples can be computed from the channel-wise activation perspective. They show that this is the case for both natural and adversarial samples. They also show that the model can be trained to be more robust against adversarial attacks. ,This paper studies the problem of adversarial training of DNNs. The authors propose a method to improve the robustness of the model to adversarial perturbations. The proposed method is based on the idea that the activation magnitudes of the adversarial examples can be computed from the channel-wise activation perspective. They show that this is the case for both natural and adversarial samples. They also show that the model can be trained to be more robust against adversarial attacks. 
315,SP:a50e9aeb17340b141f7b88d522911a5c9229f7d3,"random initialization USED-FOR gradient descent. generalization EVALUATE-FOR Neural networks. gradient descent USED-FOR Neural networks. random initialization USED-FOR Neural networks. Neural Tangent Kernel ( NTK ) USED-FOR implicit regularization effect. gradient flow / descent USED-FOR infinitely wide neural networks. implicit regularization effect FEATURE-OF gradient flow / descent. random initialization USED-FOR gradient flow / descent. initialization CONJUNCTION optimization. optimization CONJUNCTION initialization. generalization performance CONJUNCTION initialization. initialization CONJUNCTION generalization performance. optimization USED-FOR finite width networks. initialization USED-FOR finite width networks. optimization CONJUNCTION overparametrization. overparametrization CONJUNCTION optimization. initialization CONJUNCTION optimization. optimization CONJUNCTION initialization. generalization performance EVALUATE-FOR overparametrization. hidden layer width CONJUNCTION scaled ) random initialization. scaled ) random initialization CONJUNCTION hidden layer width. low - dimensional manifold FEATURE-OF network parameters. min - norm solution USED-FOR linear case. O(h−1/2 ) upper - bound FEATURE-OF operator norm distance. network CONJUNCTION min - norm solution. min - norm solution CONJUNCTION network. operator norm distance FEATURE-OF network. operator norm distance EVALUATE-FOR min - norm solution. OtherScientificTerm are regularization, and imbalance of the network weights. Method are non - asymptotic analysis, overparametrized single - hidden layer linear networks, and gradient flow. Metric are squared loss, and convergence rate. Generic is manifold. ","This paper studies the implicit regularization effect of gradient flow/decay in neural networks. The authors consider the case of infinite-width neural networks with random initialization. They show that the convergence rate of the gradient flow is O(h−1/2) for the linear case and O(1/\sqrt{n}^n) for non-linear case. They also show that for finite-width networks, the generalization performance of the network is bounded by the operator norm distance between the network weights. ","This paper studies the implicit regularization effect of gradient flow/decay in neural networks. The authors consider the case of infinite-width neural networks with random initialization. They show that the convergence rate of the gradient flow is O(h−1/2) for the linear case and O(1/\sqrt{n}^n) for non-linear case. They also show that for finite-width networks, the generalization performance of the network is bounded by the operator norm distance between the network weights. "
324,SP:7341f8e456c0b80a59595f1cc145b776add3db3f,Deep networks COMPARE shallow ones. shallow ones COMPARE Deep networks. approximation EVALUATE-FOR shallow ones. tractable algorithms USED-FOR deep models. deep networks COMPARE shallow ones. shallow ones COMPARE deep networks. approximation USED-FOR kernels. tractable ) kernel methods USED-FOR over - parameterized regime. gradient descent USED-FOR deep networks. architecture USED-FOR kernel. eigenvalue decay FEATURE-OF integral operator. kernels COMPARE shallow ” two - layer counterpart. shallow ” two - layer counterpart COMPARE kernels. kernels USED-FOR ReLU activations. deep fully - connected networks USED-FOR kernels. approximation properties EVALUATE-FOR kernels. kernel framework USED-FOR deep architectures. differentiability properties FEATURE-OF kernel function. sphere FEATURE-OF kernels. kernel function USED-FOR eigenvalue decays. differentiability properties USED-FOR eigenvalue decays. ,This paper studies the kernel approximation properties of deep neural networks. The authors provide tractable kernels for the over-parameterized regime. They show that the kernel function of a deep network can be approximated by the kernel of a two-layer network. They also show that kernels are tractable for deep networks with ReLU activations. ,This paper studies the kernel approximation properties of deep neural networks. The authors provide tractable kernels for the over-parameterized regime. They show that the kernel function of a deep network can be approximated by the kernel of a two-layer network. They also show that kernels are tractable for deep networks with ReLU activations. 
333,SP:3dd495394b880cf2fa055ee3fe218477625d2605,amplified value estimates CONJUNCTION suboptimal policies. suboptimal policies CONJUNCTION amplified value estimates. overestimation problem PART-OF deep value learning. underestimation bias CONJUNCTION instability. instability CONJUNCTION underestimation bias. algorithm USED-FOR overestimation. overestimation issues FEATURE-OF continuous control. algorithm USED-FOR policy improvement. deep reinforcement learning USED-FOR continuous control. deep reinforcement learning USED-FOR overestimation issues. combined value of weighted critics USED-FOR policy. weight factor USED-FOR independent critics. method USED-FOR policy improvement. algorithms USED-FOR continuous control. algorithms COMPARE algorithms. algorithms COMPARE algorithms. classical control tasks EVALUATE-FOR method. OtherScientificTerm is function approximation errors. ,"This paper studies the problem of overestimation in deep reinforcement learning. The authors propose a method to mitigate the overestimation issue. The proposed method is based on the idea of weighted critics, where each critic is a weighted sum of independent critics. The method is evaluated on a variety of continuous control tasks and shows that the proposed method outperforms existing methods.","This paper studies the problem of overestimation in deep reinforcement learning. The authors propose a method to mitigate the overestimation issue. The proposed method is based on the idea of weighted critics, where each critic is a weighted sum of independent critics. The method is evaluated on a variety of continuous control tasks and shows that the proposed method outperforms existing methods."
342,SP:a7f72a5f99f2e3e1a643e9bb83bf0416a859ec06,"inverse reinforcement learning ( IRL ) problem USED-FOR reward functions. expert demonstrations USED-FOR reward functions. policy USED-FOR reward functions. expert demonstrations USED-FOR policies. ill - posed inverse problem HYPONYM-OF IRL problem. IRL problem USED-FOR well - posed expectation optimization problem. solution USED-FOR SIRL problem. solution USED-FOR learning task. solutions USED-FOR IRL problem. solution USED-FOR solutions. formulation USED-FOR IRL problem. objectworld EVALUATE-FOR approach. OtherScientificTerm are probability distribution over reward functions, and probability distribution. ","This paper studies the inverse reinforcement learning (IRL) problem, which is a well-posed expectation optimization problem. The authors propose a new formulation of the IRL problem, called the ill-posed inverse problem (ILP), which is an extension of the SIRL problem. In the ILP problem, the agent is given a reward function, and the agent has to learn a policy that maximizes the probability distribution over the reward function.  The authors show that the ILP problem can be formulated as an inverse RL problem, and that the proposed ILP formulation can be used as a learning task.","This paper studies the inverse reinforcement learning (IRL) problem, which is a well-posed expectation optimization problem. The authors propose a new formulation of the IRL problem, called the ill-posed inverse problem (ILP), which is an extension of the SIRL problem. In the ILP problem, the agent is given a reward function, and the agent has to learn a policy that maximizes the probability distribution over the reward function.  The authors show that the ILP problem can be formulated as an inverse RL problem, and that the proposed ILP formulation can be used as a learning task."
351,SP:ee628e3ddc01de3f915b04834245c2250015e4d0,"Self - training algorithms USED-FOR model. model USED-FOR pseudolabels. model USED-FOR pseudolabels. neural networks USED-FOR unlabeled data. self - training USED-FOR linear models. unsupervised domain adaptation CONJUNCTION unsupervised learning. unsupervised learning CONJUNCTION unsupervised domain adaptation. self - training USED-FOR semi - supervised learning. deep networks USED-FOR semi - supervised learning. deep networks USED-FOR unsupervised domain adaptation. semi - supervised learning CONJUNCTION unsupervised domain adaptation. unsupervised domain adaptation CONJUNCTION semi - supervised learning. self - training USED-FOR unsupervised domain adaptation. deep networks USED-FOR self - training. self - training CONJUNCTION input - consistency regularization. input - consistency regularization CONJUNCTION self - training. accuracy EVALUATE-FOR minimizers of population objectives. self - training USED-FOR minimizers of population objectives. input - consistency regularization USED-FOR minimizers of population objectives. margin CONJUNCTION Lipschitzness. Lipschitzness CONJUNCTION margin. sample complexity guarantees FEATURE-OF neural nets. margin FEATURE-OF neural nets. Lipschitzness FEATURE-OF neural nets. input consistency regularization USED-FOR self - training algorithms. OtherScientificTerm are neighborhoods, ground - truth labels, and generalization bounds. Generic is assumptions. ",This paper studies the problem of self-training in the context of unsupervised domain adaptation and semi-supervised learning. The authors consider the case of linear models with Lipschitzness and margin. They show that the sample complexity of the self-trained model is bounded by the margin of the model. They also provide a generalization bound for self-trainers. ,This paper studies the problem of self-training in the context of unsupervised domain adaptation and semi-supervised learning. The authors consider the case of linear models with Lipschitzness and margin. They show that the sample complexity of the self-trained model is bounded by the margin of the model. They also provide a generalization bound for self-trainers. 
360,SP:daa229d78712808420aad4c50604fc28fd2a4aba,"hierarchical models USED-FOR video prediction. stochastic recurrent estimator USED-FOR long - term prediction. car driving CONJUNCTION human dancing. human dancing CONJUNCTION car driving. it USED-FOR complicated scene structures. video prediction COMPARE approaches. approaches COMPARE video prediction. Generic is method. OtherScientificTerm are semantic structures, structures, and discrete semantic structure space. Method is videoto - video translation. ","This paper proposes a method for video prediction based on hierarchical models. The proposed method is based on a stochastic recurrent estimator (SRM), which is used to estimate the long-term prediction of a video. The method is evaluated on a variety of video prediction tasks, including car driving, human dancing, and video translation. ","This paper proposes a method for video prediction based on hierarchical models. The proposed method is based on a stochastic recurrent estimator (SRM), which is used to estimate the long-term prediction of a video. The method is evaluated on a variety of video prediction tasks, including car driving, human dancing, and video translation. "
369,SP:e50b1931800daa7de577efd3edca523771227b3f,"undirected graph CONJUNCTION directed graph. directed graph CONJUNCTION undirected graph. Iterated Graph Neural Network System ( IGNNS ) HYPONYM-OF Graph Neural Networks ( GNNs ). Iterated Function System ( IFS ) HYPONYM-OF fractal geometry. Iterated Function System ( IFS ) PART-OF IGNNS. adjoint probability vector USED-FOR IFS layer. affine transformations USED-FOR IGNNS. geometric properties FEATURE-OF IGNNS. dynamical system USED-FOR IGNNS. dynamical system USED-FOR geometric properties. Frobenius norm FEATURE-OF constant matrix. IGNNS USED-FOR IFS. Hausdorff distance FEATURE-OF fractal set of IFS. Cora CONJUNCTION PubMed. PubMed CONJUNCTION Cora. citeser CONJUNCTION Cora. Cora CONJUNCTION citeser. citation network datasets USED-FOR semi - supervised node classification. PubMed HYPONYM-OF citation network datasets. citeser HYPONYM-OF citation network datasets. Cora HYPONYM-OF citation network datasets. OtherScientificTerm are graph nodes, latent space, and node features. Method are high - level representation of graph nodes, and fractal representation of graph nodes. Generic is method. ","This paper studies the problem of learning a high-level representation of graph nodes in the latent space of a graph neural network. The authors propose to use the Iterated Function System (IFS) as an extension of graph neural networks (GNNs). The IFS is a dynamical system that can be viewed as a function of a constant matrix and a Frobenius norm. In particular, the authors show that the Hausdorff distance of the IFS can be represented as a fractal set of IFS, which can be used as a representation of the node features. The paper also shows that the Frobius norm of the Frobedius norm can be expressed as the Frobitius distance between the node feature vector and the adjoint probability vector of the graph node. ","This paper studies the problem of learning a high-level representation of graph nodes in the latent space of a graph neural network. The authors propose to use the Iterated Function System (IFS) as an extension of graph neural networks (GNNs). The IFS is a dynamical system that can be viewed as a function of a constant matrix and a Frobenius norm. In particular, the authors show that the Hausdorff distance of the IFS can be represented as a fractal set of IFS, which can be used as a representation of the node features. The paper also shows that the Frobius norm of the Frobedius norm can be expressed as the Frobitius distance between the node feature vector and the adjoint probability vector of the graph node. "
378,SP:89d65999a0600ec4f81daf6232fb5897676b3ce3,"modeling complex relations CONJUNCTION modeling isomorphic graphs. modeling isomorphic graphs CONJUNCTION modeling complex relations. GG - GAN USED-FOR graphs. GG - GAN COMPARE state - of - the - art methods. state - of - the - art methods COMPARE GG - GAN. GG - GAN USED-FOR distribution statistics. Task is graph generation. OtherScientificTerm are similarity function, complex relations, isomorphic graphs, latent distribution, and problem - specific knowledge. Method are geometric interpretation, and Wasserstein GAN. ",This paper proposes a new method for graph generation based on geometric interpretation of the Wasserstein GAN (GG-GAN). The authors show that GG-GAN is able to generate graphs that are isomorphic to complex relations and isomorphistic to isomorphic graphs. The authors also show that the proposed method can be applied to the problem of graph generation. ,This paper proposes a new method for graph generation based on geometric interpretation of the Wasserstein GAN (GG-GAN). The authors show that GG-GAN is able to generate graphs that are isomorphic to complex relations and isomorphistic to isomorphic graphs. The authors also show that the proposed method can be applied to the problem of graph generation. 
387,SP:4f9388c18e44995fb1c6830256c520ff47a2e6ee,"activation values FEATURE-OF network. Minimum Description Length principle USED-FOR problem. rules CONJUNCTION super - charge prototyping. super - charge prototyping CONJUNCTION rules. Generic are method, and they. Method are neural network, and unsupervised EXPLAINN algorithm. OtherScientificTerm are noise - robust rules, class - specific traits, and convolutional layers. Material is activation data. ","This paper proposes an unsupervised method for learning noise-robust rules for neural networks. The authors propose to use the Minimum Description Length (MDSL) principle to solve the problem of learning a set of rules for a class-specific subset of the activation values of a neural network. The proposed method is based on the MDSL principle, and the authors show that the proposed method outperforms the state-of-the-art baselines. ","This paper proposes an unsupervised method for learning noise-robust rules for neural networks. The authors propose to use the Minimum Description Length (MDSL) principle to solve the problem of learning a set of rules for a class-specific subset of the activation values of a neural network. The proposed method is based on the MDSL principle, and the authors show that the proposed method outperforms the state-of-the-art baselines. "
396,SP:fc75d8d62ac5cc4cdde1b923ae54659a0dfba28b,"worst - case guarantees FEATURE-OF fixed - dataset policy optimization algorithms. algorithms USED-FOR regime. principle USED-FOR algorithms. tabular gridworld CONJUNCTION deep learning. deep learning CONJUNCTION tabular gridworld. MinAtar environments EVALUATE-FOR deep learning. Method are naı̈ve approaches, pessimism principle, and pessimistic algorithms. OtherScientificTerm are erroneous value overestimation, and policy. ",This paper studies the worst-case guarantees of fixed-dataset policy optimization algorithms under the pessimism principle. The authors show that the pessimistic algorithms are guaranteed to converge to the worst case under certain assumptions. They also show that pessimistic algorithms can be used to improve the performance of deep learning and deep learning-based deep learning algorithms. ,This paper studies the worst-case guarantees of fixed-dataset policy optimization algorithms under the pessimism principle. The authors show that the pessimistic algorithms are guaranteed to converge to the worst case under certain assumptions. They also show that pessimistic algorithms can be used to improve the performance of deep learning and deep learning-based deep learning algorithms. 
405,SP:363661edd15a06a800b51abc1541a3191311ee0e,"Neural ordinary differential equations ( Neural ODEs ) HYPONYM-OF deeplearning models. continuous depth FEATURE-OF deeplearning models. naive method CONJUNCTION adaptive checkpoint adjoint method ( ACA ). adaptive checkpoint adjoint method ( ACA ) CONJUNCTION naive method. continuous case FEATURE-OF numerical estimation of the gradient. accuracy EVALUATE-FOR gradient estimation. accuracy EVALUATE-FOR reverse - time trajectory. constant memory cost FEATURE-OF ALF Integrator ( MALI ). heavy memory burden CONJUNCTION inaccuracy. inaccuracy CONJUNCTION heavy memory burden. MALI COMPARE ResNet. ResNet COMPARE MALI. MALI COMPARE adjoint method. adjoint method COMPARE MALI. MALI USED-FOR Neural ODE. MALI COMPARE methods. methods COMPARE MALI. image recognition tasks EVALUATE-FOR MALI. tasks EVALUATE-FOR MALI. ImageNet USED-FOR Neural ODE. image recognition tasks HYPONYM-OF tasks. tasks EVALUATE-FOR MALI. MALI USED-FOR continuous generative models. image recognition tasks EVALUATE-FOR MALI. adjoint method USED-FOR time series modeling. MALI USED-FOR time series modeling. Metric is memory cost. OtherScientificTerm are integration time, and solver steps. Method are asynchronous leapfrog ( ALF ) solver, and pypi package. ",This paper proposes an asynchronous leapfrog (ALF) solver for Neural ODEs. The proposed method is based on the adaptive checkpoint adjoint method (ACA) and the naive method (ACA). The authors show that the proposed method has a constant memory cost and is able to achieve better accuracy than existing methods. The authors also show that their method can be applied to continuous generative models.,This paper proposes an asynchronous leapfrog (ALF) solver for Neural ODEs. The proposed method is based on the adaptive checkpoint adjoint method (ACA) and the naive method (ACA). The authors show that the proposed method has a constant memory cost and is able to achieve better accuracy than existing methods. The authors also show that their method can be applied to continuous generative models.
414,SP:45b6d522ed9a2ecda2db0a3d45688ed3b0f32875,methodology EVALUATE-FOR complex scene conditional generation models. model USED-FOR seen conditionings. seen object combinations FEATURE-OF unseen conditionings. unseen object combinations USED-FOR unseen conditionings. methods USED-FOR recognizable scenes. compositionality USED-FOR unseen conditionings. compositionality USED-FOR methods. seen conditionings USED-FOR methods. seen object combinations USED-FOR unseen conditionings. unseen object combinations FEATURE-OF conditionings. image quality degradation EVALUATE-FOR methods. semantically aware losses USED-FOR generation process. robustness EVALUATE-FOR unseen conditionings. robustness FEATURE-OF unseen conditionings. instance - wise spatial conditioning normalizations USED-FOR compositionality. scene - graph perceptual similarity HYPONYM-OF semantically aware losses. Generic is models. Method is pipeline components. ,"This paper proposes a method to improve the robustness of scene conditional generation models to unseen conditionings. The proposed method is based on the idea of compositionality, which is an important aspect of scene-graph perceptual similarity. The authors propose to use instance-wise spatial conditioning normalization to reduce the perceptual similarity between unseen and seen conditionings in the generation process. The method is evaluated on a variety of image quality degradation and robustness metrics. ","This paper proposes a method to improve the robustness of scene conditional generation models to unseen conditionings. The proposed method is based on the idea of compositionality, which is an important aspect of scene-graph perceptual similarity. The authors propose to use instance-wise spatial conditioning normalization to reduce the perceptual similarity between unseen and seen conditionings in the generation process. The method is evaluated on a variety of image quality degradation and robustness metrics. "
423,SP:77bce8c5d383f6be82ebc694bf66fb1a408ad751,"expressive power CONJUNCTION learning. learning CONJUNCTION expressive power. multi - hop operators FEATURE-OF graph. multi - hop operators USED-FOR node features. GA - MLPs USED-FOR non - isomorphic graphs. WeisfeilerLehman ( WL ) test CONJUNCTION GNNs. GNNs CONJUNCTION WeisfeilerLehman ( WL ) test. operators USED-FOR GA - MLPs. GA - MLPs CONJUNCTION GNNs. GNNs CONJUNCTION GA - MLPs. expressive power EVALUATE-FOR GNNs. expressive power EVALUATE-FOR GA - MLPs. node - level functions USED-FOR them. GNNs COMPARE GA - MLPs. GA - MLPs COMPARE GNNs. GA - MLPs USED-FOR attributed walks. community detection EVALUATE-FOR GA - MLPs. GNNs USED-FOR learning. operator family USED-FOR GA - MLPs. Generic is alternative. Method is learnable node - wise functions. OtherScientificTerm are node - wise functions, and rooted graphs. Task is graph isomorphism testing. ","This paper proposes a new method for graph isomorphism testing based on the Weisfeiler-Lehman (WL) test. The proposed method is based on GNNs and GA-MLPs. The main contribution of the paper is the introduction of the multi-hop operator family, which is a family of operators that can be used to learn node-wise functions. The authors show that the proposed method outperforms existing GNN-based methods on the WL test and community detection tasks.","This paper proposes a new method for graph isomorphism testing based on the Weisfeiler-Lehman (WL) test. The proposed method is based on GNNs and GA-MLPs. The main contribution of the paper is the introduction of the multi-hop operator family, which is a family of operators that can be used to learn node-wise functions. The authors show that the proposed method outperforms existing GNN-based methods on the WL test and community detection tasks."
432,SP:5c0783e92017fc808ebd44a7d1aa7f6b92baacd8,"model complexity EVALUATE-FOR Reinforcement Learning ( RL ) agents. real - world applications EVALUATE-FOR Reinforcement Learning ( RL ) agents. robotics HYPONYM-OF real - world applications. acting USED-FOR distributed RL settings. unaccelerated hardware USED-FOR acting. CPUs HYPONYM-OF unaccelerated hardware. model complexity EVALUATE-FOR supervised learning. distillation USED-FOR learning progress. large capacity learner model COMPARE small capacity actor model. small capacity actor model COMPARE large capacity learner model. system USED-FOR acting. transformer models COMPARE LSTMs. LSTMs COMPARE transformer models. procedure USED-FOR partially - observable environments. computational complexity EVALUATE-FOR transformer models. transformer models CONJUNCTION LSTMs. LSTMs CONJUNCTION transformer models. fast inference CONJUNCTION total training time. total training time CONJUNCTION fast inference. Actor - Learner Distillation USED-FOR transformer learner model. total training time EVALUATE-FOR LSTM actor model. fast inference EVALUATE-FOR LSTM actor model. Actor - Learner Distillation USED-FOR memory environments. OtherScientificTerm are compute, model size, intractable experiment run times, actor - latency ” constrained settings, and model capacity. ","This paper proposes a method to reduce the computational complexity of the actor-learner model by distilling the learning progress from the actor to the learner. The method is based on the Actor-Learner Distillation (ALD) method, which distills the learning process from the large capacity learner model to the small capacity actor model. The authors show that the proposed method is able to significantly reduce the total training time and inference time of the LSTM and transformer models. ","This paper proposes a method to reduce the computational complexity of the actor-learner model by distilling the learning progress from the actor to the learner. The method is based on the Actor-Learner Distillation (ALD) method, which distills the learning process from the large capacity learner model to the small capacity actor model. The authors show that the proposed method is able to significantly reduce the total training time and inference time of the LSTM and transformer models. "
441,SP:ccd59c3acb3d0886030451bbaea68fb83ef4dfa5,"benchmarks USED-FOR problem. Meta - Dataset HYPONYM-OF benchmarks. universal features USED-FOR few - shot classification. Meta - Dataset EVALUATE-FOR URT. model USED-FOR cross - domain generalization. Task are Few - shot classification, multi - domain few - shot image classification, and multi - domain setting. Material is diverse data sources. Method are feature representations, Universal Representation Transformer ( URT ) layer, and domain - specific representations. Generic is it. OtherScientificTerm is attention score heatmaps. ",This paper proposes a universal representation transformer (URT) for few-shot image classification. The proposed method is based on the Universal Representation Transformer (URT) framework. The authors show that URT is able to generalize well to multi-domain settings. They also show that the proposed method can be used to improve the performance of cross-domain generalization. ,This paper proposes a universal representation transformer (URT) for few-shot image classification. The proposed method is based on the Universal Representation Transformer (URT) framework. The authors show that URT is able to generalize well to multi-domain settings. They also show that the proposed method can be used to improve the performance of cross-domain generalization. 
450,SP:beaa3dfef4bdf3d8fea64d4cf86911f45edd2873,"non - stationary stream of unlabeled data USED-FOR salient representations. representations USED-FOR classification tasks. Self - Taught Associative Memory ( STAM ) HYPONYM-OF online clustering module. architecture USED-FOR UPL problem. online clustering module PART-OF architecture. online clustering CONJUNCTION novelty detection. novelty detection CONJUNCTION online clustering. novelty detection CONJUNCTION forgetting outliers. forgetting outliers CONJUNCTION novelty detection. novelty detection USED-FOR Layered hierarchies of STAM modules. online clustering USED-FOR Layered hierarchies of STAM modules. UPL context EVALUATE-FOR latter. Task is Unsupervised Progressive Learning ( UPL ) problem. Material is limited labeled data. Method are prototypical representations, and STAM architecture. ","This paper proposes a self-taught associative memory (STAM) module for unsupervised Progressive Learning (UPL) problems. The proposed module is based on the Self-Taught Associative Memory (SATAM) framework, which is an extension of the STAM framework. The main contribution of this paper is the proposed STAM module, which consists of two modules: online clustering and novelty detection. The novelty detection module is used to detect outliers, and the novelty detection modules are used for forgetting outliers. Experiments show that the proposed module outperforms the state-of-the-art in the UPL setting.","This paper proposes a self-taught associative memory (STAM) module for unsupervised Progressive Learning (UPL) problems. The proposed module is based on the Self-Taught Associative Memory (SATAM) framework, which is an extension of the STAM framework. The main contribution of this paper is the proposed STAM module, which consists of two modules: online clustering and novelty detection. The novelty detection module is used to detect outliers, and the novelty detection modules are used for forgetting outliers. Experiments show that the proposed module outperforms the state-of-the-art in the UPL setting."
459,SP:f7a8e5a580524d54f4a0cd08bd3cb0a0a074528b,"communication topology CONJUNCTION data partitioning. data partitioning CONJUNCTION communication topology. network size CONJUNCTION communication topology. communication topology CONJUNCTION network size. models COMPARE models. models COMPARE models. data partitioning HYPONYM-OF parameters. communication topology HYPONYM-OF parameters. network size HYPONYM-OF parameters. network topology CONJUNCTION learning rate. learning rate CONJUNCTION network topology. generalization gap FEATURE-OF decentralized deep learning. Method are deep learning models, on - device learning over networks, decentralized training, centralized training, communication efficient training schemes, and training schemes. OtherScientificTerm are large compute clusters, and consensus distance. ",This paper studies the generalization gap between decentralized and centralized deep learning models. The authors show that decentralized models tend to perform worse than centralized models in terms of generalization performance. They show that this is due to two factors: (1) communication topology and (2) network size. They also show that centralized models perform better than decentralized models when the number of compute clusters is large. ,This paper studies the generalization gap between decentralized and centralized deep learning models. The authors show that decentralized models tend to perform worse than centralized models in terms of generalization performance. They show that this is due to two factors: (1) communication topology and (2) network size. They also show that centralized models perform better than decentralized models when the number of compute clusters is large. 
468,SP:08ab81a53ae0b51b214442f2f9d6edca0df9118c,activity recognition CONJUNCTION natural language processing. natural language processing CONJUNCTION activity recognition. sequence alignment approaches CONJUNCTION representation learning. representation learning CONJUNCTION sequence alignment approaches. Sequence metric learning USED-FOR applications. sequential multi - variate data USED-FOR applications. natural language processing HYPONYM-OF applications. activity recognition HYPONYM-OF sequential multi - variate data. natural language processing HYPONYM-OF sequential multi - variate data. sequence alignment approaches USED-FOR applications. activity recognition HYPONYM-OF applications. representation learning USED-FOR applications. synchronized trajectories CONJUNCTION distance between similar sequences. distance between similar sequences CONJUNCTION synchronized trajectories. dynamical systems USED-FOR synchronized trajectories. siamese recurrent neural network USED-FOR distance between similar sequences. sub - networks CONJUNCTION dynamical systems. dynamical systems CONJUNCTION sub - networks. dynamical systems PART-OF siamese recurrent network. sub - networks PART-OF siamese recurrent network. gate PART-OF classical Gated Recurrent Unit architecture. neural network model USED-FOR coupling. gate USED-FOR neural network model. gate USED-FOR coupling. model USED-FOR synchronization of unaligned multi - variate sequences. model USED-FOR similarity metric. similarity metric CONJUNCTION synchronization of unaligned multi - variate sequences. synchronization of unaligned multi - variate sequences CONJUNCTION similarity metric. coupling USED-FOR siamese Gated Recurrent Unit architecture. activity recognition dataset EVALUATE-FOR siamese Gated Recurrent Unit architecture. Method is dynamical system theory. ,This paper proposes a novel siamese Gated Recurrent Unit architecture for sequential multi-variate data. The proposed model is inspired by dynamical system theory and is able to learn the distance between similar sequences and synchronous trajectories. The authors show that the proposed model can be used for synchronization of unaligned multi-variate sequences and synchronization of synchronized trajectories in dynamical systems. They also show that their model can learn the similarity metric between unaligned and synchronized sequences. ,This paper proposes a novel siamese Gated Recurrent Unit architecture for sequential multi-variate data. The proposed model is inspired by dynamical system theory and is able to learn the distance between similar sequences and synchronous trajectories. The authors show that the proposed model can be used for synchronization of unaligned multi-variate sequences and synchronization of synchronized trajectories in dynamical systems. They also show that their model can learn the similarity metric between unaligned and synchronized sequences. 
477,SP:e32bb6044bcb26cad3b0161db19170d726c40fae,"codistillation PART-OF distributed training setup. models COMPARE models. models COMPARE models. codistillation USED-FOR models. synchronization mechanism USED-FOR models. synchronous data - parallel methods USED-FOR models. synchronization mechanism USED-FOR models. synchronous data - parallel methods USED-FOR models. codistillation USED-FOR models. batch sizes CONJUNCTION learning rate schedules. learning rate schedules CONJUNCTION batch sizes. it USED-FOR distributed computing environment. Method is Codistillation. OtherScientificTerm are auxiliary loss, model replicas, large batch sizes, and moderate batch sizes. Metric is accuracy. ","This paper studies the problem of synchronous data-parallel training in distributed computing. The authors propose a new synchronization mechanism called codistillation to improve the performance of distributed training. The proposed method is based on the idea of auxiliary loss, which can be used to synchronize the training of different models in a distributed computing environment. The main contribution of the paper is to show that the proposed method outperforms the baselines in terms of accuracy, batch size, and learning rate. ","This paper studies the problem of synchronous data-parallel training in distributed computing. The authors propose a new synchronization mechanism called codistillation to improve the performance of distributed training. The proposed method is based on the idea of auxiliary loss, which can be used to synchronize the training of different models in a distributed computing environment. The main contribution of the paper is to show that the proposed method outperforms the baselines in terms of accuracy, batch size, and learning rate. "
486,SP:cd03bc0b12cf44e9d538d274de7dfe44acdb1e35,"capacity CONJUNCTION complexity. complexity CONJUNCTION capacity. stochastic gradient descent ( SGD ) USED-FOR deep learning. SGD USED-FOR local minimum. SGD iterates USED-FOR heavy - tailed stationary distribution. algorithm parameters CONJUNCTION b. b CONJUNCTION algorithm parameters. independent and identically distributed Gaussian data USED-FOR linear regression problem. dimension CONJUNCTION curvature. curvature CONJUNCTION dimension. algorithm parameters CONJUNCTION dimension. dimension CONJUNCTION algorithm parameters. algorithm parameters USED-FOR tails. SGD USED-FOR deep learning. synthetic data CONJUNCTION fully connected neural networks. fully connected neural networks CONJUNCTION synthetic data. OtherScientificTerm are eigenvalues of the Hessian, batch size b, stochastic gradient noise, tail - index ’, network weights, and Hessian. Metric is tail - index. Task are generalization, and quadratic optimization. ","This paper studies the tail-index of stochastic gradient descent (SGD) iterates. The authors show that SGD iterates have a heavy-tailed stationary distribution, which is defined as the eigenvalue of the Hessian of the weights of the stationary distribution. They show that this is a generalization property of SGD. They also show that the tail index is a function of the batch size b and the algorithm parameters. ","This paper studies the tail-index of stochastic gradient descent (SGD) iterates. The authors show that SGD iterates have a heavy-tailed stationary distribution, which is defined as the eigenvalue of the Hessian of the weights of the stationary distribution. They show that this is a generalization property of SGD. They also show that the tail index is a function of the batch size b and the algorithm parameters. "
495,SP:89f995142f8a2fcdc8c7b9f2e2cd1a4f75df3226,models USED-FOR community detection. spectral manipulations USED-FOR models. bandpass filtering USED-FOR GCN. high - frequencies USED-FOR community detection. images HYPONYM-OF Euclidean graph. spectral components USED-FOR supervised community detection task. graph structure USED-FOR cascade of filtering. low - frequency domain FEATURE-OF spectral components. low frequencies USED-FOR classifiers. Task is nodes classification. Method is GCNs. ,This paper studies the spectral components of GCNs for supervised community detection. The authors show that the spectral component of a GCN can be manipulated in the low-frequency domain. They also show that a cascade of filtering can be used to improve the performance of the GCN. ,This paper studies the spectral components of GCNs for supervised community detection. The authors show that the spectral component of a GCN can be manipulated in the low-frequency domain. They also show that a cascade of filtering can be used to improve the performance of the GCN. 
504,SP:7fc7e37c699a1bb738c65f0c6fa983203c6fd067,"graph structure USED-FOR Graph neural networks ( GNNs ). structure COMPARE real - world applications. real - world applications COMPARE structure. structure CONJUNCTION GNN parameters. GNN parameters CONJUNCTION structure. taskspecific supervision USED-FOR structure. taskspecific supervision USED-FOR GNN parameters. method USED-FOR supervision. method USED-FOR graph structure. self - supervision USED-FOR method. models USED-FOR task - specific graph structure. SLAPS COMPARE models. models COMPARE SLAPS. OtherScientificTerm are task - specific latent structure, graph structures, and Self - supervision. Method is GNN. ","This paper proposes a self-supervision method for graph neural networks (GNNs). The key idea is to learn a task-specific latent structure, which is then used to learn the GNN parameters. The proposed method is evaluated on a number of benchmark datasets, and shows that the proposed method outperforms baselines.","This paper proposes a self-supervision method for graph neural networks (GNNs). The key idea is to learn a task-specific latent structure, which is then used to learn the GNN parameters. The proposed method is evaluated on a number of benchmark datasets, and shows that the proposed method outperforms baselines."
513,SP:8f8e1fa4cd025fc056a60c0b6ba9915e8617447f,Continual Learning USED-FOR catastrophic forgetting. supervised training USED-FOR they. model USED-FOR label - agnostic incremental setting. network confusion USED-FOR novelty detection method. class - imbalance USED-FOR detection method. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. SVHN CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION SVHN. MNIST CONJUNCTION SVHN. SVHN CONJUNCTION MNIST. CIFAR-100 CONJUNCTION CRIB. CRIB CONJUNCTION CIFAR-100. image classification benchmarks EVALUATE-FOR approach. CRIB HYPONYM-OF image classification benchmarks. MNIST HYPONYM-OF image classification benchmarks. CIFAR-100 HYPONYM-OF image classification benchmarks. CIFAR-10 HYPONYM-OF image classification benchmarks. SVHN HYPONYM-OF image classification benchmarks. ,"This paper proposes a novel novelty detection method for label-agnostic incremental learning. The novelty detection is based on the idea of class-imbalance detection, which is used to detect the class-overbalance in the incremental learning setting. The proposed method is evaluated on CIFAR-10, SVHN, MNIST, and CRIB. ","This paper proposes a novel novelty detection method for label-agnostic incremental learning. The novelty detection is based on the idea of class-imbalance detection, which is used to detect the class-overbalance in the incremental learning setting. The proposed method is evaluated on CIFAR-10, SVHN, MNIST, and CRIB. "
522,SP:1d242517748c52f2be8f0613316cda3a54d1d2f7,robotics CONJUNCTION autonomous cars. autonomous cars CONJUNCTION robotics. domain expertise USED-FOR specifications. natural language constraints USED-FOR safe RL. HAZARDWORLD HYPONYM-OF multi - task benchmark. agent USED-FOR multi - task benchmark. free - form text USED-FOR constraints. agent USED-FOR tasks. modular architecture FEATURE-OF agent. policy network USED-FOR policy. constraint interpreter USED-FOR spatial and temporal representations of forbidden states. constraint interpreter USED-FOR textual constraints. representations USED-FOR policy. minimal constraint violations FEATURE-OF policy. representations USED-FOR policy network. policy network PART-OF model. constraint interpreter PART-OF model. method COMPARE approaches. approaches COMPARE method. rewards CONJUNCTION constraint violations. constraint violations CONJUNCTION rewards. HAZARDWORLD EVALUATE-FOR method. constraint violations EVALUATE-FOR method. rewards EVALUATE-FOR method. Task is safe reinforcement learning ( RL ). OtherScientificTerm is mathematical form. ,"This paper proposes a novel approach for safe reinforcement learning (RL) based on natural language constraints. The authors propose to use a constraint interpreter to represent the constraints in a mathematical form, and a policy network to learn the representations of forbidden states. The proposed approach is evaluated on the HAZARD benchmark and shows that the proposed approach outperforms the baselines. ","This paper proposes a novel approach for safe reinforcement learning (RL) based on natural language constraints. The authors propose to use a constraint interpreter to represent the constraints in a mathematical form, and a policy network to learn the representations of forbidden states. The proposed approach is evaluated on the HAZARD benchmark and shows that the proposed approach outperforms the baselines. "
531,SP:bc9f37b4622868a92f9812d2ea901def79229d41,"few - shot semantic edge detection USED-FOR boundaries of novel categories. few - shot semantic edge detection HYPONYM-OF few - shot learning challenge. image generation CONJUNCTION medical imaging. medical imaging CONJUNCTION image generation. semantic segmentation CONJUNCTION localization. localization CONJUNCTION semantic segmentation. object reconstruction CONJUNCTION image generation. image generation CONJUNCTION object reconstruction. boundary information USED-FOR semantic segmentation. boundary information USED-FOR localization. boundary information USED-FOR object reconstruction. Few - shot semantic edge detection USED-FOR recovery of accurate boundaries. small - scale FEATURE-OF semantic segmentation module. semantic segmentation module USED-FOR CAFENet. predicted segmentation mask USED-FOR attention map. multi - split matching USED-FOR regularization method. meta - training USED-FOR metric - learning problem. highdimensional vectors USED-FOR metric - learning problem. FSE-1000 CONJUNCTION SBD-5. SBD-5 CONJUNCTION FSE-1000. them EVALUATE-FOR CAFENet. FSE-1000 HYPONYM-OF datasets. SBD-5 HYPONYM-OF datasets. CAFENet COMPARE baseline methods. baseline methods COMPARE CAFENet. fine - tuning or few - shot segmentation USED-FOR CAFENet. fine - tuning or few - shot segmentation USED-FOR baseline methods. Method are meta - learning strategy, and decoder module. OtherScientificTerm are lack of semantic information, and low - dimensional sub - vectors. ","This paper proposes a meta-learning approach for few-shot semantic edge detection. The proposed approach is based on the idea of multi-split matching, which is a regularization method to improve the performance of the decoder and the semantic segmentation module. The authors show that the proposed approach outperforms the baselines on several benchmark datasets. ","This paper proposes a meta-learning approach for few-shot semantic edge detection. The proposed approach is based on the idea of multi-split matching, which is a regularization method to improve the performance of the decoder and the semantic segmentation module. The authors show that the proposed approach outperforms the baselines on several benchmark datasets. "
540,SP:5e99fee48137d3d3d88017a02f7285ce35dce970,"explainability EVALUATE-FOR GNN. feature attribution USED-FOR explanation generation. causal interpretability FEATURE-OF GNNs. It USED-FOR graph feature. causal attribution FEATURE-OF graph feature. edge HYPONYM-OF graph feature. Causal Screening USED-FOR GNN model. Causal Screening USED-FOR model - agnostic tool. Causal Screening COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE Causal Screening. graph classification datasets EVALUATE-FOR Causal Screening. predictive accuracy CONJUNCTION contrastivity. contrastivity CONJUNCTION predictive accuracy. predictive accuracy HYPONYM-OF quantitative metrics. contrastivity HYPONYM-OF quantitative metrics. Method is graph neural networks ( GNNs ). OtherScientificTerm are graph features, features, cause - effect, and sanity checks. Metric is statistical interpretability. Generic is method. ","This paper proposes a method to improve the interpretability of graph neural networks (GNNs) by using causal attribution. The proposed method is based on the idea of causal attribution, which is an extension of previous work on the problem of causal interpretability in GNNs. The idea is to use a model-agnostic tool that can be applied to any GNN model. The method is evaluated on a number of graph classification datasets and shows improved interpretability compared to existing methods. ","This paper proposes a method to improve the interpretability of graph neural networks (GNNs) by using causal attribution. The proposed method is based on the idea of causal attribution, which is an extension of previous work on the problem of causal interpretability in GNNs. The idea is to use a model-agnostic tool that can be applied to any GNN model. The method is evaluated on a number of graph classification datasets and shows improved interpretability compared to existing methods. "
549,SP:5b3d76b9e67bc39a813979b5d232a59f597d257d,"parameter norms HYPONYM-OF simplicity. measure EVALUATE-FOR network ’s simplicity. measure USED-FOR model. convolutional networks USED-FOR model. CIFAR-10 USED-FOR convolutional networks. flatness of minima CONJUNCTION optimization speed. optimization speed CONJUNCTION flatness of minima. models ’ margin CONJUNCTION flatness of minima. flatness of minima CONJUNCTION models ’ margin. mutual information EVALUATE-FOR measures. mutual information EVALUATE-FOR measure. measure COMPARE measures. measures COMPARE measure. flatness of minima USED-FOR measures. optimization speed USED-FOR measures. models ’ margin USED-FOR measures. measure COMPARE flatness - based measures. flatness - based measures COMPARE measure. Method is over - parameterized neural networks. Generic is they. Task are machine learning, and pruning. OtherScientificTerm are Occam ’s razor, and network ’s parameters. Metric is training loss. ",This paper proposes a new measure to measure the simplicity of a neural network. The measure is based on Occam’s razor. The authors show that the flatness of minima and the optimization speed of the network are the two main measures for measuring the simplicity. They also show that models’ margin and optimization speed are the most important measures.,This paper proposes a new measure to measure the simplicity of a neural network. The measure is based on Occam’s razor. The authors show that the flatness of minima and the optimization speed of the network are the two main measures for measuring the simplicity. They also show that models’ margin and optimization speed are the most important measures.
558,SP:835d01ee91523fb29595cae8339dfe49de7d3a7c,Discrete representations USED-FOR temporally - extended tasks. expert specifications USED-FOR they. deep reinforcement learning USED-FOR long - horizon tasks. exploratory video data USED-FOR temporally - abstracted discrete representations. mutual information maximization objective USED-FOR temporally - abstracted discrete representations. abstract states USED-FOR low - level model - predictive controller. DORP USED-FOR low - level model - predictive controller. DORP USED-FOR abstract states. DORP USED-FOR long - horizon tasks. it USED-FOR binary properties. key - and - door HYPONYM-OF binary properties. ,This paper proposes a novel method for learning discrete representations for long-horizon tasks. The authors propose a mutual information maximization (MIMO) objective to learn discrete representations that are able to capture both key-and-door properties. They show that the proposed method is able to achieve good performance on a variety of tasks. They also show that their method can be used to learn a low-level model-predictive controller.,This paper proposes a novel method for learning discrete representations for long-horizon tasks. The authors propose a mutual information maximization (MIMO) objective to learn discrete representations that are able to capture both key-and-door properties. They show that the proposed method is able to achieve good performance on a variety of tasks. They also show that their method can be used to learn a low-level model-predictive controller.
567,SP:2e31a542a7a60b1d425d95dd26e62374ba799cb8,"performance CONJUNCTION compression rate. compression rate CONJUNCTION performance. Mixed - precision quantization USED-FOR deep neural networks. compression rate EVALUATE-FOR deep neural networks. performance EVALUATE-FOR deep neural networks. compression rate EVALUATE-FOR Mixed - precision quantization. neural architecture search USED-FOR vast search space. manuallydesigned search space USED-FOR methods. neural architecture search USED-FOR methods. bit - level sparsity quantization ( BSQ ) USED-FOR mixed - precision quantization. bit - level sparsity quantization ( BSQ ) USED-FOR inducing bit - level sparsity. BSQ USED-FOR dynamic precision reduction. BSQ USED-FOR mixed - precision quantization scheme. BSQ USED-FOR all - zero bits. mixed - precision quantization scheme USED-FOR model. hyperparameter USED-FOR gradient - based optimization process. gradient - based optimization process USED-FOR method. BSQ COMPARE methods. methods COMPARE BSQ. accuracy CONJUNCTION bit reduction. bit reduction CONJUNCTION accuracy. model architectures EVALUATE-FOR BSQ. CIFAR-10 and ImageNet datasets EVALUATE-FOR model architectures. bit reduction EVALUATE-FOR BSQ. accuracy EVALUATE-FOR BSQ. CIFAR-10 and ImageNet datasets EVALUATE-FOR BSQ. Generic are it, and approaches. OtherScientificTerm are quantization scheme, optimal quantization scheme, independent trainable variable, and weight elements. Method is differentiable bit - sparsity regularizer. Metric is compression. ",This paper proposes a new method for bit-level sparsity quantization (BSQ) for deep neural networks. BSQ is a differentiable bit-sparsity regularizer that can be applied to all-zero bits. The authors show that the proposed method is able to achieve better performance than existing methods on CIFAR-10 and ImageNet datasets. They also show that BSQ can be used for dynamic precision reduction.,This paper proposes a new method for bit-level sparsity quantization (BSQ) for deep neural networks. BSQ is a differentiable bit-sparsity regularizer that can be applied to all-zero bits. The authors show that the proposed method is able to achieve better performance than existing methods on CIFAR-10 and ImageNet datasets. They also show that BSQ can be used for dynamic precision reduction.
576,SP:9b8ae88357e03447c73c792ff5c173ddc3d365e7,memory consumption CONJUNCTION faster computation. faster computation CONJUNCTION memory consumption. bitwise operations FEATURE-OF quantized networks. bitwise operations USED-FOR faster computation. generalization capabilities EVALUATE-FOR they. quantized networks USED-FOR gradient based adversarial attacks. robustness FEATURE-OF quantized networks. robustness FEATURE-OF quantized models. gradient vanishing issues FEATURE-OF quantized models. temperature scaling approach USED-FOR decision boundary. forward - backward signal propagation PART-OF network. forward - backward signal propagation USED-FOR gradient vanishing. adversarially trained models CONJUNCTION floating - point networks. floating - point networks CONJUNCTION adversarially trained models. temperature scaled attacks COMPARE attacks. attacks COMPARE temperature scaled attacks. CIFAR-10/100 datasets CONJUNCTION multiple network architectures. multiple network architectures CONJUNCTION CIFAR-10/100 datasets. near - perfect success rate EVALUATE-FOR quantized networks. quantized networks EVALUATE-FOR temperature scaled attacks. adversarially trained models EVALUATE-FOR attacks. floating - point networks EVALUATE-FOR attacks. near - perfect success rate EVALUATE-FOR temperature scaled attacks. Method is Neural network quantization. ,This paper proposes a temperature scaling approach for gradient based adversarial attacks against quantized networks. The proposed method is based on the forward-backward signal propagation of the decision boundary of the quantized network. The authors show that the proposed method can achieve near-perfect success rate on CIFAR-10/100 and multiple network architectures. They also show that temperature scaling attacks can be applied to adversarially trained models.,This paper proposes a temperature scaling approach for gradient based adversarial attacks against quantized networks. The proposed method is based on the forward-backward signal propagation of the decision boundary of the quantized network. The authors show that the proposed method can achieve near-perfect success rate on CIFAR-10/100 and multiple network architectures. They also show that temperature scaling attacks can be applied to adversarially trained models.
585,SP:16dddbe1432e4ffbf4b2a9180bf3c67495bd9e81,prototype trajectories PART-OF interpretable recurrent neural network ( RNN ) model. ProtoryNet HYPONYM-OF interpretable recurrent neural network ( RNN ) model. prototype theory USED-FOR ProtoryNet. prototype USED-FOR ProtoryNet. RNN backbone USED-FOR temporal pattern. RNN backbone USED-FOR prototypes. temporal pattern FEATURE-OF prototypes. method COMPARE prototype - based method. prototype - based method COMPARE method. ProtoryNet COMPARE prototype - based methods. prototype - based methods COMPARE ProtoryNet. Material is text sequence. Generic is model. ,"This paper proposes a prototype-based method for interpretable recurrent neural network (RNN) models. ProtoryNet is an interpretable RNN model that learns a sequence of prototypes from text. The proposed method is based on prototype theory, which is an extension of prototype theory to RNNs. The authors show that the proposed method outperforms existing methods in terms of interpretability. ","This paper proposes a prototype-based method for interpretable recurrent neural network (RNN) models. ProtoryNet is an interpretable RNN model that learns a sequence of prototypes from text. The proposed method is based on prototype theory, which is an extension of prototype theory to RNNs. The authors show that the proposed method outperforms existing methods in terms of interpretability. "
594,SP:ac8a9afa6e87f9c36d080c2e7085c4e096af64ff,Hidden Markov models ( HMMs ) USED-FOR disease progression modeling. patient covariates USED-FOR estimation. local optima FEATURE-OF HMMs. HMRNN COMPARE discrete - observation HMM. discrete - observation HMM COMPARE HMRNN. likelihood function FEATURE-OF HMRNN. hidden Markov recurrent neural networks ( HMRNNs ) HYPONYM-OF recurrent neural networks ( RNNs ). HMRNN CONJUNCTION predictive neural networks. predictive neural networks CONJUNCTION HMRNN. patient covariate information USED-FOR predictive neural networks. Baum - Welch algorithm USED-FOR HMRNN parameter estimates. HMRNN USED-FOR parameter estimation. HMRNN CONJUNCTION neural networks. neural networks CONJUNCTION HMRNN. neural networks USED-FOR parameter estimation. Alzheimer ’s disease dataset EVALUATE-FOR HMRNN. HMRNN ’s solution COMPARE HMM. HMM COMPARE HMRNN ’s solution. HMRNN ’s solution USED-FOR clinical interpretation. HMRNN ’s solution USED-FOR disease forecasting. HMM USED-FOR clinical interpretation. OtherScientificTerm is patient health state. ,"This paper studies the problem of learning hidden Markov models (HMMs) for disease progression modeling. The authors propose a new model called HMRNN, which can be seen as a variant of the discrete-observation HMM. The proposed model is based on the Baum-Welch algorithm, and the authors show that the proposed model can be used for disease prediction. They also show that their model is able to estimate the patient covariate information better than discrete-optimal HMMs. ","This paper studies the problem of learning hidden Markov models (HMMs) for disease progression modeling. The authors propose a new model called HMRNN, which can be seen as a variant of the discrete-observation HMM. The proposed model is based on the Baum-Welch algorithm, and the authors show that the proposed model can be used for disease prediction. They also show that their model is able to estimate the patient covariate information better than discrete-optimal HMMs. "
603,SP:6355337707f1dd373813290e26e9c0a264b993f9,"phenotypes FEATURE-OF structural and functional properties of neuronal types. supervised learning approach USED-FOR gene expression data. components USED-FOR phenotypic characteristics. phenotypic feature CONJUNCTION feature combination. feature combination CONJUNCTION phenotypic feature. sparsity - based regularization algorithm USED-FOR feature combination. sparsity - based regularization algorithm USED-FOR phenotypic feature. sparsity - based regularization algorithm USED-FOR approach. dendritic and axonal phenotypes FEATURE-OF single - cell RNA - Seq dataset. Drosophila T4 / T5 neurons FEATURE-OF single - cell RNA - Seq dataset. single - cell RNA - Seq dataset EVALUATE-FOR approach. analysis EVALUATE-FOR methods. Task are neurobiology, and linear transformation of gene expressions. Material is Single - cell RNA sequencing. OtherScientificTerm are neuronal phenotypes, phenotypic factor, and genetic organization. Generic is method. Method is factorized linear discriminant analysis ( FLDA ). ",This paper proposes a novel method for learning the phenotypic factor of a single-cell RNA-Seq dataset. The proposed method is based on factorized linear discriminant analysis (FLDA) and uses a sparsity-based regularization algorithm to improve the performance of the model. The method is evaluated on Drosophila T4/T5 and dendritic and axonal phenotypes. ,This paper proposes a novel method for learning the phenotypic factor of a single-cell RNA-Seq dataset. The proposed method is based on factorized linear discriminant analysis (FLDA) and uses a sparsity-based regularization algorithm to improve the performance of the model. The method is evaluated on Drosophila T4/T5 and dendritic and axonal phenotypes. 
612,SP:28e61a4f51f9f7283397d6336ea114375ae6a004,"Saliency maps USED-FOR image classifier. interpretability method USED-FOR posterior distribution. posterior distribution FEATURE-OF saliency map. random variable FEATURE-OF saliency map. image USED-FOR classifier ’s predictive probability. approximate posterior USED-FOR classifier. OtherScientificTerm are likelihood function, prior distribution, positive correlation, and auxiliary information. Method is variational approximation. Generic is It. ","This paper studies the interpretability of saliency maps for image classifiers. The authors propose a new interpretability method for the posterior distribution of a saliency map, which is a variational approximation of the prior distribution of an image classifier. The proposed method is based on the notion of positive correlation, and the authors show that the proposed method can be used to approximate the posterior of a classifier with respect to an image. ","This paper studies the interpretability of saliency maps for image classifiers. The authors propose a new interpretability method for the posterior distribution of a saliency map, which is a variational approximation of the prior distribution of an image classifier. The proposed method is based on the notion of positive correlation, and the authors show that the proposed method can be used to approximate the posterior of a classifier with respect to an image. "
621,SP:01acd8b88768d86bcf21b8c20a930d706c5645a7,"Pretrained text encoders USED-FOR natural language processing ( NLP ) tasks. BERT HYPONYM-OF Pretrained text encoders. social bias FEATURE-OF pretrained NLP models. sentence - level fairness EVALUATE-FOR pretrained encoders. neural debiasing method USED-FOR pretrained sentence encoder. fair filter ( FairFil ) network USED-FOR debiased representations. filtered embeddings CONJUNCTION bias words. bias words CONJUNCTION filtered embeddings. contrastive learning framework USED-FOR FairFil. real - world datasets EVALUATE-FOR FairFil. FairFil USED-FOR bias degree. FairFil USED-FOR pretrained text encoders. bias degree FEATURE-OF pretrained text encoders. FairFil USED-FOR downstream tasks. Task is word - level debiasing. OtherScientificTerm are pretrained encoder outputs, and rich semantic information. Method are post hoc method, and text encoders. ","This paper proposes a neural debiasing method for sentence-level fairness. The proposed method is based on a contrastive learning framework. The authors propose a fair filter (FairFil) network to learn the debiased representations of a pretrained sentence encoder. FairFil is a post hoc method, and is evaluated on a number of real-world datasets. ","This paper proposes a neural debiasing method for sentence-level fairness. The proposed method is based on a contrastive learning framework. The authors propose a fair filter (FairFil) network to learn the debiased representations of a pretrained sentence encoder. FairFil is a post hoc method, and is evaluated on a number of real-world datasets. "
630,SP:f2f505d3f07ca3bb2f16f6f6f5d00fee98da6531,"certified models USED-FOR machine learning security. certified models USED-FOR adversarial perturbations. randomized smoothing USED-FOR models. l2 perturbations FEATURE-OF models. test accuracy CONJUNCTION average certified robust radius. average certified robust radius CONJUNCTION test accuracy. sample - wise randomized smoothing USED-FOR noise levels. sample - wise randomized smoothing USED-FOR defense. robust regions USED-FOR certification. accuracy - robustness trade - off FEATURE-OF transductive setting. transductive setting EVALUATE-FOR method. accuracy - robustness trade - off EVALUATE-FOR method. Task is certifying l2 perturbations. OtherScientificTerm are Gaussian noise, and noise level. Method is pretrain - to - finetune framework. Generic is model. Material is CIFAR-10 and MNIST datasets. ","This paper proposes a new method for certifying the robustness of a model against adversarial perturbations. The proposed method is based on randomized smoothing, which is a popular method for detecting adversarial examples. The method is evaluated on CIFAR-10 and MNIST datasets and shows that the proposed method can achieve better performance than existing methods. ","This paper proposes a new method for certifying the robustness of a model against adversarial perturbations. The proposed method is based on randomized smoothing, which is a popular method for detecting adversarial examples. The method is evaluated on CIFAR-10 and MNIST datasets and shows that the proposed method can achieve better performance than existing methods. "
639,SP:692c7b9f6d982bbc5a22e566296a97e8a530b87c,probabilistic method USED-FOR unsupervised recovery of corrupted data. method USED-FOR posteriors of clean values. degraded samples USED-FOR method. variational methods USED-FOR collapsed densities. reduced entropy condition approximate inference method USED-FOR rich posteriors. imputation CONJUNCTION de - noising. de - noising CONJUNCTION imputation. missing values CONJUNCTION noise. noise CONJUNCTION missing values. variational methods USED-FOR de - noising. variational methods USED-FOR imputation. model COMPARE variational methods. variational methods COMPARE model. real data sets USED-FOR variational methods. real data sets USED-FOR de - noising. data recovery task EVALUATE-FOR model. propagating uncertainty USED-FOR downstream tasks. classification accuracy EVALUATE-FOR imputation. model USED-FOR downstream tasks. OtherScientificTerm is solution space. ,This paper proposes a variational method for unsupervised recovery of corrupted data. The method is based on a reduced entropy condition approximate inference method. The authors show that the proposed method is able to recover the posteriors of clean values and de-noising from corrupted samples. They also show that their method can be used to improve the performance of downstream tasks.,This paper proposes a variational method for unsupervised recovery of corrupted data. The method is based on a reduced entropy condition approximate inference method. The authors show that the proposed method is able to recover the posteriors of clean values and de-noising from corrupted samples. They also show that their method can be used to improve the performance of downstream tasks.
648,SP:4b7d050f57507166992034e5e264cccab3cb874f,"Self - attention mechanism USED-FOR graph neural networks ( GNNs ). Self - attention mechanism USED-FOR graph representation learning task. multi - hop context information USED-FOR attention computation. long - range interactions PART-OF GNN. MAGNA USED-FOR attention. diffusion prior USED-FOR MAGNA. MAGNA USED-FOR large - scale structural information. MAGNA USED-FOR informative attention. Cora CONJUNCTION Citeseer. Citeseer CONJUNCTION Cora. Citeseer CONJUNCTION Pubmed. Pubmed CONJUNCTION Citeseer. node classification CONJUNCTION knowledge graph completion benchmarks. knowledge graph completion benchmarks CONJUNCTION node classification. knowledge graph completion benchmarks EVALUATE-FOR MAGNA. MAGNA COMPARE state - of - the - art. state - of - the - art COMPARE MAGNA. relative error reduction EVALUATE-FOR state - of - the - art. node classification EVALUATE-FOR MAGNA. MAGNA COMPARE MAGNA. MAGNA COMPARE MAGNA. knowledge graph completion benchmarks EVALUATE-FOR MAGNA. MAGNA COMPARE state - of - the - art. state - of - the - art COMPARE MAGNA. node classification EVALUATE-FOR MAGNA. Citeseer EVALUATE-FOR state - of - the - art. Pubmed EVALUATE-FOR state - of - the - art. Cora EVALUATE-FOR state - of - the - art. Cora EVALUATE-FOR MAGNA. relative error reduction EVALUATE-FOR MAGNA. relative error reduction EVALUATE-FOR MAGNA. large - scale Open Graph Benchmark dataset EVALUATE-FOR MAGNA. WN18RR CONJUNCTION FB15k237. FB15k237 CONJUNCTION WN18RR. Method is attention mechanism. OtherScientificTerm are nodes, network context, attention scores, and receptive field. Generic is network. Task is knowledge graph completion. Metric is performance metrics. ",This paper proposes a self-attention mechanism for graph representation learning. The proposed method is based on diffusion prior. The authors show that the proposed method outperforms state-of-the-art methods on knowledge graph completion and node classification tasks.,This paper proposes a self-attention mechanism for graph representation learning. The proposed method is based on diffusion prior. The authors show that the proposed method outperforms state-of-the-art methods on knowledge graph completion and node classification tasks.
657,SP:36310d761deb19e71c8a57de19b48f857707d48b,"test EVALUATE-FOR text model. multitask accuracy EVALUATE-FOR test. multitask accuracy EVALUATE-FOR text model. computer science CONJUNCTION law. law CONJUNCTION computer science. US history CONJUNCTION computer science. computer science CONJUNCTION US history. elementary mathematics CONJUNCTION US history. US history CONJUNCTION elementary mathematics. elementary mathematics CONJUNCTION computer science. computer science CONJUNCTION elementary mathematics. tasks PART-OF test. elementary mathematics HYPONYM-OF tasks. law HYPONYM-OF tasks. US history HYPONYM-OF tasks. computer science HYPONYM-OF tasks. world knowledge CONJUNCTION problem solving ability. problem solving ability CONJUNCTION world knowledge. world knowledge FEATURE-OF models. problem solving ability FEATURE-OF models. GPT-3 model COMPARE random chance. random chance COMPARE GPT-3 model. models COMPARE GPT-3 model. GPT-3 model COMPARE models. near random - chance accuracy EVALUATE-FOR models. tasks EVALUATE-FOR models. lopsided performance FEATURE-OF Models. morality CONJUNCTION law. law CONJUNCTION morality. nearrandom accuracy EVALUATE-FOR they. academic and professional understanding FEATURE-OF model. Metric are accuracy, and expert - level accuracy. ","This paper presents a multi-task multitask accuracy test for text models. The authors compare the performance of GPT-3 model and random chance models on a variety of tasks, including law, computer science, US history, and mathematics. They show that the GPT3 model outperforms the random chance model on most of the tasks. They also find that the model performs better than random chance on some tasks. ","This paper presents a multi-task multitask accuracy test for text models. The authors compare the performance of GPT-3 model and random chance models on a variety of tasks, including law, computer science, US history, and mathematics. They show that the GPT3 model outperforms the random chance model on most of the tasks. They also find that the model performs better than random chance on some tasks. "
666,SP:7d7d34ba6e9fb36f2658cf4be44b137cdd73d34c,"pre - training approach USED-FOR table semantic parsing. GRAPPA HYPONYM-OF pre - training approach. synchronous context - free grammar ( SCFG ) USED-FOR synthetic question - SQL pairs. GRAPPA USED-FOR structural properties. structural properties PART-OF pre - training language model. structural properties PART-OF table semantic parsing. synthetic data USED-FOR GRAPPA. masked language modeling ( MLM ) USED-FOR pre - training process. model USED-FOR real - world data. table - and - language datasets USED-FOR masked language modeling ( MLM ). OtherScientificTerm are compositional inductive bias, and pre - trained embeddings. Method is pre - training strategy. ",This paper proposes a pre-training approach for table semantic parsing based on synchronous context-free grammar (SCFG). The proposed method is based on the SCFG framework and is able to learn synthetically generated question-SQL pairs from synthetic data. The authors show that the proposed method outperforms the baselines on both synthetic and real-world datasets. ,This paper proposes a pre-training approach for table semantic parsing based on synchronous context-free grammar (SCFG). The proposed method is based on the SCFG framework and is able to learn synthetically generated question-SQL pairs from synthetic data. The authors show that the proposed method outperforms the baselines on both synthetic and real-world datasets. 
675,SP:ebbb25902804b4f9f4985311c5debe2ef0ad7c7c,"random matrix analysis USED-FOR Gaussian mixture data model. random matrix analysis USED-FOR MTL LS - SVM. small - dimensional ) statistics FEATURE-OF deterministic limit. single - task LS - SVMs COMPARE MTL approach. MTL approach COMPARE single - task LS - SVMs. sufficient statistics USED-FOR method. MTL LS - SVM method COMPARE multi - task and transfer learning techniques. multi - task and transfer learning techniques COMPARE MTL LS - SVM method. Method are multi - task and transfer learning methods, MTL LS - SVM algorithm, and cross - validation procedure. OtherScientificTerm is hyperparameters. ","This paper studies the problem of multi-task and transfer learning in the context of single-task SVM. The authors propose a new method for learning a Gaussian mixture data model, called MTL LS-SVM, based on random matrix analysis and cross-validation procedure. The main contribution of the paper is to provide a deterministic limit on the number of hyperparameters of the MTL SVM algorithm. The paper also proposes a cross validation procedure to validate the performance of the proposed method. ","This paper studies the problem of multi-task and transfer learning in the context of single-task SVM. The authors propose a new method for learning a Gaussian mixture data model, called MTL LS-SVM, based on random matrix analysis and cross-validation procedure. The main contribution of the paper is to provide a deterministic limit on the number of hyperparameters of the MTL SVM algorithm. The paper also proposes a cross validation procedure to validate the performance of the proposed method. "
684,SP:2be727b1333122fef3abfd2f7c576d2fc467893f,group equivariant conditional neural process ( EquivCNP ) HYPONYM-OF metalearning method. permutation invariance FEATURE-OF data set. data space FEATURE-OF transformation equivariance. permutation invariance FEATURE-OF metalearning method. transformation equivariance FEATURE-OF it. rotation and scaling equivariance HYPONYM-OF group equivariance. EquivCNPs USED-FOR group symmetries. decomposition theorem USED-FOR permutation - invariant and group - equivariant maps. infinite - dimensional latent space USED-FOR group symmetries. decomposition theorem USED-FOR EquivCNPs. infinite - dimensional latent space USED-FOR EquivCNPs. Lie group convolutional layers USED-FOR architecture. EquivCNP COMPARE CNPs. CNPs COMPARE EquivCNP. translation equivariance FEATURE-OF EquivCNP. 1D regression task EVALUATE-FOR EquivCNP. 1D regression task EVALUATE-FOR CNPs. EquivCNP USED-FOR zero - shot generalization. zero - shot generalization USED-FOR image - completion task. EquivCNP USED-FOR image - completion task. Lie group equivariance PART-OF EquivCNP. Lie group equivariance USED-FOR EquivCNP. Method is conditional neural processes ( CNPs ). OtherScientificTerm is symmetry of real - world data. ,"This paper studies the group equivariant conditional neural process (EquivCNP), which is a metalearning method that is based on the Lie group convolutional layers. The authors prove the decomposition theorem for permutation-invariant and group-equivariant maps in the infinite-dimensional latent space, and show that EquivCNPs are invariant to rotation and scaling equivariance. They also show that the group symmetries can be represented as Lie group maps, which is an extension of the work of [1]. Experiments are conducted to show the effectiveness of the proposed method. ","This paper studies the group equivariant conditional neural process (EquivCNP), which is a metalearning method that is based on the Lie group convolutional layers. The authors prove the decomposition theorem for permutation-invariant and group-equivariant maps in the infinite-dimensional latent space, and show that EquivCNPs are invariant to rotation and scaling equivariance. They also show that the group symmetries can be represented as Lie group maps, which is an extension of the work of [1]. Experiments are conducted to show the effectiveness of the proposed method. "
693,SP:a54b0358a0a2900f76a2da7a0a99348805c8d66a,"autoregressive models CONJUNCTION maximum likelihood estimation. maximum likelihood estimation CONJUNCTION autoregressive models. approaches USED-FOR text generation. autoregressive models USED-FOR approaches. maximum likelihood estimation USED-FOR approaches. mismatched history distributions FEATURE-OF exposure bias. expert demonstrations USED-FOR offline reinforcement learning ( RL ) problem. offline reinforcement learning ( RL ) problem USED-FOR text generation. demonstrations USED-FOR easy - to - optimize algorithm. importance weighting USED-FOR easy - to - optimize algorithm. optimization issues FEATURE-OF prior RL approaches. online data collection USED-FOR prior RL approaches. MLE CONJUNCTION policy gradient. policy gradient CONJUNCTION MLE. summarization CONJUNCTION question generation. question generation CONJUNCTION summarization. question generation CONJUNCTION machine translation. machine translation CONJUNCTION question generation. models COMPARE those. those COMPARE models. automatic and human evaluation EVALUATE-FOR models. policy gradient USED-FOR question generation. MLE USED-FOR question generation. GOLD USED-FOR those. policy gradient USED-FOR summarization. policy gradient USED-FOR machine translation. summarization EVALUATE-FOR those. summarization EVALUATE-FOR models. MLE USED-FOR models. MLE USED-FOR those. policy gradient USED-FOR those. GOLD USED-FOR models. models USED-FOR exposure bias. decoding algorithms USED-FOR models. Generic is paradigm. OtherScientificTerm are mismatched learning objective, and model - generated histories. ","This paper studies the problem of offline reinforcement learning (RL) for text generation, where the goal is to generate a sequence of expert demonstrations that can be used to improve the performance of an offline RL model. The authors propose two approaches to tackle this problem: (1) MLE and (2) the importance weighting approach. They show that MLE outperforms the state-of-the-art in terms of performance on both automatic and human evaluation. They also show that the importance-weighting approach is able to reduce the exposure bias in the offline RL setting. ","This paper studies the problem of offline reinforcement learning (RL) for text generation, where the goal is to generate a sequence of expert demonstrations that can be used to improve the performance of an offline RL model. The authors propose two approaches to tackle this problem: (1) MLE and (2) the importance weighting approach. They show that MLE outperforms the state-of-the-art in terms of performance on both automatic and human evaluation. They also show that the importance-weighting approach is able to reduce the exposure bias in the offline RL setting. "
702,SP:e77eca51db362909681965092186af2e502aaedc,"intermediate activations USED-FOR back - propagation. gradient - isolated modules PART-OF network. local supervision USED-FOR network. early layers FEATURE-OF task - relevant information. E2E loss FEATURE-OF local modules. information propagation ( InfoPro ) loss USED-FOR local modules. reconstruction loss CONJUNCTION normal cross - entropy / contrastive term. normal cross - entropy / contrastive term CONJUNCTION reconstruction loss. ImageNet CONJUNCTION Cityscapes. Cityscapes CONJUNCTION ImageNet. STL-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION STL-10. SVHN CONJUNCTION STL-10. STL-10 CONJUNCTION SVHN. InfoPro COMPARE E2E training. E2E training COMPARE InfoPro. CIFAR CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR. CIFAR CONJUNCTION STL-10. STL-10 CONJUNCTION CIFAR. datasets EVALUATE-FOR InfoPro. memory footprint EVALUATE-FOR E2E training. Cityscapes HYPONYM-OF datasets. ImageNet HYPONYM-OF datasets. CIFAR HYPONYM-OF datasets. STL-10 HYPONYM-OF datasets. SVHN HYPONYM-OF datasets. method USED-FOR training acceleration. local modules USED-FOR training acceleration. method USED-FOR local modules. Method are deep networks, and locally supervised learning. Metric is GPUs memory footprint. Generic are model, and algorithm. OtherScientificTerm are useful information, task - irrelevant information, InfoPro loss, surrogate optimization objective, and GPU memory constraint. ","This paper proposes InfoPro, a method to accelerate the training of local modules in deep neural networks. InfoPro is based on the idea of information propagation (InfoPro) loss, which is a surrogate optimization objective for back-propagation. The paper shows that InfoPro can be used to speed up training by reducing the memory footprint of the local modules. The method is evaluated on several benchmark datasets.","This paper proposes InfoPro, a method to accelerate the training of local modules in deep neural networks. InfoPro is based on the idea of information propagation (InfoPro) loss, which is a surrogate optimization objective for back-propagation. The paper shows that InfoPro can be used to speed up training by reducing the memory footprint of the local modules. The method is evaluated on several benchmark datasets."
711,SP:21e44dddd20db1768de0dab869f8b0d3d5a598b7,Graph neural networks ( GNNs ) USED-FOR framework. expressive power FEATURE-OF learning representation of nodes and graphs. expressive power FEATURE-OF GNNs. multiple aggregation functions HYPONYM-OF complex neighborhood aggregation functions. injective aggregation function HYPONYM-OF complex neighborhood aggregation functions. aggregation function USED-FOR expressive power. framework USED-FOR GNNs. framework USED-FOR expressive power. expressive power EVALUATE-FOR GNNs. diverse sampling USED-FOR diverse neighborhoods. diverse sampling USED-FOR representation of target node. GNN model USED-FOR representation of diverse neighborhoods. representation of diverse neighborhoods USED-FOR representation of target node. rooted sub - graphs HYPONYM-OF diverse neighborhoods. diversity of different neighborhoods USED-FOR expressive power. GCN CONJUNCTION GAT. GAT CONJUNCTION GCN. GNNs EVALUATE-FOR framework. GAT HYPONYM-OF GNNs. GCN HYPONYM-OF GNNs. multi - class node classification task CONJUNCTION multi - label node classification task. multi - label node classification task CONJUNCTION multi - class node classification task. benchmark datasets CONJUNCTION multi - label node classification task. multi - label node classification task CONJUNCTION benchmark datasets. benchmark datasets EVALUATE-FOR multi - class node classification task. dataset USED-FOR multi - label node classification task. method USED-FOR GNN models. framework USED-FOR GNN models. framework USED-FOR GNNs. Method is layer - wise neighborhood aggregation. ,"This paper proposes a novel neighborhood aggregation framework for graph neural networks (GNNs) to improve the expressive power of GNNs. The proposed method is based on the injective aggregation function (IGF), which is a family of complex neighborhood aggregation functions. The authors show that the proposed method can be applied to several GNN models, including GCN, GAT, and GAT-GAN. The method is evaluated on a number of benchmark datasets, including multi-class node classification and multi-label node classification. ","This paper proposes a novel neighborhood aggregation framework for graph neural networks (GNNs) to improve the expressive power of GNNs. The proposed method is based on the injective aggregation function (IGF), which is a family of complex neighborhood aggregation functions. The authors show that the proposed method can be applied to several GNN models, including GCN, GAT, and GAT-GAN. The method is evaluated on a number of benchmark datasets, including multi-class node classification and multi-label node classification. "
720,SP:e9a8956f067a55b794508ac69f93b4b0290a664c,network transmission failures CONJUNCTION hardware errors. hardware errors CONJUNCTION network transmission failures. defenses USED-FOR corruptions. video machine learning models USED-FOR bit - level network and file corruptions. robustness EVALUATE-FOR video machine learning models. common action recognition CONJUNCTION multi - object tracking tasks. multi - object tracking tasks CONJUNCTION common action recognition. corruption levels FEATURE-OF network and file corruptions. defenses USED-FOR bit - level corruptions. corruption - agnostic and corruption - aware defenses HYPONYM-OF defenses. corruption - agnostic defenses COMPARE no - defense baseline. no - defense baseline COMPARE corruption - agnostic defenses. adversarial training HYPONYM-OF corruption - agnostic defenses. Bit - corruption Augmented Training ( BAT ) HYPONYM-OF corruptionaware baseline. model invariance USED-FOR corruptions. knowledge of bit - level corruptions FEATURE-OF corruptionaware baseline. BAT COMPARE corruption - agnostic defenses. corruption - agnostic defenses COMPARE BAT. BAT COMPARE no - defense baseline. no - defense baseline COMPARE BAT. highly - corrupted videos EVALUATE-FOR no - defense baseline. highly - corrupted videos EVALUATE-FOR BAT. Material is clean / near - clean data. ,"This paper proposes a corruption-aware training method for video machine learning models. The proposed method, Bit-corruption Augmented Training (BAT), is based on adversarial training and corruption-agnostic training to improve the robustness of the model against corruptions. The authors show that the proposed method outperforms the baseline in terms of robustness to corruptions and robustness against adversarial attacks. BAT is also shown to be more robust to corruption levels than the no-defense baseline.","This paper proposes a corruption-aware training method for video machine learning models. The proposed method, Bit-corruption Augmented Training (BAT), is based on adversarial training and corruption-agnostic training to improve the robustness of the model against corruptions. The authors show that the proposed method outperforms the baseline in terms of robustness to corruptions and robustness against adversarial attacks. BAT is also shown to be more robust to corruption levels than the no-defense baseline."
729,SP:35f77a7dcce3f6e09db0db9d22207a6da1fdbe5c,"Representation learning models USED-FOR graphs. machine learning algorithms USED-FOR feature spaces. feature spaces FEATURE-OF nodes. skip - gram embedding approach USED-FOR implicit tensor factorization. implicit tensor factorization USED-FOR tensor representations of time - varying graphs. skip - gram embedding approach USED-FOR tensor representations of time - varying graphs. learned representations COMPARE state - of - the - art methods. state - of - the - art methods COMPARE learned representations. learned representations USED-FOR downstream tasks. state - of - the - art methods USED-FOR downstream tasks. approach USED-FOR downstream tasks. network reconstruction HYPONYM-OF downstream tasks. method USED-FOR contagion risk. method USED-FOR early risk awareness. disease spreading HYPONYM-OF dynamical processes. contact tracing data USED-FOR early risk awareness. Material is real - world networks. Generic are techniques, and approaches. ",This paper proposes an implicit tensor factorization method for learning tensor representations of time-varying graphs. The key idea is to use skip-gram embedding to learn tensor embeddings of the nodes in the feature space. The authors show that the proposed method outperforms state-of-the-art methods on a number of downstream tasks. ,This paper proposes an implicit tensor factorization method for learning tensor representations of time-varying graphs. The key idea is to use skip-gram embedding to learn tensor embeddings of the nodes in the feature space. The authors show that the proposed method outperforms state-of-the-art methods on a number of downstream tasks. 
738,SP:bac0a2d3478dd277cb1ceafedd7fff64e107a222,self - supervised language modeling USED-FOR logical reasoning. self - supervised language modeling USED-FOR mathematical formulas. logical reasoning abilities EVALUATE-FOR language models. evaluation ( downstream ) tasks EVALUATE-FOR logical reasoning abilities. evaluation ( downstream ) tasks EVALUATE-FOR language models. language models USED-FOR formal mathematics. skip - tree task USED-FOR language models. models COMPARE models. models COMPARE models. skip - tree task USED-FOR models. mathematical reasoning abilities FEATURE-OF models. skipsequence tasks USED-FOR models. ,"This paper proposes a skip-sequence task for self-supervised language modeling for logical reasoning. The skip sequence task is an extension of the skip-tree task, which is used to evaluate the ability of language models to solve logical reasoning problems. The authors show that the proposed skip sequence tasks can be used to improve the performance of existing language models. The paper also shows that the models are able to perform better than existing models on a number of downstream tasks.","This paper proposes a skip-sequence task for self-supervised language modeling for logical reasoning. The skip sequence task is an extension of the skip-tree task, which is used to evaluate the ability of language models to solve logical reasoning problems. The authors show that the proposed skip sequence tasks can be used to improve the performance of existing language models. The paper also shows that the models are able to perform better than existing models on a number of downstream tasks."
747,SP:808f6d3af382876f5518e8e3a14ea73cc59c0a2b,"robustness EVALUATE-FOR defense model. robustness PART-OF adversarial robustness research. defense model PART-OF adversarial robustness research. Obfuscated gradients HYPONYM-OF gradient masking. Obfuscated gradients USED-FOR defense methods. Margin Decomposition ( MD ) attack USED-FOR margin loss. attackability FEATURE-OF terms. Margin Decomposition ( MD ) attack USED-FOR imbalanced gradients. two - stage process USED-FOR terms. two - stage process USED-FOR attackability. models USED-FOR imbalanced gradients. label smoothing USED-FOR models. PGD attack EVALUATE-FOR PGD robustness. PGD robustness EVALUATE-FOR MD attacks. attack USED-FOR defenses. PGD robustness EVALUATE-FOR defenses. PGD robustness EVALUATE-FOR attack. adversarial robustness EVALUATE-FOR imbalanced gradients. OtherScientificTerm are Imbalanced Gradients, and gradient. Metric is overestimated adversarial robustness. Method is defense models. ","This paper studies the problem of adversarial robustness against imbalanced gradients. The authors propose two terms: margin loss and gradient masking. Margin loss is used to improve the robustness of a defense model against adversarial perturbations, while masking the gradients is used for improving robustness to label smoothing. The paper also proposes a two-stage MD attack to attack the margin loss of the defense model. The MD attack is based on the Margin Decomposition (MD) attack, and the authors show that the MD attack can improve the PGD robustness. ","This paper studies the problem of adversarial robustness against imbalanced gradients. The authors propose two terms: margin loss and gradient masking. Margin loss is used to improve the robustness of a defense model against adversarial perturbations, while masking the gradients is used for improving robustness to label smoothing. The paper also proposes a two-stage MD attack to attack the margin loss of the defense model. The MD attack is based on the Margin Decomposition (MD) attack, and the authors show that the MD attack can improve the PGD robustness. "
756,SP:2180e15ad0bbecc98e043b41f6525d2a8061d304,rich multi - type symbolic language USED-FOR linear algebra. proving semantic equivalence FEATURE-OF complex expressions. expressions USED-FOR system. typed trees HYPONYM-OF complex expressions. rich multi - type symbolic language USED-FOR expressions. rich multi - type symbolic language USED-FOR system. graph - to - sequence deep learning system USED-FOR axiomatic proofs of equivalence. operators PART-OF expressions. scalars PART-OF expressions. incremental graph - to - sequence networks USED-FOR complex and verifiable symbolic reasoning. robustness EVALUATE-FOR system. zero false positives EVALUATE-FOR It. average true positive coverage EVALUATE-FOR It. OtherScientificTerm is axioms of equivalence. ,"This paper proposes an incremental graph-to-sequence deep learning system for proving semantic equivalence of complex expressions. The system is built on top of a graph to sequence deep learning framework, and is able to solve the problem of proving equivalence in linear algebra. The key idea of the system is to learn a set of operators that can be used to prove equivalence, and then use these operators to train a neural network to solve a linear algebra problem. Experiments show that the proposed system is robust to zero false positives and zero false negatives.","This paper proposes an incremental graph-to-sequence deep learning system for proving semantic equivalence of complex expressions. The system is built on top of a graph to sequence deep learning framework, and is able to solve the problem of proving equivalence in linear algebra. The key idea of the system is to learn a set of operators that can be used to prove equivalence, and then use these operators to train a neural network to solve a linear algebra problem. Experiments show that the proposed system is robust to zero false positives and zero false negatives."
765,SP:19e32803278a7ad2be5343187468cd2e26335bc8,"it USED-FOR multimodal setting. Transformers USED-FOR language domain. Transformers USED-FOR multimodal setting. language model USED-FOR visual model. multimodal Transformers USED-FOR audio - visual video representation learning. modality - specific and modality - shared parts PART-OF Transformer. low - rank approximation USED-FOR parameter sharing scheme. approach USED-FOR Transformers. approach USED-FOR model. model CONJUNCTION Transformers. Transformers CONJUNCTION model. CNN embedding space FEATURE-OF instance similarity. instance similarity USED-FOR negative sampling approach. it USED-FOR audio - visual classification tasks. Method is vision module. OtherScientificTerm are cross - modal information, and memory requirement. Material is Kinetics-700. ",This paper proposes a new approach for multimodal video representation learning. The proposed approach is based on a low-rank approximation of the instance similarity between modality-specific and modality shared parts of the Transformer. The approach is evaluated on Kinetics-700 and is shown to outperform the baselines.,This paper proposes a new approach for multimodal video representation learning. The proposed approach is based on a low-rank approximation of the instance similarity between modality-specific and modality shared parts of the Transformer. The approach is evaluated on Kinetics-700 and is shown to outperform the baselines.
774,SP:a5c22c090413ef4448db8e7f5b39332b3db6c73f,"few - shot learning USED-FOR online, continual setting. large scale indoor imagery USED-FOR visual experience. large scale indoor imagery USED-FOR few - shot learning dataset. spatiotemporal contextual information USED-FOR contextual prototypical memory model. Task are human and machine - learning environments, and online few - shot learning setting. Generic are setting, and models. OtherScientificTerm are spatiotemporal context, and Object classes. Method is few - shot learning approaches. ","This paper proposes a method for online few-shot learning in the continual learning setting. The method is based on contextual prototypical memory, which is an approach to learn a memory model for spatiotemporal contextual information. The model is trained on a large-scale indoor imagery dataset, and is able to learn the object classes in the context of the dataset. The paper shows that the proposed method outperforms a number of baselines in the online setting.","This paper proposes a method for online few-shot learning in the continual learning setting. The method is based on contextual prototypical memory, which is an approach to learn a memory model for spatiotemporal contextual information. The model is trained on a large-scale indoor imagery dataset, and is able to learn the object classes in the context of the dataset. The paper shows that the proposed method outperforms a number of baselines in the online setting."
783,SP:9c87f7778b8ee5d3e65fb1204b8067f12aac94e1,"distribution shift CONJUNCTION gbv rowing. gbv rowing CONJUNCTION distribution shift. temporal graphs USED-FOR GNNs. GNN architectures CONJUNCTION scalable GNN techniques. scalable GNN techniques CONJUNCTION GNN architectures. vertices CONJUNCTION edges. edges CONJUNCTION vertices. accuracy EVALUATE-FOR GNN ’s receptive field. Method is graph neural networks ( GNNs ). OtherScientificTerm are full graph, and temporal window. ",This paper studies the problem of learning graph neural networks (GNNs) from temporal graphs. The authors propose a new method to learn GNNs from temporal graph data. They show that the receptive field of a GNN can be represented as a temporal window of a full graph. They also show that this temporal window can be used to learn a graph neural network. ,This paper studies the problem of learning graph neural networks (GNNs) from temporal graphs. The authors propose a new method to learn GNNs from temporal graph data. They show that the receptive field of a GNN can be represented as a temporal window of a full graph. They also show that this temporal window can be used to learn a graph neural network. 
792,SP:dce0bbc266a9ac746f0db5099836fa57a3055f4a,"Representation learning USED-FOR deep reinforcement learning ( RL ). visualized input USED-FOR Representation learning. visualized input USED-FOR feature space. technique USED-FOR representation feature space. cross - state self - constraint(CSSC ) HYPONYM-OF technique. constraint USED-FOR general feature recognition. implicit feedback USED-FOR constraint. learning process USED-FOR general feature recognition. generalization EVALUATE-FOR constraint. generalization EVALUATE-FOR method. OpenAI ProcGen benchmark EVALUATE-FOR method. Method is RL agent. OtherScientificTerm are representation similarity, and Procgen games. ",This paper proposes a self-constraint method for representation learning in deep reinforcement learning. The proposed method is based on the cross-state self-conflict (CSSC) framework. The authors show that the proposed method can be used to improve the generalization performance of deep RL agents in ProcGen games. The method is evaluated on the OpenAI ProcGen benchmark.,This paper proposes a self-constraint method for representation learning in deep reinforcement learning. The proposed method is based on the cross-state self-conflict (CSSC) framework. The authors show that the proposed method can be used to improve the generalization performance of deep RL agents in ProcGen games. The method is evaluated on the OpenAI ProcGen benchmark.
801,SP:3dda3d53fdc4bd8045db22cac740322e31e67bcf,"GNNs USED-FOR real - world applications. robustness FEATURE-OF GNNs. adversarial attacks FEATURE-OF GNNs. model parameters CONJUNCTION model predictions. model predictions CONJUNCTION model parameters. restricted near - black - box setup FEATURE-OF GNNs. attacks CONJUNCTION influence maximization problem. influence maximization problem CONJUNCTION attacks. influence maximization problem FEATURE-OF graph. adversarial attack FEATURE-OF GNNs. strategies COMPARE baseline adversarial attack strategies. baseline adversarial attack strategies COMPARE strategies. GNN models COMPARE baseline adversarial attack strategies. baseline adversarial attack strategies COMPARE GNN models. GNN models EVALUATE-FOR strategies. Method are Graph neural networks ( GNNs ), and near - black - box attack strategies. Material is realistic setups. OtherScientificTerm is features. ","This paper studies the adversarial robustness of graph neural networks (GNNs) in the restricted near-black-box setting. The authors show that adversarial attacks on GNNs do not improve the robustness, but rather degrade it. They also show that GNN models are more vulnerable to the influence maximization problem than baseline adversarial attack strategies.","This paper studies the adversarial robustness of graph neural networks (GNNs) in the restricted near-black-box setting. The authors show that adversarial attacks on GNNs do not improve the robustness, but rather degrade it. They also show that GNN models are more vulnerable to the influence maximization problem than baseline adversarial attack strategies."
810,SP:fca0583b19bd08f59fdb0e46f86a4b27495dd0df,"directed acyclic graphs ( DAGs ) USED-FOR learning causal structures. ( weighted ) adjacency matrix FEATURE-OF DAG causal model. low rank assumption FEATURE-OF ( weighted ) adjacency matrix. low rank assumption USED-FOR DAG causal model. methods USED-FOR causal structure learning. interpretable graphical conditions CONJUNCTION low rank assumption. low rank assumption CONJUNCTION interpretable graphical conditions. assumption USED-FOR methods. maximum rank FEATURE-OF hubs. low rank FEATURE-OF scale - free networks. rank FEATURE-OF DAG. they COMPARE algorithms. algorithms COMPARE they. OtherScientificTerm are causal structures, graphs, and low rank condition. Task is high dimensional settings. Method is low rank adaptations. ","This paper studies the problem of learning causal structures in directed acyclic graphs (DAGs). The authors propose a low-rank assumption for DAGs, which is based on the (weighted) adjacency matrix of the DAG causal model. The authors show that the low rank assumption can be used to improve the performance of existing DAG learning algorithms. They also show that low rank can be applied to scale-free networks. ","This paper studies the problem of learning causal structures in directed acyclic graphs (DAGs). The authors propose a low-rank assumption for DAGs, which is based on the (weighted) adjacency matrix of the DAG causal model. The authors show that the low rank assumption can be used to improve the performance of existing DAG learning algorithms. They also show that low rank can be applied to scale-free networks. "
819,SP:f2c8172adcb82ed1c0e047ffed65412f3f1c1ac7,"neural architecture search ( NAS ) CONJUNCTION hyper - parameter optimization ( HPO ). hyper - parameter optimization ( HPO ) CONJUNCTION neural architecture search ( NAS ). automated data augmentation ( DA ) CONJUNCTION neural architecture search ( NAS ). neural architecture search ( NAS ) CONJUNCTION automated data augmentation ( DA ). components PART-OF automated machine learning ( AutoML ) pipeline. automated data augmentation ( DA ) PART-OF automated machine learning ( AutoML ) pipeline. automated data augmentation ( DA ) HYPONYM-OF components. hyper - parameter optimization ( HPO ) HYPONYM-OF components. neural architecture search ( NAS ) HYPONYM-OF components. components USED-FOR joint optimization. it USED-FOR NAS. end - to - end solution USED-FOR ready - to - use model. hyper - parameters CONJUNCTION data augmentation policies. data augmentation policies CONJUNCTION hyper - parameters. co - optimization USED-FOR neural architectures. co - optimization USED-FOR method. data augmentation policies USED-FOR method. DiffAutoML COMPARE end - to - end AutoML algorithms. end - to - end AutoML algorithms COMPARE DiffAutoML. DiffAutoML COMPARE multi - stage AutoML algorithms. multi - stage AutoML algorithms COMPARE DiffAutoML. ImageNet EVALUATE-FOR end - to - end AutoML algorithms. computational efficiency EVALUATE-FOR multi - stage AutoML algorithms. ImageNet EVALUATE-FOR DiffAutoML. NAS CONJUNCTION HPO. HPO CONJUNCTION NAS. automated DA CONJUNCTION NAS. NAS CONJUNCTION automated DA. en - to - end manner FEATURE-OF HPO. Generic is component. OtherScientificTerm is search dimension. Task is search and retraining stages. Method are differentiable joint optimization solution, model retraining, and retraining. ","This paper proposes a new method for end-to-end multi-stage AutoML, DiffAutoML, that combines hyper-parameter optimization (HPO), neural architecture search (NAS) and automated data augmentation (DA). The authors propose a differentiable joint optimization solution, which can be used for both the search and retraining stages. The proposed method is evaluated on ImageNet and compared to a number of state-of-the-art methods.","This paper proposes a new method for end-to-end multi-stage AutoML, DiffAutoML, that combines hyper-parameter optimization (HPO), neural architecture search (NAS) and automated data augmentation (DA). The authors propose a differentiable joint optimization solution, which can be used for both the search and retraining stages. The proposed method is evaluated on ImageNet and compared to a number of state-of-the-art methods."
828,SP:1ab30867e0bd8b6b65fad602cd80bada70b3f1ec,Prior Networks HYPONYM-OF models. interpretable measures of uncertainty EVALUATE-FOR models. tasks EVALUATE-FOR ensemble approaches. calibration CONJUNCTION uncertainty estimates. uncertainty estimates CONJUNCTION calibration. They USED-FOR ensemble of models. accuracy CONJUNCTION calibration. calibration CONJUNCTION accuracy. uncertainty estimates PART-OF model. Ensemble Distribution Distillation ( EnD ) USED-FOR ensemble of models. Prior Networks USED-FOR classification tasks. Prior Networks CONJUNCTION EnD. EnD CONJUNCTION Prior Networks. EnD USED-FOR regression tasks. Prior Networks USED-FOR regression tasks. synthetic data CONJUNCTION UCI datasets. UCI datasets CONJUNCTION synthetic data. UCI datasets CONJUNCTION monocular depth estimation tasks. monocular depth estimation tasks CONJUNCTION UCI datasets. monocular depth estimation tasks EVALUATE-FOR Regression Prior Networks. UCI datasets EVALUATE-FOR Regression Prior Networks. synthetic data EVALUATE-FOR Regression Prior Networks. They COMPARE ensemble approaches. ensemble approaches COMPARE They. OtherScientificTerm is Normal - Wishart distribution. ,"This paper proposes a new ensemble of models, called Ensemble Distribution Distillation (EnD), to improve the performance of prior networks for regression tasks. EnD is based on the notion of ensemble distribution distillation (EDD), which is an extension of Ensemble Prior Networks (ENN). The authors show that EnD outperforms EnN on a variety of regression tasks, including UCI, monocular depth estimation, and synthetic data. The authors also show that the ensemble of EnD models can be used for classification tasks.","This paper proposes a new ensemble of models, called Ensemble Distribution Distillation (EnD), to improve the performance of prior networks for regression tasks. EnD is based on the notion of ensemble distribution distillation (EDD), which is an extension of Ensemble Prior Networks (ENN). The authors show that EnD outperforms EnN on a variety of regression tasks, including UCI, monocular depth estimation, and synthetic data. The authors also show that the ensemble of EnD models can be used for classification tasks."
837,SP:fe2aa4706defcac74e529d0cc3e1622d77451eca,catastrophic forgetting FEATURE-OF Neural networks. problem USED-FOR large - scale supervised classification. catastrophic forgetting USED-FOR few - shot classification problems. few - shot tasks USED-FOR Few - shot metalearning algorithms. Bayesian online meta - learning framework USED-FOR sequential few - shot tasks problems. Bayesian online meta - learning framework USED-FOR catastrophic forgetting. catastrophic forgetting CONJUNCTION sequential few - shot tasks problems. sequential few - shot tasks problems CONJUNCTION catastrophic forgetting. Laplace approximation CONJUNCTION variational inference. variational inference CONJUNCTION Laplace approximation. MAML PART-OF Bayesian online learning algorithm. Laplace approximation USED-FOR Bayesian online learning algorithm. variational inference USED-FOR Bayesian online learning algorithm. MAML PART-OF framework. framework USED-FOR few - shot classification. sequentially arriving datasets USED-FOR few - shot classification. framework USED-FOR catastrophic forgetting. framework USED-FOR online meta - learning. online meta - learning USED-FOR few - shot classification settings. Material is sequential datasets. Generic is algorithm. Method is meta - learned model. Task is sequentially arriving few - shot tasks. ,This paper proposes a Bayesian online meta-learning framework for few-shot classification problems. The proposed method is based on the Laplace approximation and variational inference. The authors show that the proposed method outperforms the baselines in terms of catastrophic forgetting and meta-learnability. ,This paper proposes a Bayesian online meta-learning framework for few-shot classification problems. The proposed method is based on the Laplace approximation and variational inference. The authors show that the proposed method outperforms the baselines in terms of catastrophic forgetting and meta-learnability. 
846,SP:89d2765946e70455105a608d998c3b900969cb8d,"expressive power EVALUATE-FOR higher - order GNNs. computational cost CONJUNCTION expressive power. expressive power CONJUNCTION computational cost. model USED-FOR subgraphs. RNP - GNNs COMPARE higher - order k - GNN. higher - order k - GNN COMPARE RNP - GNNs. higher - order k - GNN CONJUNCTION Local Relational Pooling ( LRP ) networks. Local Relational Pooling ( LRP ) networks CONJUNCTION higher - order k - GNN. RNP - GNNs COMPARE Local Relational Pooling ( LRP ) networks. Local Relational Pooling ( LRP ) networks COMPARE RNP - GNNs. computational complexity EVALUATE-FOR higher - order k - GNN. computational complexity EVALUATE-FOR Local Relational Pooling ( LRP ) networks. computational complexity EVALUATE-FOR RNP - GNNs. Task is learning with graphs. Method are recursive pooling technique of local neighborhoods, and low - order GNNs. OtherScientificTerm is local neighborhoods. ","This paper proposes a recursive pooling technique for learning subgraphs. The proposed method is motivated by the observation that low-order GNNs tend to be more expressive than higher-order ones. The authors show that this is due to the fact that the number of nodes in a subgraph is much smaller than that of a high-order one. To address this issue, the authors propose to use a local pooling method to learn subgraph subsets of the original subgraph. They show that the proposed method outperforms the baselines in terms of expressive power, computational cost, and computational complexity.","This paper proposes a recursive pooling technique for learning subgraphs. The proposed method is motivated by the observation that low-order GNNs tend to be more expressive than higher-order ones. The authors show that this is due to the fact that the number of nodes in a subgraph is much smaller than that of a high-order one. To address this issue, the authors propose to use a local pooling method to learn subgraph subsets of the original subgraph. They show that the proposed method outperforms the baselines in terms of expressive power, computational cost, and computational complexity."
855,SP:c43f5deb340555d78599a3496318514a826b1aae,"competitive environments CONJUNCTION games. games CONJUNCTION competitive environments. GANs HYPONYM-OF games. irregular behaviors FEATURE-OF systems. Multiplicative Weights Update ( MWU ) HYPONYM-OF learning algorithms. canonical game decomposition USED-FOR zero - sum and coordination components. volume - expansion argument USED-FOR characterizations. canonical game decomposition USED-FOR volume - expansion argument. components USED-FOR volume - changing behaviors. matrix domination CONJUNCTION linear program. linear program CONJUNCTION matrix domination. general games CONJUNCTION graphical games. graphical games CONJUNCTION general games. MWU CONJUNCTION OMWU. OMWU CONJUNCTION MWU. MWU USED-FOR potential games. OMWU USED-FOR potential games. local equivalence of volume change USED-FOR multi - player games. Method is Machine Learning. Material is two - person zero - sum games. OtherScientificTerm are Lyapunov chaos, cumulative payoff space, persistent chaos, and zero - sum games. Task are normal - form game settings, and bimatrix games. ","This paper studies the dynamics of two-player zero-sum games in the context of Multiplicative Weights Update (MWU) and OMWU. In particular, the authors consider the case where the game is played in a cumulative payoff space with Lyapunov chaos. The authors show that the game dynamics can be decomposed into two parts: (1) the dynamics in the payoff space, and (2) the behavior of the players in the game space. They show that in the case of two players, the dynamics can change with the number of players. They also show that this behavior can be modeled as a local equivalence of the volume change of the game. They then propose a novel algorithm for learning the dynamics.","This paper studies the dynamics of two-player zero-sum games in the context of Multiplicative Weights Update (MWU) and OMWU. In particular, the authors consider the case where the game is played in a cumulative payoff space with Lyapunov chaos. The authors show that the game dynamics can be decomposed into two parts: (1) the dynamics in the payoff space, and (2) the behavior of the players in the game space. They show that in the case of two players, the dynamics can change with the number of players. They also show that this behavior can be modeled as a local equivalence of the volume change of the game. They then propose a novel algorithm for learning the dynamics."
864,SP:0cf756ba6b172f9b29e84945c093dfd89ae62803,"adaptive algorithms USED-FOR deep learning. AMSGrad CONJUNCTION Radam. Radam CONJUNCTION AMSGrad. Radam HYPONYM-OF adaptive algorithms. AMSGrad HYPONYM-OF adaptive algorithms. convergence rate EVALUATE-FOR adaptive algorithms. marginal regret bound minimization HYPONYM-OF proximal function of adaptive algorithms. adaptive algorithms COMPARE adaptive algorithms. adaptive algorithms COMPARE adaptive algorithms. marginal optimality FEATURE-OF adaptive algorithms. deep learning EVALUATE-FOR adaptive algorithms. Generic are modifications, and algorithm. ","This paper studies the marginal regret bound minimization of adaptive algorithms for deep learning. The authors show that adaptive algorithms converge to a proximal function of the minimizer of the adaptive algorithm, which is the proximal minimizer that minimizes the regret of the original algorithm. They also show that the convergence rate for adaptive algorithms converges to the optimal minimizer for the adaptive algorithms. ","This paper studies the marginal regret bound minimization of adaptive algorithms for deep learning. The authors show that adaptive algorithms converge to a proximal function of the minimizer of the adaptive algorithm, which is the proximal minimizer that minimizes the regret of the original algorithm. They also show that the convergence rate for adaptive algorithms converges to the optimal minimizer for the adaptive algorithms. "
873,SP:b6b594fc555bd12b33f156970f0665e2bf793484,"risk management USED-FOR real - world decision - making problems. mean - variance criterion HYPONYM-OF risk management approaches. quadratic utility function USED-FOR risk management. reward - constrained variance minimization CONJUNCTION regularization. regularization CONJUNCTION reward - constrained variance minimization. EQUM COMPARE mean - variance RL methods. mean - variance RL methods COMPARE EQUM. double sampling USED-FOR mean - variance RL methods. RL and financial data EVALUATE-FOR EQUM. Method are expected quadratic utility maximization ( EQUM ), and mean - variance control. Task is agent utility maximization. ","This paper studies the problem of risk minimization in the context of risk-aware RL. The authors propose a new algorithm for the problem, called expected quadratic utility maximization (EQUM), which is based on the idea of mean-variance control (MVC). The authors show that the proposed algorithm outperforms the state-of-the-art methods in terms of variance minimization, reward-constrained variance minimisation, and regularization. ","This paper studies the problem of risk minimization in the context of risk-aware RL. The authors propose a new algorithm for the problem, called expected quadratic utility maximization (EQUM), which is based on the idea of mean-variance control (MVC). The authors show that the proposed algorithm outperforms the state-of-the-art methods in terms of variance minimization, reward-constrained variance minimisation, and regularization. "
882,SP:bf9d66f713b6502d274143c6273b2d071a0c045e,"neural networks USED-FOR auxiliary tasks. auxiliary tasks PART-OF coherent loss. network USED-FOR coherent objective function. network USED-FOR nonlinear interactions. network USED-FOR auxiliary task. it COMPARE methods. methods COMPARE it. tasks EVALUATE-FOR AuxiLearn. image segmentation HYPONYM-OF tasks. Task are multi - task learning setting, and designing useful auxiliary tasks. Generic is framework. OtherScientificTerm are implicit differentiation, useful auxiliaries, and low data regime. ","This paper proposes AuxiLearn, a framework for learning auxiliary tasks for multi-task learning. The proposed method is based on the idea that auxiliary tasks can be learned by learning a coherent objective function, which can be used as an auxiliary task. The authors show that the proposed method can learn auxiliary tasks in a low-data regime. They also show that their method outperforms existing methods on image segmentation and image classification tasks.","This paper proposes AuxiLearn, a framework for learning auxiliary tasks for multi-task learning. The proposed method is based on the idea that auxiliary tasks can be learned by learning a coherent objective function, which can be used as an auxiliary task. The authors show that the proposed method can learn auxiliary tasks in a low-data regime. They also show that their method outperforms existing methods on image segmentation and image classification tasks."
891,SP:3070fd64f8eb4d7ece6521cb975fd1fe64d6329f,out - of - training - distribution sentences PART-OF Neural Machine Translation. Bayesian Deep Learning equivalent of Transformer models USED-FOR out - of - training - distribution sentences. measure USED-FOR long sequences of discrete random variables. approaches USED-FOR long sentences. measure USED-FOR Transformer model. dropout approximate inference USED-FOR Transformer model. WMT13 CONJUNCTION Europarl. Europarl CONJUNCTION WMT13. dropout uncertainty USED-FOR measure. model COMPARE German. German COMPARE model. measure USED-FOR Dutch source sentences. measure USED-FOR German - English translation. WMT13 USED-FOR German - English translation. Europarl USED-FOR German - English translation. ,This paper proposes a new measure for out-of-training-distribution (OOD) sentences. The proposed measure is based on the dropout uncertainty of the Transformer model. The authors show that the proposed measure can be used to improve the performance of Transformer models for long sequences of discrete random variables. They also show that it can be applied to the case where the model is trained in a Bayesian fashion. ,This paper proposes a new measure for out-of-training-distribution (OOD) sentences. The proposed measure is based on the dropout uncertainty of the Transformer model. The authors show that the proposed measure can be used to improve the performance of Transformer models for long sequences of discrete random variables. They also show that it can be applied to the case where the model is trained in a Bayesian fashion. 
900,SP:d5a1d9596b8329312533b3a0047c815f8e71a201,"SNIP CONJUNCTION GraSP. GraSP CONJUNCTION SNIP. GraSP CONJUNCTION SynFlow. SynFlow CONJUNCTION GraSP. SynFlow CONJUNCTION magnitude pruning. magnitude pruning CONJUNCTION SynFlow. GraSP CONJUNCTION magnitude pruning. magnitude pruning CONJUNCTION GraSP. methods COMPARE random pruning. random pruning COMPARE methods. accuracy EVALUATE-FOR magnitude pruning. they COMPARE magnitude pruning. magnitude pruning COMPARE they. methods USED-FOR per - weight pruning decisions. Task is pruning neural networks. Method are neural networks, and pruning heuristics. ","This paper studies the problem of per-weight pruning of neural networks. The authors propose three pruning heuristics: magnitude pruning, SNIP, and GraSP. They show that the magnitude-based pruning method outperforms the random pruning methods in terms of accuracy. They also show that SNIP is more accurate than magnitude-wise pruning. ","This paper studies the problem of per-weight pruning of neural networks. The authors propose three pruning heuristics: magnitude pruning, SNIP, and GraSP. They show that the magnitude-based pruning method outperforms the random pruning methods in terms of accuracy. They also show that SNIP is more accurate than magnitude-wise pruning. "
909,SP:1f6b266021da24bbf02b5a47f2b5eb23b4912166,semi - honest server CONJUNCTION Byzantine malicious clients. Byzantine malicious clients CONJUNCTION semi - honest server. FED - LEARNING HYPONYM-OF federated learning protocol. robust mean estimator USED-FOR FED - LEARNING. FED - LEARNING HYPONYM-OF FL protocol. FED - LEARNING USED-FOR dimension - free estimation error. robust mean estimator USED-FOR FL protocol. FL protocol USED-FOR dimension - free estimation error. FilterL2 HYPONYM-OF robust mean estimator. secure aggregation USED-FOR FED - LEARNING. FilterL2 CONJUNCTION secure aggregation. secure aggregation CONJUNCTION FilterL2. optimal or close - to - optimal performance EVALUATE-FOR FED - LEARNING. OtherScientificTerm is shards. Method is robust FL protocols. ,"This paper proposes a new federated learning (FL) protocol called FED-LEARNING, which is based on the robust mean estimator. The authors show that the proposed method is able to achieve optimal or close-to-optimal performance in terms of dimension-free estimation error and secure aggregation. They also show that their method is robust to Byzantine malicious clients. ","This paper proposes a new federated learning (FL) protocol called FED-LEARNING, which is based on the robust mean estimator. The authors show that the proposed method is able to achieve optimal or close-to-optimal performance in terms of dimension-free estimation error and secure aggregation. They also show that their method is robust to Byzantine malicious clients. "
918,SP:9f89ff90b203d86a569e3d5148546942f5bf2093,"aircraft CONJUNCTION robot morphology. robot morphology CONJUNCTION aircraft. design input USED-FOR unknown objective function. robot morphology HYPONYM-OF domains. aircraft HYPONYM-OF domains. method USED-FOR function. offline MBO methods USED-FOR highdimensional problems. high - capacity deep neural network function approximators USED-FOR highdimensional problems. high - capacity deep neural network function approximators USED-FOR offline MBO methods. evaluation tasks EVALUATE-FOR field. Design - Bench HYPONYM-OF offline MBO tasks. Design - Bench HYPONYM-OF benchmark suite. benchmark suite EVALUATE-FOR offline MBO tasks. unified evaluation protocol USED-FOR benchmark suite. unified evaluation protocol USED-FOR offline MBO tasks. biology CONJUNCTION material science. material science CONJUNCTION biology. material science CONJUNCTION robotics. robotics CONJUNCTION material science. benchmark EVALUATE-FOR offline MBO methods. Generic are problems, and benchmarks. OtherScientificTerm are feedback, and objective function. Task is data - driven offline MBO setting. ","This paper proposes a new offline MBO setting where the objective function is unknown, and the goal is to learn a function that can be used to solve the problem. The authors propose a new benchmark, Design-Bench, to evaluate the performance of different offline learning algorithms. The benchmark is based on the design-bench benchmark, which is used to evaluate offline learning methods on a variety of tasks. The paper also proposes a unified evaluation protocol for offline learning. ","This paper proposes a new offline MBO setting where the objective function is unknown, and the goal is to learn a function that can be used to solve the problem. The authors propose a new benchmark, Design-Bench, to evaluate the performance of different offline learning algorithms. The benchmark is based on the design-bench benchmark, which is used to evaluate offline learning methods on a variety of tasks. The paper also proposes a unified evaluation protocol for offline learning. "
927,SP:073958946c266bf760d1ad66bd39bc28a24c8521,"self - supervised generative models USED-FOR ELBO. self - supervised generative models USED-FOR multimodal models. generalized ELBO formulation USED-FOR multimodal data. methods PART-OF objective. method COMPARE state - of - the - art models. state - of - the - art models COMPARE method. selfsupervised, generative learning tasks EVALUATE-FOR state - of - the - art models. selfsupervised, generative learning tasks EVALUATE-FOR method. OtherScientificTerm are real - world phenomena, posterior approximation functions, semantic coherence, and joint data distribution. Generic is them. Task is machine learning research. ",This paper proposes a generalized ELBO formulation for multimodal data. The main idea is to use the joint data distribution as the objective of the ELBO objective. The authors show that the proposed method can be applied to self-supervised generative learning tasks. The proposed method is evaluated on a variety of tasks and shows that it outperforms state-of-the-art methods.,This paper proposes a generalized ELBO formulation for multimodal data. The main idea is to use the joint data distribution as the objective of the ELBO objective. The authors show that the proposed method can be applied to self-supervised generative learning tasks. The proposed method is evaluated on a variety of tasks and shows that it outperforms state-of-the-art methods.
936,SP:98004554447b82b3d2eb9724ec551250eec7a595,"Bayesian Optimization ( BO ) USED-FOR optimizing expensive black - box functions. priors USED-FOR PrBO. probabilistic model USED-FOR pseudo - posterior. BO USED-FOR pseudo - posterior. priors CONJUNCTION BO. BO CONJUNCTION priors. priors CONJUNCTION probabilistic model. probabilistic model CONJUNCTION priors. BO CONJUNCTION probabilistic model. probabilistic model CONJUNCTION BO. PrBO USED-FOR pseudo - posterior. probabilistic model USED-FOR PrBO. BO USED-FOR PrBO. priors USED-FOR PrBO. PrBO COMPARE state - of - the - art methods. state - of - the - art methods COMPARE PrBO. PrBO COMPARE random search. random search COMPARE PrBO. state - of - the - art methods COMPARE random search. random search COMPARE state - of - the - art methods. real - world hardware design application EVALUATE-FOR PrBO. it USED-FOR misleading priors. OtherScientificTerm are function evaluations, machine learning hyperparameters, and user priors. Method are Prior - guided Bayesian Optimization ( PrBO ), and optimization process. ","This paper proposes a prior-guided Bayesian Optimization (PrBO) method for black-box optimization. PrBO uses a probabilistic model to learn a pseudo-prior for each function evaluation, which is then used to guide the optimization process. The authors show that PrBO outperforms the state-of-the-art methods on a variety of benchmarks. ","This paper proposes a prior-guided Bayesian Optimization (PrBO) method for black-box optimization. PrBO uses a probabilistic model to learn a pseudo-prior for each function evaluation, which is then used to guide the optimization process. The authors show that PrBO outperforms the state-of-the-art methods on a variety of benchmarks. "
945,SP:e0e9cd5f39a60b5db1c4363ffdc2c593300ef43a,"Deep generative models USED-FOR real - world data. execution time FEATURE-OF computational cost. binary weights USED-FOR neural networks. binary neural networks USED-FOR generative models. binary neural networks USED-FOR generative models. computational cost EVALUATE-FOR models. techniques USED-FOR deep generative models. ResNet VAE and Flow++ models HYPONYM-OF deep generative models. Generic is they. Metric is complexity. Method are binary weight normalization, binarized generative models, binary models, and regular models. ","This paper studies the computational cost of binary neural networks for deep generative models. The authors propose a binary weight normalization method to reduce the computational complexity of binary models. They show that the proposed method can be applied to both VAE and Flow++ models, and show that it can reduce the computation cost of both models. ","This paper studies the computational cost of binary neural networks for deep generative models. The authors propose a binary weight normalization method to reduce the computational complexity of binary models. They show that the proposed method can be applied to both VAE and Flow++ models, and show that it can reduce the computation cost of both models. "
954,SP:7e9a83552c0ff001d3090a5a7162013b5dc6f47f,"frameworks USED-FOR DL. adversarial training USED-FOR DL. approach USED-FOR DL. adversarial training USED-FOR approach. robustness EVALUATE-FOR DL. adversarial training USED-FOR robustness. norm - bounded perturbations FEATURE-OF DL. robustness EVALUATE-FOR approach. unbounded shifts in the data distribution FEATURE-OF DL. natural, out - of - distribution shifts FEATURE-OF robustness. perturbation - based adversarial robustness CONJUNCTION model - based robust deep learning. model - based robust deep learning CONJUNCTION perturbation - based adversarial robustness. paradigm USED-FOR models of natural variation. model - based robust training algorithms USED-FOR DL. robustness EVALUATE-FOR DL. robustness EVALUATE-FOR model - based robust training algorithms. adversarial training CONJUNCTION domain adaptation techniques. domain adaptation techniques CONJUNCTION adversarial training. ERM CONJUNCTION adversarial training. adversarial training CONJUNCTION ERM. classifiers COMPARE classifiers. classifiers COMPARE classifiers. algorithms COMPARE classifiers. classifiers COMPARE algorithms. domain adaptation techniques USED-FOR classifiers. ERM USED-FOR classifiers. algorithms USED-FOR classifiers. ERM USED-FOR classifiers. domain adaptation techniques USED-FOR classifiers. adversarial training USED-FOR classifiers. adversarial training USED-FOR classifiers. algorithms COMPARE baseline methods. baseline methods COMPARE algorithms. top-1 accuracy EVALUATE-FOR baseline methods. top-1 accuracy EVALUATE-FOR algorithms. Method is deep learning ( DL ). OtherScientificTerm are natural variation, data distribution, and natural conditions. Material are images, ImageNet, ImageNet - c, and natural, out - ofdistribution data. Generic are models, and methods. ","This paper proposes a new framework for deep learning (DL) based on adversarial training and model-based robust deep learning. The proposed framework is based on perturbation-based adversarial robustness, which can be applied to both natural and out-of-distribution data. The authors show that the proposed framework outperforms baseline methods on ImageNet, ImageNet-c, and CIFAR-10. ","This paper proposes a new framework for deep learning (DL) based on adversarial training and model-based robust deep learning. The proposed framework is based on perturbation-based adversarial robustness, which can be applied to both natural and out-of-distribution data. The authors show that the proposed framework outperforms baseline methods on ImageNet, ImageNet-c, and CIFAR-10. "
963,SP:011dab90d225550e77235cbec1615e583ae3297e,polynomial complexity FEATURE-OF exact convex optimization formulations. ReLU activations USED-FOR Convolutional Neural Networks ( CNNs ). convex analytic framework USED-FOR convex optimization problems. convex optimization problems USED-FOR twoand three - layer CNN architectures. semi - infinite duality USED-FOR convex analytic framework. ` 2 norm regularized convex program USED-FOR two - layer CNNs. ` 1 regularized convex program USED-FOR sparsity. spectral domain FEATURE-OF sparsity. ` 1 regularized convex program USED-FOR multi - layer circular CNN training problems. ReLU layer USED-FOR multi - layer circular CNN training problems. ReLU layers USED-FOR three - layer CNNs. approach USED-FOR pooling methods. convex regularizers USED-FOR implicit architectural bias. OtherScientificTerm is data dimension. ,This paper studies the problem of convex optimization of ReLU activations for two-layer and three-layer convolutional neural networks (CNNs). The authors propose a convex analytic framework to solve the problem. The authors show that the duality of the convex formulation of the problem is non-trivial in the spectral domain. They also show that a regularized convex program can be used to address the issue of sparsity in the data dimension. ,This paper studies the problem of convex optimization of ReLU activations for two-layer and three-layer convolutional neural networks (CNNs). The authors propose a convex analytic framework to solve the problem. The authors show that the duality of the convex formulation of the problem is non-trivial in the spectral domain. They also show that a regularized convex program can be used to address the issue of sparsity in the data dimension. 
972,SP:98760a3b1a5058a485a5a1ed1b778c1d4fb2ff22,"rich sensory modalities USED-FOR Robotic manipulation tasks. Human - robot interaction USED-FOR teaching robots. probabilistic generative model USED-FOR optimisation problem. high - capacity neural network USED-FOR model. latent variables USED-FOR model. latent variables CONJUNCTION high - level notions. high - level notions CONJUNCTION latent variables. table - top robot manipulation tasks EVALUATE-FOR approach. PR2 robot USED-FOR table - top robot manipulation tasks. visual information CONJUNCTION arm joint positions. arm joint positions CONJUNCTION visual information. arm joint positions CONJUNCTION arm joint efforts. arm joint efforts CONJUNCTION arm joint positions. robot USED-FOR visual information. robot USED-FOR arm joint positions. arm joint efforts FEATURE-OF robot. OtherScientificTerm are soft sponge, restricted vocabulary, and sponge. Material is rich data streams. Generic are alignment, and tasks. ","This paper proposes a probabilistic generative model for the task of robot manipulation. The model is based on the idea of soft sponges, which is an extension of the SoftSponges framework. The key idea is to learn a soft sponge that is able to capture both the visual information and the high-level notions of the task. The soft sponge is then used to train a neural network to learn the latent variables of the soft sponge. The proposed method is evaluated on a number of table-top robot manipulation tasks. ","This paper proposes a probabilistic generative model for the task of robot manipulation. The model is based on the idea of soft sponges, which is an extension of the SoftSponges framework. The key idea is to learn a soft sponge that is able to capture both the visual information and the high-level notions of the task. The soft sponge is then used to train a neural network to learn the latent variables of the soft sponge. The proposed method is evaluated on a number of table-top robot manipulation tasks. "
981,SP:e171d8c4eadf73852734c0fb8a74a69d80969e4b,low - resource scenarios FEATURE-OF overfitting. tasks EVALUATE-FOR large - scale pretrained language models. general - purpose feature extractors USED-FOR models. Variational Information Bottleneck ( VIB ) USED-FOR irrelevant features. Variational Information Bottleneck ( VIB ) USED-FOR fine - tuning. method USED-FOR overfitting. fine - tuning USED-FOR low - resource target tasks. Variational Information Bottleneck ( VIB ) USED-FOR low - resource target tasks. VIB model USED-FOR sentence representations. natural language inference datasets USED-FOR sentence representations. generalization EVALUATE-FOR VIB model. low - resource datasets EVALUATE-FOR method. method USED-FOR transfer learning. low - resource scenarios FEATURE-OF transfer learning. low - resource scenarios EVALUATE-FOR method. generalization EVALUATE-FOR it. Generic is they. OtherScientificTerm is features. Material is out - of - domain datasets. ,"This paper proposes a new method for fine-tuning pretrained language models for low-resource tasks. The proposed method is based on the Variational Information Bottleneck (VIB) method, which is a general-purpose feature extractor that can be used to remove irrelevant features from the model. The authors show that the proposed method can be applied to transfer learning and transfer learning on out-of-domain datasets. ","This paper proposes a new method for fine-tuning pretrained language models for low-resource tasks. The proposed method is based on the Variational Information Bottleneck (VIB) method, which is a general-purpose feature extractor that can be used to remove irrelevant features from the model. The authors show that the proposed method can be applied to transfer learning and transfer learning on out-of-domain datasets. "
990,SP:a1ab99bee74a0a1310537beced0d89dc1e5ad7be,Natural images HYPONYM-OF projections of 3D objects. 2D image plane FEATURE-OF projections of 3D objects. they USED-FOR 3D object structures. 2D generative models USED-FOR natural image manifold. GANs HYPONYM-OF 2D generative models. knowledge USED-FOR 3D shapes of objects. 2D GAN USED-FOR 3D geometric cues. RGB images USED-FOR 2D GAN. pre - trained GAN USED-FOR 3D shape. rich 3D knowledge PART-OF pre - trained GAN. unsupervised manner USED-FOR 3D shape. iterative strategy USED-FOR diverse viewpoint and lighting variations. diverse viewpoint and lighting variations FEATURE-OF GAN image manifold. iterative strategy USED-FOR framework. 2D keypoint CONJUNCTION 3D annotations. 3D annotations CONJUNCTION 2D keypoint. cars CONJUNCTION buildings. buildings CONJUNCTION cars. it USED-FOR 3D shapes. human faces CONJUNCTION cars. cars CONJUNCTION human faces. precision FEATURE-OF 3D shapes. precision EVALUATE-FOR it. relighting CONJUNCTION object rotation. object rotation CONJUNCTION relighting. 3D shapes USED-FOR image editing. object rotation HYPONYM-OF image editing. relighting HYPONYM-OF image editing. 3D shape reconstruction CONJUNCTION face rotation. face rotation CONJUNCTION 3D shape reconstruction. approach COMPARE methods. methods COMPARE approach. methods USED-FOR face rotation. approach USED-FOR face rotation. methods USED-FOR 3D shape reconstruction. approach USED-FOR 3D shape reconstruction. OtherScientificTerm is object shapes. ,"This paper proposes a new method for 3D shape reconstruction based on pre-trained GANs. The method is based on the idea of pre-training a 2D GAN on a set of RGB images, which is then used to learn a 3D representation of the image manifold. The authors show that their method is able to achieve state-of-the-art performance on a wide range of image editing tasks, including object rotation, face rotation, and relighting. ","This paper proposes a new method for 3D shape reconstruction based on pre-trained GANs. The method is based on the idea of pre-training a 2D GAN on a set of RGB images, which is then used to learn a 3D representation of the image manifold. The authors show that their method is able to achieve state-of-the-art performance on a wide range of image editing tasks, including object rotation, face rotation, and relighting. "
999,SP:eac0679dfee4dae78c1e515f8b325c9523b795dc,"recognition methods USED-FOR imbalanced classification. tail accuracies CONJUNCTION head accuracies. head accuracies CONJUNCTION tail accuracies. class re - balancing / re - weighting USED-FOR recognition methods. model bias CONJUNCTION variance analysis. variance analysis CONJUNCTION model bias. RoutIng Diverse Experts ( RIDE ) HYPONYM-OF long - tailed classifier. It USED-FOR model variance. It USED-FOR model bias. computational cost EVALUATE-FOR dynamic expert routing module. distribution - aware diversity loss USED-FOR model bias. CIFAR100 - LT CONJUNCTION ImageNet - LT. ImageNet - LT CONJUNCTION CIFAR100 - LT. ImageNet - LT CONJUNCTION iNaturalist 2018 benchmarks. iNaturalist 2018 benchmarks CONJUNCTION ImageNet - LT. RIDE COMPARE state - of - the - art. state - of - the - art COMPARE RIDE. ImageNet - LT EVALUATE-FOR RIDE. iNaturalist 2018 benchmarks EVALUATE-FOR RIDE. CIFAR100 - LT EVALUATE-FOR RIDE. CIFAR100 - LT EVALUATE-FOR state - of - the - art. backbone networks CONJUNCTION long - tailed algorithms. long - tailed algorithms CONJUNCTION backbone networks. universal framework USED-FOR backbone networks. universal framework USED-FOR long - tailed algorithms. long - tailed algorithms CONJUNCTION training mechanisms. training mechanisms CONJUNCTION long - tailed algorithms. It HYPONYM-OF universal framework. It USED-FOR backbone networks. Material are Natural data, and tail data. OtherScientificTerm are dynamic view of the training data, hard negatives, and tail. Method is long - tail classifiers. Metric is head - tail model bias gap. ","This paper proposes a new method for long-tailed classifiers. The proposed method is based on a distribution-aware diversity loss to mitigate the model bias and variance analysis. The authors show that the proposed method outperforms state-of-the-art long-tail classifiers on CIFAR-10, ImageNet-LT, and iNaturalist 2018 benchmarks. ","This paper proposes a new method for long-tailed classifiers. The proposed method is based on a distribution-aware diversity loss to mitigate the model bias and variance analysis. The authors show that the proposed method outperforms state-of-the-art long-tail classifiers on CIFAR-10, ImageNet-LT, and iNaturalist 2018 benchmarks. "
1008,SP:f4d0e821de6830722a3458fd40d8d6793a107827,"pruning criteria USED-FOR redundant filters. Channel pruning USED-FOR compressing convolutional neural networks ( CNNs ). pruning criteria USED-FOR filters ’ Importance Score. layer - wise pruning CONJUNCTION global pruning. global pruning CONJUNCTION layer - wise pruning. layer - wise pruning USED-FOR pruning criteria. global pruning USED-FOR pruning criteria. Gaussian - alike distribution FEATURE-OF convolutional filters. Material is convolutional neural networks ( CNNs ). Generic is criteria. OtherScientificTerm are pruned structures, and Convolutional Weight Distribution Assumption. Method is ` 1 and ` 2 pruning. ","This paper studies the problem of channel pruning for compressing convolutional neural networks (CNNs). Channel pruning is an important problem in computer vision. The authors propose two criteria for pruning: global pruning and layer-wise pruning. Global pruning can be seen as a regularization of the Importance Score of the filters, while layer pruning has been shown to be a regularisation of the weight distribution of the weights of the layers.  The authors show that the global and layer pruned CNNs have a Gaussian-like distribution, which is a result of the fact that the weights in the layers are Gaussians. They also show that layer prunings can be regarded as a special case of global prune.","This paper studies the problem of channel pruning for compressing convolutional neural networks (CNNs). Channel pruning is an important problem in computer vision. The authors propose two criteria for pruning: global pruning and layer-wise pruning. Global pruning can be seen as a regularization of the Importance Score of the filters, while layer pruning has been shown to be a regularisation of the weight distribution of the weights of the layers.  The authors show that the global and layer pruned CNNs have a Gaussian-like distribution, which is a result of the fact that the weights in the layers are Gaussians. They also show that layer prunings can be regarded as a special case of global prune."
1017,SP:eadb827653b2e1b608bb923d5549089cb2482d90,"code completion CONJUNCTION code summarization. code summarization CONJUNCTION code completion. code search CONJUNCTION code completion. code completion CONJUNCTION code search. Pre - trained models USED-FOR programming language. Pre - trained models USED-FOR code - related tasks. code search HYPONYM-OF code - related tasks. code summarization HYPONYM-OF code - related tasks. code completion HYPONYM-OF code - related tasks. code snippet USED-FOR pre - trained models. pre - trained model USED-FOR programming language. GraphCodeBERT HYPONYM-OF pre - trained model. inherent structure of code USED-FOR pre - trained model. data flow USED-FOR pre - training stage. data flow USED-FOR semantic - level structure of code. abstract syntax tree ( AST ) HYPONYM-OF syntactic - level structure of code. Transformer USED-FOR GraphCodeBERT. graph - guided masked attention function USED-FOR code structure. graph - guided masked attention function USED-FOR model. code translation CONJUNCTION code refinement. code refinement CONJUNCTION code translation. clone detection CONJUNCTION code translation. code translation CONJUNCTION clone detection. code search CONJUNCTION clone detection. clone detection CONJUNCTION code search. tasks EVALUATE-FOR model. code refinement HYPONYM-OF tasks. code search HYPONYM-OF tasks. clone detection HYPONYM-OF tasks. code translation HYPONYM-OF tasks. pre - training tasks USED-FOR GraphCodeBERT. code structure CONJUNCTION pre - training tasks. pre - training tasks CONJUNCTION code structure. code structure USED-FOR GraphCodeBERT. structure - level attentions COMPARE token - level attentions. token - level attentions COMPARE structure - level attentions. model COMPARE token - level attentions. token - level attentions COMPARE model. structure - level attentions USED-FOR model. OtherScientificTerm are code semantics, semantic - level structure, hierarchy of AST, and code structure edges. Task are code understanding process, masked language modeling, and structure - aware pre - training tasks. Generic is downstream tasks. ","This paper proposes a pre-trained model for code understanding. The proposed model is based on a Transformer-based model, which is able to learn the structure of a code snippet. The model is trained using a graph-guided masked attention function, and the model is evaluated on three tasks: code search, code completion, and code translation. The paper shows that the proposed model outperforms the state-of-the-art model on all three tasks.","This paper proposes a pre-trained model for code understanding. The proposed model is based on a Transformer-based model, which is able to learn the structure of a code snippet. The model is trained using a graph-guided masked attention function, and the model is evaluated on three tasks: code search, code completion, and code translation. The paper shows that the proposed model outperforms the state-of-the-art model on all three tasks."
1026,SP:2c7a128e19cd2d39b0ca1b946b01604c3f7cead5,"drug discovery CONJUNCTION material design. material design CONJUNCTION drug discovery. drug discovery HYPONYM-OF scientific fields. material design HYPONYM-OF scientific fields. distribution FEATURE-OF labeled result data. it USED-FOR regression model. skewed data USED-FOR it. approach USED-FOR regression models. accuracy EVALUATE-FOR regression models. accuracy EVALUATE-FOR approach. skewed dataset USED-FOR regression models. forcing algorithm USED-FOR regression. domain knowledge USED-FOR true distribution. neural networks USED-FOR regression model. pLogP CONJUNCTION Diamond. Diamond CONJUNCTION pLogP. pLogP HYPONYM-OF real - world datasets. real - world datasets EVALUATE-FOR approach. Diamond HYPONYM-OF real - world datasets. datasets EVALUATE-FOR approach. approach COMPARE regression models. regression models COMPARE approach. root mean squared error EVALUATE-FOR regression. root mean squared error EVALUATE-FOR regression models. datasets EVALUATE-FOR regression models. adjustment of the distribution USED-FOR regression models. regression EVALUATE-FOR approach. root mean squared error EVALUATE-FOR approach. Generic is method. OtherScientificTerm are regression outputs, and estimated ‘ true ’ distribution. Material is unlabeled data. Method is adversarial network. ","This paper proposes an adversarial approach to improve the performance of regression models on unlabeled data. The proposed method is based on the idea that the true distribution of the data is not known, and that the model should be able to estimate the true one based on domain knowledge. The authors show that the proposed method outperforms existing methods on a number of real-world datasets. ","This paper proposes an adversarial approach to improve the performance of regression models on unlabeled data. The proposed method is based on the idea that the true distribution of the data is not known, and that the model should be able to estimate the true one based on domain knowledge. The authors show that the proposed method outperforms existing methods on a number of real-world datasets. "
1035,SP:fee1e40275fa743aa6ad011ae742b3ea3fd137df,"Compositional generalization HYPONYM-OF algebraic capacity. known components USED-FOR algebraic capacity. human intelligence USED-FOR out - of - distribution generalization. It PART-OF human intelligence. It USED-FOR out - of - distribution generalization. model USED-FOR representations. representations USED-FOR inference. regularized hidden representations USED-FOR auxiliary reconstruction network. approach COMPARE baselines. baselines COMPARE approach. accuracy EVALUATE-FOR approach. compositional representations USED-FOR it. compositional generalization CONJUNCTION artificial intelligence research. artificial intelligence research CONJUNCTION compositional generalization. Method are neural networks, and extraction network. OtherScientificTerm are extraction ability, divergence of distributions, and compositionality. Task is transferability of compositionality. ","This paper studies the problem of compositional generalization, which is an important problem in machine learning. The authors propose a new method for learning compositional representations that can be used for out-of-distribution generalization. The proposed method is based on the idea of regularized hidden representations, which are learned by a regularized reconstruction network. The method is evaluated on synthetic and real-world datasets.","This paper studies the problem of compositional generalization, which is an important problem in machine learning. The authors propose a new method for learning compositional representations that can be used for out-of-distribution generalization. The proposed method is based on the idea of regularized hidden representations, which are learned by a regularized reconstruction network. The method is evaluated on synthetic and real-world datasets."
1044,SP:ffab573a977c819e86601de74690c29a39c264cd,"Poisoning attacks USED-FOR Reinforcement Learning ( RL ) systems. RL algorithm USED-FOR Poisoning attacks. poisoning methods USED-FOR supervised learning. supervised learning USED-FOR RL. poisoning methods USED-FOR RL. generic poisoning framework USED-FOR online RL. heterogeneous poisoning models USED-FOR RL. heterogeneous poisoning models USED-FOR generic poisoning framework. poisoning method USED-FOR policy - based RL agents. strategic poisoning algorithm USED-FOR on - policy deep RL agents. stability radius FEATURE-OF RL. stability radius HYPONYM-OF metric. stability radius USED-FOR VA2C - P. metric USED-FOR VA2C - P. Task are learning, and poisoning RL. OtherScientificTerm are Markov Decision Process ( MDP ), MDP, and limited attacking budget. Method are RL algorithms, and poisoning algorithm. Material is deep RL agents. ","This paper studies the problem of poisoning in online reinforcement learning. The authors propose a new poisoning algorithm, VA2C-P, which is based on the idea that the stability radius of an agent is a function of the number of times that the agent has been poisoned in the MDP. They show that the proposed algorithm can be used to improve the stability of a policy-based RL agent in the presence of poisoning attacks. They also provide a theoretical analysis of their algorithm. ","This paper studies the problem of poisoning in online reinforcement learning. The authors propose a new poisoning algorithm, VA2C-P, which is based on the idea that the stability radius of an agent is a function of the number of times that the agent has been poisoned in the MDP. They show that the proposed algorithm can be used to improve the stability of a policy-based RL agent in the presence of poisoning attacks. They also provide a theoretical analysis of their algorithm. "
1053,SP:06ebd437ff2d1b5068f7a651716d3c1a60c2a001,Checkpointing USED-FOR deep learning models. restricted memory budgets FEATURE-OF deep learning models. static computation graphs USED-FOR checkpointing techniques. greedy online algorithm USED-FOR checkpointing. Dynamic Tensor Rematerialization ( DTR ) HYPONYM-OF greedy online algorithm. Dynamic Tensor Rematerialization ( DTR ) USED-FOR online algorithm. DTR USED-FOR anN -layer linear feedforward network. O(N ) tensor operations USED-FOR DTR. Ω ( √ N ) memory budget FEATURE-OF anN -layer linear feedforward network. DTR COMPARE optimal static checkpointing. optimal static checkpointing COMPARE DTR. tensor allocations CONJUNCTION operator calls. operator calls CONJUNCTION tensor allocations. DTR prototype PART-OF PyTorch. lightweight metadata PART-OF tensors. OtherScientificTerm is eviction policy. Method is dynamic models. ,"This paper proposes a dynamic online algorithm for checkpointing. The proposed algorithm is based on the idea of dynamic tensor re-materialization (DTR), which is a greedy online algorithm. The authors show that the proposed algorithm outperforms the optimal static checkpointing algorithm on PyTorch.","This paper proposes a dynamic online algorithm for checkpointing. The proposed algorithm is based on the idea of dynamic tensor re-materialization (DTR), which is a greedy online algorithm. The authors show that the proposed algorithm outperforms the optimal static checkpointing algorithm on PyTorch."
1062,SP:20efc610911443724b56f57f857060d0e0302243,"manually annotated evaluation sets USED-FOR task. method USED-FOR hallucination detection. synthetic data USED-FOR pretrained language models. pretrained language models USED-FOR method. machine translation CONJUNCTION abstract text summarization. abstract text summarization CONJUNCTION machine translation. machine translation EVALUATE-FOR approach. abstract text summarization EVALUATE-FOR approach. average F1 EVALUATE-FOR benchmark datasets. average F1 EVALUATE-FOR approach. token - level hallucination labels USED-FOR fine - grained loss. fine - grained loss PART-OF low - resource machine translation. Method is Neural sequence models. Generic are they, model, and baseline methods. OtherScientificTerm is automatically inserted hallucinations. Material is annotated data. ",This paper proposes a novel method for hallucination detection based on neural sequence models. The authors propose to use token-level hallucination labels and fine-grained loss for low-resource machine translation and abstract text summarization tasks. The proposed method is evaluated on three benchmark datasets and shows promising results.,This paper proposes a novel method for hallucination detection based on neural sequence models. The authors propose to use token-level hallucination labels and fine-grained loss for low-resource machine translation and abstract text summarization tasks. The proposed method is evaluated on three benchmark datasets and shows promising results.
1071,SP:3d0d026888cf87073df5bd74edd986f15351ff5a,"Conditional Generative Adversarial Networks ( cGAN ) USED-FOR images. idea USED-FOR architecture. NAS USED-FOR architecture. NAS USED-FOR idea. reduction of training data USED-FOR class generator. latter USED-FOR class - specific information. regular and class - modulated convolutions PART-OF search space. weight - sharing pipeline CONJUNCTION mixed - architecture optimization. mixed - architecture optimization CONJUNCTION weight - sharing pipeline. weight - sharing pipeline USED-FOR search algorithm. Markov decision process PART-OF search algorithm. Markov decision process USED-FOR sampling policy. CIFAR10 CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION CIFAR10. CIFAR100 EVALUATE-FOR approach. CIFAR10 EVALUATE-FOR approach. FID scores FEATURE-OF image generation quality. OtherScientificTerm are class - level distributions, and generating architecture. Metric is search cost. Method are moving average, and cGAN models. ","This paper proposes a new method to improve the performance of conditional generative adversarial networks (cGANs) by reducing the number of training samples. The main idea is to use a class-modulated convolutional neural network (NAS) to learn the class-specific information from the training data. The authors show that the proposed method outperforms existing methods on CIFAR-10, Cifar-100, and ImageNet. ","This paper proposes a new method to improve the performance of conditional generative adversarial networks (cGANs) by reducing the number of training samples. The main idea is to use a class-modulated convolutional neural network (NAS) to learn the class-specific information from the training data. The authors show that the proposed method outperforms existing methods on CIFAR-10, Cifar-100, and ImageNet. "
1080,SP:8cdf6e8af07daaec6680c2bed6c1787a53580584,estimation of causal effects USED-FOR Decision - making. observational data USED-FOR estimation of causal effects. regularization framework USED-FOR unconfoundedness. orthogonality constraint USED-FOR unconfoundedness. asymptotically normal estimator USED-FOR average causal effect. estimators COMPARE asymptotic variance. asymptotic variance COMPARE estimators. regularization framework USED-FOR deep orthogonal networks. deep orthogonal networks USED-FOR unconfounded treatments ( DONUT ). DONUT COMPARE state - of - the - art. state - of - the - art COMPARE DONUT. benchmark datasets EVALUATE-FOR DONUT. benchmark datasets USED-FOR causal inference. benchmark datasets EVALUATE-FOR state - of - the - art. OtherScientificTerm is treatment assignment. ,"This paper proposes a novel method for unconfounded causal inference based on orthogonality constraint. The authors propose a novel regularization framework for the estimation of the average causal effect. The proposed method is based on the idea of orthogonal neural networks, which can be seen as an extension of deep orthogonic networks.  The authors show that the proposed method outperforms the state-of-the-art in terms of unconfoundness. ","This paper proposes a novel method for unconfounded causal inference based on orthogonality constraint. The authors propose a novel regularization framework for the estimation of the average causal effect. The proposed method is based on the idea of orthogonal neural networks, which can be seen as an extension of deep orthogonic networks.  The authors show that the proposed method outperforms the state-of-the-art in terms of unconfoundness. "
1089,SP:77ec2512837df5c0a94000602dc2ef5c03fe41dd,"style transfer CONJUNCTION multitask learning. multitask learning CONJUNCTION style transfer. multitask learning HYPONYM-OF deep learning techniques. style transfer HYPONYM-OF deep learning techniques. affine transformations of features USED-FOR deep learning techniques. affine parameters USED-FOR features. parameters PART-OF BatchNorm. randomly chosen parameters PART-OF network. affine parameters USED-FOR deep learning. shifting and rescaling random features USED-FOR neural networks. Method is affine transform. OtherScientificTerm are random initializations, and random features. Metric is accuracy. ","This paper studies the problem of style transfer and style transfer in deep learning. The authors propose a new method called BatchNorm, which is based on the idea of affine transformations of features. The key idea is to use the affine transformation of features to improve the performance of a deep learning model. This is done by shifting and rescaling random features in the training process. The proposed method is evaluated on a variety of tasks, including style transfer, multi-task learning, and multitask learning. ","This paper studies the problem of style transfer and style transfer in deep learning. The authors propose a new method called BatchNorm, which is based on the idea of affine transformations of features. The key idea is to use the affine transformation of features to improve the performance of a deep learning model. This is done by shifting and rescaling random features in the training process. The proposed method is evaluated on a variety of tasks, including style transfer, multi-task learning, and multitask learning. "
1098,SP:6683ceea773ff6d7fb613e503c583bb2979c7e89,"model USED-FOR test - time adaptation. model USED-FOR confidence. method USED-FOR normalization statistics. method USED-FOR channel - wise affine transformations. Tent USED-FOR source - free domain adaptation. corrupted ImageNet CONJUNCTION CIFAR-10/100. CIFAR-10/100 CONJUNCTION corrupted ImageNet. Tent USED-FOR semantic segmentation. GTA CONJUNCTION Cityscapes. Cityscapes CONJUNCTION GTA. Tent USED-FOR image classification. Tent USED-FOR digit recognition. source - free domain adaptation USED-FOR digit recognition. generalization error EVALUATE-FOR image classification. Tent USED-FOR Tent. CIFAR-10/100 USED-FOR image classification. corrupted ImageNet USED-FOR image classification. VisDA - C benchmark EVALUATE-FOR Tent. generalization error EVALUATE-FOR Tent. Metric is entropy. Material are SVHN, and MNIST / MNIST - M / USPS. Method is test - time optimization. ","This paper proposes Tent, a method for source-free domain adaptation. Tent is based on the idea of channel-wise affine transformations, which can be used to improve the test-time performance of the model. The method is evaluated on the VisDA-C benchmark and the CIFAR-10/100 dataset. Tent outperforms the state-of-the-art in terms of generalization error and generalization performance on the SVHN, MNIST/MNIST-M, and USPS datasets. ","This paper proposes Tent, a method for source-free domain adaptation. Tent is based on the idea of channel-wise affine transformations, which can be used to improve the test-time performance of the model. The method is evaluated on the VisDA-C benchmark and the CIFAR-10/100 dataset. Tent outperforms the state-of-the-art in terms of generalization error and generalization performance on the SVHN, MNIST/MNIST-M, and USPS datasets. "
1107,SP:ed544ee661580592063aa17aee8924cc99919130,Uncertainty quantification USED-FOR machine learning systems. recurrent timesteps FEATURE-OF stochastic discrete state transitions. stochastic discrete state transitions USED-FOR recurrent neural networks ( RNNs ). uncertainty quantification COMPARE method. method COMPARE uncertainty quantification. method USED-FOR deterministic and probabilistic automata. well - calibrated models USED-FOR real - world classification tasks. method USED-FOR well - calibrated models. explorationexploitation trade - off FEATURE-OF reinforcement learning. method USED-FOR out - of - distribution detection. method USED-FOR explorationexploitation trade - off. Generic is model. OtherScientificTerm is recurrent state transition distribution. ,This paper proposes a method for out-of-distribution (OOD) detection for recurrent neural networks (RNNs). The method is based on the exploration-exploitation trade-off between uncertainty quantification and uncertainty quantization. The authors show that the proposed method outperforms existing methods for out of distribution detection in a number of real-world classification tasks. The proposed method is also applicable to deterministic and probabilistic automata.,This paper proposes a method for out-of-distribution (OOD) detection for recurrent neural networks (RNNs). The method is based on the exploration-exploitation trade-off between uncertainty quantification and uncertainty quantization. The authors show that the proposed method outperforms existing methods for out of distribution detection in a number of real-world classification tasks. The proposed method is also applicable to deterministic and probabilistic automata.
1116,SP:a38c523196f68a90b5db45671f9dbd87981a024c,"Protecting data privacy PART-OF deep learning ( DL ). stochastic differential equation principled residual perturbation USED-FOR privacy - preserving DL. Gaussian noise USED-FOR residual mapping of ResNets. residual perturbation USED-FOR differential privacy ( DP ). generalization gap FEATURE-OF DL. residual perturbation USED-FOR generalization gap. residual perturbation COMPARE DP stochastic gradient descent ( DPSGD ). DP stochastic gradient descent ( DPSGD ) COMPARE residual perturbation. DP stochastic gradient descent ( DPSGD ) USED-FOR membership privacy protection. residual perturbation USED-FOR DL models ’ utility. residual perturbation USED-FOR membership privacy protection. ResNet8 USED-FOR IDC dataset classification. residual perturbation USED-FOR perfect membership privacy. residual perturbation COMPARE DPSGD. DPSGD COMPARE residual perturbation. accuracy EVALUATE-FOR DPSGD. accuracy EVALUATE-FOR residual perturbation. Task is data privacy. OtherScientificTerm are utility degradation, and ResNets. ",This paper studies the problem of privacy-preserving deep learning. The authors propose a principled residual perturbation method to improve the generalization performance of deep learning models. The proposed method is based on the stochastic differential equation (SDE) and the notion of differential privacy (DP). The authors show that the proposed method outperforms DPSGD and DPSGD-based methods in terms of generalization and membership privacy. They also show that their method is able to achieve better membership privacy than DPSGD.,This paper studies the problem of privacy-preserving deep learning. The authors propose a principled residual perturbation method to improve the generalization performance of deep learning models. The proposed method is based on the stochastic differential equation (SDE) and the notion of differential privacy (DP). The authors show that the proposed method outperforms DPSGD and DPSGD-based methods in terms of generalization and membership privacy. They also show that their method is able to achieve better membership privacy than DPSGD.
1125,SP:9cbe32c1317889d6a3ec1b0798112d9b82cc7f67,"limited computational resources USED-FOR inference. natural language processing EVALUATE-FOR transformers. computational efficiency USED-FOR inference. model USED-FOR inference scenario. inefficiency CONJUNCTION redundancy. redundancy CONJUNCTION inefficiency. PoWER - BERT USED-FOR inefficiency. PoWER - BERT USED-FOR redundancy. it USED-FOR inference scenarios. extension USED-FOR large - scale transformer. Length - Adaptive Transformer HYPONYM-OF large - scale transformer. LengthDrop HYPONYM-OF structural variant of dropout. LengthDrop USED-FOR transformer. multi - objective evolutionary search USED-FOR length configuration. accuracy EVALUATE-FOR length configuration. PoWER - BERT USED-FOR token - level classification. sequence - level classification USED-FOR token - level classification. PoWER - BERT USED-FOR sequence - level classification. span - based question - answering HYPONYM-OF token - level classification. SQuAD 1.1 CONJUNCTION MNLI - m. MNLI - m CONJUNCTION SQuAD 1.1. MNLI - m CONJUNCTION SST-2. SST-2 CONJUNCTION MNLI - m. accuracyefficiency trade - off EVALUATE-FOR setups. accuracyefficiency trade - off EVALUATE-FOR approach. SST-2 HYPONYM-OF setups. SQuAD 1.1 HYPONYM-OF setups. MNLI - m HYPONYM-OF setups. Generic are they, and approaches. Metric is computational complexity. OtherScientificTerm are computational budget, Drop - and - Restore, and word - vectors. ","This paper proposes a novel extension of the Drop-and-Restore approach for large-scale transformers. The proposed extension is called Length-Adaptive Transformer (LAT), which is a structural variant of dropout. The authors show that the proposed LAT is able to reduce the computational complexity of the model while maintaining the accuracy. They also show that LAT can be applied to sequence-level classification and span-based question answering. ","This paper proposes a novel extension of the Drop-and-Restore approach for large-scale transformers. The proposed extension is called Length-Adaptive Transformer (LAT), which is a structural variant of dropout. The authors show that the proposed LAT is able to reduce the computational complexity of the model while maintaining the accuracy. They also show that LAT can be applied to sequence-level classification and span-based question answering. "
1134,SP:e5b4098ea22a5da2b9659219dc24f885c493a011,"1 - WL test USED-FOR distinguishing graph structures. expressiveness EVALUATE-FOR graph neural networks ( GNNs ). neighborhood aggregation GNNs COMPARE 1 - WL test. 1 - WL test COMPARE neighborhood aggregation GNNs. neighborhood aggregation GNNs USED-FOR distinguishing graph structures. aggregators PART-OF GNNs. aggregators USED-FOR expressiveness. aggregation coefficient matrix USED-FOR aggregators. aggregation coefficient matrix USED-FOR injective aggregators. aggregators CONJUNCTION injective aggregators. injective aggregators CONJUNCTION aggregators. aggregation coefficient matrix USED-FOR aggregation. It USED-FOR rank of hidden features. nonlinear units USED-FOR aggregation - based GNNs. ExpandingConv CONJUNCTION CombConv. CombConv CONJUNCTION ExpandingConv. CombConv HYPONYM-OF GNN layers. ExpandingConv HYPONYM-OF GNN layers. models USED-FOR large and densely connected graphs. OtherScientificTerm are graph structures, weak distinguishing strength, and low - rank transformations. Method is WL test. Generic is it. ",This paper studies the 1-WL test for graph neural networks (GNNs) and shows that aggregation-based GNNs are more expressive than neighborhood aggregation GNN (NAGN) and injective aggregators (IGN). The authors show that the aggregation coefficient matrix of neighborhood aggregation can be used to estimate the rank of hidden features in GNN layers. The authors also show that neighborhood aggregation is able to outperform neighborhood aggregation by a large margin. ,This paper studies the 1-WL test for graph neural networks (GNNs) and shows that aggregation-based GNNs are more expressive than neighborhood aggregation GNN (NAGN) and injective aggregators (IGN). The authors show that the aggregation coefficient matrix of neighborhood aggregation can be used to estimate the rank of hidden features in GNN layers. The authors also show that neighborhood aggregation is able to outperform neighborhood aggregation by a large margin. 
1143,SP:4dd6fb8e5a356af270d3b296ce3d50ae5753513c,"robustness EVALUATE-FOR generative models. generalization CONJUNCTION robustness. robustness CONJUNCTION generalization. generalization EVALUATE-FOR generative models. method USED-FOR quantifying disentanglement. conditional submanifolds PART-OF representation. topological similarity FEATURE-OF conditional submanifolds. generative model USED-FOR method. unsupervised and supervised variants PART-OF method. method COMPARE models. models COMPARE method. Task are Learning disentangled representations, and measuring disentanglement. ","This paper proposes a method for quantifying disentanglement between two conditional submanifolds in a generative model. The method is based on the notion of topological similarity, which is a measure of the similarity between two representations. The key idea is to measure the disentangledness of a conditional sub-manifold by measuring the difference between the topological similarities between the two sub-submanifolded representations. This is done by computing the difference of the topology of the conditional subfolders of the two representations, and then computing the distance between the sub-folders. The authors show that the proposed method is able to achieve better disentangling performance than existing methods. ","This paper proposes a method for quantifying disentanglement between two conditional submanifolds in a generative model. The method is based on the notion of topological similarity, which is a measure of the similarity between two representations. The key idea is to measure the disentangledness of a conditional sub-manifold by measuring the difference between the topological similarities between the two sub-submanifolded representations. This is done by computing the difference of the topology of the conditional subfolders of the two representations, and then computing the distance between the sub-folders. The authors show that the proposed method is able to achieve better disentangling performance than existing methods. "
1152,SP:ef1ee7b77e1c2fb3d76db27049a3bce42760d14e,"unauthorized exploitation of personal data USED-FOR commercial models. sample - wise and class - wise forms FEATURE-OF error - minimizing noise. personal data USED-FOR deep learning models. Method is deep learning. Task are unauthorized data exploitation, and face recognition. OtherScientificTerm are Error - minimizing noise, and noise. Generic is model. Metric is normal data utility. ","This paper studies the problem of unauthorized data exploitation in the context of face recognition. Specifically, the authors consider the case where a model is trained on a set of samples and the data is not available to the model, but the model is used to learn a classifier. The authors show that the noise produced by the model can be seen as a function of the sample size and the class of the data. They also show that this noise can be viewed as a measure of the utility of the model. ","This paper studies the problem of unauthorized data exploitation in the context of face recognition. Specifically, the authors consider the case where a model is trained on a set of samples and the data is not available to the model, but the model is used to learn a classifier. The authors show that the noise produced by the model can be seen as a function of the sample size and the class of the data. They also show that this noise can be viewed as a measure of the utility of the model. "
1161,SP:4e8a835174f20df36d3d8d27fbcbbf2c68490032,chess CONJUNCTION Go. Go CONJUNCTION chess. Go CONJUNCTION shogi. shogi CONJUNCTION Go. MuZero USED-FOR game - playing agents. MuZero COMPARE AlphaZero. AlphaZero COMPARE MuZero. game - playing agents COMPARE AlphaZero. AlphaZero COMPARE game - playing agents. model of environmental dynamics USED-FOR MuZero. deterministic environments USED-FOR MuZero. MuZero USED-FOR Nondeterministic MuZero ( NDMZ ). Nondeterministic Monte Carlo Tree Search CONJUNCTION extensive - form games. extensive - form games CONJUNCTION Nondeterministic Monte Carlo Tree Search. MuZero network architecture CONJUNCTION tree search. tree search CONJUNCTION MuZero network architecture. chance player PART-OF MuZero network architecture. chance player PART-OF tree search. Nondeterministic Monte Carlo Tree Search USED-FOR NDMZ. extensive - form games USED-FOR NDMZ. NDMZ USED-FOR chance. chance player PART-OF NDMZ. NDMZ USED-FOR model. model USED-FOR game. Method is MuZero algorithm. Material is Atari suite. OtherScientificTerm is environmental dynamics. ,"This paper proposes a new algorithm, Nondeterministic MuZero (NDMZ), for learning a game-playing agent in deterministic environments. NDMZ is based on the MuZero algorithm, which is an extension of MuZero. The main idea is to use a model of environmental dynamics to model the game dynamics, and then use the model to learn an agent that can play the game. The proposed algorithm is evaluated on extensive-form games (e.g. chess, Go, and shogi) and is shown to outperform AlphaZero on these games.","This paper proposes a new algorithm, Nondeterministic MuZero (NDMZ), for learning a game-playing agent in deterministic environments. NDMZ is based on the MuZero algorithm, which is an extension of MuZero. The main idea is to use a model of environmental dynamics to model the game dynamics, and then use the model to learn an agent that can play the game. The proposed algorithm is evaluated on extensive-form games (e.g. chess, Go, and shogi) and is shown to outperform AlphaZero on these games."
1170,SP:73ae9c167dac3d92788a08891b0831f3e4997140,"Hierarchical approaches USED-FOR reinforcement learning. Hierarchical approaches USED-FOR learning. Hierarchical approaches USED-FOR data efficiency. Hindsight Off - policy Options ( HO2 ) HYPONYM-OF off - policy option learning algorithm. temporal and action abstraction USED-FOR option framework. flat policies COMPARE mixture policies. mixture policies COMPARE flat policies. mixture policies COMPARE option policies. option policies COMPARE mixture policies. flat policies COMPARE on - policy option methods. on - policy option methods COMPARE flat policies. off - policy training CONJUNCTION backpropagation. backpropagation CONJUNCTION off - policy training. policy components USED-FOR backpropagation. dynamic programming inference procedure USED-FOR off - policy training. dynamic programming inference procedure USED-FOR backpropagation. HO2 COMPARE option learning methods. option learning methods COMPARE HO2. raw pixel inputs USED-FOR simulated robot manipulation tasks. intuitive extension USED-FOR temporal abstraction. OtherScientificTerm are abstractions, data - generating behavior policy, trust - region constraints, and pre - trained options. Method are policy optimization, off - policy optimization, and action and temporal abstraction. Task is off - policy option learning. ","This paper proposes Hindsight Off-policy Options (HO2), an option learning algorithm for reinforcement learning. HO2 is based on the idea of Hindsight off-policy option learning (HO1), which is an extension of the Hindsight option learning framework. The main difference between HO2 and HO1 is that HO2 uses a dynamic programming inference procedure to learn the policy components for backpropagation, while HO1 uses a static programming procedure for option learning. The proposed method is shown to outperform HO1 and HO2 on simulated robot manipulation tasks.","This paper proposes Hindsight Off-policy Options (HO2), an option learning algorithm for reinforcement learning. HO2 is based on the idea of Hindsight off-policy option learning (HO1), which is an extension of the Hindsight option learning framework. The main difference between HO2 and HO1 is that HO2 uses a dynamic programming inference procedure to learn the policy components for backpropagation, while HO1 uses a static programming procedure for option learning. The proposed method is shown to outperform HO1 and HO2 on simulated robot manipulation tasks."
1179,SP:f79d9722256fb6b258bc1310bf1f6fb842303a0a,Reinforcement learning ( RL ) algorithms USED-FOR maximizing the expected cumulative return. drug discovery HYPONYM-OF applications. objective function USED-FOR expected maximum reward. functional form USED-FOR Bellman equation. Bellman operators USED-FOR functional form. formulation USED-FOR synthesizable molecule generation. real - world drug discovery pipeline FEATURE-OF synthesizable molecule generation. Generic is framework. Method is RL agent. OtherScientificTerm is expected cumulative return. ,"This paper studies the problem of maximizing the expected cumulative return (ECR) of an RL agent in the context of drug discovery. In particular, the authors consider the case where the agent has access to a large number of samples and the goal is to maximize the expected maximum reward. The authors propose a Bellman equation that is a functional form of Bellman operators, which can be expressed as a function of the agent's reward function. They show that the proposed algorithm can be used to solve the problem in a real-world drug discovery setting. ","This paper studies the problem of maximizing the expected cumulative return (ECR) of an RL agent in the context of drug discovery. In particular, the authors consider the case where the agent has access to a large number of samples and the goal is to maximize the expected maximum reward. The authors propose a Bellman equation that is a functional form of Bellman operators, which can be expressed as a function of the agent's reward function. They show that the proposed algorithm can be used to solve the problem in a real-world drug discovery setting. "
1188,SP:bd4b1781448def4327214c78f07538d285119ef9,"neural networks USED-FOR fixed output dimension. neural network architectures USED-FOR output features. neural networks USED-FOR features. Contextual HyperNetwork ( CHN ) HYPONYM-OF auxiliary model. base model USED-FOR feature. CHN COMPARE re - training and fine - tuning approaches. re - training and fine - tuning approaches COMPARE CHN. neural network USED-FOR CHN. CHN USED-FOR partial variational autoencoder ( P - VAE ). partial variational autoencoder ( P - VAE ) HYPONYM-OF deep generative model. deep generative model USED-FOR missing features. missing features PART-OF sparsely - observed data. CHN USED-FOR CHNs. e - learning CONJUNCTION healthcare tasks. healthcare tasks CONJUNCTION e - learning. system COMPARE imputation and meta - learning baselines. imputation and meta - learning baselines COMPARE system. recommender systems CONJUNCTION e - learning. e - learning CONJUNCTION recommender systems. imputation and meta - learning baselines USED-FOR recommender systems. few - shot learning USED-FOR features. system USED-FOR features. few - shot learning EVALUATE-FOR imputation and meta - learning baselines. healthcare tasks EVALUATE-FOR system. recommender systems EVALUATE-FOR system. few - shot learning EVALUATE-FOR system. Method is deep learning. Task are online learning settings, and recommender system. ","This paper proposes Contextual HyperNetwork (CHN), an auxiliary model for learning missing features from sparsely-observed data. The proposed model is based on the partial variational autoencoder (P-VAE) framework. The authors show that the proposed model outperforms the baselines in few-shot learning and recommender systems. ","This paper proposes Contextual HyperNetwork (CHN), an auxiliary model for learning missing features from sparsely-observed data. The proposed model is based on the partial variational autoencoder (P-VAE) framework. The authors show that the proposed model outperforms the baselines in few-shot learning and recommender systems. "
1197,SP:8e4677cc6071a33397347679308165c10dca2aae,data inefficiency CONJUNCTION catastrophic forgetting. catastrophic forgetting CONJUNCTION data inefficiency. Bayesian paradigm USED-FOR deep learning. poor calibration CONJUNCTION data inefficiency. data inefficiency CONJUNCTION poor calibration. Bayesian inference USED-FOR high - dimensional parameter spaces. high - dimensional parameter spaces FEATURE-OF deep neural networks. deep neural networks USED-FOR Bayesian inference. restrictive approximations USED-FOR Bayesian inference. model parameters USED-FOR inference. expressive posterior approximations USED-FOR full model. Bayesian deep learning method USED-FOR full covariance Gaussian posterior approximation. Bayesian deep learning method USED-FOR point estimate. subnetwork USED-FOR full covariance Gaussian posterior approximation. subnetwork selection procedure USED-FOR posterior uncertainty. approach COMPARE point - estimated networks. point - estimated networks COMPARE approach. approach COMPARE methods. methods COMPARE approach. full network USED-FOR expressive posterior approximations. expressive posterior approximations USED-FOR methods. OtherScientificTerm is point estimates. ,This paper proposes a Bayesian deep learning method for Bayesian inference. The proposed method is based on a subnetwork selection procedure to select the optimal subnetwork for the full model. The authors show that the proposed method outperforms existing methods in terms of robustness to catastrophic forgetting and data inefficiency. ,This paper proposes a Bayesian deep learning method for Bayesian inference. The proposed method is based on a subnetwork selection procedure to select the optimal subnetwork for the full model. The authors show that the proposed method outperforms existing methods in terms of robustness to catastrophic forgetting and data inefficiency. 
1206,SP:be361952fe9de545f68b8a060f790d54c6755998,generalization CONJUNCTION applicability. applicability CONJUNCTION generalization. generalization EVALUATE-FOR embedding techniques. state representations USED-FOR Model - free reinforcement learning approaches. approach USED-FOR jointly learning embeddings. model USED-FOR embeddings. generic architecture USED-FOR policy. these USED-FOR policy. these USED-FOR generic architecture. embedded representations USED-FOR generalization. approach USED-FOR embedded representations. it COMPARE models. models COMPARE it. approach COMPARE it. it COMPARE approach. gaming EVALUATE-FOR it. recommender systems EVALUATE-FOR it. approach COMPARE models. models COMPARE approach. state / action spaces FEATURE-OF discrete / continuous domains. discrete / continuous domains EVALUATE-FOR models. discrete / continuous domains EVALUATE-FOR it. recommender systems EVALUATE-FOR approach. gaming EVALUATE-FOR approach. Method is reinforcement learning. Generic is approaches. Material is discrete and continuous domains. OtherScientificTerm is embedding spaces. ,"This paper proposes a model-free reinforcement learning approach to jointly learn embeddings for both discrete and continuous domains. The proposed approach is based on the idea that the embedding space of a policy can be represented as a set of embedding spaces, which can be used to learn a policy that can be applied to any discrete or continuous domain. The approach is evaluated on a number of tasks, including recommender systems and reinforcement learning. The paper shows that the proposed approach outperforms the baselines in terms of generalization and applicability.","This paper proposes a model-free reinforcement learning approach to jointly learn embeddings for both discrete and continuous domains. The proposed approach is based on the idea that the embedding space of a policy can be represented as a set of embedding spaces, which can be used to learn a policy that can be applied to any discrete or continuous domain. The approach is evaluated on a number of tasks, including recommender systems and reinforcement learning. The paper shows that the proposed approach outperforms the baselines in terms of generalization and applicability."
1215,SP:ebb6bffcc4c2129e09ef5561c19df43c42ad18c0,"viewmaker networks HYPONYM-OF generative models. Viewmakers HYPONYM-OF stochastic bounded adversaries. they USED-FOR views. cropping CONJUNCTION color jitter. color jitter CONJUNCTION cropping. transfer accuracy EVALUATE-FOR welltuned SimCLR augmentations. color jitter HYPONYM-OF transformations. cropping HYPONYM-OF transformations. speech recordings CONJUNCTION wearable sensor data. wearable sensor data CONJUNCTION speech recordings. wearable sensor data EVALUATE-FOR baseline augmentations. speech recordings EVALUATE-FOR baseline augmentations. Viewmaker views CONJUNCTION handcrafted views. handcrafted views CONJUNCTION Viewmaker views. transfer performance EVALUATE-FOR they. robustness EVALUATE-FOR they. viewmakers USED-FOR representation learning algorithms. Viewmaker networks USED-FOR unsupervised learning. Viewmaker networks USED-FOR complex and diverse input - dependent views. complex and diverse input - dependent views USED-FOR unsupervised learning. Task is unsupervised representation learning. Generic is models. Method is unsupervised representation learning methods. OtherScientificTerm are ` p - bounded perturbation, common image corruptions, and domain expertise. Material is CIFAR-10. ","This paper proposes a viewmaker network for unsupervised representation learning. Viewmaker networks are a generative model that can be used to learn a set of views that are robust to common image corruptions (e.g., cropping, color jitter, etc.). The authors show that the robustness of viewmaker networks can be improved by adding a stochastic bounded perturbation to the input image. The authors also show that their method is able to transfer well to a new domain. ","This paper proposes a viewmaker network for unsupervised representation learning. Viewmaker networks are a generative model that can be used to learn a set of views that are robust to common image corruptions (e.g., cropping, color jitter, etc.). The authors show that the robustness of viewmaker networks can be improved by adding a stochastic bounded perturbation to the input image. The authors also show that their method is able to transfer well to a new domain. "
1224,SP:ef7735be9423ad53059505c170e75201ca134573,"autonomous driving CONJUNCTION air traffic management. air traffic management CONJUNCTION autonomous driving. deep learning models USED-FOR high - assurance systems. air traffic management CONJUNCTION medical diagnosis. medical diagnosis CONJUNCTION air traffic management. medical diagnosis HYPONYM-OF high - assurance systems. autonomous driving HYPONYM-OF high - assurance systems. air traffic management HYPONYM-OF high - assurance systems. statistical, geometric, or topological signatures USED-FOR techniques. detection approaches USED-FOR outliers. KMNIST CONJUNCTION F - MNIST. F - MNIST CONJUNCTION KMNIST. CIFAR10 ( for SVHN ) CONJUNCTION KMNIST. KMNIST CONJUNCTION CIFAR10 ( for SVHN ). SVHN CONJUNCTION MNIST. MNIST CONJUNCTION SVHN. CIFAR10 CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR10. Imagenet CONJUNCTION LSUN. LSUN CONJUNCTION Imagenet. WideResNet CONJUNCTION DenseNet. DenseNet CONJUNCTION WideResNet. DenseNet CONJUNCTION LeNet5. LeNet5 CONJUNCTION DenseNet. in - distribution data CONJUNCTION Imagenet. Imagenet CONJUNCTION in - distribution data. ResNet34 CONJUNCTION WideResNet. WideResNet CONJUNCTION ResNet34. F - MNIST USED-FOR OOD data. F - MNIST HYPONYM-OF DNN architectures. SVHN USED-FOR in - distribution data. MNIST CONJUNCTION Imagenet. Imagenet CONJUNCTION MNIST. MNIST USED-FOR in - distribution data. SVHN CONJUNCTION Imagenet. Imagenet CONJUNCTION SVHN. ResNet34 HYPONYM-OF DNN architectures. LeNet5 HYPONYM-OF DNN architectures. DenseNet HYPONYM-OF DNN architectures. WideResNet HYPONYM-OF DNN architectures. Method are Deep neural networks ( DNNs ), and integrated","This paper proposes a method to detect outliers in deep neural networks (DNNs). The method is based on the observation that DNNs can be trained to detect out-of-distribution (OOD) outliers. The authors propose a method for detecting OOD outliers based on statistical, geometric, or topological signatures. The proposed method is evaluated on MNIST, CIFAR-10, Imagenet, and LSUN datasets. ","This paper proposes a method to detect outliers in deep neural networks (DNNs). The method is based on the observation that DNNs can be trained to detect out-of-distribution (OOD) outliers. The authors propose a method for detecting OOD outliers based on statistical, geometric, or topological signatures. The proposed method is evaluated on MNIST, CIFAR-10, Imagenet, and LSUN datasets. "
1233,SP:33920ec7ffefe3c1525cd5d4d53a851210d519da,"hierarchical VAE COMPARE PixelCNN. PixelCNN COMPARE hierarchical VAE. log - likelihood EVALUATE-FOR PixelCNN. natural image benchmarks EVALUATE-FOR PixelCNN. VAEs USED-FOR autoregressive models. VAEs USED-FOR models. autoregressive models COMPARE VAEs. VAEs COMPARE autoregressive models. loglikelihood EVALUATE-FOR autoregressive models. loglikelihood EVALUATE-FOR VAEs. ImageNet CONJUNCTION FFHQ. FFHQ CONJUNCTION ImageNet. stochastic depth FEATURE-OF VAE. PixelCNN COMPARE VAEs. VAEs COMPARE PixelCNN. likelihoods EVALUATE-FOR VAEs. VAE USED-FOR hierarchical visual representations. FFHQ-256 USED-FOR VAE. VAEs USED-FOR global features. VAEs USED-FOR local details. multiscale generative procedure COMPARE PixelCNN. PixelCNN COMPARE multiscale generative procedure. log - likelihood EVALUATE-FOR PixelCNN. log - likelihood EVALUATE-FOR multiscale generative procedure. Generic is they. OtherScientificTerm are insufficient depth, and Low resolution High resolution. Material is high - resolution images. Method is generative process. ",This paper proposes a new generative model for hierarchical VAEs. The proposed method is based on the idea that VAEs can be used to generate high-resolution images in a hierarchical fashion. The authors show that the proposed method outperforms the state-of-the-art PixelCNN and FFHQ-256 generative models on a variety of image benchmarks. They also show that their method is able to achieve better performance than PixelCNN in terms of log-likelihood. ,This paper proposes a new generative model for hierarchical VAEs. The proposed method is based on the idea that VAEs can be used to generate high-resolution images in a hierarchical fashion. The authors show that the proposed method outperforms the state-of-the-art PixelCNN and FFHQ-256 generative models on a variety of image benchmarks. They also show that their method is able to achieve better performance than PixelCNN in terms of log-likelihood. 
1242,SP:0a4e6c8017a1294fe2424799a0048d58eaf04cb3,"methods USED-FOR unsupervised visual representations. uninformative examples PART-OF this. randomly sampled negative examples USED-FOR NCE. semi - hard negatives USED-FOR contrastive representations. bias CONJUNCTION variance. variance CONJUNCTION bias. estimators COMPARE NCE. NCE COMPARE estimators. variance EVALUATE-FOR NCE. CMC CONJUNCTION MoCo. MoCo CONJUNCTION CMC. IR CONJUNCTION CMC. CMC CONJUNCTION IR. image benchmarks EVALUATE-FOR linear evaluation. models USED-FOR approach. IR HYPONYM-OF models. linear evaluation EVALUATE-FOR approach. MoCo HYPONYM-OF models. accuracy EVALUATE-FOR approach. image benchmarks EVALUATE-FOR approach. CMC HYPONYM-OF models. instance segmentation CONJUNCTION key - point detection. key - point detection CONJUNCTION instance segmentation. object detection CONJUNCTION instance segmentation. instance segmentation CONJUNCTION object detection. features USED-FOR image distributions. object detection CONJUNCTION key - point detection. key - point detection CONJUNCTION object detection. features USED-FOR downstream tasks. Meta - Dataset collection FEATURE-OF image distributions. key - point detection HYPONYM-OF downstream tasks. object detection HYPONYM-OF downstream tasks. instance segmentation HYPONYM-OF downstream tasks. Method are contrastive learning, metric learning, and mutual information estimators. OtherScientificTerm are noise - contrastive estimation ( NCE ) bound, mutual information, and lower - bounds of mutual information. ","This paper proposes a novel method for learning contrastive representations for unsupervised learning. The authors propose to use semi-hard negatives as negative examples in contrastive learning. They show that the proposed method outperforms existing methods in terms of variance, bias, and mutual information. They also provide a lower bound on the mutual information of the learned representations. ","This paper proposes a novel method for learning contrastive representations for unsupervised learning. The authors propose to use semi-hard negatives as negative examples in contrastive learning. They show that the proposed method outperforms existing methods in terms of variance, bias, and mutual information. They also provide a lower bound on the mutual information of the learned representations. "
1251,SP:613a0e2d8cbe703f37c182553801be7537333f64,"gradient sharing mechanism USED-FOR machine learning systems. gradient sharing mechanism USED-FOR Private training data. federated learning ( FL ) HYPONYM-OF machine learning systems. data leakage attack USED-FOR batch data. shared aggregated gradients USED-FOR batch data. catastrophic data leakage PART-OF federated learning ( CAFE ). data leakage attacks COMPARE CAFE. CAFE COMPARE data leakage attacks. CAFE USED-FOR large - batch data leakage attack. data recovery quality EVALUATE-FOR large - batch data leakage attack. data recovery quality EVALUATE-FOR CAFE. CAFE USED-FOR private data. vertical and horizontal FL settings EVALUATE-FOR CAFE. shared aggregated gradients USED-FOR private data. vertical case HYPONYM-OF FL. data leakage risks FEATURE-OF learning settings. OtherScientificTerm are batch size, data leakage, and training gradients. Generic is method. ",This paper studies the problem of catastrophic data leakage in federated learning (FL). The authors propose a new method called CAFE to address the issue of large-batch data leakage. The proposed method is based on the idea of aggregated gradients for private training data. The authors show that the proposed method can be used to mitigate the data leakage problem in both vertical and horizontal FL settings. ,This paper studies the problem of catastrophic data leakage in federated learning (FL). The authors propose a new method called CAFE to address the issue of large-batch data leakage. The proposed method is based on the idea of aggregated gradients for private training data. The authors show that the proposed method can be used to mitigate the data leakage problem in both vertical and horizontal FL settings. 
1260,SP:ce229295081ff04b26f33829f2c3396b90897b5d,physics CONJUNCTION vision. vision CONJUNCTION physics. vision CONJUNCTION robotics. robotics CONJUNCTION vision. Unsupervised learning of interactions USED-FOR physics. physics CONJUNCTION robotics. robotics CONJUNCTION physics. Unsupervised learning of interactions USED-FOR vision. Unsupervised learning of interactions USED-FOR robotics. multi - agent trajectories USED-FOR Unsupervised learning of interactions. neural relational inference USED-FOR static relations. deep generative model USED-FOR dynamic relations. simulated physics system USED-FOR dynamic relation scenarios. periodic and additive dynamics HYPONYM-OF dynamic relation scenarios. training scheme CONJUNCTION model architecture. model architecture CONJUNCTION training scheme. dynamic relational inference accuracy EVALUATE-FOR model architecture. model USED-FOR coordination and competition patterns. real - world multi - agent basketball trajectories USED-FOR model. real - world multi - agent basketball trajectories USED-FOR coordination and competition patterns. Task is dynamic relational inference. OtherScientificTerm is interactions. ,This paper proposes a deep generative model for dynamic relational inference. The model is trained on a simulated physics system with periodic and additive dynamics. The authors show that the model outperforms baselines in terms of dynamic inference accuracy. ,This paper proposes a deep generative model for dynamic relational inference. The model is trained on a simulated physics system with periodic and additive dynamics. The authors show that the model outperforms baselines in terms of dynamic inference accuracy. 
1269,SP:9f4b77d39f1deca28324fb637a0a77e89976baa8,"Collaborative filtering USED-FOR predicting potential user - item ratings. latent factors PART-OF user - item rating matrix. transductive setting USED-FOR user - specific latent factors. inductive collaborative filtering framework USED-FOR hidden relational graph. rating matrix USED-FOR hidden relational graph. model USED-FOR inductively computing user - specific representations. expressiveness EVALUATE-FOR feature - driven inductive models. model COMPARE feature - driven inductive models. feature - driven inductive models COMPARE model. feature USED-FOR inductively computing user - specific representations. model COMPARE transductive models. transductive models COMPARE model. model USED-FOR inductive learning. cold - start users EVALUATE-FOR them. matrix completion benchmarks EVALUATE-FOR inductive learning. matrix completion benchmarks EVALUATE-FOR model. OtherScientificTerm are user - item ratings, dense weighted graphs, historical rating patterns, relational graphs, and latent space. Method are base matrix factorization model, and relation inference model. ","This paper proposes a new model for learning user-item rating models. The model is based on the collaborative filtering framework, where each user is given a latent variable and a relational graph, and the goal is to learn a latent factorization model for each latent variable. The proposed model is evaluated on a number of matrix completion tasks, and is shown to outperform the baselines. ","This paper proposes a new model for learning user-item rating models. The model is based on the collaborative filtering framework, where each user is given a latent variable and a relational graph, and the goal is to learn a latent factorization model for each latent variable. The proposed model is evaluated on a number of matrix completion tasks, and is shown to outperform the baselines. "
1278,SP:9f9e9b0e37e59267d8516ab914bd619c53fbc9ec,"autoencoder - based disentangled representation learning methods USED-FOR disentanglement. disentangled representation learning CONJUNCTION reconstruction quality. reconstruction quality CONJUNCTION disentangled representation learning. detail information FEATURE-OF image data. correlated latent variables USED-FOR detail information. deep generative model USED-FOR missing correlated latent variables. deep generative model USED-FOR low - quality reconstruction. β - TCVAE HYPONYM-OF disentangled representation learning method. disentangled representation learning method USED-FOR disentangled factors. normalizing flows CONJUNCTION mixtures of Gaussians. mixtures of Gaussians CONJUNCTION normalizing flows. likelihood - based models CONJUNCTION implicit models. implicit models CONJUNCTION likelihood - based models. implicit models CONJUNCTION tractable models. tractable models CONJUNCTION implicit models. variational autoencoders CONJUNCTION implicit models. implicit models CONJUNCTION variational autoencoders. generative adversarial networks HYPONYM-OF implicit models. variational autoencoders HYPONYM-OF likelihood - based models. tractable models HYPONYM-OF model classes. likelihood - based models HYPONYM-OF model classes. mixtures of Gaussians HYPONYM-OF tractable models. implicit models HYPONYM-OF model classes. normalizing flows HYPONYM-OF tractable models. multi - stage model COMPARE state - of - the - art methods. state - of - the - art methods COMPARE multi - stage model. reconstruction quality EVALUATE-FOR state - of - the - art methods. disentanglement EVALUATE-FOR state - of - the - art methods. disentanglement EVALUATE-FOR multi - stage model. reconstruction quality EVALUATE-FOR multi - stage model. OtherScientificTerm are statistical independence of the latent factors, and D - separation. Generic are approach, and model. Method is multi - stage modelling approach. ","This paper proposes a multi-stage disentangled representation learning method for image disentanglement. The main idea is to use a deep generative model to disentangle the latent factors in the latent space, and then use a variational autoencoder to reconstruct the image. The authors show that the proposed method outperforms state-of-the-art methods in terms of reconstruction quality and disentangling performance. ","This paper proposes a multi-stage disentangled representation learning method for image disentanglement. The main idea is to use a deep generative model to disentangle the latent factors in the latent space, and then use a variational autoencoder to reconstruct the image. The authors show that the proposed method outperforms state-of-the-art methods in terms of reconstruction quality and disentangling performance. "
1287,SP:e398873e29b05176e1d52dc6f86a59a4f405e6fd,"Mutual information ( MI ) maximization USED-FOR learning representations of data. representations USED-FOR learning. representations USED-FOR reinforcement learning ( RL ). representations USED-FOR RL. MI objectives USED-FOR representations. samples of high - dimensional observations USED-FOR MI. state representation USED-FOR optimal policy. objectives USED-FOR insufficient representations. visual observations FEATURE-OF simulated game environment. OtherScientificTerm are irrelevant and redundant information, MI based objectives, and structure of the MDP. Task is control. Generic is methods. ","This paper studies the problem of learning representations of data in reinforcement learning. The authors propose a new objective called mutual information maximization (MI) that maximizes the mutual information between the state representation of the policy and the representations of high-dimensional observations. The proposed objective is motivated by the observation that the representation of high dimensional observations can be used to learn representations of low dimensional observations, which can then be used as the basis for learning representations for high dimensional data. The paper shows that the proposed objective can be applied to a variety of RL tasks, including reinforcement learning, reinforcement learning with reinforcement learning (RL), and reinforcement learning in a simulated game environment. Experiments are conducted to demonstrate the effectiveness of the proposed MI objective.","This paper studies the problem of learning representations of data in reinforcement learning. The authors propose a new objective called mutual information maximization (MI) that maximizes the mutual information between the state representation of the policy and the representations of high-dimensional observations. The proposed objective is motivated by the observation that the representation of high dimensional observations can be used to learn representations of low dimensional observations, which can then be used as the basis for learning representations for high dimensional data. The paper shows that the proposed objective can be applied to a variety of RL tasks, including reinforcement learning, reinforcement learning with reinforcement learning (RL), and reinforcement learning in a simulated game environment. Experiments are conducted to demonstrate the effectiveness of the proposed MI objective."
1296,SP:881185782a9ec32fcbab14b42b78bf94edeba4b0,"finite dimensional representation FEATURE-OF semi - infinite dual. finite - dimensional convex copositive program USED-FOR non - convex neural network training problem. global optima CONJUNCTION copositive programs. copositive programs CONJUNCTION global optima. neural networks CONJUNCTION copositive programs. copositive programs CONJUNCTION neural networks. neural networks USED-FOR global optima. neural networks USED-FOR copositive programs. semi - nonnegative matrix factorization USED-FOR neural networks. semi - nonnegative matrix factorization USED-FOR copositive programs. algorithms USED-FOR global minimum. algorithms USED-FOR vector output neural network training problem. global minimum FEATURE-OF vector output neural network training problem. computational complexity EVALUATE-FOR filter size. computational complexity EVALUATE-FOR convolutional architectures. global optimum FEATURE-OF neural network training problem. soft - thresholded SVD USED-FOR neural network training problem. OtherScientificTerm is convex semi - infinite dual. Method are copositive relaxation, and Stochastic Gradient Descent. ",This paper studies the problem of learning a non-convex convex copositive program from a finite-dimensional representation. The authors show that the global minimum of a convex program can be obtained by solving a soft-thresholded SVD problem. They also show that this problem can be solved by a neural network trained on a vector output neural network. ,This paper studies the problem of learning a non-convex convex copositive program from a finite-dimensional representation. The authors show that the global minimum of a convex program can be obtained by solving a soft-thresholded SVD problem. They also show that this problem can be solved by a neural network trained on a vector output neural network. 
1305,SP:8613b2fcfd076d3e28a9940bad0c490a6557c10c,"vision USED-FOR learning disentangled, object - centric scene representations. unsupervised object segmentation USED-FOR LORL. MONet and Slot Attention HYPONYM-OF unsupervised object segmentation. algorithms USED-FOR object - centric representation. properties CONJUNCTION spatial relationships. spatial relationships CONJUNCTION properties. object categories CONJUNCTION properties. properties CONJUNCTION object categories. representations USED-FOR concepts. object categories HYPONYM-OF concepts. spatial relationships HYPONYM-OF concepts. properties HYPONYM-OF concepts. object - centric concepts USED-FOR object - centric representations. language USED-FOR object - centric concepts. LORL CONJUNCTION unsupervised segmentation algorithms. unsupervised segmentation algorithms CONJUNCTION LORL. LORL USED-FOR object segmentation. LORL USED-FOR MONet and Slot Attention. MONet and Slot Attention USED-FOR object segmentation. language USED-FOR LORL. concepts USED-FOR tasks. LORL CONJUNCTION segmentation algorithms. segmentation algorithms CONJUNCTION LORL. concepts CONJUNCTION segmentation algorithms. segmentation algorithms CONJUNCTION concepts. segmentation algorithms USED-FOR tasks. LORL USED-FOR concepts. MONet HYPONYM-OF segmentation algorithms. referring expression comprehension HYPONYM-OF tasks. OtherScientificTerm is language input. ","This paper proposes a new unsupervised object segmentation method for learning disentangled, object-centric scene representations. The proposed method, called LORL, is a language-based approach to learn object-related concepts. The key idea is to learn a set of concepts for each object in the scene, and then use these concepts to learn an object-based representation of the scene. The method is evaluated on a variety of tasks, including object segmentations, object classification, and referring expression comprehension. ","This paper proposes a new unsupervised object segmentation method for learning disentangled, object-centric scene representations. The proposed method, called LORL, is a language-based approach to learn object-related concepts. The key idea is to learn a set of concepts for each object in the scene, and then use these concepts to learn an object-based representation of the scene. The method is evaluated on a variety of tasks, including object segmentations, object classification, and referring expression comprehension. "
1314,SP:5e73b99c9942dd85bf70a65ad3e3c6a45d69b66b,"fact triplets PART-OF KG. fact triplets USED-FOR embedding methods. logic rules USED-FOR rich background information. rules USED-FOR reasoning. EM - RBR USED-FOR multi - relation reasoning link prediction. relational background knowledge USED-FOR multi - relation reasoning link prediction. rules FEATURE-OF relational background knowledge. relational background knowledge USED-FOR EM - RBR. FB15k CONJUNCTION WN18. WN18 CONJUNCTION FB15k. EM - RBR COMPARE models. models COMPARE EM - RBR. dataset EVALUATE-FOR model. WN18 EVALUATE-FOR models. FB15k EVALUATE-FOR EM - RBR. FB15k EVALUATE-FOR models. Task are Knowledge graph completion, and representation of the knowledge graph. OtherScientificTerm are knowledge graph, algebraic space, and relational patterns. Method are embedding models, and embedding. Generic is framework. Metric is prediction accuracy. Material is FB15k - R. ","This paper proposes a new embedding method for multi-relational reasoning link prediction based on relational background knowledge. The proposed method, EM-RBR, is based on the fact triplet embedding framework. The key idea is to learn a set of logic rules that can be applied to the knowledge graph. The authors show that the proposed method outperforms baselines on the FB15k-R dataset.","This paper proposes a new embedding method for multi-relational reasoning link prediction based on relational background knowledge. The proposed method, EM-RBR, is based on the fact triplet embedding framework. The key idea is to learn a set of logic rules that can be applied to the knowledge graph. The authors show that the proposed method outperforms baselines on the FB15k-R dataset."
1323,SP:19b74093512c4e5f8c504e96c590ac1eb7e2ce9b,"video game HYPONYM-OF structured, dynamic environment. procedural knowledge USED-FOR Black - box models. monolithic hidden state USED-FOR Black - box models. modularity FEATURE-OF knowledge. architecture USED-FOR declarative and procedural knowledge. schemata USED-FOR state updates. active modules CONJUNCTION passive external knowledge sources. passive external knowledge sources CONJUNCTION active modules. passive external knowledge sources USED-FOR state updates. object files HYPONYM-OF active modules. schemata HYPONYM-OF passive external knowledge sources. active modules PART-OF architecture. attention USED-FOR object files. LSTM CONJUNCTION GRU. GRU CONJUNCTION LSTM. input - output interface FEATURE-OF drop - in replacement. drop - in replacement PART-OF architecture. LSTM HYPONYM-OF normal recurrent networks. GRU HYPONYM-OF normal recurrent networks. OtherScientificTerm are declarative knowledge, systematicity, and object tokens. Generic is they. Metric is generalization. Material is intuitive physics benchmark. ","This paper proposes a new method for learning declarative and procedural knowledge in a video game. The method is based on the idea of modularity, which allows for the use of multiple active and passive external knowledge sources. The authors show that the proposed method is able to generalize better than existing methods. The main contribution of the paper is the introduction of a drop-in replacement method for the input-output interface. The proposed method can be applied to both LSTM and GRU models. ","This paper proposes a new method for learning declarative and procedural knowledge in a video game. The method is based on the idea of modularity, which allows for the use of multiple active and passive external knowledge sources. The authors show that the proposed method is able to generalize better than existing methods. The main contribution of the paper is the introduction of a drop-in replacement method for the input-output interface. The proposed method can be applied to both LSTM and GRU models. "
1332,SP:42a3c0453ab136537b5944a577d63412f3c22560,"Neural module networks ( NMN ) USED-FOR image - grounded tasks. synthetic images USED-FOR Visual Question Answering ( VQA ). Visual Question Answering ( VQA ) HYPONYM-OF image - grounded tasks. NMN USED-FOR video - grounded language tasks. complexity EVALUATE-FOR visual tasks. tasks COMPARE visual tasks. visual tasks COMPARE tasks. visual temporal variance FEATURE-OF visual tasks. complexity EVALUATE-FOR tasks. NMN approaches USED-FOR image - grounded tasks. information retrieval process USED-FOR video - grounded language tasks. VilNMN USED-FOR action - based inputs. VilNMN USED-FOR language components. video QA CONJUNCTION video - grounded dialogues. video - grounded dialogues CONJUNCTION video QA. video - grounded language tasks EVALUATE-FOR VilNMN. video - grounded dialogues HYPONYM-OF video - grounded language tasks. video QA HYPONYM-OF video - grounded language tasks. Method are neural modules, and neural module networks. OtherScientificTerm is visual cues. ","This paper proposes a neural module network (NMN) architecture for video-grounded language tasks. The proposed architecture is based on the idea of neural module networks (NMNs) for image-based tasks, and is able to perform well on video-based language tasks such as visual question answering (VQA) and video QA. The authors show that the proposed VilNMN architecture can achieve state-of-the-art performance on these tasks. ","This paper proposes a neural module network (NMN) architecture for video-grounded language tasks. The proposed architecture is based on the idea of neural module networks (NMNs) for image-based tasks, and is able to perform well on video-based language tasks such as visual question answering (VQA) and video QA. The authors show that the proposed VilNMN architecture can achieve state-of-the-art performance on these tasks. "
1341,SP:126ce41b7f44975e5962f8bcb43f61bf2ed315c4,"empirical game analysis CONJUNCTION deep reinforcement learning ( Deep RL ). deep reinforcement learning ( Deep RL ) CONJUNCTION empirical game analysis. Deep RL USED-FOR best response. mixture of opponent policies USED-FOR best response. PSRO USED-FOR Deep RL training. algorithms USED-FOR PSRO. PSRO USED-FOR policies. policies PART-OF empirical game. Mixed - Opponents USED-FOR pure - strategy opponent. strategy ’s action - value estimates COMPARE policies. policies COMPARE strategy ’s action - value estimates. strategy ’s action - value estimates USED-FOR pure - strategy opponent. algorithms USED-FOR PSRO. algorithms USED-FOR game. Method are Policy - Space Response Oracles ( PSRO ), and Mixed - Oracles. Task is learning policies in multiagent systems. Generic are algorithm, first, second, and policy. OtherScientificTerm is unobserved distribution of opponents. ","This paper studies the problem of learning policies in multi-agent reinforcement learning. The authors propose a new method called Policy-Space Response Oracles (PSRO), which is based on the idea of Mixed-Opponents (MOPs). The main idea is to use a mixture of opponent policies to learn the best response for each agent in an empirical game. In particular, the authors consider the case where there is an unobserved distribution of opponents, and the goal is to learn a policy that maximizes the mutual information between the mixed-opponents and the pure-strategy opponent. The proposed method is evaluated on a number of simulated and real-world datasets.","This paper studies the problem of learning policies in multi-agent reinforcement learning. The authors propose a new method called Policy-Space Response Oracles (PSRO), which is based on the idea of Mixed-Opponents (MOPs). The main idea is to use a mixture of opponent policies to learn the best response for each agent in an empirical game. In particular, the authors consider the case where there is an unobserved distribution of opponents, and the goal is to learn a policy that maximizes the mutual information between the mixed-opponents and the pure-strategy opponent. The proposed method is evaluated on a number of simulated and real-world datasets."
1350,SP:33e0b6099b32a6a2c0f2c7a8caa57ba2935d8b00,attention mechanism CONJUNCTION duration predictor. duration predictor CONJUNCTION attention mechanism. Tacotron 2 text - to - speech model USED-FOR Non - Attentive Tacotron. unaligned duration ratio CONJUNCTION word deletion rate. word deletion rate CONJUNCTION unaligned duration ratio. metrics USED-FOR large - scale robustness evaluation. pre - trained speech recognition model USED-FOR metrics. pre - trained speech recognition model USED-FOR large - scale robustness evaluation. Gaussian upsampling USED-FOR Non - Attentive Tacotron. Non - Attentive Tacotron COMPARE Tacotron 2. Tacotron 2 COMPARE Non - Attentive Tacotron. 5 - scale mean opinion score USED-FOR naturalness. 5 - scale mean opinion score EVALUATE-FOR Non - Attentive Tacotron. fine - grained variational auto - encoder USED-FOR duration predictor. semi - supervised or unsupervised manner USED-FOR duration predictor. semi - supervised or unsupervised manner USED-FOR method. fine - grained variational auto - encoder USED-FOR method. Metric is robustness. Method is supervised training. ,"This paper proposes a method to improve the robustness of Tacotron 2 text-to-speech models. The proposed method is based on a pre-trained speech recognition model and a Gaussian upsampling method for the duration predictor and attention mechanism. The duration predictor is trained using a fine-grained variational auto-encoder, while the attention mechanism is learned using a Gaussian upsampled Gaussian process. The authors show that the proposed method outperforms the existing methods in terms of naturalness and robustness evaluation. ","This paper proposes a method to improve the robustness of Tacotron 2 text-to-speech models. The proposed method is based on a pre-trained speech recognition model and a Gaussian upsampling method for the duration predictor and attention mechanism. The duration predictor is trained using a fine-grained variational auto-encoder, while the attention mechanism is learned using a Gaussian upsampled Gaussian process. The authors show that the proposed method outperforms the existing methods in terms of naturalness and robustness evaluation. "
1359,SP:ab9532306d294f85db84b9419ce826f046a7d95e,"layout estimation USED-FOR planning and navigation. planning and navigation USED-FOR robotics applications. layout estimation USED-FOR robotics applications. self - driving HYPONYM-OF robotics applications. supervised end - to - end framework USED-FOR estimation of bird ’s eye view layout. deep learning networks USED-FOR disparity estimation. deep learning networks USED-FOR network. internal bird ’s eye view feature representation USED-FOR layout estimation. stereo images USED-FOR features. features USED-FOR disparity feature volume. stereo images USED-FOR disparity feature volume. scene structure FEATURE-OF coarse - grained information. rich bird ’s eye view representation USED-FOR spatial reasoning. IPM features USED-FOR rich bird ’s eye view representation. IPM features CONJUNCTION projected feature volume. projected feature volume CONJUNCTION IPM features. projected feature volume USED-FOR rich bird ’s eye view representation. representation USED-FOR BEV semantic map. IPM features USED-FOR supervisory signal. supervisory signal USED-FOR stereo features. IPM features USED-FOR stereo features. datasets EVALUATE-FOR approach. synthetically generated dataset EVALUATE-FOR approach. synthetically generated dataset HYPONYM-OF datasets. datasets EVALUATE-FOR baseline techniques. Method are explicit depth estimation, and inverse perspective mapping ( IPM ). Generic is it. OtherScientificTerm are bird ’s eye view coordinates, bird ’s eye view, and fine - grained texture information. ",This paper proposes a method for bird’s eye view layout estimation. The method is based on the inverse perspective mapping (IPM) framework. The proposed method is evaluated on synthetic and real-world datasets. The authors show that the proposed method achieves state-of-the-art results.,This paper proposes a method for bird’s eye view layout estimation. The method is based on the inverse perspective mapping (IPM) framework. The proposed method is evaluated on synthetic and real-world datasets. The authors show that the proposed method achieves state-of-the-art results.
1368,SP:3a151e18a5e623e9bf6e39a6065bfba1d5156fc1,over - parameterization CONJUNCTION over - smoothing. over - smoothing CONJUNCTION over - parameterization. vanishing gradients CONJUNCTION over - parameterization. over - parameterization CONJUNCTION vanishing gradients. Relational Graph Neural Networks ( GNN ) COMPARE GNNs. GNNs COMPARE Relational Graph Neural Networks ( GNN ). methods USED-FOR GNNs. normalization techniques CONJUNCTION skip connection. skip connection CONJUNCTION normalization techniques. normalization techniques PART-OF methods. learning long - range patterns FEATURE-OF multi - relational graphs. GNNs USED-FOR learning long - range patterns. relation - aware GNN architecture USED-FOR long - range modeling between nodes. vector - based approach USED-FOR relation - aware GNN architecture. gated skip connections USED-FOR relation - aware GNN architecture. Graph Attention Network USED-FOR relation - aware GNN architecture. method COMPARE architectures. architectures COMPARE method. method COMPARE GNN variants. GNN variants COMPARE method. GNN variants COMPARE architectures. architectures COMPARE GNN variants. GNN variants USED-FOR deeper configurations. Method is deeper networks. Material is synthetic and real data. ,This paper proposes a relation-aware GNN architecture for learning long-range patterns in multi-relational graphs. The authors propose a vector-based approach to learn the relations between nodes in a graph. The proposed method is based on the Graph Attention Network (GAN) architecture. The method is evaluated on synthetic and real-world datasets.,This paper proposes a relation-aware GNN architecture for learning long-range patterns in multi-relational graphs. The authors propose a vector-based approach to learn the relations between nodes in a graph. The proposed method is based on the Graph Attention Network (GAN) architecture. The method is evaluated on synthetic and real-world datasets.
1377,SP:f9906d99f6ae5e32dda548bdccce9ae92d25b205,"Symbolic techniques USED-FOR neural network properties. Satisfiability Modulo Theory ( SMT ) solvers USED-FOR Symbolic techniques. gradient - based methods CONJUNCTION symbolic techniques. symbolic techniques CONJUNCTION gradient - based methods. technique USED-FOR gradient - based methods. technique USED-FOR minimal regions. technique USED-FOR large networks. Integrated Gradients USED-FOR gradient information. gradient information USED-FOR approach. SMT constraints USED-FOR minimal input mask discovery problem. approach USED-FOR mask regions. approach USED-FOR minimal masks. ImageNet CONJUNCTION Beer Reviews. Beer Reviews CONJUNCTION ImageNet. MNIST CONJUNCTION ImageNet. ImageNet CONJUNCTION MNIST. saliency scores EVALUATE-FOR gradient - based methods. approach COMPARE gradient - based methods. gradient - based methods COMPARE approach. saliency scores EVALUATE-FOR approach. MNIST EVALUATE-FOR technique. Task are model explanation, and neural network ’s prediction. OtherScientificTerm are threshold, mask, and saliency map. ","This paper proposes a new method to solve the minimal input mask discovery problem. The key idea is to use the Satisfiability Modulo Theory (SMT) solvers for solving the SMT problem, which is a well-studied problem in machine learning. The authors show that the proposed method is able to solve SMT problems in a more efficient way than gradient-based methods. The main contribution of the paper is the use of Integrated Gradients (IG) to learn the saliency map of the input mask. The method is evaluated on MNIST, Beer Reviews, and ImageNet. ","This paper proposes a new method to solve the minimal input mask discovery problem. The key idea is to use the Satisfiability Modulo Theory (SMT) solvers for solving the SMT problem, which is a well-studied problem in machine learning. The authors show that the proposed method is able to solve SMT problems in a more efficient way than gradient-based methods. The main contribution of the paper is the use of Integrated Gradients (IG) to learn the saliency map of the input mask. The method is evaluated on MNIST, Beer Reviews, and ImageNet. "
1386,SP:fc96fe4d0eeb0723bb7e4c9120c77981fc14731c,3D pose estimation HYPONYM-OF computer vision. deep learning approaches USED-FOR 3D pose estimation. deep neural networks CONJUNCTION 3D generative representations of objects. 3D generative representations of objects CONJUNCTION deep neural networks. deep neural networks PART-OF unified neural architecture. 3D generative representations of objects PART-OF unified neural architecture. generative vision models USED-FOR partial occlusion. NeMo HYPONYM-OF unified neural architecture. generative model of neural feature activations PART-OF dense 3D mesh. generative model of neural feature activations USED-FOR NeMo. differentiable rendering USED-FOR 3D object pose. NeMo CONJUNCTION feature representation of the target image. feature representation of the target image CONJUNCTION NeMo. reconstruction error EVALUATE-FOR NeMo. feature representations PART-OF mesh. local optima FEATURE-OF reconstruction loss. feature extractor USED-FOR feature representations. contrastive learning USED-FOR feature representations. contrastive learning USED-FOR feature extractor. occluded - PASCAL3D+ CONJUNCTION ObjectNet3D. ObjectNet3D CONJUNCTION occluded - PASCAL3D+. PASCAL3D+ CONJUNCTION occluded - PASCAL3D+. occluded - PASCAL3D+ CONJUNCTION PASCAL3D+. NeMo COMPARE deep networks. deep networks COMPARE NeMo. NeMo USED-FOR partial occlusion. regular data EVALUATE-FOR NeMo. mesh representation USED-FOR object geometry. 3D geometry USED-FOR 3D pose estimation. cuboid USED-FOR mesh representation. mesh representation USED-FOR NeMo. cuboid USED-FOR object geometry. Generic is code. ,"This paper proposes a new method for 3D pose estimation based on a generative model of neural feature activations. The proposed method, called NeMo, consists of two components: a feature extractor and a reconstruction loss. The reconstruction loss is based on contrastive learning, which is used to learn the feature representations of the target image and the feature representation of the feature map of the object. The feature map is then used to generate a dense 3D mesh, which can then be used to estimate the 3D object pose. Experiments show that the proposed method outperforms the state-of-the-art methods on both synthetic and real datasets.","This paper proposes a new method for 3D pose estimation based on a generative model of neural feature activations. The proposed method, called NeMo, consists of two components: a feature extractor and a reconstruction loss. The reconstruction loss is based on contrastive learning, which is used to learn the feature representations of the target image and the feature representation of the feature map of the object. The feature map is then used to generate a dense 3D mesh, which can then be used to estimate the 3D object pose. Experiments show that the proposed method outperforms the state-of-the-art methods on both synthetic and real datasets."
1395,SP:bde5b5b05d4a10634bd21a90cf0d8d22e2cda22d,"large scale retrieval - based applications USED-FOR Feature Compatible Learning ( FCL ). approaches USED-FOR feature compatible learning. old training data CONJUNCTION classifiers. classifiers CONJUNCTION old training data. old training data USED-FOR approaches. classifiers USED-FOR approaches. approach USED-FOR feature compatible learning. features USED-FOR approach. unified framework USED-FOR FCL. model USED-FOR pseudo classifier. random walk algorithm USED-FOR it. model USED-FOR embedding features. ImageNet ILSVRC 2012 and Places365 data EVALUATE-FOR approach. Method are embedding model, and Non - Inherent Feature Compatible Learning. Generic is old model. ","This paper proposes a novel framework for feature compatible learning (FCL) for large-scale retrieval-based applications. The proposed framework is based on the idea of non-inherent feature-compatibility learning (NIL), which is an extension of feature-compatible learning (NCL). The key idea is to use a pseudo classifier as the embedding model for the feature-matching task, and then use a random walk algorithm to learn the pseudo classifiers. The authors show that the proposed method is able to achieve better performance on ImageNet ILSVRC 2012 and Places365 datasets.","This paper proposes a novel framework for feature compatible learning (FCL) for large-scale retrieval-based applications. The proposed framework is based on the idea of non-inherent feature-compatibility learning (NIL), which is an extension of feature-compatible learning (NCL). The key idea is to use a pseudo classifier as the embedding model for the feature-matching task, and then use a random walk algorithm to learn the pseudo classifiers. The authors show that the proposed method is able to achieve better performance on ImageNet ILSVRC 2012 and Places365 datasets."
1404,SP:a9aa11e7ee77d9f6957266e4ad822c7dc0f82354,"generalization error EVALUATE-FOR deep neural networks ( DNNs ). validation accuracy EVALUATE-FOR model selection. gradient norm USED-FOR model selection criterion. generalization error CONJUNCTION gradient norm measures. gradient norm measures CONJUNCTION generalization error. efficiency CONJUNCTION effectiveness. effectiveness CONJUNCTION efficiency. approximated gradient norm USED-FOR models. approximated gradient norm USED-FOR hyper - parameter search objectives. generalization error EVALUATE-FOR models. gradient norm CONJUNCTION generalization error. generalization error CONJUNCTION gradient norm. BOHB HYPONYM-OF bandit - based or population - based algorithms. gradient norm USED-FOR models. gradient norm USED-FOR generalization. generalization EVALUATE-FOR models. gradient norm COMPARE algorithms. algorithms COMPARE gradient norm. models COMPARE algorithms. algorithms COMPARE models. architectures USED-FOR models. Method are neural network architectures, hyper - parameter optimization, and DNNs. Metric are gradient complexity, computation cost, and computation overhead. OtherScientificTerm are loss gradient, and gradient norm objectives. ","This paper studies the generalization error of deep neural networks (DNNs) under the hyper-parameter optimization setting. The authors propose a new objective called BOHB, which is based on bandit-based or population-based algorithms. The main contribution of the paper is that the authors propose to use the gradient norm as a criterion for model selection in the hyperparameter search setting. They show that the proposed objective can be used to improve the performance of DNNs in terms of generalization, efficiency, and computation overhead. ","This paper studies the generalization error of deep neural networks (DNNs) under the hyper-parameter optimization setting. The authors propose a new objective called BOHB, which is based on bandit-based or population-based algorithms. The main contribution of the paper is that the authors propose to use the gradient norm as a criterion for model selection in the hyperparameter search setting. They show that the proposed objective can be used to improve the performance of DNNs in terms of generalization, efficiency, and computation overhead. "
1413,SP:13359456defb953dd2d19e1f879100ce392d6be6,"Wikipedia HYPONYM-OF Encyclopedias. entity linking CONJUNCTION open - domain question answering. open - domain question answering CONJUNCTION entity linking. open - domain question answering HYPONYM-OF knowledge - intensive tasks. entity linking HYPONYM-OF knowledge - intensive tasks. weight vectors USED-FOR entity representations. entity meta information USED-FOR entity representations. memory footprint USED-FOR dense representations. vector dot product USED-FOR entity affinity. GENRE HYPONYM-OF system. context CONJUNCTION entity name. entity name CONJUNCTION context. vocabulary size COMPARE entity count. entity count COMPARE vocabulary size. datasets USED-FOR entity disambiguation. datasets EVALUATE-FOR approach. Generic is approaches. Method are classifiers, autoregressive formulation, and encoder - decoder architecture. OtherScientificTerm are missing fine - grained interactions, softmax loss, and entities. Material is negative data. ","This paper proposes an autoregressive framework for entity disambiguation. The key idea is to learn a vector dot product for each entity, which is then used to learn the entity affinity. The proposed method is evaluated on a number of datasets and shows promising results. ","This paper proposes an autoregressive framework for entity disambiguation. The key idea is to learn a vector dot product for each entity, which is then used to learn the entity affinity. The proposed method is evaluated on a number of datasets and shows promising results. "
1422,SP:9dfb808ce4c045c45436b35ceb03bc6fe6ed9745,"infinite time horizon FEATURE-OF unknown congestion functions. fe CONJUNCTION congestion function. congestion function CONJUNCTION fe. observation USED-FOR routing decisions. algorithm USED-FOR ce. algorithm USED-FOR routing decisions. observation USED-FOR algorithm. total cost CONJUNCTION minimum cost. minimum cost CONJUNCTION total cost. cumulative regret FEATURE-OF algorithm. space complexity CONJUNCTION time complexity. time complexity CONJUNCTION space complexity. time complexity EVALUATE-FOR algorithm. space complexity EVALUATE-FOR algorithm. Task is routing users. OtherScientificTerm are unknown distribution, routing requests, and regret. Material is New York City road networks. ","This paper studies the problem of routing users in an infinite time horizon with unknown congestion functions. In particular, the authors consider the case where there is an unknown congestion function and the users have a finite time horizon. The authors propose a new algorithm to solve this problem. The proposed algorithm is based on the idea that the congestion function can be decomposed into two parts: (1) the total cost and (2) the minimum cost, which is a function of the number of users and the time complexity of the network. The paper shows that the proposed algorithm achieves a regret bound of $O(\sqrt{T})$ for all users and $T$ for each user. ","This paper studies the problem of routing users in an infinite time horizon with unknown congestion functions. In particular, the authors consider the case where there is an unknown congestion function and the users have a finite time horizon. The authors propose a new algorithm to solve this problem. The proposed algorithm is based on the idea that the congestion function can be decomposed into two parts: (1) the total cost and (2) the minimum cost, which is a function of the number of users and the time complexity of the network. The paper shows that the proposed algorithm achieves a regret bound of $O(\sqrt{T})$ for all users and $T$ for each user. "
1431,SP:580ac3b74951bef5d5772e4471b01a805ff3dd68,"BERT HYPONYM-OF Masked Language Models ( MLMs ). uniform masking USED-FOR MLM. PMI - Masking HYPONYM-OF masking strategy. Pointwise Mutual Information ( PMI ) USED-FOR masking strategy. entity / phrase masking CONJUNCTION random - span masking. random - span masking CONJUNCTION entity / phrase masking. whole - word masking CONJUNCTION entity / phrase masking. entity / phrase masking CONJUNCTION whole - word masking. PMIMasking COMPARE prior more heuristic approaches. prior more heuristic approaches COMPARE PMIMasking. entity / phrase masking HYPONYM-OF prior more heuristic approaches. whole - word masking HYPONYM-OF prior more heuristic approaches. random - span masking HYPONYM-OF random uniform token masking. entity / phrase masking HYPONYM-OF random uniform token masking. whole - word masking HYPONYM-OF random uniform token masking. PMI - Masking COMPARE prior masking approaches. prior masking approaches COMPARE PMI - Masking. OtherScientificTerm are shallow local signals, token n - gram, and collocation. Task is pretraining inefficiency. ","This paper proposes a new masking strategy for BERT based on Pointwise Mutual Information (PMI) for masking. PMI-Masking is an extension of BERT, which is a masking method for masked language models (MLMs). The authors propose to use PMI to improve the efficiency of masking in BERT. The authors show that PMI masking outperforms other masking methods in terms of accuracy and training time. They also show that the proposed method is more efficient than the baselines.","This paper proposes a new masking strategy for BERT based on Pointwise Mutual Information (PMI) for masking. PMI-Masking is an extension of BERT, which is a masking method for masked language models (MLMs). The authors propose to use PMI to improve the efficiency of masking in BERT. The authors show that PMI masking outperforms other masking methods in terms of accuracy and training time. They also show that the proposed method is more efficient than the baselines."
1440,SP:038cdd2df643edccb16dfd72e6eb123f6a6c0839,"Amortised inference USED-FOR sequential latent - variable models ( LVMs ). Bayesian filter HYPONYM-OF mixture of smoothing posteriors. ELBO objective USED-FOR partially - conditioned amortised posteriors. partially - conditioned amortised posteriors USED-FOR products of smoothing posteriors. traffic flow CONJUNCTION handwritten digits. handwritten digits CONJUNCTION traffic flow. handwritten digits CONJUNCTION aerial vehicle dynamics. aerial vehicle dynamics CONJUNCTION handwritten digits. aerial vehicle dynamics HYPONYM-OF scenarios. traffic flow HYPONYM-OF scenarios. handwritten digits HYPONYM-OF scenarios. generative modelling CONJUNCTION multi - step prediction. multi - step prediction CONJUNCTION generative modelling. fully - conditioned approximate posteriors USED-FOR generative modelling. OtherScientificTerm are evidence lower bound ( ELBO ), variational posteriors, posteriors, and approximate posteriors. Generic is setting. Method is generative model. ","This paper studies the problem of amortised inference for sequential latent variable models (LVMs). The authors propose a new objective called evidence lower bound (ELBO) for partially-conditioned posteriors, which is a mixture of smoothing posteriors and variational posteriors. The authors show that the ELBO objective can be used to improve the performance of partially-conditional amortized posteriors in a variety of settings, including generative modelling, multi-step prediction, and multi-modal learning. ","This paper studies the problem of amortised inference for sequential latent variable models (LVMs). The authors propose a new objective called evidence lower bound (ELBO) for partially-conditioned posteriors, which is a mixture of smoothing posteriors and variational posteriors. The authors show that the ELBO objective can be used to improve the performance of partially-conditional amortized posteriors in a variety of settings, including generative modelling, multi-step prediction, and multi-modal learning. "
1449,SP:f2574c0d6cdec78389fa1301d6a10976d1756279,"statistical properties FEATURE-OF distributed kernel ridge regression. distributed kernel ridge regression CONJUNCTION random features ( DKRR - RF ). random features ( DKRR - RF ) CONJUNCTION distributed kernel ridge regression. statistical properties FEATURE-OF random features ( DKRR - RF ). divide - and - conquer technique CONJUNCTION random features. random features CONJUNCTION divide - and - conquer technique. O(|D| ) memory CONJUNCTION O(|D| ) time. O(|D| ) time CONJUNCTION O(|D| ) memory. statistical accuracy EVALUATE-FOR KRR. random features USED-FOR KRR. divide - and - conquer technique USED-FOR KRR. O(|D| ) time USED-FOR KRR. O(|D| ) memory USED-FOR KRR. communication strategy USED-FOR DKRR - RF. OtherScientificTerm are optimal generalization bounds, generalization bounds, and average information. Generic is theoretical bounds. ","This paper studies the generalization properties of distributed kernel ridge regression (DKRR) and random features (RD). In particular, the authors consider the case of DKRR-RF, which is an extension of the DKRR framework. The authors show that under certain assumptions, KRR and RF are optimal generalization bounds. They also show that O(|D|) memory and O(O(D|D) time are the optimal bounds for KRR. ","This paper studies the generalization properties of distributed kernel ridge regression (DKRR) and random features (RD). In particular, the authors consider the case of DKRR-RF, which is an extension of the DKRR framework. The authors show that under certain assumptions, KRR and RF are optimal generalization bounds. They also show that O(|D|) memory and O(O(D|D) time are the optimal bounds for KRR. "
1458,SP:129872706a12d89f0886c2ad0fd4083d0632343c,"search step USED-FOR architectures. validation performance EVALUATE-FOR architectures. accuracy EVALUATE-FOR weight - sharing architectures. RandomNAS USED-FOR architectures. global search space(GS ) FEATURE-OF architectures. top - performing architectures PART-OF GS. proxy search space ( PS ) USED-FOR RandomNAS. EPS HYPONYM-OF Proxy Search Space. EPS HYPONYM-OF RandomNAS - based approach. EPS COMPARE state - of - the - art. state - of - the - art COMPARE EPS. NASBench-201 EVALUATE-FOR EPS. image classification CONJUNCTION natural language processing. natural language processing CONJUNCTION image classification. DARTS - like search spaces USED-FOR tasks. EPS USED-FOR tasks. search time EVALUATE-FOR EPS. natural language processing HYPONYM-OF tasks. image classification HYPONYM-OF tasks. Method is NAS approach. Metric are achievable accuracy, and RandomNAS ’s search efficiency. OtherScientificTerm are NAS search space, and ground - truth ranking. ","This paper proposes a proxy search space (PS) based on RandomNAS to improve the search efficiency of weight-sharing architectures. The proposed method is based on the notion of global search space, which is an extension of the DARTS-like search space. The authors show that the proposed method outperforms the state-of-the-art in terms of search efficiency and validation performance on NASBench-201. ","This paper proposes a proxy search space (PS) based on RandomNAS to improve the search efficiency of weight-sharing architectures. The proposed method is based on the notion of global search space, which is an extension of the DARTS-like search space. The authors show that the proposed method outperforms the state-of-the-art in terms of search efficiency and validation performance on NASBench-201. "
1467,SP:27701f374d0b7e8b269d9133d6c3a10bca03b548,"it CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION it. imitation learning CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION imitation learning. imitation learning CONJUNCTION meta reinforcement learning. meta reinforcement learning CONJUNCTION imitation learning. imitation learning USED-FOR method. Probabilistic Embeddings USED-FOR method. PERIL USED-FOR exploration policies. Dual inference strategies USED-FOR PERIL. imitation learning COMPARE approach. approach COMPARE imitation learning. uncertainties FEATURE-OF it. it EVALUATE-FOR approach. meta - RL USED-FOR PERIL. sparse rewards FEATURE-OF meta - RL benchmarks. Method are Imitation learning, and meta reinforcement learning ( meta - RL ). Task is exploration. Material is interaction data. Metric is adaptation rates. ","This paper proposes a meta-RL method called PERIL. PERIL is based on the idea of Probabilistic Embeddings (PERIL), which is a meta RL method that combines imitation learning and reinforcement learning. The authors show that PERIL outperforms the state-of-the-art in terms of exploration performance on a number of tasks. The main contribution of the paper is that the authors propose a new method for meta-learning that combines the benefits of imitation learning with reinforcement learning and meta RL. PerIL is evaluated on a variety of tasks and shows that it outperforms existing methods.","This paper proposes a meta-RL method called PERIL. PERIL is based on the idea of Probabilistic Embeddings (PERIL), which is a meta RL method that combines imitation learning and reinforcement learning. The authors show that PERIL outperforms the state-of-the-art in terms of exploration performance on a number of tasks. The main contribution of the paper is that the authors propose a new method for meta-learning that combines the benefits of imitation learning with reinforcement learning and meta RL. PerIL is evaluated on a variety of tasks and shows that it outperforms existing methods."
1476,SP:118758f563fa6e9e46d52a6f250005c06cf2f19f,"learning method USED-FOR image classification. overparameterized convolutional neural networks USED-FOR image classification. overparameterized convolutional neural networks HYPONYM-OF learning method. overparameterized convolutional neural networks CONJUNCTION gradient based optimization. gradient based optimization CONJUNCTION overparameterized convolutional neural networks. 3 - layer overparameterized convolutional network CONJUNCTION stochastic gradient descent ( SGD ). stochastic gradient descent ( SGD ) CONJUNCTION 3 - layer overparameterized convolutional network. orthogonal patches PART-OF images. 3 - layer overparameterized convolutional network USED-FOR images. pattern detectors CONJUNCTION detected patterns. detected patterns CONJUNCTION pattern detectors. SGD USED-FOR setting. pattern statistics USED-FOR dot - product. learning algorithm USED-FOR PSI. learning algorithm USED-FOR setting. sample complexity EVALUATE-FOR learning algorithm. overparameterized CNNs USED-FOR MNIST. non - orthogonal patches FEATURE-OF MNIST. non - orthogonal patches USED-FOR overparameterized CNNs. Task is image classification task. OtherScientificTerm are Pattern Statistics Inductive Bias ( PSI ), filter dimension, and VC dimension lower bound. Generic is it. ","This paper proposes a new learning algorithm for overparameterized convolutional neural networks (CNNs) for image classification. The proposed method is based on the notion of pattern statistics inductive bias (PSI), which is a regularization term for the dot product of the input and the output of the classifier. The authors show that PSI can be used to improve the sample complexity of overparametrized CNNs in the presence of non-orthogonal patches. They also show that the PSI lower bound is lower bounded by the VC dimension lower bound. ","This paper proposes a new learning algorithm for overparameterized convolutional neural networks (CNNs) for image classification. The proposed method is based on the notion of pattern statistics inductive bias (PSI), which is a regularization term for the dot product of the input and the output of the classifier. The authors show that PSI can be used to improve the sample complexity of overparametrized CNNs in the presence of non-orthogonal patches. They also show that the PSI lower bound is lower bounded by the VC dimension lower bound. "
1485,SP:a051b615da3a99562d2cd2dfbec5cd78af98d9b4,approach USED-FOR representation learning. Contrastive learning USED-FOR representation learning. contrastive learning USED-FOR representation of documents. topic modeling assumptions FEATURE-OF document classification. topic posterior information FEATURE-OF representation of documents. procedure USED-FOR semi - supervised setup. linear classifiers USED-FOR document classification tasks. representations USED-FOR linear classifiers. OtherScientificTerm is embeddings of data. Method is linear models. ,This paper proposes a method for semi-supervised document classification based on contrastive learning. The method is based on the topic modeling assumption that the topic posterior information of documents can be used to learn the representation of documents. The authors show that the proposed method is able to achieve better performance than the state-of-the-art on a variety of document classification tasks. ,This paper proposes a method for semi-supervised document classification based on contrastive learning. The method is based on the topic modeling assumption that the topic posterior information of documents can be used to learn the representation of documents. The authors show that the proposed method is able to achieve better performance than the state-of-the-art on a variety of document classification tasks. 
1494,SP:8d011d4a77ced1f8cd849181d5293420f161ffd3,"vision CONJUNCTION language. language CONJUNCTION vision. Multimodal learning USED-FOR generative models. it USED-FOR generalisable representations. related ” multimodal data USED-FOR models. contrastive framework USED-FOR generative model learning. contrastive framework USED-FOR model. method USED-FOR multimodal learning. framework USED-FOR generative model. Task is learning generalisable representations. Method is multimodal variational autoencoder ( VAE ) models. Material is unlabeled, unpaired multimodal data. ",This paper proposes a contrastive framework for multimodal generative models. The proposed method is based on the contrastive learning framework. The authors show that the proposed method can be applied to both unlabeled and unpaired data.,This paper proposes a contrastive framework for multimodal generative models. The proposed method is based on the contrastive learning framework. The authors show that the proposed method can be applied to both unlabeled and unpaired data.
1503,SP:3a0d3f1d63cd57b0613c40176e694435ed3eee50,"Variational autoencoders ( VAEs ) HYPONYM-OF likelihood - based generative models. base prior distribution CONJUNCTION reweighting factor. reweighting factor CONJUNCTION base prior distribution. reweighting factor USED-FOR energy - based prior. base prior distribution USED-FOR energy - based prior. it USED-FOR hierarchical VAEs. latent variable groups FEATURE-OF hierarchical VAEs. noise contrastive estimation USED-FOR reweighting factor. CIFAR-10 CONJUNCTION CelebA 64. CelebA 64 CONJUNCTION CIFAR-10. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. CelebA 64 CONJUNCTION CelebA HQ 256 datasets. CelebA HQ 256 datasets CONJUNCTION CelebA 64. generative EVALUATE-FOR VAEs. noise contrastive priors USED-FOR VAEs. noise contrastive priors USED-FOR generative. MNIST EVALUATE-FOR VAEs. Generic is they. OtherScientificTerm are prior, tempering, prior hole problem, prior distribution, aggregate approximate posterior, latent space, and aggregate posterior. ",This paper studies the prior of variational autoencoders (VAEs). The authors propose a reweighting factor for the energy-based prior and a noise contrastive prior for the base prior distribution. They show that the reweighted prior can be used to improve the generative performance of hierarchical VAEs. The authors also show that their approach can be applied to the prior hole problem.,This paper studies the prior of variational autoencoders (VAEs). The authors propose a reweighting factor for the energy-based prior and a noise contrastive prior for the base prior distribution. They show that the reweighted prior can be used to improve the generative performance of hierarchical VAEs. The authors also show that their approach can be applied to the prior hole problem.
1512,SP:86b813ac0f5211a7c45884451f59f3ebaeeb4b83,"convex regularizers USED-FOR learner ’s policy. degenerate solutions HYPONYM-OF constant rewards. convex regularizers USED-FOR Regularized IRL. constant rewards USED-FOR expert ’s behavior. practical methods USED-FOR them. them USED-FOR regularized IRL. practical methods USED-FOR regularized IRL. tractable solutions CONJUNCTION practical methods. practical methods CONJUNCTION tractable solutions. maximum - entropy IRL framework USED-FOR methods. Shannon - entropy regularizers USED-FOR them. theoretical backing USED-FOR IRL method. IRL method USED-FOR discrete and continuous controls. Method is Inverse Reinforcement Learning ( IRL ). OtherScientificTerm are expert behavior, and reward functions. Generic are solutions, and tasks. ","This paper studies the problem of inverse reinforcement learning (IRL), where the goal is to learn a policy that maximizes the entropy of the reward function of the expert. The authors propose a theoretical framework for this problem, which is based on convex regularizers. They show that the optimal solution of this problem can be obtained by maximizing the maximum entropy of an expert's reward function. They also provide theoretical backing for the proposed method. ","This paper studies the problem of inverse reinforcement learning (IRL), where the goal is to learn a policy that maximizes the entropy of the reward function of the expert. The authors propose a theoretical framework for this problem, which is based on convex regularizers. They show that the optimal solution of this problem can be obtained by maximizing the maximum entropy of an expert's reward function. They also provide theoretical backing for the proposed method. "
1521,SP:6f4a520cdc9901c2c87a7e887ce2535ad0b36f69,"hard binary gates USED-FOR LS or shared paths. CLSR USED-FOR LS or shared paths. hard binary gates USED-FOR token representations. hard binary gates USED-FOR CLSR. translation signals CONJUNCTION budget constraints. budget constraints CONJUNCTION translation signals. LS capacity PART-OF MNMT. translation signals USED-FOR MNMT. CLSR COMPARE many - to - one translation. many - to - one translation COMPARE CLSR. one - to - many translation COMPARE many - to - one translation. many - to - one translation COMPARE one - to - many translation. LS computation USED-FOR top and/or bottom encoder / decoder layers. unbalanced training data USED-FOR many - to - one translation. LS modeling USED-FOR MNMT. OPUS-100 and WMT datasets EVALUATE-FOR Transformer. CLSR USED-FOR one - to - many translation. shared capacity CONJUNCTION LS capacity. LS capacity CONJUNCTION shared capacity. LS capacity USED-FOR multilingual translation. Task is multilingual neural machine translation ( MNMT ). Method are conditional language - specific routing ( CLSR ), and multilingual Transformers. Generic is gates. ",This paper proposes a new method for multilingual neural machine translation (MNMT) based on conditional language-specific routing (CLSR). CLSR is an extension of the hard binary gates (hard binary gates) used in many-to-many translation. CLSR can be seen as a special case of conditional language specific routing (LSR) and can be used for both shared and top-and-bottom encoder/decoder layers. The authors show that CLSR improves the performance of multilingual MNMT on OPUS-100 and WMT datasets. ,This paper proposes a new method for multilingual neural machine translation (MNMT) based on conditional language-specific routing (CLSR). CLSR is an extension of the hard binary gates (hard binary gates) used in many-to-many translation. CLSR can be seen as a special case of conditional language specific routing (LSR) and can be used for both shared and top-and-bottom encoder/decoder layers. The authors show that CLSR improves the performance of multilingual MNMT on OPUS-100 and WMT datasets. 
1530,SP:1e932b21e9557b1bbc1950c4e1701f5a3ecf50df,"Wasserstein distributional normalization ( WDN ) algorithm USED-FOR noisy labels. Wasserstein distributional normalization ( WDN ) algorithm USED-FOR accurate classification. noisy labels USED-FOR accurate classification. geometric constraints FEATURE-OF uncertain samples. Wasserstein ball USED-FOR them. WDN COMPARE state - of - the - art methods. state - of - the - art methods COMPARE WDN. Clothing1 M and CIFAR-10/100 datasets EVALUATE-FOR state - of - the - art methods. Clothing1 M and CIFAR-10/100 datasets EVALUATE-FOR WDN. WDN COMPARE classification methods. classification methods COMPARE WDN. accuracy EVALUATE-FOR it. accuracy EVALUATE-FOR WDN. OtherScientificTerm are small loss criteria, geometric relationship, and diverse noisy labels. Generic is relation. ",This paper proposes a Wasserstein distributional normalization (WDN) algorithm for noisy labels. The WDN algorithm is based on the idea that the WSD can be used to reduce the geometric relationship between noisy labels and uncertain samples. The authors show that WDN can be applied to a wide range of noisy labels (e.g. Clothing1M and CIFAR-10/100). The authors also show that the proposed algorithm is able to achieve better classification performance than existing methods. ,This paper proposes a Wasserstein distributional normalization (WDN) algorithm for noisy labels. The WDN algorithm is based on the idea that the WSD can be used to reduce the geometric relationship between noisy labels and uncertain samples. The authors show that WDN can be applied to a wide range of noisy labels (e.g. Clothing1M and CIFAR-10/100). The authors also show that the proposed algorithm is able to achieve better classification performance than existing methods. 
1539,SP:e0029422e28c250dfb8c62c29a15b375030069e8,predictive accuracy EVALUATE-FOR Convolutional image classifiers. uncertainty quantification techniques USED-FOR probability estimates. uncertainty quantification techniques USED-FOR network. network USED-FOR probability estimates. Platt scaling HYPONYM-OF uncertainty quantification techniques. algorithm USED-FOR predictive set. algorithm USED-FOR classifier. user - specified probability FEATURE-OF predictive set. algorithm COMPARE Platt scaling. Platt scaling COMPARE algorithm. formal finite - sample coverage guarantee FEATURE-OF model. formal finite - sample coverage guarantee FEATURE-OF algorithm. conformal prediction algorithm USED-FOR method. scheme COMPARE approaches. approaches COMPARE scheme. scheme COMPARE stand - alone Platt scaling baseline. stand - alone Platt scaling baseline COMPARE scheme. Imagenet - V2 EVALUATE-FOR scheme. Imagenet CONJUNCTION Imagenet - V2. Imagenet - V2 CONJUNCTION Imagenet. Imagenet EVALUATE-FOR scheme. ResNet-152 CONJUNCTION classifiers. classifiers CONJUNCTION ResNet-152. classifiers EVALUATE-FOR scheme. Imagenet - V2 EVALUATE-FOR approaches. Imagenet - V2 CONJUNCTION ResNet-152. ResNet-152 CONJUNCTION Imagenet - V2. ResNet-152 EVALUATE-FOR scheme. Imagenet - V2 CONJUNCTION classifiers. classifiers CONJUNCTION Imagenet - V2. coverage EVALUATE-FOR approaches. coverage EVALUATE-FOR scheme. OtherScientificTerm is formal guarantees. ,"This paper proposes a new method to improve the performance of Convolutional Neural Networks (CNNs). The proposed method is based on Platt scaling, which is a popular method for improving the coverage of the predictive set of a classifier. The authors propose to use a conformal prediction algorithm to learn a set of probability estimates for each classifier, which are then used to train a new classifier that is trained on the new set of probabilities. The method is evaluated on Imagenet-V2 and ResNet-152, and shows that the proposed method outperforms existing methods.","This paper proposes a new method to improve the performance of Convolutional Neural Networks (CNNs). The proposed method is based on Platt scaling, which is a popular method for improving the coverage of the predictive set of a classifier. The authors propose to use a conformal prediction algorithm to learn a set of probability estimates for each classifier, which are then used to train a new classifier that is trained on the new set of probabilities. The method is evaluated on Imagenet-V2 and ResNet-152, and shows that the proposed method outperforms existing methods."
1548,SP:bf93641cbeaaa147ad0307de694e20adc23c290a,"algorithm USED-FOR Wasserstein-2 barycenters. input convex neural networks CONJUNCTION cycle - consistency regularization. cycle - consistency regularization CONJUNCTION input convex neural networks. entropic or quadratic regularization USED-FOR approaches. minimax optimization USED-FOR approach. low - dimensional qualitative scenarios CONJUNCTION high - dimensional quantitative experiments. high - dimensional quantitative experiments CONJUNCTION low - dimensional qualitative scenarios. high - dimensional quantitative experiments EVALUATE-FOR approach. low - dimensional qualitative scenarios EVALUATE-FOR approach. OtherScientificTerm are Wasserstein barycenters, and error bounds. Method is optimal transport. ","This paper studies the problem of optimal transport for Wasserstein-2 barycenters, which is an important problem in machine learning. The main contribution of this paper is to propose a new algorithm to solve the problem. The proposed algorithm is based on the minimax optimization of the optimal transport problem, which can be viewed as a variant of the regularization problem in the context of convex neural networks and cycle-consistency regularization. The authors show that the proposed algorithm can be used to solve this problem in a variety of settings, including low-dimensional and high-dimensional settings. They also show that their algorithm is able to achieve better error bounds than existing methods.","This paper studies the problem of optimal transport for Wasserstein-2 barycenters, which is an important problem in machine learning. The main contribution of this paper is to propose a new algorithm to solve the problem. The proposed algorithm is based on the minimax optimization of the optimal transport problem, which can be viewed as a variant of the regularization problem in the context of convex neural networks and cycle-consistency regularization. The authors show that the proposed algorithm can be used to solve this problem in a variety of settings, including low-dimensional and high-dimensional settings. They also show that their algorithm is able to achieve better error bounds than existing methods."
1557,SP:39aae6a094f7141bee6d4fa78be03fd20cf12b13,"multiple manifold problem HYPONYM-OF binary classification task. deep fully - connected neural network USED-FOR multiple manifold problem. randomly - initialized gradient descent USED-FOR manifolds. randomlyinitialized network CONJUNCTION gradients. gradients CONJUNCTION randomlyinitialized network. nonasymptotic framework USED-FOR generalization of networks. neural tangent kernel PART-OF deep fullyconnected ReLU networks. NTK regime FEATURE-OF generalization of networks. structured data USED-FOR generalization of networks. martingale concentration USED-FOR statistical dependencies. approach USED-FOR network architectures. Task are machine vision, and practically - motivated model problem. OtherScientificTerm are manifold configuration, network depth L, geometric and statistical properties of the data, network width n, i.i.d. samples, depth, fitting resource, class manifolds, width, and statistical resource. Method are nonasymptotic analysis of training overparameterized neural networks, and random network. ","This paper proposes a nonasymptotic analysis of training overparameterized neural networks in the NTK regime. The main contribution of the paper is to show that the generalization performance of deep fully-connected ReLU networks is bounded by the number of samples in the training set. The authors also provide a theoretical analysis of the martingale concentration of the network depth L, which is a measure of the statistical dependence of the data and the network width n. ","This paper proposes a nonasymptotic analysis of training overparameterized neural networks in the NTK regime. The main contribution of the paper is to show that the generalization performance of deep fully-connected ReLU networks is bounded by the number of samples in the training set. The authors also provide a theoretical analysis of the martingale concentration of the network depth L, which is a measure of the statistical dependence of the data and the network width n. "
1566,SP:c5afd0a7485aa8dc732f6fa90d81a85a8bb51b3c,"supervised learning methods USED-FOR subroutines. subroutines USED-FOR reinforcement learning algorithm. supervised learning methods USED-FOR reinforcement learning algorithm. weighted target actions USED-FOR policy. supervised learning steps PART-OF approach. one HYPONYM-OF supervised learning steps. supervised learning methods USED-FOR method. experience replay USED-FOR off - policy data. it COMPARE RL algorithms. RL algorithms COMPARE it. AWR COMPARE RL algorithms. RL algorithms COMPARE AWR. OpenAI Gym benchmark tasks EVALUATE-FOR AWR. AWR USED-FOR policies. AWR COMPARE off - policy algorithms. off - policy algorithms COMPARE AWR. environmental interactions FEATURE-OF static datasets. off - policy algorithms USED-FOR policies. complex simulated characters FEATURE-OF continuous control tasks. continuous control tasks EVALUATE-FOR algorithm. Method is advantage - weighted regression ( AWR ). OtherScientificTerm are value function, and continuous and discrete actions. ","This paper proposes a reinforcement learning algorithm for continuous control tasks. The proposed method is based on advantage-weighted regression (AWR). The main idea is to learn a set of subroutines that can be used to train an off-policy policy, and then use the learned subroutine to train a policy that maximizes the performance of the off policy policy. The method is evaluated on a number of continuous control benchmarks, and is shown to outperform the baselines. ","This paper proposes a reinforcement learning algorithm for continuous control tasks. The proposed method is based on advantage-weighted regression (AWR). The main idea is to learn a set of subroutines that can be used to train an off-policy policy, and then use the learned subroutine to train a policy that maximizes the performance of the off policy policy. The method is evaluated on a number of continuous control benchmarks, and is shown to outperform the baselines. "
1575,SP:54da307c1f9aac020ae7e3c439653765dbd8b3fe,Heterogeneous assignment of bitwidths FEATURE-OF layers. parametrized sinusoidal regularizer USED-FOR WaveQ. WaveQ USED-FOR quantized weights. training USED-FOR stochastic gradient descent. sinusoidal regularizer USED-FOR stochastic gradient descent. quantized weights CONJUNCTION heterogeneous bitwidths. heterogeneous bitwidths CONJUNCTION quantized weights. WaveQ HYPONYM-OF gradient - based mechanism. gradient - based mechanism USED-FOR quantized weights. gradient - based mechanism USED-FOR heterogeneous bitwidths. ResNet-20 CONJUNCTION SVHN. SVHN CONJUNCTION ResNet-20. ResNet-18 CONJUNCTION ResNet-20. ResNet-20 CONJUNCTION ResNet-18. MobileNet CONJUNCTION ResNet-18. ResNet-18 CONJUNCTION MobileNet. heterogeneous bitwidth assignment USED-FOR quantization. CIFAR10 CONJUNCTION MobileNet. MobileNet CONJUNCTION CIFAR10. AlexNet CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION AlexNet. SVHN CONJUNCTION VGG-11. VGG-11 CONJUNCTION SVHN. compute efficiency CONJUNCTION accuracy. accuracy CONJUNCTION compute efficiency. WaveQ USED-FOR heterogeneous bitwidth assignment. heterogeneous bitwidth assignment USED-FOR deep networks. WaveQ USED-FOR deep networks. accuracy EVALUATE-FOR WaveQ. compute efficiency EVALUATE-FOR WaveQ. VGG-11 HYPONYM-OF deep networks. AlexNet HYPONYM-OF deep networks. SVHN HYPONYM-OF deep networks. MobileNet HYPONYM-OF deep networks. ResNet-20 HYPONYM-OF deep networks. CIFAR10 HYPONYM-OF deep networks. ResNet-18 HYPONYM-OF deep networks. predetermined bitwidths USED-FOR WaveQ. DoReFa CONJUNCTION WRPN. WRPN CONJUNCTION DoReFa. accuracy EVALUATE-FOR quantized training algorithms. quantized training algorithms EVALUATE-FOR,"This paper proposes WaveQ, a method for quantized training for heterogeneous bitwidths. WaveQ is a parametrized sinusoidal regularizer for stochastic gradient descent. The authors show that WaveQ can be applied to both quantized weights and heterogeneous weights. They also show that the method is computationally efficient and can be used to train deep networks.","This paper proposes WaveQ, a method for quantized training for heterogeneous bitwidths. WaveQ is a parametrized sinusoidal regularizer for stochastic gradient descent. The authors show that WaveQ can be applied to both quantized weights and heterogeneous weights. They also show that the method is computationally efficient and can be used to train deep networks."
1584,SP:84ced6627d1dc3e78c9ffc726174e76db5f77795,"data augmentation methods USED-FOR translation. monolingual data USED-FOR data augmentation methods. data augmentation methods USED-FOR neural machine translation ( NMT ). in - domain monolingual data USED-FOR it. backtranslation HYPONYM-OF data augmentation methods. data augmentation method USED-FOR neural machine translation. small and large scale datasets EVALUATE-FOR method. method COMPARE baseline models. baseline models COMPARE method. small and large scale datasets EVALUATE-FOR baseline models. Method is neural machine translation models. OtherScientificTerm are aligned word pairs, and bilingual embeddings. ","This paper proposes a data augmentation method for neural machine translation (NMT). The proposed method is based on backtranslation, which augments monolingual data with bilingual embeddings to improve the performance of NMT models. The method is evaluated on both small and large scale datasets and is shown to outperform baseline models.","This paper proposes a data augmentation method for neural machine translation (NMT). The proposed method is based on backtranslation, which augments monolingual data with bilingual embeddings to improve the performance of NMT models. The method is evaluated on both small and large scale datasets and is shown to outperform baseline models."
1593,SP:c1890bcafac6ac8fd5a3d2ff2dd1c37b71865a5a,"Federated learning USED-FOR distributed data privacy. data quantity CONJUNCTION data quality. data quality CONJUNCTION data quantity. Shapley Value PART-OF game theory. Shapley Value COMPARE method. method COMPARE Shapley Value. maintaining real - time EVALUATE-FOR method. data quality EVALUATE-FOR method. data quantity EVALUATE-FOR method. Method are contribution measurement mechanism, real - time contribution measurement method, and pseudo - distributed training. Generic is mechanism. OtherScientificTerm is contribution rate. Material is Penn Treebank dataset. ","This paper proposes a new contribution measurement method for federated learning. The contribution measurement mechanism is based on Shapley Value, which is an extension of game theory. The authors show that the proposed method can be applied to both real-time and pseudo-distribution settings. The proposed method is evaluated on the Penn Treebank dataset.","This paper proposes a new contribution measurement method for federated learning. The contribution measurement mechanism is based on Shapley Value, which is an extension of game theory. The authors show that the proposed method can be applied to both real-time and pseudo-distribution settings. The proposed method is evaluated on the Penn Treebank dataset."
1602,SP:b766979b4d3b15a039db4e5eebd8353521aea4bb,"graph structure FEATURE-OF fully - observable case. nearlylinear time algorithm USED-FOR problem. dimension - independent error guarantee FEATURE-OF nearlylinear time algorithm. error guarantees EVALUATE-FOR robust algorithms. robust learning of Bayesian networks CONJUNCTION robust mean estimation. robust mean estimation CONJUNCTION robust learning of Bayesian networks. Task is learning Bayesian networks. Method are Bayesian networks, Bayesian network, and robust mean estimation algorithm. Generic is algorithm. ","This paper studies the problem of robust learning of Bayesian networks. The authors propose a nearly-linear time algorithm for learning Bayesian neural networks with robust mean estimation and robust mean learning. The proposed algorithm is based on the notion of dimension-independent error, which is a generalization of the previous work on the problem. The main contribution of the paper is to show that the proposed algorithm can be used to solve the robust learning problem in the fully-observable case. ","This paper studies the problem of robust learning of Bayesian networks. The authors propose a nearly-linear time algorithm for learning Bayesian neural networks with robust mean estimation and robust mean learning. The proposed algorithm is based on the notion of dimension-independent error, which is a generalization of the previous work on the problem. The main contribution of the paper is to show that the proposed algorithm can be used to solve the robust learning problem in the fully-observable case. "
1611,SP:b3d507bd8fe8876f3a4f7696bc0483d0052484c8,"raw high - dimensional observations USED-FOR autonomous agents. images HYPONYM-OF raw high - dimensional observations. shaped reward functions USED-FOR model - based reinforcement learning ( RL ). short - horizon reasoning USED-FOR shaped reward functions. trajectory optimization USED-FOR long - horizon reasoning. it USED-FOR image - based setting. probabilistic latent variable models USED-FOR algorithm. probabilistic latent variable models USED-FOR it. approach USED-FOR longer - horizon visual planning. latent collocation method ( LatCo ) USED-FOR approach. latent collocation method ( LatCo ) USED-FOR longer - horizon visual planning. approach COMPARE prior model - based approaches. prior model - based approaches COMPARE approach. sparse rewards CONJUNCTION long - term goals. long - term goals CONJUNCTION sparse rewards. sparse rewards FEATURE-OF visual control tasks. long - term goals FEATURE-OF visual control tasks. visual control tasks EVALUATE-FOR prior model - based approaches. visual control tasks EVALUATE-FOR approach. Method are temporally extended reasoning, myopic, short - sighted planning, and collocation - based planning. OtherScientificTerm is latent variables. ","This paper proposes a method for long-horizon visual planning based on a probabilistic latent variable model (LatCo) for image-based RL. The LatCo method is based on the idea of long-term planning, which is an extension of previous work on long-range planning. The authors propose to use the latent variable models to learn a trajectory optimization algorithm to solve the problem. The proposed method is evaluated on a number of visual control tasks and shows promising results. ","This paper proposes a method for long-horizon visual planning based on a probabilistic latent variable model (LatCo) for image-based RL. The LatCo method is based on the idea of long-term planning, which is an extension of previous work on long-range planning. The authors propose to use the latent variable models to learn a trajectory optimization algorithm to solve the problem. The proposed method is evaluated on a number of visual control tasks and shows promising results. "
1620,SP:cfe57a61dc20207b64b7fff45f7cb33126dce558,"Bayesian neural networks COMPARE neural networks. neural networks COMPARE Bayesian neural networks. tempered ” or “ cold ” posterior USED-FOR uncertainty. BNNs USED-FOR image classification. CIFAR-10 HYPONYM-OF image benchmark datasets. generative model USED-FOR curation. generative model USED-FOR Bayesian account of cold posteriors. likelihood COMPARE tempered likelihoods. tempered likelihoods COMPARE likelihood. generative model USED-FOR likelihood. Method is Bayesian inference / decision theory. OtherScientificTerm are posterior, and prior. ","This paper proposes a generative model for curating the posterior of Bayesian neural networks (BNNs) for image classification tasks. The model is based on the notion of “cold posterior”, which is defined as the difference between the likelihood of the posterior and the prior of the BNN. The authors show that the cold posterior can be viewed as a combination of tempered likelihoods and cold prior. The paper also shows that the generative models can be used for curation of BNNs. ","This paper proposes a generative model for curating the posterior of Bayesian neural networks (BNNs) for image classification tasks. The model is based on the notion of “cold posterior”, which is defined as the difference between the likelihood of the posterior and the prior of the BNN. The authors show that the cold posterior can be viewed as a combination of tempered likelihoods and cold prior. The paper also shows that the generative models can be used for curation of BNNs. "
1629,SP:4ebd3874ecea94ed9d0ca7b2fb13bf246b556938,non - autoregressive neural machine translation COMPARE autoregressive machine translation. autoregressive machine translation COMPARE non - autoregressive neural machine translation. GPUs USED-FOR autoregressive machine translation. latter COMPARE former. former COMPARE latter. non - autoregressive models COMPARE autoregressive baselines. autoregressive baselines COMPARE non - autoregressive models. translation quality - speed tradeoffs EVALUATE-FOR autoregressive baselines. translation quality - speed tradeoffs EVALUATE-FOR non - autoregressive models. accuracy EVALUATE-FOR autoregressive baselines. encoders USED-FOR autoregressive models. single - layer autoregressive decoder COMPARE non - autoregressive models. non - autoregressive models COMPARE single - layer autoregressive decoder. inference speed EVALUATE-FOR single - layer autoregressive decoder. inference speed EVALUATE-FOR non - autoregressive models. suboptimal layer allocation CONJUNCTION insufficient speed measurement. insufficient speed measurement CONJUNCTION suboptimal layer allocation. autoregressive baselines COMPARE non - autoregressive methods. non - autoregressive methods COMPARE autoregressive baselines. speed disadvantage EVALUATE-FOR autoregressive baselines. speed disadvantage EVALUATE-FOR non - autoregressive methods. OtherScientificTerm is knowledge distillation. Task is machine translation. ,"This paper studies the trade-off between autoregressive and non-autoregressive machine translation. The authors propose a single-layer autorgressive decoder, which can be used to train a single autoroder and a single decoder with a single encoder and decoder. They show that the proposed method is able to achieve comparable or better performance compared to existing autorative methods. They also show that autoratively trained models are able to perform better than non-auto-regressive models in terms of accuracy.","This paper studies the trade-off between autoregressive and non-autoregressive machine translation. The authors propose a single-layer autorgressive decoder, which can be used to train a single autoroder and a single decoder with a single encoder and decoder. They show that the proposed method is able to achieve comparable or better performance compared to existing autorative methods. They also show that autoratively trained models are able to perform better than non-auto-regressive models in terms of accuracy."
1638,SP:ce8cf444681a8e38408c6485029fe42b89a1f172,"test error EVALUATE-FOR deep neural network ( DNN ). bell - shaped variance USED-FOR model - wise double descent. test error EVALUATE-FOR DNN. test error HYPONYM-OF epoch - wise double descent. bias - variance analysis USED-FOR epoch - wise double descent. variance USED-FOR zero - one loss. metric USED-FOR diversity of model updates. stochastic gradients of random training batches USED-FOR diversity of model updates. It USED-FOR generalization ability. It USED-FOR DNN. generalization ability EVALUATE-FOR DNN. It USED-FOR early stopping. zero - one loss USED-FOR DNN. validation set USED-FOR early stopping. Method are statistical learning theory, and bias - variance decomposition. OtherScientificTerm are double descent, model complexity, U - shaped curve, OV, and unknown ) test error. Generic is descent. Metric is optimization variance ( OV ). ","This paper studies the bias-variance analysis of the test error of epoch-wise double descent for deep neural networks (DNNs). The authors show that the variance of test error is bell-shaped and can be used as a measure of model complexity and generalization ability. The authors also show that zero-one loss for DNNs can be obtained by decomposing the OV of the training data into two parts. The first part is a function of the number of training samples, and the second part is an optimization variance function. ","This paper studies the bias-variance analysis of the test error of epoch-wise double descent for deep neural networks (DNNs). The authors show that the variance of test error is bell-shaped and can be used as a measure of model complexity and generalization ability. The authors also show that zero-one loss for DNNs can be obtained by decomposing the OV of the training data into two parts. The first part is a function of the number of training samples, and the second part is an optimization variance function. "
1647,SP:8d8b738c676938952e62a6b2aea42e79518ece06,"meta - learning techniques PART-OF few - shot learning. It USED-FOR meta - initialization of model parameters. labeled training data USED-FOR It. labeled training data USED-FOR meta - initialization of model parameters. few - shot learning USED-FOR MAML. MAML USED-FOR adversarial robustness. generalization CONJUNCTION robustness. robustness CONJUNCTION generalization. robustness USED-FOR meta - model. adversarial robustness FEATURE-OF MAML. robustness USED-FOR task - specific fine - tuning stage. training protocol USED-FOR latter. robust regularization USED-FOR MAML. fast adversarial attack generation CONJUNCTION computationally - light fine - tuning. computationally - light fine - tuning CONJUNCTION fast adversarial attack generation. unlabeled data augmentation CONJUNCTION fast adversarial attack generation. fast adversarial attack generation CONJUNCTION unlabeled data augmentation. auxiliary contrastive learning task USED-FOR MAML. auxiliary contrastive learning task USED-FOR adversarial robustness. adversarial robustness FEATURE-OF MAML. methods USED-FOR robust few - shot learning. OtherScientificTerm are robustness - promoting regularization, and meta - update stage. Metric is robustness adaptation. ",This paper studies the problem of few-shot learning with adversarial robustness (MAML). The authors propose a robustness-promoting regularization method to improve the robustness of the meta-model. The authors show that the proposed method is robust to adversarial perturbations in the training stage and robust to the fine-tuning stage. They also propose an auxiliary contrastive learning task to boost the robust performance of MAML. ,This paper studies the problem of few-shot learning with adversarial robustness (MAML). The authors propose a robustness-promoting regularization method to improve the robustness of the meta-model. The authors show that the proposed method is robust to adversarial perturbations in the training stage and robust to the fine-tuning stage. They also propose an auxiliary contrastive learning task to boost the robust performance of MAML. 
1656,SP:1fdce0afe8fd8c082f62f1a4b9823830d81860e8,"optimization algorithms USED-FOR optimizer. Learning - to - learn USED-FOR optimizers. optimization algorithms USED-FOR Learning - to - learn. metagradient descent USED-FOR meta - objective. trajectory USED-FOR metagradient descent. metagradient descent USED-FOR approach. step size USED-FOR quadratic loss. backpropagation USED-FOR meta - gradient. neural networks USED-FOR learned optimizers. OtherScientificTerm are metagradient explosion / vanishing problems, metagradient, and numerical issues. Method is learning - to - learn approach. Task is gradient explosion / vanishing problems. ",This paper proposes a meta-learning approach for learning optimizers in the setting of metagradient explosion/vanishing problems. The main idea is to use meta-gradient descent to learn the meta-objective of the optimizer. The meta-goal is to minimize the quadratic loss of the optimization algorithm. The authors show that this meta-optimizer can be used to solve the problem of gradient explosion and vanishing problems. ,This paper proposes a meta-learning approach for learning optimizers in the setting of metagradient explosion/vanishing problems. The main idea is to use meta-gradient descent to learn the meta-objective of the optimizer. The meta-goal is to minimize the quadratic loss of the optimization algorithm. The authors show that this meta-optimizer can be used to solve the problem of gradient explosion and vanishing problems. 
1665,SP:c8a9ab50888585b58369c4fb425be1170c96c14d,"methods USED-FOR large and complex graph problems. neural networks USED-FOR methods. dual views USED-FOR representations. neighborhood aggregation capability FEATURE-OF GVCLN. loss functions CONJUNCTION supervised loss. supervised loss CONJUNCTION loss functions. loss functions USED-FOR view - consistent representations. view - consistent loss USED-FOR consistent representation. view - consistent loss USED-FOR views. supervised loss CONJUNCTION view - consistent loss. view - consistent loss CONJUNCTION supervised loss. view - consistent loss CONJUNCTION pseudo - label loss. pseudo - label loss CONJUNCTION view - consistent loss. known labeled set USED-FOR supervised loss. common high - confidence predictions USED-FOR pseudo - label loss. GVCLN USED-FOR view - consistent representations. loss functions USED-FOR view - consistent representations. loss functions USED-FOR GVCLN. Citeseer CONJUNCTION PubMed. PubMed CONJUNCTION Citeseer. Cora CONJUNCTION Citeseer. Citeseer CONJUNCTION Cora. node classification tasks EVALUATE-FOR GVCLN. Task are acquisition of ground - truth labels, semisupervised learning, and classification tasks. OtherScientificTerm are viewing angles, observation objects, observation representations, and node features. ","This paper proposes a new loss function called GVCLN that combines view-consistent loss and pseudo-label loss to improve the performance of semisupervised learning on node classification tasks. The proposed loss function is based on the view-convergence loss, which is used to capture the dual views of two views. The paper also proposes a neighborhood aggregation capability that can be used to combine the two loss functions. Experiments show that the proposed method is able to achieve better performance than the baselines. ","This paper proposes a new loss function called GVCLN that combines view-consistent loss and pseudo-label loss to improve the performance of semisupervised learning on node classification tasks. The proposed loss function is based on the view-convergence loss, which is used to capture the dual views of two views. The paper also proposes a neighborhood aggregation capability that can be used to combine the two loss functions. Experiments show that the proposed method is able to achieve better performance than the baselines. "
1674,SP:9c8619d2c0df81c1222ba28cecbacc42408d0019,"physics bias USED-FOR neural networks. neural networks USED-FOR dynamics of systems. coordinates USED-FOR conserved quantities. cyclic coordinates HYPONYM-OF coordinates. Hamiltonian dynamics USED-FOR classical systems. canonical transformations USED-FOR coordinates. Hamiltonian dynamics USED-FOR loss functions. loss functions USED-FOR coordinates. network USED-FOR conserved quantities. network COMPARE networks. networks COMPARE network. network USED-FOR dynamics of the system. Hamiltonian USED-FOR networks. classical physics systems EVALUATE-FOR method. synthetic and experimental data EVALUATE-FOR method. symmetry orbits PART-OF phase space. lower dimensional sub - spaces of phase space HYPONYM-OF phase space. lower dimensional sub - spaces of phase space HYPONYM-OF symmetry orbits. analytic formulae USED-FOR networks. conserved quantities USED-FOR networks. ( angular ) momentum HYPONYM-OF conserved quantities. OtherScientificTerm are dynamics, and symmetries. Task is description of physical systems. ",This paper proposes a new method for learning the dynamics of physical systems. The main idea is to use the Hamiltonian dynamics of the system to learn the loss function of a neural network. The proposed method is evaluated on synthetic and experimental data and shows that the proposed method outperforms existing methods. ,This paper proposes a new method for learning the dynamics of physical systems. The main idea is to use the Hamiltonian dynamics of the system to learn the loss function of a neural network. The proposed method is evaluated on synthetic and experimental data and shows that the proposed method outperforms existing methods. 
1683,SP:d1e78b1759eef8fc16e5b7ad7f0e290e9dc5dea0,"models USED-FOR graph representation learning tasks. Graph neural networks ( GNNs ) USED-FOR graph representation learning tasks. Graph neural networks ( GNNs ) HYPONYM-OF models. gradient boosted decision trees ( GBDT ) COMPARE machine learning methods. machine learning methods COMPARE gradient boosted decision trees ( GBDT ). heterogeneous tabular data EVALUATE-FOR machine learning methods. approach USED-FOR graphs with tabular node features. GNN models USED-FOR networks. homogeneous sparse features FEATURE-OF networks. GBDT CONJUNCTION GNN. GNN CONJUNCTION GBDT. architecture USED-FOR GBDT. architecture USED-FOR GNN. GBDT model USED-FOR heterogeneous features. GNN USED-FOR graph structure. endto - end optimization USED-FOR model. Material is heterogeneous setting. OtherScientificTerm are trees, and graphs with tabular features. Method is GBDT and GNN models. ",This paper proposes a novel approach to learn heterogeneous tabular features for graphs with tabular node features. The proposed approach is based on gradient boosted decision trees (GBDT) and Graph Neural Networks (GNNs). The authors show that GBDT and GNNs can be used together to learn a heterogeneous graph representation learning task. The authors also show that the proposed approach can be applied to heterogeneous data. ,This paper proposes a novel approach to learn heterogeneous tabular features for graphs with tabular node features. The proposed approach is based on gradient boosted decision trees (GBDT) and Graph Neural Networks (GNNs). The authors show that GBDT and GNNs can be used together to learn a heterogeneous graph representation learning task. The authors also show that the proposed approach can be applied to heterogeneous data. 
1692,SP:9f9dbff2fe7defd41b9ed1a6c9dcad07e932dea7,"Meta - learning USED-FOR fast adaptation. train - validation split USED-FOR meta - learning. train - validation split COMPARE non - splitting method. non - splitting method COMPARE train - validation split. training EVALUATE-FOR non - splitting method. per - task data USED-FOR non - splitting method. train - validation split USED-FOR linear centroid meta - learning problem. splitting method COMPARE non - splitting method. non - splitting method COMPARE splitting method. regularization parameter CONJUNCTION split ratio. split ratio CONJUNCTION regularization parameter. split ratio USED-FOR methods. regularization parameter USED-FOR methods. asymptotic excess risk EVALUATE-FOR non - splitting method. non - splitting method COMPARE splitting method. splitting method COMPARE non - splitting method. Generic are predictor, and model. Method are linear models, splitting and non - splitting methods, and data splitting. OtherScientificTerm is data distribution. ","This paper studies the problem of data splitting in meta-learning. The authors propose two methods for data splitting: (1) train-validation split and (2) non-splitting split. The splitting method is based on the idea that the training data should be split into per-task and per-training data, and the non-split method uses the split ratio as a regularization parameter.  The authors show that the splitting method outperforms the non splitting method in terms of the asymptotic excess risk. ","This paper studies the problem of data splitting in meta-learning. The authors propose two methods for data splitting: (1) train-validation split and (2) non-splitting split. The splitting method is based on the idea that the training data should be split into per-task and per-training data, and the non-split method uses the split ratio as a regularization parameter.  The authors show that the splitting method outperforms the non splitting method in terms of the asymptotic excess risk. "
1701,SP:bb566eda95867f83a80664b2f685ad373147c87b,"noisy training data USED-FOR hard confident examples. physics USED-FOR momentum. non - simple patterns PART-OF hard confident examples. Me - Momentum USED-FOR hard confident examples. classification EVALUATE-FOR Me - Momentum. OtherScientificTerm are decision boundary, hard examples, and simple patterns. Method are classifiers, deep learning paradigm, deep neural networks, and classifier. Task are Extracting confident examples, and extracting hard confident examples. Material are noisy labels, and inaccurately labeled examples. Generic is approach. ","This paper proposes a method for extracting hard confident examples from noisy training data. The method is based on the idea of momentum, which is an extension of momentum in physics. The key idea is to learn a momentum function that is independent of the noisy labels. The authors show that this momentum function can be used to extract the hard-confident examples.","This paper proposes a method for extracting hard confident examples from noisy training data. The method is based on the idea of momentum, which is an extension of momentum in physics. The key idea is to learn a momentum function that is independent of the noisy labels. The authors show that this momentum function can be used to extract the hard-confident examples."
1710,SP:ca57b693e5eff372c872f42d66b18b8aa1d07c87,certified defenses USED-FOR data poisoning attacks. certified defenses USED-FOR majority vote mechanism. k nearest neighbors ( kNN ) CONJUNCTION radius nearest neighbors ( rNN ). radius nearest neighbors ( rNN ) CONJUNCTION k nearest neighbors ( kNN ). intrinsic majority vote mechanisms FEATURE-OF Nearest neighbor algorithms. radius nearest neighbors ( rNN ) HYPONYM-OF Nearest neighbor algorithms. k nearest neighbors ( kNN ) HYPONYM-OF Nearest neighbor algorithms. kNN CONJUNCTION rNN. rNN CONJUNCTION kNN. intrinsic majority vote mechanisms USED-FOR certified robustness guarantees. intrinsic majority vote mechanisms USED-FOR rNN. intrinsic majority vote mechanisms USED-FOR kNN. MNIST CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION MNIST. kNN CONJUNCTION rNN. rNN CONJUNCTION kNN. intrinsic certified robustness guarantees EVALUATE-FOR rNN. intrinsic certified robustness guarantees EVALUATE-FOR kNN. intrinsic certified robustness guarantees COMPARE certified defenses. certified defenses COMPARE intrinsic certified robustness guarantees. Task is Data poisoning attacks. Method is machine learning model. OtherScientificTerm is voter. ,"This paper studies the problem of data poisoning attacks against machine learning models. The authors propose a new method for certifying the robustness of a machine learning model to data poisoning. The proposed method is based on the notion of ""intrinsic majority vote"", which is defined as the difference between the certified and non-certified certified robustness guarantees of the model. The method is tested on MNIST, CIFAR10, and MNIST-10.","This paper studies the problem of data poisoning attacks against machine learning models. The authors propose a new method for certifying the robustness of a machine learning model to data poisoning. The proposed method is based on the notion of ""intrinsic majority vote"", which is defined as the difference between the certified and non-certified certified robustness guarantees of the model. The method is tested on MNIST, CIFAR10, and MNIST-10."
1719,SP:6cfe70be8ac34d6f61009e7e583e537e9adeb648,"training time CONJUNCTION model. model CONJUNCTION training time. model EVALUATE-FOR it. training time EVALUATE-FOR it. stochastic gradient decent ( SGD ) method USED-FOR deep learning models. batch size selection problem USED-FOR graph neural network ( GNN ). SGD method USED-FOR graph neural network ( GNN ). variance of gradients CONJUNCTION compute time. compute time CONJUNCTION variance of gradients. compute time FEATURE-OF mini - batch. compute time FEATURE-OF metric. variance of gradients PART-OF metric. formula USED-FOR optimal batch size. estimator USED-FOR gradients. randomness USED-FOR estimator. Ogbnarxiv CONJUNCTION Reddit. Reddit CONJUNCTION Ogbnarxiv. Ogbn - products CONJUNCTION Ogbnarxiv. Ogbnarxiv CONJUNCTION Ogbn - products. FastGCN CONJUNCTION GraphSAINT. GraphSAINT CONJUNCTION FastGCN. Reddit CONJUNCTION Pubmed. Pubmed CONJUNCTION Reddit. ClusterGCN CONJUNCTION FastGCN. FastGCN CONJUNCTION ClusterGCN. GraphSAINT HYPONYM-OF datasets. FastGCN HYPONYM-OF datasets. Pubmed HYPONYM-OF datasets. Ogbn - products HYPONYM-OF datasets. Reddit HYPONYM-OF datasets. Ogbnarxiv HYPONYM-OF datasets. deep learning models COMPARE GNNs. GNNs COMPARE deep learning models. large batch sizes USED-FOR GNNs. OtherScientificTerm are Batch size, batch - size, and batch size. Method are decent model, and GNN. ",This paper studies the batch size selection problem of graph neural networks (GNNs). The authors propose a stochastic gradient decent (SGD) method to select the optimal batch size for GNNs. The proposed method is based on the variance of gradients and compute time of mini-batch. The authors show that the proposed method outperforms the baselines on several benchmark datasets. ,This paper studies the batch size selection problem of graph neural networks (GNNs). The authors propose a stochastic gradient decent (SGD) method to select the optimal batch size for GNNs. The proposed method is based on the variance of gradients and compute time of mini-batch. The authors show that the proposed method outperforms the baselines on several benchmark datasets. 
1728,SP:30d97322709cd292a49f936c767099f11b0e2913,"neural network classifiers USED-FOR real - world applications. confidence scores USED-FOR detecting misclassification errors. framework USED-FOR detecting misclassification errors. framework USED-FOR confidence scores. Gaussian Processes USED-FOR calibrated confidence scores. confidence estimation methods COMPARE approach. approach COMPARE confidence estimation methods. UCI datasets EVALUATE-FOR confidence estimation methods. method USED-FOR neural network classifiers. deep learning architecture USED-FOR vision task. OtherScientificTerm are lowconfidence predictions, and classifier ’s inherent confidence indicators. Metric is confidence metrics. Method is RED. Material is out - of - distribution and adversarial samples. ",This paper proposes a new method for detecting misclassification errors in neural network classifiers. The proposed method is based on Gaussian Processes (GPs). The authors propose a method to calibrate the confidence scores of a classifier based on its inherent confidence indicators. The method is evaluated on UCI and CIFAR-10 datasets.,This paper proposes a new method for detecting misclassification errors in neural network classifiers. The proposed method is based on Gaussian Processes (GPs). The authors propose a method to calibrate the confidence scores of a classifier based on its inherent confidence indicators. The method is evaluated on UCI and CIFAR-10 datasets.
1737,SP:131b3da98f56d3af273171f496b217b90754a0a7,information retrieval PART-OF natural language processing systems. open domain question answering HYPONYM-OF natural language processing systems. methods COMPARE continuous representations. continuous representations COMPARE methods. neural networks USED-FOR continuous representations. hand - crafted features USED-FOR methods. supervised data USED-FOR retriever model. supervised data USED-FOR methods. retriever models USED-FOR downstream tasks. technique USED-FOR retriever models. technique USED-FOR downstream tasks. approach USED-FOR synthetic labels. attention scores USED-FOR task. synthetic labels USED-FOR retriever. reader model USED-FOR task. attention scores USED-FOR reader model. approach USED-FOR task. reader model USED-FOR approach. attention scores USED-FOR approach. retrieved documents USED-FOR approach. retrieved documents USED-FOR task. question answering EVALUATE-FOR method. Task is knowledge distillation. ,This paper proposes a novel approach for open domain question answering. The authors propose to use synthetic labels for the retriever model and a reader model for the reader model. The reader model is trained on a set of synthetic labels and the attention scores are used for downstream tasks. Experiments show that the proposed approach outperforms the state-of-the-art.,This paper proposes a novel approach for open domain question answering. The authors propose to use synthetic labels for the retriever model and a reader model for the reader model. The reader model is trained on a set of synthetic labels and the attention scores are used for downstream tasks. Experiments show that the proposed approach outperforms the state-of-the-art.
1746,SP:a516fff3cabc13cea1b8ed07dbf9eb1acb7dbb0e,"exploration USED-FOR reinforcement learned ( RL ) controllers. software engineering CONJUNCTION controller synthesis. controller synthesis CONJUNCTION software engineering. constraints FEATURE-OF constrained Markov decision process. controller synthesis USED-FOR safety methods. software engineering USED-FOR safety methods. formal languages USED-FOR them. finite automata USED-FOR constraint violations. finite automata USED-FOR constraints. Constraint states USED-FOR dense cost function. Constraint states USED-FOR MDP state. methods USED-FOR RL algorithms. constraints USED-FOR RL algorithms. Safety Gym HYPONYM-OF constraints. Atari environments HYPONYM-OF constraints. OtherScientificTerm are safety conditions, safety critical situations, and joint MDP / constraint dynamics. Method are safe controller, and learning. ","This paper studies the problem of learning a safe controller for reinforcement learning (RL) controllers. The authors propose a framework for learning a controller that is safe in the presence of constraint violations. The framework is based on the idea of constrained Markov decision processes (MDPs), which is an extension of constrained MDPs in the context of reinforcement learning. Constraint states are defined in a formal language, and the authors show that they can be used to learn controllers that are safe in an MDP. ","This paper studies the problem of learning a safe controller for reinforcement learning (RL) controllers. The authors propose a framework for learning a controller that is safe in the presence of constraint violations. The framework is based on the idea of constrained Markov decision processes (MDPs), which is an extension of constrained MDPs in the context of reinforcement learning. Constraint states are defined in a formal language, and the authors show that they can be used to learn controllers that are safe in an MDP. "
1755,SP:e18cfc1502c4087422d3baf655c244d4f3924a76,"binary classification algorithm USED-FOR models. decision tree learning HYPONYM-OF binary classification algorithm. first - class transparency FEATURE-OF models. decision tree model USED-FOR comprehensibility of classifications. Cascading Decision Trees HYPONYM-OF decision tree model. decision path CONJUNCTION explanation path. explanation path CONJUNCTION decision path. monolithic decision tree COMPARE decision subtrees. decision subtrees COMPARE monolithic decision tree. subtree USED-FOR features. subtrees USED-FOR positive classification. model COMPARE decision tree model. decision tree model COMPARE model. datasets CONJUNCTION real - world applications. real - world applications CONJUNCTION datasets. datasets EVALUATE-FOR model. real - world applications EVALUATE-FOR model. positive classifications EVALUATE-FOR model. explanation depth EVALUATE-FOR model. real - world applications EVALUATE-FOR algorithm. datasets EVALUATE-FOR algorithm. Method are decision trees, cascading decision subtrees, and cascading decision trees. OtherScientificTerm is decision paths. ","This paper proposes a new binary classification algorithm for decision trees. The proposed algorithm is based on the idea of cascading decision trees (CDFs), which is an extension of the decision tree model. The authors show that cascading CDFs can be used to improve the first-class transparency of classifications. They also show that the proposed algorithm can be applied to real-world applications.","This paper proposes a new binary classification algorithm for decision trees. The proposed algorithm is based on the idea of cascading decision trees (CDFs), which is an extension of the decision tree model. The authors show that cascading CDFs can be used to improve the first-class transparency of classifications. They also show that the proposed algorithm can be applied to real-world applications."
1764,SP:0508336b2ec032b9b98a1039e94ea223f3987cec,"random, static sparsity pattern FEATURE-OF weight tensors. random, static sparsity pattern USED-FOR models. training accuarcy EVALUATE-FOR model. Gaussian Process kernels USED-FOR models. sparse finite - width model kernel CONJUNCTION infinite - width kernel. infinite - width kernel CONJUNCTION sparse finite - width model kernel. Method is neural networks. OtherScientificTerm are network width, and model width. ",This paper studies the sparsity pattern of weight tensors in neural networks. The authors show that the random and static sparsity patterns of the weights of a weight tensor can be seen as a function of the width of the network and the number of layers. They show that this is the case for Gaussian Process kernels and infinite-width model kernels. They also show that it is possible to obtain a sparse finite-width Gaussian process kernel for any network. ,This paper studies the sparsity pattern of weight tensors in neural networks. The authors show that the random and static sparsity patterns of the weights of a weight tensor can be seen as a function of the width of the network and the number of layers. They show that this is the case for Gaussian Process kernels and infinite-width model kernels. They also show that it is possible to obtain a sparse finite-width Gaussian process kernel for any network. 
1773,SP:92e5a610ed13ada6d25d433b03ac06fa5eebd963,world knowledge CONJUNCTION entities. entities CONJUNCTION world knowledge. Knowledge graphs ( KGs ) USED-FOR world knowledge. entities PART-OF Knowledge graphs ( KGs ). they COMPARE pre - trained language models. pre - trained language models COMPARE they. KG USED-FOR language modeling. joint pre - training framework USED-FOR knowledge graph. knowledge graph CONJUNCTION language. language CONJUNCTION knowledge graph. joint pre - training framework USED-FOR language. JAKET USED-FOR knowledge graph. JAKET USED-FOR language. JAKET HYPONYM-OF joint pre - training framework. knowledge module CONJUNCTION language module. language module CONJUNCTION knowledge module. knowledge module CONJUNCTION language module. language module CONJUNCTION knowledge module. knowledge module USED-FOR embeddings. language module USED-FOR context - aware initial embeddings. knowledge - aware NLP tasks EVALUATE-FOR framework. knowledge in language understanding USED-FOR framework. OtherScientificTerm is graph. Method is pre - trained model. ,This paper proposes a joint pre-training framework for knowledge graph and language models. The authors propose a knowledge graph-language model that combines knowledge graph embedding and language embedding to improve the performance of pre-trained language models on knowledge-aware NLP tasks. The proposed method is evaluated on a number of knowledge-based NLP benchmarks and shows that it outperforms the state-of-the-art. ,This paper proposes a joint pre-training framework for knowledge graph and language models. The authors propose a knowledge graph-language model that combines knowledge graph embedding and language embedding to improve the performance of pre-trained language models on knowledge-aware NLP tasks. The proposed method is evaluated on a number of knowledge-based NLP benchmarks and shows that it outperforms the state-of-the-art. 
1782,SP:1db95a377f3d5ed129aa0511f840f647375e3528,"hand - designed loss functions USED-FOR specific orders. domain - specific insight USED-FOR approaches. unsupervised learner USED-FOR autoregressive orders. neural network USED-FOR variational inference. learner HYPONYM-OF neural network. autoregressive ordering USED-FOR variational inference. algorithm USED-FOR end - to - end optimization. policy gradients USED-FOR algorithm. algorithm USED-FOR autoregressive orders. algorithm COMPARE fixed orders. fixed orders COMPARE algorithm. sequence modeling tasks EVALUATE-FOR algorithm. autoregressive orders COMPARE fixed orders. fixed orders COMPARE autoregressive orders. sequence modeling tasks EVALUATE-FOR solution. Task is language modeling. OtherScientificTerm are predefined ordering, insertion operations, domain - specific prior, latent variable, and variational lower bound. Metric is time complexity. ",This paper proposes an unsupervised learning algorithm for learning autoregressive orders. The proposed algorithm is based on the idea of learning a latent variable that can be used as a prior for the learning of a variational lower bound. The authors show that this latent variable can be learned using a neural network. They also show that the proposed algorithm achieves better performance than the state-of-the-art in terms of time complexity.,This paper proposes an unsupervised learning algorithm for learning autoregressive orders. The proposed algorithm is based on the idea of learning a latent variable that can be used as a prior for the learning of a variational lower bound. The authors show that this latent variable can be learned using a neural network. They also show that the proposed algorithm achieves better performance than the state-of-the-art in terms of time complexity.
1791,SP:1c310f02acda4aa14e4d043c8d6de8c94a8ecf44,"Graph Convolutional Networks ( GCNs ) USED-FOR graph - related applications. large graphs USED-FOR GCNs. evolving parameters USED-FOR optimization. doubly variance reduction schema USED-FOR sampling method. O(1 / T ) convergence rate EVALUATE-FOR it. schema USED-FOR sampling methods. them USED-FOR large real - world graphs. OtherScientificTerm are computational and memory issues, nodes, memory budget, and induced variance. Method are sampling - based methods, variance of sampling methods, forward propagation, and backward propagation. Generic is works. Metric is theoretical convergence guarantees. ",This paper studies the problem of minimizing the variance of sampling methods in graph convolutional networks (GCNs). The authors propose a doubly variance reduction (DVRC) scheme to reduce the variance in the forward propagation and backward propagation of GCNs. The authors show that the proposed DVRC scheme converges to O(1/T) convergence rate for large graphs. They also provide theoretical convergence guarantees for the proposed scheme.,This paper studies the problem of minimizing the variance of sampling methods in graph convolutional networks (GCNs). The authors propose a doubly variance reduction (DVRC) scheme to reduce the variance in the forward propagation and backward propagation of GCNs. The authors show that the proposed DVRC scheme converges to O(1/T) convergence rate for large graphs. They also provide theoretical convergence guarantees for the proposed scheme.
1800,SP:02e100a9ad4eedab8cba043d3726f022bc09a3af,"deep neural network methods USED-FOR image manipulation tasks. conditional adversarial generator USED-FOR complex image manipulations. edges CONJUNCTION segmentation. segmentation CONJUNCTION edges. primitive input representation USED-FOR generator. Task are Image manipulation, and single image training. Generic are task, network, method, and it. Method are deep methods, and augmentation method. ","This paper proposes a conditional adversarial generator (CAG) for image manipulation tasks. The proposed method is based on the idea of augmenting the input representation of the generator with a primitive input representation to improve the performance of the model. The authors show that the proposed method outperforms the state-of-the-art methods on a number of tasks, including image manipulation, segmentation, and edge manipulation. ","This paper proposes a conditional adversarial generator (CAG) for image manipulation tasks. The proposed method is based on the idea of augmenting the input representation of the generator with a primitive input representation to improve the performance of the model. The authors show that the proposed method outperforms the state-of-the-art methods on a number of tasks, including image manipulation, segmentation, and edge manipulation. "
1809,SP:4d7c1e30fa8eb3e7c67a4ec3bccc5d3ef713a773,"malware detection CONJUNCTION cloud computing. cloud computing CONJUNCTION malware detection. biomedical analysis CONJUNCTION malware detection. malware detection CONJUNCTION biomedical analysis. Detecting the Maximum Common Subgraph ( MCS ) USED-FOR biomedical analysis. heuristics in search USED-FOR MCS solvers. Graph Neural Network based model USED-FOR MCS detection. GLSEARCH HYPONYM-OF Graph Neural Network based model. branch and bound algorithm USED-FOR backbone search algorithm. model USED-FOR subgraphs. branch and bound algorithm USED-FOR subgraphs. branch and bound algorithm USED-FOR model. search process USED-FOR supervision. imitation learning stage USED-FOR agent. search process USED-FOR DQN. search CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION search. framework USED-FOR reinforcement learning. framework USED-FOR search. synthetic and real - world large graph pairs EVALUATE-FOR model. MCS solvers CONJUNCTION neural graph matching network models. neural graph matching network models CONJUNCTION MCS solvers. model COMPARE neural graph matching network models. neural graph matching network models COMPARE model. model COMPARE MCS solvers. MCS solvers COMPARE model. synthetic and real - world large graph pairs EVALUATE-FOR neural graph matching network models. synthetic and real - world large graph pairs EVALUATE-FOR MCS solvers. OtherScientificTerm are Maximum Common Subgraph ( MCS ), large graph pairs, limited search budget, and node selection decision. Task are drug design, extraction of common substructures, and MCS computation. Method is node selection heuristics. ",This paper proposes a new method for detecting the Maximum Common Subgraph (MCS) in graph neural networks (GNNs). The main idea is to use a graph neural network to learn a backbone search algorithm for finding the subgraphs of a large graph. The proposed method is evaluated on synthetic and real-world large graph pairs and is shown to outperform existing methods. ,This paper proposes a new method for detecting the Maximum Common Subgraph (MCS) in graph neural networks (GNNs). The main idea is to use a graph neural network to learn a backbone search algorithm for finding the subgraphs of a large graph. The proposed method is evaluated on synthetic and real-world large graph pairs and is shown to outperform existing methods. 
1818,SP:581c6d218e75b0df808bc2c83c8731a94e94a5b3,"PC2WF USED-FOR wireframe model. network USED-FOR wireframe. vertices CONJUNCTION edges. edges CONJUNCTION vertices. model USED-FOR architecture. it USED-FOR candidate vertices. features USED-FOR it. candidate edges USED-FOR wireframe. ground truth wireframes FEATURE-OF synthetic dataset. real - world dataset EVALUATE-FOR model. synthetic dataset EVALUATE-FOR model. model COMPARE baselines. baselines COMPARE model. model USED-FOR wireframe abstractions. OtherScientificTerm are 3D point cloud, line segments, feature vectors, and corner vertices. Task is Recovering the wireframe. Generic is It. ","This paper proposes a new method for learning a 3D wireframe model. The proposed method, PC2WF, is based on the idea of candidate vertices and edges. The paper shows that the proposed method is able to recover the ground truth wireframe from the 3D point cloud. The method is evaluated on a synthetic dataset and a real-world dataset. ","This paper proposes a new method for learning a 3D wireframe model. The proposed method, PC2WF, is based on the idea of candidate vertices and edges. The paper shows that the proposed method is able to recover the ground truth wireframe from the 3D point cloud. The method is evaluated on a synthetic dataset and a real-world dataset. "
1827,SP:3e0fd62d9815d7de5e5139a1d6d2e80eea917154,"assumptions FEATURE-OF distribution of noise. assumptions USED-FOR stochastic optimization. uniform bound USED-FOR moments of the gradient noise. optimal convergence rates FEATURE-OF stochastic optimization. optimization algorithms USED-FOR neural network training. noise level FEATURE-OF stochastic gradients. convergence rates FEATURE-OF stochastic gradient methods. adaptive step size methods COMPARE SGD. SGD COMPARE adaptive step size methods. Method are neural networks, online estimator of the noise level, and RMSProp. OtherScientificTerm are noise, nonstationary behavior of noise, stochastic oracle, noise variation, step - size, noise variability, noise statistics, and theoretical guarantees. ","This paper studies the convergence of stochastic gradient methods for neural networks. The authors consider the case where the noise level is non-stationary and the gradient noise is not uniform. They derive a uniform bound for the moments of the gradient gradient noise, and show that this uniform bound is a generalization of the convergence rate of SGD. They also provide theoretical guarantees for the noise variance of the noise. They show that the convergence rates of the SGD and RMSProp algorithms converge to the optimal convergence rate. ","This paper studies the convergence of stochastic gradient methods for neural networks. The authors consider the case where the noise level is non-stationary and the gradient noise is not uniform. They derive a uniform bound for the moments of the gradient gradient noise, and show that this uniform bound is a generalization of the convergence rate of SGD. They also provide theoretical guarantees for the noise variance of the noise. They show that the convergence rates of the SGD and RMSProp algorithms converge to the optimal convergence rate. "
1836,SP:71c4e6ab911962d730461eda0f2d72d810fc017c,Prior word alignment USED-FOR translation. statistical machine translation ( SMT ) models USED-FOR word alignment. method USED-FOR neural machine translation ( NMT ). prior word alignment information USED-FOR neural machine translation ( NMT ). prior word alignment information USED-FOR method. dictionaries USED-FOR approaches. decoding speed EVALUATE-FOR methods. model PART-OF neural MT model. learning model USED-FOR target information. target information USED-FOR MT input. method COMPARE baseline model. baseline model COMPARE method. English - Korean EVALUATE-FOR baseline model. English - Korean EVALUATE-FOR method. Generic is prior. Task is decoding process. Method is enhancement learning model. OtherScientificTerm is prior alignment information. ,This paper proposes a method to improve the decoding speed of neural machine translation (NMT) models by incorporating prior word alignment information into the training process. The proposed method is based on the idea that the prior alignment information can be used to enhance the decoding performance of a neural MT model. The authors show that the proposed method outperforms the baseline model on English-Korean and Chinese-English tasks. ,This paper proposes a method to improve the decoding speed of neural machine translation (NMT) models by incorporating prior word alignment information into the training process. The proposed method is based on the idea that the prior alignment information can be used to enhance the decoding performance of a neural MT model. The authors show that the proposed method outperforms the baseline model on English-Korean and Chinese-English tasks. 
1845,SP:c26255a8ad441f11cfbe18fd6dad14773aca4a2b,MDP Playground HYPONYM-OF Reinforcement Learning ( RL ) algorithms. MDP Playground HYPONYM-OF benchmark. benchmark EVALUATE-FOR Reinforcement Learning ( RL ) algorithms. dimensions of hardness FEATURE-OF MDP Playground. stochasticity CONJUNCTION image representations. image representations CONJUNCTION stochasticity. image representations CONJUNCTION irrelevant features. irrelevant features CONJUNCTION image representations. sparsity of rewards CONJUNCTION stochasticity. stochasticity CONJUNCTION sparsity of rewards. time unit CONJUNCTION action range. action range CONJUNCTION time unit. irrelevant features CONJUNCTION time unit. time unit CONJUNCTION irrelevant features. delayed rewards CONJUNCTION rewardable sequences. rewardable sequences CONJUNCTION delayed rewards. rewardable sequences CONJUNCTION sparsity of rewards. sparsity of rewards CONJUNCTION rewardable sequences. action range HYPONYM-OF hardness dimensions. time unit HYPONYM-OF hardness dimensions. sparsity of rewards HYPONYM-OF hardness dimensions. stochasticity HYPONYM-OF hardness dimensions. irrelevant features HYPONYM-OF hardness dimensions. delayed rewards HYPONYM-OF hardness dimensions. image representations HYPONYM-OF hardness dimensions. rewardable sequences HYPONYM-OF hardness dimensions. benchmarks EVALUATE-FOR RL algorithms. benchmarks EVALUATE-FOR RL algorithms. fine - grained control FEATURE-OF environments ’ hardness. MDP Playground USED-FOR adaptive and intelligent RL algorithms. MDP Playground EVALUATE-FOR algorithms. OtherScientificTerm is hardness. Material is OpenAI Gym. Generic is dimensions. ,"This paper proposes a new benchmark, MDP Playground, for fine-grained control of RL algorithms. The benchmark is built on top of the OpenAI Gym, which is an open-source benchmark for RL algorithms, and is designed to evaluate the performance of different RL algorithms on a variety of environments. The proposed benchmark is based on the idea that the hardness of an environment is a function of the number of rewards and the time-to-goal ratio. The authors propose to use the hardness as a measure of the difficulty of a given task, and show that it can be used as a metric for evaluating the performance on a number of different environments. ","This paper proposes a new benchmark, MDP Playground, for fine-grained control of RL algorithms. The benchmark is built on top of the OpenAI Gym, which is an open-source benchmark for RL algorithms, and is designed to evaluate the performance of different RL algorithms on a variety of environments. The proposed benchmark is based on the idea that the hardness of an environment is a function of the number of rewards and the time-to-goal ratio. The authors propose to use the hardness as a measure of the difficulty of a given task, and show that it can be used as a metric for evaluating the performance on a number of different environments. "
1854,SP:e8cbe62252aa671a6deaf12b97063063dfc6d1b0,"they USED-FOR overconfident predictions. approaches USED-FOR classification models. Isotonic Regression USED-FOR regression calibration. Isotonic Regression USED-FOR regression calibration. formulation USED-FOR quantile regularizer. quantile regularizer USED-FOR probabilistic regression model. approaches USED-FOR regression models. Dropout VI CONJUNCTION Deep Ensembles. Deep Ensembles CONJUNCTION Dropout VI. approach USED-FOR regression models. calibration USED-FOR regression models. architectures USED-FOR uncertainty estimates. approach USED-FOR calibration. architectures USED-FOR regression models. Deep Ensembles HYPONYM-OF uncertainty estimates. Dropout VI HYPONYM-OF uncertainty estimates. Method are Deep learning models, quantile calibration, and entropy estimation. Generic are it, model, and method. ",This paper proposes a quantile regularizer for regression calibration. The proposed method is based on the Isotonic Regression (IR) framework. The authors show that the proposed method can be used to calibrate a probabilistic regression model. They also show that it can be applied to a number of deep learning models. ,This paper proposes a quantile regularizer for regression calibration. The proposed method is based on the Isotonic Regression (IR) framework. The authors show that the proposed method can be used to calibrate a probabilistic regression model. They also show that it can be applied to a number of deep learning models. 
1863,SP:9c71ab8dcc433b59d9da3f0db377b74a369112bc,"6 - DoF localisation CONJUNCTION 3D dense reconstruction in spatial environments. 3D dense reconstruction in spatial environments CONJUNCTION 6 - DoF localisation. deep state - space model USED-FOR approximate Bayesian inference. approximate Bayesian inference USED-FOR 3D dense reconstruction in spatial environments. approximate Bayesian inference USED-FOR 6 - DoF localisation. multiple - view geometry CONJUNCTION rigid - body dynamics. rigid - body dynamics CONJUNCTION multiple - view geometry. rigid - body dynamics USED-FOR learning and domain knowledge. multiple - view geometry USED-FOR learning and domain knowledge. learning and domain knowledge USED-FOR approach. neural networks CONJUNCTION differentiable raycaster. differentiable raycaster CONJUNCTION neural networks. variational inference CONJUNCTION neural networks. neural networks CONJUNCTION variational inference. realistic unmanned aerial vehicle flight data EVALUATE-FOR approach. model USED-FOR generative prediction and planning. OtherScientificTerm is spatial environments. Method are visual SLAM solutions, and visual - inertial odometry systems. ",This paper proposes a deep state-space model for 3D dense reconstruction in spatial environments. The model is based on an approximate Bayesian inference approach. The authors show that the proposed model is able to achieve state-of-the-art performance on a variety of datasets.,This paper proposes a deep state-space model for 3D dense reconstruction in spatial environments. The model is based on an approximate Bayesian inference approach. The authors show that the proposed model is able to achieve state-of-the-art performance on a variety of datasets.
1872,SP:bacb279ab6d1997bf44b7b2af583f29679219c36,"textual descriptions USED-FOR generalization of control policies. symbol grounding CONJUNCTION control policy. control policy CONJUNCTION symbol grounding. environment rewards USED-FOR supervision. environment rewards USED-FOR EMMA. free - form natural language FEATURE-OF text manuals. framework EVALUATE-FOR model. crowd - sourcing USED-FOR free - form natural language. crowd - sourcing USED-FOR text manuals. zeroshot generalization EVALUATE-FOR EMMA. noisy descriptions FEATURE-OF grounding. EMMA USED-FOR grounding. OtherScientificTerm are prior knowledge, concrete supervision, and dynamics. Generic is policies. Method is multi - modal entity - conditioned attention module. ","This paper proposes a framework for generalization of control policies based on text descriptions. The framework is based on a multi-modal entity-conditioned attention module (EMMA), which is trained to learn a representation of the environment and a grounding mechanism for grounding. The grounding mechanism is learned by learning a representation for the grounding mechanism, which is then used to learn an environment reward. The authors show that EMMA can generalize well to noisy descriptions and zeroshot generalization. Empirical results show that the proposed framework outperforms baselines on a number of tasks.","This paper proposes a framework for generalization of control policies based on text descriptions. The framework is based on a multi-modal entity-conditioned attention module (EMMA), which is trained to learn a representation of the environment and a grounding mechanism for grounding. The grounding mechanism is learned by learning a representation for the grounding mechanism, which is then used to learn an environment reward. The authors show that EMMA can generalize well to noisy descriptions and zeroshot generalization. Empirical results show that the proposed framework outperforms baselines on a number of tasks."
1881,SP:d90da59c651ae3e97af1cf85f3ab1f12cd56d149,"Policy gradient algorithms USED-FOR decision making and control tasks. high sample complexity CONJUNCTION instability issues. instability issues CONJUNCTION high sample complexity. instability issues EVALUATE-FOR methods. high sample complexity EVALUATE-FOR methods. approach USED-FOR critic. actor - critic framework FEATURE-OF critic. mean value COMPARE absolute value. absolute value COMPARE mean value. continuous control tasks CONJUNCTION algorithms. algorithms CONJUNCTION continuous control tasks. sparse rewards USED-FOR method. Method are actor - critic algorithms, actor - critic, and gradient estimator. OtherScientificTerm is value function. ","This paper proposes an actor-critic framework for policy gradient algorithms. The key idea is to use a policy gradient estimator to estimate the mean value of the policy gradient of the actor and the critic, which is then used as a reward function. The authors show that the proposed method outperforms the baselines on a number of continuous control tasks. ","This paper proposes an actor-critic framework for policy gradient algorithms. The key idea is to use a policy gradient estimator to estimate the mean value of the policy gradient of the actor and the critic, which is then used as a reward function. The authors show that the proposed method outperforms the baselines on a number of continuous control tasks. "
1890,SP:62d79bf04817bba3fdffb2c0c9209923a8428533,"network USED-FOR overparameterization. overparameterized regime FEATURE-OF depth. locality of the relevant feature FEATURE-OF classification rule. initialization CONJUNCTION infinitesimal learning rate. infinitesimal learning rate CONJUNCTION initialization. finite networks COMPARE neural tangent kernel ( NTK ). neural tangent kernel ( NTK ) COMPARE finite networks. infinitely wide network USED-FOR neural tangent kernel ( NTK ). initialization USED-FOR infinitely wide network. infinitesimal learning rate FEATURE-OF infinitely wide network. depth dependence FEATURE-OF generalization performance. feature learning COMPARE lazy learning. lazy learning COMPARE feature learning. NTK USED-FOR depth dependence. generalization performance EVALUATE-FOR NTK. Task are generalization, and machinelearning tasks. OtherScientificTerm are local and global labels, classification rules, local labels, and global labels. ",This paper studies the generalization performance of neural tangent kernels (NTK) in the overparameterized regime. The authors show that NTK outperforms finite NTK in terms of generalization. The main contribution of the paper is the analysis of the depth dependence of NTK. The paper also shows that the NTK is more robust to overparametrization. ,This paper studies the generalization performance of neural tangent kernels (NTK) in the overparameterized regime. The authors show that NTK outperforms finite NTK in terms of generalization. The main contribution of the paper is the analysis of the depth dependence of NTK. The paper also shows that the NTK is more robust to overparametrization. 
1899,SP:9f8a9299ee67b9c707b241ce84cf41f4917ef735,"sample complexity EVALUATE-FOR representation. representation learning USED-FOR few - shot learning. i.i.d. task assumption USED-FOR Ω ( 1 T ) barrier. high - dimensional linear regression CONJUNCTION neural networks. neural networks CONJUNCTION high - dimensional linear regression. representation learning USED-FOR high - dimensional linear regression. representation learning USED-FOR neural networks. representation learning USED-FOR representation learning. Material is n2 ( n1 ) data. Generic is common representation. Task is sample size reduction. Metric is risk bound. OtherScientificTerm are linear representation class, and representation function class. ","This paper studies the sample complexity of representation learning in few-shot learning. In particular, the authors consider the case where the number of samples is small and the task is to reduce the sample size of a linear representation class to a common representation class. The authors show that the risk bound for this problem is $\tilde{O}(\sqrt{1 T})$ for the case when the data is n2 (n1) data. They also show that this risk bound can be extended to the case of high-dimensional linear regression. ","This paper studies the sample complexity of representation learning in few-shot learning. In particular, the authors consider the case where the number of samples is small and the task is to reduce the sample size of a linear representation class to a common representation class. The authors show that the risk bound for this problem is $\tilde{O}(\sqrt{1 T})$ for the case when the data is n2 (n1) data. They also show that this risk bound can be extended to the case of high-dimensional linear regression. "
1908,SP:e29ce50c1c28f9264613736b6c2d20afc4f312c1,lowestlevel features FEATURE-OF network robustness. robustness EVALUATE-FOR networks. semantic features USED-FOR networks. black - box approach USED-FOR features. network USED-FOR features. features USED-FOR provably robust neighborhoods. provably robust neighborhoods CONJUNCTION adversarial examples. adversarial examples CONJUNCTION provably robust neighborhoods. robust features CONJUNCTION adversarial examples. adversarial examples CONJUNCTION robust features. weak features USED-FOR adversarial examples. robust features USED-FOR provably robust neighborhoods. PCA features EVALUATE-FOR approach. provably robust neighborhoods COMPARE neighborhoods. neighborhoods COMPARE provably robust neighborhoods. adversarial examples COMPARE state - of - the - art. state - of - the - art COMPARE adversarial examples. L2 distortion EVALUATE-FOR state - of - the - art. L2 distortion EVALUATE-FOR adversarial examples. attack USED-FOR ensemble adversarial training. Method is neural networks. Task is neural networks ’ robustness. OtherScientificTerm is perturbations. ,This paper proposes a black-box approach to improve the robustness of neural networks against adversarial perturbations. The proposed method is based on the observation that adversarial examples are more likely to have weak features than robust features. The authors then propose a black box approach to learn robust features for adversarial training. The method is evaluated on PCA and L2-distortion datasets and shows that the proposed method outperforms the baselines. ,This paper proposes a black-box approach to improve the robustness of neural networks against adversarial perturbations. The proposed method is based on the observation that adversarial examples are more likely to have weak features than robust features. The authors then propose a black box approach to learn robust features for adversarial training. The method is evaluated on PCA and L2-distortion datasets and shows that the proposed method outperforms the baselines. 
1917,SP:e3fdb96a8c321a86b136e765abe796019d6f9c7a,"transfer learning CONJUNCTION multi - task learning. multi - task learning CONJUNCTION transfer learning. Meta - learning CONJUNCTION transfer learning. transfer learning CONJUNCTION Meta - learning. uniform similarity USED-FOR approaches. method USED-FOR clusters of related tasks. sample complexity EVALUATE-FOR these. expectation - maximization algorithm USED-FOR method. policies USED-FOR agent. expectation step EVALUATE-FOR policies. method COMPARE multi - task learning algorithms. multi - task learning algorithms COMPARE method. complex bipedal walker tasks CONJUNCTION Atari games. Atari games CONJUNCTION complex bipedal walker tasks. discrete and continuous control tasks CONJUNCTION complex bipedal walker tasks. complex bipedal walker tasks CONJUNCTION discrete and continuous control tasks. complex bipedal walker tasks EVALUATE-FOR approach. discrete and continuous control tasks EVALUATE-FOR approach. sample complexity EVALUATE-FOR approaches. Method are reinforcement learning agents, and maximization step. Task is training. OtherScientificTerm is policy. ","This paper proposes a method for multi-task learning. The main idea is to learn a policy that maximizes the sample complexity of a cluster of related tasks, and then use this maximization step to train a new policy for each task. The authors show that the proposed method outperforms the state-of-the-art methods on a variety of tasks. They also show that their method can be applied to multi-target learning.","This paper proposes a method for multi-task learning. The main idea is to learn a policy that maximizes the sample complexity of a cluster of related tasks, and then use this maximization step to train a new policy for each task. The authors show that the proposed method outperforms the state-of-the-art methods on a variety of tasks. They also show that their method can be applied to multi-target learning."
1926,SP:b3805eb7114391ed15d5806b1c3eb383bff44250,"self - supervised framework USED-FOR generalizable representations. generalizable representations USED-FOR non - stationary time series. local smoothness USED-FOR neighborhoods in time with stationary properties. local smoothness USED-FOR approach. debiased contrastive objective USED-FOR framework. framework USED-FOR time series representations. method COMPARE unsupervised representation learning approaches. unsupervised representation learning approaches COMPARE method. clustering and classification tasks EVALUATE-FOR multiple datasets. clustering and classification tasks EVALUATE-FOR method. Material are Time series, time series data, and labeling data. Method is Temporal Neighborhood Coding ( TNC ). OtherScientificTerm are encoding space, neighborhood, and distribution of non - neighboring signals. Task is medical field. ","This paper proposes a self-supervised framework for learning representations for non-stationary time series. The proposed method is based on a debiased contrastive objective, which aims to improve the generalizability of time series representations. The method is evaluated on a variety of tasks, including clustering, clustering and classification.","This paper proposes a self-supervised framework for learning representations for non-stationary time series. The proposed method is based on a debiased contrastive objective, which aims to improve the generalizability of time series representations. The method is evaluated on a variety of tasks, including clustering, clustering and classification."
1935,SP:60b2ea4624997d6ccf862742fb9eb21b819d7eb1,"Conditional computation CONJUNCTION modular networks. modular networks CONJUNCTION Conditional computation. Conditional computation USED-FOR multitask learning and other problems. modular networks USED-FOR multitask learning and other problems. fully - differentiable approach USED-FOR modular networks. modules USED-FOR knowledge transfer. knowledge transfer USED-FOR tasks. soft weight sharing USED-FOR tasks. transfer learning CONJUNCTION domain adaptation. domain adaptation CONJUNCTION transfer learning. method USED-FOR self - organization of modules. transfer learning HYPONYM-OF tasks. multi - task learning CONJUNCTION transfer learning. transfer learning CONJUNCTION multi - task learning. domain adaptation HYPONYM-OF tasks. self - organization of modules USED-FOR multi - task learning. tasks EVALUATE-FOR method. it USED-FOR unsupervised multi - source domain adaptation. architectures USED-FOR image classification tasks. accuracy EVALUATE-FOR architectures. approach USED-FOR architectures. it USED-FOR adaptation. computation order USED-FOR modules. accuracy EVALUATE-FOR approach. order of pretrained modules USED-FOR adaptation. IMAGENET HYPONYM-OF image classification tasks. Task is problem solving. OtherScientificTerm are order of computation, and parameter increase. ","This paper proposes a fully differentiable approach to learn modular networks for multi-task learning and transfer learning. The authors propose a self-organization of modules in order to improve the performance of multi-source domain adaptation, transfer learning and knowledge transfer. The proposed method is based on the idea of soft weight sharing, which is an extension of previous work on soft-weight sharing. The method is evaluated on a variety of tasks, including transfer learning, multi task learning, and domain adaptation. ","This paper proposes a fully differentiable approach to learn modular networks for multi-task learning and transfer learning. The authors propose a self-organization of modules in order to improve the performance of multi-source domain adaptation, transfer learning and knowledge transfer. The proposed method is based on the idea of soft weight sharing, which is an extension of previous work on soft-weight sharing. The method is evaluated on a variety of tasks, including transfer learning, multi task learning, and domain adaptation. "
1944,SP:cae669c631e11fe703bf6cb511404866b19f474a,"local optima FEATURE-OF objective function. hyperparameter USED-FOR data variance. hyperparameter USED-FOR local optima. variance parameter USED-FOR VAE. variance parameter USED-FOR smoothness. gradient FEATURE-OF smoothness. It USED-FOR regularization. variance parameter USED-FOR It. variance parameter USED-FOR regularization. Fréchet inception distance ( FID ) EVALUATE-FOR Generation models. MNIST and CelebA datasets USED-FOR Fréchet inception distance ( FID ) of images. objectives USED-FOR Generation models. Method are Variational autoencoders ( VAEs ), and AR - ELBO. OtherScientificTerm are posterior collapse, latent space, oversmoothness, and linear approximated objective function. Generic are parameter, and model. ",This paper studies the variance parameter of Variational Autoencoders (VAEs). The authors show that the variance of the objective function is a function of the posterior collapse of the latent space. The authors also show that this variance parameter can be used as a regularizer to improve the generalization performance of VAEs.  ,This paper studies the variance parameter of Variational Autoencoders (VAEs). The authors show that the variance of the objective function is a function of the posterior collapse of the latent space. The authors also show that this variance parameter can be used as a regularizer to improve the generalization performance of VAEs.  
1953,SP:cb3c10afbdd8a49cdc23e3ea71ea46ab27253b85,non i.i.d. variational autoencoders USED-FOR global dependencies. non i.i.d. variational autoencoders USED-FOR deep generative model. semi - supervised alternatives USED-FOR global modeling. mixture model CONJUNCTION global Gaussian latent variable. global Gaussian latent variable CONJUNCTION mixture model. global modeling USED-FOR deep generative models. semi - supervised alternatives USED-FOR deep generative models. semi - supervised alternatives COMPARE approach. approach COMPARE semi - supervised alternatives. local or data - dependent space FEATURE-OF mixture model. mixture model PART-OF approach. global Gaussian latent variable PART-OF approach. induced latent global space USED-FOR interpretable disentangled representations. user - defined regularization FEATURE-OF evidence lower bound. domain alignment USED-FOR model. shared attributes CONJUNCTION defined sequences of digits images. defined sequences of digits images CONJUNCTION shared attributes. face images CONJUNCTION defined sequences of digits images. defined sequences of digits images CONJUNCTION face images. shared attributes FEATURE-OF face images. face images HYPONYM-OF non - trivial underlying structures. Method is beta - VAE. OtherScientificTerm is global space. ,This paper proposes a new method for learning disentangled representations in a global latent space. The proposed method is based on a mixture model and a global Gaussian latent variable. The authors show that the proposed method can achieve better disentanglement performance than the baselines. They also show that their method is able to achieve a lower bound on the evidence lower bound. ,This paper proposes a new method for learning disentangled representations in a global latent space. The proposed method is based on a mixture model and a global Gaussian latent variable. The authors show that the proposed method can achieve better disentanglement performance than the baselines. They also show that their method is able to achieve a lower bound on the evidence lower bound. 
1962,SP:33792375012ff9dcffab598cc8fe5ebc71c98af4,"images CONJUNCTION videos. videos CONJUNCTION images. images HYPONYM-OF visual data. videos HYPONYM-OF visual data. visual data USED-FOR representation learning approaches. representations COMPARE visualonly representations. visualonly representations COMPARE representations. human interaction and attention cues USED-FOR approach. body part movements CONJUNCTION gaze. gaze CONJUNCTION body part movements. gaze FEATURE-OF human interactions. human interactions FEATURE-OF dataset. body part movements FEATURE-OF human interactions. gaze FEATURE-OF dataset. body part movements FEATURE-OF dataset. dynamics prediction ( physics ) CONJUNCTION walkable surface estimation ( affordance ). walkable surface estimation ( affordance ) CONJUNCTION dynamics prediction ( physics ). scene classification ( semantic ) CONJUNCTION action recognition ( temporal ). action recognition ( temporal ) CONJUNCTION scene classification ( semantic ). action recognition ( temporal ) CONJUNCTION depth estimation ( geometric ). depth estimation ( geometric ) CONJUNCTION action recognition ( temporal ). depth estimation ( geometric ) CONJUNCTION dynamics prediction ( physics ). dynamics prediction ( physics ) CONJUNCTION depth estimation ( geometric ). muscly - supervised ” representation USED-FOR interaction and attention cues. muscly - supervised ” representation USED-FOR target tasks. muscly - supervised ” representation COMPARE MoCo. MoCo COMPARE muscly - supervised ” representation. walkable surface estimation ( affordance ) HYPONYM-OF target tasks. scene classification ( semantic ) HYPONYM-OF target tasks. dynamics prediction ( physics ) HYPONYM-OF target tasks. depth estimation ( geometric ) HYPONYM-OF target tasks. action recognition ( temporal ) HYPONYM-OF target tasks. human ’s interactions USED-FOR representation learning. cues USED-FOR visual embedding. representation COMPARE self - supervised vision - only techniques. self - supervised vision - only techniques COMPARE representation. Task are representations of visual data, and computer vision. OtherScientificTerm is first person observations. ","This paper proposes a new representation learning method based on human interaction and attention cues for learning representations of visual data. The proposed method, called MoCo, is a self-supervised representation learning approach that uses human interactions and attention to learn a representation of the visual data from first person observations. The method is evaluated on three tasks: scene classification (semantic), action recognition (temporal) and depth estimation (geometric). MoCo is shown to outperform the state-of-the-art methods on all three tasks.","This paper proposes a new representation learning method based on human interaction and attention cues for learning representations of visual data. The proposed method, called MoCo, is a self-supervised representation learning approach that uses human interactions and attention to learn a representation of the visual data from first person observations. The method is evaluated on three tasks: scene classification (semantic), action recognition (temporal) and depth estimation (geometric). MoCo is shown to outperform the state-of-the-art methods on all three tasks."
1971,SP:6873a5e80e6142983c9bbd22931bfded7eed2f59,"pretrained model COMPARE model. model COMPARE pretrained model. generalization EVALUATE-FOR model. generalization EVALUATE-FOR pretrained model. learning rate USED-FOR pretraining. neural network training CONJUNCTION pretraining. pretraining CONJUNCTION neural network training. OtherScientificTerm are Negative pretraining, negative pretraining effect, learning process, learning task - level, discretization of data distribution, model - level, negative pretraining effects, and negative pretraining. Method is neural networks. Generic is interventions. ",This paper studies the effect of negative pretraining on the generalization ability of neural networks. The authors show that the learning rate of a neural network trained on the same task as the one trained on a different task is a function of the discretization of the data distribution. They also show that this discretisation of data distribution is responsible for the performance degradation of the pretrained model. ,This paper studies the effect of negative pretraining on the generalization ability of neural networks. The authors show that the learning rate of a neural network trained on the same task as the one trained on a different task is a function of the discretization of the data distribution. They also show that this discretisation of data distribution is responsible for the performance degradation of the pretrained model. 
1980,SP:5d27e5a301ed4f224fb2baecad77006a9fbb2189,"adversarially perturbed inputs FEATURE-OF classifier ’s robustness. bi - level optimization algorithm USED-FOR adversarially trained classifiers. bi - level optimization algorithm USED-FOR safe spots. ImageNet datasets EVALUATE-FOR adversarially trained classifiers. they USED-FOR smoothed classifiers. empirical and certified robustness EVALUATE-FOR smoothed classifiers. empirical and certified robustness EVALUATE-FOR they. safe spot inducing model training scheme CONJUNCTION safe spot generation method. safe spot generation method CONJUNCTION safe spot inducing model training scheme. out - of - distribution detection algorithm USED-FOR near - distribution outliers. safe spot generation method USED-FOR out - of - distribution detection algorithm. Task is adversarial defense. Method are classifier, and classifiers. Material is natural images. OtherScientificTerm is adversarial attacks. ","This paper proposes a bi-level optimization algorithm to generate safe spots for adversarially trained classifiers. The proposed method is based on the idea of out-of-distribution detection, which is an extension of the safe spot generation method. The authors also propose a safe spot inducing model training scheme to improve the robustness of classifiers against adversarial attacks. Experiments on ImageNet and CIFAR-10 demonstrate the effectiveness of the proposed method.","This paper proposes a bi-level optimization algorithm to generate safe spots for adversarially trained classifiers. The proposed method is based on the idea of out-of-distribution detection, which is an extension of the safe spot generation method. The authors also propose a safe spot inducing model training scheme to improve the robustness of classifiers against adversarial attacks. Experiments on ImageNet and CIFAR-10 demonstrate the effectiveness of the proposed method."
1989,SP:1350ab543b6a5cf579827835fb27011751cc047f,"regularities CONJUNCTION order. order CONJUNCTION regularities. regularities FEATURE-OF temporal dimension. order FEATURE-OF temporal dimension. grid based convolutions USED-FOR video processing. point spatio - temporal ( PST ) convolution USED-FOR informative representations of point cloud sequences. PST convolution USED-FOR point cloud sequences. temporal convolution USED-FOR dynamics of the spatial regions. spatial convolution USED-FOR local structure. time dimension FEATURE-OF dynamics of the spatial regions. deep network USED-FOR features of point cloud sequences. PST convolution PART-OF deep network. PST convolution USED-FOR features of point cloud sequences. hierarchical manner USED-FOR point cloud sequences. PSTNet HYPONYM-OF deep network. PSTNet USED-FOR point cloud sequences. 3D action recognition CONJUNCTION 4D semantic segmentation datasets. 4D semantic segmentation datasets CONJUNCTION 3D action recognition. 4D semantic segmentation datasets EVALUATE-FOR PSTNet. Material is Point cloud sequences. OtherScientificTerm are spatial dimension, and 3D space. ","This paper proposes a new convolutional neural network architecture for point cloud sequences. The proposed architecture is based on the idea of point spatio-temporal (PST) convolution, which is an extension of grid-based convolutions for video processing. In particular, the authors propose a hierarchical structure of the convolution layer, which can be used to capture the dynamics of the spatial regions. The authors show that the proposed architecture outperforms the state-of-the-art in 3D action recognition and 4D semantic segmentation tasks.","This paper proposes a new convolutional neural network architecture for point cloud sequences. The proposed architecture is based on the idea of point spatio-temporal (PST) convolution, which is an extension of grid-based convolutions for video processing. In particular, the authors propose a hierarchical structure of the convolution layer, which can be used to capture the dynamics of the spatial regions. The authors show that the proposed architecture outperforms the state-of-the-art in 3D action recognition and 4D semantic segmentation tasks."
1998,SP:a808583e924f85ec847c6b2597bae5c3eeec0ca7,"source TTS model USED-FOR personal voice. Custom voice HYPONYM-OF text to speech ( TTS ) service. text to speech ( TTS ) service PART-OF commercial speech platforms. Custom voice USED-FOR TTS adaptation. adaptive TTS system USED-FOR customization of new voices. AdaSpeech HYPONYM-OF adaptive TTS system. utterance and phoneme level FEATURE-OF acoustic information. acoustic encoder USED-FOR utterance - level vector. acoustic predictor USED-FOR phonemelevel vectors. one USED-FOR phoneme - level vectors. acoustic encoder CONJUNCTION one. one CONJUNCTION acoustic encoder. acoustic encoder USED-FOR phoneme - level vectors. utterance - level vector USED-FOR inference. adaptation parameters CONJUNCTION voice quality. voice quality CONJUNCTION adaptation parameters. speaker embedding USED-FOR adaptation. mel - spectrogram decoder PART-OF AdaSpeech. part CONJUNCTION speaker embedding. speaker embedding CONJUNCTION part. conditional layer normalization USED-FOR mel - spectrogram decoder. conditional layer normalization PART-OF AdaSpeech. acoustic conditions PART-OF LibriTTS. acoustic conditions FEATURE-OF VCTK and LJSpeech datasets. LibriTTS datasets USED-FOR source TTS model. VCTK and LJSpeech datasets USED-FOR it. adaptation data USED-FOR it. AdaSpeech COMPARE baseline methods. baseline methods COMPARE AdaSpeech. adaptation quality EVALUATE-FOR baseline methods. AdaSpeech USED-FOR custom voice. adaptation quality EVALUATE-FOR AdaSpeech. Method is adaptation model. Material are source speech data, and audio samples. OtherScientificTerm is memory usage. ","This paper proposes AdaSpeech, an adaptive TTS system for text-to-speech (TTS) adaptation. The proposed system is built on top of the existing AdaTTS framework. The main idea is to combine the acoustic encoder and speaker embedding into a single model, which is then used to learn the utterance-level and phoneme-level vectors of the source TTS model. The authors also propose a conditional layer normalization method to improve the performance of the decoder. Experiments on LibriTTS and VCTK show that the proposed system outperforms AdaTNS and AdaTSP.","This paper proposes AdaSpeech, an adaptive TTS system for text-to-speech (TTS) adaptation. The proposed system is built on top of the existing AdaTTS framework. The main idea is to combine the acoustic encoder and speaker embedding into a single model, which is then used to learn the utterance-level and phoneme-level vectors of the source TTS model. The authors also propose a conditional layer normalization method to improve the performance of the decoder. Experiments on LibriTTS and VCTK show that the proposed system outperforms AdaTNS and AdaTSP."
2007,SP:66f56cc202aed1382a342e13ecfe0c5af87f6fee,sparse networks COMPARE dense neural architectures. dense neural architectures COMPARE sparse networks. regularizers USED-FOR dense networks. activation functions CONJUNCTION regularizers. regularizers CONJUNCTION activation functions. optimizers CONJUNCTION activation functions. activation functions CONJUNCTION optimizers. activation functions USED-FOR dense networks. regularizers USED-FOR sparse networks. gradient flow USED-FOR sparse networks. training regime USED-FOR gradient flow. tailoring optimization USED-FOR sparse networks. OtherScientificTerm is initialization. Task is training sparse networks. ,"This paper studies the problem of training sparse neural networks with regularizers and optimizers. The authors propose a new training regime for training sparse networks. The training regime is based on gradient flow, which is a generalization of the gradient flow for dense networks. They show that the training regime can be used to train sparse networks with different regularizers, optimizers, and activation functions. They also provide a theoretical analysis of the convergence rate of the proposed training regime.","This paper studies the problem of training sparse neural networks with regularizers and optimizers. The authors propose a new training regime for training sparse networks. The training regime is based on gradient flow, which is a generalization of the gradient flow for dense networks. They show that the training regime can be used to train sparse networks with different regularizers, optimizers, and activation functions. They also provide a theoretical analysis of the convergence rate of the proposed training regime."
2016,SP:d9f17344cd266b16a70c37d891b2c64a6d454908,"Graph Convolutional Neural Networks ( GCN ) HYPONYM-OF message passing algorithms. Label Propagation ( LPA ) CONJUNCTION Graph Convolutional Neural Networks ( GCN ). Graph Convolutional Neural Networks ( GCN ) CONJUNCTION Label Propagation ( LPA ). Label Propagation ( LPA ) HYPONYM-OF message passing algorithms. graphs USED-FOR message passing algorithms. GCN USED-FOR node feature information. LPA USED-FOR node label information. LPA CONJUNCTION GCN. GCN CONJUNCTION LPA. LPA CONJUNCTION GCN. GCN CONJUNCTION LPA. LPA USED-FOR node classification. GCN CONJUNCTION LPA. LPA CONJUNCTION GCN. end - to - end model USED-FOR node classification. GCN USED-FOR node classification. GCN PART-OF end - to - end model. LPA PART-OF end - to - end model. LPA USED-FOR GCN. LPA USED-FOR regularization. model COMPARE feature - based attention models. feature - based attention models COMPARE model. attention weights USED-FOR model. node labels USED-FOR attention weights. real - world graphs EVALUATE-FOR model. model COMPARE GCN - based methods. GCN - based methods COMPARE model. real - world graphs EVALUATE-FOR GCN - based methods. node classification accuracy EVALUATE-FOR GCN - based methods. node classification accuracy EVALUATE-FOR model. OtherScientificTerm are edges of the graph, feature / label, feature / label influence, and edge weights. Task are feature / label smoothing, and classification. Method is unified model. ","This paper proposes a unified model for node classification and label smoothing. The proposed model is based on GCN, LPA and label propagation. The authors show that the proposed model outperforms GCN and LPA on several benchmark datasets. ","This paper proposes a unified model for node classification and label smoothing. The proposed model is based on GCN, LPA and label propagation. The authors show that the proposed model outperforms GCN and LPA on several benchmark datasets. "
2025,SP:c5883e3a59e6575eff044251b38175a6ed024034,"VC - dimension CONJUNCTION Rademacher complexity ( R - Complexity ). Rademacher complexity ( R - Complexity ) CONJUNCTION VC - dimension. complexity EVALUATE-FOR classifier ’s function space. complexity FEATURE-OF generalization gap. generalization gap EVALUATE-FOR classifier ’s function space. classifier CONJUNCTION generator function spaces. generator function spaces CONJUNCTION classifier. R - Complexity EVALUATE-FOR generator function spaces. R - Complexity EVALUATE-FOR classifier. generalization performance EVALUATE-FOR generator space. invariances CONJUNCTION local smoothness. local smoothness CONJUNCTION invariances. generator space USED-FOR constraints. invariances HYPONYM-OF constraints. local smoothness HYPONYM-OF constraints. classifier CONJUNCTION generator. generator CONJUNCTION classifier. invariance co - complexity term CONJUNCTION dissociation co - complexity term. dissociation co - complexity term CONJUNCTION invariance co - complexity term. classifier USED-FOR generator. invariant transformations FEATURE-OF generator. invariance co - complexity term PART-OF It. invariant transformations FEATURE-OF classifier. dissociation co - complexity term PART-OF It. invariance co - complexity FEATURE-OF classifier. CNN architecture CONJUNCTION transformation - equivariant extensions. transformation - equivariant extensions CONJUNCTION CNN architecture. Co - complexity USED-FOR classifiers. Metric are generalization error bounds, generalization error, co - complexity, dissociation co - complexity, and training error. OtherScientificTerm are ground truth label generating function ( LGF ), LGF, ground truth labels, and function space. Generic is it. ","This paper studies the generalization gap between classifiers and generator function spaces in terms of the co-complexity of the classifier's function space and the generator function space. The authors show that the generalisation gap is bounded by a term called dissociation co-computation, which is defined as the difference between the training error of a classifier and a generator. They also show that this term can be used to define a new generalization error bound for classifiers that is lower than the one for the generator. ","This paper studies the generalization gap between classifiers and generator function spaces in terms of the co-complexity of the classifier's function space and the generator function space. The authors show that the generalisation gap is bounded by a term called dissociation co-computation, which is defined as the difference between the training error of a classifier and a generator. They also show that this term can be used to define a new generalization error bound for classifiers that is lower than the one for the generator. "
2034,SP:9bb36be61f1d4db88d806092219eba39bf1b99db,"neural network quantization USED-FOR task. end - toend retraining USED-FOR neural network quantization. Post - training Quantization ( PTQ ) HYPONYM-OF neural network quantization. PTQ USED-FOR quantized models. PTQ COMPARE Quantization - Aware Training ( QAT ). Quantization - Aware Training ( QAT ) COMPARE PTQ. Quantization - Aware Training ( QAT ) USED-FOR quantized models. bitwidth FEATURE-OF PTQ. BRECQ HYPONYM-OF PTQ framework. neural networks USED-FOR BRECQ. crosslayer dependency CONJUNCTION generalization error. generalization error CONJUNCTION crosslayer dependency. crosslayer dependency EVALUATE-FOR BRECQ. generalization error EVALUATE-FOR BRECQ. mixed precision technique PART-OF framework. handcrafted and searched neural architectures USED-FOR image classification and object detection tasks. 4 - bit ResNet CONJUNCTION MobileNetV2. MobileNetV2 CONJUNCTION 4 - bit ResNet. PTQ COMPARE QAT. QAT COMPARE PTQ. MobileNetV2 COMPARE QAT. QAT COMPARE MobileNetV2. MobileNetV2 EVALUATE-FOR PTQ. 4 - bit ResNet EVALUATE-FOR PTQ. OtherScientificTerm are INT2, quantization, and inter - layer and intra - layer sensitivity. Metric is second - order error. ","This paper proposes a new method for post-training quantization, BRECQ, which is an end-to-end retraining method for neural network quantization. The main idea is to use a mixed precision technique to reduce the second-order error of the quantized model. The authors show that the proposed method outperforms QAT and PTQ in terms of generalization error, cross-layer dependency, and inter-layer and intra-layer sensitivity. Experiments are conducted on a number of image classification and object detection tasks.","This paper proposes a new method for post-training quantization, BRECQ, which is an end-to-end retraining method for neural network quantization. The main idea is to use a mixed precision technique to reduce the second-order error of the quantized model. The authors show that the proposed method outperforms QAT and PTQ in terms of generalization error, cross-layer dependency, and inter-layer and intra-layer sensitivity. Experiments are conducted on a number of image classification and object detection tasks."
2043,SP:3035318ac36cad693a5e4ee7bed43db8df6fb492,"large labeled datasets USED-FOR deep learning deployment. distributional shift CONJUNCTION labeling cost. labeling cost CONJUNCTION distributional shift. medicine HYPONYM-OF real - world applications. dataset properties USED-FOR calibration. dataset properties COMPARE architecture. architecture COMPARE dataset properties. common strategies USED-FOR class imbalance. dataset properties USED-FOR calibration. dataset curation USED-FOR calibration. Method are Neural networks, and neural networks. Metric are downstream prediction accuracy, accuracy, and calibration error. OtherScientificTerm are model uncertainty, label quality, label noise, small dataset sizes, and network expressivity. Generic is complementary approach. Task is dataset imbalance. ","This paper studies the problem of class imbalance in deep learning. The authors propose a method to mitigate the class imbalance problem by curating the training data. The method is based on the idea that the label quality of a classifier is affected by the size of the training dataset, and the authors show that the proposed method can be used to mitigate class imbalance.","This paper studies the problem of class imbalance in deep learning. The authors propose a method to mitigate the class imbalance problem by curating the training data. The method is based on the idea that the label quality of a classifier is affected by the size of the training dataset, and the authors show that the proposed method can be used to mitigate class imbalance."
2052,SP:17d90f9d3f5891ac56f5ed6375a21d0c1517fd62,"information exchange CONJUNCTION cooperation. cooperation CONJUNCTION information exchange. Effective communication USED-FOR information exchange. Effective communication USED-FOR cooperation. symbolic channels USED-FOR emergent communication. 3D environment FEATURE-OF joints. non - uniform distribution of intents CONJUNCTION commonknowledge energy cost. commonknowledge energy cost CONJUNCTION non - uniform distribution of intents. agents USED-FOR protocols. OtherScientificTerm are discrete cheap - talk channels, and latent feature. Method is emergent protocols. Generic is modality. Material is training curricula. ","This paper studies the problem of emergent communication in the context of 3D joints. The authors propose a new method to train agents to communicate in a 3D environment. The key idea is to use symbolic channels, which are discrete cheap-talk channels, to share information between agents. They show that the proposed method is able to achieve better performance than existing methods in terms of communication efficiency. They also show that their method can be applied to a variety of environments. ","This paper studies the problem of emergent communication in the context of 3D joints. The authors propose a new method to train agents to communicate in a 3D environment. The key idea is to use symbolic channels, which are discrete cheap-talk channels, to share information between agents. They show that the proposed method is able to achieve better performance than existing methods in terms of communication efficiency. They also show that their method can be applied to a variety of environments. "
2061,SP:5ba686e2eef369fa49b10ba3f41f102740836859,Generating high quality uncertainty estimates USED-FOR sequential regression. deep recurrent networks HYPONYM-OF sequential regression. real world non - stationary signals CONJUNCTION drift. drift CONJUNCTION real world non - stationary signals. method USED-FOR symmetric and asymmetric uncertainty estimates. method COMPARE baselines. baselines COMPARE method. drift and non drift scenarios EVALUATE-FOR baselines. sequential regression USED-FOR real - world applications. modeling toolbox USED-FOR sequential uncertainty quantification. Generic is approaches. OtherScientificTerm is stationarity. ,This paper proposes a new method for sequential regression. The proposed method is based on deep recurrent networks and is able to generate high-quality uncertainty estimates for both symmetric and asymmetric uncertainty estimates. The method is evaluated on both drift and non-divergence scenarios. The results show that the proposed method outperforms the baselines in both non-stationary and drift scenarios.,This paper proposes a new method for sequential regression. The proposed method is based on deep recurrent networks and is able to generate high-quality uncertainty estimates for both symmetric and asymmetric uncertainty estimates. The method is evaluated on both drift and non-divergence scenarios. The results show that the proposed method outperforms the baselines in both non-stationary and drift scenarios.
2070,SP:0a58694abd6898a925b1d917ad2a68eefd0567e9,metric measure spaces USED-FOR machine learning problems. metric space HYPONYM-OF Comparing metric measure spaces. probability distribution FEATURE-OF metric space. Gromov - Wasserstein ( GW ) distance HYPONYM-OF metric measure spaces. probability distribution FEATURE-OF metric measure spaces. distance CONJUNCTION upper - bounding relaxation. upper - bounding relaxation CONJUNCTION distance. upper - bounding relaxation HYPONYM-OF Unbalanced Gromov - Wasserstein formulations. distance HYPONYM-OF Unbalanced Gromov - Wasserstein formulations. They USED-FOR metric spaces. isometries FEATURE-OF positive measures. positive measures FEATURE-OF metric spaces. formulation USED-FOR positive and definite divergence. relaxation of the mass conservation constraint USED-FOR positive and definite divergence. quadratically - homogeneous divergence USED-FOR relaxation of the mass conservation constraint. quadratically - homogeneous divergence USED-FOR positive and definite divergence. entropic regularization approach USED-FOR large scale optimal transport problems. entropic regularization approach USED-FOR divergence. parallelizable and GPU - friendly iterative scheme USED-FOR non - convex optimization problem. distance FEATURE-OF mm - spaces. distance USED-FOR formulation. isometries FEATURE-OF distance. isometries FEATURE-OF mm - spaces. conic lifting USED-FOR distance. synthetic examples CONJUNCTION domain adaptation data. domain adaptation data CONJUNCTION synthetic examples. unbalanced divergence USED-FOR ML. domain adaptation data CONJUNCTION Positive - Unlabeled learning task. Positive - Unlabeled learning task CONJUNCTION domain adaptation data. synthetic examples CONJUNCTION Positive - Unlabeled learning task. Positive - Unlabeled learning task CONJUNCTION synthetic examples. Task is quadratic assignment problem. OtherScientificTerm is GW distance. ,"This paper studies the problem of unbalanced Gromov-Wasserstein (GW) distance, which is an important problem in machine learning. The authors propose a new formulation of the GW distance, called quadratically-homogeneous GW distance (QGWD), which is based on the relaxation of the mass conservation constraint. They show that QGWD can be used as a parallelizable and GPU-friendly iterative scheme for non-convex optimization problems. They also show that the quadratic-homogenous GWD can also be used for large scale optimal transport problems.","This paper studies the problem of unbalanced Gromov-Wasserstein (GW) distance, which is an important problem in machine learning. The authors propose a new formulation of the GW distance, called quadratically-homogeneous GW distance (QGWD), which is based on the relaxation of the mass conservation constraint. They show that QGWD can be used as a parallelizable and GPU-friendly iterative scheme for non-convex optimization problems. They also show that the quadratic-homogenous GWD can also be used for large scale optimal transport problems."
2079,SP:47dcefd5515e772f29e03219c01713e2403643ce,"computational cost CONJUNCTION memory consumption. memory consumption CONJUNCTION computational cost. Network pruning USED-FOR memory consumption. compression ratios EVALUATE-FOR saliencybased pruning. sparse parameters FEATURE-OF well - trainable networks. pruning method USED-FOR pruned networks. all - alive pruning ( AAP ) HYPONYM-OF pruning method. trainable weights USED-FOR pruned networks. AAP USED-FOR saliency - based pruning methods. AAP USED-FOR model architectures. saliency - based pruning methods CONJUNCTION model architectures. model architectures CONJUNCTION saliency - based pruning methods. one - shot pruning CONJUNCTION dynamic pruning. dynamic pruning CONJUNCTION one - shot pruning. iterative pruning CONJUNCTION one - shot pruning. one - shot pruning CONJUNCTION iterative pruning. pruning methods USED-FOR AAP. dynamic pruning USED-FOR AAP. accuracy EVALUATE-FOR AAP. benchmark datasets EVALUATE-FOR AAP. dynamic pruning HYPONYM-OF pruning methods. iterative pruning HYPONYM-OF pruning methods. one - shot pruning HYPONYM-OF pruning methods. Material is low - resource devices. Metric is accuracy loss. Method is network pruning. OtherScientificTerm are model capacity, and dead connections. ","This paper proposes a new pruning method, called all-alive pruning (AAP), to reduce the computational cost of saliency-based pruning methods. The main contribution of the paper is to propose a new method to prune the weights of well-trained networks with sparse parameters. The authors show that the proposed method is able to achieve better compression ratios compared to existing saliency based pruning algorithms. They also show that AAP can be applied to dynamic pruning, iterative pruning and one-shot pruning.","This paper proposes a new pruning method, called all-alive pruning (AAP), to reduce the computational cost of saliency-based pruning methods. The main contribution of the paper is to propose a new method to prune the weights of well-trained networks with sparse parameters. The authors show that the proposed method is able to achieve better compression ratios compared to existing saliency based pruning algorithms. They also show that AAP can be applied to dynamic pruning, iterative pruning and one-shot pruning."
2088,SP:9eb7b946e00085b89844c485bcd94a392146d2b7,"Generative Adversarial Net ( GAN ) USED-FOR latent space. approaches USED-FOR task. Generative Adversarial Net ( GAN ) USED-FOR latent - space transformations. latent space CONJUNCTION latent - space transformations. latent - space transformations CONJUNCTION latent space. Generative Adversarial Net ( GAN ) USED-FOR approaches. Generative Adversarial Net ( GAN ) USED-FOR task. global image identity CONJUNCTION diminished photo - realism. diminished photo - realism CONJUNCTION global image identity. attribute edits CONJUNCTION global image identity. global image identity CONJUNCTION attribute edits. content loss CONJUNCTION adversarial loss. adversarial loss CONJUNCTION content loss. maintenance of image identity CONJUNCTION photo - realism. photo - realism CONJUNCTION maintenance of image identity. attribute regression USED-FOR transformation functions. adversarial loss USED-FOR maintenance of image identity. content loss USED-FOR maintenance of image identity. quantitative evaluation strategies EVALUATE-FOR controllable editing. image identity CONJUNCTION realism. realism CONJUNCTION image identity. model USED-FOR singleand multipleattribute editing. model USED-FOR targeted image manipulation. natural and synthetic images EVALUATE-FOR model. Task are Controllable semantic image editing, and qualitative evaluation. OtherScientificTerm are image attributes, and multiple attribute transformations. ","This paper proposes a GAN-based model for controllable semantic image editing. The proposed model is based on the GAN framework. The model is trained on a set of synthetic and real-world images, and is evaluated on both quantitative and qualitative evaluation. The paper shows that the proposed model achieves state-of-the-art performance on a number of image editing tasks.","This paper proposes a GAN-based model for controllable semantic image editing. The proposed model is based on the GAN framework. The model is trained on a set of synthetic and real-world images, and is evaluated on both quantitative and qualitative evaluation. The paper shows that the proposed model achieves state-of-the-art performance on a number of image editing tasks."
2097,SP:d9d9d5ade0253be2733d8b035f755ebf82e7e18b,"mode dropping CONJUNCTION unstable training. unstable training CONJUNCTION mode dropping. Generative Adversarial Networks ( GAN ) USED-FOR synthesizing sequences of discrete elements. unstable training HYPONYM-OF synthesizing sequences of discrete elements. mode dropping HYPONYM-OF synthesizing sequences of discrete elements. binary classifier PART-OF discriminator. Feature Statistics Alignment ( FSA ) paradigm USED-FOR fine - grained signals. latent high - dimensional representation space FEATURE-OF fine - grained signals. FSA USED-FOR mean statistics. finite - dimensional feature space FEATURE-OF real data. approach USED-FOR discrete sequence generation. synthetic and real benchmark datasets EVALUATE-FOR approach. quantitative evaluation EVALUATE-FOR approach. Gumbel - Softmax based GAN framework USED-FOR sequence generation. feature alignment regularization USED-FOR Gumbel - Softmax based GAN framework. OtherScientificTerm are learning signals, and binary classification feedback. Task is adversarial training. ",This paper proposes a novel GAN framework for discrete sequence generation. The proposed framework is based on the Feature Statistics Alignment (FSA) paradigm. The authors propose a novel feature alignment regularization scheme to improve the performance of GANs. The paper also proposes a new binary classifier for generating discrete sequences of discrete elements. Experiments on synthetic and real-world datasets demonstrate the effectiveness of the proposed method. ,This paper proposes a novel GAN framework for discrete sequence generation. The proposed framework is based on the Feature Statistics Alignment (FSA) paradigm. The authors propose a novel feature alignment regularization scheme to improve the performance of GANs. The paper also proposes a new binary classifier for generating discrete sequences of discrete elements. Experiments on synthetic and real-world datasets demonstrate the effectiveness of the proposed method. 
2106,SP:3ffa34b54779998f473f4e9a52287bcd0485cec8,"progressive rewards FEATURE-OF reinforcement learning tasks. tasks HYPONYM-OF reinforcement learning tasks. Spectral DQN USED-FOR reward. it COMPARE benchmarks. benchmarks COMPARE it. it COMPARE benchmarks. benchmarks COMPARE it. it COMPARE it. it COMPARE it. Spectral DQN USED-FOR reward progressivity. OtherScientificTerm are rewards, training loss, and extreme reward progressivity. Method are value - based deep reinforcement learning agents, and value - based methods. Generic are agent, and approach. Material is Atari games. ","This paper studies the problem of reward progressivity in value-based reinforcement learning. The authors propose Spectral DQN, a method to improve the performance of reinforcement learning agents on Atari games. The main contribution of the paper is the introduction of Spectral QN, which is an extension of the Spectral-DQN framework to Atari games, which has been used in previous work. In particular, the authors show that the proposed method is able to achieve state-of-the-art performance on the Atari games and outperforms the baselines.","This paper studies the problem of reward progressivity in value-based reinforcement learning. The authors propose Spectral DQN, a method to improve the performance of reinforcement learning agents on Atari games. The main contribution of the paper is the introduction of Spectral QN, which is an extension of the Spectral-DQN framework to Atari games, which has been used in previous work. In particular, the authors show that the proposed method is able to achieve state-of-the-art performance on the Atari games and outperforms the baselines."
2115,SP:bff215c695b302ce31311f2dd105dace06307cfc,representations USED-FOR task. usable information FEATURE-OF representation. deep network USED-FOR representation. minimal sufficient representations USED-FOR task. learning - rate CONJUNCTION small batch size. small batch size CONJUNCTION learning - rate. learning - rate FEATURE-OF Stochastic Gradient Descent. Stochastic Gradient Descent USED-FOR implicit regularization. neuroscience literature USED-FOR perceptual decision - making tasks. Generic is it. Method is minimal sufficient representation. OtherScientificTerm is learning dynamics. Task is image classification tasks. ,This paper studies the learning dynamics of Stochastic Gradient Descent (SGD) in deep neural networks. The authors show that SGD can be used to learn representations that are sufficient for a given task. They also show that the learning rate of SGD is a function of the learning-rate and the batch size of the training data. They show that this is a regularization term for SGD and that it can be applied to any learning dynamics. ,This paper studies the learning dynamics of Stochastic Gradient Descent (SGD) in deep neural networks. The authors show that SGD can be used to learn representations that are sufficient for a given task. They also show that the learning rate of SGD is a function of the learning-rate and the batch size of the training data. They show that this is a regularization term for SGD and that it can be applied to any learning dynamics. 
2124,SP:c175ea892c831c2d0c38aded9b5e86d25b86545c,"robust adversarial learning CONJUNCTION inverse reinforcement learning. inverse reinforcement learning CONJUNCTION robust adversarial learning. Min - max optimization USED-FOR machine learning problems. nonconvexstrongly - concave min - max optimization HYPONYM-OF machine learning problems. robust adversarial learning HYPONYM-OF machine learning problems. inverse reinforcement learning HYPONYM-OF machine learning problems. variance reduction algorithm SREDA USED-FOR problem. accuracy level FEATURE-OF optimal complexity dependence. initialization accuracy CONJUNCTION -dependent stepsize. -dependent stepsize CONJUNCTION initialization accuracy. -dependent stepsize USED-FOR per - iteration progress. convergence guarantee EVALUATE-FOR SREDA. initialization accuracy USED-FOR convergence guarantee. analytical framework USED-FOR SREDA. SREDA - Boost COMPARE SREDA. SREDA COMPARE SREDA - Boost. SREDA - Boost USED-FOR zeroth - order variance reduction algorithm. ZO - SREDA - Boost COMPARE complexity dependence on. complexity dependence on COMPARE ZO - SREDA - Boost. ZO - SREDA - Boost HYPONYM-OF zeroth - order variance reduction algorithm. variance reduction technique USED-FOR zeroth - order algorithm. zeroth - order algorithm USED-FOR min - max optimization problems. variance reduction technique USED-FOR min - max optimization problems. OtherScientificTerm are restrictive initialization requirement, and gradients. ","This paper studies the variance reduction problem in nonconvex-strongly-concave min-max optimization. The authors propose a new variance reduction algorithm SREDA-Boost, which is based on the idea of optimal complexity dependence on the initialization accuracy and the per-iteration progress of the algorithm. They show that the convergence guarantee of the proposed algorithm is guaranteed by the convergence rate of the per iteration progress. They also provide a theoretical analysis of the convergence of the method. ","This paper studies the variance reduction problem in nonconvex-strongly-concave min-max optimization. The authors propose a new variance reduction algorithm SREDA-Boost, which is based on the idea of optimal complexity dependence on the initialization accuracy and the per-iteration progress of the algorithm. They show that the convergence guarantee of the proposed algorithm is guaranteed by the convergence rate of the per iteration progress. They also provide a theoretical analysis of the convergence of the method. "
2133,SP:c1617e79182c6d06c611ced9d892d7b2da5fd9eb,"COCO EVALUATE-FOR state - of - the - art. it USED-FOR models. Example based object detection USED-FOR Detection of Novel Objects. Task are one - shot object detection, few - shot learning, and data annotation. OtherScientificTerm are generalization gap, Object categories, and object categories. Metric is generalization. Method are few - shot detection models, and metric learning approaches. ","This paper studies the problem of few-shot object detection, where the goal is to detect novel objects in a single shot. The authors propose a metric learning approach to measure the generalization gap between different classes of objects. The metric is based on the observation that the generalisation gap between two classes of object can be reduced to zero when the classifier is trained on the same set of objects, and to zero if the classifiers are trained on different sets of objects in the same class. The paper shows that the metric can be used to improve the performance of the model on COCO, a novel object detection task. ","This paper studies the problem of few-shot object detection, where the goal is to detect novel objects in a single shot. The authors propose a metric learning approach to measure the generalization gap between different classes of objects. The metric is based on the observation that the generalisation gap between two classes of object can be reduced to zero when the classifier is trained on the same set of objects, and to zero if the classifiers are trained on different sets of objects in the same class. The paper shows that the metric can be used to improve the performance of the model on COCO, a novel object detection task. "
2149,SP:a472784ddb36f88e6e468f282fbd7ad74f8f7d75,"3D representations USED-FOR modeling clean mesh surfaces. occupancy fields CONJUNCTION signed distance functions ( SDF ). signed distance functions ( SDF ) CONJUNCTION occupancy fields. Implicit neural shape functions USED-FOR 3D representations. signed distance functions ( SDF ) HYPONYM-OF Implicit neural shape functions. occupancy fields HYPONYM-OF Implicit neural shape functions. representations USED-FOR single - view object reconstruction. Existing approaches USED-FOR single - view object reconstruction. representations USED-FOR Existing approaches. supervision signals USED-FOR Existing approaches. spatial gradient FEATURE-OF implicit field. supervision USED-FOR single - view reconstruction. feature map USED-FOR spatial gradient. real - world scenes USED-FOR single view implicit surface reconstructions. scanned dataset USED-FOR single view implicit surface reconstructions. ShapeNet CONJUNCTION ScannetV2. ScannetV2 CONJUNCTION ShapeNet. ShapeNet HYPONYM-OF datasets. ScannetV2 HYPONYM-OF datasets. model USED-FOR 3D implicit surface reconstruction. RGB image USED-FOR model. Task are real - world scenarios, and training on large - scale scenes. Material are ideal watertight geometric training data, large - scale scenes, internet, Internet, and pix3d dataset. OtherScientificTerm are training signal, spatial gradients, feature maps, and dense 3D supervision. Generic is this. Method are Pix3D, and DGS module. ","This paper proposes a new method for 3D implicit surface reconstruction. The proposed method is based on the DGS module, which is trained on a large-scale dataset, Pix3D, and is able to achieve state-of-the-art results on ScannetV2, ShapeNet, and Pix3d. The method is evaluated on both synthetic and real-world datasets.","This paper proposes a new method for 3D implicit surface reconstruction. The proposed method is based on the DGS module, which is trained on a large-scale dataset, Pix3D, and is able to achieve state-of-the-art results on ScannetV2, ShapeNet, and Pix3d. The method is evaluated on both synthetic and real-world datasets."
2165,SP:b89ec0b50475bfb23399719ca36aa137b389fbf6,"deep learning algorithms CONJUNCTION distributed training. distributed training CONJUNCTION deep learning algorithms. distributed training CONJUNCTION hardware design. hardware design CONJUNCTION distributed training. hardware design USED-FOR large models. GPT-3 CONJUNCTION Switch Transformer. Switch Transformer CONJUNCTION GPT-3. distributed training USED-FOR large models. Switch Transformer HYPONYM-OF extreme - scale models. GPT-3 HYPONYM-OF extreme - scale models. limited resources USED-FOR extreme - scale model training. memory footprint USED-FOR extreme - scale model training. Pseudo - to - Real USED-FOR high - memoryfootprint - required large models. training strategy USED-FOR high - memoryfootprint - required large models. Pseudo - to - Real HYPONYM-OF training strategy. Pseudo - to - Real CONJUNCTION large models. large models CONJUNCTION Pseudo - to - Real. architecture of sequential layers USED-FOR large models. GPUs USED-FOR state - of - the - art. Granular CPU offloading USED-FOR CPU memory. technique USED-FOR CPU memory. Granular CPU offloading HYPONYM-OF technique. Metric are model convergence, and carbon footprint. Method is large model. OtherScientificTerm is GPU utilities. Task is greener AI. ","This paper studies the problem of reducing the memory footprint of large models. The authors propose a new training strategy, Pseudo-to-Real (PTR), which is based on the idea of sequential layers. They show that the proposed method can reduce the number of CPUs required for training large models by up to 50% while maintaining the same performance. They also show that PTR can be applied to existing GPU architectures.","This paper studies the problem of reducing the memory footprint of large models. The authors propose a new training strategy, Pseudo-to-Real (PTR), which is based on the idea of sequential layers. They show that the proposed method can reduce the number of CPUs required for training large models by up to 50% while maintaining the same performance. They also show that PTR can be applied to existing GPU architectures."
2181,SP:cd1e11b270f74d5dca9efd9fe1903c0a24bcba12,"Energy - based models ( EBMs ) HYPONYM-OF generative models. maximum likelihood estimation USED-FOR generative models. Gibbs distribution USED-FOR energy. Fenchel duality USED-FOR variational principles. variational principles USED-FOR maximum likelihood EBMs. shallow overparametrized neural network energies USED-FOR maximum likelihood EBMs. shallow overparametrized neural network energies USED-FOR variational principles. dual formulation USED-FOR training algorithm. particles CONJUNCTION neurons. neurons CONJUNCTION particles. particles PART-OF sample space. parameter space FEATURE-OF neurons. dual formulation USED-FOR active regime. maximum likelihood CONJUNCTION score matching training. score matching training CONJUNCTION maximum likelihood. intermediate parameter setups USED-FOR dual algorithm. Generic are approach, and algorithm. ","This paper proposes a dual formulation of maximum likelihood estimation for energy-based models (EBMs). The dual formulation is based on the Fenchel duality of variational principles. The authors show that the dual formulation can be applied to both the active and passive regime of EBMs. They show that for the active regime, the proposed dual formulation converges to the optimal solution in terms of the maximum likelihood. For the passive regime, they show that they can converge to an optimal solution. They also show that their dual formulation generalizes well to the score matching setting. ","This paper proposes a dual formulation of maximum likelihood estimation for energy-based models (EBMs). The dual formulation is based on the Fenchel duality of variational principles. The authors show that the dual formulation can be applied to both the active and passive regime of EBMs. They show that for the active regime, the proposed dual formulation converges to the optimal solution in terms of the maximum likelihood. For the passive regime, they show that they can converge to an optimal solution. They also show that their dual formulation generalizes well to the score matching setting. "
2197,SP:4ff82f679a321ed61e02c50d5997c4e179441a0e,"differentially private ERM USED-FOR convex functions. lower bounds FEATURE-OF convex functions. lower bounds FEATURE-OF differentially private ERM. logarithmic terms USED-FOR lower bounds. biased mean property USED-FOR fingerprinting codes. ` 2 loss function COMPARE linear functions. linear functions COMPARE ` 2 loss function. ` 2 loss function USED-FOR pure - DP. Method are approximate - DP, and DP - ERM. Material is constrained case. OtherScientificTerm are unconstrained case, auxiliary dimension, ` 2 loss, and one - way marginals. Generic is it. ","This paper studies the problem of differentially private ERM for convex functions. The authors prove lower bounds for the convex case and the unconstrained case. The lower bounds are based on the notion of `2 loss, which is defined as the sum of the logarithmic terms of the one-way marginals of a convex function. They show that under certain assumptions, the lower bounds can be obtained for the constrained case. They also show that the lower bound can be derived for the non-convex case. ","This paper studies the problem of differentially private ERM for convex functions. The authors prove lower bounds for the convex case and the unconstrained case. The lower bounds are based on the notion of `2 loss, which is defined as the sum of the logarithmic terms of the one-way marginals of a convex function. They show that under certain assumptions, the lower bounds can be obtained for the constrained case. They also show that the lower bound can be derived for the non-convex case. "
2213,SP:c4b4914d64e76427435bee0da345fe33b1db7d27,"gradient flow USED-FOR machine learning applications. Wasserstein metric FEATURE-OF space of probability densities. approach USED-FOR Wasserstein gradient flow. finite difference USED-FOR approach. finite difference USED-FOR Wasserstein gradient flow. scalable proximal gradient type algorithm USED-FOR Wasserstein gradient flow. variational formulation of the objective function USED-FOR JKO proximal map. variational formulation of the objective function USED-FOR method. primal - dual optimization USED-FOR JKO proximal map. heat equation CONJUNCTION porous medium equation. porous medium equation CONJUNCTION heat equation. framework USED-FOR Wasserstein gradient flows. porous medium equation HYPONYM-OF Wasserstein gradient flows. heat equation HYPONYM-OF Wasserstein gradient flows. OtherScientificTerm are grid, and inner and outer loops. Task is primal - dual problem. Generic is algorithm. ",This paper proposes a scalable proximal gradient type algorithm for Wasserstein gradient flow. The main idea is to use a variational formulation of the objective function to approximate the proximal map between the inner and outer loops. The authors show that the proposed method can be used to solve the primal-dual optimization problem. The proposed method is shown to outperform the baselines.,This paper proposes a scalable proximal gradient type algorithm for Wasserstein gradient flow. The main idea is to use a variational formulation of the objective function to approximate the proximal map between the inner and outer loops. The authors show that the proposed method can be used to solve the primal-dual optimization problem. The proposed method is shown to outperform the baselines.
2229,SP:01f652a6b323db3585376a3a8e975a73ec4fed0b,ML algorithm CONJUNCTION hyper - parameter configuration. hyper - parameter configuration CONJUNCTION ML algorithm. ML algorithm USED-FOR AutoML problem. approach USED-FOR meta - features. Optimal Transport procedure USED-FOR meta - features. MetaBu meta - features USED-FOR topology. hyper - parameter configurations USED-FOR AutoML. MetaBu meta - features USED-FOR AutoML systems. AutoSkLearn CONJUNCTION Probabilistic Matrix Factorization. Probabilistic Matrix Factorization CONJUNCTION AutoSkLearn. OpenML CC-18 benchmark EVALUATE-FOR AutoML systems. OpenML CC-18 benchmark EVALUATE-FOR MetaBu meta - features. AutoSkLearn HYPONYM-OF AutoML systems. Probabilistic Matrix Factorization HYPONYM-OF AutoML systems. topology USED-FOR intrinsic dimensionality. intrinsic dimensionality FEATURE-OF OpenML benchmark. MetaBu meta - features USED-FOR intrinsic dimensionality. MetaBu meta - features USED-FOR topology. Method is MetaBu. OtherScientificTerm is manually designed meta - features. ,"This paper proposes a meta-feature-based approach to learn meta-features for AutoML problems. The proposed approach is based on the Optimal Transport procedure (OTP), which is an extension of the meta-transport method for meta-learning. The main contribution of the paper is that the proposed method is able to learn a set of hyper-parameter configurations that can be used as meta-parameters for the optimization of AutoML systems. The method is evaluated on the OpenML CC-18 benchmark and is shown to outperform the baseline methods. ","This paper proposes a meta-feature-based approach to learn meta-features for AutoML problems. The proposed approach is based on the Optimal Transport procedure (OTP), which is an extension of the meta-transport method for meta-learning. The main contribution of the paper is that the proposed method is able to learn a set of hyper-parameter configurations that can be used as meta-parameters for the optimization of AutoML systems. The method is evaluated on the OpenML CC-18 benchmark and is shown to outperform the baseline methods. "
2245,SP:e789c71cef2094ff2bac51b523ca912f1f04c2c9,"Federated learning ( FL ) USED-FOR distributed learning framework. robustness FEATURE-OF models. Split - Mix FL strategy USED-FOR heterogeneous participants. base sub - networks USED-FOR customization. communication CONJUNCTION storage. storage CONJUNCTION communication. storage CONJUNCTION inference. inference CONJUNCTION storage. split - mix strategy USED-FOR customization. method COMPARE heterogeneous - architecture FL methods. heterogeneous - architecture FL methods COMPARE method. in - situ customization EVALUATE-FOR heterogeneous - architecture FL methods. in - situ customization EVALUATE-FOR method. Task is FL scenarios. OtherScientificTerm are hardware and inference dynamics, and inference requirements. Method are FL approaches, and FL. ",This paper proposes a method for heterogeneous federated learning (FL). The authors propose a split-mix FL strategy that allows heterogeneous participants to customize their own sub-networks. The authors show that the proposed method outperforms heterogeneous-architecture FL methods in terms of robustness and in-situ customization. They also show that their method can be applied to heterogeneous hardware and inference dynamics.,This paper proposes a method for heterogeneous federated learning (FL). The authors propose a split-mix FL strategy that allows heterogeneous participants to customize their own sub-networks. The authors show that the proposed method outperforms heterogeneous-architecture FL methods in terms of robustness and in-situ customization. They also show that their method can be applied to heterogeneous hardware and inference dynamics.
2261,SP:0fd50d89ffec376d136aa915c9c4e6ae281f5014,"extragradient - type algorithm USED-FOR nonconvex - nonconcave minimax problems. local solution USED-FOR general minimax problems. first order methods USED-FOR variational inequalities. problem class USED-FOR non - trivial structures. algorithms USED-FOR limit cycles. algorithm USED-FOR constrained and regularized problems. adaptive stepsize USED-FOR stepsizes. adaptive stepsize PART-OF algorithm. limit cycles FEATURE-OF operator. it USED-FOR training of generative adversarial networks. variant USED-FOR training of generative adversarial networks. stochastic oracles USED-FOR variant. monotone setting FEATURE-OF it. OtherScientificTerm are weak Minty variational inequality ( MVI ), and weak MVI. Generic is scheme. Method are generative adversarial networks, and stochastic algorithm. ","This paper studies the problem of nonconvex-nonconcave minimax minimax optimization. The authors propose an extension of the weak Minty variational inequality (MVI) algorithm, which is used to solve the local minimax problem in the monotone setting. The main contribution of this paper is to introduce a new variant of the MVI algorithm, called the weak MVI-based algorithm. The proposed algorithm is based on the idea of adaptive stepsize, which allows the algorithm to be applied to non-trivial problems. The algorithm is shown to outperform existing algorithms in the constrained and regularized setting, and is also shown to be able to be used to train generative adversarial networks.","This paper studies the problem of nonconvex-nonconcave minimax minimax optimization. The authors propose an extension of the weak Minty variational inequality (MVI) algorithm, which is used to solve the local minimax problem in the monotone setting. The main contribution of this paper is to introduce a new variant of the MVI algorithm, called the weak MVI-based algorithm. The proposed algorithm is based on the idea of adaptive stepsize, which allows the algorithm to be applied to non-trivial problems. The algorithm is shown to outperform existing algorithms in the constrained and regularized setting, and is also shown to be able to be used to train generative adversarial networks."
2277,SP:af22742091277b726f67e7155b412dd35f29e804,"neural contextual bandits HYPONYM-OF contextual bandits. learning algorithm USED-FOR raw feature vector. upper confidence bound ( UCB ) approach USED-FOR last linear layer ( shallow exploration ). upper confidence bound ( UCB ) approach USED-FOR learning algorithm. finitetime regret EVALUATE-FOR algorithm. neural contextual bandit algorithms COMPARE approach. approach COMPARE neural contextual bandit algorithms. deep neural network USED-FOR it. OtherScientificTerm are reward generating function, and learning time horizon. ","This paper studies the problem of contextual bandits, where the goal is to maximize the reward of the last linear layer of a deep neural network. The authors propose a new learning algorithm for this problem, which is based on the UCB approach. The main idea is to use the upper confidence bound (UCB) approach for last linear layers of deep neural networks to learn a learning algorithm that maximizes the regret of the final linear layer. The proposed algorithm is evaluated on a number of benchmark datasets and shows that it outperforms the baselines. ","This paper studies the problem of contextual bandits, where the goal is to maximize the reward of the last linear layer of a deep neural network. The authors propose a new learning algorithm for this problem, which is based on the UCB approach. The main idea is to use the upper confidence bound (UCB) approach for last linear layers of deep neural networks to learn a learning algorithm that maximizes the regret of the final linear layer. The proposed algorithm is evaluated on a number of benchmark datasets and shows that it outperforms the baselines. "
2293,SP:a9a2c21110e00f19882d27bef0063c422a15e576,"Training action space selection USED-FOR reinforcement learning ( RL ). Shapley - inspired methodology USED-FOR action space categorization. Monte Carlo simulation USED-FOR unnecessary explorations. Monte Carlo simulation PART-OF methodology. cloud infrastructure resource tuning case study EVALUATE-FOR methodology. It USED-FOR search space. it USED-FOR RL model design. data - driven methodology USED-FOR reinforcement learning algorithms. OtherScientificTerm are complex state - action relationships, and exponential - time shapley computations. ",This paper proposes a Shapley-inspired methodology for action space selection in reinforcement learning (RL). The proposed method is based on Monte Carlo simulation. The authors show that the proposed method can be used to reduce the number of unnecessary explorations in the search space. The method is evaluated on a cloud infrastructure resource tuning case study.,This paper proposes a Shapley-inspired methodology for action space selection in reinforcement learning (RL). The proposed method is based on Monte Carlo simulation. The authors show that the proposed method can be used to reduce the number of unnecessary explorations in the search space. The method is evaluated on a cloud infrastructure resource tuning case study.
2309,SP:0e0adc42f6025034d341dc9c17b3f6251afebc2f,"approach USED-FOR probably approximately correct ( PAC ) prediction sets. importance weights USED-FOR algorithm. confidence intervals FEATURE-OF importance weights. DomainNet CONJUNCTION ImageNet. ImageNet CONJUNCTION DomainNet. approach USED-FOR covariate shifts. DomainNet USED-FOR approach. ImageNet USED-FOR approach. PAC constraint FEATURE-OF approaches. PAC constraint FEATURE-OF algorithm. Method are machine learning, predictive model, and uncertainty quantification algorithms. OtherScientificTerm are data distribution, and covariate shift. Generic is shifts. Metric is average normalized size. ","This paper proposes a new approach for probably approximately correct (PAC) prediction sets. The proposed approach is based on the idea of importance weights, which can be used to estimate the confidence intervals of the importance weights of the prediction set. The authors show that the proposed approach can be applied to a wide range of uncertainty quantification algorithms. The method is evaluated on two datasets, DomainNet and ImageNet, and is shown to outperform the baselines.","This paper proposes a new approach for probably approximately correct (PAC) prediction sets. The proposed approach is based on the idea of importance weights, which can be used to estimate the confidence intervals of the importance weights of the prediction set. The authors show that the proposed approach can be applied to a wide range of uncertainty quantification algorithms. The method is evaluated on two datasets, DomainNet and ImageNet, and is shown to outperform the baselines."
2325,SP:0c522ffa2c90eb88296ad0c7999200a72b8755e2,"generalization error EVALUATE-FOR iterative SSL algorithms. information - theoretic principles USED-FOR generalization error. binary Gaussian mixture model HYPONYM-OF model. benchmark datasets EVALUATE-FOR model. MNIST and CIFAR datasets HYPONYM-OF benchmark datasets. OtherScientificTerm are model parameters, class conditional variances, and pseudo - labelling iterations. ","This paper studies the generalization error of iterative SSL algorithms for binary Gaussian mixture models. The authors consider the case where the model parameters and class conditional variances are different, and they show that the generalisation error is bounded by the number of pseudo-labelling iterations. They also provide a theoretical analysis that shows that this is a generalization bound. ","This paper studies the generalization error of iterative SSL algorithms for binary Gaussian mixture models. The authors consider the case where the model parameters and class conditional variances are different, and they show that the generalisation error is bounded by the number of pseudo-labelling iterations. They also provide a theoretical analysis that shows that this is a generalization bound. "
2341,SP:570149eb8fb97928f94312e40bdc48dfe9885848,"model - free reinforcement learning algorithms USED-FOR policy. intentional action sequences USED-FOR high value regions. intentional action sequences USED-FOR plans. it USED-FOR plans. multi - step plans USED-FOR temporally coordinated exploration. GPM USED-FOR temporally coordinated exploration. multi - step plans USED-FOR GPM. GPM USED-FOR it. crude initial plan generator USED-FOR GPM. benchmark environments EVALUATE-FOR baseline methods. OtherScientificTerm are inefficient exploration, single step nature, single step level, consistent movement, and multi - step plan. Method are Generative Planning method ( GPM ), generative planning, and actionrepeat strategy. ",This paper proposes a generative planning method (GPM) for multi-step exploration. GPM is a model-free reinforcement learning algorithm that learns an initial plan generator to generate a sequence of actions that can be used for future exploration. The authors show that the proposed method outperforms baselines on a number of benchmark environments. ,This paper proposes a generative planning method (GPM) for multi-step exploration. GPM is a model-free reinforcement learning algorithm that learns an initial plan generator to generate a sequence of actions that can be used for future exploration. The authors show that the proposed method outperforms baselines on a number of benchmark environments. 
2357,SP:ce6a93847209a0926ed0be5190378a3f61db1935,"deep linear and nonlinear matrix factorizations USED-FOR machine learning. deep learning CONJUNCTION tensor decomposition. tensor decomposition CONJUNCTION deep learning. matrices CONJUNCTION tensors. tensors CONJUNCTION matrices. factorization methods USED-FOR matrix and tensor completion problems. methods COMPARE matrix and tensor factorization methods. matrix and tensor factorization methods COMPARE methods. generalization error bounds EVALUATE-FOR matrix and tensor factorization methods. generalization error bounds EVALUATE-FOR methods. synthetic data and real datasets EVALUATE-FOR methods. recovery accuracy EVALUATE-FOR baselines. methods COMPARE baselines. baselines COMPARE methods. synthetic data and real datasets EVALUATE-FOR baselines. recovery accuracy EVALUATE-FOR methods. Method are deep nonlinear matrix factorization methods, and multi - mode deep matrix and tensor factorizations. ","This paper studies the problem of matrix and tensor factorization in machine learning. The authors propose a new multi-mode deep matrix factorization method, which can be applied to both linear and nonlinear matrices and tensors. The main contribution of the paper is a theoretical analysis of the generalization error bounds of the proposed method. The results show that the method is able to achieve better generalization performance than the baselines on synthetic and real-world datasets. ","This paper studies the problem of matrix and tensor factorization in machine learning. The authors propose a new multi-mode deep matrix factorization method, which can be applied to both linear and nonlinear matrices and tensors. The main contribution of the paper is a theoretical analysis of the generalization error bounds of the proposed method. The results show that the method is able to achieve better generalization performance than the baselines on synthetic and real-world datasets. "
2373,SP:931661154975d94fc5ba1bc89d7a7fdf643df8f2,"interpretation technique USED-FOR structured output models. features USED-FOR structured model. function USED-FOR interpreter. energy - based training process USED-FOR interpreter function. structural information PART-OF model. simulated and real data sets EVALUATE-FOR method. OtherScientificTerm are output variables, computational path of output variables, feature, output variable, and input space. Method are structured models, and structured output model. ",This paper proposes a new method for learning structured output models. The authors propose to use energy-based training to train a structured model and then use the learned model to interpret the input data. The proposed method is evaluated on simulated and real-world datasets. ,This paper proposes a new method for learning structured output models. The authors propose to use energy-based training to train a structured model and then use the learned model to interpret the input data. The proposed method is evaluated on simulated and real-world datasets. 
2389,SP:cf9b6963c32d8689f7203dd41b17461676d08739,"approaches USED-FOR distributional DRL. reward function USED-FOR agent behavior. variance reduction measures USED-FOR on - policy learning. asymptotically consistent estimate of the policy gradient USED-FOR CDF - based objectives. sampling USED-FOR asymptotically consistent estimate of the policy gradient. sampling USED-FOR CDF - based objectives. algorithm USED-FOR agents. risk profiles FEATURE-OF penalty - based formulations. accumulation of positive rewards CONJUNCTION frequency of incurred penalties. frequency of incurred penalties CONJUNCTION accumulation of positive rewards. OpenAI Safety Gym environments FEATURE-OF penalty - based formulations. penalty - based formulations USED-FOR agents. risk profiles FEATURE-OF agents. risk profile COMPARE Proximal Policy Optimization ( PPO ). Proximal Policy Optimization ( PPO ) COMPARE risk profile. risk profile COMPARE PPO. PPO COMPARE risk profile. Proximal Policy Optimization ( PPO ) COMPARE PPO. PPO COMPARE Proximal Policy Optimization ( PPO ). Proximal Policy Optimization ( PPO ) CONJUNCTION positive reward. positive reward CONJUNCTION Proximal Policy Optimization ( PPO ). positive reward COMPARE PPO. PPO COMPARE positive reward. Lagrangians USED-FOR cost levels. positive reward EVALUATE-FOR risk profile. Lagrangians USED-FOR PPO. Method is deep reinforcement learning ( DRL ) agents. Generic are policy, approach, and technique. Task is human decision - making. OtherScientificTerm are distributional context, projected distribution of returns, distribution of full - episode outcomes, cumulative distribution function ( CDF ), relative quality, continuous and discrete action spaces, and policy gradient. ",This paper studies the problem of on-policy learning in the context of distributional reinforcement learning (DRL). The authors propose a new method to learn a policy that maximizes the cumulative distribution function (CDF) of the full-episode outcomes. The authors show that the proposed method is able to achieve better performance than PPO and Proximal Policy Optimization (PPO) on the OpenAI Safety Gym environments. They also show that PPO outperforms PPO in terms of the risk profile.,This paper studies the problem of on-policy learning in the context of distributional reinforcement learning (DRL). The authors propose a new method to learn a policy that maximizes the cumulative distribution function (CDF) of the full-episode outcomes. The authors show that the proposed method is able to achieve better performance than PPO and Proximal Policy Optimization (PPO) on the OpenAI Safety Gym environments. They also show that PPO outperforms PPO in terms of the risk profile.
2405,SP:fa405481f36da10f8ca8d9d5c066458236806a12,"Interactive Neural Process ( INP ) HYPONYM-OF Bayesian active learning framework. Bayesian active learning framework USED-FOR deep learning surrogate model. Interactive Neural Process ( INP ) USED-FOR simulation. deep sequence model CONJUNCTION active learning. active learning CONJUNCTION deep sequence model. neural process CONJUNCTION deep sequence model. deep sequence model CONJUNCTION neural process. deep sequence model USED-FOR framework. neural process USED-FOR framework. active learning USED-FOR framework. spatiotemporal neural process model USED-FOR simulator dynamics. model USED-FOR latent process. latent process USED-FOR intrinsic uncertainty. latent information gain USED-FOR acquisition function. Bayesian active learning algorithms USED-FOR simulator. approach COMPARE random sampling. random sampling COMPARE approach. sample complexity EVALUATE-FOR random sampling. high dimension FEATURE-OF random sampling. sample complexity EVALUATE-FOR approach. framework USED-FOR rapid simulation and scenario exploration. framework USED-FOR complex infectious disease simulator. Task is Stochastic simulations. OtherScientificTerm are fine - grained resolution, and theoretical analysis. ",This paper proposes a Bayesian active learning framework for simulating infectious disease simulation. The proposed method is based on a deep learning surrogate model and a neural process model. The authors show that the proposed method outperforms existing methods in terms of sample complexity and sample efficiency. They also show that their method is able to achieve better sample complexity than random sampling. ,This paper proposes a Bayesian active learning framework for simulating infectious disease simulation. The proposed method is based on a deep learning surrogate model and a neural process model. The authors show that the proposed method outperforms existing methods in terms of sample complexity and sample efficiency. They also show that their method is able to achieve better sample complexity than random sampling. 
2421,SP:fdabafe7d5ca2239a241eba04e1f16cb1ac2316b,"Differentially Private ( DP ) learning USED-FOR large deep learning models of text. hyperparameters USED-FOR DP optimization. hyperparameters CONJUNCTION fine - tuning objectives. fine - tuning objectives CONJUNCTION hyperparameters. pretraining procedure USED-FOR fine - tuning objectives. private NLP models COMPARE private training approaches. private training approaches COMPARE private NLP models. private training approaches CONJUNCTION nonprivate baselines. nonprivate baselines CONJUNCTION private training approaches. private NLP models COMPARE nonprivate baselines. nonprivate baselines COMPARE private NLP models. moderately - sized corpora USED-FOR DP optimization. DP optimization USED-FOR pretrained models. moderately - sized corpora USED-FOR pretrained models. linear layer PART-OF model. per - example gradients USED-FOR linear layer. memory saving technique USED-FOR clipping. clipping PART-OF DP - SGD. large Transformers USED-FOR DP - SGD. memory saving technique USED-FOR DP - SGD. privately training Transformers COMPARE non - private training. non - private training COMPARE privately training Transformers. technique USED-FOR privately training Transformers. memory cost EVALUATE-FOR non - private training. memory cost EVALUATE-FOR privately training Transformers. modest run - time overhead EVALUATE-FOR non - private training. DP optimization USED-FOR high - dimensional models. pretrained models USED-FOR private learning. Task is NLP tasks. Metric is computational overhead. Method is large pretrained models. OtherScientificTerm are noise, and dimension - dependent performance degradation. ",This paper proposes a new method for differentially private (DP) learning for large deep learning models of text. The method is based on the idea of clipping the per-example gradients of the linear layer of the model to reduce the memory cost of the training process. The authors show that this method can be applied to both non-private and private training of large Transformers. They also show that it can be used to improve the performance of private NLP models.,This paper proposes a new method for differentially private (DP) learning for large deep learning models of text. The method is based on the idea of clipping the per-example gradients of the linear layer of the model to reduce the memory cost of the training process. The authors show that this method can be applied to both non-private and private training of large Transformers. They also show that it can be used to improve the performance of private NLP models.
2437,SP:33008c957718d546ecb2d7b8800ef5b03700ace4,"skeletal structure CONJUNCTION joint attributes. joint attributes CONJUNCTION skeletal structure. length CONJUNCTION size. size CONJUNCTION length. size CONJUNCTION strength. strength CONJUNCTION size. strength HYPONYM-OF joint attributes. length HYPONYM-OF joint attributes. size HYPONYM-OF joint attributes. design procedure PART-OF decision - making process. agent PART-OF decision - making process. design procedure USED-FOR agent. skeletal structure CONJUNCTION joint attributes. joint attributes CONJUNCTION skeletal structure. transform actions USED-FOR joint attributes. transform actions USED-FOR skeletal structure. control actions USED-FOR design. message passing USED-FOR joint - specific actions. policy gradient methods USED-FOR approach. approach USED-FOR joint optimization of agent design and control. joint optimization of agent design and control CONJUNCTION experience sharing. experience sharing CONJUNCTION joint optimization of agent design and control. experience sharing USED-FOR approach. approach COMPARE prior methods. prior methods COMPARE approach. Transform2Act COMPARE prior methods. prior methods COMPARE Transform2Act. Transform2Act HYPONYM-OF approach. convergence speed EVALUATE-FOR approach. convergence speed EVALUATE-FOR prior methods. giraffes CONJUNCTION squids. squids CONJUNCTION giraffes. squids CONJUNCTION spiders. spiders CONJUNCTION squids. OtherScientificTerm are agent ’s functionality, and design space. Generic is function. Method are optimal controller, conditional policy, and graph - based policy. Metric is sample efficiency. ","This paper proposes a new method for joint optimization of agent design and control. The proposed method is based on the idea of message passing, where the agent is given a set of joint attributes and the goal is to learn a conditional policy that maximizes the mutual information between the joint attributes of the agent and the controller. The authors show that the proposed method can achieve better sample efficiency than the baselines. ","This paper proposes a new method for joint optimization of agent design and control. The proposed method is based on the idea of message passing, where the agent is given a set of joint attributes and the goal is to learn a conditional policy that maximizes the mutual information between the joint attributes of the agent and the controller. The authors show that the proposed method can achieve better sample efficiency than the baselines. "
2453,SP:46e8c6a9d7729e5112b3c9f8ff91d9557ea524c1,"view synthesis CONJUNCTION 3D object representation and rendering. 3D object representation and rendering CONJUNCTION view synthesis. Implicit neural representations USED-FOR tasks. multi - layer perceptrons ( MLPs ) USED-FOR Implicit neural representations. 3D object representation and rendering HYPONYM-OF tasks. view synthesis HYPONYM-OF tasks. MLP USED-FOR image, video, or 3D object. MLP USED-FOR training. coordinate - based MLPs USED-FOR implicit neural representations. inference CONJUNCTION training. training CONJUNCTION inference. CoordX USED-FOR initial layers. coordinate - based MLPs USED-FOR inference. coordinate - based MLPs USED-FOR training. layers USED-FOR intermediate features. accuracy EVALUATE-FOR baseline MLP. training CONJUNCTION inference. inference CONJUNCTION training. architecture USED-FOR implicit neural representation tasks. speedup EVALUATE-FOR baseline model. Generic are representations, approach, and them. Method is split MLP architecture. OtherScientificTerm is memory overheads. ","This paper proposes a split MLP architecture for implicit neural representation learning. The authors propose to use coordinate-based MLPs for training and inference, and then use the initial layers of the model to learn intermediate features for inference and inference. The proposed method is evaluated on a variety of tasks, including 3D object representation learning, view synthesis, and rendering. ","This paper proposes a split MLP architecture for implicit neural representation learning. The authors propose to use coordinate-based MLPs for training and inference, and then use the initial layers of the model to learn intermediate features for inference and inference. The proposed method is evaluated on a variety of tasks, including 3D object representation learning, view synthesis, and rendering. "
2469,SP:3ea7edab6ae65758b99615be07b7778188a6ff9f,"method USED-FOR object - centric representations of visual scenes. appearance CONJUNCTION 3D pose. 3D pose CONJUNCTION appearance. structured representation USED-FOR shape. shape CONJUNCTION appearance. appearance CONJUNCTION shape. structured representation USED-FOR appearance. localized neural radiance field USED-FOR 2D views of the scene. object representation USED-FOR localized neural radiance field. differentiable rendering process USED-FOR localized neural radiance field. differentiable rendering process USED-FOR 2D views of the scene. reconstruction loss USED-FOR model. inferred scenes USED-FOR representations. 3D object representations USED-FOR visual reasoning task. CATER dataset USED-FOR 3D object representations. Method are INFERNO, and neural 3D rendering. OtherScientificTerm are annotations, rendered scenes, and supervision. ","This paper proposes a method for learning object-centric representations of visual scenes. The proposed method is based on a differentiable rendering process and a reconstruction loss. The method is evaluated on the CATER dataset, where it is shown to outperform the state-of-the-art methods.","This paper proposes a method for learning object-centric representations of visual scenes. The proposed method is based on a differentiable rendering process and a reconstruction loss. The method is evaluated on the CATER dataset, where it is shown to outperform the state-of-the-art methods."
2485,SP:05c61145f3fc9486728aca19c4543065fe04e99c,"feature attribution framework USED-FOR GNN ’s prediction. features USED-FOR GNN ’s prediction. edges HYPONYM-OF features. subgraph USED-FOR model prediction. distribution shift USED-FOR out - ofdistribution problem. surrogate variable USED-FOR subgraphs. generative model USED-FOR unbiased estimation of subgraph importance. explanation fidelity EVALUATE-FOR DSE. Method are graph neural networks ( GNNs ), GNN, in - depth causal analysis, Deconfounded Subgraph Evaluation ( DSE ), and front - door adjustment. OtherScientificTerm are influential subgraph, subgraph importance, OOD effect, explanatory subgraph, and data distribution. Task is evaluation. ","This paper proposes Deconfounded Subgraph Evaluation (DSE), a feature attribution framework for graph neural networks (GNNs). DSE is based on the idea that the importance of a subgraph is determined by the OOD effect of the underlying subgraph. The authors propose a generative model to estimate the subgraph importance of each node in a graph, which is then used as a surrogate variable for the prediction of the next node in the graph. The paper also proposes a front-door adjustment method to improve the explanation fidelity of DSE. Experiments show that DSE can improve the performance of GNNs.","This paper proposes Deconfounded Subgraph Evaluation (DSE), a feature attribution framework for graph neural networks (GNNs). DSE is based on the idea that the importance of a subgraph is determined by the OOD effect of the underlying subgraph. The authors propose a generative model to estimate the subgraph importance of each node in a graph, which is then used as a surrogate variable for the prediction of the next node in the graph. The paper also proposes a front-door adjustment method to improve the explanation fidelity of DSE. Experiments show that DSE can improve the performance of GNNs."
2501,SP:bb74fef9222f227343909f3936f1a8cd2322bbeb,"pretrained models COMPARE active learners. active learners COMPARE pretrained models. spurious correlations CONJUNCTION latent minority groups. latent minority groups CONJUNCTION spurious correlations. latent minority groups CONJUNCTION domain shifts. domain shifts CONJUNCTION latent minority groups. pretrained models COMPARE random sampling. random sampling COMPARE pretrained models. spurious correlations FEATURE-OF image and text datasets. data USED-FOR pretrained models. uncertainty sampling USED-FOR data. accuracy EVALUATE-FOR pretrained models. uncertainty sampling USED-FOR pretrained models. minority classes CONJUNCTION informative examples. informative examples CONJUNCTION minority classes. spurious feature CONJUNCTION class label. class label CONJUNCTION spurious feature. active learning COMPARE unpretrained models. unpretrained models COMPARE active learning. Active learning USED-FOR task ambiguity. Pretraining USED-FOR models. Pretraining USED-FOR task ambiguity. Pretraining USED-FOR active learners. active learners USED-FOR task ambiguity. disambiguating examples USED-FOR active learners. Method are machine learning systems, and pretraining process. Material is few - shot settings. OtherScientificTerm is shape. ","This paper studies the problem of active learning in few-shot settings. The authors show that active learning can improve the performance of pre-trained models in the presence of spurious correlations, domain shifts, and latent minority groups. They also show that uncertainty sampling can be used to improve the accuracy of active learners. ","This paper studies the problem of active learning in few-shot settings. The authors show that active learning can improve the performance of pre-trained models in the presence of spurious correlations, domain shifts, and latent minority groups. They also show that uncertainty sampling can be used to improve the accuracy of active learners. "
2517,SP:f5e9fc0b1b6a41e43ba4dd0cfd99d5ec7008eedf,GRAPHIX HYPONYM-OF pre - trained graph edit model. automatically detecting and fixing bugs CONJUNCTION code quality issues. code quality issues CONJUNCTION automatically detecting and fixing bugs. pre - trained graph edit model USED-FOR automatically detecting and fixing bugs. code quality issues FEATURE-OF Java programs. pre - trained graph edit model USED-FOR code quality issues. sequence - tosequence models COMPARE GRAPHIX. GRAPHIX COMPARE sequence - tosequence models. abstract syntax structure of code USED-FOR GRAPHIX. multi - head graph encoder USED-FOR GRAPHIX. model USED-FOR graph edit actions. graph edit actions USED-FOR automated program repair. model USED-FOR automated program repair. autoregressive tree decoder PART-OF model. pre - training strategy USED-FOR GRAPHIX. pre - training strategy USED-FOR model. implicit knowledge of program structures USED-FOR model. deleted sub - tree reconstruction HYPONYM-OF pre - training strategy. unlabeled source code USED-FOR implicit knowledge of program structures. bug fixing task USED-FOR downstream learning. pre - training objective CONJUNCTION bug fixing task. bug fixing task CONJUNCTION pre - training objective. pre - training objective USED-FOR downstream learning. abstract and concrete code USED-FOR GRAPHIX. Wild Java benchmark EVALUATE-FOR GRAPHIX. CodeBERT CONJUNCTION BART. BART CONJUNCTION CodeBERT. GRAPHIX COMPARE pre - trained Transformer models. pre - trained Transformer models COMPARE GRAPHIX. GRAPHIX COMPARE baselines. baselines COMPARE GRAPHIX. baselines COMPARE pre - trained Transformer models. pre - trained Transformer models COMPARE baselines. GRAPHIX COMPARE BART. BART COMPARE GRAPHIX. GRAPHIX COMPARE CodeBERT. CodeBERT COMPARE GRAPHIX. BART HYPONYM-OF baselines. CodeBERT HYPONYM-OF baselines. GRAPHIX USED-FOR structural and semantic code patterns. Material is abstract and concrete source code. ,This paper proposes a pre-trained graph edit model for automatically detecting and fixing bugs in Java programs. The proposed model is based on a multi-head graph encoder and a tree decoder. The model is evaluated on the Wild Java benchmark and shows promising results.,This paper proposes a pre-trained graph edit model for automatically detecting and fixing bugs in Java programs. The proposed model is based on a multi-head graph encoder and a tree decoder. The model is evaluated on the Wild Java benchmark and shows promising results.
2533,SP:c7b724c671def2800694fcc2625fa48d98c7cfe6,Federated Adversarial Training ( FAT ) USED-FOR data privacy and governance issues. adversarial attack FEATURE-OF model robustness. inner - maximization optimization of Adversarial Training USED-FOR data heterogeneity. lower bound USED-FOR Federated Learning. convergence FEATURE-OF FAT. convergence EVALUATE-FOR α - weighted mechanism. adversarial learning methods CONJUNCTION federated optimization methods. federated optimization methods CONJUNCTION adversarial learning methods. α - WFAT COMPARE FAT. FAT COMPARE α - WFAT. benchmark datasets EVALUATE-FOR α - WFAT. benchmark datasets EVALUATE-FOR FAT. adversarial learning methods USED-FOR FAT. adversarial learning methods USED-FOR α - WFAT. Method is inner - maximization of Adversarial Training. ,"This paper studies the problem of federated adversarial training (FAT) in the context of data privacy and governance. The authors propose a new algorithm, called α-WFAT, which is based on the inner-maximization optimization of Adversarial Training (AT). The authors show that under certain assumptions, the algorithm converges to a lower bound on the convergence of FAT under data heterogeneity. They also show that the algorithm is robust to adversarial attacks. ","This paper studies the problem of federated adversarial training (FAT) in the context of data privacy and governance. The authors propose a new algorithm, called α-WFAT, which is based on the inner-maximization optimization of Adversarial Training (AT). The authors show that under certain assumptions, the algorithm converges to a lower bound on the convergence of FAT under data heterogeneity. They also show that the algorithm is robust to adversarial attacks. "
2549,SP:ff3c787512035e2af20778d53586752852196be9,"LML USED-FOR supervised learning. models USED-FOR semi - supervised continual learning exceptions. Mako HYPONYM-OF wrapper tool. wrapper tool PART-OF supervised LML frameworks. data programming USED-FOR wrapper tool. data programming USED-FOR Mako. Mako USED-FOR continual semi - supervised learning. labeled data USED-FOR continual semi - supervised learning. tool COMPARE fully labeled data. fully labeled data COMPARE tool. per - task accuracy CONJUNCTION resistance. resistance CONJUNCTION per - task accuracy. resistance EVALUATE-FOR catastrophic forgetting. resistance EVALUATE-FOR tool. per - task accuracy EVALUATE-FOR tool. CIFAR-10 CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION CIFAR-10. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. accuracy CONJUNCTION catastrophic forgetting prevention. catastrophic forgetting prevention CONJUNCTION accuracy. Mako USED-FOR unlabeled data. Mako USED-FOR LML tools. LML tools USED-FOR supervised learning. fully labeled data USED-FOR supervised learning. accuracy EVALUATE-FOR supervised learning. image classification data sets USED-FOR LML task sequences. CIFAR100 HYPONYM-OF image classification data sets. CIFAR-10 HYPONYM-OF image classification data sets. MNIST HYPONYM-OF image classification data sets. ORDisCo CONJUNCTION DistillMatch. DistillMatch CONJUNCTION ORDisCo. CNNL CONJUNCTION ORDisCo. ORDisCo CONJUNCTION CNNL. Mako COMPARE them. them COMPARE Mako. baseline semi - supervised LML tools COMPARE Mako. Mako COMPARE baseline semi - supervised LML tools. DistillMatch HYPONYM-OF baseline semi - supervised LML tools. CNNL HYPONYM-OF baseline semi - supervised LML tools. accuracy EVALUATE-FOR Mako. ORDisCo HYPONYM-OF baseline semi - supervised LML tools. Task are Lifelong machine learning ( LML ), and human learning process. Method is LML methods. OtherScientificTerm is knowledge base overhead. ",This paper proposes a new semi-supervised continual learning framework called Mako. Mako is based on the idea of data programming and is able to learn from unlabeled data. The authors show that the proposed framework outperforms the baselines in terms of per-task accuracy and catastrophic forgetting.,This paper proposes a new semi-supervised continual learning framework called Mako. Mako is based on the idea of data programming and is able to learn from unlabeled data. The authors show that the proposed framework outperforms the baselines in terms of per-task accuracy and catastrophic forgetting.
2565,SP:447df6679b2880def833d4f444bf10e61cdf0e1c,"adversarial examples USED-FOR Evading adversarial example detection defenses. attack techniques USED-FOR adversarial examples. Selective Projected Gradient Descent CONJUNCTION Orthogonal Projected Gradient Descent. Orthogonal Projected Gradient Descent CONJUNCTION Selective Projected Gradient Descent. Selective Projected Gradient Descent CONJUNCTION attack techniques. attack techniques CONJUNCTION Selective Projected Gradient Descent. technique USED-FOR detection defenses. accuracy EVALUATE-FOR technique. Generic are model, and attacks. OtherScientificTerm is gradients. Method is gradient - based attacks. Metric is detection rate. ",This paper studies the problem of detection of adversarial examples in the presence of gradient-based attacks. The authors propose a new detection method based on the Orthogonal Projected Gradient Descent (OPD) and Selective Projected Descent techniques. They show that the proposed method outperforms existing detection methods in terms of detection accuracy and detection rate. ,This paper studies the problem of detection of adversarial examples in the presence of gradient-based attacks. The authors propose a new detection method based on the Orthogonal Projected Gradient Descent (OPD) and Selective Projected Descent techniques. They show that the proposed method outperforms existing detection methods in terms of detection accuracy and detection rate. 
2581,SP:5eef907024017849303477eed92f317438c87a69,data valuation CONJUNCTION model valuation. model valuation CONJUNCTION data valuation. feature interpretation CONJUNCTION data valuation. data valuation CONJUNCTION feature interpretation. model valuation USED-FOR ensembles. Valuation problems PART-OF machine learning applications. model valuation HYPONYM-OF Valuation problems. data valuation HYPONYM-OF Valuation problems. feature interpretation HYPONYM-OF Valuation problems. Shapley value CONJUNCTION Banzhaf value. Banzhaf value CONJUNCTION Shapley value. game - theoretic criteria USED-FOR problems. Banzhaf value HYPONYM-OF game - theoretic criteria. Shapley value HYPONYM-OF game - theoretic criteria. energy - based treatment USED-FOR cooperative games. maximum entropy principle USED-FOR energy - based treatment. one - step fixed point iteration USED-FOR ELBO objective. mean - field variational inference USED-FOR classical game - theoretic valuation criteria. mean - field variational inference USED-FOR energy - based model. one - step fixed point iteration USED-FOR classical game - theoretic valuation criteria. uniform initializations USED-FOR variational valuations. game - theoretic axioms FEATURE-OF variational valuations. decoupling error CONJUNCTION valuation. valuation CONJUNCTION decoupling error. valuation FEATURE-OF synthetic and real - world valuation problems. valuation EVALUATE-FOR Variational Index. decoupling error EVALUATE-FOR Variational Index. synthetic and real - world valuation problems EVALUATE-FOR Variational Index. Generic is criteria. Method is fixed point iteration. ,"This paper studies the problem of variational valuation in the context of cooperative games. The authors propose a new algorithm for solving the problem, called Variational Index, which is based on mean-field variational inference. The main idea is to use the Banzhaf value and Shapley value of the game-theoretic game to approximate the value function of a variational model. The paper shows that the proposed algorithm can be used to solve the ELBO problem in a one-step fixed point iteration. The method is evaluated on synthetic and real-world datasets. ","This paper studies the problem of variational valuation in the context of cooperative games. The authors propose a new algorithm for solving the problem, called Variational Index, which is based on mean-field variational inference. The main idea is to use the Banzhaf value and Shapley value of the game-theoretic game to approximate the value function of a variational model. The paper shows that the proposed algorithm can be used to solve the ELBO problem in a one-step fixed point iteration. The method is evaluated on synthetic and real-world datasets. "
2597,SP:1257373629c8584c001b69677ebd73e5f0c20d08,"out - of - sample prediction error FEATURE-OF Epistemic uncertainty. approach USED-FOR epistemic uncertainty. intrinsic unpredictability HYPONYM-OF estimate of aleatoric uncertainty. active learning CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION active learning. estimator USED-FOR interactive learning environments. estimator USED-FOR epistemic uncertainty. active learning USED-FOR interactive learning environments. reinforcement learning USED-FOR interactive learning environments. sequential model optimization CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION sequential model optimization. methods USED-FOR uncertainty estimation. methods USED-FOR tasks. uncertainty estimation USED-FOR tasks. sequential model optimization HYPONYM-OF tasks. reinforcement learning HYPONYM-OF tasks. uncertainty estimates USED-FOR estimating uncertainty. DEUP USED-FOR probabilistic classification of images. uncertainty estimates USED-FOR probabilistic classification of images. synergistic drug combinations FEATURE-OF estimating uncertainty. DEUP USED-FOR uncertainty estimates. OtherScientificTerm are model variance, and generalization error. Method is Direct Epistemic Uncertainty Prediction ( DEUP ). ","This paper proposes a new method for estimating epistemic uncertainty. The method is based on the idea that the uncertainty of a model is a measure of intrinsic unpredictability, i.e., it is a function of the model variance and generalization error. The authors propose a method to estimate this uncertainty based on an estimator called Direct Epistemic Uncertainty Prediction (DEUP). The authors show that DEUP can be used to estimate the uncertainty for a number of tasks, including reinforcement learning, active learning, and sequential model optimization. ","This paper proposes a new method for estimating epistemic uncertainty. The method is based on the idea that the uncertainty of a model is a measure of intrinsic unpredictability, i.e., it is a function of the model variance and generalization error. The authors propose a method to estimate this uncertainty based on an estimator called Direct Epistemic Uncertainty Prediction (DEUP). The authors show that DEUP can be used to estimate the uncertainty for a number of tasks, including reinforcement learning, active learning, and sequential model optimization. "
2613,SP:fd3c33c9237d0f1e33d896858a46c48da2216fe3,Product quantization ( PQ ) USED-FOR approximate nearest neighbor ( ANN ) search systems. Product quantization ( PQ ) CONJUNCTION space rotation. space rotation CONJUNCTION Product quantization ( PQ ). disk storage USED-FOR embeddings. rotation learning methods USED-FOR quantization distortion. quantization distortion FEATURE-OF fixed embeddings. block Givens coordinate descent algorithms USED-FOR rotation matrix. geometric intuitions USED-FOR block Givens coordinate descent algorithms. Lie group theory USED-FOR geometric intuitions. special orthogonal group SO(n ) HYPONYM-OF geometric intuitions. special orthogonal group SO(n ) HYPONYM-OF Lie group theory. SVD method COMPARE Givens algorithms. Givens algorithms COMPARE SVD method. runtime EVALUATE-FOR GPUs. GPUs USED-FOR Givens algorithms. runtime EVALUATE-FOR Givens algorithms. vanilla product quantization USED-FOR end - to - end training scenario. vanilla product quantization USED-FOR They. end - to - end training scenario EVALUATE-FOR They. Task is inner product computation. OtherScientificTerm is convex objectives. ,"This paper studies the problem of approximate nearest neighbor (ANN) search in the context of product quantization (PQ) and space rotation. In particular, the authors consider the case where the inner product computation is computationally expensive due to the quantization distortion of fixed embeddings. The authors propose a block Givens coordinate descent algorithm (Givens-SVD) to solve this problem. The proposed algorithm is based on the Lie group theory of the orthogonal group SO(n) and is shown to be able to solve the problem in the end-to-end training scenario. ","This paper studies the problem of approximate nearest neighbor (ANN) search in the context of product quantization (PQ) and space rotation. In particular, the authors consider the case where the inner product computation is computationally expensive due to the quantization distortion of fixed embeddings. The authors propose a block Givens coordinate descent algorithm (Givens-SVD) to solve this problem. The proposed algorithm is based on the Lie group theory of the orthogonal group SO(n) and is shown to be able to solve the problem in the end-to-end training scenario. "
2629,SP:6ec2c8456ab95f7d028d00b591dab3eadc549eb8,"sensory information USED-FOR conceptual abstractions. Structure Mapping Theory USED-FOR human analogical reasoning. ( higher - order ) relations USED-FOR analogical mappings. two - stage neural framework USED-FOR visual analogies. Raven ’s Progressive Matrices HYPONYM-OF abstract visual reasoning test of fluid intelligence. Neural Structure Mapping ( NSM ) HYPONYM-OF two - stage neural framework. Raven ’s Progressive Matrices USED-FOR visual analogies. multi - task visual relationship encoder CONJUNCTION neural module net - based analogy inference engine. neural module net - based analogy inference engine CONJUNCTION multi - task visual relationship encoder. neural module net - based analogy inference engine USED-FOR framework. raw visual input USED-FOR multi - task visual relationship encoder. multi - task visual relationship encoder USED-FOR framework. structure USED-FOR analogical reasoning. NSM approach USED-FOR relational structure. Generic is them. Task are human intelligence, and Abstract reasoning. OtherScientificTerm are analogies, and novel domains. Material is known domains. Method is machine learning ( ML ) models. ",This paper proposes a two-stage neural framework for visual analogical reasoning. The main idea is to use Raven’s Progressive Matrices (RPM) as a framework for neural structure mapping (NSM) to learn visual analogies. The authors propose a neural module net-based analogy inference engine and a multi-task visual relationship encoder. The proposed framework is evaluated on a variety of visual reasoning tasks and shows promising results. ,This paper proposes a two-stage neural framework for visual analogical reasoning. The main idea is to use Raven’s Progressive Matrices (RPM) as a framework for neural structure mapping (NSM) to learn visual analogies. The authors propose a neural module net-based analogy inference engine and a multi-task visual relationship encoder. The proposed framework is evaluated on a variety of visual reasoning tasks and shows promising results. 
2645,SP:0e8c3a3dba649d496292b41228801feb8507d3b4,contrastive self - supervised method USED-FOR nucleotide genome representation learning. Self - GenomeNet HYPONYM-OF contrastive self - supervised method. Self - GenomeNet HYPONYM-OF self - supervised framework. method USED-FOR latent space. reverse - complement of genomic sequences USED-FOR method. reverse - complement of genomic sequences USED-FOR latent space. framework USED-FOR semantic representations. features USED-FOR context network. context network USED-FOR framework. encoder network USED-FOR features. context network USED-FOR semantic representations. unsupervised contrastive loss USED-FOR network. method COMPARE deep learning methods. deep learning methods COMPARE method. self - supervised and semi - supervised settings USED-FOR deep learning methods. self - supervised and semi - supervised settings USED-FOR method. representations USED-FOR datasets. Material is nucleotide genomic data. OtherScientificTerm is domain - specific characteristics. ,This paper proposes a self-supervised method for nucleotide genome representation learning. The proposed method is based on a contrastive loss and a context network. The authors show that the proposed method outperforms the baselines in both self- and semi-supervision settings.,This paper proposes a self-supervised method for nucleotide genome representation learning. The proposed method is based on a contrastive loss and a context network. The authors show that the proposed method outperforms the baselines in both self- and semi-supervision settings.
2661,SP:2af5c866ed17f156b406153d3261baaa42cf95fb,"low - level vision theory USED-FOR steerable filters. steerable feed - forward learning - based approach USED-FOR point clouds. spherical decision surfaces PART-OF steerable feed - forward learning - based approach. 3D steerability constraint USED-FOR hypersphere neurons. conformal embedding of Euclidean space USED-FOR 3D steerability constraint. conformal embedding of Euclidean space USED-FOR hypersphere neurons. synthetic point set CONJUNCTION real - world 3D skeleton data. real - world 3D skeleton data CONJUNCTION synthetic point set. spherical filter banks USED-FOR invariant class predictions. online optimization USED-FOR invariant class predictions. invariant class predictions USED-FOR known point sets. online optimization USED-FOR spherical filter banks. unknown orientations FEATURE-OF invariant class predictions. unknown orientations FEATURE-OF known point sets. Method is steerable convolutional neural networks. OtherScientificTerm are 3D geometry, rotational equivariance, and model parameters. Task is learning representations of point sets. ",This paper proposes a steerable convolutional neural network for learning representations of point clouds. The proposed method is based on steerable feed-forward learning. The authors propose a conformal embedding of Euclidean space and a 3D steerability constraint for the hypersphere neurons. The method is evaluated on synthetic and real-world 3D skeleton data. ,This paper proposes a steerable convolutional neural network for learning representations of point clouds. The proposed method is based on steerable feed-forward learning. The authors propose a conformal embedding of Euclidean space and a 3D steerability constraint for the hypersphere neurons. The method is evaluated on synthetic and real-world 3D skeleton data. 
2677,SP:14330a1a1c33ec18de096ffb038ba06f04c7dccb,Pre - trained language models ( PLMs ) USED-FOR continual learning. continual learning USED-FOR natural language problems. continual learning methods CONJUNCTION PLMs. PLMs CONJUNCTION continual learning methods. PLMs CONJUNCTION CL approaches. CL approaches CONJUNCTION PLMs. benchmarks EVALUATE-FOR continual learning. benchmarks EVALUATE-FOR CL approaches. PLMs CONJUNCTION CL methods. CL methods CONJUNCTION PLMs. representativeness probing analyses USED-FOR PLMs ’ performance characteristics. representativeness probing analyses USED-FOR layer - wise and task - wise manner. layer - wise and task - wise manner FEATURE-OF PLMs ’ performance characteristics. Task is Continual learning ( CL ). Generic is model. OtherScientificTerm is forgetting. Method is continual learning techniques. ,"This paper studies the problem of continual learning (CL) in the context of pre-trained language models (PLMs) and continual learning methods (CL methods). The authors propose a new continual learning benchmark for PLMs and CL methods. The benchmark is designed to evaluate the performance of CL methods and PLMs on a variety of tasks. The authors also propose a set of representativeness probing analyses to evaluate PLMs’ performance in both layer-wise and task-wise manner. The results show that CL methods perform better than PLMs in terms of performance on the task-level, while PLMs outperform CL methods on the layer-level. ","This paper studies the problem of continual learning (CL) in the context of pre-trained language models (PLMs) and continual learning methods (CL methods). The authors propose a new continual learning benchmark for PLMs and CL methods. The benchmark is designed to evaluate the performance of CL methods and PLMs on a variety of tasks. The authors also propose a set of representativeness probing analyses to evaluate PLMs’ performance in both layer-wise and task-wise manner. The results show that CL methods perform better than PLMs in terms of performance on the task-level, while PLMs outperform CL methods on the layer-level. "
2693,SP:adb11a3bd1af2b68720f8f1b48639e31f65295fd,"model poisoning attacks COMPARE centralized learning. centralized learning COMPARE model poisoning attacks. Federated learning COMPARE centralized learning. centralized learning COMPARE Federated learning. Federated learning USED-FOR model poisoning attacks. multi - party, distributed learning HYPONYM-OF Federated learning. model updates USED-FOR global model. Bulyan CONJUNCTION FABA. FABA CONJUNCTION Bulyan. FABA CONJUNCTION FoolsGold. FoolsGold CONJUNCTION FABA. Krum CONJUNCTION Bulyan. Bulyan CONJUNCTION Krum. FoolsGold HYPONYM-OF Byzantine - resilient federated learning algorithms. Krum HYPONYM-OF Byzantine - resilient federated learning algorithms. FABA HYPONYM-OF Byzantine - resilient federated learning algorithms. Bulyan HYPONYM-OF Byzantine - resilient federated learning algorithms. defense USED-FOR directed deviation attack. TESSERACT HYPONYM-OF defense. TESSERACT USED-FOR directed deviation attack. learning algorithms CONJUNCTION models. models CONJUNCTION learning algorithms. reputation scores USED-FOR TESSERACT. TESSERACT USED-FOR attack. robustness EVALUATE-FOR TESSERACT. Method are untargeted model poisoning attack, and federated learning. OtherScientificTerm are gradient updates, and gradient flips. Metric is test error rate. Task is model poisoning attack. ","This paper studies the problem of model poisoning attacks in federated learning. The authors propose TESSERACT, a defense against directed deviation attacks. The proposed defense is based on the notion of reputation scores, which can be used to measure the robustness of a model to a directed deviation attack. Experiments show that the proposed defense outperforms the baselines in terms of robustness.","This paper studies the problem of model poisoning attacks in federated learning. The authors propose TESSERACT, a defense against directed deviation attacks. The proposed defense is based on the notion of reputation scores, which can be used to measure the robustness of a model to a directed deviation attack. Experiments show that the proposed defense outperforms the baselines in terms of robustness."
2709,SP:2047e943d2337d4fc6b0a269f43c7dfbd8ed9141,"regularization CONJUNCTION model selection. model selection CONJUNCTION regularization. debiasing USED-FOR regularization. debiasing USED-FOR model selection. debiasing USED-FOR √ n - consistent and asymptotically normal estimation. double robustness CONJUNCTION Neyman orthogonality. Neyman orthogonality CONJUNCTION double robustness. functional - specific theoretical derivation USED-FOR influence function. correction term PART-OF plug - in estimator of the functional. Neural Nets CONJUNCTION Random Forests. Random Forests CONJUNCTION Neural Nets. Neural Nets USED-FOR Riesz representation of the linear functional. Random Forests USED-FOR Riesz representation of the linear functional. Neural Nets USED-FOR automatic debiasing procedure. Random Forests USED-FOR automatic debiasing procedure. value query oracle access USED-FOR linear functional. value query oracle access USED-FOR method. representation layers USED-FOR functions. stochastic gradient descent minimization USED-FOR Riesz representer and regression loss. stochastic gradient descent minimization USED-FOR multi - tasking Neural Net debiasing method. Random Forest method USED-FOR locally linear representation of the Riesz function. methodology USED-FOR arbitrary functionals. neural net based estimator USED-FOR average treatment effect functional. it COMPARE neural net based estimator. neural net based estimator COMPARE it. method USED-FOR estimating average marginal effects. gasoline demand FEATURE-OF semi - synthetic data of gasoline price changes. semi - synthetic data of gasoline price changes EVALUATE-FOR method. continuous treatments USED-FOR estimating average marginal effects. Task are causal and policy effects of interest, and Debiasing. ",This paper proposes a neural net debiasing method for estimating the average marginal effects of continuous treatment effects of interest. The proposed method is based on the Riesz representation of the linear functional. The authors show that the proposed method can be used to estimate the average treatment effect functional in a multi-task setting. They also show that their method is asymptotically normal.,This paper proposes a neural net debiasing method for estimating the average marginal effects of continuous treatment effects of interest. The proposed method is based on the Riesz representation of the linear functional. The authors show that the proposed method can be used to estimate the average treatment effect functional in a multi-task setting. They also show that their method is asymptotically normal.
2725,SP:96e1da163020441f9724985ae15674233e0cfe0d,"finite - time convergence CONJUNCTION sample complexity. sample complexity CONJUNCTION finite - time convergence. sample complexity EVALUATE-FOR algorithm. single - agent actorcritic algorithms USED-FOR reinforcement learning. sample complexity bound EVALUATE-FOR single - agent actorcritic algorithms. Method is actor - critic algorithm. OtherScientificTerm are average reward, global average reward, and communication network. Generic is problem. Task is MARL setting. ","This paper studies the sample complexity of actor-critic algorithms for reinforcement learning. The authors consider the problem of finding a global average reward for a single agent in a multi-agent reinforcement learning (MARL) setting, where the agent has access to a communication network and the goal is to maximize the average reward of all agents in the network.  The authors show that under certain conditions, the agent can achieve a sample complexity bound of $\mathcal{O}(\sqrt{T}(\log T)$, where $\log T$ is the number of agents and $T$ is a function of the communication network size.  They also show that this bound holds for the case where all agents have access to the same communication network.","This paper studies the sample complexity of actor-critic algorithms for reinforcement learning. The authors consider the problem of finding a global average reward for a single agent in a multi-agent reinforcement learning (MARL) setting, where the agent has access to a communication network and the goal is to maximize the average reward of all agents in the network.  The authors show that under certain conditions, the agent can achieve a sample complexity bound of $\mathcal{O}(\sqrt{T}(\log T)$, where $\log T$ is the number of agents and $T$ is a function of the communication network size.  They also show that this bound holds for the case where all agents have access to the same communication network."
2741,SP:8475e89f143c727e33147b652c2d0b3cdb420382,"contrastive learning USED-FOR large - scale self - supervised learning. approach USED-FOR large - scale self - supervised learning. contrastive learning HYPONYM-OF approach. conditional independence assumption USED-FOR guarantee. theory COMPARE contrastive methods. contrastive methods COMPARE theory. synthetic and real - world datasets EVALUATE-FOR contrastive methods. synthetic and real - world datasets EVALUATE-FOR theory. contrastive learning USED-FOR class - separated representations. Generic is it. OtherScientificTerm are positive samples, and intra - class samples. Task is surrogate task. ","This paper studies the problem of class-separated representation learning in the context of self-supervised learning. The main contribution of the paper is a theoretical analysis of the conditional independence assumption in contrastive learning, which provides a theoretical guarantee for the class separation of positive samples and intra-class samples. The authors show that under certain conditions, the class separability of positive and negative samples can be guaranteed. They also show that this guarantee holds for both synthetic and real-world datasets.","This paper studies the problem of class-separated representation learning in the context of self-supervised learning. The main contribution of the paper is a theoretical analysis of the conditional independence assumption in contrastive learning, which provides a theoretical guarantee for the class separation of positive samples and intra-class samples. The authors show that under certain conditions, the class separability of positive and negative samples can be guaranteed. They also show that this guarantee holds for both synthetic and real-world datasets."
2757,SP:b491314336c503b276e34e410cf461cb81294890,"Speech restoration USED-FOR distortions in speech signals. speech denoising CONJUNCTION speech declipping. speech declipping CONJUNCTION speech denoising. Prior methods USED-FOR single - task speech restoration ( SSR ). speech declipping HYPONYM-OF single - task speech restoration ( SSR ). speech denoising HYPONYM-OF single - task speech restoration ( SSR ). SSR systems USED-FOR speech restoration tasks. SSR systems USED-FOR speech restoration problem. speech super - resolution HYPONYM-OF speech restoration tasks. generative framework USED-FOR GSR task. VoiceFixer1 HYPONYM-OF generative framework. VoiceFixer1 USED-FOR GSR task. analysis stage CONJUNCTION synthesis stage. synthesis stage CONJUNCTION analysis stage. VoiceFixer USED-FOR speech analysis. synthesis stage USED-FOR speech analysis. synthesis stage PART-OF VoiceFixer. analysis stage PART-OF VoiceFixer. ResUNet USED-FOR analysis stage. neural vocoder USED-FOR synthesis stage. ResUNet CONJUNCTION neural vocoder. neural vocoder CONJUNCTION ResUNet. neural vocoder USED-FOR analysis stage. additive noise CONJUNCTION room reverberation. room reverberation CONJUNCTION additive noise. room reverberation CONJUNCTION low - resolution, and clipping distortions. low - resolution, and clipping distortions CONJUNCTION room reverberation. additive noise USED-FOR VoiceFixer. room reverberation FEATURE-OF VoiceFixer. low - resolution, and clipping distortions EVALUATE-FOR VoiceFixer. baseline GSR model COMPARE speech denoising SSR model. speech denoising SSR model COMPARE baseline GSR model. mean opinion score ( MOS ) EVALUATE-FOR speech denoising SSR model. mean opinion score ( MOS ) EVALUATE-FOR baseline GSR model. VoiceFixer COMPARE GSR baseline model. GSR baseline model COMPARE VoiceFixer. MOS score EVALUATE-FOR GSR baseline model. MOS score EVALUATE-FOR VoiceFixer. old movies CONJUNCTION historical speeches. historical speeches CONJUNCTION old movies. VoiceFixer USED-FOR historical speeches.","This paper proposes a generative framework for single-task speech restoration (SSR). The proposed method, VoiceFixer1, consists of three stages: synthesis stage, analysis stage and denoising stage. The synthesis stage consists of a neural vocoder and a neural synthesizer, while the analysis stage is a combination of two existing methods: ResUNet and ResNet. The authors show that the proposed method is able to achieve state-of-the-art performance on a range of speech restoration tasks, including speech super-resolution, room reverberation and clipping distortions. ","This paper proposes a generative framework for single-task speech restoration (SSR). The proposed method, VoiceFixer1, consists of three stages: synthesis stage, analysis stage and denoising stage. The synthesis stage consists of a neural vocoder and a neural synthesizer, while the analysis stage is a combination of two existing methods: ResUNet and ResNet. The authors show that the proposed method is able to achieve state-of-the-art performance on a range of speech restoration tasks, including speech super-resolution, room reverberation and clipping distortions. "
2773,SP:c80a7392ec6147395a664734601fb389a1eb4470,"Residual Tensor Networks ( MVSRTN ) USED-FOR multivariate time series. Residual Tensor Networks ( MVSRTN ) USED-FOR Variable Space. tensor network USED-FOR variable space. low - rank approximation USED-FOR variable space. low - rank approximation USED-FOR tensor network. translation invariance FEATURE-OF network. tensor components USED-FOR translation invariance. tensor components USED-FOR network. it USED-FOR space - approximated tensor network. seriesvariable encoder USED-FOR variable space. skip - connection layer USED-FOR dissemination of information. scale HYPONYM-OF dissemination of information. multivariate time series forecasting benchmark datasets EVALUATE-FOR method. Material is Multivariate time series. OtherScientificTerm are latent space, time window, and long - term sequences. Generic is framework. Method is N - order residual connection approach. ",This paper proposes a new method for multivariate time series forecasting based on residual tensor networks (MVSRNets). The proposed method is based on a low-rank approximation of the tensor network and a skip-connection layer. The authors show that the proposed method outperforms the state-of-the-art in terms of forecasting performance on three benchmark datasets. ,This paper proposes a new method for multivariate time series forecasting based on residual tensor networks (MVSRNets). The proposed method is based on a low-rank approximation of the tensor network and a skip-connection layer. The authors show that the proposed method outperforms the state-of-the-art in terms of forecasting performance on three benchmark datasets. 
2789,SP:0a92939e6a1c88bfeb4fd1dea9ee7be4fd60d967,"Neighbor sampling USED-FOR Graph Neural Networks ( GNNs ). large graphs USED-FOR Graph Neural Networks ( GNNs ). Stochastic Compositional Optimization ( SCO ) problems USED-FOR samplingbased GNN training. SCO algorithms USED-FOR samplingbased GNN training. SCO algorithms USED-FOR GNNs. moving averages USED-FOR aggregated features. moving averages USED-FOR they. large graphs USED-FOR GNNs. GPU memory limit CONJUNCTION CPU memory limit. CPU memory limit CONJUNCTION GPU memory limit. GPU memory limit FEATURE-OF moving averages. SCO algorithms USED-FOR GNN training. sparse moving averages USED-FOR SCO algorithms. moving averages USED-FOR algorithm. fixed size buffer USED-FOR algorithm. convergence rate EVALUATE-FOR SCO algorithm. algorithm USED-FOR SCO algorithm. convergence rate EVALUATE-FOR algorithm. Adam SGD USED-FOR GNN training. algorithm COMPARE Adam SGD. Adam SGD COMPARE algorithm. algorithm USED-FOR GNN training. memory overhead EVALUATE-FOR algorithm. OtherScientificTerm are graph, graph size, and buffer size. ",This paper studies the problem of Stochastic Compositional Optimization (SCO) for GNN training. The authors propose a new SCO algorithm based on moving averages. The proposed algorithm is based on Adam SGD and is shown to converge to a fixed size buffer. Experiments show that the proposed algorithm outperforms existing SCO algorithms.,This paper studies the problem of Stochastic Compositional Optimization (SCO) for GNN training. The authors propose a new SCO algorithm based on moving averages. The proposed algorithm is based on Adam SGD and is shown to converge to a fixed size buffer. Experiments show that the proposed algorithm outperforms existing SCO algorithms.
2805,SP:72e0cac289dce803582053614ec9ee93e783c838,"random hashes USED-FOR Jaccard ( resemblance ) similarity. Minwise hashing ( MinHash ) USED-FOR random hashes. massive binary ( 0/1 ) data USED-FOR Jaccard ( resemblance ) similarity. large - scale learning models CONJUNCTION approximate near neighbor search. approximate near neighbor search CONJUNCTION large - scale learning models. massive data USED-FOR approximate near neighbor search. independent random permutations USED-FOR MinHash. that COMPARE classical MinHash. classical MinHash COMPARE that. Jaccard estimation variance EVALUATE-FOR classical MinHash. circulant manner FEATURE-OF independent random permutations. permutations USED-FOR it. estimation accuracy EVALUATE-FOR it. Method are Circulant MinHash ( C - MinHash ), and C - MinHash variant. Generic is method. ","This paper proposes Circulant MinHash (C-MinHash), a variant of the classical MinHash that uses independent random permutations to approximate the Jaccard (identity) similarity between two random hashes. The authors show that C-minHash is able to achieve better estimation accuracy than the original MinHash in terms of the variance of the estimation of the similarity between the two random hash functions. They also show that their method can be used for approximate near neighbor search.","This paper proposes Circulant MinHash (C-MinHash), a variant of the classical MinHash that uses independent random permutations to approximate the Jaccard (identity) similarity between two random hashes. The authors show that C-minHash is able to achieve better estimation accuracy than the original MinHash in terms of the variance of the estimation of the similarity between the two random hash functions. They also show that their method can be used for approximate near neighbor search."
2821,SP:d254b38331b6b6f30de398bae09380cd5c951698,Adversarial training ( AT ) USED-FOR adversarial robustness. adversarial robustness EVALUATE-FOR single lpthreat models. lp - threat models USED-FOR adversarial robustness. training scheme USED-FOR adversarial robustness. training scheme USED-FOR union of lp - threat models. adversarial training USED-FOR lp - threat model. E - AT scheme USED-FOR lp - robust model. multiple - norm robustness EVALUATE-FOR state - of - the - art. multiple norm robustness FEATURE-OF ImageNet models. CIFAR-10 EVALUATE-FOR multiple - norm robustness. CIFAR-10 EVALUATE-FOR state - of - the - art. adversarial robustness FEATURE-OF threat models. CIFAR-10 EVALUATE-FOR SOTA l1 - robustness. Task is safety - critical systems. OtherScientificTerm is lp - balls. Metric is multiple norm adversarial robustness. ,"This paper proposes a new training scheme for adversarial training of lp-robust models. The proposed method, called E-AT, is based on a union of adversarial models trained on a single lp threat model and a multi-norm adversarial model trained on multiple lp threats. The method is evaluated on CIFAR-10 and ImageNet and shows that the proposed method outperforms the baselines. ","This paper proposes a new training scheme for adversarial training of lp-robust models. The proposed method, called E-AT, is based on a union of adversarial models trained on a single lp threat model and a multi-norm adversarial model trained on multiple lp threats. The method is evaluated on CIFAR-10 and ImageNet and shows that the proposed method outperforms the baselines. "
2837,SP:4c2928f6772664d63c02c29f913b476e1c932983,MTL models COMPARE single - task counterpart. single - task counterpart COMPARE MTL models. private encoders CONJUNCTION gates. gates CONJUNCTION private encoders. gates CONJUNCTION private decoders. private decoders CONJUNCTION gates. public encoder CONJUNCTION private encoders. private encoders CONJUNCTION public encoder. private encoder CONJUNCTION gate. gate CONJUNCTION private encoder. gate CONJUNCTION private decoder. private decoder CONJUNCTION gate. public encoder CONJUNCTION private encoder. private encoder CONJUNCTION public encoder. storage cost FEATURE-OF inference stage. SMTL USED-FOR gate. SMTL USED-FOR gates. benchmark datasets EVALUATE-FOR methods. Method is Multi - Task Learning ( MTL ). Generic is problem. OtherScientificTerm is negative sharing. Task is safe multi - task learning. ,"This paper studies the problem of safe multi-task learning (MTL). In particular, the authors focus on the case where there is negative sharing between private encoders and gates. To address this problem, they propose a new inference stage called SMTL. The authors show that SMTL is able to improve the performance of the gate and private decoders.","This paper studies the problem of safe multi-task learning (MTL). In particular, the authors focus on the case where there is negative sharing between private encoders and gates. To address this problem, they propose a new inference stage called SMTL. The authors show that SMTL is able to improve the performance of the gate and private decoders."
2853,SP:c4cee0d44198559c417750ec4729d26b41061929,"energy - based sequence models USED-FOR partition functions. expressive parametric families USED-FOR energy - based sequence models. model selection CONJUNCTION learning model parameters. learning model parameters CONJUNCTION model selection. partition functions FEATURE-OF sequence model families. asymptotic guarantees FEATURE-OF statistical procedures. OtherScientificTerm are model parameters, partition function, rational number, reduced expressiveness, and computability concerns. Generic is they. Task is sequence modeling. Method is model parametrizations. ","This paper studies the expressive parametric families of energy-based sequence models. The authors consider the case where the partition function of a sequence model is a rational number, and they show that for any partition function, there exists a family of partition functions that satisfy the asymptotic guarantees. They also show that the partition functions of energy based sequence models can be expressed as a function of the number of parameters of the model. ","This paper studies the expressive parametric families of energy-based sequence models. The authors consider the case where the partition function of a sequence model is a rational number, and they show that for any partition function, there exists a family of partition functions that satisfy the asymptotic guarantees. They also show that the partition functions of energy based sequence models can be expressed as a function of the number of parameters of the model. "
2869,SP:f1eb66f24a14808d404f9ad9773ef4288efa060e,"Wasserstein distance USED-FOR large - scale machine learning problems. random projection USED-FOR sliced Wasserstein distance. computational efficiency EVALUATE-FOR sliced Wasserstein distance. augmented sliced Wasserstein distances ( ASWDs ) HYPONYM-OF distance metrics. neural networks USED-FOR higher - dimensional hypersurfaces. they USED-FOR complex structures of the data distribution. gradient ascent USED-FOR hypersurfaces. ASWD COMPARE Wasserstein variants. Wasserstein variants COMPARE ASWD. Wasserstein variants USED-FOR synthetic and real - world problems. synthetic and real - world problems EVALUATE-FOR ASWD. Metric is computational cost. OtherScientificTerm are projections, ( random ) linear projections, and nonlinear projections. Method is injective neural network architecture. ","This paper proposes augmented sliced Wasserstein distances (ASWDs), which is an extension of the sliced WSWD (Wasserstein distance) which is a popular metric for measuring the computational efficiency of deep neural networks. The proposed ASWDs are based on random projections of the data distribution. The authors show that the proposed method is computationally efficient and can be applied to both synthetic and real-world problems. ","This paper proposes augmented sliced Wasserstein distances (ASWDs), which is an extension of the sliced WSWD (Wasserstein distance) which is a popular metric for measuring the computational efficiency of deep neural networks. The proposed ASWDs are based on random projections of the data distribution. The authors show that the proposed method is computationally efficient and can be applied to both synthetic and real-world problems. "
2885,SP:ff2433f2de48d4ed8017e27bd6cf606845cdea9e,coordinated exploration and behaviour PART-OF multi - agent systems. framework USED-FOR multi - agent reinforcement learners ( MARL ). reinforcement learning ( RL ) CONJUNCTION switching controls. switching controls CONJUNCTION reinforcement learning ( RL ). switching controls USED-FOR LIGS. LIGS USED-FOR learning process. intrinsic rewards USED-FOR learning process. LIGS USED-FOR intrinsic rewards. reinforcement learning ( RL ) USED-FOR LIGS. LIGS USED-FOR systems of RL agents. sparse rewards USED-FOR systems of RL agents. it USED-FOR joint policies. multi - agent RL algorithms USED-FOR LIGS. Foraging CONJUNCTION StarCraft. StarCraft CONJUNCTION Foraging. LIGS framework USED-FOR Foraging. LIGS framework USED-FOR StarCraft. Method is reinforcement learners ( RL ). ,"This paper proposes a framework for multi-agent reinforcement learning (MRL) called LIGS. The proposed framework is based on the idea of LIGs, which is an extension of the LIG framework for reinforcement learning. The main contribution of the paper is the introduction of a new reward function, which can be used to incentivize the agents to learn a joint policy that maximizes the mutual information between agents. The paper also proposes a new algorithm for learning the joint policy. Experiments show that the proposed method outperforms the baselines on a variety of RL tasks.","This paper proposes a framework for multi-agent reinforcement learning (MRL) called LIGS. The proposed framework is based on the idea of LIGs, which is an extension of the LIG framework for reinforcement learning. The main contribution of the paper is the introduction of a new reward function, which can be used to incentivize the agents to learn a joint policy that maximizes the mutual information between agents. The paper also proposes a new algorithm for learning the joint policy. Experiments show that the proposed method outperforms the baselines on a variety of RL tasks."
2901,SP:9eadc19f7f712c488cf50d091f372092f6352930,multi - hop QA systems COMPARE DOCHOPPER. DOCHOPPER COMPARE multi - hop QA systems. document information CONJUNCTION q. q CONJUNCTION document information. QA tasks EVALUATE-FOR DOCHOPPER. datasets EVALUATE-FOR DOCHOPPER. inference time EVALUATE-FOR DOCHOPPER. Generic is model. Method is compact neural representation of q. ,"This paper proposes a new model for multi-hop QA, called DOCHOPPER, which learns a compact neural representation of q and document information. The authors show that the proposed model is able to outperform the state-of-the-art models on a number of benchmark datasets. They also show that their model can be used to improve the inference time of multi-hoc QA systems.","This paper proposes a new model for multi-hop QA, called DOCHOPPER, which learns a compact neural representation of q and document information. The authors show that the proposed model is able to outperform the state-of-the-art models on a number of benchmark datasets. They also show that their model can be used to improve the inference time of multi-hoc QA systems."
2917,SP:4e79b326bbda5d1509e88869dde9886764366d41,"modalities USED-FOR voice search request. voice casting USED-FOR audiovisual productions. it USED-FOR modalities. it USED-FOR voice recommendation system. it USED-FOR voice search request. characteristic extraction USED-FOR voice casting. taxonomy USED-FOR comedian voices. taxonomy USED-FOR annotation protocol. Label Refining HYPONYM-OF semi - supervised learning method. vocal characteristics HYPONYM-OF refined labels. clustering algorithm USED-FOR refined representation extractor. refined labels USED-FOR refined representation extractor. representation extractor USED-FOR method. clustering algorithm USED-FOR refined labels. Label Refining USED-FOR method. subsidiary corpus USED-FOR voice characteristics. OtherScientificTerm are characteristic, and priori knowledge. Material is MassEffect 3 video game. ",This paper proposes a semi-supervised learning method for voice casting. The proposed method is based on the idea of self-supervision and self-distribution. The method is evaluated on the MassEffect 3 video game and shows promising results. ,This paper proposes a semi-supervised learning method for voice casting. The proposed method is based on the idea of self-supervision and self-distribution. The method is evaluated on the MassEffect 3 video game and shows promising results. 
2933,SP:9c399331a3b4a55d7e1ff9298f82a38b75b4f87d,"federated and split learning HYPONYM-OF Distributed collaborative learning approaches. Vision Transformer ( ViT ) USED-FOR common representation. Vision Transformer ( ViT ) USED-FOR computer vision applications. global attention USED-FOR common representation. distributed learning framework USED-FOR image processing tasks. ViT USED-FOR distributed learning framework. task - agnostic Vision Transformer CONJUNCTION task - specific head / tail. task - specific head / tail CONJUNCTION task - agnostic Vision Transformer. task - specific heads and tails CONJUNCTION task - agnostic Transformer body. task - agnostic Transformer body CONJUNCTION task - specific heads and tails. features PART-OF representation. global attention USED-FOR Transformer body. task - agnostic learning USED-FOR Transformer. task - specific learning USED-FOR heads. alternating training strategy USED-FOR task - specific learning. method USED-FOR task - specific network. multi - task learning EVALUATE-FOR method. Method is neural networks. Generic are they, applications, and translation. OtherScientificTerm is customer - specific head and tail. Material is medical image data. ","This paper proposes a new distributed learning framework for multi-task learning. The proposed method is based on federated and split learning, where each task-specific task is represented by a task-agnostic Transformer, and the task-generalized Transformer is used to represent the global representation. The authors show that the proposed method outperforms the state-of-the-art in terms of performance on a number of tasks. ","This paper proposes a new distributed learning framework for multi-task learning. The proposed method is based on federated and split learning, where each task-specific task is represented by a task-agnostic Transformer, and the task-generalized Transformer is used to represent the global representation. The authors show that the proposed method outperforms the state-of-the-art in terms of performance on a number of tasks. "
2949,SP:249a72ef4e9cf02221243428174bb749068af6b2,"misspecified reward functions USED-FOR RL agents. misspecified rewards FEATURE-OF RL environments. action space resolution CONJUNCTION observation space noise. observation space noise CONJUNCTION action space resolution. model capacity CONJUNCTION action space resolution. action space resolution CONJUNCTION model capacity. observation space noise CONJUNCTION training time. training time CONJUNCTION observation space noise. agent capabilities USED-FOR reward hacking. training time HYPONYM-OF agent capabilities. model capacity HYPONYM-OF agent capabilities. observation space noise HYPONYM-OF agent capabilities. action space resolution HYPONYM-OF agent capabilities. proxy reward CONJUNCTION true reward. true reward CONJUNCTION proxy reward. capability thresholds HYPONYM-OF phase transitions. anomaly detection task USED-FOR aberrant policies. Task are Reward hacking, and ML systems. OtherScientificTerm is reward misspecifications. Method is baseline detectors. ","This paper studies the problem of reward hacking in reinforcement learning (RL). The authors propose a new anomaly detection task to detect anomalous behavior in RL systems. The proposed task is based on the observation space noise, action space resolution, and model capacity. The authors show that anomaly detection is effective in detecting anomalous behaviors.","This paper studies the problem of reward hacking in reinforcement learning (RL). The authors propose a new anomaly detection task to detect anomalous behavior in RL systems. The proposed task is based on the observation space noise, action space resolution, and model capacity. The authors show that anomaly detection is effective in detecting anomalous behaviors."
2965,SP:1c8d06fe0b2a79d5d0c0f317692c2ee869d1cc0c,Kullback – Leibler ( KL ) divergence CONJUNCTION arbitary differeitiable f divergence. arbitary differeitiable f divergence CONJUNCTION Kullback – Leibler ( KL ) divergence. f -TVO USED-FOR Thermodynamic Variational Objective ( TVO ). f -TVO USED-FOR dual function of model evidence f∗(p(x ) ). log model evidence PART-OF TVO. dual function of model evidence f∗(p(x ) ) COMPARE log model evidence. log model evidence COMPARE dual function of model evidence f∗(p(x ) ). deformed χ - geometry perspective USED-FOR f -TVO. variational posterior distribution CONJUNCTION true posterior distribution. true posterior distribution CONJUNCTION variational posterior distribution. χ - exponential family exponential USED-FOR f -TVO. χ - path USED-FOR f -TVO. reparameterization trick CONJUNCTION Monte Carlo approximation. Monte Carlo approximation CONJUNCTION reparameterization trick. reparameterization trick PART-OF f -TVO. Monte Carlo approximation PART-OF f -TVO. VAE CONJUNCTION Bayesian neural network. Bayesian neural network CONJUNCTION VAE. f -TVO COMPARE cooresponding baseline f -divergence variational inference. cooresponding baseline f -divergence variational inference COMPARE f -TVO. Bayesian neural network EVALUATE-FOR f -TVO. VAE EVALUATE-FOR f -TVO. Bayesian neural network EVALUATE-FOR cooresponding baseline f -divergence variational inference. OtherScientificTerm is deformed geodesic. ,"This paper studies the dual function of model evidence f-TVO, which is a special case of the Thermodynamic Variational Objective (TVO). The authors show that fTVO can be decomposed into two parts: (1) a dual function between the true posterior distribution and the variational posterior distribution, and (2) a deformed χ-geometry perspective of the model evidence. The authors also show that the deformed geodesic can be used as a reparameterization trick to improve the convergence of fTVOs. ","This paper studies the dual function of model evidence f-TVO, which is a special case of the Thermodynamic Variational Objective (TVO). The authors show that fTVO can be decomposed into two parts: (1) a dual function between the true posterior distribution and the variational posterior distribution, and (2) a deformed χ-geometry perspective of the model evidence. The authors also show that the deformed geodesic can be used as a reparameterization trick to improve the convergence of fTVOs. "
2981,SP:d4ce49411198fe65b8f4c2d80af222e0732a4728,"critics ’ initialization USED-FOR ensemble - based actor - critic exploration. approximated UCB CONJUNCTION weighted Bellman backup. weighted Bellman backup CONJUNCTION approximated UCB. strategy COMPARE approximated UCB. approximated UCB COMPARE strategy. weighted Bellman backup COMPARE clipped double Q - Learning. clipped double Q - Learning COMPARE weighted Bellman backup. additive action noise USED-FOR exploration. weighted Bellman backup USED-FOR strategy. actors ’ initialization USED-FOR training. posterior sampling USED-FOR strategy. methods USED-FOR policies. Method are deep reinforcement learning ( RL ), RL toolbox, and ED2. Generic are task, and tools. Material is continuous control setting. OtherScientificTerm is evaluation runs. Task is continuous control tasks. ",This paper proposes an ensemble-based actor-critic exploration method for continuous control tasks. The proposed method is based on weighted Bellman backup (UCB) and weighted double Q-learning (Q-learning). The authors show that the proposed method outperforms the baselines in terms of exploration performance.,This paper proposes an ensemble-based actor-critic exploration method for continuous control tasks. The proposed method is based on weighted Bellman backup (UCB) and weighted double Q-learning (Q-learning). The authors show that the proposed method outperforms the baselines in terms of exploration performance.
2997,SP:21819b54433fa274657d9fe418f66407eee83eeb,"natural language processing CONJUNCTION face recognition. face recognition CONJUNCTION natural language processing. lending CONJUNCTION college admission. college admission CONJUNCTION lending. Supervised learning models USED-FOR domains. college admission CONJUNCTION natural language processing. natural language processing CONJUNCTION college admission. lending HYPONYM-OF domains. face recognition HYPONYM-OF domains. college admission HYPONYM-OF domains. natural language processing HYPONYM-OF domains. fairness notions USED-FOR fairness issues. fair predictor USED-FOR constrained optimization problem. Equalized Loss ( EL ) HYPONYM-OF fairness notion. prediction error / loss USED-FOR fairness notion. algorithms USED-FOR global optimum. algorithms USED-FOR non - convex problem. global optimum USED-FOR non - convex problem. convex programming tools USED-FOR algorithms. ELminimizer algorithm USED-FOR EL fair predictor. non - convex optimization problem USED-FOR EL fair predictor. convex constrained optimizations USED-FOR non - convex optimization problem. algorithm USED-FOR sub - optimal EL fair predictor. algorithm COMPARE ELminimizer. ELminimizer COMPARE algorithm. unconstrained convex programming tools USED-FOR algorithm. unconstrained convex programming tools USED-FOR sub - optimal EL fair predictor. real - world data EVALUATE-FOR algorithms. Generic are models, it, and constraint. OtherScientificTerm are protected social groups, and loss function. Method is learning process. ","This paper studies the problem of learning a fair predictor for a constrained optimization problem. The authors propose a new algorithm for this problem, called Equalized Loss (EL) fair predictor, which is a variant of Equalized Fair Prediction (EFP). The main contribution of this paper is to introduce a new formulation of the EL fair predictor. The proposed algorithm is based on convex programming tools and can be used to solve the non-convex optimization problem of the fair predictor problem. Experiments show that the proposed algorithm achieves sub-optimal EL fair predictors. ","This paper studies the problem of learning a fair predictor for a constrained optimization problem. The authors propose a new algorithm for this problem, called Equalized Loss (EL) fair predictor, which is a variant of Equalized Fair Prediction (EFP). The main contribution of this paper is to introduce a new formulation of the EL fair predictor. The proposed algorithm is based on convex programming tools and can be used to solve the non-convex optimization problem of the fair predictor problem. Experiments show that the proposed algorithm achieves sub-optimal EL fair predictors. "
3013,SP:336c1b8a7f293a78dfab18e7b454b0ec39822293,"neural networks USED-FOR cognitive capacity. meaningful learning USED-FOR systematic generalization. compositional skills FEATURE-OF models. semantic connections USED-FOR models. semantic connections USED-FOR compositional skills. semantic links USED-FOR models. RNNs CONJUNCTION CNNs. CNNs CONJUNCTION RNNs. CNNs CONJUNCTION Transformers. Transformers CONJUNCTION CNNs. SCAN CONJUNCTION real - world datasets. real - world datasets CONJUNCTION SCAN. real - world datasets USED-FOR semantic parsing. semantic linking USED-FOR sequenceto - sequence models. RNNs HYPONYM-OF sequenceto - sequence models. CNNs HYPONYM-OF sequenceto - sequence models. Transformers PART-OF sequenceto - sequence models. prior knowledge CONJUNCTION semantic linking. semantic linking CONJUNCTION prior knowledge. prior knowledge USED-FOR systematic generalization. semantic linking USED-FOR systematic generalization. inductive learning COMPARE deductive learning. deductive learning COMPARE inductive learning. neural networks USED-FOR systematic generalization. learning schemes USED-FOR neural networks. learning schemes USED-FOR systematic generalization. Material is SCAN dataset. Method are meaningful learning principle, and data augmentation techniques. OtherScientificTerm is inductive or deductive manner. Generic is them. ",This paper proposes a meaningful learning principle for systematic generalization of neural networks. The authors show that meaningful learning can be applied to both inductive learning and deductive learning. They also show that semantic linking and prior knowledge can be used to improve the generalization performance of neural network models. They further show that prior knowledge and semantic linking can be combined to improve generalization. ,This paper proposes a meaningful learning principle for systematic generalization of neural networks. The authors show that meaningful learning can be applied to both inductive learning and deductive learning. They also show that semantic linking and prior knowledge can be used to improve the generalization performance of neural network models. They further show that prior knowledge and semantic linking can be combined to improve generalization. 
3029,SP:5d758b9125e716c92dde5cfcc8aad67adbd30ba0,"method USED-FOR 3D shape representation. multi - scale wavelet decomposition USED-FOR method. up / down - sampling USED-FOR hierarchies. sub - bands components PART-OF 3D shapes. lifting scheme USED-FOR high or low sub - bands components. Transformers USED-FOR AWT - Net. shape features USED-FOR them. 3D shape classification and segmentation benchmarks EVALUATE-FOR AWT - Net. OtherScientificTerm are decomposition tree, approximation or detail wavelet coefficients, features, and wavelet coefficients. Method are multi - resolution wavelet analysis, and holistic representations. ","This paper proposes a multi-scale wavelet decomposition method for 3D shape representation learning. The proposed method is based on a lifting scheme, where each wavelet is decomposed into high or low sub-band components. The authors show that the proposed method can be used to learn a hierarchical representation of 3D shapes. The method is evaluated on 3D classification and segmentation tasks.","This paper proposes a multi-scale wavelet decomposition method for 3D shape representation learning. The proposed method is based on a lifting scheme, where each wavelet is decomposed into high or low sub-band components. The authors show that the proposed method can be used to learn a hierarchical representation of 3D shapes. The method is evaluated on 3D classification and segmentation tasks."
3045,SP:b89c04e2f8e94c7d0c3686edac835a86fab2d528,"pretrained language model USED-FOR natural language generation tasks. prefixtuning CONJUNCTION adapters. adapters CONJUNCTION prefixtuning. Lightweight finetuning COMPARE full finetuning. full finetuning COMPARE Lightweight finetuning. adapters HYPONYM-OF Lightweight finetuning. prefixtuning HYPONYM-OF Lightweight finetuning. lightweight finetuning COMPARE full finetuning in - distribution ( ID ). full finetuning in - distribution ( ID ) COMPARE lightweight finetuning. ID CONJUNCTION OOD. OOD CONJUNCTION ID. full and lightweight finetuning USED-FOR methods. ID CONJUNCTION OOD. OOD CONJUNCTION ID. full and lightweight finetuning USED-FOR ID. full and lightweight finetuning USED-FOR OOD. cocktail finetuning USED-FOR full finetuning. model CONJUNCTION cocktail finetuning. cocktail finetuning CONJUNCTION model. distillation USED-FOR lightweight model. lightweight model USED-FOR full finetuning. distillation USED-FOR full finetuning. distillation USED-FOR OOD behavior. distillation USED-FOR model. distillation USED-FOR ID data. OOD behavior FEATURE-OF model. Method are pretrained model, and lightweight and full finetuning models. Task is multiclass logistic regression setting. ","This paper studies the effect of full and lightweight finetuning for natural language generation tasks. In particular, the authors show that full-finetuning in-distribution (ID) and full-weighted ID (FID) outperform the lightweight model in the multiclass logistic regression setting. The authors also show that distillation can be used to improve the OOD behavior of the model. ","This paper studies the effect of full and lightweight finetuning for natural language generation tasks. In particular, the authors show that full-finetuning in-distribution (ID) and full-weighted ID (FID) outperform the lightweight model in the multiclass logistic regression setting. The authors also show that distillation can be used to improve the OOD behavior of the model. "
3061,SP:28fe2b3deb6a8f24f26d48240da38d280673b8f2,"health CONJUNCTION governance. governance CONJUNCTION health. pointillistically labeled data USED-FOR data - hungry ML algorithms. Data programming USED-FOR probabilistic training labels. domain experts USED-FOR labelling functions. approach USED-FOR iterative and interactive improvement of weakly supervised models. WARM HYPONYM-OF Active Refinement of Weakly Supervised Models. active learning USED-FOR approach. active learning USED-FOR weakly supervised models. probabilistic accuracy EVALUATE-FOR label model. expert labelling functions PART-OF weak supervision model. probabilistic labels USED-FOR downstream classifiers. real - world medical classification datasets EVALUATE-FOR WARM. WARM USED-FOR probabilistic labels. accuracy EVALUATE-FOR probabilistic labels. accuracy EVALUATE-FOR WARM. domain shift CONJUNCTION artificial noise. artificial noise CONJUNCTION domain shift. population characteristics CONJUNCTION noisy initial labelling functions. noisy initial labelling functions CONJUNCTION population characteristics. noisy initial labelling functions FEATURE-OF WARM. population characteristics FEATURE-OF WARM. WARM USED-FOR weakly supervised systems. Method are Supervised machine learning ( ML ), and ML methods. Task are clinical research, and data collection. Generic is framework. OtherScientificTerm are weak supervision, and Gradient updates. ","This paper proposes a method for improving the performance of weakly supervised models. The proposed method is based on Active Refinement of Weakly Supervised Models (WARM), which is an iterative and interactive improvement method for learning a probabilistic training label for a weakly-supervised model. The key idea of WARM is to use domain experts to train a model that is robust to domain shift and noisy initial labelling functions. Experiments show that WARM improves the performance on a number of real-world medical classification datasets.","This paper proposes a method for improving the performance of weakly supervised models. The proposed method is based on Active Refinement of Weakly Supervised Models (WARM), which is an iterative and interactive improvement method for learning a probabilistic training label for a weakly-supervised model. The key idea of WARM is to use domain experts to train a model that is robust to domain shift and noisy initial labelling functions. Experiments show that WARM improves the performance on a number of real-world medical classification datasets."
3077,SP:f663a1e64155f1d4c890a6fefae596f67ef3cb11,"group annotated training data USED-FOR classification model. empirical risk minimization ( ERM ) objective USED-FOR models. it COMPARE ERM. ERM COMPARE it. Group - DRO COMPARE ERM. ERM COMPARE Group - DRO. ERM USED-FOR minority groups. Group - DRO USED-FOR minority groups. algorithm USED-FOR learning of features. algorithm COMPARE baselines. baselines COMPARE algorithm. ERM CONJUNCTION Group - DRO. Group - DRO CONJUNCTION ERM. minority groups FEATURE-OF benchmarks. Group - DRO HYPONYM-OF baselines. benchmarks EVALUATE-FOR Group - DRO. ERM HYPONYM-OF baselines. benchmarks EVALUATE-FOR baselines. benchmarks EVALUATE-FOR algorithm. algorithm USED-FOR smooth nonconvex functions. descent method USED-FOR algorithm. OtherScientificTerm are distribution shift, and learning of shared / common features. Task is domain generalization. Metric is regularized loss. ","This paper proposes a new empirical risk minimization (ERM) objective for group annotated training data. The authors propose a regularized loss for smooth nonconvex functions, which they call Group-DRO. They show that the proposed algorithm outperforms existing ERM and Group DRO algorithms on a number of benchmark datasets. They also show that their algorithm can be applied to minority groups.","This paper proposes a new empirical risk minimization (ERM) objective for group annotated training data. The authors propose a regularized loss for smooth nonconvex functions, which they call Group-DRO. They show that the proposed algorithm outperforms existing ERM and Group DRO algorithms on a number of benchmark datasets. They also show that their algorithm can be applied to minority groups."
3093,SP:faad5fe1eefbcc2e24638383d0bde7ad7975ff4e,"influential features USED-FOR prediction. bivariate methods USED-FOR feature interactions. bivariate methods USED-FOR black - box models. univariate explanation USED-FOR higher - order. feature interactions PART-OF black - box models. univariate explanation USED-FOR explainability. directionality USED-FOR influential features. directional explanations USED-FOR feature interactions. Shapley value explanations USED-FOR bivariate method. IMDB CONJUNCTION Census. Census CONJUNCTION IMDB. CIFAR10 CONJUNCTION IMDB. IMDB CONJUNCTION CIFAR10. method COMPARE state - of - the - art. state - of - the - art COMPARE method. Drug, and gene data EVALUATE-FOR method. Drug, and gene data EVALUATE-FOR state - of - the - art. IMDB EVALUATE-FOR state - of - the - art. IMDB EVALUATE-FOR method. Census EVALUATE-FOR state - of - the - art. Census EVALUATE-FOR method. CIFAR10 EVALUATE-FOR method. CIFAR10 EVALUATE-FOR state - of - the - art. Method are machine learning algorithms, and explanation methods. Generic are they, and graph. OtherScientificTerm are directed graph, and features. ",This paper proposes a bivariate explanation method for black-box models. The proposed method is based on Shapley value explanations (Shapley-value explanations) for feature interactions in a directed graph. The authors show that the proposed method outperforms the state-of-the-art bivariate methods on a number of datasets. ,This paper proposes a bivariate explanation method for black-box models. The proposed method is based on Shapley value explanations (Shapley-value explanations) for feature interactions in a directed graph. The authors show that the proposed method outperforms the state-of-the-art bivariate methods on a number of datasets. 
3109,SP:c8a4254e6fc2d2e7d1d41a76bb64f78f22a8639d,"observed behaviour USED-FOR models of human decision - making. clinical care HYPONYM-OF real - world policies. framework USED-FOR interpretable policy learning. probabilistic tree policies USED-FOR physician actions. Policy Extraction USED-FOR interpretable policy learning. decision Trees ( POETREE ) USED-FOR Policy Extraction. medical history USED-FOR probabilistic tree policies. representation of patient history USED-FOR decision tree policies. complexity USED-FOR modelling task. Fullydifferentiable tree architectures USED-FOR modelling task. recurrence USED-FOR representation of patient history. patient information USED-FOR decision tree policies. policy learning method COMPARE stateof - the - art. stateof - the - art COMPARE policy learning method. policy learning method USED-FOR decision support systems. it USED-FOR decision support systems. real and synthetic medical datasets EVALUATE-FOR stateof - the - art. real and synthetic medical datasets EVALUATE-FOR policy learning method. Method is policy learning approaches. Generic is they. Task are decision - making process, and optimization. ","This paper proposes a new method for learning interpretable policy learning for medical decision-making. The proposed method is based on decision trees (POETREE), which is a framework for learning probabilistic tree policies that can be used to learn interpretable policies for medical care. The method is evaluated on a number of real and synthetic medical datasets, and is shown to outperform existing methods. ","This paper proposes a new method for learning interpretable policy learning for medical decision-making. The proposed method is based on decision trees (POETREE), which is a framework for learning probabilistic tree policies that can be used to learn interpretable policies for medical care. The method is evaluated on a number of real and synthetic medical datasets, and is shown to outperform existing methods. "
3125,SP:5630707c9d0d9e21fce2efddef874e373bfed026,"Data augmentation ( DA ) USED-FOR deep learning models. search USED-FOR automated DA methods. image level FEATURE-OF search. joint optimal augmentation policies USED-FOR patches. Patch AutoAugment HYPONYM-OF fine - grained automated DA approach. Stanford Cars CONJUNCTION FGVC - Aircraft. FGVC - Aircraft CONJUNCTION Stanford Cars. FGVC - Aircraft CONJUNCTION Pascal VOC 2007. Pascal VOC 2007 CONJUNCTION FGVC - Aircraft. CUB-200 - 2011 CONJUNCTION Stanford Cars. Stanford Cars CONJUNCTION CUB-200 - 2011. image classification CONJUNCTION fine - grained image recognition. fine - grained image recognition CONJUNCTION image classification. ImageNet CONJUNCTION CUB-200 - 2011. CUB-200 - 2011 CONJUNCTION ImageNet. CIFAR-100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-100. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. fine - grained image recognition CONJUNCTION object detection. object detection CONJUNCTION fine - grained image recognition. CUB-200 - 2011 CONJUNCTION FGVC - Aircraft. FGVC - Aircraft CONJUNCTION CUB-200 - 2011. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. CIFAR-100 HYPONYM-OF object detection. CIFAR-10 HYPONYM-OF object detection. object detection EVALUATE-FOR method. fine - grained image recognition EVALUATE-FOR method. image classification EVALUATE-FOR method. method COMPARE DA methods. DA methods COMPARE method. computational resources EVALUATE-FOR method. computational resources EVALUATE-FOR DA methods. OtherScientificTerm are DA policies, grid of patches, augmentation policy, semantics, and team reward. Task is exploration of diversity in local regions. Generic are it, and agents. ","This paper proposes a method for fine-grained automated data augmentation (DA) based on patch auto-augmentation. The proposed method is based on the idea of joint optimal augmentation policies (JAPs), which are learned by a team of agents. The method is evaluated on CIFAR-10, ImageNet, CUB-200-2011, and FGVC-Aircraft datasets. The results show that the proposed method outperforms the state-of-the-art in terms of performance and computational efficiency.","This paper proposes a method for fine-grained automated data augmentation (DA) based on patch auto-augmentation. The proposed method is based on the idea of joint optimal augmentation policies (JAPs), which are learned by a team of agents. The method is evaluated on CIFAR-10, ImageNet, CUB-200-2011, and FGVC-Aircraft datasets. The results show that the proposed method outperforms the state-of-the-art in terms of performance and computational efficiency."
3141,SP:bb2a13a4d366140fc0c3e941c354cc674f6a904f,"adversarial vulnerability FEATURE-OF deep neural networks. deep neural networks PART-OF machine learning. machine learning USED-FOR adversarial vulnerability. causality USED-FOR distribution change. causal reasoning USED-FOR distribution change. adversarial attacks FEATURE-OF distribution change. causal formulations USED-FOR intuition of adversarial attacks. causal formulations USED-FOR robust DNNs. causal graph USED-FOR generation process of adversarial examples. adversarial distribution USED-FOR intuition of adversarial attacks. models USED-FOR origin of adversarial vulnerability. spurious correlations USED-FOR origin of adversarial vulnerability. spurious correlations USED-FOR adversarial distribution alignment method. causality USED-FOR adversarial vulnerability. OtherScientificTerm are causal perspective, natural and adversarial distribution, and natural and adversarial distributions. Method is causal understanding. Generic is method. ","This paper studies the problem of adversarial vulnerability of deep neural networks (DNNs) in the context of causal reasoning. The authors propose a new adversarial distribution alignment method based on spurious correlations between the natural and adversarial distributions. The proposed method is based on the notion of ""causal graph"", which is a graph that maps the distribution of the adversarial examples to the causal graph of the natural distribution.  The authors show that the proposed method can be used to identify the origin of the distribution change in DNNs. They also show that their method is robust to adversarial attacks. ","This paper studies the problem of adversarial vulnerability of deep neural networks (DNNs) in the context of causal reasoning. The authors propose a new adversarial distribution alignment method based on spurious correlations between the natural and adversarial distributions. The proposed method is based on the notion of ""causal graph"", which is a graph that maps the distribution of the adversarial examples to the causal graph of the natural distribution.  The authors show that the proposed method can be used to identify the origin of the distribution change in DNNs. They also show that their method is robust to adversarial attacks. "
3157,SP:9f09449a47464efb5458d0732df7664865558e6f,"Continual learning USED-FOR catastrophic forgetting of deep neural networks. network layer FEATURE-OF convolutional filters. filter atoms USED-FOR convolutional filters. filter atom swapping USED-FOR continual learning. filter subspace FEATURE-OF convolutional layer. models USED-FOR forgetting. scheme USED-FOR continual learning. atom swapping framework USED-FOR model ensemble. optimization schemes CONJUNCTION convolutional network structures. convolutional network structures CONJUNCTION optimization schemes. method USED-FOR optimization schemes. method USED-FOR convolutional network structures. benchmark datasets EVALUATE-FOR method. method COMPARE state - of - the - art methods. state - of - the - art methods COMPARE method. accuracy CONJUNCTION scalability. scalability CONJUNCTION accuracy. benchmark datasets EVALUATE-FOR state - of - the - art methods. accuracy EVALUATE-FOR state - of - the - art methods. scalability EVALUATE-FOR state - of - the - art methods. scalability EVALUATE-FOR method. accuracy EVALUATE-FOR method. Method is deep neural networks. OtherScientificTerm are low - rank filter subspace, subspace coefficients, and continual learning settings. ","This paper proposes a method for continual learning of convolutional filters. The proposed method is based on the atom swapping framework, which swaps filter atoms in the low-rank filter subspace of the network layer. The authors show that the proposed method outperforms state-of-the-art methods on several benchmark datasets. ","This paper proposes a method for continual learning of convolutional filters. The proposed method is based on the atom swapping framework, which swaps filter atoms in the low-rank filter subspace of the network layer. The authors show that the proposed method outperforms state-of-the-art methods on several benchmark datasets. "
3173,SP:b806dd540708b39c10d3c165ea7d394a02376805,"Stein variational gradient descent ( SVGD ) HYPONYM-OF deterministic inference algorithm. variance collapse FEATURE-OF SVGD. SVGD update COMPARE gradient descent. gradient descent COMPARE SVGD update. maximum mean discrepancy ( MMD ) objective EVALUATE-FOR SVGD update. maximum mean discrepancy ( MMD ) objective EVALUATE-FOR gradient descent. proportional asymptotic limit FEATURE-OF variance collapse. SVGD USED-FOR variance collapse. SVGD CONJUNCTION MMD - descent. MMD - descent CONJUNCTION SVGD. equilibrium variance USED-FOR SVGD. equilibrium variance USED-FOR MMD - descent. equilibrium variance USED-FOR learning high - dimensional isotropic Gaussians. Task are variance collapse phenomenon, and variance estimation. Method are deterministic updates, and high - dimensional isotropic Gaussians. OtherScientificTerm is near - orthogonality condition. ","This paper studies the variance collapse phenomenon of Stein variational gradient descent (SVGD). The authors show that under certain assumptions, SVGD is near-orthogonal to the equilibrium variance of MMD-decay. The authors also show that the variance of SVGD update is proportional asymptotically to the maximum mean discrepancy (MMD) objective. ","This paper studies the variance collapse phenomenon of Stein variational gradient descent (SVGD). The authors show that under certain assumptions, SVGD is near-orthogonal to the equilibrium variance of MMD-decay. The authors also show that the variance of SVGD update is proportional asymptotically to the maximum mean discrepancy (MMD) objective. "
3189,SP:3721f1b12d87e95f5aa4c977a1714d5c54cb70f7,"Noisy labels ( NL ) CONJUNCTION adversarial examples. adversarial examples CONJUNCTION Noisy labels ( NL ). measure USED-FOR intrinsic geometric property. AT COMPARE NL. NL COMPARE AT. sample selection USED-FOR NL. PGD steps USED-FOR sample selection. AT COMPARE training. training COMPARE AT. NL COMPARE training. training COMPARE NL. AT HYPONYM-OF NL correction. AT USED-FOR NL. smoothing effects FEATURE-OF AT. AT USED-FOR general - purpose robust learning criterion. NL USED-FOR AT. natural accuracy EVALUATE-FOR AT. Generic are models, and they. OtherScientificTerm are projected gradient descent ( PGD ) steps, adversarial example, class boundary, noisy - class boundary, and NL corrections. Metric is robustness. Material is natural data. ",This paper studies the problem of robustness to adversarial examples. The authors propose a general-purpose robust learning criterion based on the intrinsic geometric property of noisy labels (NL). The authors show that the smoothing effect of the adversarial example can be used to improve the robustness of the model. They also show that AT is an alternative to NL correction. ,This paper studies the problem of robustness to adversarial examples. The authors propose a general-purpose robust learning criterion based on the intrinsic geometric property of noisy labels (NL). The authors show that the smoothing effect of the adversarial example can be used to improve the robustness of the model. They also show that AT is an alternative to NL correction. 
3205,SP:2bcf42173d9d82fb3e517405deba4aa3d6f9d8d6,"language processing CONJUNCTION protein folding. protein folding CONJUNCTION language processing. classification CONJUNCTION language processing. language processing CONJUNCTION classification. Neural network models USED-FOR tasks. classification HYPONYM-OF tasks. language processing HYPONYM-OF tasks. protein folding HYPONYM-OF tasks. adversarial inputs FEATURE-OF reliability. small input perturbations HYPONYM-OF adversarial inputs. neural networks USED-FOR critical systems. expected robustness EVALUATE-FOR neural network model. statistical method EVALUATE-FOR neural network model. statistical method USED-FOR expected robustness. random input perturbation USED-FOR misclassification. robustness EVALUATE-FOR models. neural network certification USED-FOR safety - critical applications. risk and robustness assessments USED-FOR risk mitigation. risk mitigation USED-FOR neural network certification. categorial basis USED-FOR risk mitigation. categorial basis USED-FOR risk and robustness assessments. Generic are model, method, and approach. OtherScientificTerm is Adversarial inputs. Method are Robustness Measurement and Assessment ( RoMA ), RoMA, verification methods, and classification network. Metric are model ’s robustness, robustness levels, and categorial robustness. ","This paper proposes a new method for risk and robustness assessment of neural networks. The authors propose a statistical method to measure the expected robustness of a neural network model. The proposed method is based on the categorial robustness, which is a measure of the risk of misclassification of a classifier. The method is evaluated on a variety of tasks, including classification, language processing, and protein folding. ","This paper proposes a new method for risk and robustness assessment of neural networks. The authors propose a statistical method to measure the expected robustness of a neural network model. The proposed method is based on the categorial robustness, which is a measure of the risk of misclassification of a classifier. The method is evaluated on a variety of tasks, including classification, language processing, and protein folding. "
3221,SP:6ba17dd4b31a39478abd995df894447675f2f974,"chunking USED-FOR cognitive science. HCM USED-FOR representations. non - i.i.d sequential data USED-FOR HCM. non - i.i.d sequential data USED-FOR representations. learning guarantees USED-FOR HCM. approaches USED-FOR representation learning. cognitive science CONJUNCTION theories of chunking. theories of chunking CONJUNCTION cognitive science. theories of chunking USED-FOR approaches. OtherScientificTerm are proximity, minimal atomic sequential units, sequential dependence, and partial representational structure. Method are hierarchical chunking model ( HCM ), and hierarchy of chunk representation. ","This paper proposes a hierarchical chunking model (HCM) for representation learning. The proposed method is based on the hierarchical clustering model (HCM), which is an extension of the HCM framework. The authors show that HCM is able to learn a hierarchy of chunk representations, which can be used to learn representations for non-i.i.d sequential data. They also provide theoretical guarantees for the learning of HCM. ","This paper proposes a hierarchical chunking model (HCM) for representation learning. The proposed method is based on the hierarchical clustering model (HCM), which is an extension of the HCM framework. The authors show that HCM is able to learn a hierarchy of chunk representations, which can be used to learn representations for non-i.i.d sequential data. They also provide theoretical guarantees for the learning of HCM. "
3237,SP:625e3908502fd5be949bb915116ed7569ba84298,"gradient flow FEATURE-OF neural reparametrization. graph convolutional network ( GCN ) USED-FOR neural network architecture. GCN USED-FOR aggregation function. gradients of the loss function USED-FOR aggregation function. network synchronization CONJUNCTION persistent homology optimization. persistent homology optimization CONJUNCTION network synchronization. method USED-FOR optimization problems. persistent homology optimization HYPONYM-OF optimization problems. network synchronization HYPONYM-OF optimization problems. OtherScientificTerm are optimization variables, maximum speed up, and Hessian. Method is neural network. Task is optimization. ","This paper studies the problem of gradient flow in graph convolutional networks (GCN). The authors show that the gradient flow of a GCN can be decomposed into two parts: (1) the gradient of the loss function and (2) the gradients of the aggregation function of the GCN. The authors then propose a new algorithm for gradient flow. The proposed algorithm is evaluated on a variety of optimization problems, including persistent homology optimization and network synchronization.","This paper studies the problem of gradient flow in graph convolutional networks (GCN). The authors show that the gradient flow of a GCN can be decomposed into two parts: (1) the gradient of the loss function and (2) the gradients of the aggregation function of the GCN. The authors then propose a new algorithm for gradient flow. The proposed algorithm is evaluated on a variety of optimization problems, including persistent homology optimization and network synchronization."
3253,SP:80346eeafb0a6d1d556c304a3f8753aff037469b,Deep convolutional neural networks ( DCNNs ) USED-FOR image data. DCNNs USED-FOR supervised learning of image data. pre - labeled images USED-FOR real - world problems. SVMnet USED-FOR non - parametric image classification. SVMnet HYPONYM-OF method. method COMPARE DCNNs. DCNNs COMPARE method. accuracy EVALUATE-FOR DCNNs. SVMs COMPARE neural networks. neural networks COMPARE SVMs. accuracy EVALUATE-FOR method. DCNN architectures COMPARE SVMnet. SVMnet COMPARE DCNN architectures. DCNN architectures COMPARE SVMnet. SVMnet COMPARE DCNN architectures. ResNet-50 COMPARE SVMnet. SVMnet COMPARE ResNet-50. accuracy EVALUATE-FOR SVMnet. ResNet-50 HYPONYM-OF DCNN architectures. Material is labeled “ ground truth ” images. OtherScientificTerm is real - world cases. ,"This paper proposes a new method for non-parametric image classification. The proposed method, called SVMnet, is based on the SVMNet architecture. The authors show that the proposed method is able to achieve better performance than the state-of-the-art DCNN and SVMs on a variety of image classification tasks. ","This paper proposes a new method for non-parametric image classification. The proposed method, called SVMnet, is based on the SVMNet architecture. The authors show that the proposed method is able to achieve better performance than the state-of-the-art DCNN and SVMs on a variety of image classification tasks. "
3269,SP:a18f4697f350a864866dac871f581b8fc67e8088,"large graphs USED-FOR GNNs. distributed algorithm USED-FOR GNN training. centralized storage CONJUNCTION model learning. model learning CONJUNCTION centralized storage. excessive communication costs CONJUNCTION large memory overheads. large memory overheads CONJUNCTION excessive communication costs. excessive communication costs FEATURE-OF distributed GNN training methods. Learn Locally, Correct Globally ( LLCG ) HYPONYM-OF distributed GNN training technique. local machine USED-FOR GNN. local machine PART-OF LLCG. local data USED-FOR GNN. Global Server Corrections USED-FOR locally learned models. Global Server Corrections USED-FOR server. distributed methods USED-FOR GNNs. periodic model averaging USED-FOR distributed methods. global corrections USED-FOR fast convergence rate. global corrections USED-FOR residual error. real - world datasets EVALUATE-FOR LLCG. efficiency EVALUATE-FOR LLCG. Method are Graph Neural Networks ( GNNs ), and locally trained model. OtherScientificTerm are graph, privacy concern, dependency between nodes, node dependency, and irreducible residual error. Metric are scalability, and communication and memory overhead. ","This paper proposes a new distributed GNN training method, called Learn Locally, Correct Globally (LLG). The proposed method is based on the idea of global server corrections (GCS), which can be applied to both local and global GNNs. The authors show that the proposed method outperforms the baselines in terms of both communication and memory overhead.","This paper proposes a new distributed GNN training method, called Learn Locally, Correct Globally (LLG). The proposed method is based on the idea of global server corrections (GCS), which can be applied to both local and global GNNs. The authors show that the proposed method outperforms the baselines in terms of both communication and memory overhead."
3285,SP:6805f2245484fc91b5c13aa5f09e5478b810f97f,"model USED-FOR Anytime inference. image classification PART-OF anytime visual recognition. unified and end - toend model approach USED-FOR anytime pixel - level recognition. depth and spatial resolution FEATURE-OF features. redesigned exit architecture CONJUNCTION spatial adaptivity. spatial adaptivity CONJUNCTION redesigned exit architecture. full model USED-FOR anytime inference. spatial adaptivity USED-FOR anytime inference. spatial adaptivity USED-FOR full model. redesigned exit architecture USED-FOR full model. accuracy EVALUATE-FOR full model. semantic segmentation EVALUATE-FOR approach. approach USED-FOR anytime inference. Cityscapes semantic segmentation CONJUNCTION MPII human pose estimation. MPII human pose estimation CONJUNCTION Cityscapes semantic segmentation. Cityscapes semantic segmentation EVALUATE-FOR approach. MPII human pose estimation EVALUATE-FOR approach. total FLOPs EVALUATE-FOR models. total FLOPs EVALUATE-FOR approach. accuracy - computation curve EVALUATE-FOR method. deep equilibrium networks CONJUNCTION feature - based stochastic sampling approach. feature - based stochastic sampling approach CONJUNCTION deep equilibrium networks. method COMPARE them. them COMPARE method. accuracy - computation curve EVALUATE-FOR them. method COMPARE deep equilibrium networks. deep equilibrium networks COMPARE method. method COMPARE feature - based stochastic sampling approach. feature - based stochastic sampling approach COMPARE method. OtherScientificTerm are exits, and prior predictions. Metric is total computation. Method is spatially adaptive approach. Task is human pose estimation. ","This paper proposes a novel approach for any-time inference. The authors propose a unified and end-to-end model approach for anytime pixel-level recognition. The proposed approach is based on a new exit architecture and spatial adaptivity, which allows the model to adapt to changes in depth and spatial resolution of features. Experiments show that the proposed approach outperforms the state-of-the-art in terms of accuracy and computation. ","This paper proposes a novel approach for any-time inference. The authors propose a unified and end-to-end model approach for anytime pixel-level recognition. The proposed approach is based on a new exit architecture and spatial adaptivity, which allows the model to adapt to changes in depth and spatial resolution of features. Experiments show that the proposed approach outperforms the state-of-the-art in terms of accuracy and computation. "
3301,SP:1a75aaef7ba0d2de5804514f0de39d9c769f419b,Neural Processes USED-FOR stochastic processes. neural networks USED-FOR stochastic processes. Modeling functional uncertainty PART-OF learning stochastic processes. bootstrap method USED-FOR functional uncertainty. Gaussian assumption FEATURE-OF latent variable. bootstrap method USED-FOR Bootstrapping Neural Processes ( B(A)NP ). B(A)NP USED-FOR bootstrapping. ANP CONJUNCTION BANP. BANP CONJUNCTION ANP. NeuBANP USED-FOR bootstrap distribution of random functions. encoder CONJUNCTION loss function. loss function CONJUNCTION encoder. Bayesian optimization CONJUNCTION contextual multi - armed bandit. contextual multi - armed bandit CONJUNCTION Bayesian optimization. Bayesian optimization EVALUATE-FOR models. sequential decision - making tasks EVALUATE-FOR NP methods. NeuBANP COMPARE NP methods. NP methods COMPARE NeuBANP. functional uncertainty modeling EVALUATE-FOR NeuBANP. functional uncertainty modeling EVALUATE-FOR method. sequential decision - making tasks EVALUATE-FOR NeuBANP. Generic is approach. Method is Neural Bootstrapping Attentive Neural Processes ( NeuBANP ). ,"This paper proposes a new bootstrap method for bootstrapping neural processes (B(A)NP). The proposed method is based on the bootstrap distribution of random functions of the latent variable, which is a Gaussian assumption. The authors show that the proposed method outperforms existing bootstrap methods on a number of tasks, including Bayesian optimization, contextual multi-armed bandit, and sequential decision-making tasks.","This paper proposes a new bootstrap method for bootstrapping neural processes (B(A)NP). The proposed method is based on the bootstrap distribution of random functions of the latent variable, which is a Gaussian assumption. The authors show that the proposed method outperforms existing bootstrap methods on a number of tasks, including Bayesian optimization, contextual multi-armed bandit, and sequential decision-making tasks."
3317,SP:34e1b51ff5d524490332aed51b9c411209c89a20,"promoter classification CONJUNCTION transaction factor binding sites prediction. transaction factor binding sites prediction CONJUNCTION promoter classification. regulatory genome modeling USED-FOR regulatory downstream tasks. regulatory genome modeling PART-OF genome biology research. promoter classification HYPONYM-OF regulatory downstream tasks. transaction factor binding sites prediction HYPONYM-OF regulatory downstream tasks. deep learning methods USED-FOR genome sequences. approach USED-FOR pre - training genome data. genome data USED-FOR multi - modal and self - supervised manner. multi - modal and self - supervised manner USED-FOR pre - training genome data. robustness EVALUATE-FOR model. pre - training tasks USED-FOR model. pre - training tasks USED-FOR robustness. genome sequences USED-FOR ATAC - seq dataset. ATAC - seq dataset EVALUATE-FOR model. transaction factor binding sites prediction CONJUNCTION disease risk estimation. disease risk estimation CONJUNCTION transaction factor binding sites prediction. disease risk estimation CONJUNCTION splicing sites prediction. splicing sites prediction CONJUNCTION disease risk estimation. promoter classification CONJUNCTION transaction factor binding sites prediction. transaction factor binding sites prediction CONJUNCTION promoter classification. regulatory downstream tasks EVALUATE-FOR GeneBERT. splicing sites prediction HYPONYM-OF regulatory downstream tasks. promoter classification HYPONYM-OF regulatory downstream tasks. transaction factor binding sites prediction HYPONYM-OF regulatory downstream tasks. disease risk estimation HYPONYM-OF regulatory downstream tasks. OtherScientificTerm is regulatory elements. Generic is them. Task is biological applications. Material are 1d sequence of genome data, and large - scale regulatory genomics data. ","This paper proposes GeneBERT, a method for pre-training a model for regulatory genome modeling. The proposed method is based on multi-modal and self-supervised learning. The method is evaluated on the ATAC-seq dataset and is shown to be able to achieve state-of-the-art performance on several downstream tasks. ","This paper proposes GeneBERT, a method for pre-training a model for regulatory genome modeling. The proposed method is based on multi-modal and self-supervised learning. The method is evaluated on the ATAC-seq dataset and is shown to be able to achieve state-of-the-art performance on several downstream tasks. "
3333,SP:841b12443d0274e34b78940f220b17d36798899b,"method USED-FOR detecting OOD samples. IGEOOD USED-FOR detecting OOD samples. IGEOOD HYPONYM-OF method. IGEOOD USED-FOR pre - trained neural network. geodesic ( FisherRao ) distance USED-FOR discriminator. confidence scores CONJUNCTION features. features CONJUNCTION confidence scores. features PART-OF deep neural network. confidence scores USED-FOR discriminator. logits outputs USED-FOR confidence scores. features USED-FOR discriminator. deep neural network USED-FOR discriminator. IGEOOD COMPARE state - of - the - art methods. state - of - the - art methods COMPARE IGEOOD. Method are machine learning ( ML ) systems, and ML model. OtherScientificTerm are OOD samples, and data distributions. Material is OOD data. ",This paper proposes a method for detecting out-of-distribution (OOD) samples in machine learning systems. The method is based on the geodesic (FisherRao) distance between features and confidence scores. The proposed method is evaluated on synthetic and real-world datasets. ,This paper proposes a method for detecting out-of-distribution (OOD) samples in machine learning systems. The method is based on the geodesic (FisherRao) distance between features and confidence scores. The proposed method is evaluated on synthetic and real-world datasets. 
3349,SP:2fdca838ac3453e44cff395f1b760d839a5813bf,"translations CONJUNCTION rotations. rotations CONJUNCTION translations. group FEATURE-OF identity - preserving transformations. identity - preserving transformations USED-FOR representations of objects. translations HYPONYM-OF group. translations HYPONYM-OF identity - preserving transformations. group equivariance USED-FOR representation. Cover ’s Function Counting Theorem USED-FOR linearly separable and group - invariant binary dichotomies. linearly separable and group - invariant binary dichotomies USED-FOR equivariant representations of objects. element - wise nonlinearities CONJUNCTION global and local pooling. global and local pooling CONJUNCTION element - wise nonlinearities. convolutions CONJUNCTION element - wise nonlinearities. element - wise nonlinearities CONJUNCTION convolutions. relation USED-FOR operations. global and local pooling HYPONYM-OF operations. convolutions HYPONYM-OF operations. element - wise nonlinearities HYPONYM-OF operations. OtherScientificTerm are Equivariance, separable dichotomies, group action, and local pooling. Generic is theory. ","This paper studies group equivariance in the context of identity-preserving transformations and group action. The authors show that under certain conditions, the group action is invariant to separable and group-invariant binary dichotomies. They also show that global pooling and global and local pooling are equivariant to group action, and show that element-wise nonlinearities are also invariant. Finally, the authors provide a generalization of the Cover’s Function Counting Theorem to the case of identity preserving transformations.","This paper studies group equivariance in the context of identity-preserving transformations and group action. The authors show that under certain conditions, the group action is invariant to separable and group-invariant binary dichotomies. They also show that global pooling and global and local pooling are equivariant to group action, and show that element-wise nonlinearities are also invariant. Finally, the authors provide a generalization of the Cover’s Function Counting Theorem to the case of identity preserving transformations."
3365,SP:47889067620e5ac2e304681769af9d1d930f6d2b,"models USED-FOR machine learning objectives. concept drift CONJUNCTION mitigating biases. mitigating biases CONJUNCTION concept drift. robustness HYPONYM-OF machine learning objectives. mitigating biases HYPONYM-OF machine learning objectives. concept drift HYPONYM-OF machine learning objectives. counterfactual explanations CONJUNCTION concept activation vectors. concept activation vectors CONJUNCTION counterfactual explanations. it USED-FOR models. pretrained models USED-FOR approach. prior ideas USED-FOR CCE. counterfactual explanations HYPONYM-OF prior ideas. concept activation vectors HYPONYM-OF prior ideas. CCE USED-FOR spurious correlation. spurious correlations USED-FOR models. CCE USED-FOR models. data USED-FOR models. medical applications EVALUATE-FOR CCE. Generic are model, and systematic approach. Method are conceptual counterfactual explanations ( CCE ), and classifier. OtherScientificTerm are human - understandable concepts, faint stripes, and model mistakes. ",This paper proposes a systematic approach for learning counterfactual explanations (CCEs) that can be used to mitigate spurious correlations in the training data. CCE is based on the idea of concept activation vectors (CAVs) that are used as prior ideas in prior works. The authors show that CCE can reduce spurious correlations between the training and test data. They also show that the proposed approach is able to reduce spurious correlation between the test data and CCEs. ,This paper proposes a systematic approach for learning counterfactual explanations (CCEs) that can be used to mitigate spurious correlations in the training data. CCE is based on the idea of concept activation vectors (CAVs) that are used as prior ideas in prior works. The authors show that CCE can reduce spurious correlations between the training and test data. They also show that the proposed approach is able to reduce spurious correlation between the test data and CCEs. 
3381,SP:4aa5f00830fda36b6ca2f53d88c3a8a963058ec0,"3D point cloud applications EVALUATE-FOR Kernel Point Convolution ( KPConv ). KPConv network USED-FOR mobile scenarios. KPConv USED-FOR neighbor - kernel correlation. Euclidean distance USED-FOR neighbor - kernel correlation. module USED-FOR KPConv. efficiency EVALUATE-FOR KPConv. Mobile Attention Kernel Point Convolution ( MAKPConv ) USED-FOR KPConv. Mobile Attention Kernel Point Convolution ( MAKPConv ) HYPONYM-OF module. efficiency EVALUATE-FOR module. depthwise kernel USED-FOR resource consumption. depthwise kernel USED-FOR MAKPConv. Inverted Residual Bottleneck ( IRB ) USED-FOR design space. MAKPConv USED-FOR 3D networks. Wide & Deep Predictor USED-FOR dense and sparse neural architecture representations. carrying feature engineering USED-FOR neural architecture representations. Wide & Deep Predictor USED-FOR error in performance prediction. searchable features USED-FOR carrying feature engineering. predictor USED-FOR design space. 3D point cloud classification and segmentation benchmarks EVALUATE-FOR NAS - crafted MAKPConv network. NAScrafted model SPVNAS COMPARE NAS - crafted MAKPConv network. NAS - crafted MAKPConv network COMPARE NAScrafted model SPVNAS. Multiply - Accumulates EVALUATE-FOR NAS - crafted MAKPConv network. mIOU EVALUATE-FOR NAS - crafted MAKPConv network. OtherScientificTerm are kernel relationship, and weak representation power. Method is Neighbor - Kernel attention. Metric is representation power. ",This paper proposes a new module for kernel point convolution (KPConv) for 3D point cloud applications. The proposed module is based on the Inverted Residual Bottleneck (IRB) method. The authors show that the proposed module can reduce the computational cost of KPConv by using a depthwise kernel instead of a depth-wise kernel. They also show that their proposed module outperforms the state-of-the-art NAS-crafted model SPVNAS on several benchmark datasets.,This paper proposes a new module for kernel point convolution (KPConv) for 3D point cloud applications. The proposed module is based on the Inverted Residual Bottleneck (IRB) method. The authors show that the proposed module can reduce the computational cost of KPConv by using a depthwise kernel instead of a depth-wise kernel. They also show that their proposed module outperforms the state-of-the-art NAS-crafted model SPVNAS on several benchmark datasets.
3397,SP:bf7d2e765c435a943ec9257cfa43d070a64c2b67,robust overfitting CONJUNCTION robustness overestimation. robustness overestimation CONJUNCTION robust overfitting. robustness overestimation CONJUNCTION robustness - accuracy trade - off. robustness - accuracy trade - off CONJUNCTION robustness overestimation. problems PART-OF adversarial training. robustness - accuracy trade - off HYPONYM-OF problems. robust overfitting HYPONYM-OF problems. robustness overestimation HYPONYM-OF problems. lowquality samples PART-OF dataset. strategy USED-FOR data quality. learning behaviors USED-FOR strategy. learning behaviors USED-FOR data quality. data quality CONJUNCTION problems. problems CONJUNCTION data quality. problems FEATURE-OF adversarial training. data quality CONJUNCTION adversarial training. adversarial training CONJUNCTION data quality. robust overfitting CONJUNCTION robustness overestimation. robustness overestimation CONJUNCTION robust overfitting. Material is low - quality data. Metric is adversarial robustness. ,"This paper studies the problem of adversarial robustness in the context of low-quality data. Specifically, the authors study the trade-off between robustness and robustness-accuracy trade-offs in adversarial training. The authors show that the tradeoff between the two can be seen as a function of the number of samples in the dataset and the quality of the training data. They then propose a new strategy to mitigate this tradeoff. The proposed strategy is based on the observation that the robustness of the data is not necessarily correlated with the data quality. The paper then proposes a new algorithm to mitigate the trade off between the adversarial accuracy and data quality, and shows that the proposed algorithm outperforms the baselines in terms of robustness. ","This paper studies the problem of adversarial robustness in the context of low-quality data. Specifically, the authors study the trade-off between robustness and robustness-accuracy trade-offs in adversarial training. The authors show that the tradeoff between the two can be seen as a function of the number of samples in the dataset and the quality of the training data. They then propose a new strategy to mitigate this tradeoff. The proposed strategy is based on the observation that the robustness of the data is not necessarily correlated with the data quality. The paper then proposes a new algorithm to mitigate the trade off between the adversarial accuracy and data quality, and shows that the proposed algorithm outperforms the baselines in terms of robustness. "
3413,SP:99a36b28752bfc101877bfd0da436e6fb19c69d3,"neural network USED-FOR multivariate functions of bounded second mixed derivatives. Korobov functions HYPONYM-OF multivariate functions of bounded second mixed derivatives. upper bounds USED-FOR shallow and deep neural networks. quantities USED-FOR shallow and deep neural networks. upper bounds USED-FOR quantities. bounds FEATURE-OF activation functions. ReLU HYPONYM-OF activation functions. continuous function approximator USED-FOR Korobov functions. neural networks HYPONYM-OF near - optimal function approximators. OtherScientificTerm are training parameters, and curse of dimensionality. ","This paper considers the problem of learning multivariate functions of bounded second mixed derivatives. The authors consider the case where the training parameters of a deep neural network are bounded second, and the function approximator is a continuous function approximated by a ReLU function. They show that for any function $f$ that is bounded second in dimension $d$ and $n$ in dimension $\mathcal{O}(\sqrt{d}(d)$, there exists an upper bound on the dimensionality of $f$. The authors show that this upper bound holds for both shallow and deep neural networks. They also show that it holds for ReLU functions.","This paper considers the problem of learning multivariate functions of bounded second mixed derivatives. The authors consider the case where the training parameters of a deep neural network are bounded second, and the function approximator is a continuous function approximated by a ReLU function. They show that for any function $f$ that is bounded second in dimension $d$ and $n$ in dimension $\mathcal{O}(\sqrt{d}(d)$, there exists an upper bound on the dimensionality of $f$. The authors show that this upper bound holds for both shallow and deep neural networks. They also show that it holds for ReLU functions."
3429,SP:a0112febb28e518e87142e7cbb7e3586d06cae0b,"Populations USED-FOR language. agent population size FEATURE-OF speaker - listener Lewis Game. agent population size USED-FOR emergent language properties. training speed CONJUNCTION network capacity. network capacity CONJUNCTION training speed. speaker - listener asymmetry USED-FOR language structure. network capacity HYPONYM-OF diversity factors. training speed HYPONYM-OF diversity factors. relative difference of factors USED-FOR emergent language properties. Material are sociolinguistic literature, and structured languages. Method is neural agents. OtherScientificTerm are agent community, population heterogeneity, confounding factors, training speed heterogeneities, and simulated communities. ",This paper studies the emergent language properties of the speaker-listener Lewis Game. The authors show that the agent population size and training speed have a significant impact on the emergence of language properties. They also show that training speed and network capacity are important factors for the diversity of the language. ,This paper studies the emergent language properties of the speaker-listener Lewis Game. The authors show that the agent population size and training speed have a significant impact on the emergence of language properties. They also show that training speed and network capacity are important factors for the diversity of the language. 
3445,SP:462112ea1a59ab8101ed9d908c5d838edeb844ca,"Graph Neural Networks ( GNNs ) USED-FOR node classification task. node features CONJUNCTION graph topology. graph topology CONJUNCTION node features. node features USED-FOR Graph Neural Networks ( GNNs ). heterophilic graphs EVALUATE-FOR models. homophily FEATURE-OF GNNs. polynomial graph filters USED-FOR models. models USED-FOR polynomials. spectrum FEATURE-OF adaptive polynomial filters. model USED-FOR filter. classification accuracy EVALUATE-FOR model. node classification task EVALUATE-FOR polynomials. model COMPARE polynomial filter - based approaches. polynomial filter - based approaches COMPARE model. model COMPARE models. models COMPARE model. models COMPARE polynomial filter - based approaches. polynomial filter - based approaches COMPARE models. OtherScientificTerm are connected nodes, overdetermined system of equations, and eigencomponents. Method are polynomial graph filter models, eigendecomposition of the graph, and latent polynomial filters. Material is anonymized code. ","This paper studies the problem of node classification on heterophilic graphs. The authors propose an adaptive polynomial graph filter model that is able to learn the eigendecomposition of the graph, eigencomponents of the eigenfunctions, and latent polynomials of the nodes. The proposed model is evaluated on the node classification task and shows that the proposed model outperforms existing methods. ","This paper studies the problem of node classification on heterophilic graphs. The authors propose an adaptive polynomial graph filter model that is able to learn the eigendecomposition of the graph, eigencomponents of the eigenfunctions, and latent polynomials of the nodes. The proposed model is evaluated on the node classification task and shows that the proposed model outperforms existing methods. "
3461,SP:903545b1b340ec5c13070e0f25f550c444de4124,biomedical structure prediction CONJUNCTION social relationship analysis. social relationship analysis CONJUNCTION biomedical structure prediction. graphs FEATURE-OF Shortest Distance Queries ( SDQs ). Shortest Distance Queries ( SDQs ) HYPONYM-OF network analysis. social relationship analysis HYPONYM-OF network analysis. biomedical structure prediction HYPONYM-OF network analysis. Approximate algorithms of SDQs USED-FOR complex graph applications. reduced complexity FEATURE-OF Approximate algorithms of SDQs. approaches USED-FOR embedding - based distance prediction. accuracy EVALUATE-FOR embedding - based distance prediction. efficiency EVALUATE-FOR embedding - based distance prediction. truncated random walk CONJUNCTION Pointwise Mutual Information ( PMI)-based optimization. Pointwise Mutual Information ( PMI)-based optimization CONJUNCTION truncated random walk. predictor USED-FOR global extraction of nodes ’ mutual shortest distance. Pointwise Mutual Information ( PMI)-based optimization USED-FOR Embedding - based distance prediction. truncated random walk USED-FOR Embedding - based distance prediction. Random walk HYPONYM-OF unstrained node sequence. limited distance exploration FEATURE-OF Random walk. graph domain FEATURE-OF intrinsic metric. distance range FEATURE-OF Betweenness Centrality(BC)-based random walk. intrinsic metric USED-FOR distance range. intrinsic metric EVALUATE-FOR Betweenness Centrality(BC)-based random walk. steering optimization objective USED-FOR global distance matrix. strategy USED-FOR global distance matrix. maximum likelihood optimization COMPARE PMI - based optimization. PMI - based optimization COMPARE maximum likelihood optimization. strategy USED-FOR distance relation. Distance Resampling ( DR ) COMPARE PMI - based optimization. PMI - based optimization COMPARE Distance Resampling ( DR ). maximum likelihood optimization USED-FOR Distance Resampling ( DR ). walk paths USED-FOR Distance Resampling ( DR ). steering optimization objective USED-FOR strategy. steering optimization objective USED-FOR distance relation. method COMPARE methods. methods COMPARE method. realworld graph datasets USED-FOR SDQ problems. SDQ problems EVALUATE-FOR method. SDQ problems EVALUATE-FOR methods. realworld graph datasets EVALUATE-FOR method. realworld graph datasets,This paper proposes a new method for embedding-based distance prediction in graphs. The proposed method is based on Pointwise Mutual Information (PMI)-based optimization and truncated random walk. The authors show that the proposed method outperforms existing methods in terms of accuracy and reduced complexity. They also show that their method can be applied to a variety of graph tasks.,This paper proposes a new method for embedding-based distance prediction in graphs. The proposed method is based on Pointwise Mutual Information (PMI)-based optimization and truncated random walk. The authors show that the proposed method outperforms existing methods in terms of accuracy and reduced complexity. They also show that their method can be applied to a variety of graph tasks.
3477,SP:13db440061fed785f05bb41d0767225403ecf7a1,"fact - checking CONJUNCTION open dialogue. open dialogue CONJUNCTION fact - checking. question answering CONJUNCTION fact - checking. fact - checking CONJUNCTION question answering. web corpus USED-FOR knowledge - dependent downstream tasks. Large Language Models ( LMs ) USED-FOR world knowledge. open dialogue HYPONYM-OF knowledge - dependent downstream tasks. question answering HYPONYM-OF knowledge - dependent downstream tasks. fact - checking HYPONYM-OF knowledge - dependent downstream tasks. world knowledge PART-OF LMs. Continual Knowledge Learning ( CKL ) HYPONYM-OF continual learning ( CL ) problem. retention of time - invariant world knowledge CONJUNCTION update of outdated knowledge. update of outdated knowledge CONJUNCTION retention of time - invariant world knowledge. update of outdated knowledge CONJUNCTION acquisition of new knowledge. acquisition of new knowledge CONJUNCTION update of outdated knowledge. metric USED-FOR retention of time - invariant world knowledge. metric USED-FOR acquisition of new knowledge. metric USED-FOR update of outdated knowledge. benchmark USED-FOR retention of time - invariant world knowledge. benchmark CONJUNCTION metric. metric CONJUNCTION benchmark. CKL USED-FOR ever - changing LMs1. OtherScientificTerm are real - world scenarios, catastrophic forgetting, invariant knowledge, and knowledge forgetting. Task is maintenance of ever - changing LMs. Generic are baselines, and CL setups. Method is parameter expansion. ","This paper studies the continual learning (CL) problem, where the goal is to maintain invariant knowledge in the context of a large language model (LMs). The authors propose a new benchmark for continual knowledge learning (CKL) that measures the retention of time-invariant world knowledge, acquisition of new knowledge, and update of outdated knowledge. The benchmark is based on a web corpus of knowledge-dependent downstream tasks (fact-checking, open dialogue, question answering). ","This paper studies the continual learning (CL) problem, where the goal is to maintain invariant knowledge in the context of a large language model (LMs). The authors propose a new benchmark for continual knowledge learning (CKL) that measures the retention of time-invariant world knowledge, acquisition of new knowledge, and update of outdated knowledge. The benchmark is based on a web corpus of knowledge-dependent downstream tasks (fact-checking, open dialogue, question answering). "
3493,SP:639fd88482330389019fb5be7446a909b99a8609,"approach USED-FOR supervised learning task. Decision trees USED-FOR supervised learning task. Decision trees USED-FOR applications. medical imaging CONJUNCTION computer vision. computer vision CONJUNCTION medical imaging. computer vision HYPONYM-OF applications. medical imaging HYPONYM-OF applications. feature CONJUNCTION threshold. threshold CONJUNCTION feature. exhaustive search algorithm USED-FOR criterion minimization problem. stochastic approach USED-FOR criterion minimization. algorithm COMPARE exhaustive search. exhaustive search COMPARE algorithm. algorithm COMPARE decision tree learning methods. decision tree learning methods COMPARE algorithm. algorithm COMPARE baseline non - stochastic approach. baseline non - stochastic approach COMPARE algorithm. baseline non - stochastic approach HYPONYM-OF decision tree learning methods. algorithm COMPARE baseline algorithm. baseline algorithm COMPARE algorithm. accuracy CONJUNCTION computational cost. computational cost CONJUNCTION accuracy. computational cost EVALUATE-FOR algorithm. accuracy EVALUATE-FOR algorithm. computational cost EVALUATE-FOR baseline algorithm. accuracy EVALUATE-FOR baseline algorithm. algorithm USED-FOR Haar tree. MNIST dataset FEATURE-OF Haar tree. tree COMPARE axis - aligned tree. axis - aligned tree COMPARE tree. MNIST COMPARE axis - aligned tree. axis - aligned tree COMPARE MNIST. MNIST EVALUATE-FOR tree. test accuracy EVALUATE-FOR tree. OtherScientificTerm are leaf nodes, stopping criterion, node, features, and criterion. Method is oblique trees. Metric is training times. ","This paper proposes a new algorithm for decision tree learning. The algorithm is based on a stochastic approach to the criterion minimization problem. The main idea is to use an exhaustive search algorithm to find the node that minimizes the threshold of the criterion. The authors show that their algorithm outperforms the baseline non-stochastic approach in terms of accuracy, computational cost, and test accuracy. They also show that the algorithm is computationally efficient.","This paper proposes a new algorithm for decision tree learning. The algorithm is based on a stochastic approach to the criterion minimization problem. The main idea is to use an exhaustive search algorithm to find the node that minimizes the threshold of the criterion. The authors show that their algorithm outperforms the baseline non-stochastic approach in terms of accuracy, computational cost, and test accuracy. They also show that the algorithm is computationally efficient."
3509,SP:7f2640f18294519a5abb1daaa226800d2377a5e0,Learning rate schedulers USED-FOR deep neural networks. SGD USED-FOR problems. optimizing quadratic objectives HYPONYM-OF problems. eigenvalue distribution FEATURE-OF Hessian matrix. Eigencurve HYPONYM-OF learning rate schedules. SGD USED-FOR quadratic objectives. Eigencurve USED-FOR SGD. learning rate schedules USED-FOR SGD. minimax optimal convergence rates EVALUATE-FOR learning rate schedules. Eigencurve COMPARE step decay. step decay COMPARE Eigencurve. step decay USED-FOR image classification tasks. image classification tasks EVALUATE-FOR Eigencurve. CIFAR-10 USED-FOR image classification tasks. learning rate schedulers USED-FOR eigencurve. theory USED-FOR learning rate schedulers. schedulers COMPARE cosine decay. cosine decay COMPARE schedulers. schedulers USED-FOR optimal shape. cosine decay USED-FOR optimal shape. schedulers COMPARE cosine decay. cosine decay COMPARE schedulers. ,"This paper studies the problem of learning rate schedulers for deep neural networks. In particular, the authors propose a new learning rate schedule based on the eigencurve of the Hessian matrix. The authors show that the learning rate schedules can converge to the minimax optimal convergence rate for quadratic objectives. They also show that cosine decay and step decay can be used to improve the convergence rate.","This paper studies the problem of learning rate schedulers for deep neural networks. In particular, the authors propose a new learning rate schedule based on the eigencurve of the Hessian matrix. The authors show that the learning rate schedules can converge to the minimax optimal convergence rate for quadratic objectives. They also show that cosine decay and step decay can be used to improve the convergence rate."
3525,SP:8cfafcf0de6de33a8fd298593eeea82376b4697a,"Offline reinforcement learning USED-FOR control policies. total variation distance FEATURE-OF model. imaginary rollout horizon HYPONYM-OF hyperparameters. Bayesian Optimization USED-FOR hyperparameters. Material is online data collection. Method are offline model - based reinforcement learning, dynamics model, probabilistic model, and uncertainty heuristics. OtherScientificTerm are model uncertainty, pessimistic MDP, MDP, pessimistic return, and estimated model uncertainty. Generic are Existing methods, heuristics, and protocols. ","This paper studies the problem of offline model-based reinforcement learning, where the goal is to learn a probabilistic model that can be used for offline reinforcement learning. The authors propose to use Bayesian Optimization (BO) to optimize the hyperparameters of the dynamics model, and propose a pessimistic MDP, which is a variant of the pessimistic return (PDP) for offline RL. They show that the pessimistic PDP can be computed by minimizing the total variation distance between the estimated model uncertainty and the true model uncertainty. They also provide a theoretical analysis of the uncertainty heuristics. ","This paper studies the problem of offline model-based reinforcement learning, where the goal is to learn a probabilistic model that can be used for offline reinforcement learning. The authors propose to use Bayesian Optimization (BO) to optimize the hyperparameters of the dynamics model, and propose a pessimistic MDP, which is a variant of the pessimistic return (PDP) for offline RL. They show that the pessimistic PDP can be computed by minimizing the total variation distance between the estimated model uncertainty and the true model uncertainty. They also provide a theoretical analysis of the uncertainty heuristics. "
3541,SP:3833662cf92249d83e65a1200f9e2890b5b23e95,"methods USED-FOR sampling. critic networks USED-FOR policy learning. TD - error HYPONYM-OF features. TD - error USED-FOR sampled experiences. sampled experiences USED-FOR Q - values. auxiliary features USED-FOR TD - error. auxiliary features USED-FOR sampling. learnable features USED-FOR experience replay method. curriculum learning USED-FOR predicting Q - values. MaPER USED-FOR curriculum learning. MaPER COMPARE vanilla PER. vanilla PER COMPARE MaPER. computational overhead COMPARE vanilla PER. vanilla PER COMPARE computational overhead. critic network USED-FOR curriculum learning. critic network USED-FOR predicting Q - values. tasks EVALUATE-FOR MaPER. offpolicy MfRL CONJUNCTION MbRL. MbRL CONJUNCTION offpolicy MfRL. MaPER USED-FOR MbRL. off - policy MfRL algorithms USED-FOR policy optimization procedure. MaPER USED-FOR offpolicy MfRL. off - policy MfRL algorithms PART-OF MbRL. Method are Experience replay, and model - based RL ( MbRL ). ","This paper proposes a new method for experience replay in model-based reinforcement learning (MbRL). The proposed method, MaPER, is based on the idea of curriculum learning, which is an extension of Performer (Performer) to the context of off-policy MfRL (MfRL). In particular, the proposed method uses a critic network to predict the Q-value of an experience, and a curriculum learning network to learn the TD-error of the sampled experiences. The authors show that MaPER outperforms Performer on a number of benchmarks.","This paper proposes a new method for experience replay in model-based reinforcement learning (MbRL). The proposed method, MaPER, is based on the idea of curriculum learning, which is an extension of Performer (Performer) to the context of off-policy MfRL (MfRL). In particular, the proposed method uses a critic network to predict the Q-value of an experience, and a curriculum learning network to learn the TD-error of the sampled experiences. The authors show that MaPER outperforms Performer on a number of benchmarks."
3557,SP:0db83e057c21ac10fe91624876498d8456797492,"fast learning CONJUNCTION training safety. training safety CONJUNCTION fast learning. human knowledge PART-OF reinforcement learning. Human intervention USED-FOR human knowledge. trial - and - error exploration CONJUNCTION human ’s partial demonstration. human ’s partial demonstration CONJUNCTION trial - and - error exploration. human ’s partial demonstration USED-FOR HACO. agent USED-FOR proxy values. HACO USED-FOR agent. HACO USED-FOR proxy state - action values. environmental reward USED-FOR HACO. safe driving benchmark EVALUATE-FOR sample efficiency. sample efficiency EVALUATE-FOR HACO. safe driving benchmark EVALUATE-FOR HACO. reinforcement learning CONJUNCTION imitation learning baselines. imitation learning baselines CONJUNCTION reinforcement learning. Method are learning agent, and Human - AI Copilot Optimization ( HACO ). OtherScientificTerm are human expert, trivial behaviors, human interventions, and human intervention budget. Generic is It. ","This paper proposes Human-AI Copilot Optimization (HACO), a reinforcement learning method that leverages human intervention to improve the sample efficiency of the learning agent. HACO learns a proxy state-action value for the agent that is learned from the human expert's partial demonstration. The proxy value is then used to optimize the agent’s performance in the presence of human intervention. The proposed method is evaluated on the safe driving benchmark and shows that it outperforms the baseline methods.","This paper proposes Human-AI Copilot Optimization (HACO), a reinforcement learning method that leverages human intervention to improve the sample efficiency of the learning agent. HACO learns a proxy state-action value for the agent that is learned from the human expert's partial demonstration. The proxy value is then used to optimize the agent’s performance in the presence of human intervention. The proposed method is evaluated on the safe driving benchmark and shows that it outperforms the baseline methods."
3573,SP:7fda4f67daf3eb27cdfafe8f8a3f8d719da956c3,"long - horizon unsegmented demonstrations USED-FOR subskills. hierarchical structure USED-FOR Transferring and reorganizing modular sub - skills. Dual Meta Imitation Learning ( DMIL ) HYPONYM-OF hierarchical meta imitation learning method. highlevel network USED-FOR sub - skill adaptation. likelihood of state - action pairs USED-FOR supervision. likelihood of state - action pairs USED-FOR sub - skill. high - level network adaptation USED-FOR DMIL. highlevel network USED-FOR DMIL. likelihood of state - action pairs USED-FOR DMIL. DMIL CONJUNCTION Expectation - Maximization algorithm. Expectation - Maximization algorithm CONJUNCTION DMIL. iterative training process USED-FOR DMIL. Kitchen environment EVALUATE-FOR few - shot imitation learning. Method are Hierarchical Imitation learning ( HIL ), and model - agnostic meta - learning. OtherScientificTerm is high - level network. ","This paper proposes Dual Meta Imitation Learning (DMIL), a hierarchical meta-learning method for few-shot imitation learning. DMIL learns a high-level network for learning modular sub-skills, and a low-level one for learning the high level skills. The high level network is used to learn the likelihood of state-action pairs for each sub-skill, and the low level one is used for learning a low level skill. The authors show that DMIL outperforms the state-of-the-art HIL methods on the Kitchen environment.","This paper proposes Dual Meta Imitation Learning (DMIL), a hierarchical meta-learning method for few-shot imitation learning. DMIL learns a high-level network for learning modular sub-skills, and a low-level one for learning the high level skills. The high level network is used to learn the likelihood of state-action pairs for each sub-skill, and the low level one is used for learning a low level skill. The authors show that DMIL outperforms the state-of-the-art HIL methods on the Kitchen environment."
3589,SP:fb0efa670729796471a7a562b231172103bb8749,Graph neural networks ( GNNs ) HYPONYM-OF deep learning models. deep learning models USED-FOR graph data. node features USED-FOR they. graph without node feature USED-FOR networks. number of degrees HYPONYM-OF graph - based node features. embeddings HYPONYM-OF input node representation. approach USED-FOR node embeddings. node embeddings CONJUNCTION GNNs. GNNs CONJUNCTION node embeddings. graphics processing unit ( GPU ) memory FEATURE-OF GNNs. embedding compression methods USED-FOR natural language processing ( NLP ) models. bit vector COMPARE float - point vector. float - point vector COMPARE bit vector. bit vector USED-FOR node. parameters USED-FOR compression method. GNNs USED-FOR parameters. node embedding compression method COMPARE alternatives. alternatives COMPARE node embedding compression method. Generic is network. Material is industrial scale graph data. ,This paper proposes a node embedding compression method for graph neural networks (GNNs). The proposed method is based on the idea that node embeddings can be decomposed into node features and number of degrees. The authors show that the proposed method can be used to compress node features in GNNs. The method is evaluated on industrial scale graph data.,This paper proposes a node embedding compression method for graph neural networks (GNNs). The proposed method is based on the idea that node embeddings can be decomposed into node features and number of degrees. The authors show that the proposed method can be used to compress node features in GNNs. The method is evaluated on industrial scale graph data.
3605,SP:15c243829ed3b2505ed1e122bd499089f8a862da,domain adaptation USED-FOR learning invariant representations. domain - adversarial training USED-FOR learning invariant representations. asymptotic convergence guarantees FEATURE-OF optimizer. gradient descent USED-FOR domain - adversarial training. optimal solutions PART-OF domain - adversarial training. local Nash equilibria USED-FOR optimal solutions. gradient descent CONJUNCTION high - order ODE solvers. high - order ODE solvers CONJUNCTION gradient descent. Runge – Kutta HYPONYM-OF high - order ODE solvers. optimizers COMPARE optimizers. optimizers COMPARE optimizers. drop - in replacement COMPARE optimizers. optimizers COMPARE drop - in replacement. optimizers USED-FOR drop - in replacement. learning rates FEATURE-OF optimizers. optimizers PART-OF domain - adversarial framework. Generic is approach. Method is domain - adversarial methods. ,This paper studies the problem of learning invariant representations for domain adaptation. The authors propose a novel domain-adaptive learning framework that combines gradient descent and high-order ODE solvers to improve the performance of domain-adversarial training. The key idea is to use the local Nash equilibria of the optimizer to find the optimal solution of the optimization problem. The proposed framework is evaluated on a variety of benchmark datasets and shows that the proposed method outperforms drop-in replacement and gradient descent. ,This paper studies the problem of learning invariant representations for domain adaptation. The authors propose a novel domain-adaptive learning framework that combines gradient descent and high-order ODE solvers to improve the performance of domain-adversarial training. The key idea is to use the local Nash equilibria of the optimizer to find the optimal solution of the optimization problem. The proposed framework is evaluated on a variety of benchmark datasets and shows that the proposed method outperforms drop-in replacement and gradient descent. 
3621,SP:0ecbaf1770642b6ac5c9786ba2d18408310fc225,"regularization methods USED-FOR machine learning models. loss function USED-FOR Flooding. individual Flood HYPONYM-OF regularizer. iFlood USED-FOR models. instance - level constraints FEATURE-OF training loss. instance - level constraints USED-FOR iFlood. it USED-FOR applications. it USED-FOR models. iFlood COMPARE regularizers. regularizers COMPARE iFlood. image classification and language understanding tasks EVALUATE-FOR models. iFlood USED-FOR models. Generic is one. OtherScientificTerm are under - fitted instances, inductive biases, and instance - level. Metric is generalization ability. ","This paper proposes a new regularization method called iFlood, which is based on the idea of individual Flooding. The idea is to use instance-level constraints to improve the generalization ability of the training loss. The authors show that the proposed method outperforms existing regularization methods on image classification and language understanding tasks. ","This paper proposes a new regularization method called iFlood, which is based on the idea of individual Flooding. The idea is to use instance-level constraints to improve the generalization ability of the training loss. The authors show that the proposed method outperforms existing regularization methods on image classification and language understanding tasks. "
3637,SP:e6622975c9889cf6d3357ab439c2e268c4f4200e,"Reinforcement learning USED-FOR complex tasks. Reinforcement learning USED-FOR policies. policies USED-FOR complex tasks. methods USED-FOR long - horizon tasks. Hierarchical reinforcement learning USED-FOR lowlevel skills. Hierarchical reinforcement learning USED-FOR action abstractions. action abstractions USED-FOR lowlevel skills. lower - level policies USED-FOR state abstraction. approach USED-FOR representation. value functions USED-FOR lower - level skill. value functions USED-FOR approach. value functions USED-FOR representation. value functions USED-FOR representation. approach COMPARE model - free and model - based methods. model - free and model - based methods COMPARE approach. maze - solving and robotic manipulation tasks EVALUATE-FOR approach. zero - shot generalization EVALUATE-FOR model - free and model - based methods. zero - shot generalization EVALUATE-FOR approach. OtherScientificTerm are horizon, lower - level skills, Hierarchies, and space states. Method is Value Function Spaces. ","This paper proposes a Hierarchical Reinforcement Learning (HRL) method for learning low-level skills for long-horizon tasks. The key idea is to use Hierarchies to learn a lower-level skill that can be used as a representation for a higher-level task. This is achieved by learning a Hierarchy of value functions for each skill, which is then used to represent the state space. The authors show that the proposed method is able to generalize better than existing methods on maze-solving and robotic manipulation tasks.","This paper proposes a Hierarchical Reinforcement Learning (HRL) method for learning low-level skills for long-horizon tasks. The key idea is to use Hierarchies to learn a lower-level skill that can be used as a representation for a higher-level task. This is achieved by learning a Hierarchy of value functions for each skill, which is then used to represent the state space. The authors show that the proposed method is able to generalize better than existing methods on maze-solving and robotic manipulation tasks."
3653,SP:6adcd2a71ce70922c4cbe155d49f105964faee8f,"one - shot probabilistic decoders USED-FOR vector - shaped prior. generative adversarial networks ( GAN ) CONJUNCTION normalizing flows. normalizing flows CONJUNCTION generative adversarial networks ( GAN ). variational autoencoders ( VAE ) CONJUNCTION generative adversarial networks ( GAN ). generative adversarial networks ( GAN ) CONJUNCTION variational autoencoders ( VAE ). functions PART-OF variational autoencoders ( VAE ). functions USED-FOR drug discovery. Transformer layers CONJUNCTION graph neural networks. graph neural networks CONJUNCTION Transformer layers. them CONJUNCTION prior vector. prior vector CONJUNCTION them. Transformer layers USED-FOR prior vector. Transformer layers USED-FOR them. graph neural networks USED-FOR them. architecture USED-FOR exchangeable distributions. VAEs CONJUNCTION GANs. GANs CONJUNCTION VAEs. exchangeability USED-FOR VAEs. exchangeability USED-FOR GANs. Top - n HYPONYM-OF deterministic, non - exchangeable set creation mechanism. VAE CONJUNCTION GAN. GAN CONJUNCTION VAE. Top - n USED-FOR VAE. i.i.d. generation USED-FOR VAE. i.i.d. generation USED-FOR GAN. Top - n COMPARE i.i.d. generation. i.i.d. generation COMPARE Top - n. Top - n USED-FOR complex dependencies. Top - n COMPARE i.i.d. generation. i.i.d. generation COMPARE Top - n. SetMNIST reconstruction EVALUATE-FOR Top - n. SetMNIST reconstruction EVALUATE-FOR i.i.d. generation. algorithm USED-FOR molecule generation methods. algorithm USED-FOR one - shot generation. Task is Set and graph generation. OtherScientificTerm are normal distribution, and equivariance. Material are synthetic molecule - like dataset, and QM9 dataset. ","This paper proposes Top-n, a deterministic, non-exchangeable set creation mechanism for one-shot probabilistic decoders. Top-N is based on the Transformer layers and the graph neural networks. The authors show that Top-Ns can be used to generate a set with equivariance and exchangeability. They also show that top-ns can be applied to GANs and VAEs.","This paper proposes Top-n, a deterministic, non-exchangeable set creation mechanism for one-shot probabilistic decoders. Top-N is based on the Transformer layers and the graph neural networks. The authors show that Top-Ns can be used to generate a set with equivariance and exchangeability. They also show that top-ns can be applied to GANs and VAEs."
3669,SP:f1f1df92e3e7c6b3b9e326a78a708c0d5d990c83,Deep Ritz Method ( DRM ) CONJUNCTION Physics - Informed Neural Networks ( PINNs ). Physics - Informed Neural Networks ( PINNs ) CONJUNCTION Deep Ritz Method ( DRM ). deep learning techniques USED-FOR elliptic partial differential equations ( PDEs ). Deep Ritz Method ( DRM ) USED-FOR deep learning techniques. random samples USED-FOR deep learning techniques. Physics - Informed Neural Networks ( PINNs ) USED-FOR deep learning techniques. hypercube FEATURE-OF Schrödinger equation. Schrödinger equation HYPONYM-OF prototype elliptic PDE. upper bounds USED-FOR problem. upper and lower bounds USED-FOR methods. upper bounds USED-FOR upper and lower bounds. rate generalization bound USED-FOR upper bounds. rate generalization bound USED-FOR problem. PINN CONJUNCTION DRM. DRM CONJUNCTION PINN. DRM USED-FOR minimax optimal bounds. PINN USED-FOR minimax optimal bounds. Sobolev spaces FEATURE-OF minimax optimal bounds. dimension dependent power law USED-FOR deep PDE solvers. power law USED-FOR deep model accuracy. OtherScientificTerm is zero Dirichlet boundary condition. Task is quantummechanical systems. Method is Deep Ritz Method. Generic is it. ,"This paper studies the problem of solving elliptic partial differential equations (PDEs) using deep learning methods. The authors consider the Schrödinger equation, which is a prototype elliptic PDE, and the setting of quantummechanical systems. In this setting, the authors show that the Ritz method (DRM) and physics-informed neural networks (PINNs) can be used to solve PDEs. They also provide upper and lower bounds for the rate generalization bound and the dimension dependent power law for deep PDE solvers.","This paper studies the problem of solving elliptic partial differential equations (PDEs) using deep learning methods. The authors consider the Schrödinger equation, which is a prototype elliptic PDE, and the setting of quantummechanical systems. In this setting, the authors show that the Ritz method (DRM) and physics-informed neural networks (PINNs) can be used to solve PDEs. They also provide upper and lower bounds for the rate generalization bound and the dimension dependent power law for deep PDE solvers."
3685,SP:80614db60d27a48c3c1b1882844e298666b798d4,Machine learning ( ML ) robustness CONJUNCTION generalization. generalization CONJUNCTION Machine learning ( ML ) robustness. data distribution shift FEATURE-OF they. adversarial and natural settings FEATURE-OF data distribution shift. other USED-FOR one. norm of the last layer CONJUNCTION Jacobian norm. Jacobian norm CONJUNCTION norm of the last layer. Jacobian norm CONJUNCTION data augmentations ( DA ). data augmentations ( DA ) CONJUNCTION Jacobian norm. function class regularization process USED-FOR domain generalization. adversarial training USED-FOR robustness. function class regularization USED-FOR robustness. DA USED-FOR generalization. DA USED-FOR regularization. robustness CONJUNCTION generalization. generalization CONJUNCTION robustness. Generic is theoretical framework. OtherScientificTerm is sufficient conditions. ,This paper studies the relationship between robustness and generalization in adversarial and natural settings. The authors consider the Jacobian norm of the last layer and the data augmentations (DA) regularization process. They show that adversarial training and DA are sufficient conditions for robustness to generalize. They also show that DA is sufficient for generalization to adversarial settings. ,This paper studies the relationship between robustness and generalization in adversarial and natural settings. The authors consider the Jacobian norm of the last layer and the data augmentations (DA) regularization process. They show that adversarial training and DA are sufficient conditions for robustness to generalize. They also show that DA is sufficient for generalization to adversarial settings. 
3701,SP:4d49bcb069a76f108c0e2de50750827f45eb5676,"Meta - learning USED-FOR quick learning of few - shot tasks. meta - training tasks USED-FOR meta - knowledge. Wellgeneralized meta - knowledge USED-FOR fast adaptation. methods PART-OF framework. deconfounder approaches USED-FOR methods. Dropout USED-FOR meta - knowledge. deconfounder algorithms USED-FOR memorization. causal perspective USED-FOR memorization. causal perspective USED-FOR deconfounder algorithms. benchmark datasets USED-FOR memorization. benchmark datasets EVALUATE-FOR deconfounder algorithms. Task is task - specific adaptation. Method are regularizer - based and augmentation - based methods, meta - learning, and front - door adjustment. OtherScientificTerm are causality, and universal label space. ","This paper proposes a meta-learning framework for few-shot learning. The key idea is to use a causal perspective to learn the meta-knowledge of a task, which is then used to guide the learning of new tasks. The proposed method is evaluated on a number of benchmark datasets and compared to several baselines. ","This paper proposes a meta-learning framework for few-shot learning. The key idea is to use a causal perspective to learn the meta-knowledge of a task, which is then used to guide the learning of new tasks. The proposed method is evaluated on a number of benchmark datasets and compared to several baselines. "
3717,SP:adfe205b335cf87bd4e470efd9f72bb639a4451c,"ad hoc teamwork HYPONYM-OF problem. full observability CONJUNCTION fixed and predefined teammates ’ types. fixed and predefined teammates ’ types CONJUNCTION full observability. fixed and predefined teammates ’ types HYPONYM-OF assumptions. full observability HYPONYM-OF assumptions. reinforcement learning framework USED-FOR autonomous agent. ODITS HYPONYM-OF reinforcement learning framework. information - based regularizer USED-FOR proxy representations of the learned variables. local observations USED-FOR information - based regularizer. local observations USED-FOR proxy representations of the learned variables. ODITS COMPARE baselines. baselines COMPARE ODITS. ad hoc teamwork tasks EVALUATE-FOR ODITS. ad hoc teamwork tasks EVALUATE-FOR baselines. Task is Autonomous agents. OtherScientificTerm are teammates, and partial observability. ","This paper proposes a reinforcement learning framework for ad hoc teamwork. The proposed method is based on an information-based regularizer, which learns a proxy representation of the learned variables. Experiments show that the proposed method outperforms baselines on a variety of tasks.","This paper proposes a reinforcement learning framework for ad hoc teamwork. The proposed method is based on an information-based regularizer, which learns a proxy representation of the learned variables. Experiments show that the proposed method outperforms baselines on a variety of tasks."
3733,SP:7b04b45c4dd237d69321d280dcdcbc89fb362015,"missing values PART-OF high - dimensional data. approach USED-FOR down - stream analysis. imputation CONJUNCTION model estimation. model estimation CONJUNCTION imputation. model estimation USED-FOR down - stream analysis. imputation PART-OF approach. model estimation PART-OF approach. algorithm USED-FOR imputation. normalizing flow ( NF ) model USED-FOR data space. latent space FEATURE-OF imputation. normalizing flow ( NF ) model USED-FOR algorithm. Expectation - Maximization ( EM ) algorithm USED-FOR imputation. EMFlow COMPARE methods. methods COMPARE EMFlow. predictive accuracy CONJUNCTION speed of algorithmic convergence. speed of algorithmic convergence CONJUNCTION predictive accuracy. high - dimensional multivariate and image datasets EVALUATE-FOR EMFlow. speed of algorithmic convergence EVALUATE-FOR EMFlow. predictive accuracy EVALUATE-FOR EMFlow. speed of algorithmic convergence EVALUATE-FOR methods. predictive accuracy EVALUATE-FOR methods. Method are data mining and machine learning methods, EMFlow algorithm, and NF alternatively. ","This paper proposes a new method for down-stream analysis of missing values in high-dimensional data. The authors propose to use a normalizing flow (NF) model for imputation and model estimation, and use an Expectation-Maximization (EM) algorithm to optimize the imputation of the latent space of the NF model. They show that the proposed method outperforms existing methods in terms of predictive accuracy and speed of algorithmic convergence. ","This paper proposes a new method for down-stream analysis of missing values in high-dimensional data. The authors propose to use a normalizing flow (NF) model for imputation and model estimation, and use an Expectation-Maximization (EM) algorithm to optimize the imputation of the latent space of the NF model. They show that the proposed method outperforms existing methods in terms of predictive accuracy and speed of algorithmic convergence. "
3749,SP:a151ae8afae0a073b0df83a74fd084dfe3753a48,rectified linear units ( ReLUs ) USED-FOR DNNs. gates USED-FOR neural path kernel ( NPK ). rotational invariance CONJUNCTION ensemble structure. ensemble structure CONJUNCTION rotational invariance. global pooling CONJUNCTION skip connection. skip connection CONJUNCTION global pooling. convolution USED-FOR NPK. ensemble structure USED-FOR NPK. convolution USED-FOR ensemble structure. convolution USED-FOR rotational invariance. skip connection FEATURE-OF convolution. global pooling FEATURE-OF convolution. gates USED-FOR weights. external masks USED-FOR weights. weights PART-OF network. gates USED-FOR external masks. ReLUs FEATURE-OF DNNs. deep linear network USED-FOR pre - activations. DNNs USED-FOR ‘ black box’-ness. disentanglement CONJUNCTION interpretable re - arrangement of the computations. interpretable re - arrangement of the computations CONJUNCTION disentanglement. interpretable re - arrangement of the computations PART-OF DNN. ReLUs USED-FOR interpretable re - arrangement of the computations. ReLUs USED-FOR DNN. path space FEATURE-OF weights network. DLGN USED-FOR computations. primal ’ linearity CONJUNCTION dual ’ linearity. dual ’ linearity CONJUNCTION primal ’ linearity. path space FEATURE-OF dual ’ linearity. ‘ mathematically ’ interpretable linearities PART-OF DLGN. dual ’ linearity HYPONYM-OF ‘ mathematically ’ interpretable linearities. primal ’ linearity HYPONYM-OF ‘ mathematically ’ interpretable linearities. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. DGN CONJUNCTION DLGN. DLGN CONJUNCTION DGN. DNN CONJUNCTION DGN. DGN CONJUNCTION DNN. DLGN COMPARE DNNs. DNNs COMPARE DLGN. ‘ disentangled and interpretable ’ computations PART-OF DLGN. entang,"This paper proposes a deep linear network (DLGN) based on ReLUs. The authors show that DLGN is able to achieve better interpretability and disentanglement of the computations compared to DNNs. In particular, DLGN achieves dual linearity and primal linearity in the path space. They also show that the weights of DLGN are more interpretable than those of DNN. ","This paper proposes a deep linear network (DLGN) based on ReLUs. The authors show that DLGN is able to achieve better interpretability and disentanglement of the computations compared to DNNs. In particular, DLGN achieves dual linearity and primal linearity in the path space. They also show that the weights of DLGN are more interpretable than those of DNN. "
3765,SP:5676944f4983676b5ad843fdb190bf029ad647bb,Swin CONJUNCTION PVT. PVT CONJUNCTION Swin. Vision Transformer ( ViT ) USED-FOR computer vision tasks. PVT HYPONYM-OF Vision Transformer ( ViT ). Swin HYPONYM-OF Vision Transformer ( ViT ). Layer Normalization ( LN ) USED-FOR models. Transformers USED-FOR inductive bias. LN USED-FOR positional context. positional context HYPONYM-OF inductive bias. Dynamic Token Normalization ( DTN ) HYPONYM-OF normalizer. it USED-FOR normalization methods. unified formulation USED-FOR it. global contextual information CONJUNCTION local positional context. local positional context CONJUNCTION global contextual information. Transformers USED-FOR local positional context. Transformers USED-FOR global contextual information. DTN USED-FOR Transformers. DTN USED-FOR intra - token and inter - token manners. PVT CONJUNCTION LeViT. LeViT CONJUNCTION PVT. BigBird CONJUNCTION Reformer. Reformer CONJUNCTION BigBird. Swin CONJUNCTION PVT. PVT CONJUNCTION Swin. LeViT CONJUNCTION T2T - ViT. T2T - ViT CONJUNCTION LeViT. T2T - ViT CONJUNCTION BigBird. BigBird CONJUNCTION T2T - ViT. ViT CONJUNCTION Swin. Swin CONJUNCTION ViT. DTN USED-FOR vision transformers. ViT HYPONYM-OF vision transformers. LeViT HYPONYM-OF vision transformers. Swin HYPONYM-OF vision transformers. T2T - ViT HYPONYM-OF vision transformers. PVT HYPONYM-OF vision transformers. Reformer HYPONYM-OF vision transformers. BigBird HYPONYM-OF vision transformers. transformer COMPARE baseline model. baseline model COMPARE transformer. DTN USED-FOR transformer. computational overhead EVALUATE-FOR baseline model. DTN COMPARE LN. LN COMPARE DTN. accuracy EVALUATE-FOR Long ListOps. Long - Range Arena FEATURE-OF,"This paper proposes Dynamic Token Normalization (DTN), a new normalization method for vision transformers. The proposed method is based on a unified formulation of LN and transformer-based normalization methods. The authors show that DTN can be applied to both intra-token and inter-token manners. They also show that the proposed method can be used to reduce the computational overhead of transformer models.","This paper proposes Dynamic Token Normalization (DTN), a new normalization method for vision transformers. The proposed method is based on a unified formulation of LN and transformer-based normalization methods. The authors show that DTN can be applied to both intra-token and inter-token manners. They also show that the proposed method can be used to reduce the computational overhead of transformer models."
3781,SP:7a04efbf835c238bbdf70a8b8decee4ec2907a3a,deep learning models USED-FOR expressive functions. SGD USED-FOR deep learning models. theoretical models USED-FOR spectral bias. methodologies USED-FOR spectral bias. spectral bias FEATURE-OF image classification networks. interventions USED-FOR generalization. spectral bias FEATURE-OF networks. regularization USED-FOR learning of high frequencies. models COMPARE ones. ones COMPARE models. models USED-FOR high frequencies. function frequency CONJUNCTION image frequency. image frequency CONJUNCTION function frequency. low frequencies PART-OF natural images. low frequencies USED-FOR spectral bias. natural images USED-FOR spectral bias. neural networks USED-FOR image classification. OtherScientificTerm is Spectral bias. Method is deep models. ,This paper studies the spectral bias of deep learning models. The authors propose a new regularization method to improve the generalization performance of deep neural networks. The proposed method is based on the observation that the high frequencies of natural images are more likely to be associated with spectral bias than the low frequencies of high frequencies. The paper also proposes a regularization scheme to improve generalization. The experiments show the effectiveness of the proposed method. ,This paper studies the spectral bias of deep learning models. The authors propose a new regularization method to improve the generalization performance of deep neural networks. The proposed method is based on the observation that the high frequencies of natural images are more likely to be associated with spectral bias than the low frequencies of high frequencies. The paper also proposes a regularization scheme to improve generalization. The experiments show the effectiveness of the proposed method. 
3797,SP:9ef61f2064db8ac3b01b16694a744b274bdbbe83,"Exploration PART-OF reinforcement learning ( RL ). monolithic behaviour policy USED-FOR methods. nonmonolithic exploration USED-FOR RL. algorithmic components USED-FOR switching mechanism. two - mode exploration CONJUNCTION switching. switching CONJUNCTION two - mode exploration. sub - episodic time - scales FEATURE-OF switching. sub - episodic time - scales FEATURE-OF two - mode exploration. switching USED-FOR Atari. two - mode exploration USED-FOR Atari. OtherScientificTerm are exploratory behaviours, switching triggers, and hyper - parameter - tuning burden. ","This paper studies the switching mechanism of two-mode exploration in reinforcement learning (RL). The authors propose a novel algorithm for switching between two modes of exploration. The switching mechanism is based on two components: (1) the switching triggers, and (2) the hyper-parameter-tuning burden. The authors show that the proposed algorithm outperforms the baselines on Atari games. ","This paper studies the switching mechanism of two-mode exploration in reinforcement learning (RL). The authors propose a novel algorithm for switching between two modes of exploration. The switching mechanism is based on two components: (1) the switching triggers, and (2) the hyper-parameter-tuning burden. The authors show that the proposed algorithm outperforms the baselines on Atari games. "
3813,SP:c44d3749d8883fae7eb2a6378417fca28d25a4c9,initialization scheme USED-FOR k - median problem. metric space FEATURE-OF k - median problem. metric embedding tree structure USED-FOR initialization scheme. discrete space HYPONYM-OF metric space. search algorithm USED-FOR initial centers. search algorithm USED-FOR local search algorithm. differential privacy ( DP ) USED-FOR private initial centers. HST initialization HYPONYM-OF method. k - median++ USED-FOR non - DP setting. initialization method USED-FOR non - DP setting. initialization method USED-FOR initial centers. HST initialization USED-FOR initial centers. k - median++ HYPONYM-OF initialization method. DP local search CONJUNCTION private HST initialization. private HST initialization CONJUNCTION DP local search. Method is clustering algorithms. Task is construction of metric embedding tree structure. OtherScientificTerm is privacy constraint. Generic is methods. ,"This paper proposes a new initialization scheme for the k-median clustering problem in differential privacy (DP) setting. The authors show that the proposed method, k-Median++, can be viewed as an extension of private HST initialization (HST) and DP local search (DP local search). The authors also provide theoretical analysis of the performance of the proposed algorithm. ","This paper proposes a new initialization scheme for the k-median clustering problem in differential privacy (DP) setting. The authors show that the proposed method, k-Median++, can be viewed as an extension of private HST initialization (HST) and DP local search (DP local search). The authors also provide theoretical analysis of the performance of the proposed algorithm. "
3829,SP:84c9eb6623e7950585d80a664dd51b3ecc356dea,"planning USED-FOR agent. representation USED-FOR visual perception tasks. agent USED-FOR complex dynamics of the real - world. agent USED-FOR representation. complicated dynamics CONJUNCTION broader domain. broader domain CONJUNCTION complicated dynamics. broader domain FEATURE-OF real - life datasets. complicated dynamics FEATURE-OF real - life datasets. narrow benchmarks EVALUATE-FOR video prediction models. underfitting USED-FOR low quality predictions. FitVid HYPONYM-OF architecture. image augmentation techniques USED-FOR it. FitVid COMPARE models. models COMPARE FitVid. video prediction benchmarks EVALUATE-FOR models. video prediction benchmarks EVALUATE-FOR FitVid. metrics EVALUATE-FOR models. metrics EVALUATE-FOR FitVid. Generic are task, they, and state - of - theart models. Method is video models. OtherScientificTerm is overfitting. ","This paper proposes a new architecture for video prediction, FitVid, which is based on the idea of learning a video model that can be used to predict complex dynamics of the real-world. The model is trained on a variety of video datasets, and is able to outperform state-of-the-art models on a wide range of video prediction tasks. The proposed method is evaluated on three video prediction benchmarks, and compared to a number of baselines. ","This paper proposes a new architecture for video prediction, FitVid, which is based on the idea of learning a video model that can be used to predict complex dynamics of the real-world. The model is trained on a variety of video datasets, and is able to outperform state-of-the-art models on a wide range of video prediction tasks. The proposed method is evaluated on three video prediction benchmarks, and compared to a number of baselines. "
3845,SP:6eb5ce1d85928a3af759d75016089c535941d0b0,generalization EVALUATE-FOR machine learning algorithm. neural network HYPONYM-OF machine learning algorithm. model USED-FOR test loss. model USED-FOR stochastic gradient descent ( SGD ). data structure USED-FOR test loss dynamics. arbitrary covariance structure FEATURE-OF features. Gaussian features CONJUNCTION arbitrary features. arbitrary features CONJUNCTION Gaussian features. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. Gaussian model USED-FOR test loss. Gaussian model USED-FOR nonlinear random - feature models. nonlinear random - feature models CONJUNCTION deep neural networks. deep neural networks CONJUNCTION nonlinear random - feature models. theory USED-FOR Gaussian features. Gaussian model USED-FOR deep neural networks. real datasets EVALUATE-FOR deep neural networks. SGD USED-FOR deep neural networks. MNIST HYPONYM-OF real datasets. CIFAR-10 HYPONYM-OF real datasets. fixed compute budget FEATURE-OF optimal batch size. feature correlation structure USED-FOR optimal batch size. small batch sizes USED-FOR SGD. theory USED-FOR stochastic gradient descent. framework USED-FOR training and test error. real data EVALUATE-FOR framework. fixed subsampled training set USED-FOR stochastic gradient descent. OtherScientificTerm is structure of the data distribution. ,"This paper studies the test loss dynamics of stochastic gradient descent (SGD) under the assumption that the data structure of the data distribution is invariant to the covariance structure of features. The authors propose to use a Gaussian model as a test loss for SGD. The proposed method is based on the theory of Gaussian features, and the authors show that it can be used to improve the test error of SGD on a fixed subsampled training set. They also show that the optimal batch size of the training set is a function of the feature correlation structure. ","This paper studies the test loss dynamics of stochastic gradient descent (SGD) under the assumption that the data structure of the data distribution is invariant to the covariance structure of features. The authors propose to use a Gaussian model as a test loss for SGD. The proposed method is based on the theory of Gaussian features, and the authors show that it can be used to improve the test error of SGD on a fixed subsampled training set. They also show that the optimal batch size of the training set is a function of the feature correlation structure. "
3861,SP:a530dd966911e387a90e3cbf9f51c8cab6152723,"Stochastic gradient descent ( SGD ) USED-FOR nonlinear, nonconvex problem. learning rate CONJUNCTION model. model CONJUNCTION learning rate. AMSGrad USED-FOR local maxima. sharp minima USED-FOR SGD. Method are deep neural networks, and minimal neural network - like construction. Task is optimization problems. ","This paper studies the problem of non-convex optimization of SGD with sharp minima. The authors consider the case where the learning rate and the model are non-linear, and the minima of the gradient descent problem are nonlinear. In this setting, the authors show that the sharp maxima of local minima can be obtained by minimizing the AMSGrad, which is a generalization of the sharp minimum of the SGD problem. The paper also shows that the minimization of local maxima can also be achieved by using a minimal neural network-like construction. ","This paper studies the problem of non-convex optimization of SGD with sharp minima. The authors consider the case where the learning rate and the model are non-linear, and the minima of the gradient descent problem are nonlinear. In this setting, the authors show that the sharp maxima of local minima can be obtained by minimizing the AMSGrad, which is a generalization of the sharp minimum of the SGD problem. The paper also shows that the minimization of local maxima can also be achieved by using a minimal neural network-like construction. "
3877,SP:22d01913b78ef447b064c65a646fa301b861d3f7,"gradientbased hyperparameter optimization ( HO ) methods USED-FOR hyperparameters. Implicit Function Theorem ( IFT ) based methods USED-FOR online optimization. high - dimensional hyperparameters CONJUNCTION horizon length. horizon length CONJUNCTION high - dimensional hyperparameters. short horizon bias FEATURE-OF short horizon approximations. knowledge distillation USED-FOR second - order term. Jacobian - vector product ( JVP ) USED-FOR HO step. hyperparameter dimension CONJUNCTION horizon length. horizon length CONJUNCTION hyperparameter dimension. method USED-FOR online optimization. hyperparameter dimension USED-FOR method. meta - learning methods CONJUNCTION benchmark datasets. benchmark datasets CONJUNCTION meta - learning methods. meta - learning methods EVALUATE-FOR method. benchmark datasets EVALUATE-FOR method. Method are gradient - based meta - learning methods, inner - optimization, Unrolled differentiation methods, and HO method. ",This paper proposes a method for online optimization of hyperparameter optimization (HO) based on the Implicit Function Theorem (IFT). The authors propose to use the Jacobian-vector product (JVP) to approximate the second-order term of the hyperparameters in the optimization step. The authors show that the proposed method is able to achieve better performance than existing methods on several benchmark datasets. ,This paper proposes a method for online optimization of hyperparameter optimization (HO) based on the Implicit Function Theorem (IFT). The authors propose to use the Jacobian-vector product (JVP) to approximate the second-order term of the hyperparameters in the optimization step. The authors show that the proposed method is able to achieve better performance than existing methods on several benchmark datasets. 
3893,SP:a64b26faef315c3ece590322291bab198932c604,"tasks USED-FOR meta - learning. meta - learning USED-FOR learning of new tasks. globally shared metalearner USED-FOR tasks. globally shared metalearner USED-FOR meta - learning. customization CONJUNCTION generalization. generalization CONJUNCTION customization. task clustering USED-FOR task - aware modulation. methods USED-FOR task representation. baselearner model USED-FOR task - specific optimization process. features USED-FOR task representation. features CONJUNCTION learning path. learning path CONJUNCTION features. features USED-FOR task representation. learning path USED-FOR task representation. geometric quantities USED-FOR learning path. path representation USED-FOR downstream clustering and modulation. meta path learner USED-FOR path representation. shortcut tunnel USED-FOR feature cluster assignments. shortcut tunnel USED-FOR path. path CONJUNCTION feature cluster assignments. feature cluster assignments CONJUNCTION path. few - shot image classification CONJUNCTION cold - start recommendation. cold - start recommendation CONJUNCTION few - shot image classification. CTML COMPARE baselines. baselines COMPARE CTML. real - world application domains EVALUATE-FOR CTML. real - world application domains EVALUATE-FOR baselines. cold - start recommendation EVALUATE-FOR CTML. cold - start recommendation HYPONYM-OF real - world application domains. few - shot image classification HYPONYM-OF real - world application domains. OtherScientificTerm is task heterogeneity. Method are global meta - learner, rehearsed task learning, and rehearsed learning. ","This paper proposes a method for meta-learning of new tasks. The proposed method is based on a global meta-learner model that learns a task-specific optimization process for each task. The method is evaluated on a number of benchmark tasks, including few-shot image classification and cold-start recommendation. The authors show that the proposed method outperforms baselines in terms of generalization and customization.","This paper proposes a method for meta-learning of new tasks. The proposed method is based on a global meta-learner model that learns a task-specific optimization process for each task. The method is evaluated on a number of benchmark tasks, including few-shot image classification and cold-start recommendation. The authors show that the proposed method outperforms baselines in terms of generalization and customization."
3909,SP:f7a7c81ed2b6e9eb958b8b751deed8166622540c,"in - distribution ( ID ) data EVALUATE-FOR deep neural networks. methods USED-FOR near OOD samples. labeled data USED-FOR near OOD samples. ensemble - based procedure USED-FOR semi - supervised novelty detection ( SSND ). ensemble - based procedure USED-FOR detection. unlabeled ID and OOD samples USED-FOR ensemble - based procedure. regularization USED-FOR OOD data. regularization USED-FOR It. approach COMPARE SSND methods. SSND methods COMPARE approach. image data sets CONJUNCTION medical image data sets. medical image data sets CONJUNCTION image data sets. medical image data sets EVALUATE-FOR SSND methods. image data sets EVALUATE-FOR SSND methods. medical image data sets EVALUATE-FOR approach. image data sets EVALUATE-FOR approach. Task is expert evaluation. Method is OOD detection algorithms. Material are near OOD data, and ID data. Metric is computational cost. ",This paper proposes an ensemble-based method for semi-supervised novelty detection (SSND). The proposed method is based on the idea of ensemble regularization. The authors show that the proposed method outperforms the state-of-the-art methods on both ID and OOD data. ,This paper proposes an ensemble-based method for semi-supervised novelty detection (SSND). The proposed method is based on the idea of ensemble regularization. The authors show that the proposed method outperforms the state-of-the-art methods on both ID and OOD data. 
3925,SP:a0ee0e08b4bb578836fd5e0781e8713f254569fb,"multi - agent trajectory prediction USED-FOR safe control of robotic systems. representation USED-FOR planning. encoder - decoder architectures USED-FOR scene - consistent multi - agent trajectories. Latent Variable Sequential Set Transformers HYPONYM-OF encoder - decoder architectures. Latent Variable Sequential Set Transformers USED-FOR scene - consistent multi - agent trajectories. AutoBots ” HYPONYM-OF architectures. temporal and social dimensions FEATURE-OF equivariant processing. model USED-FOR single - agent prediction case. Argoverse vehicle prediction challenge EVALUATE-FOR model. global nuScenes vehicle motion prediction leaderboard EVALUATE-FOR model. synthetic partition of TrajNet++ dataset EVALUATE-FOR model. model USED-FOR socially - consistent predictions. multi - agent setting EVALUATE-FOR model. synthetic partition of TrajNet++ dataset EVALUATE-FOR socially - consistent predictions. synthetic partition of TrajNet++ dataset EVALUATE-FOR multi - agent setting. desktop GPU ( 1080 Ti ) USED-FOR models. Method are encoder, and decoder. OtherScientificTerm is sequential structure. Material is Omniglot data. Generic is method. ",This paper proposes a method for multi-agent trajectory prediction based on Latent Variable Sequential Set Transformers (LVSST). The proposed method is based on the idea of equivariant processing and is able to capture both temporal and social dimensions. The proposed model is evaluated on the Omniglot dataset and the global nuScenes vehicle motion prediction leaderboard. ,This paper proposes a method for multi-agent trajectory prediction based on Latent Variable Sequential Set Transformers (LVSST). The proposed method is based on the idea of equivariant processing and is able to capture both temporal and social dimensions. The proposed model is evaluated on the Omniglot dataset and the global nuScenes vehicle motion prediction leaderboard. 
3941,SP:e253d49bbfadb76b2f7c4e7cdd1cc33d0cebc3e7,"methods USED-FOR image classification models. baseline explanation technique COMPARE concept - based and counterfactual explanations. concept - based and counterfactual explanations COMPARE baseline explanation technique. baseline COMPARE concept - based explanations. concept - based explanations COMPARE baseline. Counterfactual explanations COMPARE baseline. baseline COMPARE Counterfactual explanations. invertible neural network USED-FOR Counterfactual explanations. technical evaluations CONJUNCTION proxy tasks. proxy tasks CONJUNCTION technical evaluations. Generic are they, and model. Method is synthetic dataset generator. ","This paper proposes a new method for generating counterfactual explanations for image classification models. The method is based on the idea of invertible neural networks. The authors show that the proposed method can be applied to both concept-based explanations (e.g., concept-free explanations) and Counterfactual Explanations (i.e., concept based explanations). The authors also show that their method outperforms baseline explanations in terms of accuracy. ","This paper proposes a new method for generating counterfactual explanations for image classification models. The method is based on the idea of invertible neural networks. The authors show that the proposed method can be applied to both concept-based explanations (e.g., concept-free explanations) and Counterfactual Explanations (i.e., concept based explanations). The authors also show that their method outperforms baseline explanations in terms of accuracy. "
3957,SP:6495caf14ebb8b9c3cbf50a5f05ec1eb600864fe,deep networks USED-FOR backdoor data poisoning attacks. model USED-FOR inference. iterative training procedure USED-FOR poisoned data. boosting framework USED-FOR clean data. boosting framework USED-FOR poisoned data. bootstrapped measure of generalization USED-FOR algorithm. method USED-FOR dirty label backdoor attack. approach COMPARE defenses. defenses COMPARE approach. OtherScientificTerm is malicious data. Method is ensemble of weak learners. ,"This paper proposes a new method for backdoor data poisoning attacks against deep neural networks. The authors propose a boosting framework to improve the robustness of the network against backdoor attacks. The proposed method is based on a bootstrapped measure of generalization, which is used to measure the generalization ability of an ensemble of weak learners. The method is evaluated on a variety of datasets and shows that the proposed method outperforms existing defenses.","This paper proposes a new method for backdoor data poisoning attacks against deep neural networks. The authors propose a boosting framework to improve the robustness of the network against backdoor attacks. The proposed method is based on a bootstrapped measure of generalization, which is used to measure the generalization ability of an ensemble of weak learners. The method is evaluated on a variety of datasets and shows that the proposed method outperforms existing defenses."
3973,SP:f0ad7cbc190113bb4612b7beca98d07aeff661fd,"MLTC USED-FOR modeling label correlations. single - label text classification methods COMPARE MLTC. MLTC COMPARE single - label text classification methods. document representation learning USED-FOR single - label text classification methods. label - correlation simplification CONJUNCTION sequencing label sets. sequencing label sets CONJUNCTION label - correlation simplification. sequencing label sets CONJUNCTION label - correlation overload. label - correlation overload CONJUNCTION sequencing label sets. It USED-FOR inductive bias. sequencing label sets HYPONYM-OF inductive bias. label - correlation simplification HYPONYM-OF inductive bias. latent label representations USED-FOR label correlations. latent labels USED-FOR contextual encodings. benchmark datasets EVALUATE-FOR It. label - correlation utilization CONJUNCTION document representation. document representation CONJUNCTION label - correlation utilization. token embeddings COMPARE latent labels. latent labels COMPARE token embeddings. embeddings FEATURE-OF latent labels. task information FEATURE-OF they. Task is Multi - label text classification ( MLTC ). OtherScientificTerm are complex label dependencies, and text tokens. Generic are method, and BERT. Method are latent - label encodings, latent and distributed correlation modeling, and latent label embeddings. ",This paper proposes a new method for multi-label text classification based on latent and distributed correlation modeling. The proposed method is based on the idea of label-correlation simplification and label-relation utilization. The authors show that the proposed method outperforms the state-of-the-art single label text classification methods on a number of benchmark datasets. ,This paper proposes a new method for multi-label text classification based on latent and distributed correlation modeling. The proposed method is based on the idea of label-correlation simplification and label-relation utilization. The authors show that the proposed method outperforms the state-of-the-art single label text classification methods on a number of benchmark datasets. 
3989,SP:6e54083a06942f2c41e1796a9f911d3dd9bab0cc,"images CONJUNCTION audio. audio CONJUNCTION images. deep convolutional networks USED-FOR tasks. deep convolutional networks USED-FOR highdimensional data. highdimensional data USED-FOR tasks. images HYPONYM-OF highdimensional data. audio HYPONYM-OF highdimensional data. convolution and pooling layers FEATURE-OF hierarchical kernels. convolutional kernel networks USED-FOR hierarchical kernels. norm USED-FOR spatial similarities. pooling layers USED-FOR norm. additive models of interaction terms PART-OF RKHS. pooling layers USED-FOR spatial similarities. pooling CONJUNCTION patches. patches CONJUNCTION pooling. sample complexity guarantees EVALUATE-FOR patches. Generic are they, and terms. Method is kernel methods. Material is vision datasets. OtherScientificTerm are functional space, inductive bias, generalization bounds, and regularities. ","This paper studies the generalization properties of RKHS (RKHS), a family of deep convolutional neural networks with pooling and convolution layers. The authors show that, for a given input image and a pooling layer, the sample complexity of the network is bounded by the norm of the interaction terms in the functional space of the input image. They also show that for a certain subset of the pooling layers, the bound is also bounded by a regularization term. ","This paper studies the generalization properties of RKHS (RKHS), a family of deep convolutional neural networks with pooling and convolution layers. The authors show that, for a given input image and a pooling layer, the sample complexity of the network is bounded by the norm of the interaction terms in the functional space of the input image. They also show that for a certain subset of the pooling layers, the bound is also bounded by a regularization term. "
4005,SP:7bee8d65c68765cbfe38767743fec27981879d34,"Neural Tangent Kernel ( NTK ) PART-OF deep learning. NTK USED-FOR training and generalization of NN architectures. infinite width limit FEATURE-OF NTK. NTK USED-FOR NNs. architecture search CONJUNCTION meta - learning. meta - learning CONJUNCTION architecture search. NTK USED-FOR finite widths. compute and memory requirements FEATURE-OF NTK computation. NTK computation PART-OF finite width networks. compute and memory requirements FEATURE-OF finite width NTK. neural networks USED-FOR algorithms. algorithms USED-FOR finite width NTK. compute and memory requirements EVALUATE-FOR algorithms. attention CONJUNCTION recurrence. recurrence CONJUNCTION attention. convolutions CONJUNCTION attention. attention CONJUNCTION convolutions. general - purpose JAX function transformations USED-FOR differentiable computation. algorithms USED-FOR differentiable computation. convolutions CONJUNCTION recurrence. recurrence CONJUNCTION convolutions. general - purpose JAX function transformations USED-FOR algorithms. recurrence HYPONYM-OF algorithms. convolutions HYPONYM-OF general - purpose JAX function transformations. attention HYPONYM-OF algorithms. recurrence HYPONYM-OF general - purpose JAX function transformations. attention HYPONYM-OF general - purpose JAX function transformations. recurrence HYPONYM-OF differentiable computation. convolutions HYPONYM-OF differentiable computation. attention HYPONYM-OF differentiable computation. OtherScientificTerm are neural network ( NN ) Jacobians, and hyper - parameters. Method is NN architectures. ","This paper studies the problem of compute and memory requirements of neural tangent kernel (NTK) computation for finite width neural networks. The authors propose a number of algorithms for NTK computation, including convolutions, attention, and recurrence. The main contribution of this paper is to provide a theoretical analysis of the computational complexity of NTK computations in the infinite width limit. The paper also provides theoretical analysis on the convergence of the proposed algorithms. ","This paper studies the problem of compute and memory requirements of neural tangent kernel (NTK) computation for finite width neural networks. The authors propose a number of algorithms for NTK computation, including convolutions, attention, and recurrence. The main contribution of this paper is to provide a theoretical analysis of the computational complexity of NTK computations in the infinite width limit. The paper also provides theoretical analysis on the convergence of the proposed algorithms. "
4021,SP:1df605fc5fc828304f7b836724d8fd6c233ff80c,"policy USED-FOR expected return. problem setting USED-FOR real - world scenarios. safety constraints FEATURE-OF policy. policy USED-FOR offline RL setting. estimation error FEATURE-OF offpolicy evaluation. offline constrained RL algorithm USED-FOR policy. stationary distribution FEATURE-OF policy. stationary distribution corrections FEATURE-OF optimal policy. algorithm USED-FOR stationary distribution corrections. algorithm USED-FOR cost - conservative policy. returns FEATURE-OF optimal policy. constraint satisfaction CONJUNCTION return - maximization. return - maximization CONJUNCTION constraint satisfaction. COptiDICE COMPARE baseline algorithms. baseline algorithms COMPARE COptiDICE. COptiDICE USED-FOR policies. return - maximization FEATURE-OF policies. constraint satisfaction FEATURE-OF policies. constraint satisfaction EVALUATE-FOR COptiDICE. return - maximization EVALUATE-FOR COptiDICE. Task is offline constrained reinforcement learning ( RL ) problem. OtherScientificTerm are cost constraints, and cost upper bound. Material is pre - collected dataset. ","This paper proposes an offline constrained reinforcement learning (RL) algorithm, called COptiDICE, to solve the problem of offline constrained RL. The main contribution of the paper is to propose a cost-conservative policy for offline RL, which is based on the assumption that the cost upper bound of the off-policy evaluation error is a function of the stationary distribution of the policy. The authors show that the proposed algorithm is able to find the optimal policy in the offline constrained setting, and show that it can achieve better performance than the baselines in terms of return-maximization and constraint satisfaction.","This paper proposes an offline constrained reinforcement learning (RL) algorithm, called COptiDICE, to solve the problem of offline constrained RL. The main contribution of the paper is to propose a cost-conservative policy for offline RL, which is based on the assumption that the cost upper bound of the off-policy evaluation error is a function of the stationary distribution of the policy. The authors show that the proposed algorithm is able to find the optimal policy in the offline constrained setting, and show that it can achieve better performance than the baselines in terms of return-maximization and constraint satisfaction."
4037,SP:5a10c13eb78d26a25dac74601419deb68c53cb75,"dataparallel CONJUNCTION model - parallel training algorithms. model - parallel training algorithms CONJUNCTION dataparallel. parallelization strategies USED-FOR GRU. model - parallel training algorithms HYPONYM-OF parallelization strategies. dataparallel HYPONYM-OF parallelization strategies. training time EVALUATE-FOR approaches. parallel training scheme USED-FOR GRU. parallel - in - time HYPONYM-OF parallel training scheme. multigrid reduction in time ( MGRIT ) solver USED-FOR parallel training scheme. hierarchical correction of the hidden state USED-FOR end - to - end communication. parallel training scheme COMPARE serial approach. serial approach COMPARE parallel training scheme. HMDB51 dataset EVALUATE-FOR parallel training scheme. speedup EVALUATE-FOR serial approach. speedup EVALUATE-FOR parallel training scheme. parallelization strategy COMPARE parallel GRU algorithm. parallel GRU algorithm COMPARE parallelization strategy. sequence length FEATURE-OF parallelization strategy. Task is Parallelizing Gated Recurrent Unit ( GRU ) networks. Method are MGRIT, and gradient descent. OtherScientificTerm is processors. Material is image sequence. ",This paper proposes a parallel training scheme for Gated Recurrent Unit (GRU) networks. The authors propose a multigrid reduction in time (MGRIT) solver to solve the problem of parallel training of GRUs. The proposed method is based on the hierarchical correction of the hidden state and the end-to-end communication. Experiments on the HMDB51 dataset demonstrate the effectiveness of the proposed method.,This paper proposes a parallel training scheme for Gated Recurrent Unit (GRU) networks. The authors propose a multigrid reduction in time (MGRIT) solver to solve the problem of parallel training of GRUs. The proposed method is based on the hierarchical correction of the hidden state and the end-to-end communication. Experiments on the HMDB51 dataset demonstrate the effectiveness of the proposed method.
4053,SP:fb935a5c44d7df6958d39ab1ef877956df08994e,"Functional magnetic resonance imaging ( fMRI ) HYPONYM-OF noisy measurement of brain activity. measurement resolution USED-FOR spatiotemporal averaging. PCA CONJUNCTION shared response modeling ( SRM ). shared response modeling ( SRM ) CONJUNCTION PCA. linear methods USED-FOR they. shared response modeling ( SRM ) HYPONYM-OF linear methods. PCA HYPONYM-OF linear methods. neural network USED-FOR common embedding. common space USED-FOR extensible manifold. classification accuracy EVALUATE-FOR stimulus features. framework USED-FOR applications. OtherScientificTerm are environmental differences, intrinsic dimension, brain activity, intrinsic structure, and noise. Generic is approaches. Material is raw fMRI signals. Task is cross - subject translation of fMRI signals. ","This paper proposes a novel framework for cross-subject translation of fMRI signals. The authors propose to use a shared response modeling (SRM) framework to learn a common embedding of the input fMRI signal, which is then used to train a neural network to translate the input signal to the target domain. The proposed method is evaluated on a number of datasets and shows that the proposed method outperforms existing methods. ","This paper proposes a novel framework for cross-subject translation of fMRI signals. The authors propose to use a shared response modeling (SRM) framework to learn a common embedding of the input fMRI signal, which is then used to train a neural network to translate the input signal to the target domain. The proposed method is evaluated on a number of datasets and shows that the proposed method outperforms existing methods. "
4069,SP:95ed80753116005f1f7bae24c855d350f4af85a1,"biological phenomena CONJUNCTION self - driving cars. self - driving cars CONJUNCTION biological phenomena. Detecting out - of - distribution examples USED-FOR safety - critical machine learning applications. detecting novel biological phenomena HYPONYM-OF safety - critical machine learning applications. self - driving cars HYPONYM-OF safety - critical machine learning applications. benchmarks USED-FOR large - scale settings. ImageNet-21 K USED-FOR PASCAL VOC and COCO multilabel anomaly detectors. benchmark USED-FOR anomaly segmentation. road anomalies FEATURE-OF segmentation benchmark. segmentation benchmark USED-FOR benchmark. detector COMPARE prior methods. prior methods COMPARE detector. maximum logit USED-FOR detector. OtherScientificTerm are small - scale settings, and real - world settings. Task is out - of - distribution detection. Material is high - resolution images. Method is ImageNet multiclass anomaly detectors. ","This paper proposes a new benchmark for out-of-distribution anomaly detection. The benchmark is based on ImageNet-21K, which is a multi-class multiclass anomaly detector. The authors show that the proposed benchmark outperforms existing benchmarks in terms of detection accuracy and segmentation performance.","This paper proposes a new benchmark for out-of-distribution anomaly detection. The benchmark is based on ImageNet-21K, which is a multi-class multiclass anomaly detector. The authors show that the proposed benchmark outperforms existing benchmarks in terms of detection accuracy and segmentation performance."
4085,SP:abbab40e40ef09c8dccd16661af3c2a4461ebb1a,"parametric models USED-FOR intransitive tournaments. d dimensional node representations USED-FOR parametric models. d dimensional representations USED-FOR class of tournaments. theory USED-FOR parametric tournament representations. d dimensional representations USED-FOR class of tournaments. forbidden configurations FEATURE-OF tournament classes. tournaments PART-OF forbidden flip class. rank 2 tournaments CONJUNCTION locally - transitive tournaments. locally - transitive tournaments CONJUNCTION rank 2 tournaments. tournament class USED-FOR minimum feedback arc set problem. Quicksort procedure USED-FOR minimum feedback arc set problem. coned - doubly regular tournament FEATURE-OF flip class. forbidden configuration FEATURE-OF flip class. minimum dimension USED-FOR tournaments. upper bound USED-FOR smallest representation dimension. flip class FEATURE-OF tournament. flip class FEATURE-OF feedback arc set. OtherScientificTerm are Real world tournaments, union of flip classes, rank d tournament class, and sign - rank of matrices. ","This paper studies the problem of learning parametric tournament representations for intransitive tournaments. The authors propose a new class of tournaments, called the flip class, which is a subset of the rank d tournament class. The flip class is defined as the subset of tournaments that are forbidden to be played in a given tournament class, and the authors show that this class can be seen as a sub-class of rank d tournaments. They also show that the minimum dimension of the feedback arc set of a tournament class is bounded by the sign-rank of matrices of the tournament class and the number of tournaments in the class. ","This paper studies the problem of learning parametric tournament representations for intransitive tournaments. The authors propose a new class of tournaments, called the flip class, which is a subset of the rank d tournament class. The flip class is defined as the subset of tournaments that are forbidden to be played in a given tournament class, and the authors show that this class can be seen as a sub-class of rank d tournaments. They also show that the minimum dimension of the feedback arc set of a tournament class is bounded by the sign-rank of matrices of the tournament class and the number of tournaments in the class. "
4101,SP:d39765dcc8950d4fc1d43e4c167208736578882e,context dataset USED-FOR Neural processes ( NPs ). identifier USED-FOR task. context representation USED-FOR identifier. dataset USED-FOR context representation. NPs USED-FOR identifier. context representation USED-FOR NPs. dataset USED-FOR NPs. network architectures CONJUNCTION aggregation functions. aggregation functions CONJUNCTION network architectures. NPs USED-FOR context embedding approaches. prediction accuracy EVALUATE-FOR NPs. permutation invariant FEATURE-OF aggregation functions. stochastic attention mechanism USED-FOR context information. stochastic attention mechanism USED-FOR NPs. NPs USED-FOR context information. method USED-FOR context embedding. method USED-FOR NPs. NPs USED-FOR context embedding. information theory USED-FOR method. features USED-FOR NPs. method USED-FOR context embedding. noisy data sets CONJUNCTION restricted task distributions. restricted task distributions CONJUNCTION noisy data sets. noisy data sets USED-FOR method. context embeddings USED-FOR NPs. predator - prey model CONJUNCTION image completion. image completion CONJUNCTION predator - prey model. 1D regression CONJUNCTION predator - prey model. predator - prey model CONJUNCTION 1D regression. approach COMPARE NPs. NPs COMPARE approach. 1D regression USED-FOR NPs. predator - prey model USED-FOR NPs. predator - prey model USED-FOR approach. image completion USED-FOR approach. 1D regression EVALUATE-FOR approach. MovieLens-10k dataset HYPONYM-OF real - world problem. real - world problem EVALUATE-FOR method. MovieLens-10k dataset EVALUATE-FOR method. ,"This paper proposes a method for learning context embeddings for Neural Processes (NPs). The proposed method is based on the information theory of information theory. The key idea is to learn a context representation for NPs that is permutation invariant, which can be applied to any NPs. The method is evaluated on the MovieLens-10k dataset and is shown to outperform existing methods. ","This paper proposes a method for learning context embeddings for Neural Processes (NPs). The proposed method is based on the information theory of information theory. The key idea is to learn a context representation for NPs that is permutation invariant, which can be applied to any NPs. The method is evaluated on the MovieLens-10k dataset and is shown to outperform existing methods. "
4117,SP:f6e7229b653a5a56a2993864cdb70809f5b6f9b4,"Transformer language models USED-FOR NLP tasks. prototype networks PART-OF model architecture. architecture COMPARE language models. language models COMPARE architecture. user interactions USED-FOR it. Metric is interpretability. Generic are black - box models, and network. OtherScientificTerm is human capabilities. Method is data - driven approaches. ","This paper proposes a new architecture for transformer-based NLP models. The proposed architecture is based on a prototype network, which is a combination of prototype networks and a transformer language model. The paper shows that the proposed architecture outperforms existing models in terms of interpretability and interpretability of user interactions. The authors also show that the performance of the proposed model is comparable to the state-of-the-art.","This paper proposes a new architecture for transformer-based NLP models. The proposed architecture is based on a prototype network, which is a combination of prototype networks and a transformer language model. The paper shows that the proposed architecture outperforms existing models in terms of interpretability and interpretability of user interactions. The authors also show that the performance of the proposed model is comparable to the state-of-the-art."
4133,SP:60ba9cb4c42cecde6379ec0279434dece822a2b1,"Catastrophic forgetting PART-OF continual learning. Trust Region Gradient Projection ( TRGP ) USED-FOR continual learning. Trust Region Gradient Projection ( TRGP ) USED-FOR forward knowledge transfer. continual learning USED-FOR forward knowledge transfer. scaled weight projection USED-FOR frozen weights. frozen weights PART-OF trust region. layer - wise scaling matrix USED-FOR scaled weight projection. layer - wise scaling matrix USED-FOR frozen weights. TRGP USED-FOR knowledge transfer. scaling matrices CONJUNCTION model. model CONJUNCTION scaling matrices. approach COMPARE state - of - the - art methods. state - of - the - art methods COMPARE approach. Generic are methods, and task. OtherScientificTerm are optimization space, task correlation, layer - wise and single - shot manner, and subspaces of old tasks. Method is norm of gradient projection. ",This paper proposes a method for continual learning in the context of continual learning. The proposed method is based on the idea of Trust Region Gradient Projection (TRGP). The key idea of TRGP is to use the norm of the gradient projection of the trust region as a surrogate for the scaling matrix of the weights in the layer-wise and single-shot manner. The authors show that the proposed method outperforms state-of-the-art methods on a number of benchmark tasks.,This paper proposes a method for continual learning in the context of continual learning. The proposed method is based on the idea of Trust Region Gradient Projection (TRGP). The key idea of TRGP is to use the norm of the gradient projection of the trust region as a surrogate for the scaling matrix of the weights in the layer-wise and single-shot manner. The authors show that the proposed method outperforms state-of-the-art methods on a number of benchmark tasks.
4149,SP:25414fe1c6203f9b623c5317a4ffaba478085c4c,Optimization CONJUNCTION generalization. generalization CONJUNCTION Optimization. generalization PART-OF machine learning. Optimization PART-OF machine learning. framework USED-FOR generalization. framework USED-FOR optimization. optimization CONJUNCTION generalization. generalization CONJUNCTION optimization. gradient flow algorithm USED-FOR length of optimization trajectory. length of optimization trajectory USED-FOR generalization error. initialization USED-FOR gradient flow. estimate USED-FOR length - based generalization bound. short optimization paths USED-FOR generalization. it USED-FOR generalization estimates. kernel regression CONJUNCTION overparameterized two - layer ReLU neural networks. overparameterized two - layer ReLU neural networks CONJUNCTION kernel regression. underdetermined lp linear regression CONJUNCTION kernel regression. kernel regression CONJUNCTION underdetermined lp linear regression. it USED-FOR machine learning models. generalization estimates USED-FOR machine learning models. overparameterized two - layer ReLU neural networks HYPONYM-OF machine learning models. underdetermined lp linear regression HYPONYM-OF machine learning models. kernel regression HYPONYM-OF machine learning models. Generic is approach. OtherScientificTerm is explicit length estimate. ,This paper studies the generalization bound for optimization and generalization in the context of machine learning. The authors propose a new framework for estimating the length of the optimization trajectory for a given optimization problem. The main contribution of the paper is to derive a bound for generalization based on the length-based generalization bounds. The bound is based on a gradient flow algorithm. The paper also provides a theoretical analysis of this bound. ,This paper studies the generalization bound for optimization and generalization in the context of machine learning. The authors propose a new framework for estimating the length of the optimization trajectory for a given optimization problem. The main contribution of the paper is to derive a bound for generalization based on the length-based generalization bounds. The bound is based on a gradient flow algorithm. The paper also provides a theoretical analysis of this bound. 
4165,SP:a9e5d81f7ba88f4052730f255cf48cb40ed80942,"Adversarial examples USED-FOR deep learning systems. high - frequency noise FEATURE-OF adversarial examples. CIFAR-10 CONJUNCTION ImageNet - derived datasets. ImageNet - derived datasets CONJUNCTION CIFAR-10. ImageNet - derived datasets USED-FOR models. CIFAR-10 USED-FOR models. frequency constraints USED-FOR robust models. Task is attacks. Generic are examples, and framework. Method is frequency - based understanding of adversarial examples. OtherScientificTerm is frequency - based explanation. ","This paper proposes a frequency-based understanding of adversarial examples, which can be used to improve the robustness of deep learning models against high-frequency noise. The authors propose a novel framework for understanding the frequency of high-frequencies in adversarial attacks. The proposed framework is based on the observation that the frequency constraint of a given example can be understood as a function of the number of frequencies in the example, and the authors propose to use this framework to improve robustness to high frequency noise. ","This paper proposes a frequency-based understanding of adversarial examples, which can be used to improve the robustness of deep learning models against high-frequency noise. The authors propose a novel framework for understanding the frequency of high-frequencies in adversarial attacks. The proposed framework is based on the observation that the frequency constraint of a given example can be understood as a function of the number of frequencies in the example, and the authors propose to use this framework to improve robustness to high frequency noise. "
4181,SP:5d94dbfd10dc2ef86415853cc41f414a24962d4f,"relational inductive bias ( homophily assumption ) USED-FOR graph structures. graph structures USED-FOR Graph Neural Networks ( GNNs ). GNNs COMPARE NNs. NNs COMPARE GNNs. GNNs COMPARE graph - agnostic NNs. graph - agnostic NNs COMPARE GNNs. NNs USED-FOR real - world tasks. real - world tasks EVALUATE-FOR GNNs. aggregation operation USED-FOR GNNs. similarity matrix USED-FOR GNNs. graph structure CONJUNCTION features. features CONJUNCTION graph structure. features USED-FOR GNNs. similarity matrix USED-FOR metrics. metrics COMPARE homophily metrics. homophily metrics COMPARE metrics. synthetic graphs EVALUATE-FOR homophily metrics. diversification operation USED-FOR harmful heterophily. diversification CONJUNCTION identity channels. identity channels CONJUNCTION diversification. aggregation CONJUNCTION diversification. diversification CONJUNCTION aggregation. Adaptive Channel Mixing ( ACM ) framework USED-FOR aggregation. identity channels USED-FOR harmful heterophily. Adaptive Channel Mixing ( ACM ) framework USED-FOR diversification. Adaptive Channel Mixing ( ACM ) framework USED-FOR harmful heterophily. identity channels USED-FOR GNN layer. Adaptive Channel Mixing ( ACM ) framework USED-FOR identity channels. diversification USED-FOR GNN layer. GNN layer USED-FOR harmful heterophily. realworld node classification tasks EVALUATE-FOR ACM - augmented baselines. They COMPARE GNNs. GNNs COMPARE They. tasks EVALUATE-FOR They. tasks EVALUATE-FOR GNNs. OtherScientificTerm are Heterophily, and heterophily. Method is filterbanks. ","This paper studies the problem of heterophily in graph neural networks (GNNs). In particular, the authors propose a new aggregation operation for GNNs, called Adaptive Channel Mixing (ACM), which is based on the adaptive channel mixing (ACP) framework. The authors show that ACM is able to reduce the harmful heterophilicity of GNN layers. They also show that the proposed ACM-augmented baselines can be used to improve the performance of existing GNN models.","This paper studies the problem of heterophily in graph neural networks (GNNs). In particular, the authors propose a new aggregation operation for GNNs, called Adaptive Channel Mixing (ACM), which is based on the adaptive channel mixing (ACP) framework. The authors show that ACM is able to reduce the harmful heterophilicity of GNN layers. They also show that the proposed ACM-augmented baselines can be used to improve the performance of existing GNN models."
4197,SP:fd4ab1cb777b541c22a923c1c86d82ac1d8384fd,generalizability FEATURE-OF deep RL approach. RL training techniques USED-FOR deep learning architecture. deep learning architecture PART-OF proposition. equivariance USED-FOR training. local search heuristics USED-FOR value landscape. local search heuristics CONJUNCTION RL training. RL training CONJUNCTION local search heuristics. RL training USED-FOR value landscape. proposition COMPARE deep RL methods. deep RL methods COMPARE proposition. random and realistic TSP problems EVALUATE-FOR deep RL methods. random and realistic TSP problems EVALUATE-FOR proposition. Method is Deep reinforcement learning ( RL ). Material is larger - sized instances. Generic is approach. Task is ablation study. ,"This paper proposes a deep reinforcement learning (RL) approach for ablation study. The authors propose a new approach to learn a deep RL model that is generalizable to larger-sized instances. The proposed approach is based on the notion of equivariance, which is defined as the difference between the value landscape and the training landscape of the model. The paper shows that the proposed approach outperforms existing deep RL methods on a number of ablation studies. ","This paper proposes a deep reinforcement learning (RL) approach for ablation study. The authors propose a new approach to learn a deep RL model that is generalizable to larger-sized instances. The proposed approach is based on the notion of equivariance, which is defined as the difference between the value landscape and the training landscape of the model. The paper shows that the proposed approach outperforms existing deep RL methods on a number of ablation studies. "
4213,SP:8aa471b92e2671d471107c087164378f45fb204f,"Federated learning ( FL ) HYPONYM-OF privacy - preserving collaborative learning paradigm. framework USED-FOR non - IID issue. local generative adversarial network ( GAN ) USED-FOR synthetic data. parameter server ( PS ) USED-FOR global shared synthetic dataset. confident threshold USED-FOR pseudo labeling. pseudo labeling USED-FOR PS. local private dataset CONJUNCTION labeled synthetic dataset. labeled synthetic dataset CONJUNCTION local private dataset. artificial noise USED-FOR local model gradients. local GANs USED-FOR privacy. differential privacy USED-FOR local GANs. artificial noise USED-FOR local GANs. framework COMPARE baseline methods. baseline methods COMPARE framework. supervised and semi - supervised settings FEATURE-OF benchmark datasets. supervised and semi - supervised settings EVALUATE-FOR framework. supervised and semi - supervised settings EVALUATE-FOR baseline methods. benchmark datasets EVALUATE-FOR framework. benchmark datasets EVALUATE-FOR baseline methods. Generic is it. OtherScientificTerm are IID ( independent and identically distributed ) data, and data distributions. Material are differentially private synthetic data, and global dataset. Task is global aggregation. Method is local models. ",This paper proposes a federated learning (FL) framework to address the non-IID issue. The proposed framework is based on the local generative adversarial network (GAN) framework. The local GAN is trained on a global shared synthetic dataset and then aggregated to a local private dataset. The method is evaluated on both supervised and semi-supervised settings.,This paper proposes a federated learning (FL) framework to address the non-IID issue. The proposed framework is based on the local generative adversarial network (GAN) framework. The local GAN is trained on a global shared synthetic dataset and then aggregated to a local private dataset. The method is evaluated on both supervised and semi-supervised settings.
4229,SP:7656b0bd5eb7e46359d8111e5534a07744f5d7ae,"classifier USED-FOR classifier. Gaussian noise USED-FOR classifier. robustness EVALUATE-FOR classifier. accuracy CONJUNCTION ( adversarial ) robustness. ( adversarial ) robustness CONJUNCTION accuracy. training method USED-FOR smoothed classifiers. sample - wise control of robustness USED-FOR training method. robustness CONJUNCTION prediction confidence. prediction confidence CONJUNCTION robustness. robustness EVALUATE-FOR smoothed classifiers. prediction confidence FEATURE-OF smoothed classifiers. certified robustness EVALUATE-FOR training methods. method COMPARE training methods. training methods COMPARE method. certified robustness EVALUATE-FOR method. OtherScientificTerm are ` 2 - adversarial perturbations, noise, adversarial robustness, training objective, and worst - case ( adversarial ) objective. Method is randomized smoothing. Generic is control. ",This paper proposes a randomized smoothing method to improve the robustness of smoothed classifiers against adversarial perturbations. The proposed method is based on the idea of sample-wise control of robustness. The authors show that the proposed method outperforms existing methods in terms of certified robustness and prediction confidence. ,This paper proposes a randomized smoothing method to improve the robustness of smoothed classifiers against adversarial perturbations. The proposed method is based on the idea of sample-wise control of robustness. The authors show that the proposed method outperforms existing methods in terms of certified robustness and prediction confidence. 
4245,SP:8fdfed1c38ae00a0063ab41f72fa26826f5f4570,"Wikipedia dataset USED-FOR pretraining BERT. histogram of sequence lengths USED-FOR packing. linear complexity EVALUATE-FOR algorithms. packing order FEATURE-OF Wikipedia dataset. model USED-FOR dataset. OtherScientificTerm are padding tokens, padding, near optimal packing, and 2x speed - up. Method is packing algorithms. Material is packed dataset. Metric is convergence. ","This paper studies the problem of pretraining BERT models on Wikipedia. The authors propose a new method to train a BERT model on the Wikipedia dataset. The proposed method is based on the idea of padding tokens, which is an extension of previous work on padding. The paper shows that the proposed method converges to a near optimal packing algorithm with linear complexity, and shows that it can achieve 2x speed-up compared to existing methods. ","This paper studies the problem of pretraining BERT models on Wikipedia. The authors propose a new method to train a BERT model on the Wikipedia dataset. The proposed method is based on the idea of padding tokens, which is an extension of previous work on padding. The paper shows that the proposed method converges to a near optimal packing algorithm with linear complexity, and shows that it can achieve 2x speed-up compared to existing methods. "
4261,SP:460d4cc1a5c01e34abe37d9eb1b74dd3734b1d55,adaptive tree search algorithm USED-FOR high - scoring outputs. adaptive tree search algorithm HYPONYM-OF Monte Carlo tree search. translation models USED-FOR high - scoring outputs. algorithm USED-FOR models. autoregressivity CONJUNCTION conditional independence assumptions. conditional independence assumptions CONJUNCTION autoregressivity. algorithm COMPARE beam search. beam search COMPARE algorithm. decoding bias USED-FOR autoregressive models. algorithm USED-FOR autoregressive models. adaptive tree search algorithm COMPARE beam search. beam search COMPARE adaptive tree search algorithm. reranking techniques USED-FOR models. beam search USED-FOR autoregressive models. adaptive tree search algorithm COMPARE reranking techniques. reranking techniques COMPARE adaptive tree search algorithm. model scores EVALUATE-FOR adaptive tree search algorithm. BLEU EVALUATE-FOR translation model objectives. noisy channel model CONJUNCTION objective. objective CONJUNCTION noisy channel model. autoregressive models CONJUNCTION noisy channel model. noisy channel model CONJUNCTION autoregressive models. expected automatic metric scores CONJUNCTION noisy channel model. noisy channel model CONJUNCTION expected automatic metric scores. autoregressive models USED-FOR expected automatic metric scores. decoder USED-FOR search. objective HYPONYM-OF models. autoregressive models HYPONYM-OF models. beam search bias USED-FOR models. noisy channel model HYPONYM-OF models. search USED-FOR models. beam search CONJUNCTION reranking based methods. reranking based methods CONJUNCTION beam search. OtherScientificTerm is search objective. Task is decoding. Generic is objectives. ,This paper proposes an adaptive tree search algorithm to find high-scoring outputs for autoregressive models. The main idea is to use the decoding bias of the decoder to optimize the search objective. The authors show that the proposed algorithm outperforms beam search and reranking based methods in terms of model scores. ,This paper proposes an adaptive tree search algorithm to find high-scoring outputs for autoregressive models. The main idea is to use the decoding bias of the decoder to optimize the search objective. The authors show that the proposed algorithm outperforms beam search and reranking based methods in terms of model scores. 
4277,SP:ff7b9e6ff5303f8a4f0321d06d9d9573e4853c5f,"normal distribution FEATURE-OF task. Energy Based Model ( EBM ) USED-FOR intractability of abnormal distribution. iterative optimization procedure USED-FOR Langevin Dynamics ( LD ). Langevin Dynamics ( LD ) USED-FOR EBM. iterative optimization procedure USED-FOR EBM. anomaly detector USED-FOR task. adaptive sparse coding layer USED-FOR anomaly detector. OtherScientificTerm are anomaly, normal population, plug and play feature, and sparse coding layer. Method are AI solutions, EBMs, and meta learning scheme. ","This paper proposes a meta-learning approach for learning energy-based models (EBMs) that can be used to detect anomalies in the normal distribution. The proposed approach is based on the Langevin Dynamics (LD) framework. The authors show that the proposed approach can be applied to a variety of tasks, including anomaly detection, anomaly detection and anomaly detection. The method is evaluated on a number of benchmark datasets. ","This paper proposes a meta-learning approach for learning energy-based models (EBMs) that can be used to detect anomalies in the normal distribution. The proposed approach is based on the Langevin Dynamics (LD) framework. The authors show that the proposed approach can be applied to a variety of tasks, including anomaly detection, anomaly detection and anomaly detection. The method is evaluated on a number of benchmark datasets. "
4293,SP:801a61d01d3b159f301013b182150a80fbfe8fa2,propaganda CONJUNCTION news. news CONJUNCTION propaganda. news CONJUNCTION social media. social media CONJUNCTION news. QA systems USED-FOR misinformation. misinformation FEATURE-OF QA models. large - scale dataset USED-FOR problem. CONTRAQA HYPONYM-OF large - scale dataset. contradicting contexts USED-FOR QA models. question answering CONJUNCTION misinformation detection. misinformation detection CONJUNCTION question answering. counter - measure USED-FOR misinformation - aware QA system. misinformation detection PART-OF counter - measure. question answering PART-OF counter - measure. misinformation detection PART-OF misinformation - aware QA system. Method is QA model. OtherScientificTerm is real and fake information. ,"This paper studies the problem of misinformation detection and counter-measure in the context of QA. The authors propose a new dataset called CONTRAQA, which is a large-scale dataset of contradicting contexts. The dataset is used to train a QA system that is able to distinguish between real and fake information. The paper also proposes a new counter measure for misinformation detection that is based on question answering and question answering. ","This paper studies the problem of misinformation detection and counter-measure in the context of QA. The authors propose a new dataset called CONTRAQA, which is a large-scale dataset of contradicting contexts. The dataset is used to train a QA system that is able to distinguish between real and fake information. The paper also proposes a new counter measure for misinformation detection that is based on question answering and question answering. "
4309,SP:7effe51275b9b2e14b3e099533e410e09f5b7c5a,"embodiment CONJUNCTION morphology. morphology CONJUNCTION embodiment. expert demonstrations USED-FOR imitation agent. embodiment USED-FOR imitation agent. morphology USED-FOR imitation agent. method USED-FOR cross - domain imitation. GromovWasserstein distance USED-FOR method. GWIL USED-FOR optimality. rigid transformation of the expert domain CONJUNCTION arbitrary transformation of the state - action space. arbitrary transformation of the state - action space CONJUNCTION rigid transformation of the expert domain. GWIL USED-FOR continuous control domains. Task is Cross - domain imitation learning. OtherScientificTerm is stationary distributions. Generic are they, and theory. Method is Gromov - Wasserstein Imitation Learning ( GWIL ). ",This paper proposes a new method for cross-domain imitation learning. The method is based on the Gromov-Wasserstein distance (GWIL) between expert demonstrations and the imitation agent. The authors show that GWIL is able to achieve better performance than the state-of-the-art on continuous control tasks. The paper also provides a theoretical analysis of GWIL.,This paper proposes a new method for cross-domain imitation learning. The method is based on the Gromov-Wasserstein distance (GWIL) between expert demonstrations and the imitation agent. The authors show that GWIL is able to achieve better performance than the state-of-the-art on continuous control tasks. The paper also provides a theoretical analysis of GWIL.
4325,SP:9ec000cd9c15e3c9988a41921c465b42e7d41877,"self - supervised learning ( SSL ) USED-FOR computer vision. labeling cost FEATURE-OF computer vision. labeling cost EVALUATE-FOR self - supervised learning ( SSL ). SSL USED-FOR invariant visual representations. contrastive loss EVALUATE-FOR representation invariant. hidden layer PART-OF projection head. hierarchical projection head USED-FOR raw representations of the backbone. hierarchical projection head USED-FOR HCCL. cross - level contrastive learning USED-FOR HCCL. generalization ability EVALUATE-FOR visual representations. generalization ability EVALUATE-FOR HCCL. HCCL USED-FOR SSL frameworks. detection CONJUNCTION segmentation. segmentation CONJUNCTION detection. classification CONJUNCTION detection. detection CONJUNCTION classification. segmentation CONJUNCTION few - shot learning tasks. few - shot learning tasks CONJUNCTION segmentation. HCCL USED-FOR detection. HCCL USED-FOR segmentation. few - shot learning tasks EVALUATE-FOR HCCL. classification EVALUATE-FOR HCCL. HCCL COMPARE methods. methods COMPARE HCCL. benchmark datasets EVALUATE-FOR HCCL. benchmark datasets EVALUATE-FOR methods. Method are SSL methods, and Hierarchical Cross Contrastive Learning(HCCL ). Generic is approach. OtherScientificTerm are latent spaces, and latent features. ","This paper proposes Hierarchical Cross Contrastive Learning (HCCL), a method for self-supervised learning (SSL) that is based on hierarchical cross-level contrastive learning. HCCL uses a hierarchical projection head to learn the latent representations of the backbone and the hidden layer of the projection head. The proposed method is evaluated on a number of benchmark datasets and compared to several baselines. ","This paper proposes Hierarchical Cross Contrastive Learning (HCCL), a method for self-supervised learning (SSL) that is based on hierarchical cross-level contrastive learning. HCCL uses a hierarchical projection head to learn the latent representations of the backbone and the hidden layer of the projection head. The proposed method is evaluated on a number of benchmark datasets and compared to several baselines. "
4341,SP:943b0a3f94ba270bb7c0dc1e1f363e53bc5cf8ae,"Real economies HYPONYM-OF sequential imperfect - information game. heterogeneous, interacting strategic agents PART-OF sequential imperfect - information game. Dynamic general equilibrium models USED-FOR economic activity. Dynamic general equilibrium models USED-FOR interactions. economic activity CONJUNCTION interactions. interactions CONJUNCTION economic activity. Dynamic general equilibrium models USED-FOR systems. analytical and computational methods USED-FOR explicit equilibria. structured learning curricula CONJUNCTION GPU - only simulation and training. GPU - only simulation and training CONJUNCTION structured learning curricula. market clearing HYPONYM-OF unrealistic assumptions. GPU implementation USED-FOR training and analyzing economies. real - business - cycle models HYPONYM-OF DGE models. approach USED-FOR real - business - cycle models. RL policies CONJUNCTION economic intuitions. economic intuitions CONJUNCTION RL policies. meta - game -Nash equilibria PART-OF open RBC models. approximate best - response analyses USED-FOR meta - game -Nash equilibria. Method are joint learning, and meta - game. OtherScientificTerm are reward function, consumer ’s expendable income, -Nash equilibria, analytical tractability, and worker - consumers. Task is economic simulations. ","This paper proposes a meta-game-Nash model for real-business-cycle (RBC) models. The model is based on the idea of meta-learning, which is an extension of Nash equilibrium. The authors show that the proposed model is able to learn the optimal equilibrium of the meta-games, and show that it can be used to train and analyze RL models. They also show that their model can be applied to a number of real-world problems. ","This paper proposes a meta-game-Nash model for real-business-cycle (RBC) models. The model is based on the idea of meta-learning, which is an extension of Nash equilibrium. The authors show that the proposed model is able to learn the optimal equilibrium of the meta-games, and show that it can be used to train and analyze RL models. They also show that their model can be applied to a number of real-world problems. "
4357,SP:f885c992df9c685f806a653398736432ba38bd80,public API USED-FOR machine learning model. robustness CONJUNCTION model utility. model utility CONJUNCTION robustness. defenses USED-FOR model stealing. query access USED-FOR model extraction. differential privacy USED-FOR calibration. victim model USED-FOR method. Task is model extraction attacks. Generic is model. Metric is computational effort. OtherScientificTerm is proof - of - work. Method is machine learning practitioners. ,"This paper proposes a method to defend against model extraction attacks. The method is based on differential privacy, and the authors show that the proposed method is able to protect the model from model stealing attacks. ","This paper proposes a method to defend against model extraction attacks. The method is based on differential privacy, and the authors show that the proposed method is able to protect the model from model stealing attacks. "
4373,SP:39845a353e75e2f854c3dc649db3817d89ad9875,"Neural Ordinary Differential Equations ( ODEs ) USED-FOR generative models of images. Continuous Normalizing Flows ( CNFs ) USED-FOR generative models of images. models USED-FOR exact likelihood calculation. exact likelihood calculation CONJUNCTION invertible generation / density estimation. invertible generation / density estimation CONJUNCTION exact likelihood calculation. models USED-FOR invertible generation / density estimation. approach COMPARE prior methods. prior methods COMPARE approach. likelihood values FEATURE-OF image datasets. likelihood values EVALUATE-FOR approach. GPU USED-FOR prior methods. image datasets EVALUATE-FOR approach. training time EVALUATE-FOR prior methods. Method is MRCNF ). OtherScientificTerm are conditional distribution, fine image, coarse image, and log likelihood. ",This paper proposes a method for continuous normalizing flows (CNFs) that can be used for invertible generation and density estimation. The proposed method is based on Neural Ordinary Differential Equations (ODEs) and can be applied to the case where the conditional distribution of an image is a mixture of fine and coarse images. The authors show that the proposed method outperforms the existing methods in terms of training time and accuracy. ,This paper proposes a method for continuous normalizing flows (CNFs) that can be used for invertible generation and density estimation. The proposed method is based on Neural Ordinary Differential Equations (ODEs) and can be applied to the case where the conditional distribution of an image is a mixture of fine and coarse images. The authors show that the proposed method outperforms the existing methods in terms of training time and accuracy. 
4389,SP:d09c2fad308249261a9742505e4ccaed2b3578b3,"Label noise PART-OF real - world datasets. robust training techniques USED-FOR DNNs. DNNs USED-FOR corrupted patterns. overfitting USED-FOR corrupted patterns. noisy supervisions USED-FOR model. and training - free solution USED-FOR detect noisy labels. neighborhood information USED-FOR methods. nearby representations USED-FOR local voting. noisy label consensuses USED-FOR local voting. one HYPONYM-OF methods. local voting USED-FOR one. ranking - based approach USED-FOR one. representations USED-FOR ranking - based approach. worst - case error bound EVALUATE-FOR ranking - based method. training - free solutions COMPARE training - based baselines. training - based baselines COMPARE training - free solutions. synthetic and real - world label noise EVALUATE-FOR training - free solutions. synthetic and real - world label noise EVALUATE-FOR training - based baselines. Method is generalization of deep neural networks ( DNNs ). Generic is approach. OtherScientificTerm are noisy labels, and clean label. ","This paper proposes a new method to detect noisy labels in deep neural networks. The method is based on a ranking-based approach, which is a combination of two existing methods: one based on local voting and the other based on neighborhood information. The proposed method is evaluated on synthetic and real-world label noise. The results show that the proposed method outperforms the existing methods in terms of worst-case error bound.","This paper proposes a new method to detect noisy labels in deep neural networks. The method is based on a ranking-based approach, which is a combination of two existing methods: one based on local voting and the other based on neighborhood information. The proposed method is evaluated on synthetic and real-world label noise. The results show that the proposed method outperforms the existing methods in terms of worst-case error bound."
4405,SP:ae81e2a23bf6042bf8b04ba41b957bb625268b7e,"robustness EVALUATE-FOR RL agents. state observations USED-FOR strongest / optimal adversarial perturbations. strongest / optimal adversarial perturbations USED-FOR reinforcement learning ( RL ) agent. Existing works USED-FOR adversarial RL. heuristics - based methods USED-FOR Existing works. attacking method USED-FOR optimal attacks. designed function CONJUNCTION RL - based learner. RL - based learner CONJUNCTION designed function. algorithm COMPARE RL - based works. RL - based works COMPARE algorithm. PA - AD COMPARE RL - based works. RL - based works COMPARE PA - AD. PA - AD HYPONYM-OF algorithm. PA - AD COMPARE attacking methods. attacking methods COMPARE PA - AD. attacking methods USED-FOR Atari and MuJoCo environments. PA - AD USED-FOR Atari and MuJoCo environments. PA - AD USED-FOR adversarial training. empirical robustness EVALUATE-FOR PA - AD. OtherScientificTerm are optimal adversary, optimal attack, agent, large state space, policy perturbation direction, policy perturbation directions, large state spaces, and strong adversaries. Method is RL - based adversary. ","This paper proposes a new adversarial training method for reinforcement learning (RL) agents. The main idea is to learn a policy that is robust to the strongest/optimal adversarial perturbations, and then use this policy to train an RL-based adversary that can be used to attack the agent. The authors show that the proposed method outperforms existing works on Atari and MuJoCo environments.","This paper proposes a new adversarial training method for reinforcement learning (RL) agents. The main idea is to learn a policy that is robust to the strongest/optimal adversarial perturbations, and then use this policy to train an RL-based adversary that can be used to attack the agent. The authors show that the proposed method outperforms existing works on Atari and MuJoCo environments."
4421,SP:f7b7dfafb03090a2c940ba738234a6c80bd5ad0e,"cumulative reward FEATURE-OF monotonous policies. diversity CONJUNCTION novelty. novelty CONJUNCTION diversity. novelty FEATURE-OF policies. diversity FEATURE-OF policies. policy generation workflow USED-FOR diverse and well - performing policies. novelty metric USED-FOR novelty - seeking problem. behavioral novelty FEATURE-OF multi - objective optimization approaches. constrained optimization literature FEATURE-OF interior point method. interior point method USED-FOR Interior Policy Differentiation ( IPD ). Interior Policy Differentiation ( IPD ) HYPONYM-OF policy seeking algorithm. constrained optimization USED-FOR novelty - seeking problem. IPD COMPARE novelty - seeking methods. novelty - seeking methods COMPARE IPD. benchmark environments EVALUATE-FOR IPD. benchmark environments EVALUATE-FOR novelty - seeking methods. Task is problem - solving. Generic are problem, and metric. Method are reinforcement learning algorithms, and learning algorithms. OtherScientificTerm is novelty of generated policies. ","This paper studies the problem of novelty-seeking in reinforcement learning, where the goal is to generate diverse and well-performing policies. The authors propose a novel metric to measure the novelty of the generated policies, and propose an algorithm to solve the problem. The novelty is defined as the sum of diversity and diversity of the policies generated by the policy generation workflow, and the authors show that the proposed method outperforms existing methods in terms of novelty. ","This paper studies the problem of novelty-seeking in reinforcement learning, where the goal is to generate diverse and well-performing policies. The authors propose a novel metric to measure the novelty of the generated policies, and propose an algorithm to solve the problem. The novelty is defined as the sum of diversity and diversity of the policies generated by the policy generation workflow, and the authors show that the proposed method outperforms existing methods in terms of novelty. "
4437,SP:82b7860146bf3f772bdcd5b448d62136ff6d5177,"accuracy EVALUATE-FOR automatic speech recognition. quality of speech USED-FOR human perception. Reverberation FEATURE-OF audio reflecting off surfaces. audio modality USED-FOR reverberation. reverberation effects FEATURE-OF audio stream. real - world 3D scans of homes FEATURE-OF realistic acoustic renderings of speech. large - scale dataset EVALUATE-FOR task. realistic acoustic renderings of speech USED-FOR large - scale dataset. speech enhancement CONJUNCTION speech recognition. speech recognition CONJUNCTION speech enhancement. speech recognition CONJUNCTION speaker identification. speaker identification CONJUNCTION speech recognition. it COMPARE audio - only methods. audio - only methods COMPARE it. simulated and real imagery USED-FOR speech enhancement. approach USED-FOR speech enhancement. approach USED-FOR speech recognition. simulated and real imagery USED-FOR approach. OtherScientificTerm are audio - visual observations, visual environment, room geometry, speaker location, visual scene, and room acoustics. Method is end - to - end approach. ",This paper presents a method for generating realistic acoustic renderings of speech. The method is based on the observation that the reverberation effects of audio modality can be used to improve the quality of speech recognition and speech enhancement. The authors propose a method that combines the acoustic rendering of speech and speaker identification. The proposed method is evaluated on a large-scale dataset of 3D audio-visual observations and is shown to outperform audio-only methods.,This paper presents a method for generating realistic acoustic renderings of speech. The method is based on the observation that the reverberation effects of audio modality can be used to improve the quality of speech recognition and speech enhancement. The authors propose a method that combines the acoustic rendering of speech and speaker identification. The proposed method is evaluated on a large-scale dataset of 3D audio-visual observations and is shown to outperform audio-only methods.
4453,SP:7abf578ef0d3b67d87437bdd1cff129f72c102c6,"model USED-FOR extrapolation. methods USED-FOR extrapolation. position representation method USED-FOR extrapolation. Attention with Linear Biases ( ALiBi ) HYPONYM-OF position method. it USED-FOR query - key attention scores. positional embeddings USED-FOR word embeddings. perplexity EVALUATE-FOR sinusoidal position embedding model. method COMPARE sinusoidal position embedding model. sinusoidal position embedding model COMPARE method. perplexity EVALUATE-FOR method. it COMPARE position methods. position methods COMPARE it. WikiText-103 benchmark EVALUATE-FOR it. WikiText-103 benchmark EVALUATE-FOR position methods. Method are transformer model, and ALiBi. OtherScientificTerm are memory, and recency. ","This paper proposes a new position representation method, Attention with Linear Biases (ALiBi), for query-key attention scores. ALiBi is based on the transformer model, which is used to learn a position embedding for each query key. The proposed method is evaluated on the WikiText-103 benchmark and shows that it outperforms the baselines. ","This paper proposes a new position representation method, Attention with Linear Biases (ALiBi), for query-key attention scores. ALiBi is based on the transformer model, which is used to learn a position embedding for each query key. The proposed method is evaluated on the WikiText-103 benchmark and shows that it outperforms the baselines. "
4469,SP:45ba2709aca444f50a133d71f33be9d2c1f887e8,unconstrained max - min form FEATURE-OF multi - objective dynamic regret. multi - objective dynamic regret PART-OF Multi - Objective Online Convex Optimization. regret USED-FOR zero - order multi - objective bandit setting. it COMPARE regret. regret COMPARE it. vanilla min - norm solver CONJUNCTION L1 - regularized min - norm solver. L1 - regularized min - norm solver CONJUNCTION vanilla min - norm solver. variants USED-FOR composite gradient. Online Mirror Multiple Descent algorithm USED-FOR composite gradient. variants USED-FOR Online Mirror Multiple Descent algorithm. L1 - regularized min - norm solver USED-FOR composite gradient. vanilla min - norm solver USED-FOR composite gradient. regret bounds FEATURE-OF variants. lower bound FEATURE-OF L1 - regularized variant. Task is multi - objective online learning. Method is first - order gradient - based methods. Generic is algorithm. ,This paper studies the multi-objective online learning setting where the objective function is a bandit problem. The authors consider the case where the goal is to maximize the regret of the bandit algorithm in the zero-order bandit setting. They show that the regret bound of the online gradient-based algorithm is upper bounded by the max-min form and lower bound by the L1-regularized version of the algorithm. They also provide a lower bound for the online mirror multiple descent algorithm. ,This paper studies the multi-objective online learning setting where the objective function is a bandit problem. The authors consider the case where the goal is to maximize the regret of the bandit algorithm in the zero-order bandit setting. They show that the regret bound of the online gradient-based algorithm is upper bounded by the max-min form and lower bound by the L1-regularized version of the algorithm. They also provide a lower bound for the online mirror multiple descent algorithm. 
4485,SP:d07fd26d0cb245e1fd1343472dd3c8300c39752a,"intelligence USED-FOR real - world problems. Learning continually PART-OF intelligence. simple scenarios CONJUNCTION low - dimensional benchmarks. low - dimensional benchmarks CONJUNCTION simple scenarios. generative models USED-FOR replay patterns. simplified assumptions USED-FOR it. generative models USED-FOR Generative replay. simple scenarios HYPONYM-OF simplified assumptions. low - dimensional benchmarks HYPONYM-OF simplified assumptions. OtherScientificTerm are catastrophic forgetting, and learning experiences. Method are continual learning, replay approach, and generative replay approaches. Metric is classification accuracy. Generic are they, and approach. Material are ImageNet-1000, and high - dimensional data. ","This paper proposes a generative replay approach for continual learning. The key idea is to use generative models to predict the replay patterns of a given task, and then use the learned patterns to learn a new task. The authors show that the proposed method outperforms baselines on ImageNet-1000 and ImageNet100. The paper also shows that the method can be applied to low-dimensional and high-dimensional datasets.","This paper proposes a generative replay approach for continual learning. The key idea is to use generative models to predict the replay patterns of a given task, and then use the learned patterns to learn a new task. The authors show that the proposed method outperforms baselines on ImageNet-1000 and ImageNet100. The paper also shows that the method can be applied to low-dimensional and high-dimensional datasets."
4501,SP:ec18e1450dd918b1ca95e301bc9262e072d77b52,"modularity maximization CONJUNCTION NCut minimization. NCut minimization CONJUNCTION modularity maximization. graph partitioning ( GP ) FEATURE-OF NP - hard combinatorial optimization problems. NP - hard combinatorial optimization problems USED-FOR network systems. NCut minimization HYPONYM-OF NP - hard combinatorial optimization problems. NCut minimization HYPONYM-OF graph partitioning ( GP ). modularity maximization HYPONYM-OF NP - hard combinatorial optimization problems. modularity maximization HYPONYM-OF graph partitioning ( GP ). machine learning techniques USED-FOR Existing methods. heuristic strategies USED-FOR GP methods. inductive graph partitioning ( IGP ) framework USED-FOR NP - hard challenge. transductive GP methods COMPARE inductive graph partitioning ( IGP ) framework. inductive graph partitioning ( IGP ) framework COMPARE transductive GP methods. inductive graph partitioning ( IGP ) framework USED-FOR graphs. dual graph neural network USED-FOR IGP. historical graph snapshots USED-FOR dual graph neural network. model USED-FOR graphs. model USED-FOR online GP. quality CONJUNCTION efficiency. efficiency CONJUNCTION quality. graphs USED-FOR online GP. IGP USED-FOR online GP. IGP HYPONYM-OF framework. graphs USED-FOR online GP. benchmarks EVALUATE-FOR IGP. efficiency EVALUATE-FOR state - of - the - art baselines. IGP COMPARE state - of - the - art baselines. state - of - the - art baselines COMPARE IGP. benchmarks EVALUATE-FOR state - of - the - art baselines. efficiency EVALUATE-FOR IGP. OtherScientificTerm are NP - hardness, quality degradation, and optimization. Method is GP. Metric is complexity. Generic is system. ",This paper proposes a novel approach to the problem of graph partitioning (GP) for combinatorial optimization problems. The authors propose a dual graph neural network (PGN) model to learn a graph model for online GP. The proposed method is evaluated on a number of benchmark datasets and shows competitive performance compared to existing methods. ,This paper proposes a novel approach to the problem of graph partitioning (GP) for combinatorial optimization problems. The authors propose a dual graph neural network (PGN) model to learn a graph model for online GP. The proposed method is evaluated on a number of benchmark datasets and shows competitive performance compared to existing methods. 
4517,SP:ad28c185efd966eea1f44a6ff474900812b4705a,Multiresolution Equivariant Graph Variational Autoencoders ( MGVAE ) HYPONYM-OF hierarchical generative model. hierarchical generative model USED-FOR graphs. multiresolution and equivariant manner USED-FOR graphs. higher order message passing USED-FOR graph. MGVAE USED-FOR graph. MGVAE USED-FOR resolution level. higher order message passing USED-FOR resolution level. higher order message passing USED-FOR MGVAE. MGVAE USED-FOR hierarchical generative model. hierarchical generative model USED-FOR hierarchy of coarsened graphs. node ordering FEATURE-OF framework. general graph generation CONJUNCTION molecular generation. molecular generation CONJUNCTION general graph generation. unsupervised molecular representation learning USED-FOR molecular properties. molecular generation CONJUNCTION unsupervised molecular representation learning. unsupervised molecular representation learning CONJUNCTION molecular generation. link prediction CONJUNCTION graph - based image generation. graph - based image generation CONJUNCTION link prediction. MGVAE COMPARE generative tasks. generative tasks COMPARE MGVAE. unsupervised molecular representation learning CONJUNCTION link prediction. link prediction CONJUNCTION unsupervised molecular representation learning. citation graphs USED-FOR link prediction. unsupervised molecular representation learning HYPONYM-OF generative tasks. general graph generation HYPONYM-OF generative tasks. graph - based image generation HYPONYM-OF generative tasks. link prediction HYPONYM-OF generative tasks. molecular generation HYPONYM-OF generative tasks. Generic is it. OtherScientificTerm is hierarchy of latent distributions. ,"This paper proposes a hierarchical generative model for graph generation. The proposed model is based on multiresolution and equivariant graph variational autoencoders (MGVAE). MGVAE learns a hierarchy of coarsened graphs, where each node in the hierarchy is represented by a latent distribution. The latent distribution is then passed through a higher order message passing mechanism. The authors show that the proposed model achieves state-of-the-art performance on graph-based image generation and link prediction tasks.","This paper proposes a hierarchical generative model for graph generation. The proposed model is based on multiresolution and equivariant graph variational autoencoders (MGVAE). MGVAE learns a hierarchy of coarsened graphs, where each node in the hierarchy is represented by a latent distribution. The latent distribution is then passed through a higher order message passing mechanism. The authors show that the proposed model achieves state-of-the-art performance on graph-based image generation and link prediction tasks."
4533,SP:29e2e1daa6a32ef71ad225bd2fc27e33dece86c5,"Nonlinear ICA HYPONYM-OF machine learning. framework USED-FOR nonlinear ICA. volume - preserving transformation FEATURE-OF mixing function. artificial data CONJUNCTION synthesized images. synthesized images CONJUNCTION artificial data. synthesized images EVALUATE-FOR theory. volume - preserving flow - based models USED-FOR framework. artificial data EVALUATE-FOR theory. framework USED-FOR interpretable features. real - world images EVALUATE-FOR framework. OtherScientificTerm are independent components ( sources ), and temporal structure. Generic are sources, and methods. ","This paper studies the problem of nonlinear ICA, which is an important problem in machine learning. The authors propose a novel framework for learning the mixing function of a mixing function, which they call volume-preserving flow-based models. The mixing function is defined as a volume preserving transformation that preserves the temporal structure of the source and target components. The proposed framework is tested on synthetic and real-world datasets. ","This paper studies the problem of nonlinear ICA, which is an important problem in machine learning. The authors propose a novel framework for learning the mixing function of a mixing function, which they call volume-preserving flow-based models. The mixing function is defined as a volume preserving transformation that preserves the temporal structure of the source and target components. The proposed framework is tested on synthetic and real-world datasets. "
4549,SP:288ce587a277299765bdd4cea75a8c23e12de2b0,"Graph convolutional networks USED-FOR representation learning of networked data. single message passing strategy USED-FOR they. sharing scheme USED-FOR filters. sharing scheme USED-FOR spectral graph convolutional operators. sharing scheme USED-FOR filters. BankGCN HYPONYM-OF graph convolution operator. sharing scheme USED-FOR spectral methods. BankGCN USED-FOR multi - channel signals. adaptive filters USED-FOR BankGCN. graphs USED-FOR multi - channel signals. filters PART-OF filter bank. filters PART-OF subspaces. frequency response FEATURE-OF filters. filter bank CONJUNCTION signal decomposition. signal decomposition CONJUNCTION filter bank. filter bank USED-FOR spectral characteristics. signal decomposition USED-FOR spectral characteristics. spectral characteristics FEATURE-OF graph data. benchmark graph datasets EVALUATE-FOR BankGCN. Method are message passing graph convolutional networks ( MPGCNs ), and MPGCNs. OtherScientificTerm are low - frequency information, graph features, and single ‘ low - pass ’ features. Task is overfitting problems. Generic is compact architecture. ",This paper proposes a new message passing graph convolutional network (MPGCN) based on the sharing scheme. The authors show that the proposed BankGCN is able to handle multi-channel signals with low-frequency information. The proposed method is evaluated on several benchmark graph datasets. ,This paper proposes a new message passing graph convolutional network (MPGCN) based on the sharing scheme. The authors show that the proposed BankGCN is able to handle multi-channel signals with low-frequency information. The proposed method is evaluated on several benchmark graph datasets. 
4565,SP:3abcd0700eaf11d964c280d996a1dd4f34280b1c,"pretrain - finetune paradigm USED-FOR deep learning. model USED-FOR downstream tasks. self - supervised pre - training COMPARE supervised pre - training. supervised pre - training COMPARE self - supervised pre - training. transferability EVALUATE-FOR self - supervised pre - training. supervised methods USED-FOR pre - training stage. supervised pretraining model USED-FOR downstream tasks. transferability EVALUATE-FOR supervised pre - training methods. It USED-FOR overfitting upstream tasks. method USED-FOR large datasets. state - of - the - art methods USED-FOR supervised and self - supervised pre - training. LOOK COMPARE state - of - the - art methods. state - of - the - art methods COMPARE LOOK. LOOK USED-FOR supervised and self - supervised pre - training. multiple downstream tasks EVALUATE-FOR LOOK. Material is ImageNet. OtherScientificTerm are negligence of valuable intra - class semantic difference, visual contents, multi - mode distribution, and intra - class difference. Generic is methods. Task is overfit of upstream tasks. Method is supervised pre - training method. ","This paper proposes a method for self-supervised pre-training for downstream tasks. The proposed method is based on the pretraining-finetune paradigm, where a pre-trained model is trained on a set of downstream tasks and then fine-tuned for the downstream tasks using a supervised pretraining model. The method is evaluated on ImageNet, CIFAR-10, and ImageNet-100. The results show that the proposed method outperforms the state-of-the-art supervised pre-train methods in terms of transferability and transferability to new tasks.","This paper proposes a method for self-supervised pre-training for downstream tasks. The proposed method is based on the pretraining-finetune paradigm, where a pre-trained model is trained on a set of downstream tasks and then fine-tuned for the downstream tasks using a supervised pretraining model. The method is evaluated on ImageNet, CIFAR-10, and ImageNet-100. The results show that the proposed method outperforms the state-of-the-art supervised pre-train methods in terms of transferability and transferability to new tasks."
4581,SP:2b3916ba24094c286117126e11032820f8c7c50a,"wrinkling of cheeks CONJUNCTION formation of dimples. formation of dimples CONJUNCTION wrinkling of cheeks. Morphable Models ( 3DMMs ) USED-FOR fine details. PCA - based representations USED-FOR fine details. FaceDet3D HYPONYM-OF method. single image geometric facial details USED-FOR method. vertex displacement map USED-FOR facial details. method USED-FOR facial geometric details. FDS USED-FOR detailed geometry. hallucinated details PART-OF smooth proxy geometry. facial details FEATURE-OF detailed geometry. Neural Rendering USED-FOR detailed geometry. zoom - in FEATURE-OF predicted facial details. Predicted Facial Detail CONJUNCTION Render of Predicted Facial Detail. Render of Predicted Facial Detail CONJUNCTION Predicted Facial Detail. OtherScientificTerm are Facial Expressions, 3D face geometry, smile, edit expressions, and Proxy Shading. Method is Neural Renderer. Task is Facial detail hallucination and rendering. ","This paper proposes FaceDet3D, a new method for 3D face rendering. The proposed method is based on 3D Morphable Models (3DMMs) and uses PCA-based representations to capture fine details of facial details. The method is evaluated on a number of datasets and shows promising results. ","This paper proposes FaceDet3D, a new method for 3D face rendering. The proposed method is based on 3D Morphable Models (3DMMs) and uses PCA-based representations to capture fine details of facial details. The method is evaluated on a number of datasets and shows promising results. "
4597,SP:c7c50c44fdafb15b962e04d713cac309e57bc54b,"neural representations USED-FOR NLP models. neural representations CONJUNCTION linguistic factors. linguistic factors CONJUNCTION neural representations. syntactic roles PART-OF factors. latent variables CONJUNCTION realizations of syntactic roles. realizations of syntactic roles CONJUNCTION latent variables. attention USED-FOR deep probabilistic generative model. Attention - Driven Variational Autoencoder ( ADVAE ) HYPONYM-OF probabilistic model. attention USED-FOR ADVAEs. evaluation protocol EVALUATE-FOR disentanglement. disentanglement FEATURE-OF realizations of syntactic roles. evaluation protocol USED-FOR realizations of syntactic roles. attention maxima CONJUNCTION latent variable perturbations. latent variable perturbations CONJUNCTION attention maxima. latent variable perturbations USED-FOR decoder. attention maxima USED-FOR encoder. latent variable perturbations USED-FOR protocol. attention maxima USED-FOR protocol. ADVAE USED-FOR syntactic roles. sequence VAEs CONJUNCTION Transformer VAEs. Transformer VAEs CONJUNCTION sequence VAEs. ADVAE COMPARE Transformer VAEs. Transformer VAEs COMPARE ADVAE. ADVAE COMPARE sequence VAEs. sequence VAEs COMPARE ADVAE. raw English text PART-OF SNLI dataset. latent variables USED-FOR realizations of syntactic roles. Generic is they. OtherScientificTerm are decomposition of predicative structures, and supervision. Method is Transformer - based machine translation models. Task are disentanglement of syntactic roles, and unsupervised controllable content generation. ","This paper proposes an attention-driven variational autoencoder (ADVAE) model for unsupervised controllable content generation. The proposed model is based on Transformer-based machine translation models. The authors propose an evaluation protocol for disentanglement of syntactic roles, and show that ADVAE outperforms the state-of-the-art Transformer VAEs on the SNLI dataset. ","This paper proposes an attention-driven variational autoencoder (ADVAE) model for unsupervised controllable content generation. The proposed model is based on Transformer-based machine translation models. The authors propose an evaluation protocol for disentanglement of syntactic roles, and show that ADVAE outperforms the state-of-the-art Transformer VAEs on the SNLI dataset. "
4613,SP:57ace99a05a76b7d7427619cb6881fc87d74160f,"Discovering successful coordinated behaviors PART-OF Multi - Agent Reinforcement Learning ( MARL ). joint action space USED-FOR it. mechanism USED-FOR sufficient exploration and coordination. framework USED-FOR coordination protocols. sparse rewards CONJUNCTION partial observability. partial observability CONJUNCTION sparse rewards. StarCraft Multi - Agent Challenge HYPONYM-OF exploration scheme. exploration scheme USED-FOR complex cooperative strategies. methods COMPARE baselines. baselines COMPARE methods. Method is intrinsic motivation functions. OtherScientificTerm are agents ’ interactions, and counterfactual rollouts. Generic is approach. ","This paper proposes a new exploration method for multi-agent reinforcement learning (MARL) based on counterfactual rollouts. The proposed method is motivated by the observation that in MARL, there is a lack of exploration and coordination in the joint action space. To address this issue, the authors propose a novel exploration scheme based on the StarCraft Multi-Agent Challenge (MAMC) framework. The authors show that the proposed method outperforms the baselines in terms of exploration efficiency, partial observability, and sparse rewards. ","This paper proposes a new exploration method for multi-agent reinforcement learning (MARL) based on counterfactual rollouts. The proposed method is motivated by the observation that in MARL, there is a lack of exploration and coordination in the joint action space. To address this issue, the authors propose a novel exploration scheme based on the StarCraft Multi-Agent Challenge (MAMC) framework. The authors show that the proposed method outperforms the baselines in terms of exploration efficiency, partial observability, and sparse rewards. "
4629,SP:66784b2f0f08680057670dfea49a4ae88f7a2b38,"fine - tuning approaches COMPARE editing algorithms. editing algorithms COMPARE fine - tuning approaches. Gradient Decomposition ( MEND ) USED-FOR Model Editor Networks. Model Editor Networks USED-FOR post - hoc editing. MEND USED-FOR gradient. low - rank decomposition of the gradient USED-FOR transformation. low - rank decomposition of the gradient USED-FOR gradient. fine - tuning USED-FOR gradient. low - rank decomposition of the gradient USED-FOR MEND. MEND USED-FOR edits. edits USED-FOR pre - trained model. GPU USED-FOR MEND. MEND USED-FOR model editing. approach USED-FOR model editing. Method are large pre - trained models, large neural networks, and small auxiliary editing networks. Generic are models, and model. OtherScientificTerm are targeted edits, and pre - trained model ’s behavior. ","This paper proposes a method for post-hoc editing of pre-trained models. The proposed method, called Gradient Decomposition (MEND), is based on the idea of low-rank decomposition of the gradient of the model. The authors show that the proposed method can be applied to a large pre-training model and small auxiliary editing networks. Experiments show that MEND is able to improve the performance of model editing on a variety of datasets. ","This paper proposes a method for post-hoc editing of pre-trained models. The proposed method, called Gradient Decomposition (MEND), is based on the idea of low-rank decomposition of the gradient of the model. The authors show that the proposed method can be applied to a large pre-training model and small auxiliary editing networks. Experiments show that MEND is able to improve the performance of model editing on a variety of datasets. "
4645,SP:ea9a6880083555a89f5ed22dca21ba2dc109c1a2,learning abilities CONJUNCTION physical and structural knowledge. physical and structural knowledge CONJUNCTION learning abilities. artificial neural networks CONJUNCTION physical and structural knowledge. physical and structural knowledge CONJUNCTION artificial neural networks. learning abilities FEATURE-OF artificial neural networks. FINN USED-FOR constituents of partial differential equations ( PDEs ). numerical simulation USED-FOR physical and structural knowledge. artificial neural networks USED-FOR FINN. learning abilities PART-OF FINN. oneand two - dimensional PDEs EVALUATE-FOR FINN. modeling accuracy CONJUNCTION outof - distribution generalization ability. outof - distribution generalization ability CONJUNCTION modeling accuracy. initial and boundary conditions FEATURE-OF outof - distribution generalization ability. modeling accuracy EVALUATE-FOR FINN. outof - distribution generalization ability EVALUATE-FOR FINN. diffusion - sorption HYPONYM-OF oneand two - dimensional PDEs. FINN COMPARE physics - aware models. physics - aware models COMPARE FINN. FINN COMPARE machine learning. machine learning COMPARE FINN. machine learning CONJUNCTION physics - aware models. physics - aware models CONJUNCTION machine learning. FINN COMPARE calibrated physical model. calibrated physical model COMPARE FINN. calibrated physical model USED-FOR sparse real - world data. sparse real - world data USED-FOR FINN. generalization abilities EVALUATE-FOR FINN. diffusionsorption scenario FEATURE-OF sparse real - world data. Task is spatiotemporal advection - diffusion processes. OtherScientificTerm is unknown retardation factor. ,This paper proposes a new method for learning the constituents of partial differential equations (PDEs) based on numerical simulation. The method is based on a neural network that is trained to predict the initial and boundary conditions of PDEs. The authors show that the proposed method outperforms state-of-the-art physics-aware models in the diffusionsorption and diffusion-sorption settings. The paper also shows that the method is able to generalize well to out-of distribution settings.,This paper proposes a new method for learning the constituents of partial differential equations (PDEs) based on numerical simulation. The method is based on a neural network that is trained to predict the initial and boundary conditions of PDEs. The authors show that the proposed method outperforms state-of-the-art physics-aware models in the diffusionsorption and diffusion-sorption settings. The paper also shows that the method is able to generalize well to out-of distribution settings.
4661,SP:d369e2144544908fbcaaa53aab9555d71080ced8,"unsupervised machine learning ( ML ) models CONJUNCTION representations of computer programs. representations of computer programs CONJUNCTION unsupervised machine learning ( ML ) models. representations of computer programs USED-FOR representations of computer programs. unsupervised machine learning ( ML ) models USED-FOR representations of computer programs. abstract syntax tree ( AST)-related information CONJUNCTION runtime information. runtime information CONJUNCTION abstract syntax tree ( AST)-related information. brain representations USED-FOR static and dynamic properties of code. runtime information HYPONYM-OF static and dynamic properties of code. abstract syntax tree ( AST)-related information HYPONYM-OF static and dynamic properties of code. brain representations USED-FOR representations. representations USED-FOR ML models. Material is Python code. Metric is complexity. Method are Multiple Demand system, and machine learned representations of code. OtherScientificTerm is code. ",This paper studies the problem of learning machine learning representations of computer programs. The authors propose a method to learn representations of code that can capture both static and dynamic properties of code. The proposed method is based on the Multiple Demand system (MDS). The authors show that the proposed method can learn representations that capture both runtime information and abstract syntax tree (AST)-related information. They also show that their method can be applied to unsupervised machine learning models.,This paper studies the problem of learning machine learning representations of computer programs. The authors propose a method to learn representations of code that can capture both static and dynamic properties of code. The proposed method is based on the Multiple Demand system (MDS). The authors show that the proposed method can learn representations that capture both runtime information and abstract syntax tree (AST)-related information. They also show that their method can be applied to unsupervised machine learning models.
4677,SP:ab5a8934846776a7be7d0ac1973d41fd6aae89fc,phoneme decoder CONJUNCTION mel - spectrogram synthesizer. mel - spectrogram synthesizer CONJUNCTION phoneme decoder. speech encoder CONJUNCTION phoneme decoder. phoneme decoder CONJUNCTION speech encoder. mel - spectrogram synthesizer CONJUNCTION attention module. attention module CONJUNCTION mel - spectrogram synthesizer. attention module PART-OF Translatotron 2. mel - spectrogram synthesizer PART-OF Translatotron 2. phoneme decoder PART-OF Translatotron 2. speech encoder PART-OF Translatotron 2. Translatotron 2 COMPARE Translatotron. Translatotron COMPARE Translatotron 2. babbling CONJUNCTION long pause. long pause CONJUNCTION babbling. translation quality CONJUNCTION predicted speech naturalness. predicted speech naturalness CONJUNCTION translation quality. robustness EVALUATE-FOR predicted speech. long pause HYPONYM-OF over - generation. babbling HYPONYM-OF over - generation. predicted speech naturalness EVALUATE-FOR Translatotron 2. robustness EVALUATE-FOR Translatotron 2. translation quality EVALUATE-FOR Translatotron. translation quality EVALUATE-FOR Translatotron 2. model USED-FOR production deployment. Translatotron COMPARE it. it COMPARE Translatotron. method CONJUNCTION concatenation - based data augmentation. concatenation - based data augmentation CONJUNCTION method. Material is translated speech. Method is Translatotron 2 model. ,"This paper presents Translatotron 2, a new model for speech synthesis and decoder. The model consists of a speech encoder, a mel-spectrogram synthesizer, an attention module, and a phoneme decoder, which can be combined to produce a speech synthesis model. The proposed model is evaluated on a variety of speech synthesis tasks, including babbling, long pause, and babbling with long pauses. The results show that the proposed model outperforms the state-of-the-art on all tasks.","This paper presents Translatotron 2, a new model for speech synthesis and decoder. The model consists of a speech encoder, a mel-spectrogram synthesizer, an attention module, and a phoneme decoder, which can be combined to produce a speech synthesis model. The proposed model is evaluated on a variety of speech synthesis tasks, including babbling, long pause, and babbling with long pauses. The results show that the proposed model outperforms the state-of-the-art on all tasks."
4693,SP:296102e60b842923c94f579f524fa1147328ee4b,"attribute - based representations USED-FOR concept learning. zeroshot learning HYPONYM-OF attribute - based learning paradigms. supervised learning COMPARE selfsupervised pre - training. selfsupervised pre - training COMPARE supervised learning. predictability of test attributes USED-FOR model. predictability of test attributes USED-FOR generalization ability. generalization ability EVALUATE-FOR model. OtherScientificTerm are Semantic concepts, mappings, and random splits of the attribute space. Task is rapid learning of attributes. Method is few - shot learning of semantic classes. ","This paper proposes a method for few-shot learning of attribute-based concepts. The method is based on zeroshot learning, where a set of attributes are randomly split into a few classes, and the goal is to learn a model that can generalize well to new attributes. The authors show that the proposed method is able to achieve state-of-the-art performance on a variety of tasks. The paper also shows that the method can be used to improve the performance of self-supervised pre-training.","This paper proposes a method for few-shot learning of attribute-based concepts. The method is based on zeroshot learning, where a set of attributes are randomly split into a few classes, and the goal is to learn a model that can generalize well to new attributes. The authors show that the proposed method is able to achieve state-of-the-art performance on a variety of tasks. The paper also shows that the method can be used to improve the performance of self-supervised pre-training."
4709,SP:0f69e20b9f97439d19e7a93144c8d877cedcd714,relative entropy gradient sampler ( REGS ) USED-FOR sampling from unnormalized distributions. particle method USED-FOR nonlinear transforms. REGS HYPONYM-OF particle method. gradient flow USED-FOR path of probability distributions. path of probability distributions USED-FOR reference distribution. density of evolving particles CONJUNCTION unnormalized target density. unnormalized target density CONJUNCTION density of evolving particles. density ratios FEATURE-OF density of evolving particles. density ratios CONJUNCTION unnormalized target density. unnormalized target density CONJUNCTION density ratios. density ratios FEATURE-OF velocity fields. velocity fields FEATURE-OF ODE system. ODE system USED-FOR It. particle evolution USED-FOR ODE system. nonparametric approach USED-FOR logarithmic density ratio. neural networks USED-FOR nonparametric approach. REGS COMPARE sampling methods. sampling methods COMPARE REGS. multimodal 1D and 2D mixture distributions CONJUNCTION Bayesian logistic regression. Bayesian logistic regression CONJUNCTION multimodal 1D and 2D mixture distributions. real datasets EVALUATE-FOR Bayesian logistic regression. Method is Wasserstein gradient flow of relative entropy. ,"This paper proposes a particle method for sampling from unnormalized distributions. The method is based on the relative entropy gradient sampler (REGS), which is a non-parametric approach to sampling from the path of probability distributions of the reference distribution. The authors show that REGS can be used to sample from the trajectory of an ODE system, and that it can be combined with Bayesian logistic regression (BLEU) and multi-modal 1D and 2D mixture distributions. Experiments are conducted on real-world datasets.","This paper proposes a particle method for sampling from unnormalized distributions. The method is based on the relative entropy gradient sampler (REGS), which is a non-parametric approach to sampling from the path of probability distributions of the reference distribution. The authors show that REGS can be used to sample from the trajectory of an ODE system, and that it can be combined with Bayesian logistic regression (BLEU) and multi-modal 1D and 2D mixture distributions. Experiments are conducted on real-world datasets."
4725,SP:a69b894166482ccd7a3a9db53e0f5a7e6ecff89a,quantum machine learning techniques USED-FOR classical image classification. quantum neural network USED-FOR inference. qubits USED-FOR encoding schemes. quantum systems USED-FOR framework. encoding mechanism USED-FOR approach. accuracy EVALUATE-FOR classical neural networks. framework COMPARE classical neural networks. classical neural networks COMPARE framework. personal laptop FEATURE-OF MNIST dataset. accuracy EVALUATE-FOR framework. quantum computers CONJUNCTION classical simulation. classical simulation CONJUNCTION quantum computers. work USED-FOR quantum machine learning and classification. classical datasets USED-FOR quantum machine learning and classification. OtherScientificTerm is quantum states. Generic is technique. ,"This paper proposes a framework for quantum machine learning and classification based on quantum neural networks. The main idea is to use a quantum neural network to encode information from quantum systems into a classical neural network, which is then used for inference. The authors show that the proposed method is able to achieve state-of-the-art results on MNIST and CIFAR-10 datasets. ","This paper proposes a framework for quantum machine learning and classification based on quantum neural networks. The main idea is to use a quantum neural network to encode information from quantum systems into a classical neural network, which is then used for inference. The authors show that the proposed method is able to achieve state-of-the-art results on MNIST and CIFAR-10 datasets. "
4741,SP:acf3825e96d1b7c66cdc339fc5de77330b8e8e90,"data privacy FEATURE-OF face recognition. framework USED-FOR federated learning face recognition. communicating auxiliary and privacy - agnostic information USED-FOR framework. communicating auxiliary and privacy - agnostic information USED-FOR federated learning face recognition. Differentially Private Local Clustering ( DPLC ) mechanism USED-FOR sanitized clusters. local class centers USED-FOR sanitized clusters. consensus - aware recognition loss USED-FOR global consensuses. IJB - B CONJUNCTION IJB - C. IJB - C CONJUNCTION IJB - B. large - scale dataset EVALUATE-FOR method. Method are federated learning ( FL ) paradigm, FL methods, and PrivacyFace. Generic is task. Task is recognition. OtherScientificTerm are privacy leakage, privacy - utility paradox, discriminative features, and lightweight overhead. ","This paper proposes a new framework for federated learning face recognition, PrivacyFace. The proposed framework is based on the Differentially Private Local Clustering (DPLC) mechanism. The DPLC mechanism is used to sanitize the local class centers in the federated setting. The paper also proposes a consensus-aware recognition loss to mitigate the privacy-utility paradox. Experiments show that the proposed framework can achieve state-of-the-art performance on a large-scale dataset. ","This paper proposes a new framework for federated learning face recognition, PrivacyFace. The proposed framework is based on the Differentially Private Local Clustering (DPLC) mechanism. The DPLC mechanism is used to sanitize the local class centers in the federated setting. The paper also proposes a consensus-aware recognition loss to mitigate the privacy-utility paradox. Experiments show that the proposed framework can achieve state-of-the-art performance on a large-scale dataset. "
4757,SP:408d9e1299ee05b89855df9742b608626692b40d,Transfer - learning methods USED-FOR data - scarce target domain. data - rich source domain USED-FOR model. model USED-FOR Transfer - learning methods. strategy COMPARE method. method COMPARE strategy. fine - tuning USED-FOR model. linear probing USED-FOR intermediate layers. classification head USED-FOR target - domain. features USED-FOR classification head. Visual Task Adaptation Benchmark ( VTAB ) EVALUATE-FOR Head2Toe. Head2Toe COMPARE fine - tuning. fine - tuning COMPARE Head2Toe. Visual Task Adaptation Benchmark ( VTAB ) EVALUATE-FOR Head2Toe. Head2Toe COMPARE Head2Toe. Head2Toe COMPARE Head2Toe. fine - tuning USED-FOR Head2Toe. Head2Toe USED-FOR out - of - distribution transfer. Generic is source model. OtherScientificTerm is pretrained layers. ,This paper proposes a new method for out-of-distribution transfer. The method is based on the idea of linear probing to identify intermediate layers of the source model. The proposed method is evaluated on the Visual Task Adaptation Benchmark (VTAB) and shows that the proposed method outperforms fine-tuning methods. ,This paper proposes a new method for out-of-distribution transfer. The method is based on the idea of linear probing to identify intermediate layers of the source model. The proposed method is evaluated on the Visual Task Adaptation Benchmark (VTAB) and shows that the proposed method outperforms fine-tuning methods. 
4773,SP:d6f11fb32851f97af287f962f83220d27a8bc76a,dynamics models USED-FOR model - based planning. Dynamics models USED-FOR image - based games. fully observable states FEATURE-OF image - based games. models USED-FOR Text - Based Games ( TBGs ). noisy text observations FEATURE-OF partially observable states. planning algorithms USED-FOR decision - making problems. planning algorithms USED-FOR text domains. text domains USED-FOR decision - making problems. OOTD USED-FOR memory graph. OOTD model USED-FOR beliefs of object states. OOTD model USED-FOR dynamics. independently parameterized transition layers USED-FOR beliefs of object states. variational objectives USED-FOR stochasticity of predicted dynamics. object - supervised and self - supervised settings USED-FOR variational objectives. OOTD - based planner COMPARE model - free baselines. model - free baselines COMPARE OOTD - based planner. sample efficiency CONJUNCTION running scores. running scores CONJUNCTION sample efficiency. running scores EVALUATE-FOR OOTD - based planner. sample efficiency EVALUATE-FOR OOTD - based planner. running scores EVALUATE-FOR model - free baselines. sample efficiency EVALUATE-FOR model - free baselines. OtherScientificTerm is object - irrelevant information. ,"This paper proposes an OOTD-based planning method for text-based games (TBGs). The proposed method is based on the idea of object-to-out-of-distribution (OOTD) model, which is used to model the dynamics of an object in a memory graph. The authors show that the proposed method outperforms model-free baselines in terms of both sample efficiency and running scores. The method is evaluated on object-supervised and self-supervision settings.","This paper proposes an OOTD-based planning method for text-based games (TBGs). The proposed method is based on the idea of object-to-out-of-distribution (OOTD) model, which is used to model the dynamics of an object in a memory graph. The authors show that the proposed method outperforms model-free baselines in terms of both sample efficiency and running scores. The method is evaluated on object-supervised and self-supervision settings."
4789,SP:3e5ac4add9ab8a986fdf027b6e6a7d59698b3031,"cost - sensitive loss FEATURE-OF label taxonomy. tractable method USED-FOR hierarchical learning problem. hierarchical cost - sensitive loss CONJUNCTION layer - wise abstaining losses. layer - wise abstaining losses CONJUNCTION hierarchical cost - sensitive loss. bijective mapping USED-FOR hierarchical cost - sensitive loss. symmetry assumptions USED-FOR bijective mapping. distributionally robust learning framework USED-FOR learningto - abstain problems. large - scale bird dataset CONJUNCTION cell classification problems. cell classification problems CONJUNCTION large - scale bird dataset. LAM COMPARE methods. methods COMPARE LAM. high accuracy regions FEATURE-OF hierarchical cost - sensitive loss. hierarchical cost - sensitive loss EVALUATE-FOR LAM. high accuracy regions EVALUATE-FOR LAM. perclass loss - adjustment heuristic USED-FOR performance profile. perclass loss - adjustment heuristic USED-FOR LAM. cost design USED-FOR user requirements. cost design USED-FOR optimizable cost functions. user requirements CONJUNCTION optimizable cost functions. optimizable cost functions CONJUNCTION user requirements. Task is cost - sensitive hierarchical classification. OtherScientificTerm are hierarchy, cost - sensitive hierarchical loss, non - convexity, and taxonomy. Metric is accuracy. ",This paper proposes a tractable method for learning to abstain loss for hierarchical learning. The proposed method is based on the bijective mapping of the hierarchical cost-sensitive loss and the layer-wise abstaining loss. The authors show that the proposed method outperforms existing methods on a number of classification tasks. ,This paper proposes a tractable method for learning to abstain loss for hierarchical learning. The proposed method is based on the bijective mapping of the hierarchical cost-sensitive loss and the layer-wise abstaining loss. The authors show that the proposed method outperforms existing methods on a number of classification tasks. 
4805,SP:7997a1b59ef2fc0127c3fff02d191f5d655168f8,"spatial location CONJUNCTION semantic identity label. semantic identity label CONJUNCTION spatial location. estimating sound sources ’ temporal location CONJUNCTION spatial location. spatial location CONJUNCTION estimating sound sources ’ temporal location. multi - channel sound raw waveforms USED-FOR semantic identity label. multi - channel sound raw waveforms USED-FOR estimating sound sources ’ temporal location. single - scale filter bank USED-FOR sound waveforms. STFT CONJUNCTION LogMel. LogMel CONJUNCTION STFT. they USED-FOR hand - engineered features. limited time - frequency resolution capability FEATURE-OF sound waveforms. parameter tuning USED-FOR they. STFT HYPONYM-OF hand - engineered features. LogMel HYPONYM-OF hand - engineered features. parameter tuning USED-FOR hand - engineered features. frequency resolution FEATURE-OF synperiodic filter. time - frequency resolution trade - off EVALUATE-FOR synperiodic filter. synperiodic filter bank USED-FOR multi - scale perception. synperiodic filter bank USED-FOR downsampled waveform. synperiodic filter bank group USED-FOR dynamic multi - scale time - frequency representation. multi - scale perception USED-FOR synperiodic filter bank group. time and frequency domain advantage FEATURE-OF multi - scale perception. semantic identity label CONJUNCTION spatial location representation. spatial location representation CONJUNCTION semantic identity label. Transformer - like backbone USED-FOR semantic identity label. Transformer - like backbone USED-FOR spatial location representation. parallel soft - stitched branches PART-OF Transformer - like backbone. Transformer - like backbone PART-OF synperiodic filter bank group front - end. direction of arrival estimation task CONJUNCTION physical location estimation task. physical location estimation task CONJUNCTION direction of arrival estimation task. direction of arrival estimation task EVALUATE-FOR framework. physical location estimation task EVALUATE-FOR framework. OtherScientificTerm are complex waveform mixture, periodicity term, and temporal length. Generic are representation, and Existing methods. Method are parameterized synperiodic filter banks, and synperiodic filter banks. Material is raw waveform. ","This paper proposes a new method for learning multi-scale time-frequency representation of sound waveforms. The proposed method is based on a transformer-based architecture, which is able to capture the temporal and spatial information of the waveform. The authors show that the proposed method outperforms the state-of-the-art in both time and frequency domain. They also show that their method can be applied to the task of spatial location estimation. ","This paper proposes a new method for learning multi-scale time-frequency representation of sound waveforms. The proposed method is based on a transformer-based architecture, which is able to capture the temporal and spatial information of the waveform. The authors show that the proposed method outperforms the state-of-the-art in both time and frequency domain. They also show that their method can be applied to the task of spatial location estimation. "
4821,SP:8bc53935566be2b70403f4b46fe94686d5eae1a1,"self - training USED-FOR model. iterative self - training USED-FOR model. implicit curriculum USED-FOR iterative self - training. method USED-FOR virtual samples. intermediate distributions USED-FOR virtual samples. iterative self - training CONJUNCTION GIFT. GIFT CONJUNCTION iterative self - training. self - training CONJUNCTION iterative self - training. iterative self - training CONJUNCTION self - training. GIFT USED-FOR model. domain adaptation methods USED-FOR GIFT. natural distribution shifts FEATURE-OF benchmarks. benchmarks EVALUATE-FOR iterative self - training. benchmarks EVALUATE-FOR self - training. benchmarks EVALUATE-FOR GIFT. Task is domain adaptation. Method are learning domain invariant representations, and iterative selftraining. Generic is assumptions. OtherScientificTerm is synthetic distribution shifts. ",This paper proposes a method for learning domain invariant representations. The method is based on iterative self-training (GTT) and iterative selftraining (ESL). The authors show that the proposed method outperforms the baselines on several benchmark datasets. ,This paper proposes a method for learning domain invariant representations. The method is based on iterative self-training (GTT) and iterative selftraining (ESL). The authors show that the proposed method outperforms the baselines on several benchmark datasets. 
4837,SP:862d6d76692aee384adc70fd845f0b89cfda93d3,"representations USED-FOR multi - modal retrieval. recommendation CONJUNCTION search. search CONJUNCTION recommendation. representations USED-FOR applications. search HYPONYM-OF applications. recommendation HYPONYM-OF applications. video titles or audio transcripts USED-FOR video - text retrieval literature. method USED-FOR representations. videos USED-FOR representations. attention - based mechanism USED-FOR model. comments USED-FOR method. video - text retrieval benchmarks EVALUATE-FOR method. Generic is benchmarks. OtherScientificTerm are modalities, and user comments. ",This paper proposes a method for multi-modal video-text retrieval. The proposed method is based on the attention-based mechanism. The authors show that the proposed method outperforms the baselines on a number of benchmark datasets. ,This paper proposes a method for multi-modal video-text retrieval. The proposed method is based on the attention-based mechanism. The authors show that the proposed method outperforms the baselines on a number of benchmark datasets. 
4853,SP:a35eb46f391e1a1e347e7243245ca69f4c0f129f,"policy USED-FOR latent - conditioned trajectories. discriminator USED-FOR distinguishability. trajectories USED-FOR latents. penalization USED-FOR exploration. objective USED-FOR epistemic uncertainty. objective USED-FOR intrinsic reward. intrinsic reward COMPARE pseudocount - based methods. pseudocount - based methods COMPARE intrinsic reward. objective COMPARE pseudocount - based methods. pseudocount - based methods COMPARE objective. DISDAIN USED-FOR skill learning. tabular grid world EVALUATE-FOR DISDAIN. DISDAIN USED-FOR pessimism. OtherScientificTerm are Unsupervised skill learning objectives, extrinsic rewards, inherent pessimism, information gain auxiliary objective, and discriminators. ","This paper proposes a new method for unsupervised skill learning, DISDAIN, which uses an information gain auxiliary objective to improve the epistemic uncertainty of the intrinsic reward. The authors show that the proposed method outperforms existing methods on the tabular grid world. The main contribution of the paper is the use of a discriminator to learn the discriminator. The discriminator is based on the notion of distinguishability, and the authors show empirically that this discriminator can be used to improve intrinsic reward in the context of skill learning. ","This paper proposes a new method for unsupervised skill learning, DISDAIN, which uses an information gain auxiliary objective to improve the epistemic uncertainty of the intrinsic reward. The authors show that the proposed method outperforms existing methods on the tabular grid world. The main contribution of the paper is the use of a discriminator to learn the discriminator. The discriminator is based on the notion of distinguishability, and the authors show empirically that this discriminator can be used to improve intrinsic reward in the context of skill learning. "
4869,SP:4eafae76b923b75534cd28e6e04774ea69e3c2d1,deep neural networks USED-FOR chemistry. deep neural networks USED-FOR generating molecules. spanning tree CONJUNCTION residual edges. residual edges CONJUNCTION spanning tree. spanning tree USED-FOR molecular graph generation. tree - constructive operations USED-FOR molecular graph connectivity. formulation USED-FOR sparsity of molecular graphs. intermediate graph structure USED-FOR framework. intermediate graph structure USED-FOR construction process. chemical valence rules FEATURE-OF molecular graphs. Transformer architecture USED-FOR tree construction procedure. tree - based relative positional encodings USED-FOR tree construction procedure. tree - based relative positional encodings USED-FOR Transformer architecture. Fréchet ChemNet distance CONJUNCTION fragment similarity. fragment similarity CONJUNCTION Fréchet ChemNet distance. validity CONJUNCTION Fréchet ChemNet distance. Fréchet ChemNet distance CONJUNCTION validity. QM9 CONJUNCTION ZINC250k. ZINC250k CONJUNCTION QM9. ZINC250k CONJUNCTION MOSES benchmarks. MOSES benchmarks CONJUNCTION ZINC250k. QM9 EVALUATE-FOR framework. MOSES benchmarks EVALUATE-FOR framework. metrics EVALUATE-FOR framework. fragment similarity EVALUATE-FOR framework. validity EVALUATE-FOR framework. validity HYPONYM-OF metrics. fragment similarity HYPONYM-OF metrics. Fréchet ChemNet distance HYPONYM-OF metrics. STGG USED-FOR penalized LogP value of molecules. Task is maximizing penalized LogP value of molecules. ,This paper proposes a transformer-based method for molecular graph generation. The proposed method is based on a tree-based relative positional encodings (STGG) and a spanning tree-constructive operations (STGC). The authors show that the proposed method outperforms state-of-the-art methods on several benchmark datasets. ,This paper proposes a transformer-based method for molecular graph generation. The proposed method is based on a tree-based relative positional encodings (STGG) and a spanning tree-constructive operations (STGC). The authors show that the proposed method outperforms state-of-the-art methods on several benchmark datasets. 
4885,SP:3a19340d6af65e3f949dda839a6d233369891c46,"image generation CONJUNCTION face recognition. face recognition CONJUNCTION image generation. Polynomial neural networks ( PNNs ) USED-FOR image generation. Polynomial neural networks ( PNNs ) USED-FOR face recognition. spectral bias FEATURE-OF low - frequency functions. spectral bias FEATURE-OF neural networks. spectral analysis USED-FOR Neural Tangent Kernel ( NTK ). Neural Tangent Kernel ( NTK ) USED-FOR PNNs. spectral analysis USED-FOR PNNs. parametrization of PNNs USED-FOR learning. Π - Net family USED-FOR learning. parametrization of PNNs HYPONYM-OF Π - Net family. polynomials USED-FOR multiplicative interactions. OtherScientificTerm are high - frequency information, and theoretical bias. Task is training. ","This paper studies the spectral bias of neural networks. The authors show that neural networks with low-frequency functions tend to have higher spectral bias than neural networks that have high frequency functions. They show that this is due to the multiplicative interactions between high-frequency information and low frequency information. They then propose a new family of PNNs, called the Π-Net family, which is a family of polynomials that can be used for learning. They also provide a theoretical analysis of this family. ","This paper studies the spectral bias of neural networks. The authors show that neural networks with low-frequency functions tend to have higher spectral bias than neural networks that have high frequency functions. They show that this is due to the multiplicative interactions between high-frequency information and low frequency information. They then propose a new family of PNNs, called the Π-Net family, which is a family of polynomials that can be used for learning. They also provide a theoretical analysis of this family. "
4901,SP:ebed8b8a25cead3629832c2ba52caf0059971d3d,"memory CONJUNCTION computational costs. computational costs CONJUNCTION memory. hidden subnetworks PART-OF randomly initialized NNs. edge - popup algorithm USED-FOR hidden subnetworks. subnetworks PART-OF randomly initialized NNs. disguised subnetworks HYPONYM-OF subnetworks. disguised subnetworks COMPARE hidden counterparts. hidden counterparts COMPARE disguised subnetworks. unmasking process USED-FOR subnetworks. unmasking process USED-FOR sparse subnetwork mask. two - stage algorithm USED-FOR disguised subnetworks. operations USED-FOR two - stage algorithm. random initialization USED-FOR subnetwork. ResNet-18 CONJUNCTION ResNet-50. ResNet-50 CONJUNCTION ResNet-18. ResNet-50 CONJUNCTION WideResNet-28. WideResNet-28 CONJUNCTION ResNet-50. PaB COMPARE counterparts. counterparts COMPARE PaB. PaB COMPARE edge - popup. edge - popup COMPARE PaB. CIFAR-10/100 datasets EVALUATE-FOR PaB. edge - popup COMPARE counterparts. counterparts COMPARE edge - popup. large - scale models EVALUATE-FOR PaB. CIFAR-10/100 datasets EVALUATE-FOR large - scale models. WideResNet-28 HYPONYM-OF large - scale models. ResNet-18 HYPONYM-OF large - scale models. ResNet-50 HYPONYM-OF large - scale models. Method are Sparse neural networks ( NNs ), pruningand - finetuning pipeline, random networks, and training algorithm. OtherScientificTerm are disguise, latent weights, and approximated gradients. ","This paper proposes a new method for training sparse neural networks (SNNs) that is based on the edge-popup algorithm. The authors propose a two-stage algorithm for learning sparse subnetworks. The first stage is to learn a sparse subnetwork mask, which is then used to perform pruning and finetuning. The second stage is an unmasking process, where the masked subnetwork is used to learn the hidden subnetwork. The method is evaluated on CIFAR-10/100, ResNet-18, and WideResNet-28.","This paper proposes a new method for training sparse neural networks (SNNs) that is based on the edge-popup algorithm. The authors propose a two-stage algorithm for learning sparse subnetworks. The first stage is to learn a sparse subnetwork mask, which is then used to perform pruning and finetuning. The second stage is an unmasking process, where the masked subnetwork is used to learn the hidden subnetwork. The method is evaluated on CIFAR-10/100, ResNet-18, and WideResNet-28."
4917,SP:fa9e46f1dc70dbe87ff53a6b8dd5419c14b40ef3,"it USED-FOR risk - sensitive domains. robust GNN model USED-FOR adversarial attacks. Unified Graph Neural Network ( GUGNN ) framework USED-FOR graph. graph CONJUNCTION features. features CONJUNCTION graph. robust GNN model(R - GUGNN ) USED-FOR adversarial attacks. operations USED-FOR robust GNN model(R - GUGNN ). operations USED-FOR it. similarity of two adjacent nodes ’ features CONJUNCTION sparsity of real - world graphs. sparsity of real - world graphs CONJUNCTION similarity of two adjacent nodes ’ features. small eigenvalues FEATURE-OF perturbed graphs. operation USED-FOR graph. convolution operation USED-FOR features. Laplacian smoothness CONJUNCTION prior knowledge. prior knowledge CONJUNCTION Laplacian smoothness. Laplacian smoothness USED-FOR convolution operation. real - world datasets EVALUATE-FOR R - GUGNN. real - world datasets EVALUATE-FOR baselines. R - GUGNN COMPARE baselines. baselines COMPARE R - GUGNN. Method is Graph Neural Networks ( GNNs ). OtherScientificTerm are graphs, and denoising features. Generic is they. Task is cleaning perturbed graph structure. ","This paper proposes a novel framework for learning robust GNNs against adversarial attacks. The proposed framework is based on the Unified Graph Neural Network (GUGNN) framework, which is an extension of the GUGNN framework. The key idea is to use Laplacian smoothness and prior knowledge to improve the robustness of a GNN model. The authors show that the proposed method outperforms baselines on several benchmark datasets. ","This paper proposes a novel framework for learning robust GNNs against adversarial attacks. The proposed framework is based on the Unified Graph Neural Network (GUGNN) framework, which is an extension of the GUGNN framework. The key idea is to use Laplacian smoothness and prior knowledge to improve the robustness of a GNN model. The authors show that the proposed method outperforms baselines on several benchmark datasets. "
4933,SP:fcb14510ef8541f320ef1c3cab4c0c017e2e15b9,approach USED-FOR texture mapping. it USED-FOR document image unwarping. texture mapping USED-FOR 3D surface. method USED-FOR surface parameterization. 3D surface positions CONJUNCTION 2D texture - space coordinates. 2D texture - space coordinates CONJUNCTION 3D surface positions. continuous bijective mapping USED-FOR 3D surface positions. 2D texture - space coordinates FEATURE-OF continuous bijective mapping. continuous bijective mapping USED-FOR method. multi - view images CONJUNCTION rendering loss. rendering loss CONJUNCTION multi - view images. surface parameterization network PART-OF differentiable rendering pipeline. rendering loss USED-FOR surface parameterization network. multi - view images USED-FOR surface parameterization network. differentiable rendering techniques USED-FOR implicit surfaces. 3D scene reconstruction CONJUNCTION view synthesis. view synthesis CONJUNCTION 3D scene reconstruction. differentiable rendering techniques USED-FOR 3D scene reconstruction. methods USED-FOR appearance color. texture map extraction CONJUNCTION texture editing. texture editing CONJUNCTION texture map extraction. differentiable renderer USED-FOR implicit surfaces. texture extraction USED-FOR document - unwarping. approach USED-FOR high - frequency textures. high - frequency textures FEATURE-OF arbitrary document shapes. synthetic and real scenarios FEATURE-OF arbitrary document shapes. it USED-FOR document texture editing. system USED-FOR document texture editing. it USED-FOR system. Method is explicit surface parameterization. Generic is they. ,This paper proposes a new method for texture mapping for 3D surface reconstruction and texture editing. The method is based on the idea of continuous bijective mapping between 2D and 3D texture-space coordinates. The authors propose a differentiable rendering pipeline that uses a surface parameterization network and a rendering loss to learn the surface parameters. The proposed method is evaluated on synthetic and real-world datasets. ,This paper proposes a new method for texture mapping for 3D surface reconstruction and texture editing. The method is based on the idea of continuous bijective mapping between 2D and 3D texture-space coordinates. The authors propose a differentiable rendering pipeline that uses a surface parameterization network and a rendering loss to learn the surface parameters. The proposed method is evaluated on synthetic and real-world datasets. 
4949,SP:28ac9848fb69d1c59fd751fbeee9a4ac799db897,"subtle regions USED-FOR fine - grained features. strided operations USED-FOR representation. Convolutional Neural Network USED-FOR representation. strided operations USED-FOR Convolutional Neural Network. downsampling algorithm USED-FOR network. scale of receptive field CONJUNCTION granularity of feature. granularity of feature CONJUNCTION scale of receptive field. trade - off mechanism USED-FOR ARP. ARP USED-FOR network. image classification CONJUNCTION image retrieval. image retrieval CONJUNCTION image classification. pooling operation COMPARE state - of - the - arts. state - of - the - arts COMPARE pooling operation. image classification EVALUATE-FOR state - of - the - arts. Task is Fine - grained recognition. OtherScientificTerm are feature resolution, fine - grained information, resolution of sub - sampled feature, and learning - based parameters. ",This paper proposes a novel downsampling method for fine-grained image recognition. The proposed method is based on a trade-off between the granularity of the feature and the scale of the receptive field. The paper shows that the proposed method outperforms the pooling method in terms of feature resolution and the number of sub-sampled features. ,This paper proposes a novel downsampling method for fine-grained image recognition. The proposed method is based on a trade-off between the granularity of the feature and the scale of the receptive field. The paper shows that the proposed method outperforms the pooling method in terms of feature resolution and the number of sub-sampled features. 
4965,SP:8648453f5a7c5e9b99a8fdbaa340f4e2b4d048d0,"distribution shifts FEATURE-OF neural networks. structural information USED-FOR prediction. structural information FEATURE-OF graph. OOD problem USED-FOR node - level prediction. domain - invariant learning approach USED-FOR GNNs. invariant graph features USED-FOR prediction. invariant graph features USED-FOR GNNs. graphs USED-FOR OOD problem. graphs USED-FOR node - level prediction. Explore - to - Extrapolate Risk Minimization HYPONYM-OF domain - invariant learning approach. graph editers HYPONYM-OF multiple context explorers. cross - domain transfers CONJUNCTION dynamic graph evolution. dynamic graph evolution CONJUNCTION cross - domain transfers. artificial spurious features CONJUNCTION cross - domain transfers. cross - domain transfers CONJUNCTION artificial spurious features. real - world datasets USED-FOR distribution shifts. method USED-FOR distribution shifts. real - world datasets EVALUATE-FOR method. Material are Euclidean data, and graph - structured data. Method are invariant models, and OOD solution. OtherScientificTerm is virtual environments. Generic is model. ",This paper proposes a domain-invariant learning approach for graph-structured data. The proposed approach is based on the exploration-to-extrapolate Risk Minimization (ERM) framework. The authors show that the proposed method is able to learn invariant graph features for node-level prediction. They also show that their approach can be applied to real-world datasets.,This paper proposes a domain-invariant learning approach for graph-structured data. The proposed approach is based on the exploration-to-extrapolate Risk Minimization (ERM) framework. The authors show that the proposed method is able to learn invariant graph features for node-level prediction. They also show that their approach can be applied to real-world datasets.
4981,SP:870cd8794f7ff48fbed71c2abc9fb7dad51bd343,"contrastive learning USED-FOR time series data. encoder USED-FOR robust and discriminative representations. augmentations USED-FOR contrastive learning. ad - hoc manual selection EVALUATE-FOR time series augmentations. prefabricated human priors USED-FOR rule of thumb. rule of thumb USED-FOR augmented samples. augmentations of time series data USED-FOR contrastive learning tasks. meta - learning mechanism USED-FOR information - aware approach. InfoTS HYPONYM-OF information - aware approach. meta - learner CONJUNCTION encoder. encoder CONJUNCTION meta - learner. classification task EVALUATE-FOR leading baselines. accuracy EVALUATE-FOR classification task. MSE EVALUATE-FOR forecasting task. accuracy EVALUATE-FOR leading baselines. datasets EVALUATE-FOR forecasting task. forecasting task EVALUATE-FOR leading baselines. datasets EVALUATE-FOR leading baselines. classification task EVALUATE-FOR forecasting task. datasets EVALUATE-FOR classification task. Method are contrastive learning approaches, information theory, and contrastive representation learning. Material is image and language domains. OtherScientificTerm is sub - optimal solutions. ","This paper proposes InfoTS, a meta-learning approach for contrastive learning. InfoTS is based on the idea that augmentations of time series data can be used to improve the robustness of the encoder and the discriminative performance of the decoder. The proposed method is evaluated on a variety of tasks, including classification, forecasting, and MSE. ","This paper proposes InfoTS, a meta-learning approach for contrastive learning. InfoTS is based on the idea that augmentations of time series data can be used to improve the robustness of the encoder and the discriminative performance of the decoder. The proposed method is evaluated on a variety of tasks, including classification, forecasting, and MSE. "
4997,SP:6bc677d060ba4ab09f6da61458680e7a7976644b,"winning tickets PART-OF task. theoretical physics FEATURE-OF renormalization group theory. iterative magnitude pruning USED-FOR winning tickets. method USED-FOR winning tickets. iterative magnitude pruning HYPONYM-OF method. iterative magnitude pruning HYPONYM-OF renormalization group scheme. numerical and theoretical tools USED-FOR winning ticket universality. winning ticket universality FEATURE-OF large scale lottery ticket experiments. Task are Lottery Ticket Hypothesis, and machine learning. Generic is tasks. ",This paper studies the Lottery Ticket Hypothesis (LTH) and proposes a novel method for finding winning tickets. The authors propose a new method to find winning tickets that is based on the renormalization group theory. They show that the winning tickets can be found using iterative magnitude pruning. They also show that this method can be applied to large scale lottery ticket experiments.,This paper studies the Lottery Ticket Hypothesis (LTH) and proposes a novel method for finding winning tickets. The authors propose a new method to find winning tickets that is based on the renormalization group theory. They show that the winning tickets can be found using iterative magnitude pruning. They also show that this method can be applied to large scale lottery ticket experiments.
5013,SP:59ce2e6c3674157d6fa990316812d0823c1ec586,"Transformer - based models USED-FOR information retrieval problem. BERT USED-FOR information retrieval problem. BERT HYPONYM-OF Transformer - based models. joint embedding USED-FOR cross - attention ( CA ) models. separate embeddings USED-FOR dual - encoder ( DE ) models. cross - attention ( CA ) models HYPONYM-OF models. DE models USED-FOR scores. real - world problems EVALUATE-FOR DE models. CA COMPARE DE models. DE models COMPARE CA. real - world problems EVALUATE-FOR CA. benchmark neural re - ranking datasets EVALUATE-FOR distillation strategy. Method are CA models, and cross - attention. ",This paper proposes a new dual-encoder (DE) model for BERT-based Transformer-based models. The authors propose a novel distillation strategy to improve the performance of DE models. They show that the proposed method is able to achieve better performance than the state-of-the-art on a number of benchmark datasets. ,This paper proposes a new dual-encoder (DE) model for BERT-based Transformer-based models. The authors propose a novel distillation strategy to improve the performance of DE models. They show that the proposed method is able to achieve better performance than the state-of-the-art on a number of benchmark datasets. 
5029,SP:679e57a2027ff1855e5dc80bd3ec91f6489cc747,"Off - policy methods USED-FOR Policy Optimization ( PO ) algorithms. IS PART-OF Monte Carlo simulation. variance minimization approach USED-FOR IS. variance minimization approach PART-OF Monte Carlo simulation. behavioral distribution USED-FOR sampling. variance minimization USED-FOR performance improvement tool. variance minimization COMPARE off - policy learning. off - policy learning COMPARE variance minimization. PO algorithm USED-FOR Policy Optimization. variance minimization USED-FOR Policy Optimization. Optimal Policy Evaluation ( POPE ) USED-FOR Policy Optimization. small batch sizes FEATURE-OF robustness. Method is Importance Sampling ( IS ). OtherScientificTerm are behavioral policy, and trust region. Material is continuous RL benchmarks. ",This paper studies the problem of policy optimization in the context of Importance Sampling (IS). The authors propose a variance minimization approach to improve the robustness of the policy optimization algorithm. The authors show that the proposed method is robust to small batch sizes and can be applied to a variety of off-policy methods. They also show that their method is more robust to large batch sizes. ,This paper studies the problem of policy optimization in the context of Importance Sampling (IS). The authors propose a variance minimization approach to improve the robustness of the policy optimization algorithm. The authors show that the proposed method is robust to small batch sizes and can be applied to a variety of off-policy methods. They also show that their method is more robust to large batch sizes. 
5045,SP:b169c94c8fcc4f13cafdbafbe18eb26cb471ea0b,"Graph Neural Networks ( GNNs ) USED-FOR atomic simulations. Graph Neural Networks ( GNNs ) USED-FOR catalyst discovery. GNNs USED-FOR task. triplets CONJUNCTION quadruplets of atoms. quadruplets of atoms CONJUNCTION triplets. they USED-FOR higher - order interactions. higher - order interactions PART-OF graphs. method USED-FOR GNNs. method USED-FOR graphs. GPUs USED-FOR graphs. force MAE metric EVALUATE-FOR S2EF task. AFbT metric EVALUATE-FOR IS2RS task. force MAE metric EVALUATE-FOR graph - parallelized models. OtherScientificTerm is climate change. Generic is models. Method are Graph Parallelism, and DimeNet++ and GemNet models. ",This paper proposes a graph parallelism method for graph neural networks (GNNs) for atomic simulations. The proposed method is based on DimeNet++ and GemNet models. The method is evaluated on the IS2RS and S2EF tasks. The authors show that the proposed method outperforms the state-of-the-art methods in terms of force MAE and AFbT. ,This paper proposes a graph parallelism method for graph neural networks (GNNs) for atomic simulations. The proposed method is based on DimeNet++ and GemNet models. The method is evaluated on the IS2RS and S2EF tasks. The authors show that the proposed method outperforms the state-of-the-art methods in terms of force MAE and AFbT. 
5061,SP:352c177d89b9460acee0c78364e6d9c153c6a93c,"language models USED-FOR text generation tasks. meandering CONJUNCTION incoherent content. incoherent content CONJUNCTION meandering. incoherent content HYPONYM-OF structural flaws. meandering HYPONYM-OF structural flaws. Time Control ( TC ) HYPONYM-OF language model. latent stochastic process USED-FOR language model. representation USED-FOR TC. representation USED-FOR language model. stochastic process USED-FOR document plan. text infilling CONJUNCTION discourse coherence. discourse coherence CONJUNCTION text infilling. domain - specific methods COMPARE TC. TC COMPARE domain - specific methods. domain - specific methods CONJUNCTION fine - tuning GPT2. fine - tuning GPT2 CONJUNCTION domain - specific methods. fine - tuning GPT2 COMPARE TC. TC COMPARE fine - tuning GPT2. text domains EVALUATE-FOR fine - tuning GPT2. discourse coherence EVALUATE-FOR TC. text infilling EVALUATE-FOR TC. ordering CONJUNCTION text length consistency. text length consistency CONJUNCTION ordering. long text generation settings EVALUATE-FOR TC. TC USED-FOR text structure. OtherScientificTerm are stochastic process of interest, and latent plan. ","This paper proposes a new language model called Time Control (TC) for text generation tasks. The proposed model is based on a latent stochastic process of interest (LSTM) that is used to learn a document plan. The authors show that the proposed model outperforms the state-of-the-art GPT2 and fine-tuning methods on several tasks, including text infilling, discourse coherence, and text length consistency. ","This paper proposes a new language model called Time Control (TC) for text generation tasks. The proposed model is based on a latent stochastic process of interest (LSTM) that is used to learn a document plan. The authors show that the proposed model outperforms the state-of-the-art GPT2 and fine-tuning methods on several tasks, including text infilling, discourse coherence, and text length consistency. "
5077,SP:56a74403d4471cd95641dc669f5eac89a2c93144,"representation of objects USED-FOR higher - level concepts. framework USED-FOR object - centric representation. single 2D images USED-FOR object - centric representation. object - centric models COMPARE model. model COMPARE object - centric models. model USED-FOR segmenting objects. network USED-FOR latent code space. geometric shape CONJUNCTION texture / color. texture / color CONJUNCTION geometric shape. supervision USED-FOR model. specificity FEATURE-OF object segmentation. predictive learning USED-FOR models. approach USED-FOR symbolic representation. OtherScientificTerm are prediction error of future sensory input, moving objects, latent causes, 3D environment, and clustering colors. Material is synthetic dataset. ","This paper proposes a framework for object-centric representation learning. The proposed method is based on the idea of latent code space, which is used to learn a symbolic representation of objects in a latent space. The method is evaluated on a synthetic dataset and shows that the proposed method outperforms existing methods on object segmentation tasks. ","This paper proposes a framework for object-centric representation learning. The proposed method is based on the idea of latent code space, which is used to learn a symbolic representation of objects in a latent space. The method is evaluated on a synthetic dataset and shows that the proposed method outperforms existing methods on object segmentation tasks. "
5093,SP:244f5d31ec93b7a4bfc4b257ee6cdd5cfdb18a38,"traffic management CONJUNCTION public safety. public safety CONJUNCTION traffic management. Spatio - temporal ( ST ) prediction task USED-FOR traffic management. Spatio - temporal ( ST ) prediction task USED-FOR public safety. mobility forecasting USED-FOR traffic management. mobility forecasting HYPONYM-OF Spatio - temporal ( ST ) prediction task. spatial and temporal domains FEATURE-OF features. independent variables PART-OF latent representation. semantic factors FEATURE-OF independent variables. It USED-FOR mobility forecasting models. It USED-FOR spatial and temporal features. VAE - based architecture USED-FOR disentangled representation. real spatio - temporal data USED-FOR mobility forecasting. real spatio - temporal data USED-FOR disentangled representation. deep generative model USED-FOR latent representation. temporal dynamics CONJUNCTION spatially varying component. spatially varying component CONJUNCTION temporal dynamics. deep generative model USED-FOR reconstructions. non - informative features USED-FOR method. Task is mobility forecasting problems. Generic are they, and models. Method are dynamic and static components, and Disentangled representation learning. Material is spatio - temporal datasets. ",This paper proposes a VAE-based method for disentangled representation learning for mobility forecasting. The method is based on the VAE framework and is able to disentangle the dynamic and static components of the latent representation. The authors show that the proposed method can be applied to both spatial and temporal data. The proposed method is evaluated on a number of real-world mobility forecasting datasets.,This paper proposes a VAE-based method for disentangled representation learning for mobility forecasting. The method is based on the VAE framework and is able to disentangle the dynamic and static components of the latent representation. The authors show that the proposed method can be applied to both spatial and temporal data. The proposed method is evaluated on a number of real-world mobility forecasting datasets.
5109,SP:cf781d756cf0bed5f7cdeb94be49e6d4409eeda4,"deep learning framework USED-FOR probabilistic interpolation of irregularly sampled time series. temporal VAE architecture USED-FOR uncertainty. heteroscedastic output layer USED-FOR variable uncertainty. input layer CONJUNCTION temporal VAE architecture. temporal VAE architecture CONJUNCTION input layer. temporal VAE architecture CONJUNCTION heteroscedastic output layer. heteroscedastic output layer CONJUNCTION temporal VAE architecture. heteroscedastic output layer USED-FOR output interpolations. variable uncertainty FEATURE-OF output interpolations. input layer USED-FOR input observation sparsity. temporal VAE architecture PART-OF HeTVAE. input layer PART-OF HeTVAE. heteroscedastic output layer PART-OF HeTVAE. architecture COMPARE deep latent variable models. deep latent variable models COMPARE architecture. homoscedastic output layers USED-FOR deep latent variable models. Material is Irregularly sampled time series. Method are deep learning models, Heteroscedastic Temporal Variational Autoencoder ( HeTVAE ), and sparse and irregular sampling. OtherScientificTerm is input sparsity. ","This paper proposes a new deep learning framework for probabilistic interpolation of irregularly sampled time series. The authors propose a heteroscedastic temporal variational autoencoder (HeTVAE) architecture that combines the temporal VAE architecture and the input-output layer to improve the performance of the model. The proposed method is evaluated on a variety of datasets, including MNIST, CIFAR-10, and FashionMNIST, and is shown to outperform the state-of-the-art.","This paper proposes a new deep learning framework for probabilistic interpolation of irregularly sampled time series. The authors propose a heteroscedastic temporal variational autoencoder (HeTVAE) architecture that combines the temporal VAE architecture and the input-output layer to improve the performance of the model. The proposed method is evaluated on a variety of datasets, including MNIST, CIFAR-10, and FashionMNIST, and is shown to outperform the state-of-the-art."
5125,SP:80b8488b5a7c29014b0fefbc16698afac42250a0,"structure HYPONYM-OF computational graph. importance CONJUNCTION coherence. coherence CONJUNCTION importance. coherence HYPONYM-OF proxies. importance HYPONYM-OF proxies. statistical methods USED-FOR proxies. proxies USED-FOR partitionings. network weights CONJUNCTION correlations of activations. correlations of activations CONJUNCTION network weights. correlations of activations USED-FOR edges. spectrally clustering USED-FOR partitionings. network weights USED-FOR edges. ones HYPONYM-OF partitionings. weights USED-FOR partitionings. weights USED-FOR ones. graph - based partitioning USED-FOR modularity. graph - based partitioning USED-FOR deep neural networks. Method is neural network. OtherScientificTerm are functionality, and neurons. Task is non - runtime analysis. ","This paper proposes a graph-based partitioning method for deep neural networks. The proposed method is based on the idea of partitioning a graph into subgraphs, which are then partitioned into a set of nodes, each of which is partitioned based on its importance, coherence, and importance of each node. The authors show that the proposed method can be applied to a wide range of deep neural network models, and show that it is able to achieve better performance than existing methods. ","This paper proposes a graph-based partitioning method for deep neural networks. The proposed method is based on the idea of partitioning a graph into subgraphs, which are then partitioned into a set of nodes, each of which is partitioned based on its importance, coherence, and importance of each node. The authors show that the proposed method can be applied to a wide range of deep neural network models, and show that it is able to achieve better performance than existing methods. "
5141,SP:0d7cbb544bc39203c9c18b4fee56fc94cbe78375,"mobile devices FEATURE-OF speech - interactive features. lottery ticket hypothesis USED-FOR sparse subnetworks. lottery ticket hypothesis USED-FOR lightweight speech recognition models. noise FEATURE-OF speech. CTC CONJUNCTION RNN - Transducer, and Transformer models. RNN - Transducer, and Transformer models CONJUNCTION CTC. winning tickets COMPARE full models. full models COMPARE winning tickets. sparsity USED-FOR noise robustness. Method are Lightweight speech recognition models, and speech models. Generic are systems, and full model. Task is open - world personalization. OtherScientificTerm are structured sparsity, backbones, full model weights, and background noises. ","This paper studies the lottery ticket hypothesis for sparse subnetworks in speech recognition models. Specifically, the authors propose to use lottery tickets to estimate the sparsity of the full model weights, and then use this sparsity as a measure of the robustness of the model to noisy background noise. The authors show that the lottery tickets can be used to estimate sparsity in the backbones of CTC, RNN-Transducer, and Transformer models. They also show that winning tickets are more robust to background noise than full models.","This paper studies the lottery ticket hypothesis for sparse subnetworks in speech recognition models. Specifically, the authors propose to use lottery tickets to estimate the sparsity of the full model weights, and then use this sparsity as a measure of the robustness of the model to noisy background noise. The authors show that the lottery tickets can be used to estimate sparsity in the backbones of CTC, RNN-Transducer, and Transformer models. They also show that winning tickets are more robust to background noise than full models."
5157,SP:cb9530f5517f1092513c200b3f32e55420fdd768,"random weights USED-FOR Deep neural networks. skip connections CONJUNCTION Hadamard transforms. Hadamard transforms CONJUNCTION skip connections. skip connections USED-FOR ResNet architectures. Hadamard transforms USED-FOR ResNet architectures. batch normalization USED-FOR network training. random weights USED-FOR network initialization. image classification datasets EVALUATE-FOR ZerO. ImageNet HYPONYM-OF image classification datasets. OtherScientificTerm is stable signal propagation. Metric are variance, and reproducibility. Method are random weight initialization, and residual networks. ","This paper studies the problem of random weight initialization in deep neural networks. The authors propose a new method called ZerO, which is based on batch normalization to improve the reproducibility of random weights. They show that ZerO can be used to reduce the variance of the random weights in the initialization process. They also show that the variance can be reduced to zero in the case of skip connections, Hadamard transforms, and residual networks. Experiments are conducted on ImageNet and CIFAR-10.","This paper studies the problem of random weight initialization in deep neural networks. The authors propose a new method called ZerO, which is based on batch normalization to improve the reproducibility of random weights. They show that ZerO can be used to reduce the variance of the random weights in the initialization process. They also show that the variance can be reduced to zero in the case of skip connections, Hadamard transforms, and residual networks. Experiments are conducted on ImageNet and CIFAR-10."
5173,SP:16618b226e42a07095dcf9204ce4c0e3b2ed8bd8,"minimax formulation USED-FOR backdoors. backdoors PART-OF poisoned model. minimax formulation USED-FOR poisoned model. clean data USED-FOR minimax formulation. backdoor removal PART-OF formulation. implicit hypergradient USED-FOR algorithm. robustness EVALUATE-FOR minimax. clean data USED-FOR minimax. I - BAU COMPARE state - ofart backdoor defenses. state - ofart backdoor defenses COMPARE I - BAU. state - ofart backdoor defenses USED-FOR backdoor attacks. I - BAU USED-FOR backdoor attacks. I - BAU COMPARE baseline. baseline COMPARE I - BAU. attack settings CONJUNCTION poison ratio. poison ratio CONJUNCTION attack settings. poison ratio CONJUNCTION clean data size. clean data size CONJUNCTION poison ratio. it COMPARE baseline. baseline COMPARE it. single - target attack setting EVALUATE-FOR baseline. single - target attack setting EVALUATE-FOR it. computation USED-FOR I - BAU. OtherScientificTerm is inner and outer optimization. Metric is convergence. Generic are its, and baselines. ","This paper studies the problem of backdoor defense against backdoor attacks. The authors propose a new formulation of the minimax formulation, called I-BAU, which is based on the idea of implicit hypergradient. The main contribution of this paper is the formulation of a new algorithm for backdoor defense. The paper shows that the proposed algorithm is robust to backdoor attacks, and is able to achieve better performance than the baselines. ","This paper studies the problem of backdoor defense against backdoor attacks. The authors propose a new formulation of the minimax formulation, called I-BAU, which is based on the idea of implicit hypergradient. The main contribution of this paper is the formulation of a new algorithm for backdoor defense. The paper shows that the proposed algorithm is robust to backdoor attacks, and is able to achieve better performance than the baselines. "
5189,SP:7260bd50f600a481ec7710792b63f518218e0eaf,"random permutations COMPARE with - replacement sampling. with - replacement sampling COMPARE random permutations. permutations COMPARE random. random COMPARE permutations. smooth second derivatives FEATURE-OF 1 - dimensional strongly convex functions. random permutations USED-FOR strongly convex functions. easy - to - construct permutations USED-FOR accelerated convergence. easy - to - construct permutations COMPARE random. random COMPARE easy - to - construct permutations. easy - to - construct permutations USED-FOR quadratic, strongly - convex functions. convergence characterization USED-FOR optimal permutations. Method is permutation - based SGD. Metric is convergence gap. ","This paper studies the convergence of permutation-based SGD algorithms for strongly convex functions with smooth second derivatives. The authors show that the convergence rate of the optimal permutation is bounded by the number of permutations, which is a result of the fact that the smoothness of the second derivatives of the function is bounded. They also provide a convergence analysis for the smooth second derivative of the permutation. They show that this convergence rate is bounded in the case that the permutations are easy to construct, and they show that it is also bounded in cases where the second derivative is smooth. ","This paper studies the convergence of permutation-based SGD algorithms for strongly convex functions with smooth second derivatives. The authors show that the convergence rate of the optimal permutation is bounded by the number of permutations, which is a result of the fact that the smoothness of the second derivatives of the function is bounded. They also provide a convergence analysis for the smooth second derivative of the permutation. They show that this convergence rate is bounded in the case that the permutations are easy to construct, and they show that it is also bounded in cases where the second derivative is smooth. "
5205,SP:39062dbbe9a30a7b47fa51179c15db34a3380a0b,transformations USED-FOR flow models. method USED-FOR transformation layers. unique transformations USED-FOR transformation layers. mixed distribution USED-FOR architecture optimization. invertibility FEATURE-OF NF architecture. discrete space USED-FOR architecture optimization. global minimum FEATURE-OF approximate upper bound. approximate upper bound USED-FOR mixture NF. block - wise alternating optimization algorithm USED-FOR architecture optimization. block - wise alternating optimization algorithm USED-FOR deep flow models. architecture optimization USED-FOR deep flow models. Metric is performance - cost trade - offs. Method is flow architecture. ,"This paper studies the problem of architecture optimization for flow models. In particular, the authors consider the case where the model is trained with a mixed distribution, and the goal is to find a global minimum of the invertibility of the model. The authors propose a new algorithm for this problem. The algorithm is based on the block-wise alternating optimization algorithm, which is used to optimize the architecture of a flow model in a discrete space. The proposed algorithm is shown to be able to find the global minimum in the discrete space, and it is shown that it can also be used to obtain a global upper bound on the performance-cost trade-off.","This paper studies the problem of architecture optimization for flow models. In particular, the authors consider the case where the model is trained with a mixed distribution, and the goal is to find a global minimum of the invertibility of the model. The authors propose a new algorithm for this problem. The algorithm is based on the block-wise alternating optimization algorithm, which is used to optimize the architecture of a flow model in a discrete space. The proposed algorithm is shown to be able to find the global minimum in the discrete space, and it is shown that it can also be used to obtain a global upper bound on the performance-cost trade-off."
5221,SP:d2656ae0259accc5207234fc4206f6f7be9598d9,self - supervised learning ( SSL ) methods USED-FOR representations. Intrinsic Dimension ( ID ) USED-FOR representation space. Intrinsic Dimension ( ID ) USED-FOR expressiveness. KNN classifier USED-FOR Kmeans cluster labels. Kmeans cluster labels USED-FOR held - out representations. learning speed EVALUATE-FOR KNN classifier. KNN classifier USED-FOR Cluster Learnability ( CL ). learning speed EVALUATE-FOR Cluster Learnability ( CL ). contrastive losses CONJUNCTION pretext tasks. pretext tasks CONJUNCTION contrastive losses. ID CONJUNCTION CL. CL CONJUNCTION ID. model architecture CONJUNCTION human labels. human labels CONJUNCTION model architecture. data augmentation CONJUNCTION model architecture. model architecture CONJUNCTION data augmentation. ID USED-FOR downstream classification. CL USED-FOR downstream classification. CL COMPARE techniques. techniques COMPARE CL. ID COMPARE techniques. techniques COMPARE ID. contrastive losses USED-FOR techniques. pretext tasks USED-FOR techniques. DeepCluster USED-FOR representations. modification USED-FOR DeepCluster. ImageNet benchmarks EVALUATE-FOR DeepCluster. intermediate checkpoints USED-FOR SSL algorithms. framework USED-FOR SSL algorithms. framework USED-FOR intermediate checkpoints. Generic is architectures. ,"This paper proposes DeepCluster, a self-supervised learning method that uses Intrinsic Dimension (ID) and Cluster Learnability (CL) to improve the learning speed of SSL algorithms. The proposed method is based on the idea that ID and CL can be used as intermediate checkpoints for SSL algorithms, and that CL can improve the performance of existing SSL algorithms in terms of learning speed and generalization ability. The method is evaluated on ImageNet, CIFAR-10, and ImageNet-100. ","This paper proposes DeepCluster, a self-supervised learning method that uses Intrinsic Dimension (ID) and Cluster Learnability (CL) to improve the learning speed of SSL algorithms. The proposed method is based on the idea that ID and CL can be used as intermediate checkpoints for SSL algorithms, and that CL can improve the performance of existing SSL algorithms in terms of learning speed and generalization ability. The method is evaluated on ImageNet, CIFAR-10, and ImageNet-100. "
5237,SP:4f5c00469e4425751db5efbc355085a5e8709def,"Deep neural networks USED-FOR adversarial examples. query efficiency EVALUATE-FOR black - box attacks. segmentation priors USED-FOR black - box attacks. salient region FEATURE-OF perturbations. query efficiency CONJUNCTION success rate. success rate CONJUNCTION query efficiency. imperceptibility performance EVALUATE-FOR blackbox attacks. segmentation priors USED-FOR blackbox attacks. Saliency Attack HYPONYM-OF gradient - free black - box attack. attacks USED-FOR perturbations. approach USED-FOR perturbations. approach USED-FOR detection - based defense. Task are blackbox setting, and adversarial attacks. Metric is imperceptibility. ","This paper proposes a new adversarial defense method for black-box adversarial attacks. The proposed method, called saliency attack, is based on segmentation priors and is able to detect perturbations in the salient region of the perturbation. The authors show that the proposed method outperforms existing methods in terms of query efficiency, success rate, and imperceptibility performance. ","This paper proposes a new adversarial defense method for black-box adversarial attacks. The proposed method, called saliency attack, is based on segmentation priors and is able to detect perturbations in the salient region of the perturbation. The authors show that the proposed method outperforms existing methods in terms of query efficiency, success rate, and imperceptibility performance. "
5253,SP:779821ed85084f8bf1b29d8822b312989b186ee9,"Synthesis planning CONJUNCTION reaction outcome prediction. reaction outcome prediction CONJUNCTION Synthesis planning. Synthesis planning HYPONYM-OF computer - aided organic chemistry. reaction outcome prediction HYPONYM-OF computer - aided organic chemistry. Natural language approaches USED-FOR problem. Natural language approaches USED-FOR end - to - end formulation. Natural language approaches USED-FOR SMILES - to - SMILES translation. Transformer models USED-FOR text generation. Transformer models CONJUNCTION molecular graph encoders. molecular graph encoders CONJUNCTION Transformer models. molecular graph encoders USED-FOR Graph2SMILES model. Transformer models USED-FOR Graph2SMILES model. Graph2SMILES USED-FOR task. Transformer USED-FOR task. Graph2SMILES USED-FOR Transformer. molecule(s)-to - molecule(s ) transformations USED-FOR task. Graph2SMILES USED-FOR end - to - end architecture. global attention encoder USED-FOR long - range and intermolecular interactions. graph - aware positional embedding USED-FOR global attention encoder. top-1 accuracy EVALUATE-FOR Transformer baselines. Graph2SMILES COMPARE Transformer baselines. Transformer baselines COMPARE Graph2SMILES. reaction outcome prediction CONJUNCTION one - step retrosynthesis. one - step retrosynthesis CONJUNCTION reaction outcome prediction. reaction outcome prediction EVALUATE-FOR Transformer baselines. USPTO_480k and USPTO_STEREO datasets USED-FOR reaction outcome prediction. reaction outcome prediction EVALUATE-FOR Graph2SMILES. USPTO_50k dataset USED-FOR one - step retrosynthesis. top-1 accuracy EVALUATE-FOR Graph2SMILES. Method are data - driven approaches, machine translation model architectures, SMILES representations, SMILES augmentation, and encoder. Task is data preprocessing. OtherScientificTerm are molecular structures, input data augmentation, and local chemical environments. ","This paper proposes a Transformer-based approach for molecular graph encoder-decoder (MGC) and SMILES-to-SMILES translation. The proposed approach is based on a transformer-based architecture, which is able to handle both long-range and inter-molecular interactions. The authors also propose a global attention encoder to capture the global attention of long-term interactions. Experiments show that the proposed approach achieves state-of-the-art performance on two synthetic and one-step retrosynthesis tasks.","This paper proposes a Transformer-based approach for molecular graph encoder-decoder (MGC) and SMILES-to-SMILES translation. The proposed approach is based on a transformer-based architecture, which is able to handle both long-range and inter-molecular interactions. The authors also propose a global attention encoder to capture the global attention of long-term interactions. Experiments show that the proposed approach achieves state-of-the-art performance on two synthetic and one-step retrosynthesis tasks."
5269,SP:ce3cde67564679a8d9a0539f1e12551390b91475,reinforcement learning ( RL ) methods USED-FOR task - oriented dialogues setting. task - oriented dialogues setting USED-FOR automatic disease diagnosis. reinforcement learning ( RL ) methods USED-FOR automatic disease diagnosis. RL tasks COMPARE action space. action space COMPARE RL tasks. action space FEATURE-OF disease diagnosis. approaches USED-FOR problem. hierarchical policy PART-OF dialogue policy learning. symptom checkers CONJUNCTION disease classifier. disease classifier CONJUNCTION symptom checkers. master model USED-FOR low level model. low level policy PART-OF high level policy. disease classifier PART-OF low level policy. symptom checkers PART-OF low level policy. master model PART-OF high level policy. hierarchical framework COMPARE systems. systems COMPARE hierarchical framework. accuracy CONJUNCTION symptom recall. symptom recall CONJUNCTION accuracy. symptom recall EVALUATE-FOR systems. accuracy EVALUATE-FOR systems. disease diagnosis EVALUATE-FOR systems. hierarchical framework USED-FOR disease diagnosis. symptom recall EVALUATE-FOR hierarchical framework. accuracy EVALUATE-FOR hierarchical framework. Task is offline consultation process. ,"This paper proposes a hierarchical framework for task-oriented dialogues setting for automatic disease diagnosis. The proposed method is based on a hierarchical policy learning approach, where the high level policy learns a low level model and a high level model learns a disease classifier and a symptom checker. The authors show that the proposed method outperforms the state-of-the-art in terms of symptom recall and accuracy. ","This paper proposes a hierarchical framework for task-oriented dialogues setting for automatic disease diagnosis. The proposed method is based on a hierarchical policy learning approach, where the high level policy learns a low level model and a high level model learns a disease classifier and a symptom checker. The authors show that the proposed method outperforms the state-of-the-art in terms of symptom recall and accuracy. "
5285,SP:bd9cb543b5f199ab45e1bf8609c683f12ceb7659,Federated Learning ( FL ) USED-FOR ML training ecosystem. distributed training USED-FOR data privacy. edge devices USED-FOR distributed training. FL COMPARE centralized training. centralized training COMPARE FL. Addressing label deficiency PART-OF FL. framework USED-FOR algorithms. SSFL ) HYPONYM-OF self - supervised and personalized federated learning framework. centralized self - supervised learning methods USED-FOR FL setting. SimSiam networks COMPARE FedAvg algorithm. FedAvg algorithm COMPARE SimSiam networks. Ditto CONJUNCTION local fine - tuning. local fine - tuning CONJUNCTION Ditto. perFedAvg CONJUNCTION Ditto. Ditto CONJUNCTION perFedAvg. algorithms USED-FOR supervised personalization algorithms. algorithms USED-FOR self - supervised learning. supervised personalization algorithms USED-FOR self - supervised learning. local fine - tuning HYPONYM-OF self - supervised learning. perFedAvg HYPONYM-OF self - supervised learning. Ditto HYPONYM-OF self - supervised learning. personalization CONJUNCTION consensus. consensus CONJUNCTION personalization. Per - SSFL USED-FOR personalization. Per - SSFL HYPONYM-OF personalized federated self - supervised learning algorithm. distributed training system USED-FOR SSFL. distributed training system CONJUNCTION evaluation protocol. evaluation protocol CONJUNCTION distributed training system. evaluation protocol USED-FOR SSFL. supervised learning CONJUNCTION unsupervised learning. unsupervised learning CONJUNCTION supervised learning. unsupervised learning USED-FOR FL. evaluation accuracy EVALUATE-FOR unsupervised learning. synthetic non - I.I.D. dataset CONJUNCTION intrinsically non - I.I.D. dataset. intrinsically non - I.I.D. dataset CONJUNCTION synthetic non - I.I.D. dataset. training system USED-FOR synthetic non - I.I.D. dataset. evaluation accuracy EVALUATE-FOR supervised learning. supervised learning USED-FOR FL. CIFAR-10 USED-FOR synthetic non - I.I.D. dataset. batch size CONJUNCTION non-I.I.D.ness. non-I.I.D.ness CONJUNCTION batch,This paper proposes a personalized federated learning (SSFL) framework for self-supervised and personalized learning. The proposed framework is based on FedAvg and Ditto algorithms. The authors show that the proposed framework outperforms FedAvg on CIFAR-10 and synthetic non-I.I.D. dataset.,This paper proposes a personalized federated learning (SSFL) framework for self-supervised and personalized learning. The proposed framework is based on FedAvg and Ditto algorithms. The authors show that the proposed framework outperforms FedAvg on CIFAR-10 and synthetic non-I.I.D. dataset.
5301,SP:8ff52b027a3c2a464b2c2fedb768c092b0fc6ca5,"deep neural networks ( DNN ) CONJUNCTION partial differential equations ( PDEs ). partial differential equations ( PDEs ) CONJUNCTION deep neural networks ( DNN ). PDEs USED-FOR DNN architectures. PDEs USED-FOR ResNet - like DNN. adjustment operator USED-FOR DNN. adjustment operator USED-FOR ResNet - like DNN. adjustment operator USED-FOR PDEs. training method USED-FOR DNN models. PDEs theory USED-FOR training method. robustness EVALUATE-FOR training method. training method USED-FOR networks. PDEs USED-FOR networks. training method USED-FOR DNN. robustness EVALUATE-FOR DNN. generalization gap FEATURE-OF DNN. training method USED-FOR generalization gap. method USED-FOR DNN. DNN COMPARE baseline model. baseline model COMPARE DNN. method USED-FOR DNN. generalization EVALUATE-FOR DNN. OtherScientificTerm are neural network design space, overfitting, and adversarial perturbations. Method is neural network structures. ",This paper proposes a training method for deep neural networks (DNNs) based on PDEs. The proposed method is based on the observation that the generalization gap between DNNs and ResNet-like DNN models can be reduced to a small number of perturbations. The authors show that the proposed method can be applied to any DNN with a ResNet architecture. They also show that their method is able to improve the robustness of DNN against adversarial perturbation.,This paper proposes a training method for deep neural networks (DNNs) based on PDEs. The proposed method is based on the observation that the generalization gap between DNNs and ResNet-like DNN models can be reduced to a small number of perturbations. The authors show that the proposed method can be applied to any DNN with a ResNet architecture. They also show that their method is able to improve the robustness of DNN against adversarial perturbation.
5317,SP:d44f0ebc2847695ecb4ed0bb3df61d6cd8cc6a40,deep learning models USED-FOR emergence of language. emergent language USED-FOR task. simulated agents USED-FOR emergent language. language games FEATURE-OF emergence of language. expressivity FEATURE-OF emergent languages. expressivity FEATURE-OF emergent languages. contrastive loss COMPARE referential loss. referential loss COMPARE contrastive loss. Generic is languages. Metric is complexity. OtherScientificTerm is message type collapse. ,This paper studies the emergent language problem in the context of language games. The authors show that emergent languages are more expressive than referential languages in terms of message type collapse and expressivity. They also show that contrastive loss can be used to improve the expressivity of emergent-language models. ,This paper studies the emergent language problem in the context of language games. The authors show that emergent languages are more expressive than referential languages in terms of message type collapse and expressivity. They also show that contrastive loss can be used to improve the expressivity of emergent-language models. 
5333,SP:892558b9f4fb53ed5ca2a7ee440b7d728b1886d6,"exploration - exploitation dilemma PART-OF reinforcement learning. them USED-FOR reinforcement learning setting. exploration CONJUNCTION exploitation. exploitation CONJUNCTION exploration. bandit setting FEATURE-OF Uncertainty - based exploration strategies. method USED-FOR exploration. Sample Average Uncertainty ( SAU ) USED-FOR exploration. Sample Average Uncertainty ( SAU ) USED-FOR method. exploration USED-FOR bandit problems. value predictions USED-FOR it. δ - exploration HYPONYM-OF exploration strategy. SAU USED-FOR sequential Reinforcement Learning scenario. bandits USED-FOR SAU. Deep Q - learning case EVALUATE-FOR δ - exploration. OtherScientificTerm are posterior distributions, and model posterior distributions. Task is generic sequential decision tasks. Method are reward models, and Reinforcement Learning. ","This paper studies the exploration-exploitation dilemma in reinforcement learning in the bandit setting. The authors propose a new exploration strategy, called Sample Average Uncertainty (SAU), which is based on the idea of sample-average uncertainty (SAMU). The authors show that SAU can be used to improve the performance of exploration in bandit settings. They also show that it can be applied to the Deep Q-learning setting. ","This paper studies the exploration-exploitation dilemma in reinforcement learning in the bandit setting. The authors propose a new exploration strategy, called Sample Average Uncertainty (SAU), which is based on the idea of sample-average uncertainty (SAMU). The authors show that SAU can be used to improve the performance of exploration in bandit settings. They also show that it can be applied to the Deep Q-learning setting. "
5349,SP:2f6e266b03939c96434834579999707d3268c5d6,"spatio - temporal complexity CONJUNCTION continuity of videos. continuity of videos CONJUNCTION spatio - temporal complexity. deep learning era USED-FOR long video generation. implicit neural representations ( INRs ) USED-FOR continuous signal. generative adversarial network USED-FOR video generation. motion discriminator USED-FOR unnatural motions. INR - based video generator USED-FOR motion dynamics. video extrapolation CONJUNCTION non - autoregressive video generation. non - autoregressive video generation CONJUNCTION video extrapolation. long video synthesis CONJUNCTION video extrapolation. video extrapolation CONJUNCTION long video synthesis. datasets EVALUATE-FOR DIGAN. long video synthesis HYPONYM-OF datasets. video extrapolation HYPONYM-OF datasets. UCF-101 EVALUATE-FOR FVD score. 128×128 resolution FEATURE-OF 128 frame videos. FVD score EVALUATE-FOR DIGAN. 128 frame videos USED-FOR DIGAN. UCF-101 EVALUATE-FOR DIGAN. OtherScientificTerm are video distribution, 3D grids of RGB values, scale of generated videos, continuous dynamics, INRs of video, and space and time coordinates. Method is parameterized neural network. Material is long frame sequences. ","This paper proposes a generative adversarial network (GAN) for long video generation. The proposed method is based on implicit neural representations (INRs) and is able to generate long video sequences. The method is evaluated on UCF-101, video extrapolation and long video synthesis datasets.","This paper proposes a generative adversarial network (GAN) for long video generation. The proposed method is based on implicit neural representations (INRs) and is able to generate long video sequences. The method is evaluated on UCF-101, video extrapolation and long video synthesis datasets."
5365,SP:878325384328c885ced7af0ebf31bbf79287c169,Private multi - winner voting USED-FOR revealing k - hot binary vectors. bounded differential privacy guarantee FEATURE-OF revealing k - hot binary vectors. task PART-OF machine learning literature. Binary HYPONYM-OF privacy - preserving multi - label mechanisms. Powerset voting HYPONYM-OF privacy - preserving multi - label mechanisms. composition USED-FOR Binary voting. ` 2 norm FEATURE-OF τ voting. binary vector USED-FOR Powerset voting. Powerset voting COMPARE Binary voting. Binary voting COMPARE Powerset voting. mechanisms USED-FOR privacy - preserving multi - label learning. canonical single - label technique USED-FOR mechanisms. PATE HYPONYM-OF canonical single - label technique. canonical single - label technique USED-FOR privacy - preserving multi - label learning. large real - world healthcare data CONJUNCTION multi - label benchmarks. multi - label benchmarks CONJUNCTION large real - world healthcare data. techniques COMPARE DPSGD. DPSGD COMPARE techniques. large real - world healthcare data EVALUATE-FOR DPSGD. multi - label benchmarks EVALUATE-FOR DPSGD. large real - world healthcare data EVALUATE-FOR techniques. multi - label benchmarks EVALUATE-FOR techniques. centralized setting EVALUATE-FOR techniques. mechanisms USED-FOR models. mechanisms USED-FOR multi - site ( distributed ) setting. multi - site ( distributed ) setting FEATURE-OF models. Material is healthcare. OtherScientificTerm is power set. Method is multi - label CaPC. ,"This paper studies the problem of privacy-preserving multi-label learning in multi-site (distributed) setting. The authors propose a new multi-winner voting mechanism, called DPSGD, which is based on the notion of binary voting. DPSGD is a variant of Binary Voting (BV) and Powerset Voting (PVP). DPSGD uses a composition of binary vectors and a power set. The paper shows that DPSGD achieves a bounded differential privacy guarantee. The proposed DPSGD outperforms Binary Voting and PVP in the centralized setting, and DPSGD performs better in the distributed setting.","This paper studies the problem of privacy-preserving multi-label learning in multi-site (distributed) setting. The authors propose a new multi-winner voting mechanism, called DPSGD, which is based on the notion of binary voting. DPSGD is a variant of Binary Voting (BV) and Powerset Voting (PVP). DPSGD uses a composition of binary vectors and a power set. The paper shows that DPSGD achieves a bounded differential privacy guarantee. The proposed DPSGD outperforms Binary Voting and PVP in the centralized setting, and DPSGD performs better in the distributed setting."
5381,SP:2488d3697a4ea732526b3ef11fbbd93e27d42e81,"dataset CONJUNCTION regularization. regularization CONJUNCTION dataset. architecture CONJUNCTION optimizer. optimizer CONJUNCTION architecture. batch size CONJUNCTION dataset. dataset CONJUNCTION batch size. optimizer CONJUNCTION batch size. batch size CONJUNCTION optimizer. optimizer CONJUNCTION learning rate schedule. learning rate schedule CONJUNCTION optimizer. tuned optimizer COMPARE optimizer. optimizer COMPARE tuned optimizer. grafting USED-FOR non - adaptive learning rate correction. non - adaptive learning rate correction USED-FOR SGD. non - adaptive learning rate correction USED-FOR BERT model. Method are large neural networks, optimizer grafting, optimizer hyperparameter search, and deep learning. OtherScientificTerm are implicit step size schedule, and empirical performance. Task is optimizer comparisons. ","This paper studies the problem of optimizer grafting in deep learning. In particular, the authors focus on the case of BERT, where the learning rate of the optimizer is not adaptive. The authors propose a non-adaptive learning rate correction method that can be applied to BERT. The proposed method is evaluated on several benchmark datasets.","This paper studies the problem of optimizer grafting in deep learning. In particular, the authors focus on the case of BERT, where the learning rate of the optimizer is not adaptive. The authors propose a non-adaptive learning rate correction method that can be applied to BERT. The proposed method is evaluated on several benchmark datasets."
5397,SP:83b82c145f446c1a29e863362c6ceed018e93e2b,"RL algorithms USED-FOR policy. algorithm USED-FOR online RL. algorithm USED-FOR offline demonstration data. sub - optimal behavior policy USED-FOR offline demonstration data. sparse reward settings FEATURE-OF online RL. policy improvement step CONJUNCTION policy guidance step. policy guidance step CONJUNCTION policy improvement step. offline demonstration data USED-FOR policy guidance step. LOGO USED-FOR policy. algorithm USED-FOR incomplete observation setting. censored version of the true state observation PART-OF demonstration data. sparse rewards CONJUNCTION censored state. censored state CONJUNCTION sparse rewards. algorithm COMPARE approaches. approaches COMPARE algorithm. censored state FEATURE-OF benchmark environments. sparse rewards FEATURE-OF benchmark environments. benchmark environments EVALUATE-FOR approaches. benchmark environments EVALUATE-FOR algorithm. LOGO USED-FOR obstacle avoidance. LOGO USED-FOR trajectory tracking. trajectory tracking CONJUNCTION obstacle avoidance. obstacle avoidance CONJUNCTION trajectory tracking. mobile robot USED-FOR trajectory tracking. mobile robot USED-FOR obstacle avoidance. mobile robot USED-FOR LOGO. LOGO USED-FOR approach. Task is real - world reinforcement learning ( RL ). OtherScientificTerm are sparsity of reward feedback, sparse reward function, fine grain feedback, exploration actions, feedback, guidance, sub - optimal policy, and learning episode. Material is offline data. Generic is it. ","This paper proposes a new offline RL algorithm called LOGO for offline demonstration data. The idea is to learn a sub-optimal behavior policy for offline demonstrations using a censored version of the true state observation, which is then used to guide the policy in the offline learning episode. The proposed method is evaluated on a number of benchmark environments and shows that the proposed method outperforms existing offline RL algorithms.","This paper proposes a new offline RL algorithm called LOGO for offline demonstration data. The idea is to learn a sub-optimal behavior policy for offline demonstrations using a censored version of the true state observation, which is then used to guide the policy in the offline learning episode. The proposed method is evaluated on a number of benchmark environments and shows that the proposed method outperforms existing offline RL algorithms."
5413,SP:cf857736e3dc01325948488c791cbafc24b1c0fe,"scalarization problem USED-FOR Pareto front. Linear Scalarization CONJUNCTION domain decomposition. domain decomposition CONJUNCTION Linear Scalarization. Multi - Task Learning ( MTL ) solvers USED-FOR Pareto solutions. Linear Scalarization PART-OF Multi - Task Learning ( MTL ) solvers. domain decomposition PART-OF Multi - Task Learning ( MTL ) solvers. Linear Scalarization USED-FOR Pareto solutions. MTL solvers USED-FOR real - world applications. non - convex functions CONJUNCTION constraints. constraints CONJUNCTION non - convex functions. Hybrid Neural Pareto Front ( HNPF ) USED-FOR non - convex functions. Hybrid Neural Pareto Front ( HNPF ) USED-FOR constraints. Hybrid Neural Pareto Front ( HNPF ) HYPONYM-OF two stage Pareto framework. Stage-1 neural network USED-FOR weak Pareto front. Fritz - John Conditions ( FJC ) USED-FOR Stage-1 neural network. FJC guided diffusive manifold USED-FOR weak Pareto front. low - cost Pareto filter USED-FOR strong Pareto subset. strong Pareto subset PART-OF weak front. low - cost Pareto filter USED-FOR weak front. Method is Fixed - point iterative strategies. OtherScientificTerm are convexity assumptions, Pareto definition, and convexity. Task is benchmarking and verification. Generic is approach. ",This paper proposes a two-stage Pareto front method for solving the problem of convexity. The main contribution of the paper is to introduce a new framework for the problem. The authors show that the proposed method is able to solve non-convex and convex problems in two stages. The first stage is based on the FJC guided diffusive manifold (FJC) and the second stage consists of a Stage-1 neural network and Stage-2 neural network to solve the weak front and strong front. Experiments are conducted to show the effectiveness of the proposed framework.,This paper proposes a two-stage Pareto front method for solving the problem of convexity. The main contribution of the paper is to introduce a new framework for the problem. The authors show that the proposed method is able to solve non-convex and convex problems in two stages. The first stage is based on the FJC guided diffusive manifold (FJC) and the second stage consists of a Stage-1 neural network and Stage-2 neural network to solve the weak front and strong front. Experiments are conducted to show the effectiveness of the proposed framework.
5429,SP:0085e0bb1a265a3925540fbc4873aae60b8d01ce,"library of diverse expert models COMPARE single generalist model. single generalist model COMPARE library of diverse expert models. library of diverse expert models USED-FOR task. single generalist model USED-FOR task. related task - specific teachers USED-FOR recognition tasks. related task - specific teachers USED-FOR consolidated image feature representation. student model PART-OF knowledge distillation. downstream transferability EVALUATE-FOR task - agnostic generalist model. distillation of task - specific teachers COMPARE task - agnostic generalist model. task - agnostic generalist model COMPARE distillation of task - specific teachers. teacher representations USED-FOR distillation of task - specific teachers. generalist teacher USED-FOR representations. generalist teacher PART-OF task - specific teacher(s ). unlabeled proxy dataset USED-FOR multihead, multi - task distillation method. task - specific teacher(s ) USED-FOR representations. multi - task joint training oracle EVALUATE-FOR method. Generic is library. OtherScientificTerm are teacher, and ImageNet pre - trained features. ","This paper proposes a method for knowledge distillation for multi-task joint training. The idea is to distill the knowledge of task-specific teachers into the generalist teacher and the student model for each task. The student model distills the knowledge from the student to the teacher, and the teacher distills it back to the student for the task. Experiments show that the proposed method outperforms the state-of-the-art in terms of downstream transferability. ","This paper proposes a method for knowledge distillation for multi-task joint training. The idea is to distill the knowledge of task-specific teachers into the generalist teacher and the student model for each task. The student model distills the knowledge from the student to the teacher, and the teacher distills it back to the student for the task. Experiments show that the proposed method outperforms the state-of-the-art in terms of downstream transferability. "
5445,SP:ab0d024d4060235df45182dab584c36db16d8e31,"Quantifying the data uncertainty USED-FOR learning tasks. valid coverage CONJUNCTION efficiency. efficiency CONJUNCTION valid coverage. valid coverage FEATURE-OF prediction sets. low length HYPONYM-OF efficiency. constrained empirical risk minimization ( ERM ) problem USED-FOR prediction set. empirical coverage FEATURE-OF prediction set. approximate valid population coverage CONJUNCTION near - optimal efficiency. near - optimal efficiency CONJUNCTION approximate valid population coverage. function class PART-OF conformalization step. meta - algorithm USED-FOR conformal prediction algorithms. near - optimal efficiency EVALUATE-FOR it. approximate valid population coverage EVALUATE-FOR it. it USED-FOR ERM problem. non - differentiable coverage constraint PART-OF it. differentiable surrogate losses CONJUNCTION Lagrangians. Lagrangians CONJUNCTION differentiable surrogate losses. gradient - based algorithm USED-FOR it. constrained ERM USED-FOR gradient - based algorithm. Lagrangians USED-FOR gradient - based algorithm. Lagrangians USED-FOR constrained ERM. differentiable surrogate losses USED-FOR gradient - based algorithm. differentiable surrogate losses USED-FOR constrained ERM. minimum - volume prediction sets CONJUNCTION label prediction sets. label prediction sets CONJUNCTION minimum - volume prediction sets. prediction intervals CONJUNCTION minimum - volume prediction sets. minimum - volume prediction sets CONJUNCTION prediction intervals. algorithm COMPARE approaches. approaches COMPARE algorithm. label prediction sets USED-FOR image classification. efficiency EVALUATE-FOR approaches. minimum - volume prediction sets USED-FOR multi - output regression. algorithm USED-FOR applications. approaches USED-FOR applications. image classification HYPONYM-OF applications. efficiency EVALUATE-FOR algorithm. prediction intervals HYPONYM-OF applications. minimum - volume prediction sets HYPONYM-OF applications. label prediction sets HYPONYM-OF applications. OtherScientificTerm is prediction interval. Method are Conformal prediction, and conformal prediction. ","This paper proposes a meta-algorithm for conformal prediction. The proposed algorithm is based on the idea of constrained empirical risk minimization (ERM), which is an important problem in the context of conformal learning. The authors show that the proposed algorithm achieves near-optimal efficiency and near-optimality in terms of the empirical coverage of the prediction set. They also show that their algorithm can be combined with differentiable surrogate losses and Lagrangians to obtain a gradient-based algorithm. The paper also shows that their method can be applied to a variety of applications such as multi-output regression and label prediction. ","This paper proposes a meta-algorithm for conformal prediction. The proposed algorithm is based on the idea of constrained empirical risk minimization (ERM), which is an important problem in the context of conformal learning. The authors show that the proposed algorithm achieves near-optimal efficiency and near-optimality in terms of the empirical coverage of the prediction set. They also show that their algorithm can be combined with differentiable surrogate losses and Lagrangians to obtain a gradient-based algorithm. The paper also shows that their method can be applied to a variety of applications such as multi-output regression and label prediction. "
5461,SP:f6b88e1fa1a84d82302d960c6a596fc2ba320bf5,"reinforcement learning based approach USED-FOR query object localization. ordinal metric learning USED-FOR exemplary set. exemplary set USED-FOR transferable reward signal. ordinal metric learning USED-FOR transferable reward signal. method COMPARE fine - tuning approaches. fine - tuning approaches COMPARE method. method USED-FOR test - time policy adaptation. annotated images USED-FOR fine - tuning approaches. corrupted MNIST CONJUNCTION CU - Birds. CU - Birds CONJUNCTION corrupted MNIST. CU - Birds CONJUNCTION COCO datasets. COCO datasets CONJUNCTION CU - Birds. COCO datasets EVALUATE-FOR approach. corrupted MNIST EVALUATE-FOR approach. OtherScientificTerm are reward signals, and transferable reward. ","This paper proposes a new method for query object localization based on reinforcement learning. The proposed method is based on the notion of transferable reward, which is defined as the difference between the reward of the query object and the transferable rewards of the target query object. The authors show that the proposed method outperforms existing methods on corrupted MNIST, corrupted CU-Birds, and corrupted COCO datasets. ","This paper proposes a new method for query object localization based on reinforcement learning. The proposed method is based on the notion of transferable reward, which is defined as the difference between the reward of the query object and the transferable rewards of the target query object. The authors show that the proposed method outperforms existing methods on corrupted MNIST, corrupted CU-Birds, and corrupted COCO datasets. "
5477,SP:0b23c5683b72dac05a7436cf3b49bd76263801d9,"Transformers USED-FOR vision tasks. feature matching CONJUNCTION stereo. stereo CONJUNCTION feature matching. object detection CONJUNCTION feature matching. feature matching CONJUNCTION object detection. them USED-FOR vision tasks. dense predictions USED-FOR vision tasks. object detection HYPONYM-OF vision tasks. feature matching HYPONYM-OF vision tasks. quadtree transformer USED-FOR attention. token pyramids USED-FOR quadtree transformer. flops reduction EVALUATE-FOR stereo matching. quadtree attention USED-FOR vision tasks. top-1 accuracy EVALUATE-FOR ImageNet classification. ScanNet USED-FOR feature matching. feature matching EVALUATE-FOR quadtree attention. flops reduction EVALUATE-FOR quadtree attention. feature matching HYPONYM-OF vision tasks. Metric are quadratic computational complexity, and computational complexity. Method is QuadTree Attention. OtherScientificTerm is attention scores. Task is COCO object detection. ",This paper proposes a quadratic transformer for vision tasks. The proposed method is based on token pyramids. The authors show that the proposed method can reduce the computational complexity of vision tasks by up to 1.5x. They also show that their method is able to achieve top-1 accuracy on COCO object detection and feature matching tasks.,This paper proposes a quadratic transformer for vision tasks. The proposed method is based on token pyramids. The authors show that the proposed method can reduce the computational complexity of vision tasks by up to 1.5x. They also show that their method is able to achieve top-1 accuracy on COCO object detection and feature matching tasks.
5493,SP:63bcbaf0c5644aaba863cf60fa10db763f382ee8,"reusable options USED-FOR unknown task distribution. options USED-FOR transfer learning. options CONJUNCTION state transitions. state transitions CONJUNCTION options. MI USED-FOR method. scalable approximation USED-FOR MI maximization. scalable approximation USED-FOR InfoMax Termination Critic ( IMTC ) algorithm. gradient ascent USED-FOR scalable approximation. gradient ascent USED-FOR MI maximization. extrinsic rewards CONJUNCTION intrinsic rewards. intrinsic rewards CONJUNCTION extrinsic rewards. IMTC USED-FOR diversity of learned options. IMTC USED-FOR quick adaptation. IMTC USED-FOR complex domains. Method are reinforcement learning, and mutual information ( MI ) based skill learning. OtherScientificTerm is reusable building blocks. Task is learning reusable options. ","This paper proposes InfoMax Termination Critic (IMTC), a method for learning reusable options for transfer learning in reinforcement learning. The idea is to learn a set of reusable options that can be used to transfer from one task to another. The authors propose a scalable approximation of the InfoMax termination criterion for MI-based skill learning. They show that the proposed method is able to achieve MI-maximization in the presence of a large number of options. They also show that their method can be applied to a variety of tasks.","This paper proposes InfoMax Termination Critic (IMTC), a method for learning reusable options for transfer learning in reinforcement learning. The idea is to learn a set of reusable options that can be used to transfer from one task to another. The authors propose a scalable approximation of the InfoMax termination criterion for MI-based skill learning. They show that the proposed method is able to achieve MI-maximization in the presence of a large number of options. They also show that their method can be applied to a variety of tasks."
5509,SP:79da8f6cacc8386e02bab32154e7eaefbe2c683c,"Open - World Object Detection HYPONYM-OF object detection paradigm. openworld object detector USED-FOR unknown objects. independent modules USED-FOR unknown categories. independent modules USED-FOR incremental learning. node PART-OF semantic topology. discriminative feature representations USED-FOR constraint. semantic topology COMPARE open - world object detectors. open - world object detectors COMPARE semantic topology. semantic topology USED-FOR open - world object detection. well - trained language model USED-FOR semantic topology. OtherScientificTerm are Semantic Topology, and features. Method are open - world object detector, and detector. Metric is absolute open - set error. ","This paper proposes a new open-world object detection framework, called Open-World Object Detection (OWOD), which is based on semantic topology. OOD is an open-set object detection paradigm where the open set is the set of objects that are known to the detector, and the detector is trained to detect unknown objects. The paper proposes to use a discriminative feature representation to learn the discriminator, which is then used to train the detector. The proposed method is evaluated on a number of benchmark datasets and shows that the proposed method outperforms the baselines.","This paper proposes a new open-world object detection framework, called Open-World Object Detection (OWOD), which is based on semantic topology. OOD is an open-set object detection paradigm where the open set is the set of objects that are known to the detector, and the detector is trained to detect unknown objects. The paper proposes to use a discriminative feature representation to learn the discriminator, which is then used to train the detector. The proposed method is evaluated on a number of benchmark datasets and shows that the proposed method outperforms the baselines."
5525,SP:97f618558f4add834e5930fd177f012a753247dc,Deep learning USED-FOR vision and natural language processing. computation CONJUNCTION human labeling effort. human labeling effort CONJUNCTION computation. deep learning models COMPARE ones. ones COMPARE deep learning models. dataset USED-FOR ones. Prior methods USED-FOR submodular objective functions. predicted class labels CONJUNCTION decision boundaries. decision boundaries CONJUNCTION predicted class labels. balancing constraints FEATURE-OF predicted class labels. balancing constraints FEATURE-OF decision boundaries. matroids HYPONYM-OF algebraic structure. algebraic structure USED-FOR linear independence. vector spaces FEATURE-OF linear independence. constant approximation guarantees FEATURE-OF greedy algorithm. matroids USED-FOR constraints. CIFAR-100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-100. CIFAR10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR10. ImageNet CONJUNCTION long - tailed datasets. long - tailed datasets CONJUNCTION ImageNet. long - tailed datasets HYPONYM-OF classification datasets. CIFAR-100 - LT HYPONYM-OF long - tailed datasets. long - tailed datasets EVALUATE-FOR baselines. ImageNet HYPONYM-OF classification datasets. CIFAR10 HYPONYM-OF classification datasets. classification datasets EVALUATE-FOR baselines. CIFAR-100 HYPONYM-OF classification datasets. Generic is models. ,This paper proposes a greedy algorithm for learning submodular objective functions for deep learning models. The main idea is to use matroids as the objective function for the submodularity of the learned objective function. The authors show that the proposed algorithm is guaranteed to satisfy constant approximation guarantees. They also show that their algorithm can be applied to long-tailed classification tasks. ,This paper proposes a greedy algorithm for learning submodular objective functions for deep learning models. The main idea is to use matroids as the objective function for the submodularity of the learned objective function. The authors show that the proposed algorithm is guaranteed to satisfy constant approximation guarantees. They also show that their algorithm can be applied to long-tailed classification tasks. 
5541,SP:e0432ff922708c6c6e59124d27c1386605930346,"models USED-FOR semantic segmentation. adaptive inference strategy USED-FOR semantic segmentation. Instance - adaptive Batch Normalization ( IaBN ) USED-FOR normalization layers. feature statistics USED-FOR normalization layers. test - time training ( TTT ) approach USED-FOR semantic segmentation. Seg - TTT HYPONYM-OF test - time training ( TTT ) approach. self - supervised loss USED-FOR Seg - TTT. self - supervised loss USED-FOR model parameters. techniques COMPARE baseline. baseline COMPARE techniques. accuracy EVALUATE-FOR generalization methods. techniques COMPARE generalization methods. generalization methods COMPARE techniques. generalization USED-FOR semantic segmentation. accuracy EVALUATE-FOR techniques. Metric is Out - of - distribution robustness. Generic are model, and complementary techniques. ","This paper proposes a test-time training (TTT) approach for semantic segmentation. The proposed approach is based on Instance-Adaptive Batch Normalization (IaBN), which is an adaptive inference strategy that uses a self-supervised loss to improve the robustness of the model parameters to out-of-distribution (OOD) samples. The authors show that the proposed approach outperforms baselines in terms of OOD accuracy and generalization performance. ","This paper proposes a test-time training (TTT) approach for semantic segmentation. The proposed approach is based on Instance-Adaptive Batch Normalization (IaBN), which is an adaptive inference strategy that uses a self-supervised loss to improve the robustness of the model parameters to out-of-distribution (OOD) samples. The authors show that the proposed approach outperforms baselines in terms of OOD accuracy and generalization performance. "
5557,SP:427100edad574722a6525ca917e84f817ff60d7e,contrastive loss USED-FOR mappings. mappings USED-FOR contrastive loss. Material is tabular data. Generic is method. Method is default set rule of hyperparameters selection. ,"This paper studies the problem of choosing a set of hyperparameters for tabular data. The authors propose to use contrastive loss as the default set rule for hyperparameter selection in tabular learning. The proposed method is based on the idea of contrastive mapping, which is an extension of the contrastive learning framework. The main contribution of the paper is to propose a new method for selecting hyperparametrized hyperparamets. The method is evaluated on a variety of tabular datasets, and the results show that the proposed method outperforms the baselines. ","This paper studies the problem of choosing a set of hyperparameters for tabular data. The authors propose to use contrastive loss as the default set rule for hyperparameter selection in tabular learning. The proposed method is based on the idea of contrastive mapping, which is an extension of the contrastive learning framework. The main contribution of the paper is to propose a new method for selecting hyperparametrized hyperparamets. The method is evaluated on a variety of tabular datasets, and the results show that the proposed method outperforms the baselines. "
5573,SP:7782a99e3c41ff523c0c56bfbe399c855a77acf2,dimensional approach USED-FOR psychiatric classification. mining corresponded nosological relations USED-FOR low - dimensional embedding space. diagnostic information USED-FOR optimal embedding space. dual utilisation of diagnostic information PART-OF conditional variational auto - encoder. major depressive disorder CONJUNCTION schizophrenia. schizophrenia CONJUNCTION major depressive disorder. autism spectrum disorder CONJUNCTION major depressive disorder. major depressive disorder CONJUNCTION autism spectrum disorder. approaches USED-FOR synthetic functional connectivity features. autism spectrum disorder HYPONYM-OF nosological relation. major depressive disorder HYPONYM-OF nosological relation. empirical neuropsychiatric neuroimaging datasets EVALUATE-FOR approach. OtherScientificTerm is neuropsychiatric disorders. ,"This paper proposes a new framework for neuropsychiatric classification based on the notion of ""nosological relations"". The authors propose a conditional variational auto-encoder (CVAE) model that learns to learn a low-dimensional embedding space for a set of diagnostic information, which is then used to learn the corresponding functional connectivity features. The authors show that the proposed model is able to achieve state-of-the-art performance on a wide range of datasets. ","This paper proposes a new framework for neuropsychiatric classification based on the notion of ""nosological relations"". The authors propose a conditional variational auto-encoder (CVAE) model that learns to learn a low-dimensional embedding space for a set of diagnostic information, which is then used to learn the corresponding functional connectivity features. The authors show that the proposed model is able to achieve state-of-the-art performance on a wide range of datasets. "
5589,SP:b3feb15b01e519e5b2e28b1c4a144056c493e2bc,quantum neural networks USED-FOR quantum learning tasks. trainable quantum tensor network ( QTN ) USED-FOR quantum embedding. variational quantum circuit ( VQC ) USED-FOR quantum embedding. QTN - VQC HYPONYM-OF end - to - end learning framework. parametric tensor - train network CONJUNCTION tensor product encoding. tensor product encoding CONJUNCTION parametric tensor - train network. tensor product encoding USED-FOR quantum embedding. parametric tensor - train network USED-FOR feature extraction. architecture USED-FOR QTN. parametric tensor - train network PART-OF architecture. parametric tensor - train network PART-OF QTN. tensor product encoding PART-OF QTN. tensor product encoding PART-OF architecture. QTN USED-FOR quantum embedding. QTN USED-FOR end - to - end parametric model pipeline. QTN - VQC HYPONYM-OF end - to - end parametric model pipeline. QTN USED-FOR quantum embedding. MNIST dataset EVALUATE-FOR QTN. QTN COMPARE quantum embedding approaches. quantum embedding approaches COMPARE QTN. ,This paper proposes a novel end-to-end learning framework for quantum embedding. The authors propose a trainable quantum tensor network (QTN) that combines tensor-train and tensor product encoding. The QTN-VQC framework is based on the variational quantum circuit (VQQC) framework. The proposed method is evaluated on MNIST and CIFAR-10.,This paper proposes a novel end-to-end learning framework for quantum embedding. The authors propose a trainable quantum tensor network (QTN) that combines tensor-train and tensor product encoding. The QTN-VQC framework is based on the variational quantum circuit (VQQC) framework. The proposed method is evaluated on MNIST and CIFAR-10.
5605,SP:c4b03a1b477ac94438d63beb29ef86d77acf1b1e,"networks USED-FOR task. networks USED-FOR algorithms. neural networks USED-FOR high - level computational processes. algorithm USED-FOR low - dimensional manifolds. meta - model USED-FOR hidden states. meta - model USED-FOR DYNAMO. model PART-OF meta - model. pre - trained neural networks USED-FOR DYNAMO. model embedding vectors USED-FOR manifold. model embedding vector USED-FOR model. RNNs CONJUNCTION CNNs. CNNs CONJUNCTION RNNs. DYNAMO USED-FOR RNNs. DYNAMO USED-FOR CNNs. model embedding spaces USED-FOR applications. neural networks USED-FOR operable neural network. model embedding spaces USED-FOR clustering of neural networks. neural networks USED-FOR task. high - level computational processes USED-FOR clustering of neural networks. semi - supervised learning HYPONYM-OF applications. clustering of neural networks HYPONYM-OF applications. optimization USED-FOR semi - supervised learning. high - level computational processes FEATURE-OF topology of RNN dynamics. RNNs USED-FOR fixed - point analysis of meta - models. Method are deep neural networks, and neural network model. Generic is models. OtherScientificTerm are reparameterization, and model embedding space. ","This paper studies the problem of clustering of neural networks on low-dimensional manifolds. The authors propose a new algorithm called DYNAMO, which is a meta-learning algorithm that learns a model embedding vector for each layer of a neural network. The proposed algorithm is based on the idea of meta-training, where the model is trained on a set of pre-trained neural networks, and the meta-model is used to learn the hidden states of the network.  The authors show that the proposed algorithm can be applied to RNNs, CNNs, and meta-models. They also show that it can be used for semi-supervised learning. ","This paper studies the problem of clustering of neural networks on low-dimensional manifolds. The authors propose a new algorithm called DYNAMO, which is a meta-learning algorithm that learns a model embedding vector for each layer of a neural network. The proposed algorithm is based on the idea of meta-training, where the model is trained on a set of pre-trained neural networks, and the meta-model is used to learn the hidden states of the network.  The authors show that the proposed algorithm can be applied to RNNs, CNNs, and meta-models. They also show that it can be used for semi-supervised learning. "
5621,SP:29a42fdae15b9da955513f71e3100ebd0146a28a,"constraint - based approach COMPARE direct prediction. direct prediction COMPARE constraint - based approach. direct prediction USED-FOR simulation engines. constraint - based approach USED-FOR simulation engines. framework USED-FOR constraint - based learned simulation. trainable function approximator USED-FOR scalar constraint function. gradient descent USED-FOR constraint solver. graph neural network USED-FOR constraint function. graph neural network CONJUNCTION gradient descent. gradient descent CONJUNCTION graph neural network. gradient descent USED-FOR method. graph neural network USED-FOR method. backpropagation USED-FOR architecture. colliding irregular shapes CONJUNCTION splashing fluids. splashing fluids CONJUNCTION colliding irregular shapes. bouncing balls CONJUNCTION colliding irregular shapes. colliding irregular shapes CONJUNCTION bouncing balls. simulated ropes CONJUNCTION bouncing balls. bouncing balls CONJUNCTION simulated ropes. physical domains EVALUATE-FOR model. colliding irregular shapes HYPONYM-OF physical domains. splashing fluids HYPONYM-OF physical domains. simulated ropes HYPONYM-OF physical domains. bouncing balls HYPONYM-OF physical domains. model COMPARE simulators. simulators COMPARE model. forward learned simulators USED-FOR constraint - based framework. numerical methods USED-FOR learned models. Method are physical simulators, forward model, and forward approaches. Task is constraint satisfaction problem. Metric is simulation accuracy. OtherScientificTerm is hand - designed constraints. ","This paper proposes a new method for constraint-based learned simulation. The method is based on graph neural networks and gradient descent. The authors propose to use gradient descent to solve the constraint satisfaction problem, which is a well-studied problem in the physical simulation community. The main contribution of this paper is the use of gradient descent and graph neural network to solve this problem. The proposed method is evaluated on a variety of physical simulation tasks, including splashing fluids, bouncing balls, and colliding irregular shapes. ","This paper proposes a new method for constraint-based learned simulation. The method is based on graph neural networks and gradient descent. The authors propose to use gradient descent to solve the constraint satisfaction problem, which is a well-studied problem in the physical simulation community. The main contribution of this paper is the use of gradient descent and graph neural network to solve this problem. The proposed method is evaluated on a variety of physical simulation tasks, including splashing fluids, bouncing balls, and colliding irregular shapes. "
5637,SP:db07c2c0afdf27692dc504c9c54387c20211d469,"exploration CONJUNCTION few - shot adaptation. few - shot adaptation CONJUNCTION exploration. policies USED-FOR exploration. diverse behaviors FEATURE-OF policies. controlling robots HYPONYM-OF real - world scenarios. iterative reproduction CONJUNCTION selection of policies. selection of policies CONJUNCTION iterative reproduction. iterative reproduction PART-OF evolutionary techniques. selection of policies USED-FOR methods. iterative reproduction USED-FOR methods. evolutionary techniques USED-FOR methods. EDO - CS HYPONYM-OF Evolutionary Diversity Optimization algorithm. Clusteringbased Selection USED-FOR Evolutionary Diversity Optimization algorithm. Clusteringbased Selection USED-FOR EDO - CS. EDO - CS COMPARE methods. methods COMPARE EDO - CS. EDO - CS USED-FOR policies. continuous control tasks EVALUATE-FOR EDO - CS. EDO - CS COMPARE EDO - CS. EDO - CS COMPARE EDO - CS. Method is Reinforcement Learning ( RL ). Generic is task. OtherScientificTerm are selection mechanisms, and clusters. Task are reproduction, and reproduction process. ",This paper proposes an evolutionary diversity optimization algorithm for reinforcement learning. The main idea is to use a clustering-based selection mechanism to optimize the diversity of policies. The authors show that the proposed method outperforms existing methods on a number of continuous control tasks. ,This paper proposes an evolutionary diversity optimization algorithm for reinforcement learning. The main idea is to use a clustering-based selection mechanism to optimize the diversity of policies. The authors show that the proposed method outperforms existing methods on a number of continuous control tasks. 
5653,SP:e51123a76713f1a1031d252e092985bd9b298fdf,"Markovian noise CONJUNCTION general consensus - type interaction. general consensus - type interaction CONJUNCTION Markovian noise. general consensus - type interaction USED-FOR multi - agent linear stochastic approximation algorithm. Markovian noise USED-FOR multi - agent linear stochastic approximation algorithm. time - varying directed graph USED-FOR interconnection structure. doubly stochastic matrices USED-FOR interconnection. finite - time bounds USED-FOR mean - square error. interaction matrices FEATURE-OF uniformly strongly connected graph sequences. stochastic matrices USED-FOR consensus - type algorithm. OtherScientificTerm are local stochastic approximation process, interconnection matrix, ordinary differential equation, interconnection matrices, local equilibria, communication, constant and time - varying step - sizes, and convex combination. Method are consensus - based stochastic approximation algorithms, and push - type distributed stochastic approximation algorithm. Generic is algorithm. ","This paper considers the problem of learning a distributed stochastic approximation algorithm for a time-varying directed graph. The authors propose a new algorithm for this problem. The algorithm is based on the general consensus-type interaction between two agents, and the authors show that the convergence of the algorithm is bounded by the mean-square error of the local equilibria of the two agents.  The authors also provide a finite-time bound for the convergence. ","This paper considers the problem of learning a distributed stochastic approximation algorithm for a time-varying directed graph. The authors propose a new algorithm for this problem. The algorithm is based on the general consensus-type interaction between two agents, and the authors show that the convergence of the algorithm is bounded by the mean-square error of the local equilibria of the two agents.  The authors also provide a finite-time bound for the convergence. "
5669,SP:f7f96d545a907887396393aba310974f4d3f75ff,"ones HYPONYM-OF methods. equivariant Graph Neural Networks ( GNNs ) USED-FOR methods. equivariant Graph Neural Networks ( GNNs ) USED-FOR ones. forward kinematics information FEATURE-OF structural object. it USED-FOR forward kinematics information. generalized coordinates USED-FOR forward kinematics information. generalized coordinates USED-FOR it. forward kinematics USED-FOR geometrical constraints. dynamics of constrained systems COMPARE unconstrained counterparts. unconstrained counterparts COMPARE dynamics of constrained systems. equivariant message passing USED-FOR GMN. orthogonality - equivariant functions USED-FOR equivariant message passing. molecular dynamics prediction CONJUNCTION human motion capture. human motion capture CONJUNCTION molecular dynamics prediction. sticks CONJUNCTION hinges. hinges CONJUNCTION sticks. particles CONJUNCTION sticks. sticks CONJUNCTION particles. prediction accuracy CONJUNCTION constraint satisfaction. constraint satisfaction CONJUNCTION prediction accuracy. GMN COMPARE GNNs. GNNs COMPARE GMN. constraint satisfaction CONJUNCTION data efficiency. data efficiency CONJUNCTION constraint satisfaction. real - world datasets USED-FOR molecular dynamics prediction. real - world datasets USED-FOR human motion capture. particles PART-OF simulated systems. sticks PART-OF simulated systems. simulated systems EVALUATE-FOR GMN. constraint satisfaction EVALUATE-FOR GMN. constraint satisfaction EVALUATE-FOR GNNs. data efficiency EVALUATE-FOR GMN. data efficiency EVALUATE-FOR GNNs. prediction accuracy EVALUATE-FOR GNNs. prediction accuracy EVALUATE-FOR GMN. Task are machine learning, and interacting systems. OtherScientificTerm is constrained systems. Method are Graph Mechanics Network ( GMN ), and equivariant formulation. ","This paper proposes an equivariant graph neural network (GNN) based on orthogonality-equivariant message passing (EQNN) to solve the problem of learning the dynamics of constrained systems. The proposed method is based on the idea that the forward kinematics information of a structural object can be represented as a generalized coordinate system, which is then used as a message passing function. The authors show that the proposed method outperforms existing GNNs in terms of prediction accuracy, constraint satisfaction, and data efficiency. ","This paper proposes an equivariant graph neural network (GNN) based on orthogonality-equivariant message passing (EQNN) to solve the problem of learning the dynamics of constrained systems. The proposed method is based on the idea that the forward kinematics information of a structural object can be represented as a generalized coordinate system, which is then used as a message passing function. The authors show that the proposed method outperforms existing GNNs in terms of prediction accuracy, constraint satisfaction, and data efficiency. "
5685,SP:ee0b94238c3fde59cb8b67a687b77984fe7d3454,"framework USED-FOR federated learning. partial model personalization USED-FOR federated learning. partial model personalization USED-FOR framework. full model personalization COMPARE partial model personalization. partial model personalization COMPARE full model personalization. domain knowledge USED-FOR model. domain knowledge USED-FOR partial model personalization. federated optimization algorithms USED-FOR partially personalized models. them USED-FOR deep learning models. algorithms USED-FOR minimizing smooth nonconvex functions. alternating update algorithm COMPARE simultaneous update algorithm. simultaneous update algorithm COMPARE alternating update algorithm. partial model personalization USED-FOR full model personalization. personalized parameters USED-FOR partial model personalization. personalized parameters USED-FOR full model personalization. OtherScientificTerm are on - device memory footprint, shared and personal parameters, shared parameters, and smooth nonconvex functions. Material is real - world image and text datasets. ","This paper proposes a framework for federated learning with partial model personalization. The proposed framework is based on the idea that the shared and personal parameters can be minimized by minimizing smooth nonconvex functions. The authors show that the proposed framework can be applied to a variety of federated optimization algorithms, including alternating update and simultaneous update. They also provide a theoretical analysis of the performance of the proposed method.","This paper proposes a framework for federated learning with partial model personalization. The proposed framework is based on the idea that the shared and personal parameters can be minimized by minimizing smooth nonconvex functions. The authors show that the proposed framework can be applied to a variety of federated optimization algorithms, including alternating update and simultaneous update. They also provide a theoretical analysis of the performance of the proposed method."
5701,SP:eb54e84275266d8909fcbfe1589da1c4396c3164,"contrastive learning HYPONYM-OF unsupervised representation learning. contrastive learning PART-OF machine learning. augmentation operations USED-FOR representations. framework USED-FOR unsupervised learning of object representations. CLTT USED-FOR viewing sequences. CLTT USED-FOR supervised learning. ThreeDWorld ( TDW ) USED-FOR near - photorealistic training environment. ThreeDWorld ( TDW ) USED-FOR data set. near - photorealistic training environment USED-FOR data set. CLTT USED-FOR linear classification. OtherScientificTerm are supervision, positive pairs, temporal structure, and representational similarity. Material are biology, and natural videos. Method are object representations, Contrastive Learning Through Time ( CLTT ), contrastive learning methods, and fully supervised setting. Generic is data sets. ","This paper proposes CLTT, a framework for unsupervised learning of object representations. The proposed method is based on contrastive learning through time (CLTT), which is an extension of Contrastive Learning Through Time (CLT). CLTT leverages the temporal structure of video sequences to learn the representation of an object. The method is evaluated on a synthetic dataset and a real-world dataset. The results show that CLTT outperforms CLT in terms of linear classification and linear classification accuracy.","This paper proposes CLTT, a framework for unsupervised learning of object representations. The proposed method is based on contrastive learning through time (CLTT), which is an extension of Contrastive Learning Through Time (CLT). CLTT leverages the temporal structure of video sequences to learn the representation of an object. The method is evaluated on a synthetic dataset and a real-world dataset. The results show that CLTT outperforms CLT in terms of linear classification and linear classification accuracy."
5717,SP:2fb4af247b5022710b681037faca2420207a507a,deterministic transition model USED-FOR goal - directed planning. Monte Carlo Tree Search USED-FOR deterministic control problems. function approximators USED-FOR MCTS. MCTS USED-FOR continuous domains. MCTS USED-FOR AlphaZero family of algorithms. function approximators USED-FOR tree. algorithms USED-FOR control problems. sparse rewards FEATURE-OF control problems. goal - directed domains HYPONYM-OF sparse rewards. AlphaZero USED-FOR goal - directed planning tasks. Hindsight Experience Replay USED-FOR AlphaZero. application USED-FOR quantum compiling domain. application HYPONYM-OF simulated domains. quantum compiling domain HYPONYM-OF simulated domains. simulated domains EVALUATE-FOR approach. OtherScientificTerm is positive reward. ,"This paper proposes a new algorithm for goal-directed planning based on Monte Carlo Tree Search (MCTS). The proposed algorithm, AlphaZero, is based on the MCTS family of algorithms. The main contribution of this paper is to introduce a new family of algorithm for the goal-driven planning problem. The algorithm is based upon the idea of Hindsight Experience Replay (HRE), which is an extension of the Hindsight experience replay (HVR) algorithm. The authors show that the proposed algorithm outperforms the existing algorithms in a number of tasks. ","This paper proposes a new algorithm for goal-directed planning based on Monte Carlo Tree Search (MCTS). The proposed algorithm, AlphaZero, is based on the MCTS family of algorithms. The main contribution of this paper is to introduce a new family of algorithm for the goal-driven planning problem. The algorithm is based upon the idea of Hindsight Experience Replay (HRE), which is an extension of the Hindsight experience replay (HVR) algorithm. The authors show that the proposed algorithm outperforms the existing algorithms in a number of tasks. "
5733,SP:e2d33c7331db7f52b84ad1018152564d91a9f126,"network capacity CONJUNCTION data replay. data replay CONJUNCTION network capacity. network capacity USED-FOR methods. data replay USED-FOR methods. virtual Feature Encoding Layer ( FEL ) USED-FOR network structures. iteratively updated optimizer CONJUNCTION virtual Feature Encoding Layer ( FEL ). virtual Feature Encoding Layer ( FEL ) CONJUNCTION iteratively updated optimizer. iteratively updated optimizer USED-FOR gradient. task descriptors USED-FOR virtual Feature Encoding Layer ( FEL ). iteratively updated optimizer PART-OF RGO. virtual Feature Encoding Layer ( FEL ) PART-OF RGO. task descriptors USED-FOR network structures. 20 - split - CIFAR100 CONJUNCTION 20 - split - miniImageNet. 20 - split - miniImageNet CONJUNCTION 20 - split - CIFAR100. RGO COMPARE baselines. baselines COMPARE RGO. continual classification benchmarks EVALUATE-FOR baselines. 20 - split - miniImageNet EVALUATE-FOR RGO. 20 - split - CIFAR100 EVALUATE-FOR RGO. continual classification benchmarks EVALUATE-FOR RGO. method USED-FOR continual learning capabilities. average accuracy EVALUATE-FOR Single - Task Learning ( STL ). Single - Task Learning ( STL ) COMPARE method. method COMPARE Single - Task Learning ( STL ). average accuracy EVALUATE-FOR method. continual learning capabilities USED-FOR learning models. gradient descent USED-FOR learning models. Method are Continual Learning ( CL ), neural networks, and Recursive Gradient Optimization ( RGO ). Generic is approach. ",This paper proposes a method for continual learning (CL) based on the Recursive Gradient Optimization (RGO) framework. The authors propose to use a virtual feature encoding layer (FEL) and an iteratively updated optimizer to improve the performance of the learning model. The proposed method is evaluated on a variety of continual learning benchmarks and shows that the proposed method outperforms the baselines. ,This paper proposes a method for continual learning (CL) based on the Recursive Gradient Optimization (RGO) framework. The authors propose to use a virtual feature encoding layer (FEL) and an iteratively updated optimizer to improve the performance of the learning model. The proposed method is evaluated on a variety of continual learning benchmarks and shows that the proposed method outperforms the baselines. 
5749,SP:511226b467019dcd85e9ebf8b9b56f8f1b3ef889,"fine - tuning USED-FOR them. aligned StyleGAN models USED-FOR image - to - image translation. human faces CONJUNCTION churches. churches CONJUNCTION human faces. latent spaces FEATURE-OF child model. churches HYPONYM-OF distant data domains. human faces HYPONYM-OF distant data domains. aligned models USED-FOR tasks. image translation CONJUNCTION automatic cross - domain image morphing. automatic cross - domain image morphing CONJUNCTION image translation. child domain USED-FOR zero - shot vision tasks. fine - tuning CONJUNCTION inversion. inversion CONJUNCTION fine - tuning. fine - tuning USED-FOR approach. inversion USED-FOR approach. Method are aligned generative models, and StyleGAN. Generic are models, and they. OtherScientificTerm are architecture, and supervision. Task are transfer learning, and model alignment. ",This paper proposes a method for learning aligned generative models for image-to-image translation and zero-shot vision tasks. The proposed method is based on the idea of inversion and fine-tuning. The method is evaluated on a number of tasks and is shown to outperform the baselines. ,This paper proposes a method for learning aligned generative models for image-to-image translation and zero-shot vision tasks. The proposed method is based on the idea of inversion and fine-tuning. The method is evaluated on a number of tasks and is shown to outperform the baselines. 
5765,SP:0e13f831c211626195c118487f2fff36a6e293f6,Comparing structured objects PART-OF learning tasks. graphs HYPONYM-OF Comparing structured objects. Optimal Transport ( OT ) USED-FOR Gromov - Wasserstein ( GW ) distance. nodes connectivity relations USED-FOR GW. GW USED-FOR graphs. probability measures USED-FOR GW. conservation of mass USED-FOR OT. graph dictionary CONJUNCTION partition learning. partition learning CONJUNCTION graph dictionary. semi - relaxed Gromov - Wasserstein divergence USED-FOR it. partition learning HYPONYM-OF tasks. graph dictionary HYPONYM-OF tasks. it USED-FOR graph dictionary learning algorithm. partitioning CONJUNCTION clustering. clustering CONJUNCTION partitioning. clustering CONJUNCTION completion. completion CONJUNCTION clustering. graphs USED-FOR complex tasks. completion HYPONYM-OF complex tasks. completion HYPONYM-OF graphs. partitioning HYPONYM-OF graphs. partitioning HYPONYM-OF complex tasks. clustering HYPONYM-OF graphs. clustering HYPONYM-OF complex tasks. OtherScientificTerm is nodes. ,"This paper studies the Gromov-Wasserstein (GW) distance between two nodes in a graph. The authors propose Optimal Transport (OT) which is a semi-relaxed version of the GW distance between nodes in the graph. OT is based on the conservation of mass of the two nodes, and the authors show that OT can be used to learn a graph dictionary for graph dictionary learning and partitioning tasks. They also provide a theoretical analysis of OT.","This paper studies the Gromov-Wasserstein (GW) distance between two nodes in a graph. The authors propose Optimal Transport (OT) which is a semi-relaxed version of the GW distance between nodes in the graph. OT is based on the conservation of mass of the two nodes, and the authors show that OT can be used to learn a graph dictionary for graph dictionary learning and partitioning tasks. They also provide a theoretical analysis of OT."
5781,SP:d6d144be11230070ae9395db70b7c7743540bad4,"prediction CONJUNCTION collaboration. collaboration CONJUNCTION prediction. Models USED-FOR prediction. Models USED-FOR collaboration. ones HYPONYM-OF categories. ones HYPONYM-OF categories. imitation learning USED-FOR ones. predicting policies USED-FOR systematic suboptimality. Bayesian inference USED-FOR systematic deviations. Boltzmann policy distribution ( BPD ) USED-FOR human policies. sampling CONJUNCTION inference. inference CONJUNCTION sampling. generative and sequence models USED-FOR sampling. generative and sequence models USED-FOR inference. high - dimensional continuous space FEATURE-OF policies. BPD USED-FOR prediction of human behavior. prediction of human behavior CONJUNCTION human - AI collaboration. human - AI collaboration CONJUNCTION prediction of human behavior. BPD USED-FOR human - AI collaboration. human - AI collaboration CONJUNCTION imitation learning - based human models. imitation learning - based human models CONJUNCTION human - AI collaboration. prediction of human behavior CONJUNCTION imitation learning - based human models. imitation learning - based human models CONJUNCTION prediction of human behavior. BPD USED-FOR imitation learning - based human models. OtherScientificTerm are human behavior, reward function, Boltzmann rationality, and trajectories. Generic are former, and models. Material is human data. ",This paper studies the problem of learning a Boltzmann policy distribution (BPD) for human policies in a continuous space. The authors propose to use Bayesian inference to learn a BPD for the prediction of human policies. They show that the BPD can be used for both imitation learning and imitation learning-based human models. They also show that BPD is useful for human-AI collaboration. ,This paper studies the problem of learning a Boltzmann policy distribution (BPD) for human policies in a continuous space. The authors propose to use Bayesian inference to learn a BPD for the prediction of human policies. They show that the BPD can be used for both imitation learning and imitation learning-based human models. They also show that BPD is useful for human-AI collaboration. 
5797,SP:401ef5fe2022e926b0321258efac1f369f186ace,"Quantization of deep neural networks ( DNN ) USED-FOR compressing and accelerating DNN models. Data - free quantization ( DFQ ) HYPONYM-OF approach. accuracy EVALUATE-FOR DFQ solutions. sub - second quantization time FEATURE-OF on - the - fly DFQ framework. inference - only devices USED-FOR networks. SQuant HYPONYM-OF on - the - fly DFQ framework. discrete domain FEATURE-OF data - free optimization objective. computation complexity EVALUATE-FOR objective solver. algorithm USED-FOR objective solver. back - propagation USED-FOR algorithm. computation complexity EVALUATE-FOR algorithm. fine - tuning CONJUNCTION synthetic datasets. synthetic datasets CONJUNCTION fine - tuning. SQuant COMPARE data - free post - training quantization. data - free post - training quantization COMPARE SQuant. SQuant USED-FOR data - free quantization process. SQuant USED-FOR sub - second level. synthetic datasets EVALUATE-FOR SQuant. accuracy EVALUATE-FOR models. sub - second level FEATURE-OF data - free quantization process. 4 - bit quantization FEATURE-OF models. accuracy EVALUATE-FOR SQuant. Method are DNN models, and network architecture. OtherScientificTerm are privacy - sensitive and confidential scenarios, DNN task loss, Hessian - based optimization objective, diagonal sub - items, and weight tensor. Material is synthetic data. Metric is computation and memory requirements. ","This paper proposes a data-free quantization (DFQ) framework for deep neural networks. The proposed method is based on the Hessian-based optimization objective, which is a discrete domain formulation of the objective solver. The authors show that the proposed method can achieve sub-second quantization time in the discrete domain. The method is evaluated on synthetic and real-world datasets.","This paper proposes a data-free quantization (DFQ) framework for deep neural networks. The proposed method is based on the Hessian-based optimization objective, which is a discrete domain formulation of the objective solver. The authors show that the proposed method can achieve sub-second quantization time in the discrete domain. The method is evaluated on synthetic and real-world datasets."
5813,SP:fa4bc3f6ad3f2a0113a930fb49d68660d63910e8,"stock market partitioning CONJUNCTION sleep stage labelling. sleep stage labelling CONJUNCTION stock market partitioning. sleep stage labelling CONJUNCTION human activity recognition. human activity recognition CONJUNCTION sleep stage labelling. Time series USED-FOR tasks. human activity recognition HYPONYM-OF tasks. stock market partitioning HYPONYM-OF tasks. sleep stage labelling HYPONYM-OF tasks. sliding window USED-FOR sub - sequences. overlapping stride FEATURE-OF sliding window. sliding window USED-FOR time series. accuracy EVALUATE-FOR segmentation. approach USED-FOR approximate breakpoints. it USED-FOR long - term dependencies. bi - pass architecture USED-FOR SegTime. Task are time series segmentation, and classification. OtherScientificTerm are precise breakpoints, sliding windows, and label changing frequency. ","This paper proposes a bi-pass architecture for time series segmentation. The proposed approach is based on the idea of sliding windows, which can be seen as a sliding window for sub-sequences of a time series. The authors show that the proposed approach can be used for segmentation and classification tasks.","This paper proposes a bi-pass architecture for time series segmentation. The proposed approach is based on the idea of sliding windows, which can be seen as a sliding window for sub-sequences of a time series. The authors show that the proposed approach can be used for segmentation and classification tasks."
5829,SP:8ad1b170f0392a132a3816c9cd28fb7332343e65,Graph Neural Networks ( GNNs ) USED-FOR graph data. faithfulness problems CONJUNCTION unnatural artifacts. unnatural artifacts CONJUNCTION faithfulness problems. methods USED-FOR approximation based and perturbation based approaches. faithful explanation USED-FOR GNN predictions. subgraph level interpretation algorithm USED-FOR complex interactions. GNN characteristics USED-FOR algorithm. synthetic and real - world datasets EVALUATE-FOR DEGREE. node classification and graph classification tasks EVALUATE-FOR DEGREE. Method is GNNs. Generic is models. OtherScientificTerm is graph nodes. ,"This paper proposes a new approach to improve the faithfulness of graph neural networks (GNNs) in the presence of unnatural artifacts and faithfulness problems. Specifically, the authors propose a subgraph level interpretation algorithm for GNNs. The proposed approach is based on the idea of faithful explanation, which is an extension of the faithful explanation method for graph classification. The authors show that the proposed approach can be applied to both approximation and perturbation based approaches. Experiments on synthetic and real-world datasets demonstrate the effectiveness of the approach.","This paper proposes a new approach to improve the faithfulness of graph neural networks (GNNs) in the presence of unnatural artifacts and faithfulness problems. Specifically, the authors propose a subgraph level interpretation algorithm for GNNs. The proposed approach is based on the idea of faithful explanation, which is an extension of the faithful explanation method for graph classification. The authors show that the proposed approach can be applied to both approximation and perturbation based approaches. Experiments on synthetic and real-world datasets demonstrate the effectiveness of the approach."
5845,SP:b28a9d1ad4c539d07d53e39376cbd76024d7745c,"strided convolutions CONJUNCTION pooling layers. pooling layers CONJUNCTION strided convolutions. downsampling operators USED-FOR resolution of intermediate representations. downsampling operators PART-OF Convolutional neural networks. strided convolutions HYPONYM-OF downsampling operators. pooling layers HYPONYM-OF downsampling operators. computational complexity EVALUATE-FOR architecture. hyperparameter FEATURE-OF layers. stride FEATURE-OF hyperparameter. crossvalidation CONJUNCTION discrete optimization. discrete optimization CONJUNCTION crossvalidation. architecture search HYPONYM-OF discrete optimization. gradient descent USED-FOR search space. DiffStride HYPONYM-OF downsampling layer. learnable strides FEATURE-OF downsampling layer. layer USED-FOR resizing. layer USED-FOR cropping mask. Fourier domain FEATURE-OF cropping mask. DiffStride USED-FOR downsampling layers. audio and image classification EVALUATE-FOR solution. CIFAR10 CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION CIFAR10. CIFAR100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR100. layer PART-OF ResNet-18 architecture. computational complexity EVALUATE-FOR architecture. regularization term USED-FOR architecture. computational complexity EVALUATE-FOR regularization term. accuracy CONJUNCTION efficiency. efficiency CONJUNCTION accuracy. efficiency EVALUATE-FOR regularization. accuracy EVALUATE-FOR regularization. ImageNet EVALUATE-FOR regularization. OtherScientificTerm are shift - invariance, integer factor of downsampling, strides, random stride configurations, and learnable variables. Metric is computational cost. Generic is them. ","This paper proposes a new downsampling layer for convolutional neural networks. The proposed method is based on DiffStride, which is a regularization term for downsampled convolutions. The authors show that the proposed method outperforms existing methods on CIFAR-10 and ImageNet. ","This paper proposes a new downsampling layer for convolutional neural networks. The proposed method is based on DiffStride, which is a regularization term for downsampled convolutions. The authors show that the proposed method outperforms existing methods on CIFAR-10 and ImageNet. "
5861,SP:54cdc6fe43ed138231f26daf699119f2a16473d0,"image segmentation CONJUNCTION node classification. node classification CONJUNCTION image segmentation. Models USED-FOR image segmentation. Models USED-FOR node classification. node classification CONJUNCTION tasks. tasks CONJUNCTION node classification. Models USED-FOR tasks. method USED-FOR strictly local models. collective certificate USED-FOR softly local models. localized randomized smoothing approach USED-FOR certificate. collective guarantees FEATURE-OF locally smoothed model. image segmentation and node classification tasks EVALUATE-FOR locally smoothed model. prediction quality EVALUATE-FOR locally smoothed model. Metric is collective robustness certificate. OtherScientificTerm are small receptive field, and random perturbation strength. ","This paper proposes a new method to improve the robustness of locally smoothed models. The proposed method is based on the notion of a ""collective robustness certificate"", which is defined as the sum of the mutual information between the perturbation strength of the local model and that of the global model. The authors show that the proposed method can be applied to a wide range of tasks, including image segmentation, node classification, and node classification. Experiments are conducted to show the effectiveness of the proposed approach.","This paper proposes a new method to improve the robustness of locally smoothed models. The proposed method is based on the notion of a ""collective robustness certificate"", which is defined as the sum of the mutual information between the perturbation strength of the local model and that of the global model. The authors show that the proposed method can be applied to a wide range of tasks, including image segmentation, node classification, and node classification. Experiments are conducted to show the effectiveness of the proposed approach."
5877,SP:aacc31e83886c4c997412a1e51090202075eda86,"Normalizing flows USED-FOR general - purpose density estimators. domain - specific knowledge USED-FOR real world applications. general - purpose transformations CONJUNCTION structured layers. structured layers CONJUNCTION general - purpose transformations. general - purpose transformations PART-OF embedded - model flows ( EMF ). equivalent bijective transformations USED-FOR user - specified differentiable probabilistic models. EMFs USED-FOR desirable properties. multimodality CONJUNCTION hierarchical coupling. hierarchical coupling CONJUNCTION multimodality. hierarchical coupling CONJUNCTION continuity. continuity CONJUNCTION hierarchical coupling. EMFs USED-FOR multimodality. EMFs USED-FOR hierarchical coupling. continuity HYPONYM-OF desirable properties. multimodality HYPONYM-OF desirable properties. hierarchical coupling HYPONYM-OF desirable properties. EMFs USED-FOR variational inference. structure of the prior model PART-OF variational architecture. approach COMPARE alternative methods. alternative methods COMPARE approach. common structured inference problems EVALUATE-FOR alternative methods. common structured inference problems EVALUATE-FOR approach. Method are normalizing flows, gated structured layers, and prior model. OtherScientificTerm is domain - specific inductive biases. Generic are layers, and models. ","This paper proposes a new normalizing flow (EMF) method for variational inference. EMF is an extension of normalizing flows that can be applied to differentiable probabilistic models. The authors show that EMF can be used to improve the generalization properties of the prior model. The paper also shows that EMFs can be combined with structured layers and general-purpose transformations to achieve the desirable properties of multimodality, hierarchical coupling, and continuity. Experiments show that the proposed method outperforms the state-of-the-art.","This paper proposes a new normalizing flow (EMF) method for variational inference. EMF is an extension of normalizing flows that can be applied to differentiable probabilistic models. The authors show that EMF can be used to improve the generalization properties of the prior model. The paper also shows that EMFs can be combined with structured layers and general-purpose transformations to achieve the desirable properties of multimodality, hierarchical coupling, and continuity. Experiments show that the proposed method outperforms the state-of-the-art."
5893,SP:825a254c0725008143b260ead840ae35f9f096d1,"entities CONJUNCTION objects. objects CONJUNCTION entities. conceptual structure FEATURE-OF rich conceptual structure. LMs USED-FOR grounded world representation. LMs USED-FOR conceptual domain. it USED-FOR concepts. grid world FEATURE-OF concepts. GPT-2 CONJUNCTION GPT-3. GPT-3 CONJUNCTION GPT-2. GPT-2 HYPONYM-OF generative language models. GPT-3 HYPONYM-OF generative language models. text - only models USED-FOR rich conceptual structure. Method are text - only language models ( LMs ), and grounded language models. Generic are representation, and model. OtherScientificTerm is conceptual structure of language. ","This paper proposes a method for learning a generative language model that can capture the rich conceptual structure of text-only language models (GPT-2 and GPT-3). The method is based on the idea that text-based language models can be used to represent concepts in a grid-world. The authors show that the proposed method is able to capture concepts in the grid world, and that it can be applied to a number of existing GPT models, including GPT2, GPT3, and LMs based on GPT. ","This paper proposes a method for learning a generative language model that can capture the rich conceptual structure of text-only language models (GPT-2 and GPT-3). The method is based on the idea that text-based language models can be used to represent concepts in a grid-world. The authors show that the proposed method is able to capture concepts in the grid world, and that it can be applied to a number of existing GPT models, including GPT2, GPT3, and LMs based on GPT. "
5909,SP:702029739062693e3f96051cbb38f20c53f2a223,"Reinforcement learning USED-FOR phenomena. biases PART-OF learning process. biases USED-FOR shaped rewards. shaped rewards USED-FOR emergent phenomena. sender - receiver navigation game USED-FOR shaped rewards. Generic are they, and rewards. OtherScientificTerm are base reward, inductive bias, semantics, and environmental variables of interest. Task is emergent language experimentation. Material is emergent language. Metric is entropy. ","This paper studies emergent language learning in the context of a sender-receiver navigation game, where the goal is to learn a language that can be used to solve a navigation task. The authors show that the base reward of the game is a function of the semantics of the environment, and that the learned language has an inductive bias. They also show that this bias can be leveraged to improve the performance of the learner. ","This paper studies emergent language learning in the context of a sender-receiver navigation game, where the goal is to learn a language that can be used to solve a navigation task. The authors show that the base reward of the game is a function of the semantics of the environment, and that the learned language has an inductive bias. They also show that this bias can be leveraged to improve the performance of the learner. "
5925,SP:146ef14e569e10172a7dc602acd3fadf2c3bef8b,"deep multilingual language models USED-FOR cross - lingual knowledge. neural modeling USED-FOR deep multilingual language models. unparallel texts USED-FOR cross - lingual knowledge. invariance FEATURE-OF feature representations. invariance FEATURE-OF transfer. prior shift estimation CONJUNCTION correction. correction CONJUNCTION prior shift estimation. representation alignment CONJUNCTION prior shift estimation. prior shift estimation CONJUNCTION representation alignment. unsupervised cross - lingual learning method USED-FOR representation alignment. unsupervised cross - lingual learning method USED-FOR prior shift estimation. importance - weighted domain alignment ( IWDA ) HYPONYM-OF unsupervised cross - lingual learning method. method COMPARE semi - supervised learning techniques. semi - supervised learning techniques COMPARE method. Method are cross - lingually shared representations, and multilingual representations. OtherScientificTerm are distributional shift in class priors, and prior shifts. ",This paper proposes an unsupervised cross-lingual learning method for unparallel text classification. The method is based on importance-weighted domain alignment (IWDA) and prior shift estimation (NDI). The authors show that the proposed method is able to achieve state-of-the-art performance in terms of prior shift and representation alignment. ,This paper proposes an unsupervised cross-lingual learning method for unparallel text classification. The method is based on importance-weighted domain alignment (IWDA) and prior shift estimation (NDI). The authors show that the proposed method is able to achieve state-of-the-art performance in terms of prior shift and representation alignment. 
5941,SP:461ed47339e08dafea90a7c015d2f20e534daeb7,"Meta - learning USED-FOR artificial intelligence. algorithm USED-FOR problem. algorithm USED-FOR meta - learner. metric USED-FOR meta - optimisation. gradients USED-FOR meta - learning. bootstrapping mechanism USED-FOR meta - learning horizon. backpropagation USED-FOR bootstrapping mechanism. it USED-FOR multi - task meta - learning. Atari ALE benchmark EVALUATE-FOR model - free agents. it USED-FOR exploration. ε - greedy Q - learning agent USED-FOR exploration. Task is metaoptimisation problem. Method are metalearner, and bootstrapping. Metric is ( pseudo-)metric. OtherScientificTerm is update rule. ","This paper proposes a new meta-learning algorithm for multi-task learning. The proposed algorithm is based on bootstrapping, which is a bootstrapped method for learning a meta-learner. The authors show that the proposed algorithm can be used to learn a meta learner that is able to learn the meta-optimisation horizon. They also show that this method can be combined with backpropagation to improve the performance of the algorithm. ","This paper proposes a new meta-learning algorithm for multi-task learning. The proposed algorithm is based on bootstrapping, which is a bootstrapped method for learning a meta-learner. The authors show that the proposed algorithm can be used to learn a meta learner that is able to learn the meta-optimisation horizon. They also show that this method can be combined with backpropagation to improve the performance of the algorithm. "
5957,SP:49435d70bf8e16d5dbf34577cf8d3a5b21b1f25a,"model - free agents USED-FOR generalization. generalization ability EVALUATE-FOR model - based agents. model - based agents COMPARE model - free counterparts. model - free counterparts COMPARE model - based agents. generalization ability EVALUATE-FOR model - based agents. generalization ability EVALUATE-FOR model - free counterparts. MuZero HYPONYM-OF model - based agent. procedural and task generalization EVALUATE-FOR its. self - supervised representation learning CONJUNCTION procedural data diversity. procedural data diversity CONJUNCTION self - supervised representation learning. planning CONJUNCTION self - supervised representation learning. self - supervised representation learning CONJUNCTION planning. generalization CONJUNCTION data efficiency. data efficiency CONJUNCTION generalization. data efficiency EVALUATE-FOR Procgen. planning HYPONYM-OF procedural generalization. Procgen EVALUATE-FOR techniques. generalization EVALUATE-FOR techniques. data efficiency EVALUATE-FOR techniques. factors USED-FOR task generalization. Meta - World USED-FOR task generalization. single - task, model - free paradigm CONJUNCTION self - supervised model - based agents. self - supervised model - based agents CONJUNCTION single - task, model - free paradigm. rich, procedural, multi - task environments FEATURE-OF self - supervised model - based agents. single - task, model - free paradigm USED-FOR generalizable agents. Method is model - based reinforcement learning. OtherScientificTerm is internal model of the world. Task is transfer. ","This paper presents MuZero, a self-supervised model-based RL agent that is able to generalize to new tasks in a meta-world setting. MuZero is a model-free agent that learns a model of the world, which is then used to learn a task-specific representation of the environment. The authors show that MuZero can generalize better than a single-task, multi-task RL agent in the Meta-World setting. They also show that the MuZero agent can generalise better than the single task RL agent.","This paper presents MuZero, a self-supervised model-based RL agent that is able to generalize to new tasks in a meta-world setting. MuZero is a model-free agent that learns a model of the world, which is then used to learn a task-specific representation of the environment. The authors show that MuZero can generalize better than a single-task, multi-task RL agent in the Meta-World setting. They also show that the MuZero agent can generalise better than the single task RL agent."
5973,SP:ba80e35d452d894181d51624183b60541c0f3704,"fixed graph USED-FOR relational inductive biases. graph neural networks HYPONYM-OF Machine learning frameworks. fixed graph USED-FOR Machine learning frameworks. network inverse ( deconvolution ) problem USED-FOR graph learning task. eigendecomposition - based spectral methods CONJUNCTION iterative optimization solutions. iterative optimization solutions CONJUNCTION eigendecomposition - based spectral methods. Graph Deconvolution Network ( GDN ) HYPONYM-OF parameterized neural network architecture. GDNs USED-FOR link prediction or edge - weight regression tasks. GDNs USED-FOR distribution of graphs. loss function USED-FOR GDNs. loss function USED-FOR link prediction or edge - weight regression tasks. layers USED-FOR graph objects. GDNs USED-FOR larger - sized graphs. graph objects COMPARE node features. node features COMPARE graph objects. GDN USED-FOR graph recovery. synthetic data USED-FOR GDN. synthetic data USED-FOR graph recovery. Human Connectome Project - Young Adult neuroimaging dataset EVALUATE-FOR model. inferring structural brain networks USED-FOR model. functional connectivity USED-FOR inferring structural brain networks. functional connectivity USED-FOR model. Material is network data. OtherScientificTerm are graphs, graph convolutional relationship, observed and latent graphs, and structural brain networks. Task is inferring graph structure. Method is proximal gradient iterations. ","This paper proposes a graph neural network architecture called Graph Deconvolution Network (GDN) for graph learning. GDN is based on the network inverse (NCI) problem, which is an extension of the graph convolutional relationship (GCR) problem. The authors propose a new loss function for the GCR problem, and show that GDN can be used for link prediction and edge-weight regression tasks. The proposed method is evaluated on the Human Connectome Project-Young Adult neuroimaging dataset and shows promising results.","This paper proposes a graph neural network architecture called Graph Deconvolution Network (GDN) for graph learning. GDN is based on the network inverse (NCI) problem, which is an extension of the graph convolutional relationship (GCR) problem. The authors propose a new loss function for the GCR problem, and show that GDN can be used for link prediction and edge-weight regression tasks. The proposed method is evaluated on the Human Connectome Project-Young Adult neuroimaging dataset and shows promising results."
5989,SP:91fd4189bf04aca4ccd1288ec8459e1edb29d378,"Reward shaping ( RS ) USED-FOR reinforcement learning ( RL ). manually engineered shaping - reward functions USED-FOR RS. domain knowledge USED-FOR It. Reinforcement Learning Optimising Shaping Algorithm ( ROSA ) HYPONYM-OF automated RS framework. Markov game USED-FOR shaping - reward function. agent ( Controller ) USED-FOR optimal policy. agent ( Controller ) USED-FOR task. optimal policy USED-FOR task. shaped rewards USED-FOR agent ( Controller ). switching controls USED-FOR reward - shaping agent ( Shaper ). shaped rewards USED-FOR optimal policy. ROSA USED-FOR shapingreward function. shapingreward function USED-FOR task. RL algorithms USED-FOR ROSA. RS algorithms USED-FOR sparse reward environments. OtherScientificTerm are sparse or uninformative rewards, shaping rewards, and congenial properties. Task is autonomous learning. ",This paper proposes a reinforcement learning algorithm for reward-shaping (RS) based on the Markov game framework. The proposed algorithm is based on an existing framework for reward shaping (ROSA) and is able to learn the optimal policy for any task. The authors show that the proposed algorithm outperforms the baselines on a number of tasks. The paper also shows that the algorithm can be applied to the task of autonomous learning.,This paper proposes a reinforcement learning algorithm for reward-shaping (RS) based on the Markov game framework. The proposed algorithm is based on an existing framework for reward shaping (ROSA) and is able to learn the optimal policy for any task. The authors show that the proposed algorithm outperforms the baselines on a number of tasks. The paper also shows that the algorithm can be applied to the task of autonomous learning.
6005,SP:20abe4d70152590c3c44fcb50c5d0293e25874ff,Vertical Federated Learning ( VFL ) HYPONYM-OF distributed learning paradigm. backdoor attacks FEATURE-OF VFL. robustness EVALUATE-FOR VFL. horizontal federated learning COMPARE VFL. VFL COMPARE horizontal federated learning. RVFR HYPONYM-OF VFL training and inference framework. RVFR USED-FOR uncorrupted features. RVFR USED-FOR model. RVFR USED-FOR inferencephase adversarial and missing feature attacks. RVFR COMPARE baselines. baselines COMPARE RVFR. robustness EVALUATE-FOR baselines. robustness EVALUATE-FOR RVFR. Method is global model. OtherScientificTerm is features. Task is inference - phase attacks. Material is NUS - WIDE and CIFAR-10 datasets. ,"This paper proposes a new method to improve the robustness of vertical federated learning (VFL) against backdoor attacks. The proposed method, called Robust Reinforcement Learning (RVFR), is based on the idea that the training and inference phase of a VFL model should be independent of each other. The authors show that the proposed method is robust to missing features and adversarial and missing feature attacks in the inference phase. The method is evaluated on two datasets, NUS-WIDE and CIFAR-10. ","This paper proposes a new method to improve the robustness of vertical federated learning (VFL) against backdoor attacks. The proposed method, called Robust Reinforcement Learning (RVFR), is based on the idea that the training and inference phase of a VFL model should be independent of each other. The authors show that the proposed method is robust to missing features and adversarial and missing feature attacks in the inference phase. The method is evaluated on two datasets, NUS-WIDE and CIFAR-10. "
6021,SP:3fbc5ebb4c598e849b3ecbb2886289e20bf1ea14,question answering CONJUNCTION fact checking. fact checking CONJUNCTION question answering. Information retrieval USED-FOR knowledge intensive tasks. Information retrieval HYPONYM-OF natural language processing. natural language processing USED-FOR knowledge intensive tasks. fact checking HYPONYM-OF knowledge intensive tasks. question answering HYPONYM-OF knowledge intensive tasks. dense retrievers COMPARE sparse methods. sparse methods COMPARE dense retrievers. dense retrievers PART-OF information retrieval. neural networks USED-FOR dense retrievers. term - frequency USED-FOR sparse methods. datasets EVALUATE-FOR models. BM25 HYPONYM-OF term - frequency methods. contrastive learning USED-FOR unsupervised dense retrievers. it USED-FOR retrieval. model COMPARE BM25. BM25 COMPARE model. BEIR benchmark EVALUATE-FOR model. model COMPARE BM25. BM25 COMPARE model. MS MARCO dataset EVALUATE-FOR technique. pre - training USED-FOR technique. pre - training CONJUNCTION fine - tuning. fine - tuning CONJUNCTION pre - training. fine - tuning EVALUATE-FOR technique. MS MARCO dataset EVALUATE-FOR pre - training. MS MARCO dataset EVALUATE-FOR fine - tuning. BEIR benchmark EVALUATE-FOR technique. Generic is they. Material is new domains. OtherScientificTerm is supervision. ,This paper proposes a novel method for unsupervised dense retrieval. The method is based on contrastive learning. The authors show that the proposed method outperforms the baselines on the BEIR benchmark and fine-tuning on the MS MARCO dataset. ,This paper proposes a novel method for unsupervised dense retrieval. The method is based on contrastive learning. The authors show that the proposed method outperforms the baselines on the BEIR benchmark and fine-tuning on the MS MARCO dataset. 
6037,SP:ed4e2896dc882bd089f420f719da232d706097c5,fine - tuning CONJUNCTION linear probing. linear probing CONJUNCTION fine - tuning. fine - tuning HYPONYM-OF methods. linear probing HYPONYM-OF methods. accuracy in - distribution ( ID ) EVALUATE-FOR fine - tuning. accuracy EVALUATE-FOR fine - tuning. fine - tuning COMPARE linear probing. linear probing COMPARE fine - tuning. DomainNet CONJUNCTION CIFAR. CIFAR CONJUNCTION DomainNet. Breeds - Entity30 CONJUNCTION DomainNet. DomainNet CONJUNCTION Breeds - Entity30. distribution shift datasets EVALUATE-FOR fine - tuning. Breeds - Living17 CONJUNCTION Breeds - Entity30. Breeds - Entity30 CONJUNCTION Breeds - Living17. CIFAR10.1 CONJUNCTION FMoW. FMoW CONJUNCTION CIFAR10.1. accuracy OOD EVALUATE-FOR linear probing. CIFAR CONJUNCTION CIFAR10.1. CIFAR10.1 CONJUNCTION CIFAR. accuracy ID EVALUATE-FOR linear probing. CIFAR HYPONYM-OF distribution shift datasets. FMoW HYPONYM-OF distribution shift datasets. Breeds - Living17 HYPONYM-OF distribution shift datasets. CIFAR10.1 HYPONYM-OF distribution shift datasets. DomainNet HYPONYM-OF distribution shift datasets. Breeds - Entity30 HYPONYM-OF distribution shift datasets. accuracy OOD EVALUATE-FOR fine - tuning. accuracy ID EVALUATE-FOR fine - tuning. fine - tuning USED-FOR pretrained features. fine - tuning CONJUNCTION linear probing. linear probing CONJUNCTION fine - tuning. linear probing COMPARE fine - tuning. fine - tuning COMPARE linear probing. ID and OOD accuracy EVALUATE-FOR fine - tuning. linear probing CONJUNCTION fine - tuning. fine - tuning CONJUNCTION linear probing. fine - tuning COMPARE fine - tuning. fine - tuning COMPARE fine - tuning. ID CONJUNCTION OOD. OOD CONJUNCTION ID. datasets EVALUATE-FOR fine - tuning. Method is pretrained model,This paper studies the effect of fine-tuning on the accuracy in-distribution (ID) and OOD accuracy (OD) of a pre-trained model on a set of distribution shift datasets. The authors propose a method to fine-tune the accuracy of the model on the ID and OD datasets. They show that the method is able to improve the accuracy on ID and ID accuracy on OOD datasets. ,This paper studies the effect of fine-tuning on the accuracy in-distribution (ID) and OOD accuracy (OD) of a pre-trained model on a set of distribution shift datasets. The authors propose a method to fine-tune the accuracy of the model on the ID and OD datasets. They show that the method is able to improve the accuracy on ID and ID accuracy on OOD datasets. 
6053,SP:96f4f90488c15167d85261a883cd70fc15e06bb9,"meta - learning COMPARE L2DNC. L2DNC COMPARE meta - learning. meta - learning USED-FOR L2DNC. meta - learning algorithms USED-FOR L2DNC problem. meta - learning - based methodology USED-FOR unlabeled data. meta - learning - based methodology USED-FOR it. L2DNC COMPARE labeling in causality. labeling in causality COMPARE L2DNC. unseen - class data USED-FOR seen - class data. Material are labeled data, and limited data. Method is clustering models. OtherScientificTerm is high - level semantic features. ","This paper proposes a meta-learning-based method for the problem of unlabeled data classification. The authors show that the L2DNC problem is similar to the labeling in causality problem (L2C) problem, but the problem is different from L2C in the sense that there are no labeled data, and only limited labeled data. The proposed method is based on meta learning and meta-supervised clustering. The paper also shows that the proposed method can be applied to unseen-class data. ","This paper proposes a meta-learning-based method for the problem of unlabeled data classification. The authors show that the L2DNC problem is similar to the labeling in causality problem (L2C) problem, but the problem is different from L2C in the sense that there are no labeled data, and only limited labeled data. The proposed method is based on meta learning and meta-supervised clustering. The paper also shows that the proposed method can be applied to unseen-class data. "
6069,SP:262a5aaa4e675b2aac6bd14d3aa007bf411ce550,"POMDPs USED-FOR model - based RL agents. online and offline experiences USED-FOR causal transition model. offline experiences USED-FOR agent. reinforcement learning CONJUNCTION causality. causality CONJUNCTION reinforcement learning. causal inference problem USED-FOR model - based reinforcement learning. offline data USED-FOR learning. methodology USED-FOR offline data. latent - based causal transition model USED-FOR interventional and observational regimes. latent - based causal transition model USED-FOR method. latent variable USED-FOR deconfounding. deconfounding USED-FOR POMDP transition model. latent variable USED-FOR POMDP transition model. generalization guarantees EVALUATE-FOR it. synthetic toy problems EVALUATE-FOR it. generalization guarantees EVALUATE-FOR method. synthetic toy problems EVALUATE-FOR method. OtherScientificTerm are online experiences, privileged information, and learning agent. Material are interventional data, and observational data. Method is causal framework of do - calculus. ","This paper proposes a causal inference framework for model-based reinforcement learning based on POMDPs. The framework is based on the causal framework of do-calculus, which is used to model the causal transition between online and offline experiences. The proposed method is evaluated on synthetic and real-world datasets. The authors show that the proposed method outperforms the baselines in terms of generalization and generalization guarantees.","This paper proposes a causal inference framework for model-based reinforcement learning based on POMDPs. The framework is based on the causal framework of do-calculus, which is used to model the causal transition between online and offline experiences. The proposed method is evaluated on synthetic and real-world datasets. The authors show that the proposed method outperforms the baselines in terms of generalization and generalization guarantees."
6085,SP:bcb4e7e5c137edf04a9ea2fde014b0984c6ef89b,"textual knowledge corpus USED-FOR retriever. Wikipedia HYPONYM-OF textual knowledge corpus. retriever USED-FOR text generation systems. methods USED-FOR generator. retriever CONJUNCTION generator. generator CONJUNCTION retriever. methods USED-FOR retriever. generating informative utterances in conversations HYPONYM-OF open - ended generation tasks. retriever CONJUNCTION generator. generator CONJUNCTION retriever. informative conversations EVALUATE-FOR retriever. generator USED-FOR it. Wizard of Wikipedia dataset FEATURE-OF informative conversations. retriever USED-FOR it. posterior - guided training USED-FOR informative conversations. evidence lower bound ( ELBo ) USED-FOR it. posterior distribution Q USED-FOR guide retriever. OtherScientificTerm are top-10, and generator ’s responses. Method is end - to - end system. ","This paper proposes an end-to-end system for generating informative utterances in text generation tasks. The proposed method is based on the idea of posterior-guided training, where the retriever is trained on the top-10 responses of the generator, and the generator is trained to generate informative responses. The authors show that the proposed method outperforms the baselines on Wizard of Wikipedia dataset. ","This paper proposes an end-to-end system for generating informative utterances in text generation tasks. The proposed method is based on the idea of posterior-guided training, where the retriever is trained on the top-10 responses of the generator, and the generator is trained to generate informative responses. The authors show that the proposed method outperforms the baselines on Wizard of Wikipedia dataset. "
6101,SP:bec15075409c71f98f3698bc35e34eeb4862d94f,"machine learning USED-FOR combinatorial optimization. real - world problems USED-FOR large graphs. influence maximization HYPONYM-OF NP - hard problem. influence estimation HYPONYM-OF # P - hard counting problem. influence estimation CONJUNCTION influence maximization. influence maximization CONJUNCTION influence estimation. influence estimation HYPONYM-OF problems. influence maximization HYPONYM-OF problems. Graph Neural Network ( GNN ) USED-FOR upper bound of influence estimation. GLIE HYPONYM-OF Graph Neural Network ( GNN ). small simulated graphs USED-FOR it. GLIE USED-FOR influence estimation. real graphs USED-FOR influence estimation. it USED-FOR influence maximization. GLIE CONJUNCTION simulated influence estimation. simulated influence estimation CONJUNCTION GLIE. simulated influence estimation USED-FOR Lazy Forward optimization. GLIE USED-FOR Lazy Forward optimization. time complexity CONJUNCTION quality of influence. quality of influence CONJUNCTION time complexity. first HYPONYM-OF Q - network. GLIE ’s predictions USED-FOR Q - network. second USED-FOR provably submodular function. GLIE ’s representations USED-FOR provably submodular function. time efficiency CONJUNCTION influence spread. influence spread CONJUNCTION time efficiency. latter COMPARE SOTA benchmarks. SOTA benchmarks COMPARE latter. influence spread EVALUATE-FOR latter. time efficiency EVALUATE-FOR latter. Generic are perspective, and approaches. Task is small graph problems. OtherScientificTerm are predictions ranking, and computational overhead. Metric is accuracy. ","This paper studies the problem of influence estimation in combinatorial optimization. The authors propose a graph neural network (GNN) based approach for the problem. The main contribution of the paper is the use of GNNs for influence estimation and Lazy Forward optimization. In particular, the authors show that the GNN can be used to compute the upper bound of the influence estimation upper bound for the Q-network. The paper also shows that the proposed approach can be applied to influence maximization and influence estimation. ","This paper studies the problem of influence estimation in combinatorial optimization. The authors propose a graph neural network (GNN) based approach for the problem. The main contribution of the paper is the use of GNNs for influence estimation and Lazy Forward optimization. In particular, the authors show that the GNN can be used to compute the upper bound of the influence estimation upper bound for the Q-network. The paper also shows that the proposed approach can be applied to influence maximization and influence estimation. "
6117,SP:0c55b1f5e544e1e9510a12981107ae6c9f1eeb2e,"active learning strategies USED-FOR domain adaptation. localized class of functions USED-FOR labeling. Rademacher average CONJUNCTION localized discrepancy. localized discrepancy CONJUNCTION Rademacher average. localized discrepancy FEATURE-OF loss functions. generalization error bounds USED-FOR active learning strategies. regularity condition FEATURE-OF loss functions. Kmedoids algorithm USED-FOR large data set. theoretical bounds USED-FOR Kmedoids algorithm. algorithm COMPARE active learning techniques. active learning techniques COMPARE algorithm. active learning techniques USED-FOR domain adaptation. algorithm USED-FOR domain adaptation. large data sets EVALUATE-FOR algorithm. OtherScientificTerm are assumption of Lipschitz functions, and discrepancy distance. ","This paper studies the problem of active learning for domain adaptation. In particular, the authors consider the case where the loss function is a Lipschitz function and the label is a localized class of functions. The authors show that the generalization error bounds for active learning algorithms can be derived from the Rademacher average and the discrepancy distance between the label and the target label. They also provide theoretical bounds for the Kmedoids algorithm. ","This paper studies the problem of active learning for domain adaptation. In particular, the authors consider the case where the loss function is a Lipschitz function and the label is a localized class of functions. The authors show that the generalization error bounds for active learning algorithms can be derived from the Rademacher average and the discrepancy distance between the label and the target label. They also provide theoretical bounds for the Kmedoids algorithm. "
6133,SP:f63c10ba7d6f5ef1c167faa8a221b3ab5cc06006,variational approximation USED-FOR Bayesian neural networks. singular statistical models USED-FOR neural networks. posterior distribution FEATURE-OF singular model. desingularization map HYPONYM-OF algebraic - geometrical transformation. generalized gamma mean - field variational family USED-FOR leading order term. leading order term FEATURE-OF model evidence. generalized gamma mean - field variational family USED-FOR model evidence. desingularization USED-FOR generalized gamma mean - field variational family. Affine coupling layers USED-FOR unknown desingularization map. source distribution USED-FOR normalizing flow. normalizing flow USED-FOR methodology. generalized gamma USED-FOR normalizing flow. Generic is approximation. Method is singular learning theory. OtherScientificTerm is mixture of standard forms. ,"This paper studies the problem of singular statistical models. The authors consider the case of Bayesian neural networks, where the model is a singular statistical model and the data distribution is a mixture of standard forms.  The authors propose a new normalizing flow, which is a generalized gamma-means-field variational family that is based on the desingularization map of the source distribution and the leading order term of the model evidence.  They show that the proposed method can be used to approximate the posterior distribution of a singular model. ","This paper studies the problem of singular statistical models. The authors consider the case of Bayesian neural networks, where the model is a singular statistical model and the data distribution is a mixture of standard forms.  The authors propose a new normalizing flow, which is a generalized gamma-means-field variational family that is based on the desingularization map of the source distribution and the leading order term of the model evidence.  They show that the proposed method can be used to approximate the posterior distribution of a singular model. "
6149,SP:9ba33d09bd68d8598e2aff428ecca5060922a4dc,"model USED-FOR domain generalization ( DG ) problem setting. known data distributions USED-FOR model. methods COMPARE empirical risk minimisation baseline. empirical risk minimisation baseline COMPARE methods. learning - theoretic generalisation bound USED-FOR DG. learning - theoretic generalisation bound USED-FOR domain generalisation. Rademacher complexity FEATURE-OF model. empirical risk - predictor complexity trade - off EVALUATE-FOR methods. regularised ERM USED-FOR domain generalisation. Task are general purpose DG, and DG problem. Material is DomainBed benchmark. ",This paper studies the problem of domain generalization (DGD) in the setting of risk minimization. The authors propose a new risk minimisation method based on regularised ERM. The proposed method is based on the learning-theoretic generalisation bound for the domain generalisation problem. The method is evaluated on the DomainBed benchmark.,This paper studies the problem of domain generalization (DGD) in the setting of risk minimization. The authors propose a new risk minimisation method based on regularised ERM. The proposed method is based on the learning-theoretic generalisation bound for the domain generalisation problem. The method is evaluated on the DomainBed benchmark.
6165,SP:b1f622cbc827e880f98de9e99eca498584efe011,overlays USED-FOR maximum n - times coverage problem. multi - set multi - cover problem USED-FOR Maximum n - times coverage. integer linear programming CONJUNCTION sequential greedy optimization. sequential greedy optimization CONJUNCTION integer linear programming. solutions USED-FOR n - times coverage. integer linear programming USED-FOR solutions. sequential greedy optimization USED-FOR solutions. it USED-FOR pan - strain COVID-19 vaccine design. pan - strain COVID-19 vaccine design COMPARE designs. designs COMPARE pan - strain COVID-19 vaccine design. maximum n - times coverage USED-FOR peptide vaccine design. predicted population coverage EVALUATE-FOR pan - strain COVID-19 vaccine design. predicted population coverage EVALUATE-FOR designs. Task is min - cost n - times coverage problem. OtherScientificTerm is HLA molecules. ,"This paper studies the maximum n-times coverage problem, which is a multi-set multi-cover problem. The authors propose to solve the problem using integer linear programming and sequential greedy optimization. The proposed method is evaluated on the pan-strain COVID-19 vaccine design.","This paper studies the maximum n-times coverage problem, which is a multi-set multi-cover problem. The authors propose to solve the problem using integer linear programming and sequential greedy optimization. The proposed method is evaluated on the pan-strain COVID-19 vaccine design."
6181,SP:11ad277db038a77d5935e7504cc640e74bfc4efe,"supervised learning algorithms USED-FOR SNNs. power consumption CONJUNCTION computational capability. computational capability CONJUNCTION power consumption. information encoding CONJUNCTION power consumption. power consumption CONJUNCTION information encoding. information encoding EVALUATE-FOR spiking neural networks ( SNNs ). weight initialization USED-FOR SNN training. It USED-FOR gradient generation. asymptotic formula USED-FOR response curve. asymptotic formula USED-FOR spiking neurons. initialization method USED-FOR gradient vanishing. slant asymptote USED-FOR initialization method. coding schemes USED-FOR classification tasks. method COMPARE deep learning initialization methods. deep learning initialization methods COMPARE method. method COMPARE SNN initialization methods. SNN initialization methods COMPARE method. deep learning initialization methods CONJUNCTION SNN initialization methods. SNN initialization methods CONJUNCTION deep learning initialization methods. model accuracy EVALUATE-FOR deep learning initialization methods. model accuracy EVALUATE-FOR SNN initialization methods. coding schemes EVALUATE-FOR method. training speed CONJUNCTION model accuracy. model accuracy CONJUNCTION training speed. MNIST and CIFAR10 dataset EVALUATE-FOR coding schemes. MNIST and CIFAR10 dataset USED-FOR classification tasks. training speed EVALUATE-FOR method. model accuracy EVALUATE-FOR method. Method is backpropagation. OtherScientificTerm are neuron response distribution, and training hyperparameters. Generic is methods. ","This paper proposes a new initialization method for spiking neural networks (SNNs) based on the slant asymptotical formula for the response curve of spiking neurons. The authors show that the proposed method can be used to improve the performance of SNN initialization methods in terms of training speed, accuracy, and computational efficiency. They also show that their method is able to achieve better performance than the baselines. ","This paper proposes a new initialization method for spiking neural networks (SNNs) based on the slant asymptotical formula for the response curve of spiking neurons. The authors show that the proposed method can be used to improve the performance of SNN initialization methods in terms of training speed, accuracy, and computational efficiency. They also show that their method is able to achieve better performance than the baselines. "
6197,SP:f7e8602b40b37f26277e3f44f60a11f879978986,"Federated learning USED-FOR machine learning models. mobile devices FEATURE-OF decentralized client data. decentralized client data USED-FOR machine learning models. model USED-FOR federated learning systems. mixture of distributions USED-FOR it. block - cyclic pattern USED-FOR distribution shift. light - weight branches USED-FOR network. image classification CONJUNCTION next word prediction. next word prediction CONJUNCTION image classification. algorithm USED-FOR distribution shift. image classification EVALUATE-FOR algorithm. model EVALUATE-FOR algorithm. Stack Overflow dataset USED-FOR next word prediction. EMNIST and CIFAR datasets USED-FOR image classification. OtherScientificTerm are periodically shifting distributions, and daytime and nighttime modes. Method are Federated Expectation - Maximization algorithm, and mixture model. ",This paper proposes a new algorithm for federated learning. The proposed algorithm is based on the block-cyclic pattern of distribution shift. The authors show that the proposed algorithm can be applied to the problem of distributed client data. The algorithm is evaluated on the Stack Overflow and EMNIST datasets.,This paper proposes a new algorithm for federated learning. The proposed algorithm is based on the block-cyclic pattern of distribution shift. The authors show that the proposed algorithm can be applied to the problem of distributed client data. The algorithm is evaluated on the Stack Overflow and EMNIST datasets.
6213,SP:e38efcfcf63f0488b6e20a74a86b78aad1ead363,"accuracy EVALUATE-FOR winning ticket ” subnetwork. pruning USED-FOR DN ’s decision boundary. pruning DN nodes USED-FOR decision boundary. spline interpretation of DNs USED-FOR theory and visualization tools. DN spline theory CONJUNCTION lottery ticket hypothesis of DNs. lottery ticket hypothesis of DNs CONJUNCTION DN spline theory. early - bird ( EB ) phenomenon FEATURE-OF DN ’s spline mappings. pruning strategy USED-FOR DN nodes. spline partition regions FEATURE-OF DN nodes. spline - based DN pruning approach COMPARE state - of - the - art methods. state - of - the - art methods COMPARE spline - based DN pruning approach. accuracy EVALUATE-FOR state - of - the - art methods. networks EVALUATE-FOR spline - based DN pruning approach. spline - based DN pruning approach USED-FOR training FLOPs. accuracy EVALUATE-FOR spline - based DN pruning approach. Method are deep network ( DN ) training, and pruning technique. Generic is model. OtherScientificTerm is spline ’s partition. ","This paper proposes a new pruning strategy for deep neural networks (DNs) based on the lottery ticket hypothesis of DNs. Specifically, the authors propose a spline-based DN pruning approach that prunes the decision boundary of DN nodes based on their spline partition regions. The authors show that the proposed method outperforms state-of-the-art methods in terms of accuracy and training FLOPs. ","This paper proposes a new pruning strategy for deep neural networks (DNs) based on the lottery ticket hypothesis of DNs. Specifically, the authors propose a spline-based DN pruning approach that prunes the decision boundary of DN nodes based on their spline partition regions. The authors show that the proposed method outperforms state-of-the-art methods in terms of accuracy and training FLOPs. "
6229,SP:64ce86f8bd8572f699809c808aea8364fbbe4ef3,"data representation USED-FOR optimal predictors. inner - level USED-FOR optimal group predictors. bi - level optimization USED-FOR problem. inner optimization CONJUNCTION implicit differentiation. implicit differentiation CONJUNCTION inner optimization. implicit differentiation CONJUNCTION optimization path. optimization path CONJUNCTION implicit differentiation. implicit differentiation USED-FOR implicit path alignment algorithm. inner optimization USED-FOR implicit path alignment algorithm. sufficiency rule FEATURE-OF bi - level objective. error gap EVALUATE-FOR implicit approach. classification and regression settings EVALUATE-FOR method. Task are fair representation learning perspective, and fairness measurement. Generic is representation. Method are inner - level optimization, and fair representation learning. ","This paper studies the problem of fair representation learning from the perspective of bi-level optimization. In particular, the authors propose an implicit approach to the problem that combines inner optimization and implicit differentiation. The authors show that the proposed approach is able to achieve better performance than existing methods in both classification and regression settings. ","This paper studies the problem of fair representation learning from the perspective of bi-level optimization. In particular, the authors propose an implicit approach to the problem that combines inner optimization and implicit differentiation. The authors show that the proposed approach is able to achieve better performance than existing methods in both classification and regression settings. "
6245,SP:9bd0a519881297066ee60ccf62ee27e4c109047d,"value - based methods USED-FOR Offline reinforcement learning ( RL ). temporal difference ( TD ) learning USED-FOR value - based methods. supervised learning methods USED-FOR optimal policies. methods COMPARE value - based approximate dynamic programming algorithms. value - based approximate dynamic programming algorithms COMPARE methods. design decisions PART-OF methods. policy architectures HYPONYM-OF design decisions. large sequence models CONJUNCTION value - based weighting schemes. value - based weighting schemes CONJUNCTION large sequence models. RvS methods COMPARE prior methods. prior methods COMPARE RvS methods. optimal data FEATURE-OF datasets. datasets HYPONYM-OF offline RL benchmarks. offline RL benchmarks EVALUATE-FOR prior methods. offline RL benchmarks EVALUATE-FOR RvS methods. Task is offline RL problem. Generic is task. Material is suboptimal data. Method is reinforcement learning via supervised learning ( RvS ). OtherScientificTerm are conditioning variable, and model capacity. ","This paper studies the problem of offline reinforcement learning via supervised learning (RvS). In this setting, the goal is to learn a policy that maximizes the performance of the agent on a set of suboptimal data. The authors propose to use temporal difference (TD) learning to learn the optimal policy for each sub-dataset. The proposed method is evaluated on a number of offline RL benchmarks and shows that the proposed method outperforms the state-of-the-art.","This paper studies the problem of offline reinforcement learning via supervised learning (RvS). In this setting, the goal is to learn a policy that maximizes the performance of the agent on a set of suboptimal data. The authors propose to use temporal difference (TD) learning to learn the optimal policy for each sub-dataset. The proposed method is evaluated on a number of offline RL benchmarks and shows that the proposed method outperforms the state-of-the-art."
6261,SP:af89e1cdd2b39df9982ca5cd9446ec66a4d317f2,computational cognitive mechanisms USED-FOR exploration algorithms. spatial information USED-FOR structure of unobserved spaces. Hierarchical Bayesian framework USED-FOR program induction. program induction USED-FOR compositional formation of proposed maps of complex spaces. Map Induction USED-FOR cognitive process. distribution of strong spatial priors USED-FOR model. computational framework USED-FOR human exploration behavior. computational framework COMPARE non - inductive models. non - inductive models COMPARE computational framework. Map Induction USED-FOR approximate planning algorithms. Task is behavioral Map Induction Task. Method is Map Induction framework. ,"This paper proposes a Bayesian framework for program induction in the context of behavioral map induction. The proposed method is based on a Hierarchical Bayesian approach to program induction, which is a generalization of the Bayesian program induction framework. The authors show that the proposed method can be applied to the task of behavioral Map Induction, and show that it can be used to improve the performance of exploration algorithms. ","This paper proposes a Bayesian framework for program induction in the context of behavioral map induction. The proposed method is based on a Hierarchical Bayesian approach to program induction, which is a generalization of the Bayesian program induction framework. The authors show that the proposed method can be applied to the task of behavioral Map Induction, and show that it can be used to improve the performance of exploration algorithms. "
6277,SP:cccdcc95c4177b5531bad23b662060fdd0d88849,"differentiable approach USED-FOR probabilistic factors. probabilistic factors USED-FOR inference. nonparametric belief propagation algorithm USED-FOR differentiable approach. nonparametric belief propagation algorithm USED-FOR inference. probabilistic factors PART-OF graphical model. domain - specific features FEATURE-OF probabilistic factors. domain - specific features USED-FOR nonparametric belief propagation methods. differentiable neural network USED-FOR factors. labeled data USED-FOR optimization routine. differentiable neural network USED-FOR crafted factor. optimization routine USED-FOR factors. differentiable neural networks CONJUNCTION belief propagation algorithm. belief propagation algorithm CONJUNCTION differentiable neural networks. method USED-FOR marginal posterior samples. differentiable neural networks USED-FOR method. end - to - end training USED-FOR marginal posterior samples. end - to - end training USED-FOR method. differentiable nonparametric belief propagation ( DNBP ) method COMPARE learned baselines. learned baselines COMPARE differentiable nonparametric belief propagation ( DNBP ) method. articulated pose tracking tasks EVALUATE-FOR differentiable nonparametric belief propagation ( DNBP ) method. learned factors USED-FOR tracking. Method are hand - crafted approaches, and Gradient Descent. OtherScientificTerm is Feature Extractor. ",This paper proposes a differentiable non-parametric belief propagation (DNBP) method. The key idea is to use differentiable neural networks to learn the probabilistic factors in a graphical model. The authors show that the proposed method outperforms the state-of-the-art baselines on articulated pose tracking tasks. ,This paper proposes a differentiable non-parametric belief propagation (DNBP) method. The key idea is to use differentiable neural networks to learn the probabilistic factors in a graphical model. The authors show that the proposed method outperforms the state-of-the-art baselines on articulated pose tracking tasks. 
6293,SP:76e858a6ef79a3bd861803395e25d7f65fd29a00,"deep learning - based modeling of molecules USED-FOR in silico drug discovery. scaffold USED-FOR drug discovery projects. MoLeR HYPONYM-OF graph - based model. MoLeR COMPARE approaches. approaches COMPARE MoLeR. MoLeR COMPARE methods. methods COMPARE MoLeR. MoLeR COMPARE them. them COMPARE MoLeR. them COMPARE approaches. approaches COMPARE them. scaffoldbased tasks EVALUATE-FOR MoLeR. scaffoldbased tasks EVALUATE-FOR them. unconstrained molecular optimization tasks EVALUATE-FOR methods. unconstrained molecular optimization tasks EVALUATE-FOR MoLeR. Method are generative models, and generative procedure. OtherScientificTerm are fragmentby - fragment, scaffolds, and generation history. Generic is constraint. ","This paper proposes a graph-based generative model for in silico drug discovery. The proposed method is based on the idea of scaffold-based models, which can be used to generate a scaffold of molecules that can then be used for optimization. The method is evaluated on a number of synthetic and real-world optimization tasks. The authors show that the proposed method outperforms several baselines.","This paper proposes a graph-based generative model for in silico drug discovery. The proposed method is based on the idea of scaffold-based models, which can be used to generate a scaffold of molecules that can then be used for optimization. The method is evaluated on a number of synthetic and real-world optimization tasks. The authors show that the proposed method outperforms several baselines."
6309,SP:318b3c294a475960c13a4914b035fd3a2ea84661,"Imitation learning algorithms USED-FOR policy. deterministic experts USED-FOR imitation learning. stationary reward FEATURE-OF reinforcement learning. reduction USED-FOR continuous control tasks. Task is recovery of expert reward. OtherScientificTerm is total variation distance. Method are imitation learner, and adversarial imitation learning. ","This paper studies the problem of imitation learning with deterministic experts. The authors show that the total variation distance between an expert and an imitation learner is a function of the expert’s total variation. They also show that if the expert is deterministic, then it is possible to learn a policy that is better than the policy learned by a deterministic expert. ","This paper studies the problem of imitation learning with deterministic experts. The authors show that the total variation distance between an expert and an imitation learner is a function of the expert’s total variation. They also show that if the expert is deterministic, then it is possible to learn a policy that is better than the policy learned by a deterministic expert. "
6325,SP:fd1a9b4c5ee36159286f4a35fa93ed0c23120906,reweighting algorithms USED-FOR machine learning models. reweighting algorithms USED-FOR fairness. machine learning models USED-FOR fairness. overparameterized setting FEATURE-OF algorithms. reweighting USED-FOR overparameterized model. worst - group test performance COMPARE ERM. ERM COMPARE worst - group test performance. overparameterized model USED-FOR ERM interpolator. reweighting algorithms USED-FOR interpolator. reweighting algorithms COMPARE ERM. ERM COMPARE reweighting algorithms. interpolator COMPARE ERM. ERM COMPARE interpolator. worst - group performance EVALUATE-FOR ERM. regularization CONJUNCTION data augmentation. data augmentation CONJUNCTION regularization. regularization USED-FOR reweighting algorithms. data augmentation USED-FOR reweighting algorithms. worst - group test performance EVALUATE-FOR reweighting algorithms. OtherScientificTerm is model parameters. Method is theoretical backing. Generic is model. ,"This paper studies the problem of reweighting algorithms in the overparameterized setting. The authors propose ERM, which is a method to reweight the model parameters of an overparametrized model in order to improve the performance of the model in the worst-case setting. They show that ERM outperforms the existing methods in terms of worst-group test performance. They also provide theoretical backing for the proposed method.","This paper studies the problem of reweighting algorithms in the overparameterized setting. The authors propose ERM, which is a method to reweight the model parameters of an overparametrized model in order to improve the performance of the model in the worst-case setting. They show that ERM outperforms the existing methods in terms of worst-group test performance. They also provide theoretical backing for the proposed method."
6341,SP:318ace9202e42d1d278eb79fe1853138e1d00a06,"Multi - agent reinforcement learning ( MARL ) USED-FOR emergent behavior. emergent behavior PART-OF complex agent - based simulations. model USED-FOR human - irrationality. Rational Inattention ( RI ) model HYPONYM-OF model. model PART-OF human - like RL agents. RI USED-FOR cost of cognitive information processing. mutual information USED-FOR RI. mutual information USED-FOR cost of cognitive information processing. multi - timestep dynamics CONJUNCTION information channels. information channels CONJUNCTION multi - timestep dynamics. heterogeneous processing costs FEATURE-OF information channels. RI USED-FOR information asymmetry. RIRL USED-FOR equilibrium behaviors. RIRL USED-FOR AI agents. AI agents USED-FOR real human behavior. Method are RL agents, and RIRL framework. OtherScientificTerm are human behavior, rational assumptions, Principal ’s inattention, inattention, and rationality assumptions. Metric is Agent welfare. ","This paper proposes a new model for multi-agent reinforcement learning (MARL) based on Rational Inattention (RIRL). The model is based on the observation that human behavior is subject to human-irrationality, and the authors propose to use this observation to improve the performance of RL agents. The authors show that the RIRL model is able to capture the information asymmetry between information channels and multi-timestep dynamics in MARL, and that it can be used to improve agent welfare. ","This paper proposes a new model for multi-agent reinforcement learning (MARL) based on Rational Inattention (RIRL). The model is based on the observation that human behavior is subject to human-irrationality, and the authors propose to use this observation to improve the performance of RL agents. The authors show that the RIRL model is able to capture the information asymmetry between information channels and multi-timestep dynamics in MARL, and that it can be used to improve agent welfare. "
6357,SP:100c91da177504d89f1819f4fdce72ebcf848902,perturbations USED-FOR Audio adversarial attacks. lp - norm CONJUNCTION auditory masking. auditory masking CONJUNCTION lp - norm. auditory masking USED-FOR magnitude spectrogram. lp - norm CONJUNCTION waveform. waveform CONJUNCTION lp - norm. perturbations USED-FOR audio adversarial attacks. phaseoriented algorithm USED-FOR imperceptible audio adversarial examples. energy dissipation FEATURE-OF imperceptible audio adversarial examples. PhaseFool HYPONYM-OF phaseoriented algorithm. energy patterns PART-OF spectrogram. spectrogram consistency USED-FOR short - time Fourier transform ( STFT ). weighted loss function USED-FOR PhaseFool. imperceptibility EVALUATE-FOR PhaseFool. weighted loss function USED-FOR imperceptibility. PhaseFool COMPARE imperceptible counterparts. imperceptible counterparts COMPARE PhaseFool. PhaseFool USED-FOR full - sentence imperceptible audio adversarial examples. realistic simulated environmental distortions USED-FOR adversarial examples. PhaseFool USED-FOR adversarial examples. phase - oriented energy dissipation FEATURE-OF audio adversarial example. PhaseFool COMPARE perturbations. perturbations COMPARE PhaseFool. perturbations FEATURE-OF audio waveform. phase - oriented energy dissipation COMPARE perturbations. perturbations COMPARE phase - oriented energy dissipation. phase - oriented energy dissipation USED-FOR PhaseFool. Method is automatic speech recognition ( ASR ) model. OtherScientificTerm is phase spectrogram. ,"This paper proposes a phase-oriented algorithm for audio adversarial attacks. The proposed method is based on the idea of phase-based energy dissipation, which can be used to improve the imperceptibility of audio perturbations. The authors show that the proposed method can achieve better performance than previous approaches on a range of adversarial examples, including full-sentence imperceptible audio examples. ","This paper proposes a phase-oriented algorithm for audio adversarial attacks. The proposed method is based on the idea of phase-based energy dissipation, which can be used to improve the imperceptibility of audio perturbations. The authors show that the proposed method can achieve better performance than previous approaches on a range of adversarial examples, including full-sentence imperceptible audio examples. "
6373,SP:713c57555a88d922516f42e7ff0ddd5bfbd90a24,Non - contrastive methods USED-FOR representations. Non - contrastive methods USED-FOR self - supervised learning. BYOL HYPONYM-OF Non - contrastive methods. BYOL HYPONYM-OF self - supervised learning. augmentation process USED-FOR representation. DirectPred USED-FOR predictor. DirectSet(α ) USED-FOR projection matrix. sample complexity EVALUATE-FOR downstream tasks. DirectSet(α ) USED-FOR downstream tasks. DirectSet(α ) USED-FOR linear network. sample complexity EVALUATE-FOR DirectSet(α ). weight decay USED-FOR implicit threshold. eigen - decomposition step USED-FOR DirectPred. CIFAR-100 CONJUNCTION STL-10. STL-10 CONJUNCTION CIFAR-100. STL-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION STL-10. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. DirectCopy COMPARE DirectPred. DirectPred COMPARE DirectCopy. CIFAR-10 EVALUATE-FOR DirectCopy. ImageNet EVALUATE-FOR DirectCopy. CIFAR-10 EVALUATE-FOR DirectPred. CIFAR-10 CONJUNCTION STL-10. STL-10 CONJUNCTION CIFAR-10. Generic is approaches. Task is augmentation. ,"This paper proposes a new method for self-supervised learning. The proposed method, DirectSet(α), is based on BYOL and DirectCopy. The main contribution of the paper is to propose a new augmentation method called DirectSet (α) to improve the sample complexity of BYOL. The authors show that the proposed method outperforms BYOL on CIFAR-10, STL-10 and ImageNet. ","This paper proposes a new method for self-supervised learning. The proposed method, DirectSet(α), is based on BYOL and DirectCopy. The main contribution of the paper is to propose a new augmentation method called DirectSet (α) to improve the sample complexity of BYOL. The authors show that the proposed method outperforms BYOL on CIFAR-10, STL-10 and ImageNet. "
6389,SP:8433900e40c5c5df1f003dd1d4fb08c7aafd51f8,method USED-FOR learning long - term sequential dependencies. Long Expressive Memory ( LEM ) USED-FOR learning long - term sequential dependencies. it USED-FOR input - output maps. it USED-FOR sequential tasks. long - term dependencies FEATURE-OF sequential tasks. time - discretization USED-FOR system. system of multiscale ordinary differential equations CONJUNCTION time - discretization. time - discretization CONJUNCTION system of multiscale ordinary differential equations. system of multiscale ordinary differential equations USED-FOR LEM. rigorous bounds USED-FOR exploding and vanishing gradients problem. rigorous bounds USED-FOR LEM. LEM USED-FOR dynamical systems. accuracy EVALUATE-FOR LEM. gated recurrent units CONJUNCTION long short - term memory models. long short - term memory models CONJUNCTION gated recurrent units. recurrent neural networks CONJUNCTION gated recurrent units. gated recurrent units CONJUNCTION recurrent neural networks. LEM COMPARE recurrent neural networks. recurrent neural networks COMPARE LEM. image and time - series classification CONJUNCTION dynamical systems prediction. dynamical systems prediction CONJUNCTION image and time - series classification. LEM COMPARE long short - term memory models. long short - term memory models COMPARE LEM. dynamical systems prediction CONJUNCTION keyword spotting and language modeling. keyword spotting and language modeling CONJUNCTION dynamical systems prediction. LEM COMPARE gated recurrent units. gated recurrent units COMPARE LEM. image and time - series classification CONJUNCTION keyword spotting and language modeling. keyword spotting and language modeling CONJUNCTION image and time - series classification. Method is gradient - based recurrent sequential learning methods. ,"This paper proposes a method for learning long-term sequential dependencies. The method is based on gradient-based recurrent sequential learning methods and is able to solve exploding and vanishing gradients problems. The proposed method is evaluated on a variety of tasks, including image and time-series classification, dynamical systems prediction and language modeling.","This paper proposes a method for learning long-term sequential dependencies. The method is based on gradient-based recurrent sequential learning methods and is able to solve exploding and vanishing gradients problems. The proposed method is evaluated on a variety of tasks, including image and time-series classification, dynamical systems prediction and language modeling."
6405,SP:dbf896dd31627b27f0a902c716aff940e5ab7ac2,"symmetry USED-FOR architectures. architectures USED-FOR deep learning. rotation CONJUNCTION permutation equivariance. permutation equivariance CONJUNCTION rotation. translation CONJUNCTION rotation. rotation CONJUNCTION translation. rotationand permutation - equivariant architectures USED-FOR deep learning. rotationand permutation - equivariant architectures USED-FOR small point clouds. products USED-FOR reductions. reductions PART-OF rotationand permutation - equivariant architectures. attention mechanism USED-FOR reductions. rotation invariance CONJUNCTION covariance. covariance CONJUNCTION rotation invariance. attention USED-FOR permutation equivariance. chemistry CONJUNCTION biology. biology CONJUNCTION chemistry. physics CONJUNCTION chemistry. chemistry CONJUNCTION physics. models USED-FOR architectures. Method are geometric deep learning, and geometric algebra. Task is physical sciences. OtherScientificTerm are twoor three - dimensional space, and vector. ","This paper proposes a geometric deep learning algorithm for small point clouds. The proposed algorithm is based on the rotation-equivariant and permutation equivariant algorithms. The authors show that the proposed algorithm can be used to reduce the number of points in a point cloud to a single point. The paper also shows that the algorithm is able to learn a reduction function that is invariant to rotation, rotation invariance, and rotation equivariance. The algorithm is shown to be able to achieve state-of-the-art performance on a number of datasets.","This paper proposes a geometric deep learning algorithm for small point clouds. The proposed algorithm is based on the rotation-equivariant and permutation equivariant algorithms. The authors show that the proposed algorithm can be used to reduce the number of points in a point cloud to a single point. The paper also shows that the algorithm is able to learn a reduction function that is invariant to rotation, rotation invariance, and rotation equivariance. The algorithm is shown to be able to achieve state-of-the-art performance on a number of datasets."
6421,SP:5a6099feb5da2c35f99d4d76c7e0ff3cd3e9c196,order fulfillment problem HYPONYM-OF combinatorial optimization problems. it USED-FOR online retailing. combinatorial optimization problems PART-OF supply chain management. order fulfillment problem HYPONYM-OF supply chain management. exact mathematical programming methods USED-FOR problem. machine learning method USED-FOR it. tripartite graph USED-FOR machine learning method. edge - featureembedded graph attention mechanism USED-FOR assignment policy. edge - feature - embedded graph attention USED-FOR optimization problem. edge - feature - embedded graph attention USED-FOR heterogeneous information. edge - feature - embedded graph attention USED-FOR high - dimensional edge features. model COMPARE baseline heuristic method. baseline heuristic method COMPARE model. optimality EVALUATE-FOR baseline heuristic method. optimality EVALUATE-FOR model. online inference time COMPARE mathematical programming methods. mathematical programming methods COMPARE online inference time. ,This paper proposes a novel approach to solve the order fulfillment problem. The authors propose to use edge-feature-embedded graph attention mechanism to learn a tripartite graph which is then used to learn an assignment policy. The proposed approach is evaluated on a number of online optimization problems and shows promising results. ,This paper proposes a novel approach to solve the order fulfillment problem. The authors propose to use edge-feature-embedded graph attention mechanism to learn a tripartite graph which is then used to learn an assignment policy. The proposed approach is evaluated on a number of online optimization problems and shows promising results. 
6437,SP:7f7f8245914ecc5b00570916bbcdb6c9b49d26de,"natural language processing models USED-FOR inference process. inferring Concepts PART-OF dialogue summarization task. CODC inference module CONJUNCTION knowledge attention module. knowledge attention module CONJUNCTION CODC inference module. knowledge attention module PART-OF neural summarization model. CODC inference module PART-OF framework. knowledge attention module PART-OF framework. CODC USED-FOR evaluation metric. evaluation metric EVALUATE-FOR methods. automatic evaluation metrics EVALUATE-FOR out - of - context inference. automatic evaluation metrics EVALUATE-FOR natural language generation. CODC inference CONJUNCTION automatic evaluation metrics. automatic evaluation metrics CONJUNCTION CODC inference. natural language generation EVALUATE-FOR out - of - context inference. CODC inference EVALUATE-FOR summarization model. automatic evaluation metrics EVALUATE-FOR summarization model. CIDEr HYPONYM-OF automatic evaluation metrics. model EVALUATE-FOR model. Material are human dialogues, and WordNet. OtherScientificTerm are pragmatics studies, and Dialogue Context ( CODC ). ",This paper proposes a framework for dialogue summarization based on Dialogue Context (CODC) and Knowledge Attention (KA). The proposed framework is based on a neural summarization model with a knowledge attention module and a CODC inference module. The proposed method is evaluated on natural language generation and out-of-context inference tasks. ,This paper proposes a framework for dialogue summarization based on Dialogue Context (CODC) and Knowledge Attention (KA). The proposed framework is based on a neural summarization model with a knowledge attention module and a CODC inference module. The proposed method is evaluated on natural language generation and out-of-context inference tasks. 
6446,SP:e1591b266d6c329c6c07f4e5234253249ab1db8c,"entity embeddings USED-FOR Artificial Intelligence. embedding models USED-FOR Horn rules. embedding strategies USED-FOR monotonic and non - monotonic attribute dependencies. OtherScientificTerm are embeddings, vectors, semantic dependencies, and attribute dependencies. Method is attribute embeddings. ","This paper studies the problem of learning embeddings for monotonic and non-monotonic attribute dependencies. The authors propose a new embedding model that is able to learn monotonicity and nonmonotonicity for attribute embedding. The proposed model is based on the notion of ""attribute dependencies"", which is a special case of the ""Horn rule"". The authors show that the proposed model can learn the embedding for both monotonically monotonical and monotone attributes. The paper also shows that the learned embedding can be used to learn embedding strategies that are able to generalize well to non-Monotonic attributes. ","This paper studies the problem of learning embeddings for monotonic and non-monotonic attribute dependencies. The authors propose a new embedding model that is able to learn monotonicity and nonmonotonicity for attribute embedding. The proposed model is based on the notion of ""attribute dependencies"", which is a special case of the ""Horn rule"". The authors show that the proposed model can learn the embedding for both monotonically monotonical and monotone attributes. The paper also shows that the learned embedding can be used to learn embedding strategies that are able to generalize well to non-Monotonic attributes. "
6455,SP:794cca5205d667900ceb9a1332b6272320752ef4,transformer - based models USED-FOR natural language processing tasks. transformers USED-FOR natural language. commonsense reasoning CONJUNCTION logical reasoning. logical reasoning CONJUNCTION commonsense reasoning. mathematical reasoning CONJUNCTION commonsense reasoning. commonsense reasoning CONJUNCTION mathematical reasoning. transformers USED-FOR reasoning tasks. mathematical reasoning HYPONYM-OF reasoning tasks. logical reasoning HYPONYM-OF reasoning tasks. commonsense reasoning HYPONYM-OF reasoning tasks. ,"This paper proposes a transformer-based model for natural language processing tasks. The proposed model is based on the Transformer architecture. The authors show that the proposed model outperforms the state-of-the-art models on a number of tasks, including logical reasoning, mathematical reasoning, commonsense reasoning, and mathematical reasoning. ","This paper proposes a transformer-based model for natural language processing tasks. The proposed model is based on the Transformer architecture. The authors show that the proposed model outperforms the state-of-the-art models on a number of tasks, including logical reasoning, mathematical reasoning, commonsense reasoning, and mathematical reasoning. "
6464,SP:3a16ffa27e7ef0684e6d0f3ee744787aef108a07,"paradigms PART-OF machine learning. complexity FEATURE-OF tasks. algorithmic procedures USED-FOR representation transformations. domaingeneral framework USED-FOR algorithmic procedures. compositional recursive learner HYPONYM-OF domaingeneral framework. compositional recursive learner USED-FOR compositional generalization. compositional approach COMPARE baselines. baselines COMPARE compositional approach. compositional approach COMPARE learner. learner COMPARE compositional approach. Generic are it, and they. Task are generalization, and compositional generalization problem. OtherScientificTerm are compositional structure of the task distribution, compositional problem graph, and shared subproblems. ","This paper proposes a compositional recursive learner for generalization. The key idea is to learn the compositional structure of the task distribution, which is then used to learn an algorithm for compositional generalisation. The authors show that the proposed method outperforms the baselines in terms of generalization performance.","This paper proposes a compositional recursive learner for generalization. The key idea is to learn the compositional structure of the task distribution, which is then used to learn an algorithm for compositional generalisation. The authors show that the proposed method outperforms the baselines in terms of generalization performance."
6468,SP:7f91f3805bd643e3b796e885b00f88a77aa49d15,"weight pruning HYPONYM-OF network model compression. activation pruning CONJUNCTION weight pruning. weight pruning CONJUNCTION activation pruning. weight pruning PART-OF Integral Pruning ( IP ) technique. activation pruning PART-OF Integral Pruning ( IP ) technique. IPnet HYPONYM-OF network. execution efficiency EVALUATE-FOR network. activation functions FEATURE-OF network models. datasets EVALUATE-FOR network models. network models EVALUATE-FOR IPnet. datasets EVALUATE-FOR IPnet. testing accuracy EVALUATE-FOR IPnet. IPnet COMPARE dense models. dense models COMPARE IPnet. computation cost EVALUATE-FOR IPnet. Method are deep neural networks ( DNNs ), and DNNs. Task is compression. OtherScientificTerm are weights, connections, sparsity, and activation and weight numbers. ","This paper proposes a method for weight pruning and activation pruning in deep neural networks. The authors propose a method called Integral Pruning (IP) to reduce the number of activation and weight numbers in a network. The method is based on the idea of Integral pruning, which prunes the weights and activations of a network to reduce its sparsity.  The authors show that the proposed method is able to achieve competitive performance on several benchmark datasets. ","This paper proposes a method for weight pruning and activation pruning in deep neural networks. The authors propose a method called Integral Pruning (IP) to reduce the number of activation and weight numbers in a network. The method is based on the idea of Integral pruning, which prunes the weights and activations of a network to reduce its sparsity.  The authors show that the proposed method is able to achieve competitive performance on several benchmark datasets. "
6472,SP:d34277109f713f78abd3b911c7a38baf18c8c8c1,"discovery of relevant features USED-FOR task. Machine learning driven feature selection USED-FOR discovery. methodology USED-FOR False Discovery Rate. Generative Adversarial Networks framework USED-FOR knockoffs. stability network CONJUNCTION power network. power network CONJUNCTION stability network. generator CONJUNCTION discriminator. discriminator CONJUNCTION generator. discriminator CONJUNCTION stability network. stability network CONJUNCTION discriminator. networks CONJUNCTION generator. generator CONJUNCTION networks. generator CONJUNCTION stability network. stability network CONJUNCTION generator. discriminator CONJUNCTION power network. power network CONJUNCTION discriminator. networks PART-OF model. discriminator PART-OF model. stability network PART-OF model. generator PART-OF model. power network PART-OF model. model USED-FOR feature selection. it COMPARE model. model COMPARE it. it COMPARE knockoff generation model. knockoff generation model COMPARE it. model USED-FOR non - Gaussian settings. knockoff generation model USED-FOR Gaussian setting. non - Gaussian settings EVALUATE-FOR it. real - world dataset EVALUATE-FOR it. real - world dataset EVALUATE-FOR model. Task are Feature selection, and overfitting in prediction. OtherScientificTerm are relevant genetic factors, features, and feature distribution. Metric is expert time. Method is Knockoff framework. ","This paper proposes a new method for generating knockoffs for feature selection. The method is based on the idea of Generative Adversarial Networks (GANs), which is a generative adversarial network with a discriminator and a generator. The proposed method is evaluated on a number of real-world datasets and shows that it outperforms the baselines in terms of the False Discovery Rate (FDR). ","This paper proposes a new method for generating knockoffs for feature selection. The method is based on the idea of Generative Adversarial Networks (GANs), which is a generative adversarial network with a discriminator and a generator. The proposed method is evaluated on a number of real-world datasets and shows that it outperforms the baselines in terms of the False Discovery Rate (FDR). "
6476,SP:7bf79b020c2cafaced61f2595ad17e8238c3dc5d,"Pruning techniques CONJUNCTION Winograd convolution. Winograd convolution CONJUNCTION Pruning techniques. Winograd convolution USED-FOR CNN computation. Pruning techniques USED-FOR CNN computation. Winograd transformation USED-FOR sparsity. learning rates USED-FOR Winograd - domain retraining. ReLU function PART-OF Winograd domain. pruning method USED-FOR Winograd - domain weight sparsity. spatial - Winograd pruning HYPONYM-OF pruning method. importance factor matrix USED-FOR weight importance. importance factor matrix USED-FOR weight gradients. weight importance CONJUNCTION weight gradients. weight gradients CONJUNCTION weight importance. pruning PART-OF Winograd domain. CIFAR-100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-100. CIFAR10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR10. models EVALUATE-FOR method. datasets EVALUATE-FOR method. datasets EVALUATE-FOR models. CIFAR10 HYPONYM-OF datasets. ImageNet HYPONYM-OF datasets. CIFAR-100 HYPONYM-OF datasets. Winograddomain sparsities EVALUATE-FOR method. Method are Deep convolutional neural networks ( CNNs ), sparse Winograd convolution, and Winograd - domain network. Generic are they, and technique. OtherScientificTerm are weight sparsity, network structure, network structures, spatial - domain weights, and spatial - domain sparsity. ","This paper proposes a method to reduce the sparsity of convolutional neural networks by pruning the weights in the Winograd domain. The authors propose a spatial-Winograd pruning method, which prunes the weights of the network in the spatial-domain. The method is based on the notion of importance factor matrix, which is a weighted sum of the importance factor of the weight gradients and the weight importance of the ReLU function. The proposed method is evaluated on CIFAR-10, ImageNet, and Cifar-100 datasets. ","This paper proposes a method to reduce the sparsity of convolutional neural networks by pruning the weights in the Winograd domain. The authors propose a spatial-Winograd pruning method, which prunes the weights of the network in the spatial-domain. The method is based on the notion of importance factor matrix, which is a weighted sum of the importance factor of the weight gradients and the weight importance of the ReLU function. The proposed method is evaluated on CIFAR-10, ImageNet, and Cifar-100 datasets. "
6480,SP:35e050c84f55f30b5a958128fa5bdaa1cb3f7e90,Adversarially Learned Mixture Model ( AMM ) HYPONYM-OF generative model. generative model USED-FOR unsupervised or semi - supervised data clustering. adversarially optimized method USED-FOR conditional dependence. AMM USED-FOR conditional dependence. AMM HYPONYM-OF adversarially optimized method. AMM USED-FOR semantic separation of complex data. MNIST and SVHN datasets EVALUATE-FOR AMM. unsupervised clustering error rates EVALUATE-FOR AMM. MNIST and SVHN datasets EVALUATE-FOR AMM. AMM USED-FOR semi - supervised extension. SVHN dataset EVALUATE-FOR semi - supervised extension. classification error rate EVALUATE-FOR semi - supervised extension. Material is labeled data. ,"This paper proposes a generative model for unsupervised or semi-supervised data clustering. The proposed method is based on adversarially learned Mixture Model (AMM), which is an extension of Adversarially Learned Mixture Models (AMMs). AMM is used to learn the conditional dependence between two data points, which is then used to improve the clustering performance. The authors show that the proposed method outperforms AMMs on MNIST and SVHN datasets. ","This paper proposes a generative model for unsupervised or semi-supervised data clustering. The proposed method is based on adversarially learned Mixture Model (AMM), which is an extension of Adversarially Learned Mixture Models (AMMs). AMM is used to learn the conditional dependence between two data points, which is then used to improve the clustering performance. The authors show that the proposed method outperforms AMMs on MNIST and SVHN datasets. "
6484,SP:c65ea3a1cc796e65465e8b4dc05ae103316e2cb3,"stochastic binary layers USED-FOR gradients. reparameterization USED-FOR ARM estimator. REINFORCE CONJUNCTION reparameterization. reparameterization CONJUNCTION REINFORCE. variable augmentation CONJUNCTION REINFORCE. REINFORCE CONJUNCTION variable augmentation. variable augmentation USED-FOR ARM estimator. REINFORCE USED-FOR ARM estimator. adaptive variance reduction USED-FOR Monte Carlo integration. ARM estimator USED-FOR adaptive variance reduction. REINFORCE estimator USED-FOR augmented space. antithetic sampling USED-FOR augmented space. variance - reduction mechanism USED-FOR ARM estimator. antithetic sampling USED-FOR variance - reduction mechanism. auto - encoding variational inference CONJUNCTION maximum likelihood estimation. maximum likelihood estimation CONJUNCTION auto - encoding variational inference. ARM estimator USED-FOR discrete latent variable models. ARM estimator USED-FOR maximum likelihood estimation. ARM estimator USED-FOR auto - encoding variational inference. stochastic binary layers FEATURE-OF discrete latent variable models. Metric is computational complexity. OtherScientificTerm are common random numbers, and Python code. ","This paper proposes an adaptive variance reduction (ARM) estimator for discrete latent variable models with stochastic binary layers. The main contribution of the paper is to introduce a variance-reduction mechanism that can be used to reduce the variance of the estimator in the augmented space. The proposed estimator can be applied to Monte Carlo integration, auto-encoding variational inference, and maximum likelihood estimation. Experiments show that the proposed method outperforms the state-of-the-art in terms of computational complexity. ","This paper proposes an adaptive variance reduction (ARM) estimator for discrete latent variable models with stochastic binary layers. The main contribution of the paper is to introduce a variance-reduction mechanism that can be used to reduce the variance of the estimator in the augmented space. The proposed estimator can be applied to Monte Carlo integration, auto-encoding variational inference, and maximum likelihood estimation. Experiments show that the proposed method outperforms the state-of-the-art in terms of computational complexity. "
6488,SP:c54ee7a7d321a487257d2554c7e689967cf0ceaa,Modularity PART-OF deep learning libraries. probabilistic distributions CONJUNCTION probabilistic model. probabilistic model CONJUNCTION probabilistic distributions. modularity FEATURE-OF probabilistic programming language. Bayesian nonparametric models HYPONYM-OF probabilistic model. probabilistic modules HYPONYM-OF re - usable building blocks. re - usable building blocks PART-OF modular probabilistic programming language. probabilistic distributions FEATURE-OF random variables. random variables PART-OF probabilistic module. probabilistic distributions PART-OF probabilistic module. inference methods PART-OF probabilistic module. pre - specified inference methods USED-FOR probabilistic modules. pre - specified inference methods USED-FOR variational inference. probabilistic modules PART-OF MXFusion. Gaussian process models EVALUATE-FOR probabilistic modules. real data EVALUATE-FOR probabilistic modules. Method is probabilistic programming. ,"This paper proposes a new probabilistic programming language, called MXFusion, which is based on the idea of modularity. The core idea of the proposed model is to combine probabilistics with Bayesian nonparametric models. The proposed model can be viewed as a combination of a probabilism model and a variational inference module. The authors show that the proposed method is able to achieve better performance than the state-of-the-art on a variety of benchmarks. ","This paper proposes a new probabilistic programming language, called MXFusion, which is based on the idea of modularity. The core idea of the proposed model is to combine probabilistics with Bayesian nonparametric models. The proposed model can be viewed as a combination of a probabilism model and a variational inference module. The authors show that the proposed method is able to achieve better performance than the state-of-the-art on a variety of benchmarks. "
6492,SP:b65eb92fcbea57626721a156be6e6cbbad3c071c,"heuristically designed pruning schedules CONJUNCTION hyperparameters. hyperparameters CONJUNCTION heuristically designed pruning schedules. heuristically designed pruning schedules USED-FOR iterative optimization procedure. pruning PART-OF methods. iterative optimization procedure USED-FOR pruning. approach USED-FOR network. structurally important connections PART-OF network. connection sensitivity USED-FOR structurally important connections. network USED-FOR task. structurally important connections USED-FOR task. connection sensitivity USED-FOR saliency criterion. pretraining CONJUNCTION complex pruning schedule. complex pruning schedule CONJUNCTION pretraining. it USED-FOR architecture variations. pruning USED-FOR sparse network. method COMPARE reference network. reference network COMPARE method. accuracy EVALUATE-FOR reference network. method USED-FOR architectures. method USED-FOR sparse networks. sparse networks COMPARE reference network. reference network COMPARE sparse networks. Tiny - ImageNet classification tasks EVALUATE-FOR method. convolutional, residual and recurrent networks HYPONYM-OF architectures. MNIST EVALUATE-FOR method. Tiny - ImageNet classification tasks EVALUATE-FOR reference network. MNIST EVALUATE-FOR reference network. accuracy EVALUATE-FOR method. methods COMPARE approach. approach COMPARE methods. Task are Pruning large neural networks, and training. Metric is reduced space and time complexity. ","This paper proposes a method for pruning large neural networks. The method is based on the idea of saliency criterion, which is an iterative optimization procedure to find the most important connections in the network. The authors show that the proposed method is able to reduce the space and time complexity of pruning while maintaining the performance of the original network. They also show that their method can be applied to a variety of architectures.","This paper proposes a method for pruning large neural networks. The method is based on the idea of saliency criterion, which is an iterative optimization procedure to find the most important connections in the network. The authors show that the proposed method is able to reduce the space and time complexity of pruning while maintaining the performance of the original network. They also show that their method can be applied to a variety of architectures."
6496,SP:986b9781534ffec84619872cd269ad48d235f869,"inference algorithm USED-FOR decoding neural sequence models. Beam search HYPONYM-OF inference algorithm. Beam search USED-FOR decoding neural sequence models. greedy search COMPARE beam search. beam search COMPARE greedy search. beam search USED-FOR non - greedy local decisions. beam widths USED-FOR beam search. beam search algorithm USED-FOR sequence synthesis tasks. evaluation score FEATURE-OF sequences. highly non - greedy decisions USED-FOR beam search. constrained beam search USED-FOR beam search degradation. methods USED-FOR search. OtherScientificTerm are beam width, early and highly non - greedy decisions, and ( conditional ) probability. Metric is evaluation scores. Material is copies. ",This paper proposes a beam search algorithm for decoding neural sequence models. The proposed method is based on the idea that beam search can be used to find high-probability local decisions in the early and highly non-greedy part of the search process. The authors show that the proposed method outperforms the state-of-the-art beam search methods in terms of the evaluation score of a sequence. They also show that their method can be applied to sequence synthesis tasks.,This paper proposes a beam search algorithm for decoding neural sequence models. The proposed method is based on the idea that beam search can be used to find high-probability local decisions in the early and highly non-greedy part of the search process. The authors show that the proposed method outperforms the state-of-the-art beam search methods in terms of the evaluation score of a sequence. They also show that their method can be applied to sequence synthesis tasks.
6500,SP:b2a8f5c3a417390582f26981fe0c81c16d2bb07d,"method USED-FOR sample efficiency. demonstrations USED-FOR method. curriculum USED-FOR task. Backplay COMPARE competitive methods. competitive methods COMPARE Backplay. Backplay USED-FOR large grid worlds. sample efficiency EVALUATE-FOR competitive methods. training speed EVALUATE-FOR Backplay. sample efficiency EVALUATE-FOR Backplay. reward shaping CONJUNCTION behavioral cloning. behavioral cloning CONJUNCTION reward shaping. behavioral cloning CONJUNCTION reverse curriculum generation. reverse curriculum generation CONJUNCTION behavioral cloning. Task is Model - free reinforcement learning ( RL ). OtherScientificTerm are policy, and sparse rewards. Generic is approach. ","This paper proposes a new method for model-free reinforcement learning (MRL). The proposed method, called Backplay, is based on the idea of learning a curriculum for a given task. The curriculum consists of two steps: (1) learning a policy that maximizes the sample efficiency and (2) learning an environment that is similar to the environment in which the policy is learned. The authors show that the proposed method outperforms the state-of-the-art in terms of training speed and sample efficiency. ","This paper proposes a new method for model-free reinforcement learning (MRL). The proposed method, called Backplay, is based on the idea of learning a curriculum for a given task. The curriculum consists of two steps: (1) learning a policy that maximizes the sample efficiency and (2) learning an environment that is similar to the environment in which the policy is learned. The authors show that the proposed method outperforms the state-of-the-art in terms of training speed and sample efficiency. "
6504,SP:426c98718b2dbad640380ec4ccb2b656958389bc,"Model compression USED-FOR large neural networks. model size CONJUNCTION accuracy. accuracy CONJUNCTION model size. hand - crafted heuristics USED-FOR compression techniques. AlexNet CONJUNCTION VGG16. VGG16 CONJUNCTION AlexNet. hyper - parameters USED-FOR method. expert knowledge USED-FOR method. OtherScientificTerm are compression ratio, compression ratios, Hessian, and pruning criterion. Method are Multi - Layer Pruning method ( MLPrune ), and Kroneckerfactored Approximate Curvature method. Generic is state - of - theart. Material is ImageNet. ","This paper proposes a new method for model compression. The proposed method is based on the Kronecker-factored Approximate Curvature (KCA) method, which is a popular heuristic for model pruning. The method is evaluated on ImageNet and VGG16 and achieves state-of-the-art results. ","This paper proposes a new method for model compression. The proposed method is based on the Kronecker-factored Approximate Curvature (KCA) method, which is a popular heuristic for model pruning. The method is evaluated on ImageNet and VGG16 and achieves state-of-the-art results. "
6508,SP:b97549a4c1f4b2407f97576fed46c25cbf669009,"sample quality CONJUNCTION training stability. training stability CONJUNCTION sample quality. Generative Adversarial Networks ( GANs ) USED-FOR real - world applications. training stability EVALUATE-FOR GAN variants. sample quality EVALUATE-FOR GAN variants. architectural choices USED-FOR GAN learning. analytic framework USED-FOR GANs. unit-, object-, and scene - level FEATURE-OF GANs. object concepts FEATURE-OF interpretable units. segmentation - based network dissection method USED-FOR interpretable units. object concepts PART-OF images. models CONJUNCTION datasets. datasets CONJUNCTION models. internal representations CONJUNCTION models. models CONJUNCTION internal representations. artifact - causing units USED-FOR GANs. framework USED-FOR applications. open source interpretation tools USED-FOR GAN models. Generic are they, and units. Method is GAN. OtherScientificTerm is interventions. ","This paper proposes a new analytic framework for GANs, which is based on segmentation-based network dissection. The proposed method is able to disentangle the unit-level, object-level and scene-level aspects of GAN learning. The method is evaluated on a variety of datasets, including CIFAR-10, ImageNet, MNIST, and Cifar-100. The results show that the proposed method outperforms existing methods in terms of sample quality and training stability.","This paper proposes a new analytic framework for GANs, which is based on segmentation-based network dissection. The proposed method is able to disentangle the unit-level, object-level and scene-level aspects of GAN learning. The method is evaluated on a variety of datasets, including CIFAR-10, ImageNet, MNIST, and Cifar-100. The results show that the proposed method outperforms existing methods in terms of sample quality and training stability."
6512,SP:252c20661ef36f8c32f7412db315747925d3a3d0,"parameter ` distances COMPARE function L distances. function L distances COMPARE parameter ` distances. space FEATURE-OF networks. L distance USED-FOR optimization. L - space FEATURE-OF network. loss curvature HYPONYM-OF local approximations. Method is neural network. OtherScientificTerm are L Hilbert space, optimization trajectory, catastrophic forgetting, learning rule, and function distances. Generic are distances, and applications. Metric is L/ ` ratio. Task is multitask learning. ","This paper studies the problem of multi-task learning in the L-space of neural networks. The authors show that the loss curvature of a neural network is a function of the number of parameters in the network, and that the L distance between the parameters of the network and the function L distances of the function is a measure of the distance between two points in the space. The paper also shows that the convergence rate of the learning rule is bounded by the L/\sqrt{L} ratio of the L distances between the two points. ","This paper studies the problem of multi-task learning in the L-space of neural networks. The authors show that the loss curvature of a neural network is a function of the number of parameters in the network, and that the L distance between the parameters of the network and the function L distances of the function is a measure of the distance between two points in the space. The paper also shows that the convergence rate of the learning rule is bounded by the L/\sqrt{L} ratio of the L distances between the two points. "
6516,SP:f6cb7efaef82aff9849c8e157bfe5db5092a6271,measurements of dynamic entities PART-OF Biological data. DyMoN HYPONYM-OF neural network framework. probability distribution FEATURE-OF deep generative Markov model. deep generative Markov model USED-FOR neural network framework. deep generative Markov model USED-FOR DyMoN. Dynamics Modeling Network HYPONYM-OF deep neural network framework. DyMoN USED-FOR idiosyncrasies of biological data. noise CONJUNCTION sparsity. sparsity CONJUNCTION noise. data USED-FOR probability distributions. trajectories HYPONYM-OF probability distributions. dimensionality reduction methods USED-FOR trajectories. probability distributions USED-FOR DyMoN. training efficiency CONJUNCTION accuracy. accuracy CONJUNCTION training efficiency. Kalman filters CONJUNCTION hidden Markov models. hidden Markov models CONJUNCTION Kalman filters. deep models COMPARE shallow models. shallow models COMPARE deep models. hidden Markov models HYPONYM-OF shallow models. Kalman filters HYPONYM-OF shallow models. DyMoN USED-FOR biological systems. DyMoN USED-FOR features of the dynamics. OtherScientificTerm is longitudinal measurements. Material is biological data. Generic is model. ,"This paper proposes a deep generative Markov model (DyMoN) for biological data. DyMoN is based on the idea of dimensionality reduction, which is used to reduce the dimensionality of trajectories. The authors also propose a deep neural network framework to learn the dynamics of biological systems. The proposed method is evaluated on synthetic and real-world datasets.","This paper proposes a deep generative Markov model (DyMoN) for biological data. DyMoN is based on the idea of dimensionality reduction, which is used to reduce the dimensionality of trajectories. The authors also propose a deep neural network framework to learn the dynamics of biological systems. The proposed method is evaluated on synthetic and real-world datasets."
6520,SP:4828e4160b70ea11e364b48db24cb68cdf86edfc,"graph Laplacian USED-FOR unsupervised classification method. classification method COMPARE architecture. architecture COMPARE classification method. approximate linear map CONJUNCTION spectral clustering theory. spectral clustering theory CONJUNCTION approximate linear map. spectral clustering theory USED-FOR generative adversarial networks. approximate linear map USED-FOR generative adversarial networks. spectral clustering theory USED-FOR dimension reduced spaces. framework USED-FOR images. approximate linear connector network C USED-FOR cerebral cortex. spectral clustering HYPONYM-OF unsupervised learning. unsupervised classification method USED-FOR method. Method are human visual recognition system, and connector network. OtherScientificTerm is human brains. ","This paper proposes a graph Laplacian-based unsupervised learning method for image classification based on spectral clustering theory. The proposed method is based on the idea of approximate linear connector networks (CLNs), which is an extension of the CLNs proposed in [1]. The authors show that CLNs can be used to learn a neural network that is able to classify images in dimension-reduced spaces. The authors also show that the proposed method can be applied to image classification tasks. ","This paper proposes a graph Laplacian-based unsupervised learning method for image classification based on spectral clustering theory. The proposed method is based on the idea of approximate linear connector networks (CLNs), which is an extension of the CLNs proposed in [1]. The authors show that CLNs can be used to learn a neural network that is able to classify images in dimension-reduced spaces. The authors also show that the proposed method can be applied to image classification tasks. "
6524,SP:d5f5f6a83f0290415ea94b3740a95360a8fa16e3,permuted set USED-FOR permutation - invariant representation. bottleneck PART-OF set models. model USED-FOR permutations and set representations. number sorting CONJUNCTION image mosaics. image mosaics CONJUNCTION number sorting. classification CONJUNCTION visual question answering. visual question answering CONJUNCTION classification. image mosaics CONJUNCTION classification. classification CONJUNCTION image mosaics. image mosaics CONJUNCTION visual question answering. visual question answering CONJUNCTION image mosaics. image mosaics USED-FOR classification. explicit or implicit supervision USED-FOR model. explicit or implicit supervision USED-FOR permutations and set representations. OtherScientificTerm is Representations of sets. Method is Permutation - Optimisation module. ,"This paper studies the problem of learning permutation-invariant representations for set models. The authors propose a method to learn permutation invariant representation for permutations and set representations. The proposed method is based on the Permutation-Optimisation module. The method is evaluated on a number of tasks, including number sorting, classification, and visual question answering. ","This paper studies the problem of learning permutation-invariant representations for set models. The authors propose a method to learn permutation invariant representation for permutations and set representations. The proposed method is based on the Permutation-Optimisation module. The method is evaluated on a number of tasks, including number sorting, classification, and visual question answering. "
6528,SP:cf74c553bae2b1194beaba4df1545d35e66aa5b3,"Projective Subspace Networks ( PSN ) HYPONYM-OF deep learning paradigm. deep learning paradigm USED-FOR non - linear embeddings. limited supervision USED-FOR non - linear embeddings. PSN approach USED-FOR end - to - end learning. Method are learning techniques, and PSN. OtherScientificTerm are dynamical environments, affine subspace, projective subspace, and higher - order information datapoints. Task is lifelong learning. Generic is modeling. ","This paper proposes a new deep learning paradigm called Projective Subspace Networks (PSN) for learning non-linear embeddings in dynamical environments. The main idea of PSN is to learn an affine subspace, which is then used to learn the embedding of a dynamical system. The authors show that PSN can be applied to a variety of tasks, including lifelong learning, end-to-end learning, and reinforcement learning. Experiments show that the proposed method outperforms existing deep learning methods.","This paper proposes a new deep learning paradigm called Projective Subspace Networks (PSN) for learning non-linear embeddings in dynamical environments. The main idea of PSN is to learn an affine subspace, which is then used to learn the embedding of a dynamical system. The authors show that PSN can be applied to a variety of tasks, including lifelong learning, end-to-end learning, and reinforcement learning. Experiments show that the proposed method outperforms existing deep learning methods."
6532,SP:d7544bc4a0ae3237daa207e789a522363fb5170d,"meta - learning method USED-FOR fast adaptation. CAML HYPONYM-OF meta - learning method. context parameters CONJUNCTION shared parameters. shared parameters CONJUNCTION context parameters. gradient steps USED-FOR task - specific loss. gradient steps USED-FOR context parameters. approaches COMPARE method. method COMPARE approaches. method USED-FOR networks. memory writes CONJUNCTION network communication. network communication CONJUNCTION memory writes. memory writes USED-FOR training. approach COMPARE MAML. MAML COMPARE approach. approach USED-FOR task embeddings. task - specific learning rate EVALUATE-FOR approach. context parameters USED-FOR task embeddings. OtherScientificTerm are model parameters, overfitting, and partitionings of the parameter vectors. Generic are model, network, and task. Method is distributed machine learning systems. ","This paper proposes a meta-learning method for distributed machine learning. The proposed method is based on the notion of task-specific loss, which is used to learn a task specific loss for each task. The authors show that the proposed method can be used to reduce the overfitting problem in distributed learning. They also show that their method is able to achieve faster learning rate than MAML. ","This paper proposes a meta-learning method for distributed machine learning. The proposed method is based on the notion of task-specific loss, which is used to learn a task specific loss for each task. The authors show that the proposed method can be used to reduce the overfitting problem in distributed learning. They also show that their method is able to achieve faster learning rate than MAML. "
6536,SP:8a5e86b6770a3c08f861fbf682296dc3a6c02204,"Utility providers USED-FOR data privacy. natural informationtheoretic bounds FEATURE-OF utility - privacy trade - off. explicit learning architectures USED-FOR privacypreserving representations. utility algorithms USED-FOR privacy requirements. gender CONJUNCTION emotion detection. emotion detection CONJUNCTION gender. use cases EVALUATE-FOR framework. face recognition USED-FOR mobile devices. mobile devices HYPONYM-OF application. subject - withinsubject HYPONYM-OF use cases. Task are privacy protecting challenges, and facial verification. Generic are paradigm, bound, and algorithm. Method are machine learning algorithms, privacy - preserving representations, sanitization process, space - preserving transformations, and face identity detector. OtherScientificTerm are gender - and - subject, gender attribute, emotion - and - gender, and independent variables. ",This paper studies the privacy-preserving trade-off between utility and privacy. The authors propose a new framework for learning privacy preserving representations for face recognition and face identity detection. The framework is based on the natural information theoretic bounds on the utility-privacy tradeoff between privacy and privacy preserving. The paper also proposes a sanitization process to sanitize the representation space. The proposed framework is evaluated on a number of face recognition applications and shows that the proposed method outperforms existing methods. ,This paper studies the privacy-preserving trade-off between utility and privacy. The authors propose a new framework for learning privacy preserving representations for face recognition and face identity detection. The framework is based on the natural information theoretic bounds on the utility-privacy tradeoff between privacy and privacy preserving. The paper also proposes a sanitization process to sanitize the representation space. The proposed framework is evaluated on a number of face recognition applications and shows that the proposed method outperforms existing methods. 
6540,SP:6b0e9a8f0c046a767dce8790489b3e90e12e2c46,"training instability FEATURE-OF Generative Adversarial Networks ( GANs ). backpropagation signal USED-FOR generator. task difficulty FEATURE-OF discriminator. progressive augmentation USED-FOR generator. progressive augmentation USED-FOR GAN objective. Fashion - MNIST CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION Fashion - MNIST. CIFAR10 CONJUNCTION CELEBA. CELEBA CONJUNCTION CIFAR10. MNIST CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION MNIST. image generation task EVALUATE-FOR approach. Method are hyper - parameter tuning, progressive augmentation of GANs ( PAGAN ), and GAN training. OtherScientificTerm are fragile training behaviour, and input space. ","This paper studies the problem of training GANs with unstable training behaviour. The authors propose a progressive augmentation method to address the issue of training instability in GAN training. The proposed method is based on hyper-parameter tuning, and the authors show that the proposed method can improve the performance of PAGAN on several image generation tasks. ","This paper studies the problem of training GANs with unstable training behaviour. The authors propose a progressive augmentation method to address the issue of training instability in GAN training. The proposed method is based on hyper-parameter tuning, and the authors show that the proposed method can improve the performance of PAGAN on several image generation tasks. "
6544,SP:c210982ccdd134d4b293dbe144990398eefe1a86,"models USED-FOR neural responses. models USED-FOR primary visual cortex ( V1 ). convolutional neural networks ( CNNs ) USED-FOR V1 activity. orientation selectivity CONJUNCTION phase invariance. phase invariance CONJUNCTION orientation selectivity. orientation selectivity FEATURE-OF V1 neurons. phase invariance FEATURE-OF V1 neurons. V1 neurons USED-FOR features. framework USED-FOR common features. rotation - equivariant convolutional neural network USED-FOR framework. mouse primary visual cortex FEATURE-OF natural images. two - photon imaging USED-FOR rotation - equivariant CNN. rotation - equivariant network COMPARE regular CNN. regular CNN COMPARE rotation - equivariant network. rotation - equivariant network USED-FOR common features. feature maps USED-FOR regular CNN. Method is energy models. Task are V1 computations, and nonlinear functional organization of visual cortex. OtherScientificTerm is neural activity. ","This paper proposes a rotation-equivariant convolutional neural network (RENN) for the primary visual cortex (V1). RENN is an extension of the rotation equivariant CNN framework, which is used to model the rotation of the V1 neurons. The authors show that the proposed method is able to capture the rotation invariance and phase invariance properties of V1 activity. They also show that rotation-Equivariant RENNs can capture the orientation selectivity and phase selectivity of the neurons. ","This paper proposes a rotation-equivariant convolutional neural network (RENN) for the primary visual cortex (V1). RENN is an extension of the rotation equivariant CNN framework, which is used to model the rotation of the V1 neurons. The authors show that the proposed method is able to capture the rotation invariance and phase invariance properties of V1 activity. They also show that rotation-Equivariant RENNs can capture the orientation selectivity and phase selectivity of the neurons. "
6548,SP:f17090812ace9c83d418b17bf165649232c223e3,"large datasets USED-FOR neural networks. algorithm USED-FOR robust, communication - efficient learning. algorithm USED-FOR SIGNSGD. algorithm COMPARE full - precision, distributed SGD. full - precision, distributed SGD COMPARE algorithm. communication COMPARE full - precision, distributed SGD. full - precision, distributed SGD COMPARE communication. communication USED-FOR algorithm. convergence USED-FOR parameter regime. parameter regime FEATURE-OF ADAM. large and mini - batch settings FEATURE-OF SIGNSGD. majority vote USED-FOR sign gradients. SGD COMPARE majority vote. majority vote COMPARE SGD. Pytorch USED-FOR distributed training system. time EVALUATE-FOR resnet50. collective communications library ( NCCL ) COMPARE framework. framework COMPARE collective communications library ( NCCL ). Imagenet USED-FOR resnet50. parameter server PART-OF framework. time EVALUATE-FOR framework. AWS p3.2xlarge machines USED-FOR resnet50. Generic is networks. OtherScientificTerm are communicating gradients, machine counts, network faults, gradient vector, server, class of adversaries, and gradient estimate. ","This paper proposes a new method for distributed training of deep neural networks. The proposed method, called SIGNSGD, is based on Pytorch. The authors show that the proposed method is able to converge faster than full-precision, distributed SGD in the parameter regime. The main contribution of the paper is that the authors propose a new algorithm, called ADAM, which uses a majority vote to estimate the sign gradients of the gradient vector of the parameter server. The method is evaluated on several benchmark datasets and shows that it outperforms the state-of-the-art.","This paper proposes a new method for distributed training of deep neural networks. The proposed method, called SIGNSGD, is based on Pytorch. The authors show that the proposed method is able to converge faster than full-precision, distributed SGD in the parameter regime. The main contribution of the paper is that the authors propose a new algorithm, called ADAM, which uses a majority vote to estimate the sign gradients of the gradient vector of the parameter server. The method is evaluated on several benchmark datasets and shows that it outperforms the state-of-the-art."
6552,SP:0ceece0754a1fe9c46a978bb2854932905685fa4,approach USED-FOR realistic and high - fidelity stock market data. generative adversarial networks USED-FOR approach. conditional Wasserstein GAN USED-FOR history dependence of orders. stock market FEATURE-OF history dependence of orders. finite history dependence FEATURE-OF stochastic process. stochastic process USED-FOR order stream. actual market and synthetic data EVALUATE-FOR approach. Material is real data. ,This paper proposes a conditional Wasserstein GAN (WGAN) method for predicting the history dependence of orders in a stock market. The WGAN is a generative adversarial network (GAN) that learns to predict the history of an order in a stochastic process with finite history dependence. The authors show that the WGAN can be applied to both synthetic and real-world stock market data. ,This paper proposes a conditional Wasserstein GAN (WGAN) method for predicting the history dependence of orders in a stock market. The WGAN is a generative adversarial network (GAN) that learns to predict the history of an order in a stochastic process with finite history dependence. The authors show that the WGAN can be applied to both synthetic and real-world stock market data. 
6556,SP:ba66503753b3c57781b435c55c47fc9f69450e65,"deep reinforcement learning ( DRL ) algorithm USED-FOR arbitrary errors. deep reinforcement learning ( DRL ) algorithm USED-FOR applications. robotics HYPONYM-OF applications. RL agents USED-FOR observed rewards. reward confusion matrix USED-FOR observed rewards. RL agents USED-FOR noisy environments. approaches USED-FOR supervised learning. supervised learning USED-FOR framework. noisy data USED-FOR approaches. noisy data USED-FOR supervised learning. approaches USED-FOR framework. sample complexity EVALUATE-FOR approach. convergence CONJUNCTION sample complexity. sample complexity CONJUNCTION convergence. convergence EVALUATE-FOR approach. policies COMPARE baselines. baselines COMPARE policies. expected rewards EVALUATE-FOR policies. estimated surrogate reward USED-FOR policies. Atari games EVALUATE-FOR PPO algorithm. Method are reinforcement learning ( RL ) models, and unbiased reward estimator aided robust RL framework. OtherScientificTerm are noises, observed reward channel, perturbed rewards, and unbiased surrogate rewards. Task is noisy RL problems. Generic is solution. Material is DRL platforms. Metric is error rates. ",This paper proposes an unbiased reward estimator aided robust RL framework for noisy RL problems. The proposed method is based on the idea of reward confusion matrix (PPO) to estimate the expected reward of an agent in a noisy environment. The authors show that the proposed PPO algorithm converges to the optimal policy in terms of sample complexity and convergence rate. They also show that their method is robust to perturbations in the reward space.,This paper proposes an unbiased reward estimator aided robust RL framework for noisy RL problems. The proposed method is based on the idea of reward confusion matrix (PPO) to estimate the expected reward of an agent in a noisy environment. The authors show that the proposed PPO algorithm converges to the optimal policy in terms of sample complexity and convergence rate. They also show that their method is robust to perturbations in the reward space.
6560,SP:0e62f75b81b696bf794932d0ceee60e9f665f1da,"network structure USED-FOR halting time. overparametrization USED-FOR weight space traversal. power - law - like relationship USED-FOR average step size. Method is gradient descent. OtherScientificTerm are computational requirements, model ’s width, gradient vectors, and traversal. Generic are larger models, and applications. ","This paper studies the problem of weight space traversal in gradient descent. The authors show that the average step size of the gradient descent algorithm is bounded by a power-law-like relationship between the width of the weights and the number of steps needed to traverse the weight space. They also show that this power law holds for any overparametrization of the overparameterized weights. They show that if the weights are large enough, then the average weight size can be bounded by the power law. They further show that for any weight-space traversal algorithm, the average gradient size is bounded in terms of the weight size. ","This paper studies the problem of weight space traversal in gradient descent. The authors show that the average step size of the gradient descent algorithm is bounded by a power-law-like relationship between the width of the weights and the number of steps needed to traverse the weight space. They also show that this power law holds for any overparametrization of the overparameterized weights. They show that if the weights are large enough, then the average weight size can be bounded by the power law. They further show that for any weight-space traversal algorithm, the average gradient size is bounded in terms of the weight size. "
6564,SP:40e210d36298e2eafd06d9dc45312ea4fd586ade,"learning framework USED-FOR setting. reinforcement learning USED-FOR online optimization problems. reinforcement learning USED-FOR theoretically optimal algorithms. theoretically optimal algorithms USED-FOR online optimization problems. algorithms CONJUNCTION complexity theory. complexity theory CONJUNCTION algorithms. universal and high - entropy training sets HYPONYM-OF adversarial distributions. online knapsack problem CONJUNCTION secretary problem. secretary problem CONJUNCTION online knapsack problem. AdWords problem CONJUNCTION online knapsack problem. online knapsack problem CONJUNCTION AdWords problem. online knapsack problem EVALUATE-FOR ideas. secretary problem EVALUATE-FOR ideas. AdWords problem EVALUATE-FOR ideas. optimal algorithms USED-FOR problems. online primal - dual framework USED-FOR problems. OtherScientificTerm are distributions, and worst case. Method is learner. Generic is models. ",This paper proposes an online primal-dual framework for online optimization problems. The framework is based on the idea of universal and high-entropy training sets. The authors show that the optimal algorithms for these problems can be derived from the complexity theory. They also show that this framework can be applied to the secretary problem and the online knapsack problem.,This paper proposes an online primal-dual framework for online optimization problems. The framework is based on the idea of universal and high-entropy training sets. The authors show that the optimal algorithms for these problems can be derived from the complexity theory. They also show that this framework can be applied to the secretary problem and the online knapsack problem.
6568,SP:b99732087f5a929ab248acdcd7a943bce8671510,"inductive biases PART-OF deep reinforcement learning algorithms. domain knowledge CONJUNCTION pretuned hyperparameters. pretuned hyperparameters CONJUNCTION domain knowledge. domain knowledge HYPONYM-OF inductive biases. pretuned hyperparameters HYPONYM-OF inductive biases. fixed components COMPARE adaptive solutions. adaptive solutions COMPARE fixed components. components COMPARE adaptive components. adaptive components COMPARE components. learning EVALUATE-FOR systems. continuous control problems EVALUATE-FOR systems. adaptive components USED-FOR system. tasks EVALUATE-FOR system. deep RL algorithms USED-FOR tasks. deep reinforcement learning ( RL ) community USED-FOR tasks. Go CONJUNCTION Chess. Chess CONJUNCTION Go. board - games CONJUNCTION video - games. video - games CONJUNCTION board - games. video - games CONJUNCTION 3D navigation tasks. 3D navigation tasks CONJUNCTION video - games. Atari HYPONYM-OF video - games. Go HYPONYM-OF board - games. Chess HYPONYM-OF board - games. tuning USED-FOR these. these USED-FOR new domains. tuning USED-FOR new domains. inductive biases FEATURE-OF agents. AlphaZero COMPARE AlphaGo algorithm. AlphaGo algorithm COMPARE AlphaZero. Go - specific inductive biases CONJUNCTION human data. human data CONJUNCTION Go - specific inductive biases. Chess CONJUNCTION Shogi. Shogi CONJUNCTION Chess. AlphaZero USED-FOR Chess. AlphaZero USED-FOR Shogi. biases USED-FOR AlphaZero. Go USED-FOR AlphaZero. generality CONJUNCTION performance. performance CONJUNCTION generality. inductive biases PART-OF algorithms. domain knowledge CONJUNCTION pretuned learning parameters. pretuned learning parameters CONJUNCTION domain knowledge. domain knowledge PART-OF Inductive biases. pretuned learning parameters PART-OF Inductive biases. domain knowledge CONJUNCTION pretune parameters. pretune parameters CONJUNCTION domain knowledge. Generic are problems, and approach. OtherScientificTerm is hyper - parameters. Method are domain - specific components, RL algorithms, AlphaZero algorithm, and learning algorithm. ","This paper studies the problem of learning deep reinforcement learning algorithms for continuous control tasks. The authors propose a new algorithm, AlphaZero, that combines domain knowledge, hyperparameters, and inductive biases. They show that AlphaZero outperforms the state-of-the-art AlphaGo algorithm on a variety of tasks, including Go, Chess, Shogi, and Atari. They also show that the AlphaZero algorithm outperforms AlphaGo on Atari and Shogi. ","This paper studies the problem of learning deep reinforcement learning algorithms for continuous control tasks. The authors propose a new algorithm, AlphaZero, that combines domain knowledge, hyperparameters, and inductive biases. They show that AlphaZero outperforms the state-of-the-art AlphaGo algorithm on a variety of tasks, including Go, Chess, Shogi, and Atari. They also show that the AlphaZero algorithm outperforms AlphaGo on Atari and Shogi. "
6572,SP:47b0c8a984480eb353b36fd877d9775213fb1a5f,photo - realistic unknown environments FEATURE-OF navigational instruction. visual - textual co - grounding module CONJUNCTION progress monitor. progress monitor CONJUNCTION visual - textual co - grounding module. complementary components PART-OF self - monitoring agent. visual - textual co - grounding module HYPONYM-OF complementary components. progress monitor HYPONYM-OF complementary components. ablation studies USED-FOR primary components. ablation studies USED-FOR approach. benchmark EVALUATE-FOR selfmonitoring agent. method USED-FOR state. method USED-FOR art. Generic is task. OtherScientificTerm is navigation progress. Metric is success rate. ,This paper proposes a self-monitoring agent for navigation tasks. The proposed method is based on a combination of two components: a visual-textual co-grounding module and a progress monitor. The two components are used to guide the agent through the navigation process. The authors show that the proposed method outperforms state-of-the-art methods on a benchmark.,This paper proposes a self-monitoring agent for navigation tasks. The proposed method is based on a combination of two components: a visual-textual co-grounding module and a progress monitor. The two components are used to guide the agent through the navigation process. The authors show that the proposed method outperforms state-of-the-art methods on a benchmark.
6576,SP:7e70c97e9b7b182e974b071c93baafef8b11cf90,input - output examples USED-FOR Neural program synthesis. encoder USED-FOR embedding. decoder USED-FOR program. syntax FEATURE-OF embedding. encoder USED-FOR encoder - decoder architecture. embedding USED-FOR decoder. encoder - decoder architecture USED-FOR neural program synthesis approaches. approaches USED-FOR tasks. approaches USED-FOR tasks. tasks EVALUATE-FOR state - of - the - art approach. tasks HYPONYM-OF tasks. accuracy EVALUATE-FOR state - of - the - art approach. Karel HYPONYM-OF tasks. Karel HYPONYM-OF tasks. FlashFill HYPONYM-OF tasks. execution - guided synthesis CONJUNCTION synthesizer ensemble. synthesizer ensemble CONJUNCTION execution - guided synthesis. techniques USED-FOR semantic information. synthesizer ensemble HYPONYM-OF techniques. execution - guided synthesis HYPONYM-OF techniques. synthesizer ensemble HYPONYM-OF semantic information. execution - guided synthesis HYPONYM-OF semantic information. techniques CONJUNCTION encoder - decoder - style neural program synthesizer. encoder - decoder - style neural program synthesizer CONJUNCTION techniques. accuracy EVALUATE-FOR techniques. Karel dataset EVALUATE-FOR techniques. ,"This paper proposes a new neural program synthesis framework for input-output examples. The key idea is to use an encoder-decoder architecture to learn the embedding of a program, and a decoder to synthesize the program. The proposed method is evaluated on a variety of tasks, including FlashFill, Karel, Execution-Guided Synthesis, and Execution-guided Synthesis. ","This paper proposes a new neural program synthesis framework for input-output examples. The key idea is to use an encoder-decoder architecture to learn the embedding of a program, and a decoder to synthesize the program. The proposed method is evaluated on a variety of tasks, including FlashFill, Karel, Execution-Guided Synthesis, and Execution-guided Synthesis. "
6580,SP:dc7dfc1eec473800580dba309446871122be6040,"stability FEATURE-OF batch normalization ( BN ). BN USED-FOR simplified model. ordinary least squares ( OLS ) HYPONYM-OF simplified model. modeling approach USED-FOR problem. scaling law CONJUNCTION convergence. convergence CONJUNCTION scaling law. gradient descent USED-FOR OLS. arbitrary learning rates FEATURE-OF weights. convergence FEATURE-OF arbitrary learning rates. convergence CONJUNCTION acceleration effects. acceleration effects CONJUNCTION convergence. BN USED-FOR gradient descent. mathematical principles USED-FOR batch normalization. OtherScientificTerm is learning rates. Task are OLS problem, and supervised learning problems. ","This paper studies the stability of batch normalization (BN) in the context of the OLS problem. The authors consider the case where the learning rate of the model is arbitrarily large and the weights are arbitrarily small. They show that under certain conditions, BN converges to an OLS solution that is stable. They also show that the convergence of BN is guaranteed by the scaling law and convergence of gradient descent. ","This paper studies the stability of batch normalization (BN) in the context of the OLS problem. The authors consider the case where the learning rate of the model is arbitrarily large and the weights are arbitrarily small. They show that under certain conditions, BN converges to an OLS solution that is stable. They also show that the convergence of BN is guaranteed by the scaling law and convergence of gradient descent. "
6584,SP:9984d73a1fcfce932cfcafb4d200f70b07723bf3,data noising USED-FOR recurrent neural network language models. theoretical perspective USED-FOR data noising. data noising HYPONYM-OF Bayesian recurrent neural networks. variational distribution FEATURE-OF Bayesian recurrent neural networks. variational framework USED-FOR data noising. variational smoothing CONJUNCTION element - wise variational smoothing method. element - wise variational smoothing method CONJUNCTION variational smoothing. tied input and output embedding matrices CONJUNCTION element - wise variational smoothing method. element - wise variational smoothing method CONJUNCTION tied input and output embedding matrices. tied input and output embedding matrices USED-FOR variational smoothing. benchmark language modeling datasets EVALUATE-FOR data noising methods. Method is mixture of Gaussians. OtherScientificTerm is unigram distribution. Generic is method. ,"This paper studies the problem of data noising in Bayesian recurrent neural networks. The authors propose a variational framework for the problem. The framework is based on variational smoothing, which is a mixture of Gaussians and element-wise smoothing. The proposed framework is tested on a number of benchmark language modeling datasets. ","This paper studies the problem of data noising in Bayesian recurrent neural networks. The authors propose a variational framework for the problem. The framework is based on variational smoothing, which is a mixture of Gaussians and element-wise smoothing. The proposed framework is tested on a number of benchmark language modeling datasets. "
6588,SP:f4a914d3df1a5a21a7365ba78279420f39210884,"saliency maps USED-FOR classification. classifier - dependent methods USED-FOR Extracting saliency maps. approach USED-FOR saliency maps. approach COMPARE weakly - supervised localization techniques. weakly - supervised localization techniques COMPARE approach. Method are classifier - agnostic saliency map extraction, and classifier. Material is ImageNet dataset. ",This paper proposes a classifier-agnostic method for extracting saliency maps from ImageNet. The proposed method is based on the idea of classifier agnostic saliency map extraction. The method is evaluated on the ImageNet dataset and shows that the proposed method outperforms weakly-supervised methods. ,This paper proposes a classifier-agnostic method for extracting saliency maps from ImageNet. The proposed method is based on the idea of classifier agnostic saliency map extraction. The method is evaluated on the ImageNet dataset and shows that the proposed method outperforms weakly-supervised methods. 
6592,SP:df038354c6a7638116a98d150aa4a8f5f2b0a2da,deep nets USED-FOR task. net USED-FOR task. net USED-FOR model. deep nets USED-FOR task. knowledge flow USED-FOR deep net model. teachers HYPONYM-OF deep nets. tasks EVALUATE-FOR they. output spaces FEATURE-OF tasks. fine - tuning COMPARE knowledge exchange ’ methods. knowledge exchange ’ methods COMPARE fine - tuning. supervised and reinforcement learning tasks EVALUATE-FOR fine - tuning. approach COMPARE fine - tuning. fine - tuning COMPARE approach. supervised and reinforcement learning tasks EVALUATE-FOR approach. ,This paper proposes a knowledge flow method for fine-tuning deep neural networks. The key idea is to use a teacher model to guide the learning of a deep net model. The teacher model is trained using the knowledge flow between the input and the output of the deep net. The authors show that the proposed method is able to improve the performance of deep nets on both supervised and reinforcement learning tasks. ,This paper proposes a knowledge flow method for fine-tuning deep neural networks. The key idea is to use a teacher model to guide the learning of a deep net model. The teacher model is trained using the knowledge flow between the input and the output of the deep net. The authors show that the proposed method is able to improve the performance of deep nets on both supervised and reinforcement learning tasks. 
6596,SP:a72072879f7c61270d952f06d9ce995e8150632c,"spatial and temporal dimensions FEATURE-OF higher - order inter - dependencies. higher - order inter - dependencies FEATURE-OF data streams. soft - clustering USED-FOR compact dynamical model. dynamics USED-FOR compact dynamical model. predictive accuracy EVALUATE-FOR mathematical representation. stochastic calculus PART-OF information theory inspired approach. predictive ability CONJUNCTION causal interdependence ( relatedness ) constraints. causal interdependence ( relatedness ) constraints CONJUNCTION predictive ability. data streams CONJUNCTION compact model. compact model CONJUNCTION data streams. maximization of the compression of the state variables USED-FOR model construction. convergence FEATURE-OF learning algorithm. iterative scheme USED-FOR model parameters. high - dimensional Gaussian case study EVALUATE-FOR framework. compression and prediction accuracy EVALUATE-FOR dynamical systems. algorithm USED-FOR prediction. reduced dimensions FEATURE-OF prediction. real - world dataset of multimodal sentiment intensity EVALUATE-FOR algorithm. Task are Extracting relevant information, modeling complex systems, and causal inference. Metric is accuracy. Generic is tasks. Material is high - dimensional heterogeneous data streams. ","This paper proposes an information theory inspired approach to learn a compact dynamical model for high-dimensional heterogeneous data streams. The proposed approach is based on soft-clustering, which is an information theoretic framework for learning a dynamical system. The authors show that the proposed approach converges to a compact model in terms of predictive accuracy and compression of the state variables. They also show that their approach can be applied to the case of high dimensional Gaussian systems.","This paper proposes an information theory inspired approach to learn a compact dynamical model for high-dimensional heterogeneous data streams. The proposed approach is based on soft-clustering, which is an information theoretic framework for learning a dynamical system. The authors show that the proposed approach converges to a compact model in terms of predictive accuracy and compression of the state variables. They also show that their approach can be applied to the case of high dimensional Gaussian systems."
6600,SP:2b03b7ea1264c2671d29e8fa5f3a828412ea7996,"variational autoencoder USED-FOR single neural probabilistic model. stochastic variational Bayes USED-FOR model. feature imputation CONJUNCTION image inpainting problems. image inpainting problems CONJUNCTION feature imputation. synthetic data CONJUNCTION feature imputation. feature imputation CONJUNCTION synthetic data. synthetic data EVALUATE-FOR approach. OtherScientificTerm are observed features, and features. ",This paper proposes a variational autoencoder for single neural probabilistic models. The proposed method is based on the variational variational Bayes (VAE) framework. The authors show that the proposed VAE can be applied to the problem of feature imputation and image inpainting. The paper also shows that the VAE model can be used to solve the image imputation problem. ,This paper proposes a variational autoencoder for single neural probabilistic models. The proposed method is based on the variational variational Bayes (VAE) framework. The authors show that the proposed VAE can be applied to the problem of feature imputation and image inpainting. The paper also shows that the VAE model can be used to solve the image imputation problem. 
6604,SP:f46f0cb43274fb20cba91ef7318305f668bc6928,"architecture design USED-FOR neural network models. tasks EVALUATE-FOR neural network models. intermediate layer activations USED-FOR back - propagation. memory footprint FEATURE-OF models. spatial resolutions FEATURE-OF layers. approximation strategy USED-FOR network. memory footprint FEATURE-OF network. training EVALUATE-FOR approximation strategy. lower - precision approximations USED-FOR activations. approximate activations USED-FOR backward pass. approximation USED-FOR gradients. CIFAR CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR. ImageNet EVALUATE-FOR approach. CIFAR EVALUATE-FOR approach. 32 - bit floating - point activations USED-FOR approach. training and validation performance EVALUATE-FOR approach. OtherScientificTerm are Limited GPU memory, memory, and forward and backward pass. Generic are architecture, and forward pass. Metric are computational expense, and memory usage. ","This paper proposes a method to reduce the memory footprint of neural network models. The main idea is to use lower-precision approximations of intermediate layer activations for back-propagation and forward pass. The proposed method is evaluated on CIFAR-10, ImageNet, and ImageNet-100. ","This paper proposes a method to reduce the memory footprint of neural network models. The main idea is to use lower-precision approximations of intermediate layer activations for back-propagation and forward pass. The proposed method is evaluated on CIFAR-10, ImageNet, and ImageNet-100. "
6608,SP:6ad33c6fbdee78c13d9190601637e07d20fe024f,"Face completion HYPONYM-OF task. complexity CONJUNCTION controllable attributes of filled - in fragments. controllable attributes of filled - in fragments CONJUNCTION complexity. end - to - end framework USED-FOR generative adversarial networks ( GANs ). conditional vectors USED-FOR controllable attributes. end - to - end framework USED-FOR system. mean inference time FEATURE-OF images. system USED-FOR structural and appearance variations. mean inference time FEATURE-OF feed - forward pass of computation. 1024× 1024 resolution FEATURE-OF images. feed - forward pass of computation USED-FOR system. approach COMPARE face completion methods. face completion methods COMPARE approach. OtherScientificTerm are high resolution, and frequency components. Generic are model, and network. ","This paper proposes an end-to-end framework for face completion. The proposed method is based on a generative adversarial network (GAN) framework. The authors propose to use conditional vectors to capture the controllable attributes of filled-in fragments, which can be used to improve the performance of the model. The method is evaluated on a number of face completion tasks, and is shown to be competitive with state-of-the-art methods.","This paper proposes an end-to-end framework for face completion. The proposed method is based on a generative adversarial network (GAN) framework. The authors propose to use conditional vectors to capture the controllable attributes of filled-in fragments, which can be used to improve the performance of the model. The method is evaluated on a number of face completion tasks, and is shown to be competitive with state-of-the-art methods."
6612,SP:a300122021e93d695af85e158f2b402d21525bc8,"high - precision accumulators USED-FOR systems. precision requirements FEATURE-OF partial sum accumulations. reduced accumulation precision USED-FOR deep learning training. statistical approach USED-FOR reduced accumulation precision. analysis USED-FOR benchmark networks. ImageNet ResNet 18 CONJUNCTION ImageNet AlexNet. ImageNet AlexNet CONJUNCTION ImageNet ResNet 18. single precision floating - point baseline FEATURE-OF networks. OtherScientificTerm are numerical precision, multiply - accumulate units, accumulation precision, ensemble of partial sums, accumulation, and computation hardware. Metric is quality of convergence. Generic are variance, and equations. Task is areaand power - optimal systems. ","This paper studies the problem of reducing the accumulation precision of partial sum accumulators. The authors propose a statistical approach to reduce the precision of the sum of partial sums, and show that this can be achieved by minimizing the variance of the convergence of the total sum. The paper also provides a theoretical analysis of the effect of the variance on the quality of convergence. ","This paper studies the problem of reducing the accumulation precision of partial sum accumulators. The authors propose a statistical approach to reduce the precision of the sum of partial sums, and show that this can be achieved by minimizing the variance of the convergence of the total sum. The paper also provides a theoretical analysis of the effect of the variance on the quality of convergence. "
6616,SP:3a1655a2efdf0246f459b6f82a2948aafc7438a9,"gradient flow CONJUNCTION gradient descent. gradient descent CONJUNCTION gradient flow. risk convergence CONJUNCTION asymptotic weight matrix alignment. asymptotic weight matrix alignment CONJUNCTION risk convergence. gradient descent USED-FOR deep linear networks. asymptotic weight matrix alignment HYPONYM-OF implicit regularization. gradient flow USED-FOR deep linear networks. asymptotic weight matrix alignment USED-FOR gradient flow. linearly separable data USED-FOR deep linear networks. gradient flow USED-FOR strictly decreasing loss functions. normalized ith weight matrix COMPARE rank-1 approximation. rank-1 approximation COMPARE normalized ith weight matrix. decreasing step sizes FEATURE-OF gradient descent. linear function COMPARE maximum margin solution. maximum margin solution COMPARE linear function. binary cross entropy FEATURE-OF logistic loss. weight matrices USED-FOR linear function. network USED-FOR linear function. Metric is risk. OtherScientificTerm are rank-1 matrices, and alignment phenomenon. ","This paper studies the asymptotic weight matrix alignment phenomenon in deep linear networks. The authors show that the risk convergence of a deep linear network converges to the maximum margin solution of the logistic loss with decreasing step sizes. They also show that for strictly decreasing loss functions, the gradient flow can converge to the max margin solution. ","This paper studies the asymptotic weight matrix alignment phenomenon in deep linear networks. The authors show that the risk convergence of a deep linear network converges to the maximum margin solution of the logistic loss with decreasing step sizes. They also show that for strictly decreasing loss functions, the gradient flow can converge to the max margin solution. "
6620,SP:868dd531fe7886b0260295d25b75cc6d6d28f12d,dialogue topic CONJUNCTION speaker sentiments. speaker sentiments CONJUNCTION dialogue topic. speaker identity CONJUNCTION dialogue topic. dialogue topic CONJUNCTION speaker identity. hredGAN architecture USED-FOR utterance attributes. speaker sentiments HYPONYM-OF utterance attributes. speaker identity HYPONYM-OF utterance attributes. dialogue topic HYPONYM-OF utterance attributes. persona - based HRED generator ( PHRED ) CONJUNCTION conditional discriminator. conditional discriminator CONJUNCTION persona - based HRED generator ( PHRED ). persona - based HRED generator ( PHRED ) PART-OF phredGAN. conditional discriminator PART-OF phredGAN. persona - based HRED generator ( PHRED ) PART-OF system. conditional discriminator PART-OF system. approaches USED-FOR conditional discriminator. phredGANd HYPONYM-OF dual discriminator system. dual discriminator system CONJUNCTION adversarial discriminator. adversarial discriminator CONJUNCTION dual discriminator system. phredGANa HYPONYM-OF approaches. phredGANd HYPONYM-OF approaches. Ubuntu Dialogue Corpus ( UDC ) CONJUNCTION TV series transcripts. TV series transcripts CONJUNCTION Ubuntu Dialogue Corpus ( UDC ). phredGAN COMPARE persona SeqSeq model. persona SeqSeq model COMPARE phredGAN. Big Bang Theory and Friends FEATURE-OF TV series transcripts. conversational datasets EVALUATE-FOR phredGAN. Ubuntu Dialogue Corpus ( UDC ) HYPONYM-OF conversational datasets. TV series transcripts HYPONYM-OF conversational datasets. quantitative measures CONJUNCTION crowd - sourced human evaluation. crowd - sourced human evaluation CONJUNCTION quantitative measures. datasets CONJUNCTION ones. ones CONJUNCTION datasets. attribute modalities FEATURE-OF customer - agent interactions. Ubuntu dataset FEATURE-OF customer - agent interactions. ones EVALUATE-FOR phredGAN. attribute modalities FEATURE-OF datasets. attribute modalities FEATURE-OF ones. datasets EVALUATE-FOR phredGAN. weak attribute modalities FEATURE-OF datasets. Big Bang Theory HYPONYM-OF weak attribute modalities. Task is multi - turn dialogue scenario. Method is attribute representation. ,"This paper proposes a new model for multi-turn dialogue. The proposed model, called phredGAN, is based on the hredGAN architecture. The main idea is to use a conditional discriminator and a persona-based HRED generator (PHRED) to generate utterance attributes for each turn of the dialogue. Experiments show that the proposed model outperforms the baselines on a number of datasets.","This paper proposes a new model for multi-turn dialogue. The proposed model, called phredGAN, is based on the hredGAN architecture. The main idea is to use a conditional discriminator and a persona-based HRED generator (PHRED) to generate utterance attributes for each turn of the dialogue. Experiments show that the proposed model outperforms the baselines on a number of datasets."
6624,SP:017b66d6262427cca551ef50006784498ffc741d,"language CONJUNCTION vision. vision CONJUNCTION language. vision CONJUNCTION action. action CONJUNCTION vision. virtual environment FEATURE-OF action. language PART-OF goal - driven collaborative task. vision PART-OF goal - driven collaborative task. action PART-OF goal - driven collaborative task. movable clip art objects PART-OF virtual world. virtual world USED-FOR game. natural language USED-FOR two - way communication. protocols CONJUNCTION metrics. metrics CONJUNCTION protocols. imitation learning CONJUNCTION goal - driven training. goal - driven training CONJUNCTION imitation learning. nearest - neighbor techniques CONJUNCTION neural network approaches. neural network approaches CONJUNCTION nearest - neighbor techniques. models USED-FOR task. nearest - neighbor techniques PART-OF models. neural network approaches PART-OF models. imitation learning USED-FOR neural network approaches. goal - driven training USED-FOR neural network approaches. game EVALUATE-FOR models. live human agents USED-FOR game. fully automated evaluation EVALUATE-FOR models. Task is Collaborative image - Drawing game. Method is CoDraw. OtherScientificTerm are clip art pieces, and crosstalk condition. Material is CoDraw dataset. Generic is testbed. ","This paper proposes a new collaborative image-drawing task called CoDraw. The task is a two-way communication task, where the goal is to draw a clip art piece from a set of clip art objects. The goal is a goal-driven collaborative task, and the task is to communicate with a human agent in a virtual environment. The authors propose a new dataset, CoDraw, to evaluate the performance of a number of models on the task. The dataset consists of a large number of clips drawn from the CoDraw dataset, which is used as a testbed for the proposed task. ","This paper proposes a new collaborative image-drawing task called CoDraw. The task is a two-way communication task, where the goal is to draw a clip art piece from a set of clip art objects. The goal is a goal-driven collaborative task, and the task is to communicate with a human agent in a virtual environment. The authors propose a new dataset, CoDraw, to evaluate the performance of a number of models on the task. The dataset consists of a large number of clips drawn from the CoDraw dataset, which is used as a testbed for the proposed task. "
6628,SP:d5126851b9e75b49522d953ee2b253e3e6c836ba,model spaces USED-FOR machine learning. neural networks USED-FOR potential functions. potential functions USED-FOR undirected models. neural networks USED-FOR Neural random fields ( NRFs ). approach USED-FOR NRFs. gradient information USED-FOR model sampling. inclusive - NRF approach USED-FOR continuous data. inclusive - divergence minimized auxiliary generator USED-FOR approach. inclusive - divergence minimized auxiliary generator USED-FOR NRFs. images HYPONYM-OF continuous data. unsupervised / supervised image generation CONJUNCTION semi - supervised classification. semi - supervised classification CONJUNCTION unsupervised / supervised image generation. inclusive - NRFs USED-FOR unsupervised / supervised image generation. inclusive - NRFs USED-FOR semi - supervised classification. random fields USED-FOR tasks. inclusive - NRFs USED-FOR random fields. CIFAR-10 EVALUATE-FOR sample generation quality. unsupervised and supervised settings EVALUATE-FOR inclusiveNRFs. sample generation quality EVALUATE-FOR inclusiveNRFs. CIFAR-10 EVALUATE-FOR inclusiveNRFs. SVHN CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION SVHN. MNIST CONJUNCTION SVHN. SVHN CONJUNCTION MNIST. Semi - supervised inclusive - NRFs COMPARE generative model based semi - supervised learning methods. generative model based semi - supervised learning methods COMPARE Semi - supervised inclusive - NRFs. classification EVALUATE-FOR generative model based semi - supervised learning methods. generation EVALUATE-FOR Semi - supervised inclusive - NRFs. classification EVALUATE-FOR Semi - supervised inclusive - NRFs. ,This paper proposes a novel approach for learning neural random fields (NRFs) for unsupervised and semi-supervised learning. The authors propose a new approach called inclusive-divergence-minimized auxiliary generator (IDAM) that is based on the idea of using the gradient information of the model sampling for model sampling. They show that the proposed method outperforms the baselines in both unsupervision and semi supervised learning. They also show that their method can be used for classification and image generation tasks. ,This paper proposes a novel approach for learning neural random fields (NRFs) for unsupervised and semi-supervised learning. The authors propose a new approach called inclusive-divergence-minimized auxiliary generator (IDAM) that is based on the idea of using the gradient information of the model sampling for model sampling. They show that the proposed method outperforms the baselines in both unsupervision and semi supervised learning. They also show that their method can be used for classification and image generation tasks. 
6632,SP:0841febf2e95da495b41e12ded491ba5e9633538,"Deep learning models USED-FOR graphs. Deep learning models USED-FOR tasks. graph neural networks USED-FOR node classification. training time attacks FEATURE-OF graph neural networks. meta - gradients USED-FOR bilevel problem. meta - gradients USED-FOR training - time attacks. small graph perturbations USED-FOR graph convolutional networks. they COMPARE baseline. baseline COMPARE they. relational information USED-FOR baseline. algorithm USED-FOR perturbations. Metric is robustness. OtherScientificTerm are discrete graph structure, and graph. Task is unsupervised embeddings. Generic is attacks. Method is classifiers. ","This paper studies the problem of adversarial training of graph convolutional neural networks (GNNs) in the presence of small graph perturbations. The authors propose a meta-gradient-based attack method to improve the robustness of GNNs against small graph adversarial attacks. The proposed method is based on meta-gradients, which can be applied to the bilevel problem. The method is evaluated on a number of benchmark datasets and shows that the proposed method outperforms the baselines.","This paper studies the problem of adversarial training of graph convolutional neural networks (GNNs) in the presence of small graph perturbations. The authors propose a meta-gradient-based attack method to improve the robustness of GNNs against small graph adversarial attacks. The proposed method is based on meta-gradients, which can be applied to the bilevel problem. The method is evaluated on a number of benchmark datasets and shows that the proposed method outperforms the baselines."
6636,SP:beb54248806f7a68beb60167c3dbbd45b34dad83,"Wasserstein Autoencoder ( WAE ) HYPONYM-OF generative models. Sliced - Wasserstein Autoencoders ( SWAE ) CONJUNCTION WAE - MMD ( WAE. WAE - MMD ( WAE CONJUNCTION Sliced - Wasserstein Autoencoders ( SWAE ). Cramer - Wold AutoEncoder ( CWAE ) HYPONYM-OF generative model. maximum mean discrepancy based distance function USED-FOR WAE - MMD ( WAE. CramerWold kernel HYPONYM-OF characteristic kernel. characteristic kernel USED-FOR CWAE cost function. CWAE COMPARE WAE - MMD. WAE - MMD COMPARE CWAE. OtherScientificTerm are normal prior, and distance function. Method are optimization procedure, and SWAE. ",This paper studies the problem of optimizing the Wasserstein autoencoder (WAE) and the Cramer-Wold auto-encoder (CWAE) models. The authors propose to use the maximum mean discrepancy based distance function (MMD) to optimize the WAE-MMD and CWAE models. They show that the CWAE cost function can be approximated by a CramerWold kernel. They also provide theoretical analysis of the convergence of the proposed method.,This paper studies the problem of optimizing the Wasserstein autoencoder (WAE) and the Cramer-Wold auto-encoder (CWAE) models. The authors propose to use the maximum mean discrepancy based distance function (MMD) to optimize the WAE-MMD and CWAE models. They show that the CWAE cost function can be approximated by a CramerWold kernel. They also provide theoretical analysis of the convergence of the proposed method.
6640,SP:57538c4cac6a4510a0c79e6da3deffae4d6c3b91,"It USED-FOR many - class ” problem. It USED-FOR class hierarchy. coarse - class label HYPONYM-OF class hierarchy. convolutional neural network ( CNN ) USED-FOR features. memory - augmented attention module CONJUNCTION multi - layer perceptron ( MLP ). multi - layer perceptron ( MLP ) CONJUNCTION memory - augmented attention module. multi - layer perceptron ( MLP ) USED-FOR MahiNet. memory - augmented attention module PART-OF MahiNet. convolutional neural network ( CNN ) USED-FOR MahiNet. attention module CONJUNCTION KNN classifier. KNN classifier CONJUNCTION attention module. linear classifier USED-FOR MLP. training strategies USED-FOR supervised learning. MahiNet USED-FOR supervised learning. training strategies USED-FOR MahiNet. mcfsOmniglot ” ( re - splitted Omniglot ) USED-FOR MCFS problem. benchmark datasets USED-FOR MCFS problem. mcfsOmniglot ” ( re - splitted Omniglot ) HYPONYM-OF benchmark datasets. MahiNet COMPARE models. models COMPARE MahiNet. MCFS classification tasks EVALUATE-FOR models. supervised learning and meta - learning scenarios EVALUATE-FOR models. supervised learning and meta - learning scenarios EVALUATE-FOR MahiNet. MCFS classification tasks EVALUATE-FOR MahiNet. Task are MCFS, MCFS learning, and few - shot ” problem. OtherScientificTerm is fine class. Material is ImageNet. ","This paper proposes a new model for the few-shot classification problem. The proposed model, MahiNet, consists of two components: a memory-augmented attention module and a multi-layer perceptron (MLP). The attention module consists of a linear classifier and a convolutional neural network (CNN). The MLP consists of an attention module, and a KNN classifier. The authors show that the proposed model outperforms the state-of-the-art models on several benchmark datasets.","This paper proposes a new model for the few-shot classification problem. The proposed model, MahiNet, consists of two components: a memory-augmented attention module and a multi-layer perceptron (MLP). The attention module consists of a linear classifier and a convolutional neural network (CNN). The MLP consists of an attention module, and a KNN classifier. The authors show that the proposed model outperforms the state-of-the-art models on several benchmark datasets."
6644,SP:ae9b6f7f2bd29ad1d24c4acbe1ecd345fcd6a081,Recurrent neural networks ( RNNs ) USED-FOR natural language. neural speed reading HYPONYM-OF inference. Structural - Jump - LSTM HYPONYM-OF neural speed reading model. LSTM CONJUNCTION agents. agents CONJUNCTION LSTM. one CONJUNCTION one. one CONJUNCTION one. one HYPONYM-OF agents. one HYPONYM-OF agents. one PART-OF model. one PART-OF model. LSTM PART-OF model. agents PART-OF model. model COMPARE neural reading models. neural reading models COMPARE model. accuracy EVALUATE-FOR vanilla LSTM. Structural - Jump - LSTM COMPARE vanilla LSTM. vanilla LSTM COMPARE Structural - Jump - LSTM. model COMPARE Structural - Jump - LSTM. Structural - Jump - LSTM COMPARE model. neural reading models COMPARE Structural - Jump - LSTM. Structural - Jump - LSTM COMPARE neural reading models. accuracy EVALUATE-FOR Structural - Jump - LSTM. floating point operations ( FLOP ) reduction EVALUATE-FOR Structural - Jump - LSTM. Method is RNNs. Metric is inference time. OtherScientificTerm is end of text markers. ,"This paper proposes a neural speed reading model called Structural-jump-LSTM, which is a combination of LSTM and agents. The authors show that the proposed model is able to reduce the number of floating-point operations (FLOPs) by up to 50% compared to vanilla LSTMs. They also show that their model outperforms the state-of-the-art in terms of accuracy. ","This paper proposes a neural speed reading model called Structural-jump-LSTM, which is a combination of LSTM and agents. The authors show that the proposed model is able to reduce the number of floating-point operations (FLOPs) by up to 50% compared to vanilla LSTMs. They also show that their model outperforms the state-of-the-art in terms of accuracy. "
6648,SP:9be782b532e64c6aad140531a17fbba1dd3342cd,"Mahalanobis distance function USED-FOR convolutional features. Mahalanobis distance function USED-FOR tight clusters. Mahalanobis distance function USED-FOR nonlinear radial basis convolutional feature transformation. MNIST CONJUNCTION ISBI ISIC skin lesion. ISBI ISIC skin lesion CONJUNCTION MNIST. ISBI ISIC skin lesion CONJUNCTION NIH ChestX - ray14. NIH ChestX - ray14 CONJUNCTION ISBI ISIC skin lesion. NIH ChestX - ray14 HYPONYM-OF image classification and segmentation data - sets. MNIST HYPONYM-OF image classification and segmentation data - sets. ISBI ISIC skin lesion HYPONYM-OF image classification and segmentation data - sets. image classification and segmentation data - sets EVALUATE-FOR method. robustness EVALUATE-FOR method. it COMPARE non - gradient masking defense strategies. non - gradient masking defense strategies COMPARE it. method USED-FOR deep convolutional neural networks. deep convolutional neural networks USED-FOR adversarial perturbations. Method is deep convolutional models. Generic is them. OtherScientificTerm are carefully crafted adversarial perturbations, clusters, small adversarial perturbations, and decision boundary. Material is clean data. ","This paper proposes a novel method to defend deep convolutional neural networks against small adversarial perturbations. The proposed method is based on the Mahalanobis distance function (MDF) which is a non-linear radial basis convolution. The authors show that the proposed method can be used to defend against small perturbation in the decision boundary of deep convolutions. The method is evaluated on MNIST, ISBI, and NIH ChestX-ray14.","This paper proposes a novel method to defend deep convolutional neural networks against small adversarial perturbations. The proposed method is based on the Mahalanobis distance function (MDF) which is a non-linear radial basis convolution. The authors show that the proposed method can be used to defend against small perturbation in the decision boundary of deep convolutions. The method is evaluated on MNIST, ISBI, and NIH ChestX-ray14."
6652,SP:b08dc82d5098474ddd68ab13003013ee6e7ba989,"exploratory behaviors USED-FOR local optima. exploratory behaviors USED-FOR Reinforcement learning agents. immediate dithering perturbation CONJUNCTION temporally consistent exploration. temporally consistent exploration CONJUNCTION immediate dithering perturbation. immediate dithering perturbation PART-OF behaviors. temporally consistent exploration PART-OF behaviors. sparse rewards CONJUNCTION long term information. long term information CONJUNCTION sparse rewards. stochastic policy model USED-FOR tasks. sparse rewards FEATURE-OF tasks. long term information FEATURE-OF tasks. global random variable USED-FOR conditional distribution. dropout USED-FOR reinforcement learning policies. inherent temporal consistency FEATURE-OF them. factors USED-FOR NADPEx policy. gradients ’ alignment HYPONYM-OF factors. naive exploration CONJUNCTION parameter noise. parameter noise CONJUNCTION naive exploration. NADPEx USED-FOR tasks. sparse reward USED-FOR NADPEx. naive exploration USED-FOR NADPEx. parameter noise USED-FOR NADPEx. sparse reward USED-FOR tasks. mujoco benchmark USED-FOR continuous control. mujoco benchmark EVALUATE-FOR It. Method are on - policy temporally consistent exploration strategy, and deep reinforcement learning agents. OtherScientificTerm are reward signals, and policy space. ","This paper studies the problem of learning an exploration strategy that is on-policy temporally consistent. The authors propose a stochastic policy model called NADPEx, which is based on a global random variable and a conditional distribution. They show that the proposed method is able to achieve state-of-the-art performance on the Mujoco continuous control task. They also show that their method can be applied to a variety of tasks.","This paper studies the problem of learning an exploration strategy that is on-policy temporally consistent. The authors propose a stochastic policy model called NADPEx, which is based on a global random variable and a conditional distribution. They show that the proposed method is able to achieve state-of-the-art performance on the Mujoco continuous control task. They also show that their method can be applied to a variety of tasks."
6656,SP:304930c105cf036ab48e9653926a5f61879dfea6,"gradient - based metric USED-FOR network. nonlinearity coefficient ( NLC ) HYPONYM-OF metric. NLC USED-FOR test error. NLC USED-FOR architecture search and design. Method are neural architectures, expert hand - tuning, and fully - connected feedforward networks. Task is architecture design. OtherScientificTerm is exploding or vanishing gradients. Generic are guideline, and it. ",This paper proposes a new metric for measuring the nonlinearity of a neural network. The proposed metric is based on the idea of exploding or vanishing gradients. The authors show that the exploding gradients can be used to measure the test error of neural networks. The paper also shows that the NLC is a good metric for evaluating the performance of a network.,This paper proposes a new metric for measuring the nonlinearity of a neural network. The proposed metric is based on the idea of exploding or vanishing gradients. The authors show that the exploding gradients can be used to measure the test error of neural networks. The paper also shows that the NLC is a good metric for evaluating the performance of a network.
6660,SP:17d8dc884e15131636a8c2490085ce42c05433c1,"bias amplification FEATURE-OF classifiers. inductive bias in gradient descent methods USED-FOR bias amplification. feature - wise bias amplification HYPONYM-OF bias. features FEATURE-OF model. targeted feature selection USED-FOR feature - wise bias amplification. feature selection algorithms USED-FOR bias amplification. they USED-FOR convolutional neural networks. feature selection algorithms USED-FOR linear models. bias amplification PART-OF linear models. algorithms USED-FOR reduced bias. synthetic and real data EVALUATE-FOR algorithms. Method is machine learning model. OtherScientificTerm are moderately - predictive “ weak ” features, and predictive bias. Material is insufficient training data. Metric is accuracy. ",This paper studies the problem of bias amplification in classifiers. The authors propose two algorithms to reduce the bias of a classifier in the presence of insufficient training data. The main contribution of the paper is to show that bias amplification can be reduced by using feature-wise bias amplification. The proposed algorithms are evaluated on synthetic and real-world datasets. ,This paper studies the problem of bias amplification in classifiers. The authors propose two algorithms to reduce the bias of a classifier in the presence of insufficient training data. The main contribution of the paper is to show that bias amplification can be reduced by using feature-wise bias amplification. The proposed algorithms are evaluated on synthetic and real-world datasets. 
6664,SP:2b84207c0015dba126d4ef4a89ef9cc29656f2f8,"over - parametrization USED-FOR generalization. over - parametrization USED-FOR neural networks. neural networks USED-FOR generalization. normalized margin CONJUNCTION generalization error bounds. generalization error bounds CONJUNCTION normalized margin. global minimizer FEATURE-OF weakly - regularized cross - entropy loss. generalization error bounds FEATURE-OF deep networks. maximum normalized margin FEATURE-OF global minimizer. two - layer networks FEATURE-OF infinite - width neural network. generalization guarantees EVALUATE-FOR infinite - width neural network. neural net margin COMPARE kernel methods. kernel methods COMPARE neural net margin. kernel methods HYPONYM-OF infinite feature methods. generalization guarantees EVALUATE-FOR kernel methods. infinite - neuron viewpoint USED-FOR analyzing optimization. perturbed gradient flow USED-FOR global optimizer. perturbed gradient flow USED-FOR infinite - size networks. polynomial time FEATURE-OF global optimizer. Method are margin - based perspective, and multi - layer feedforward relu networks. Material is natural instances. ","This paper studies the generalization properties of neural networks with infinite-width and multi-layer networks. The authors show that the global minimizer of the neural network is a weakly-regularized cross-entropy loss, and that the maximum normalized margin of the network can be used as a global optimizer. They also provide generalization error bounds for two-layer neural networks and infinite-size neural networks. ","This paper studies the generalization properties of neural networks with infinite-width and multi-layer networks. The authors show that the global minimizer of the neural network is a weakly-regularized cross-entropy loss, and that the maximum normalized margin of the network can be used as a global optimizer. They also provide generalization error bounds for two-layer neural networks and infinite-size neural networks. "
6668,SP:91459c66bb597751ffce8410e283ce3f094bdd5f,"image captions HYPONYM-OF natural language descriptions. generator CONJUNCTION discriminator. discriminator CONJUNCTION generator. object pathway USED-FOR generator. object pathway USED-FOR discriminator. semantic layout USED-FOR approach. image background CONJUNCTION image layout. image layout CONJUNCTION image background. global pathway USED-FOR image background. global pathway USED-FOR image layout. Multi - MNIST CONJUNCTION CLEVR. CLEVR CONJUNCTION Multi - MNIST. CLEVR CONJUNCTION MSCOCO data set. MSCOCO data set CONJUNCTION CLEVR. Multi - MNIST CONJUNCTION MSCOCO data set. MSCOCO data set CONJUNCTION Multi - MNIST. global image characteristics CONJUNCTION image background. image background CONJUNCTION global image characteristics. object pathway USED-FOR features. global pathway USED-FOR image background. global pathway USED-FOR global image characteristics. object pathway COMPARE global pathway. global pathway COMPARE object pathway. Method is Generative Adversarial Networks ( GANs ). OtherScientificTerm are bounding boxes, and complex scenes. ","This paper proposes a novel method for generating captions for image captions. The method is based on the idea of object pathway, which is an object pathway that is used to train a discriminator and a generator. The proposed method is evaluated on three datasets: CLEVR, Multi-MNIST, and MSCOCO. The results show that the proposed method outperforms the baselines. ","This paper proposes a novel method for generating captions for image captions. The method is based on the idea of object pathway, which is an object pathway that is used to train a discriminator and a generator. The proposed method is evaluated on three datasets: CLEVR, Multi-MNIST, and MSCOCO. The results show that the proposed method outperforms the baselines. "
6672,SP:fbfe2c90a70a6adf39fa4d4a3c28f6b5adbc6c06,models USED-FOR policy improvement. learning models USED-FOR global model methods. local model methods USED-FOR system dynamics. representations USED-FOR simple dynamics. local models USED-FOR policy learning in complex systems. real Sawyer robotic arm USED-FOR manipulation task. manipulation task HYPONYM-OF robotics tasks. robotics tasks EVALUATE-FOR approach. camera images USED-FOR manipulation task. OtherScientificTerm is local improvements. ,"This paper proposes a method to improve the performance of local model methods for policy learning in complex systems. The method is based on the idea that local models can be used to learn a representation of the system dynamics, which can then be used for policy improvement. The proposed method is evaluated on the Sawyer robotic arm manipulation task and is shown to outperform existing methods. ","This paper proposes a method to improve the performance of local model methods for policy learning in complex systems. The method is based on the idea that local models can be used to learn a representation of the system dynamics, which can then be used for policy improvement. The proposed method is evaluated on the Sawyer robotic arm manipulation task and is shown to outperform existing methods. "
6676,SP:9a4c7d9df6685347e75e0ae72928225b7622a73c,"reinforcement learning algorithms USED-FOR real experience. models USED-FOR Learning policies. POMDPs FEATURE-OF learning policies. off - policy experience USED-FOR learning policies. structural causal models USED-FOR counterfactual evaluation of arbitrary policies. It USED-FOR counterfactual evaluation of arbitrary policies. structural causal models USED-FOR It. CF - GPS COMPARE vanilla model - based RL algorithms. vanilla model - based RL algorithms COMPARE CF - GPS. logged data USED-FOR CF - GPS. off - policy algorithms COMPARE CF - GPS. CF - GPS COMPARE off - policy algorithms. model USED-FOR CF - GPS. experience data USED-FOR algorithm. Importance Sampling USED-FOR off - policy algorithms. CF - GPS USED-FOR Guided Policy Search. counterfactual methods USED-FOR reparameterization - based algorithms. Stochastic Value Gradient HYPONYM-OF reparameterization - based algorithms. Task are simulating plausible experience de novo, modelbased policy evaluation and search, and de novo synthesis of data. OtherScientificTerm are logged, real experience, counterfactual actions, and off - policy episodes. ",This paper proposes a new approach to counterfactual evaluation of off-policy experience in reinforcement learning. The proposed approach is based on the idea of Importance Sampling (IS) and Counterfactual Experiments (CF-GPS). The authors show that the proposed approach outperforms the baselines in terms of performance on a number of tasks. ,This paper proposes a new approach to counterfactual evaluation of off-policy experience in reinforcement learning. The proposed approach is based on the idea of Importance Sampling (IS) and Counterfactual Experiments (CF-GPS). The authors show that the proposed approach outperforms the baselines in terms of performance on a number of tasks. 
6680,SP:9371d08e2b3a821e40cc9d4757c22f6cdb731b6a,"flat local minima of loss surface USED-FOR generalization. parameter space FEATURE-OF flat local minima of loss surface. parameter space FEATURE-OF loss surface. generalization EVALUATE-FOR loss surface. parameter space CONJUNCTION input space. input space CONJUNCTION parameter space. input space FEATURE-OF decision surfaces. parameter space FEATURE-OF decision surfaces. input space FEATURE-OF decision surface. adversarial robustness indicator USED-FOR neural network ’s intrinsic robustness property. method USED-FOR network ’s intrinsic adversarial robustness. Method are neural network generalization, robust training method, and adversarial training. OtherScientificTerm are adversarial settings, and adversarial attacks. Metric is adversarial robustness. ",This paper studies the problem of adversarial robustness in neural networks. The authors propose a new metric to measure the robustness of a neural network to adversarial attacks. The metric is based on the flat local minima of the loss surface of the input space and the parameter space of the decision surface. The paper also proposes a robust training method for adversarial training. ,This paper studies the problem of adversarial robustness in neural networks. The authors propose a new metric to measure the robustness of a neural network to adversarial attacks. The metric is based on the flat local minima of the loss surface of the input space and the parameter space of the decision surface. The paper also proposes a robust training method for adversarial training. 
6684,SP:6f94f59bc936a11d95ded7309dc2458fee6d2595,"autonomous drones CONJUNCTION wearable devices. wearable devices CONJUNCTION autonomous drones. energy consumption PART-OF DNN training. weighted sparse projection CONJUNCTION input masking. input masking CONJUNCTION weighted sparse projection. end - to - end DNN training framework USED-FOR quantitative energy consumption guarantees. weighted sparse projection USED-FOR end - to - end DNN training framework. input masking USED-FOR end - to - end DNN training framework. weighted sparse projection USED-FOR quantitative energy consumption guarantees. input masking USED-FOR quantitative energy consumption guarantees. optimization problem USED-FOR DNN training. DNN training process USED-FOR constrained optimization. quantitative DNN energy estimation PART-OF DNN training process. approximate algorithm USED-FOR optimization problem. framework USED-FOR DNNs. prior energy - saving methods COMPARE framework. framework COMPARE prior energy - saving methods. Method is Deep Neural Networks ( DNNs ). OtherScientificTerm are energy budget, optimization constraint, and energy budgets. ",This paper proposes an end-to-end DNN training framework to reduce the energy consumption of deep neural networks. The authors propose to use weighted sparse projection and input masking to improve the energy efficiency of the training process. They also propose an approximate algorithm to solve the optimization problem. The paper shows that the proposed method outperforms existing methods in terms of energy efficiency.,This paper proposes an end-to-end DNN training framework to reduce the energy consumption of deep neural networks. The authors propose to use weighted sparse projection and input masking to improve the energy efficiency of the training process. They also propose an approximate algorithm to solve the optimization problem. The paper shows that the proposed method outperforms existing methods in terms of energy efficiency.
6688,SP:7f07f3fa8a10b48bb380a7c84bc012ce3541122b,"Addressing uncertainty USED-FOR autonomous systems. latent model parameters FEATURE-OF posterior distribution. belief distribution USED-FOR expected long - term reward. universal policy USED-FOR Bayesian value function. policy optimization algorithms USED-FOR universal policy. universal policy USED-FOR exploration - exploitation trade - off. policy optimization algorithms USED-FOR Bayesian Policy Optimization. policy optimization algorithms USED-FOR algorithm. policy network architecture USED-FOR belief distribution. observable state FEATURE-OF belief distribution. method COMPARE Partially Observable Markov Decision Process solvers. Partially Observable Markov Decision Process solvers COMPARE method. method COMPARE algorithms. algorithms COMPARE method. algorithms USED-FOR model uncertainty. algorithms COMPARE Partially Observable Markov Decision Process solvers. Partially Observable Markov Decision Process solvers COMPARE algorithms. method USED-FOR model uncertainty. OtherScientificTerm are continuous latent parameter space, and belief distributions. ","This paper proposes a universal policy optimization algorithm for Bayesian policy optimization. The main idea is to learn a belief distribution over a continuous latent parameter space, which is then used to optimize a Bayesian value function. The authors show that the proposed algorithm can be used to solve the exploration-exploitation trade-off problem. The paper also shows that the method can be applied to partially observable Markov Decision Process solvers.","This paper proposes a universal policy optimization algorithm for Bayesian policy optimization. The main idea is to learn a belief distribution over a continuous latent parameter space, which is then used to optimize a Bayesian value function. The authors show that the proposed algorithm can be used to solve the exploration-exploitation trade-off problem. The paper also shows that the method can be applied to partially observable Markov Decision Process solvers."
6692,SP:3823faee83bc07a989934af5495dafd003c27921,"unified framework USED-FOR unsupervised representations of entities. optimal transport USED-FOR representations. method USED-FOR uncertainty. sentence similarity CONJUNCTION word entailment detection. word entailment detection CONJUNCTION sentence similarity. unsupervised representations USED-FOR text. tasks EVALUATE-FOR it. word entailment detection HYPONYM-OF tasks. sentence similarity HYPONYM-OF tasks. approach USED-FOR unsupervised or supervised problem. co - occurrence structure FEATURE-OF unsupervised or supervised problem. sequence data HYPONYM-OF co - occurrence structure. Wasserstein distances CONJUNCTION Wasserstein barycenters. Wasserstein barycenters CONJUNCTION Wasserstein distances. Wasserstein distances USED-FOR framework. OtherScientificTerm are histogram ( or distribution ), entities, distributions, and optimal transport map. Method is rich and powerful feature representations. ",This paper proposes a novel unsupervised representation learning framework for learning representations of entities. The proposed framework is based on the Wasserstein distance between two entities and the optimal transport map between them. The authors show that the proposed method can be used to learn representations for both un-supervised and supervised tasks. The method is evaluated on two tasks: sentence similarity and word entailment detection.,This paper proposes a novel unsupervised representation learning framework for learning representations of entities. The proposed framework is based on the Wasserstein distance between two entities and the optimal transport map between them. The authors show that the proposed method can be used to learn representations for both un-supervised and supervised tasks. The method is evaluated on two tasks: sentence similarity and word entailment detection.
6696,SP:9ce5b80147ea2c7d0711ec98e31f4bbb5eac534e,model - based reinforcement learning methods COMPARE model - free approaches. model - free approaches COMPARE model - based reinforcement learning methods. model - based reinforcement learning methods USED-FOR complex simulated environments. dynamics model CONJUNCTION planner. planner CONJUNCTION dynamics model. recursions USED-FOR long - range planning. recursions USED-FOR state estimates. model accuracy CONJUNCTION performance. performance CONJUNCTION model accuracy. task reward EVALUATE-FOR model - free approaches. Material is MuJoCo environments. OtherScientificTerm is long planning horizons. ,This paper proposes a model-based reinforcement learning method for long-range planning in MuJoCo environments. The main idea is to use a dynamics model and a planner to jointly learn the dynamics model for planning and the planner for learning the planner. The dynamics model is used to estimate the state of the environment and planner is used for planning. The authors show that the proposed method outperforms the state-of-the-art model-free RL methods on the MuJoco task. ,This paper proposes a model-based reinforcement learning method for long-range planning in MuJoCo environments. The main idea is to use a dynamics model and a planner to jointly learn the dynamics model for planning and the planner for learning the planner. The dynamics model is used to estimate the state of the environment and planner is used for planning. The authors show that the proposed method outperforms the state-of-the-art model-free RL methods on the MuJoco task. 
6700,SP:da14205470819495a3aad69d64de4033749d4d3e,2or 3 - bit precision HYPONYM-OF ultra - low precision. precision highway USED-FOR ultralow - precision computation. precision highway USED-FOR convolutional and recurrent neural networks. precision highway USED-FOR accumulated quantization error. accumulated quantization error FEATURE-OF convolutional and recurrent neural networks. hardware accelerator EVALUATE-FOR overhead. method COMPARE quantization methods. quantization methods COMPARE method. 3 - bit weight / activation quantization CONJUNCTION 2 - bit quantization. 2 - bit quantization CONJUNCTION 3 - bit weight / activation quantization. top-1 accuracy loss EVALUATE-FOR ResNet-50. 3 - bit weight / activation quantization EVALUATE-FOR method. ResNet-50 EVALUATE-FOR 2 - bit quantization. top-1 accuracy loss EVALUATE-FOR 2 - bit quantization. accuracy loss EVALUATE-FOR method. top-1 accuracy loss EVALUATE-FOR method. method COMPARE method. method COMPARE method. LSTM USED-FOR language modeling. 2 - bit quantization FEATURE-OF LSTM. 2 - bit quantization USED-FOR method. 2 - bit quantization FEATURE-OF method. Method is Neural network quantization. ,This paper proposes a new method for quantization of convolutional and recurrent neural networks with 2-bit and 3-bit precision. The proposed method is based on a precision highway approach to compute the quantization error for each layer of the network. The method is evaluated on the ResNet-50 and CIFAR-10 datasets. The paper shows that the proposed method can achieve better performance than existing quantization methods. ,This paper proposes a new method for quantization of convolutional and recurrent neural networks with 2-bit and 3-bit precision. The proposed method is based on a precision highway approach to compute the quantization error for each layer of the network. The method is evaluated on the ResNet-50 and CIFAR-10 datasets. The paper shows that the proposed method can achieve better performance than existing quantization methods. 
6704,SP:0355b54430b39b52df94014d78289dd6e1e81795,"method USED-FOR image restoration problems. deblurring CONJUNCTION super - resolution. super - resolution CONJUNCTION deblurring. denoising CONJUNCTION deblurring. deblurring CONJUNCTION denoising. denoising HYPONYM-OF image restoration problems. super - resolution HYPONYM-OF image restoration problems. deblurring HYPONYM-OF image restoration problems. constrained optimization problem USED-FOR problem. Generative Adversarial Network ( GAN ) USED-FOR density estimation model. OtherScientificTerm are posteriori probability of latent variables, latent variables, and degraded image. Material is MNIST dataset. ",This paper studies the problem of image restoration and denoising. The authors propose a generative adversarial network (GAN) based method to estimate the posteriori probability of latent variables of a degraded image. The proposed method is based on the GAN framework and is able to achieve state-of-the-art performance on the MNIST dataset. ,This paper studies the problem of image restoration and denoising. The authors propose a generative adversarial network (GAN) based method to estimate the posteriori probability of latent variables of a degraded image. The proposed method is based on the GAN framework and is able to achieve state-of-the-art performance on the MNIST dataset. 
6708,SP:2feef921a0563d52fde1c074da754f73e6cabef8,"softmax outputs CONJUNCTION feature responses. feature responses CONJUNCTION softmax outputs. large "" teacher "" network CONJUNCTION compact "" student "" network. compact "" student "" network CONJUNCTION large "" teacher "" network. full training data USED-FOR knowledge distillation methods. method USED-FOR knowledge distillation. conv - layer PART-OF student - net. layer PART-OF conv - layer. layer PART-OF conv - layer. computation cost FEATURE-OF conv - layer. teacher - net CONJUNCTION student - net constructing. student - net constructing CONJUNCTION teacher - net. OtherScientificTerm are human cognition, feature map sizes, and block - level outputs. ",This paper proposes a new method for knowledge distillation based on convolutional neural networks (CNNs). The authors propose to use the conv-layer of the teacher-net to learn the output of the student-net. The authors show that the proposed method is computationally efficient and can be applied to a wide range of tasks. ,This paper proposes a new method for knowledge distillation based on convolutional neural networks (CNNs). The authors propose to use the conv-layer of the teacher-net to learn the output of the student-net. The authors show that the proposed method is computationally efficient and can be applied to a wide range of tasks. 
6712,SP:ca491b166bd8bf1a7c71657471a2f58b7fd36609,task relatedness USED-FOR transferability. transferred representations USED-FOR classification problems. H - score HYPONYM-OF evaluation function. evaluation function EVALUATE-FOR transferred representations. asymptotic error probability FEATURE-OF decision function. information theoretic approach USED-FOR H - score. asymptotic error probability FEATURE-OF H - score. transferred feature USED-FOR asymptotic error probability. transferred feature USED-FOR H - score. source tasks USED-FOR task transfer learning problems. transferability USED-FOR source tasks. it USED-FOR inference problems. recognition tasks USED-FOR 3D indoor - scene understanding. classification HYPONYM-OF inference problems. recognition tasks HYPONYM-OF inference problems. Task is task transfer learning. Metric is task transferability. OtherScientificTerm is representations. Generic is metric. Method is transfer learning policies. Material is synthetic and real image data. ,This paper studies the problem of task transfer learning. The authors propose a new metric called H-score to measure the transferability between source tasks and target tasks. The metric is based on the asymptotic error probability of the decision function of the source task and the target task. The proposed metric is used to evaluate transferability of source tasks. ,This paper studies the problem of task transfer learning. The authors propose a new metric called H-score to measure the transferability between source tasks and target tasks. The metric is based on the asymptotic error probability of the decision function of the source task and the target task. The proposed metric is used to evaluate transferability of source tasks. 
6716,SP:c6884b04001bd0d43aa47e2d72ebbe2bbc89ab3d,"Planning USED-FOR complex languages. planning phase USED-FOR global sentence structure. planning phase PART-OF neural machine translation. discrete structural representations USED-FOR syntactic information. approach USED-FOR discrete structural representations. beam search USED-FOR structural codes. discrete codes USED-FOR word generation. codes USED-FOR pure structural variations. codes USED-FOR translation. structural planning USED-FOR global sentence structure. Method is language generation models. Metric are structural diversity metric, and diversity scores. Material is sampled paraphrase translations. ","This paper proposes a novel approach to learning structural codes for neural machine translation (NMT) models. The proposed approach is based on the idea of structural planning, which is a planning phase in NMT models that aims to learn discrete structural representations for syntactic information. The authors propose to use beam search to find discrete codes for each word in a sentence, and then use these codes for word generation. The method is evaluated on a number of NMT tasks, and the results show that the proposed method outperforms baselines in terms of structural diversity.","This paper proposes a novel approach to learning structural codes for neural machine translation (NMT) models. The proposed approach is based on the idea of structural planning, which is a planning phase in NMT models that aims to learn discrete structural representations for syntactic information. The authors propose to use beam search to find discrete codes for each word in a sentence, and then use these codes for word generation. The method is evaluated on a number of NMT tasks, and the results show that the proposed method outperforms baselines in terms of structural diversity."
6720,SP:51810c5f8d40d9ec40469349f1612bf2eefe9aad,"discriminator PART-OF generative adversarial network ( SGAN ). SGAN COMPARE integral probability metric ( IPM ) GANs. integral probability metric ( IPM ) GANs COMPARE SGAN. real data COMPARE randomly sampled fake data. randomly sampled fake data COMPARE real data. real data COMPARE fake data. fake data COMPARE real data. approaches USED-FOR GAN loss functions. Relativistic GANs ( RGANs ) CONJUNCTION Relativistic average GANs ( RaGANs ). Relativistic average GANs ( RaGANs ) CONJUNCTION Relativistic GANs ( RGANs ). Relativistic GANs ( RGANs ) HYPONYM-OF them. Relativistic average GANs ( RaGANs ) HYPONYM-OF them. IPM - based GANs HYPONYM-OF RGANs. identity function USED-FOR RGANs. identity function USED-FOR IPM - based GANs. RaGANs COMPARE non - relativistic counterparts. non - relativistic counterparts COMPARE RaGANs. RGANs COMPARE non - relativistic counterparts. non - relativistic counterparts COMPARE RGANs. images COMPARE ones. ones COMPARE images. WGAN - GP CONJUNCTION SGAN. SGAN CONJUNCTION WGAN - GP. RGANs CONJUNCTION RaGANs. RaGANs CONJUNCTION RGANs. GAN CONJUNCTION LSGAN. LSGAN CONJUNCTION GAN. RaGAN COMPARE WGAN - GP. WGAN - GP COMPARE RaGAN. WGAN - GP CONJUNCTION spectral normalization. spectral normalization CONJUNCTION WGAN - GP. discriminator update CONJUNCTION generator update. generator update CONJUNCTION discriminator update. spectral normalization USED-FOR SGAN. SGAN USED-FOR images. WGAN - GP USED-FOR images. gradient penalty USED-FOR RaGAN. SGAN USED-FOR ones. WGAN - GP USED-FOR ones. spectral normalization USED-FOR ones. Method are generator G, divergence minimization, and relativistic discriminator. OtherScientificTerm is priori knowledge. Generic is code.","This paper proposes a new loss function for generative adversarial networks (GANs) based on integral probability metric (IPM) GANs. The proposed loss function is based on the identity function of the discriminator. The authors show that this loss function can be used to improve the performance of existing GAN models. They also show that the proposed loss functions can be applied to existing RGANs, RaGANs, and WGAN-GP models.","This paper proposes a new loss function for generative adversarial networks (GANs) based on integral probability metric (IPM) GANs. The proposed loss function is based on the identity function of the discriminator. The authors show that this loss function can be used to improve the performance of existing GAN models. They also show that the proposed loss functions can be applied to existing RGANs, RaGANs, and WGAN-GP models."
6724,SP:8df1599919dcb3329553e75ffb19059f192542ea,"catastrophic forgetting FEATURE-OF neural network architectures. approach USED-FOR problem. approach USED-FOR model. solver HYPONYM-OF model. Task is AI and lifelong learning systems. Method are continual learning methods, and Parameter Generation. ",This paper studies the problem of catastrophic forgetting in continual learning. The authors propose a new method to solve the problem. The key idea is to learn a solver for the problem and then use this solver to train a new model. The proposed method is evaluated on a variety of continual learning tasks and shows promising results. ,This paper studies the problem of catastrophic forgetting in continual learning. The authors propose a new method to solve the problem. The key idea is to learn a solver for the problem and then use this solver to train a new model. The proposed method is evaluated on a variety of continual learning tasks and shows promising results. 
6728,SP:1342b6e11d1ccf04ee95b63d8b7a88b184dee43e,"artificial agents USED-FOR them. rich and orderly structure USED-FOR systems. behavioral dynamics FEATURE-OF multi - agent systems. rich and orderly structure FEATURE-OF behavioral dynamics. rich and orderly structure FEATURE-OF multi - agent systems. Relational Forward Models ( RFM ) USED-FOR multi - agent learning. they USED-FOR interpretable intermediate representations. discrete entities USED-FOR models. RFM modules USED-FOR learning systems. learning systems COMPARE non - augmented baselines. non - augmented baselines COMPARE learning systems. RFM modules PART-OF agents. RFM modules COMPARE non - augmented baselines. non - augmented baselines COMPARE RFM modules. Generic is networks. OtherScientificTerm are multi - agent environments, and social interactions. Method is autonomous systems. ","This paper studies the problem of multi-agent learning in the context of social interactions. The authors propose a new approach to learn a relational forward model (RFM) that is able to capture the interplay between multiple agents in a system. The proposed approach is based on the Relational Forward Model (RFM) framework, which is used to learn the intermediate representations of discrete entities (e.g. agents) in a social interaction setting.  The authors show that the proposed approach outperforms the baselines in terms of performance on a number of tasks. ","This paper studies the problem of multi-agent learning in the context of social interactions. The authors propose a new approach to learn a relational forward model (RFM) that is able to capture the interplay between multiple agents in a system. The proposed approach is based on the Relational Forward Model (RFM) framework, which is used to learn the intermediate representations of discrete entities (e.g. agents) in a social interaction setting.  The authors show that the proposed approach outperforms the baselines in terms of performance on a number of tasks. "
6732,SP:f2f01c7c4fb68c25d6e5ac56cbf79615ed1ee9ee,"reinforcement learning USED-FOR real world problems. expert behavior USED-FOR reward function. method USED-FOR rewards. prior USED-FOR approach. images USED-FOR rewards. OtherScientificTerm are oracle reward function, reward functions, and expressive reward functions. Method is Inverse reinforcement learning ( IRL ). Material is datasets of demonstrations. Task is IRL. Generic is tasks. ",This paper proposes a method for inverse reinforcement learning (IRL) where the goal is to learn an oracle reward function that is expressive of the expert behavior. The authors propose a method to learn the reward function from a dataset of demonstrations. The reward function is learned by learning a prior that predicts the expert's reward function. The method is evaluated on a variety of tasks and shows that the proposed method outperforms baselines. ,This paper proposes a method for inverse reinforcement learning (IRL) where the goal is to learn an oracle reward function that is expressive of the expert behavior. The authors propose a method to learn the reward function from a dataset of demonstrations. The reward function is learned by learning a prior that predicts the expert's reward function. The method is evaluated on a variety of tasks and shows that the proposed method outperforms baselines. 
6736,SP:4c2f45c7fd0cac662a33be602985cf360b45fe4d,"framework USED-FOR data efficient and versatile learning. framework USED-FOR Meta - Learning approximate Probabilistic Inference. Meta - Learning approximate Probabilistic Inference USED-FOR Prediction. ML - PIP HYPONYM-OF framework. ML - PIP USED-FOR methods. probabilistic interpretations of meta - learning USED-FOR methods. probabilistic interpretations of meta - learning USED-FOR ML - PIP. VERSA HYPONYM-OF framework. flexible and versatile amortization network USED-FOR framework. flexible and versatile amortization network USED-FOR VERSA. benchmark datasets EVALUATE-FOR method. benchmark datasets EVALUATE-FOR VERSA. few - shot ShapeNet view reconstruction task EVALUATE-FOR approach. Material is few - shot learning datasets. OtherScientificTerm are task - specific parameters, and second derivatives. Task are optimization, inference, and classification. Method is inference networks. ","This paper proposes a framework for meta-learning approximate Probabilistic Inference (ML-PIP) for few-shot learning. The proposed method is based on the idea of probabilistic approximations of meta learning, which can be applied to the task-specific parameters and second derivatives. The authors show that the proposed method outperforms the state-of-the-art methods on several benchmark datasets. ","This paper proposes a framework for meta-learning approximate Probabilistic Inference (ML-PIP) for few-shot learning. The proposed method is based on the idea of probabilistic approximations of meta learning, which can be applied to the task-specific parameters and second derivatives. The authors show that the proposed method outperforms the state-of-the-art methods on several benchmark datasets. "
6740,SP:44e0f63ffee15796ba6135463134084bb370627b,"deep learning architecture USED-FOR classifying structured objects. ultrafine - grained datasets USED-FOR deep learning architecture. localvisual features CONJUNCTION neighboring class information. neighboring class information CONJUNCTION localvisual features. linear - chain CRFs USED-FOR images. visual features COMPARE class - structure information. class - structure information COMPARE visual features. convolutional layers USED-FOR visual features. CRF pairwise potential matrix USED-FOR class - structure information. parametrization USED-FOR nonlinear objective function. surrogate likelihood USED-FOR local likelihood approximation. local likelihood approximation USED-FOR CRF. surrogate likelihood USED-FOR CRF. integrated batch - normalization USED-FOR CRF. integrated batch - normalization USED-FOR local likelihood approximation. CRF methods USED-FOR contextual relationships. model COMPARE CRF methods. CRF methods COMPARE model. method COMPARE linear CRF parametrization. linear CRF parametrization COMPARE method. unnormalized likelihood optimization CONJUNCTION RNN modeling. RNN modeling CONJUNCTION unnormalized likelihood optimization. linear CRF parametrization CONJUNCTION unnormalized likelihood optimization. unnormalized likelihood optimization CONJUNCTION linear CRF parametrization. linear CRF parametrization COMPARE RNN modeling. RNN modeling COMPARE linear CRF parametrization. method COMPARE RNN modeling. RNN modeling COMPARE method. OtherScientificTerm are context - based semantic similarity space, visual similarities, and contextual information. Material is images of retail - store product displays. ",This paper proposes a new method for learning a linear-chain convolutional neural network (CRF) model for image classification. The proposed method is based on the idea of learning a pairwise potential matrix (POMDP) for the CRF. The authors show that the proposed method can be used to approximate the local likelihood of a given image. They also show that their method is able to achieve better performance than existing methods. ,This paper proposes a new method for learning a linear-chain convolutional neural network (CRF) model for image classification. The proposed method is based on the idea of learning a pairwise potential matrix (POMDP) for the CRF. The authors show that the proposed method can be used to approximate the local likelihood of a given image. They also show that their method is able to achieve better performance than existing methods. 
6744,SP:18be2cb182761b64fa232c1b7d1899882e5bcf15,"global structure CONJUNCTION fine - scale waveform coherence. fine - scale waveform coherence CONJUNCTION global structure. audio synthesis HYPONYM-OF machine learning task. global structure FEATURE-OF human perception. fine - scale waveform coherence FEATURE-OF human perception. local structure FEATURE-OF Autoregressive models. WaveNet HYPONYM-OF Autoregressive models. global latent conditioning CONJUNCTION parallel sampling. parallel sampling CONJUNCTION global latent conditioning. global latent conditioning FEATURE-OF Generative Adversarial Networks ( GANs ). parallel sampling USED-FOR Generative Adversarial Networks ( GANs ). log magnitudes CONJUNCTION instantaneous frequencies. instantaneous frequencies CONJUNCTION log magnitudes. GANs USED-FOR high - fidelity and locally - coherent audio. log magnitudes USED-FOR GANs. spectral domain FEATURE-OF sufficient frequency resolution. sufficient frequency resolution FEATURE-OF log magnitudes. sufficient frequency resolution FEATURE-OF instantaneous frequencies. spectral domain FEATURE-OF instantaneous frequencies. sufficient frequency resolution USED-FOR GANs. GANs COMPARE WaveNet baselines. WaveNet baselines COMPARE GANs. NSynth dataset EVALUATE-FOR GANs. automated and human evaluation metrics EVALUATE-FOR WaveNet baselines. automated and human evaluation metrics EVALUATE-FOR GANs. Method is iterative sampling. OtherScientificTerm are global latent structure, and locally - coherent audio waveforms. ",This paper studies the problem of generating high-fidelity and locally-coherent audio. The authors propose a new method for generating high fidelity and locally coherent audio based on GANs. The method is based on parallel sampling and global latent conditioning. The proposed method is evaluated on the NSynth dataset and compared to WaveNet baselines. ,This paper studies the problem of generating high-fidelity and locally-coherent audio. The authors propose a new method for generating high fidelity and locally coherent audio based on GANs. The method is based on parallel sampling and global latent conditioning. The proposed method is evaluated on the NSynth dataset and compared to WaveNet baselines. 
6748,SP:0c0f078c208600f541a76ecaae49cf9a98588736,"training accuracy EVALUATE-FOR Neural networks. mixed integer program USED-FOR piecewise - linear neural networks. verifier COMPARE state - of - the - art. state - of - the - art COMPARE verifier. finding minimum adversarial distortions EVALUATE-FOR verifier. tight formulations USED-FOR non - linearities. tight formulations CONJUNCTION presolve algorithm. presolve algorithm CONJUNCTION tight formulations. tight formulations USED-FOR computational speedup. verifier USED-FOR networks. adversarial accuracy EVALUATE-FOR MNIST classifier. adversarial accuracy EVALUATE-FOR perturbations. perturbations FEATURE-OF MNIST classifier. adversarial example USED-FOR classifier. bounded l∞ norm FEATURE-OF perturbations. robust training procedures CONJUNCTION network architectures. network architectures CONJUNCTION robust training procedures. adversarial examples COMPARE first - order attack. first - order attack COMPARE adversarial examples. Task are Verification of networks, and verification of piecewise - linear neural networks. OtherScientificTerm are minimum adversarial distortions, ReLUs, and norm - bounded perturbations. Method is convolutional and residual networks. Material is MNIST and CIFAR-10 datasets. ",This paper studies the problem of verifying the robustness of piecewise-linear neural networks against adversarial perturbations. The authors propose to use a mixed integer program to find the minimum adversarial distortions for piecewise linear neural networks. They show that the proposed method outperforms the state-of-the-art methods on MNIST and CIFAR-10 datasets. ,This paper studies the problem of verifying the robustness of piecewise-linear neural networks against adversarial perturbations. The authors propose to use a mixed integer program to find the minimum adversarial distortions for piecewise linear neural networks. They show that the proposed method outperforms the state-of-the-art methods on MNIST and CIFAR-10 datasets. 
6752,SP:dc48dbfb8f4f25d3ceb7be607e8f2e0bc8f99f14,rich structure FEATURE-OF real world tasks. repeated structure USED-FOR learning. default policy HYPONYM-OF component. component PART-OF KL regularized expected reward objective. it USED-FOR reusable behaviours. reusable behaviours USED-FOR policy. information bottleneck approaches CONJUNCTION variational EM algorithm. variational EM algorithm CONJUNCTION information bottleneck approaches. default policy CONJUNCTION policy. policy CONJUNCTION default policy. default policy USED-FOR tasks. Method is fixed default policy. Generic is strategy. Material is discrete and continuous action domains. ,"This paper studies the problem of learning a policy that can be reused across different tasks. The authors propose to use a KL regularized expected reward objective (KLPRO) as a regularization term for the default policy. The KLPRO is an extension of the variational EM algorithm, and the authors show that it can be used to improve the performance of the learned policy. They also show that the proposed method can be applied to continuous and discrete action domains.","This paper studies the problem of learning a policy that can be reused across different tasks. The authors propose to use a KL regularized expected reward objective (KLPRO) as a regularization term for the default policy. The KLPRO is an extension of the variational EM algorithm, and the authors show that it can be used to improve the performance of the learned policy. They also show that the proposed method can be applied to continuous and discrete action domains."
6756,SP:08a6a48b05e2c00d77a73413cbba52cda08e184c,conversation history USED-FOR Conversational machine comprehension. single - turn models USED-FOR history. FLOW USED-FOR single - turn models. intermediate representations PART-OF FLOW. intermediate representations PART-OF mechanism. alternating parallel processing structure USED-FOR FLOW. alternating parallel processing structure USED-FOR intermediate representations. approaches COMPARE FLOW. FLOW COMPARE approaches. latent semantics PART-OF FLOW. CoQA CONJUNCTION QuAC. QuAC CONJUNCTION CoQA. FLOWQA HYPONYM-OF model. conversational challenges EVALUATE-FOR FLOWQA. conversational challenges EVALUATE-FOR model. tasks EVALUATE-FOR FLOW. FLOWQA COMPARE models. models COMPARE FLOWQA. sequential instruction understanding USED-FOR conversational machine comprehension. SCONE EVALUATE-FOR models. accuracy EVALUATE-FOR FLOWQA. SCONE EVALUATE-FOR FLOWQA. Metric is F1. ,"This paper proposes FLOWQA, a sequential instruction understanding model for conversational machine comprehension. The model is based on FLOW, which is an extension of the FLOW model. FLOW is a sequential model that learns a latent representation of the instruction history, which can be used to learn intermediate representations of intermediate representations. The authors show that FLOW outperforms the state-of-the-art models on SCONE and CoQA tasks.","This paper proposes FLOWQA, a sequential instruction understanding model for conversational machine comprehension. The model is based on FLOW, which is an extension of the FLOW model. FLOW is a sequential model that learns a latent representation of the instruction history, which can be used to learn intermediate representations of intermediate representations. The authors show that FLOW outperforms the state-of-the-art models on SCONE and CoQA tasks."
6760,SP:fbb7bb8b4f75715f139c702750b28e7e87aa0e1f,"Programming languages HYPONYM-OF machine learning. generative models USED-FOR generating static snapshots of code. source code HYPONYM-OF dynamic object. synthetic data USED-FOR edit patterns. synthetic data USED-FOR neural networks. generalization USED-FOR edit patterns. large - scale dataset USED-FOR models. fine - grained edits PART-OF large - scale dataset. Method are generative models of source code, and attentional and pointer network components. OtherScientificTerm are static snapshots of code, and source code files. Generic are it, and tools. Metric is scalability. ",This paper proposes a generative model for generating static snapshots of code. The model is based on the idea that a static snapshot of a program is a dynamic object that can be used to train a neural network. The authors propose to use a large-scale dataset to train the model. They show that the model is able to generalize well to a large number of edits. They also show that their model generalizes well to fine-grained edits.,This paper proposes a generative model for generating static snapshots of code. The model is based on the idea that a static snapshot of a program is a dynamic object that can be used to train a neural network. The authors propose to use a large-scale dataset to train the model. They show that the model is able to generalize well to a large number of edits. They also show that their model generalizes well to fine-grained edits.
6764,SP:dbb06f953788696f65013765f0a4e6967444fa0f,"strategy USED-FOR multi - class classification. pairwise similarity HYPONYM-OF annotation. pairwise similarity USED-FOR strategy. binary classifier USED-FOR pairwise similarity prediction. method USED-FOR binary classifier. submodule USED-FOR multi - class classifier. loss function USED-FOR neural network - based models. probabilistic graphical model USED-FOR it. method COMPARE state of the art. state of the art COMPARE method. learning paradigms EVALUATE-FOR state of the art. learning paradigms EVALUATE-FOR method. multi - class labels FEATURE-OF learning multi - class classification. accuracy EVALUATE-FOR state of the art. accuracy EVALUATE-FOR method. OtherScientificTerm is class - specific labels. Method is meta classification learning. Generic are approach, and framework. ","This paper proposes a meta-learning method for multi-class classification. The proposed method is based on the idea of pairwise similarity prediction, which is used to learn a binary classifier for each class. The authors propose a probabilistic graphical model to learn the binary classifiers and a submodule for the meta-classifier. They show that the proposed method outperforms the state-of-the-art in terms of classification accuracy. ","This paper proposes a meta-learning method for multi-class classification. The proposed method is based on the idea of pairwise similarity prediction, which is used to learn a binary classifier for each class. The authors propose a probabilistic graphical model to learn the binary classifiers and a submodule for the meta-classifier. They show that the proposed method outperforms the state-of-the-art in terms of classification accuracy. "
6768,SP:c5c84ea1945b79b70521e0b73f762ad643175020,"visual scene USED-FOR interpretation of quantifier statements. inference mechanisms USED-FOR interpretation of quantifier statements. cognitive concepts USED-FOR strategies. deep learning models USED-FOR visual question answering. spatial arrangement of the scene HYPONYM-OF confounding factors. Task is psycholinguistics. Method are FiLM visual question answering model, and approximate number system. OtherScientificTerm is Weber ’s law. Generic is system. ","This paper studies the problem of visual question answering in the context of psycholinguistics. In particular, the authors consider the question answering problem where the questioner is given a set of quantifier statements, and the task is to answer the question in a way that maximizes the probability of answering the question given the answer. The authors propose an approximate number system (FiLM) that is based on the Weber’s law. The FiLM model is trained to predict the answer to the question, and is able to learn a number system that is more robust to confounding factors such as the spatial arrangement of the scene. ","This paper studies the problem of visual question answering in the context of psycholinguistics. In particular, the authors consider the question answering problem where the questioner is given a set of quantifier statements, and the task is to answer the question in a way that maximizes the probability of answering the question given the answer. The authors propose an approximate number system (FiLM) that is based on the Weber’s law. The FiLM model is trained to predict the answer to the question, and is able to learn a number system that is more robust to confounding factors such as the spatial arrangement of the scene. "
6772,SP:0fb732fe65ef1081b046a6aa6e1972e40cfdc247,"embedding models USED-FOR link prediction in relational knowledge graphs. relational facts PART-OF Knowledge graphs. Bayesian framework USED-FOR knowledge graphs. it USED-FOR gradient based optimization. divergences FEATURE-OF non - Bayesian treatment. gradient based optimization USED-FOR divergences. hyperparameters USED-FOR gradient based optimization. Models COMPARE state - of - the - art. state - of - the - art COMPARE Models. hyperparameters COMPARE state - of - the - art. state - of - the - art COMPARE hyperparameters. hyperparameters USED-FOR Models. Material is relational knowledge graphs. Task is small data problem. OtherScientificTerm is parameter uncertainty. Method are variational inference, and Bayesian approach. ",This paper proposes a Bayesian framework for link prediction in relational knowledge graphs. The proposed method is based on variational inference. The authors show that the proposed method outperforms the state-of-the-art in terms of the number of hyperparameters and the variance of the parameter uncertainty. They also show that their method can be applied to non-Bayesian optimization problems.,This paper proposes a Bayesian framework for link prediction in relational knowledge graphs. The proposed method is based on variational inference. The authors show that the proposed method outperforms the state-of-the-art in terms of the number of hyperparameters and the variance of the parameter uncertainty. They also show that their method can be applied to non-Bayesian optimization problems.
6776,SP:5ff0668b433a190d87d5833d8b2a8ca04daa299c,"dimension reduction USED-FOR visualization or prediction enhancement. online learning approaches USED-FOR supervised dimension reduction. algorithm USED-FOR supervised dimension reduction. sliced inverse regression ( SIR ) HYPONYM-OF algorithm. sliced inverse regression ( SIR ) USED-FOR algorithm. algorithm USED-FOR subspace of significant factors. intrinsic lower dimensionality FEATURE-OF subspace of significant factors. overlapping technique USED-FOR algorithm. real data applications EVALUATE-FOR algorithms. Method are Online learning, and incremental sliced inverse regression ( ISIR ). Task is online dimension reduction. Generic is it. ","This paper proposes an online learning algorithm for supervised dimension reduction. The proposed algorithm is based on sliced inverse regression (SIR), which is an extension of SIR to the online learning setting. The main contribution of the paper is to propose a new algorithm for online learning based on SIR. The paper also proposes an overlapping technique to reduce the dimensionality of the subspace of significant factors. Experiments show that the proposed algorithm outperforms the state-of-the-art online learning algorithms. ","This paper proposes an online learning algorithm for supervised dimension reduction. The proposed algorithm is based on sliced inverse regression (SIR), which is an extension of SIR to the online learning setting. The main contribution of the paper is to propose a new algorithm for online learning based on SIR. The paper also proposes an overlapping technique to reduce the dimensionality of the subspace of significant factors. Experiments show that the proposed algorithm outperforms the state-of-the-art online learning algorithms. "
6780,SP:4d5b993c6be6e55bdf98eca9a3b23a1bab5d2499,"models USED-FOR intra - modal and cross - modal interactions. models USED-FOR unexpected missing or noisy modalities. intra - modal and cross - modal interactions USED-FOR prediction. models USED-FOR prediction. multimodal data USED-FOR joint generative - discriminative objective. model USED-FOR representations. model USED-FOR independent factors. multimodal discriminative and modality - specific generative factors HYPONYM-OF independent factors. joint multimodal features USED-FOR discriminative tasks. Multimodal discriminative factors USED-FOR discriminative tasks. joint multimodal features PART-OF Multimodal discriminative factors. sentiment prediction HYPONYM-OF discriminative tasks. model USED-FOR multimodal representations. multimodal datasets EVALUATE-FOR multimodal representations. model USED-FOR missing modalities. factorized representations USED-FOR multimodal learning. Task is Learning multimodal representations. OtherScientificTerm are multiple modalities, and Modality - specific generative factors. ",This paper proposes a joint generative-discriminative objective for multi-modal learning. The joint objective is based on multimodal discriminative and modality-specific generative factors. The authors show that the joint objective can be used for missing modalities and noisy modalities.,This paper proposes a joint generative-discriminative objective for multi-modal learning. The joint objective is based on multimodal discriminative and modality-specific generative factors. The authors show that the joint objective can be used for missing modalities and noisy modalities.
6784,SP:cae76d3c3da91e50fe29cc3b6e204bb3e0793d7e,"shared conditional WaveNet core CONJUNCTION independent learned embeddings. independent learned embeddings CONJUNCTION shared conditional WaveNet core. independent learned embeddings USED-FOR multi - speaker model. shared conditional WaveNet core USED-FOR multi - speaker model. fixed weights USED-FOR neural network. stochastic gradient descent USED-FOR architecture. trained neural network encoder USED-FOR speaker embedding. sample naturalness CONJUNCTION voice similarity. voice similarity CONJUNCTION sample naturalness. approaches USED-FOR multi - speaker neural network. Method are meta - learning approach, and TTS system. Generic are network, and strategies. OtherScientificTerm is WaveNet core. Material is audio data. ","This paper proposes a meta-learning approach for multi-speaker neural networks. The proposed method is based on a shared conditional WaveNet core and independent learned embeddings. The authors show that the proposed method outperforms the baselines in terms of sample naturalness, sample similarity, and voice similarity. ","This paper proposes a meta-learning approach for multi-speaker neural networks. The proposed method is based on a shared conditional WaveNet core and independent learned embeddings. The authors show that the proposed method outperforms the baselines in terms of sample naturalness, sample similarity, and voice similarity. "
6788,SP:e80d6118fc3b9ff3195fea2f6adac88e59d350c2,"Robust estimation HYPONYM-OF statistics. Huber ’s -contamination model USED-FOR Robust estimation. Tukey ’s median CONJUNCTION estimators. estimators CONJUNCTION Tukey ’s median. estimators HYPONYM-OF Statistically optimal procedures. depth functions USED-FOR estimators. Tukey ’s median HYPONYM-OF Statistically optimal procedures. f -GANs CONJUNCTION depth functions. depth functions CONJUNCTION f -GANs. f -Learning USED-FOR depth functions. f -Learning USED-FOR f -GANs. depth functions USED-FOR statistically optimal robust estimators. total variation distance FEATURE-OF variational lower bounds. f -Learning USED-FOR variational lower bounds. variational lower bounds USED-FOR depth functions. GANs USED-FOR computing robust estimators. Gaussian distribution CONJUNCTION elliptical distributions. elliptical distributions CONJUNCTION Gaussian distribution. statistically optimal robust location estimators USED-FOR Gaussian distribution. statistically optimal robust location estimators USED-FOR elliptical distributions. hidden layers PART-OF GANs. discriminator networks PART-OF GANs. hidden layers PART-OF discriminator networks. OtherScientificTerm are computational intractability, and first moment. Method is f GANs. ","This paper studies the problem of finding robust estimators for deep neural networks (f-GANs). The authors propose to use the Huber’s-contamination model (Huber et al., 2017) to find the optimal depth function for f-GAN. The authors show that this is computationally intractable in the case of fGANs, but can be computed in the presence of a Gaussian distribution. They also show that f-Learning can be used to compute the robust estimator for the Gaussian and elliptical distributions.","This paper studies the problem of finding robust estimators for deep neural networks (f-GANs). The authors propose to use the Huber’s-contamination model (Huber et al., 2017) to find the optimal depth function for f-GAN. The authors show that this is computationally intractable in the case of fGANs, but can be computed in the presence of a Gaussian distribution. They also show that f-Learning can be used to compute the robust estimator for the Gaussian and elliptical distributions."
6792,SP:861c5336fda684e5bdd8a05f0af10dd442bf5339,"symmetry CONJUNCTION repetition. repetition CONJUNCTION symmetry. higher - level, abstract regularities PART-OF scene. repetition HYPONYM-OF higher - level, abstract regularities. symmetry HYPONYM-OF higher - level, abstract regularities. vision recognition modules CONJUNCTION scene representations. scene representations CONJUNCTION vision recognition modules. symbolic program USED-FOR scene programs. model USED-FOR scene programs. hierarchical, object - based scene representation USED-FOR model. hierarchical, object - based scene representation USED-FOR scene programs. compositional structure FEATURE-OF real images. synthetic data EVALUATE-FOR model. complex visual analogy - making CONJUNCTION scene extrapolation. scene extrapolation CONJUNCTION complex visual analogy - making. scene programs USED-FOR applications. scene extrapolation HYPONYM-OF applications. complex visual analogy - making HYPONYM-OF applications. Task is Human scene perception. ","This paper proposes a method for learning symbolic program for scene representation learning. The method is based on a hierarchical, object-based scene representation and a vision recognition module. The proposed method is evaluated on synthetic and real-world datasets. The results show that the proposed method outperforms state-of-the-art methods. ","This paper proposes a method for learning symbolic program for scene representation learning. The method is based on a hierarchical, object-based scene representation and a vision recognition module. The proposed method is evaluated on synthetic and real-world datasets. The results show that the proposed method outperforms state-of-the-art methods. "
6796,SP:a8df2aa6870a05f8580117f433e07e70a5342930,"long sequence data USED-FOR Recurrent neural networks. methods USED-FOR RNN state updates. memory FEATURE-OF network. methods PART-OF architectures. timing - gated LSTM RNN model USED-FOR reducing state updates. longer memory persistence CONJUNCTION error - gradient flow. error - gradient flow CONJUNCTION longer memory persistence. time gate USED-FOR longer memory persistence. model USED-FOR long temporal dependencies. model COMPARE LSTM. LSTM COMPARE model. non - optimal initialization USED-FOR time gate parameters. temporal curriculum learning schedule USED-FOR g - LSTM. computational budget term USED-FOR network. convergence time EVALUATE-FOR LSTM. computational budget term USED-FOR training loss. long sequences EVALUATE-FOR LSTM. Task are vanishing gradient problem, and network update. OtherScientificTerm are neuron, and neuron state. ","This paper studies the problem of learning a recurrent neural network with long sequence data. The authors propose a time-gated LSTM RNN model that can be used to reduce the number of updates in the training process. The proposed method is based on the idea of temporal curriculum learning, which is an extension of previous work on learning a curriculum learning schedule. The paper shows that the proposed method converges faster than existing methods in terms of convergence time. ","This paper studies the problem of learning a recurrent neural network with long sequence data. The authors propose a time-gated LSTM RNN model that can be used to reduce the number of updates in the training process. The proposed method is based on the idea of temporal curriculum learning, which is an extension of previous work on learning a curriculum learning schedule. The paper shows that the proposed method converges faster than existing methods in terms of convergence time. "
6800,SP:e39bcc2ee6db054f0f1d8e8d04291a78488886ae,"ensembling USED-FOR first and second order deep feature statistics. re - training CONJUNCTION pre - processing. pre - processing CONJUNCTION re - training. pre - processing CONJUNCTION model. model CONJUNCTION pre - processing. method COMPARE state - of - the - art. state - of - the - art COMPARE method. benchmarking tasks EVALUATE-FOR state - of - the - art. benchmarking tasks EVALUATE-FOR method. TinyImageNet resize FEATURE-OF out - of - distribution dataset. true negative rate EVALUATE-FOR method. DenseNet USED-FOR method. DenseNet USED-FOR in - distribution ( CIFAR-100 ). Method are deep neural networks, and plug - and - play detection procedure. OtherScientificTerm is feature maps. ",This paper proposes a plug-and-play detection method for out-of-distribution (OOD) detection. The proposed method is based on the idea of embedding the feature maps of a deep neural network into a single image. The authors show that the proposed method can detect the true negative rate of the out of distribution dataset. The method is evaluated on CIFAR-100 and TinyImageNet.,This paper proposes a plug-and-play detection method for out-of-distribution (OOD) detection. The proposed method is based on the idea of embedding the feature maps of a deep neural network into a single image. The authors show that the proposed method can detect the true negative rate of the out of distribution dataset. The method is evaluated on CIFAR-100 and TinyImageNet.
6804,SP:827f95cdefae78e38a9c4b5718fcf294606a1989,model recovery USED-FOR data classification. neural network USED-FOR weight vectors. Gaussian inputs USED-FOR empirical risk function. cross entropy USED-FOR empirical risk function. gradient descent USED-FOR one - hidden - layer neural networks. near - optimal sample CONJUNCTION computational complexity. computational complexity CONJUNCTION near - optimal sample. global convergence guarantee USED-FOR empirical risk minimization. computational complexity EVALUATE-FOR network input dimension. gradient descent USED-FOR cross entropy. cross entropy USED-FOR global convergence guarantee. cross entropy USED-FOR empirical risk minimization. gradient descent USED-FOR global convergence guarantee. gradient descent USED-FOR empirical risk minimization. OtherScientificTerm is sigmoid activations. Metric is sample complexity. Method is tensor method. ,This paper studies the problem of model recovery for one-hidden-layer neural networks. The authors consider the case where the input dimension of the network is Gaussian and the weights of the weights are Gaussian. They prove a global convergence guarantee for the empirical risk minimization of the model recovery problem. They show that the global convergence of the risk minimizer is guaranteed by the cross-entropy of the loss function. They also show that gradient descent can be used to obtain a near-optimal sample. ,This paper studies the problem of model recovery for one-hidden-layer neural networks. The authors consider the case where the input dimension of the network is Gaussian and the weights of the weights are Gaussian. They prove a global convergence guarantee for the empirical risk minimization of the model recovery problem. They show that the global convergence of the risk minimizer is guaranteed by the cross-entropy of the loss function. They also show that gradient descent can be used to obtain a near-optimal sample. 
6808,SP:2b4a39b997934ccf0e6b5fcb4d1e62253592b05f,"method USED-FOR salient convolutional channels. convolutional layers USED-FOR features. small auxiliary connections PART-OF convolutional layers. small auxiliary connections USED-FOR FBS. it USED-FOR convolution. channel pruning methods COMPARE it. it COMPARE channel pruning methods. it USED-FOR full network structures. it USED-FOR CNNs. stochastic gradient descent USED-FOR FBS - augmented networks. channel pruning CONJUNCTION dynamic execution schemes. dynamic execution schemes CONJUNCTION channel pruning. FBS COMPARE channel pruning. channel pruning COMPARE FBS. FBS COMPARE dynamic execution schemes. dynamic execution schemes COMPARE FBS. ImageNet classification EVALUATE-FOR FBS. VGG-16 CONJUNCTION ResNet-18. ResNet-18 CONJUNCTION VGG-16. VGG-16 EVALUATE-FOR FBS. ResNet-18 EVALUATE-FOR FBS. Method are deep convolutional neural networks, and feature boosting and suppression ( FBS ). Metric are computational and memory resources, and top-5 accuracy loss. OtherScientificTerm is channels. ","This paper proposes feature boosting and suppression (FBS), which augments the convolutional layers of deep neural networks with small auxiliary connections to reduce the computational and memory costs. The authors show that the proposed method can be used to improve the performance of CNNs. The proposed method is evaluated on ImageNet classification and ResNet.","This paper proposes feature boosting and suppression (FBS), which augments the convolutional layers of deep neural networks with small auxiliary connections to reduce the computational and memory costs. The authors show that the proposed method can be used to improve the performance of CNNs. The proposed method is evaluated on ImageNet classification and ResNet."
6812,SP:2b1813a3cc39d6e1eba546b456bf8d1f9cc8657c,mixed Nash Equilibria ( NE ) perspective FEATURE-OF Generative Adversarial Networks ( GANs ). algorithmic framework USED-FOR GANs. prox methods USED-FOR algorithmic framework. infinite - dimensional two - player game USED-FOR algorithmic framework. infinite - dimensional two - player game USED-FOR GANs. procedure USED-FOR prox methods. sampling routines USED-FOR prox methods. approach COMPARE methods. methods COMPARE approach. Adam CONJUNCTION RMSProp. RMSProp CONJUNCTION Adam. SGD CONJUNCTION Adam. Adam CONJUNCTION SGD. methods USED-FOR pure strategy equilibria. SGD HYPONYM-OF pure strategy equilibria. quality EVALUATE-FOR approach. OtherScientificTerm is mixed NE. ,"This paper studies the problem of optimizing the mixed Nash Equilibria (NE) of GANs from an algorithmic perspective. The authors propose a new algorithm to solve the problem. The proposed algorithm is based on a two-player game, where the goal is to maximize the Nash equilibria of the two players. The paper also proposes a sampling procedure to sample from the mixed NE. Experiments show that the proposed algorithm outperforms existing prox methods in terms of performance. ","This paper studies the problem of optimizing the mixed Nash Equilibria (NE) of GANs from an algorithmic perspective. The authors propose a new algorithm to solve the problem. The proposed algorithm is based on a two-player game, where the goal is to maximize the Nash equilibria of the two players. The paper also proposes a sampling procedure to sample from the mixed NE. Experiments show that the proposed algorithm outperforms existing prox methods in terms of performance. "
6816,SP:79ece684e3c4aca516b4ec41aa8fcb7d86449784,"deep neural networks USED-FOR method. approach USED-FOR model. pretrained network USED-FOR problems. depth - wise convolutions HYPONYM-OF low - parameter layers. approach USED-FOR sequential transfer learning. multi - task learning problems EVALUATE-FOR logits - only fine - tuning. Generic are task, and network. Method are Single Shot MultiBox Detection ( SSD ) model, 1000 - class image classification model, and SSD feature extractor. Metric is transfer - learning accuracy. OtherScientificTerm is singletask. ","This paper proposes a method for sequential transfer learning for multi-task learning. The proposed method is based on the idea of depth-wise convolutions, which can be applied to low-parameter layers of deep neural networks. The authors show that the proposed method achieves better transfer learning performance compared to logits-only fine-tuning. ","This paper proposes a method for sequential transfer learning for multi-task learning. The proposed method is based on the idea of depth-wise convolutions, which can be applied to low-parameter layers of deep neural networks. The authors show that the proposed method achieves better transfer learning performance compared to logits-only fine-tuning. "
6820,SP:82b8270b33110e50b5914246f3ca75d3bdbffb6e,"Normalization methods PART-OF deep learning toolbox. batch normalization ( BN ) HYPONYM-OF normalization method. method COMPARE BN. BN COMPARE method. method COMPARE normalization techniques. normalization techniques COMPARE method. BN CONJUNCTION normalization techniques. normalization techniques CONJUNCTION BN. single and multi - task datasets EVALUATE-FOR method. Generic are They, and approach. OtherScientificTerm are manually tuned learning rate schedules, and multi - modal distributions. Method is normalization. ","This paper proposes a new normalization method for batch normalization. The proposed method is based on batch normalisation (BN), which is a popular normalization technique for multi-modal distributions. The authors show that the proposed method outperforms BN on both single-task and multi-task datasets. They also show that their method can be combined with BN to improve the performance of BN. ","This paper proposes a new normalization method for batch normalization. The proposed method is based on batch normalisation (BN), which is a popular normalization technique for multi-modal distributions. The authors show that the proposed method outperforms BN on both single-task and multi-task datasets. They also show that their method can be combined with BN to improve the performance of BN. "
6824,SP:034c3bc2b2fe4991f56f168ea7b4b552c500b9ad,"pruning technique USED-FOR subnetworks. test accuracy EVALUATE-FOR network. OtherScientificTerm are lottery ticket hypothesis, winning tickets, and initialization lottery. Task is training. ","This paper studies the lottery ticket hypothesis, which is a well-studied topic in machine learning. The authors propose a pruning technique for training deep neural networks. The pruning is based on the idea of lottery tickets, and the authors show that the pruning can be used to improve the test accuracy of a neural network. ","This paper studies the lottery ticket hypothesis, which is a well-studied topic in machine learning. The authors propose a pruning technique for training deep neural networks. The pruning is based on the idea of lottery tickets, and the authors show that the pruning can be used to improve the test accuracy of a neural network. "
6828,SP:08c662296c7cf346f027e462d29184275fd6a102,"exploration FEATURE-OF reinforcement learning. attentive dynamics model ( ADM ) USED-FOR controllable elements of the observations. state representation USED-FOR exploration purposes. contingency information USED-FOR state representation. contingency information USED-FOR exploration purposes. high - level information CONJUNCTION supervisory data. supervisory data CONJUNCTION high - level information. actor - critic algorithm CONJUNCTION count - based exploration. count - based exploration CONJUNCTION actor - critic algorithm. expert demonstrations CONJUNCTION high - level information. high - level information CONJUNCTION expert demonstrations. representation USED-FOR count - based exploration. RAM states HYPONYM-OF high - level information. representation USED-FOR actor - critic algorithm. contingency - awareness USED-FOR exploration problems. exploration problems PART-OF reinforcement learning. contingency - awareness USED-FOR reinforcement learning. Method are Arcade Learning Element ( ALE ), and ADM. Material are Atari games, and MONTEZUMA ’S REVENGE. ","This paper proposes a method for learning a state representation for exploration in reinforcement learning. The proposed method is based on the attentive dynamics model (ADM) which is used to model the dynamics of the environment. The authors show that the ADM can be used to learn a representation of the state representation of an agent, which can then be used for exploration. The method is evaluated on the Atari games MONTEZUMA and REVENGE. ","This paper proposes a method for learning a state representation for exploration in reinforcement learning. The proposed method is based on the attentive dynamics model (ADM) which is used to model the dynamics of the environment. The authors show that the ADM can be used to learn a representation of the state representation of an agent, which can then be used for exploration. The method is evaluated on the Atari games MONTEZUMA and REVENGE. "
6832,SP:614f742a75039b1509343d53e0fb4a6d4088ab3e,"HyperGAN HYPONYM-OF generative network. generative network USED-FOR weight parameters. weight parameters PART-OF deep neural networks. generative network USED-FOR deep neural networks. HyperGAN USED-FOR latent space. HyperGAN USED-FOR low dimensional noise. architecture COMPARE generative adversarial networks. generative adversarial networks COMPARE architecture. generated network parameter distribution CONJUNCTION unknown true parameter distribution. unknown true parameter distribution CONJUNCTION generated network parameter distribution. KL - divergence FEATURE-OF generated network parameter distribution. KL - divergence FEATURE-OF unknown true parameter distribution. HyperGAN USED-FOR classification. HyperGAN COMPARE fully supervised learning. fully supervised learning COMPARE HyperGAN. HyperGAN USED-FOR MNIST and CIFAR-10 datasets. HyperGAN COMPARE ensembles. ensembles COMPARE HyperGAN. HyperGAN USED-FOR uncertainty. uncertainty EVALUATE-FOR ensembles. HyperGAN - generated ensembles USED-FOR out of distribution data. out of distribution data CONJUNCTION adversarial examples. adversarial examples CONJUNCTION out of distribution data. HyperGAN - generated ensembles USED-FOR adversarial examples. HyperGAN USED-FOR uncertainty estimates. OtherScientificTerm are classification loss, and rich distribution of effective parameters. Material is inlier data. ","This paper proposes HyperGAN, a generative adversarial network (GAN) architecture for low dimensional noise. HyperGAN is based on the idea of generative generative networks (GNNs), which can be seen as an extension of GANs that can be applied to low dimensional data. The authors show that HyperGAN can be used to generate adversarial examples for out-of-distribution (OOD) and out of distribution (OOD) data. They also show that hyperGAN is able to estimate the uncertainty of the classifier in the latent space, which is an important contribution of the paper. ","This paper proposes HyperGAN, a generative adversarial network (GAN) architecture for low dimensional noise. HyperGAN is based on the idea of generative generative networks (GNNs), which can be seen as an extension of GANs that can be applied to low dimensional data. The authors show that HyperGAN can be used to generate adversarial examples for out-of-distribution (OOD) and out of distribution (OOD) data. They also show that hyperGAN is able to estimate the uncertainty of the classifier in the latent space, which is an important contribution of the paper. "
6836,SP:230b3e008e687e03a8b914084b93fc81609051c0,Variational Auto Encoder ( VAE ) HYPONYM-OF generative latent variable model. generative latent variable model USED-FOR representation learning. Variational Auto Encoder ( VAE ) USED-FOR representation learning. continuous valued latent variables USED-FOR VAEs. differentiable estimate USED-FOR ELBO. reparametrized sampling USED-FOR differentiable estimate. Stochastic Gradient Descend ( SGD ) USED-FOR it. discrete valued latent variables USED-FOR VAEs. binary or categorically valued latent representations USED-FOR VAEs. differentiable estimator USED-FOR ELBO. importance sampling USED-FOR differentiable estimator. benchmark datasets EVALUATE-FOR VAEs architectures. Bernoulli and Categorically distributed latent representations USED-FOR VAEs architectures. variational auto encoder ( VAE ) HYPONYM-OF generative model. it HYPONYM-OF generative model. It USED-FOR model. VAE USED-FOR tasks. data generation CONJUNCTION data interpolation. data interpolation CONJUNCTION data generation. density estimation CONJUNCTION data generation. data generation CONJUNCTION density estimation. data interpolation CONJUNCTION outlier and anomaly detection. outlier and anomaly detection CONJUNCTION data interpolation. VAE USED-FOR density estimation. outlier and anomaly detection CONJUNCTION clustering. clustering CONJUNCTION outlier and anomaly detection. VAE USED-FOR data generation. VAE USED-FOR outlier and anomaly detection. density estimation HYPONYM-OF tasks. data interpolation HYPONYM-OF tasks. clustering HYPONYM-OF tasks. outlier and anomaly detection HYPONYM-OF tasks. data generation HYPONYM-OF tasks. VAE HYPONYM-OF latent variable model. Generic is approach. Method is VARIATIONAL AUTO ENCODER. OtherScientificTerm is nonlinear dependent elements. ,"This paper proposes a variational auto-encoder (VAE) model for representation learning. The proposed model is based on the idea of ELBO, which is a generative model with discrete valued latent variables. The key idea is to learn a differentiable estimator of the ELBO using importance sampling and reparametrized sampling. Experiments show that the proposed model outperforms existing VAE models on several benchmark datasets.","This paper proposes a variational auto-encoder (VAE) model for representation learning. The proposed model is based on the idea of ELBO, which is a generative model with discrete valued latent variables. The key idea is to learn a differentiable estimator of the ELBO using importance sampling and reparametrized sampling. Experiments show that the proposed model outperforms existing VAE models on several benchmark datasets."
6840,SP:153fe1172e689b345729c0c848cfb38bdae0e5f7,feed forward neural network COMPARE training approaches. training approaches COMPARE feed forward neural network. robustness EVALUATE-FOR training approaches. robustness EVALUATE-FOR feed forward neural network. mean field description of a Boltzmann machine USED-FOR pre - trained building block. MNIST dataset EVALUATE-FOR method. data augmentation CONJUNCTION adversarial training. adversarial training CONJUNCTION data augmentation. adversarial resistance EVALUATE-FOR method. OtherScientificTerm is adversarial attacks. Method is Boltzmann machine. ,This paper proposes a method for training a Boltzmann machine that is robust to adversarial attacks. The main idea is to use the mean field description as a pre-trained building block to train a neural network. The method is evaluated on MNIST and CIFAR-10. The authors show that the proposed method outperforms the baselines in terms of robustness and robustness to data augmentation.,This paper proposes a method for training a Boltzmann machine that is robust to adversarial attacks. The main idea is to use the mean field description as a pre-trained building block to train a neural network. The method is evaluated on MNIST and CIFAR-10. The authors show that the proposed method outperforms the baselines in terms of robustness and robustness to data augmentation.
6844,SP:40ade446aa4a700cb1519b9115e8d6cdf33db4a4,"visible region PART-OF minimal image. deep neural networks ( DNNs ) COMPARE DNNs. DNNs COMPARE deep neural networks ( DNNs ). object location FEATURE-OF DNNs. DNN recognition ability FEATURE-OF natural images. robustness EVALUATE-FOR DNNs. natural images FEATURE-OF DNNs. Material is Minimal images. Metric are human recognition accuracy, and accuracy. OtherScientificTerm are invariance, and adversarial patterns. ",This paper studies the robustness of DNNs against adversarial attacks on natural images. The authors show that DNN-based models are more robust to adversarial perturbations in the visible region of a minimal image. They also show that the adversarial patterns are invariant to changes in the location of the hidden region of the image. ,This paper studies the robustness of DNNs against adversarial attacks on natural images. The authors show that DNN-based models are more robust to adversarial perturbations in the visible region of a minimal image. They also show that the adversarial patterns are invariant to changes in the location of the hidden region of the image. 
6848,SP:8ab0bb3eb38958d607fe6b6ebbd921b8abdf149d,"multi - agent reinforcement learning ( MARL ) USED-FOR optimal collaboration. worker agents HYPONYM-OF self - interested agents. super agent USED-FOR them. super agent USED-FOR optimal coordination. manager HYPONYM-OF super agent. agent modeling CONJUNCTION policy learning. policy learning CONJUNCTION agent modeling. approach USED-FOR multi - agent management problems. environments EVALUATE-FOR approach. Resource Collection and Crafting EVALUATE-FOR approach. Resource Collection and Crafting HYPONYM-OF environments. approach USED-FOR optimal ad - hoc teaming. approach USED-FOR worker agents ’ minds. generalization FEATURE-OF optimal ad - hoc teaming. OtherScientificTerm are policy, and contracts. Generic is agents. Task is ad - hoc worker teaming. ","This paper proposes a multi-agent reinforcement learning (MARL) framework for ad-hoc worker teaming. The proposed framework is based on the idea of a super-agent, which is a combination of a manager and a worker agent. The super agent is trained to learn a policy that maximizes the mutual information between the worker agent and the manager. The manager is trained on the policy learned by the super agent. Experiments show that the proposed framework outperforms the baselines in a number of environments.","This paper proposes a multi-agent reinforcement learning (MARL) framework for ad-hoc worker teaming. The proposed framework is based on the idea of a super-agent, which is a combination of a manager and a worker agent. The super agent is trained to learn a policy that maximizes the mutual information between the worker agent and the manager. The manager is trained on the policy learned by the super agent. Experiments show that the proposed framework outperforms the baselines in a number of environments."
6852,SP:50a5e5227932ff1196706f53fb82f1785da45e2a,"time series USED-FOR recurrent neural network ( RNN ) solutions. asynchronous time series HYPONYM-OF series. unified RNN USED-FOR feature types. RNN framework USED-FOR sequential features. time features USED-FOR cell ’s memory state. sequential level FEATURE-OF time features. Method are RNN cells, and modeling framework. OtherScientificTerm are sparse and dense features, static ( whole sequence level ) features, and encoder output. Task is cell updates. ","This paper proposes a unified RNN framework for asynchronous time series. The proposed framework is based on the idea that sequential features of time series can be represented by a single RNN cell. The authors show that the sequential features can be expressed as a combination of sparse and dense features, and the encoder output can be decomposed into static (total sequence level) and dynamic (total sequential level) features. The paper also shows that the proposed framework can be used to model the memory state of RNN cells. ","This paper proposes a unified RNN framework for asynchronous time series. The proposed framework is based on the idea that sequential features of time series can be represented by a single RNN cell. The authors show that the sequential features can be expressed as a combination of sparse and dense features, and the encoder output can be decomposed into static (total sequence level) and dynamic (total sequential level) features. The paper also shows that the proposed framework can be used to model the memory state of RNN cells. "
6856,SP:f2c3dd2b485d6307847c759a5609b7ebe24b7058,"Method are neural model, feedforward neural network, and recurrent neural networks. OtherScientificTerm are propositional formula, and propositional atoms. Generic are network, and model. ","This paper studies the problem of learning a propositional formula for a sequence of atoms in a neural network. The authors propose to use a feedforward neural network to solve the problem. The proposed method is based on the idea that the propositional formulas can be represented as a set of propositional atoms, which can then be represented by a recurrent neural network (RNN). The authors show that the proposed method can be used to train a recurrent RNN on the propositions.","This paper studies the problem of learning a propositional formula for a sequence of atoms in a neural network. The authors propose to use a feedforward neural network to solve the problem. The proposed method is based on the idea that the propositional formulas can be represented as a set of propositional atoms, which can then be represented by a recurrent neural network (RNN). The authors show that the proposed method can be used to train a recurrent RNN on the propositions."
6860,SP:845ae21e5758a8aabfa610c291fdcc5f61af7748,random mini - batches USED-FOR Training neural networks. accuracy EVALUATE-FOR network. speed of learning EVALUATE-FOR network. difficulty FEATURE-OF mini - batches. curriculum learning USED-FOR problem. CNNs USED-FOR image recognition. CIFAR-10 CONJUNCTION CIFAR-100 datasets. CIFAR-100 datasets CONJUNCTION CIFAR-10. performance EVALUATE-FOR small and competitive networks. learning speed EVALUATE-FOR small and competitive networks. competitive ” teacher ” network USED-FOR transfer learning. Imagenet database USED-FOR competitive ” teacher ” network. CIFAR-100 datasets EVALUATE-FOR small and competitive networks. CIFAR-10 EVALUATE-FOR small and competitive networks. transfer learning USED-FOR difficulty. approach COMPARE Self - Paced Learning. Self - Paced Learning COMPARE approach. Metric is difficulty measure. OtherScientificTerm is ” teacher ” network. Generic is method. ,"This paper proposes a method to improve the performance of mini-batch learning in training neural networks. The method is based on curriculum learning, where a teacher network is used to train a student network and the student network is trained to match the teacher network's performance. The teacher network learns a difficulty measure that measures the difficulty of the task at hand. The student network learns the difficulty measure based on the teacher’s performance on the task. The paper shows that the proposed method outperforms self-paced learning on CIFAR-10 and Imagenet datasets. ","This paper proposes a method to improve the performance of mini-batch learning in training neural networks. The method is based on curriculum learning, where a teacher network is used to train a student network and the student network is trained to match the teacher network's performance. The teacher network learns a difficulty measure that measures the difficulty of the task at hand. The student network learns the difficulty measure based on the teacher’s performance on the task. The paper shows that the proposed method outperforms self-paced learning on CIFAR-10 and Imagenet datasets. "
6864,SP:b33a6a1fe4bbae422ba001cbe656f31d07a62025,"generalization guarantees FEATURE-OF neural networks. pre - activation values FEATURE-OF network. Method are overparameterized deep networks, stochastic gradient descent ( SGD ), and PAC - Bayesian framework. OtherScientificTerm are small random noise, weight matrices, wide training loss minimum, and wide test loss minimum. Generic are approach, framework, matrices, and prior approaches. Metric is generalization guarantee. ","This paper studies the generalization properties of overparameterized deep neural networks. The authors propose a PAC-Bayesian framework for overparametrized deep networks, which is based on the idea that the training loss of a neural network is a function of the weight matrices of the weights of the network. They show that under certain assumptions on the weights, they can obtain a generalization guarantee that is close to the test loss. They also show that this generalization guarantees can be extended to the case where the weights are randomly sampled from the training set. ","This paper studies the generalization properties of overparameterized deep neural networks. The authors propose a PAC-Bayesian framework for overparametrized deep networks, which is based on the idea that the training loss of a neural network is a function of the weight matrices of the weights of the network. They show that under certain assumptions on the weights, they can obtain a generalization guarantee that is close to the test loss. They also show that this generalization guarantees can be extended to the case where the weights are randomly sampled from the training set. "
6868,SP:d0533cb69d938d4128d17b1a6d8aeb8d1ca6e3fd,"Deep neural networks USED-FOR symbolic reasoning. Deep neural networks USED-FOR learning abstractions. symbolic reasoning CONJUNCTION learning abstractions. learning abstractions CONJUNCTION symbolic reasoning. discrete latent variables FEATURE-OF Deep neural networks. perplexity EVALUATE-FOR VAE. CIFAR-10 EVALUATE-FOR VAE. CIFAR-10 HYPONYM-OF datasets. datasets EVALUATE-FOR VAE. training technique USED-FOR VQ - VAE. it CONJUNCTION sequence level knowledge distillation. sequence level knowledge distillation CONJUNCTION it. nonautoregressive machine translation model COMPARE greedy autoregressive baseline Transformer. greedy autoregressive baseline Transformer COMPARE nonautoregressive machine translation model. accuracy EVALUATE-FOR greedy autoregressive baseline Transformer. it USED-FOR nonautoregressive machine translation model. sequence level knowledge distillation USED-FOR nonautoregressive machine translation model. accuracy EVALUATE-FOR nonautoregressive machine translation model. EM USED-FOR discrete autoencoder. Method are discrete latent variable models, vector quantized autoencoders ( VQ - VAE ), and Expectation Maximization ( EM ) algorithm. Task is inference. ","This paper proposes a novel approach for learning discrete latent variables in discrete latent variable models. The authors propose a new training technique called Expectation Maximization (EM) to improve the performance of VQ-VAE, which is a VAE-based model for learning abstractions. They show that the proposed method outperforms greedy autoregressive baseline Transformer and non-autoregressive machine translation models on CIFAR-10, CIFARS-100, and ImageNet. They also show that EM can be applied to learn abstractions in the context of machine translation.","This paper proposes a novel approach for learning discrete latent variables in discrete latent variable models. The authors propose a new training technique called Expectation Maximization (EM) to improve the performance of VQ-VAE, which is a VAE-based model for learning abstractions. They show that the proposed method outperforms greedy autoregressive baseline Transformer and non-autoregressive machine translation models on CIFAR-10, CIFARS-100, and ImageNet. They also show that EM can be applied to learn abstractions in the context of machine translation."
6872,SP:60628f7db9cfcac3f0dbe6ce0b2a161310525ba0,"Multi - view learning USED-FOR self - supervision. Distributional hypothesis USED-FOR self - supervision. multi - view frameworks USED-FOR sentence representations. learning architectures USED-FOR sentence meaning. generative objective CONJUNCTION discriminative one. discriminative one CONJUNCTION generative objective. generative objective USED-FOR framework. multi - view frameworks COMPARE single - view learnt counterparts. single - view learnt counterparts COMPARE multi - view frameworks. multi - view frameworks USED-FOR representations. single - view learnt counterparts USED-FOR representations. Material is large unlabelled corpora. Generic are frameworks, and representation. Method are Recurrent Neural Network ( RNN ), and linear model. Task is downstream tasks. ","This paper studies the problem of self-supervision in multi-view learning. The authors propose a new framework, Multi-View Learning (MLL), which is based on the Distributional Hypothesis (DDH). The authors show that MLL is able to learn representations that are better than the single-view counterparts. They also show that the proposed MLL can be used for downstream tasks. ","This paper studies the problem of self-supervision in multi-view learning. The authors propose a new framework, Multi-View Learning (MLL), which is based on the Distributional Hypothesis (DDH). The authors show that MLL is able to learn representations that are better than the single-view counterparts. They also show that the proposed MLL can be used for downstream tasks. "
6876,SP:f5da908b5f6c19a059d2447b9cda15af5e12dc55,"Distributed optimization USED-FOR large - scale machine learning problems. stragglers HYPONYM-OF slow nodes. Anytime Minibatch HYPONYM-OF online distributed optimization method. fixed communication time FEATURE-OF minibatch gradients. consensus USED-FOR minibatch gradients. dual averaging USED-FOR primal variables. approach COMPARE it. it COMPARE approach. Amazon EC2 EVALUATE-FOR approach. Method are distributed optimization techniques, and convergence analysis. OtherScientificTerm are gradients, and compute node performance. Metric is wall time. ",This paper proposes an online distributed optimization method called Anytime Minibatch (OMB) for minibatch gradients. The proposed method is based on the dual averaging of primal variables. The authors show that the proposed method converges to a fixed communication time in the case of stragglers. The paper also provides theoretical analysis of the convergence of the method.,This paper proposes an online distributed optimization method called Anytime Minibatch (OMB) for minibatch gradients. The proposed method is based on the dual averaging of primal variables. The authors show that the proposed method converges to a fixed communication time in the case of stragglers. The paper also provides theoretical analysis of the convergence of the method.
6880,SP:f167ad4bb1e140f692ec71c8baf0a59bff7bbc6f,"approach USED-FOR reinforcement learning. task - independent intrinsic reward function USED-FOR approach. peripheral pulse measurements USED-FOR task - independent intrinsic reward function. reward functions USED-FOR sparse and skewed rewards. reward functions USED-FOR reinforcement learning settings. reward functions USED-FOR sample efficiency. sparse and skewed rewards FEATURE-OF reinforcement learning settings. it USED-FOR learning. simulated driving environment EVALUATE-FOR this. OtherScientificTerm are intrinsic feedback, Physiological changes, biological preparations, and human autonomic nervous system responses. Task is learning stage. ","This paper proposes a novel intrinsic reward function based on peripheral pulse measurements to improve sample efficiency in reinforcement learning. The proposed method is based on the idea of intrinsic feedback, which can be used to improve the sample efficiency of reinforcement learning in sparse and skewed reward settings. The method is evaluated on a simulated driving environment and shows that the proposed method outperforms baselines. ","This paper proposes a novel intrinsic reward function based on peripheral pulse measurements to improve sample efficiency in reinforcement learning. The proposed method is based on the idea of intrinsic feedback, which can be used to improve the sample efficiency of reinforcement learning in sparse and skewed reward settings. The method is evaluated on a simulated driving environment and shows that the proposed method outperforms baselines. "
6884,SP:2db0ece25ebfb4d5e3aa8eb145964ce4be19409f,"they USED-FOR predictive distributions. NPs USED-FOR conditional distributions. NPs USED-FOR observed data. attention PART-OF NPs. Method is Neural Processes ( NPs ). OtherScientificTerm are regression functions, and functions. Metric is linear complexity. Task is underfitting. Generic is this. ","This paper studies the problem of underfitting in neural processes (NP). Specifically, the authors consider the case where the model is trained to predict the conditional distribution of the observed data. The authors show that underfitting can be reduced to a linear complexity problem, which can be solved by minimizing the sum of the linear complexity of the model and the model's attention. They also show that this can be achieved by using attention in NPs. ","This paper studies the problem of underfitting in neural processes (NP). Specifically, the authors consider the case where the model is trained to predict the conditional distribution of the observed data. The authors show that underfitting can be reduced to a linear complexity problem, which can be solved by minimizing the sum of the linear complexity of the model and the model's attention. They also show that this can be achieved by using attention in NPs. "
6888,SP:26535b26a3178050d8aae56b7c9669c9d2408ac8,credit assignment USED-FOR pre - adaptation behavior. sample - efficiency EVALUATE-FOR metatraining. credit assignment USED-FOR gradient - based Meta - RL. meta - learning algorithm USED-FOR estimating meta - policy gradients. meta - learning algorithm USED-FOR poor credit assignment. algorithm USED-FOR meta - learning. statistical distance FEATURE-OF pre - adaptation and adapted policies. pre - adaptation and adapted policies USED-FOR meta - policy search. approach COMPARE Meta - RL algorithms. Meta - RL algorithms COMPARE approach. approach USED-FOR pre - adaptation policy behavior. wall - clock time CONJUNCTION asymptotic performance. asymptotic performance CONJUNCTION wall - clock time. sample - efficiency CONJUNCTION wall - clock time. wall - clock time CONJUNCTION sample - efficiency. asymptotic performance EVALUATE-FOR Meta - RL algorithms. wall - clock time EVALUATE-FOR Meta - RL algorithms. asymptotic performance EVALUATE-FOR approach. wall - clock time EVALUATE-FOR approach. sample - efficiency EVALUATE-FOR Meta - RL algorithms. sample - efficiency EVALUATE-FOR approach. Task is Credit assignment. Generic is it. Method is task identification strategies. OtherScientificTerm is meta - policy gradients. ,"This paper proposes a meta-learning algorithm for credit assignment in meta-RL. The proposed method is based on gradient-based Meta-RL, where the goal is to estimate the gradients of pre-adapted and adapted policies. The authors show that the proposed method outperforms the baselines in terms of asymptotic performance, sample efficiency, and wall-clock time. ","This paper proposes a meta-learning algorithm for credit assignment in meta-RL. The proposed method is based on gradient-based Meta-RL, where the goal is to estimate the gradients of pre-adapted and adapted policies. The authors show that the proposed method outperforms the baselines in terms of asymptotic performance, sample efficiency, and wall-clock time. "
6892,SP:be5f2c827605914206f5645087b94a50f59f9214,"classifier USED-FOR satisfiability. NeuroSAT HYPONYM-OF message passing neural network. message passing neural network USED-FOR SAT problems. it COMPARE SAT solvers. SAT solvers COMPARE it. NeuroSAT USED-FOR problems. SAT solvers COMPARE NeuroSAT. NeuroSAT COMPARE SAT solvers. it COMPARE NeuroSAT. NeuroSAT COMPARE it. clique detection CONJUNCTION dominating set. dominating set CONJUNCTION clique detection. graph coloring CONJUNCTION clique detection. clique detection CONJUNCTION graph coloring. it USED-FOR SAT problems. dominating set CONJUNCTION vertex cover problems. vertex cover problems CONJUNCTION dominating set. NeuroSAT USED-FOR distributions. graph coloring HYPONYM-OF SAT problems. clique detection HYPONYM-OF SAT problems. OtherScientificTerm are random SAT problems, and small random graphs. ","This paper proposes a message passing neural network for solving SAT problems. The proposed method, NeuroSAT, is based on the message passing network (MNN) and is able to solve SAT problems on small random graphs. The authors show that the proposed method outperforms a number of state-of-the-art SAT solvers on a variety of SAT problems, including clique detection, dominating set, and vertex cover problems.","This paper proposes a message passing neural network for solving SAT problems. The proposed method, NeuroSAT, is based on the message passing network (MNN) and is able to solve SAT problems on small random graphs. The authors show that the proposed method outperforms a number of state-of-the-art SAT solvers on a variety of SAT problems, including clique detection, dominating set, and vertex cover problems."
6896,SP:a99fddee87b684b2783ef3a21f8c15c19631953b,policy USED-FOR autonomous driving. imitation learning USED-FOR policy. behavior cloning USED-FOR complex driving scenarios. perception system CONJUNCTION controller. controller CONJUNCTION perception system. perturbations FEATURE-OF expert ’s driving. synthesized data USED-FOR learner. perturbations FEATURE-OF synthesized data. robustness EVALUATE-FOR model. losses USED-FOR imitation loss. causal factors FEATURE-OF model. OtherScientificTerm is collisions. ,"This paper proposes a method for imitation learning for autonomous driving. The method is based on the idea of behavior cloning, where a learner learns to imitate the behavior of an expert in order to improve the performance of the learner. The key idea of the method is to learn a loss function that captures the causal factors of the expert’s driving, and then use this loss function to learn an imitation loss function. The paper shows that the proposed method is able to achieve better performance than baselines on a variety of driving tasks.","This paper proposes a method for imitation learning for autonomous driving. The method is based on the idea of behavior cloning, where a learner learns to imitate the behavior of an expert in order to improve the performance of the learner. The key idea of the method is to learn a loss function that captures the causal factors of the expert’s driving, and then use this loss function to learn an imitation loss function. The paper shows that the proposed method is able to achieve better performance than baselines on a variety of driving tasks."
6900,SP:f5be102f16ed9ac70a2e9e2580111226fb0d8b71,"language modeling CONJUNCTION content recommendation. content recommendation CONJUNCTION language modeling. machine learning models USED-FOR tasks. content recommendation CONJUNCTION advertising. advertising CONJUNCTION content recommendation. image classification CONJUNCTION language modeling. language modeling CONJUNCTION image classification. data USED-FOR machine learning models. data USED-FOR tasks. advertising HYPONYM-OF tasks. image classification HYPONYM-OF tasks. language modeling HYPONYM-OF tasks. content recommendation HYPONYM-OF tasks. CIFAR10 CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR10. dataset USED-FOR model. SVHN EVALUATE-FOR approach. CIFAR10 EVALUATE-FOR approach. Material are text, and images. Method are small proxy model, and large target model. ","This paper proposes a proxy model for image classification and language modeling. The proposed method is based on the idea of using a small proxy model and a large target model. The proxy model is trained on a small subset of the target dataset, and the target model is used to train the proxy model on the large target dataset. The paper shows that the proposed method outperforms the baselines on CIFAR10 and SVHN.","This paper proposes a proxy model for image classification and language modeling. The proposed method is based on the idea of using a small proxy model and a large target model. The proxy model is trained on a small subset of the target dataset, and the target model is used to train the proxy model on the large target dataset. The paper shows that the proposed method outperforms the baselines on CIFAR10 and SVHN."
6904,SP:4332dfe46b715595e9f1dd3f6a79b82a646b4c23,"formulation of planning USED-FOR it. probabilistic inference problem USED-FOR formulation of planning. probabilistic inference problem USED-FOR it. classical methods USED-FOR control. classical methods USED-FOR inference. Sequential Monte Carlo CONJUNCTION Bayesian smoothing. Bayesian smoothing CONJUNCTION Sequential Monte Carlo. Bayesian smoothing USED-FOR control. classical methods USED-FOR Sequential Monte Carlo. classical methods USED-FOR Bayesian smoothing. inference USED-FOR control. classical methods USED-FOR Sequential Monte Carlo Planning. classical methods USED-FOR algorithm. Sequential Monte Carlo Planning USED-FOR continuous control tasks. Sequential Monte Carlo Planning USED-FOR multimodal policies. Method is sampling methods. OtherScientificTerm are continuous domains, and fixed computational budget. ","This paper studies the problem of sequential Monte Carlo planning for continuous control problems. The authors propose a probabilistic inference problem for the formulation of planning, which can be seen as an extension of Bayesian smoothing. They show that the proposed method is computationally efficient and can be applied to a wide range of continuous control tasks. They also show that it can be used to improve the sample efficiency of existing sampling methods. ","This paper studies the problem of sequential Monte Carlo planning for continuous control problems. The authors propose a probabilistic inference problem for the formulation of planning, which can be seen as an extension of Bayesian smoothing. They show that the proposed method is computationally efficient and can be applied to a wide range of continuous control tasks. They also show that it can be used to improve the sample efficiency of existing sampling methods. "
6908,SP:d3e4e2c267fd9ae536ab1816d5c1ba8e8fec19be,"Neural networks USED-FOR small adversarial perturbations. Adversarial robustness COMPARE clean accuracy. clean accuracy COMPARE Adversarial robustness. adversarial training HYPONYM-OF robust training method. robustness EVALUATE-FOR adversarial trained model. semantics - preserving transformations FEATURE-OF data distribution. distribution USED-FOR adversarial trained model. clean accuracy CONJUNCTION robust accuracy. robust accuracy CONJUNCTION clean accuracy. clean accuracy EVALUATE-FOR Bayes classifier. robust accuracy EVALUATE-FOR Bayes classifier. MNIST CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION MNIST. semantically - identical variants USED-FOR CIFAR10. semantically - identical variants USED-FOR MNIST. robustness accuracies EVALUATE-FOR adversarially trained models. adversarial robustness EVALUATE-FOR neural networks. Generic are models, and them. Metric is clean accuracies. OtherScientificTerm is input data distribution. ","This paper studies the problem of adversarial robustness in neural networks. The authors show that the clean accuracy and robustness of a neural network trained with adversarial perturbations are not the same, and that adversarial training can be seen as an extension of the clean-accuracy and clean-robustness problem. They also show that clean accuracy is not the only measure of robustness, but also robustness is also a measure of clean accuracy.  ","This paper studies the problem of adversarial robustness in neural networks. The authors show that the clean accuracy and robustness of a neural network trained with adversarial perturbations are not the same, and that adversarial training can be seen as an extension of the clean-accuracy and clean-robustness problem. They also show that clean accuracy is not the only measure of robustness, but also robustness is also a measure of clean accuracy.  "
6912,SP:a49fd0479a977c8fb45199210f9ff7dd2c0dabaf,transformation of layer weights COMPARE layer outputs. layer outputs COMPARE transformation of layer weights. normalization technique USED-FOR batch normalization. transformation of layer weights USED-FOR normalization technique. positive and negative weights USED-FOR layer output. SVHN CONJUNCTION ILSVRC 2012 ImageNet. ILSVRC 2012 ImageNet CONJUNCTION SVHN. CIFAR-10/100 CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR-10/100. CIFAR-10/100 HYPONYM-OF benchmarks. ILSVRC 2012 ImageNet HYPONYM-OF benchmarks. benchmarks EVALUATE-FOR method. SVHN HYPONYM-OF benchmarks. Generic is technique. ,"This paper proposes a new normalization technique for batch normalization. The proposed method is based on the idea that the output of a batch normalizer should be different from the input of the normalizer. The authors show that the proposed method can be used to improve the performance of batch normalizers on a variety of image classification tasks. The method is evaluated on CIFAR-10/100, SVHN, and ILSVRC 2012.","This paper proposes a new normalization technique for batch normalization. The proposed method is based on the idea that the output of a batch normalizer should be different from the input of the normalizer. The authors show that the proposed method can be used to improve the performance of batch normalizers on a variety of image classification tasks. The method is evaluated on CIFAR-10/100, SVHN, and ILSVRC 2012."
6916,SP:8188f15c8521099305aa8664e05f102ee6cea402,Memorization USED-FOR over - parameterized neural networks. Memorization USED-FOR generalization. implicit regularization effect USED-FOR stochastic gradient descent. learning rates FEATURE-OF implicit regularization effect. learning rates FEATURE-OF stochastic gradient descent. loss statistics USED-FOR mislabeled examples. algorithm USED-FOR mislabeled examples. artificial and real - world mislabeled examples FEATURE-OF datasets. datasets EVALUATE-FOR ODD. Method is DATA DENOISING ( ODD ). OtherScientificTerm is computational overhead. ,"This paper studies the problem of data denoising (DD) in the context of over-parameterized neural networks. The authors propose a new algorithm for ODD, which is based on the idea of implicit regularization. The main contribution of the paper is to show that ODD can be reduced to ODD under certain assumptions. The paper also provides a theoretical analysis of ODD.","This paper studies the problem of data denoising (DD) in the context of over-parameterized neural networks. The authors propose a new algorithm for ODD, which is based on the idea of implicit regularization. The main contribution of the paper is to show that ODD can be reduced to ODD under certain assumptions. The paper also provides a theoretical analysis of ODD."
6920,SP:fbf023a772013e6eca62f92982aecf857c16a428,Pretrained language models USED-FOR downstream NLP task. analysis framework USED-FOR pretraining and downstream tasks. latent variables FEATURE-OF posterior distribution. latent variable generative model of text USED-FOR pretraining and downstream tasks. head tuning CONJUNCTION prompt tuning. prompt tuning CONJUNCTION head tuning. frozen pretrained model USED-FOR classifier. Hidden Markov Model ( HMM ) CONJUNCTION HMM. HMM CONJUNCTION Hidden Markov Model ( HMM ). latent memory component USED-FOR long - term dependencies. natural language FEATURE-OF long - term dependencies. Hidden Markov Model ( HMM ) HYPONYM-OF generative model. HMM HYPONYM-OF generative model. latent memory component USED-FOR HMM. classification heads USED-FOR downstream task. non - degeneracy conditions FEATURE-OF HMM. memory - augmented HMM COMPARE vanilla HMM. vanilla HMM COMPARE memory - augmented HMM. prompt tuning USED-FOR downstream guarantees. recovery guarantees FEATURE-OF memory - augmented HMM. non - degeneracy conditions FEATURE-OF downstream guarantees. long - term memory USED-FOR task - relevant information. HMMs USED-FOR synthetically generated data. Generic is models. Task is downstream classifier. ,This paper proposes a framework for analyzing latent variable generative models (HMMs) for downstream NLP tasks. The authors propose to use the latent variable generation model of a generative model of text as a pretrained model for downstream tasks. They show that the HMM is able to recover long-term dependencies between downstream tasks and pretrained models. They also show that HMM can be used to improve the downstream performance of downstream tasks in a non-degeneracy setting.,This paper proposes a framework for analyzing latent variable generative models (HMMs) for downstream NLP tasks. The authors propose to use the latent variable generation model of a generative model of text as a pretrained model for downstream tasks. They show that the HMM is able to recover long-term dependencies between downstream tasks and pretrained models. They also show that HMM can be used to improve the downstream performance of downstream tasks in a non-degeneracy setting.
6936,SP:217c4205a99f9b37283137826c4be6ab9bfb4e8e,invariant features USED-FOR classifier. transferability USED-FOR domain generalization. total variation CONJUNCTION Wasserstein distance. Wasserstein distance CONJUNCTION total variation. algorithms USED-FOR domain generalization. feature embeddings USED-FOR domain generalization. transferability FEATURE-OF feature embeddings. algorithms USED-FOR feature embeddings. algorithms USED-FOR transferability. RotatedMNIST CONJUNCTION PACS. PACS CONJUNCTION RotatedMNIST. PACS CONJUNCTION Office - Home. Office - Home CONJUNCTION PACS. algorithm USED-FOR transferable features. benchmark datasets EVALUATE-FOR it. RotatedMNIST HYPONYM-OF benchmark datasets. Office - Home HYPONYM-OF benchmark datasets. PACS HYPONYM-OF benchmark datasets. algorithm COMPARE state - of - the - art algorithms. state - of - the - art algorithms COMPARE algorithm. Task is Out - of - distribution generalization. Generic is model. OtherScientificTerm is transferable ” features. ,"This paper studies the problem of transferability of feature embeddings for out-of-distribution (OOD) generalization. The authors propose a transferability-based method for learning transferable features. The transferability is defined as the difference between the total variation and the Wasserstein distance between the classifier and the target classifier. The main contribution of this paper is to propose a new transferability bound for transferability. The proposed method is evaluated on three benchmark datasets, PACS, Office-Home, and RotatedMNIST. The results show that the proposed method outperforms existing transferability bounds.","This paper studies the problem of transferability of feature embeddings for out-of-distribution (OOD) generalization. The authors propose a transferability-based method for learning transferable features. The transferability is defined as the difference between the total variation and the Wasserstein distance between the classifier and the target classifier. The main contribution of this paper is to propose a new transferability bound for transferability. The proposed method is evaluated on three benchmark datasets, PACS, Office-Home, and RotatedMNIST. The results show that the proposed method outperforms existing transferability bounds."
6952,SP:46f5874c8cbdb0832e92adcea85ca8a1b9ddc28a,"Reward USED-FOR reinforcement - learning agents. partial ordering CONJUNCTION partial ordering. partial ordering CONJUNCTION partial ordering. reward USED-FOR tasks. polynomial - time algorithms USED-FOR Markov reward function. OtherScientificTerm are acceptable behaviors, and reward function. ",This paper studies the problem of learning a Markov reward function for reinforcement learning. The authors propose a polynomial-time algorithm to solve the problem. They show that the reward function is polynomially time and can be expressed as a function of the number of states and actions. They also show that this algorithm can be used to solve partial ordering and partial ordering problems. ,This paper studies the problem of learning a Markov reward function for reinforcement learning. The authors propose a polynomial-time algorithm to solve the problem. They show that the reward function is polynomially time and can be expressed as a function of the number of states and actions. They also show that this algorithm can be used to solve partial ordering and partial ordering problems. 
6968,SP:9d8b57d60a0e59f9d9a90605094e8ef895f1c7de,"approaches USED-FOR generalization. sequential structure FEATURE-OF RL problem. approaches USED-FOR RL problem. approaches USED-FOR sequential structure. implicit partial observability USED-FOR generalization. generalization USED-FOR RL. epistemic POMDP HYPONYM-OF induced partially observed Markov decision process. induced partially observed Markov decision process USED-FOR generalization. ensemble - based technique USED-FOR partially observed problem. algorithm COMPARE methods. methods COMPARE algorithm. generalization EVALUATE-FOR methods. epistemic POMDP USED-FOR algorithm. generalization EVALUATE-FOR algorithm. Procgen benchmark suite EVALUATE-FOR algorithm. Procgen benchmark suite EVALUATE-FOR methods. Task is Generalization. Method are reinforcement learning ( RL ) systems, supervised learning, supervised learning methods, and POMDPs. OtherScientificTerm are epistemic uncertainty, fullyobserved MDPs, failure modes of algorithms, and partial observability. ","This paper proposes an ensemble-based algorithm for partially observed Markov Decision Processes (POMDPs), which is an extension of the epistemic POMDP framework. The authors show that the proposed algorithm is able to achieve better generalization performance than the state-of-the-art methods on the Procgen benchmark. The main contribution of the paper is the introduction of an ensemble based algorithm for epistemic MDPs. The algorithm is based on the idea of epistemic uncertainty, which is used to improve the generalization ability of the algorithm. ","This paper proposes an ensemble-based algorithm for partially observed Markov Decision Processes (POMDPs), which is an extension of the epistemic POMDP framework. The authors show that the proposed algorithm is able to achieve better generalization performance than the state-of-the-art methods on the Procgen benchmark. The main contribution of the paper is the introduction of an ensemble based algorithm for epistemic MDPs. The algorithm is based on the idea of epistemic uncertainty, which is used to improve the generalization ability of the algorithm. "
6984,SP:10de45510320b7ddb7ffb18b33e67f7cad609418,Hessian matrix of value functions USED-FOR Model - agnostic meta - reinforcement learning. framework USED-FOR higherorder derivatives of value functions. off - policy evaluation USED-FOR framework. framework USED-FOR prior approaches. framework USED-FOR estimates. auto - differentiation libraries USED-FOR estimates. OtherScientificTerm is biased Hessian estimates. ,This paper proposes a meta-reinforcement learning framework for model-agnostic meta-learning. The proposed framework is based on the Hessian matrix of value functions (Hessian Matrix of Value Functions (HMMF). The authors propose to use auto-differentiation libraries to compute Hessian estimates for off-policy evaluation. The authors show that the proposed framework outperforms the baselines on a number of tasks. ,This paper proposes a meta-reinforcement learning framework for model-agnostic meta-learning. The proposed framework is based on the Hessian matrix of value functions (Hessian Matrix of Value Functions (HMMF). The authors propose to use auto-differentiation libraries to compute Hessian estimates for off-policy evaluation. The authors show that the proposed framework outperforms the baselines on a number of tasks. 
7000,SP:54a60315416c6e304f59741490c335fb1e2ce95d,"approach USED-FOR communication constraints. approach USED-FOR distributed learning problem. communication constraints FEATURE-OF distributed learning problem. central server USED-FOR distributed learning problem. algorithm COMPARE algorithms. algorithms COMPARE algorithm. algorithm USED-FOR bidirectional compression. convergence rate EVALUATE-FOR algorithms. convergence rate EVALUATE-FOR algorithm. MCM HYPONYM-OF algorithm. gradients FEATURE-OF local servers. perturbed models USED-FOR gradients. model compression CONJUNCTION memory mechanism. memory mechanism CONJUNCTION model compression. memory mechanism PART-OF MCM. model compression PART-OF MCM. worker dependent randomized - models CONJUNCTION partial participation. partial participation CONJUNCTION worker dependent randomized - models. Method is downlink compression. OtherScientificTerm are local models, global model, and perturbation. Task is convergence proofs. ","This paper studies the problem of downlink compression in distributed learning. The authors propose a new algorithm called MCM, which is a bidirectional compression algorithm that combines the memory mechanism and model compression. They show that the proposed algorithm converges faster than existing algorithms in terms of convergence rate. They also show that their algorithm can be applied to the case of worker-dependent randomized-model and partial participation. ","This paper studies the problem of downlink compression in distributed learning. The authors propose a new algorithm called MCM, which is a bidirectional compression algorithm that combines the memory mechanism and model compression. They show that the proposed algorithm converges faster than existing algorithms in terms of convergence rate. They also show that their algorithm can be applied to the case of worker-dependent randomized-model and partial participation. "
7016,SP:6b19f16c429ffa7f613b57d082bde3794a8e29e0,"causal inference USED-FOR stress testing. counterfactual invariance USED-FOR outof - domain model. causal structure USED-FOR counterfactual invariance. regularization schemes USED-FOR counterfactual invariance. regularization schemes USED-FOR causal structures. causal structure FEATURE-OF domain shift guarantees. domain shift guarantees FEATURE-OF counterfactual invariance. OtherScientificTerm are spurious correlation, spurious correlations, and counterfactual examples. Method are model, and machine learning. Generic are models, and schemes. Task is text classification. ",This paper studies the problem of counterfactual invariance in out-of-domain (OOD) models. The authors propose two regularization schemes to improve the robustness of OOD models to spurious correlations. The paper also provides a theoretical analysis of domain shift guarantees.,This paper studies the problem of counterfactual invariance in out-of-domain (OOD) models. The authors propose two regularization schemes to improve the robustness of OOD models to spurious correlations. The paper also provides a theoretical analysis of domain shift guarantees.
7032,SP:6ff26839a14991597555ead4c82eb6ddb61e4dbc,"limited data USED-FOR GANs. data augmentations CONJUNCTION model regularization. model regularization CONJUNCTION data augmentations. generator USED-FOR real data distribution. APA USED-FOR overfitting. method COMPARE approaches. approaches COMPARE method. generator USED-FOR APA. model regularization USED-FOR approaches. data augmentations USED-FOR approaches. APA USED-FOR low - data regime. low - data regime FEATURE-OF synthesis quality. synthesis quality EVALUATE-FOR APA. It USED-FOR GANs. StyleGAN2 HYPONYM-OF GANs. Method are Generative adversarial networks ( GANs ), and Adaptive Pseudo Augmentation ( APA ). Material are high - fidelity images, and generated images. OtherScientificTerm are discriminator overfitting, and discriminator. Generic are strategy, and training strategy. ","This paper proposes a new method for improving the performance of GANs in the low-data regime. The proposed method is called Adaptive Pseudo Augmentation (APA) and is based on the idea that the discriminator overfits in the high-fidelity regime. APA uses a generator to generate high-quality images, and then augments the generated images with pseudo-augmentations to reduce the overfitting. The method is evaluated on StyleGAN2 and StyleGAN3.","This paper proposes a new method for improving the performance of GANs in the low-data regime. The proposed method is called Adaptive Pseudo Augmentation (APA) and is based on the idea that the discriminator overfits in the high-fidelity regime. APA uses a generator to generate high-quality images, and then augments the generated images with pseudo-augmentations to reduce the overfitting. The method is evaluated on StyleGAN2 and StyleGAN3."
7048,SP:0f3fcffb6dfbf344bd5ef73c3f6d3d84d2f13887,Causal inference CONJUNCTION discovery. discovery CONJUNCTION Causal inference. observational data USED-FOR discovery. observational data USED-FOR Causal inference. formalization USED-FOR causal inference. Rubin ’s framework USED-FOR multivariate point processes. average treatment effect ( ATE ) CONJUNCTION propensity scores. propensity scores CONJUNCTION average treatment effect ( ATE ). Rubin ’s framework USED-FOR average treatment effect ( ATE ). Rubin ’s framework USED-FOR propensity scores. multivariate recurrent event streams USED-FOR causal inference. multivariate point process USED-FOR data. joint probability distribution USED-FOR i.i.d. data. joint probability distribution COMPARE multivariate point process. multivariate point process COMPARE joint probability distribution. causal inference framework COMPARE baseline pairwise causal association scores. baseline pairwise causal association scores COMPARE causal inference framework. synthetic and real - world event datasets EVALUATE-FOR causal inference framework. Method is point process causal framework. Generic is measure. ,This paper proposes a new framework for causal inference based on multivariate recurrent event streams. The proposed framework is based on the notion of average treatment effect (ATE) and propensity scores. The authors show that the proposed framework can be applied to both synthetic and real-world event datasets. ,This paper proposes a new framework for causal inference based on multivariate recurrent event streams. The proposed framework is based on the notion of average treatment effect (ATE) and propensity scores. The authors show that the proposed framework can be applied to both synthetic and real-world event datasets. 
7064,SP:5db39fbba518e24a22b99c8256491295048ec417,Graph neural networks ( GNNs ) USED-FOR graph representation learning. residual connections PART-OF message passing. they USED-FOR GNNs. message passing USED-FOR GNNs. abnormal node features FEATURE-OF GNNs. node features PART-OF graphs. GNNs USED-FOR abnormal features. resilience FEATURE-OF GNNs. AirGNN1 HYPONYM-OF GNN. Adaptive residual USED-FOR GNN. Task is real - world applications. OtherScientificTerm is abnormal feature scenarios. Generic is algorithm. ,"This paper studies the problem of adaptive message passing in GNNs. The authors propose a new algorithm, AirGNN1, which is based on the idea of adaptive residuals. They show that the adaptive residual can be used to improve the robustness of the GNN to abnormal node features. They also show that it can be applied to any GNN architecture. ","This paper studies the problem of adaptive message passing in GNNs. The authors propose a new algorithm, AirGNN1, which is based on the idea of adaptive residuals. They show that the adaptive residual can be used to improve the robustness of the GNN to abnormal node features. They also show that it can be applied to any GNN architecture. "
7080,SP:66c5acd36a5fb74478d3f5ecaffc8479868dbe81,"Thompson sampling policy PART-OF Bayesian ‘ optimistic ’ policies. algorithm USED-FOR policies. optimistic set FEATURE-OF policies. zero - sum matrix games CONJUNCTION constrained bandits. constrained bandits CONJUNCTION zero - sum matrix games. regret analysis USED-FOR bilinear saddle - point problems. regret analysis USED-FOR optimistic policies. zero - sum matrix games HYPONYM-OF bilinear saddle - point problems. Thompson sampling USED-FOR policies. policy USED-FOR convex optimization problem. optimistic set FEATURE-OF policy. log - concavity CONJUNCTION unimodality. unimodality CONJUNCTION log - concavity. unimodality CONJUNCTION smoothness. smoothness CONJUNCTION unimodality. procedure USED-FOR posteriors. regularization CONJUNCTION constraints. constraints CONJUNCTION regularization. Task are online sequential decision problems, and stochastic multi - armed bandit case. Metric is Bayesian regret. Generic are problem, and it. OtherScientificTerm are linear regret, posterior, and exploration - exploitation tradeoff. Method is variational Bayesian optimistic sampling ’ ( VBOS ). ","This paper studies the problem of Bayesian Optimistic Sampling (BOS) in the stochastic multi-armed bandit setting. In particular, the authors consider the case where the agent has access to a Thompson sampling policy, and the goal is to find a policy that maximizes the regret of the agent. The authors propose a variational Bayesian optimistic sampling (VBOS) algorithm to solve this problem. The proposed algorithm is based on the Thompson sampling algorithm, which is used to sample from the optimistic set of policies. The paper shows that the proposed VBOS algorithm can be used to solve the convex optimization problem in the constrained bandit case and the zero-sum matrix game in the saddle point case.","This paper studies the problem of Bayesian Optimistic Sampling (BOS) in the stochastic multi-armed bandit setting. In particular, the authors consider the case where the agent has access to a Thompson sampling policy, and the goal is to find a policy that maximizes the regret of the agent. The authors propose a variational Bayesian optimistic sampling (VBOS) algorithm to solve this problem. The proposed algorithm is based on the Thompson sampling algorithm, which is used to sample from the optimistic set of policies. The paper shows that the proposed VBOS algorithm can be used to solve the convex optimization problem in the constrained bandit case and the zero-sum matrix game in the saddle point case."
7096,SP:c1b7b550b9f90bd5e9bf5218e22d1977ed1686a5,"without - replacement sampling orders COMPARE uniform - iid - sampling. uniform - iid - sampling COMPARE without - replacement sampling orders. Without - replacement sampling USED-FOR SGD without variance reduction. convergence analysis CONJUNCTION rates of variance reduction. rates of variance reduction CONJUNCTION convergence analysis. without - replacement sampling orders USED-FOR composite finite - sum minimization. rates of variance reduction EVALUATE-FOR without - replacement sampling orders. random reshuffling CONJUNCTION cyclic sampling. cyclic sampling CONJUNCTION random reshuffling. Prox - DFinito HYPONYM-OF Finito. random reshuffling USED-FOR convergence rates. rates EVALUATE-FOR full - batch gradient descent. variance - reduction USED-FOR without - replacement sampling. cyclic order USED-FOR cyclic sampling. variance reduction USED-FOR uniform - iid - sampling. Prox - DFinito USED-FOR data - heterogeneous scenario. optimal cyclic sampling USED-FOR Prox - DFinito. sample - size - independent convergence rate EVALUATE-FOR Prox - DFinito. method USED-FOR optimal cyclic ordering. Method is stochastic algorithm. OtherScientificTerm are convex and strongly convex scenarios, and optimal fixed ordering. ","This paper studies the problem of without-replacement sampling in stochastic gradient descent (SGD). In particular, the authors consider the case where the data distribution is strongly convex and the sample size is heterogeneous. The authors show that the optimal cyclic sampling order for SGD with uniform-iid-sampling is the optimal fixed ordering. They also show that without replacement sampling orders can be used to improve the sample-size-independent convergence rate of full-batch gradient descent. ","This paper studies the problem of without-replacement sampling in stochastic gradient descent (SGD). In particular, the authors consider the case where the data distribution is strongly convex and the sample size is heterogeneous. The authors show that the optimal cyclic sampling order for SGD with uniform-iid-sampling is the optimal fixed ordering. They also show that without replacement sampling orders can be used to improve the sample-size-independent convergence rate of full-batch gradient descent. "
7112,SP:35c14ef59d9ed68f3f6e3a8cac95fdf3216a9d8f,"algorithmic components USED-FOR reinforcement learning ( RL ) algorithms. relative entropy policy search ( REPS ) USED-FOR policy learning. simulated and real - world robotic domains EVALUATE-FOR policy learning. stochastic, gradient - based solvers USED-FOR REPS. first - order optimization methods USED-FOR REPS objective. sub - optimality FEATURE-OF policy. convergence rates FEATURE-OF sub - optimality. convergence rates FEATURE-OF policy. first - order optimization methods USED-FOR policy. technique USED-FOR parameter updates. generative access USED-FOR parameter updates. generative access USED-FOR Markov decision process. favorable convergence FEATURE-OF parameter updates. Markov decision process USED-FOR technique. generative access USED-FOR technique. OtherScientificTerm are exact gradients, near - optimality, stochastic gradients, and optimal regularized policy. ","This paper studies the relative entropy policy search (REPS) problem in reinforcement learning (RL). The authors propose a new algorithm for REPS, which is based on generative access. The authors show that the proposed algorithm achieves favorable convergence rates and sub-optimality in the case of stochastic gradient-based solvers. They also show that their algorithm can be applied to the Markov decision process. ","This paper studies the relative entropy policy search (REPS) problem in reinforcement learning (RL). The authors propose a new algorithm for REPS, which is based on generative access. The authors show that the proposed algorithm achieves favorable convergence rates and sub-optimality in the case of stochastic gradient-based solvers. They also show that their algorithm can be applied to the Markov decision process. "
7128,SP:3945d1fb07900d63b7706ca0bce5e451ebfe476b,deep neural networks ( DNNs ) USED-FOR 3D point cloud processing. knowledge representations USED-FOR 3D point cloud processing. deep neural networks ( DNNs ) USED-FOR knowledge representations. translation CONJUNCTION scale. scale CONJUNCTION translation. scale CONJUNCTION local 3D structures. local 3D structures CONJUNCTION scale. rotation CONJUNCTION translation. translation CONJUNCTION rotation. representation complexity EVALUATE-FOR DNN. metrics CONJUNCTION representation complexity. representation complexity CONJUNCTION metrics. metrics USED-FOR spatial smoothness. metrics EVALUATE-FOR DNN. spatial smoothness FEATURE-OF encoding 3D structures. DNNs USED-FOR representation problems. Generic is method. Method is adversarial training. ,This paper studies the problem of learning knowledge representations for 3D point cloud processing. The authors propose an adversarial training method to improve the representation complexity of DNNs. The proposed method is based on the idea that the representation of a point cloud can be represented as a weighted sum of local 3D structures and global 3D structure. The paper shows that the proposed method can achieve better representation complexity and spatial smoothness compared to existing methods. ,This paper studies the problem of learning knowledge representations for 3D point cloud processing. The authors propose an adversarial training method to improve the representation complexity of DNNs. The proposed method is based on the idea that the representation of a point cloud can be represented as a weighted sum of local 3D structures and global 3D structure. The paper shows that the proposed method can achieve better representation complexity and spatial smoothness compared to existing methods. 
7144,SP:81db7f494ca61d3586adb505bf5d2e6e9e2c2bd0,"economics CONJUNCTION game theory. game theory CONJUNCTION economics. game theory CONJUNCTION computer science. computer science CONJUNCTION game theory. economics USED-FOR design of optimal auctions. methods USED-FOR approximating optimal auctions. deep learning USED-FOR approximating optimal auctions. deep learning USED-FOR methods. allocation fairness CONJUNCTION diversity. diversity CONJUNCTION allocation fairness. auction mechanisms USED-FOR socially desirable constraints. diversity HYPONYM-OF socially desirable constraints. allocation fairness HYPONYM-OF socially desirable constraints. neural - network - based auction mechanisms USED-FOR constraints. neural - network - based auction mechanisms USED-FOR PreferenceNet. method COMPARE neural - network based auction designs. neural - network based auction designs COMPARE method. metric EVALUATE-FOR method. metric USED-FOR auction allocations. socially desirable constraints FEATURE-OF auction allocations. human subject research USED-FOR approach. Method is strategyproof, revenuemaximizing auction designs. OtherScientificTerm are restricted settings, optimal auctions, and real human preferences. Generic are baselines, and they. Metric is maximizing revenue. ","This paper proposes a method for finding optimal auctions in a restricted setting. The authors propose a neural network-based auction mechanism, PreferenceNet, that is able to find optimal auctions that satisfy the following constraints: (1) fairness, (2) diversity, (3) allocation fairness, and (4) allocation diversity. The proposed method is shown to outperform the state-of-the-art methods in terms of revenue. ","This paper proposes a method for finding optimal auctions in a restricted setting. The authors propose a neural network-based auction mechanism, PreferenceNet, that is able to find optimal auctions that satisfy the following constraints: (1) fairness, (2) diversity, (3) allocation fairness, and (4) allocation diversity. The proposed method is shown to outperform the state-of-the-art methods in terms of revenue. "
7160,SP:ee24606a968ab17b7827e7f3982af11636f6a2ee,"user - level differential privacy USED-FOR personalization of supervised learning. user - level privacy guarantees EVALUATE-FOR approach. non - private approaches USED-FOR algorithms. nearly optimal estimation error guarantees FEATURE-OF algorithms. exponential mechanism - based algorithm USED-FOR information - theoretic upper bound. OtherScientificTerm are shared structure, and joint, user - level differential privacy. Task is linear regression problems. ",This paper studies the problem of user-level differential privacy in the context of supervised learning. The authors propose an exponential mechanism-based algorithm for solving linear regression problems. The proposed algorithm is based on the idea of joint differential privacy (JDP). The authors show that the proposed algorithm can be used to achieve nearly optimal estimation error guarantees. They also provide an information-theoretic upper bound for their algorithm.,This paper studies the problem of user-level differential privacy in the context of supervised learning. The authors propose an exponential mechanism-based algorithm for solving linear regression problems. The proposed algorithm is based on the idea of joint differential privacy (JDP). The authors show that the proposed algorithm can be used to achieve nearly optimal estimation error guarantees. They also provide an information-theoretic upper bound for their algorithm.
7176,SP:3925fc528de17b8b2e93808f5440ea0503895b75,"human - adversarial examples USED-FOR them. examples EVALUATE-FOR state - of - the - art models. Material are Visual Question Answering dataset ( VQA v2 ), adversarial examples, and Adversarial VQA ( AdVQA ) benchmark. Metric is human accuracy. Method are VQA models, and VQA model. Generic is model. ","This paper presents Adversarial VQA (AdVQA), a benchmark for evaluating the performance of state-of-the-art visual question answering models against human-adversarial examples. The benchmark is built on top of the Visual Question Answering dataset, which contains a large number of adversarial examples that are generated by human experts. The authors show that the performance on AdVQAs is comparable to that of human experts, and that the model is able to outperform the human experts on the AdVs benchmark. The paper also shows that AdVs outperforms human experts in terms of human accuracy.","This paper presents Adversarial VQA (AdVQA), a benchmark for evaluating the performance of state-of-the-art visual question answering models against human-adversarial examples. The benchmark is built on top of the Visual Question Answering dataset, which contains a large number of adversarial examples that are generated by human experts. The authors show that the performance on AdVQAs is comparable to that of human experts, and that the model is able to outperform the human experts on the AdVs benchmark. The paper also shows that AdVs outperforms human experts in terms of human accuracy."
7192,SP:04f90c10f4ceca0dace727ad875265ce405fff9f,"Medial entorhinal cortex ( MEC ) USED-FOR navigational and memory related behaviors. they USED-FOR MEC functionality. they USED-FOR behavior. response profiles FEATURE-OF heterogeneous ” cells. models USED-FOR response profiles. response profiles FEATURE-OF stereotypical and heterogeneous MEC cells. task - optimized neural network models COMPARE grid cell - centric models. grid cell - centric models COMPARE task - optimized neural network models. grid cell - centric models USED-FOR MEC neuronal response profiles. task - optimized neural network models USED-FOR MEC neuronal response profiles. gated nonlinearities CONJUNCTION intermediate place cell representation. intermediate place cell representation CONJUNCTION gated nonlinearities. intermediate place cell representation HYPONYM-OF network architecture. gated nonlinearities HYPONYM-OF network architecture. heterogeneous cells COMPARE grid and border cells. grid and border cells COMPARE heterogeneous cells. heterogeneous cells USED-FOR downstream functional outcomes. path integration HYPONYM-OF downstream functional outcomes. spatial response selectivity FEATURE-OF MEC cells. reward - modulated path integration USED-FOR MEC model. non - spatial rewards FEATURE-OF MEC cells. variable - reward conditions FEATURE-OF neural recordings. OtherScientificTerm are MEC, grid, stereotypical response profiles, MEC neurons, stereotypical firing patterns, heterogeneous MEC cells, and response patterns. Method are computational approach, statistical analysis, and goal - driven modeling approach. Generic is model. Task is Neural Information Processing Systems. ",This paper presents a method for learning the response profiles of MEC neurons. The authors propose a goal-driven model to learn the response patterns of heterogeneous MEC cells. The proposed method is based on the idea of reward-modulated path integration (RMA). The authors show that the proposed method outperforms state-of-the-art baselines on a variety of tasks. ,This paper presents a method for learning the response profiles of MEC neurons. The authors propose a goal-driven model to learn the response patterns of heterogeneous MEC cells. The proposed method is based on the idea of reward-modulated path integration (RMA). The authors show that the proposed method outperforms state-of-the-art baselines on a variety of tasks. 
7208,SP:57f9812fa5e7d0c66d412beb035301684d760746,"them USED-FOR physical real - world tasks. KL - regularized reinforcement learning USED-FOR deep reinforcement learning algorithms. sample efficiency EVALUATE-FOR deep reinforcement learning algorithms. KL - regularized reinforcement learning USED-FOR them. expert demonstrations USED-FOR KL - regularized reinforcement learning. sample efficiency EVALUATE-FOR KL - regularized reinforcement learning. behavioral reference policies USED-FOR KL - regularized reinforcement learning. expert demonstrations USED-FOR behavioral reference policies. expert demonstrations USED-FOR KL - regularized reinforcement learning. sample efficiency CONJUNCTION online policy. online policy CONJUNCTION sample efficiency. KL - regularized reinforcement learning COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE KL - regularized reinforcement learning. locomotion and dexterous hand manipulation tasks EVALUATE-FOR KL - regularized reinforcement learning. locomotion and dexterous hand manipulation tasks EVALUATE-FOR state - of - the - art approaches. non - parametric behavioral reference policies USED-FOR pathology. OtherScientificTerm are pathological training dynamics, and behavioral policy classes. Method is online learning. ",This paper studies the problem of learning behavioral reference policies for pathological training dynamics. The authors propose a new approach to learn a behavioral reference policy based on expert demonstrations. The proposed approach is based on KL-regularized reinforcement learning (KL-RL). The authors show that KL-RL improves sample efficiency and sample efficiency of online RL algorithms. They also show that the proposed approach can be used to learn non-parametric behavioral policy classes. ,This paper studies the problem of learning behavioral reference policies for pathological training dynamics. The authors propose a new approach to learn a behavioral reference policy based on expert demonstrations. The proposed approach is based on KL-regularized reinforcement learning (KL-RL). The authors show that KL-RL improves sample efficiency and sample efficiency of online RL algorithms. They also show that the proposed approach can be used to learn non-parametric behavioral policy classes. 
7224,SP:cb38b58054581db865d8c2a4065f062724ca0a5e,"teacher - student framework USED-FOR kernel regression. neural tangent kernel PART-OF convolutional architectures. neural tangent kernel USED-FOR convolutional ’ kernels. filter size FEATURE-OF convolutional architectures. teacher - student framework USED-FOR problem. convolutional ’ kernels USED-FOR teacher - student framework. convolutional ’ kernels USED-FOR problem. locality USED-FOR learning curve exponent β. physics USED-FOR heuristic methods. ridge USED-FOR kernel regression. Method is Convolutional neural networks. OtherScientificTerm are ridgeless case, translational invariance, natural universality assumption, and learning curve exponents. ","This paper studies the problem of kernel regression in convolutional neural networks. The authors propose a teacher-student framework for kernel regression, which is based on the notion of translational invariance. They show that the learning curve exponent $\beta$ is invariant in the ridgeless case. They also show that in the non-ridgeless setting, $\beta = \mathcal{O}(\sqrt{T}(\log T)$ is a convex function of the learning rate $\log T$.  ","This paper studies the problem of kernel regression in convolutional neural networks. The authors propose a teacher-student framework for kernel regression, which is based on the notion of translational invariance. They show that the learning curve exponent $\beta$ is invariant in the ridgeless case. They also show that in the non-ridgeless setting, $\beta = \mathcal{O}(\sqrt{T}(\log T)$ is a convex function of the learning rate $\log T$.  "
7240,SP:7e35e4e610e75c922f2b5219ce625e417f010eeb,"Variational Autoencoders ( VAEs ) USED-FOR representations of complex data distributions. Variational Autoencoders ( VAEs ) HYPONYM-OF probabilistic models. probabilistic models USED-FOR representations of complex data distributions. model USED-FOR latent representations. uni - modal Gaussian distribution USED-FOR latent representations. regularized autoencoders USED-FOR deterministic autoencoding framework. ex - post density estimation step USED-FOR they. latent space FEATURE-OF model. deterministic autoencoding framework USED-FOR latent space. expressive multi - modal latent distributions USED-FOR deterministic autoencoding framework. latent distribution USED-FOR encoded data. sample quality EVALUATE-FOR model. continuous and discrete domains EVALUATE-FOR model. Method are VAEs, and variational training procedure. OtherScientificTerm is VAE objective. Generic are models, and training procedure. ",This paper proposes a variational autoencoder (VAE) framework for learning latent representations of complex data distributions. The proposed method is based on the uni-modal Gaussian distribution and the deterministic autoencoders framework. The authors show that the proposed method outperforms the baselines in both continuous and discrete domains. ,This paper proposes a variational autoencoder (VAE) framework for learning latent representations of complex data distributions. The proposed method is based on the uni-modal Gaussian distribution and the deterministic autoencoders framework. The authors show that the proposed method outperforms the baselines in both continuous and discrete domains. 
7256,SP:6232d8738592c9728feddec4462e61903a17d131,adversarial examples USED-FOR Deep learning models. Autoencoder USED-FOR self - supervised ) adversarial detection. benign examples USED-FOR Autoencoder. autoencoder structure USED-FOR disentangled represen8 tations of images. disentangled represen8 tations of images USED-FOR adversarial examples. class features CONJUNCTION semantic features. semantic features CONJUNCTION class features. paired class / semantic features CONJUNCTION paired class / semantic features. paired class / semantic features CONJUNCTION paired class / semantic features. paired class / semantic features USED-FOR autoencoder. paired class / semantic features USED-FOR autoencoder. discriminator network USED-FOR autoencoder. generalization ability FEATURE-OF autoencoder. AUC CONJUNCTION FPR. FPR CONJUNCTION AUC. FPR CONJUNCTION TPR. TPR CONJUNCTION FPR. method COMPARE self - supervised detection methods. self - supervised detection methods COMPARE method. adversarial attacks CONJUNCTION victim models. victim models CONJUNCTION adversarial attacks. method COMPARE it. it COMPARE method. AUC CONJUNCTION TPR. TPR CONJUNCTION AUC. victim models USED-FOR self - supervised detection methods. adversarial attacks USED-FOR method. adversarial attacks FEATURE-OF self - supervised detection methods. AUC HYPONYM-OF measurements. measurements EVALUATE-FOR it. TPR HYPONYM-OF measurements. AUC EVALUATE-FOR it. FPR HYPONYM-OF measurements. AUC EVALUATE-FOR method. CIFAR-10 EVALUATE-FOR method. Autoencoder - based detectors COMPARE method. method COMPARE Autoencoder - based detectors. method USED-FOR adaptive adversary. Metric is reconstruction error. ,"This paper proposes an autoencoder-based self-supervised adversarial detection method. The proposed method is based on disentangling the disentangled represen8 tations of images into two parts: class features and semantic features. The class features are learned by a discriminator network, while the semantic features are learnt by a classifier network. The authors show that the proposed method outperforms the state-of-the-art in terms of AUC, FPR, and TPR on CIFAR-10. ","This paper proposes an autoencoder-based self-supervised adversarial detection method. The proposed method is based on disentangling the disentangled represen8 tations of images into two parts: class features and semantic features. The class features are learned by a discriminator network, while the semantic features are learnt by a classifier network. The authors show that the proposed method outperforms the state-of-the-art in terms of AUC, FPR, and TPR on CIFAR-10. "
7272,SP:e9d9ad4fb9dc3cb25f7282c0979a8ccb252f692a,"it USED-FOR brain activity. embedding space FEATURE-OF syntactic structure. low signal - to - noise ratio FEATURE-OF neuroimaging tools. functional Magnetic Resonance Imaging ( fMRI ) HYPONYM-OF neuroimaging tools. multi - dimensional features USED-FOR syntactic structure. features CONJUNCTION fMRI recordings. fMRI recordings CONJUNCTION features. fMRI recordings USED-FOR brain representation of syntax. fMRI recordings USED-FOR natural text. features USED-FOR brain representation of syntax. syntactic structure - based features USED-FOR brain activity. complexity metrics USED-FOR processing load. language system FEATURE-OF brain activity. OtherScientificTerm are semantics, semantic processing load, semantic representation of the stimulus words, syntactic processing load, and syntactic features. Generic is approaches. Task is syntax. ","This paper proposes a new method for learning syntactic representations of brain activity using fMRI. The proposed method is based on the idea that fMRI is a low-signal-to-noise (L2N) imaging technique that can be used to learn syntactic representation of the brain activity. The authors show that the proposed method can capture syntactic structure-based features in fMRI recordings, and that it is able to capture the semantic processing load of the language system. ","This paper proposes a new method for learning syntactic representations of brain activity using fMRI. The proposed method is based on the idea that fMRI is a low-signal-to-noise (L2N) imaging technique that can be used to learn syntactic representation of the brain activity. The authors show that the proposed method can capture syntactic structure-based features in fMRI recordings, and that it is able to capture the semantic processing load of the language system. "
7288,SP:b4ad4632cd55a85b5403e936c4bd828e484473f7,deep generative models USED-FOR real - world applications. deep generative models USED-FOR Controllable generation. compositional ability USED-FOR concept combinations. compositional ability FEATURE-OF models. energybased models ( EBMs ) USED-FOR compositional generation. attributes USED-FOR compositional generation. them USED-FOR high - resolution image generation. EBM USED-FOR pre - trained generative model. latent space FEATURE-OF pre - trained generative model. latent space FEATURE-OF EBM. EBM USED-FOR them. EBM USED-FOR high - resolution image generation. StyleGAN HYPONYM-OF pre - trained generative model. EBM formulation USED-FOR joint distribution. it USED-FOR sampling. ordinary differential equation ( ODE ) USED-FOR sampling. pre - trained generator USED-FOR controllable generation. controllable generation USED-FOR attribute classifier. latent space USED-FOR Sampling. ODEs USED-FOR Sampling. conditional sampling CONJUNCTION sequential editing. sequential editing CONJUNCTION conditional sampling. method COMPARE state - of - the - art. state - of - the - art COMPARE method. sequential editing EVALUATE-FOR state - of - the - art. conditional sampling EVALUATE-FOR state - of - the - art. conditional sampling EVALUATE-FOR method. sequential editing EVALUATE-FOR method. method USED-FOR attribute combinations. method USED-FOR compositional generation. energy functions CONJUNCTION logical operators. logical operators CONJUNCTION energy functions. compositionality FEATURE-OF generating photo - realistic images. OtherScientificTerm is hyperparameters. Material is photo - realistic images. ,This paper proposes a new method for compositional generation using energy-based models (EBMs). The authors propose to use a pre-trained generative model to learn a joint distribution of attributes that can be used to generate compositional images. The joint distribution is defined as a combination of energy functions and logical operators. The authors show that the proposed method is able to achieve better compositional performance than state-of-the-art methods. ,This paper proposes a new method for compositional generation using energy-based models (EBMs). The authors propose to use a pre-trained generative model to learn a joint distribution of attributes that can be used to generate compositional images. The joint distribution is defined as a combination of energy functions and logical operators. The authors show that the proposed method is able to achieve better compositional performance than state-of-the-art methods. 
7304,SP:f5bf6d43bcc90a3bc5f2157fcc041f18224f95e0,common global parameters USED-FOR K - armed stochastic bandits. geometric structure USED-FOR collaborative algorithm. local feature vectors CONJUNCTION raw data. raw data CONJUNCTION local feature vectors. collaborative algorithm USED-FOR heterogeneity. geometric structure FEATURE-OF linear rewards. Fed - PE USED-FOR heterogeneity. Fed - PE HYPONYM-OF collaborative algorithm. near - optimal regrets FEATURE-OF disjoint and shared parameter cases. logarithmic communication costs FEATURE-OF near - optimal regrets. multi - client G - optimal design USED-FOR Fed - PE. tight minimax regret lower bound USED-FOR disjoint parameter case. collinearly - dependent policies HYPONYM-OF concept. synthetic and real - world datasets EVALUATE-FOR algorithms. Method is federated linear contextual bandits model. ,"This paper studies the problem of federated linear contextual bandits with shared global parameters. The authors propose Fed-PE, a collaborative algorithm for solving the problem. The main contribution of the paper is a new regret lower bound for the disjoint and shared parameter cases. The regret upper bound is tight for the shared parameter case, and the regret lower bounds are tighter for the non-shared parameter case. Experiments on synthetic and real-world datasets demonstrate the effectiveness of the proposed algorithm. ","This paper studies the problem of federated linear contextual bandits with shared global parameters. The authors propose Fed-PE, a collaborative algorithm for solving the problem. The main contribution of the paper is a new regret lower bound for the disjoint and shared parameter cases. The regret upper bound is tight for the shared parameter case, and the regret lower bounds are tighter for the non-shared parameter case. Experiments on synthetic and real-world datasets demonstrate the effectiveness of the proposed algorithm. "
7320,SP:d3ff3012c614638c8d86322cfe461a9383f082ab,"model USED-FOR conservative planning. explicit uncertainty quantification USED-FOR incorporating conservatism. explicit uncertainty quantification USED-FOR model - based algorithms. deep neural networks USED-FOR Uncertainty estimation. complex models USED-FOR Uncertainty estimation. deep neural networks HYPONYM-OF complex models. uncertainty estimation USED-FOR offline model - based RL. offline dataset CONJUNCTION data. data CONJUNCTION offline dataset. COMBO USED-FOR value function. model - based offline RL algorithm USED-FOR value function. rollouts USED-FOR data. COMBO HYPONYM-OF model - based offline RL algorithm. data USED-FOR value function. offline dataset USED-FOR value function. policy improvement guarantee FEATURE-OF COMBO. COMBO COMPARE offline RL. offline RL COMPARE COMBO. offline RL USED-FOR problems. COMBO USED-FOR problems. COMBO COMPARE offline RL methods. offline RL methods COMPARE COMBO. generalization FEATURE-OF problems. offline RL benchmarks EVALUATE-FOR offline RL methods. image - based tasks HYPONYM-OF offline RL benchmarks. Method is dynamics model. Material is logged experience. Task are offline reinforcement learning ( offline RL ), and explicit uncertainty estimation. OtherScientificTerm is model rollouts. ","This paper proposes a model-based offline RL algorithm called COMBO. COMBO is based on the idea of explicit uncertainty quantification, which is used to improve the policy improvement guarantee of offline RL algorithms. The authors show that COMBO outperforms existing offline RL methods in terms of generalization and generalization to new offline RL problems. The main contribution of the paper is the introduction of COMBO, which uses a dynamic dynamics model to estimate the uncertainty of the value function of the offline dataset. The proposed method is evaluated on several offline RL benchmarks and shows improved performance over baselines.","This paper proposes a model-based offline RL algorithm called COMBO. COMBO is based on the idea of explicit uncertainty quantification, which is used to improve the policy improvement guarantee of offline RL algorithms. The authors show that COMBO outperforms existing offline RL methods in terms of generalization and generalization to new offline RL problems. The main contribution of the paper is the introduction of COMBO, which uses a dynamic dynamics model to estimate the uncertainty of the value function of the offline dataset. The proposed method is evaluated on several offline RL benchmarks and shows improved performance over baselines."
7336,SP:ca6f11ed297290e487890660d9a9a088aa106801,"stochastic gradient descent USED-FOR neural networks. deep learning training USED-FOR evolution of features. stochastic differential equations ( SDEs ) USED-FOR evolution of features. backpropagation USED-FOR features. drift term PART-OF SDE. sharp phase transition phenomenon FEATURE-OF intra - class impact. neural collapse of the features HYPONYM-OF geometric structure. local elasticity USED-FOR neural networks. synthesized dataset of geometric shapes CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION synthesized dataset of geometric shapes. Method are deep learning models, modeling strategy, and SDEs. Generic is models. OtherScientificTerm are feature spaces, and vanishing training loss. ",This paper studies the sharp phase transition phenomenon in stochastic differential equations (SDEs). The authors show that the intra-class impact of SDEs in deep learning models is due to the vanishing training loss and the local elasticity of the feature space. The authors also show that this phenomenon is also observed in the case of backpropagation. ,This paper studies the sharp phase transition phenomenon in stochastic differential equations (SDEs). The authors show that the intra-class impact of SDEs in deep learning models is due to the vanishing training loss and the local elasticity of the feature space. The authors also show that this phenomenon is also observed in the case of backpropagation. 
7352,SP:a1cb0ca55bc919125f4dad5bcc6e0ad6c2527c1e,"DRL methods USED-FOR neural network policies. learning programmatic policies USED-FOR generalization. input / output state pairs CONJUNCTION expert demonstrations. expert demonstrations CONJUNCTION input / output state pairs. decision trees CONJUNCTION state machines. state machines CONJUNCTION decision trees. state machines CONJUNCTION predefined program templates. predefined program templates CONJUNCTION state machines. expert demonstrations HYPONYM-OF supervision. predefined program templates HYPONYM-OF limited policy representations. input / output state pairs HYPONYM-OF supervision. state machines HYPONYM-OF limited policy representations. limited policy representations USED-FOR works. decision trees HYPONYM-OF limited policy representations. supervision USED-FOR works. framework USED-FOR program. program embedding space USED-FOR program. framework USED-FOR task - solving programs. framework COMPARE DRL and program synthesis baselines. DRL and program synthesis baselines COMPARE framework. methods USED-FOR program embedding. Method are deep reinforcement learning ( DRL ) methods, and two - stage learning scheme. Generic is task. OtherScientificTerm are reward signals, and programs. ","This paper proposes a two-stage learning scheme for learning programmatic policies. The first stage learns a program embedding space, which is then used to learn a set of state machines, decision trees, and expert demonstrations. The second stage learns an embedding for each state machine, decision tree, expert demonstrations, and input/output state pairs. Experiments show that the proposed method outperforms the baselines.","This paper proposes a two-stage learning scheme for learning programmatic policies. The first stage learns a program embedding space, which is then used to learn a set of state machines, decision trees, and expert demonstrations. The second stage learns an embedding for each state machine, decision tree, expert demonstrations, and input/output state pairs. Experiments show that the proposed method outperforms the baselines."
7368,SP:4be92f235a78f030c4f09c920dc41eab0ba69aa8,"scientific machine learning USED-FOR physicsinformed neural network ( PINN ) models. machine learning methodologies USED-FOR model. soft constraints FEATURE-OF empirical loss function. soft constraints FEATURE-OF physical domain knowledge. physical domain knowledge USED-FOR approach. PINN methodologies USED-FOR models. they USED-FOR physical phenomena. soft regularization USED-FOR subtle problems. soft regularization USED-FOR PINNs. PDE - based differential operators PART-OF soft regularization. PINN ’s setup USED-FOR loss landscape. solutions USED-FOR failure modes. curriculum regularization USED-FOR PINN ’s loss term. curriculum regularization USED-FOR approach. PDE regularization USED-FOR PINN ’s loss term. approach USED-FOR problem. sequence - to - sequence learning task USED-FOR problem. methods COMPARE regular PINN training. regular PINN training COMPARE methods. Task are physical interest, and learning differential equations. OtherScientificTerm is differential equations. Method are NN architecture, and NN. ","This paper proposes a method for physics-informed neural network (PINN) training. The method is based on the idea of curriculum regularization, which is used to regularize the empirical loss function of PINNs. The authors show that the proposed method is able to achieve state-of-the-art performance on a sequence-to-sequence learning task. ","This paper proposes a method for physics-informed neural network (PINN) training. The method is based on the idea of curriculum regularization, which is used to regularize the empirical loss function of PINNs. The authors show that the proposed method is able to achieve state-of-the-art performance on a sequence-to-sequence learning task. "
7384,SP:cfd501bca783590a78305f0592f537e8f20bce27,"domaininvariant representations USED-FOR domain shift. domaininvariant representations USED-FOR unsupervised domain adaptation ( UDA ). self - training USED-FOR UDA. self - training USED-FOR unlabeled target data. Cycle Self - Training ( CST ) HYPONYM-OF self - training algorithm. CST USED-FOR target pseudo - labels. source - trained classifier USED-FOR CST. source - trained classifier USED-FOR target pseudo - labels. shared representations USED-FOR classifier. CST USED-FOR classifier. target pseudo - labels USED-FOR CST. target pseudo - labels USED-FOR classifier. Tsallis entropy USED-FOR confidence - friendly regularization. invariant feature learning CONJUNCTION vanilla self - training. vanilla self - training CONJUNCTION invariant feature learning. CST COMPARE state - of - the - arts. state - of - the - arts COMPARE CST. visual recognition and sentiment analysis benchmarks EVALUATE-FOR state - of - the - arts. visual recognition and sentiment analysis benchmarks EVALUATE-FOR CST. OtherScientificTerm are hardness or impossibility theorems, distributional shift, and pseudo - labels. Generic is forward step. ","This paper proposes Cycle Self-Training (CST), a self-training method for unsupervised domain adaptation (UDA). The proposed method is based on the idea that the source-trained classifier can be used to learn target pseudo-labels, and the target pseudo labels can be shared between the source and target classifiers. The authors show that the proposed method outperforms existing methods on several benchmark datasets. ","This paper proposes Cycle Self-Training (CST), a self-training method for unsupervised domain adaptation (UDA). The proposed method is based on the idea that the source-trained classifier can be used to learn target pseudo-labels, and the target pseudo labels can be shared between the source and target classifiers. The authors show that the proposed method outperforms existing methods on several benchmark datasets. "
7400,SP:af51c83f2f16cbbd4eb087adb978d7dc1c2d7d76,"unsupervised representation learning CONJUNCTION structured network pruning. structured network pruning CONJUNCTION unsupervised representation learning. deep learning USED-FOR compact representations of features. neural network USED-FOR compact representations of features. DiscriminAtive Masking ( DAM ) HYPONYM-OF single - stage structured pruning method. graph representation learning CONJUNCTION structured pruning. structured pruning CONJUNCTION graph representation learning. recommendation system CONJUNCTION graph representation learning. graph representation learning CONJUNCTION recommendation system. dimensionality reduction CONJUNCTION recommendation system. recommendation system CONJUNCTION dimensionality reduction. structured pruning USED-FOR image classification. representation learning CONJUNCTION structured pruning. structured pruning CONJUNCTION representation learning. applications USED-FOR structured pruning. applications USED-FOR representation learning. applications EVALUATE-FOR DAM approach. graph representation learning HYPONYM-OF applications. dimensionality reduction HYPONYM-OF applications. recommendation system HYPONYM-OF applications. learning objective FEATURE-OF DAM. Generic are state - of - the - art methods, and systematic approach. Method are fine - tuning, and dam - pytorch. OtherScientificTerm is masking layer. ","This paper proposes a single-stage structured pruning method, called DiscriminAtive Masking (DAM), which is based on the idea of discretizing the masking layer of a deep neural network. The authors show that the proposed method is able to achieve state-of-the-art performance in a number of applications, including graph representation learning, graph pruning, and dimensionality reduction. The paper also shows that DAM can be applied to the problem of fine-tuning the pruning process.","This paper proposes a single-stage structured pruning method, called DiscriminAtive Masking (DAM), which is based on the idea of discretizing the masking layer of a deep neural network. The authors show that the proposed method is able to achieve state-of-the-art performance in a number of applications, including graph representation learning, graph pruning, and dimensionality reduction. The paper also shows that DAM can be applied to the problem of fine-tuning the pruning process."
7416,SP:f831d25830efa88434b43e900241a5ad81119360,"compositional reasoning CONJUNCTION reuse of knowledge. reuse of knowledge CONJUNCTION compositional reasoning. they USED-FOR systematic generalization. architecture USED-FOR inference. self - attention network USED-FOR inference. functions HYPONYM-OF system of modules. architecture USED-FOR capacity extension. architecture USED-FOR computation. image classification CONJUNCTION visual abstract reasoning. visual abstract reasoning CONJUNCTION image classification. settings EVALUATE-FOR Neural Interpreters. settings EVALUATE-FOR it. image classification EVALUATE-FOR it. Raven Progressive Matrices USED-FOR visual abstract reasoning. visual abstract reasoning HYPONYM-OF settings. image classification HYPONYM-OF settings. Neural Interpreters COMPARE vision transformer. vision transformer COMPARE Neural Interpreters. former EVALUATE-FOR Neural Interpreters. Neural Interpreters COMPARE state - of - the - art. state - of - the - art COMPARE Neural Interpreters. latter EVALUATE-FOR Neural Interpreters. systematic generalization EVALUATE-FOR state - of - the - art. systematic generalization EVALUATE-FOR Neural Interpreters. Method is neural network architectures. Generic are model, and task. ","This paper proposes a new architecture for neural network architectures, called Neural Interpreters, which is based on the Raven Progressive Matrices (RPM). The authors propose a self-attention network to perform inference and capacity extension. The authors show that the proposed architecture outperforms state-of-the-art models in a variety of tasks, including image classification, compositional reasoning, and visual abstract reasoning. ","This paper proposes a new architecture for neural network architectures, called Neural Interpreters, which is based on the Raven Progressive Matrices (RPM). The authors propose a self-attention network to perform inference and capacity extension. The authors show that the proposed architecture outperforms state-of-the-art models in a variety of tasks, including image classification, compositional reasoning, and visual abstract reasoning. "
7432,SP:b78c78fd0b10a94466c049e97c59a56ea5455df6,"fine - tuning strategies USED-FOR transfer. transfer USED-FOR challenging domains. fine - tuning strategies USED-FOR challenging domains. pre - trained policies USED-FOR exploration. Behavior Transfer ( BT ) HYPONYM-OF technique. pre - trained policies USED-FOR technique. intrinsic motivation 10 objectives USED-FOR complex behaviors. large - scale pre - training CONJUNCTION intrinsic motivation 10 objectives. intrinsic motivation 10 objectives CONJUNCTION large - scale pre - training. BT CONJUNCTION fine - tuning strategies. fine - tuning strategies CONJUNCTION BT. BT USED-FOR pre - trained 11 policies. Generic is it. Task are reinforcement learning, unsupervised pre - training phase, reinforcement learning problem, and structured exploration. Method is fine3 tuning neural network weights. OtherScientificTerm are rewards, neural network weights, pre - training, and pre - trained 15 policies. Material is supervised domains. ","This paper studies the problem of behavior transfer (BT) in reinforcement learning. The authors propose an unsupervised pre-training phase, where a pre-trained policy is fine-tuned to be more transferable to a new task. They show that the proposed method is able to improve the transfer performance of the learned policies on a variety of tasks, including reinforcement learning, reinforcement learning with intrinsic motivation, and reinforcement learning without intrinsic motivation. They also show that their method can improve the performance of pre-learned policies on tasks that are difficult to transfer. ","This paper studies the problem of behavior transfer (BT) in reinforcement learning. The authors propose an unsupervised pre-training phase, where a pre-trained policy is fine-tuned to be more transferable to a new task. They show that the proposed method is able to improve the transfer performance of the learned policies on a variety of tasks, including reinforcement learning, reinforcement learning with intrinsic motivation, and reinforcement learning without intrinsic motivation. They also show that their method can improve the performance of pre-learned policies on tasks that are difficult to transfer. "
7448,SP:8f0eb77f64b185627b7a82005e0b9e368197c8cd,"machine learning approaches USED-FOR ranking. performance metrics of interest CONJUNCTION surrogate loss functions. surrogate loss functions CONJUNCTION performance metrics of interest. gradient - based methods USED-FOR surrogate loss functions. sorting operation PART-OF ranking metrics. ranking metrics EVALUATE-FOR surrogates. differentiable surrogates USED-FOR ranking. PiRank HYPONYM-OF differentiable surrogates. continuous, temperature - controlled relaxation USED-FOR sorting operator. NeuralSort USED-FOR continuous, temperature - controlled relaxation. continuous, temperature - controlled relaxation USED-FOR PiRank. continuous, temperature - controlled relaxation USED-FOR differentiable surrogates. NeuralSort USED-FOR differentiable surrogates. PiRank USED-FOR metrics. PiRank COMPARE approaches. approaches COMPARE PiRank. OtherScientificTerm are model parameters, and zero temperature. Task are real - world applications, and training. Method is divideand - conquer extension. ","This paper proposes a differentiable surrogate loss function for differentiable ranking metrics. The proposed method, PiRank, is based on NeuralSort, which is a continuous-temperature-controlled relaxation of the sorting operator. The authors show that the proposed method outperforms existing gradient-based methods on a number of different ranking metrics, including CIFAR-10, CIFar-100, and FashionMNIST. ","This paper proposes a differentiable surrogate loss function for differentiable ranking metrics. The proposed method, PiRank, is based on NeuralSort, which is a continuous-temperature-controlled relaxation of the sorting operator. The authors show that the proposed method outperforms existing gradient-based methods on a number of different ranking metrics, including CIFAR-10, CIFar-100, and FashionMNIST. "
7464,SP:c4d1c99a2d53e90336c7e110738bc1eb8a38f3b4,"they USED-FOR near - term quantum devices. machine learning USED-FOR problem. methods USED-FOR VQE structure optimization. reinforcement learning algorithm USED-FOR ansatzes. reinforcement learning algorithm USED-FOR economic circuits. feedback - driven curriculum learning method USED-FOR learning problem. complexity EVALUATE-FOR learning problem. it USED-FOR circuit depth. feedback - driven curriculum learning method USED-FOR algorithm. chemical accuracy EVALUATE-FOR benchmark problem. Task are Variational Quantum Eigensolvers ( VQEs ), and optimization of the VQE ansatz. OtherScientificTerm are variational ansatz, near - term restrictions, VQE ansatz, low depth, and ground energy estimates. Method is learning algorithm. ",This paper proposes a reinforcement learning algorithm for the optimization of variational quantum entanglement (VQE) ansatz. The authors propose a curriculum learning method to learn a learning algorithm to learn the depth of the VQE ansatz for near-term quantum devices. The proposed method is evaluated on a number of benchmarks and compared to existing methods. ,This paper proposes a reinforcement learning algorithm for the optimization of variational quantum entanglement (VQE) ansatz. The authors propose a curriculum learning method to learn a learning algorithm to learn the depth of the VQE ansatz for near-term quantum devices. The proposed method is evaluated on a number of benchmarks and compared to existing methods. 
7480,SP:4fc9a0b34192e1b3587c8e2128851c6aebddd26b,"Transductive inference USED-FOR few - shot learning. it COMPARE inductive counterpart. inductive counterpart COMPARE it. statistics of the unlabeled query set USED-FOR few - shot task. it USED-FOR few - shot task. statistics of the unlabeled query set USED-FOR it. class - balanced tasks USED-FOR inference. few - shot benchmarks EVALUATE-FOR inference. class - balanced tasks FEATURE-OF few - shot benchmarks. arbitrary and unknown label marginals FEATURE-OF unlabeled query sets. few - shot tasks USED-FOR inference. arbitrary class distributions FEATURE-OF few - shot tasks. arbitrary class distributions USED-FOR inference. Dirichlet - distributed random variables USED-FOR marginal probabilities. arbitrary class distributions FEATURE-OF testing tasks. α - divergences USED-FOR mutual - information loss. transductive α - divergence optimization COMPARE state - of - the - art methods. state - of - the - art methods COMPARE transductive α - divergence optimization. OtherScientificTerm are artificial regularity, marginal label probability, uniform distribution, class - balance artefact, simplex, class - distribution variations, and few - shot settings. Method are transductive methods, and inductive methods. ","This paper studies the problem of few-shot inference in the context of Dirichlet-distributed random variables. In particular, the authors consider the case where the class-balance of the data distribution is not uniform and the marginal probabilities of the query set are not known. The authors propose a new method for learning the marginal probability of the unlabeled query set. The proposed method is based on the idea of mutual information loss, which is the mutual information between the class distribution and the label distribution. They show that the proposed method outperforms the state-of-the-art methods on a number of benchmark tasks.","This paper studies the problem of few-shot inference in the context of Dirichlet-distributed random variables. In particular, the authors consider the case where the class-balance of the data distribution is not uniform and the marginal probabilities of the query set are not known. The authors propose a new method for learning the marginal probability of the unlabeled query set. The proposed method is based on the idea of mutual information loss, which is the mutual information between the class distribution and the label distribution. They show that the proposed method outperforms the state-of-the-art methods on a number of benchmark tasks."
7496,SP:eb760d20f3820827c41358ff191d22f4fb78847e,"imbalanced memory distribution FEATURE-OF convolutional neural network ( CNN ) designs. overlapping patches CONJUNCTION computation overhead. computation overhead CONJUNCTION overlapping patches. receptive field redistribution USED-FOR receptive field. receptive field redistribution USED-FOR FLOPs. receptive field redistribution USED-FOR computation overhead. neural architecture CONJUNCTION inference scheduling. inference scheduling CONJUNCTION neural architecture. peak memory usage EVALUATE-FOR networks. neural networks USED-FOR MCUNetV2. Patch - based inference USED-FOR networks. visual wake words dataset EVALUATE-FOR accuracy. MCU EVALUATE-FOR MCUNetV2. ImageNet accuracy EVALUATE-FOR MCUNetV2. peak memory usage EVALUATE-FOR Patch - based inference. accuracy EVALUATE-FOR MCUNetV2. visual wake words dataset EVALUATE-FOR MCUNetV2. MCUNetV2 USED-FOR object detection. tiny devices USED-FOR object detection. mAP EVALUATE-FOR MCUNetV2. memory bottleneck PART-OF tinyML. image classification HYPONYM-OF vision applications. Task are Tiny deep learning, and Manually redistributing the receptive field. OtherScientificTerm are limited memory size, feature map, and peak memory. Metric is memory usage. Generic is network. Method are generic patch - by - patch inference scheduling, naive implementation, and neural architecture search. Material is Pascal VOC. ","This paper proposes a new method for patch-by-patch inference for tiny deep learning. The method is based on the idea of patch-based inference, which is an extension of the previous work on patch by patch inference. The main contribution of the paper is to propose a new algorithm for patch based inference, called MCUNetV2. The authors show that the proposed method is able to reduce the number of FLOPs and the memory usage of the network. The paper also shows that the method can be applied to a variety of applications such as image classification and object detection. ","This paper proposes a new method for patch-by-patch inference for tiny deep learning. The method is based on the idea of patch-based inference, which is an extension of the previous work on patch by patch inference. The main contribution of the paper is to propose a new algorithm for patch based inference, called MCUNetV2. The authors show that the proposed method is able to reduce the number of FLOPs and the memory usage of the network. The paper also shows that the method can be applied to a variety of applications such as image classification and object detection. "
7512,SP:b147639f58dd3197beb928c609d636e853c6bdd6,"unstructured dynamic environments FEATURE-OF Bayesian automated mechanism design. optimal mechanism USED-FOR principal ’s utility. algorithm USED-FOR optimal mechanisms. linear program formulation USED-FOR algorithm. constant factor FEATURE-OF principal ’s optimal utility. unstructured environments FEATURE-OF automated dynamic mechanism design. time complexity EVALUATE-FOR algorithm. optimality CONJUNCTION computational tractability. computational tractability CONJUNCTION optimality. solution USED-FOR problem. Markov decision processes USED-FOR memoryless mechanisms. algorithms USED-FOR synthetic dynamic environments. algorithms USED-FOR algorithms. OtherScientificTerm are self - interested strategic agent, payments, individual - rationality constraints, time horizon, and strategic behavior. Task is dynamic mechanism design. ","This paper studies the problem of Bayesian automated mechanism design in unstructured dynamic environments. The authors propose a linear program formulation of the problem, which is based on the idea that the optimal mechanism is a function of the principal’s utility and the time horizon. They show that the algorithm is computationally tractable and time-efficient. They also provide a theoretical analysis of the algorithm.","This paper studies the problem of Bayesian automated mechanism design in unstructured dynamic environments. The authors propose a linear program formulation of the problem, which is based on the idea that the optimal mechanism is a function of the principal’s utility and the time horizon. They show that the algorithm is computationally tractable and time-efficient. They also provide a theoretical analysis of the algorithm."
7528,SP:1c9c01a77aee3bf00e33bffd6be9ec49d2e5ba29,Graph Neural Networks ( GNNs ) architectures USED-FOR tasks. Neural Architecture Search ( NAS ) USED-FOR GNN architectures. GNN architectures USED-FOR tasks. Neural Architecture Search ( NAS ) COMPARE manually designed architectures. manually designed architectures COMPARE Neural Architecture Search ( NAS ). Neural Architecture Search ( NAS ) USED-FOR tasks. NAS USED-FOR GNN architectures. NAS USED-FOR GNN structures. gradient based NAS methods USED-FOR architectures. gradient based NAS USED-FOR searching suboptimal GNN architectures. graph structure learning USED-FOR search procedure. graph structure learning USED-FOR denoising process. denoising process USED-FOR search procedure. Structure Optimization ( GASSO ) USED-FOR Graph differentiable Architecture Search model. graph structure learning USED-FOR graph neural architectures. real - world graph datasets EVALUATE-FOR GASSO model. GASSO model COMPARE baselines. baselines COMPARE GASSO model. real - world graph datasets EVALUATE-FOR baselines. OtherScientificTerm is graph. Method is gradient descent. ,"This paper proposes a new method for searching suboptimal GNN architectures. The proposed method, called GASSO, is based on graph structure optimization (GASSO) and denoising (GND) to find the best GNN architecture for each node in a graph. The method is evaluated on several benchmark datasets and shows promising results. ","This paper proposes a new method for searching suboptimal GNN architectures. The proposed method, called GASSO, is based on graph structure optimization (GASSO) and denoising (GND) to find the best GNN architecture for each node in a graph. The method is evaluated on several benchmark datasets and shows promising results. "
7544,SP:1ff7a4f6f2ef647b7a9d224f8250b46b7935359a,"Clustering HYPONYM-OF unsupervised learning problem. metric space FEATURE-OF clusters. fairness FEATURE-OF algorithm. cost FEATURE-OF clustering objective. upper bound USED-FOR clustering problem. upper bound USED-FOR constraint. constraint USED-FOR clustering problem. upper bound USED-FOR clustering objective. it USED-FOR equality of representation. group utilitarian objective CONJUNCTION group egalitarian objective. group egalitarian objective CONJUNCTION group utilitarian objective. group egalitarian objective CONJUNCTION group leximin objective. group leximin objective CONJUNCTION group egalitarian objective. group leximin objective CONJUNCTION group egalitarian objective. group egalitarian objective CONJUNCTION group leximin objective. group leximin objective HYPONYM-OF fairness objectives. group egalitarian objective HYPONYM-OF fairness objectives. group utilitarian objective HYPONYM-OF fairness objectives. algorithms USED-FOR them. lower bounds USED-FOR approximation of the utilitarian and egalitarian objectives. heuristic algorithm USED-FOR leximin objective. impossibility results USED-FOR natural fairness objectives. real - world datasets EVALUATE-FOR algorithms. Method is fair clustering. OtherScientificTerm are group membership, group fairness, and utilitarian and egalitarian objectives. Generic is model. ","This paper studies the problem of fair clustering, which is an unsupervised learning problem where the goal is to learn a clustering model that is fair to all groups. The authors propose a new objective for the problem, called the fairness objective. The fairness objective is an upper bound on the cost of the clustering objective, and the authors show that it is equivalent to the group egalitarian objective and the group leximin objective. They also provide lower bounds for the group utilitarian and egalitarian objectives, and show that they can be approximated by a heuristic algorithm. ","This paper studies the problem of fair clustering, which is an unsupervised learning problem where the goal is to learn a clustering model that is fair to all groups. The authors propose a new objective for the problem, called the fairness objective. The fairness objective is an upper bound on the cost of the clustering objective, and the authors show that it is equivalent to the group egalitarian objective and the group leximin objective. They also provide lower bounds for the group utilitarian and egalitarian objectives, and show that they can be approximated by a heuristic algorithm. "
7560,SP:581faa3e1fd39ddefc2740985fa8f94cacdf2b64,"neural - network - based graph generative models USED-FOR real - world network characteristics. high triangle density HYPONYM-OF real - world network characteristics. variational graph autoencoders CONJUNCTION CELL. CELL CONJUNCTION variational graph autoencoders. NetGAN CONJUNCTION variational graph autoencoders. variational graph autoencoders CONJUNCTION NetGAN. Erdös - Rényi and stochastic block models CONJUNCTION generative models. generative models CONJUNCTION Erdös - Rényi and stochastic block models. CELL HYPONYM-OF generative models. NetGAN HYPONYM-OF generative models. variational graph autoencoders HYPONYM-OF generative models. generative models PART-OF models. Erdös - Rényi and stochastic block models PART-OF models. edge independent models USED-FOR graphs. real - world social networks CONJUNCTION graphs. graphs CONJUNCTION real - world social networks. generative model USED-FOR graph statistics. overlap CONJUNCTION accuracy. accuracy CONJUNCTION overlap. overlap EVALUATE-FOR generative model. accuracy EVALUATE-FOR generative model. Method is edge independent random graph models. OtherScientificTerm are graph, and bounded overlap condition. Generic is model. ","This paper studies the problem of learning graph generative models for graph statistics. The authors propose to use edge-independent random graph models (e.g., CELL and NetGAN) to generate graphs with high triangle density and high accuracy. They show that such models can be used to learn graph statistics for real-world graphs. They also show that these models are able to learn graphs with bounded overlap. ","This paper studies the problem of learning graph generative models for graph statistics. The authors propose to use edge-independent random graph models (e.g., CELL and NetGAN) to generate graphs with high triangle density and high accuracy. They show that such models can be used to learn graph statistics for real-world graphs. They also show that these models are able to learn graphs with bounded overlap. "
7576,SP:0d77c22df0830cb675b11ad883d014e3a1933c8e,"backpropagation CONJUNCTION training. training CONJUNCTION backpropagation. ReLU′(0 ) USED-FOR neural network. ReLU′(0 ) USED-FOR backpropagation. VGG CONJUNCTION ResNet. ResNet CONJUNCTION VGG. MNIST CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION MNIST. CIFAR10 CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR10. SVHN CONJUNCTION ImageNet. ImageNet CONJUNCTION SVHN. networks CONJUNCTION datasets. datasets CONJUNCTION networks. ReLU′(0 ) USED-FOR precision levels. ImageNet HYPONYM-OF datasets. MNIST HYPONYM-OF datasets. CIFAR10 HYPONYM-OF datasets. SVHN HYPONYM-OF datasets. ResNet HYPONYM-OF networks. VGG HYPONYM-OF networks. test accuracy EVALUATE-FOR ReLU′(0 ) = 1. batch - norm CONJUNCTION ADAM. ADAM CONJUNCTION batch - norm. batch - norm HYPONYM-OF reconditioning approaches. ADAM HYPONYM-OF reconditioning approaches. OtherScientificTerm are default precision, deep learning problems, backpropagation outputs, and double precision. Method are training methods, and vanilla SGD training. Task is algorithmic differentiation of nonsmooth problems. ","This paper studies the problem of backpropagation in deep learning. The authors propose a new metric called ReLU′(0) to measure the precision of a neural network trained with vanilla SGD. They show that this metric is a good measure of the accuracy of the network, and that it can be used to compare the performance of different training methods on different datasets. They also show that the ReLU’(0)-norm can be applied to batch-norm and ADAM.","This paper studies the problem of backpropagation in deep learning. The authors propose a new metric called ReLU′(0) to measure the precision of a neural network trained with vanilla SGD. They show that this metric is a good measure of the accuracy of the network, and that it can be used to compare the performance of different training methods on different datasets. They also show that the ReLU’(0)-norm can be applied to batch-norm and ADAM."
7592,SP:73e6281bf556a6ae92bdcf8d68e6e8973bc8b56b,"generalization CONJUNCTION transfer. transfer CONJUNCTION generalization. robustness CONJUNCTION generalization. generalization CONJUNCTION robustness. transfer CONJUNCTION computational efficiency. computational efficiency CONJUNCTION transfer. minimizing information USED-FOR supervised learning setting. method USED-FOR policies. RPC ) USED-FOR policies. method USED-FOR RPC ). information bottlenecks CONJUNCTION model - based RL. model - based RL CONJUNCTION information bottlenecks. model - based RL CONJUNCTION bits - back coding. bits - back coding CONJUNCTION model - based RL. latent - space model CONJUNCTION policy. policy CONJUNCTION latent - space model. method USED-FOR policy. method USED-FOR latent - space model. method COMPARE prior methods. prior methods COMPARE method. reward EVALUATE-FOR information bottleneck. compression EVALUATE-FOR prior methods. method COMPARE information bottleneck. information bottleneck COMPARE method. compression EVALUATE-FOR method. reward EVALUATE-FOR method. compression USED-FOR policies. method USED-FOR policies. Method are reinforcement learning ( RL ) algorithms, and RL algorithms. Task is RL setting. OtherScientificTerm are past information, and decision making. ","This paper proposes a method for minimizing the information bottleneck in reinforcement learning (RL) algorithms. The idea is to learn a policy that maximizes the mutual information between the current state and the past state, and then use this information to optimize the reward function. The authors show that this method can be applied to both model-based RL and bit-back coding. They also show that the proposed method outperforms prior methods in terms of transfer and generalization.","This paper proposes a method for minimizing the information bottleneck in reinforcement learning (RL) algorithms. The idea is to learn a policy that maximizes the mutual information between the current state and the past state, and then use this information to optimize the reward function. The authors show that this method can be applied to both model-based RL and bit-back coding. They also show that the proposed method outperforms prior methods in terms of transfer and generalization."
7608,SP:dff08f0b290f3d138fd0299933052f3dc363b2d3,"Transformer architecture USED-FOR sequence processing. graphs HYPONYM-OF data structures. node PART-OF graph. full Laplacian spectrum USED-FOR learned positional encoding ( LPE ). learned positional encoding ( LPE ) USED-FOR Spectral Attention Network ( SAN ). LPE PART-OF node features. LPE USED-FOR fully - connected Transformer. node features PART-OF graph. model USED-FOR similar sub - structures. model USED-FOR distinguishing graphs. heat transfer CONJUNCTION electric interaction. electric interaction CONJUNCTION heat transfer. Transformer USED-FOR modeling of physical phenomenons. over - squashing HYPONYM-OF information bottleneck. information bottleneck PART-OF GNNs. electric interaction HYPONYM-OF modeling of physical phenomenons. heat transfer HYPONYM-OF modeling of physical phenomenons. datasets EVALUATE-FOR model. model COMPARE state - of - theart GNNs. state - of - theart GNNs COMPARE model. model COMPARE attention - based model. attention - based model COMPARE model. graph benchmarks EVALUATE-FOR fully - connected architecture. graph benchmarks EVALUATE-FOR attention - based model. OtherScientificTerm are Laplacian, resonance, and physical phenomenons. ","This paper proposes a new transformer architecture for sequence processing. The proposed architecture is based on the Spectral Attention Network (SAN). The proposed model is able to learn the full Laplacian spectrum of the node features, which is then used to learn positional encoding (LPE) for node features. Experiments show that the proposed model outperforms state-of-the-art GNNs on several graph benchmarks.","This paper proposes a new transformer architecture for sequence processing. The proposed architecture is based on the Spectral Attention Network (SAN). The proposed model is able to learn the full Laplacian spectrum of the node features, which is then used to learn positional encoding (LPE) for node features. Experiments show that the proposed model outperforms state-of-the-art GNNs on several graph benchmarks."
7624,SP:f2bee0c4a6c558970538b422e5e36750447cd9bc,"Task is two - alternative elections. OtherScientificTerm are state variable, private information, and private signals. Method is Bayes Nash equilibrium. ","This paper studies the problem of two-alternate elections. The authors consider the case where the state variable is private and the private information is not available to the public. They show that the Bayes Nash equilibrium between the public and private state variables is a Bayesian Nash equilibrium. They also show that under certain assumptions, the private state variable can be represented as a function of the public state variable. They further show that this is the case when the public information is available to both the private and public parties.","This paper studies the problem of two-alternate elections. The authors consider the case where the state variable is private and the private information is not available to the public. They show that the Bayes Nash equilibrium between the public and private state variables is a Bayesian Nash equilibrium. They also show that under certain assumptions, the private state variable can be represented as a function of the public state variable. They further show that this is the case when the public information is available to both the private and public parties."
7640,SP:0823bd0dbb8045648e81a4c93e9782069cf2c605,"Hessian USED-FOR parameter interactions. Hessian FEATURE-OF neural network. secondorder derivatives of the loss USED-FOR Hessian. secondorder derivatives of the loss USED-FOR parameter interactions. model design CONJUNCTION optimization. optimization CONJUNCTION model design. optimization CONJUNCTION generalization. generalization CONJUNCTION optimization. model design HYPONYM-OF deep learning. optimization HYPONYM-OF deep learning. low - rank approximations CONJUNCTION heuristics. heuristics CONJUNCTION low - rank approximations. theoretical tools USED-FOR Hessian map. Hessian rank FEATURE-OF deep linear networks. rectified and hyperbolic tangent networks HYPONYM-OF models. model architecture USED-FOR rank deficiency. Generic are It, and bounds. OtherScientificTerm are network structure, and numerical Hessian rank. Method is overparameterized neural networks. ","This paper studies the problem of rank deficiency in deep linear networks. The authors consider the case where the Hessian of a neural network is over-parameterized. They show that the rank deficiency is a function of the number of parameters in the network, which is a generalization property of the network. They also provide a theoretical analysis of the problem. ","This paper studies the problem of rank deficiency in deep linear networks. The authors consider the case where the Hessian of a neural network is over-parameterized. They show that the rank deficiency is a function of the number of parameters in the network, which is a generalization property of the network. They also provide a theoretical analysis of the problem. "
7656,SP:24cdcb12fca34680d8b34bc61c51b9003368228a,metric USED-FOR LSBD. metric EVALUATE-FOR LSBD methods. metric USED-FOR LSBD. semi - supervised 6 method USED-FOR LSBD representations. LSBD - VAE HYPONYM-OF semi - supervised 6 method. metric USED-FOR LSBD - VAE. LSBD - VAE USED-FOR LSBD representations. methods USED-FOR LSBD representations. LSBD - VAE CONJUNCTION methods. methods CONJUNCTION LSBD - VAE. common VAE - based disentanglement methods USED-FOR LSBD representations. disentanglement metrics EVALUATE-FOR LSBD representations. limited supervision USED-FOR methods. limited supervision USED-FOR LSBD representations. OtherScientificTerm is Linear Symmetry - Based Disentanglement ( LSBD ). Method is linearly disentangled representations. Task is disentanglement. Metric is DLSBD. ,"This paper proposes a new disentangled representation metric for linear symmetry-based disentanglement (LSBD). The proposed metric is based on the VAE-based VAE, which is a semi-supervised 6-layer VAE method. The authors show that the proposed metric can be used to evaluate the performance of LSBD-VAE and other VAE based disentangling methods. The paper also provides theoretical analysis of the proposed method.","This paper proposes a new disentangled representation metric for linear symmetry-based disentanglement (LSBD). The proposed metric is based on the VAE-based VAE, which is a semi-supervised 6-layer VAE method. The authors show that the proposed metric can be used to evaluate the performance of LSBD-VAE and other VAE based disentangling methods. The paper also provides theoretical analysis of the proposed method."
7672,SP:374bfeb067fcea966c97e1721d65cd9d03d26ed3,Deep state - space models ( DSSMs ) USED-FOR temporal predictions. dynamics of observed sequence data USED-FOR Deep state - space models ( DSSMs ). dynamics of observed sequence data USED-FOR temporal predictions. evidence lower bound USED-FOR They. model USED-FOR dynamics. approach USED-FOR DSSMs. constrained optimisation framework USED-FOR DSSMs. constrained optimisation framework USED-FOR approach. amortised variational inference CONJUNCTION Bayesian filtering / smoothing. Bayesian filtering / smoothing CONJUNCTION amortised variational inference. extended Kalman VAE ( EKVAE ) COMPARE RNN - based DSSMs. RNN - based DSSMs COMPARE extended Kalman VAE ( EKVAE ). amortised variational inference PART-OF extended Kalman VAE ( EKVAE ). Bayesian filtering / smoothing PART-OF extended Kalman VAE ( EKVAE ). constrained optimisation framework USED-FOR DSSMs. system identification and prediction accuracy EVALUATE-FOR DSSMs. system identification and prediction accuracy EVALUATE-FOR constrained optimisation framework. EKVAE COMPARE models. models COMPARE EKVAE. EKVAE USED-FOR dynamical systems. EKVAE USED-FOR state - space representations. static and dynamic features PART-OF state - space representations. prediction accuracy EVALUATE-FOR EKVAE. prediction accuracy EVALUATE-FOR models. Task is maximising the evidence lower bound. ,This paper proposes an extension of the extended Kalman VAE (EKVAE) framework for deep state-space models (DSSMs). The authors propose a constrained optimisation framework for DSSMs that maximizes the evidence lower bound of the proposed model. The proposed method is based on amortised variational inference and Bayesian filtering/smoothing. The authors show that the proposed method outperforms state-of-the-art RNN-based DSSM on both system identification and prediction accuracy. ,This paper proposes an extension of the extended Kalman VAE (EKVAE) framework for deep state-space models (DSSMs). The authors propose a constrained optimisation framework for DSSMs that maximizes the evidence lower bound of the proposed model. The proposed method is based on amortised variational inference and Bayesian filtering/smoothing. The authors show that the proposed method outperforms state-of-the-art RNN-based DSSM on both system identification and prediction accuracy. 
7688,SP:15ed638782cc0398df38ec49eed5c5ca9962d3b9,"Explanation techniques USED-FOR introspecting black - box models. deep inversion approach USED-FOR counterfactual explanations. deep models USED-FOR images. training distribution USED-FOR deep models. deep inversion methods USED-FOR counterfactuals. deep inversion methods USED-FOR conditional image synthesis. DISC USED-FOR Synthesizing Counterfactuals. deep inversion USED-FOR DISC. image priors USED-FOR DISC. counterfactuals USED-FOR classifier decision boundaries. DISC USED-FOR counterfactuals. counterfactuals USED-FOR visually meaningful explanations. Task is model prediction. OtherScientificTerm are discernible changes, data manifold, manifold consistency objective, and unknown test - time corruptions. Method are deep classifier, and progressive optimization strategy. ","This paper proposes a new method for generating counterfactual explanations for black-box models. The method is based on deep inversion (DIC) methods, and is motivated by the observation that the classifier decision boundary can be affected by the training distribution of the model. The authors propose to use the DIC method to generate a set of images that can be used to train a classifier. The proposed method is evaluated on MNIST, CIFAR-10, and ImageNet datasets. ","This paper proposes a new method for generating counterfactual explanations for black-box models. The method is based on deep inversion (DIC) methods, and is motivated by the observation that the classifier decision boundary can be affected by the training distribution of the model. The authors propose to use the DIC method to generate a set of images that can be used to train a classifier. The proposed method is evaluated on MNIST, CIFAR-10, and ImageNet datasets. "
7704,SP:e536acfe82bb5e41fa61929d44dad0b8f7c5ab19,"causal inference problem USED-FOR this. empirical objective USED-FOR algorithm. algorithm USED-FOR region of heterogeneity. algorithm COMPARE baselines. baselines COMPARE algorithm. real - world healthcare datasets EVALUATE-FOR algorithm. OtherScientificTerm are drug - related offenses, inter - decision - maker disagreement, generalization bound, and clinical knowledge. ","This paper studies the problem of generalization in the context of drug-related offenses. The authors propose a novel algorithm for this problem. The algorithm is based on the assumption that the decision-maker is aware of the heterogeneity in the data distribution, and the authors prove a generalization bound for the generalization of the algorithm. ","This paper studies the problem of generalization in the context of drug-related offenses. The authors propose a novel algorithm for this problem. The algorithm is based on the assumption that the decision-maker is aware of the heterogeneity in the data distribution, and the authors prove a generalization bound for the generalization of the algorithm. "
7720,SP:8fa76926a21fb41c5c9fd357246c06a42ae26b9f,"perspective USED-FOR image synthesis. visual token generation problem USED-FOR task. paradigms COMPARE formulation. formulation COMPARE paradigms. flexible local manipulation USED-FOR image regions. formulation USED-FOR flexible local manipulation. latent tokens USED-FOR visual tokens. latent tokens USED-FOR it. constant content tokens CONJUNCTION style tokens. style tokens CONJUNCTION constant content tokens. style tokens PART-OF latent space. visual tokens USED-FOR TokenGAN. constant content tokens HYPONYM-OF visual tokens. style tokens HYPONYM-OF visual tokens. TokenGAN USED-FOR image synthesis. style tokens USED-FOR TokenGAN. Transformer USED-FOR attention mechanism. attention mechanism USED-FOR styles. attention mechanism USED-FOR TokenGAN. FFHQ CONJUNCTION LSUN CHURCH. LSUN CHURCH CONJUNCTION FFHQ. image synthesis benchmarks EVALUATE-FOR TokenGAN. LSUN CHURCH HYPONYM-OF image synthesis benchmarks. FFHQ HYPONYM-OF image synthesis benchmarks. generator USED-FOR high - fidelity images. 1024× 1024 size FEATURE-OF high - fidelity images. OtherScientificTerm are latent code, content tokens, and convolutions. Method is token - based generator. ","This paper proposes a token-based approach for image synthesis. The key idea is to learn a set of visual tokens that can be used to generate high-quality images. The approach is based on a transformer-based architecture, and is evaluated on several image synthesis benchmarks. The paper shows that the proposed approach is able to achieve state-of-the-art results.","This paper proposes a token-based approach for image synthesis. The key idea is to learn a set of visual tokens that can be used to generate high-quality images. The approach is based on a transformer-based architecture, and is evaluated on several image synthesis benchmarks. The paper shows that the proposed approach is able to achieve state-of-the-art results."
7736,SP:fa34d40d07c0f154a69841b241a2743fe721f95c,"min - norm interpolators CONJUNCTION max - margin classifiers. max - margin classifiers CONJUNCTION min - norm interpolators. overparameterization USED-FOR min - norm interpolators. overparameterization USED-FOR variance. variance FEATURE-OF min - norm interpolators. overparameterization USED-FOR max - margin classifiers. ridge regularization USED-FOR high dimensions. generalization EVALUATE-FOR avoiding interpolation. ridge regularization USED-FOR avoiding interpolation. linear regression CONJUNCTION classification. classification CONJUNCTION linear regression. linear regression FEATURE-OF robust risk. classification FEATURE-OF robust risk. OtherScientificTerm are noise, and interpolation. Task is robust overfitting. ",This paper studies the problem of robust overfitting in the presence of noise in the training data. The authors propose a ridge regularization technique to reduce the variance of min-norm interpolators and max-margin classifiers. They show that this technique is effective for both linear regression and linear regression with noise. They also show that the ridge regularisation technique is also effective for linear regression. ,This paper studies the problem of robust overfitting in the presence of noise in the training data. The authors propose a ridge regularization technique to reduce the variance of min-norm interpolators and max-margin classifiers. They show that this technique is effective for both linear regression and linear regression with noise. They also show that the ridge regularisation technique is also effective for linear regression. 
7752,SP:09f080f47db81b513af26add851822c5c32bb94e,"canonical primitive USED-FOR arbitrarily ordered point cloud. sphere HYPONYM-OF canonical primitive. primitive USED-FOR unordered point clouds. unordered point clouds PART-OF canonical surface. annotation CONJUNCTION selfsupervised part segmentation network. selfsupervised part segmentation network CONJUNCTION annotation. method USED-FOR unaligned input point clouds. selfsupervised part segmentation network USED-FOR method. annotation USED-FOR method. rotation range FEATURE-OF unaligned input point clouds. 3D semantic keypoint transfer CONJUNCTION part segmentation transfer. part segmentation transfer CONJUNCTION 3D semantic keypoint transfer. model COMPARE correspondence learning methods. correspondence learning methods COMPARE model. 3D semantic keypoint transfer EVALUATE-FOR model. part segmentation transfer EVALUATE-FOR model. Method are canonical point autoencoder ( CPAE ), and autoencoder. Material is 3D shapes. OtherScientificTerm is primitive surface. Generic is models. ",This paper proposes a canonical point autoencoder (CPAE) for unordered point clouds. The proposed method is based on the idea that the canonical surface of a point cloud can be represented as a sphere. The authors propose a self-supervised part segmentation network and a canonical surface annotation network. Experiments on 3D semantic keypoint transfer and 3D segmentation transfer demonstrate the effectiveness of the proposed method.,This paper proposes a canonical point autoencoder (CPAE) for unordered point clouds. The proposed method is based on the idea that the canonical surface of a point cloud can be represented as a sphere. The authors propose a self-supervised part segmentation network and a canonical surface annotation network. Experiments on 3D semantic keypoint transfer and 3D segmentation transfer demonstrate the effectiveness of the proposed method.
7768,SP:8f28988012f8dca74c90316f7feeda15d49af2c5,"Domain generalization ( DG ) methods USED-FOR generalizability. empirical risk minimization ( ERM ) approach COMPARE methods. methods COMPARE empirical risk minimization ( ERM ) approach. complex, non - convex loss function USED-FOR ERM. sharp minima USED-FOR sub - optimal generalizability. domain generalization gap EVALUATE-FOR flat minima. method USED-FOR flat minima. SWAD COMPARE vanilla SWA. vanilla SWA COMPARE SWAD. SWAD USED-FOR flatter minima. VLCS CONJUNCTION OfficeHome. OfficeHome CONJUNCTION VLCS. OfficeHome CONJUNCTION TerraIncognita. TerraIncognita CONJUNCTION OfficeHome. TerraIncognita CONJUNCTION DomainNet. DomainNet CONJUNCTION TerraIncognita. PACS CONJUNCTION VLCS. VLCS CONJUNCTION PACS. DG benchmarks EVALUATE-FOR SWAD. outof - domain accuracy EVALUATE-FOR SWAD. PACS HYPONYM-OF DG benchmarks. OfficeHome HYPONYM-OF DG benchmarks. DomainNet HYPONYM-OF DG benchmarks. VLCS HYPONYM-OF DG benchmarks. TerraIncognita HYPONYM-OF DG benchmarks. SWAD COMPARE generalization methods. generalization methods COMPARE SWAD. data augmentation and consistency regularization methods HYPONYM-OF generalization methods. SWAD CONJUNCTION DG method. DG method CONJUNCTION SWAD. SWAD USED-FOR DG. DG method USED-FOR DG. SWAD USED-FOR DG methods. Method are DomainBed, and Stochastic Weight Averaging Densely ( SWAD ). OtherScientificTerm is overfitting. Metric is in - domain generalizability. ","This paper proposes a new method for domain generalization (DG) based on Stochastic Weight Averaging Densely (SWAD). SWAD is an empirical risk minimization (ERM) approach that uses a complex, non-convex loss function to reduce the sub-optimal generalizability gap. The authors show that SWAD can achieve flat minima in the out-of-domain generalization gap. SWAD also outperforms vanilla SWA on several DG benchmarks. ","This paper proposes a new method for domain generalization (DG) based on Stochastic Weight Averaging Densely (SWAD). SWAD is an empirical risk minimization (ERM) approach that uses a complex, non-convex loss function to reduce the sub-optimal generalizability gap. The authors show that SWAD can achieve flat minima in the out-of-domain generalization gap. SWAD also outperforms vanilla SWA on several DG benchmarks. "
7784,SP:5068e491ee0ae7282cd98ef966b471389e2ab069,"initialization time CONJUNCTION query time. query time CONJUNCTION initialization time. evaluation metric CONJUNCTION optimization. optimization CONJUNCTION evaluation metric. weight - sharing CONJUNCTION supervised learning. supervised learning CONJUNCTION weight - sharing. supervised learning CONJUNCTION zero - cost proxies. zero - cost proxies CONJUNCTION supervised learning. learning curve extrapolation CONJUNCTION weight - sharing. weight - sharing CONJUNCTION learning curve extrapolation. zero - cost proxies HYPONYM-OF techniques. learning curve extrapolation HYPONYM-OF techniques. supervised learning HYPONYM-OF techniques. weight - sharing HYPONYM-OF techniques. technique USED-FOR predictor - based NAS frameworks. Task are neural architecture search ( NAS ), and performance predictors. Method are neural networks, neural architectures, and performance prediction methods. Metric are correlationand rank - based performance measures, and predictive power. OtherScientificTerm is predictors. Generic is code. ","This paper proposes a new method to improve the predictive power of neural architecture search (NAS) methods. The method is based on the observation that the performance of a predictor-based NAS method can be affected by the initialization time, query time, and optimization time of the predictor. The authors propose a method to reduce the query time and optimize the optimization time for the predictor based NAS method. The proposed method is evaluated on a number of NAS benchmarks and shows that the proposed method outperforms the baselines. ","This paper proposes a new method to improve the predictive power of neural architecture search (NAS) methods. The method is based on the observation that the performance of a predictor-based NAS method can be affected by the initialization time, query time, and optimization time of the predictor. The authors propose a method to reduce the query time and optimize the optimization time for the predictor based NAS method. The proposed method is evaluated on a number of NAS benchmarks and shows that the proposed method outperforms the baselines. "
7800,SP:c883fe9c7f4a5f950340ac79b6d7194278b1a1eb,Dirichlet posterior sampling CONJUNCTION privacy 4 guarantees. privacy 4 guarantees CONJUNCTION Dirichlet posterior sampling. exponential families FEATURE-OF differential privacy of posterior sampling. truncated concentrated differential privacy ( tCDP ) USED-FOR privacy guarantee. privacy guarantee FEATURE-OF Dirichlet posterior sampling. Dirichlet posterior sampling USED-FOR Multinomial8 Dirichlet sampling. Dirichlet posterior sampling USED-FOR private normalized histogram publishing. accuracy guarantees EVALUATE-FOR Dirichlet posterior sampling. Metric is inherent privacy. OtherScientificTerm is Dirichlet posterior 1 distribution. Method is posterior sampling. ,"This paper studies the privacy guarantee of Dirichlet posterior sampling in the context of normalized histogram publishing. In particular, the authors consider the case where the posterior distribution of the histogram is assumed to be discrete and the authors show that the privacy guarantees of posterior sampling are guaranteed by truncated concentrated differential privacy (tCDP). The authors also provide a theoretical analysis of the convergence of tCDP to the exponential family of exponential families. ","This paper studies the privacy guarantee of Dirichlet posterior sampling in the context of normalized histogram publishing. In particular, the authors consider the case where the posterior distribution of the histogram is assumed to be discrete and the authors show that the privacy guarantees of posterior sampling are guaranteed by truncated concentrated differential privacy (tCDP). The authors also provide a theoretical analysis of the convergence of tCDP to the exponential family of exponential families. "
7816,SP:aaea75b9c614f77e8025922780f9a8dd9c9d4aab,clustering CONJUNCTION semi - supervised learning. semi - supervised learning CONJUNCTION clustering. Random walks USED-FOR machine learning algorithms. parallel algorithm USED-FOR random walks. random walks USED-FOR it. random walk USED-FOR algorithm. technique USED-FOR parallel local clustering algorithm. algorithm COMPARE approaches. approaches COMPARE algorithm. Generic is method. OtherScientificTerm is graph. ,"This paper proposes a parallel local clustering algorithm based on random walks. The algorithm is based on the random walk technique, which is used in semi-supervised learning and clustering. The authors show that the proposed algorithm outperforms the baselines in terms of speed and accuracy. The paper also shows that the algorithm is computationally efficient. ","This paper proposes a parallel local clustering algorithm based on random walks. The algorithm is based on the random walk technique, which is used in semi-supervised learning and clustering. The authors show that the proposed algorithm outperforms the baselines in terms of speed and accuracy. The paper also shows that the algorithm is computationally efficient. "
7832,SP:5739081ab7aaf71d389705c28f14a316fbb0a728,statistical mechanics USED-FOR replica method. typical sample complexity EVALUATE-FOR ` 1 - LinR. paramagnetic phase FEATURE-OF random regular graphs. ` 1 - LinR USED-FOR model selection. order of sample complexity EVALUATE-FOR model selection. order of sample complexity EVALUATE-FOR ` 1 - LinR. method USED-FOR nonasymptotic behavior. precision CONJUNCTION recall. recall CONJUNCTION precision. method USED-FOR ` 1 - LinR. nonasymptotic behavior FEATURE-OF ` 1 - LinR. precision HYPONYM-OF nonasymptotic behavior. recall HYPONYM-OF nonasymptotic behavior. ` 1 - LogR CONJUNCTION interaction screening. interaction screening CONJUNCTION ` 1 - LogR. method USED-FOR ` 1 - regularized M -estimators. interaction screening HYPONYM-OF ` 1 - regularized M -estimators. ` 1 - LogR HYPONYM-OF ` 1 - regularized M -estimators. Task is Ising model selection. OtherScientificTerm is model misspecification. Metric is M. Method is Ising model. ,This paper proposes a replica method for model selection in the context of Ising model selection. The proposed method is based on the observation that the paramagnetic phase of random regular graphs is paramagnetic in nature. The authors propose to use this observation to improve the sample complexity of 1-regularized M-estimators. They show that the proposed method outperforms existing methods in terms of sample complexity and nonasymptotic behavior. ,This paper proposes a replica method for model selection in the context of Ising model selection. The proposed method is based on the observation that the paramagnetic phase of random regular graphs is paramagnetic in nature. The authors propose to use this observation to improve the sample complexity of 1-regularized M-estimators. They show that the proposed method outperforms existing methods in terms of sample complexity and nonasymptotic behavior. 
7848,SP:9d6202ab0010166f383d6d064aebe02ae97a1dfc,"k - means USED-FOR datasets. fuzzy or soft k - means objective HYPONYM-OF kmeans problem. framework USED-FOR clustering. similarity queries USED-FOR polynomial - time approximation algorithm. algorithms USED-FOR fuzzy clustering. k - means USED-FOR fuzzy k - means objective. non - negative matrix factorization HYPONYM-OF nonconvex problem. Lloyd - type algorithms CONJUNCTION alternating - minimization algorithms. alternating - minimization algorithms CONJUNCTION Lloyd - type algorithms. similarity queries USED-FOR problem. real - world applications EVALUATE-FOR algorithms. real - world datasets EVALUATE-FOR algorithms. Method is semisupervised active clustering framework. OtherScientificTerm are similarity, O(poly(k ) log n ) similarity queries, and local minima. Metric is polynomialtime - complexity. ","This paper studies the problem of fuzzy clustering, which is a nonconvex optimization problem with a soft k-means objective. The authors propose a new algorithm for solving the problem, which they call active clustering. The algorithm is based on the idea that the objective function is polynomial-time approximated by a similarity query. They show that the algorithm can be solved polynomially in polynome time. They also show that their algorithm is computationally efficient. ","This paper studies the problem of fuzzy clustering, which is a nonconvex optimization problem with a soft k-means objective. The authors propose a new algorithm for solving the problem, which they call active clustering. The algorithm is based on the idea that the objective function is polynomial-time approximated by a similarity query. They show that the algorithm can be solved polynomially in polynome time. They also show that their algorithm is computationally efficient. "
7864,SP:a8057c4708dceb4f934e449080043037a70fabf7,"models USED-FOR planning. value functions CONJUNCTION policies. policies CONJUNCTION value functions. computation USED-FOR policies. computation USED-FOR value functions. model CONJUNCTION value function. value function CONJUNCTION model. approach COMPARE planning methods. planning methods COMPARE approach. Dyna HYPONYM-OF planning methods. self - consistency USED-FOR policy evaluation. self - consistency USED-FOR control. policy evaluation CONJUNCTION control. control CONJUNCTION policy evaluation. tabular and function approximation settings EVALUATE-FOR these. Method are reinforcement learning ( RL ) agents, model - based RL, and self - consistency updates. OtherScientificTerm is environment interactions. ","This paper proposes Dyna, a model-based reinforcement learning (RL) algorithm for planning. Dyna learns a model that learns a value function and a policy, and then updates the value function using a self-consistency update. The authors show that Dyna outperforms the baselines in both tabular and function approximation settings. They also provide a theoretical analysis of the performance of Dyna.","This paper proposes Dyna, a model-based reinforcement learning (RL) algorithm for planning. Dyna learns a model that learns a value function and a policy, and then updates the value function using a self-consistency update. The authors show that Dyna outperforms the baselines in both tabular and function approximation settings. They also provide a theoretical analysis of the performance of Dyna."
7880,SP:cd01ab8f03cfb1bb067478ca82944d3a42826ca4,"Episodic training USED-FOR models. few - shot learning USED-FOR models. Episodic training PART-OF few - shot learning. method USED-FOR episode sampling distributions. difficulty USED-FOR method. curriculum HYPONYM-OF sampling schemes. few - shot learning accuracies EVALUATE-FOR episodic training algorithms. algorithms CONJUNCTION network architectures. network architectures CONJUNCTION algorithms. network architectures CONJUNCTION protocols. protocols CONJUNCTION network architectures. few - shot learning datasets CONJUNCTION algorithms. algorithms CONJUNCTION few - shot learning datasets. network architectures EVALUATE-FOR method. algorithms EVALUATE-FOR method. few - shot learning datasets EVALUATE-FOR method. protocols EVALUATE-FOR method. Material is limited labelled data. Method are episodic training, and sampling method. OtherScientificTerm is episode difficulty. ",This paper proposes a sampling method for episodic training of few-shot learning models. The sampling method is based on the idea that the difficulty of episodic learning is a function of the number of episodes in the training set. The authors propose a curriculum for learning the episodic sampling distribution and show that this curriculum can be applied to a wide range of sampling schemes. The method is evaluated on a variety of benchmarks and shows that it outperforms existing methods. ,This paper proposes a sampling method for episodic training of few-shot learning models. The sampling method is based on the idea that the difficulty of episodic learning is a function of the number of episodes in the training set. The authors propose a curriculum for learning the episodic sampling distribution and show that this curriculum can be applied to a wide range of sampling schemes. The method is evaluated on a variety of benchmarks and shows that it outperforms existing methods. 
7896,SP:b31b1ee7067d4da70916986ba13e80bb14e2fdfe,"logistic bandits USED-FOR binary rewards. logistic bandits HYPONYM-OF ones. ones PART-OF generalized linear bandits. logistic bandits HYPONYM-OF generalized linear bandits. algorithms USED-FOR logistic bandits. unknown parameter FEATURE-OF MNL probabilistic model. MNL - UCB HYPONYM-OF upper confidence bound ( UCB)-based algorithm. MNL - UCB USED-FOR problem. Generic is extension. Method is multinomial logit ( MNL ). OtherScientificTerm are revenue parameter, problemdependent constants, and loose regret bounds. ","This paper studies the problem of learning a multinomial logit (MNL) probabilistic model for logistic bandits. The authors propose a UCB-based algorithm for this problem, which is based on the upper confidence bound (UCB)-based algorithm. The UCB algorithm can be seen as a general extension of the UCB method for learning a multi-modal logit. The main contribution of the paper is to show that UCB can be extended to the case where there is an unknown parameter. The paper also provides a theoretical analysis of the regret bounds of the proposed algorithm.","This paper studies the problem of learning a multinomial logit (MNL) probabilistic model for logistic bandits. The authors propose a UCB-based algorithm for this problem, which is based on the upper confidence bound (UCB)-based algorithm. The UCB algorithm can be seen as a general extension of the UCB method for learning a multi-modal logit. The main contribution of the paper is to show that UCB can be extended to the case where there is an unknown parameter. The paper also provides a theoretical analysis of the regret bounds of the proposed algorithm."
7912,SP:0eaf058ed224464f6682cbbd80f716c89759f467,max - min entropy framework USED-FOR reinforcement learning ( RL ). maximum entropy RL USED-FOR model - free sample - based learning. soft actor - critic ( SAC ) algorithm USED-FOR maximum entropy RL. maximum entropy RL USED-FOR policies. entropy USED-FOR exploration. entropy FEATURE-OF low - entropy states. max - min entropy framework USED-FOR algorithm. algorithm USED-FOR Markov decision processes ( MDPs ). algorithm COMPARE RL algorithms. RL algorithms COMPARE algorithm. ,This paper proposes a soft actor-critic (SAC) algorithm for maximum entropy reinforcement learning (RL). The SAC algorithm is based on the max-min entropy framework. The authors show that the proposed algorithm is able to learn a policy that maximizes the entropy of low-entropy states. They also show that their algorithm can be applied to MDPs.,This paper proposes a soft actor-critic (SAC) algorithm for maximum entropy reinforcement learning (RL). The SAC algorithm is based on the max-min entropy framework. The authors show that the proposed algorithm is able to learn a policy that maximizes the entropy of low-entropy states. They also show that their algorithm can be applied to MDPs.
7928,SP:19107a648d3d23403a8693b065ee842833a0b893,"cross - sectional data USED-FOR learning task. continuous - time Markov chains USED-FOR problem. approximate likelihood maximization method USED-FOR continuous - time Markov chains. synthetic and real cancer data EVALUATE-FOR approach. OtherScientificTerm are genetic mutations, time order, and underspecification. Task is biomedical applications. Generic is methods. ",This paper proposes an approximate likelihood maximization method for continuous-time Markov chains. The proposed method is motivated by the fact that the time order of the Markov chain is not always the same for all time steps. The authors propose to learn the time-order for each step by minimizing the likelihood of the next step in the chain. The method is evaluated on synthetic and real-world datasets and shows promising results.,This paper proposes an approximate likelihood maximization method for continuous-time Markov chains. The proposed method is motivated by the fact that the time order of the Markov chain is not always the same for all time steps. The authors propose to learn the time-order for each step by minimizing the likelihood of the next step in the chain. The method is evaluated on synthetic and real-world datasets and shows promising results.
7944,SP:68dfc737a8ea591da2c7fe048a5b8995c89e1fec,Document intelligence USED-FOR business applications. self - supervised learning methods USED-FOR annotation efforts. large - scale unlabeled document datasets USED-FOR self - supervised learning methods. self - supervised objectives FEATURE-OF models. models USED-FOR annotation efforts. unified pretraining framework USED-FOR document understanding. UDoc HYPONYM-OF unified pretraining framework. UDoc USED-FOR document understanding tasks. Transformer USED-FOR UDoc. multimodal embeddings USED-FOR Transformer. semantic region USED-FOR words and visual features. it USED-FOR generic representation. representation USED-FOR similarities. self - supervised losses USED-FOR it. self - supervised losses USED-FOR generic representation. pretraining procedure USED-FOR joint representations. pretraining procedure USED-FOR downstream tasks. Material is documents. Method is document pretraining methods. ,This paper proposes a unified pretraining framework for self-supervised learning for document understanding tasks. The proposed method is based on Transformer and is able to learn a joint representation of words and visual features. The authors also propose a self-sustaining loss to improve the performance of the model. The paper also proposes a novel pretraining procedure for downstream tasks.,This paper proposes a unified pretraining framework for self-supervised learning for document understanding tasks. The proposed method is based on Transformer and is able to learn a joint representation of words and visual features. The authors also propose a self-sustaining loss to improve the performance of the model. The paper also proposes a novel pretraining procedure for downstream tasks.
7960,SP:de6c4c1a418d1ebadc294d77dda18612c163d9c0,"individual fairness FEATURE-OF data clustering problems. lp - norm objectives FEATURE-OF data clustering problems. k - MEDIAN HYPONYM-OF data clustering problems. k - MEDIAN HYPONYM-OF lp - norm objectives. objective guarantees FEATURE-OF l∞ or k - CENTER objective. clustering algorithm USED-FOR l∞ or k - CENTER objective. objective guarantees FEATURE-OF clustering algorithm. local - search algorithm USED-FOR lp - norms. kMEDIAN CONJUNCTION k - MEANS. k - MEANS CONJUNCTION kMEDIAN. algorithms USED-FOR problem. Linear Programming ( LP ) techniques USED-FOR algorithms. theoretical fairness guarantees COMPARE MV20. MV20 COMPARE theoretical fairness guarantees. sparsification technique USED-FOR algorithm. run - time EVALUATE-FOR algorithm. run - time EVALUATE-FOR sparsification technique. Generic are dataset, concept, and objective. OtherScientificTerm is individual fairness constraint. Metric are fairness, and worst - case guarantee. Method is LP rounding techniques. ","This paper studies the problem of individual fairness in data clustering. The authors consider the case where the objective is the l-norm of a set of data points, and the goal is to find a clustering algorithm that maximizes the lp-norm for each data point in the set. The main contribution of this paper is to provide a theoretical guarantee for the worst-case guarantee of the objective. The paper also provides a theoretical analysis of the performance of the proposed algorithm. ","This paper studies the problem of individual fairness in data clustering. The authors consider the case where the objective is the l-norm of a set of data points, and the goal is to find a clustering algorithm that maximizes the lp-norm for each data point in the set. The main contribution of this paper is to provide a theoretical guarantee for the worst-case guarantee of the objective. The paper also provides a theoretical analysis of the performance of the proposed algorithm. "
7976,SP:feb4664dfd5066cff582f6b4f9b17c6169049ceb,"MAX - K - CUT CONJUNCTION correlation clustering. correlation clustering CONJUNCTION MAX - K - CUT. MAX - K - CUT HYPONYM-OF graph partitioning problems. correlation clustering HYPONYM-OF graph partitioning problems. MAX - K - CUT CONJUNCTION MAX - AGREE variant of correlation clustering. MAX - AGREE variant of correlation clustering CONJUNCTION MAX - K - CUT. methods USED-FOR MAX - K - CUT. methods USED-FOR approximation guarantees. methods USED-FOR SDPs. approximation guarantees CONJUNCTION MAX - K - CUT. MAX - K - CUT CONJUNCTION approximation guarantees. O(n ) constraints USED-FOR SDPs. polynomial - time Gaussian sampling - based algorithms USED-FOR problems. O(n + |E| ) memory USED-FOR polynomial - time Gaussian sampling - based algorithms. approach CONJUNCTION sparsification. sparsification CONJUNCTION approach. OtherScientificTerm are memory bottleneck, and dense graphs. Metric are storage complexity, and approximation ratio. ","This paper studies the problem of graph partitioning in the context of O(n + |E|) memory. The authors propose two methods for solving the problem, MAX-K-CUT and MAX-AGREE, which are based on the idea that the memory requirements of the problem can be reduced by minimizing the approximation ratio between the solution and the original solution. In particular, the authors consider the case where the number of nodes in the graph is O(N) times larger than the size of the graph. They show that the solution can be approximated by a polynomial-time Gaussian sampling-based algorithm, which can be used to solve the problem. They also propose a sparsification approach to reduce the memory cost of the solution.","This paper studies the problem of graph partitioning in the context of O(n + |E|) memory. The authors propose two methods for solving the problem, MAX-K-CUT and MAX-AGREE, which are based on the idea that the memory requirements of the problem can be reduced by minimizing the approximation ratio between the solution and the original solution. In particular, the authors consider the case where the number of nodes in the graph is O(N) times larger than the size of the graph. They show that the solution can be approximated by a polynomial-time Gaussian sampling-based algorithm, which can be used to solve the problem. They also propose a sparsification approach to reduce the memory cost of the solution."
7992,SP:cfd6cf88a823729c281059e179788248238a6ed7,predicting inter - frame motion information USED-FOR video prediction tasks. Motion - Aware Unit ( MAU ) USED-FOR inter - frame motion information. temporal receptive field FEATURE-OF predictive units. attention module CONJUNCTION fusion module. fusion module CONJUNCTION attention module. modules PART-OF MAU. fusion module HYPONYM-OF modules. attention module HYPONYM-OF modules. attention module PART-OF MAU. fusion module PART-OF MAU. attention module USED-FOR attention map. historical temporal states PART-OF augmented motion information ( AMI ). attention map USED-FOR historical temporal states. predictive unit USED-FOR temporal dynamics. receptive field USED-FOR predictive unit. receptive field USED-FOR temporal dynamics. fusion module USED-FOR augmented motion information ( AMI ). unit USED-FOR predictive models. encoders CONJUNCTION decoders. decoders CONJUNCTION encoders. information recalling scheme USED-FOR encoders. information recalling scheme USED-FOR decoders. video prediction CONJUNCTION early action recognition tasks. early action recognition tasks CONJUNCTION video prediction. early action recognition tasks EVALUATE-FOR MAU. video prediction EVALUATE-FOR MAU. MAU COMPARE methods. methods COMPARE MAU. tasks EVALUATE-FOR methods. tasks EVALUATE-FOR MAU. OtherScientificTerm is historical spatial states. ,"This paper proposes a new model for video prediction based on a motion-aware unit. The proposed model is based on an attention module and a fusion module. The attention module is used to map historical temporal states into a temporal receptive field, while the fusion module uses augmented motion information (AMI) to capture the temporal dynamics. The fusion module is also used to recall historical temporal information from decoders and encoders. Experiments show that the proposed model outperforms the state-of-the-art methods on several video prediction tasks.","This paper proposes a new model for video prediction based on a motion-aware unit. The proposed model is based on an attention module and a fusion module. The attention module is used to map historical temporal states into a temporal receptive field, while the fusion module uses augmented motion information (AMI) to capture the temporal dynamics. The fusion module is also used to recall historical temporal information from decoders and encoders. Experiments show that the proposed model outperforms the state-of-the-art methods on several video prediction tasks."
8008,SP:07fa7cd4344d77a6e0f180d4f251cc8356b5202f,"neural net approximation of the Q function USED-FOR Deep Reinforcement Learning ( RL ). neural net approximations USED-FOR nonlinear RL. ReLU and polynomial activation functions PART-OF two - layer neural networks. two - layer neural networks USED-FOR function approximation. algorithm USED-FOR generative model setting. algebraic dimension FEATURE-OF sample complexity. Task is RL. OtherScientificTerm are neural net function class, and deterministic dynamics. Method is linear ( or eluder dimension ) methods. ",This paper studies the problem of learning a function approximation of the Q function for deep reinforcement learning. The authors show that the sample complexity of the function approximation can be reduced to the algebraic dimension of the neural net function class. They show that this is the case for ReLU and polynomial activation functions. They then propose an algorithm for learning the function approximations of the two-layer neural networks. They also show that their algorithm can be applied to the generative model setting.,This paper studies the problem of learning a function approximation of the Q function for deep reinforcement learning. The authors show that the sample complexity of the function approximation can be reduced to the algebraic dimension of the neural net function class. They show that this is the case for ReLU and polynomial activation functions. They then propose an algorithm for learning the function approximations of the two-layer neural networks. They also show that their algorithm can be applied to the generative model setting.
8024,SP:cac881243abde92a28c110f5bd84d115ed189bda,"representations USED-FOR zero - shot transfer. Deep Metric Learning ( DML ) USED-FOR representations. priori unknown test distributions USED-FOR zero - shot transfer. ooDML benchmark USED-FOR generalization. ooDML USED-FOR generalization. ooDML USED-FOR train - to - test distribution shifts. benchmark EVALUATE-FOR DML methods. few - shot DML USED-FOR generalization. unknown test shifts FEATURE-OF generalization. OtherScientificTerm are distribution shifts, train - test splits, out - of - distribution shifts, and distribution shift. Method are DML, and ooDML1. Generic is methods. ","This paper studies the problem of zero-shot transfer in deep metric learning (DML). The authors propose a new benchmark, called ooDML, which is a few-shot DML benchmark that is designed to measure the generalization performance of DML methods in the presence of distribution shift. The authors show that the proposed benchmark is able to capture the phenomenon of out-of-distribution (OOD) shift in the training distribution, and that it can be used as a benchmark to evaluate the performance of different DML algorithms. ","This paper studies the problem of zero-shot transfer in deep metric learning (DML). The authors propose a new benchmark, called ooDML, which is a few-shot DML benchmark that is designed to measure the generalization performance of DML methods in the presence of distribution shift. The authors show that the proposed benchmark is able to capture the phenomenon of out-of-distribution (OOD) shift in the training distribution, and that it can be used as a benchmark to evaluate the performance of different DML algorithms. "
8040,SP:bacff3685476855a32549d03095375649fd89df2,"outlier detection algorithm CONJUNCTION hyperparameter(s ). hyperparameter(s ) CONJUNCTION outlier detection algorithm. model HYPONYM-OF hyperparameter(s ). data - driven approach USED-FOR UOMS. METAOD HYPONYM-OF data - driven approach. meta - learning USED-FOR data - driven approach. meta - learning USED-FOR METAOD. model selection USED-FOR clustering. UOMS problem COMPARE model selection. model selection COMPARE UOMS problem. model evaluations CONJUNCTION model comparisons. model comparisons CONJUNCTION model evaluations. model USED-FOR dataset. METAOD USED-FOR model. detection models USED-FOR METAOD. historical outlier detection benchmark datasets EVALUATE-FOR detection models. meta - learning framework USED-FOR task similarity. metafeatures USED-FOR task similarity. meta - learning techniques USED-FOR UOMS. model COMPARE model selection. model selection COMPARE model. METAOD COMPARE model selection. model selection COMPARE METAOD. model selection COMPARE meta - learning techniques. meta - learning techniques COMPARE model selection. METAOD USED-FOR model. meta-)training EVALUATE-FOR METAOD. METAOD CONJUNCTION meta - learning database. meta - learning database CONJUNCTION METAOD. METAOD USED-FOR UOMS problem. meta - learning database USED-FOR UOMS problem. Task are unsupervised outlier detection task, unsupervised outlier model selection ( UOMS ) problem, model evaluation, and model comparison. OtherScientificTerm is universal objective function. Generic is task. ",This paper proposes a meta-learning framework for unsupervised outlier detection. The proposed method is based on meta-training and meta-evaluation. The authors show that the proposed method outperforms the baselines on three benchmark datasets. ,This paper proposes a meta-learning framework for unsupervised outlier detection. The proposed method is based on meta-training and meta-evaluation. The authors show that the proposed method outperforms the baselines on three benchmark datasets. 
8056,SP:bf1c45ef27953acab2195d54c8197d360c1e8190,"Prediction+optimization HYPONYM-OF real - world paradigm. SPO+ CONJUNCTION direct optimization. direct optimization CONJUNCTION SPO+. direct optimization HYPONYM-OF decision - focused prediction approaches. SPO+ HYPONYM-OF decision - focused prediction approaches. max operator USED-FOR real - world objectives. max operator USED-FOR soft constraints. framework USED-FOR closed - form solution. predictive parameters CONJUNCTION gradients. gradients CONJUNCTION predictive parameters. predictive parameters USED-FOR closed - form solution. gradients USED-FOR closed - form solution. synthetic linear programming CONJUNCTION portfolio optimization. portfolio optimization CONJUNCTION synthetic linear programming. portfolio optimization CONJUNCTION resource provisioning. resource provisioning CONJUNCTION portfolio optimization. method COMPARE decision - focused approaches. decision - focused approaches COMPARE method. two - staged methods CONJUNCTION decision - focused approaches. decision - focused approaches CONJUNCTION two - staged methods. method COMPARE two - staged methods. two - staged methods COMPARE method. applications EVALUATE-FOR method. method COMPARE method. method COMPARE method. soft constraints USED-FOR method. soft constraints FEATURE-OF applications. applications EVALUATE-FOR method. resource provisioning HYPONYM-OF applications. synthetic linear programming HYPONYM-OF applications. portfolio optimization HYPONYM-OF applications. Task are optimization problem, and downstream optimization problem. Method are prediction model, and analytically differentiable surrogate objective framework. Generic is they. OtherScientificTerm are soft linear and non - negative hard constraints, and theoretical bounds. ","This paper proposes a new method for prediction-optimization based on a surrogate objective framework. The main contribution of the paper is to provide theoretical bounds for the soft and non-negative hard constraints of the proposed method. The paper also provides theoretical guarantees for the closed-form solution of the surrogate objective. The method is evaluated on synthetic linear programming, portfolio optimization, and resource provisioning tasks.","This paper proposes a new method for prediction-optimization based on a surrogate objective framework. The main contribution of the paper is to provide theoretical bounds for the soft and non-negative hard constraints of the proposed method. The paper also provides theoretical guarantees for the closed-form solution of the surrogate objective. The method is evaluated on synthetic linear programming, portfolio optimization, and resource provisioning tasks."
8072,SP:41a9806ee6c0c84e046b7de79eb54dfe00de6995,"graph USED-FOR GNN. GNN USED-FOR DropGNNs. DropGNNs USED-FOR graph neighborhoods. GNN benchmarks EVALUATE-FOR DropGNNs. Method are Dropout Graph Neural Networks ( DropGNNs ), GNN frameworks, and message passing GNNs. Generic is approach. OtherScientificTerm is theoretical bounds. Metric is expressiveness. ","This paper studies the expressiveness of Dropout Graph Neural Networks (DropGNNs), which is an extension of the dropout graph neural network (GNN) framework. The authors show that DropGNN is able to achieve better expressiveness than existing GNNs on a variety of benchmark datasets. They also provide theoretical bounds on expressiveness. ","This paper studies the expressiveness of Dropout Graph Neural Networks (DropGNNs), which is an extension of the dropout graph neural network (GNN) framework. The authors show that DropGNN is able to achieve better expressiveness than existing GNNs on a variety of benchmark datasets. They also provide theoretical bounds on expressiveness. "
8088,SP:090dc0471d54e237f423034b1e1c46a510202807,"Transformers USED-FOR visual tasks. global representation capacities FEATURE-OF Transformers. local and global pattern features USED-FOR image classification. representation capacity FEATURE-OF local and global pattern features. DS - Net USED-FOR fine - grained and integrated features. DS - Net USED-FOR them. Inter - Scale Alignment module USED-FOR information interaction. Intra - scale Propagation module CONJUNCTION Inter - Scale Alignment module. Inter - Scale Alignment module CONJUNCTION Intra - scale Propagation module. Intra - scale Propagation module USED-FOR resolutions. Intra - scale Propagation module USED-FOR information interaction. contextual information USED-FOR downstream dense predictions. Vision Transformers CONJUNCTION ResNets. ResNets CONJUNCTION Vision Transformers. DS - Net COMPARE DeiT - Small. DeiT - Small COMPARE DS - Net. DS - Net COMPARE Vision Transformers. Vision Transformers COMPARE DS - Net. DS - Net COMPARE ResNets. ResNets COMPARE DS - Net. ImageNet-1k EVALUATE-FOR DeiT - Small. top-1 accuracy EVALUATE-FOR DeiT - Small. ImageNet-1k EVALUATE-FOR DS - Net. top-1 accuracy EVALUATE-FOR DS - Net. object detection CONJUNCTION instance segmentation. instance segmentation CONJUNCTION object detection. DS - Net - Small COMPARE ResNet-50. ResNet-50 COMPARE DS - Net - Small. DS - Net - Small USED-FOR object detection. mAP EVALUATE-FOR DS - Net - Small. DS - Net - Small USED-FOR instance segmentation. OtherScientificTerm are high - level local pattern information, features, and dual scales. Material is MSCOCO 2017. Generic is state - of - the - art scheme. Task is vision tasks. ","This paper proposes a new transformer architecture called DS-Net for image classification. The proposed architecture is based on the idea of dual-scale alignment, which allows for fine-grained and integrated features to be used for both local and global pattern features. The authors also propose a new Intra-scale Propagation module to improve the performance of the proposed model. Experiments on ImageNet-1k and mAP show that the proposed architecture achieves state-of-the-art performance. ","This paper proposes a new transformer architecture called DS-Net for image classification. The proposed architecture is based on the idea of dual-scale alignment, which allows for fine-grained and integrated features to be used for both local and global pattern features. The authors also propose a new Intra-scale Propagation module to improve the performance of the proposed model. Experiments on ImageNet-1k and mAP show that the proposed architecture achieves state-of-the-art performance. "
8104,SP:9dd460c3506a9a508b92baa63dff6b487e0eeca0,unified framework USED-FOR visual concepts. unified framework USED-FOR physics models of objects. visual concepts CONJUNCTION physics models of objects. physics models of objects CONJUNCTION visual concepts. videos USED-FOR physics models of objects. visual perception module CONJUNCTION concept learner. concept learner CONJUNCTION visual perception module. concept learner CONJUNCTION differentiable physics engine. differentiable physics engine CONJUNCTION concept learner. differentiable physics engine PART-OF components. visual perception module PART-OF components. concept learner HYPONYM-OF components. latent scene representations USED-FOR them. color CONJUNCTION shape. shape CONJUNCTION color. shape CONJUNCTION material. material CONJUNCTION shape. prior knowledge USED-FOR physics engine. concept learner USED-FOR visual concepts. language USED-FOR object - centric representations. object - centric representations USED-FOR visual concepts. material HYPONYM-OF visual concepts. color HYPONYM-OF visual concepts. shape HYPONYM-OF visual concepts. mass CONJUNCTION restitution. restitution CONJUNCTION mass. restitution CONJUNCTION velocity. velocity CONJUNCTION restitution. differentiable physical simulation USED-FOR physical properties. differentiable physical simulation USED-FOR differentiable physics model. video observations USED-FOR simulated trajectories. impulse - based differentiable rigid - body simulator USED-FOR differentiable physics model. grounded concepts USED-FOR differentiable physical simulation. velocity HYPONYM-OF physical properties. mass HYPONYM-OF physical properties. restitution HYPONYM-OF physical properties. concepts CONJUNCTION physical models. physical models CONJUNCTION concepts. differentiable physics PART-OF dynamic reasoning framework. VRDP COMPARE counterpart. counterpart COMPARE VRDP. accuracy EVALUATE-FOR predictive and counterfactual questions. physics models USED-FOR dynamics prediction. predictive and counterfactual questions EVALUATE-FOR VRDP. synthetic and real - world benchmarks EVALUATE-FOR dynamics prediction. accuracy EVALUATE-FOR VRDP. VRDP USED-FOR concepts. physical parameters USED-FOR VRDP. OtherScientificTerm is object - centric trajectories. Metric is interpretability. ,This paper proposes a dynamic reasoning framework for physics models of objects. The key idea is to use a differentiable rigid-body simulator (VRDP) to simulate the dynamics of an object in a video. VRDP is based on an impulse-based differentiable physics model and a concept learner. The authors show that VRDP outperforms state-of-the-art methods on both synthetic and real-world datasets. ,This paper proposes a dynamic reasoning framework for physics models of objects. The key idea is to use a differentiable rigid-body simulator (VRDP) to simulate the dynamics of an object in a video. VRDP is based on an impulse-based differentiable physics model and a concept learner. The authors show that VRDP outperforms state-of-the-art methods on both synthetic and real-world datasets. 
8120,SP:c511066c38f9793bacb4986c564eafa36e032f39,"Active learning USED-FOR minimizing labeling costs. out - of - distribution data CONJUNCTION redundancy. redundancy CONJUNCTION out - of - distribution data. submodular information measures ( SIM ) USED-FOR acquisition functions. acquisition functions USED-FOR unified active learning framework. submodular information measures ( SIM ) USED-FOR unified active learning framework. one - stop solution USED-FOR active learning. SIMILAR USED-FOR active learning. SIMILAR COMPARE active learning algorithms. active learning algorithms COMPARE SIMILAR. MNIST CONJUNCTION ImageNet. ImageNet CONJUNCTION MNIST. CIFAR-10 CONJUNCTION MNIST. MNIST CONJUNCTION CIFAR-10. ImageNet HYPONYM-OF image classification tasks. CIFAR-10 HYPONYM-OF image classification tasks. MNIST HYPONYM-OF image classification tasks. DISTIL toolkit USED-FOR SIMILAR. Method are active learning methods, and Submodular Information Measures based actIve LeARning. OtherScientificTerm are imbalance or rare classes, and rare classes. Material is large real - world datasets. ","This paper proposes a unified active learning framework, called SIMILAR, which is based on submodular information measures (SIM) and actIve LeARning (ACTIve). The main idea is to combine existing active learning methods with the DISTIL toolkit to improve the performance of active learning algorithms. The authors show that the proposed method outperforms the state-of-the-art active learning approaches on MNIST, CIFAR-10, and ImageNet. ","This paper proposes a unified active learning framework, called SIMILAR, which is based on submodular information measures (SIM) and actIve LeARning (ACTIve). The main idea is to combine existing active learning methods with the DISTIL toolkit to improve the performance of active learning algorithms. The authors show that the proposed method outperforms the state-of-the-art active learning approaches on MNIST, CIFAR-10, and ImageNet. "
8136,SP:c141dc29b487ebfaa20ee50786886b0383d938bc,"identity tests USED-FOR ranking data. Mallows model USED-FOR asymptotic and non - asymptotic settings. Mallows model USED-FOR ranking data. algorithms USED-FOR spread parameter. spread parameter FEATURE-OF Mallows model. Uniformly Most Powerful Unbiased ( UMPU ) test USED-FOR asymptotic setting. sample - optimal non - asymptotic identity test USED-FOR it. Uniformly Most Powerful Unbiased ( UMPU ) test USED-FOR one. distribution of the sufficient statistic USED-FOR it. optimal learning algorithm USED-FOR Mallows model. optimal learning algorithm USED-FOR nonasymptotic test. Mallows models USED-FOR unknown central ranking case. asymptotic setting USED-FOR case. OtherScientificTerm is central ranking. Generic are test, and tests. Material is medium sized data. ","This paper studies the problem of identifying the identity of a given data point in the asymptotic and non-asymptotic setting. The authors propose a new identity test for the non-asymptotical setting, called the Uniformly Most Powerful Unbiased (UMPU) test, which is based on the distribution of the sufficient statistic. They show that the test is sample-optimal in the nonasymnotic setting, and that it can be used to identify the true identity of the data point. They also provide an optimal learning algorithm for this test. ","This paper studies the problem of identifying the identity of a given data point in the asymptotic and non-asymptotic setting. The authors propose a new identity test for the non-asymptotical setting, called the Uniformly Most Powerful Unbiased (UMPU) test, which is based on the distribution of the sufficient statistic. They show that the test is sample-optimal in the nonasymnotic setting, and that it can be used to identify the true identity of the data point. They also provide an optimal learning algorithm for this test. "
8152,SP:4c00bcc561832b581f479905b5e3310aeb3bdce2,"sparse multi - view cameras USED-FOR free - viewpoint video. pixel - aligned features USED-FOR radiance fields. heavy occlusions CONJUNCTION dynamic articulations of body parts. dynamic articulations of body parts CONJUNCTION heavy occlusions. parametric human body model USED-FOR robust performance capture. approach USED-FOR generalizable neural radiance fields. parametric human body model USED-FOR generalizable neural radiance fields. parametric human body model USED-FOR approach. temporal transformer USED-FOR tracked visual features. skeletal body motion USED-FOR temporal transformer. skeletal body motion USED-FOR tracked visual features. multi - view transformer USED-FOR cross - attention. temporally - fused features CONJUNCTION pixel - aligned features. pixel - aligned features CONJUNCTION temporally - fused features. ZJU - MoCap and AIST datasets EVALUATE-FOR method. method COMPARE generalizable NeRF methods. generalizable NeRF methods COMPARE method. ZJU - MoCap and AIST datasets EVALUATE-FOR generalizable NeRF methods. OtherScientificTerm is appearance. Method are generalization approaches, and Neural Human Performer. ","This paper proposes a neural radiance field (NeRF) method for multi-view video capture. The proposed method is based on a parametric human body model, which is trained to capture skeletal body motion and dynamic articulations of body parts. The method is evaluated on the ZJU-MoCap and AIST datasets. The results show that the proposed method outperforms existing NeRF methods.","This paper proposes a neural radiance field (NeRF) method for multi-view video capture. The proposed method is based on a parametric human body model, which is trained to capture skeletal body motion and dynamic articulations of body parts. The method is evaluated on the ZJU-MoCap and AIST datasets. The results show that the proposed method outperforms existing NeRF methods."
8168,SP:5495d9168a8770eb2493e2d2bb6b68423e82b9e6,"recognition CONJUNCTION detection. detection CONJUNCTION recognition. Vision Transformer USED-FOR vision tasks. detection HYPONYM-OF vision tasks. recognition HYPONYM-OF vision tasks. neural architecture search USED-FOR process. architecture CONJUNCTION search space. search space CONJUNCTION architecture. E - T Error USED-FOR search dimensions. weight - sharing supernet USED-FOR E - T Error. weight - sharing supernet USED-FOR search dimensions. Swin CONJUNCTION DeiT. DeiT CONJUNCTION Swin. DeiT CONJUNCTION ViT. ViT CONJUNCTION DeiT. searched models COMPARE models. models COMPARE searched models. searched space COMPARE models. models COMPARE searched space. searched space USED-FOR searched models. S3 HYPONYM-OF searched models. ViT HYPONYM-OF models. Swin HYPONYM-OF models. DeiT HYPONYM-OF models. ImageNet EVALUATE-FOR models. object detection CONJUNCTION semantic segmentation. semantic segmentation CONJUNCTION object detection. semantic segmentation CONJUNCTION visual question answering. visual question answering CONJUNCTION semantic segmentation. S3 USED-FOR vision and vision - language tasks. semantic segmentation EVALUATE-FOR S3. visual question answering EVALUATE-FOR S3. object detection EVALUATE-FOR S3. Generic is architectures. Method is vision transformers. Task are space searching process, and vision transformer. OtherScientificTerm is Search Space. ","This paper proposes a new search space for vision transformers. The proposed search space is based on the E-T Error, which is a weighted sum of the search space and the weight-sharing supernet. The authors show that the searched space is more efficient than the original search space. They also show that their search space outperforms the original space. ","This paper proposes a new search space for vision transformers. The proposed search space is based on the E-T Error, which is a weighted sum of the search space and the weight-sharing supernet. The authors show that the searched space is more efficient than the original search space. They also show that their search space outperforms the original space. "
8184,SP:c77b83c667a9b63fe15582336a77a34e96fd667b,"learning from label proportions ( LLP ) framework USED-FOR linear threshold functions ( LTFs ). algorithm USED-FOR LTF. algorithm USED-FOR LTF. d - dimensional boolean vectors USED-FOR OR. accuracy EVALUATE-FOR LTF. LTFs USED-FOR monotone ORs. LTF HYPONYM-OF algorithm. unit - sized bags HYPONYM-OF supervised learning setup. accuracy EVALUATE-FOR algorithm. linear programming USED-FOR LTFs. techniques USED-FOR LLP setting. LTFs USED-FOR LLP learning of LTFs. LLP CONJUNCTION supervised learning. supervised learning CONJUNCTION LLP. inapproximability EVALUATE-FOR LLP learning LTFs. OtherScientificTerm are bags of feature - vectors, label proportions, bags, labeled feature - vectors, non - monochromatic bags, monotone OR, and non - monochromatic bags case. Metric are algorithmic bounds, and complexity. Generic is bound. ","This paper studies the problem of learning from label proportions (LLP) for linear threshold functions (LTFs). In particular, the authors consider the case where the label proportions are non-monochromatic. The authors propose a new algorithm for learning LTFs based on linear programming. They show that the algorithm is computationally efficient and can be applied to both supervised and unsupervised settings. They also show that their algorithm is inapproximable to the monotone case.","This paper studies the problem of learning from label proportions (LLP) for linear threshold functions (LTFs). In particular, the authors consider the case where the label proportions are non-monochromatic. The authors propose a new algorithm for learning LTFs based on linear programming. They show that the algorithm is computationally efficient and can be applied to both supervised and unsupervised settings. They also show that their algorithm is inapproximable to the monotone case."
8200,SP:2eb193c76355aac08003c9b377895202fd3bd297,extreme computational resources USED-FOR neural architecture search ( NAS ). benchmarks EVALUATE-FOR multi - fidelity techniques. learning curve extrapolation HYPONYM-OF multi - fidelity techniques. NAS - Bench-111 CONJUNCTION NAS - Bench-311. NAS - Bench-311 CONJUNCTION NAS - Bench-111. method USED-FOR surrogate benchmarks. NAS - Bench-311 CONJUNCTION NAS - Bench - NLP11. NAS - Bench - NLP11 CONJUNCTION NAS - Bench-311. singular value decomposition and noise modeling USED-FOR method. NAS - Bench-111 HYPONYM-OF surrogate benchmarks. NAS - Bench - NLP11 HYPONYM-OF surrogate benchmarks. NAS - Bench-311 HYPONYM-OF surrogate benchmarks. learning curve extrapolation framework USED-FOR single - fidelity algorithms. it COMPARE single - fidelity algorithms. single - fidelity algorithms COMPARE it. Material is tabular and surrogate benchmarks. Generic is architecture. OtherScientificTerm is architectures. Metric is validation accuracy. ,This paper proposes a learning curve extrapolation framework for multi-fidelity neural architecture search (NAS). The method is based on singular value decomposition and noise modeling. The proposed method is evaluated on NAS-Bench-311 and NAS-bench-NLP11 benchmarks.,This paper proposes a learning curve extrapolation framework for multi-fidelity neural architecture search (NAS). The method is based on singular value decomposition and noise modeling. The proposed method is evaluated on NAS-Bench-311 and NAS-bench-NLP11 benchmarks.
8216,SP:6ed1637ac697821931f685db0d476b9f7b56971a,"user - centred method USED-FOR example - based explanations. SimplEx HYPONYM-OF user - centred method. corpus USED-FOR SimplEx. Integrated Jacobian HYPONYM-OF approach. mortality prediction CONJUNCTION image classification. image classification CONJUNCTION mortality prediction. tasks EVALUATE-FOR decompositions. image classification HYPONYM-OF tasks. mortality prediction HYPONYM-OF tasks. Method are machine learning models, convoluted latent representations, mixture of corpus latent representations, and model representations. Generic are latent representations, model, and mixture. OtherScientificTerm are latent space, post - hoc explanations, and features. ","This paper proposes SimplEx, a user-centred method for generating post-hoc explanations for machine learning models. SimplEx is based on the Integrated Jacobian (IJ) framework. The core idea is to decompose the latent space of a model into a mixture of model representations and a set of post hoc explanations. The proposed method is evaluated on a number of tasks, including mortality prediction, image classification, and image classification. ","This paper proposes SimplEx, a user-centred method for generating post-hoc explanations for machine learning models. SimplEx is based on the Integrated Jacobian (IJ) framework. The core idea is to decompose the latent space of a model into a mixture of model representations and a set of post hoc explanations. The proposed method is evaluated on a number of tasks, including mortality prediction, image classification, and image classification. "
8232,SP:c8f82ec90f891d7394933483b7f926155ac363ef,image - text pairs USED-FOR multi - modal representations. Transformer USED-FOR images. CNN USED-FOR images. CNN - Transformer architecture USED-FOR VLP models. Visual relationship between visual contents USED-FOR image understanding. CNNs USED-FOR visual relation learning. visual relation CONJUNCTION inter - modal alignment. inter - modal alignment CONJUNCTION visual relation. objectives PART-OF Transformer network. learning visual relation PART-OF Transformer network. inter - modal alignment PART-OF Transformer network. learning visual relation HYPONYM-OF objectives. inter - modal alignment HYPONYM-OF objectives. design USED-FOR inter - modal alignment learning. Transformer USED-FOR inter - modal alignment learning. fully Transformer visual embedding USED-FOR inter - modal alignment. fully Transformer visual embedding USED-FOR VLP. fully Transformer visual embedding USED-FOR visual relation. Inter - Modality Flow ( IMF ) HYPONYM-OF metric. masking optimization mechanism USED-FOR inter - modality learning. Masked Feature Regression ( MFR ) USED-FOR inter - modality learning. Masked Feature Regression ( MFR ) USED-FOR Transformer. masking optimization mechanism PART-OF Transformer. Masked Feature Regression ( MFR ) HYPONYM-OF masking optimization mechanism. Transformer USED-FOR visual feature learning in VLP. Visual Entailment CONJUNCTION Visual Reasoning. Visual Reasoning CONJUNCTION Visual Entailment. Visual Question Answering ( VQA ) CONJUNCTION Visual Entailment. Visual Entailment CONJUNCTION Visual Question Answering ( VQA ). Image - Text Retrieval CONJUNCTION Visual Question Answering ( VQA ). Visual Question Answering ( VQA ) CONJUNCTION Image - Text Retrieval. vision - language tasks EVALUATE-FOR method. Visual Reasoning HYPONYM-OF vision - language tasks. Image - Text Retrieval HYPONYM-OF vision - language tasks. Visual Question Answering ( VQA ) HYPONYM-OF vision - language tasks. Visual Entailment HYPONYM-OF vision - language tasks. approach COMPARE V,This paper proposes a transformer-based approach for visual feature learning for multi-modal representation learning. The proposed approach is based on a fully-transformer visual embedding and a masking optimization mechanism for masking feature learning in VLP. The paper also proposes a new metric for inter-modality alignment learning. Experiments show the effectiveness of the proposed approach on several vision-language tasks.,This paper proposes a transformer-based approach for visual feature learning for multi-modal representation learning. The proposed approach is based on a fully-transformer visual embedding and a masking optimization mechanism for masking feature learning in VLP. The paper also proposes a new metric for inter-modality alignment learning. Experiments show the effectiveness of the proposed approach on several vision-language tasks.
8248,SP:ecc173185ec28d0ef75c60df260ac4faba059f61,"information leakage FEATURE-OF iterative randomized learning algorithm. model USED-FOR information leakage. noisy gradient descent algorithms USED-FOR problem. Rényi divergence FEATURE-OF probability distributions. probability distributions FEATURE-OF models. smooth and strongly convex loss functions COMPARE composition theorems. composition theorems COMPARE smooth and strongly convex loss functions. noisy gradient descent algorithms USED-FOR optimal utility. gradient complexity FEATURE-OF optimal utility. Generic is algorithm. OtherScientificTerm are dynamics of Rényi differential privacy loss, privacy loss, and intermediate gradient computations. ","This paper studies the problem of information leakage in iterative randomized learning. The authors consider the case where the loss function $\mathcal{O}(\sqrt{O})$ is a convex function of the distribution $O(\log O(\log \log O(O(O))$, where $O(1/\log O)$ is the number of samples in the distribution, and $\log O$ is an iterative gradient descent algorithm. They show that the optimal utility of the algorithm depends on $\log o(\log o(O)$. They also show that if $\logO(\sqrt{\log O))$ is smooth and strongly convex, then the algorithm is guaranteed to converge to the optimal solution of the problem. They also prove that the algorithm converges to an optimal solution in the case of noisy gradient descent.","This paper studies the problem of information leakage in iterative randomized learning. The authors consider the case where the loss function $\mathcal{O}(\sqrt{O})$ is a convex function of the distribution $O(\log O(\log \log O(O(O))$, where $O(1/\log O)$ is the number of samples in the distribution, and $\log O$ is an iterative gradient descent algorithm. They show that the optimal utility of the algorithm depends on $\log o(\log o(O)$. They also show that if $\logO(\sqrt{\log O))$ is smooth and strongly convex, then the algorithm is guaranteed to converge to the optimal solution of the problem. They also prove that the algorithm converges to an optimal solution in the case of noisy gradient descent."
8264,SP:acb1e0dc8d6ef5607e7d3ec9893b5364b9a6e831,large - scale machine learning CONJUNCTION embedded optimal control. embedded optimal control CONJUNCTION large - scale machine learning. First - order methods USED-FOR large - scale machine learning. First - order methods USED-FOR quadratic optimization. First - order methods USED-FOR embedded optimal control. OSQP HYPONYM-OF First - order methods. OSQP HYPONYM-OF quadratic optimization. manual hyperparameter tuning CONJUNCTION convergence time. convergence time CONJUNCTION manual hyperparameter tuning. Reinforcement Learning ( RL ) USED-FOR policy. RL policy COMPARE QP solvers. QP solvers COMPARE RL policy. RLQP COMPARE QP solvers. QP solvers COMPARE RLQP. RLQP HYPONYM-OF RL policy. RLQP USED-FOR problems. RLQP USED-FOR applications. Maros - Mészáros problems HYPONYM-OF applications. QPLIB HYPONYM-OF applications. Generic is methods. Material is QP benchmarks. ,"This paper studies the problem of quadratic optimization (QP) solvers. The authors propose a new method for solving QP solvers based on Reinforcement Learning (RL) and show that it outperforms existing methods in terms of convergence time, convergence rate, and manual hyperparameter tuning. They also show that RLQP can be used to solve QP problems in a more efficient way than existing methods. ","This paper studies the problem of quadratic optimization (QP) solvers. The authors propose a new method for solving QP solvers based on Reinforcement Learning (RL) and show that it outperforms existing methods in terms of convergence time, convergence rate, and manual hyperparameter tuning. They also show that RLQP can be used to solve QP problems in a more efficient way than existing methods. "
8280,SP:eb68e98d9baf9118381d25d4b2da030a6f78577f,"convergence rate EVALUATE-FOR model. PC - bias USED-FOR linear and non8 linear networks. PC - bias USED-FOR early stopping. early stopping USED-FOR PCA. random labels USED-FOR deep networks. Method are convolutional neural networks, and over - parametrized deep linear network model. OtherScientificTerm are asymptotic analysis, hidden layers, principal components, singular 6 values, convergence pattern, Principal Components bias ( PC - bias ), and spectral bias. Task is learning. Generic is biases. ","This paper studies the problem of early stopping in deep linear networks. The authors propose a new bias called Principal Components Bias (PC-Bias), which is a generalization of the spectral bias (SPB) to the case of linear and non-linear networks.  The authors show that PC-bias can be used to improve the convergence rate of a deep linear network. They show that under certain assumptions, PCA converges to singular 6 values. They also show that early stopping can improve the performance of the model. ","This paper studies the problem of early stopping in deep linear networks. The authors propose a new bias called Principal Components Bias (PC-Bias), which is a generalization of the spectral bias (SPB) to the case of linear and non-linear networks.  The authors show that PC-bias can be used to improve the convergence rate of a deep linear network. They show that under certain assumptions, PCA converges to singular 6 values. They also show that early stopping can improve the performance of the model. "
8296,SP:1598bad835a657e56af3261501c671897b7e9ffd,"Backdoor attack HYPONYM-OF deep neural networks ( DNNs ). defense methods USED-FOR detecting or erasing backdoors. anti - backdoor learning USED-FOR clean models. backdoor - poisoned data USED-FOR clean models. dual - task USED-FOR learning process. models USED-FOR backdoored data. backdoored data USED-FOR model. Anti - Backdoor Learning ( ABL ) USED-FOR backdoor attacks. learning scheme USED-FOR backdoor attacks. Anti - Backdoor Learning ( ABL ) HYPONYM-OF learning scheme. two - stage gradient ascent mechanism USED-FOR ABL. ABL - trained models COMPARE they. they COMPARE ABL - trained models. backdoor - poisoned data USED-FOR ABL - trained models. clean data USED-FOR ABL - trained models. clean data USED-FOR they. OtherScientificTerm are backdoor triggers, backdoor task, and backdoor target class. ","This paper proposes a new method for detecting and erasing backdoors in deep neural networks (DNNs). The proposed method, called Anti-Backdoor Learning (ABL), is based on two-stage gradient ascent mechanism. The main idea is to learn a dual-task to detect and erase backdoored data. The authors show that ABL-trained models are more robust to backdoor attacks compared to clean models. ","This paper proposes a new method for detecting and erasing backdoors in deep neural networks (DNNs). The proposed method, called Anti-Backdoor Learning (ABL), is based on two-stage gradient ascent mechanism. The main idea is to learn a dual-task to detect and erase backdoored data. The authors show that ABL-trained models are more robust to backdoor attacks compared to clean models. "
8312,SP:6c0b7cb37e285cb9342f049d7b61af4565fe01fd,"generative radiance fields USED-FOR 3Daware image synthesis. multi - view constraint USED-FOR 3D radiance fields. regularization USED-FOR 3D radiance fields. methods USED-FOR 3D radiance fields. multi - view constraint USED-FOR methods. 2D images USED-FOR 3D radiance fields. they USED-FOR 3D shapes. shading - guided generative implicit model USED-FOR shape representation. 3D shape USED-FOR realistic rendering. lighting conditions FEATURE-OF realistic rendering. lighting conditions FEATURE-OF shading. discriminator USED-FOR Gradients. surface tracking USED-FOR volume rendering strategy. approach USED-FOR photorealistic 3D - aware image synthesis. approach USED-FOR 3D shapes. approach COMPARE methods. methods COMPARE approach. approach USED-FOR image relighting. approach USED-FOR 3D shape reconstruction. OtherScientificTerm are shapecolor ambiguity, multi - lighting constraint, illumination, computational burden, and surface normals. Metric is training and inference time. ","This paper proposes a generative implicit model for 3D-aware image synthesis. The proposed method is based on the shading-guided generative model, which is able to generate 3D radiance fields in a multi-view constraint. The method is evaluated on a variety of 3D shape reconstruction tasks, and is shown to be able to achieve state-of-the-art results. The paper also shows that the proposed method can be used for image relighting.","This paper proposes a generative implicit model for 3D-aware image synthesis. The proposed method is based on the shading-guided generative model, which is able to generate 3D radiance fields in a multi-view constraint. The method is evaluated on a variety of 3D shape reconstruction tasks, and is shown to be able to achieve state-of-the-art results. The paper also shows that the proposed method can be used for image relighting."
8328,SP:4b3dad77d79507c512877867dfea6db87a78682d,"flexible machine learning models USED-FOR instrumental variable ( IV ) regression. quasi - Bayesian procedure USED-FOR IV regression. kernelized IV models USED-FOR quasi - Bayesian procedure. Bayesian modeling USED-FOR IV. Bayesian modeling COMPARE approach. approach COMPARE Bayesian modeling. approach USED-FOR approximate inference algorithm. approximate inference algorithm COMPARE point estimation methods. point estimation methods COMPARE approximate inference algorithm. time cost EVALUATE-FOR point estimation methods. time cost EVALUATE-FOR approximate inference algorithm. algorithm USED-FOR neural network models. Method is uncertainty quantification methodology. OtherScientificTerm are data generating process, and quasi - posterior. Generic is method. ","This paper proposes a quasi-Bayesian method for instrumental variable (IV) regression. The proposed method is based on the notion of quasi-posterior, which is a notion of uncertainty quantification. The authors propose to use a kernelized IV model to approximate the posterior of the data generating process. They show that the proposed method outperforms Bayesian methods in terms of time complexity and accuracy. They also show that their method can be applied to neural network models.","This paper proposes a quasi-Bayesian method for instrumental variable (IV) regression. The proposed method is based on the notion of quasi-posterior, which is a notion of uncertainty quantification. The authors propose to use a kernelized IV model to approximate the posterior of the data generating process. They show that the proposed method outperforms Bayesian methods in terms of time complexity and accuracy. They also show that their method can be applied to neural network models."
8344,SP:fd0d72d0689f170f8157dc7f79deb01348e414b3,language - specific annotated data CONJUNCTION knowledge sources. knowledge sources CONJUNCTION language - specific annotated data. translation CONJUNCTION in - language retrieval modules. in - language retrieval modules CONJUNCTION translation. multilingual autoregressive generation model CONJUNCTION CORA. CORA CONJUNCTION multilingual autoregressive generation model. annotated data USED-FOR iterative training method. high - resource languages FEATURE-OF annotated data. multilingual open QA benchmarks EVALUATE-FOR CORA. cross - lingual retrieval CONJUNCTION generation. generation CONJUNCTION cross - lingual retrieval. Method is dense passage retrieval algorithm. OtherScientificTerm is low - resource ones. Material is low - resource settings. Generic is model. ,"This paper proposes a novel multi-lingual autoregressive generation model, CORA, for multi-language open QA. The proposed method is based on the dense passage retrieval algorithm. The authors show that the proposed method outperforms the baselines in terms of accuracy and efficiency. ","This paper proposes a novel multi-lingual autoregressive generation model, CORA, for multi-language open QA. The proposed method is based on the dense passage retrieval algorithm. The authors show that the proposed method outperforms the baselines in terms of accuracy and efficiency. "
8360,SP:aa4d44b283ef4fea4335847c89fc7b5874169850,"specialized training algorithms USED-FOR domain generalization. deep neural networks COMPARE specialized training algorithms. specialized training algorithms COMPARE deep neural networks. distribution shift FEATURE-OF deep neural networks. Empirical Risk Minimization ( ERM ) USED-FOR deep neural networks. domain generalization datasets USED-FOR ERM models. Fisher information CONJUNCTION predictive entropy. predictive entropy CONJUNCTION Fisher information. predictive entropy CONJUNCTION maximum mean discrepancy. maximum mean discrepancy CONJUNCTION predictive entropy. measures USED-FOR out - of - distribution generalization. measures CONJUNCTION predictive entropy. predictive entropy CONJUNCTION measures. out - of - distribution generalization EVALUATE-FOR ERM models. Fisher information FEATURE-OF measures. maximum mean discrepancy FEATURE-OF measures. deep networks USED-FOR out - of - distribution. ERM USED-FOR deep networks. Method are domain adaptation theory, and ERMs. Generic is theory. Task are out - of - domain generalization, and generalization. ","This paper studies the problem of out-of-distribution generalization in deep neural networks. The authors propose two measures, Fisher information and predictive entropy, to measure the generalization performance of ERM-based deep networks. They show that the proposed measures can be used to evaluate the performance of deep networks on a variety of domain generalization datasets. They also provide a theoretical analysis of the relationship between the two measures.","This paper studies the problem of out-of-distribution generalization in deep neural networks. The authors propose two measures, Fisher information and predictive entropy, to measure the generalization performance of ERM-based deep networks. They show that the proposed measures can be used to evaluate the performance of deep networks on a variety of domain generalization datasets. They also provide a theoretical analysis of the relationship between the two measures."
8376,SP:340c5353a63884b49cfdc46ddb6153b28b2e894f,backdoor data poisoning attack HYPONYM-OF adversarial attack. watermarked examples USED-FOR model. backdoor data poisoning attacks USED-FOR classification problems. formal theoretical framework USED-FOR backdoor data poisoning attacks. this USED-FOR statistical and computational issues. statistical and computational issues FEATURE-OF attacks. intrinsic vulnerability FEATURE-OF learning problem. learning problem USED-FOR backdoor attack. memorization capacity HYPONYM-OF parameter. robustness FEATURE-OF natural learning problems. backdoor attacks FEATURE-OF natural learning problems. natural problem settings USED-FOR backdoor attacks. adversarial training USED-FOR backdoors. backdoor filtering CONJUNCTION robust generalization. robust generalization CONJUNCTION backdoor filtering. robust generalization HYPONYM-OF problems. backdoor filtering HYPONYM-OF problems. Generic is assumptions. Method is learning algorithm. ,"This paper studies the problem of backdoor data poisoning attacks. The authors propose a theoretical framework to study the intrinsic vulnerability of backdoor attacks. They show that the learning problem of a backdoor attack can be decomposed into two parts: (1) a learning problem where the model is trained with adversarial training, and (2) an adversarial learning problem with watermarked examples. The learning problem is defined as the learning of a model with a watermarked example. The paper also provides a theoretical analysis of the memorization capacity of the learning algorithm. ","This paper studies the problem of backdoor data poisoning attacks. The authors propose a theoretical framework to study the intrinsic vulnerability of backdoor attacks. They show that the learning problem of a backdoor attack can be decomposed into two parts: (1) a learning problem where the model is trained with adversarial training, and (2) an adversarial learning problem with watermarked examples. The learning problem is defined as the learning of a model with a watermarked example. The paper also provides a theoretical analysis of the memorization capacity of the learning algorithm. "
8392,SP:4c925cde6e5b9813946452fdd6b47816e2490f49,"networks COMPARE ones. ones COMPARE networks. Large width limits PART-OF deep learning research. representational power FEATURE-OF networks. capacity CONJUNCTION width. width CONJUNCTION capacity. neural networks USED-FOR Deep Gaussian Processes ( Deep GP ). Deep Gaussian Processes ( Deep GP ) HYPONYM-OF nonparametric hierarchical models. neural nets HYPONYM-OF nonparametric hierarchical models. they USED-FOR modeling task. width USED-FOR neural networks. nonparametric Deep GP USED-FOR Gaussian processes. mixture of data - adaptable basis functions FEATURE-OF posterior. width CONJUNCTION depth. depth CONJUNCTION width. depth USED-FOR model. non - Gaussianity FEATURE-OF model. hidden units USED-FOR neural networks. L2 regularization USED-FOR neural networks. OtherScientificTerm are computational practicalities, GP behavior, adaptability, and Gaussian prior on parameters. Method are Deep GP, and hierarchical models. ","This paper studies the generalization of Deep Gaussian Processes (DGP) to non-parametric hierarchical models. The authors show that the width and depth of Deep GP can be reduced to the same size as the width of a neural network. They also show that this can be achieved by using a mixture of data-adaptable basis functions, which can be used as a basis function for the posterior of the GP. The paper also shows that the proposed method can be applied to a variety of deep neural networks. ","This paper studies the generalization of Deep Gaussian Processes (DGP) to non-parametric hierarchical models. The authors show that the width and depth of Deep GP can be reduced to the same size as the width of a neural network. They also show that this can be achieved by using a mixture of data-adaptable basis functions, which can be used as a basis function for the posterior of the GP. The paper also shows that the proposed method can be applied to a variety of deep neural networks. "
8408,SP:3f74dc3dc2cb444b3097aae1288dad5355e9a4d4,"algorithmic framework USED-FOR challenges. systems heterogeneity CONJUNCTION infrequent and imprecise communication. infrequent and imprecise communication CONJUNCTION systems heterogeneity. objective heterogeneity CONJUNCTION systems heterogeneity. systems heterogeneity CONJUNCTION objective heterogeneity. challenges PART-OF FL. FedLin HYPONYM-OF algorithmic framework. infrequent and imprecise communication HYPONYM-OF challenges. objective heterogeneity HYPONYM-OF challenges. systems heterogeneity HYPONYM-OF challenges. speed - accuracy conflict FEATURE-OF FL algorithms. FedLin USED-FOR linear convergence. matching upper and lower bounds FEATURE-OF convergence rate. convergence rate FEATURE-OF FedLin. matching upper and lower bounds FEATURE-OF FedLin. compression level FEATURE-OF convergence rate. gradient sparsification USED-FOR FedLin. linear convergence rates FEATURE-OF FedLin. gradient sparsification USED-FOR FL. Task is federated learning ( FL ) setup. Method is statistical model. Generic are framework, and they. OtherScientificTerm are global minimum, sub - linear rate, fast convergence, clients ’ local loss functions, objective and systems heterogeneity, infrequent, periodic communication, and tight linear convergence rate guarantees. Metric is accuracy. ","This paper studies the problem of federated learning (FL) in the setting of systems heterogeneity, infrequent and imprecise communication, and systems heterogeneity. The authors propose a new algorithm, FedLin, to address these challenges. FedLin is based on gradient sparsification and is able to achieve sub-linear convergence rate guarantees for the global minimum and sublinear convergence rates for the local minimum. In addition, the authors show that FedLin can be used to solve the speed-accuracy conflict problem.","This paper studies the problem of federated learning (FL) in the setting of systems heterogeneity, infrequent and imprecise communication, and systems heterogeneity. The authors propose a new algorithm, FedLin, to address these challenges. FedLin is based on gradient sparsification and is able to achieve sub-linear convergence rate guarantees for the global minimum and sublinear convergence rates for the local minimum. In addition, the authors show that FedLin can be used to solve the speed-accuracy conflict problem."
8424,SP:29f44f2f7d0e9748eed6732ed19ca3335acb04e3,Sliced - Wasserstein distance ( SW ) USED-FOR machine learning applications. Sliced - Wasserstein distance ( SW ) COMPARE Wasserstein distance. Wasserstein distance COMPARE Sliced - Wasserstein distance ( SW ). Monte Carlo USED-FOR SW. perspective USED-FOR SW. one - dimensional projections FEATURE-OF highdimensional random vector. concentration of measure phenomenon USED-FOR perspective. concentration of measure phenomenon USED-FOR SW. deterministic approximation USED-FOR SW. method COMPARE Monte Carlo approximation. Monte Carlo approximation COMPARE method. weak dependence condition FEATURE-OF data distribution. nonasymptotical guarantees USED-FOR approach. generative modeling problem EVALUATE-FOR approximation. Generic is it. OtherScientificTerm is random projections. Metric is approximation error. Material is synthetic datasets. ,"This paper studies the problem of estimating the sliced-Wasserstein distance (SW) between two points in a data distribution. The authors consider the case where the data distribution is non-asymptotical, and the objective function is a random vector. They propose a deterministic approximation of the SW function, which is based on the concentration of measure phenomenon. They show that the proposed method can be approximated by Monte Carlo. They also show that their method is more robust to the weak dependence condition. ","This paper studies the problem of estimating the sliced-Wasserstein distance (SW) between two points in a data distribution. The authors consider the case where the data distribution is non-asymptotical, and the objective function is a random vector. They propose a deterministic approximation of the SW function, which is based on the concentration of measure phenomenon. They show that the proposed method can be approximated by Monte Carlo. They also show that their method is more robust to the weak dependence condition. "
8440,SP:7d5ec55a01247b65e4a8f1973d448214585d6baa,neural language models CONJUNCTION translation models. translation models CONJUNCTION neural language models. translation models CONJUNCTION language tagging tasks. language tagging tasks CONJUNCTION translation models. language tagging tasks USED-FOR representations. neural language models USED-FOR representations. translation models USED-FOR representations. networks USED-FOR language tasks. hidden representations USED-FOR networks. computer vision USED-FOR encoder - decoder transfer learning method. hidden representations USED-FOR feature spaces. language models CONJUNCTION translation models. translation models CONJUNCTION language models. word embeddings CONJUNCTION syntactic and semantic tasks. syntactic and semantic tasks CONJUNCTION word embeddings. syntactic and semantic tasks CONJUNCTION word embeddings. word embeddings CONJUNCTION syntactic and semantic tasks. method USED-FOR low - dimensional structure. it USED-FOR NLP ( natural language processing ) tasks. language representation embedding USED-FOR low - dimensional structure. feature space USED-FOR human brain responses. representation embedding USED-FOR feature space. natural language stimuli USED-FOR human brain responses. fMRI USED-FOR natural language stimuli. fMRI USED-FOR human brain responses. metric USED-FOR brain ’s natural language processing hierarchy. principal dimension USED-FOR metric. principal dimension FEATURE-OF structure. structure USED-FOR metric. embedding USED-FOR brain ’s natural language representation structure. ,"This paper proposes a new metric for measuring the structure of natural language representations in the brain. The proposed metric is based on an encoder-decoder transfer learning method that learns a low-dimensional representation embedding for the feature space of a neural language model. The authors show that the proposed metric can be used to measure the structure in the natural language processing hierarchy. The method is evaluated on a variety of NLP tasks, including word embeddings, syntactic and semantic tasks, and language tagging tasks. ","This paper proposes a new metric for measuring the structure of natural language representations in the brain. The proposed metric is based on an encoder-decoder transfer learning method that learns a low-dimensional representation embedding for the feature space of a neural language model. The authors show that the proposed metric can be used to measure the structure in the natural language processing hierarchy. The method is evaluated on a variety of NLP tasks, including word embeddings, syntactic and semantic tasks, and language tagging tasks. "
8456,SP:698d6c344fe94ea4ec3ce54601f5976d82d00b85,"paradigm USED-FOR unconditional variational autoencoders ( VAEs ). unconditional variational autoencoders ( VAEs ) USED-FOR few - shot conditional image generation. Diffusion - Decoding models CONJUNCTION Contrastive representations ( D2C ). Contrastive representations ( D2C ) CONJUNCTION Diffusion - Decoding models. diffusion - based prior USED-FOR generation. diffusion - based prior USED-FOR latent representations. D2C USED-FOR generation. contrastive selfsupervised learning USED-FOR representation quality. contrastive selfsupervised learning USED-FOR D2C. diffusion - based prior USED-FOR D2C. D2C USED-FOR generation tasks. D2C COMPARE diffusion models. diffusion models COMPARE D2C. D2C USED-FOR conditional generation. D2C generations COMPARE StyleGAN2 ones. StyleGAN2 ones COMPARE D2C generations. double - blind study EVALUATE-FOR human evaluators. D2C generations USED-FOR conditional image manipulation. double - blind study EVALUATE-FOR D2C generations. Method are Conditional generative models of high - dimensional images, and d2c. OtherScientificTerm are supervision signals, and manipulation constraints. ",This paper proposes a contrastive self-supervised learning method for few-shot conditional image generation based on diffusion-decoding models. The key idea is to use contrastive learning to improve the quality of the latent representations of the generated images. The authors show that the proposed method is able to achieve state-of-the-art performance on both synthetic and real-world datasets. ,This paper proposes a contrastive self-supervised learning method for few-shot conditional image generation based on diffusion-decoding models. The key idea is to use contrastive learning to improve the quality of the latent representations of the generated images. The authors show that the proposed method is able to achieve state-of-the-art performance on both synthetic and real-world datasets. 
8472,SP:36d11071cbf989e1f02232d39f52a42e781a5b2b,"contrastive learning paradigm USED-FOR representations. Edges PART-OF graph. ground - truth classes PART-OF connected sub - graphs. contrastive learning objective USED-FOR neural net representations. population augmentation graph USED-FOR spectral decomposition. spectral decomposition USED-FOR loss. contrastive learning objective USED-FOR loss. objective USED-FOR features. linear probe evaluation FEATURE-OF features. generalization bounds USED-FOR accuracy guarantees. objective COMPARE baselines. baselines COMPARE objective. features COMPARE baselines. baselines COMPARE features. objective USED-FOR features. benchmark vision datasets EVALUATE-FOR objective. benchmark vision datasets EVALUATE-FOR baselines. Task is self - supervised learning. OtherScientificTerm are conditional independence of the positive pairs, correlated positive pairs, conditional independence of positive pairs, and augmentation graph. Method is contrastive learning. Metric is training contrastive loss. ","This paper proposes a novel contrastive learning framework for self-supervised learning. The proposed method is based on the idea of population augmentation graph, which is an extension of the graph-based contrastive loss. The authors show that the proposed method outperforms the baselines in terms of generalization and accuracy. The paper also provides theoretical analysis of the generalization bounds. ","This paper proposes a novel contrastive learning framework for self-supervised learning. The proposed method is based on the idea of population augmentation graph, which is an extension of the graph-based contrastive loss. The authors show that the proposed method outperforms the baselines in terms of generalization and accuracy. The paper also provides theoretical analysis of the generalization bounds. "
8488,SP:ae4bc7f2a00feb13e458ab17804c06709374ceee,"parameterized complexity EVALUATE-FOR Bayesian Network Structure Learning ( BNSL ). complexity EVALUATE-FOR BNSL. parameterization USED-FOR fixed - parameter tractability. feedback edge set HYPONYM-OF parameterization. lower bounds USED-FOR complexity classification of BNSL. complexity classification EVALUATE-FOR BNSL. complexity EVALUATE-FOR BNSL. additive representation USED-FOR BNSL. OtherScientificTerm are superstructure, graph parameters, and treewidth. Task are fixed - parameter tractable, and Polytree Learning. Method is non - zero representation. ","This paper studies the complexity of Bayesian Network Structure Learning (BNSL) with parameterized complexity. The authors show that BNSL has a fixed-parameter tractability bound on the number of edges and treewidth of the feedback edge set. They also provide lower bounds on the complexity classification of the complexity. They show that the complexity is bounded by the additive representation of the graph parameters, which is a non-zero representation. ","This paper studies the complexity of Bayesian Network Structure Learning (BNSL) with parameterized complexity. The authors show that BNSL has a fixed-parameter tractability bound on the number of edges and treewidth of the feedback edge set. They also provide lower bounds on the complexity classification of the complexity. They show that the complexity is bounded by the additive representation of the graph parameters, which is a non-zero representation. "
8504,SP:3dc67f04c04466b0fe5aebb01c7578cd24caee0c,active learning algorithm USED-FOR binary classification tasks. active learning algorithm USED-FOR streaming setting. streaming setting USED-FOR binary classification tasks. model USED-FOR surrogate loss. algorithm USED-FOR model. labeled and weak - labeled points USED-FOR surrogate loss. weak labels USED-FOR algorithm. theoretical guarantees FEATURE-OF general agnostic setting. Uncertainty Sampling HYPONYM-OF active learning algorithm. algorithm COMPARE baselines. baselines COMPARE algorithm. Margin Algorithm CONJUNCTION Uncertainty Sampling. Uncertainty Sampling CONJUNCTION Margin Algorithm. generalization and label complexity bounds EVALUATE-FOR algorithm. Margin Algorithm HYPONYM-OF baselines. Uncertainty Sampling HYPONYM-OF baselines. Material is real - world datasets. ,This paper proposes an active learning algorithm for binary classification in streaming setting. The proposed algorithm is based on the Margin Algorithm and Uncertainty Sampling algorithms. The authors show that the proposed algorithm achieves better generalization and label complexity bounds compared to existing active learning algorithms. They also provide theoretical guarantees for general agnostic setting. ,This paper proposes an active learning algorithm for binary classification in streaming setting. The proposed algorithm is based on the Margin Algorithm and Uncertainty Sampling algorithms. The authors show that the proposed algorithm achieves better generalization and label complexity bounds compared to existing active learning algorithms. They also provide theoretical guarantees for general agnostic setting. 
8520,SP:0e5812d8ed33d5b6d9d59dbb2312c7b1c9363f3d,"classifiers USED-FOR invariant feature representations. classifier ’s function space USED-FOR generalization. complexity USED-FOR generalization. complexity FEATURE-OF classifier ’s function space. KC FEATURE-OF functions. measure USED-FOR generalization error bounds. complexity EVALUATE-FOR measure. complexity USED-FOR generalization error bounds. Kolmogorov Growth ( KG ) HYPONYM-OF measure. Occam ’s razor USED-FOR neural networks. generalization ability EVALUATE-FOR classifiers. approach USED-FOR classifiers. generalization ability EVALUATE-FOR approach. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. KG CONJUNCTION test accuracies. test accuracies CONJUNCTION KG. Kolmogorov Growth HYPONYM-OF function complexity prior. Method are classifier, complexity theory, network - to - network regularization, N2N regularization, and cross - entropy baselines. Metric is Kolmogorov complexity ( KC ). OtherScientificTerm are classification function, network trajectory, low KG zone, and training data sizes. Generic is bounds. Task is learning. ","This paper studies the generalization ability of neural networks under the Kolmogorov complexity (KG) measure. The authors show that the KG measure is a measure of the complexity of the classifier's function space, and that it can be used to derive generalization error bounds. They also show that under certain conditions, the KGs of a classifier can be reduced to a low KG zone, which is a lower bound on the generalizability of the network. They show that this lower bound can be obtained for any classifier, and they show that it holds for all classifiers. ","This paper studies the generalization ability of neural networks under the Kolmogorov complexity (KG) measure. The authors show that the KG measure is a measure of the complexity of the classifier's function space, and that it can be used to derive generalization error bounds. They also show that under certain conditions, the KGs of a classifier can be reduced to a low KG zone, which is a lower bound on the generalizability of the network. They show that this lower bound can be obtained for any classifier, and they show that it holds for all classifiers. "
8536,SP:d5608d3317c2b246375eb14006b9e6a6026e0ab6,"self - supervised methods USED-FOR image representation learning. self - supervised methods USED-FOR embedding vectors. encoders USED-FOR constant or non - informative vectors. regularizations terms USED-FOR embeddings. term CONJUNCTION term. term CONJUNCTION term. variance FEATURE-OF term. threshold FEATURE-OF variance. term HYPONYM-OF regularizations terms. term HYPONYM-OF regularizations terms. batch normalization CONJUNCTION feature - wise normalization. feature - wise normalization CONJUNCTION batch normalization. feature - wise normalization CONJUNCTION output quantization. output quantization CONJUNCTION feature - wise normalization. output quantization CONJUNCTION stop gradient. stop gradient CONJUNCTION output quantization. stop gradient CONJUNCTION memory banks. memory banks CONJUNCTION stop gradient. weight sharing CONJUNCTION batch normalization. batch normalization CONJUNCTION weight sharing. approaches USED-FOR problem. approaches COMPARE VICReg. VICReg COMPARE approaches. output quantization CONJUNCTION memory banks. memory banks CONJUNCTION output quantization. techniques USED-FOR VICReg. downstream tasks EVALUATE-FOR VICReg. stop gradient HYPONYM-OF techniques. weight sharing HYPONYM-OF techniques. memory banks HYPONYM-OF techniques. feature - wise normalization HYPONYM-OF techniques. output quantization HYPONYM-OF techniques. batch normalization HYPONYM-OF techniques. variance regularization term USED-FOR methods. Generic is method. OtherScientificTerm are collapse problem, and branches. ",This paper proposes a new regularization term for self-supervised image representation learning. The proposed term is based on the idea that the variance of an embedding is a function of the number of branches in the embedding space. The authors show that the proposed term can be used to improve the performance of existing methods on several downstream tasks.,This paper proposes a new regularization term for self-supervised image representation learning. The proposed term is based on the idea that the variance of an embedding is a function of the number of branches in the embedding space. The authors show that the proposed term can be used to improve the performance of existing methods on several downstream tasks.
8552,SP:bc8a9fcf7de41f1a1b0c6d0fc3fdcac5c5f87613,"model USED-FOR RL algorithms. model USED-FOR reward. expected returns EVALUATE-FOR RL algorithms. Bayesian model USED-FOR reward. Bayesian model USED-FOR Information Directed Reward Learning ( IDRL ). prior active reward learning methods COMPARE IDRL. IDRL COMPARE prior active reward learning methods. reward model USED-FOR policy. Task are reinforcement learning ( RL ) applications, and RL setting. OtherScientificTerm are binary preferences, expert queries, and reward approximation error. Metric is information gain. Generic is it. ","This paper proposes a Bayesian model for Information Directed Reward Learning (IDRL), which is an active RL algorithm that uses a reward model to estimate the expected return of a policy in an RL setting. The proposed method is based on the idea of Bayesian Bayes model, which can be used to learn a reward function for an agent. The authors show that the proposed method outperforms prior active reward learning methods in terms of expected return. ","This paper proposes a Bayesian model for Information Directed Reward Learning (IDRL), which is an active RL algorithm that uses a reward model to estimate the expected return of a policy in an RL setting. The proposed method is based on the idea of Bayesian Bayes model, which can be used to learn a reward function for an agent. The authors show that the proposed method outperforms prior active reward learning methods in terms of expected return. "
8568,SP:d88f2bb3ed48deb04fae1b8f008ca69d8566819f,Deep learning USED-FOR features. Deep learning USED-FOR machine learning pipelines. features PART-OF machine learning pipelines. algorithms USED-FOR neural network parameters. deep learning USED-FOR parameters. it USED-FOR parameter prediction. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. large - scale dataset USED-FOR parameter prediction. diverse computational graphs of neural architectures FEATURE-OF large - scale dataset. DEEPNETS-1 M HYPONYM-OF large - scale dataset. ImageNet USED-FOR parameter prediction. CIFAR-10 USED-FOR parameter prediction. DEEPNETS-1 M HYPONYM-OF diverse computational graphs of neural architectures. graph neural networks USED-FOR hypernetwork. accuracy EVALUATE-FOR it. CIFAR-10 EVALUATE-FOR it. ImageNet EVALUATE-FOR networks. top-5 accuracy EVALUATE-FOR networks. task CONJUNCTION model. model CONJUNCTION task. model USED-FOR neural architectures. OtherScientificTerm is CPU. Method is ResNet-50. Task is training networks. ,"This paper proposes a method for parameter prediction of deep neural networks. The method is based on graph neural networks (GNNs) and is trained on a large-scale dataset (CIFAR-10, ImageNet, and DEEPNETS-1M). The authors show that the proposed method is able to achieve state-of-the-art performance on ImageNet and DeepNet. The authors also show that their method can be applied to the task of parameter prediction.","This paper proposes a method for parameter prediction of deep neural networks. The method is based on graph neural networks (GNNs) and is trained on a large-scale dataset (CIFAR-10, ImageNet, and DEEPNETS-1M). The authors show that the proposed method is able to achieve state-of-the-art performance on ImageNet and DeepNet. The authors also show that their method can be applied to the task of parameter prediction."
8584,SP:8b233a2a5049ccda84e8840b97b800ffc5862e16,"distortion EVALUATE-FOR estimator. perception constraint FEATURE-OF minimal distortion. closed form expression USED-FOR distortion - perception ( DP ) function. mean squared - error ( MSE ) distortion CONJUNCTION Wasserstein-2 perception index. Wasserstein-2 perception index CONJUNCTION mean squared - error ( MSE ) distortion. closed form expression USED-FOR Wasserstein-2 perception index. mean squared - error ( MSE ) distortion EVALUATE-FOR distortion - perception ( DP ) function. closed form expression USED-FOR estimators. closed form expression USED-FOR Gaussian setting. global MSE minimizer CONJUNCTION minimizer. minimizer CONJUNCTION global MSE minimizer. global MSE minimizer CONJUNCTION MSE. MSE CONJUNCTION global MSE minimizer. minimizer FEATURE-OF MSE. perfect perceptual quality constraint FEATURE-OF minimizer. perfect perceptual quality constraint FEATURE-OF MSE. minimizer HYPONYM-OF tradeoff. global MSE minimizer HYPONYM-OF tradeoff. estimators USED-FOR estimators. stochastic transformation of the former USED-FOR latter. Metric are perception - distortion tradeoff, fidelity, and perceptual quality. Task is image restoration. OtherScientificTerm are statistics of natural images, perception - distortion plane, DP function, DP curve, and geodesic in Wasserstein space. ","This paper studies the minimization of the mean squared-error (MSE) distortion and the Wasserstein-2 perception index (WPI) in the context of image restoration. The authors propose a closed form expression of the MSE and WPI for the Gaussian setting, and show that the global MSE minimizer and the global WPI minimizer are equivalent to the minimizer of the WPI and MSE, respectively. They also show that a stochastic transformation of the former is necessary to obtain the global minimizer.","This paper studies the minimization of the mean squared-error (MSE) distortion and the Wasserstein-2 perception index (WPI) in the context of image restoration. The authors propose a closed form expression of the MSE and WPI for the Gaussian setting, and show that the global MSE minimizer and the global WPI minimizer are equivalent to the minimizer of the WPI and MSE, respectively. They also show that a stochastic transformation of the former is necessary to obtain the global minimizer."
8600,SP:60ce257ca7c1dbbc88e4f36bad40f7eeb133368a,textual features CONJUNCTION neighbourhood information. neighbourhood information CONJUNCTION textual features. low - dimensional embeddings USED-FOR nodes. textual features USED-FOR low - dimensional embeddings. neighbourhood information USED-FOR low - dimensional embeddings. pretrained language models CONJUNCTION graph neural networks. graph neural networks CONJUNCTION pretrained language models. graph neural networks USED-FOR techniques. textual features FEATURE-OF nodes. language models USED-FOR textual features. graph neural networks USED-FOR textual embeddings. layerwise GNN components CONJUNCTION transformer blocks of language models. transformer blocks of language models CONJUNCTION layerwise GNN components. layerwise GNN components PART-OF GraphFormers. text encoding CONJUNCTION graph aggregation. graph aggregation CONJUNCTION text encoding. graph aggregation PART-OF iterative workflow. text encoding PART-OF iterative workflow. manipulated data CONJUNCTION original data. original data CONJUNCTION manipulated data. model PART-OF progressive learning strategy. manipulated data USED-FOR model. original data USED-FOR model. GraphFormers COMPARE SOTA baselines. SOTA baselines COMPARE GraphFormers. large - scale benchmark datasets EVALUATE-FOR GraphFormers. running efficiency EVALUATE-FOR SOTA baselines. running efficiency EVALUATE-FOR GraphFormers. OtherScientificTerm is textual graph. Method is cascaded model architecture. Generic is architecture. Task is independent modeling of textual features. ,This paper proposes a new model architecture for the task of independent modeling of textual features. The proposed model is based on graph neural networks (GNNs) and transformer blocks of language models. The authors show that the proposed model outperforms SOTA baselines on several benchmark datasets. ,This paper proposes a new model architecture for the task of independent modeling of textual features. The proposed model is based on graph neural networks (GNNs) and transformer blocks of language models. The authors show that the proposed model outperforms SOTA baselines on several benchmark datasets. 
8616,SP:199a281592df47d71c57fdcbd24b40a0b0de9d76,algorithms USED-FOR learning tasks. userlevel differential privacy constraints FEATURE-OF learning tasks. empirical risk minimization CONJUNCTION stochastic convex optimization. stochastic convex optimization CONJUNCTION empirical risk minimization. stochastic convex optimization CONJUNCTION learning hypothesis classes. learning hypothesis classes CONJUNCTION stochastic convex optimization. high - dimensional mean estimation CONJUNCTION empirical risk minimization. empirical risk minimization CONJUNCTION high - dimensional mean estimation. smooth losses FEATURE-OF empirical risk minimization. finite metric entropy FEATURE-OF learning hypothesis classes. O(1 / n ) rate FEATURE-OF privacy cost. mean estimation CONJUNCTION stochastic convex optimization. stochastic convex optimization CONJUNCTION mean estimation. algorithms USED-FOR mean estimation. algorithms USED-FOR stochastic convex optimization. minimax optimality FEATURE-OF algorithms. lower bounds USED-FOR minimax optimality. lower bounds USED-FOR algorithms. techniques USED-FOR private mean estimation. techniques USED-FOR arbitrary dimension. error scaling FEATURE-OF techniques. private mean estimation USED-FOR algorithms. arbitrary dimension FEATURE-OF private mean estimation. techniques USED-FOR algorithms. Method is user - level DP. OtherScientificTerm is information leaks. ,"This paper studies the problem of user-level differential privacy (DP) in the context of stochastic convex optimization, mean estimation, and empirical risk minimization. The authors consider the case where the privacy cost is O(1/n) times larger than the privacy rate of the algorithm. They show that for any arbitrary dimension, there exists a private mean estimation algorithm that achieves O(\sqrt{n}) times larger privacy rate than the algorithm in the original setting. They also provide lower bounds on the error scaling of the algorithms. ","This paper studies the problem of user-level differential privacy (DP) in the context of stochastic convex optimization, mean estimation, and empirical risk minimization. The authors consider the case where the privacy cost is O(1/n) times larger than the privacy rate of the algorithm. They show that for any arbitrary dimension, there exists a private mean estimation algorithm that achieves O(\sqrt{n}) times larger privacy rate than the algorithm in the original setting. They also provide lower bounds on the error scaling of the algorithms. "
8632,SP:ad5b98e656cac6eb931f80d852c397d117cf1609,"they USED-FOR deep learning. infinite width / channel limit FEATURE-OF Deep neural networks ( DNNs ). deep learning USED-FOR finite DNNs. self - consistent Gaussian Process theory USED-FOR finite - DNN and feature learning effects. noisy gradient descent USED-FOR DNNs. this USED-FOR toy model. feature learning regime CONJUNCTION lazy learning regime. lazy learning regime CONJUNCTION feature learning regime. CIFAR-10 USED-FOR Myrtle5 CNN. self - consistent theory USED-FOR finite - DNN effects. self - consistent theory USED-FOR feature learning. feature learning HYPONYM-OF finite - DNN effects. Method is Gaussian Processes ( GPs ). Generic are model, and theory. ",This paper studies the effect of finite-DNN effects on feature learning in the infinite width/channel limit of deep neural networks (DNNs). The authors propose a self-consistent Gaussian Process (GPs) theory to explain the finite-dNN effects. The authors show that the Gaussian process can be viewed as a toy model that can be used to train a toy DNN with noisy gradient descent. They also show that this toy model can be applied to the feature learning regime. ,This paper studies the effect of finite-DNN effects on feature learning in the infinite width/channel limit of deep neural networks (DNNs). The authors propose a self-consistent Gaussian Process (GPs) theory to explain the finite-dNN effects. The authors show that the Gaussian process can be viewed as a toy model that can be used to train a toy DNN with noisy gradient descent. They also show that this toy model can be applied to the feature learning regime. 
8648,SP:231655b9fad6c76eb0ff1ba305ed421f5c293623,"inductive biases USED-FOR compositional communication. training framework USED-FOR inductive biases. signaling games USED-FOR compositionality. noise levels USED-FOR compositionality. conflict count CONJUNCTION context independence. context independence CONJUNCTION conflict count. topographical similarity CONJUNCTION conflict count. conflict count CONJUNCTION topographical similarity. context independence HYPONYM-OF compositionality metrics. topographical similarity HYPONYM-OF compositionality metrics. conflict count HYPONYM-OF compositionality metrics. Task is Communication. OtherScientificTerm are complex signals, and noisy channel. Generic is model. ",This paper studies the problem of learning compositional communication in a noisy communication setting. The authors propose a novel training framework to train a model that learns to learn compositionality metrics. The proposed framework is based on the notion of topographical similarity and topographical independence. The paper also proposes a new compositionality metric to measure the relationship between noise levels and compositionality. Experiments show that the proposed method is able to learn compositional metrics that are more robust to noise levels.,This paper studies the problem of learning compositional communication in a noisy communication setting. The authors propose a novel training framework to train a model that learns to learn compositionality metrics. The proposed framework is based on the notion of topographical similarity and topographical independence. The paper also proposes a new compositionality metric to measure the relationship between noise levels and compositionality. Experiments show that the proposed method is able to learn compositional metrics that are more robust to noise levels.
8664,SP:9d326254d77a188baf5bde39229c09b3966b5418,"ResMLP HYPONYM-OF architecture. multi - layer perceptrons USED-FOR image classification. architecture USED-FOR image classification. multi - layer perceptrons USED-FOR ResMLP. multi - layer perceptrons USED-FOR architecture. It HYPONYM-OF residual network. heavy data - augmentation CONJUNCTION distillation. distillation CONJUNCTION heavy data - augmentation. training strategy USED-FOR it. distillation USED-FOR training strategy. heavy data - augmentation USED-FOR training strategy. ImageNet EVALUATE-FOR it. self - supervised setup USED-FOR ResMLP models. labelled dataset USED-FOR priors. model USED-FOR machine translation. Timm library CONJUNCTION pre - trained models. pre - trained models CONJUNCTION Timm library. OtherScientificTerm are image patches, and channels. Method is two - layer feed - forward network. ",This paper proposes a two-layer feed-forward network for image classification. The proposed method is based on the multi-layer perceptron architecture. The authors show that the proposed method outperforms the state-of-the-art on ImageNet and a self-supervised dataset for machine translation. ,This paper proposes a two-layer feed-forward network for image classification. The proposed method is based on the multi-layer perceptron architecture. The authors show that the proposed method outperforms the state-of-the-art on ImageNet and a self-supervised dataset for machine translation. 
8680,SP:6dabaca9a77620b7c4019bf5f9c2a88628fc691c,"regret guarantees USED-FOR geometric problem of contextual search. multiclass classification CONJUNCTION binary classification. binary classification CONJUNCTION multiclass classification. Task is multi - class classification. Metric is misclassification rate. Method are nearest neighbor partition, and reduction technique. OtherScientificTerm is Euclidean distance. ","This paper studies the problem of contextual search for multi-class classification. The authors propose a new method to reduce the misclassification rate in the context of the Euclidean distance problem. The proposed method is based on the notion of nearest neighbor partition, which is an extension of the nearest neighbor reduction (NNDP) framework. The main contribution of the paper is to show that the NNDP can be reduced to a single-class NDP. The paper also shows that the proposed method can be extended to the case of multiclass classification and binary classification.","This paper studies the problem of contextual search for multi-class classification. The authors propose a new method to reduce the misclassification rate in the context of the Euclidean distance problem. The proposed method is based on the notion of nearest neighbor partition, which is an extension of the nearest neighbor reduction (NNDP) framework. The main contribution of the paper is to show that the NNDP can be reduced to a single-class NDP. The paper also shows that the proposed method can be extended to the case of multiclass classification and binary classification."
8696,SP:5c0114535065d5125349f00bafdbccc911461ede,"Methods USED-FOR Visual Question Anwering ( VQA ). dataset biases COMPARE reasoning. reasoning COMPARE dataset biases. attention layers PART-OF VQA model. attention layers FEATURE-OF reasoning patterns. perfect ( oracle ) visual inputs USED-FOR they. method USED-FOR knowledge transfer. regularization term PART-OF loss function. regularization term USED-FOR method. sample complexity EVALUATE-FOR program prediction. PAC - learning USED-FOR theoretical analysis. GQA dataset EVALUATE-FOR approach. Task is generalization. Method is deep neural networks. Generic are models, and transfer. OtherScientificTerm is reasoning operations. ",This paper proposes a PAC-learning method for Visual Question Anwering (VQA). The proposed method is based on a regularization term in the loss function. The authors show that the proposed method outperforms baselines on the GQA dataset.,This paper proposes a PAC-learning method for Visual Question Anwering (VQA). The proposed method is based on a regularization term in the loss function. The authors show that the proposed method outperforms baselines on the GQA dataset.
8712,SP:40fd96105e77063de4a07d4b36fe19385434c533,"neurons of fixed precision FEATURE-OF dynamically growing memory module. 54 - neuron bounded - precision RNN USED-FOR Universal Turing Machine. growing memory modules PART-OF 54 - neuron bounded - precision RNN. Turing completeness FEATURE-OF unbounded - precision and boundedprecision RNNs. Method are recurrent neural networks ( RNNs ), RNNs, memory module, and stack - augmented RNNs. OtherScientificTerm are unbounded precision, simulated machine ’s time, and memory size. Metric is time complexity. ","This paper studies the problem of unbounded precision and bounded precision RNNs. The authors show that unbounded-precision RNN can be used as a universal Turing machine, which is a generalization of the Turing machine. They also show that bounded precision and unbounded memory modules can be combined to form a new RNN with unbounded and bounded memory modules.   ","This paper studies the problem of unbounded precision and bounded precision RNNs. The authors show that unbounded-precision RNN can be used as a universal Turing machine, which is a generalization of the Turing machine. They also show that bounded precision and unbounded memory modules can be combined to form a new RNN with unbounded and bounded memory modules.   "
8728,SP:3f33489b98ba6145fd4e334669493f15a63455f4,"Estimating the data uncertainty USED-FOR regression tasks. quantile function USED-FOR Estimating the data uncertainty. vanilla algorithm USED-FOR quantiles. uncertainty estimation algorithms USED-FOR quantiles. vanilla setting USED-FOR realizable linear quantile function. under - coverage bias FEATURE-OF quantile regression. α CONJUNCTION d / n. d / n CONJUNCTION α. quantile regression USED-FOR α - quantile. highdimensional parameter estimation error USED-FOR under - coverage bias. sample size CONJUNCTION model capacity. model capacity CONJUNCTION sample size. simulated and real data EVALUATE-FOR theory. model capacity FEATURE-OF under - coverage bias. OtherScientificTerm are asymptotic guarantees, coverage level, and noise distribution. ",This paper studies the under-covering bias of quantile regression under the assumption that the quantile function is linear in the data distribution. The authors show that under-comparison bias is a result of the high-dimensional parameter estimation error and the model capacity. They show that this bias can be reduced to a low-dimensional bias in the case where the sample size and model capacity are small. They also show that the bias is bounded by the noise distribution. ,This paper studies the under-covering bias of quantile regression under the assumption that the quantile function is linear in the data distribution. The authors show that under-comparison bias is a result of the high-dimensional parameter estimation error and the model capacity. They show that this bias can be reduced to a low-dimensional bias in the case where the sample size and model capacity are small. They also show that the bias is bounded by the noise distribution. 
8744,SP:ecb9c7c11dfb450d8e76504d42309b1888023d26,"strict memory budget USED-FOR classifiers. learning USED-FOR incremental phase. static and ad hoc strategy USED-FOR memory allocation. dynamic memory management strategy USED-FOR incremental phases. incremental phases CONJUNCTION object classes. object classes CONJUNCTION incremental phases. dynamic memory management strategy USED-FOR object classes. reinforcement learning USED-FOR reinforced memory management ( RMM ). it USED-FOR tasks. RMM USED-FOR pseudo CIL tasks. policy function USED-FOR pseudo CIL tasks. policy function USED-FOR RMM. tasks HYPONYM-OF pseudo CIL tasks. it USED-FOR replaying - based CIL method. it USED-FOR memory management. LUCIR+AANets CONJUNCTION POD+AANets. POD+AANets CONJUNCTION LUCIR+AANets. RMM USED-FOR top - performing baselines. POD+AANets HYPONYM-OF top - performing baselines. LUCIR+AANets HYPONYM-OF top - performing baselines. ImageNet - Subset CONJUNCTION ImageNet - Full. ImageNet - Full CONJUNCTION ImageNet - Subset. CIFAR-100 CONJUNCTION ImageNet - Subset. ImageNet - Subset CONJUNCTION CIFAR-100. CIFAR-100 HYPONYM-OF benchmarks. ImageNet - Subset HYPONYM-OF benchmarks. ImageNet - Full HYPONYM-OF benchmarks. Method are Class - Incremental Learning ( CIL ), and RMM training. Task are replaying, and CIL. ","This paper proposes a reinforcement learning-based reinforcement learning approach for class-incremental learning (CIL). The key idea is to use reinforcement learning to improve the performance of the classifier during the incremental phase of CIL. This is done by learning a policy function that can be used to train a classifier on a set of pseudo-CIL tasks. The proposed approach is evaluated on a number of CIFAR-10, ImageNet-100, and LUCIR+AANet benchmarks. The paper shows that the proposed approach outperforms the baselines in terms of performance. ","This paper proposes a reinforcement learning-based reinforcement learning approach for class-incremental learning (CIL). The key idea is to use reinforcement learning to improve the performance of the classifier during the incremental phase of CIL. This is done by learning a policy function that can be used to train a classifier on a set of pseudo-CIL tasks. The proposed approach is evaluated on a number of CIFAR-10, ImageNet-100, and LUCIR+AANet benchmarks. The paper shows that the proposed approach outperforms the baselines in terms of performance. "
8760,SP:1137ed24393a24f24e9a36e1586e6924a55d627e,"it USED-FOR speeding up stochastic gradient descent ( SGD ). Ω ( √ T ) communications USED-FOR local gradient steps. Ω ( √ T ) communications USED-FOR it. linear speed - up EVALUATE-FOR √ N or N communications. optimal convergence rate EVALUATE-FOR one - shot averaging. Method are stochastic gradient descent ( SGD ), Local SGD method, Local SGD, and Local SGD scheme. OtherScientificTerm are SGD steps, stochastic gradients, communication, parallelism, Ω(N ) communications, and twice differentiability. Task is linear reduction in the variance. Metric is error. ","This paper studies the problem of speeding up stochastic gradient descent (SGD) in the context of parallelism. The authors propose a new local SGD method, called Local SGD, which is based on two differentiability properties: (1) parallelism, and (2) twice differentiability. They show that the convergence rate of the proposed method converges to the optimal convergence rate for one-shot averaging. They also provide a theoretical analysis of their method.","This paper studies the problem of speeding up stochastic gradient descent (SGD) in the context of parallelism. The authors propose a new local SGD method, called Local SGD, which is based on two differentiability properties: (1) parallelism, and (2) twice differentiability. They show that the convergence rate of the proposed method converges to the optimal convergence rate for one-shot averaging. They also provide a theoretical analysis of their method."
8776,SP:8a78fee6173dc6639dfd9e33a10d0c8432a08512,"Online Lazy Gradient Descent USED-FOR optimisation. strongly convex domain FEATURE-OF optimisation. O ( √ N ) regret EVALUATE-FOR algorithm. expected regret EVALUATE-FOR it. pseudo - regret CONJUNCTION expected regret. expected regret CONJUNCTION pseudo - regret. order bounds FEATURE-OF strongly convex domains. order bounds FEATURE-OF expected regret. order bounds FEATURE-OF pseudo - regret. OtherScientificTerm are adversarial opponents, i.i.d opponents, O(logN ) bounds, and simplex. Method is metaalgorithm. ","This paper studies the online Lazy Gradient Descent (OGD) problem in the strongly convex domain. The authors consider the case where there are adversarial opponents, i.e. a set of $n$-strongly-convex points $n$, and the goal is to minimize the expected regret of the optimizer. They show that under certain assumptions, the algorithm can achieve O(logN) regret, which is the upper bound of the expected and pseudo-regret of the algorithm. They also show that the algorithm achieves O(n^2) order bounds on the pseudo-reward and expected regret. ","This paper studies the online Lazy Gradient Descent (OGD) problem in the strongly convex domain. The authors consider the case where there are adversarial opponents, i.e. a set of $n$-strongly-convex points $n$, and the goal is to minimize the expected regret of the optimizer. They show that under certain assumptions, the algorithm can achieve O(logN) regret, which is the upper bound of the expected and pseudo-regret of the algorithm. They also show that the algorithm achieves O(n^2) order bounds on the pseudo-reward and expected regret. "
8792,SP:59e7ff1cdee42c9623615f6105c0e0f44e7b75a5,"Affine - Invariant ( AI ) geometry USED-FOR Riemannian optimization. Bures - Wasserstein ( BW ) geometry USED-FOR Riemannian optimization. Bures - Wasserstein ( BW ) geometry COMPARE Affine - Invariant ( AI ) geometry. Affine - Invariant ( AI ) geometry COMPARE Bures - Wasserstein ( BW ) geometry. symmetric positive definite ( SPD ) matrix manifold FEATURE-OF Riemannian optimization. linear dependence FEATURE-OF BW metric. BW metric USED-FOR Riemannian optimization problems. ill - conditioned SPD matrices USED-FOR Riemannian optimization problems. non - negative curvature FEATURE-OF BW geometry. OtherScientificTerm are SPD matrices, AI metric, and non - positively curved AI geometry. Metric is convergence rates. Method are cost functions, and AI geometry. Generic is applications. ","This paper studies the Affine Invariant (AI) and Bures-Wasserstein (BW) geometry for Riemannian optimization problems. The authors show that the non-negative curvature of Affine-invariant AI geometry is non-negligible. They also show that BW geometry has a linear dependence on the cost function, which is a result of the fact that the cost functions are non-positive definite. They show that this linear dependence can be extended to the case of ill-conditioned SPD matrices, and show that it can be used as a metric for the convergence rate of the algorithm. ","This paper studies the Affine Invariant (AI) and Bures-Wasserstein (BW) geometry for Riemannian optimization problems. The authors show that the non-negative curvature of Affine-invariant AI geometry is non-negligible. They also show that BW geometry has a linear dependence on the cost function, which is a result of the fact that the cost functions are non-positive definite. They show that this linear dependence can be extended to the case of ill-conditioned SPD matrices, and show that it can be used as a metric for the convergence rate of the algorithm. "
8808,SP:1f835a54c74d396ae2e8620b01bed0ec53646f3a,"Dynaboard HYPONYM-OF evaluation - as - a - service framework. evaluation - as - a - service framework USED-FOR holistic model comparison. Dynaboard CONJUNCTION Dynabench platform. Dynabench platform CONJUNCTION Dynaboard. platform USED-FOR NLP models. reproducibility CONJUNCTION accessibility. accessibility CONJUNCTION reproducibility. accessibility CONJUNCTION backwards compatibility. backwards compatibility CONJUNCTION accessibility. benchmarking USED-FOR NLP. memory use CONJUNCTION throughput. throughput CONJUNCTION memory use. throughput CONJUNCTION robustness. robustness CONJUNCTION throughput. robustness HYPONYM-OF metrics. memory use HYPONYM-OF metrics. throughput HYPONYM-OF metrics. Dynascore HYPONYM-OF utility - based aggregation. NLP models COMPARE benchmarks. benchmarks COMPARE NLP models. OtherScientificTerm is selfreported metrics. Generic are paradigm, models, and task. Material is leaderboards. ","This paper proposes a framework for benchmarking NLP models. The framework is based on Dynaboard, which is an evaluation-as-a-service framework for holistic model comparison. The proposed framework is built on top of Dynascore, a utility-based aggregation framework, and Dynabench, a multi-task benchmarking framework. The authors show that the proposed framework outperforms the baselines on a number of metrics, including robustness, throughput, and memory use.","This paper proposes a framework for benchmarking NLP models. The framework is based on Dynaboard, which is an evaluation-as-a-service framework for holistic model comparison. The proposed framework is built on top of Dynascore, a utility-based aggregation framework, and Dynabench, a multi-task benchmarking framework. The authors show that the proposed framework outperforms the baselines on a number of metrics, including robustness, throughput, and memory use."
8824,SP:44dc6a69f5d65ca0b271177ac67d1beb12a154a0,"filmmaking CONJUNCTION video production. video production CONJUNCTION filmmaking. re - recording actors ’ dialogues USED-FOR filmmaking. re - recording actors ’ dialogues USED-FOR video production. re - recording actors ’ dialogues USED-FOR Dubbing. Neural Dubber HYPONYM-OF neural network model. Neural Dubber USED-FOR automatic video dubbing ( AVD ) task. neural network model USED-FOR automatic video dubbing ( AVD ) task. Neural Dubber USED-FOR synthesizing human speech. synthesizing human speech HYPONYM-OF automatic video dubbing ( AVD ) task. lip movement USED-FOR Neural Dubber. Neural Dubber USED-FOR speech. timbre FEATURE-OF Neural Dubber. timbre FEATURE-OF speech. chemistry lecture single - speaker dataset CONJUNCTION LRS2 multi - speaker dataset. LRS2 multi - speaker dataset CONJUNCTION chemistry lecture single - speaker dataset. Neural Dubber COMPARE TTS models. TTS models COMPARE Neural Dubber. chemistry lecture single - speaker dataset EVALUATE-FOR Neural Dubber. LRS2 multi - speaker dataset EVALUATE-FOR Neural Dubber. Neural Dubber USED-FOR speech audios. speech audios COMPARE TTS models. TTS models COMPARE speech audios. speech quality EVALUATE-FOR TTS models. speech quality EVALUATE-FOR Neural Dubber. Neural Dubber USED-FOR high - fidelity speech. qualitative and quantitative evaluations EVALUATE-FOR Neural Dubber. Generic is It. Material are pre - recorded videos, and multi - speaker setting. ","This paper proposes Neural Dubber, a neural network model for automatic video dubbing (AVD) task. Dubbing is an important task in video production and filmmaking. The authors propose a method to synthesize human speech from pre-recorded videos. The proposed method is evaluated on a number of video datasets, including LRS2 multi-speaker dataset, chemistry lecture dataset, and multi-Speaker dataset. The results show that the proposed method outperforms the state-of-the-art TTS models on all datasets. ","This paper proposes Neural Dubber, a neural network model for automatic video dubbing (AVD) task. Dubbing is an important task in video production and filmmaking. The authors propose a method to synthesize human speech from pre-recorded videos. The proposed method is evaluated on a number of video datasets, including LRS2 multi-speaker dataset, chemistry lecture dataset, and multi-Speaker dataset. The results show that the proposed method outperforms the state-of-the-art TTS models on all datasets. "
8840,SP:24ea12428bd675459f0509aa7cee821fa236382e,"Federated learning USED-FOR healthcare sector. neural network training USED-FOR COVID-19 diagnosis. chest X - ray ( CXR ) images USED-FOR neural network training. chest X - ray ( CXR ) images USED-FOR COVID-19 diagnosis. Vision Transformer USED-FOR split learning. Vision Transformer HYPONYM-OF deep learning architecture. decomposable configuration FEATURE-OF deep learning architecture. framework COMPARE data - centralized training. data - centralized training COMPARE framework. CXR datasets USED-FOR framework. CXR datasets USED-FOR non - independent and identically distributed data distribution. framework CONJUNCTION heterogeneous multi - task clients. heterogeneous multi - task clients CONJUNCTION framework. Transformer USED-FOR collaborative learning. collaborative learning USED-FOR medical imaging. Transformer USED-FOR medical imaging. Method are neural network, and network architecture. Generic are it, network, and methods. Material are decentralized data, and patient CXR data. OtherScientificTerm are data privacy, and network bandwidth. Task is diagnosis of COVID-19. ","This paper proposes a new method for collaborative learning for medical imaging. The proposed method is based on the Vision Transformer architecture, which is a decomposable configuration of deep learning models. The authors show that the proposed method can be applied to a wide range of medical imaging datasets, including CXR, CIFAR-10, and COVID-19 datasets. The method is evaluated on a variety of datasets, and the results show that it outperforms the state-of-the-art. ","This paper proposes a new method for collaborative learning for medical imaging. The proposed method is based on the Vision Transformer architecture, which is a decomposable configuration of deep learning models. The authors show that the proposed method can be applied to a wide range of medical imaging datasets, including CXR, CIFAR-10, and COVID-19 datasets. The method is evaluated on a variety of datasets, and the results show that it outperforms the state-of-the-art. "
8856,SP:2065a8cb8b53140569b64fca1f00f7230f1ae2cc,"neural implicit representations USED-FOR 3D reconstruction. expressiveness CONJUNCTION flexibility. flexibility CONJUNCTION expressiveness. flexibility EVALUATE-FOR neural implicit representations. expressiveness EVALUATE-FOR neural implicit representations. oriented point cloud USED-FOR indicator function. differentiable PSR layer USED-FOR explicit 3D point representation. 3D mesh USED-FOR explicit 3D point representation. Chamfer distance HYPONYM-OF surface reconstruction metrics. oriented point clouds USED-FOR shapes. points CONJUNCTION patches. patches CONJUNCTION points. patches CONJUNCTION meshes. meshes CONJUNCTION patches. SAP USED-FOR topology - agnostic, watertight manifold surfaces. explicit representations COMPARE SAP. SAP COMPARE explicit representations. meshes HYPONYM-OF explicit representations. points HYPONYM-OF explicit representations. patches HYPONYM-OF explicit representations. SAP USED-FOR surface reconstruction. SAP USED-FOR learning - based reconstruction. unoriented point clouds CONJUNCTION learning - based reconstruction. learning - based reconstruction CONJUNCTION unoriented point clouds. learning - based reconstruction USED-FOR surface reconstruction. unoriented point clouds USED-FOR surface reconstruction. Metric are slow inference time, and inference time. Method are ubiquitous point cloud representation, differentiable point - to - mesh layer, and Poisson Surface Reconstruction ( PSR ). OtherScientificTerm is implicit indicator field. ","This paper proposes a differentiable point-to-mesh (SAP) layer for 3D surface reconstruction. The main contribution of this paper is to introduce a new point cloud representation for surface reconstruction, which is differentiable and can be used for learning-based reconstruction and learning-free learning. The proposed method is evaluated on a variety of surface reconstruction metrics, including Chamfer distance, topology-agnostic surface reconstruction and surface reconstruction on watertight manifold surfaces. The paper shows that the proposed method outperforms the state-of-the-art in terms of speed and accuracy.","This paper proposes a differentiable point-to-mesh (SAP) layer for 3D surface reconstruction. The main contribution of this paper is to introduce a new point cloud representation for surface reconstruction, which is differentiable and can be used for learning-based reconstruction and learning-free learning. The proposed method is evaluated on a variety of surface reconstruction metrics, including Chamfer distance, topology-agnostic surface reconstruction and surface reconstruction on watertight manifold surfaces. The paper shows that the proposed method outperforms the state-of-the-art in terms of speed and accuracy."
8872,SP:76b64e6b104818ed26e9331d134df0125d84291c,inverse problems USED-FOR recovering representations of corrupted data. pre - trained representation learning network R(x ) USED-FOR clean images. CLIP HYPONYM-OF clean images. representations USED-FOR corrupted images. supervised inversion method USED-FOR representations. forward operator USED-FOR corrupted version A(x ). contrastive objective USED-FOR supervised inversion method. blurring CONJUNCTION additive noise. additive noise CONJUNCTION blurring. linear probe USED-FOR robust representations. additive noise CONJUNCTION random pixel masking. random pixel masking CONJUNCTION additive noise. end - to - end supervised baselines USED-FOR classifying images. accuracy EVALUATE-FOR end - to - end supervised baselines. linear probe USED-FOR classifying images. linear probe COMPARE end - to - end supervised baselines. end - to - end supervised baselines COMPARE linear probe. distortions FEATURE-OF classifying images. random pixel masking HYPONYM-OF distortions. blurring HYPONYM-OF distortions. additive noise HYPONYM-OF distortions. ImageNet EVALUATE-FOR method. distortion FEATURE-OF method. method COMPARE end - to - end baselines. end - to - end baselines COMPARE method. forward operators FEATURE-OF labeled data. labeled data USED-FOR method. Material is images. ,"This paper proposes a supervised inversion method for recovering representations of corrupted images. The proposed method is based on a contrastive objective, which is used to learn representations of the corrupted version A(x) from the clean version. The authors show that the proposed method outperforms end-to-end supervised baselines on ImageNet and CLIP datasets. ","This paper proposes a supervised inversion method for recovering representations of corrupted images. The proposed method is based on a contrastive objective, which is used to learn representations of the corrupted version A(x) from the clean version. The authors show that the proposed method outperforms end-to-end supervised baselines on ImageNet and CLIP datasets. "
8888,SP:ca846ae9653fa843a6a64ce7361d44a0c31c5990,backpropagation USED-FOR local training of nodes. neural networks USED-FOR Structural credit assignment. reinforcement learning method USED-FOR node. REINFORCE USED-FOR node. global reward signal USED-FOR reinforcement learning method. REINFORCE HYPONYM-OF reinforcement learning method. reinforcement learning approaches USED-FOR learning. finite - horizon reinforcement learning problem USED-FOR neural network. off - policy learning HYPONYM-OF reinforcement learning. on - policy REINFORCE approach USED-FOR suboptimal solutions. on - policy REINFORCE approach CONJUNCTION variance reduction approaches. variance reduction approaches CONJUNCTION on - policy REINFORCE approach. networks of agents USED-FOR correlated samples. Generic is approach. Method is off - policy approach. OtherScientificTerm is stochasticity. ,"This paper proposes a new reinforcement learning method, REINFORCE, for the problem of structural credit assignment in reinforcement learning. The proposed method is based on backpropagation, which is an off-policy reinforcement learning approach. The authors show that the proposed method outperforms the baselines on a finite-horizon reinforcement learning problem. ","This paper proposes a new reinforcement learning method, REINFORCE, for the problem of structural credit assignment in reinforcement learning. The proposed method is based on backpropagation, which is an off-policy reinforcement learning approach. The authors show that the proposed method outperforms the baselines on a finite-horizon reinforcement learning problem. "
8904,SP:7aa09356b2c85d54933c0d0d89a3f8fe2e37b27b,"parallel, hierarchical specialized pathways PART-OF visual system. pathways USED-FOR behaviours. visual recognition and movement FEATURE-OF behaviours. deep neural networks USED-FOR ventral, recognition pathway. deep ANN USED-FOR pathways. model USED-FOR ventral and the dorsal pathways. loss function USED-FOR ventral and the dorsal pathways. loss function USED-FOR model. models USED-FOR mouse visual cortex. self - supervised predictive loss function USED-FOR parallel pathways. parallel pathways USED-FOR deep neural network architecture. self - supervised predictive loss function USED-FOR deep neural network architecture. self - supervised predictive learning approach USED-FOR functional specialization. self - supervised predictive learning approach USED-FOR parallel pathway architectures. functional specialization FEATURE-OF mammalian visual systems. Material is mice. Task is recognition and movement behaviours. OtherScientificTerm is dorsal and ventral pathways. ",This paper proposes a self-supervised predictive loss function for parallel pathways in the mouse visual cortex. The proposed loss function is based on a deep neural network architecture. The authors show that the proposed method is able to learn functional specialization between the dorsal and ventral pathways. The method is evaluated on a variety of tasks including visual recognition and movement.,This paper proposes a self-supervised predictive loss function for parallel pathways in the mouse visual cortex. The proposed loss function is based on a deep neural network architecture. The authors show that the proposed method is able to learn functional specialization between the dorsal and ventral pathways. The method is evaluated on a variety of tasks including visual recognition and movement.
8920,SP:fa11c4da16c01c6a3449f15b25a6e4e228ebbf4a,"deep hierarchical topic models USED-FOR semantically meaningful topics. them PART-OF topic hierarchy. text corpus USED-FOR deep hierarchical topic models. text corpus USED-FOR semantically meaningful topics. prior belief USED-FOR learning of the topic hierarchy. knowledge graph HYPONYM-OF prior belief. TopicNet HYPONYM-OF deep hierarchical topic model. symmetric and asymmetric similarities FEATURE-OF Gaussian embedding vectors. TopicNet USED-FOR symmetric and asymmetric similarities. Gaussian - distributed embedding vector USED-FOR TopicNet. evidence lower bound CONJUNCTION regularization term. regularization term CONJUNCTION evidence lower bound. auto - encoding variational inference network USED-FOR model parameters. stochastic gradient descent USED-FOR regularization term. regularization term USED-FOR model parameters. evidence lower bound USED-FOR model parameters. stochastic gradient descent USED-FOR model parameters. deep topic models USED-FOR discovering deeper interpretable topics. TopicNet COMPARE deep topic models. deep topic models COMPARE TopicNet. TopicNet USED-FOR discovering deeper interpretable topics. TopicNet USED-FOR document representations. deep topic models USED-FOR document representations. OtherScientificTerm are prior structural knowledge, inductive bias, shared embedding space, and prior semantic hierarchies. Task is learning. ","This paper proposes TopicNet, a deep hierarchical topic model that learns the topic hierarchy from a large text corpus. TopicNet is based on a knowledge graph, which is used to learn a prior belief for each topic in a topic hierarchy. The prior belief is learned by a variational inference network. The authors show that TopicNet achieves better performance than other deep topic models on a number of benchmark datasets. ","This paper proposes TopicNet, a deep hierarchical topic model that learns the topic hierarchy from a large text corpus. TopicNet is based on a knowledge graph, which is used to learn a prior belief for each topic in a topic hierarchy. The prior belief is learned by a variational inference network. The authors show that TopicNet achieves better performance than other deep topic models on a number of benchmark datasets. "
8936,SP:b7ad495901eb2f73a8a26aa5c9325908451cfe09,Image - level contrastive representation learning USED-FOR transfer learning. generality USED-FOR transfer learning. specificity FEATURE-OF downstream task. design principle USED-FOR alignment. self - supervised pretext task CONJUNCTION downstream task. downstream task CONJUNCTION self - supervised pretext task. alignment USED-FOR self - supervised pretext task. alignment USED-FOR downstream task. pretraining method USED-FOR object detection. object - level translation invariance CONJUNCTION scale invariance. scale invariance CONJUNCTION object - level translation invariance. dedicated modules USED-FOR detection pipeline. selective search bounding boxes USED-FOR object proposals. object detection properties FEATURE-OF pretraining. FPN HYPONYM-OF detection pipeline. dedicated modules PART-OF pretraining network architecture. FPN HYPONYM-OF dedicated modules. scale invariance HYPONYM-OF object detection properties. object - level translation invariance HYPONYM-OF object detection properties. selective search bounding boxes USED-FOR object - level representations. COCO detection EVALUATE-FOR transfer. Selective Object COntrastive learning ( SoCo ) HYPONYM-OF method. transfer EVALUATE-FOR method. Mask R - CNN framework USED-FOR COCO detection. ,"This paper proposes a novel method for object detection based on image-level contrastive representation learning. The proposed method, called Selective Object COntrastive Learning (SoCo), is based on the design principle of self-supervised alignment, which is used to train a pretraining network that is able to learn object-level representations that are invariant to changes in the target domain. The method is evaluated on the COCO detection task and is shown to achieve state-of-the-art performance.","This paper proposes a novel method for object detection based on image-level contrastive representation learning. The proposed method, called Selective Object COntrastive Learning (SoCo), is based on the design principle of self-supervised alignment, which is used to train a pretraining network that is able to learn object-level representations that are invariant to changes in the target domain. The method is evaluated on the COCO detection task and is shown to achieve state-of-the-art performance."
8952,SP:b4dcb19fd97a906ed37e6af407260f0dedbbd402,"Vehicle routing problems ( VRPs ) HYPONYM-OF combinatorial problems. heuristic or learning - based works USED-FOR decent solutions. small problem instances EVALUATE-FOR decent solutions. learning - augmented local search framework USED-FOR large - scale VRP. linear number of subproblems COMPARE exponential. exponential COMPARE linear number of subproblems. spatial locality USED-FOR linear number of subproblems. regression USED-FOR subproblem selection. method USED-FOR VRPs. solution qualities EVALUATE-FOR VRPs. method USED-FOR VRP solvers. solution qualities EVALUATE-FOR method. subproblem selection COMPARE heuristic or random selection. heuristic or random selection COMPARE subproblem selection. variants CONJUNCTION solvers. solvers CONJUNCTION variants. VRP distributions CONJUNCTION variants. variants CONJUNCTION VRP distributions. OtherScientificTerm are subproblems, and black box subsolver. Method is Transformer. ",This paper proposes a learning-augmented local search framework for large-scale vehicle routing problems (VRPs). The key idea is to use regression to select the subproblems that are most likely to be found in the solution space. The proposed method is evaluated on a number of VRPs and shows that the proposed method outperforms the baselines.,This paper proposes a learning-augmented local search framework for large-scale vehicle routing problems (VRPs). The key idea is to use regression to select the subproblems that are most likely to be found in the solution space. The proposed method is evaluated on a number of VRPs and shows that the proposed method outperforms the baselines.
8968,SP:f5c80f76cb1e651fd808e7da4bfe6fdfd75b7155,"dynamic data distributions USED-FOR Continual learning. learning - triggered synaptic expansion CONJUNCTION synaptic convergence. synaptic convergence CONJUNCTION learning - triggered synaptic expansion. learning - triggered synaptic expansion USED-FOR biological neural networks. synaptic Expansion - Convergence ( AFEC ) FEATURE-OF Active Forgetting. visual classification tasks CONJUNCTION Atari reinforcement tasks. Atari reinforcement tasks CONJUNCTION visual classification tasks. CIFAR-10 regression tasks CONJUNCTION visual classification tasks. visual classification tasks CONJUNCTION CIFAR-10 regression tasks. AFEC USED-FOR learning of new tasks. continual learning benchmarks EVALUATE-FOR AFEC. continual learning benchmarks EVALUATE-FOR AFEC. Atari reinforcement tasks HYPONYM-OF continual learning benchmarks. visual classification tasks HYPONYM-OF continual learning benchmarks. CIFAR-10 regression tasks HYPONYM-OF continual learning benchmarks. OtherScientificTerm are knowledge transfer, and forward knowledge transfer. Task is continual learning. Method are biological active forgetting, and Bayesian continual learning. Generic are approach, method, and them. ","This paper proposes a method for continual learning based on Bayesian continual learning. The method is based on the idea of active forgetting, which is a generalization of the active forgetting phenomenon in biological neural networks. The authors show that active forgetting can be used to improve the performance of continual learning on a variety of tasks, including reinforcement learning, CIFAR-10 regression, and Atari reinforcement learning. They also show that AFEC can be applied to continual learning in the context of biological active forgetting.","This paper proposes a method for continual learning based on Bayesian continual learning. The method is based on the idea of active forgetting, which is a generalization of the active forgetting phenomenon in biological neural networks. The authors show that active forgetting can be used to improve the performance of continual learning on a variety of tasks, including reinforcement learning, CIFAR-10 regression, and Atari reinforcement learning. They also show that AFEC can be applied to continual learning in the context of biological active forgetting."
8984,SP:a5945ec13e2f362df03b42511d44827ef081f4c3,Zeroth - order ( ZO ) optimization USED-FOR tasks. query - based black - box adversarial attacks CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION query - based black - box adversarial attacks. query - based black - box adversarial attacks HYPONYM-OF tasks. reinforcement learning HYPONYM-OF tasks. prior information PART-OF gradient estimation procedure. finite differences USED-FOR gradient estimation procedure. finite differences USED-FOR prior information. greedy descent framework CONJUNCTION gradient estimators. gradient estimators CONJUNCTION greedy descent framework. convergence FEATURE-OF prior - guided ZO algorithms. greedy descent framework USED-FOR prior - guided ZO algorithms. prior information USED-FOR accelerated random search ( ARS ) algorithm. convergence analysis USED-FOR accelerated random search ( ARS ) algorithm. numerical benchmarks CONJUNCTION adversarial attacks. adversarial attacks CONJUNCTION numerical benchmarks. OtherScientificTerm is convergence guarantee. Method is greedy descent methods. ,This paper studies the convergence of prior-guided ZO algorithms in the context of greedy descent. The authors propose a greedy descent framework for greedy descent based on finite differences between the gradient estimator and the prior information. The convergence analysis is based on the convergence analysis of accelerated random search (ARS) algorithm. The paper also provides a theoretical analysis of the convergence guarantee of the greedy descent algorithm. ,This paper studies the convergence of prior-guided ZO algorithms in the context of greedy descent. The authors propose a greedy descent framework for greedy descent based on finite differences between the gradient estimator and the prior information. The convergence analysis is based on the convergence analysis of accelerated random search (ARS) algorithm. The paper also provides a theoretical analysis of the convergence guarantee of the greedy descent algorithm. 
9000,SP:ef18f4188426bc01be309633b486884b0e7a81a4,"test accuracy EVALUATE-FOR unpruned network. pruned network COMPARE unpruned network. unpruned network COMPARE pruned network. computer vision CONJUNCTION natural language processing. natural language processing CONJUNCTION computer vision. deep neural network ( DNN ) USED-FOR applications. LTH PART-OF deep neural network ( DNN ). natural language processing HYPONYM-OF applications. computer vision HYPONYM-OF applications. objective function CONJUNCTION sample complexity. sample complexity CONJUNCTION objective function. guaranteed generalization EVALUATE-FOR model. algorithm USED-FOR pruned neural network. non - pruned weights PART-OF hidden layer. accelerated ) stochastic gradient descent algorithm USED-FOR algorithm. pruned neural network USED-FOR model. pruned neural network COMPARE unpruned one. unpruned one COMPARE pruned neural network. Task are lottery ticket hypothesis ( LTH ), and pruning multi - layer neural networks. Metric are generalization, and generalization of the winning ticket. OtherScientificTerm are zero generalization error, and convex region. Method is neural network model. ","This paper studies the lottery ticket hypothesis (LTH) and pruning multi-layer neural networks. The authors show that pruned neural networks are guaranteed to have zero generalization error, and the generalization of the winning ticket is guaranteed to be convex in the convex region. They also show that the pruned network can achieve better test accuracy than the unpruned network. They show that this is due to the fact that the non-pruned weights of the hidden layer of the network are not pruned. They then propose a new algorithm to speed up the pruning process. ","This paper studies the lottery ticket hypothesis (LTH) and pruning multi-layer neural networks. The authors show that pruned neural networks are guaranteed to have zero generalization error, and the generalization of the winning ticket is guaranteed to be convex in the convex region. They also show that the pruned network can achieve better test accuracy than the unpruned network. They show that this is due to the fact that the non-pruned weights of the hidden layer of the network are not pruned. They then propose a new algorithm to speed up the pruning process. "
9016,SP:3ef8660de61a1a73858934fcf8edfec104133ae7,"private synthetic data generation USED-FOR query release. differential privacy FEATURE-OF sensitive dataset. algorithmic framework USED-FOR iterative algorithms. computational bottlenecks PART-OF algorithms. generative networks CONJUNCTION exponential mechanism ( GEM ). exponential mechanism ( GEM ) CONJUNCTION generative networks. MWEM HYPONYM-OF algorithms. neural networks USED-FOR generative models. MWEM USED-FOR private entropy projection ( PEP ). PEP COMPARE algorithms. algorithms COMPARE PEP. GEM COMPARE algorithms. algorithms COMPARE GEM. GEM CONJUNCTION PEP. PEP CONJUNCTION GEM. prior information USED-FOR GEM. public data USED-FOR prior information. public data USED-FOR state - of - the - art method. OtherScientificTerm is statistical queries. Generic are framework, methods, and method. Method are gradient - based optimization, and PMWPub. Metric is accuracy. ","This paper proposes a new algorithm for private synthetic data generation. The proposed algorithm, PMWPub, is based on the exponential mechanism (GEM) and private entropy projection (PEP) algorithms. The authors show that the proposed algorithm is able to achieve state-of-the-art performance in terms of query release and accuracy. They also show that their algorithm is computationally efficient.","This paper proposes a new algorithm for private synthetic data generation. The proposed algorithm, PMWPub, is based on the exponential mechanism (GEM) and private entropy projection (PEP) algorithms. The authors show that the proposed algorithm is able to achieve state-of-the-art performance in terms of query release and accuracy. They also show that their algorithm is computationally efficient."
9032,SP:d789e92c1e4f6a44de373210cd732198a6f809be,per - pixel classification task USED-FOR semantic segmentation. mask classification USED-FOR instance - level segmentation. mask classification USED-FOR semanticand instance - level segmentation tasks. model USED-FOR mask classification. training procedure USED-FOR mask classification. MaskFormer HYPONYM-OF mask classification model. mask classification model USED-FOR binary masks. single global class label prediction USED-FOR binary masks. approaches USED-FOR semantic and panoptic segmentation tasks. mask classification - based method USED-FOR approaches. mask classification - based method USED-FOR semantic and panoptic segmentation tasks. MaskFormer COMPARE per - pixel classification baselines. per - pixel classification baselines COMPARE MaskFormer. ,This paper proposes a new per-pixel mask classification method for semantic and instance-level segmentation tasks. The proposed method is based on a single global class label prediction model for binary masks. The method is evaluated on a number of semantic and panoptic segmentation benchmarks. The results show that the proposed method outperforms the state-of-the-art baselines.,This paper proposes a new per-pixel mask classification method for semantic and instance-level segmentation tasks. The proposed method is based on a single global class label prediction model for binary masks. The method is evaluated on a number of semantic and panoptic segmentation benchmarks. The results show that the proposed method outperforms the state-of-the-art baselines.
9048,SP:3aee15083ee1c0a75fedd67a50f9d729bf5ee411,gradient descent USED-FOR random undercomplete two - layers ReLU neural networks. smooth activation function FEATURE-OF subexponential width random neural network. Material is overcomplete case. OtherScientificTerm is dimension. ,This paper studies the problem of random undercomplete two-layer ReLU neural networks with smooth activation functions. The authors show that the smooth activation function of a subexponential width random neural network can be viewed as a function of the dimension of the input space. They show that this is the case for any ReLU network with a smooth function. They also show that undercomplete ReLU networks can be seen as functions of the subexponentially width random network. ,This paper studies the problem of random undercomplete two-layer ReLU neural networks with smooth activation functions. The authors show that the smooth activation function of a subexponential width random neural network can be viewed as a function of the dimension of the input space. They show that this is the case for any ReLU network with a smooth function. They also show that undercomplete ReLU networks can be seen as functions of the subexponentially width random network. 
9064,SP:220db9ed147bbe67de5d82778720a1549656e48d,"sample quality CONJUNCTION distribution coverage. distribution coverage CONJUNCTION sample quality. distribution coverage EVALUATE-FOR Score - based generative models ( SGMs ). sample quality EVALUATE-FOR Score - based generative models ( SGMs ). network evaluations USED-FOR sampling. data space FEATURE-OF they. approach USED-FOR SGMs. latent space FEATURE-OF SGMs. variational autoencoder framework USED-FOR approach. non - continuous data USED-FOR SGMs. score - matching objective USED-FOR LSGM setting. SGM USED-FOR mismatch of the target distribution. Normal one USED-FOR mismatch of the target distribution. LSGM COMPARE generative results. generative results COMPARE LSGM. dataset EVALUATE-FOR LSGM. dataset EVALUATE-FOR generative results. FID score EVALUATE-FOR LSGM. CIFAR-10 EVALUATE-FOR LSGM. LSGM COMPARE SGMs. SGMs COMPARE LSGM. CelebA - HQ-256 EVALUATE-FOR LSGM. LSGM COMPARE them. them COMPARE LSGM. CelebA - HQ-256 EVALUATE-FOR SGMs. sampling time EVALUATE-FOR them. sample quality EVALUATE-FOR SGMs. sample quality EVALUATE-FOR LSGM. LSGM USED-FOR binary images. binarized OMNIGLOT dataset EVALUATE-FOR LSGM. Method are generative models, and LSGMs. OtherScientificTerm is score function. Task is variance reduction of the training objective. Generic is implementation. ","This paper proposes a score-based generative model (SGM) approach for non-continuous data. The authors propose a variational autoencoder framework to learn score-matching objective to improve the sample quality of the target distribution. The proposed method is evaluated on CIFAR-10, CelebA-HQ-256, and Omniglot datasets. ","This paper proposes a score-based generative model (SGM) approach for non-continuous data. The authors propose a variational autoencoder framework to learn score-matching objective to improve the sample quality of the target distribution. The proposed method is evaluated on CIFAR-10, CelebA-HQ-256, and Omniglot datasets. "
9080,SP:d681e4e28c03f610acf6817a9e57db0c41c196b4,"Neural networks COMPARE kernel methods. kernel methods COMPARE Neural networks. neural tangent kernels HYPONYM-OF kernel methods. hypothesis class USED-FOR realistic data. noise FEATURE-OF sparse signal. convolutional neural network USED-FOR noise. high - variance noise FEATURE-OF sparse signal. convolutional neural network USED-FOR data distribution. sparse signal FEATURE-OF data distribution. stochastic gradient descent USED-FOR convolutional neural network. predetermined features USED-FOR neural tangent kernel. CNN COMPARE neural tangent kernel. neural tangent kernel COMPARE CNN. CIFAR-10 and MNIST images EVALUATE-FOR CNN. neural networks COMPARE kernel methods. kernel methods COMPARE neural networks. OtherScientificTerm are complex hypothesis class, background noise, and local signal adaptivity ( LSA ) phenomenon. Task is image classification setting. ",This paper proposes a new neural tangent kernel based on stochastic gradient descent (SGD) for the problem of image classification. The proposed method is based on the idea of local signal adaptivity (LSA) phenomenon. The authors show that the proposed method outperforms existing methods on CIFAR-10 and MNIST datasets. ,This paper proposes a new neural tangent kernel based on stochastic gradient descent (SGD) for the problem of image classification. The proposed method is based on the idea of local signal adaptivity (LSA) phenomenon. The authors show that the proposed method outperforms existing methods on CIFAR-10 and MNIST datasets. 
9096,SP:c347796244fcf9b5de19c68bcc5c811b7448217d,"network USED-FOR decentralized machine learning. model USED-FOR local loss functions. known convergence rates EVALUATE-FOR GT algorithms. negative eigenvalues FEATURE-OF connectivity matrix. Method are stochastic model updates, gradient tracking ( GT ) algorithms, and GT method. OtherScientificTerm are workers ’ local data distributions, mixing parameter p, and O(p−3/2 ). Material are noiseless case, and stochastic case. ","This paper studies the convergence of gradient tracking (GT) algorithms for stochastic model updates. The authors show that the convergence rate is O(p−3/2) in the noiseless case, and O(\sqrt{n}) in the stochastically noisy case. They also show that under certain assumptions on the mixing parameter p and the negative eigenvalues of the connectivity matrix, gradient tracking algorithms converge to O(n+1/2).","This paper studies the convergence of gradient tracking (GT) algorithms for stochastic model updates. The authors show that the convergence rate is O(p−3/2) in the noiseless case, and O(\sqrt{n}) in the stochastically noisy case. They also show that under certain assumptions on the mixing parameter p and the negative eigenvalues of the connectivity matrix, gradient tracking algorithms converge to O(n+1/2)."
9112,SP:24d637e8c3489bfe50b17bf684097776ad6ee485,"Upper Confidence Bound ( UCB ) policy HYPONYM-OF optimism - based MAB algorithms. O ( log n ) regret EVALUATE-FOR it. arm - sampling behavior FEATURE-OF UCB. UCB USED-FOR arm - sampling rates. O p n log n minimax regret EVALUATE-FOR UCB. process - level characterization FEATURE-OF MAB problem. diffusion scaling FEATURE-OF UCB. diffusion scaling FEATURE-OF MAB problem. UCB USED-FOR MAB problem. UCB CONJUNCTION Thompson Sampling. Thompson Sampling CONJUNCTION UCB. incomplete learning phenomenon FEATURE-OF latter. Metric are complexity, and problem complexity. OtherScientificTerm are mean rewards, instance gap, and small ” gap worst - case lens. ","This paper studies the upper confidence bound (UCB) policy for optimism-based MAB algorithms. UCB is a variant of the Thompson Sampling algorithm, which is an optimistic MAB algorithm. The main contribution of this paper is to show that UCB can be viewed as a diffusion-scaling algorithm for MAB. The authors also provide a theoretical analysis of the regret of UCB. ","This paper studies the upper confidence bound (UCB) policy for optimism-based MAB algorithms. UCB is a variant of the Thompson Sampling algorithm, which is an optimistic MAB algorithm. The main contribution of this paper is to show that UCB can be viewed as a diffusion-scaling algorithm for MAB. The authors also provide a theoretical analysis of the regret of UCB. "
9128,SP:43c8bcc93aa034965fa6d959d44b529ffc110cc7,"Cross - Domain Recommendation ( CDR ) USED-FOR cold - start problem. domain knowledge USED-FOR cold - start problem. cold - start problem PART-OF recommender systems. domain knowledge USED-FOR Cross - Domain Recommendation ( CDR ). cold - start CONJUNCTION CDR. CDR CONJUNCTION cold - start. approaches USED-FOR CDR. approaches USED-FOR cold - start. cross - domain recommendation framework USED-FOR CDCSR problem. DisAlign HYPONYM-OF cross - domain recommendation framework. rating and auxiliary representations USED-FOR recommendation. rating and auxiliary representations USED-FOR DisAlign. Stein path alignment USED-FOR latent embedding distributions. proxy Stein path HYPONYM-OF version. DisAlign COMPARE models. models COMPARE DisAlign. Douban and Amazon datasets EVALUATE-FOR DisAlign. CDCSR setting EVALUATE-FOR models. CDCSR setting EVALUATE-FOR DisAlign. OtherScientificTerm are latent embedding discrepancy, and model degradation. Metric is efficiency. ","This paper proposes a new method for cross-domain recommendation (CDR) based on Stein path alignment. The authors propose a new approach to address the problem of cold-start in CDR, which is an important problem in recommender systems. The proposed method DisAlign is based on the idea of Stein path alignments, which can be seen as an extension of the Stein path method. The method is evaluated on the Douban and Amazon datasets, and is shown to outperform the baselines.","This paper proposes a new method for cross-domain recommendation (CDR) based on Stein path alignment. The authors propose a new approach to address the problem of cold-start in CDR, which is an important problem in recommender systems. The proposed method DisAlign is based on the idea of Stein path alignments, which can be seen as an extension of the Stein path method. The method is evaluated on the Douban and Amazon datasets, and is shown to outperform the baselines."
9144,SP:729c3e22a6f0170bdc8e1f511812dc9e9a4fd4a8,self - attention CONJUNCTION multi - layer perceptrons ( MLP ) models. multi - layer perceptrons ( MLP ) models CONJUNCTION self - attention. multi - layer perceptrons ( MLP ) models USED-FOR vision. self - attention USED-FOR vision. self - attention CONJUNCTION MLP. MLP CONJUNCTION self - attention. complexity EVALUATE-FOR MLP. complexity EVALUATE-FOR self - attention. Global Filter Network ( GFNet ) HYPONYM-OF architecture. Global Filter Network ( GFNet ) USED-FOR long - term spatial dependencies. architecture USED-FOR long - term spatial dependencies. frequency domain FEATURE-OF long - term spatial dependencies. 2D discrete Fourier transform CONJUNCTION element - wise multiplication. element - wise multiplication CONJUNCTION 2D discrete Fourier transform. global filters CONJUNCTION 2D inverse Fourier transform. 2D inverse Fourier transform CONJUNCTION global filters. self - attention layer PART-OF vision transformers. element - wise multiplication USED-FOR frequency - domain features. element - wise multiplication CONJUNCTION 2D inverse Fourier transform. 2D inverse Fourier transform CONJUNCTION element - wise multiplication. element - wise multiplication USED-FOR global filters. operations PART-OF architecture. self - attention layer PART-OF architecture. 2D discrete Fourier transform HYPONYM-OF operations. global filters HYPONYM-OF operations. element - wise multiplication HYPONYM-OF operations. 2D inverse Fourier transform HYPONYM-OF operations. accuracy / complexity trade - offs EVALUATE-FOR models. ImageNet and downstream tasks EVALUATE-FOR models. efficiency CONJUNCTION generalization ability. generalization ability CONJUNCTION efficiency. generalization ability CONJUNCTION robustness. robustness CONJUNCTION generalization ability. GFNet COMPARE transformer - style models. transformer - style models COMPARE GFNet. GFNet COMPARE CNNs. CNNs COMPARE GFNet. transformer - style models CONJUNCTION CNNs. CNNs CONJUNCTION transformer - style models. efficiency EVALUATE-FOR CNNs. generalization ability EVALUATE-FOR CNNs. robustness EVALUATE-FOR CNN,"This paper proposes a new architecture for vision transformers, called Global Filter Network (GFNet). The proposed architecture is based on the idea of global filters, which is a combination of 2D discrete Fourier transform and 2D inverse Fourier Transformer. The proposed method is evaluated on ImageNet and downstream tasks. The authors show that the proposed method outperforms the state-of-the-art models in terms of accuracy, generalization ability, and robustness.","This paper proposes a new architecture for vision transformers, called Global Filter Network (GFNet). The proposed architecture is based on the idea of global filters, which is a combination of 2D discrete Fourier transform and 2D inverse Fourier Transformer. The proposed method is evaluated on ImageNet and downstream tasks. The authors show that the proposed method outperforms the state-of-the-art models in terms of accuracy, generalization ability, and robustness."
9160,SP:e278079529d6da9e2ea26b47730dbc1256ffe2db,"high - dimensional features CONJUNCTION visual concepts. visual concepts CONJUNCTION high - dimensional features. real - world large - scale datasets USED-FOR predicting trustworthiness. focal loss CONJUNCTION true class probability confidence loss. true class probability confidence loss CONJUNCTION focal loss. cross entropy loss CONJUNCTION focal loss. focal loss CONJUNCTION cross entropy loss. cross entropy loss HYPONYM-OF trustworthiness predictors. focal loss HYPONYM-OF trustworthiness predictors. cross entropy loss HYPONYM-OF prior - art loss functions. true class probability confidence loss HYPONYM-OF prior - art loss functions. focal loss HYPONYM-OF prior - art loss functions. prior - art loss functions USED-FOR trustworthiness predictors. steep slope loss USED-FOR features. steep slope loss USED-FOR trustworthiness predictors. Vision Transformer CONJUNCTION ResNet. ResNet CONJUNCTION Vision Transformer. deep learning models USED-FOR trustworthiness predictors. trustworthiness predictors EVALUATE-FOR loss. deep learning models EVALUATE-FOR loss. Vision Transformer HYPONYM-OF deep learning models. ResNet HYPONYM-OF deep learning models. loss USED-FOR trustworthiness predictors. Method are classifier, and AI models. Material are small - scale datasets, and ImageNet. Generic is task. Metric are data complexity, and generalizability of trustworthiness predictors. OtherScientificTerm is slide - like curves. ",This paper proposes a new loss function for trustworthiness prediction. The proposed loss is based on the notion of slide-like curves. The authors show that the proposed loss can be used to improve the generalizability of trustworthiness predictors. ,This paper proposes a new loss function for trustworthiness prediction. The proposed loss is based on the notion of slide-like curves. The authors show that the proposed loss can be used to improve the generalizability of trustworthiness predictors. 
9176,SP:c5704a709f318c6e9a5c716e5e7f250acccf46a8,"adversarial examples USED-FOR Adversarial robustness. robustness FEATURE-OF adversarial attacks. robust models USED-FOR adversarial attacks. robustness EVALUATE-FOR robust models. linear components USED-FOR adversarial robustness. batch normalization CONJUNCTION maximum pooling. maximum pooling CONJUNCTION batch normalization. maximum pooling CONJUNCTION activation layers. activation layers CONJUNCTION maximum pooling. activation layers HYPONYM-OF non - linear components. maximum pooling HYPONYM-OF non - linear components. batch normalization HYPONYM-OF non - linear components. domain adaption CONJUNCTION robustness boosting. robustness boosting CONJUNCTION domain adaption. it USED-FOR tasks. it USED-FOR domain adaption. it USED-FOR robustness boosting. robustness boosting HYPONYM-OF tasks. domain adaption HYPONYM-OF tasks. OtherScientificTerm are statistical properties, and linearized sub - networks. Method is clustering strategy. ",This paper proposes a clustering strategy to improve the robustness of adversarial examples against adversarial attacks. The proposed method is based on the observation that linearized sub-networks have non-linear components that can be used to improve robustness. The authors show that this clustering technique can be applied to both batch normalization and maximum pooling. They also show that the proposed clustering method is effective for robustness boosting and domain adaption tasks.,This paper proposes a clustering strategy to improve the robustness of adversarial examples against adversarial attacks. The proposed method is based on the observation that linearized sub-networks have non-linear components that can be used to improve robustness. The authors show that this clustering technique can be applied to both batch normalization and maximum pooling. They also show that the proposed clustering method is effective for robustness boosting and domain adaption tasks.
9201,SP:590b67b1278267e966cf0b31456d981441e61bb1,approach USED-FOR end - to - end reconstruction operators. unpaired training data USED-FOR ill - posed inverse problems. unpaired training data USED-FOR approach. expected distortion CONJUNCTION Wasserstein-1 distance. Wasserstein-1 distance CONJUNCTION expected distortion. variational framework CONJUNCTION iterative unrolling. iterative unrolling CONJUNCTION variational framework. measurement space FEATURE-OF expected distortion. variational framework USED-FOR method. regularizer USED-FOR variational setting. deep neural network USED-FOR regularizer. unrolled reconstruction operator USED-FOR regularizer. reconstruction network USED-FOR variational problem. it COMPARE variational methods. variational methods COMPARE it. unrolled operator USED-FOR initialization. initialization USED-FOR it. well - posedness CONJUNCTION noise - stability guarantees. noise - stability guarantees CONJUNCTION well - posedness. noise - stability guarantees FEATURE-OF variational setting. well - posedness FEATURE-OF variational setting. end - to - end unrolled reconstruction USED-FOR approach. well - posedness FEATURE-OF approach. well - posedness FEATURE-OF end - to - end unrolled reconstruction. it COMPARE supervised data - driven reconstruction approaches. supervised data - driven reconstruction approaches COMPARE it. approach COMPARE unsupervised methods. unsupervised methods COMPARE approach. approach COMPARE it. it COMPARE approach. approach COMPARE supervised data - driven reconstruction approaches. supervised data - driven reconstruction approaches COMPARE approach. OtherScientificTerm is reconstruction. Material is X - ray computed tomography ( CT ). ,This paper proposes an end-to-end unrolled reconstruction method for ill-posed inverse problems. The proposed method is based on the variational framework. The authors show that the proposed method can achieve better well-posedness and noise-stability guarantees compared to the state-of-the-art unrolled methods.,This paper proposes an end-to-end unrolled reconstruction method for ill-posed inverse problems. The proposed method is based on the variational framework. The authors show that the proposed method can achieve better well-posedness and noise-stability guarantees compared to the state-of-the-art unrolled methods.
9226,SP:115d679338ab35829dbc594472d13cc02be5ed4c,Large - scale vision and language representation learning USED-FOR vision - language tasks. transformer - based multimodal encoder USED-FOR word tokens. transformer - based multimodal encoder USED-FOR methods. multimodal encoder USED-FOR image - text interactions. visual tokens CONJUNCTION word tokens. word tokens CONJUNCTION visual tokens. contrastive loss USED-FOR image and text representations. cross - modal attention USED-FOR vision and language representation learning. methods COMPARE method. method COMPARE methods. bounding box annotations CONJUNCTION high - resolution images. high - resolution images CONJUNCTION bounding box annotations. high - resolution images USED-FOR method. momentum distillation HYPONYM-OF self - training method. pseudo - targets USED-FOR self - training method. momentum model USED-FOR pseudo - targets. momentum model USED-FOR self - training method. downstream visionlanguage tasks EVALUATE-FOR ALBEF. ALBEF COMPARE methods. methods COMPARE ALBEF. image - text retrieval EVALUATE-FOR ALBEF. image - text retrieval EVALUATE-FOR methods. NLVR2 EVALUATE-FOR ALBEF. VQA EVALUATE-FOR ALBEF. ALBEF COMPARE state - ofthe - art. state - ofthe - art COMPARE ALBEF. VQA CONJUNCTION NLVR2. NLVR2 CONJUNCTION VQA. Material is noisy web data. Task is training tasks. OtherScientificTerm is image - text pair. ,This paper proposes a self-training method for vision and language representation learning. The proposed method is based on a transformer-based multimodal encoder and word tokens. The authors show that the proposed method outperforms the state-of-the-art methods on several downstream vision-language tasks.,This paper proposes a self-training method for vision and language representation learning. The proposed method is based on a transformer-based multimodal encoder and word tokens. The authors show that the proposed method outperforms the state-of-the-art methods on several downstream vision-language tasks.
9251,SP:e5323a171f40c109722a7ea0aebdcd53c151b72d,Markov decision processes ( MDPs ) USED-FOR offline policy evaluation ( OPE ). static datasets USED-FOR decisionmaking policies. OPE COMPARE realizable setting. realizable setting COMPARE OPE. unrealizability USED-FOR OPE. unrealizability USED-FOR OPE method. linear direct method ( DM ) HYPONYM-OF OPE method. doubly robust form FEATURE-OF OPE error. nonparametric consistency FEATURE-OF tile - coding estimators. OtherScientificTerm is approximate ) realizability assumptions. Method is hypothetical models. Task is real - world applications. ,"This paper studies offline policy evaluation (OPE) in MDPs. The authors propose a linear direct method (DM) method for OPE. The proposed method is based on the notion of unrealizability (i.e., the assumption that a given policy is non-parametric) and is able to be evaluated in a realizable setting. In particular, the authors show that the OPE error is doubly robust in the case of tile-coding estimators. The paper also shows that the proposed method can be used in the real-world setting.","This paper studies offline policy evaluation (OPE) in MDPs. The authors propose a linear direct method (DM) method for OPE. The proposed method is based on the notion of unrealizability (i.e., the assumption that a given policy is non-parametric) and is able to be evaluated in a realizable setting. In particular, the authors show that the OPE error is doubly robust in the case of tile-coding estimators. The paper also shows that the proposed method can be used in the real-world setting."
9276,SP:b45f6966fcc07f3a33f70a57e72507b16fc7bb24,"stochastic 1 first - order methods USED-FOR large - scale machine learning models. theoretical guarantees FEATURE-OF expectation of the objective value. algorithms USED-FOR small objective residual. Existing methods USED-FOR non - smooth stochastic convex optimization. complexity 7 bounds FEATURE-OF Existing methods. logarithmic dependence FEATURE-OF high - probability convergence. stepsize rules USED-FOR stochastic methods. gradient 14 clipping USED-FOR stochastic methods. extension USED-FOR strongly convex problems. Hölder - continuous gradients FEATURE-OF generalized smooth objectives. extension USED-FOR methods. non - smooth setting FEATURE-OF one. iteration and oracle complexity EVALUATE-FOR accelerated ) 17 method. OtherScientificTerm are Random behavior, suboptimal objective value, confidence level, and negative - power. Generic is algorithm. Task are NLP tasks, and non - smooth convex stochastic 12 optimization problems. ","This paper studies the problem of non-smooth convex stochastic convex optimization. The main contribution of this paper is to extend the Hölder-continuous gradient clipping (HCC) algorithm to non-convex problems. The authors show that the HCC algorithm converges to the optimal objective in a logarithmic dependence, which is an extension of the existing HCC algorithms. They also show that HCC can be extended to strongly convex problems, and show that it can be used to speed up the optimization process. ","This paper studies the problem of non-smooth convex stochastic convex optimization. The main contribution of this paper is to extend the Hölder-continuous gradient clipping (HCC) algorithm to non-convex problems. The authors show that the HCC algorithm converges to the optimal objective in a logarithmic dependence, which is an extension of the existing HCC algorithms. They also show that HCC can be extended to strongly convex problems, and show that it can be used to speed up the optimization process. "
9301,SP:a22a893e25ce739dc757861741014764e78aa820,"extreme weather early warning CONJUNCTION long - term energy consumption planning. long - term energy consumption planning CONJUNCTION extreme weather early warning. self - attention mechanisms USED-FOR long - range dependencies. Transformerbased models USED-FOR long - range dependencies. self - attention mechanisms USED-FOR Transformerbased models. point - wise self - attentions USED-FOR long series efficiency. point - wise self - attentions USED-FOR Transformers. decomposition architecture USED-FOR Autoformer. Auto - Correlation mechanism USED-FOR decomposition architecture. Auto - Correlation mechanism USED-FOR Autoformer. pre - processing convention PART-OF series decomposition. design USED-FOR Autoformer. Autoformer USED-FOR complex time series. progressive decomposition capacities USED-FOR complex time series. progressive decomposition capacities FEATURE-OF Autoformer. dependencies discovery CONJUNCTION representation aggregation. representation aggregation CONJUNCTION dependencies discovery. stochastic process theory USED-FOR Auto - Correlation mechanism. series periodicity USED-FOR dependencies discovery. representation aggregation PART-OF sub - series level. series periodicity USED-FOR Auto - Correlation mechanism. Auto - Correlation COMPARE self - attention. self - attention COMPARE Auto - Correlation. efficiency EVALUATE-FOR Auto - Correlation. accuracy EVALUATE-FOR Auto - Correlation. traffic CONJUNCTION economics. economics CONJUNCTION traffic. energy CONJUNCTION traffic. traffic CONJUNCTION energy. energy CONJUNCTION economics. economics CONJUNCTION energy. long - term forecasting EVALUATE-FOR Autoformer. practical applications FEATURE-OF benchmarks. energy HYPONYM-OF benchmarks. benchmarks EVALUATE-FOR Autoformer. accuracy EVALUATE-FOR Autoformer. traffic HYPONYM-OF benchmarks. economics HYPONYM-OF benchmarks. energy HYPONYM-OF practical applications. traffic HYPONYM-OF practical applications. economics HYPONYM-OF practical applications. OtherScientificTerm are forecasting time, and information utilization bottleneck. Task is long - term forecasting problem of time series. Generic are model, and it. Method is deep models. ",This paper proposes a new self-attention mechanism for long-term forecasting of time series. The proposed Auto-Correlation mechanism is based on stochastic process theory. The authors show that the proposed auto-correlation mechanism can be used to improve the efficiency of Transformer-based models in long-range forecasting. The paper also proposes a pre-processing convention for the series decomposition. Experiments are conducted on several benchmark datasets to demonstrate the effectiveness of the proposed method.,This paper proposes a new self-attention mechanism for long-term forecasting of time series. The proposed Auto-Correlation mechanism is based on stochastic process theory. The authors show that the proposed auto-correlation mechanism can be used to improve the efficiency of Transformer-based models in long-range forecasting. The paper also proposes a pre-processing convention for the series decomposition. Experiments are conducted on several benchmark datasets to demonstrate the effectiveness of the proposed method.
9326,SP:eeb2c3348de291c5eacac5d9de7b6b84ca030ad5,NLP systems USED-FOR compositional language. Cryptic crosswords HYPONYM-OF dominant crossword variety. character - level manipulations USED-FOR wordplay cipher. creative intelligence USED-FOR cryptics. NLP systems USED-FOR compositional language. cryptic clues USED-FOR NLP systems. dataset USED-FOR NLP systems. dataset USED-FOR compositional language. cryptic clues FEATURE-OF dataset. model USED-FOR tasks. T5 HYPONYM-OF neural language model. non - neural approaches CONJUNCTION T5. T5 CONJUNCTION non - neural approaches. unscrambling words HYPONYM-OF tasks. meta - linguistic capabilities FEATURE-OF subword - tokenized models. T5 COMPARE human solving strategies. human solving strategies COMPARE T5. wordplay part of clues USED-FOR model systematicity. curricular approach COMPARE T5 baseline. T5 baseline COMPARE curricular approach. cryptic crosswords PART-OF NLP systems. OtherScientificTerm is Cryptic clues. Method is curriculum approach. ,"This paper proposes a curriculum approach for learning to solve cryptic crossword puzzles. The key idea is to use a dataset of cryptic clues to train a neural language model that can solve the crossword. The model is trained on the dataset and is evaluated on a variety of tasks including unscrambling words, wordplay, and wordplay cipher. The paper shows that the proposed curriculum approach outperforms the state-of-the-art in terms of performance.","This paper proposes a curriculum approach for learning to solve cryptic crossword puzzles. The key idea is to use a dataset of cryptic clues to train a neural language model that can solve the crossword. The model is trained on the dataset and is evaluated on a variety of tasks including unscrambling words, wordplay, and wordplay cipher. The paper shows that the proposed curriculum approach outperforms the state-of-the-art in terms of performance."
9351,SP:7693974b70806d9b67920b8ddd2335afc4883319,"Convolutional neural networks ( CNNs ) USED-FOR visual data. image classification tasks EVALUATE-FOR ( Vision ) Transformer models ( ViT ). Vision Transformers USED-FOR tasks. ViTs CONJUNCTION CNNs. CNNs CONJUNCTION ViTs. internal representation structure USED-FOR ViTs. internal representation structure USED-FOR CNNs. uniform representations USED-FOR ViT. image classification benchmarks EVALUATE-FOR internal representation structure. image classification benchmarks EVALUATE-FOR ViTs. ViT HYPONYM-OF architectures. image classification benchmarks EVALUATE-FOR CNNs. self - attention CONJUNCTION ViT residual connections. ViT residual connections CONJUNCTION self - attention. self - attention USED-FOR aggregation of global information. ViTs USED-FOR input spatial information. intermediate features CONJUNCTION transfer learning. transfer learning CONJUNCTION intermediate features. ( pretraining ) dataset scale USED-FOR transfer learning. ( pretraining ) dataset scale USED-FOR intermediate features. MLP - Mixer HYPONYM-OF architectures. Generic is they. Method are convolutional networks, and classification methods. OtherScientificTerm are visual representations, and features. Task are early aggregation of global information, and spatial localization. ",This paper proposes a method to improve the performance of Vision Transformer models (ViTs) on image classification tasks. The authors propose to use self-attention and self-reconstruction to improve ViT performance. They show that the proposed method is able to achieve better performance than the state-of-the-art ViTs on a variety of image classification benchmarks. They also show that their method can be applied to transfer learning and transfer learning on intermediate features. ,This paper proposes a method to improve the performance of Vision Transformer models (ViTs) on image classification tasks. The authors propose to use self-attention and self-reconstruction to improve ViT performance. They show that the proposed method is able to achieve better performance than the state-of-the-art ViTs on a variety of image classification benchmarks. They also show that their method can be applied to transfer learning and transfer learning on intermediate features. 
9376,SP:dfd740399e48b946f02efdec823b8975a900f6a3,"Thompson sampling ( TS ) USED-FOR bandit area. approximation oracle USED-FOR TS. convergence analysis USED-FOR TS. exact oracle PART-OF CMAB. greedy oracle HYPONYM-OF common ( approximation ) oracle. theoretical guarantees FEATURE-OF common ( approximation ) oracle. TS USED-FOR CMAB problems. problemdependent regret lower bound USED-FOR TS. greedy oracle USED-FOR TS. TS USED-FOR CMAB. approximation oracles USED-FOR TS. Generic are It, and oracle. OtherScientificTerm are optimal solutions, reward gap, and almost matching regret upper bound. Task is combinatorial optimization problems. ",This paper studies Thompson sampling (TS) in combinatorial optimization problems. The authors show that the regret upper bound of TS is almost matching the regret lower bound of the exact oracle in the bandit area. They also provide theoretical guarantees for TS. ,This paper studies Thompson sampling (TS) in combinatorial optimization problems. The authors show that the regret upper bound of TS is almost matching the regret lower bound of the exact oracle in the bandit area. They also provide theoretical guarantees for TS. 
9401,SP:3ca7fdaba9793a61a1f9d264a551fe895e55dd99,"Federated learning HYPONYM-OF distributed learning paradigm. accuracy rates EVALUATE-FOR federated learning. total error HYPONYM-OF social good properties. hedonic game USED-FOR federated learning. average error rates USED-FOR optimality. algorithm USED-FOR optimal ( error minimizing ) arrangement of players. stability CONJUNCTION optimality. optimality CONJUNCTION stability. stability EVALUATE-FOR arrangement. optimality EVALUATE-FOR arrangement. Method are global model, and game - theoretic approach. OtherScientificTerm are error - minimizing players, federating coalitions, stable coalition partitions, stable arrangements, Price of Anarchy, and constant - factor bound. Generic is stable solutions. ","This paper studies the problem of federated learning in a hedonic game, where the goal is to minimize the total error of all players. The authors propose a new algorithm to find the optimal (error minimizing) arrangement of players in a federated game. The algorithm is based on the Price of Anarchy, which is a game theoretic formulation of the game.  The authors show that the optimal solution of the problem can be found with a constant-factor bound. ","This paper studies the problem of federated learning in a hedonic game, where the goal is to minimize the total error of all players. The authors propose a new algorithm to find the optimal (error minimizing) arrangement of players in a federated game. The algorithm is based on the Price of Anarchy, which is a game theoretic formulation of the game.  The authors show that the optimal solution of the problem can be found with a constant-factor bound. "
9426,SP:17088db004fbf4902c5c3d53e387d1b68f4d69a5,"self - supervised capsule architecture USED-FOR 3D point clouds. capsule decompositions of objects USED-FOR capsule decompositions. permutation - equivariant attention USED-FOR capsule decompositions of objects. these USED-FOR decomposition. capsule invariance / equivariance properties FEATURE-OF decomposition. canonicalization operation USED-FOR object - centric reasoning. classification labels CONJUNCTION manually - aligned training datasets. manually - aligned training datasets CONJUNCTION classification labels. classification labels USED-FOR neural network. manually - aligned training datasets USED-FOR neural network. canonicalization CONJUNCTION unsupervised classification. unsupervised classification CONJUNCTION canonicalization. method COMPARE state - of - the - art. state - of - the - art COMPARE method. 3D point cloud reconstruction CONJUNCTION canonicalization. canonicalization CONJUNCTION 3D point cloud reconstruction. object - centric representation USED-FOR method. canonicalization EVALUATE-FOR method. unsupervised classification EVALUATE-FOR method. 3D point cloud reconstruction EVALUATE-FOR state - of - the - art. 3D point cloud reconstruction EVALUATE-FOR method. self - supervised manner USED-FOR object - centric representation. Generic is process. OtherScientificTerm are attention masks, and semantic keypoints. Method is semantically consistent decomposition. ",This paper proposes a self-supervised capsule architecture for 3D point cloud reconstruction. The proposed method is based on the idea of permutation-equivariant attention (Pequivariance) and capsule invariance (Equivariance-invariant attention). The authors show that capsule decompositions of objects are semantically consistent and can be used for object-centric reasoning. The authors also show that the proposed method can be applied to unsupervised classification and canonicalization tasks. ,This paper proposes a self-supervised capsule architecture for 3D point cloud reconstruction. The proposed method is based on the idea of permutation-equivariant attention (Pequivariance) and capsule invariance (Equivariance-invariant attention). The authors show that capsule decompositions of objects are semantically consistent and can be used for object-centric reasoning. The authors also show that the proposed method can be applied to unsupervised classification and canonicalization tasks. 
9451,SP:34cc3466ff7786968f437007b6af7d9ffd4decc7,conformal method USED-FOR prediction intervals. prediction intervals USED-FOR nonparametric regression. conformal method USED-FOR nonparametric regression. black - box machine learning algorithms USED-FOR conditional distribution. approximate conditional coverage FEATURE-OF prediction intervals. histograms USED-FOR black - box machine learning algorithms. conditional coverage CONJUNCTION optimal length. optimal length CONJUNCTION conditional coverage. finite samples FEATURE-OF marginal coverage. marginal coverage FEATURE-OF prediction intervals. conformalized quantile regression CONJUNCTION distributional conformal prediction approaches. distributional conformal prediction approaches CONJUNCTION conformalized quantile regression. simulated and real data EVALUATE-FOR state - of - the - art alternatives. distributional conformal prediction approaches HYPONYM-OF state - of - the - art alternatives. conformalized quantile regression HYPONYM-OF state - of - the - art alternatives. Material is skewed data. Method is black - box model. ,"This paper proposes a conformal method for nonparametric regression. The proposed method is based on the notion of conditional coverage, which is defined as the difference between the conditional coverage and the marginal coverage of the conditional distribution. The authors show that the proposed method can be applied to both black-box and non-black-box models. The paper also provides theoretical analysis of the proposed approach.","This paper proposes a conformal method for nonparametric regression. The proposed method is based on the notion of conditional coverage, which is defined as the difference between the conditional coverage and the marginal coverage of the conditional distribution. The authors show that the proposed method can be applied to both black-box and non-black-box models. The paper also provides theoretical analysis of the proposed approach."
9476,SP:d39075aff611dd54574e7ee1a1aeacce83fdf532,"invariance USED-FOR generalisation. incorporating invariance PART-OF kernel ridge regression. effective dimension USED-FOR generalisation. feature averaging USED-FOR invariance. reproducing kernel Hilbert space CONJUNCTION kernel. kernel CONJUNCTION reproducing kernel Hilbert space. Generic is approach. OtherScientificTerm are function space perspective, and group. ","This paper studies the invariance of kernel ridge regression (KHR) in the context of feature averaging. The authors show that the effective dimension of invariance is a function of the number of samples in the group, and that it can be expressed as the sum of the average of the features of all samples in a group. They also show that invariance can be defined as the difference between the average features of the group and the average feature of a subset of samples from the group.  ","This paper studies the invariance of kernel ridge regression (KHR) in the context of feature averaging. The authors show that the effective dimension of invariance is a function of the number of samples in the group, and that it can be expressed as the sum of the average of the features of all samples in a group. They also show that invariance can be defined as the difference between the average features of the group and the average feature of a subset of samples from the group.  "
9501,SP:97fac361b69ed5871a60dc40e51900747a453df9,"assertion statements USED-FOR erroneous behavior. software programs USED-FOR they. applications EVALUATE-FOR deep learning programs. generative model USED-FOR neural network activations. DecNN HYPONYM-OF Decodable Neural Network. DecNN USED-FOR ensemble - like model. compositionality FEATURE-OF neural networks. uncertainty FEATURE-OF ensemble - like model. out - of - distribution detection CONJUNCTION adversarial example detection. adversarial example detection CONJUNCTION out - of - distribution detection. adversarial example detection CONJUNCTION calibration. calibration CONJUNCTION adversarial example detection. uncertainty USED-FOR out - of - distribution detection. uncertainty USED-FOR adversarial example detection. uncertainty USED-FOR calibration. accuracy EVALUATE-FOR neural networks. DecNN CONJUNCTION pretrained models. pretrained models CONJUNCTION DecNN. protected features USED-FOR neural networks. OtherScientificTerm is program logic. Generic are programs, and design. ","This paper proposes a generative model for deep learning. The model is based on the idea of Decodable Neural Network (DecNN), which is an ensemble-like model that can be used for out-of-distribution (OOD) detection, adversarial example detection, and calibration. The authors show that DecNN is able to achieve better performance than other generative models, and that it is more robust to adversarial examples. The paper also shows that the uncertainty of DecNN can be leveraged to improve the performance of deep learning models.","This paper proposes a generative model for deep learning. The model is based on the idea of Decodable Neural Network (DecNN), which is an ensemble-like model that can be used for out-of-distribution (OOD) detection, adversarial example detection, and calibration. The authors show that DecNN is able to achieve better performance than other generative models, and that it is more robust to adversarial examples. The paper also shows that the uncertainty of DecNN can be leveraged to improve the performance of deep learning models."
9526,SP:3f10ca1e7f8fef6cb0c5957ec2f0689fb9bed753,"Optimal transport maps USED-FOR machine learning and statistics. probability distributions FEATURE-OF Optimal transport maps. Plugin estimators USED-FOR transport maps. Plugin estimators USED-FOR computational optimal transport. rates of convergences EVALUATE-FOR plug - in estimators. barycentric projections USED-FOR plug - in estimators. stability estimate USED-FOR plug - in estimators. stability estimate USED-FOR barycentric projections. rates of convergence EVALUATE-FOR plug - in estimators. rates of convergence FEATURE-OF Wasserstein barycenter. asymptotic detection thresholds USED-FOR optimaltransport based tests of independence. probability distributions FEATURE-OF Wasserstein barycenter. Generic is maps. OtherScientificTerm are minimal smoothness assumptions, smoothness assumptions, curse of dimensionality, and Wasserstein distance. ",This paper considers the problem of computing optimal transport maps. The authors consider the case where the optimal transport map is a barycentric projection of the Wasserstein barycenter of the probability distribution. They show that the barycenters of optimal transport can be approximated by plug-in estimators. They also provide a stability analysis of barycentered estimators and show that they converge to the optimal optimal transport based on asymptotic detection thresholds.,This paper considers the problem of computing optimal transport maps. The authors consider the case where the optimal transport map is a barycentric projection of the Wasserstein barycenter of the probability distribution. They show that the barycenters of optimal transport can be approximated by plug-in estimators. They also provide a stability analysis of barycentered estimators and show that they converge to the optimal optimal transport based on asymptotic detection thresholds.
9551,SP:573fbdbe5857c4aace1dfc27e25b8d65a18c9b96,training efficiency CONJUNCTION useful feature extraction. useful feature extraction CONJUNCTION training efficiency. useful feature extraction EVALUATE-FOR dataset distillation methods. training efficiency EVALUATE-FOR dataset distillation methods. distributed kernel - based meta - learning framework USED-FOR dataset distillation. infinitely wide convolutional neural networks USED-FOR distributed kernel - based meta - learning framework. infinitely wide convolutional neural networks USED-FOR dataset distillation. test accuracy EVALUATE-FOR CIFAR10 image classification task. Fashion - MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION Fashion - MNIST. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. MNIST CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION MNIST. CIFAR-100 CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR-100. Fashion - MNIST CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION Fashion - MNIST. MNIST HYPONYM-OF settings. Fashion - MNIST HYPONYM-OF settings. CIFAR-100 HYPONYM-OF settings. CIFAR-10 HYPONYM-OF settings. they COMPARE naturally occurring data. naturally occurring data COMPARE they. distilled datasets COMPARE naturally occurring data. naturally occurring data COMPARE distilled datasets. Method is machine learning algorithms. ,"This paper proposes a distributed kernel-based meta-learning framework for dataset distillation. The proposed method is based on the idea of distillation, which is an extension of the distillation framework proposed in [1]. The authors show that the proposed method outperforms existing distillation methods on CIFAR-10, Fashion-MNIST, and Fashion-SVHN datasets. The authors also show that their method is able to outperform other distillation approaches on the MNIST classification task.","This paper proposes a distributed kernel-based meta-learning framework for dataset distillation. The proposed method is based on the idea of distillation, which is an extension of the distillation framework proposed in [1]. The authors show that the proposed method outperforms existing distillation methods on CIFAR-10, Fashion-MNIST, and Fashion-SVHN datasets. The authors also show that their method is able to outperform other distillation approaches on the MNIST classification task."
9576,SP:9837e0c68887cc1382aefd0ead01f72cde199e0d,Semi - supervised learning ( SSL ) USED-FOR model. unlabeled data USED-FOR model. unlabeled data USED-FOR Semi - supervised learning ( SSL ). label space FEATURE-OF labeled and unlabeled data. FixMatch HYPONYM-OF SSL methods. Learning representations of inliers USED-FOR OSSL. FixMatch CONJUNCTION novelty detection. novelty detection CONJUNCTION FixMatch. OpenMatch USED-FOR FixMatch. OpenMatch CONJUNCTION novelty detection. novelty detection CONJUNCTION OpenMatch. threshold USED-FOR outliers. OVA - classifier USED-FOR confidence score. open - set soft - consistency regularization loss USED-FOR outlier detection. smoothness FEATURE-OF OVA - classifier. open - set soft - consistency regularization loss USED-FOR smoothness. OpenMatch COMPARE supervised model. supervised model COMPARE OpenMatch. CIFAR10 EVALUATE-FOR supervised model. Method is SSL algorithms. ,"This paper proposes a new semi-supervised learning method for outlier detection. The proposed method, called OpenMatch, is based on FixMatch and OpenMatch. The key idea is to learn representations of inliers from unlabeled data and then use them to improve the confidence score of the classifier. The method is evaluated on CIFAR-10 and shows that OpenMatch outperforms FixMatch.","This paper proposes a new semi-supervised learning method for outlier detection. The proposed method, called OpenMatch, is based on FixMatch and OpenMatch. The key idea is to learn representations of inliers from unlabeled data and then use them to improve the confidence score of the classifier. The method is evaluated on CIFAR-10 and shows that OpenMatch outperforms FixMatch."
9601,SP:eb86d33d5d47f1cfe2c66ca2c9f852229e32a32f,"it USED-FOR achiever policy. it USED-FOR explorer. explorer CONJUNCTION achiever policy. achiever policy CONJUNCTION explorer. Latent Explorer Achiever ( LEXA ) HYPONYM-OF agent. imagined rollouts USED-FOR it. image inputs USED-FOR world model. imagined rollouts USED-FOR achiever policy. prior methods COMPARE explorer. explorer COMPARE prior methods. LEXA USED-FOR tasks. goal images zero - shot FEATURE-OF tasks. approaches USED-FOR unsupervised goal reaching. LEXA COMPARE approaches. approaches COMPARE LEXA. LEXA USED-FOR unsupervised goal reaching. prior benchmarks CONJUNCTION benchmark. benchmark CONJUNCTION prior benchmarks. test tasks EVALUATE-FOR benchmark. robotic manipulation and locomotion domains FEATURE-OF test tasks. benchmark EVALUATE-FOR approaches. test tasks EVALUATE-FOR LEXA. benchmark EVALUATE-FOR LEXA. prior benchmarks EVALUATE-FOR approaches. prior benchmarks EVALUATE-FOR LEXA. Method is artificial agents. OtherScientificTerm are complex visual environments, supervision, and achiever. ","This paper proposes Latent Explorer Achiever (LEXA), an agent that learns to reach goals in an unsupervised manner without supervision. LEXA uses a world model to learn an explorer policy and an achiever policy to guide the goal reaching. The explorer policy is trained using an RL algorithm, and the achiever is trained on the world model. Experiments are conducted on a number of robotic manipulation and locomotion tasks. ","This paper proposes Latent Explorer Achiever (LEXA), an agent that learns to reach goals in an unsupervised manner without supervision. LEXA uses a world model to learn an explorer policy and an achiever policy to guide the goal reaching. The explorer policy is trained using an RL algorithm, and the achiever is trained on the world model. Experiments are conducted on a number of robotic manipulation and locomotion tasks. "
9626,SP:2db4aba9a370df67f786157f18cbaa4167c6a46d,"trainable parameters USED-FOR Language models. networks USED-FOR tasks. task EVALUATE-FOR networks. parameter sharing CONJUNCTION factorized representations. factorized representations CONJUNCTION parameter sharing. model compression CONJUNCTION parameter sharing. parameter sharing CONJUNCTION model compression. factorized representations CONJUNCTION knowledge distillation. knowledge distillation CONJUNCTION factorized representations. reshaped and rearranged original matrix USED-FOR low - rank factorized representation. expressiveness EVALUATE-FOR low - rank layers. embedding CONJUNCTION attention. attention CONJUNCTION embedding. attention CONJUNCTION feed - forward layers. feed - forward layers CONJUNCTION attention. approach USED-FOR Transformer models. OtherScientificTerm are lottery ticket hypothesis, parameter space, self - attention layers, parameter matrix, and architecture of the network. Generic is models. Method is factorized representations of matrices. Task is deep networks. Metric is on - task performance. ","This paper studies the lottery ticket hypothesis, which is a well-studied topic in machine learning. The paper proposes a new method to learn low-rank factorized representations of matrices. The key idea is to reshape and rearrange the original matrix of the parameter matrix in the parameter space, and then use the reshaped and rearranged original matrix to form a low rank factorized representation. The authors show that the proposed method is able to improve the performance of deep neural networks in terms of on-task performance. ","This paper studies the lottery ticket hypothesis, which is a well-studied topic in machine learning. The paper proposes a new method to learn low-rank factorized representations of matrices. The key idea is to reshape and rearrange the original matrix of the parameter matrix in the parameter space, and then use the reshaped and rearranged original matrix to form a low rank factorized representation. The authors show that the proposed method is able to improve the performance of deep neural networks in terms of on-task performance. "
9651,SP:0ff862542ada5b664d615c26e7a4a95b6cbe540e,"structured source code representations USED-FOR models. syntax trees HYPONYM-OF structured source code representations. them PART-OF attention module of Transformer. path encoding methods PART-OF attention module of Transformer. path encoding PART-OF them. them PART-OF unified Transformer framework. TPTrans COMPARE baselines. baselines COMPARE TPTrans. code summarization EVALUATE-FOR approaches. Task is Learning distributed representation of source code. Method is positional encoding. OtherScientificTerm are pairwise path, tree root, and syntax tree. Generic is paths. ","This paper presents a unified Transformer framework for learning structured source code representations. The authors propose a new approach to learn a structured representation of source code, called Transformer Transformer (TPTrans). The proposed approach is based on the idea of path encoding, which is an extension of the Transformer architecture. The proposed method is evaluated on code summarization tasks and is shown to outperform the baselines. ","This paper presents a unified Transformer framework for learning structured source code representations. The authors propose a new approach to learn a structured representation of source code, called Transformer Transformer (TPTrans). The proposed approach is based on the idea of path encoding, which is an extension of the Transformer architecture. The proposed method is evaluated on code summarization tasks and is shown to outperform the baselines. "
9676,SP:727bcd651b11b7d84dd2c2d535cc85402f9117d4,"them USED-FOR high - resolution image generation. Attention - based models USED-FOR long range dependency. Transformer HYPONYM-OF Attention - based models. Generative Adversarial Networks ( GANs ) USED-FOR high - resolution image generation. multi - axis blocked self - attention USED-FOR mixing of local and global attention. global self - attention COMPARE multi - axis blocked self - attention. multi - axis blocked self - attention COMPARE global self - attention. implicit neural function FEATURE-OF multi - layer perceptrons. cross - attention USED-FOR self - modulation component. model USED-FOR synthesizing high definition images. linear computational complexity EVALUATE-FOR model. unconditional ImageNet CONJUNCTION FFHQ 256 × 256. FFHQ 256 × 256 CONJUNCTION unconditional ImageNet. FFHQ 256 × 256 EVALUATE-FOR HiT. FID scores EVALUATE-FOR HiT. unconditional ImageNet EVALUATE-FOR HiT. OtherScientificTerm are quadratic complexity of self - attention operation, low - resolution stages of the generative process, self - attention, image size, and convolutions. Method are generative process, and GANs. Task is high - resolution stages. ","This paper proposes a multi-layer self-attention model for high-resolution image generation. The proposed model is based on the Transformer architecture. The authors show that the proposed model can achieve better performance than the state-of-the-art on FFHQ, FFHQ-256, and unconditional ImageNet. ","This paper proposes a multi-layer self-attention model for high-resolution image generation. The proposed model is based on the Transformer architecture. The authors show that the proposed model can achieve better performance than the state-of-the-art on FFHQ, FFHQ-256, and unconditional ImageNet. "
9701,SP:41a6753bc56eb16040600666a859294ae36cfa9c,"query complexity EVALUATE-FOR learning geodesically convex halfspaces on graphs. Geodesic convexity HYPONYM-OF Euclidean convexity. treewidth CONJUNCTION minimum hull set size. minimum hull set size CONJUNCTION treewidth. query complexity CONJUNCTION VC dimension. VC dimension CONJUNCTION query complexity. query complexity EVALUATE-FOR Radon number. cut size FEATURE-OF labelling. approach COMPARE active learning algorithms. active learning algorithms COMPARE approach. ground - truth communities PART-OF real - world graphs. OtherScientificTerm are convex sets, diameter, and separation axioms. Material is unlabelled graph. ","This paper studies the query complexity of learning geodesically convex halfspaces on graphs. The authors consider the problem of learning a geodesic convex subset of a graph, where the vertices are convex and the edges are non-convex. They show that the number of queries required to learn the convex set is bounded by the Radon number and the minimum hull set size. They also show that if the radius of the set is larger than a certain threshold, then the queries will be much faster than if it is smaller than a threshold. ","This paper studies the query complexity of learning geodesically convex halfspaces on graphs. The authors consider the problem of learning a geodesic convex subset of a graph, where the vertices are convex and the edges are non-convex. They show that the number of queries required to learn the convex set is bounded by the Radon number and the minimum hull set size. They also show that if the radius of the set is larger than a certain threshold, then the queries will be much faster than if it is smaller than a threshold. "
9726,SP:e880db33ba8c305ef1808a02325e2d2b7da95e68,"action localization dataset EVALUATE-FOR TAL head. large action classification dataset EVALUATE-FOR video encoder. transfer learning pipeline USED-FOR temporal action localization ( TAL ) methods. video encoder USED-FOR action classification. video encoder USED-FOR task discrepancy problem. video encoder CONJUNCTION TAL head. TAL head CONJUNCTION video encoder. TAL head USED-FOR joint optimization. video encoder USED-FOR joint optimization. this USED-FOR TAL. video encoder CONJUNCTION TAL head. TAL head CONJUNCTION video encoder. temporal, spatial or spatio - temporal resolution FEATURE-OF mini - batch composition. LoFi optimization approach USED-FOR TAL methods. ResNet18 based video encoder USED-FOR method. single RGB stream FEATURE-OF ResNet18 based video encoder. OtherScientificTerm are encoder, GPU memory constraints, mid - range hardware budget, gradients, and TAL supervision loss. Method are TAL learning, and feature representations. ",This paper proposes a transfer learning pipeline for temporal action localization (TAL) methods. The proposed method is based on the LoFi optimization framework. The key idea is to use a video encoder and a TAL head to jointly optimize the task discrepancy problem. Experiments show that the proposed method outperforms state-of-the-art TAL methods.,This paper proposes a transfer learning pipeline for temporal action localization (TAL) methods. The proposed method is based on the LoFi optimization framework. The key idea is to use a video encoder and a TAL head to jointly optimize the task discrepancy problem. Experiments show that the proposed method outperforms state-of-the-art TAL methods.
9751,SP:f79e91e469a70b219cd4a2116d5f389842f265ec,"Gaussian design matrix CONJUNCTION arbitrary 2 noise distribution. arbitrary 2 noise distribution CONJUNCTION Gaussian design matrix. Gaussian design matrix FEATURE-OF convex penalty in linear models. arbitrary 2 noise distribution FEATURE-OF convex penalty in linear models. gradient - Lipschitz loss function USED-FOR M - estimators. Huber loss CONJUNCTION Elastic - Net penalty. Elastic - Net penalty CONJUNCTION Huber loss. heavy - tails FEATURE-OF noise distribution. Elastic - Net penalty USED-FOR robust M - estimator. Huber loss USED-FOR robust M - estimator. differentiability structure FEATURE-OF convex regularized M - estimators. adaptive criterion USED-FOR regularized M - estimators. criterion USED-FOR out - of - sample error. noise distribution CONJUNCTION covariance of the design. covariance of the design CONJUNCTION noise distribution. criterion USED-FOR out - of - sample error. OtherScientificTerm are differentiation, intermediate high - dimensional 9 regime, dimension, distribution of the residuals, and out - of - sample 14 error. Generic is derivatives. Material is Simulated data. Method is M - estimator. ","This paper studies the problem of estimating the out-of-sample error of M-estimators in the intermediate high-dimensional 9 regime. The authors consider the case where the noise distribution is arbitrary and the design matrix is Gaussian. They derive an adaptive criterion for the out of sample error of a regularized M-iterator, which is based on the gradient-Lipschitz loss function. They show that this criterion is robust to heavy-tailed noise, Huber loss, and Elastic-Net penalty. They also show that the convergence rate of the proposed criterion depends on the dimension of the residuals. ","This paper studies the problem of estimating the out-of-sample error of M-estimators in the intermediate high-dimensional 9 regime. The authors consider the case where the noise distribution is arbitrary and the design matrix is Gaussian. They derive an adaptive criterion for the out of sample error of a regularized M-iterator, which is based on the gradient-Lipschitz loss function. They show that this criterion is robust to heavy-tailed noise, Huber loss, and Elastic-Net penalty. They also show that the convergence rate of the proposed criterion depends on the dimension of the residuals. "
9776,SP:be53bc4c064402489b644332ad9c17743502d73c,"calibrated beam - based algorithm USED-FOR neural abstractive summarization. calibrated beam - based algorithm USED-FOR local optimality problem. beam search USED-FOR local optimality problem. global attention distribution FEATURE-OF calibrated beam - based algorithm. attention distribution USED-FOR global protocol. global scoring mechanism USED-FOR beam search. global scoring mechanism USED-FOR beam search. global ( attention)-aware inference COMPARE summarization models. summarization models COMPARE global ( attention)-aware inference. empirical hyper - parameters USED-FOR summarization models. empirical hyper - parameters USED-FOR global ( attention)-aware inference. Generic are design, and algorithm. Task is inference. OtherScientificTerm is corrupted attention distributions. ","This paper proposes a new method for neural abstractive summarization based on beam search. Specifically, the authors propose a global scoring mechanism for beam search, which is based on the global attention distribution. The authors show that the proposed method outperforms the baselines in terms of local optimality. The proposed method is evaluated on a number of benchmark datasets.","This paper proposes a new method for neural abstractive summarization based on beam search. Specifically, the authors propose a global scoring mechanism for beam search, which is based on the global attention distribution. The authors show that the proposed method outperforms the baselines in terms of local optimality. The proposed method is evaluated on a number of benchmark datasets."
9801,SP:4c7d14ab3304cfbf083815aa6e6d9c0e0a5fba6f,"Attention mechanism USED-FOR deep learning models. relative position encoding PART-OF deep learning models. canonical local coordinate system USED-FOR neighborhoods. attention USED-FOR manifolds. method USED-FOR feature vectors. regular field of cyclic groups USED-FOR feature fields. regular field of cyclic groups USED-FOR intermediate layers. feature fields USED-FOR intermediate layers. feature vectors USED-FOR fields. method USED-FOR expressive ability. regular field of cyclic groups USED-FOR expressive ability. position vector USED-FOR orientation of the coordinate system. ambient space FEATURE-OF orientation of the coordinate system. local coordinate system USED-FOR position vector. global coordinate system HYPONYM-OF orientation of the coordinate system. gauge equivariance USED-FOR self - attention. triangle meshes USED-FOR Gauge Equivariant Transformer ( GET ). common recognition tasks EVALUATE-FOR GET. Method are equivariant transformer, and multi - head selfattention. OtherScientificTerm are orientation of local coordinate systems, gauge equivariant, position - based and content - based information, and rotation invariance. ",This paper proposes a new self-attention mechanism based on gauge equivariant transformer (GEE) for multi-head selfattention. The key idea is to use a cyclic group of cyclic groups as a regularizer for the feature fields of the intermediate layers. The authors show that this regularizer is invariant to the rotation of the local coordinate system and the global coordinate system. The proposed method is evaluated on a number of tasks and shows that it outperforms baselines.,This paper proposes a new self-attention mechanism based on gauge equivariant transformer (GEE) for multi-head selfattention. The key idea is to use a cyclic group of cyclic groups as a regularizer for the feature fields of the intermediate layers. The authors show that this regularizer is invariant to the rotation of the local coordinate system and the global coordinate system. The proposed method is evaluated on a number of tasks and shows that it outperforms baselines.
9826,SP:19cd64baeb7db11b5ec066e6f8ccb4bc576d3588,"methods USED-FOR unsupervised learning of finite mixture models. expectation maximization CONJUNCTION Metropolis - Hastings algorithm. Metropolis - Hastings algorithm CONJUNCTION expectation maximization. Metropolis - Hastings algorithm USED-FOR approach. expectation maximization PART-OF approach. it USED-FOR shallow and deep mixture 8 models. mixtures of normalizing flows CONJUNCTION sum - product ( transform ) networks. sum - product ( transform ) networks CONJUNCTION mixtures of normalizing flows. synthetic and real - data 10 contexts EVALUATE-FOR deep models. sum - product ( transform ) networks HYPONYM-OF deep models. mixtures of normalizing flows HYPONYM-OF deep models. synthetic and real - data 10 contexts EVALUATE-FOR method. Method is finite mixture models. OtherScientificTerm are mixture, and complex, and possibly nonlinear, transformations. Metric is computational cost. ",This paper proposes a method for unsupervised learning of finite mixture models. The proposed method is based on the Metropolis-Hastings algorithm. The authors show that the proposed method outperforms the baselines on synthetic and real-world datasets. The main contribution of the paper is that the method can be applied to both shallow and deep mixture 8 models. ,This paper proposes a method for unsupervised learning of finite mixture models. The proposed method is based on the Metropolis-Hastings algorithm. The authors show that the proposed method outperforms the baselines on synthetic and real-world datasets. The main contribution of the paper is that the method can be applied to both shallow and deep mixture 8 models. 
9851,SP:aae8847c5e52d14820967ab39770ab4ae16df59c,"Sparse training USED-FOR deep neural networks. dense computation USED-FOR backward propagation step. sparse forward and backward passes USED-FOR sparse training method. global sparsity constraint FEATURE-OF continuous minimization problem. continuous minimization problem USED-FOR training process. weight update CONJUNCTION structure parameter update. structure parameter update CONJUNCTION weight update. structure parameter update HYPONYM-OF steps. weight update HYPONYM-OF steps. chain rule USED-FOR step. sparse structure USED-FOR chain rule. variance reduced policy gradient estimator USED-FOR sparse training. forward passes CONJUNCTION backward propagation. backward propagation CONJUNCTION forward passes. chain rule based gradient estimators USED-FOR variance reduced policy gradient estimator. variance reduced policy gradient estimator USED-FOR step. chain rule based gradient estimators USED-FOR step. forward passes USED-FOR variance reduced policy gradient estimator. algorithm USED-FOR training process. real - world datasets EVALUATE-FOR algorithm. OtherScientificTerm is memory usage. Method are neural networks, and gradient estimator. Generic is methods. Task is optimization process. ",This paper proposes a new method for sparse training for deep neural networks. The authors propose a global sparsity constraint for the forward and backward propagation step of the training process. The proposed method is based on the variance reduced policy gradient estimator and the chain rule based gradient estimators. The method is evaluated on several benchmark datasets.,This paper proposes a new method for sparse training for deep neural networks. The authors propose a global sparsity constraint for the forward and backward propagation step of the training process. The proposed method is based on the variance reduced policy gradient estimator and the chain rule based gradient estimators. The method is evaluated on several benchmark datasets.
9876,SP:e0aa68ab03a3ef396b0dc4be4190b328d72cfab0,"importance samplers ( IS ) CONJUNCTION Markov chain Monte Carlo ( MCMC ) samplers. Markov chain Monte Carlo ( MCMC ) samplers CONJUNCTION importance samplers ( IS ). iterated sampling - importance resampling mechanism USED-FOR π. NEO - IS CONJUNCTION iterated sampling - importance resampling mechanism. iterated sampling - importance resampling mechanism CONJUNCTION NEO - IS. NEO - IS COMPARE NEO - MCMC. NEO - MCMC COMPARE NEO - IS. NEO - MCMC USED-FOR π. NEO - IS USED-FOR NEO - MCMC. iterated sampling - importance resampling mechanism USED-FOR NEO - MCMC. NEO - MCMC USED-FOR multimodal targets. T USED-FOR conformal Hamiltonian system. NEO - IS COMPARE NEO - MCMC. NEO - MCMC COMPARE NEO - IS. discrete - time integrator USED-FOR conformal Hamiltonian system. T USED-FOR NEO - IS. T USED-FOR discrete - time integrator. NEO - MCMC USED-FOR explicit mixing time estimates. OtherScientificTerm are complex distribution π, intractable normalizing constant, invertible map T, forward and backward Orbits, proposal distribution ρ, map T, NEO, Non - Equilibrium Orbits, and normalizing constant. Generic are schemes, and methods. ","This paper studies the problem of estimating the mixing time of MCMC and importance samplers. The authors propose a new method called NEO-IS, which is based on iterated sampling-importance resampling (IS) and iterated importance sampling (NEO-IS). The authors show that the proposed method can be used to estimate the mixing times of two MCMC algorithms, namely NEO-MCMC and NEO-Is. The proposed method is shown to outperform the existing methods in terms of mixing time estimation. ","This paper studies the problem of estimating the mixing time of MCMC and importance samplers. The authors propose a new method called NEO-IS, which is based on iterated sampling-importance resampling (IS) and iterated importance sampling (NEO-IS). The authors show that the proposed method can be used to estimate the mixing times of two MCMC algorithms, namely NEO-MCMC and NEO-Is. The proposed method is shown to outperform the existing methods in terms of mixing time estimation. "
9901,SP:506dca4f64f837e32958c3c43a0c68f194a36bb3,training CONJUNCTION inference. inference CONJUNCTION training. permutation invariance CONJUNCTION equivariance. equivariance CONJUNCTION permutation invariance. permutation invariance FEATURE-OF set - function constraints. equivariance FEATURE-OF set - function constraints. property USED-FOR large scale mini - batch set encoding. Mini - Batch Consistency ( MBC ) USED-FOR large scale mini - batch set encoding. Mini - Batch Consistency ( MBC ) HYPONYM-OF property. attention - based set encoding mechanism USED-FOR set representations. mini - batch processing of sets USED-FOR attention - based set encoding mechanism. symmetries of invariance CONJUNCTION equivariance. equivariance CONJUNCTION symmetries of invariance. MBC USED-FOR method. symmetries of invariance FEATURE-OF method. method USED-FOR rich set encoding representations. rich set encoding representations USED-FOR set - structured data. Method is set encoding algorithms. OtherScientificTerm is computational and memory resources. Generic is assumptions. Task is large - scale set encoding. ,"This paper studies the problem of large-scale mini-batch set encoding. In particular, the authors consider the setting where there is a large number of sets, and each set is represented by a set-function. The authors propose a method to encode the set representations using attention. They show that the proposed method is able to achieve better performance than existing methods. ","This paper studies the problem of large-scale mini-batch set encoding. In particular, the authors consider the setting where there is a large number of sets, and each set is represented by a set-function. The authors propose a method to encode the set representations using attention. They show that the proposed method is able to achieve better performance than existing methods. "
9926,SP:b2eafdb24fa081ae8b37525d70fb4bc2d54518dc,"Diplomacy HYPONYM-OF game. human data USED-FOR policy. Diplomacy CONJUNCTION StarCraft. StarCraft CONJUNCTION Diplomacy. StarCraft CONJUNCTION Dota. Dota CONJUNCTION StarCraft. branching factors FEATURE-OF games. Diplomacy HYPONYM-OF branching factors. StarCraft HYPONYM-OF branching factors. Dota HYPONYM-OF games. Diplomacy HYPONYM-OF games. StarCraft HYPONYM-OF games. action exploration CONJUNCTION equilibrium approximation. equilibrium approximation CONJUNCTION action exploration. algorithm USED-FOR action exploration. algorithm USED-FOR equilibrium approximation. algorithm USED-FOR policy proposal network. value iteration USED-FOR algorithm. policy USED-FOR model training. equilibrium search procedure USED-FOR policy. equilibrium search procedure USED-FOR model training. algorithm USED-FOR agent. DORA USED-FOR two - player variant of Diplomacy. agent USED-FOR two - player variant of Diplomacy. DORA HYPONYM-OF agent. methods USED-FOR full - scale no - press Diplomacy. human data USED-FOR agent. agent COMPARE human - data bootstrapped agents. human - data bootstrapped agents COMPARE agent. self play USED-FOR superhuman performance. multiple equilibria FEATURE-OF Diplomacy. Diplomacy FEATURE-OF superhuman performance. Task is complex games. Method are handcrafted reward shaping, and double oracle step. OtherScientificTerm are combinatorial action spaces, and policy proposals. Generic is it. ","This paper proposes a new method for learning a policy proposal network for no-press Diplomacy. The proposed method is based on the idea of self-play, where the agent is given a set of action spaces, and the goal is to find a policy that maximizes the sum of all the actions in the action space. The agent is trained with a single oracle step, and is trained on a large set of human data. The authors show that the proposed method achieves state-of-the-art performance on a number of games, including StarCraft, Dota, and Diplomacy, and outperforms human-data bootstrapped agents.","This paper proposes a new method for learning a policy proposal network for no-press Diplomacy. The proposed method is based on the idea of self-play, where the agent is given a set of action spaces, and the goal is to find a policy that maximizes the sum of all the actions in the action space. The agent is trained with a single oracle step, and is trained on a large set of human data. The authors show that the proposed method achieves state-of-the-art performance on a number of games, including StarCraft, Dota, and Diplomacy, and outperforms human-data bootstrapped agents."
9951,SP:1ce1cef9988a07ccd2175a718b29ad23bc779429,it USED-FOR sequence modeling. attention heads PART-OF Multi - head attention. positive transfer CONJUNCTION negative interference. negative interference CONJUNCTION positive transfer. Multilingual and multi - domain learning USED-FOR sequence modeling. generalization EVALUATE-FOR non - selective attention sharing. attention sharing strategies USED-FOR multilingual and multi - domain sequence modeling. approach USED-FOR shared and specialized attention heads. attention sharing strategies USED-FOR sequence models. tasks EVALUATE-FOR attention sharing strategies. tasks EVALUATE-FOR sequence models. speech recognition HYPONYM-OF tasks. multi - head attention USED-FOR sequence models. BLEU EVALUATE-FOR multi - domain setting. BLEU EVALUATE-FOR speech - to - text translation. approach USED-FOR speech - to - text translation. multilingual setting EVALUATE-FOR approach. BLEU EVALUATE-FOR approach. BLEU EVALUATE-FOR approach. ,This paper proposes a multi-head attention method for multi-domain sequence modeling. The proposed method is based on the idea that the attention heads of a sequence model should be shared across different domains. The method is evaluated on BLEU and speech recognition tasks and shows promising results. ,This paper proposes a multi-head attention method for multi-domain sequence modeling. The proposed method is based on the idea that the attention heads of a sequence model should be shared across different domains. The method is evaluated on BLEU and speech recognition tasks and shows promising results. 
9976,SP:69c522cea4a150624bc709e1c12c0f65183c1b2a,covariate shift HYPONYM-OF distribution shift. covariate shift FEATURE-OF real - world applications. high - dimensional asymptotics FEATURE-OF random feature regression. limiting test error CONJUNCTION bias. bias CONJUNCTION limiting test error. covariate shift USED-FOR random feature regression. robustness EVALUATE-FOR overparameterized models. Method is machine learning models. OtherScientificTerm is conditional label distributions. Task is machine learning. ,"This paper studies the problem of covariate shift in random feature regression. The authors consider the case where the label distribution of a classifier is subject to a distribution shift. They show that under certain conditions, the covariance shift of the classifier can be seen as an asymptotic shift in the test error of the model. They also show that this shift can be viewed as a limiting test error in the case of overparameterized models. ","This paper studies the problem of covariate shift in random feature regression. The authors consider the case where the label distribution of a classifier is subject to a distribution shift. They show that under certain conditions, the covariance shift of the classifier can be seen as an asymptotic shift in the test error of the model. They also show that this shift can be viewed as a limiting test error in the case of overparameterized models. "
10001,SP:ff1b7a7a6295e8f40f3b5df5f6950ca9d33603e0,"Thompson sampling CONJUNCTION Bayesian sequential decision - making algorithms. Bayesian sequential decision - making algorithms CONJUNCTION Thompson sampling. Bayesian sequential decision - making algorithms USED-FOR explore / exploit trade - offs. Thompson sampling USED-FOR explore / exploit trade - offs. explore / exploit trade - offs FEATURE-OF ( contextual ) bandits. prior USED-FOR algorithms. expected reward EVALUATE-FOR Thompson sampling ( TS ). misspecified prior USED-FOR Thompson sampling ( TS ). well - specified prior USED-FOR TS. parametric form FEATURE-OF prior. universal constants FEATURE-OF it. bounded support FEATURE-OF priors. algorithms USED-FOR Bayesian meta - learning setting. generic PAC guarantees USED-FOR algorithms. Bayesian POMDPs HYPONYM-OF Bayesian decision - making setting. knowledge gradient algorithm ( KG ) HYPONYM-OF Bayesian decision - making algorithms. multi - armed and contextual bandits USED-FOR meta - learning. structured and correlated priors FEATURE-OF multi - armed and contextual bandits. structured and correlated priors USED-FOR meta - learning. OtherScientificTerm are domain knowledge, misspecification, total - variation distance, learning horizon, cardinality or structure of the action space, sensitivity analysis, and prior misspecification. Generic is bound. Method are contextual bandits, and KG ). ",This paper studies the generalization of Thompson sampling (TS) and contextual bandits (contextual bandits) to Bayesian POMDPs and meta-learning settings. The authors show that Thompson sampling can be regarded as a variant of the Thompson sampling with a parametric form. They also show that prior misspecification can be used to improve the performance of contextual bandits. ,This paper studies the generalization of Thompson sampling (TS) and contextual bandits (contextual bandits) to Bayesian POMDPs and meta-learning settings. The authors show that Thompson sampling can be regarded as a variant of the Thompson sampling with a parametric form. They also show that prior misspecification can be used to improve the performance of contextual bandits. 
10026,SP:3477b64480ed638b1c4e1f8aa73fc2e77666c89a,"PAC - learning model CONJUNCTION Equivalence - Query - learning model. Equivalence - Query - learning model CONJUNCTION PAC - learning model. sample / query complexity EVALUATE-FOR PAC - learning model. exponential separation FEATURE-OF sample / query complexity. adversarial training COMPARE training. training COMPARE adversarial training. adversarial training USED-FOR generalization. on - manifold adversarial examples USED-FOR adversarial training. Method are PAC model, Equivalence - Query model, adversarial model, and Equivalance - Query model. OtherScientificTerm are teacher, learner, PAC bound, adversarial examples, norm constraint, and adversary. Metric are adversarial robustness, and robustness. Generic is model. ",This paper studies the adversarial robustness of PAC-learning and Equivalence-query-learning models. The authors show that adversarial training can be used to improve the sample/query complexity of adversarial learning models. They also show that the PAC bound can be improved by adding a norm constraint to the norm of the classifier. ,This paper studies the adversarial robustness of PAC-learning and Equivalence-query-learning models. The authors show that adversarial training can be used to improve the sample/query complexity of adversarial learning models. They also show that the PAC bound can be improved by adding a norm constraint to the norm of the classifier. 
10051,SP:7520cc1203bb06bbe432e7cc679892e95258ed99,"models USED-FOR transfer learning. benchmarks EVALUATE-FOR task. techniques USED-FOR algorithms. methods USED-FOR PARC. PARC COMPARE methods. methods COMPARE PARC. methods USED-FOR diverse model selection. diverse model selection EVALUATE-FOR PARC. model selection USED-FOR transfer learning. Method is pretrained deep learning models. Material is large model banks. OtherScientificTerm is diversity of off - the - shelf models. Generic are model, and setting. Task is Scalable Diverse Model Selection. ","This paper proposes a new task called Scalable Diverse model selection (PARC) for transfer learning. PARC is an extension of the Scalable Model Selection (SMS) task, which aims to improve the diversity of off-the-shelf models in a transfer learning setting. The authors propose a new algorithm for diverse model selection, which is based on the PARC framework. The proposed method is evaluated on a number of transfer learning benchmarks and shows that PARC outperforms SMS in terms of transfer performance.","This paper proposes a new task called Scalable Diverse model selection (PARC) for transfer learning. PARC is an extension of the Scalable Model Selection (SMS) task, which aims to improve the diversity of off-the-shelf models in a transfer learning setting. The authors propose a new algorithm for diverse model selection, which is based on the PARC framework. The proposed method is evaluated on a number of transfer learning benchmarks and shows that PARC outperforms SMS in terms of transfer performance."
10076,SP:dcdb9c88f61ac3caf3da8255a7953c753cf048d1,"low - dimensional binary codes USED-FOR compression of high - dimensional neural representations. large bit - codes USED-FOR compression of high - dimensional neural representations. method USED-FOR Low - dimensional binary Codes ( LLC ). method USED-FOR low - dimensional binary codes. annotated attributes CONJUNCTION label meta - data. label meta - data CONJUNCTION annotated attributes. label meta - data HYPONYM-OF side - information. annotated attributes HYPONYM-OF side - information. it USED-FOR image retrieval. it USED-FOR codes. binary codes COMPARE 10 dimensional real representations. 10 dimensional real representations COMPARE binary codes. binary codes COMPARE HashNet. HashNet COMPARE binary codes. ImageNet-100 retrieval problem EVALUATE-FOR binary codes. Material is ImageNet-1 K. Metric is classification accuracy. Method is ResNet50. Task is OOD detection. Generic are baseline, and Code. OtherScientificTerm is threshold. ",This paper proposes a method for low-dimensional binary codes (LLC) for OOD detection. The proposed method is based on the idea that binary codes can be decomposed into small bit-codes and large bit-coders. The method is evaluated on ImageNet-1K and ResNet-50 and shows that it outperforms HashNet. The authors also show that the proposed method can be used for image retrieval.,This paper proposes a method for low-dimensional binary codes (LLC) for OOD detection. The proposed method is based on the idea that binary codes can be decomposed into small bit-codes and large bit-coders. The method is evaluated on ImageNet-1K and ResNet-50 and shows that it outperforms HashNet. The authors also show that the proposed method can be used for image retrieval.
10101,SP:07def8c80d05f86402ce769313480b30cd99af43,"computational / storage costs EVALUATE-FOR convolutional neural networks ( CNNs ). model compression techniques CONJUNCTION adversarial training. adversarial training CONJUNCTION model compression techniques. adversarial perturbations FEATURE-OF robustness. throughput ( frames - per - second ) EVALUATE-FOR methods. GDWS USED-FOR pre - trained network. throughput EVALUATE-FOR pre - trained network. real - life hardware FEATURE-OF pre - trained network. robustness EVALUATE-FOR GDWS. throughput EVALUATE-FOR GDWS. pre - trained models USED-FOR it. algorithms USED-FOR GDWS convolutions. 2D convolution approximator USED-FOR GDWS. complexity and error constraints USED-FOR algorithms. ImageNet datasets EVALUATE-FOR GDWS. CIFAR-10 EVALUATE-FOR GDWS. Task is robust model compression. Method are Generalized Depthwise - Separable ( GDWS ) convolution, and 2D convolution. ","This paper studies the robustness of deep convolutional neural networks (CNNs) against adversarial perturbations and model compression. The authors propose a generalization of the GDWS convolution to 2D convolutions, which can be applied to any pre-trained CNNs. The proposed method is evaluated on CIFAR-10 and ImageNet datasets. ","This paper studies the robustness of deep convolutional neural networks (CNNs) against adversarial perturbations and model compression. The authors propose a generalization of the GDWS convolution to 2D convolutions, which can be applied to any pre-trained CNNs. The proposed method is evaluated on CIFAR-10 and ImageNet datasets. "
10126,SP:9e4d04b22ce4f986aabb747a42f40c827073e39e,"Retrosynthesis prediction HYPONYM-OF organic synthesis. neural models USED-FOR task. model USED-FOR graph edits. model USED-FOR synthons. top-1 accuracy EVALUATE-FOR model. OtherScientificTerm are precursor molecules, graph topology, and chemical reaction. Method are model design, graph - based approach, and manual correction. Generic is architecture. ","This paper proposes a graph-based approach for predicting the topology of a molecule. The proposed approach is based on the idea of graph editing, which is used in graph neural networks (GNNs). The proposed method is evaluated on the synthetic synthesis task and shows promising results. ","This paper proposes a graph-based approach for predicting the topology of a molecule. The proposed approach is based on the idea of graph editing, which is used in graph neural networks (GNNs). The proposed method is evaluated on the synthetic synthesis task and shows promising results. "
10151,SP:772277d969c95924755113c86663fb0e009f24cc,"Bayesian formulation of deconditioning USED-FOR reproducing kernel Hilbert space formulation. deconditioning USED-FOR downscaling setup. conditional mean embedding estimator USED-FOR multiresolution data. solution USED-FOR deconditioning problem. posterior USED-FOR deconditioning problem. posterior USED-FOR latent field. posterior USED-FOR solution. minimax optimal convergence rate FEATURE-OF it. its EVALUATE-FOR methods. OtherScientificTerm are high - resolution ( HR ) information, LR samples, mediating variable, conditional expectation, conditional expectations, and inter - domain features. Task are statistical downscaling, and recovery of the underlying fine - grained field. Material is spatial datasets. ",This paper proposes a Bayesian formulation of deconditioning based on the reproducing kernel Hilbert space formulation for multiresolution data. The main idea is to use a conditional mean embedding estimator to estimate the conditional expectation of the latent field. The authors show that this estimator converges to a minimax optimal convergence rate for the downscaling problem. They also show that their estimator is able to recover the underlying fine-grained field.,This paper proposes a Bayesian formulation of deconditioning based on the reproducing kernel Hilbert space formulation for multiresolution data. The main idea is to use a conditional mean embedding estimator to estimate the conditional expectation of the latent field. The authors show that this estimator converges to a minimax optimal convergence rate for the downscaling problem. They also show that their estimator is able to recover the underlying fine-grained field.
10176,SP:59cfeb59cecac51fecff8f8ceb0266fc6ac22a05,"Deep sparse networks ( DSNs ) USED-FOR high - order feature interactions. highsparsity features FEATURE-OF prediction task. prediction task EVALUATE-FOR Deep sparse networks ( DSNs ). computation efficiency EVALUATE-FOR models. feature - interaction layer PART-OF DSNs. neural architecture search USED-FOR problem. distilled search space USED-FOR architectures. progressive search algorithm USED-FOR sparse prediction tasks. progressive search algorithm USED-FOR order - priority property. order - priority property FEATURE-OF sparse prediction tasks. Task is model inference. Material is real - world benchmark datasets. Metric are accuracy, and efficiency. Method is search algorithm. ","This paper studies the problem of learning sparse networks for high-order feature interactions. The authors propose a new search space for sparse prediction tasks. The search space is a distilled search space, and the proposed algorithm is based on the order-prior property of the search space. The algorithm is evaluated on a number of real-world benchmark datasets. ","This paper studies the problem of learning sparse networks for high-order feature interactions. The authors propose a new search space for sparse prediction tasks. The search space is a distilled search space, and the proposed algorithm is based on the order-prior property of the search space. The algorithm is evaluated on a number of real-world benchmark datasets. "
10201,SP:23a2171eab71c4fd3754791ca2aac9be87411cdb,algorithm USED-FOR transfer learning. pre - trained model USED-FOR task. fine - tuning HYPONYM-OF algorithm. labeled data USED-FOR pre - trained model. labeled data USED-FOR task. fine - tuning USED-FOR overfitting. noise FEATURE-OF robustness. generalization properties EVALUATE-FOR fine - tuning. noise stability FEATURE-OF fine - tuned model. self label - correction CONJUNCTION label - reweighting. label - reweighting CONJUNCTION self label - correction. layer - wise regularization CONJUNCTION self label - correction. self label - correction CONJUNCTION layer - wise regularization. interpolation between regularization and self - labeling methods PART-OF regularized self - labeling. layer - wise regularization PART-OF interpolation between regularization and self - labeling methods. self label - correction PART-OF regularized self - labeling. layer - wise regularization PART-OF regularized self - labeling. pre - trained model architectures USED-FOR image and text data sets. image and text data sets EVALUATE-FOR approach. pre - trained model architectures USED-FOR approach. image classification tasks CONJUNCTION few - shot classification task. few - shot classification task CONJUNCTION image classification tasks. approach COMPARE baseline methods. baseline methods COMPARE approach. few - shot classification task EVALUATE-FOR approach. image classification tasks EVALUATE-FOR baseline methods. image classification tasks EVALUATE-FOR approach. approach COMPARE baseline methods. baseline methods COMPARE approach. Metric is PAC - Bayes generalization bound. OtherScientificTerm is noisy labels. ,"This paper studies the problem of fine-tuning a pre-trained model for transfer learning from one task to another. The authors propose a new method for this problem, called PAC-Bayes generalization, which is based on the idea that the noise stability of a fine-tune model is a function of the number of labeled data and the amount of noise in the training data. The paper shows that the proposed method is able to achieve better generalization performance than the baseline methods on a variety of tasks, including image classification, few-shot classification, and text classification. ","This paper studies the problem of fine-tuning a pre-trained model for transfer learning from one task to another. The authors propose a new method for this problem, called PAC-Bayes generalization, which is based on the idea that the noise stability of a fine-tune model is a function of the number of labeled data and the amount of noise in the training data. The paper shows that the proposed method is able to achieve better generalization performance than the baseline methods on a variety of tasks, including image classification, few-shot classification, and text classification. "
10226,SP:f2a77f93bdc0401bbd6162a16fba25b9f90530e2,"value - at - risk ( VaR ) HYPONYM-OF tail - risk measures. finance and insurance industries HYPONYM-OF tail - risk measures. weighted sum of CVaR CONJUNCTION mean. mean CONJUNCTION weighted sum of CVaR. VaR CONJUNCTION weighted sum of CVaR. weighted sum of CVaR CONJUNCTION VaR. CVaR CONJUNCTION VaR. VaR CONJUNCTION CVaR. VaR CONJUNCTION mean. mean CONJUNCTION VaR. latter USED-FOR risk - return trade - off. risk - return trade - off FEATURE-OF finance. optimal δcorrect algorithm USED-FOR arms. heavy - tailed distributions FEATURE-OF arms. non - convex optimization problem USED-FOR algorithm. probability measures FEATURE-OF non - convex optimization problem. OtherScientificTerm are probability distributions, and arm. ","This paper studies the problem of tail-risk minimization in the context of value-at-risk (VaR) and CVaR (CVaR). In particular, the authors consider the case of heavy-tailed distributions, where the weights of the probability distributions are weighted by a weighted sum of CVaRs and VaRs. The authors show that the optimal δcorrect algorithm can be formulated as a non-convex optimization problem, and they show that it can be solved in terms of the weighting of the probabilities of the two arms. They also provide a theoretical analysis of this problem.","This paper studies the problem of tail-risk minimization in the context of value-at-risk (VaR) and CVaR (CVaR). In particular, the authors consider the case of heavy-tailed distributions, where the weights of the probability distributions are weighted by a weighted sum of CVaRs and VaRs. The authors show that the optimal δcorrect algorithm can be formulated as a non-convex optimization problem, and they show that it can be solved in terms of the weighting of the probabilities of the two arms. They also provide a theoretical analysis of this problem."
10251,SP:765942c86da1594b33268df6d0d15c682bc7eaa6,"Transformers USED-FOR computer vision tasks. Transformers USED-FOR modeling long - range dependency. self - attention mechanism USED-FOR modeling long - range dependency. intrinsic inductive bias ( IB ) USED-FOR modeling local visual structures. vision transformers USED-FOR image. 1D sequence of visual tokens USED-FOR image. 1D sequence of visual tokens USED-FOR vision transformers. training schedules USED-FOR IB. large - scale training data CONJUNCTION training schedules. training schedules CONJUNCTION large - scale training data. training schedules USED-FOR they. large - scale training data USED-FOR they. intrinsic IB USED-FOR Vision Transformer. ViTAE HYPONYM-OF convolutions. convolutions USED-FOR intrinsic IB. dilation rates FEATURE-OF convolutions. spatial pyramid reduction modules PART-OF ViTAE. it USED-FOR robust feature representation. it USED-FOR intrinsic scale invariance IB. convolution block PART-OF multi - head selfattention module. ViTAE PART-OF transformer layer. convolution block PART-OF transformer layer. convolution block PART-OF ViTAE. local features CONJUNCTION global dependencies. global dependencies CONJUNCTION local features. it USED-FOR global dependencies. it USED-FOR local features. intrinsic locality IB FEATURE-OF it. ImageNet EVALUATE-FOR ViTAE. downstream tasks EVALUATE-FOR ViTAE. ViTAE COMPARE concurrent works. concurrent works COMPARE ViTAE. ViTAE COMPARE baseline transformer. baseline transformer COMPARE ViTAE. downstream tasks EVALUATE-FOR concurrent works. downstream tasks EVALUATE-FOR baseline transformer. ImageNet CONJUNCTION downstream tasks. downstream tasks CONJUNCTION ImageNet. baseline transformer CONJUNCTION concurrent works. concurrent works CONJUNCTION baseline transformer. ImageNet EVALUATE-FOR baseline transformer. ImageNet EVALUATE-FOR concurrent works. OtherScientificTerm are local visual structures, scale variance, and rich multi - scale context. Method are feed - forward network, and pretrained models. ","This paper proposes ViTAE, a self-attention module for vision transformers, which is based on the idea of intrinsic inductive bias (IB) for modeling long-range dependency. The authors show that the intrinsic IB is invariant to large-scale training data and training schedules, and that it can be used to model global dependencies and local features. They also show that it is able to model the intrinsic scale invariance of intrinsic IB in convolutions. ","This paper proposes ViTAE, a self-attention module for vision transformers, which is based on the idea of intrinsic inductive bias (IB) for modeling long-range dependency. The authors show that the intrinsic IB is invariant to large-scale training data and training schedules, and that it can be used to model global dependencies and local features. They also show that it is able to model the intrinsic scale invariance of intrinsic IB in convolutions. "
10276,SP:5e3572a386f890c5864437985cf63b13844f338f,fine - tuning USED-FOR NLP fields. pre - trained language models USED-FOR NLP fields. pre - trained language models USED-FOR fine - tuning. adversarial examples USED-FOR it. synonyms USED-FOR word substitution attacks. word substitution attacks HYPONYM-OF adversarial examples. adversarial training HYPONYM-OF defense technique. adversarial training USED-FOR fine - tuning scenario. catastrophic forgetting FEATURE-OF it. pre - trained model USED-FOR generic and robust linguistic features. Robust Informative Fine - Tuning ( RIFT ) HYPONYM-OF adversarial fine - tuning method. objective model USED-FOR features. RIFT USED-FOR objective model. pre - trained model USED-FOR features. pre - trained weights USED-FOR one. sentiment analysis CONJUNCTION natural language inference. natural language inference CONJUNCTION sentiment analysis. RIFT COMPARE state - of - the - arts. state - of - the - arts COMPARE RIFT. NLP tasks EVALUATE-FOR state - of - the - arts. NLP tasks EVALUATE-FOR RIFT. natural language inference HYPONYM-OF NLP tasks. sentiment analysis HYPONYM-OF NLP tasks. Method is BERT - based sentiment analysis model. Task is fine - tuning process. ,"This paper proposes Robust Informative Fine-tuning (RIFT), a method for fine-tune a pre-trained language model with adversarial examples. RIFT is based on the idea that adversarial training can be used to improve the robustness of the model. The authors show that RIFT outperforms the state-of-the-art in terms of robustness against word substitution attacks and catastrophic forgetting. ","This paper proposes Robust Informative Fine-tuning (RIFT), a method for fine-tune a pre-trained language model with adversarial examples. RIFT is based on the idea that adversarial training can be used to improve the robustness of the model. The authors show that RIFT outperforms the state-of-the-art in terms of robustness against word substitution attacks and catastrophic forgetting. "
10301,SP:167a8b7e0173bffc5f08a9c2f378fe7bdf837da3,"acceleration method USED-FOR fixed - point iterations. Anderson mixing ( AM ) HYPONYM-OF acceleration method. convergence theory FEATURE-OF AM. Stochastic Anderson Mixing ( SAM ) scheme USED-FOR nonconvex stochastic optimization problems. damped projection and adaptive regularization USED-FOR AM. damped projection and adaptive regularization USED-FOR Stochastic Anderson Mixing ( SAM ) scheme. almost sure convergence CONJUNCTION worst - case iteration complexity. worst - case iteration complexity CONJUNCTION almost sure convergence. convergence theory FEATURE-OF SAM. almost sure convergence FEATURE-OF stationary points. almost sure convergence PART-OF convergence theory. worst - case iteration complexity PART-OF convergence theory. variance reduction technique PART-OF SAM. preconditioned mixing strategy USED-FOR SAM. faster convergence CONJUNCTION generalization ability. generalization ability CONJUNCTION faster convergence. preconditioned mixing strategy USED-FOR faster convergence. generalization ability EVALUATE-FOR preconditioned mixing strategy. DenseNet CONJUNCTION LSTM. LSTM CONJUNCTION DenseNet. ResNeXt CONJUNCTION DenseNet. DenseNet CONJUNCTION ResNeXt. vanilla CNN CONJUNCTION ResNets. ResNets CONJUNCTION vanilla CNN. WideResNet CONJUNCTION ResNeXt. ResNeXt CONJUNCTION WideResNet. ResNets CONJUNCTION WideResNet. WideResNet CONJUNCTION ResNets. SAM method USED-FOR neural networks. LSTM HYPONYM-OF neural networks. DenseNet HYPONYM-OF neural networks. vanilla CNN HYPONYM-OF neural networks. ResNeXt HYPONYM-OF neural networks. WideResNet HYPONYM-OF neural networks. ResNets HYPONYM-OF neural networks. image classification and language model EVALUATE-FOR method. Task are scientific computing, and machine learning problems. Metric is complexity bound. ",This paper studies the convergence of Anderson mixing (AM) for nonconvex stochastic optimization problems with stationary points. The authors consider the case where the stationary points are damped and adaptive regularization is used to improve the convergence rate. They show that the proposed method achieves almost sure convergence and the worst-case iteration complexity. They also provide a variance reduction technique to reduce the variance of the mixing strategy. ,This paper studies the convergence of Anderson mixing (AM) for nonconvex stochastic optimization problems with stationary points. The authors consider the case where the stationary points are damped and adaptive regularization is used to improve the convergence rate. They show that the proposed method achieves almost sure convergence and the worst-case iteration complexity. They also provide a variance reduction technique to reduce the variance of the mixing strategy. 
10326,SP:fe9c80cc5615705ef844d59b56413779c8d54a06,"posterior distribution USED-FOR linear inverse problem. SNIPS HYPONYM-OF stochastic algorithm. Langevin dynamics CONJUNCTION Newton ’s method. Newton ’s method CONJUNCTION Langevin dynamics. Newton ’s method USED-FOR solution. Langevin dynamics USED-FOR solution. singular value decomposition ( SVD ) USED-FOR degradation operator. singular value decomposition ( SVD ) PART-OF posterior score function. paradigm USED-FOR image deblurring. image deblurring CONJUNCTION super - resolution. super - resolution CONJUNCTION image deblurring. super - resolution CONJUNCTION compressive sensing. compressive sensing CONJUNCTION super - resolution. paradigm USED-FOR super - resolution. paradigm USED-FOR compressive sensing. OtherScientificTerm are additive white Gaussian noise, and noisy observation. Generic are approach, and algorithm. Method is iterative algorithm. Task is inverse problem. ",This paper studies the inverse problem with additive white Gaussian noise. The authors propose a stochastic algorithm for solving the linear inverse problem. The proposed algorithm is based on the Langevin dynamics and the singular value decomposition (SVD) operator. The paper shows that the proposed algorithm outperforms existing methods on a number of tasks. ,This paper studies the inverse problem with additive white Gaussian noise. The authors propose a stochastic algorithm for solving the linear inverse problem. The proposed algorithm is based on the Langevin dynamics and the singular value decomposition (SVD) operator. The paper shows that the proposed algorithm outperforms existing methods on a number of tasks. 
10351,SP:b04caddcb2dc9e9b365a76fdbf3d3eb4efcdffd9,"Instagram HYPONYM-OF social media. techniques USED-FOR illicit drug trades. meta - learning technique USED-FOR MetaHG. multimodal content CONJUNCTION relational structured information. relational structured information CONJUNCTION multimodal content. holistic framework USED-FOR illicit drug traffickers. MetaHG USED-FOR illicit drug trafficker detection. relational structured information USED-FOR illicit drug trafficker detection. MetaHG USED-FOR multimodal content. MetaHG USED-FOR relational structured information. MetaHG USED-FOR illicit drug traffickers. social media FEATURE-OF relational structured information. MetaHG HYPONYM-OF holistic framework. social media USED-FOR illicit drug traffickers. Instagram HYPONYM-OF social media. heterogeneous graph ( HG ) USED-FOR MetaHG. relation - based graph convolutional neural network USED-FOR node ( i.e., user ) representations. graph structure refinement USED-FOR sparse connection among entities. graph structure refinement USED-FOR node representation learning. sparse connection among entities PART-OF HG. graph structure refinement USED-FOR HG. HG USED-FOR relation - based graph convolutional neural network. HG USED-FOR node ( i.e., user ) representations. meta - learning algorithm USED-FOR model optimization. self - supervised module CONJUNCTION knowledge distillation module. knowledge distillation module CONJUNCTION self - supervised module. unlabeled data USED-FOR model. knowledge distillation module USED-FOR model. knowledge distillation module USED-FOR unlabeled data. self - supervised module USED-FOR model. self - supervised module USED-FOR unlabeled data. MetaHG COMPARE state - of - the - art methods. state - of - the - art methods COMPARE MetaHG. real - world data EVALUATE-FOR MetaHG. real - world data EVALUATE-FOR state - of - the - art methods. Instagram FEATURE-OF real - world data. Task are crime of drug trafficking, online drug trafficking, and model training. Material is post content. ","This paper proposes MetaHG, a meta-learning framework for detecting illicit drug traffickers on Instagram. The proposed framework is based on the heterogeneous graph (HG) framework, where each node in the graph is represented by a relation-based graph convolutional neural network (RBCN) and each user is represented as a graph. The authors propose to use a self-supervised module and a knowledge distillation module to train the model on unlabeled data. The model is evaluated on real-world Instagram data.","This paper proposes MetaHG, a meta-learning framework for detecting illicit drug traffickers on Instagram. The proposed framework is based on the heterogeneous graph (HG) framework, where each node in the graph is represented by a relation-based graph convolutional neural network (RBCN) and each user is represented as a graph. The authors propose to use a self-supervised module and a knowledge distillation module to train the model on unlabeled data. The model is evaluated on real-world Instagram data."
10376,SP:242da1384f48260d58a0e7949438611c05079197,"ReLU activations CONJUNCTION architecture. architecture CONJUNCTION ReLU activations. architecture USED-FOR neural network. ReLU activations USED-FOR neural network. neural network USED-FOR class of functions. polyhedral theory CONJUNCTION tropical geometry. tropical geometry CONJUNCTION polyhedral theory. mixed - integer optimization CONJUNCTION polyhedral theory. polyhedral theory CONJUNCTION mixed - integer optimization. hidden layer USED-FOR learning tasks. techniques USED-FOR mathematical counterbalance. mathematical counterbalance USED-FOR universal approximation theorems. polyhedral theory USED-FOR techniques. mixed - integer optimization USED-FOR techniques. upper bounds FEATURE-OF neural networks. neural networks USED-FOR neural hypothesis classes. OtherScientificTerm are layers, and neural network literature. Task is algorithmic and statistical aspects. ",This paper studies the problem of learning a class of functions that can be approximated by a neural network. The authors consider the problem in the context of polyhedral theory and mixed-integer optimization. The main contribution of the paper is to provide a theoretical analysis of the problem and to provide upper bounds on the universal approximation theorems. The paper is well-written and easy to follow. ,This paper studies the problem of learning a class of functions that can be approximated by a neural network. The authors consider the problem in the context of polyhedral theory and mixed-integer optimization. The main contribution of the paper is to provide a theoretical analysis of the problem and to provide upper bounds on the universal approximation theorems. The paper is well-written and easy to follow. 
10401,SP:8d5741aedf3125e0e790a58ec3ce81a4e2ea4dcb,"worst - case training principle USED-FOR maximal adversarial loss. min - max optimization USED-FOR AT. min - max optimization PART-OF adversarial context. framework USED-FOR adversarial attacks. min - max optimization USED-FOR adversarial attacks. framework USED-FOR min - max optimization. probability simplex FEATURE-OF domain weights. unified framework USED-FOR attack generation problems. unified framework USED-FOR crafting attacks. crafting attacks USED-FOR data transformations. crafting attacks HYPONYM-OF attack generation problems. attacking model ensembles HYPONYM-OF attack generation problems. approach COMPARE heuristic strategies. heuristic strategies COMPARE approach. robustness EVALUATE-FOR defense methods. heuristic strategies COMPARE defense methods. defense methods COMPARE heuristic strategies. approach COMPARE defense methods. defense methods COMPARE approach. robustness EVALUATE-FOR heuristic strategies. robustness EVALUATE-FOR approach. self - adjusted domain weights USED-FOR difficulty level of attack. min - max framework USED-FOR self - adjusted domain weights. Method is adversarial training ( AT ). Task are adversarial robustness, and min - max problem. OtherScientificTerm are risk sources, and universal perturbation. Metric is worst - case attack loss. ",This paper proposes a framework for adversarial training (AT) based on the worst-case training principle. The framework is based on minimizing the maximal adversarial loss in the adversarial context. The authors propose to use self-adjusting domain weights to reduce the difficulty level of attack. They show that the proposed method outperforms existing heuristic strategies in terms of adversarial robustness. ,This paper proposes a framework for adversarial training (AT) based on the worst-case training principle. The framework is based on minimizing the maximal adversarial loss in the adversarial context. The authors propose to use self-adjusting domain weights to reduce the difficulty level of attack. They show that the proposed method outperforms existing heuristic strategies in terms of adversarial robustness. 
10426,SP:9dcb74bfdbc4aa1e27f5d2adb6d2abf475e9324d,"sparse PCA CONJUNCTION tensor PCA. tensor PCA CONJUNCTION sparse PCA. model USED-FOR sparse PCA. model USED-FOR tensor PCA. Wigner form FEATURE-OF sparse PCA. polynomial - time algorithm CONJUNCTION exponential - time exhaustive search algorithm. exponential - time exhaustive search algorithm CONJUNCTION polynomial - time algorithm. polynomial - time algorithm USED-FOR algorithms. exponential - time exhaustive search algorithm USED-FOR algorithms. algorithms USED-FOR sparse vector. signal - tonoise ratio λ FEATURE-OF sparse vector. algorithms USED-FOR sparse vectors. algorithms COMPARE algorithms. algorithms COMPARE algorithms. λ FEATURE-OF sparse vectors. algorithms USED-FOR sparse PCA. signal - to - noise ratio CONJUNCTION running time. running time CONJUNCTION signal - to - noise ratio. sparse PCA CONJUNCTION tensor PCA. tensor PCA CONJUNCTION sparse PCA. lower bound USED-FOR lower bounds. lower bounds USED-FOR sparse PCA. lower bound USED-FOR sparse PCA. lower bounds USED-FOR tensor PCA. Task is sparse tensor principal component analysis. OtherScientificTerm are i.i.d. Gaussian entries, k - sparse unit vector, k - sparse signals, and sparsity k. Generic is matrix settings. Metric is low - degree likelihood ratio. ","This paper studies the problem of sparse tensor principal component analysis (Sparse PCA) and tensor PCA in the Wigner form. The authors consider the case where the number of sparse entries in the matrix is k-sparse, and the problem is formulated as a low-degree likelihood ratio problem. The paper provides a lower bound on the signal-to-noise ratio of a sparse vector, which is a measure of the likelihood ratio of the sparse vector. The lower bound is based on the lower bound of the log-likelihood of the matrix, which can be expressed as a function of the sparsity k. ","This paper studies the problem of sparse tensor principal component analysis (Sparse PCA) and tensor PCA in the Wigner form. The authors consider the case where the number of sparse entries in the matrix is k-sparse, and the problem is formulated as a low-degree likelihood ratio problem. The paper provides a lower bound on the signal-to-noise ratio of a sparse vector, which is a measure of the likelihood ratio of the sparse vector. The lower bound is based on the lower bound of the log-likelihood of the matrix, which can be expressed as a function of the sparsity k. "
10451,SP:660137b0f84e47c06dc2bee1c95b299c67e4cb67,"Multilayer - perceptrons ( MLP ) USED-FOR learning functions of high - frequencies. spatially adaptive progressive encoding ( SAPE ) scheme USED-FOR MLP networks. spatially adaptive progressive encoding ( SAPE ) scheme USED-FOR them. feedback loop USED-FOR neural optimization process. feedback loop USED-FOR progressive exposure of frequencies. regression of low dimensional signals CONJUNCTION images. images CONJUNCTION regression of low dimensional signals. representation learning of occupancy networks CONJUNCTION geometric task of mesh transfer. geometric task of mesh transfer CONJUNCTION representation learning of occupancy networks. regression of low dimensional signals CONJUNCTION representation learning of occupancy networks. representation learning of occupancy networks CONJUNCTION regression of low dimensional signals. SAPE USED-FOR applications. geometric task of mesh transfer HYPONYM-OF applications. regression of low dimensional signals HYPONYM-OF applications. images HYPONYM-OF applications. representation learning of occupancy networks HYPONYM-OF applications. OtherScientificTerm are wide frequency bands, and 3D shapes. Metric is training stability. Method is domain specific preprocessing. ","This paper proposes a new method for training multilayer-perceptron (MLP) networks for learning functions of high-frequency. The proposed method is based on the idea of spatially adaptive progressive encoding (SAPE) scheme, which is used to learn a progressive exposure of frequencies for each layer of the MLP network. The authors show that the proposed method can be applied to a wide range of applications, including representation learning of occupancy networks, regression of low dimensional signals, and geometric task of mesh transfer.","This paper proposes a new method for training multilayer-perceptron (MLP) networks for learning functions of high-frequency. The proposed method is based on the idea of spatially adaptive progressive encoding (SAPE) scheme, which is used to learn a progressive exposure of frequencies for each layer of the MLP network. The authors show that the proposed method can be applied to a wide range of applications, including representation learning of occupancy networks, regression of low dimensional signals, and geometric task of mesh transfer."
10476,SP:b03063fa82d76db341076e5f282176f4c007a202,"probability simplex constraints FEATURE-OF constrained saddle - point optimization problem. constrained saddle - point optimization problem USED-FOR equilibrium of competitive games. methods USED-FOR constrained settings. unconstrained setting FEATURE-OF extragradient methods. entropy regularization USED-FOR single - agent reinforcement learning and game theory. entropy regularization FEATURE-OF zero - sum two - player matrix games. algorithms USED-FOR approximate Nash equilibrium. approximate Nash equilibrium FEATURE-OF unregularized matrix game. knob of entropy regularization USED-FOR algorithms. policy extragradient algorithms USED-FOR entropy - regularized zero - sum Markov games. methods USED-FOR policy extragradient algorithms. linear rate FEATURE-OF policy extragradient algorithms. entropy regularization USED-FOR accelerating convergence. logarithm factors FEATURE-OF state and action spaces. OtherScientificTerm are multiplicative updates, objective function, sublinear rate, and Nash equilibrium. Method is symmetric and multiplicative updates. Metric is convergence rates. ","This paper studies the problem of maximizing the convergence rate of policy extragradient algorithms for zero-sum two-player matrix games under constrained saddle point constraints. The authors consider the case where the objective function of the game is a convex function and the action space is an unregularized matrix game. In this setting, the authors show that the rate of convergence converges to a sublinear rate, which is the case in the unconstrained setting. They also show that this rate converges faster than the linear rate in the constrained setting. ","This paper studies the problem of maximizing the convergence rate of policy extragradient algorithms for zero-sum two-player matrix games under constrained saddle point constraints. The authors consider the case where the objective function of the game is a convex function and the action space is an unregularized matrix game. In this setting, the authors show that the rate of convergence converges to a sublinear rate, which is the case in the unconstrained setting. They also show that this rate converges faster than the linear rate in the constrained setting. "
10501,SP:862223b8bd4c275f96c7e41c92daaa2ca2906194,"remote cooperation CONJUNCTION online education. online education CONJUNCTION remote cooperation. screen sharing CONJUNCTION remote cooperation. remote cooperation CONJUNCTION screen sharing. HR display USED-FOR super - resolution ( SR ). image SR methods USED-FOR natural images. image SR methods USED-FOR SCIs. pixel values USED-FOR continuous SR. implicit transformer USED-FOR image features. image features USED-FOR pixel values. LR and HR SCI pairs USED-FOR SCI1 K and SCI1K - compression datasets. continuous and discrete SR methods USED-FOR compressed and uncompressed SCIs. ITSRN COMPARE continuous and discrete SR methods. continuous and discrete SR methods COMPARE ITSRN. ITSRN USED-FOR compressed and uncompressed SCIs. Material is screen contents. OtherScientificTerm are limited terminal bandwidth, high - resolution ( HR ) screen contents, and image characteristics. Task is SCI browsing. Method are SCISR, and implicit position encoding scheme. ","This paper proposes a new method for super-resolution (SR) browsing based on implicit position encoding (ITSRN). The proposed method is based on the idea of implicit transformer, which is a transformer-based method for image SR. The authors show that the proposed method outperforms existing SR methods on the SCI-1K and SCI1K-compression datasets.","This paper proposes a new method for super-resolution (SR) browsing based on implicit position encoding (ITSRN). The proposed method is based on the idea of implicit transformer, which is a transformer-based method for image SR. The authors show that the proposed method outperforms existing SR methods on the SCI-1K and SCI1K-compression datasets."
10526,SP:3751625929b707ced417c3eb10064e4917866048,"probabilistic models USED-FOR causality. sumproduct networks ( SPNs ) USED-FOR learning interventional distributions. gate functions USED-FOR sumproduct networks ( SPNs ). neural networks HYPONYM-OF gate functions. gate function USED-FOR SPN. structural causal model USED-FOR interventional SPNs. personal health FEATURE-OF structural causal model. generative and causal modelling USED-FOR methods. Generic is so. Task is intractability of inference. Method is causal models. OtherScientificTerm are interventional distributions, arbitrarily intervened causal graph, and Pearl ’s do - operator. ",This paper proposes a structural causal model for learning interventional distributions. The model is based on sumproduct networks (SPNs) and is able to learn interventional causal graphs. The authors show that the proposed model can be used to learn the interventional distribution in a generative and causal modelling setting. The proposed model is evaluated on a number of real-world datasets. ,This paper proposes a structural causal model for learning interventional distributions. The model is based on sumproduct networks (SPNs) and is able to learn interventional causal graphs. The authors show that the proposed model can be used to learn the interventional distribution in a generative and causal modelling setting. The proposed model is evaluated on a number of real-world datasets. 
10551,SP:c857ff674ca05c1d949337cb885f056b82d981d6,"Factor analysis methods USED-FOR low dimensional, ideally interpretable representations. Factor analysis methods USED-FOR neuroimaging. Factor analysis methods USED-FOR high dimensional imaging data. high dimensional imaging data USED-FOR low dimensional, ideally interpretable representations. deep Markov factor analysis ( DMFA ) HYPONYM-OF generative model. Markov property USED-FOR low dimensional temporal embeddings. Markov property CONJUNCTION spatial inductive assumptions. spatial inductive assumptions CONJUNCTION Markov property. temporal dynamics FEATURE-OF functional magnetic resonance imaging ( fMRI ) data. Markov property USED-FOR generative model. discrete latent USED-FOR DMFA. DMFA USED-FOR fMRI data. low dimensional temporal embedding USED-FOR DMFA. DMFA USED-FOR interpretable clusters. DMFA USED-FOR nonlinear temporal dependencies. synthetic and real fMRI data EVALUATE-FOR DMFA. nonlinear temporal dependencies FEATURE-OF high dimensional imaging data. Generic is methods. OtherScientificTerm are nonlinear and complex temporal dynamics of neural processes, high spatial dimensionality, and subject and cognitive state variability. Material is imaging data. Method is neural networks. Task are fMRI - driven neuroscientific hypotheses, and capturing nonlinear temporal dependencies. ",This paper proposes a deep Markov factor analysis (DMFA) method for understanding the temporal dynamics of functional magnetic resonance imaging (fMRI) data. DMFA is a generative model that uses discrete latent variables to capture non-linear temporal dependencies in fMRI data. The authors show that DMFA can capture the temporal dependencies of high dimensional imaging data and can be used to generate interpretable clusters for interpretable neural networks. The proposed method is evaluated on both synthetic and real-world fMRI datasets.,This paper proposes a deep Markov factor analysis (DMFA) method for understanding the temporal dynamics of functional magnetic resonance imaging (fMRI) data. DMFA is a generative model that uses discrete latent variables to capture non-linear temporal dependencies in fMRI data. The authors show that DMFA can capture the temporal dependencies of high dimensional imaging data and can be used to generate interpretable clusters for interpretable neural networks. The proposed method is evaluated on both synthetic and real-world fMRI datasets.
10576,SP:855dcaa42868a29a14619d63221169495ed5dd54,"spheres CONJUNCTION tori. tori CONJUNCTION spheres. generative models USED-FOR complex geometries. tori CONJUNCTION implicit surfaces. implicit surfaces CONJUNCTION tori. manifolds USED-FOR complex geometries. implicit surfaces HYPONYM-OF manifolds. spheres HYPONYM-OF complex geometries. spheres HYPONYM-OF manifolds. tori HYPONYM-OF complex geometries. tori HYPONYM-OF manifolds. Moser Flow ( MF ) HYPONYM-OF generative models. continuous normalizing flows ( CNF ) FEATURE-OF generative models. MF USED-FOR CNF. source ( prior ) density CONJUNCTION divergence. divergence CONJUNCTION source ( prior ) density. CNF methods COMPARE model ( learned ) density. model ( learned ) density COMPARE CNF methods. divergence PART-OF neural network ( NN ). source ( prior ) density USED-FOR model ( learned ) density. divergence HYPONYM-OF local, linear differential operator. CNFs COMPARE MF. MF COMPARE CNFs. divergence USED-FOR model density. NN USED-FOR model density. MF USED-FOR universal density approximator. flow models USED-FOR sampling from general curved surfaces. training complexity EVALUATE-FOR CNFs. sample quality EVALUATE-FOR CNFs. sample quality CONJUNCTION training complexity. training complexity CONJUNCTION sample quality. synthetic geometries CONJUNCTION real - world benchmarks. real - world benchmarks CONJUNCTION synthetic geometries. density estimation CONJUNCTION sample quality. sample quality CONJUNCTION density estimation. flow models COMPARE CNFs. CNFs COMPARE flow models. density estimation EVALUATE-FOR CNFs. earth and climate sciences FEATURE-OF synthetic geometries. earth and climate sciences FEATURE-OF real - world benchmarks. earth and climate sciences EVALUATE-FOR CNFs. real - world benchmarks EVALUATE-FOR CNFs. synthetic geometries EVALUATE-FOR CNFs. Method are Euclidean ) generative models, and ODE solver. OtherScientificTerm are change - of -","This paper proposes a new generative model for continuous normalizing flows (CNF) based on the Moser flow (MF) framework. The main idea is to use the divergence between the source and prior density of the model (learned) density and the source (prior) density of a neural network (NN) to estimate the model density. The authors show that the proposed method outperforms existing CNF methods in terms of sample quality, training complexity, and density estimation. They also show that their method can be used for sampling from general curved surfaces.","This paper proposes a new generative model for continuous normalizing flows (CNF) based on the Moser flow (MF) framework. The main idea is to use the divergence between the source and prior density of the model (learned) density and the source (prior) density of a neural network (NN) to estimate the model density. The authors show that the proposed method outperforms existing CNF methods in terms of sample quality, training complexity, and density estimation. They also show that their method can be used for sampling from general curved surfaces."
10601,SP:545554de09d17df77d6169a5cc8f36022ecb355c,"framework USED-FOR unsupervised representation learning. Independent component analysis USED-FOR unsupervised representation learning. observed variables PART-OF generative process. independent causal mechanisms USED-FOR causality. assumptions USED-FOR independent causal mechanisms. approach USED-FOR nonidentifiability issues. nonidentifiability issues FEATURE-OF nonlinear blind source separation. OtherScientificTerm are latent code, mixing, statistical independence, Identifiability, and mixing process. Generic is model. Method is independent mechanism analysis. ","This paper proposes a framework for unsupervised representation learning based on independent component analysis. The framework is based on the idea of independent causal mechanisms (ICM) that can be used to identify the causal mechanisms in a generative process. In particular, the authors consider the case of non-linear blind source separation, where the source and target variables are not independent. The authors show that the proposed method is able to identify independent causal mechanism in the latent code, which is an important problem in the context of representation learning. ","This paper proposes a framework for unsupervised representation learning based on independent component analysis. The framework is based on the idea of independent causal mechanisms (ICM) that can be used to identify the causal mechanisms in a generative process. In particular, the authors consider the case of non-linear blind source separation, where the source and target variables are not independent. The authors show that the proposed method is able to identify independent causal mechanism in the latent code, which is an important problem in the context of representation learning. "
10626,SP:7df49c554d6c9fca370f049279ef7324b6f79de9,"Annealed Importance Sampling ( AIS ) HYPONYM-OF method. Hamiltonian MCMC USED-FOR Annealed Importance Sampling ( AIS ). non - differentiable transition kernels USED-FOR it. AIS - like procedure USED-FOR framework. Uncorrected Hamiltonian MCMC USED-FOR AIS - like procedure. Uncorrected Hamiltonian Annealing HYPONYM-OF AIS - like procedure. method COMPARE approaches. approaches COMPARE method. method USED-FOR tight and differentiable lower bounds. method COMPARE approaches. approaches COMPARE method. OtherScientificTerm are unnormalized target distribution, tight lower bound, and reparameterization gradients. ",This paper proposes a new method for Annealed Importance Sampling (AIS) based on Hamiltonian MCMC. The proposed method is based on the un-corrected Hamiltonian Annealing (UHMC) framework. The authors prove tight and differentiable lower bounds for the AIS-like procedure. They also show that the UHMC framework can be extended to the case where the target distribution is unnormalized. ,This paper proposes a new method for Annealed Importance Sampling (AIS) based on Hamiltonian MCMC. The proposed method is based on the un-corrected Hamiltonian Annealing (UHMC) framework. The authors prove tight and differentiable lower bounds for the AIS-like procedure. They also show that the UHMC framework can be extended to the case where the target distribution is unnormalized. 
10651,SP:b0bf070e8d7eefdfc45f236e9ecb9edfb4816e0a,"deep neural networks USED-FOR safetycritical applications. training algorithms USED-FOR neural network. training algorithms USED-FOR robustness. Certified robustness FEATURE-OF deep neural networks. robustness EVALUATE-FOR neural network. Lipschitz constant FEATURE-OF global bound. global bound USED-FOR training algorithms. non - convexity FEATURE-OF network. natural and certified accuracy EVALUATE-FOR tighter Lipschitz bound. activation functions CONJUNCTION weight matrices. weight matrices CONJUNCTION activation functions. induced norm FEATURE-OF weight matrix. global Lipschitz constant FEATURE-OF neural network. method USED-FOR plug - in module. plug - in module USED-FOR Lipschitz bound. method USED-FOR Lipschitz bound. Lipschitz bound FEATURE-OF certifiable training algorithms. upper threshold CONJUNCTION sparsity loss. sparsity loss CONJUNCTION upper threshold. ReLU CONJUNCTION MaxMin. MaxMin CONJUNCTION ReLU. sparsity loss USED-FOR network. network USED-FOR local Lipschitz bound. upper threshold USED-FOR activation functions. MaxMin HYPONYM-OF activation functions. ReLU HYPONYM-OF activation functions. method COMPARE methods. methods COMPARE method. network architectures USED-FOR method. TinyImageNet datasets EVALUATE-FOR methods. clean and certified accuracy EVALUATE-FOR methods. MNIST EVALUATE-FOR methods. TinyImageNet datasets EVALUATE-FOR method. clean and certified accuracy EVALUATE-FOR method. MNIST EVALUATE-FOR method. Generic are bound, and it. Metric is natural accuracy. OtherScientificTerm are local Lipschitz upper bound, and activation function. ",This paper studies the Lipschitz bound for certifiable training of deep neural networks. The authors propose a plug-in module that can be applied to any neural network to improve the certified accuracy of the network. The proposed method is based on the observation that the global bound of the neural network is non-convex and the local bound is a function of the weight matrices and activation functions. The paper shows that the proposed method can be used to improve both the certified and natural accuracy of a neural network. ,This paper studies the Lipschitz bound for certifiable training of deep neural networks. The authors propose a plug-in module that can be applied to any neural network to improve the certified accuracy of the network. The proposed method is based on the observation that the global bound of the neural network is non-convex and the local bound is a function of the weight matrices and activation functions. The paper shows that the proposed method can be used to improve both the certified and natural accuracy of a neural network. 
10676,SP:f6314bfd897cb996de2eaabf0d3037f41da467f3,"scalable methods USED-FOR conformal Bayesian predictive intervals. Bayesian posterior predictive distributions USED-FOR subjective beliefs. finite sample frequentist guarantees FEATURE-OF predictive confidence intervals. conformal inference USED-FOR predictive confidence intervals. conformal inference USED-FOR finite sample frequentist guarantees. add - one - in ’ importance sampling USED-FOR conformal Bayesian predictive intervals. re - weighted posterior samples of model parameters USED-FOR conformal Bayesian predictive intervals. refitting of models CONJUNCTION data - splitting. data - splitting CONJUNCTION refitting of models. approach COMPARE conformal methods. conformal methods COMPARE approach. refitting of models USED-FOR conformal methods. computational efficiency EVALUATE-FOR conformal methods. data - splitting USED-FOR conformal methods. hierarchical models HYPONYM-OF partially exchangeable settings. OtherScientificTerm are finite sample calibration guarantees, predictors, predictive intervals, and model fidelity. Method is Bayesian prediction. Metric is empirical coverage. Generic is examples. ",This paper studies the problem of learning conformal Bayesian predictive intervals for Bayesian Bayesian models. The authors propose a new approach to learn conformal predictive intervals by re-weighting the posterior samples of model parameters. They show that the proposed approach is computationally efficient compared to existing conformal methods. ,This paper studies the problem of learning conformal Bayesian predictive intervals for Bayesian Bayesian models. The authors propose a new approach to learn conformal predictive intervals by re-weighting the posterior samples of model parameters. They show that the proposed approach is computationally efficient compared to existing conformal methods. 
10701,SP:1e86c162b8e8d652a0590b66aa5f7c363955cc5b,"denoisers USED-FOR general inverse problems. priors FEATURE-OF explicit likelihood functions. regularization - by - denoising ( RED ) HYPONYM-OF frameworks. RED USED-FOR imaging tasks. RED CONJUNCTION PnP. PnP CONJUNCTION RED. PnP USED-FOR imaging tasks. convolutional neural networks ( CNNs ) HYPONYM-OF denoisers. maximum a posteriori ( MAP ) CONJUNCTION minimum mean square error ( MMSE ) estimators. minimum mean square error ( MMSE ) estimators CONJUNCTION maximum a posteriori ( MAP ). convergence FEATURE-OF RED and PnP methods. CNN denoisers USED-FOR maximum a posteriori ( MAP ). Lipschitz constant FEATURE-OF CNN denoisers. denoisers PART-OF RED and PnP schemes. denoisers USED-FOR MAP and MMSE estimators interpretation. symmetric Jacobians FEATURE-OF denoisers. backtracking step size USED-FOR RED and PnP schemes. backtracking step size USED-FOR denoisers. denoisers USED-FOR inversion method. method COMPARE RED and PnP methods. RED and PnP methods COMPARE method. imaging experiments EVALUATE-FOR RED and PnP methods. imaging experiments EVALUATE-FOR method. Method are denoising algorithms, MAP or MMSE estimators, inverse algorithms, and image denoisers. Generic is they. OtherScientificTerm are potentials, and objective function. ",This paper studies the problem of denoising in inverse problems. The authors propose a new denoiser that is based on the Lipschitz constant of CNN denoisers. The main contribution of this paper is to show that denoiser can be viewed as a regularization-by-denoising (RED) framework. The paper also shows that the proposed method converges to the maximum a posteriori (MAP) and minimum mean square error (MMSE) estimators. ,This paper studies the problem of denoising in inverse problems. The authors propose a new denoiser that is based on the Lipschitz constant of CNN denoisers. The main contribution of this paper is to show that denoiser can be viewed as a regularization-by-denoising (RED) framework. The paper also shows that the proposed method converges to the maximum a posteriori (MAP) and minimum mean square error (MMSE) estimators. 
10726,SP:da92e936f88b3842ca82c2914413b129ca35890f,rhythmic features FEATURE-OF activities. rhythmic features FEATURE-OF musical soundtrack. system USED-FOR soundtrack. them USED-FOR rhythmic sounds. models USED-FOR rhythmic sounds. human movements USED-FOR RhythmicNet. skeleton keypoints USED-FOR RhythmicNet. rhythm CONJUNCTION melody. melody CONJUNCTION rhythm. natural process of music improvisation USED-FOR RhythmicNet. RhythmicNet USED-FOR rhythm. RhythmicNet USED-FOR style pattern. body keypoints USED-FOR rhythm. body keypoints USED-FOR RhythmicNet. body keypoints USED-FOR style pattern. U - net based model USED-FOR velocity. U - net based model USED-FOR it. transformerbased model USED-FOR it. inherit sound association FEATURE-OF body movements. body movements PART-OF large scale video datasets. dance HYPONYM-OF body movements. dance HYPONYM-OF inherit sound association. large scale video datasets EVALUATE-FOR RhythmicNet. Task is video. OtherScientificTerm is free body movements. Generic is method. ,"This paper proposes a method to learn a system to generate music based on human body movements. The system is based on a transformer-based model and is able to generate a set of skeleton keypoints for each body movement. The keypoints are then used to learn the style pattern of the body movements, and the system is trained to predict the velocity of each skeleton keypoint. The method is evaluated on a number of video datasets and shows that it can generate music that is similar to human movements. ","This paper proposes a method to learn a system to generate music based on human body movements. The system is based on a transformer-based model and is able to generate a set of skeleton keypoints for each body movement. The keypoints are then used to learn the style pattern of the body movements, and the system is trained to predict the velocity of each skeleton keypoint. The method is evaluated on a number of video datasets and shows that it can generate music that is similar to human movements. "
10751,SP:0f7ff312a242a553dc9ecf35b421e58fb2d50a26,"approaches USED-FOR offline reinforcement learning ( RL ). iterative actor - critic approach USED-FOR approaches. off - policy evaluation PART-OF iterative actor - critic approach. on - policy Q estimate USED-FOR behavior policy. on - policy Q estimate USED-FOR constrained / regularized policy improvement. onestep algorithm COMPARE iterative algorithms. iterative algorithms COMPARE onestep algorithm. D4RL benchmark EVALUATE-FOR iterative algorithms. D4RL benchmark EVALUATE-FOR onestep algorithm. one - step baseline COMPARE iterative algorithms. iterative algorithms COMPARE one - step baseline. OtherScientificTerm is hyperparameters. Method are iterative approaches, and one - step algorithm. Task is repeated optimization of policies. Generic is estimates. ",This paper proposes an iterative actor-critic approach for offline reinforcement learning (RL). The main idea is to use the on-policy Q estimate of the actor to guide the off-policy evaluation of the behavior policy. The proposed method is evaluated on the D4RL benchmark and shows that it outperforms the one-step baseline.,This paper proposes an iterative actor-critic approach for offline reinforcement learning (RL). The main idea is to use the on-policy Q estimate of the actor to guide the off-policy evaluation of the behavior policy. The proposed method is evaluated on the D4RL benchmark and shows that it outperforms the one-step baseline.
10776,SP:0346eba4f587acbe3492d039066f1737360fd870,"statistics CONJUNCTION machine learning. machine learning CONJUNCTION statistics. tasks PART-OF machine learning. Low - rank and nonsmooth matrix optimization problems USED-FOR tasks. tasks PART-OF statistics. Low - rank and nonsmooth matrix optimization problems USED-FOR statistics. methods USED-FOR smooth low - rank optimization problems. convex relaxations USED-FOR problems. extragradient method USED-FOR optimal solution. maximum of smooth functions USED-FOR nonsmooth objective. initializations USED-FOR extragradient method. full - rank SVDs CONJUNCTION SVDs of rank. SVDs of rank CONJUNCTION full - rank SVDs. OtherScientificTerm are high - rank matrices, high - rank SVDs, natural generalized strict complementarity condition, low - rank SVDs, and SVDs. Task are nonsmooth problems, and nonsmooth low - rank matrix recovery tasks. Generic is method. ",This paper studies the problem of solving nonsmooth low-rank matrix optimization problems. The authors show that the optimal solution of the problem is the maximum of the smooth functions of the nonssmooth objective under the natural generalized strict complementarity condition. They then propose an extragradient method to solve the problem. The method is based on convex relaxations and is shown to be able to find a solution that is close to the optimal solutions of the high-rank SVDs of rank and SVD of rank of rank. ,This paper studies the problem of solving nonsmooth low-rank matrix optimization problems. The authors show that the optimal solution of the problem is the maximum of the smooth functions of the nonssmooth objective under the natural generalized strict complementarity condition. They then propose an extragradient method to solve the problem. The method is based on convex relaxations and is shown to be able to find a solution that is close to the optimal solutions of the high-rank SVDs of rank and SVD of rank of rank. 
10801,SP:d39f1d77d9919f897ccf82958b71be8798523923,graphs CONJUNCTION images. images CONJUNCTION graphs. images CONJUNCTION texts. texts CONJUNCTION images. texts HYPONYM-OF structured treatments. graphs HYPONYM-OF structured treatments. images HYPONYM-OF structured treatments. arbitrary models USED-FOR learning. generalized Robinson decomposition USED-FOR causal estimand. mild assumptions FEATURE-OF quasi - oracle convergence guarantee. approach COMPARE prior work. prior work COMPARE approach. small - world and molecular graphs EVALUATE-FOR approach. prior work USED-FOR CATE estimation. CATE estimation EVALUATE-FOR approach. OtherScientificTerm is regularization bias. ,This paper studies the problem of estimating the causal effect of a given treatment on a set of data points. The authors propose a generalized Robinson decomposition of the causal estimand (CATE) and derive a quasi-oracle convergence guarantee for the CATE estimand under mild assumptions. The proposed method is evaluated on small-world and molecular graphs.,This paper studies the problem of estimating the causal effect of a given treatment on a set of data points. The authors propose a generalized Robinson decomposition of the causal estimand (CATE) and derive a quasi-oracle convergence guarantee for the CATE estimand under mild assumptions. The proposed method is evaluated on small-world and molecular graphs.
10826,SP:eeb42a1e48857f976a647eb8d86d25c9012962d5,"qualitative assumptions CONJUNCTION distributions. distributions CONJUNCTION qualitative assumptions. system USED-FOR distributions. causal graph HYPONYM-OF qualitative assumptions. probability axioms CONJUNCTION do - calculus. do - calculus CONJUNCTION probability axioms. do - calculus CONJUNCTION c - factorization. c - factorization CONJUNCTION do - calculus. probability axioms USED-FOR graphical criteria. graphical criteria USED-FOR identification algorithms. matrix equations USED-FOR proxy variables. graphical criteria CONJUNCTION matrix equations. matrix equations CONJUNCTION graphical criteria. graphical criteria USED-FOR causal identification algorithm. matrix equations USED-FOR causal identification algorithm. graphically - driven formulae CONJUNCTION matrix multiplications. matrix multiplications CONJUNCTION graphically - driven formulae. enriched matrix - based criteria PART-OF graphical identification approach. marginal, conditional, and interventional distributions USED-FOR causal effect identification algorithm. Task is Causal effect identification. OtherScientificTerm are causal effect, proxy variable based identification conditions, and intermediary criteria. ","This paper proposes a new method for causal effect identification based on matrix equations and graphical criteria. The proposed method is based on the notion of proxy variables, which is an extension of the idea of matrix equation identification. The authors show that the proposed method can be applied to both marginal, conditional, and interventional distributions. The method is evaluated on a number of synthetic and real-world datasets. ","This paper proposes a new method for causal effect identification based on matrix equations and graphical criteria. The proposed method is based on the notion of proxy variables, which is an extension of the idea of matrix equation identification. The authors show that the proposed method can be applied to both marginal, conditional, and interventional distributions. The method is evaluated on a number of synthetic and real-world datasets. "
10851,SP:db15860d08418f6bc792c2ade2eade32840a12b8,"Unsupervised Environment Design ( UED ) HYPONYM-OF selfsupervised RL paradigm. random levels USED-FOR PLR. Dual Curriculum Design ( DCD ) HYPONYM-OF UED methods. PLR CONJUNCTION UED algorithm. UED algorithm CONJUNCTION PLR. UED algorithm CONJUNCTION PAIRED. PAIRED CONJUNCTION UED algorithm. PLR CONJUNCTION PAIRED. PAIRED CONJUNCTION PLR. PAIRED PART-OF DCD. PLR PART-OF DCD. UED algorithm PART-OF DCD. theory USED-FOR PLR. robustness guarantee FEATURE-OF Nash equilibria. theory USED-FOR PLR. Nash equilibria FEATURE-OF convergence. PLR⊥ USED-FOR PAIRED. PLR⊥ HYPONYM-OF method. Method are Deep reinforcement learning ( RL ) agents, Prioritized Level Replay ( PLR ), UED, and theoretical framework. OtherScientificTerm are environment and task configurations, diverse training environments, randomly - generated training levels, and theoretical guarantees. Generic is it. ","This paper studies the problem of unsupervised environment design (UED) in the context of deep reinforcement learning. The authors propose a new algorithm, Prioritized Level Replay (PLR), which is a variant of the Dual Curriculum Design (DCD) algorithm. The main contribution of the paper is a theoretical analysis of the convergence of PLR to the Nash equilibria of the UED algorithm and the dual curriculum design (DCD) algorithm, which is an extension of the previous work, PAIRED. The theoretical analysis shows that PLR converges to a Nash equilibrium in the presence of diverse training environments and diverse training levels. The paper also provides a robustness guarantee for PLR.","This paper studies the problem of unsupervised environment design (UED) in the context of deep reinforcement learning. The authors propose a new algorithm, Prioritized Level Replay (PLR), which is a variant of the Dual Curriculum Design (DCD) algorithm. The main contribution of the paper is a theoretical analysis of the convergence of PLR to the Nash equilibria of the UED algorithm and the dual curriculum design (DCD) algorithm, which is an extension of the previous work, PAIRED. The theoretical analysis shows that PLR converges to a Nash equilibrium in the presence of diverse training environments and diverse training levels. The paper also provides a robustness guarantee for PLR."
10876,SP:9ed528da4b67f22678303cfd975aafe678db6411,"( ε, δ)-differentially private algorithm USED-FOR multi - armed bandit ( MAB ) problem. shuffle model FEATURE-OF multi - armed bandit ( MAB ) problem. upper bound COMPARE regret. regret COMPARE upper bound. Metric are distribution - dependent regret, and distribution - independent regret. OtherScientificTerm is suboptimality gap. Method are centralized model, and local model. ",This paper studies the multi-armed bandit (MAB) problem with shuffle model. The authors show that the suboptimality gap of the shuffle model can be reduced to a lower bound of the regret of the shuffled model. They also show that this lower bound can be extended to the case of distribution-independent regret. ,This paper studies the multi-armed bandit (MAB) problem with shuffle model. The authors show that the suboptimality gap of the shuffle model can be reduced to a lower bound of the regret of the shuffled model. They also show that this lower bound can be extended to the case of distribution-independent regret. 
10901,SP:de2523a5fdebda3573f1063447a7818bf3ed6333,"probabilistic forecasts USED-FOR decision rules. threshold calibration HYPONYM-OF calibration. algorithm USED-FOR threshold - calibrated forecaster. uncalibrated forecaster USED-FOR algorithm. threshold loss function USED-FOR threshold decision. hospital scheduling decisions CONJUNCTION resource allocation decisions. resource allocation decisions CONJUNCTION hospital scheduling decisions. threshold calibration USED-FOR decision loss prediction. real - world settings EVALUATE-FOR threshold calibration. resource allocation decisions HYPONYM-OF real - world settings. hospital scheduling decisions HYPONYM-OF real - world settings. OtherScientificTerm are forecasted probabilities, predicted losses, cutoff, decision loss, and threshold decisions. Task are regression setting, and loss of threshold decisions. Generic is procedure. ","This paper studies the problem of threshold calibration in the regression setting. The authors propose a method to calibrate the loss function of a threshold-calibrated forecaster for the decision making process. The proposed method is based on the idea that the threshold of a decision is a function of the threshold decision of the uncalibrated model, and that the proposed method can be applied to any threshold decision. The method is evaluated on a number of real-world problems, including resource allocation and hospital scheduling. ","This paper studies the problem of threshold calibration in the regression setting. The authors propose a method to calibrate the loss function of a threshold-calibrated forecaster for the decision making process. The proposed method is based on the idea that the threshold of a decision is a function of the threshold decision of the uncalibrated model, and that the proposed method can be applied to any threshold decision. The method is evaluated on a number of real-world problems, including resource allocation and hospital scheduling. "
10926,SP:f55160db59c6f3e85f6e1ea0ec32c1a0982fbc48,method USED-FOR centroid approximation. centroid approximation USED-FOR random function. argmax distribution HYPONYM-OF random function. method USED-FOR argmax distribution. centroid points USED-FOR argmax distribution. centroid points USED-FOR method. objective function USED-FOR method. objective function USED-FOR argmax distribution. personalized dialogue systems CONJUNCTION multi - target domain adaptation. multi - target domain adaptation CONJUNCTION personalized dialogue systems. few - shot image classification CONJUNCTION personalized dialogue systems. personalized dialogue systems CONJUNCTION few - shot image classification. real - world multitask learning applications EVALUATE-FOR method. few - shot image classification HYPONYM-OF real - world multitask learning applications. multi - target domain adaptation HYPONYM-OF real - world multitask learning applications. personalized dialogue systems HYPONYM-OF real - world multitask learning applications. Task is machine learning. Method is argmax centroid method. OtherScientificTerm is Wasserstein distance. ,"This paper proposes a new method for centroid approximation based on the Wasserstein distance. The main idea is to use a random function to approximate the argmax distribution. The authors show that the proposed method can be applied to a wide range of machine learning problems, including few-shot image classification, personalized dialogue systems, multi-target domain adaptation, and multi-task learning. ","This paper proposes a new method for centroid approximation based on the Wasserstein distance. The main idea is to use a random function to approximate the argmax distribution. The authors show that the proposed method can be applied to a wide range of machine learning problems, including few-shot image classification, personalized dialogue systems, multi-target domain adaptation, and multi-task learning. "
10951,SP:ef342e3c6a16e898a49b700a9fd4f0ea6a069dcc,"adversarial manner USED-FOR preferences. preference vector USED-FOR reward function. pre - specified multi - objective reward functions FEATURE-OF preference vector. episodic learning problem USED-FOR problem. Markov decision process USED-FOR episodic learning problem. nearly minimax optimal regret bound EVALUATE-FOR model - based algorithm. nearly optimal trajectory complexity EVALUATE-FOR algorithm. Task are multi - objective reinforcement learning, and online setting. OtherScientificTerm are transitions, ( adversarial ) preference, policies, and preference - free exploration. ","This paper studies the problem of multi-objective reinforcement learning in an online setting. The authors consider the setting where the goal is to learn a policy that maximizes the reward function of a pre-specified reward function over a set of transitions. They show that the optimal regret bound for this setting is $O(\sqrt{T})$, which is a lower bound of the regret bound of a Markov decision process (MDP). They also provide an algorithm to solve this problem. ","This paper studies the problem of multi-objective reinforcement learning in an online setting. The authors consider the setting where the goal is to learn a policy that maximizes the reward function of a pre-specified reward function over a set of transitions. They show that the optimal regret bound for this setting is $O(\sqrt{T})$, which is a lower bound of the regret bound of a Markov decision process (MDP). They also provide an algorithm to solve this problem. "
10976,SP:aa84981dd503ec34d9f06aa6e5f680e267f82b04,"classification models COMPARE explanation of sequence generation models. explanation of sequence generation models COMPARE classification models. model - agnostic explanations USED-FOR text generation task. dialogue response generation HYPONYM-OF text generation task. open - ended sentences USED-FOR Dialog response generation. LERG USED-FOR sequence prediction. LERG USED-FOR explanations. unbiased approximation CONJUNCTION consistency. consistency CONJUNCTION unbiased approximation. consistency CONJUNCTION cause identification. cause identification CONJUNCTION consistency. explanation USED-FOR text generation. LERG USED-FOR text generation. explanation USED-FOR LERG. consistency HYPONYM-OF explanation. automaticand humanevaluation metrics EVALUATE-FOR task. method COMPARE methods. methods COMPARE method. task EVALUATE-FOR methods. task EVALUATE-FOR method. automaticand humanevaluation metrics EVALUATE-FOR methods. automaticand humanevaluation metrics EVALUATE-FOR method. LERG USED-FOR explicit and implicit relations. Method are generation model, and local explanation of response generation ( LERG ). OtherScientificTerm is human response. ",This paper proposes a method for dialogue response generation based on the local explanation of response generation (LERG) framework. LERG is a model-agnostic explanation framework for text generation. The authors show that LERG can be used for both explicit and implicit relations between the generated text and the human response. The method is evaluated on the Dialog response generation task and shows that the proposed method outperforms existing methods. ,This paper proposes a method for dialogue response generation based on the local explanation of response generation (LERG) framework. LERG is a model-agnostic explanation framework for text generation. The authors show that LERG can be used for both explicit and implicit relations between the generated text and the human response. The method is evaluated on the Dialog response generation task and shows that the proposed method outperforms existing methods. 
11001,SP:965413b1726617006317bbbec55673dd5d21812a,"distributed methods USED-FOR compressor. contraction property FEATURE-OF compressor. RandK HYPONYM-OF biased compressors. error compensation CONJUNCTION error feedback. error feedback CONJUNCTION error compensation. gradient compression CONJUNCTION acceleration. acceleration CONJUNCTION gradient compression. error compensation CONJUNCTION acceleration. acceleration CONJUNCTION error compensation. error compensation USED-FOR gradient compression. method COMPARE error compensated algorithms. error compensated algorithms COMPARE method. communication rounds EVALUATE-FOR method. Method are Gradient compression, error compensated gradient compression methods, and error compensated loopless Katyusha method. Metric are communication cost, and accelerated linear convergence rate. OtherScientificTerm is divergence. ",This paper studies the convergence rate of error compensated gradient compression methods. The authors show that the rate of convergence converges faster than that of error-compressed gradient compression and error-free gradient compression. They also show that error-based gradient compression can be used to accelerate the convergence of gradient compression algorithms. ,This paper studies the convergence rate of error compensated gradient compression methods. The authors show that the rate of convergence converges faster than that of error-compressed gradient compression and error-free gradient compression. They also show that error-based gradient compression can be used to accelerate the convergence of gradient compression algorithms. 
11026,SP:27c58dad7fa7743a8ff56fad863aa0dae823dccb,"machine learning framework USED-FOR edge and neuromorphic computing paradigms. training complexity CONJUNCTION biological plausibility. biological plausibility CONJUNCTION training complexity. machine learning framework USED-FOR it. biological plausibility FEATURE-OF liquid state machine ( LSM ). training complexity EVALUATE-FOR liquid state machine ( LSM ). LSM USED-FOR internal weights. LSM COMPARE multi - layer neural networks. multi - layer neural networks COMPARE LSM. LSM USED-FOR model of brain computation. synaptic plasticity CONJUNCTION brain dynamics. brain dynamics CONJUNCTION synaptic plasticity. astrocytes USED-FOR synaptic plasticity. astrocytes USED-FOR brain dynamics. neuron - astrocyte liquid state machine ( NALSM)1 USED-FOR under - performance. self - organized near - critical dynamics USED-FOR neuron - astrocyte liquid state machine ( NALSM)1. self - organized near - critical dynamics USED-FOR under - performance. astrocyte model USED-FOR global feedback. NALSM COMPARE LSM methods. LSM methods COMPARE NALSM. accuracy EVALUATE-FOR LSM methods. accuracy EVALUATE-FOR NALSM. MNIST CONJUNCTION N - MNIST. N - MNIST CONJUNCTION MNIST. accuracy EVALUATE-FOR NALSM. N - MNIST CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION N - MNIST. braininspired machine learning methods USED-FOR deep learning. deep learning USED-FOR neuromorphic computing. Task is brain computation. Method are backpropagation of gradients, and backpropagation. OtherScientificTerm are brain networks, computationally optimal critical phase transition, neuronal activity, NALSM dynamics, branching factor, edge - of - chaos, and data - specific hand - tuning. ","This paper proposes a method for learning a model of brain computation. The model is based on the neuron-astrocyte liquid state machine (NALSM) model. NALSM is an extension of the LSM model. The authors show that the model is computationally optimal for the near-critical phase transition, which is the critical phase transition between neurons and astrocytes. They also show that this model can be used to improve the performance of deep neural networks. ","This paper proposes a method for learning a model of brain computation. The model is based on the neuron-astrocyte liquid state machine (NALSM) model. NALSM is an extension of the LSM model. The authors show that the model is computationally optimal for the near-critical phase transition, which is the critical phase transition between neurons and astrocytes. They also show that this model can be used to improve the performance of deep neural networks. "
11051,SP:64ccd697d3c11d7d8947ef1b06c61d94b6a2e575,"class imbalance problem HYPONYM-OF learning node representations. asymmetric topological properties FEATURE-OF labeled nodes. graph data USED-FOR imbalance. Label Propagation algorithm USED-FOR node influence shift phenomenon. model - agnostic method ReNode USED-FOR topology - imbalance issue. influence conflict detection – based metric Totoro USED-FOR graph topology imbalance. method USED-FOR topology - imbalance issue. method USED-FOR semi - supervised node classification. topology - imbalance issue CONJUNCTION semi - supervised node classification. semi - supervised node classification CONJUNCTION topology - imbalance issue. graph neural networks ( GNNs ) USED-FOR topology imbalance. OtherScientificTerm are quantity imbalance, graph ( topology imbalance ), and class boundaries. Task are unknown topology - imbalance issue, semisupervised node classification learning, and quantityand topologyimbalance issues. ","This paper studies the problem of topology imbalance in semi-supervised node classification. The authors propose a model-agnostic method ReNode to address the problem. The proposed method is based on the influence conflict detection (ICD) metric Totoro, which is used to measure the influence shift phenomenon in the graph. The paper also proposes a label propagation algorithm to alleviate the imbalance issue. Experiments show that the proposed method outperforms baselines in terms of classification performance and classification accuracy.","This paper studies the problem of topology imbalance in semi-supervised node classification. The authors propose a model-agnostic method ReNode to address the problem. The proposed method is based on the influence conflict detection (ICD) metric Totoro, which is used to measure the influence shift phenomenon in the graph. The paper also proposes a label propagation algorithm to alleviate the imbalance issue. Experiments show that the proposed method outperforms baselines in terms of classification performance and classification accuracy."
11076,SP:ec12f0a05db75ac15ad22b34cdc2a0142bc2c72f,"d - dimensional lattice FEATURE-OF additive Gaussian noise. additive Gaussian noise USED-FOR piece - wise constant signals. partition recovery USED-FOR partition of the lattice. DCART - based procedure USED-FOR partition. regularity conditions USED-FOR DCART - based procedure. recursive dyadic partitions USED-FOR signal partition. recursive dyadic partitions USED-FOR rectangular sub - graphs. NP - hard exhaustive search method USED-FOR one. optimal regression tree estimator ( ORT ) USED-FOR partition estimator. DCART USED-FOR partition recovery. Task are signal detection or testing, and estimation. OtherScientificTerm are constancy regions of the unknown signal, and noise variance. Generic is method. ","This paper studies the problem of partition recovery for additive Gaussian noise in a d-dimensional lattice. In particular, the authors consider the case where the noise variance is bounded by a regularity condition. The authors propose a method to recover the partition of the lattice using recursive dyadic partitions. The method is based on the DCART-based procedure, and the authors show that it is NP-hard to find the optimal partition estimator for the optimal regression tree estimator. ","This paper studies the problem of partition recovery for additive Gaussian noise in a d-dimensional lattice. In particular, the authors consider the case where the noise variance is bounded by a regularity condition. The authors propose a method to recover the partition of the lattice using recursive dyadic partitions. The method is based on the DCART-based procedure, and the authors show that it is NP-hard to find the optimal partition estimator for the optimal regression tree estimator. "
11101,SP:3c65b3e69a024431cafdc1b4bfbccd432de69faf,"tasks EVALUATE-FOR deep learning models. causality - based training framework USED-FOR spurious correlations. interventional distribution COMPARE observational distribution. observational distribution COMPARE interventional distribution. Maximum Likelihood Estimation ( MLE ) USED-FOR interventional distribution. algorithms USED-FOR causal predictions. Implicit CMLE CONJUNCTION Explicit CMLE. Explicit CMLE CONJUNCTION Implicit CMLE. Implicit CMLE USED-FOR causal predictions. algorithms USED-FOR deep learning models. Explicit CMLE USED-FOR causal predictions. deep learning models USED-FOR causal predictions. Implicit CMLE HYPONYM-OF algorithms. Explicit CMLE HYPONYM-OF algorithms. observational data USED-FOR algorithms. observational data USED-FOR deep learning models. observational data USED-FOR causal predictions. observational data USED-FOR interventional distribution. Natural Language Inference ( NLI ) CONJUNCTION Image Captioning. Image Captioning CONJUNCTION Natural Language Inference ( NLI ). simulated data CONJUNCTION real - world tasks. real - world tasks CONJUNCTION simulated data. Natural Language Inference ( NLI ) HYPONYM-OF real - world tasks. Image Captioning HYPONYM-OF real - world tasks. CMLE methods COMPARE regular MLE method. regular MLE method COMPARE CMLE methods. CMLE methods USED-FOR spurious correlations. out - of - domain generalization EVALUATE-FOR regular MLE method. out - of - domain generalization EVALUATE-FOR CMLE methods. Generic is they. OtherScientificTerm are predictive clues, observed confounders, and expected negative log - likelihood. Method are general Structural Causal Model ( SCM ), and Counterfactual Maximum Likelihood Estimation ( CMLE ). ",This paper proposes a new training framework for causal inference based on the Structural Causal Model (SCM) and Counterfactual Maximum Likelihood Estimation (CMLE). The authors show that the proposed method outperforms the existing methods in terms of out-of-domain generalization and generalization to new data sets. The authors also show that their method is more robust to spurious correlations. ,This paper proposes a new training framework for causal inference based on the Structural Causal Model (SCM) and Counterfactual Maximum Likelihood Estimation (CMLE). The authors show that the proposed method outperforms the existing methods in terms of out-of-domain generalization and generalization to new data sets. The authors also show that their method is more robust to spurious correlations. 
11126,SP:c5a59c8d6db0f5491721aaaef182609c360930d3,"multi - task learning USED-FOR learning. multi - task learning COMPARE single task learning. single task learning COMPARE multi - task learning. learning COMPARE single task learning. single task learning COMPARE learning. multi - task learning objective USED-FOR average loss. gradients FEATURE-OF task objectives. heuristics USED-FOR problem. heuristics USED-FOR task gradients. Conflict - Averse Gradient descent ( CAGrad ) USED-FOR average loss function. regular gradient descent ( GD ) CONJUNCTION multiple gradient descent algorithm ( MGDA ). multiple gradient descent algorithm ( MGDA ) CONJUNCTION regular gradient descent ( GD ). multiple gradient descent algorithm ( MGDA ) PART-OF multi - objective optimization ( MOO ) literature. multiple gradient descent algorithm ( MGDA ) PART-OF It. regular gradient descent ( GD ) PART-OF It. CAGrad COMPARE multi - objective gradient manipulation methods. multi - objective gradient manipulation methods COMPARE CAGrad. OtherScientificTerm are model structures, conflicting gradients, average gradient direction, convergence guarantee, Pareto - stationary point, and worst local improvement. Generic is objective. Method is multi - task model. ","This paper studies the problem of multi-task learning. The authors propose a new objective called Conflict-Averse Gradient Descent (CAGrad) to improve the convergence of the multi-objective optimization (MOO) algorithms. CAGrad is motivated by the observation that the gradient direction of a task is not always the same as that of the target task. To overcome this issue, the authors propose two heuristics: the worst local improvement (WLI) and the worst-case convergence (WCS). Experiments show that the proposed method outperforms the state-of-the-art in terms of convergence.","This paper studies the problem of multi-task learning. The authors propose a new objective called Conflict-Averse Gradient Descent (CAGrad) to improve the convergence of the multi-objective optimization (MOO) algorithms. CAGrad is motivated by the observation that the gradient direction of a task is not always the same as that of the target task. To overcome this issue, the authors propose two heuristics: the worst local improvement (WLI) and the worst-case convergence (WCS). Experiments show that the proposed method outperforms the state-of-the-art in terms of convergence."
11151,SP:000cbfda2e26fdcfee50a628799a73b6886cfccc,"Large language models USED-FOR few - shot learning. language models USED-FOR simple algorithmic concepts. strong priors FEATURE-OF teaching problem. program induction systems CONJUNCTION humans. humans CONJUNCTION program induction systems. GPT architectures CONJUNCTION program induction systems. program induction systems CONJUNCTION GPT architectures. complexity EVALUATE-FOR concept. GPT architectures CONJUNCTION humans. humans CONJUNCTION GPT architectures. artificial intelligence CONJUNCTION machine learning. machine learning CONJUNCTION artificial intelligence. language models CONJUNCTION machine teaching. machine teaching CONJUNCTION language models. machine teaching USED-FOR artificial intelligence. OtherScientificTerm are patterns of algorithmic nature, and Occam ’s razor. Generic is models. Task is learning. ","This paper studies the problem of few-shot learning in the context of machine learning. The authors propose a new problem called Occam’s razor, which is an extension of Occam's razor to the setting where the learner has access to a large number of examples, and the goal is to learn a program that can be used to guide the learning of a language model. The main contribution of the paper is that the authors show that the problem can be solved in a way that is similar to that of learning a program in a program induction system. The paper also shows that this problem is a generalization of the problem in machine learning, and that it can be tackled in a similar way to learning a machine learning model. ","This paper studies the problem of few-shot learning in the context of machine learning. The authors propose a new problem called Occam’s razor, which is an extension of Occam's razor to the setting where the learner has access to a large number of examples, and the goal is to learn a program that can be used to guide the learning of a language model. The main contribution of the paper is that the authors show that the problem can be solved in a way that is similar to that of learning a program in a program induction system. The paper also shows that this problem is a generalization of the problem in machine learning, and that it can be tackled in a similar way to learning a machine learning model. "
11176,SP:ba01895bf1aa07a0630b8c41fc0e91effb34b4cf,"perturbation USED-FOR Adversarial examples. feature representation USED-FOR robust and non - robust features. Information Bottleneck USED-FOR feature representation. Information Bottleneck USED-FOR way. noise variation USED-FOR feature unit. information flow PART-OF feature representation. noise variation magnitude USED-FOR information flow. human - perceptible semantic information FEATURE-OF they. attack mechanism USED-FOR gradient of non - robust features. OtherScientificTerm are adversarial examples, feature space, and distilled features. Task are adversarial prediction, model prediction, and model robustness. ","This paper studies the problem of adversarial prediction, which is an important problem in machine learning. The authors propose a novel approach to improve the robustness of models against adversarial examples. The proposed approach is based on the idea of Information Bottleneck (IB), which is a technique to extract the information flow between robust and non-robust features in the feature space. In particular, the authors show that the noise variation magnitude of the feature unit can be used to extract information flow from the feature representation. ","This paper studies the problem of adversarial prediction, which is an important problem in machine learning. The authors propose a novel approach to improve the robustness of models against adversarial examples. The proposed approach is based on the idea of Information Bottleneck (IB), which is a technique to extract the information flow between robust and non-robust features in the feature space. In particular, the authors show that the noise variation magnitude of the feature unit can be used to extract information flow from the feature representation. "
11201,SP:ed67b2664359799a11cebb9eaba6da74ff1dd977,"support vector machine ( SVM ) CONJUNCTION minimum Euclidean norm least squares regression. minimum Euclidean norm least squares regression CONJUNCTION support vector machine ( SVM ). models USED-FOR high - dimensional data. support vector machine ( SVM ) HYPONYM-OF approaches. approaches USED-FOR linear models. minimum Euclidean norm least squares regression HYPONYM-OF approaches. they PART-OF models. support vector proliferation USED-FOR independent feature models. super - linear lower bound USED-FOR support vector proliferation. dimension USED-FOR support vector proliferation. super - linear lower bound FEATURE-OF dimension. sharp phase transition FEATURE-OF Gaussian feature models. geometric characterization of the problem USED-FOR lp case. l1 variant PART-OF SVM. OtherScientificTerm are support vector, upper bounds, and phase transition. Generic is transition. ","This paper studies the problem of support vector machine (SVM) and minimum Euclidean norm least squares regression (MOMS) for high-dimensional data. In particular, the authors consider the case where the support vector of a Gaussian feature model has a sharp phase transition. The authors show that for the lp case, there is a super-linear lower bound for support vector proliferation, and the authors also provide a geometric characterization of the problem. ","This paper studies the problem of support vector machine (SVM) and minimum Euclidean norm least squares regression (MOMS) for high-dimensional data. In particular, the authors consider the case where the support vector of a Gaussian feature model has a sharp phase transition. The authors show that for the lp case, there is a super-linear lower bound for support vector proliferation, and the authors also provide a geometric characterization of the problem. "
11226,SP:99f226a63902863c429cb7baefab09626d13921e,"Markov Decision Processes USED-FOR active pure exploration problem. instance - specific sample complexity EVALUATE-FOR algorithm. algorithm USED-FOR communicating MDPs. reduced exploration rate CONJUNCTION faster convergence. faster convergence CONJUNCTION reduced exploration rate. reduced exploration rate FEATURE-OF variant. ergodicity assumption USED-FOR variant. online setting USED-FOR navigation constraints. ergodic theorem USED-FOR non - homogeneous Markov chains. ergodic theorem USED-FOR analysis. OtherScientificTerm are system trajectory, and problem - dependent lower bound. Task are generative setting, and analysis of Markov Decision Processes. ","This paper studies the problem of communicating Markov Decision Processes (MDPs) in an online setting. The authors propose a new algorithm for communicating MDPs, which is based on the ergodicity assumption. They show that the proposed algorithm converges to a lower bound on the instance-specific sample complexity of the MDP. They also show that their algorithm is faster than existing algorithms. ","This paper studies the problem of communicating Markov Decision Processes (MDPs) in an online setting. The authors propose a new algorithm for communicating MDPs, which is based on the ergodicity assumption. They show that the proposed algorithm converges to a lower bound on the instance-specific sample complexity of the MDP. They also show that their algorithm is faster than existing algorithms. "
11251,SP:de4a0f5a464aa3311445cc25c4915cf0c4d975c3,"entities CONJUNCTION first - order logical ( FOL ) queries. first - order logical ( FOL ) queries CONJUNCTION entities. low - dimensional spaces FEATURE-OF first - order logical ( FOL ) queries. low - dimensional spaces FEATURE-OF entities. conjunction CONJUNCTION disjunction. disjunction CONJUNCTION conjunction. disjunction CONJUNCTION negation. negation CONJUNCTION disjunction. geometry - based QE model USED-FOR FOL operations. query embedding model USED-FOR FOL operations. Cone Embeddings ( ConE ) HYPONYM-OF geometry - based QE model. Cone Embeddings ( ConE ) HYPONYM-OF query embedding model. negation HYPONYM-OF FOL operations. disjunction HYPONYM-OF FOL operations. conjunction HYPONYM-OF FOL operations. geometric complement operators USED-FOR negation operations. embedding space FEATURE-OF geometric complement operators. ConE COMPARE state - of - the - art methods. state - of - the - art methods COMPARE ConE. benchmark datasets EVALUATE-FOR state - of - the - art methods. benchmark datasets EVALUATE-FOR ConE. Task is multi - hop reasoning over knowledge graphs. OtherScientificTerm are knowledge graphs, geometric shapes, Cartesian products of two - dimensional cones, conjunction and disjunction operations, closure of complement of cones, and cones. Method is geometry - based models. ","This paper proposes a geometry-based QE model for learning first-order logical (FOL) queries in low-dimensional spaces. The key idea is to use geometric complement operators in the embedding space of the query embedding model to capture the closure of complement of cones, and disjunction operations. The proposed method is evaluated on a number of benchmark datasets. ","This paper proposes a geometry-based QE model for learning first-order logical (FOL) queries in low-dimensional spaces. The key idea is to use geometric complement operators in the embedding space of the query embedding model to capture the closure of complement of cones, and disjunction operations. The proposed method is evaluated on a number of benchmark datasets. "
11276,SP:773b5b6d31e6899da395933eb7f9e25a6e50c406,"linear - time Legendre transform USED-FOR numerical scheme. numerical scheme USED-FOR value iteration ( VI ) algorithm. linear - time Legendre transform USED-FOR value iteration ( VI ) algorithm. conjugate domain FEATURE-OF value iteration ( VI ) algorithm. time complexity EVALUATE-FOR algorithm. error EVALUATE-FOR algorithm. convergence EVALUATE-FOR algorithm. convergence CONJUNCTION time complexity. time complexity CONJUNCTION convergence. time complexity CONJUNCTION error. error CONJUNCTION time complexity. minimization operation PART-OF primal domain. discretization USED-FOR state and input spaces. discretization USED-FOR approach. time complexity EVALUATE-FOR approach. Task is stochastic nonlinear systems. OtherScientificTerm are state and input variables, and O(X + U ). Method is VI algorithm. ","This paper studies the problem of value iteration (VI) in stochastic nonlinear systems. The authors propose a numerical scheme for value iteration in the conjugate domain, which is a special case of the primal domain. They show that the proposed method converges to the optimal solution of the value iteration problem in a linear-time Legendre transform. They also provide convergence guarantees for the method. ","This paper studies the problem of value iteration (VI) in stochastic nonlinear systems. The authors propose a numerical scheme for value iteration in the conjugate domain, which is a special case of the primal domain. They show that the proposed method converges to the optimal solution of the value iteration problem in a linear-time Legendre transform. They also provide convergence guarantees for the method. "
11301,SP:7cd593ccba4830f3383a92ef6266224cc7699706,"framework USED-FOR multimodal representations. unlabeled data USED-FOR framework. unlabeled data USED-FOR multimodal representations. convolution - free Transformer architectures USED-FOR framework. multimodal representations USED-FOR downstream tasks. VideoAudio - Text Transformer ( VATT ) USED-FOR multimodal representations. raw signals USED-FOR VideoAudio - Text Transformer ( VATT ). audio event classification CONJUNCTION image classification. image classification CONJUNCTION audio event classification. image classification CONJUNCTION text - to - video retrieval. text - to - video retrieval CONJUNCTION image classification. video action recognition CONJUNCTION audio event classification. audio event classification CONJUNCTION video action recognition. video action recognition HYPONYM-OF downstream tasks. text - to - video retrieval HYPONYM-OF downstream tasks. image classification HYPONYM-OF downstream tasks. audio event classification HYPONYM-OF downstream tasks. multimodal contrastive losses USED-FOR VATT. convolution - free VATT COMPARE ConvNet - based architectures. ConvNet - based architectures COMPARE convolution - free VATT. ConvNet - based architectures USED-FOR downstream tasks. convolution - free VATT USED-FOR downstream tasks. Kinetics-400 EVALUATE-FOR VATT ’s vision Transformer. top-1 accuracy EVALUATE-FOR VATT ’s vision Transformer. videos CONJUNCTION images. images CONJUNCTION videos. VATT ’s audio Transformer USED-FOR waveform - based audio event recognition. mAP EVALUATE-FOR VATT ’s audio Transformer. Method are modality - agnostic, single - backbone Transformer, and supervised pre - training. Material are Kinetics-600, Kinetics-700, ImageNet, and AudioSet. Generic are Transformer, and model. ","This paper proposes a multi-modal audio-text transformer (VATT) framework for learning multimodal representations for downstream tasks. The proposed method is based on a single-backbone Transformer, which is trained with supervised pre-training. The method is evaluated on Kinetics-400 and ImageNet, and achieves state-of-the-art performance on video action recognition and audio event recognition tasks. ","This paper proposes a multi-modal audio-text transformer (VATT) framework for learning multimodal representations for downstream tasks. The proposed method is based on a single-backbone Transformer, which is trained with supervised pre-training. The method is evaluated on Kinetics-400 and ImageNet, and achieves state-of-the-art performance on video action recognition and audio event recognition tasks. "
11326,SP:97f533426dce73d27768dd7afc2ddf035cf21e61,"quadratic time and space complexity EVALUATE-FOR self - attention mechanism. computation bottleneck FEATURE-OF pairwise dot products. computation bottleneck FEATURE-OF kernel machines. computational cost EVALUATE-FOR approximation schemes. accuracy EVALUATE-FOR approximation schemes. computation methods USED-FOR kernel machines. Nyström method USED-FOR computation. Nyström method USED-FOR non - positive semidefinite matrix. softmax structure CONJUNCTION Gaussian kernel. Gaussian kernel CONJUNCTION softmax structure. computation methods USED-FOR computational cost. matrix approximation error EVALUATE-FOR method. spectral norm FEATURE-OF matrix approximation error. spectral norm EVALUATE-FOR method. method COMPARE full self - attention. full self - attention COMPARE method. Long Range Arena benchmark EVALUATE-FOR method. Long Range Arena benchmark EVALUATE-FOR full self - attention. Method are Transformers, and Skyformer. OtherScientificTerm is computation resources. ",This paper proposes a new self-attention mechanism for computing pairwise dot products. The proposed method is based on the Nyström method for computing non-positive semidefinite matrices. The authors show that the proposed method can reduce the computational cost of the computation of the matrix approximator by a factor of quadratic time and space complexity. The method is evaluated on the Long Range Arena benchmark.,This paper proposes a new self-attention mechanism for computing pairwise dot products. The proposed method is based on the Nyström method for computing non-positive semidefinite matrices. The authors show that the proposed method can reduce the computational cost of the computation of the matrix approximator by a factor of quadratic time and space complexity. The method is evaluated on the Long Range Arena benchmark.
11351,SP:a6f1094a4c9f38df38c9710b9dcd6299f430fae2,"setting PART-OF problems. image - based data augmentation USED-FOR invariance. image - based data augmentation CONJUNCTION expert - aware offline data augmentation approach. expert - aware offline data augmentation approach CONJUNCTION image - based data augmentation. expert - aware offline data augmentation approach USED-FOR feedback - sensitivity. image - based data augmentation USED-FOR image perturbations. image - based data augmentation PART-OF augmented policy cloning ( APC ) approach. method USED-FOR transfer 12 of complex high - DoF behaviors. method USED-FOR policy cloning. data - efficiency EVALUATE-FOR policy cloning. approach USED-FOR algorithms. policy cloning USED-FOR transfer 12 of complex high - DoF behaviors. policy cloning PART-OF algorithms. data - efficiency EVALUATE-FOR method. Method are data - augmentation technique, and behavioral cloning. Task is policy 3 cloning setting. Metric is data efficiency. OtherScientificTerm are expert, student policy, and expert trajectories. ",This paper proposes an augmented policy cloning (APC) approach for policy cloning in the policy cloning setting. APC is a data-augmented offline data augmentation technique that augments the expert trajectories and the student trajectories to improve the transfer efficiency of policy cloning. The authors show that the proposed method is able to achieve better transfer efficiency compared to the state-of-the-art methods. They also show that APC can be applied to the high-doF setting.,This paper proposes an augmented policy cloning (APC) approach for policy cloning in the policy cloning setting. APC is a data-augmented offline data augmentation technique that augments the expert trajectories and the student trajectories to improve the transfer efficiency of policy cloning. The authors show that the proposed method is able to achieve better transfer efficiency compared to the state-of-the-art methods. They also show that APC can be applied to the high-doF setting.
11376,SP:3660d1d4a8e8f281880781ba32df7b678b705f9c,benchmarks CONJUNCTION simulated robotics environment. simulated robotics environment CONJUNCTION benchmarks. simulated robotics environment EVALUATE-FOR framework. benchmarks EVALUATE-FOR framework. Task is computer vision settings. Method is deep networks. ,"This paper proposes a framework for learning deep neural networks for computer vision tasks. The framework is based on the idea of learning deep networks that can be used in a variety of computer vision settings. The proposed framework is evaluated on a number of benchmark tasks, and is shown to outperform the state-of-the-art. ","This paper proposes a framework for learning deep neural networks for computer vision tasks. The framework is based on the idea of learning deep networks that can be used in a variety of computer vision settings. The proposed framework is evaluated on a number of benchmark tasks, and is shown to outperform the state-of-the-art. "
11401,SP:4c12852373f5f113bd47dce3e2434c5e7d61a202,"agents USED-FOR tasks. Reinforcement Learning ( RL ) USED-FOR agents. visual observations USED-FOR agents. visual observations USED-FOR tasks. data augmentation USED-FOR generalization. generalization FEATURE-OF RL. data augmentation USED-FOR off - policy RL algorithms. data augmentation USED-FOR instability. technique USED-FOR algorithms. ConvNets CONJUNCTION Vision Transformers ( ViT ). Vision Transformers ( ViT ) CONJUNCTION ConvNets. benchmarks CONJUNCTION robotic manipulation tasks. robotic manipulation tasks CONJUNCTION benchmarks. benchmarks EVALUATE-FOR Vision Transformers ( ViT ). benchmarks EVALUATE-FOR image - based RL. DeepMind Control Suite USED-FOR benchmarks. Vision Transformers ( ViT ) USED-FOR image - based RL. ConvNets USED-FOR image - based RL. state - of - the - art methods USED-FOR image - based RL. stability CONJUNCTION sample efficiency. sample efficiency CONJUNCTION stability. method COMPARE state - of - the - art methods. state - of - the - art methods COMPARE method. generalization EVALUATE-FOR state - of - the - art methods. stability FEATURE-OF ConvNets. sample efficiency FEATURE-OF ConvNets. method USED-FOR ConvNets. augmentation FEATURE-OF ConvNets. sample efficiency EVALUATE-FOR method. stability EVALUATE-FOR method. generalization EVALUATE-FOR method. method USED-FOR RL. ViT - based architectures USED-FOR method. ViT - based architectures USED-FOR RL. OtherScientificTerm are divergence, and high - variance Q - targets. Generic is problems. ",This paper proposes a method for off-policy reinforcement learning (off-policy RL) based on data augmentation to improve the stability and sample efficiency of convolutional neural networks (convNets) and ViT-based RL algorithms. The proposed method is based on the observation that convNets are unstable in high-variance Q-targets. The authors propose a method to mitigate this issue by augmenting the Q-target of the convNet with the data from the ViT model. The method is evaluated on a variety of tasks and shows that the proposed method outperforms state-of-the-art methods. ,This paper proposes a method for off-policy reinforcement learning (off-policy RL) based on data augmentation to improve the stability and sample efficiency of convolutional neural networks (convNets) and ViT-based RL algorithms. The proposed method is based on the observation that convNets are unstable in high-variance Q-targets. The authors propose a method to mitigate this issue by augmenting the Q-target of the convNet with the data from the ViT model. The method is evaluated on a variety of tasks and shows that the proposed method outperforms state-of-the-art methods. 
11426,SP:f8ca9d92c45adc4512381035856b445029e3080a,"local data USED-FOR joint model. minibatch sizes CONJUNCTION number of local updates. number of local updates CONJUNCTION minibatch sizes. WNs ’ and the server ’s update directions CONJUNCTION minibatch sizes. minibatch sizes CONJUNCTION WNs ’ and the server ’s update directions. communication rounds USED-FOR WNs. WNs USED-FOR local updates. algorithm USED-FOR ✏ -stationary solution. Õ ( ✏ 3/2 ) samples CONJUNCTION Õ ( ✏ 1 ) communication rounds. Õ ( ✏ 1 ) communication rounds CONJUNCTION Õ ( ✏ 3/2 ) samples. stochastic momentum estimator USED-FOR WN ’s and the server ’s directions. Õ ( ✏ 1 ) communication rounds USED-FOR algorithm. Õ ( ✏ 3/2 ) samples USED-FOR algorithm. near - optimal sample and communication complexities EVALUATE-FOR FL algorithm. WNs ’ and server ’s update directions CONJUNCTION minibatch sizes. minibatch sizes CONJUNCTION WNs ’ and server ’s update directions. Method are Federated Learning ( FL ), stochastic algorithms, and FL algorithms. Task is non - convex FL problem. Metric is sample and communication complexities. OtherScientificTerm is trade - off curve. ",This paper studies the problem of federated learning (FL) in the non-convex setting. The authors propose a stochastic algorithm for the problem. The main contribution of the paper is to propose a new algorithm for FL. The proposed algorithm is based on the momentum estimator of the WNs’ and the server’s update directions. The algorithm is shown to be near-optimal in terms of both sample complexity and communication complexity. ,This paper studies the problem of federated learning (FL) in the non-convex setting. The authors propose a stochastic algorithm for the problem. The main contribution of the paper is to propose a new algorithm for FL. The proposed algorithm is based on the momentum estimator of the WNs’ and the server’s update directions. The algorithm is shown to be near-optimal in terms of both sample complexity and communication complexity. 
11451,SP:bd3eecb81a17af010f2d3555434990855c1810f2,"information - theoretical framework USED-FOR non - vacuous generalization bounds. non - vacuous generalization bounds USED-FOR large models. isotropic noise USED-FOR Stochastic Gradient Langevin Dynamics ( SGLD ). Stochastic Gradient Langevin Dynamics ( SGLD ) USED-FOR large models. noise structure USED-FOR SGLD. noise structure USED-FOR information - theoretical generalization bound. expected gradient covariance USED-FOR optimal noise covariance. optimal noise COMPARE empirical gradient covariance. empirical gradient covariance COMPARE optimal noise. information - theoretical bound USED-FOR optimization analysis. matrix analysis USED-FOR optimal noise covariance. OtherScientificTerm are constraint, and prior. ",This paper studies the generalization properties of isotropic noise in Stochastic Gradient Langevin Dynamics (SGLD). The authors show that the optimal noise covariance of the SGLD is the product of the expected gradient covariance and the prior. The authors also provide an information-theoretic bound for the optimal covariance. ,This paper studies the generalization properties of isotropic noise in Stochastic Gradient Langevin Dynamics (SGLD). The authors show that the optimal noise covariance of the SGLD is the product of the expected gradient covariance and the prior. The authors also provide an information-theoretic bound for the optimal covariance. 
11476,SP:19fbd1a381598538662417a4a1885ba4ac04f5f8,Learned video compression methods COMPARE video codecs. video codecs COMPARE Learned video compression methods. prediction mode CONJUNCTION fixed network framework. fixed network framework CONJUNCTION prediction mode. prediction mode USED-FOR learned video compression schemes. fixed network framework USED-FOR learned video compression schemes. model USED-FOR prediction modes. 3D motion vector fields USED-FOR weighted 9 trilinear warping. voxel flows USED-FOR weighted 9 trilinear warping. spatial - temporal space FEATURE-OF weighted 9 trilinear warping. 3D motion vector fields USED-FOR motion compensation 8 module. motion compensation 8 module USED-FOR versatile compression. voxel flows HYPONYM-OF 3D motion vector fields. temporal reference position FEATURE-OF voxel flows. flow prediction module USED-FOR motion trajectories. flow prediction module USED-FOR multiple - reference - frame predic12 tion. unified polynomial function USED-FOR flow prediction module. unified polynomial function USED-FOR motion trajectories. flow prediction module USED-FOR voxel flows. VLVC USED-FOR versatile compression. VLVC COMPARE Versatile Video Coding 17 ( VVC ) standard. Versatile Video Coding 17 ( VVC ) standard COMPARE VLVC. R - D performance EVALUATE-FOR Versatile Video Coding 17 ( VVC ) standard. MS - SSIM EVALUATE-FOR R - D performance. R - D performance EVALUATE-FOR VLVC. MS - SSIM EVALUATE-FOR Versatile Video Coding 17 ( VVC ) standard. OtherScientificTerm is inter prediction modes. ,"This paper proposes a new video compression method called Versatile Video Coding 17 (VLVC), which is based on voxel flows. VLVC uses a 3D motion vector field and a motion compensation 8 module for video compression. The proposed method is able to achieve state-of-the-art R-D performance on the MS-SSIM benchmark. ","This paper proposes a new video compression method called Versatile Video Coding 17 (VLVC), which is based on voxel flows. VLVC uses a 3D motion vector field and a motion compensation 8 module for video compression. The proposed method is able to achieve state-of-the-art R-D performance on the MS-SSIM benchmark. "
11501,SP:ba790fdcf2deef1a1b5e1961c7c4a28dd0218420,"regret EVALUATE-FOR MT - OMD. geometry FEATURE-OF regularizer. geometry FEATURE-OF task 3 variance. OMDs USED-FOR √ NT bound. Online Gradient Descent CONJUNCTION Exponentiated 7 Gradient. Exponentiated 7 Gradient CONJUNCTION Online Gradient Descent. Exponentiated 7 Gradient HYPONYM-OF OMD. Method are Online Mirror 1 Descent ( OMD ), and closed - form updates. OtherScientificTerm are time horizon, and σ. Generic is them. Material is real - world datasets. ","This paper studies the problem of online mirror-1 descent (OMD) and online gradient descent (OGD), which is a regularizer that is used to reduce the variance of the task-3 variance. The authors show that under certain assumptions, OMDs can be seen as a closed-form version of Online Gradient Descent (OMD) and Exponentiated 7 Gradient (EGD). They also show that OMD can be viewed as an extension of Online Mirror 1 Descent, which is an online version of OMD. They also provide a new bound on the regret of an OMD, which they call the MT-OMD. ","This paper studies the problem of online mirror-1 descent (OMD) and online gradient descent (OGD), which is a regularizer that is used to reduce the variance of the task-3 variance. The authors show that under certain assumptions, OMDs can be seen as a closed-form version of Online Gradient Descent (OMD) and Exponentiated 7 Gradient (EGD). They also show that OMD can be viewed as an extension of Online Mirror 1 Descent, which is an online version of OMD. They also provide a new bound on the regret of an OMD, which they call the MT-OMD. "
11526,SP:75f80e4e7836a7575e60de7f055820c6c7065fcb,ε - error USED-FOR approximating d - dimensional ULD. finite summation of N smooth components PART-OF stronglyconvex potential. stronglyconvex potential USED-FOR underdamped Langevin diffusion ( ULD ). gradient evaluations USED-FOR discretization method. method USED-FOR strongly - log - concave distribution. gradient complexity EVALUATE-FOR gradient based sampling algorithms. method COMPARE gradient based sampling algorithms. gradient based sampling algorithms COMPARE method. gradient complexity EVALUATE-FOR method. synthetic and real - world data EVALUATE-FOR method. method COMPARE ULD approaches. ULD approaches COMPARE method. synthetic and real - world data EVALUATE-FOR ULD approaches. ,"This paper proposes a discretization method for underdamped Langevin diffusion (ULD). The discretisation method is based on the notion of strongly-convex potential, which is defined as a finite summation of N smooth components of the strongly-log-concave distribution. The authors show that the discretized ULD can be approximated by a gradient-based sampling algorithm. They also show that their method is computationally efficient.","This paper proposes a discretization method for underdamped Langevin diffusion (ULD). The discretisation method is based on the notion of strongly-convex potential, which is defined as a finite summation of N smooth components of the strongly-log-concave distribution. The authors show that the discretized ULD can be approximated by a gradient-based sampling algorithm. They also show that their method is computationally efficient."
11551,SP:22822f378c3fbc15b77eb736194b1ce7f0585072,"recurrent networks USED-FOR neural dynamics. neural dynamics USED-FOR biological continual learning. feedforward and recurrent neural networks EVALUATE-FOR methods. weight regularization CONJUNCTION projected gradient descent. projected gradient descent CONJUNCTION weight regularization. projected gradient descent PART-OF method. weight regularization PART-OF method. catastrophic forgetting FEATURE-OF optimization. prior precision USED-FOR catastrophic forgetting. gradient projection USED-FOR NCL. prior precision USED-FOR gradient projection. Bayesian weight regularization USED-FOR NCL. projection based approaches USED-FOR continual learning problems. weight regularization techniques CONJUNCTION projection based approaches. projection based approaches CONJUNCTION weight regularization techniques. method USED-FOR continual learning problems. method COMPARE weight regularization techniques. weight regularization techniques COMPARE method. method COMPARE projection based approaches. projection based approaches COMPARE method. feedforward and recurrent networks USED-FOR continual learning problems. networks USED-FOR task - specific dynamics. Method are Biological agents, artificial agents, specific parameter regularizers, and Natural Continual Learning ( NCL ). OtherScientificTerm are parameter space, gradients, and biological circuits. Task is optimization journey. ",This paper proposes a method for continual learning (CL) based on Bayesian weight regularization. The main idea is to use the prior precision of the gradient projection to prevent catastrophic forgetting in the optimization process. The authors show that the proposed method is able to achieve state-of-the-art performance on a variety of continual learning tasks. ,This paper proposes a method for continual learning (CL) based on Bayesian weight regularization. The main idea is to use the prior precision of the gradient projection to prevent catastrophic forgetting in the optimization process. The authors show that the proposed method is able to achieve state-of-the-art performance on a variety of continual learning tasks. 
11576,SP:26de056be14962312c759be5d284ef235d660f9c,"Normalizing flows HYPONYM-OF invertible neural networks. change - of - volume terms FEATURE-OF invertible neural networks. low - dimensional manifold PART-OF high - dimensional ambient space. heuristics USED-FOR approaches. heuristics USED-FOR term. methods USED-FOR gradient. automatic differentiation CONJUNCTION numerical linear algebra. numerical linear algebra CONJUNCTION automatic differentiation. gradient FEATURE-OF term. automatic differentiation USED-FOR methods. approaches USED-FOR end - to - end nonlinear manifold learning. manifolds CONJUNCTION distributions. distributions CONJUNCTION manifolds. Method is maximum likelihood. OtherScientificTerm are modelling mismatch, invertibility requirement, Injective flows, lowto high - dimensional spaces, and volume - change term. Generic are manifold, and model. Task is out - of - distribution detection. ","This paper studies the problem of out-of-distribution detection in Invertible neural networks. Invertibility is a very important problem in the context of nonlinear manifold learning. The authors propose a new term called change of volume terms, which is defined as the difference between the invertibility of a low-dimensional manifold and a high-dimensional ambient space. This term is defined in terms of the maximum likelihood of the gradient of the model under the assumption that the model is invertible.  The authors show that this term can be used as a regularizer to improve the generalization performance of the network. They also provide a theoretical analysis of this term. ","This paper studies the problem of out-of-distribution detection in Invertible neural networks. Invertibility is a very important problem in the context of nonlinear manifold learning. The authors propose a new term called change of volume terms, which is defined as the difference between the invertibility of a low-dimensional manifold and a high-dimensional ambient space. This term is defined in terms of the maximum likelihood of the gradient of the model under the assumption that the model is invertible.  The authors show that this term can be used as a regularizer to improve the generalization performance of the network. They also provide a theoretical analysis of this term. "
11601,SP:395dae632dab83f3f61bdf67eabe4d351492798c,"response time FEATURE-OF physical computational elements. inference CONJUNCTION learning. learning CONJUNCTION inference. framework USED-FOR inference. framework USED-FOR learning. inference CONJUNCTION learning. learning CONJUNCTION inference. networks of slow components USED-FOR learning. principle USED-FOR quasi - instantaneous inference. phased plasticity CONJUNCTION network relaxation phases. network relaxation phases CONJUNCTION phased plasticity. prospective energy function USED-FOR disentangled neuron and synapse dynamics. continuous - time, leaky neuronal dynamics CONJUNCTION continuously active, local plasticity. continuously active, local plasticity CONJUNCTION continuous - time, leaky neuronal dynamics. error backpropagation USED-FOR deep cortical networks. continuous - time, leaky neuronal dynamics USED-FOR error backpropagation. benchmark datasets EVALUATE-FOR learning. fully - connected and convolutional architectures USED-FOR learning. robustness EVALUATE-FOR model. model USED-FOR physical realization. spatio - temporal substrate imperfections FEATURE-OF robustness. spatio - temporal substrate imperfections FEATURE-OF model. OtherScientificTerm are neurons, response lag, delayed processing of stimuli, timing mismatch, instructive signals, biological neurons, membrane potential, and network depth. Method are hierarchical models of cortical networks, physical dynamical systems, and Latent Equilibrium. ","This paper proposes a new framework for learning neural networks with slow components. The proposed method is based on the Latent Equilibrium (LEE) framework, which is an extension of the previous work on neural networks. The authors propose to use the LEE framework to learn a disentangled neuron and synapse dynamics, which allows for quasi-instantaneous inference and learning of neural networks of slow components in the presence of spatio-temporal imperfections. The method is tested on a number of benchmark datasets, and is shown to outperform the baselines. ","This paper proposes a new framework for learning neural networks with slow components. The proposed method is based on the Latent Equilibrium (LEE) framework, which is an extension of the previous work on neural networks. The authors propose to use the LEE framework to learn a disentangled neuron and synapse dynamics, which allows for quasi-instantaneous inference and learning of neural networks of slow components in the presence of spatio-temporal imperfections. The method is tested on a number of benchmark datasets, and is shown to outperform the baselines. "
11626,SP:b937901e3230b14e36975fbab0658a52bdac4977,"Graph neural network ( GNN ) USED-FOR graph classification. node representation USED-FOR rooted subtree. GNN USED-FOR node representation. 1 - WL USED-FOR node representation. 1 - WL CONJUNCTION GNN. GNN CONJUNCTION 1 - WL. representation USED-FOR graph. rooted subtree representations PART-OF representation. rooted subtrees USED-FOR nontree graph. NGNN USED-FOR graph. rooted subgraphs COMPARE rooted subtrees. rooted subtrees COMPARE rooted subgraphs. rooted subgraphs FEATURE-OF graph. GNN USED-FOR subgraph representation. NGNN USED-FOR subgraph representation. GNN USED-FOR subgraph. NGNN USED-FOR local subgraph. GNN USED-FOR NGNN. subgraph representations USED-FOR whole - graph representation. NGNN COMPARE 1 - WL. 1 - WL COMPARE NGNN. NGNN USED-FOR r - regular graphs. NGNN COMPARE GNNs. GNNs COMPARE NGNN. GNNs COMPARE NGNN. NGNN COMPARE GNNs. time complexity EVALUATE-FOR GNNs. time complexity EVALUATE-FOR NGNN. NGNN HYPONYM-OF plug - and - play framework. plug - and - play framework CONJUNCTION base GNNs. base GNNs CONJUNCTION plug - and - play framework. NGNN CONJUNCTION base GNNs. base GNNs CONJUNCTION NGNN. NGNN COMPARE base GNNs. base GNNs COMPARE NGNN. benchmark datasets EVALUATE-FOR base GNNs. benchmark datasets EVALUATE-FOR NGNN. OtherScientificTerm are neighboring node features, subtrees, and subtree. Method is Nested Graph Neural Networks ( NGNNs ). ",This paper proposes a new method for graph classification based on Nested Graph Neural Networks (NGNNs). The proposed method is a plug-and-play framework that uses a GNN for node representation and a subgraph representation for subgraphs. The authors show that the proposed method outperforms base GNNs and 1-WL on several benchmark datasets. ,This paper proposes a new method for graph classification based on Nested Graph Neural Networks (NGNNs). The proposed method is a plug-and-play framework that uses a GNN for node representation and a subgraph representation for subgraphs. The authors show that the proposed method outperforms base GNNs and 1-WL on several benchmark datasets. 
11651,SP:7b8284aa82022ce73802bfc57238b0d82031b226,"nesting FEATURE-OF forward or reverse KL divergence. NVI USED-FOR importance sampling strategies. heuristics USED-FOR sampler. NVI USED-FOR intermediate densities. NVI USED-FOR multimodal distribution. amortized inference USED-FOR hierarchical deep generative models. heuristics USED-FOR amortized inference. heuristics USED-FOR hidden Markov model. learned annealing path USED-FOR NVI. log average weight CONJUNCTION effective sample size. effective sample size CONJUNCTION log average weight. log average weight FEATURE-OF sample quality. effective sample size FEATURE-OF sample quality. sample quality EVALUATE-FOR nested objectives. Method are nested variational inference ( NVI ), and nested importance samplers. ","This paper studies nested variational inference (NVI) for deep generative models. The authors show that nested importance sampling strategies can be used to improve the sample quality of a generative model. NVI can be seen as an extension of the amortized inference (AM) framework, which is used to learn a learned annealing path for a hidden Markov model. In particular, the authors propose to use nested importance samplers to reduce the number of samples needed to achieve the optimal sample quality. They show that the optimal sampling strategy can be obtained by minimizing the KL divergence between the mean and variance of the two distributions. They also show that NVI is able to achieve better sample quality than the optimal sampler in terms of the effective sample size.","This paper studies nested variational inference (NVI) for deep generative models. The authors show that nested importance sampling strategies can be used to improve the sample quality of a generative model. NVI can be seen as an extension of the amortized inference (AM) framework, which is used to learn a learned annealing path for a hidden Markov model. In particular, the authors propose to use nested importance samplers to reduce the number of samples needed to achieve the optimal sample quality. They show that the optimal sampling strategy can be obtained by minimizing the KL divergence between the mean and variance of the two distributions. They also show that NVI is able to achieve better sample quality than the optimal sampler in terms of the effective sample size."
11676,SP:f3792f82b28727a7a198c6eac9511391d2045a5f,"packing bound USED-FOR Piyavskii - Shubert algorithm. packing bound USED-FOR upper bound. local worst - case analysis USED-FOR learning tasks. instance - dependent lower bound COMPARE worst - case lower bounds. worst - case lower bounds COMPARE instance - dependent lower bound. Lipschitz setting FEATURE-OF worst - case lower bounds. local worst - case analysis USED-FOR instance - dependent lower bound. OtherScientificTerm is Lipschitz function f. Metric is optimal sample complexity. Method are computationally tractable DOO algorithm, and packing and integral bounds. ","This paper studies the packing bound of Piyavskii-shubert algorithm for the Lipschitz setting. The packing bound is a generalization of the packing and integral bounds for the case of instance-dependent worst-case lower bounds. The main contribution of this paper is to show that the packing upper bound can be computed in a computationally tractable DOO algorithm, and the packing lower bound can also be computed efficiently. The paper also provides a theoretical analysis of the case where the packing of the algorithm is computationally efficient. ","This paper studies the packing bound of Piyavskii-shubert algorithm for the Lipschitz setting. The packing bound is a generalization of the packing and integral bounds for the case of instance-dependent worst-case lower bounds. The main contribution of this paper is to show that the packing upper bound can be computed in a computationally tractable DOO algorithm, and the packing lower bound can also be computed efficiently. The paper also provides a theoretical analysis of the case where the packing of the algorithm is computationally efficient. "
11701,SP:6e8134eeaf524db765a6186f3de74e936243f8d4,"Deep neural networks ( DNNs ) USED-FOR tasks. Credible uncertainty estimation USED-FOR risk - sensitive applications. network USED-FOR uncertainty estimation. attack COMPARE adversarial attacks. adversarial attacks COMPARE attack. white - box setting USED-FOR scenario. Deep Ensembles CONJUNCTION MC - Dropout. MC - Dropout CONJUNCTION Deep Ensembles. vanilla softmax score CONJUNCTION Deep Ensembles. Deep Ensembles CONJUNCTION vanilla softmax score. uncertainty estimation methods USED-FOR attacks. vanilla softmax score HYPONYM-OF uncertainty estimation methods. MC - Dropout HYPONYM-OF uncertainty estimation methods. Deep Ensembles HYPONYM-OF uncertainty estimation methods. SelectiveNet CONJUNCTION selective classification architecture. selective classification architecture CONJUNCTION SelectiveNet. SelectiveNet USED-FOR attack. MobileNetV2 CONJUNCTION EfficientNetB0. EfficientNetB0 CONJUNCTION MobileNetV2. architectures EVALUATE-FOR attack. EfficientNetB0 HYPONYM-OF architectures. MobileNetV2 HYPONYM-OF architectures. OtherScientificTerm are network ’s capacity, and uncertainty estimation damage. Method is DNN. Material are black - box regime, and ImageNet. Task is uncertainty estimations. ",This paper studies the problem of adversarial attacks on uncertainty estimation in deep neural networks (DNNs). The authors propose a new attack that can be applied to any black-box DNN architecture. The authors show that the proposed attack can be used to improve the performance of DNNs in the white-box setting. The proposed attack is based on the idea that adversarial examples are more likely to be misclassified in the black box setting than the white box setting. ,This paper studies the problem of adversarial attacks on uncertainty estimation in deep neural networks (DNNs). The authors propose a new attack that can be applied to any black-box DNN architecture. The authors show that the proposed attack can be used to improve the performance of DNNs in the white-box setting. The proposed attack is based on the idea that adversarial examples are more likely to be misclassified in the black box setting than the white box setting. 
11726,SP:c5a5bf6e0bdebf5170c8fe3fedd2f3438e39cd21,"stochastic block models USED-FOR community detection. network information USED-FOR they. model USED-FOR networks. Task are community detection problem, and real - world applications. OtherScientificTerm are network, well - connected ‘ communities ’, and network structure. Method are detection algorithm, voting approaches, streaming stochastic block model ( StSBM ), voting algorithms, and streaming belief - propagation ( STREAMBP ) approach. Material is synthetic and real data. ",This paper proposes a streaming stochastic block model (StSBM) for community detection. The proposed method is based on the STREAMBP algorithm. The method is evaluated on synthetic and real-world datasets.,This paper proposes a streaming stochastic block model (StSBM) for community detection. The proposed method is based on the STREAMBP algorithm. The method is evaluated on synthetic and real-world datasets.
11751,SP:b1163857a6b06047c3531ab762642fcbed6dd294,predictor space FEATURE-OF regularization cost. l2 regularization USED-FOR regularization cost. l2 regularization USED-FOR predictor space. linear neural networks USED-FOR parameterizations of linear predictors. sparse linear ConvNets CONJUNCTION residual networks. residual networks CONJUNCTION sparse linear ConvNets. representation cost FEATURE-OF sparse linear ConvNets. representation cost FEATURE-OF residual networks. lp quasi - norms CONJUNCTION group quasi - norms. group quasi - norms CONJUNCTION lp quasi - norms. architecture CONJUNCTION parameterization. parameterization CONJUNCTION architecture. group quasi - norms CONJUNCTION k - support - norm. k - support - norm CONJUNCTION group quasi - norms. k - support - norm CONJUNCTION elastic net. elastic net CONJUNCTION k - support - norm. parameterization USED-FOR representation cost. regularizers USED-FOR linear predictors. architecture USED-FOR representation cost. group quasi - norms CONJUNCTION elastic net. elastic net CONJUNCTION group quasi - norms. l2 regularization USED-FOR representation cost. elastic net HYPONYM-OF regularizers. k - support - norm HYPONYM-OF regularizers. group quasi - norms HYPONYM-OF regularizers. lp quasi - norms HYPONYM-OF regularizers. Method is parameterizations. Task is reverse problem. ,"This paper studies the problem of regularization of linear neural networks. The authors propose a new regularizer, l2 regularization, for linear predictors. The proposed regularizer is based on the notion of l2-regularization, and the authors show that it can be used to reduce the representation cost of linear convolutional networks. They also show that the proposed regularization can be applied to a number of existing regularizers such as group quasi-norms, elastic net, and k-support-norm. ","This paper studies the problem of regularization of linear neural networks. The authors propose a new regularizer, l2 regularization, for linear predictors. The proposed regularizer is based on the notion of l2-regularization, and the authors show that it can be used to reduce the representation cost of linear convolutional networks. They also show that the proposed regularization can be applied to a number of existing regularizers such as group quasi-norms, elastic net, and k-support-norm. "
11776,SP:c9c7fc5288e24a54531b7063c028d307279fe2ef,"approach USED-FOR knowledge graphs ( KGs ). approach COMPARE case - based reasoning. case - based reasoning COMPARE approach. case - based reasoning USED-FOR artificial intelligence ( AI ). non - parametric approach USED-FOR crisp logical rules. graph path patterns USED-FOR non - parametric approach. NELL-995 CONJUNCTION FB-122. FB-122 CONJUNCTION NELL-995. accuracy EVALUATE-FOR method. NELL-995 EVALUATE-FOR models. FB-122 EVALUATE-FOR models. model USED-FOR low data settings. OtherScientificTerm are binary relation, and relation. ",This paper proposes a non-parametric approach for knowledge graph reasoning. The key idea is to use graph path patterns to learn a set of logical rules that can be used to solve the problem of knowledge graph inference. The authors show that the proposed method is able to achieve state-of-the-art performance on several benchmark datasets. ,This paper proposes a non-parametric approach for knowledge graph reasoning. The key idea is to use graph path patterns to learn a set of logical rules that can be used to solve the problem of knowledge graph inference. The authors show that the proposed method is able to achieve state-of-the-art performance on several benchmark datasets. 
11780,SP:f63e4ed39d577b50eab4f4b6d08ef912a69840ef,Transformer architecture PART-OF entity linking model. CoNLL CONJUNCTION TAC - KBP. TAC - KBP CONJUNCTION CoNLL. entity linking datasets EVALUATE-FOR model. Transformer architecture CONJUNCTION input perturbations. input perturbations CONJUNCTION Transformer architecture. negative entity candidates CONJUNCTION Transformer architecture. Transformer architecture CONJUNCTION negative entity candidates. end - to - end entity linking CONJUNCTION entity linking. entity linking CONJUNCTION end - to - end entity linking. entity linking HYPONYM-OF settings. end - to - end entity linking HYPONYM-OF settings. in - domain training data USED-FOR settings. Material is Wikipedia links. ,This paper proposes a transformer-based entity linking model for entity linking. The proposed model is based on the Transformer architecture. The authors show that the proposed model outperforms the state-of-the-art model on a number of entity linking datasets. ,This paper proposes a transformer-based entity linking model for entity linking. The proposed model is based on the Transformer architecture. The authors show that the proposed model outperforms the state-of-the-art model on a number of entity linking datasets. 
11784,SP:eaeee88e0717cda8d6f3d8ff83ebe594eba44f29,"ensemble Active Learning methods USED-FOR acquisition. intermediate training checkpoints USED-FOR ensembles. training data subset search USED-FOR large labeled datasets. acquisition functions CONJUNCTION ensemble configurations. ensemble configurations CONJUNCTION acquisition functions. initialization schemes CONJUNCTION acquisition functions. acquisition functions CONJUNCTION initialization schemes. CIFAR-100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-100. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. ImageNet HYPONYM-OF image classification benchmarks. CIFAR-10 HYPONYM-OF image classification benchmarks. CIFAR-100 HYPONYM-OF image classification benchmarks. ResNet-101 CONJUNCTION DenseNet121. DenseNet121 CONJUNCTION ResNet-101. data subsets USED-FOR deep models. ResNet-18 ensemble USED-FOR data subsets. DenseNet121 HYPONYM-OF deep models. ResNet-101 HYPONYM-OF deep models. training data distribution USED-FOR large scale vision tasks. Method are Deep Neural Networks ( DNNs ), and DNNs. Generic are datasets, they, approach, and dataset. Task is DNN ’s optimization. OtherScientificTerm is training distribution. Metric is training time. ","This paper studies the problem of ensemble active learning for training deep neural networks (DNNs). The authors propose a new method for learning ensembles of training data subsets for large-scale vision tasks. The method is based on the idea that the training data subset search can be used as an intermediate training checkpoint for DNNs. The proposed method is evaluated on CIFAR-10, ImageNet, ResNet-18, and DenseNet-101 datasets. The authors show that the proposed method outperforms existing methods in terms of training time and accuracy. ","This paper studies the problem of ensemble active learning for training deep neural networks (DNNs). The authors propose a new method for learning ensembles of training data subsets for large-scale vision tasks. The method is based on the idea that the training data subset search can be used as an intermediate training checkpoint for DNNs. The proposed method is evaluated on CIFAR-10, ImageNet, ResNet-18, and DenseNet-101 datasets. The authors show that the proposed method outperforms existing methods in terms of training time and accuracy. "
11788,SP:4a1cce61f12c68846c507130bd055b3444ac8101,"routing algorithm USED-FOR capsule networks. mechanism USED-FOR routing. sequential iterative routing CONJUNCTION concurrent iterative routing. concurrent iterative routing CONJUNCTION sequential iterative routing. inverted dot - product attention USED-FOR routing. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. routing algorithms COMPARE method. method COMPARE routing algorithms. method COMPARE CNN. CNN COMPARE method. it COMPARE CNN. CNN COMPARE it. method COMPARE it. it COMPARE method. CIFAR-10 HYPONYM-OF benchmark datasets. CIFAR-100 HYPONYM-OF benchmark datasets. benchmark datasets EVALUATE-FOR method. capsule model COMPARE CNNs. CNNs COMPARE capsule model. task EVALUATE-FOR CNNs. task EVALUATE-FOR capsule model. task EVALUATE-FOR recognizing digits. neurons per layer USED-FOR CNNs. capsule model USED-FOR recognizing digits. overlayed digit images USED-FOR task. overlayed digit images USED-FOR recognizing digits. capsule networks USED-FOR complex real - world tasks. Method are Layer Normalization, and Capsules - Inverted - Attention - Routing. OtherScientificTerm is normalization. ","This paper proposes Capsules-Inverted-Attention-Routing, a routing algorithm for capsule networks. The proposed method is based on the inverted dot-product attention (IDO) mechanism. The authors show that the proposed method outperforms existing routing algorithms on CIFAR-10 and CIFARS-100 datasets.","This paper proposes Capsules-Inverted-Attention-Routing, a routing algorithm for capsule networks. The proposed method is based on the inverted dot-product attention (IDO) mechanism. The authors show that the proposed method outperforms existing routing algorithms on CIFAR-10 and CIFARS-100 datasets."
11792,SP:99ca283c579152bc44b19c21392aeb7f6b76231b,"statistical physics FEATURE-OF parallel tempering technique. history PART-OF joint hyperparameter / model - parameter space. method USED-FOR dropout and learning rate optimization. Task are Hyperparameter optimization, and training of deep architectures. Method is deep architectures. Generic are methods, and model. OtherScientificTerm are hyperparameter space, nonlocal paths, hyperparameters, correlated noise, temperature, and overfitting. Metric are computational cost, resistance, and absolute validation error. ","This paper proposes a parallel tempering technique for hyperparameter optimization. The proposed method is based on the observation that the history of the hyperparameters in the joint hyper-parameter space is correlated with the temperature of the model. The authors show that this correlation is due to the fact that the temperature is not uniform across all hyperparametrized points in the space. To address this issue, the authors propose to use the temperature as a proxy for the dropout and learning rate of hyperparamers. They show that their method is able to reduce the computational cost by a factor of two. ","This paper proposes a parallel tempering technique for hyperparameter optimization. The proposed method is based on the observation that the history of the hyperparameters in the joint hyper-parameter space is correlated with the temperature of the model. The authors show that this correlation is due to the fact that the temperature is not uniform across all hyperparametrized points in the space. To address this issue, the authors propose to use the temperature as a proxy for the dropout and learning rate of hyperparamers. They show that their method is able to reduce the computational cost by a factor of two. "
11796,SP:beba754d96cc441712a5413c41e98863c8abf605,"Minimum Risk Training ( MRT ) CONJUNCTION Generative Adversarial Networks ( GAN ). Generative Adversarial Networks ( GAN ) CONJUNCTION Minimum Risk Training ( MRT ). Reinforcement learning ( RL ) USED-FOR text generation tasks. machine translation ( MT ) HYPONYM-OF text generation tasks. methods USED-FOR MT. RL methods USED-FOR MT. OtherScientificTerm are expected reward, pre - trained parameters, translation, training signal, and distribution curve. ","This paper studies the problem of machine translation (MT) in the context of Reinforcement Learning (RL). In particular, the authors consider the case where the target task is to generate a translation of a text from a source text to a target text, and the goal is to maximize the expected reward. The authors propose two methods for MT: (1) Minimum Risk Training (MRT) and (2) Generative Adversarial Networks (GAN). The authors show that MRT is able to achieve better performance than GANs and GAN-based RL methods on MT tasks. ","This paper studies the problem of machine translation (MT) in the context of Reinforcement Learning (RL). In particular, the authors consider the case where the target task is to generate a translation of a text from a source text to a target text, and the goal is to maximize the expected reward. The authors propose two methods for MT: (1) Minimum Risk Training (MRT) and (2) Generative Adversarial Networks (GAN). The authors show that MRT is able to achieve better performance than GANs and GAN-based RL methods on MT tasks. "
11800,SP:366b68d2490ea7569c74dc66ec0f83daa029ddd9,reinforcement learning algorithms CONJUNCTION applications. applications CONJUNCTION reinforcement learning algorithms. closed - form characterizations FEATURE-OF asymptotic variances. closed - form characterizations USED-FOR Q - value estimates. policies USED-FOR estimation errors. confidence regions USED-FOR Q - value and optimal value functions. exploration strategy COMPARE benchmark approaches. benchmark approaches COMPARE exploration strategy. Task is statistical inference. Method is policy exploration strategy. OtherScientificTerm is Q estimates. ,This paper studies the problem of estimating the Q-value of a policy in the presence of asymptotic variances. The authors propose a new exploration strategy for estimating Q-values that is based on closed-form characterizations of the confidence regions of the Q estimates. They show that this exploration strategy can be used to improve the estimation error of the optimal value function. They also show that the exploration strategy is able to reduce the variance of the estimation errors. ,This paper studies the problem of estimating the Q-value of a policy in the presence of asymptotic variances. The authors propose a new exploration strategy for estimating Q-values that is based on closed-form characterizations of the confidence regions of the Q estimates. They show that this exploration strategy can be used to improve the estimation error of the optimal value function. They also show that the exploration strategy is able to reduce the variance of the estimation errors. 
11804,SP:d922459581c3295ff315fda6e59b9f7e9147f22d,"Top - k recommendation USED-FOR largescale recommender systems. Cold - start and efficiency issues USED-FOR largescale recommender systems. Cold - start and efficiency issues FEATURE-OF Top - k recommendation. hybrid recommendation methods USED-FOR cold - start issues. efficiency EVALUATE-FOR online recommendation. online recommendation EVALUATE-FOR they. real latent space FEATURE-OF similarity search. cold - start items CONJUNCTION warm - start ones. warm - start ones CONJUNCTION cold - start items. cold - start users CONJUNCTION cold - start items. cold - start items CONJUNCTION cold - start users. collaborative generated hashing ( CGH ) USED-FOR efficiency. it USED-FOR recommendation settings. CGH USED-FOR hash functions. Minimum Description Length ( MDL ) principle USED-FOR CGH. Minimum Description Length ( MDL ) principle USED-FOR hash functions. CGH USED-FOR marketing strategy. generative step USED-FOR CGH. MDL principle USED-FOR compact and informative binary codes. content data USED-FOR compact and informative binary codes. recommendations COMPARE baselines. baselines COMPARE recommendations. public datasets EVALUATE-FOR recommendations. application USED-FOR marketing. public datasets EVALUATE-FOR baselines. OtherScientificTerm are side information, and binary codes. ","This paper proposes collaborative generated hashing (CGH), a method for improving the efficiency of top-k recommendation. The authors propose to use the Minimum Description Length (MDL) principle to reduce the size of the hash function and the number of hash functions. They also propose a generative step to improve the efficiency. The proposed method is evaluated on a number of public datasets.","This paper proposes collaborative generated hashing (CGH), a method for improving the efficiency of top-k recommendation. The authors propose to use the Minimum Description Length (MDL) principle to reduce the size of the hash function and the number of hash functions. They also propose a generative step to improve the efficiency. The proposed method is evaluated on a number of public datasets."
11808,SP:c2a5551f229211c9aa4c43686b517fcde82bbccf,"adversarial domain adaptation CONJUNCTION multi - task learning. multi - task learning CONJUNCTION adversarial domain adaptation. adversarial domain adaptation USED-FOR AITL. multi - task learning USED-FOR AITL. genomic information USED-FOR drug response. large pre - clinical pharmacogenomics datasets CONJUNCTION clinical datasets. clinical datasets CONJUNCTION large pre - clinical pharmacogenomics datasets. transfer learning USED-FOR large pre - clinical pharmacogenomics datasets. drug response outcome FEATURE-OF clinical data. cancer cell lines HYPONYM-OF large pre - clinical pharmacogenomics datasets. AITL USED-FOR input and output discrepancies. adversarial inductive transfer learning method USED-FOR input and output discrepancies. AITL HYPONYM-OF adversarial inductive transfer learning method. AITL USED-FOR precision oncology. AITL COMPARE pharmacogenomics and transfer learning baselines. pharmacogenomics and transfer learning baselines COMPARE AITL. Method is Adversarial Inductive Transfer Learning ( AITL ). Generic is method. OtherScientificTerm are input and output spaces, and output space. Task is pharmacogenomics. Material is pre - clinical and clinical datasets. ",This paper proposes a new method for adversarial inductive transfer learning (AITL) for pre-clinical pharmacogenomics and transfer learning. AITL is based on adversarial domain adaptation and multi-task learning. The authors show that the proposed method can be applied to both input and output data. The proposed method is evaluated on a number of preclinical and clinical datasets.,This paper proposes a new method for adversarial inductive transfer learning (AITL) for pre-clinical pharmacogenomics and transfer learning. AITL is based on adversarial domain adaptation and multi-task learning. The authors show that the proposed method can be applied to both input and output data. The proposed method is evaluated on a number of preclinical and clinical datasets.
11812,SP:a27f975266e990b2ab4a0ab8db1588e945d0300a,"sample complexity EVALUATE-FOR model - free approaches. fictitious trajectory rollouts USED-FOR dynamics model. fictitious trajectory rollouts USED-FOR model - free approaches. stochastic dynamics model USED-FOR uncertainty quantification. erroneously optimistic regions PART-OF dynamics model. uncertainty aware ensemble of dynamics models USED-FOR next state predictions. ensemble of dynamics models USED-FOR policy update. simulated robotic locomotion HYPONYM-OF benchmark tests. benchmark tests EVALUATE-FOR approach. approach COMPARE model - based one. model - based one COMPARE approach. approach COMPARE model - free algorithms. model - free algorithms COMPARE approach. model - free algorithms COMPARE model - based one. model - based one COMPARE model - free algorithms. learning rates CONJUNCTION asymptotic behaviour. asymptotic behaviour CONJUNCTION learning rates. asymptotic behaviour EVALUATE-FOR MBPGE. learning rates EVALUATE-FOR MBPGE. Method are RL methods, and policy gradient methods. OtherScientificTerm are next state prediction, and real and virtual total reward. ","This paper proposes an ensemble of dynamics models to improve the sample complexity of model-free policy gradient methods. The authors propose to use a stochastic dynamics model to estimate the uncertainty quantification of the next state prediction, and then use a model-based policy gradient method to update the dynamics model. The proposed method is evaluated on simulated robotic locomotion tasks, and shows that it outperforms the baselines in terms of sample complexity and learning rates.","This paper proposes an ensemble of dynamics models to improve the sample complexity of model-free policy gradient methods. The authors propose to use a stochastic dynamics model to estimate the uncertainty quantification of the next state prediction, and then use a model-based policy gradient method to update the dynamics model. The proposed method is evaluated on simulated robotic locomotion tasks, and shows that it outperforms the baselines in terms of sample complexity and learning rates."
11816,SP:2aaddb6dda434b49487857d99c9d143e2f54d350,"neural network models USED-FOR visual features. adversarial examples CONJUNCTION corrupted images. corrupted images CONJUNCTION adversarial examples. class - conditional reconstruction of the input USED-FOR adversarial examples. class - conditional reconstruction of the input USED-FOR corrupted images. misclassification CONJUNCTION reconstruction error. reconstruction error CONJUNCTION misclassification. Reconstructive Attack USED-FOR detection mechanism. reconstructive attack USED-FOR undetected adversarial examples. success rate EVALUATE-FOR reconstructive attack. CapsNets COMPARE convolutional networks. convolutional networks COMPARE CapsNets. adversarial examples USED-FOR CapsNets. visual similarity USED-FOR reconstructive attack. features USED-FOR CapsNets. Material is Adversarial examples. Method is class - conditional reconstruction. Generic is attacks. OtherScientificTerm are perturbations, and human perception. ","This paper proposes a novel adversarial detection method based on class-conditional reconstruction of the input. The proposed method, called Reconstructive Attack (CapsNets), is able to detect adversarial examples that are undetected by existing methods. The authors show that CapsNets can be used to detect undetected adversarial samples from a class-conditioned adversarial example. They also show that the proposed method outperforms existing methods in terms of detection accuracy.","This paper proposes a novel adversarial detection method based on class-conditional reconstruction of the input. The proposed method, called Reconstructive Attack (CapsNets), is able to detect adversarial examples that are undetected by existing methods. The authors show that CapsNets can be used to detect undetected adversarial samples from a class-conditioned adversarial example. They also show that the proposed method outperforms existing methods in terms of detection accuracy."
11820,SP:da88bfbe3f59ce1a24522aa5e74c9472b079664a,"gradient descent USED-FOR parameter space. function space FEATURE-OF kernel gradient descent. gradient descent USED-FOR neural network. linear model USED-FOR wide networks. linear model USED-FOR neural network. full batch gradient descent USED-FOR neural network. Edge of Chaos HYPONYM-OF initialization. initialization CONJUNCTION activation function. activation function CONJUNCTION initialization. activation function USED-FOR NTK. initialization USED-FOR NTK. OtherScientificTerm are Neural Tangent Kernel ( NTK ), and network depth. ","This paper studies the neural tangent kernel (NTK) of deep neural networks. The authors show that NTK can be viewed as a linear model of the function space of the parameter space. They show that the NTK is a generalization of the full batch gradient descent (BBD) method. They also show that for NTK, the initialization of NTK and the activation function of the neural network can be seen as a function of initialization of the initialization function and the network depth. ","This paper studies the neural tangent kernel (NTK) of deep neural networks. The authors show that NTK can be viewed as a linear model of the function space of the parameter space. They show that the NTK is a generalization of the full batch gradient descent (BBD) method. They also show that for NTK, the initialization of NTK and the activation function of the neural network can be seen as a function of initialization of the initialization function and the network depth. "
11824,SP:dd59b897384c52c20d62be73fc33184c8c226f4b,"self - supervised method USED-FOR sentence representations. injection of linguistic knowledge USED-FOR self - supervised method. sentence structures USED-FOR semantic meaning. Multiple linguistic frameworks USED-FOR sentence structures. compositional words operations USED-FOR semantic meaning. embeddings USED-FOR semantic. linguistic views USED-FOR embeddings. OtherScientificTerm are linguist diversity, views, and sentence outward form. ","This paper proposes a self-supervised method for learning sentence embeddings. The key idea is to learn a sentence embedding that can be used to learn the semantic meaning of a given sentence. The embedding is learned by learning a set of compositional words operations that are used to encode the sentence structure. The authors show that the embedding can be learned using a number of different linguistic frameworks. The method is evaluated on a variety of tasks, and the results show that it is able to learn sentences that are more expressive than the baselines.","This paper proposes a self-supervised method for learning sentence embeddings. The key idea is to learn a sentence embedding that can be used to learn the semantic meaning of a given sentence. The embedding is learned by learning a set of compositional words operations that are used to encode the sentence structure. The authors show that the embedding can be learned using a number of different linguistic frameworks. The method is evaluated on a variety of tasks, and the results show that it is able to learn sentences that are more expressive than the baselines."
11828,SP:980babd58fc2ea5f40bb22b3a9a09737f14f3f18,"methods USED-FOR niche domains. product or movie review datasets EVALUATE-FOR sentiment classification solutions. finance HYPONYM-OF niche domains. Transfer learning USED-FOR new domains. NLP transfer learning USED-FOR financial sentiment classification. FinBERT HYPONYM-OF language model. financial sentiment classification task EVALUATE-FOR language model. FinancialPhrasebank dataset USED-FOR financial sentiment classification task. BERT USED-FOR language model. OtherScientificTerm is domainspecific language. Generic is models. Material are labeled data, specific domain, and large training data sets. ","This paper proposes a transfer learning method for financial sentiment classification. The proposed method is based on BERT, which is a language model that can be applied to new domains. The method is evaluated on the FinancialPhrasebank dataset, where it is shown to outperform the state-of-the-art transfer learning methods. ","This paper proposes a transfer learning method for financial sentiment classification. The proposed method is based on BERT, which is a language model that can be applied to new domains. The method is evaluated on the FinancialPhrasebank dataset, where it is shown to outperform the state-of-the-art transfer learning methods. "
11832,SP:31c9c3a693922d5c3448e80ade920391dce261f9,musical scores CONJUNCTION text lyrics. text lyrics CONJUNCTION musical scores. Generative models USED-FOR singing voice. Generative models USED-FOR singing voice synthesis. singing voice synthesis USED-FOR singing voice waveforms. musical scores USED-FOR singing voice waveforms. text lyrics USED-FOR singing voice waveforms. pre - assigned scores CONJUNCTION lyrics. lyrics CONJUNCTION pre - assigned scores. pre - assigned scores USED-FOR singing voice generation. singing voice generation HYPONYM-OF alternative. training and inference time EVALUATE-FOR singing voice generation. pipeline USED-FOR tasks. source separation and transcription models USED-FOR data preparation. adversarial networks USED-FOR audio generation. source separation and transcription models CONJUNCTION adversarial networks. adversarial networks CONJUNCTION source separation and transcription models. adversarial networks CONJUNCTION metrics. metrics CONJUNCTION adversarial networks. Method is unconditioned or weakly conditioned singing voice generation schemes. ,"This paper proposes a method for singing voice synthesis. The method is based on the idea of pre-assigning scores and lyrics to a pre-trained singing voice generation model, which is then used to generate singing voice waveforms from the pre-attributed scores. The authors show that the proposed method is able to achieve better results than the state-of-the-art in terms of training and inference time. The paper also shows that the method can be applied to other tasks such as source separation and transcription.","This paper proposes a method for singing voice synthesis. The method is based on the idea of pre-assigning scores and lyrics to a pre-trained singing voice generation model, which is then used to generate singing voice waveforms from the pre-attributed scores. The authors show that the proposed method is able to achieve better results than the state-of-the-art in terms of training and inference time. The paper also shows that the method can be applied to other tasks such as source separation and transcription."
11836,SP:99d41c8285fd0270ff16e915ef03187a0a7005b0,computer vision CONJUNCTION audio - understanding. audio - understanding CONJUNCTION computer vision. deep neural networks USED-FOR computer vision. deep neural networks USED-FOR audio - understanding. adversarial attacks FEATURE-OF they. defensive tensorization HYPONYM-OF adversarial defence technique. latent high order factorization of the network USED-FOR adversarial defence technique. Randomization USED-FOR latent subspace. Randomization USED-FOR dense reconstructed weights. sparsity CONJUNCTION perturbations. perturbations CONJUNCTION sparsity. randomization USED-FOR perturbations. approach CONJUNCTION techniques. techniques CONJUNCTION approach. approach CONJUNCTION neural architecture. neural architecture CONJUNCTION approach. adversarial training HYPONYM-OF techniques. image classification benchmarks EVALUATE-FOR approach. audio classification task CONJUNCTION binary networks. binary networks CONJUNCTION audio classification task. binary networks EVALUATE-FOR approach. audio classification task EVALUATE-FOR approach. Generic is network. ,This paper proposes a novel adversarial defense technique for deep neural networks. The proposed method is based on the idea of latent high order factorization of the network. The authors show that the proposed method can be used to defend against adversarial attacks. The method is evaluated on a variety of image classification tasks and audio classification tasks.,This paper proposes a novel adversarial defense technique for deep neural networks. The proposed method is based on the idea of latent high order factorization of the network. The authors show that the proposed method can be used to defend against adversarial attacks. The method is evaluated on a variety of image classification tasks and audio classification tasks.
11840,SP:762729b64c1c1494de0f7410ea3662da61e93b6d,Deep learning based approaches USED-FOR urban spatiotemporal forecasting problems. graph attention network CONJUNCTION transformer. transformer CONJUNCTION graph attention network. clustered graph transformer framework USED-FOR unsmoothness issue. transformer PART-OF encoder - decoder architecture. transformer PART-OF clustered graph transformer framework. graph attention network PART-OF clustered graph transformer framework. structural components USED-FOR architectures. structural components USED-FOR deep learning models. architectures PART-OF deep learning models. gradient - based clustering method USED-FOR feature extractors. gradient - based clustering method USED-FOR spatial domain. multi - view position encoding USED-FOR periodicity and closeness of urban time series data. multi - view position encoding USED-FOR temporal domain. real datasets EVALUATE-FOR method. real datasets EVALUATE-FOR baselines. method COMPARE baselines. baselines COMPARE method. ride - hailing business FEATURE-OF real datasets. OtherScientificTerm is unsmoothness issue of urban data. Material is urban data. ,This paper proposes a clustering method for time series forecasting. The proposed method is based on a graph attention network and a transformer. The authors show that the proposed method outperforms baselines on a number of datasets.,This paper proposes a clustering method for time series forecasting. The proposed method is based on a graph attention network and a transformer. The authors show that the proposed method outperforms baselines on a number of datasets.
11844,SP:81d7c60d0d12eb268d7edeebe86422991a1d4997,"experience replay CONJUNCTION target network. target network CONJUNCTION experience replay. target network USED-FOR DQN. deep neural networks USED-FOR fitted Q iteration ( FQI ) algorithm. algorithmic and statistical rates of convergence FEATURE-OF action - value functions. action - value functions FEATURE-OF iterative policy sequence. FQI USED-FOR iterative policy sequence. geometric rate FEATURE-OF algorithmic error. deep neural network USED-FOR action - value function. experience replay CONJUNCTION target network. target network CONJUNCTION experience replay. target network USED-FOR DQN. Minimax - DQN algorithm USED-FOR zero - sum Markov game. DQN USED-FOR Minimax - DQN algorithm. Method are deep reinforcement learning, and deep Q - network ( DQN ) algorithm. OtherScientificTerm is statistical error. ","This paper studies the convergence rate of the fitted Q iteration (FQI) algorithm for deep reinforcement learning (DQN). The authors show that the algorithm converges to a zero-sum Markov game with a geometric rate of $O(\sqrt{T})$ and a statistical rate of $\Omega(T)$. They show that this rate is bounded by the geometric rate. They also show that under certain assumptions on the action-value function and the target network, the algorithm can converge to the optimal solution. ","This paper studies the convergence rate of the fitted Q iteration (FQI) algorithm for deep reinforcement learning (DQN). The authors show that the algorithm converges to a zero-sum Markov game with a geometric rate of $O(\sqrt{T})$ and a statistical rate of $\Omega(T)$. They show that this rate is bounded by the geometric rate. They also show that under certain assumptions on the action-value function and the target network, the algorithm can converge to the optimal solution. "
11848,SP:a558ffa1706ef78893528c8c23e2295a79824d2f,"weighted sum USED-FOR semantics. attention mechanism USED-FOR natural language. PhraseTransformer HYPONYM-OF attention architecture. second phase USED-FOR inductive bias. WMT16 English - German translation task EVALUATE-FOR Transformer. WMT16 English - German translation task EVALUATE-FOR PhraseTransformer. BLEU EVALUATE-FOR Transformer. BLEU EVALUATE-FOR PhraseTransformer. Transformer COMPARE PhraseTransformer. PhraseTransformer COMPARE Transformer. WMT16 English - German translation task EVALUATE-FOR BLEU. OtherScientificTerm are self - attention, word compositions, compositional attentions, hypernodes, and non - linearity. Task is attention. Generic is first phase. Method is non - linear attention. ","This paper proposes a new attention mechanism for natural language translation. The proposed method, PhraseTransformer, is based on self-attention, which is an extension of Transformer. The key idea is to add a second phase to the first phase of the Transformer, where the second phase is a weighted sum of the first and second phase. The authors show that the proposed method outperforms Transformer on the WMT16 English-German translation task. ","This paper proposes a new attention mechanism for natural language translation. The proposed method, PhraseTransformer, is based on self-attention, which is an extension of Transformer. The key idea is to add a second phase to the first phase of the Transformer, where the second phase is a weighted sum of the first and second phase. The authors show that the proposed method outperforms Transformer on the WMT16 English-German translation task. "
11852,SP:622b0593972296a95b630a4ece1e959b60fec56c,"modular neural network architecture MAIN USED-FOR algorithms. input - output examples USED-FOR algorithms. neural controller USED-FOR variable - length input tape. neural controller PART-OF MAIN. general domain - agnostic mechanism USED-FOR selection of modules. general domain - agnostic mechanism USED-FOR MAIN. input tape layout CONJUNCTION parallel history tape. parallel history tape CONJUNCTION input tape layout. parallel history tape USED-FOR It. input tape layout USED-FOR It. memoryless controller USED-FOR it. input - output examples USED-FOR reinforcement learning. reinforcement learning USED-FOR MAIN architecture. it USED-FOR policies. algorithmic tasks EVALUATE-FOR MAIN. OtherScientificTerm are modules, random access, and tape locations. ",This paper proposes a modular neural network architecture called MAIN. MAIN is based on the idea of a memoryless controller that can be used to select modules from a set of input-output examples. The authors propose a general domain-agnostic mechanism for selection of modules. The proposed method is evaluated on reinforcement learning and reinforcement learning tasks. ,This paper proposes a modular neural network architecture called MAIN. MAIN is based on the idea of a memoryless controller that can be used to select modules from a set of input-output examples. The authors propose a general domain-agnostic mechanism for selection of modules. The proposed method is evaluated on reinforcement learning and reinforcement learning tasks. 
11856,SP:d668cc809e4f6b5f3330cf75cb5f71693a123c07,"Quantization USED-FOR Deep Neural Networks. Quantized floating point representations COMPARE fixed point representations. fixed point representations COMPARE Quantized floating point representations. Quantized floating point representations USED-FOR dynamic range. fixed point representations USED-FOR dynamic range. technique USED-FOR Deep Neural Networks. floating point arithmetic FEATURE-OF quantization. quantization FEATURE-OF Deep Neural Networks. floating point arithmetic USED-FOR Deep Neural Networks. Monte Carlo Arithmetic USED-FOR inference computation. relative standard deviation FEATURE-OF neural network loss. CIFAR-10 and ImageNet datasets EVALUATE-FOR pre - trained image classification models. pre - trained image classification models EVALUATE-FOR method. CIFAR-10 and ImageNet datasets EVALUATE-FOR method. loss of significance FEATURE-OF weight parameter sets. Metric is accuracy. OtherScientificTerm are parameter distributions, network topology, Monte Carlo trials, and network topologies. Method is MCDA. ","This paper studies the problem of quantization in deep neural networks. The authors propose a new quantization technique, called MCDA, which is based on Monte Carlo Arithmetic (MCDA). The key idea is to compute the relative standard deviation (RSD) of the neural network loss for each parameter in the weight parameter set, and then compute the loss of significance (LSD) for each of the parameters in the parameter set. They show that the proposed method is able to achieve better performance than previous quantization techniques. They also show that MCDA can be applied to the case where the network topology is not known. ","This paper studies the problem of quantization in deep neural networks. The authors propose a new quantization technique, called MCDA, which is based on Monte Carlo Arithmetic (MCDA). The key idea is to compute the relative standard deviation (RSD) of the neural network loss for each parameter in the weight parameter set, and then compute the loss of significance (LSD) for each of the parameters in the parameter set. They show that the proposed method is able to achieve better performance than previous quantization techniques. They also show that MCDA can be applied to the case where the network topology is not known. "
11860,SP:eda1d368aa3b4d806020c4c430a173d1ddd13d0d,"Model - based reinforcement learning ( MBRL ) USED-FOR data - efficiently learning control of continuous tasks. function approximators CONJUNCTION planning schemes. planning schemes CONJUNCTION function approximators. function approximators USED-FOR MBRL. planning schemes USED-FOR MBRL. dynamics models USED-FOR task. method USED-FOR mismatch issue. dynamics model training USED-FOR method. Method are MBRL framework, and forward dynamics model. Task are objective mismatch issue, and downstream control task. OtherScientificTerm are Objective mismatch, and objective mismatch. Metric is downstream control performance. ","This paper proposes a method to address the problem of objective mismatch in model-based reinforcement learning (MBRL). The authors propose to use a forward dynamics model to learn the dynamics model for the downstream control task, and then use a planning scheme to improve the downstream performance. The authors show that the proposed method is able to achieve better performance than existing methods on the downstream task. ","This paper proposes a method to address the problem of objective mismatch in model-based reinforcement learning (MBRL). The authors propose to use a forward dynamics model to learn the dynamics model for the downstream control task, and then use a planning scheme to improve the downstream performance. The authors show that the proposed method is able to achieve better performance than existing methods on the downstream task. "
11864,SP:63c452f2b2cbfeea0b45831bd7dc1ac26883fd9f,"CNN classifiers USED-FOR adversarial attacks. targeted blackbox transfer - based attack EVALUATE-FOR undefended ImageNet models. intermediate feature distributions FEATURE-OF CNNs. adversarial attacks USED-FOR intermediate feature distributions. Generic are network, and methodology. Task are adversarial attack, and attacking process. Method is CNN architecture. Metric is transferability. ","This paper presents a blackbox transfer-based attack against undefended ImageNet models. The proposed method is based on adversarial attacks on the intermediate feature distributions of CNNs. The authors show that the adversarial attack can be applied to any image classifier, and that the transferability of adversarial examples can be measured by the number of times the classifier is attacked. The paper also provides a theoretical analysis of the attack. ","This paper presents a blackbox transfer-based attack against undefended ImageNet models. The proposed method is based on adversarial attacks on the intermediate feature distributions of CNNs. The authors show that the adversarial attack can be applied to any image classifier, and that the transferability of adversarial examples can be measured by the number of times the classifier is attacked. The paper also provides a theoretical analysis of the attack. "
11868,SP:a7a2ded35804c381603a1196c7f7893fdf796c05,approach USED-FOR reinforcement learning policies. latent behavioral space FEATURE-OF Wasserstein distances ( WDs ). Wasserstein distances ( WDs ) USED-FOR approach. dual formulation USED-FOR score functions. dual formulation USED-FOR WD. score functions USED-FOR policy optimization. dual formulation USED-FOR algorithms. smoothed WDs CONJUNCTION dual formulation. dual formulation CONJUNCTION smoothed WDs. WD regularizers USED-FOR algorithms. Behavior - Guided Policy Gradient CONJUNCTION Behavior - Guided Evolution Strategies. Behavior - Guided Evolution Strategies CONJUNCTION Behavior - Guided Policy Gradient. regularizers PART-OF on - policy algorithms. Behavior - Guided Evolution Strategies COMPARE methods. methods COMPARE Behavior - Guided Evolution Strategies. Behavior - Guided Policy Gradient HYPONYM-OF on - policy algorithms. Behavior - Guided Evolution Strategies HYPONYM-OF on - policy algorithms. Method is demo1. ,"This paper proposes a dual formulation of the Wasserstein distance (WD) for reinforcement learning. The authors show that the dual formulation can be applied to both smoothed and smoothed WDs, which can be used to improve the performance of on-policy optimization algorithms. They also show that this dual formulation is able to be used as a regularizer to improve performance of existing algorithms. Experiments are conducted to show the effectiveness of the proposed method.","This paper proposes a dual formulation of the Wasserstein distance (WD) for reinforcement learning. The authors show that the dual formulation can be applied to both smoothed and smoothed WDs, which can be used to improve the performance of on-policy optimization algorithms. They also show that this dual formulation is able to be used as a regularizer to improve performance of existing algorithms. Experiments are conducted to show the effectiveness of the proposed method."
11872,SP:ef1c6403597c3a6083c1ad4256449325ac99416c,"deep neural networks PART-OF supervised learning. optimization algorithm USED-FOR deep learning. interpolation property USED-FOR optimization algorithm. it USED-FOR adaptive learning - rate. SGD USED-FOR ALI - G. learning - rate EVALUATE-FOR ALI - G. SGD COMPARE ALI - G. ALI - G COMPARE SGD. constant hyper - parameter USED-FOR ALI - G. constant hyper - parameter USED-FOR learning - rate. convergence guarantees FEATURE-OF ALI - G. ALI - G USED-FOR stochastic convex setting. convergence guarantees FEATURE-OF stochastic convex setting. wide residual networks CONJUNCTION densely connected networks. densely connected networks CONJUNCTION wide residual networks. architectures CONJUNCTION tasks. tasks CONJUNCTION architectures. SVHN data set EVALUATE-FOR wide residual network. CIFAR data sets EVALUATE-FOR wide residual networks. SNLI data set EVALUATE-FOR Bi - LSTM. CIFAR data sets EVALUATE-FOR densely connected networks. ALI - G COMPARE SGD. SGD COMPARE ALI - G. ALI - G COMPARE adaptive methods. adaptive methods COMPARE ALI - G. manually tuned learning - rate schedules USED-FOR SGD. ALI - G USED-FOR drop - in replacement. ALI - G PART-OF deep learning framework. OtherScientificTerm are empirical loss, Adaptive Learning - rates, and decay schedule. Method is differentiable neural computer. ","This paper studies the problem of adaptive learning rates for deep neural networks. The authors propose a new algorithm called Adaptive Learning Rates (ALI-G) which is based on the interpolation property of SGD. In particular, the authors show that under certain assumptions, the learning rate of the algorithm is bounded by a constant hyper-parameter. They also show that the algorithm converges to a stochastic convex setting. ","This paper studies the problem of adaptive learning rates for deep neural networks. The authors propose a new algorithm called Adaptive Learning Rates (ALI-G) which is based on the interpolation property of SGD. In particular, the authors show that under certain assumptions, the learning rate of the algorithm is bounded by a constant hyper-parameter. They also show that the algorithm converges to a stochastic convex setting. "
11876,SP:6e24a1e0aff73db6ae8558f114b644965e287e36,Forming perceptual groups CONJUNCTION individuating objects in visual scenes. individuating objects in visual scenes CONJUNCTION Forming perceptual groups. Forming perceptual groups USED-FOR visual intelligence. computations USED-FOR ability. connections USED-FOR perceptual grouping. high - level object cues USED-FOR perceptual grouping. synthetic visual tasks EVALUATE-FOR neural network architectures. learning USED-FOR networks. bottom - up connections USED-FOR networks. Horizontal connections USED-FOR straining. top - down connections USED-FOR learning. high - level object cues USED-FOR learning. model USED-FOR perceptual groups. interactions USED-FOR perceptual groups. interactions PART-OF model. Material is visual scenes. Generic is task. OtherScientificTerm is Gestalt cues. Task is incremental grouping. ,"This paper proposes a new task for perceptual grouping, which is based on Gestalt cues. The task is to learn a perceptual grouping model that is able to identify objects in a visual scene. The model is trained using top-down connections and bottom-up connections. Experiments show that the proposed method outperforms the state-of-the-art methods. ","This paper proposes a new task for perceptual grouping, which is based on Gestalt cues. The task is to learn a perceptual grouping model that is able to identify objects in a visual scene. The model is trained using top-down connections and bottom-up connections. Experiments show that the proposed method outperforms the state-of-the-art methods. "
11880,SP:7a0db1e8804defc5c04e0f4dd345272c6df1ff77,"` 1 or ` 0 regularizers USED-FOR weight sparsity. ` 0 regularizer USED-FOR parameter sparsity. complex optimization techniques USED-FOR it. gradient descent USED-FOR ` 1 regularizer. Hoyer measure USED-FOR compressed sensing problems. DeepHoyer HYPONYM-OF sparsity - inducing regularizers. DeepHoyer regularizers USED-FOR sparser neural network models. DeepHoyer USED-FOR element - wise and structural pruning. Task is sparse and efficient neural network models. Method is neural network models. OtherScientificTerm are scaling of parameter values, gradients, and sparsity. Metric are shrinking rate, and accuracy level. ","This paper studies the problem of parameter sparsity in neural network models. The authors propose a new regularizer called DeepHoyer, which is based on the Hoyer measure. The proposed regularizer is a combination of two existing regularizers: `1 regularizer and `0 regularizer. The paper shows that the proposed method is able to reduce the sparsity of a neural network model while maintaining its accuracy. ","This paper studies the problem of parameter sparsity in neural network models. The authors propose a new regularizer called DeepHoyer, which is based on the Hoyer measure. The proposed regularizer is a combination of two existing regularizers: `1 regularizer and `0 regularizer. The paper shows that the proposed method is able to reduce the sparsity of a neural network model while maintaining its accuracy. "
11884,SP:5ec05ac5d72e8e0b39b15a0cd7b2f5a64e861024,"data - dependent O(log T ) regret bound FEATURE-OF strongly convex functions. Adam USED-FOR data - dependent O(log T ) regret bound. controlled step size USED-FOR strong convexity. hyperparameters USED-FOR SAdam. RMSprop USED-FOR strongly convex functions. SC - RMSprop HYPONYM-OF RMSprop. SC - RMSprop USED-FOR SAdam. optimizing strongly convex functions CONJUNCTION deep networks. deep networks CONJUNCTION optimizing strongly convex functions. OtherScientificTerm are O ( √ T ) regret bound, and time horizon. Metric is data - dependent logarithmic regret bound. Generic is method. ",This paper studies the data-dependent O(log T) regret bound for strongly convex functions. The main contribution of the paper is to derive a new regret bound based on the RMSprop algorithm. The authors show that the regret bound is O(\log T^T) dependent on the step size and the time horizon. The regret bound can be extended to the case of strong convexity. ,This paper studies the data-dependent O(log T) regret bound for strongly convex functions. The main contribution of the paper is to derive a new regret bound based on the RMSprop algorithm. The authors show that the regret bound is O(\log T^T) dependent on the step size and the time horizon. The regret bound can be extended to the case of strong convexity. 
11888,SP:9f89501e6319280b4a14b674632a300805aa485c,BlockBERT HYPONYM-OF BERT model. BERT model USED-FOR modeling long - distance dependencies. BlockBERT USED-FOR modeling long - distance dependencies. memory consumption CONJUNCTION training time. training time CONJUNCTION memory consumption. attention heads USED-FOR shortor long - range contextual information. sparse block structures PART-OF attention matrix. BERT USED-FOR model. sparse block structures USED-FOR model. paragraph lengths FEATURE-OF benchmark question answering datasets. BlockBERT COMPARE BERT - based model. BERT - based model COMPARE BlockBERT. prediction accuracy EVALUATE-FOR BERT - based model. BlockBERT COMPARE RoBERTa. RoBERTa COMPARE BlockBERT. memory USED-FOR BlockBERT. RoBERTa HYPONYM-OF BERT - based model. training time EVALUATE-FOR BlockBERT. prediction accuracy EVALUATE-FOR BlockBERT. ,"This paper proposes a BERT-based model called BlockBERT for long-range contextual information modeling. The proposed model uses sparse block structures to model long-distance dependencies between short-term and long-term contextual information. The authors show that the proposed model outperforms RoBERTa in terms of prediction accuracy, training time, and memory consumption. ","This paper proposes a BERT-based model called BlockBERT for long-range contextual information modeling. The proposed model uses sparse block structures to model long-distance dependencies between short-term and long-term contextual information. The authors show that the proposed model outperforms RoBERTa in terms of prediction accuracy, training time, and memory consumption. "
11892,SP:0f04fc2e7966f4ba53909654fc0e8b90fc405f2a,"test accuracy EVALUATE-FOR pruning approaches. test accuracy EVALUATE-FOR pruning. generalization EVALUATE-FOR pruning. pruning CONJUNCTION regularizing. regularizing CONJUNCTION pruning. generalization FEATURE-OF over - parameterized networks. noise USED-FOR regularizing. noise USED-FOR pruning. OtherScientificTerm are Pruning neural network parameters, overfitting, and instability. Method is pruned model. ",This paper studies the problem of pruning neural network parameters to improve the generalization of over-parameterized networks. The authors propose two methods to prune the parameters of a neural network: (1) regularizing and (2) noise-based pruning. The regularizing method is based on the idea that the noise in the training data can be used as a regularizer for the pruned network. The noise is used to reduce the instability of the network. Experiments on MNIST and CIFAR-10 show that the proposed method is able to improve test accuracy and generalization. ,This paper studies the problem of pruning neural network parameters to improve the generalization of over-parameterized networks. The authors propose two methods to prune the parameters of a neural network: (1) regularizing and (2) noise-based pruning. The regularizing method is based on the idea that the noise in the training data can be used as a regularizer for the pruned network. The noise is used to reduce the instability of the network. Experiments on MNIST and CIFAR-10 show that the proposed method is able to improve test accuracy and generalization. 
11896,SP:dba3f5ec3af2a4a67ed4fc36b0f37fe556354177,"framework USED-FOR automatic discovering process of neural architectures. neural architecture search ( NAS ) algorithm USED-FOR automatic discovering process of neural architectures. neural architecture search ( NAS ) algorithm CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION neural architecture search ( NAS ) algorithm. neural architecture search ( NAS ) algorithm USED-FOR framework. architecture encoders CONJUNCTION decoders. decoders CONJUNCTION architecture encoders. reinforcement learning USED-FOR embedding space. reinforcement learning USED-FOR approach. decoders USED-FOR reinforcement learning. architecture encoders USED-FOR reinforcement learning. decoders USED-FOR approach. architecture encoders USED-FOR approach. NAS approaches USED-FOR image classification task. architecture network COMPARE NAS approaches. NAS approaches COMPARE architecture network. image classification task EVALUATE-FOR architecture network. CIFAR10 USED-FOR image classification task. CIFAR10 EVALUATE-FOR NAS approaches. NASES procedure USED-FOR architecture network. architecture - embedding searching CONJUNCTION pre - training controller. pre - training controller CONJUNCTION architecture - embedding searching. architecture - embedding searching USED-FOR NASES. pre - training controller USED-FOR NASES. parameter sharing HYPONYM-OF NAS tricks. architecture USED-FOR NASES procedure. Method are neural architectures, NAS in embedding space ( NASES ), and NAS with reinforcement learning approaches. OtherScientificTerm are noncontinuous and highdimensional search spaces, and discrete and high - dimensional architecture space. Task is optimization. ",This paper proposes a framework for neural architecture search (NAS) that combines reinforcement learning and reinforcement learning to improve the performance of NAS in embedding space (NASES). The main idea is to use reinforcement learning as a pre-training controller and architecture-embedding searching as a search procedure. The authors show that the proposed method outperforms the baselines on CIFAR10 and ImageNet.,This paper proposes a framework for neural architecture search (NAS) that combines reinforcement learning and reinforcement learning to improve the performance of NAS in embedding space (NASES). The main idea is to use reinforcement learning as a pre-training controller and architecture-embedding searching as a search procedure. The authors show that the proposed method outperforms the baselines on CIFAR10 and ImageNet.
11900,SP:e2e5bebccc76a51df3cb8b64572720da97174604,"Homotopy Training Algorithm ( HTA ) USED-FOR optimization problems. neural networks USED-FOR optimization problems. decoupled systems USED-FOR HTA. low dimensional structure FEATURE-OF decoupled systems. HTA USED-FOR continuous homotopy path. continuous homotopy path USED-FOR system. homotopy solution path USED-FOR convex case. HTA USED-FOR non - convex case. CIFAR-10 USED-FOR VGG models. VGG models HYPONYM-OF examples. accuracy EVALUATE-FOR HTA. examples EVALUATE-FOR HTA. HTA USED-FOR neural networks. HTA CONJUNCTION dropout technique. dropout technique CONJUNCTION HTA. dropout technique USED-FOR neural networks. OtherScientificTerm are high dimensional coupled system, and low dimensionality. ","This paper studies the problem of homotopy training for decoupled systems with low dimensionality. The authors propose a new homotopic training algorithm called Homotopy Training Algorithm (HTA), which is based on the idea that the solution of a convex optimization problem can be represented as a continuous homotope path. They show that HTA can be applied to the convex case and the non-convex case. They also propose a dropout technique to improve the performance of HTA. ","This paper studies the problem of homotopy training for decoupled systems with low dimensionality. The authors propose a new homotopic training algorithm called Homotopy Training Algorithm (HTA), which is based on the idea that the solution of a convex optimization problem can be represented as a continuous homotope path. They show that HTA can be applied to the convex case and the non-convex case. They also propose a dropout technique to improve the performance of HTA. "
11904,SP:5d9517fa62cd97b94ff45f645e100a8ad631e281,attention USED-FOR entity representations. dot - product attention USED-FOR higher - dimensional attention. higher - dimensional attention PART-OF Transformer. tensor products of value vectors USED-FOR entity representations. inductive bias USED-FOR logical reasoning. architecture USED-FOR inductive bias. logical reasoning PART-OF deep reinforcement learning. Method is 2 - simplicial Transformer. ,"This paper proposes a 2-simplicial Transformer architecture for deep reinforcement learning. The proposed architecture is based on dot product attention, which is an extension of dot-product attention. The authors show that the proposed architecture can be used to improve the inductive bias of deep RL models. ","This paper proposes a 2-simplicial Transformer architecture for deep reinforcement learning. The proposed architecture is based on dot product attention, which is an extension of dot-product attention. The authors show that the proposed architecture can be used to improve the inductive bias of deep RL models. "
11908,SP:f66721bf3eccf2e36444c2c41303e97745f10f0e,classification and regression approaches USED-FOR task. labelled data USED-FOR classification and regression approaches. labelled data USED-FOR pose estimation. circular latent representations USED-FOR 2D rotations. datasets USED-FOR method. labelled images FEATURE-OF datasets. Task is Pose estimation. OtherScientificTerm is fixed frame of reference. Method is Conditional Variational Autoencoders ( CVAEs ). ,This paper proposes a new method for pose estimation based on Conditional Variational Autoencoders (CVAEs). The proposed method is based on the idea of circular latent representations for 2D rotations. The authors show that the proposed method can be applied to a wide range of datasets. The method is evaluated on a variety of pose estimation tasks.,This paper proposes a new method for pose estimation based on Conditional Variational Autoencoders (CVAEs). The proposed method is based on the idea of circular latent representations for 2D rotations. The authors show that the proposed method can be applied to a wide range of datasets. The method is evaluated on a variety of pose estimation tasks.
11912,SP:87dc93d26ad5ad4a8dccde1780b5b127f391cfd6,"Evolutionary Population Curriculum ( EPC ) HYPONYM-OF curriculum learning paradigm. evolutionary approach USED-FOR objective misalignment issue. evolutionary approach USED-FOR EPC. approach COMPARE baselines. baselines COMPARE approach. MARL algorithm EVALUATE-FOR EPC. MADDPG HYPONYM-OF MARL algorithm. Task is multi - agent games. Metric is complexity. OtherScientificTerm are policies, agent population, curriculum, and scaled populations. Method is MultiAgent Reinforcement Learning ( MARL ). ","This paper proposes a curriculum learning approach for multi-agent reinforcement learning. The proposed approach is based on evolutionary population curriculum learning (EPC), which is an extension of MARL. The key idea of EPC is to learn a curriculum for each agent based on the current state of the population. The authors show that the proposed method is able to achieve better performance than the baselines in terms of the number of agents and the complexity of the algorithm. ","This paper proposes a curriculum learning approach for multi-agent reinforcement learning. The proposed approach is based on evolutionary population curriculum learning (EPC), which is an extension of MARL. The key idea of EPC is to learn a curriculum for each agent based on the current state of the population. The authors show that the proposed method is able to achieve better performance than the baselines in terms of the number of agents and the complexity of the algorithm. "
11916,SP:0ea5b3247ce031f25b98cf7d42bd4290020fbed2,MULTIPLEX GRAPH NETWORKS USED-FOR DIAGRAMMATIC REASONING. ,"This paper presents a new dataset of multi-image classification models for image classification. The dataset is based on the DIAGRAMMATIC dataset, which is a large-scale collection of image classification models. The authors show that the dataset contains a large number of models that can be used for classification. They also show that there is a significant gap between the number of classes that are classified in the dataset and those that are not.","This paper presents a new dataset of multi-image classification models for image classification. The dataset is based on the DIAGRAMMATIC dataset, which is a large-scale collection of image classification models. The authors show that the dataset contains a large number of models that can be used for classification. They also show that there is a significant gap between the number of classes that are classified in the dataset and those that are not."
11920,SP:9bcb840f867f1a7108aa22a7bb14c348fda52eb0,"Bayesian inference USED-FOR deep neural networks. agent exploration CONJUNCTION prediction fairness. prediction fairness CONJUNCTION agent exploration. decision making CONJUNCTION agent exploration. agent exploration CONJUNCTION decision making. calibrated measure of uncertainty USED-FOR decision making. It USED-FOR calibrated measure of uncertainty. calibrated measure of uncertainty USED-FOR agent exploration. calibrated measure of uncertainty USED-FOR prediction fairness. It USED-FOR overfitting. decision making CONJUNCTION prediction fairness. prediction fairness CONJUNCTION decision making. Markov Chain Monte Carlo ( MCMC ) methods USED-FOR Bayesian inference. model parameters FEATURE-OF posterior distribution. optimization methods USED-FOR large scale deep learning tasks. sampling methods COMPARE optimization methods. optimization methods COMPARE sampling methods. MCMC CONJUNCTION optimization methods. optimization methods CONJUNCTION MCMC. ATMC HYPONYM-OF adaptive noise MCMC algorithm. momentum CONJUNCTION noise. noise CONJUNCTION momentum. noise USED-FOR parameter update. momentum USED-FOR parameter update. momentum USED-FOR ATMC. noise USED-FOR ATMC. classification accuracy CONJUNCTION test log - likelihood. test log - likelihood CONJUNCTION classification accuracy. Cifar10 benchmark CONJUNCTION large scale ImageNet benchmark. large scale ImageNet benchmark CONJUNCTION Cifar10 benchmark. ATMC COMPARE optimization baseline. optimization baseline COMPARE ATMC. ResNet architecture USED-FOR ATMC. ResNet architecture CONJUNCTION batch normalization. batch normalization CONJUNCTION ResNet architecture. test log - likelihood EVALUATE-FOR optimization baseline. large scale ImageNet benchmark EVALUATE-FOR ATMC. classification accuracy EVALUATE-FOR optimization baseline. Cifar10 benchmark EVALUATE-FOR ATMC. test log - likelihood EVALUATE-FOR ATMC. classification accuracy EVALUATE-FOR ATMC. ATMC COMPARE optimization baseline. optimization baseline COMPARE ATMC. calibrated measure of uncertainty EVALUATE-FOR optimization baseline. ATMC USED-FOR overfitting. ATMC COMPARE ATMC. ATMC COMPARE ATMC. calibrated measure of uncertainty EVALUATE-FOR ATMC. OtherScientificTerm are hyperparameters, and","This paper proposes a new adaptive noise MCMC algorithm for Bayesian inference. The proposed method is based on the idea of adaptive momentum, which is an extension of momentum-based MCMC. The authors show that the proposed method outperforms existing methods on CIFAR-10 and ImageNet benchmarks. ","This paper proposes a new adaptive noise MCMC algorithm for Bayesian inference. The proposed method is based on the idea of adaptive momentum, which is an extension of momentum-based MCMC. The authors show that the proposed method outperforms existing methods on CIFAR-10 and ImageNet benchmarks. "
11924,SP:8cf0614f0fbd3756453304703d00776cfc9a4b9f,"accuracy EVALUATE-FOR latter. winning tickets USED-FOR dense, randomly initialized networks. small but critical subnetworks HYPONYM-OF winning tickets. early stopping CONJUNCTION low - precision training. low - precision training CONJUNCTION early stopping. large learning rates FEATURE-OF lowcost training schemes. low - precision training HYPONYM-OF lowcost training schemes. early stopping HYPONYM-OF lowcost training schemes. lowcost training schemes USED-FOR winning tickets. connectivity patterns FEATURE-OF neural networks. mask distance metric USED-FOR EB tickets. computational overhead EVALUATE-FOR mask distance metric. mask distance USED-FOR training methods. low - cost schemes USED-FOR EB tickets. method USED-FOR deep network training. mask distance USED-FOR them. deep networks CONJUNCTION datasets. datasets CONJUNCTION deep networks. accuracy EVALUATE-FOR training methods. Method is train - prune - retrain process. OtherScientificTerm is Early - Bird ( EB ) tickets. Metric is energy savings. Generic is state - ofthe - art training methods. ","This paper studies the problem of early-bird (EB) tickets for deep neural networks. EB tickets are the winning tickets for dense, randomly initialized networks. The authors propose a new mask distance metric for EB tickets that can be used to compare the performance of different low-precision training methods. The paper shows that EB tickets can reduce the computational overhead compared to other low-probability training methods in terms of the number of training iterations. ","This paper studies the problem of early-bird (EB) tickets for deep neural networks. EB tickets are the winning tickets for dense, randomly initialized networks. The authors propose a new mask distance metric for EB tickets that can be used to compare the performance of different low-precision training methods. The paper shows that EB tickets can reduce the computational overhead compared to other low-probability training methods in terms of the number of training iterations. "
11928,SP:8aeece75c839643a02d2b3b5f3aca7cb76cf1d35,"adversarial examples FEATURE-OF deep convolutional neural networks. intrinsic dimension COMPARE pixel space dimension. pixel space dimension COMPARE intrinsic dimension. low - dimensional space USED-FOR classification. intrinsic dimension FEATURE-OF image data. image data COMPARE pixel space dimension. pixel space dimension COMPARE image data. high - dimensional input images USED-FOR classification. high - dimensional input images USED-FOR low - dimensional space. vulnerability EVALUATE-FOR neural networks. robustness EVALUATE-FOR deep neural networks. adversarial robustness EVALUATE-FOR classifier. benchmark datasets EVALUATE-FOR framework. benchmark datasets EVALUATE-FOR strong adversarial attack methods. strong adversarial attack methods EVALUATE-FOR framework. OtherScientificTerm are input dimension, lowdimensional space, regularization, and embedding regularization. Method is Embedding Regularized Classifier ( ER - Classifier ). ","This paper studies the problem of adversarial robustness of deep convolutional neural networks in low-dimensional space. The authors propose a new regularization method to improve the robustness to adversarial examples. The proposed method is based on the idea of embedding regularization, which is an extension of the embedding-regularized classifier (ER-classifier) framework. The method is evaluated on several benchmark datasets and shows that it can improve the performance of the classifier.","This paper studies the problem of adversarial robustness of deep convolutional neural networks in low-dimensional space. The authors propose a new regularization method to improve the robustness to adversarial examples. The proposed method is based on the idea of embedding regularization, which is an extension of the embedding-regularized classifier (ER-classifier) framework. The method is evaluated on several benchmark datasets and shows that it can improve the performance of the classifier."
11932,SP:efd68097f47dbfdd0208573071686a62240d1b12,"Named entity recognition ( NER ) CONJUNCTION relation extraction ( RE ). relation extraction ( RE ) CONJUNCTION Named entity recognition ( NER ). Named entity recognition ( NER ) HYPONYM-OF tasks. relation extraction ( RE ) HYPONYM-OF tasks. dependency parsers HYPONYM-OF external natural language processing ( NLP ) tools. external natural language processing ( NLP ) tools USED-FOR joint models. neural, end - to - end model USED-FOR jointly extracting entities. large, pre - trained language model USED-FOR neural, end - to - end model. recurrence USED-FOR self - attention. OtherScientificTerm is propagation of error. Method are pipeline - based systems, neural, end - to - end models, and external NLP tools. Generic are tools, and model. ","This paper proposes a new method for jointly learning a language model and an external language model for NER and relation extraction tasks. The proposed method is based on the notion of self-attention, which is an extension of the idea of recurrence. The authors show that the proposed method can be applied to a number of tasks such as NER, relation extraction, and dependency parsers. They also show that their method is able to learn a large pre-trained language model that can be used to jointly learn a neural model and external language models. ","This paper proposes a new method for jointly learning a language model and an external language model for NER and relation extraction tasks. The proposed method is based on the notion of self-attention, which is an extension of the idea of recurrence. The authors show that the proposed method can be applied to a number of tasks such as NER, relation extraction, and dependency parsers. They also show that their method is able to learn a large pre-trained language model that can be used to jointly learn a neural model and external language models. "
11936,SP:8fd4f3f8615c0a7a76ec7bfe996d2ead803f7828,"network USED-FOR task. informative representation USED-FOR tasks. network USED-FOR informative representation. image context FEATURE-OF RGB values of images. RGB values of images HYPONYM-OF low - level feature representation. DNNs USED-FOR Euclidean representation. DNNs USED-FOR algorithm. problem PART-OF machine learning. Ordinal Embedding HYPONYM-OF machine learning. approaches USED-FOR problem. approach COMPARE methods. methods COMPARE approach. real - world large datasets EVALUATE-FOR methods. real - world large datasets EVALUATE-FOR approach. Method are supervised / unsupervised DNNs, and neural networks. OtherScientificTerm is triplet comparisons. Task is unsupervised learning problems. ","This paper proposes a novel method for learning a low-level feature representation for unsupervised learning tasks. The proposed method is based on the idea of Ordinal Embedding (OED), which is an important problem in machine learning. OED is a generalization of the Euclidean representation of the triplet comparisons problem. The authors propose a new algorithm for OED, which uses a neural network to learn a representation of triplet images. The paper shows that the proposed method outperforms the state-of-the-art methods on several benchmark datasets.","This paper proposes a novel method for learning a low-level feature representation for unsupervised learning tasks. The proposed method is based on the idea of Ordinal Embedding (OED), which is an important problem in machine learning. OED is a generalization of the Euclidean representation of the triplet comparisons problem. The authors propose a new algorithm for OED, which uses a neural network to learn a representation of triplet images. The paper shows that the proposed method outperforms the state-of-the-art methods on several benchmark datasets."
11940,SP:12e7f417a7ef1ccafccff5ffb3f8f11cd2c05b20,"domain adaptation CONJUNCTION corrupted sample discovery. corrupted sample discovery CONJUNCTION domain adaptation. building insights about the learning task CONJUNCTION domain adaptation. domain adaptation CONJUNCTION building insights about the learning task. corrupted sample discovery CONJUNCTION robust learning. robust learning CONJUNCTION corrupted sample discovery. Data Valuation HYPONYM-OF meta learning framework. Reinforcement Learning ( DVRL ) USED-FOR meta learning framework. deep neural network USED-FOR data value estimator. reinforcement signal USED-FOR data value estimator. DVRL COMPARE methods. methods COMPARE DVRL. DVRL USED-FOR data value estimates. data value estimates EVALUATE-FOR methods. domain adaptation CONJUNCTION robust learning. robust learning CONJUNCTION domain adaptation. DVRL COMPARE state - of - the - art. state - of - the - art COMPARE DVRL. DVRL USED-FOR corrupted sample discovery. DVRL USED-FOR robust learning. DVRL USED-FOR domain adaptation. Task are machine learning, and Data valuation. OtherScientificTerm is data values. Method are task predictor model, and predictor model. ","This paper proposes a meta learning framework for data value estimation. The proposed method is based on Reinforcement Learning (DVRL), which is a reinforcement learning framework that leverages a deep neural network to learn a data value estimator and a task predictor model to predict the value of the data. The model is trained on a set of tasks, and the task predictor is used to learn the data value estimate. The method is evaluated on a variety of datasets, including corrupted sample discovery, domain adaptation, and robust learning. ","This paper proposes a meta learning framework for data value estimation. The proposed method is based on Reinforcement Learning (DVRL), which is a reinforcement learning framework that leverages a deep neural network to learn a data value estimator and a task predictor model to predict the value of the data. The model is trained on a set of tasks, and the task predictor is used to learn the data value estimate. The method is evaluated on a variety of datasets, including corrupted sample discovery, domain adaptation, and robust learning. "
11944,SP:e2c3374629cfd654b7b35e88507e65646d70470e,"network ’s architecture CONJUNCTION initialization parameters. initialization parameters CONJUNCTION network ’s architecture. gradient FEATURE-OF random fully connected ReLU networks. finite sized networks FEATURE-OF per layer Jacobian. initialization parameters PART-OF network. ResNets CONJUNCTION DenseNets. DenseNets CONJUNCTION ResNets. vanilla networks CONJUNCTION ResNets. ResNets CONJUNCTION vanilla networks. vanilla networks HYPONYM-OF architectures. DenseNets HYPONYM-OF architectures. ResNets HYPONYM-OF architectures. arbitrary depths FEATURE-OF norm. architecture CONJUNCTION layer ’s size. layer ’s size CONJUNCTION architecture. Method are neural networks, initialization strategy, and deep networks. Task is gradient steps post - initialization. OtherScientificTerm are Jacobian squared norm, exploding or decaying gradients, per layer Jacobian norm, and layer ’s depth. ","This paper studies the per-layer Jacobian squared norm of random fully connected ReLU networks. The authors show that the per layer Jacobian norm of a random ReLU network is a function of the initialization parameters of the network and the initialization strategy of the neural network. They also show that for finite-sized networks, exploding or decaying gradients are more likely to occur in the initialization step than in the entire initialization step. ","This paper studies the per-layer Jacobian squared norm of random fully connected ReLU networks. The authors show that the per layer Jacobian norm of a random ReLU network is a function of the initialization parameters of the network and the initialization strategy of the neural network. They also show that for finite-sized networks, exploding or decaying gradients are more likely to occur in the initialization step than in the entire initialization step. "
11948,SP:4463645f1a9abfbf472935d9eb3342919aa4e0f4,"hand - optimized libraries CONJUNCTION compilation heuristics. compilation heuristics CONJUNCTION hand - optimized libraries. genetic algorithms CONJUNCTION stochastic methods. stochastic methods CONJUNCTION genetic algorithms. compilation heuristics CONJUNCTION genetic algorithms. genetic algorithms CONJUNCTION compilation heuristics. hand - optimized libraries USED-FOR neural networks. solution USED-FOR code optimization. reinforcement learning USED-FOR CHAMELEON. reinforcement learning USED-FOR solution. domain - knowledge inspired logic USED-FOR adaptive sampling algorithm. CHAMELEON COMPARE AutoTVM. AutoTVM COMPARE CHAMELEON. optimization time EVALUATE-FOR AutoTVM. real hardware EVALUATE-FOR CHAMELEON. inference time EVALUATE-FOR deep networks. optimization time EVALUATE-FOR CHAMELEON. Metric is compilation time. Generic are methods, and them. OtherScientificTerm are hardware measurements, design space, and real hardware measurements. Task is search. ",This paper proposes a reinforcement learning-based adaptive sampling algorithm for code optimization. The algorithm is based on domain-knowledge inspired logic. The authors show that the proposed algorithm outperforms AutoTVM in terms of optimization time and inference time.,This paper proposes a reinforcement learning-based adaptive sampling algorithm for code optimization. The algorithm is based on domain-knowledge inspired logic. The authors show that the proposed algorithm outperforms AutoTVM in terms of optimization time and inference time.
11952,SP:df8483206bb88debeb24b04eb31e016368792a84,adversarial perturbations FEATURE-OF classifiers. certified robustnesses FEATURE-OF top-1 predictions. top - k predictions PART-OF real - world applications. certified robustness USED-FOR top - k predictions. randomized smoothing USED-FOR certified robustness. it USED-FOR large - scale neural networks. it USED-FOR classifier. randomized smoothing USED-FOR classifier. ` 2 norm FEATURE-OF topk predictions. tight robustness FEATURE-OF topk predictions. ` 2 norm FEATURE-OF tight robustness. Gaussian noise USED-FOR randomized smoothing. randomized smoothing USED-FOR tight robustness. top - k predictions FEATURE-OF certified robustness. CIFAR10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR10. ImageNet EVALUATE-FOR method. CIFAR10 EVALUATE-FOR method. method USED-FOR ImageNet classifier. ` 2 - norms FEATURE-OF adversarial perturbations. top-5 accuracy EVALUATE-FOR ImageNet classifier. OtherScientificTerm is noise. ,This paper proposes a method to improve the certified robustness of classifiers against adversarial perturbations. The proposed method is based on the randomized smoothing method. The method is evaluated on CIFAR-10 and ImageNet. The authors show that the proposed method can achieve tight robustness for top-k predictions. ,This paper proposes a method to improve the certified robustness of classifiers against adversarial perturbations. The proposed method is based on the randomized smoothing method. The method is evaluated on CIFAR-10 and ImageNet. The authors show that the proposed method can achieve tight robustness for top-k predictions. 
11956,SP:84a83ee258d5bc613b7d73045477018b8a56c56d,gradient signal to noise ratio ( GSNR ) USED-FOR DNNs. GSNR EVALUATE-FOR parameter. GSNR CONJUNCTION generalization gap. generalization gap CONJUNCTION GSNR. logistic regression CONJUNCTION support vector machines. support vector machines CONJUNCTION logistic regression. gradient descent optimization dynamics USED-FOR DNNs. gradient descent optimization dynamics USED-FOR GSNR. support vector machines HYPONYM-OF shallow models. logistic regression HYPONYM-OF shallow models. Method is deep neural networks ( DNNs ). OtherScientificTerm is data distribution. ,"This paper studies the gradient signal to noise ratio (GSNR) of deep neural networks (DNNs). The authors show that GSNR is a generalization gap measure for DNNs. The authors also show that the generalisation gap is a function of the number of samples in the data distribution, and that it depends on the gradient descent optimization dynamics of the DNN.  The authors then propose a new parameter, called the GSMR, that can be used to measure the generalization of a DNN to a new data distribution. The proposed parameter is used as a regularizer in the training of DNN models. ","This paper studies the gradient signal to noise ratio (GSNR) of deep neural networks (DNNs). The authors show that GSNR is a generalization gap measure for DNNs. The authors also show that the generalisation gap is a function of the number of samples in the data distribution, and that it depends on the gradient descent optimization dynamics of the DNN.  The authors then propose a new parameter, called the GSMR, that can be used to measure the generalization of a DNN to a new data distribution. The proposed parameter is used as a regularizer in the training of DNN models. "
11960,SP:fb726f0fea2ed1a009b3aacf74ac149bcf988cdd,"approach USED-FOR problem. KG entities PART-OF vector space. conjunctions CONJUNCTION existential quantifiers. existential quantifiers CONJUNCTION conjunctions. existential quantifiers USED-FOR queries. conjunctions USED-FOR queries. logical disjunctions ( ∨ ) USED-FOR Handling queries. QUERY2BOX HYPONYM-OF embedding - based framework. embedding - based framework USED-FOR massive and incomplete KGs. QUERY2BOX USED-FOR arbitrary logical queries. Disjunctive Normal Form USED-FOR QUERY2BOX. Disjunctive Normal Form FEATURE-OF queries. ∧ FEATURE-OF arbitrary logical queries. QUERY2BOX COMPARE state of the art. state of the art COMPARE QUERY2BOX. QUERY2BOX COMPARE QUERY2BOX. QUERY2BOX COMPARE QUERY2BOX. KGs EVALUATE-FOR QUERY2BOX. Task is Answering complex logical queries. OtherScientificTerm are complex query, hyper - rectangles, and disjunctions. ","This paper proposes a new approach to solve the problem of solving complex logical queries in a vector space. The key idea is to use logical disjunctions (∨) to handle complex queries. The authors propose a new embedding-based framework called QUERY2BOX, which is able to handle large and incomplete KGs. Experiments show that the proposed method outperforms state-of-the-art baselines.","This paper proposes a new approach to solve the problem of solving complex logical queries in a vector space. The key idea is to use logical disjunctions (∨) to handle complex queries. The authors propose a new embedding-based framework called QUERY2BOX, which is able to handle large and incomplete KGs. Experiments show that the proposed method outperforms state-of-the-art baselines."
11964,SP:c8bbdbf038ddec801c931ae9399b8c16b08428bc,"approaches USED-FOR stochastic optimization. Stochastic gradient descent ( SGD ) USED-FOR stochastic optimization. Stochastic gradient descent ( SGD ) HYPONYM-OF approaches. machine learning USED-FOR convex loss functions. machine learning USED-FOR nonconvex deep neural networks. machine learning USED-FOR SGD. unbiased estimator COMPARE full gradient. full gradient COMPARE unbiased estimator. consistent estimators COMPARE unbiased ones. unbiased ones COMPARE consistent estimators. convergence behavior EVALUATE-FOR consistent estimators. training algorithms USED-FOR large - scale graphs. consistent estimators USED-FOR SGD updates. Method are unbiased gradient estimator, empirical risk minimization, and consistent gradient estimator. OtherScientificTerm are graphs, and convex, convex, and nonconvex objectives. Material is synthetic and real - world data. ","This paper considers the problem of learning convex loss functions for nonconvex optimization problems. The authors consider the setting where the objective is convex and the objective function is a convex convex function, and the goal is to minimize the risk minimization of the convex objective. They show that the gradient estimator for convex optimization is consistent with the full gradient. They also show that consistent estimators can be used to train convex neural networks. ","This paper considers the problem of learning convex loss functions for nonconvex optimization problems. The authors consider the setting where the objective is convex and the objective function is a convex convex function, and the goal is to minimize the risk minimization of the convex objective. They show that the gradient estimator for convex optimization is consistent with the full gradient. They also show that consistent estimators can be used to train convex neural networks. "
11968,SP:d53ee573b8083ecf891d4d560eb8a54c30c5cb3a,resource constraints FEATURE-OF inference. neural architecture search ( NAS ) USED-FOR specialized neural network. neural architecture search ( NAS ) USED-FOR approaches. OFA network USED-FOR specialized sub - network. width CONJUNCTION kernel size. kernel size CONJUNCTION width. depth CONJUNCTION width. width CONJUNCTION depth. kernel size CONJUNCTION resolution. resolution CONJUNCTION kernel size. progressive shrinking algorithm HYPONYM-OF generalized pruning method. generalized pruning method COMPARE pruning. pruning COMPARE generalized pruning method. depth HYPONYM-OF pruning. progressive shrinking algorithm USED-FOR OFA networks. width HYPONYM-OF pruning. kernel size HYPONYM-OF pruning. It USED-FOR subnetworks. hardware platforms CONJUNCTION latency constraints. latency constraints CONJUNCTION hardware platforms. accuracy EVALUATE-FOR It. MobileNetV3 COMPARE EfficientNet. EfficientNet COMPARE MobileNetV3. OFA COMPARE MobileNetV3. MobileNetV3 COMPARE OFA. OFA COMPARE EfficientNet. EfficientNet COMPARE OFA. ImageNet top1 accuracy EVALUATE-FOR MobileNetV3. MobileNetV3 COMPARE MobileNetV3. MobileNetV3 COMPARE MobileNetV3. CO2 emission EVALUATE-FOR OFA. ImageNet top1 accuracy EVALUATE-FOR OFA. accuracy EVALUATE-FOR OFA. SOTA EVALUATE-FOR OFA. DSP classification track CONJUNCTION LPCVC. LPCVC CONJUNCTION DSP classification track. classification track CONJUNCTION detection track. detection track CONJUNCTION classification track. LPCVC CONJUNCTION classification track. classification track CONJUNCTION LPCVC. detection track HYPONYM-OF LPCVC. LPCVC EVALUATE-FOR OFA. DSP classification track EVALUATE-FOR OFA. Code CONJUNCTION pre - trained models. pre - trained models CONJUNCTION Code. Material is edge devices. Generic is it. OtherScientificTerm is diverse architectural settings. Metric is ImageNet top-1 accuracy. ,"This paper proposes a novel pruning method for neural architecture search (NAS) for deep neural networks. The authors propose a progressive shrinking algorithm for OFA networks that prunes the width, depth, and kernel size of the OFA network. They also propose a generalized pruning algorithm to prune the kernel size and resolution of OFA. They show that the proposed method is able to achieve comparable performance to MobileNetV3 and EfficientNet. ","This paper proposes a novel pruning method for neural architecture search (NAS) for deep neural networks. The authors propose a progressive shrinking algorithm for OFA networks that prunes the width, depth, and kernel size of the OFA network. They also propose a generalized pruning algorithm to prune the kernel size and resolution of OFA. They show that the proposed method is able to achieve comparable performance to MobileNetV3 and EfficientNet. "
11972,SP:1be944b5f82d33ab1feb5639792a4c06b8f0c85a,"discrete, symbolic operations USED-FOR they. Neural module networks ( NMNs ) USED-FOR questions. Neural module networks ( NMNs ) USED-FOR executable programs. modules PART-OF executable programs. synthetic visual QA domains EVALUATE-FOR Neural module networks ( NMNs ). models USED-FOR non - synthetic questions. model USED-FOR reasoning. model USED-FOR natural language. open - domain text USED-FOR non - synthetic questions. sorting CONJUNCTION counting. counting CONJUNCTION sorting. arithmetic CONJUNCTION sorting. sorting CONJUNCTION arithmetic. modules USED-FOR symbolic reasoning. probabilistic and differentiable manner USED-FOR symbolic reasoning. counting HYPONYM-OF symbolic reasoning. arithmetic HYPONYM-OF symbolic reasoning. sorting HYPONYM-OF symbolic reasoning. heuristically - obtained question program CONJUNCTION intermediate module output supervision. intermediate module output supervision CONJUNCTION heuristically - obtained question program. inductive bias USED-FOR learning. heuristically - obtained question program USED-FOR learning. intermediate module output supervision USED-FOR inductive bias. heuristically - obtained question program USED-FOR inductive bias. intermediate module output supervision USED-FOR learning. model COMPARE models. models COMPARE model. DROP dataset EVALUATE-FOR models. DROP dataset EVALUATE-FOR model. Task is Answering compositional questions. Method are NMNs, and unsupervised auxiliary loss. ","This paper proposes a neural module network (NMN) model for solving synthetic visual QA tasks. The proposed model is based on a probabilistic and differentiable model, which is able to learn a heuristically-referred question program that can be used to answer a synthetic question. The model is evaluated on the DROP dataset, where it is shown to outperform the state-of-the-art models. ","This paper proposes a neural module network (NMN) model for solving synthetic visual QA tasks. The proposed model is based on a probabilistic and differentiable model, which is able to learn a heuristically-referred question program that can be used to answer a synthetic question. The model is evaluated on the DROP dataset, where it is shown to outperform the state-of-the-art models. "
11976,SP:319922e4a316a9b9e76504f806d30ea3bffa3f99,"Network pruning USED-FOR compressing deep neural networks. approach USED-FOR pruning. connection sensitivity HYPONYM-OF saliency criterion. saliency criterion USED-FOR pruning. initialization conditions USED-FOR connection sensitivity measurements. gradient FEATURE-OF connection sensitivity. signal propagation properties FEATURE-OF pruned networks. image classification tasks EVALUATE-FOR network models. signal propagation perspective CONJUNCTION unsupervised pruning. unsupervised pruning CONJUNCTION signal propagation perspective. pruning USED-FOR arbitrarily - designed architectures. supervision USED-FOR pruning. Method are deep neural networks, data - free method, and pruning at initialization method. Generic is model. OtherScientificTerm is redundant parameters. ","This paper proposes a data-free method for pruning deep neural networks. The method is based on the idea of saliency criterion, which is used to evaluate the connection sensitivity of a network. The authors show that the proposed method can be used to prune networks without supervision.","This paper proposes a data-free method for pruning deep neural networks. The method is based on the idea of saliency criterion, which is used to evaluate the connection sensitivity of a network. The authors show that the proposed method can be used to prune networks without supervision."
11980,SP:d5899cba36329d863513b91c2db57675086abc49,"deep neural networks PART-OF machine learning research. training ‘ a priori ’ sparse networks USED-FOR method. training ‘ a priori ’ sparse networks USED-FOR layers. information bandwidth FEATURE-OF layers. sparse topology USED-FOR networks. data - free heuristic USED-FOR topology. Task is fast training. OtherScientificTerm are dense and convolutional layers, memory, and intra - layer topology. Method is sparse neural network initialization scheme. Generic are topologies, and network. ",This paper proposes a new initialization scheme for sparse neural networks. The idea is to use a data-free heuristic to learn the topology of each layer of a sparse network. The authors show that this heuristic can be used to train sparse networks with a priori knowledge of the information bandwidth of the layers. They also show that their method can be applied to a number of sparse networks. ,This paper proposes a new initialization scheme for sparse neural networks. The idea is to use a data-free heuristic to learn the topology of each layer of a sparse network. The authors show that this heuristic can be used to train sparse networks with a priori knowledge of the information bandwidth of the layers. They also show that their method can be applied to a number of sparse networks. 
11984,SP:b05a6a0f05dcc63a7e17233f20c49c465c46d194,"recurrent neural networks ( RNNs ) USED-FOR long sequence tasks. exponential explosion CONJUNCTION vanishing of signals. vanishing of signals CONJUNCTION exponential explosion. LSTM CONJUNCTION GRU. GRU CONJUNCTION LSTM. RNN architectures COMPARE vanilla RNN cells. vanilla RNN cells COMPARE RNN architectures. GRU HYPONYM-OF RNN architectures. LSTM HYPONYM-OF RNN architectures. LSTMs CONJUNCTION GRUs. GRUs CONJUNCTION LSTMs. time scales CONJUNCTION spectral properties. spectral properties CONJUNCTION time scales. spectral properties FEATURE-OF state - to - state Jacobians. mean field theory USED-FOR signal propagation. mean field theory USED-FOR time scales. mean field theory USED-FOR GRUs. time scales USED-FOR signal propagation. LSTMs FEATURE-OF signal propagation. initialization scheme USED-FOR training instabilities. quantities USED-FOR initialization scheme. initialization hyperparameters USED-FOR quantities. it COMPARE initialization. initialization COMPARE it. it EVALUATE-FOR initialization scheme. multiple sequence tasks EVALUATE-FOR initialization scheme. generalization performance EVALUATE-FOR initialization. Generic are network, and they. Method is algorithmic and architectural modifications. OtherScientificTerm is instabilities. ","This paper studies the initialization of recurrent neural networks (RNNs) in the presence of training instabilities. The authors propose a new initialization scheme based on mean field theory for RNNs. They show that the proposed initialization scheme is able to improve the generalization performance of existing RNN architectures on multiple sequence tasks. They also show that this initialization scheme can be applied to LSTMs, GRUs, and GRU.","This paper studies the initialization of recurrent neural networks (RNNs) in the presence of training instabilities. The authors propose a new initialization scheme based on mean field theory for RNNs. They show that the proposed initialization scheme is able to improve the generalization performance of existing RNN architectures on multiple sequence tasks. They also show that this initialization scheme can be applied to LSTMs, GRUs, and GRU."
11988,SP:7b65eb83b0d3149f788ab11b1ab9057b440ddd57,"intra - class diversity CONJUNCTION inter - class similarity. inter - class similarity CONJUNCTION intra - class diversity. deep learning algorithms USED-FOR classification tasks. intra - class diversity FEATURE-OF remote sensing scene image data sets. neural network USED-FOR smoothing operation. neural network USED-FOR approaches. neighboring scene images USED-FOR deep features. siamese network USED-FOR discriminative power. siamese network USED-FOR convolutional neural networks. discriminative power FEATURE-OF convolutional neural networks. neighboring scene images USED-FOR convolutional neural networks. siamese network USED-FOR approach. semantic coherence USED-FOR feature vector. It USED-FOR feature vector. semantic coherence USED-FOR It. approach COMPARE methods. methods COMPARE approach. model COMPARE baseline. baseline COMPARE model. mean squared error value EVALUATE-FOR baseline. disease density estimation task EVALUATE-FOR baseline. mean squared error value EVALUATE-FOR model. disease density estimation task EVALUATE-FOR model. prediction accuracy EVALUATE-FOR model. Metric is accuracy. Method are post - classification methods, and cleanup model. OtherScientificTerm is overhead. Generic is task. ","This paper proposes a new method for post-classification of remote sensing data. The proposed method is based on the siamese network, which is used for smoothing the smoothing operation of the feature vector. The authors show that the proposed method outperforms existing methods in terms of prediction accuracy, intra-class diversity, and inter-class similarity. The method is evaluated on the disease density estimation task and is shown to outperform existing methods.","This paper proposes a new method for post-classification of remote sensing data. The proposed method is based on the siamese network, which is used for smoothing the smoothing operation of the feature vector. The authors show that the proposed method outperforms existing methods in terms of prediction accuracy, intra-class diversity, and inter-class similarity. The method is evaluated on the disease density estimation task and is shown to outperform existing methods."
11992,SP:99c10e038939aa88fc112db10fe801b42360c8dc,"Self - supervised learning USED-FOR monocular depth estimation. geometry USED-FOR supervision. geometry USED-FOR Self - supervised learning. Depth networks USED-FOR representations. representations USED-FOR visual appearance. category - level patterns USED-FOR representations. semantic structure USED-FOR geometric representation learning. fixed pretrained semantic segmentation networks USED-FOR self - supervised representation learning. semantic labels CONJUNCTION proxy losses. proxy losses CONJUNCTION semantic labels. architecture USED-FOR self - supervised representation learning. semantic labels USED-FOR multi - task approach. proxy losses USED-FOR multi - task approach. semantic labels USED-FOR architecture. pixel - adaptive convolutions USED-FOR architecture. fixed pretrained semantic segmentation networks USED-FOR architecture. pixel - adaptive convolutions USED-FOR self - supervised representation learning. two - stage training process USED-FOR common semantic bias. common semantic bias FEATURE-OF dynamic objects. resampling USED-FOR two - stage training process. resampling USED-FOR common semantic bias. method COMPARE state of the art. state of the art COMPARE method. state of the art USED-FOR self - supervised monocular depth prediction. method USED-FOR self - supervised monocular depth prediction. OtherScientificTerm are 3D properties, self - supervised regime, and fine - grained details. ",This paper proposes a self-supervised monocular depth estimation method based on the semantic segmentation network. The proposed method is based on pixel-adaptive convolutional neural networks (PNNs) and a two-stage training process. The authors show that the proposed method outperforms state-of-the-art methods in terms of accuracy and generalization. ,This paper proposes a self-supervised monocular depth estimation method based on the semantic segmentation network. The proposed method is based on pixel-adaptive convolutional neural networks (PNNs) and a two-stage training process. The authors show that the proposed method outperforms state-of-the-art methods in terms of accuracy and generalization. 
11996,SP:e98ec7fd9c27eabd7f5bf3429f984034c2d355a2,label - flipping attacks HYPONYM-OF data poisoning attack. deep features USED-FOR linear classifiers. test - time robustness EVALUATE-FOR technique. randomized smoothing USED-FOR approach. Dogfish binary classification task EVALUATE-FOR baseline undefended classifier. ImageNet FEATURE-OF Dogfish binary classification task. certified accuracy EVALUATE-FOR classifier. accuracy EVALUATE-FOR baseline undefended classifier. multi - class classification algorithm USED-FOR label - flipping attacks. Task is label - flipping. Method is classification. Material is multi - class case. ,This paper proposes a multi-class classification algorithm to defend against label-flipping attacks. The proposed method is based on randomized smoothing. The method is tested on the Dogfish binary classification task and shows improved test-time robustness compared to the baseline undefended classifier. ,This paper proposes a multi-class classification algorithm to defend against label-flipping attacks. The proposed method is based on randomized smoothing. The method is tested on the Dogfish binary classification task and shows improved test-time robustness compared to the baseline undefended classifier. 
12000,SP:795cdeb7e4f7285f2c1ac9b9a0fbac3039201ed5,"Outlier detection CONJUNCTION novelty detection. novelty detection CONJUNCTION Outlier detection. novelty detection USED-FOR anomaly detection. Outlier detection PART-OF anomaly detection. outlier detection CONJUNCTION novelty detection. novelty detection CONJUNCTION outlier detection. backdoor poisoning attacks USED-FOR machine learning models. Differential privacy USED-FOR aggregated analysis. dataset USED-FOR aggregated analysis. random noise USED-FOR It. differential privacy USED-FOR outlier detection. differential privacy USED-FOR novelty detection. outlier detection CONJUNCTION novelty detection. novelty detection CONJUNCTION outlier detection. extension USED-FOR poisoning samples. poisoning samples PART-OF backdoor attacks. outlier detection CONJUNCTION novelty detection. novelty detection CONJUNCTION outlier detection. novelty detection CONJUNCTION backdoor attack detection. backdoor attack detection CONJUNCTION novelty detection. differential privacy USED-FOR detection. differential privacy USED-FOR outlier detection. differential privacy USED-FOR backdoor attack detection. differential privacy USED-FOR novelty detection. OtherScientificTerm are distribution, Outliers, novelties, and outliers. Method is aggregation mechanism. ","This paper studies the problem of outlier detection and backdoor poisoning detection in machine learning models. The authors propose to use differential privacy to improve the detection of outliers and outlier poisoning attacks. They show that differential privacy can be used to improve detection of backdoor poisoning attacks, and also show that it can be applied to outlier and novelty detection. They also provide a theoretical analysis of the effect of differential privacy on the detection performance. ","This paper studies the problem of outlier detection and backdoor poisoning detection in machine learning models. The authors propose to use differential privacy to improve the detection of outliers and outlier poisoning attacks. They show that differential privacy can be used to improve detection of backdoor poisoning attacks, and also show that it can be applied to outlier and novelty detection. They also provide a theoretical analysis of the effect of differential privacy on the detection performance. "
12004,SP:a5f0e531afd970144169823971d2d039bff752fb,"applications CONJUNCTION safety - critical ones. safety - critical ones CONJUNCTION applications. calibration of uncertainty prediction USED-FOR regression tasks. real - world systems FEATURE-OF regression tasks. definition USED-FOR regression uncertainty. reliability diagrams USED-FOR classification tasks. histogram - based approach USED-FOR classification tasks. definition CONJUNCTION evaluation method. evaluation method CONJUNCTION definition. reliability diagrams USED-FOR histogram - based approach. histogram - based approach USED-FOR evaluation method. synthetic, controlled problem CONJUNCTION object detection bounding - box regression task. object detection bounding - box regression task CONJUNCTION synthetic, controlled problem. Generic are method, and examples. OtherScientificTerm are uncertainty prediction, and empirical uncertainty. Method is scaling - based calibration. ","This paper proposes a new method for calibration of uncertainty prediction for regression tasks. The method is based on a histogram-based approach, which is able to capture the relationship between the uncertainty of the regression uncertainty and the empirical uncertainty. The authors show that the proposed method can be applied to both synthetic and real-world regression problems. The proposed method is evaluated on a synthetic problem and a bounding-box regression task. ","This paper proposes a new method for calibration of uncertainty prediction for regression tasks. The method is based on a histogram-based approach, which is able to capture the relationship between the uncertainty of the regression uncertainty and the empirical uncertainty. The authors show that the proposed method can be applied to both synthetic and real-world regression problems. The proposed method is evaluated on a synthetic problem and a bounding-box regression task. "
12008,SP:c422afd1df1ac98e23235830585dd0d45513064c,BERT HYPONYM-OF bidirectional Transformer language model. Tensor - Product Representations ( TPRs ) CONJUNCTION BERT. BERT CONJUNCTION Tensor - Product Representations ( TPRs ). structured - representational power CONJUNCTION Tensor - Product Representations ( TPRs ). Tensor - Product Representations ( TPRs ) CONJUNCTION structured - representational power. structured - representational power CONJUNCTION BERT. BERT CONJUNCTION structured - representational power. Tensor - Product Representations ( TPRs ) USED-FOR HUBERT1. BERT PART-OF HUBERT1. structured - representational power PART-OF HUBERT1. HUBERT COMPARE BERT. BERT COMPARE HUBERT. GLUE benchmark CONJUNCTION HANS dataset. HANS dataset CONJUNCTION GLUE benchmark. HANS dataset EVALUATE-FOR model. GLUE benchmark EVALUATE-FOR model. general language structure USED-FOR untangling data - specific semantics. Material is NLP datasets. ,This paper proposes a bidirectional Transformer language model that combines BERT and Tensor-Product Representations (TPRs) to learn a general language structure. The proposed model is evaluated on GLUE and HANS datasets. The paper shows that the proposed model outperforms BERT in terms of TPRs and structured-representational power. ,This paper proposes a bidirectional Transformer language model that combines BERT and Tensor-Product Representations (TPRs) to learn a general language structure. The proposed model is evaluated on GLUE and HANS datasets. The paper shows that the proposed model outperforms BERT in terms of TPRs and structured-representational power. 
12012,SP:117b19c4163cb3d08eda6bc7af0d48ed815b519e,Multi - agent reinforcement learning HYPONYM-OF problem. communication CONJUNCTION centralized training. centralized training CONJUNCTION communication. particle - based agents USED-FOR cooperative and competitive environments. communication USED-FOR particle - based agents. centralized training USED-FOR particle - based agents. dynamics CONJUNCTION humanoid navigation strategies. humanoid navigation strategies CONJUNCTION dynamics. interaction CONJUNCTION dynamics. dynamics CONJUNCTION interaction. Multi - Agent Reinforcement Learning CONJUNCTION Hierarchical Reinforcement Learning. Hierarchical Reinforcement Learning CONJUNCTION Multi - Agent Reinforcement Learning. multi - agent models USED-FOR simulated humanoid navigation. decentralized methods USED-FOR learning. structure USED-FOR optimization problem. goal - conditioned policies USED-FOR low - level physical controllers. balance CONJUNCTION walking. walking CONJUNCTION balance. low - level physical controllers USED-FOR balance. low - level physical controllers USED-FOR walking. lower - level controllers CONJUNCTION higher - level policies. higher - level policies CONJUNCTION lower - level controllers. decentralized heterogeneous policies USED-FOR multi - agent goal - directed collision avoidance. goal conditioned policies USED-FOR decentralized heterogeneous policies. RL techniques USED-FOR policies. methods USED-FOR RL techniques. Method is partial parameter sharing approach. OtherScientificTerm is hierarchy. Task is multi - agent problem. Material is multi - agent pursuit environment. ,"This paper studies the problem of multi-agent goal-directed collision avoidance in cooperative and competitive environments. The authors propose a decentralized approach to solve the problem. The proposed approach is based on partial parameter sharing, where each agent has a goal-conditioned policy that is shared among all other agents in the environment. The method is evaluated on a simulated humanoid navigation task, and is shown to outperform the state-of-the-art methods.","This paper studies the problem of multi-agent goal-directed collision avoidance in cooperative and competitive environments. The authors propose a decentralized approach to solve the problem. The proposed approach is based on partial parameter sharing, where each agent has a goal-conditioned policy that is shared among all other agents in the environment. The method is evaluated on a simulated humanoid navigation task, and is shown to outperform the state-of-the-art methods."
12016,SP:928640a19b0a0b1e1dc0d1b07cc99e1d51a4d817,"spatial convolution layer USED-FOR Graph Neural Networks ( GNNs ). spatial convolution layer USED-FOR feature vector. convolution layer USED-FOR nodes. convolution layer USED-FOR feature vectors. continuous feature space FEATURE-OF feature vectors. GNN USED-FOR graphs. convolution layers USED-FOR local structures. solution USED-FOR GNNs. spatial representation of the graph USED-FOR approach. point - cloud representation of the graph USED-FOR spatial representation. graph embedding method USED-FOR spatial representation. GNN USED-FOR topological structure. local feature extractor USED-FOR GNN. approach USED-FOR local feature extractor. spatial distribution of the locally extracted feature vectors USED-FOR GNN. spatial distribution of the locally extracted feature vectors USED-FOR topological structure. spatial representation USED-FOR graph down - sampling problem. pooling method COMPARE state - of - the - art methods. state - of - the - art methods COMPARE pooling method. Method are neural network, and graph pooling method. OtherScientificTerm is graph. ","This paper proposes a graph pooling method for graph neural networks (GNNs). The key idea is to use a local feature extractor for each node in the graph, and then pool the feature vectors from the extracted feature vectors. The authors show that the proposed method outperforms the state-of-the-art pooling methods on the graph down-sampling problem. ","This paper proposes a graph pooling method for graph neural networks (GNNs). The key idea is to use a local feature extractor for each node in the graph, and then pool the feature vectors from the extracted feature vectors. The authors show that the proposed method outperforms the state-of-the-art pooling methods on the graph down-sampling problem. "
12020,SP:465adf302cd8b7e6b449271a91d1d2fad844aa4d,it USED-FOR image processing. shift - equivalent prior of images USED-FOR Convolutional layer. max - pooling CONJUNCTION averagepooling. averagepooling CONJUNCTION max - pooling. averagepooling CONJUNCTION stride convolution. stride convolution CONJUNCTION averagepooling. down sampling methods USED-FOR convolutional neural networks ( CNNs ). max - pooling HYPONYM-OF convolutional neural networks ( CNNs ). stride convolution HYPONYM-OF down sampling methods. max - pooling HYPONYM-OF down sampling methods. averagepooling HYPONYM-OF down sampling methods. down sampling USED-FOR shift - equivalent. image classifications EVALUATE-FOR frequency pooling. accuracy CONJUNCTION robustness. robustness CONJUNCTION accuracy. CNNs EVALUATE-FOR frequency pooling. accuracy EVALUATE-FOR frequency pooling. robustness EVALUATE-FOR frequency pooling. ,"This paper proposes a new down-sampling method for convolutional neural networks (CNNs) called frequency pooling, which is a variant of max-pooling and averagepooling. The authors show that the proposed method is more robust to shift-equivalent prior of images than max pooling and stride convolution. They also show that it is able to improve the robustness of CNNs compared to max and average pooling. ","This paper proposes a new down-sampling method for convolutional neural networks (CNNs) called frequency pooling, which is a variant of max-pooling and averagepooling. The authors show that the proposed method is more robust to shift-equivalent prior of images than max pooling and stride convolution. They also show that it is able to improve the robustness of CNNs compared to max and average pooling. "
12024,SP:77f0f3779f9bdeb75ea5744ab494942a4943117b,"high - dimensional state spaces USED-FOR they. images HYPONYM-OF high - dimensional state spaces. technique USED-FOR deep RL agents. technique USED-FOR generalization ability. generalization ability EVALUATE-FOR deep RL agents. randomized ( convolutional ) neural network USED-FOR technique. Monte Carlo approximation USED-FOR inference method. 2D CoinRun CONJUNCTION 3D DeepMind Lab exploration. 3D DeepMind Lab exploration CONJUNCTION 2D CoinRun. 3D DeepMind Lab exploration CONJUNCTION 3D robotics control tasks. 3D robotics control tasks CONJUNCTION 3D DeepMind Lab exploration. it COMPARE regularization and data augmentation methods. regularization and data augmentation methods COMPARE it. 3D robotics control tasks EVALUATE-FOR method. 3D DeepMind Lab exploration EVALUATE-FOR method. 2D CoinRun EVALUATE-FOR method. Method is Deep reinforcement learning ( RL ) agents. Generic are agents, and It. OtherScientificTerm are robust features, varied and randomized environments, and randomization. ","This paper proposes a novel method to improve the generalization ability of deep RL agents. The proposed method is based on a randomized (convolutional) neural network and a Monte Carlo approximation of the inference method. The authors show that the proposed method outperforms existing methods on a variety of tasks, including 3D DeepMind Lab exploration, 2D CoinRun, and 3D robotics control tasks. ","This paper proposes a novel method to improve the generalization ability of deep RL agents. The proposed method is based on a randomized (convolutional) neural network and a Monte Carlo approximation of the inference method. The authors show that the proposed method outperforms existing methods on a variety of tasks, including 3D DeepMind Lab exploration, 2D CoinRun, and 3D robotics control tasks. "
12028,SP:31772a9122ec998c7c829bc4813f6147cdc30145,"image classification CONJUNCTION visual question answering. visual question answering CONJUNCTION image classification. models USED-FOR tasks. visual question answering HYPONYM-OF tasks. image classification HYPONYM-OF tasks. explanation approach USED-FOR image similarity models. score USED-FOR model. saliency map USED-FOR explanation method. saliency map USED-FOR image regions. explanations USED-FOR attribute recognition. diverse domains FEATURE-OF datasets. Polyvore Outfits HYPONYM-OF datasets. datasets EVALUATE-FOR approach. Polyvore Outfits HYPONYM-OF diverse domains. Method are deep learning model, and saliency maps. Task is classification. Generic are task, and methods. ",This paper proposes an explanation method for image similarity models. The proposed method is based on the idea of saliency maps. The saliency map is used to compute the score of the model. The score is then used to estimate the saliency of an image. The method is evaluated on a number of image classification tasks.,This paper proposes an explanation method for image similarity models. The proposed method is based on the idea of saliency maps. The saliency map is used to compute the score of the model. The score is then used to estimate the saliency of an image. The method is evaluated on a number of image classification tasks.
12032,SP:50f9dcac485552f2925839151da4dd8d82e35fcc,WaveFlow HYPONYM-OF small - footprint generative flow. auxiliary losses USED-FOR Parallel WaveNet. small - footprint generative flow USED-FOR raw audio. flowbased models USED-FOR raw audio. autoregressive flow CONJUNCTION bipartite flow. bipartite flow CONJUNCTION autoregressive flow. WaveNet HYPONYM-OF autoregressive flow. autoregressive flow PART-OF flowbased models. WaveGlow HYPONYM-OF bipartite flow. test likelihood CONJUNCTION speech fidelity. speech fidelity CONJUNCTION test likelihood. likelihood - based generative models USED-FOR raw waveforms. test likelihood EVALUATE-FOR likelihood - based generative models. speech fidelity EVALUATE-FOR likelihood - based generative models. WaveFlow USED-FOR high - fidelity speech. WaveFlow COMPARE WaveNet. WaveNet COMPARE WaveFlow. likelihood EVALUATE-FOR WaveFlow. small - footprint WaveFlow COMPARE real - time. real - time COMPARE small - footprint WaveFlow. GPU without engineered inference kernels USED-FOR real - time. Method is maximum likelihood. ,"This paper proposes WaveFlow, a generative model for audio generation. WaveFlow is a variant of WaveNet, which is an autoregressive flow with auxiliary losses. The authors show that WaveFlow achieves state-of-the-art performance in terms of test likelihood and speech fidelity. They also show that the proposed WaveFlow can be used to generate high-fidelity speech samples. ","This paper proposes WaveFlow, a generative model for audio generation. WaveFlow is a variant of WaveNet, which is an autoregressive flow with auxiliary losses. The authors show that WaveFlow achieves state-of-the-art performance in terms of test likelihood and speech fidelity. They also show that the proposed WaveFlow can be used to generate high-fidelity speech samples. "
12036,SP:963e85369978dddcd9e3130bc11453696066bbf3,"GT - GAN USED-FOR translation mapping. graph translator USED-FOR translation mapping. graph convolution and deconvolution layers USED-FOR translation mapping. graph convolution and deconvolution layers PART-OF graph translator. graph convolution and deconvolution layers PART-OF GT - GAN. graph translator PART-OF GT - GAN. GT - GAN COMPARE baseline methods. baseline methods COMPARE GT - GAN. synthetic and realworld datasets EVALUATE-FOR GT - GAN. synthetic and realworld datasets EVALUATE-FOR baseline methods. scalability EVALUATE-FOR GT - GAN. effectiveness EVALUATE-FOR GT - GAN. scalability EVALUATE-FOR baseline methods. effectiveness EVALUATE-FOR baseline methods. GT - GAN COMPARE GraphRNN. GraphRNN COMPARE GT - GAN. GraphRNN CONJUNCTION RandomVAE. RandomVAE CONJUNCTION GraphRNN. GT - GAN COMPARE RandomVAE. RandomVAE COMPARE GT - GAN. Method are Deep graph generation models, unconditioned generative models, and conditional graph discriminator. OtherScientificTerm is global and local features. ","This paper proposes a GAN-based method for graph generation. The proposed method is based on the idea of graph translator, which is a graph convolution and deconvolution layer that is used for translation mapping between global and local features. The method is evaluated on synthetic and real-world datasets and compared to baseline methods. ","This paper proposes a GAN-based method for graph generation. The proposed method is based on the idea of graph translator, which is a graph convolution and deconvolution layer that is used for translation mapping between global and local features. The method is evaluated on synthetic and real-world datasets and compared to baseline methods. "
12040,SP:962caffd236630c4079bfc7292403c1cc6861c3b,"METAGROSS ( Meta Gated Recursive Controller ) HYPONYM-OF neural sequence modeling unit. gating mechanisms PART-OF METAGROSS. inductive bias USED-FOR learning. hierarchically - structured sequence data USED-FOR learning. language HYPONYM-OF hierarchically - structured sequence data. code generation CONJUNCTION machine translation. machine translation CONJUNCTION code generation. tree traversal CONJUNCTION logical inference. logical inference CONJUNCTION tree traversal. sequential pixel - by - pixel classification CONJUNCTION semantic parsing. semantic parsing CONJUNCTION sequential pixel - by - pixel classification. sorting CONJUNCTION tree traversal. tree traversal CONJUNCTION sorting. semantic parsing CONJUNCTION code generation. code generation CONJUNCTION semantic parsing. machine translation CONJUNCTION polyphonic music modeling. polyphonic music modeling CONJUNCTION machine translation. logical inference CONJUNCTION sequential pixel - by - pixel classification. sequential pixel - by - pixel classification CONJUNCTION logical inference. tasks EVALUATE-FOR approach. polyphonic music modeling HYPONYM-OF recursive logic tasks. sequential pixel - by - pixel classification HYPONYM-OF recursive logic tasks. machine translation HYPONYM-OF recursive logic tasks. logical inference HYPONYM-OF recursive logic tasks. code generation HYPONYM-OF recursive logic tasks. semantic parsing HYPONYM-OF recursive logic tasks. sorting HYPONYM-OF recursive logic tasks. tree traversal HYPONYM-OF recursive logic tasks. Generic is unit. OtherScientificTerm are gating functions, and meta - gating. Method is recurrent model. ","This paper proposes a meta-gated recurrent controller (meta-Gated Recursive Controller) for sequence modeling. The meta-Gating mechanism is based on the idea of meta-learning, which is an inductive bias that is used to train a neural network to learn a sequence model. The proposed method is evaluated on a number of tasks, including sequential pixel-by-pixel classification, semantic parsing, tree traversal, and logical inference. The results show that the proposed method outperforms baselines on all tasks. ","This paper proposes a meta-gated recurrent controller (meta-Gated Recursive Controller) for sequence modeling. The meta-Gating mechanism is based on the idea of meta-learning, which is an inductive bias that is used to train a neural network to learn a sequence model. The proposed method is evaluated on a number of tasks, including sequential pixel-by-pixel classification, semantic parsing, tree traversal, and logical inference. The results show that the proposed method outperforms baselines on all tasks. "
12044,SP:d03aa0318f0d24a5b7c7817dfc7fba47ebec11cd,local prior matching ( LPM ) HYPONYM-OF self - supervised objective. self - supervised objective USED-FOR speech recognition. language model USED-FOR learning signal. unlabeled speech USED-FOR learning signal. language model USED-FOR LPM objective. unpaired text CONJUNCTION speech. speech CONJUNCTION unpaired text. it USED-FOR LPM. unpaired text USED-FOR it. speech USED-FOR it. unpaired text USED-FOR LPM. speech USED-FOR LPM. language model USED-FOR LPM. model USED-FOR LPM. unlabeled data USED-FOR model. labeled speech USED-FOR model. WER EVALUATE-FOR LPM. clean and noisy test set EVALUATE-FOR LPM. test sets EVALUATE-FOR fully supervised model. WER EVALUATE-FOR fully supervised model. noisy test set EVALUATE-FOR WER. WER EVALUATE-FOR LPM. noisy data USED-FOR LPM. Method is self - supervised approach. ,"This paper proposes a self-supervised approach for local prior matching (LPM) for speech recognition. The key idea is to use a pre-trained language model to learn the learning signal from unlabeled speech, and then use the learned signal to train a language model for LPM. The proposed method is evaluated on a clean and noisy test set. The results show that the proposed method outperforms the state of the art.","This paper proposes a self-supervised approach for local prior matching (LPM) for speech recognition. The key idea is to use a pre-trained language model to learn the learning signal from unlabeled speech, and then use the learned signal to train a language model for LPM. The proposed method is evaluated on a clean and noisy test set. The results show that the proposed method outperforms the state of the art."
12048,SP:e6af249608633f1776b608852a00946a5c09a357,"data poisoning USED-FOR fair and robust model training. FR - GAN USED-FOR fair and robust model training. generative adversarial networks ( GANs ) USED-FOR FR - GAN. generative adversarial networks ( GANs ) USED-FOR fair and robust model training. fairness discriminator CONJUNCTION robustness discriminator. robustness discriminator CONJUNCTION fairness discriminator. robustness discriminator HYPONYM-OF discriminators. fairness discriminator HYPONYM-OF discriminators. disparate impact CONJUNCTION equalized odds. equalized odds CONJUNCTION disparate impact. equalized odds CONJUNCTION equal opportunity. equal opportunity CONJUNCTION equalized odds. fairness measures FEATURE-OF framework. disparate impact HYPONYM-OF fairness measures. equalized odds HYPONYM-OF fairness measures. equal opportunity HYPONYM-OF fairness measures. FR - GAN USED-FOR fairness. FR - GAN COMPARE fairness methods. fairness methods COMPARE FR - GAN. fairness CONJUNCTION accuracy. accuracy CONJUNCTION fairness. accuracy EVALUATE-FOR FR - GAN. fairness EVALUATE-FOR FR - GAN. accuracy CONJUNCTION fairness. fairness CONJUNCTION accuracy. fairness EVALUATE-FOR FR - GAN. accuracy EVALUATE-FOR FR - GAN. OtherScientificTerm is bias. Method are model fairness techniques, and generator. Material is validation set. ","This paper proposes a framework for fair and robust model training. The framework is based on the idea of data poisoning, which is an important problem in fairness and robustness. The authors propose to use a generative adversarial network (GAN) as the discriminator and a fairness discriminator as the robustness discriminator. The proposed framework is evaluated on three fairness measures: Equalized Odds (EOU), Equalized Opportunity (EO), and Disparity Impact (DIO). The authors show that the proposed framework outperforms the baselines in terms of accuracy and fairness.","This paper proposes a framework for fair and robust model training. The framework is based on the idea of data poisoning, which is an important problem in fairness and robustness. The authors propose to use a generative adversarial network (GAN) as the discriminator and a fairness discriminator as the robustness discriminator. The proposed framework is evaluated on three fairness measures: Equalized Odds (EOU), Equalized Opportunity (EO), and Disparity Impact (DIO). The authors show that the proposed framework outperforms the baselines in terms of accuracy and fairness."
12052,SP:6306417f5a300629ec856495781515c6af05a363,"physics - inspired deep learning approach USED-FOR point cloud processing. static background grid CONJUNCTION Lagrangian material space. Lagrangian material space CONJUNCTION static background grid. moving particles USED-FOR Lagrangian material space. Lagrangian material space USED-FOR learning architecture. moving particles USED-FOR learning architecture. static background grid USED-FOR learning architecture. Eulerian - Lagrangian representation USED-FOR particle features. generalized, high - dimensional force field USED-FOR flow velocities. generalized, high - dimensional force field USED-FOR particle features. flow velocities USED-FOR particle features. point cloud classification and segmentation problems EVALUATE-FOR system. geometric machine learning CONJUNCTION physical simulation. physical simulation CONJUNCTION geometric machine learning. PIC / FLIP scheme USED-FOR natural flow. Task is natural flow phenomena in fluid mechanics. OtherScientificTerm are Eulerian world space, and geometric reservoir. ",This paper proposes a physics-inspired deep learning approach for point cloud processing. The proposed method is based on the PIC/FLIP framework. The authors propose to learn a Lagrangian-Lagrangian representation for particle features in the Eulerian world space and a static background grid. The method is evaluated on point cloud classification and segmentation problems. ,This paper proposes a physics-inspired deep learning approach for point cloud processing. The proposed method is based on the PIC/FLIP framework. The authors propose to learn a Lagrangian-Lagrangian representation for particle features in the Eulerian world space and a static background grid. The method is evaluated on point cloud classification and segmentation problems. 
12056,SP:0561a2174d7334e078a49ae8859a36e4d74f9b5b,"clipping USED-FOR dynamics of iterates. local minimum FEATURE-OF rate of convergence. clipping COMPARE vanilla gradient descent. vanilla gradient descent COMPARE clipping. clipping USED-FOR noise. lens USED-FOR gradient clipping. robustness HYPONYM-OF lens. label noise FEATURE-OF classification. robustness EVALUATE-FOR gradient clipping. gradient clipping USED-FOR label noise. Method are Gradient clipping, deep networks, and optimisation lens. OtherScientificTerm are loss function, and cross - entropy loss. ",This paper studies the problem of gradient clipping in deep neural networks. The authors show that clipping can be used to improve the robustness of the network to label noise. The main contribution of the paper is to show that the local minimum of the rate of convergence of the gradient clipping converges to a local minimum in the case of cross-entropy loss. They also show that gradient clipping can also be used as an optimisation lens to improve robustness. ,This paper studies the problem of gradient clipping in deep neural networks. The authors show that clipping can be used to improve the robustness of the network to label noise. The main contribution of the paper is to show that the local minimum of the rate of convergence of the gradient clipping converges to a local minimum in the case of cross-entropy loss. They also show that gradient clipping can also be used as an optimisation lens to improve robustness. 
12060,SP:414b06d86e132357a54eb844036b78a232571301,they USED-FOR imitating actions. imitation learning methods USED-FOR imitating actions. state alignment based imitation learning method USED-FOR imitator. expert demonstrations FEATURE-OF state sequences. them PART-OF reinforcement learning framework. local and global perspectives USED-FOR state alignment. regularized policy update objective USED-FOR them. regularized policy update objective USED-FOR reinforcement learning framework. imitation learning settings CONJUNCTION imitation learning settings. imitation learning settings CONJUNCTION imitation learning settings. imitation learning settings EVALUATE-FOR method. imitation learning settings EVALUATE-FOR method. Task is imitation learning problem. Method is dynamics models. ,"This paper proposes a state alignment based imitation learning method for imitation learning. The proposed method is based on the idea of local and global perspectives. The authors propose a regularized policy update objective to improve the performance of the imitator. The method is evaluated on a variety of imitation learning settings, including imitation learning, imitation learning with expert demonstrations, and reinforcement learning.","This paper proposes a state alignment based imitation learning method for imitation learning. The proposed method is based on the idea of local and global perspectives. The authors propose a regularized policy update objective to improve the performance of the imitator. The method is evaluated on a variety of imitation learning settings, including imitation learning, imitation learning with expert demonstrations, and reinforcement learning."
12064,SP:91761d68086330ce378507c152e72218ed7b2196,"Stochastic gradient descent ( SGD ) USED-FOR deep neural networks. deep gradient boosting ( DGB ) HYPONYM-OF SGD. pseudo - residual targets USED-FOR gradient boosting problem. chain rule USED-FOR back - propagated gradients. boosting problem USED-FOR weight update. linear base learner USED-FOR boosting problem. normalization procedure USED-FOR weight update formula. architecture COMPARE architecture. architecture COMPARE architecture. image recognition tasks EVALUATE-FOR architecture. input normalization layer ( INN ) USED-FOR architecture. normalization layers PART-OF architecture. image recognition tasks EVALUATE-FOR architecture. batch normalization ( BN ) COMPARE INN. INN COMPARE batch normalization ( BN ). CIFAR10 CONJUNCTION ImageNet classification tasks. ImageNet classification tasks CONJUNCTION CIFAR10. ImageNet classification tasks EVALUATE-FOR it. CIFAR10 EVALUATE-FOR it. Method are neural network, and DGB. OtherScientificTerm are intrinsic generalization properties, and forward pass. ","This paper proposes a novel approach to deep gradient boosting (DGB) for deep neural networks. The authors propose a new normalization layer (INN) to improve the generalization properties of SGD. In particular, the authors propose to use pseudo-residual targets as pseudo-regret targets for the boosting problem. The proposed method is evaluated on CIFAR-10 and ImageNet classification tasks. ","This paper proposes a novel approach to deep gradient boosting (DGB) for deep neural networks. The authors propose a new normalization layer (INN) to improve the generalization properties of SGD. In particular, the authors propose to use pseudo-residual targets as pseudo-regret targets for the boosting problem. The proposed method is evaluated on CIFAR-10 and ImageNet classification tasks. "
12068,SP:7709a8b907c5642479e7b6fb0b362efc4ead63ce,"Differentiable architecture search ( DARTS ) USED-FOR network architectures. reduced memory cost USED-FOR PC - DARTS. training stability EVALUATE-FOR PC - DARTS. batch size USED-FOR PC - DARTS. GPU - days USED-FOR search. top-1 error rate EVALUATE-FOR search. ImageNet EVALUATE-FOR top-1 error rate. Metric are memory and computing overheads, and error rate. OtherScientificTerm are super - network, edges of super - net, and edge - level parameters. Generic are approach, strategy, method, and code. Method are Partially - Connected DARTS, operation search, edge normalization, and architecture search. Material is CIFAR10. ","This paper proposes a new method for differentiable architecture search (DARTS). The proposed method is based on the idea of edge normalization, which is used to reduce the memory and computing overheads of DARTS. The method is evaluated on CIFAR-10 and ImageNet.","This paper proposes a new method for differentiable architecture search (DARTS). The proposed method is based on the idea of edge normalization, which is used to reduce the memory and computing overheads of DARTS. The method is evaluated on CIFAR-10 and ImageNet."
12072,SP:724870046e990376990ba9f73d63d331f61788d7,"control strategies USED-FOR complex control tasks. model - predictive control ( MPC ) CONJUNCTION trajectory optimization. trajectory optimization CONJUNCTION model - predictive control ( MPC ). Model - based control algorithms USED-FOR control tasks. gradients of underlying system dynamics USED-FOR control tasks. gradients of underlying system dynamics USED-FOR Model - based control algorithms. sample efficiency EVALUATE-FOR control tasks. trajectory optimization HYPONYM-OF Model - based control algorithms. model - predictive control ( MPC ) HYPONYM-OF Model - based control algorithms. gradient - based numerical optimization methods COMPARE model - based control methods. model - based control methods COMPARE gradient - based numerical optimization methods. sampling USED-FOR solution space. gradient - based methods CONJUNCTION DRL. DRL CONJUNCTION gradient - based methods. gradient - based methods USED-FOR hybrid method. DRL USED-FOR hybrid method. true gradients USED-FOR convergence rate. convergence rate FEATURE-OF actor. differentiable physical simulator USED-FOR true gradients. convergence rate EVALUATE-FOR modification. differentiable physical simulator USED-FOR modification. true gradients USED-FOR modification. deep deterministic policy gradients ( DDPG ) algorithm USED-FOR algorithm. differentiable half cheetah HYPONYM-OF one. 2D robot control tasks EVALUATE-FOR algorithm. hard contact constraints FEATURE-OF differentiable half cheetah. method USED-FOR DDPG. robustness EVALUATE-FOR method. OtherScientificTerm are initialization, and local minima. Method is Deep reinforcement learning ( DRL ). Metric is computational cost. ","This paper proposes a hybrid method for deep reinforcement learning (DRL) and model-based control (MPC). The authors propose a new method for learning the true gradients of the underlying system dynamics, which can be used to improve the sample efficiency and robustness of DRL. The proposed method is based on deep deterministic policy gradients (DDPG) algorithm. The authors show that the proposed method can achieve better sample efficiency compared to gradient-based methods. They also show that their method is robust to hard contact constraints.","This paper proposes a hybrid method for deep reinforcement learning (DRL) and model-based control (MPC). The authors propose a new method for learning the true gradients of the underlying system dynamics, which can be used to improve the sample efficiency and robustness of DRL. The proposed method is based on deep deterministic policy gradients (DDPG) algorithm. The authors show that the proposed method can achieve better sample efficiency compared to gradient-based methods. They also show that their method is robust to hard contact constraints."
12076,SP:be0202a28bcca68edb0abe4d1c0ba1af265211e3,"task - agnostic world graphs USED-FOR complex environment. nodes PART-OF world graph. hierarchical RL framework USED-FOR exploration. world graph nodes CONJUNCTION edges. edges CONJUNCTION world graph nodes. structural and connectivity knowledge USED-FOR exploration. trajectory data USED-FOR binary recurrent variational autoencoder ( VAE ). world graph USED-FOR structural and connectivity knowledge. learning phases PART-OF framework. structural and connectivity knowledge USED-FOR hierarchical RL framework. hierarchical RL framework HYPONYM-OF learning phases. world graphs USED-FOR RL. reward CONJUNCTION learning. learning CONJUNCTION reward. maze tasks EVALUATE-FOR approach. OtherScientificTerm are complex environments, and agents. Method is reinforcement learning ( RL ) agents. ",This paper proposes a hierarchical RL framework for learning from task-agnostic world graphs. The proposed method is based on a binary recurrent variational autoencoder (VAE) that learns to predict trajectories from trajectory data. The method is evaluated on a number of maze tasks and shows promising results. ,This paper proposes a hierarchical RL framework for learning from task-agnostic world graphs. The proposed method is based on a binary recurrent variational autoencoder (VAE) that learns to predict trajectories from trajectory data. The method is evaluated on a number of maze tasks and shows promising results. 
12080,SP:e8a3a0f77dab336ce50c9dc941f7350173916e04,Neural networks USED-FOR real - world tasks. neural network USED-FOR approximation of any type of continuous functions. non - interpretable black box model USED-FOR neural network. white box network HYPONYM-OF function constructing network. network USED-FOR function blocks. discretized layers USED-FOR network. discretization USED-FOR end - to - end PathNet structure. OtherScientificTerm is continuous functions. Task is reverse engineering. Method is neural networks. ,"This paper proposes a new method for learning continuous function approximations. The method is based on discretized layers of a neural network. The discretization of the network is done by discretizing the input to a black box model, which is then used to construct a function constructing network.  The authors show that the proposed method is able to approximate any type of continuous function. The authors also show that their method can be applied to reverse engineering. ","This paper proposes a new method for learning continuous function approximations. The method is based on discretized layers of a neural network. The discretization of the network is done by discretizing the input to a black box model, which is then used to construct a function constructing network.  The authors show that the proposed method is able to approximate any type of continuous function. The authors also show that their method can be applied to reverse engineering. "
12084,SP:b7f4fda6497a1c20fd57f029be5f1b2e2780e227,"approach USED-FOR control policies. Imitation learning algorithms USED-FOR control policies. Imitation learning algorithms USED-FOR approach. supervised learning methods USED-FOR control policies. supervised imitation learning USED-FOR policies. imitation learning USED-FOR policies. algorithm USED-FOR behaviors. user - provided reward functions CONJUNCTION reinforcement learning methods. reinforcement learning methods CONJUNCTION user - provided reward functions. user - provided reward functions USED-FOR algorithm. method USED-FOR goal - reaching policies. approach USED-FOR method. approach USED-FOR goal - reaching policies. approach USED-FOR imitation learning settings. self - supervised imitation learning CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION self - supervised imitation learning. it COMPARE reinforcement learning methods. reinforcement learning methods COMPARE it. reinforcement learning methods USED-FOR goal reaching problems. goal reaching problems EVALUATE-FOR it. OtherScientificTerm are expert demonstrator, expert demonstrations, and demonstrations. Task is multi - task setting. Method is suboptimal policy. Generic is tasks. ",This paper proposes an imitation learning algorithm for goal-reaching tasks. The proposed method is based on the observation that the suboptimal policy can be learned by self-supervised imitation learning and reinforcement learning. The method is evaluated on a number of multi-task tasks and shows promising results. ,This paper proposes an imitation learning algorithm for goal-reaching tasks. The proposed method is based on the observation that the suboptimal policy can be learned by self-supervised imitation learning and reinforcement learning. The method is evaluated on a number of multi-task tasks and shows promising results. 
12088,SP:1c7cf7417825208feac9fe3b3488a51ad1e72270,"Zeno++ USED-FOR non - convex problems. Zeno++ COMPARE approaches. approaches COMPARE Zeno++. OtherScientificTerm are workerserver communications, Byzantine workers, candidate gradient, optimization progress, and Byzantine failures. ","This paper studies the problem of Byzantine workers in non-convex optimization problems. The authors propose a new method to solve the problem, called Zeno++, which is based on Byzantine workers. The main contribution of the paper is to show that the candidate gradient of the Byzantine workers is a function of the number of Byzantine failures. The paper also shows that the Byzantine failures can be seen as a measure of the complexity of the problem. ","This paper studies the problem of Byzantine workers in non-convex optimization problems. The authors propose a new method to solve the problem, called Zeno++, which is based on Byzantine workers. The main contribution of the paper is to show that the candidate gradient of the Byzantine workers is a function of the number of Byzantine failures. The paper also shows that the Byzantine failures can be seen as a measure of the complexity of the problem. "
12092,SP:d16ed9bd4193d99774840783347137e938955b87,"deep neural networks ( DNNs ) HYPONYM-OF Machine learning models. defenses USED-FOR adversarial impact. Lp norm USED-FOR adversarial perturbations. unrestricted ” perturbations USED-FOR image - based visual descriptors. feature squeezing CONJUNCTION adversarially trained model. adversarially trained model CONJUNCTION feature squeezing. JPEG compression CONJUNCTION feature squeezing. feature squeezing CONJUNCTION JPEG compression. semantically aware perturbations USED-FOR JPEG compression. semantically aware perturbations USED-FOR adversarially trained model. semantically aware perturbations USED-FOR feature squeezing. ImageNet CONJUNCTION MSCOCO. MSCOCO CONJUNCTION ImageNet. image classification CONJUNCTION image captioning tasks. image captioning tasks CONJUNCTION image classification. methods USED-FOR image captioning tasks. methods USED-FOR image classification. complex datasets USED-FOR methods. complex datasets USED-FOR image captioning tasks. MSCOCO HYPONYM-OF complex datasets. ImageNet HYPONYM-OF complex datasets. OtherScientificTerm are adversarial examples, and large magnitude perturbations. Method is user studies. Material is semantic adversarial examples. Generic is attacks. ","This paper studies the adversarial impact of semantic adversarial attacks on deep neural networks (DNNs) on image classification and feature-squeezing. The authors propose two defenses against semantic attacks: (1) the Lp norm, which is a measure of the magnitude of adversarial examples, and (2) the semantically aware perturbations, which can be used to improve the robustness of DNNs to semantic attacks. The proposed defenses are tested on ImageNet, MSCOCO, and ImageNet-2.","This paper studies the adversarial impact of semantic adversarial attacks on deep neural networks (DNNs) on image classification and feature-squeezing. The authors propose two defenses against semantic attacks: (1) the Lp norm, which is a measure of the magnitude of adversarial examples, and (2) the semantically aware perturbations, which can be used to improve the robustness of DNNs to semantic attacks. The proposed defenses are tested on ImageNet, MSCOCO, and ImageNet-2."
12096,SP:f4f7dd96b7865fe2d4c6bddf82875f0c9377c3b4,large datasets USED-FOR latent representations. neural networks USED-FOR latent representations. large datasets USED-FOR neural networks. events CONJUNCTION locations. locations CONJUNCTION events. natural language understanding ( NLU ) system USED-FOR emerging entities. RL trainable controller USED-FOR representation learning. representation learning PART-OF neural encoder. neural encoder CONJUNCTION memory management role. memory management role CONJUNCTION neural encoder. controller USED-FOR read and write operations. external memory USED-FOR read and write operations. named Learning to Control ( LTC ) USED-FOR approach. memory plasticity USED-FOR few - shot learning. system USED-FOR few - shot learning of entity recognition. Stanford Task - Oriented Dialogue dataset USED-FOR few - shot learning of entity recognition. Metric is loss function. Generic is solution. ,This paper proposes a method for few-shot learning of entity recognition in natural language understanding (NLU) systems. The key idea is to train a controller that learns to control the representation learning of the neural encoder and the memory management role of the encoder. The controller is trained to control both the read and write operations. Experiments on the Stanford Task-Oriented Dialogue dataset show that the proposed method outperforms baselines.,This paper proposes a method for few-shot learning of entity recognition in natural language understanding (NLU) systems. The key idea is to train a controller that learns to control the representation learning of the neural encoder and the memory management role of the encoder. The controller is trained to control both the read and write operations. Experiments on the Stanford Task-Oriented Dialogue dataset show that the proposed method outperforms baselines.
12105,SP:2e9235485b79d0b22ec8b565b19bfa26804ccbe1,approach USED-FOR recomposable motor primitives. large - scale and diverse manipulation demonstrations FEATURE-OF recomposable motor primitives. approaches USED-FOR primitives. manually defined primitives USED-FOR approaches. manually defined primitives USED-FOR primitives. approaches USED-FOR primitive discovery. complexity EVALUATE-FOR primitive. latent representation USED-FOR primitives. motor primitives CONJUNCTION latent representation. latent representation CONJUNCTION motor primitives. hierarchical reinforcement learning setup USED-FOR robotic manipulation tasks. primitives USED-FOR robotic manipulation tasks. primitives PART-OF hierarchical reinforcement learning setup. ,This paper proposes a hierarchical reinforcement learning approach for learning motor primitives for robotic manipulation tasks. The proposed approach is based on the idea of learning a latent representation of a primitive that can be used to learn a new primitive for a given task. The method is evaluated on a number of tasks and is shown to outperform the baselines. ,This paper proposes a hierarchical reinforcement learning approach for learning motor primitives for robotic manipulation tasks. The proposed approach is based on the idea of learning a latent representation of a primitive that can be used to learn a new primitive for a given task. The method is evaluated on a number of tasks and is shown to outperform the baselines. 
12114,SP:c7c37aeebec7f33c1015f1fa3dd2a36d7b437d1c,"Transfer CONJUNCTION adaptation. adaptation CONJUNCTION Transfer. adaptation USED-FOR unknown environmental dynamics. Transfer PART-OF reinforcement learning ( RL ). adaptation PART-OF reinforcement learning ( RL ). Transfer USED-FOR unknown environmental dynamics. methods USED-FOR adaptation. experience rollouts USED-FOR adaptation. experience rollouts USED-FOR methods. general algorithm USED-FOR probe. general algorithm USED-FOR inference model. general algorithm USED-FOR latent variables of test dynamics. general algorithm USED-FOR single episode transfer. algorithms USED-FOR RL. algorithms USED-FOR variational inference. variational inference CONJUNCTION RL. RL CONJUNCTION variational inference. algorithms USED-FOR modular approach. method COMPARE adaptive approaches. adaptive approaches COMPARE method. baselines USED-FOR robust transfer. method COMPARE baselines. baselines COMPARE method. method USED-FOR robust transfer. single episode test constraint EVALUATE-FOR method. OtherScientificTerm are dense rewards, and rewards. Method is universal control policy. Generic are approach, and it. ",This paper proposes a modular approach for learning a universal control policy that is robust to the single episode transfer problem. The proposed method is based on a general algorithm for learning the latent variables of the test dynamics of the environment. The authors show that the proposed method outperforms existing methods in terms of robustness to transfer and experience rollouts. They also show that their method can be applied to the multi-episode transfer problem as well. ,This paper proposes a modular approach for learning a universal control policy that is robust to the single episode transfer problem. The proposed method is based on a general algorithm for learning the latent variables of the test dynamics of the environment. The authors show that the proposed method outperforms existing methods in terms of robustness to transfer and experience rollouts. They also show that their method can be applied to the multi-episode transfer problem as well. 
12123,SP:f2f1aff9a5b91d748b24fee0155367f650401aab,"AlphaZero COMPARE AlphaGo Zero. AlphaGo Zero COMPARE AlphaZero. generalization EVALUATE-FOR neural net. three - head network architecture USED-FOR action - value head. action - value head USED-FOR Monte Carlo tree search ( MCTS ). search efficiency EVALUATE-FOR action - value head. three - head network USED-FOR AlphaZero style learning paradigm. threehead network architecture USED-FOR AlpahZero learning. game of Hex EVALUATE-FOR threehead network architecture. architecture USED-FOR zero - style iterative learning. neural network models COMPARE those. those COMPARE neural network models. two - head counterpart PART-OF MCTS. MCTS EVALUATE-FOR neural network models. two - head counterpart USED-FOR those. two - head counterpart USED-FOR neural network models. Method are search - based reinforcement learning algorithm AlphaZero, two - head network architecture, and two - head net. Material is chess. OtherScientificTerm is policy. ","This paper proposes a three-head neural network architecture for AlphaZero, which is a search-based reinforcement learning algorithm. The main idea is to use the action-value head of the two-head network to perform Monte Carlo tree search (MCTS). The proposed method is evaluated on the game of Hex and shows that the proposed method outperforms two-headed neural networks. ","This paper proposes a three-head neural network architecture for AlphaZero, which is a search-based reinforcement learning algorithm. The main idea is to use the action-value head of the two-head network to perform Monte Carlo tree search (MCTS). The proposed method is evaluated on the game of Hex and shows that the proposed method outperforms two-headed neural networks. "
12132,SP:89d6d55107b6180109affe7522265c751640ad96,policy transfer PART-OF reinforcement learning. warm initialization CONJUNCTION imitation. imitation CONJUNCTION warm initialization. Policy transfer USED-FOR Reinforcement Learning ( RL ) tasks. imitation USED-FOR Policy transfer. warm initialization USED-FOR Policy transfer. biological world FEATURE-OF behavior transfer. adaptation reward CONJUNCTION environmental reward. environmental reward CONJUNCTION adaptation reward. method USED-FOR policies. sample complexity EVALUATE-FOR policies. sample complexity EVALUATE-FOR method. Material is randomized instances. Task is transfer of policies. Generic is mechanism. OtherScientificTerm is transition differences. ,"This paper studies the problem of policy transfer in reinforcement learning. The authors propose a novel method for policy transfer based on warm initialization and imitation. They show that warm initialization can be used to improve the sample complexity of policies, while imitation can improve the transfer of policies. They also show that imitation improves the transfer performance of policies in the presence of transition differences. ","This paper studies the problem of policy transfer in reinforcement learning. The authors propose a novel method for policy transfer based on warm initialization and imitation. They show that warm initialization can be used to improve the sample complexity of policies, while imitation can improve the transfer of policies. They also show that imitation improves the transfer performance of policies in the presence of transition differences. "
12141,SP:626021101836a635ad2d896bd66951aff31aa846,"scale changes FEATURE-OF tasks. steerable filters USED-FOR scale - equivariant convolutional networks. numerical stability EVALUATE-FOR method. computational efficiency EVALUATE-FOR method. computational efficiency CONJUNCTION numerical stability. numerical stability CONJUNCTION computational efficiency. scale equivariance CONJUNCTION local scale invariance. local scale invariance CONJUNCTION scale equivariance. models COMPARE methods. methods COMPARE models. methods USED-FOR scale equivariance. methods USED-FOR local scale invariance. models USED-FOR scale equivariance. models USED-FOR local scale invariance. MNIST - scale dataset CONJUNCTION STL-10 dataset. STL-10 dataset CONJUNCTION MNIST - scale dataset. supervised learning setting FEATURE-OF STL-10 dataset. Method are Convolutional Neural Networks ( CNNs ), CNNs, and scale - convolution. OtherScientificTerm are translation equivariance, and scale - equivariant. Generic is transformations. ",This paper proposes a new method for scale-equivariant convolutional networks. The proposed method is based on steerable filters. The authors show that the proposed method outperforms existing methods on the MNIST-10 and STL-10 datasets. ,This paper proposes a new method for scale-equivariant convolutional networks. The proposed method is based on steerable filters. The authors show that the proposed method outperforms existing methods on the MNIST-10 and STL-10 datasets. 
12150,SP:6316f750b8c69e55e61926c34e3ba5acbd7228ad,deep learning setups USED-FOR scan completion. paired training data FEATURE-OF supervision. supervision USED-FOR methods. synthetic data EVALUATE-FOR methods. real scans USED-FOR scan completion. point clouds USED-FOR approach. Matterport3D CONJUNCTION KITTI. KITTI CONJUNCTION Matterport3D. ScanNet CONJUNCTION Matterport3D. Matterport3D CONJUNCTION ScanNet. incompleteness USED-FOR realistic completions. ScanNet HYPONYM-OF real - world datasets. real - world datasets EVALUATE-FOR approach. 3D - EPN shape completion dataset EVALUATE-FOR approach. KITTI HYPONYM-OF real - world datasets. Matterport3D HYPONYM-OF real - world datasets. Task is 3D scanning solutions. Material is raw scans. OtherScientificTerm is partial scans. Generic is approaches. ,"This paper proposes a method for 3D scanning completion. The method is based on the idea of point clouds, which is an extension of the point clouds used in point cloud completion. Point clouds are used to capture the incompleteness of a 3D scan. The proposed method is evaluated on three real-world datasets, Matterport3D, KITTI, and ScanNet. The results show that the proposed method outperforms the baselines. ","This paper proposes a method for 3D scanning completion. The method is based on the idea of point clouds, which is an extension of the point clouds used in point cloud completion. Point clouds are used to capture the incompleteness of a 3D scan. The proposed method is evaluated on three real-world datasets, Matterport3D, KITTI, and ScanNet. The results show that the proposed method outperforms the baselines. "
12159,SP:270c679b322f69a943bf7f6b938dc1bf663d3c6f,"models USED-FOR systems. authentication CONJUNCTION anomaly detection. anomaly detection CONJUNCTION authentication. sensor data USED-FOR anomaly detection. sensor data USED-FOR authentication. systems USED-FOR authentication. systems USED-FOR anomaly detection. sensor data USED-FOR systems. learning systems USED-FOR private data. authentication system USED-FOR generative impersonation attacks. optimal strategies USED-FOR attacker. optimal strategies USED-FOR Gaussian source distributions. maximin game USED-FOR problem. they USED-FOR models. real - world data USED-FOR models. Method are Generative neural models, and practical learning approaches. Generic is it. Task are learning, and information theory. Material is nominally - looking artificial data. OtherScientificTerm is optimal strategy. ","This paper studies the problem of generative adversarial attacks on generative neural networks (GANs). The authors propose a maximin game to solve the problem, which is based on the information theory of information theory. In particular, the authors show that the optimal strategies for generating adversarial samples can be obtained from a Gaussian source distribution. The authors also show that adversarial examples can be generated from real-world data. ","This paper studies the problem of generative adversarial attacks on generative neural networks (GANs). The authors propose a maximin game to solve the problem, which is based on the information theory of information theory. In particular, the authors show that the optimal strategies for generating adversarial samples can be obtained from a Gaussian source distribution. The authors also show that adversarial examples can be generated from real-world data. "
12168,SP:a7e7619667892806a6f4038cbe4b1c6cd0eec0ed,robustness CONJUNCTION standard accuracy. standard accuracy CONJUNCTION robustness. natural accuracy CONJUNCTION robustness. robustness CONJUNCTION natural accuracy. limited data samples USED-FOR high dimensional distribution. sensible adversary USED-FOR defense model. Bayes rule HYPONYM-OF multi - class classifier. 0 - 1 loss FEATURE-OF Bayes rule. 0 - 1 loss FEATURE-OF multi - class classifier. sensible adversarial learning USED-FOR Bayes rule. algorithm USED-FOR robust model. sensible adversarial examples USED-FOR robust model. robust accuracy EVALUATE-FOR PGD attacks. model USED-FOR attacks. CIFAR10 EVALUATE-FOR model. perturbations USED-FOR attacks. Generic is problem. ,"This paper studies the problem of adversarial defense against PGD attacks. The authors propose a new adversarial learning algorithm for multi-class classifiers. The proposed algorithm is based on the Bayes rule, which is a generalization of the adversarial training algorithm for Bayesian classifiers, and the authors show that the proposed method is robust to PGD attack. ","This paper studies the problem of adversarial defense against PGD attacks. The authors propose a new adversarial learning algorithm for multi-class classifiers. The proposed algorithm is based on the Bayes rule, which is a generalization of the adversarial training algorithm for Bayesian classifiers, and the authors show that the proposed method is robust to PGD attack. "
12177,SP:9ca0b8d270e3fea3ba8f88c8f1ba50d8a8f7e4b8,image intensity CONJUNCTION spatial correlation. spatial correlation CONJUNCTION image intensity. spatial correlation FEATURE-OF Feature maps. image intensity FEATURE-OF Feature maps. class probabilities USED-FOR online knowledge distillation methods. adversarial training framework USED-FOR online knowledge distillation method. discriminators USED-FOR feature map distributions. discriminators USED-FOR networks. discriminator USED-FOR feature map. discriminator PART-OF network. network USED-FOR feature map distribution. network USED-FOR discriminator. Discriminators CONJUNCTION networks. networks CONJUNCTION Discriminators. Discriminators PART-OF minimax twoplayer game. minimax twoplayer game USED-FOR networks. cyclic learning scheme USED-FOR networks. method USED-FOR network architectures. method USED-FOR classification task. classification task EVALUATE-FOR network architectures. Generic is small network. ,"This paper proposes an adversarial training framework for online knowledge distillation based on the minimax twoplayer game. The proposed method is based on a cyclic learning scheme, where the discriminator is trained on the feature map distribution and the network is trained to learn a discriminator. The authors show that the proposed method outperforms the baselines on the classification task. ","This paper proposes an adversarial training framework for online knowledge distillation based on the minimax twoplayer game. The proposed method is based on a cyclic learning scheme, where the discriminator is trained on the feature map distribution and the network is trained to learn a discriminator. The authors show that the proposed method outperforms the baselines on the classification task. "
12186,SP:e43fc8747f823be6497224696adb92d45150b02d,"natural language processing tasks EVALUATE-FOR word embedding models. word embedding models USED-FOR rich semantic meanings. maximum likelihood estimation CONJUNCTION Bayesian estimation. Bayesian estimation CONJUNCTION maximum likelihood estimation. maximum likelihood estimation USED-FOR model. Bayesian estimation USED-FOR model. model COMPARE baseline methods. baseline methods COMPARE model. baseline methods USED-FOR sentiment analysis. low - frequency words FEATURE-OF sentiment analysis. sentiment analysis EVALUATE-FOR model. it USED-FOR semantic and sentiment analysis tasks. OtherScientificTerm is sentiment information. Method are sentiment word embedding model, and parameter estimating method. Task is semantic and sentiment embeddings. ",This paper proposes a new word embedding model for natural language processing tasks. The proposed model is based on a combination of Bayesian estimation and maximum likelihood estimation. The authors show that the proposed model outperforms existing word embeddings on a variety of tasks. ,This paper proposes a new word embedding model for natural language processing tasks. The proposed model is based on a combination of Bayesian estimation and maximum likelihood estimation. The authors show that the proposed model outperforms existing word embeddings on a variety of tasks. 
12195,SP:72d32a2ae382f63e055ab3eafcc9276b10fba985,"Noisy labels PART-OF real - world training data. overfitting FEATURE-OF noisy labels. maximal safe set USED-FOR early stopped network. two - phase training method USED-FOR noise - free training. Prestopping HYPONYM-OF two - phase training method. image benchmark data sets EVALUATE-FOR method. method COMPARE state - of - the - art methods. state - of - the - art methods COMPARE method. image benchmark data sets EVALUATE-FOR state - of - the - art methods. test error EVALUATE-FOR state - of - the - art methods. test error EVALUATE-FOR method. Method is deep neural network. OtherScientificTerm are label noise, and real - world noise. ",This paper proposes a two-phase training method for noise-free training of deep neural networks. The main idea is to train an early-stopped network with a maximal safe set (MSSL) to prevent overfitting in the early stages of training. The authors show that their method outperforms state-of-the-art methods on several benchmark datasets.,This paper proposes a two-phase training method for noise-free training of deep neural networks. The main idea is to train an early-stopped network with a maximal safe set (MSSL) to prevent overfitting in the early stages of training. The authors show that their method outperforms state-of-the-art methods on several benchmark datasets.
12204,SP:8316872d8b388587bf25f724c80155b25b6cb68e,framework USED-FOR generalization. reinforcement learning USED-FOR actions. regularization metrics USED-FOR generalization. generalization USED-FOR policy. action representations USED-FOR reinforcement learning architecture. policy USED-FOR zero - shot generalization. representation learning method CONJUNCTION policy. policy CONJUNCTION representation learning method. representation learning method USED-FOR zero - shot generalization. sequential decision - making environments USED-FOR zero - shot generalization. Task is intelligence. OtherScientificTerm is action ’s functionality. Method is unsupervised representation learning. ,This paper proposes an unsupervised representation learning framework for zero-shot generalization in reinforcement learning. The framework is based on the idea that the action representations learned by a policy can be used as a regularization metric to measure the generalization performance of the policy. The paper also proposes a representation learning method to improve the performance of a policy in a sequential decision-making environment. Experiments show that the proposed method outperforms baselines in terms of generalization.,This paper proposes an unsupervised representation learning framework for zero-shot generalization in reinforcement learning. The framework is based on the idea that the action representations learned by a policy can be used as a regularization metric to measure the generalization performance of the policy. The paper also proposes a representation learning method to improve the performance of a policy in a sequential decision-making environment. Experiments show that the proposed method outperforms baselines in terms of generalization.
12213,SP:f534d51192eaacc6cb6bfd365e6d959d9dd498b2,"word2vec CONJUNCTION GloVe. GloVe CONJUNCTION word2vec. vector word embeddings USED-FOR Deep learning natural language processing models. word2vec HYPONYM-OF vector word embeddings. GloVe HYPONYM-OF vector word embeddings. continuous vectors USED-FOR it. embedding vectors USED-FOR dictionary. word embedding matrix USED-FOR inference. word2ket CONJUNCTION word2ketXS. word2ketXS CONJUNCTION word2ket. word embedding matrix USED-FOR training. quantum computing USED-FOR approaches. accuracy EVALUATE-FOR natural language processing tasks. natural language processing tasks EVALUATE-FOR approach. accuracy EVALUATE-FOR approach. OtherScientificTerm are discrete sequence of words, and GPU memory. Material is text corpus. Generic is embeddings. ",This paper proposes a novel approach for learning word embeddings from discrete sequence of words. The key idea is to learn a vector word embedding matrix that can be used for training and inference. The proposed method is based on the idea of word2vec and word2ketXS. The authors show that the proposed method outperforms existing approaches on a number of tasks.,This paper proposes a novel approach for learning word embeddings from discrete sequence of words. The key idea is to learn a vector word embedding matrix that can be used for training and inference. The proposed method is based on the idea of word2vec and word2ketXS. The authors show that the proposed method outperforms existing approaches on a number of tasks.
12222,SP:3df499068ffe6c995457c2174f987cb0ae3c2551,"machine learning approach USED-FOR policy. Imitation Learning ( IL ) HYPONYM-OF machine learning approach. Imitation Learning ( IL ) USED-FOR policy. human players PART-OF video games. IL USED-FOR reinforcement learning ( RL ). they USED-FOR average ” policy. dataset USED-FOR average ” policy. behavioral descriptions USED-FOR state - action pairs. approach USED-FOR neural network policy. behavior description USED-FOR neural network policy. approach USED-FOR policy. human demonstrations USED-FOR build - order planning task. StarCraft II FEATURE-OF build - order planning task. StarCraft II FEATURE-OF human demonstrations. human demonstrations USED-FOR policy. Dimensionality reduction techniques USED-FOR lowdimensional behavioral space. high - dimensional army unit composition USED-FOR lowdimensional behavioral space. UCB1 algorithm USED-FOR policy. policy COMPARE IL baseline approach. IL baseline approach COMPARE policy. Generic is it. Method are IL approaches, and Behavioral Repertoire Imitation Learning ( BRIL ). OtherScientificTerm is repertoire of behaviors. ",This paper proposes a method for imitation learning (IL) for reinforcement learning (RL) in video games. The main idea is to use a dataset of human demonstrations to train a neural network policy to imitate the behavior of a human player in a video game. The dataset consists of a high-dimensional environment and a low-dimensional behavioral space. The authors propose a UCB1 algorithm to learn a policy from the human demonstrations. The proposed method is evaluated on the StarCraft II build-order planning task. ,This paper proposes a method for imitation learning (IL) for reinforcement learning (RL) in video games. The main idea is to use a dataset of human demonstrations to train a neural network policy to imitate the behavior of a human player in a video game. The dataset consists of a high-dimensional environment and a low-dimensional behavioral space. The authors propose a UCB1 algorithm to learn a policy from the human demonstrations. The proposed method is evaluated on the StarCraft II build-order planning task. 
12231,SP:db15d3cc3e95173ca6d4fd88313d89a739d1c910,"stochastic gradient descent optimization USED-FOR lottery ticket. weight magnitude FEATURE-OF model. gradual pruning COMPARE one - shot pruning. one - shot pruning COMPARE gradual pruning. accuracy EVALUATE-FOR one - shot pruning. accuracy EVALUATE-FOR gradual pruning. CIFAR10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR10. ImageNet EVALUATE-FOR ResNet architectures. CIFAR10 EVALUATE-FOR ResNet architectures. ImageNet FEATURE-OF ResNet50. Task is lottery ticket hypothesis. Method are small, sparsified neural networks, sparsified model, iterative pruning, and memorization capacity analysis. Generic is network. OtherScientificTerm are rewinding, transferability of the winning lottery tickets, winning ticket, structure of winning lottery tickets, lottery tickets, Pruning, complex patterns, and pruning rate. Metric is Top-1 accuracy. ","This paper studies the lottery ticket hypothesis, which is a well-studied topic in the literature. The authors propose a method to prune a sparsified neural network to improve the transferability of winning lottery tickets. The method is based on iterative pruning, where the pruning rate is determined by the weight magnitude of the model and the number of times it has been pruned. They show that this method can be used to reduce the memorization capacity of a sparser model. They also show that one-shot pruning is more effective than gradual pruning. ","This paper studies the lottery ticket hypothesis, which is a well-studied topic in the literature. The authors propose a method to prune a sparsified neural network to improve the transferability of winning lottery tickets. The method is based on iterative pruning, where the pruning rate is determined by the weight magnitude of the model and the number of times it has been pruned. They show that this method can be used to reduce the memorization capacity of a sparser model. They also show that one-shot pruning is more effective than gradual pruning. "
12240,SP:06d2a46282e34302050e81a1be8a2627acb159ee,"low - confidence detections USED-FOR unknowns. CNNs USED-FOR product operation. UDN USED-FOR product operation. CNNs USED-FOR UDN. convolutional layers USED-FOR features. product operations USED-FOR UDN. UDN USED-FOR detecting unknowns. learning process USED-FOR UDN. information - theoretic regularization strategy USED-FOR learning process. information - theoretic regularization strategy USED-FOR UDN. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. CIFAR-100 CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR-100. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. SVHN HYPONYM-OF benchmark image datasets. MNIST HYPONYM-OF benchmark image datasets. CIFAR-100 HYPONYM-OF benchmark image datasets. CIFAR-10 HYPONYM-OF benchmark image datasets. UDN COMPARE state - of - the - art methods. state - of - the - art methods COMPARE UDN. accuracy EVALUATE-FOR state - of - the - art methods. accuracy EVALUATE-FOR UDN. classification accuracy EVALUATE-FOR UDN. Method is image classification systems. Generic are they, and problem. ","This paper proposes a novel method for low-confidence detection of unknowns in image classification systems. The proposed method is based on a novel regularization strategy that uses information theoretic regularization to regularize the learning process of the convolutional layers. The method is evaluated on CIFAR-10, SVHN, and MNIST datasets. The results show that the proposed method outperforms state-of-the-art methods.","This paper proposes a novel method for low-confidence detection of unknowns in image classification systems. The proposed method is based on a novel regularization strategy that uses information theoretic regularization to regularize the learning process of the convolutional layers. The method is evaluated on CIFAR-10, SVHN, and MNIST datasets. The results show that the proposed method outperforms state-of-the-art methods."
12249,SP:fa3e729469e74cac44745008fe65c01cc97c9820,"approach USED-FOR approximate Bayesian inference. Variational inference ( VI ) USED-FOR approximate Bayesian inference. approximate Bayesian inference USED-FOR highly parameterized models. Variational inference ( VI ) HYPONYM-OF approach. deep neural networks HYPONYM-OF highly parameterized models. method USED-FOR highly flexible variational distributions. coarse approximation USED-FOR method. benchmark tasks EVALUATE-FOR method. log - likelihood CONJUNCTION ELBO. ELBO CONJUNCTION log - likelihood. variational inference methods USED-FOR deep learning. method COMPARE variational inference methods. variational inference methods COMPARE method. method USED-FOR deep learning. log - likelihood EVALUATE-FOR variational inference methods. ELBO EVALUATE-FOR variational inference methods. log - likelihood EVALUATE-FOR method. ELBO EVALUATE-FOR method. CIFAR10 EVALUATE-FOR residual networks. Task is variational inference. Generic are distribution, and it. OtherScientificTerm are variational families, and Evidence Lower BOund. Method are larger scale models, and VI. ","This paper proposes a new variational inference method for approximate Bayesian inference. The method is based on the idea that the variational family of distributions can be decomposed into a family of variational families, which can be used to approximate the Bayesian distribution. The authors show that the proposed method outperforms existing methods on a number of benchmark tasks. ","This paper proposes a new variational inference method for approximate Bayesian inference. The method is based on the idea that the variational family of distributions can be decomposed into a family of variational families, which can be used to approximate the Bayesian distribution. The authors show that the proposed method outperforms existing methods on a number of benchmark tasks. "
12258,SP:8e20d28a2a3a6f8f0b6a29a09a10fb8c7a011e86,user - defined metrics USED-FOR reinforcement learning. reinforcement learning USED-FOR Sequence generation models. correlated Monte Carlo ( MC ) rollouts USED-FOR variance control. correlated Monte Carlo ( MC ) rollouts USED-FOR policy gradient estimator. policy gradient estimator USED-FOR contextual generation of categorical sequences. rollouts USED-FOR model uncertainty. correlated MC rollouts USED-FOR binary - tree softmax models. large vocabulary scenarios FEATURE-OF high generation cost. binary actions FEATURE-OF categorical action. neural program synthesis CONJUNCTION image captioning. image captioning CONJUNCTION neural program synthesis. image captioning EVALUATE-FOR methods. neural program synthesis EVALUATE-FOR methods. gradient variance EVALUATE-FOR methods. Generic is method. OtherScientificTerm is correlation. ,"This paper proposes a new method to control the variance of the policy gradient estimator for categorical sequence generation in reinforcement learning. The proposed method is based on the idea of correlated Monte Carlo (MC) rollouts, which can be used to reduce the model uncertainty. The authors show that the proposed method can be applied to both binary-tree softmax models and neural program synthesis. ","This paper proposes a new method to control the variance of the policy gradient estimator for categorical sequence generation in reinforcement learning. The proposed method is based on the idea of correlated Monte Carlo (MC) rollouts, which can be used to reduce the model uncertainty. The authors show that the proposed method can be applied to both binary-tree softmax models and neural program synthesis. "
12267,SP:ab51af66e626b1b03bbf0de7a5237370e941925c,"Goal recognition USED-FOR applications. goal recognition design USED-FOR online goal recognition process. goal recognition control HYPONYM-OF stages. deceptive opponent modeling HYPONYM-OF stages. proactively static interdiction USED-FOR goal recognition control. worst case distinctiveness ( wcd ) USED-FOR nondistinctive path. approach USED-FOR goal recognition process. opponent ’s deceptive behavior USED-FOR approach. opponent ’s deceptive behavior USED-FOR goal recognition process. OtherScientificTerm are hard action removal, and wcd. Method is S - GRC. ","This paper proposes a method for online goal recognition based on deceptive opponent modeling. The proposed method is based on the idea of worst-case distinctiveness (wcd), which is a measure of the distance between a goal and an opponent’s goal. The authors show that the best-case wcd can be achieved by using an adversarial model to model the goal recognition process. The paper also shows that the proposed method can be applied to the problem of hard action removal.","This paper proposes a method for online goal recognition based on deceptive opponent modeling. The proposed method is based on the idea of worst-case distinctiveness (wcd), which is a measure of the distance between a goal and an opponent’s goal. The authors show that the best-case wcd can be achieved by using an adversarial model to model the goal recognition process. The paper also shows that the proposed method can be applied to the problem of hard action removal."
12276,SP:e9d173bdf0b650fd093226cfb4607032c905cf61,large mini - batch sizes USED-FOR Generative Adversarial Networks ( GANs ). Coreset - selection USED-FOR active learning. Coreset - selection USED-FOR method. Coreset - selection USED-FOR batch. training time CONJUNCTION memory usage. memory usage CONJUNCTION training time. it USED-FOR GANs. GANs USED-FOR anomaly detection. training time EVALUATE-FOR GAN variants. it USED-FOR dropped modes. technique USED-FOR GAN variants. memory usage EVALUATE-FOR GAN variants. it USED-FOR anomaly detection. dropped modes PART-OF synthetic dataset. memory usage EVALUATE-FOR technique. training time EVALUATE-FOR technique. synthetic dataset EVALUATE-FOR it. Method is GAN. Material is real ’ images. Generic is them. ,"This paper proposes a new active learning method for large mini-batch training of GANs. The proposed method is based on the idea of Coreset-selection, which is an active learning technique that selects a subset of samples from the training set to be used for active learning. The authors show that the proposed method outperforms existing active learning methods in terms of training time and memory usage. They also show that their method is able to detect dropped modes in the training data.","This paper proposes a new active learning method for large mini-batch training of GANs. The proposed method is based on the idea of Coreset-selection, which is an active learning technique that selects a subset of samples from the training set to be used for active learning. The authors show that the proposed method outperforms existing active learning methods in terms of training time and memory usage. They also show that their method is able to detect dropped modes in the training data."
12285,SP:f174ef07670a31a3ce647910c59040a19ea52d7a,artificial neurons PART-OF recurrent neural networks ( RNNs ). maximum likelihood USED-FOR recurrent neural networks ( RNNs ). restorative Brownian motion CONJUNCTION hand - drawn sketch dataset. hand - drawn sketch dataset CONJUNCTION restorative Brownian motion. information plane FEATURE-OF RNNs. hand - drawn sketch dataset HYPONYM-OF datasets. restorative Brownian motion HYPONYM-OF datasets. RNNs USED-FOR predictive information. maximum likelihood CONJUNCTION contrastive loss training. contrastive loss training CONJUNCTION maximum likelihood. noise USED-FOR hidden state. predictive information USED-FOR maximum likelihood. RNNs USED-FOR contrastive loss training. predictive information USED-FOR contrastive loss training. noise USED-FOR past information. Method is biological neurons. ,"This paper studies the relationship between maximum likelihood and predictive information in the information plane of RNNs. The authors show that the maximum likelihood of a RNN is a function of the hidden state of the network, and that the predictive information can be used for contrastive loss training and maximum likelihood training. They also show that predictive information is used to improve the performance of contrastive training. ","This paper studies the relationship between maximum likelihood and predictive information in the information plane of RNNs. The authors show that the maximum likelihood of a RNN is a function of the hidden state of the network, and that the predictive information can be used for contrastive loss training and maximum likelihood training. They also show that predictive information is used to improve the performance of contrastive training. "
12294,SP:f87a75fa12ddeb7538c4522d025e679f2c6dd237,"techniques USED-FOR delusion. methods USED-FOR delusional bias. Q - approximators USED-FOR methods. penalization scheme USED-FOR Q - labels. search framework USED-FOR premature ( implicit ) policy commitments. search framework USED-FOR Q - approximators. methods USED-FOR Q - learning. Atari games EVALUATE-FOR Q - learning. Atari games EVALUATE-FOR methods. Task is Delusional bias. Method are approximate Q - learning, and tabular value estimates. OtherScientificTerm are greedy policy class, and expressible policy class. ",This paper studies the problem of delusional bias in Q-learning. The authors propose a search framework to identify the Q-labels that lead to premature (implicit) policy commitments and penalize them. The proposed method is evaluated on Atari games and shows promising results. ,This paper studies the problem of delusional bias in Q-learning. The authors propose a search framework to identify the Q-labels that lead to premature (implicit) policy commitments and penalize them. The proposed method is evaluated on Atari games and shows promising results. 
12303,SP:827b0d2e2e3cf434c02b7f221bb9b2e0388e48b8,"approaches USED-FOR unsupervised object - oriented scene representation learning. spatial - attention CONJUNCTION scene - mixture approaches. scene - mixture approaches CONJUNCTION spatial - attention. scene - mixture approaches USED-FOR approaches. spatial - attention USED-FOR approaches. spatial - attention CONJUNCTION scene - mixture approaches. scene - mixture approaches CONJUNCTION spatial - attention. generative latent variable model USED-FOR unified probabilistic modeling framework. scene - mixture approaches USED-FOR unified probabilistic modeling framework. spatial - attention USED-FOR unified probabilistic modeling framework. SPACE HYPONYM-OF generative latent variable model. factorized object representations USED-FOR foreground objects. SPACE USED-FOR factorized object representations. SPACE USED-FOR methods. SPACE USED-FOR scalability problems. parallel spatial - attention USED-FOR methods. IODINE CONJUNCTION GENESIS. GENESIS CONJUNCTION IODINE. SPACE COMPARE SPAIR. SPAIR COMPARE SPACE. Atari CONJUNCTION 3D - Rooms. 3D - Rooms CONJUNCTION Atari. SPACE COMPARE IODINE. IODINE COMPARE SPACE. SPAIR CONJUNCTION IODINE. IODINE CONJUNCTION SPAIR. SPACE COMPARE GENESIS. GENESIS COMPARE SPACE. OtherScientificTerm are complex multi - object scenes, higher - level cognition, and complex morphology. Task is modeling real - world scenes. Generic is models. ","This paper proposes a generative latent variable model for unsupervised object-oriented scene representation learning. The proposed model is based on the idea of spatial-attention, which is an extension of spatial attention and scene-mixture approaches. The authors show that the proposed model outperforms the baselines on Atari and 3D Rooms. ","This paper proposes a generative latent variable model for unsupervised object-oriented scene representation learning. The proposed model is based on the idea of spatial-attention, which is an extension of spatial attention and scene-mixture approaches. The authors show that the proposed model outperforms the baselines on Atari and 3D Rooms. "
12312,SP:73d7d614378cbb6a8d7347dca790675674e0eadb,classification tasks EVALUATE-FOR Convolutional Neural Networks ( CNN ). depthwise convolution CONJUNCTION pointwise convolution. pointwise convolution CONJUNCTION depthwise convolution. convolution CONJUNCTION depthwise convolution. depthwise convolution CONJUNCTION convolution. convolution PART-OF depthwise separable convolution. accuracies EVALUATE-FOR convolution. method USED-FOR compressing CNN. FALCON HYPONYM-OF method. FALCON USED-FOR compressing CNN. mathematical formulation USED-FOR convolution kernel. FALCON USED-FOR convolution methods. depthwise separable convolution USED-FOR convolution methods. EHP USED-FOR convolution methods. compression CONJUNCTION computation reduction rates. computation reduction rates CONJUNCTION compression. computation reduction rates EVALUATE-FOR generalized version rank - k FALCON. accuracy EVALUATE-FOR generalized version rank - k FALCON. compression EVALUATE-FOR generalized version rank - k FALCON. FALCON PART-OF convolution unit ShuffleUnitV2. FALCON USED-FOR FALCON - branch. FALCON - branch COMPARE methods. methods COMPARE FALCON - branch. FALCON COMPARE methods. methods COMPARE FALCON. FALCON CONJUNCTION FALCON - branch. FALCON - branch CONJUNCTION FALCON. FALCON - branch COMPARE CNN models. CNN models COMPARE FALCON - branch. methods COMPARE CNN models. CNN models COMPARE methods. depthwise separable convolution USED-FOR methods. compression EVALUATE-FOR CNN models. accuracy EVALUATE-FOR CNN models. parameters CONJUNCTION floating - point operations. floating - point operations CONJUNCTION parameters. rank - k FALCON COMPARE convolution. convolution COMPARE rank - k FALCON. accuracy EVALUATE-FOR convolution. floating - point operations USED-FOR rank - k FALCON. parameters USED-FOR rank - k FALCON. accuracy EVALUATE-FOR rank - k FALCON. Generic is they. Method is heuristic approaches. ,"This paper proposes a new method for compressing convolutional neural networks (CNNs). The proposed method, called FALCON, is based on a mathematical formulation of the convolution kernel. The authors show that the proposed method can be used to compress convolution kernels and reduce the computational cost of convolution. The method is evaluated on the Shuffle Unit V2 and the rank-k classification task.","This paper proposes a new method for compressing convolutional neural networks (CNNs). The proposed method, called FALCON, is based on a mathematical formulation of the convolution kernel. The authors show that the proposed method can be used to compress convolution kernels and reduce the computational cost of convolution. The method is evaluated on the Shuffle Unit V2 and the rank-k classification task."
12321,SP:35d45ed014320d8dff22f3531f805d15fa91dafb,"Batch Normalization HYPONYM-OF normalization layers. normalization algorithm USED-FOR small batch sizes. Ghost Batch Normalization USED-FOR small and medium batch sizes. scaling and shifting parameters FEATURE-OF weight decay regularization. Batch and Group Normalization USED-FOR normalization algorithm. SVHN CONJUNCTION Caltech-256. Caltech-256 CONJUNCTION SVHN. CUB-2011 CONJUNCTION ImageNet. ImageNet CONJUNCTION CUB-2011. Oxford Flowers-102 CONJUNCTION CUB-2011. CUB-2011 CONJUNCTION Oxford Flowers-102. CIFAR-100 CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR-100. Caltech-256 CONJUNCTION Oxford Flowers-102. Oxford Flowers-102 CONJUNCTION Caltech-256. ImageNet HYPONYM-OF datasets. CIFAR-100 HYPONYM-OF datasets. CUB-2011 HYPONYM-OF datasets. Caltech-256 HYPONYM-OF datasets. Oxford Flowers-102 HYPONYM-OF datasets. SVHN HYPONYM-OF datasets. Method are neural network architectures, and deep architectures. Generic are they, and method. Task is inference normalization statistics. OtherScientificTerm is training vs. inference discrepancy. ","This paper proposes a new normalization algorithm for small and medium batch sizes. The proposed method, called Ghost Batch Normalization (Ghost BN), is based on Batch and Group Normalization. The authors show that the proposed method is able to reduce the training vs. inference discrepancy between large and small batch sizes, and that it is also able to improve the performance of deep neural networks. ","This paper proposes a new normalization algorithm for small and medium batch sizes. The proposed method, called Ghost Batch Normalization (Ghost BN), is based on Batch and Group Normalization. The authors show that the proposed method is able to reduce the training vs. inference discrepancy between large and small batch sizes, and that it is also able to improve the performance of deep neural networks. "
12330,SP:39d187474524c6b7de1ce6fd811ec53edae0a8fc,"outliers CONJUNCTION misclassifications. misclassifications CONJUNCTION outliers. manual data inspection USED-FOR privacy - sensitive datasets. metrics CONJUNCTION model parameters. model parameters CONJUNCTION metrics. model parameters HYPONYM-OF aggregated outputs. metrics HYPONYM-OF aggregated outputs. federated methods CONJUNCTION formal differential privacy guarantees. formal differential privacy guarantees CONJUNCTION federated methods. formal differential privacy guarantees FEATURE-OF generative models. federated methods USED-FOR generative models. algorithm USED-FOR differentially private federated GANs. text with differentially private federated RNNs CONJUNCTION images. images CONJUNCTION text with differentially private federated RNNs. algorithm USED-FOR images. Task are machine learning, Manual inspection of raw data, and federated learning. Generic are models, two, and methods. OtherScientificTerm are modeling hypotheses, and human - provided labels. ","This paper studies the problem of manual data inspection in the context of federated learning. In particular, the authors propose an algorithm for differentially private federated GANs. The proposed algorithm is based on the notion of differential privacy, which is a generalization of the differential privacy guarantee of previous work. The authors show that the proposed algorithm can be applied to differentially privacy-sensitive datasets, such as text and images. They also show that their algorithm can also be used to improve the performance of existing federated methods.","This paper studies the problem of manual data inspection in the context of federated learning. In particular, the authors propose an algorithm for differentially private federated GANs. The proposed algorithm is based on the notion of differential privacy, which is a generalization of the differential privacy guarantee of previous work. The authors show that the proposed algorithm can be applied to differentially privacy-sensitive datasets, such as text and images. They also show that their algorithm can also be used to improve the performance of existing federated methods."
12339,SP:1931ec4c3cd0dbb411cf1bc0f9776b7e26e3ad78,"Learning diverse and natural behaviors USED-FOR intelligent characters. method USED-FOR generating long range diverse and distinctive behaviors. method USED-FOR motion of human. non - parametric techniques CONJUNCTION parametric ones. parametric ones CONJUNCTION non - parametric techniques. non - parametric techniques USED-FOR method. parametric ones USED-FOR method. memory bank USED-FOR motion references. deep network USED-FOR synthesis. skeleton datasets EVALUATE-FOR method. method COMPARE parametric and non - parametric baselines. parametric and non - parametric baselines COMPARE method. skeleton datasets EVALUATE-FOR parametric and non - parametric baselines. OtherScientificTerm are long range diverse and distinctive behaviors, and starting and ending state. Material is animated world. ",This paper proposes a method for generating long-range diverse and distinctive behaviors from a skeleton dataset. The method is based on the idea of learning a memory bank to store motion references for each skeleton. The memory bank is then used to generate a sequence of skeleton trajectories that can be used as a reference for the next skeleton to be synthesized. The proposed method is evaluated on a number of skeleton datasets.,This paper proposes a method for generating long-range diverse and distinctive behaviors from a skeleton dataset. The method is based on the idea of learning a memory bank to store motion references for each skeleton. The memory bank is then used to generate a sequence of skeleton trajectories that can be used as a reference for the next skeleton to be synthesized. The proposed method is evaluated on a number of skeleton datasets.
12348,SP:3a09bdf2e5a17d271f890fd28113202afb9ae761,neural networks USED-FOR natural language processing tasks. model USED-FOR semantic compositions. hierarchies FEATURE-OF non - additivity and context independent importance attributions. inconsistent explanation quality EVALUATE-FOR models. contextual decomposition HYPONYM-OF hierarchical explanations. Sampling and Contextual Decomposition ( SCD ) algorithm CONJUNCTION Sampling and Occlusion ( SOC ) algorithm. Sampling and Occlusion ( SOC ) algorithm CONJUNCTION Sampling and Contextual Decomposition ( SCD ) algorithm. algorithms COMPARE hierarchical explanation algorithms. hierarchical explanation algorithms COMPARE algorithms. LSTM models CONJUNCTION BERT Transformer models. BERT Transformer models CONJUNCTION LSTM models. Human and metrics evaluation EVALUATE-FOR BERT Transformer models. Human and metrics evaluation EVALUATE-FOR algorithms. Human and metrics evaluation EVALUATE-FOR LSTM models. BERT Transformer models EVALUATE-FOR algorithms. Human and metrics evaluation EVALUATE-FOR hierarchical explanation algorithms. algorithms USED-FOR semantic composition. algorithms USED-FOR classification rules. models USED-FOR semantic composition. Task is hierarchical explanation of neural network predictions. OtherScientificTerm is word and phrase compositions. ,"This paper studies the problem of hierarchical explanation of neural network predictions for natural language processing tasks. The authors propose two algorithms for hierarchical explanation: Sampling and Occlusion (SCD) and Contextual Decomposition (SCC) algorithms. The SCD algorithm is based on sampling and occlusion, and the SCC algorithm uses contextual decomposition to decompose the word and phrase compositions. The proposed algorithms are evaluated on both human and metrics evaluation. ","This paper studies the problem of hierarchical explanation of neural network predictions for natural language processing tasks. The authors propose two algorithms for hierarchical explanation: Sampling and Occlusion (SCD) and Contextual Decomposition (SCC) algorithms. The SCD algorithm is based on sampling and occlusion, and the SCC algorithm uses contextual decomposition to decompose the word and phrase compositions. The proposed algorithms are evaluated on both human and metrics evaluation. "
12357,SP:9ec1740e58d1b07a6b1c6130ec7e23c370efb701,"AI saliency map method USED-FOR deep convolutional neural networks ( CNN ). AI saliency map method COMPARE gradient methods. gradient methods COMPARE AI saliency map method. deep convolutional neural networks ( CNN ) COMPARE gradient methods. gradient methods COMPARE deep convolutional neural networks ( CNN ). accuracy EVALUATE-FOR It. Saliency Map Order Equivalence USED-FOR saliency measures. Layer Ordered Visualization of Information USED-FOR scale / layer contributions. scale information contributions PART-OF network. it COMPARE Guided Backprop. Guided Backprop COMPARE it. layers PART-OF network. forward pass USED-FOR layers. forward pass USED-FOR method. Grad - CAM++ CONJUNCTION Smooth Grad - CAM++. Smooth Grad - CAM++ CONJUNCTION Grad - CAM++. Grad - CAM CONJUNCTION Grad - CAM++. Grad - CAM++ CONJUNCTION Grad - CAM. method COMPARE Guided Backprop. Guided Backprop COMPARE method. method COMPARE class activation methods. class activation methods COMPARE method. Guided Backprop COMPARE class activation methods. class activation methods COMPARE Guided Backprop. Smooth Grad - CAM++ HYPONYM-OF class activation methods. Grad - CAM HYPONYM-OF class activation methods. Grad - CAM++ HYPONYM-OF class activation methods. cell phones CONJUNCTION low cost industrial devices. low cost industrial devices CONJUNCTION cell phones. robots CONJUNCTION cell phones. cell phones CONJUNCTION robots. resource limited platforms USED-FOR methods. low cost industrial devices HYPONYM-OF resource limited platforms. robots HYPONYM-OF resource limited platforms. cell phones HYPONYM-OF resource limited platforms. method USED-FOR CNNs 1. Generic is technique. OtherScientificTerm are network scale, and memory footprint. Method are saliency map, and saliency map methods. Task is satellite image processing. ",This paper proposes a saliency map method for deep convolutional neural networks (CNNs). The proposed method is based on the idea of order equivalence between saliency measures. The authors show that the proposed method outperforms the state-of-the-art methods in terms of accuracy and memory footprint. ,This paper proposes a saliency map method for deep convolutional neural networks (CNNs). The proposed method is based on the idea of order equivalence between saliency measures. The authors show that the proposed method outperforms the state-of-the-art methods in terms of accuracy and memory footprint. 
12366,SP:7f11fa931f4085f7227cc870eba4a3aac4b1bf42,"Non - autoregressive models USED-FOR text generation tasks. position modeling PART-OF non - autoregressive text generation. PNAT USED-FOR text generative process. positions USED-FOR PNAT. PNAT COMPARE baselines. baselines COMPARE PNAT. machine translation CONJUNCTION paraphrase generation tasks. paraphrase generation tasks CONJUNCTION machine translation. paraphrase generation tasks EVALUATE-FOR PNAT. machine translation EVALUATE-FOR PNAT. OtherScientificTerm are positions of generated words, and latent variable. ",This paper proposes a new method for non-autoregressive text generation based on position modeling (PNAT). The method is based on the idea that the latent variable of a generative model should be modeled as a function of the position of the generated words in the latent space. The authors show that the proposed method outperforms baselines on a number of text generation tasks. ,This paper proposes a new method for non-autoregressive text generation based on position modeling (PNAT). The method is based on the idea that the latent variable of a generative model should be modeled as a function of the position of the generated words in the latent space. The authors show that the proposed method outperforms baselines on a number of text generation tasks. 
12375,SP:e1c40112901b6ff905ae0e221fd3df4f545acd08,GANs USED-FOR generative model analysis. Random Path Generative Adversarial Network ( RPGAN ) HYPONYM-OF GANs. random paths PART-OF generator network. latent space FEATURE-OF GAN. latent space PART-OF RPGAN. random paths PART-OF latent space. design USED-FOR factors of variation. natural interpretability FEATURE-OF factors of variation. generator layers USED-FOR factors of variation. layers USED-FOR image generation process. RPGAN USED-FOR image generation process. RPGAN model USED-FOR incremental learning. interpretability EVALUATE-FOR RPGAN model. generation quality EVALUATE-FOR RPGAN model. OtherScientificTerm is Gaussian distribution. ,"This paper proposes a new generative adversarial network, called Random Path Generative Adversarial Network (RPGAN), which is an extension of the Random Path Generator (RPG) model. The key idea of RPGAN is to use random paths in the latent space of the generator network to improve the interpretability of the generated images. The authors show that the proposed method outperforms existing methods in terms of generation quality and interpretability. They also show that their method is more interpretable than existing methods.","This paper proposes a new generative adversarial network, called Random Path Generative Adversarial Network (RPGAN), which is an extension of the Random Path Generator (RPG) model. The key idea of RPGAN is to use random paths in the latent space of the generator network to improve the interpretability of the generated images. The authors show that the proposed method outperforms existing methods in terms of generation quality and interpretability. They also show that their method is more interpretable than existing methods."
12384,SP:1fec5468baaccb4a956399a829b62ac47494a6ac,"convolution USED-FOR spherical neural network. efficiency CONJUNCTION rotation equivariance. rotation equivariance CONJUNCTION efficiency. DeepSphere HYPONYM-OF method. graph representation of the sampled sphere USED-FOR method. graph USED-FOR equivariance. Generic is formulation. Method are anisotropic filters, and deepsphere. ","This paper proposes a method for learning a spherical neural network with anisotropic filters. The proposed method is based on a graph representation of the sampled sphere, which is then used to compute the equivariance of the convolutional neural network. The authors show that the proposed method can achieve better efficiency than existing methods for learning spherical neural networks. ","This paper proposes a method for learning a spherical neural network with anisotropic filters. The proposed method is based on a graph representation of the sampled sphere, which is then used to compute the equivariance of the convolutional neural network. The authors show that the proposed method can achieve better efficiency than existing methods for learning spherical neural networks. "
12393,SP:ae544fa9abd539e0c2e77fdb5541f5c5194feb9f,"Learning Invariant Representations USED-FOR deep classifiers. invariance USED-FOR compression of representations. weighting representations USED-FOR representation distributions. adaptability EVALUATE-FOR weighting representations. compression CONJUNCTION invariance of learned representations. invariance of learned representations CONJUNCTION compression. adaptability EVALUATE-FOR representation. compression risk EVALUATE-FOR representation. learning weighted representations USED-FOR constraint of invariance. Metric are minimal combined domain error, and risk of compression. OtherScientificTerm are representation invariance, and constraint. Generic is bound. Material is domain adaptation benchmark. Method is adaptation methods. ","This paper studies the problem of learning invariant representations in the domain adaptation setting. The authors propose a new bound on the risk of compression of representation invariance, which they call the minimal combined domain error (MCE) bound. This bound is based on the assumption that the learned representation is invariant to the compression of the target domain. They show that the MCE bound depends on the weighting of the learned representations. They also provide a theoretical analysis of this bound. ","This paper studies the problem of learning invariant representations in the domain adaptation setting. The authors propose a new bound on the risk of compression of representation invariance, which they call the minimal combined domain error (MCE) bound. This bound is based on the assumption that the learned representation is invariant to the compression of the target domain. They show that the MCE bound depends on the weighting of the learned representations. They also provide a theoretical analysis of this bound. "
12402,SP:39126802d517f93bdcbc47708a6aa1ed13bf2800,"learning USED-FOR user interface attributes. it USED-FOR synthetic training dataset. colors CONJUNCTION border radius. border radius CONJUNCTION colors. border radius CONJUNCTION shadow or text properties. shadow or text properties CONJUNCTION border radius. shadow or text properties HYPONYM-OF attributes. border radius HYPONYM-OF attributes. colors HYPONYM-OF attributes. imitation learning USED-FOR neural policy. approach USED-FOR inferring Android Button attribute values. accuracy EVALUATE-FOR dataset. real - world Google Play Store applications FEATURE-OF dataset. dataset EVALUATE-FOR approach. accuracy EVALUATE-FOR approach. Task is user interface implementation. Method are black box rendering engine, and neural models. Metric is pixel - level accuracy. OtherScientificTerm are attribute space, and Android Button attribute values. ","This paper proposes a method for inferring user interface attributes from a synthetic training dataset. The method is based on imitation learning. The authors propose to use a black box rendering engine to learn the user interface attribute values from the synthetic dataset, which is then used to train a neural policy. The proposed method is evaluated on real-world Google Play Store applications.","This paper proposes a method for inferring user interface attributes from a synthetic training dataset. The method is based on imitation learning. The authors propose to use a black box rendering engine to learn the user interface attribute values from the synthetic dataset, which is then used to train a neural policy. The proposed method is evaluated on real-world Google Play Store applications."
12411,SP:1c5d31363faf2b8c43f2698ad426bfffcc02ad03,"Transfer learning USED-FOR neural network classifiers. data scarcity CONJUNCTION computational limitations. computational limitations CONJUNCTION data scarcity. robust feature extractors PART-OF robust networks. feature extractors USED-FOR classifiers. strategies USED-FOR models. Generic are network, and model. Method are full - scale training, robust transfer learning, lifelong learning strategies, and adversarial training. Metric are robustness, accuracy, and generalization of adversarially trained models. ",This paper studies the problem of robust transfer learning in the context of adversarial training. The authors propose a new metric to measure the generalization ability of adversarially trained models. The metric is based on the robustness of the feature extractors of robust networks to adversarial attacks. The paper also proposes a lifelong learning strategy to learn robust features from adversarial examples. Experiments are conducted on CIFAR-10 and ImageNet to demonstrate the effectiveness of the proposed metric.,This paper studies the problem of robust transfer learning in the context of adversarial training. The authors propose a new metric to measure the generalization ability of adversarially trained models. The metric is based on the robustness of the feature extractors of robust networks to adversarial attacks. The paper also proposes a lifelong learning strategy to learn robust features from adversarial examples. Experiments are conducted on CIFAR-10 and ImageNet to demonstrate the effectiveness of the proposed metric.
12420,SP:06a047ae70a1a25dc6e8f317d6e492e211ad17ce,"natural language USED-FOR complex concepts. compositionality USED-FOR complex concepts. natural language USED-FOR compositionality. neural agents USED-FOR language games. compositionality FEATURE-OF language. neural agents USED-FOR communication protocols. neural iterated learning ( NIL ) algorithm USED-FOR structured type of language. neural iterated learning ( NIL ) algorithm USED-FOR interacting neural agents. OtherScientificTerm are limited vocabulary, and compositional language. Generic is languages. Method are NIL, probabilistic model of NIL, and neural agent communication. ",This paper proposes a neural iterated learning (NIL) algorithm for learning a structured type of language. The authors propose a probabilistic model of NIL and show that it can be used to improve the communication performance of neural agents in language games. They also show that NIL can be applied to the problem of learning a compositional language. ,This paper proposes a neural iterated learning (NIL) algorithm for learning a structured type of language. The authors propose a probabilistic model of NIL and show that it can be used to improve the communication performance of neural agents in language games. They also show that NIL can be applied to the problem of learning a compositional language. 
12429,SP:add48154b31c13f48aef740e665f23694fa83681,inference CONJUNCTION learning. learning CONJUNCTION inference. black - box algorithm USED-FOR inference. black - box algorithm USED-FOR learning. Adversarial Variational Inference and Learning ( AdVIL ) USED-FOR inference. Adversarial Variational Inference and Learning ( AdVIL ) USED-FOR learning. learning USED-FOR Markov random field ( MRF ). Adversarial Variational Inference and Learning ( AdVIL ) HYPONYM-OF black - box algorithm. variational distributions USED-FOR latent variables. AdVIL USED-FOR partition function. variational distributions USED-FOR partition function. partition function FEATURE-OF MRF. variational distributions USED-FOR AdVIL. variational distributions USED-FOR negative log - likelihood. negative log - likelihood FEATURE-OF MRF. stochastic gradient descent USED-FOR minimax optimization problem. minimax optimization problem USED-FOR negative log - likelihood. contrastive divergence COMPARE AdVIL. AdVIL COMPARE contrastive divergence. AdVIL USED-FOR MRFs. black - box methods COMPARE AdVIL. AdVIL COMPARE black - box methods. AdVIL USED-FOR log partition function. OtherScientificTerm is model structure. ,"This paper proposes AdVIL, a black-box algorithm for learning Markov random field (MRF). The main idea is to learn a negative log-likelihood for the partition function of the MRF using a variational distribution over the latent variables of the model. The authors show that the proposed algorithm is able to achieve better performance than the state-of-the-art in terms of the negative log likelihood. ","This paper proposes AdVIL, a black-box algorithm for learning Markov random field (MRF). The main idea is to learn a negative log-likelihood for the partition function of the MRF using a variational distribution over the latent variables of the model. The authors show that the proposed algorithm is able to achieve better performance than the state-of-the-art in terms of the negative log likelihood. "
12438,SP:b875f6417663e43dded41b6a6f1b9ab49ad954a2,"low - dimensional state of the environment USED-FOR robot manipulation tasks. state estimator USED-FOR reward function. reinforcement learning USED-FOR policy. high - dimensional observations USED-FOR reward function. indicator reward function USED-FOR goal - conditioned reinforcement learning. continuous state spaces FEATURE-OF indicator reward function. reward balancing CONJUNCTION reward filtering. reward filtering CONJUNCTION reward balancing. methods USED-FOR convergence. indicator rewards USED-FOR methods. reward filtering HYPONYM-OF methods. indicator rewards USED-FOR convergence. reward balancing HYPONYM-OF methods. method USED-FOR tasks. continuous state spaces USED-FOR method. continuous state spaces FEATURE-OF tasks. RGB - D images HYPONYM-OF continuous state spaces. RGB - D images USED-FOR rope manipulation. rope manipulation HYPONYM-OF continuous state spaces. OtherScientificTerm are deformable objects, high - dimensional sensor inputs, positive reward, positive rewards, ground - truth state, and rewards. Method is end - to - end policy. ","This paper proposes an end-to-end reinforcement learning method for goal-conditioned reinforcement learning. The key idea is to use a state estimator to estimate the reward function of a policy in the low-dimensional state space, which is then used as a reward function to guide the policy to the ground-truth state of the environment. The authors show that the proposed method converges to a ground truth state in the continuous state space. They also show that their method is able to achieve better performance than reward balancing and reward filtering methods.","This paper proposes an end-to-end reinforcement learning method for goal-conditioned reinforcement learning. The key idea is to use a state estimator to estimate the reward function of a policy in the low-dimensional state space, which is then used as a reward function to guide the policy to the ground-truth state of the environment. The authors show that the proposed method converges to a ground truth state in the continuous state space. They also show that their method is able to achieve better performance than reward balancing and reward filtering methods."
12447,SP:8ae78a6640be13e511242eab64101f74ebc4b30a,"Robustness verification USED-FOR prediction behavior. Robustness verification USED-FOR safety guarantees. Robustness verification USED-FOR neural networks. architectures USED-FOR neural networks. robustness verification problem USED-FOR Transformers. cross - nonlinearity CONJUNCTION cross - position dependency. cross - position dependency CONJUNCTION cross - nonlinearity. self - attention layers FEATURE-OF Transformers. robustness verification algorithm USED-FOR Transformers. method COMPARE naive Interval Bound Propagation. naive Interval Bound Propagation COMPARE method. method COMPARE those. those COMPARE method. certified robustness bounds COMPARE those. those COMPARE certified robustness bounds. naive Interval Bound Propagation USED-FOR those. naive Interval Bound Propagation USED-FOR certified robustness bounds. method USED-FOR certified robustness bounds. bounds USED-FOR Transformers. OtherScientificTerm is model behavior. Task are verification, and sentiment analysis. Generic is they. ","This paper studies the problem of robustness verification for neural networks. The authors propose a new method to certify the robustness of a neural network. The proposed method is based on the idea of self-attention layers, which is an extension of the transformer architecture. The paper shows that the proposed method can be used to certify robustness bounds for a wide range of models. The method is evaluated on a variety of tasks, including sentiment analysis, cross-nonlinearity, and cross-position dependency. ","This paper studies the problem of robustness verification for neural networks. The authors propose a new method to certify the robustness of a neural network. The proposed method is based on the idea of self-attention layers, which is an extension of the transformer architecture. The paper shows that the proposed method can be used to certify robustness bounds for a wide range of models. The method is evaluated on a variety of tasks, including sentiment analysis, cross-nonlinearity, and cross-position dependency. "
12456,SP:92cb7b1e88f3c8883ae6123c19e1ba24622464e6,"self - supervised learning USED-FOR natural language processing ( NLP ) tasks. pretrained language models USED-FOR self - supervised learning. syntactic and semantic NLP tasks EVALUATE-FOR pretrained models. real - world knowledge FEATURE-OF tasks. tasks EVALUATE-FOR pretrained models. zero - shot fact completion task USED-FOR pretrained models. BERT HYPONYM-OF pretrained models. weakly supervised pretraining objective USED-FOR model. objective USED-FOR Models. fact completion task EVALUATE-FOR objective. fact completion task EVALUATE-FOR Models. TriviaQA CONJUNCTION SearchQA. SearchQA CONJUNCTION TriviaQA. SearchQA CONJUNCTION Quasar - T. Quasar - T CONJUNCTION SearchQA. WebQuestions CONJUNCTION TriviaQA. TriviaQA CONJUNCTION WebQuestions. model COMPARE BERT. BERT COMPARE model. model USED-FOR downstream tasks. FIGER HYPONYM-OF fine - grained entity typing dataset. fine - grained entity typing dataset EVALUATE-FOR model. entity - related question answering datasets EVALUATE-FOR BERT. entity - related question answering datasets EVALUATE-FOR model. WebQuestions HYPONYM-OF entity - related question answering datasets. Quasar - T HYPONYM-OF entity - related question answering datasets. TriviaQA HYPONYM-OF entity - related question answering datasets. SearchQA HYPONYM-OF entity - related question answering datasets. Method is large - scale language modeling. OtherScientificTerm are knowledge, and real - world entities. ","This paper proposes a weakly supervised pretraining objective for self-supervised learning. The proposed objective is based on the fact completion task, which is a zero-shot task that is used to evaluate the performance of a pretrained model on a set of tasks. The authors show that the proposed objective outperforms the baselines on the task of fact completion. ","This paper proposes a weakly supervised pretraining objective for self-supervised learning. The proposed objective is based on the fact completion task, which is a zero-shot task that is used to evaluate the performance of a pretrained model on a set of tasks. The authors show that the proposed objective outperforms the baselines on the task of fact completion. "
12465,SP:4395d6f3e197df478eee84e092539dc370babd97,"image collection USED-FOR discovering novel classes. setting COMPARE semi - supervised learning. semi - supervised learning COMPARE setting. self - supervised learning USED-FOR representation. supervised classification of the labelled data CONJUNCTION clustering of the unlabelled data. clustering of the unlabelled data CONJUNCTION supervised classification of the labelled data. rank statistics USED-FOR model. rank statistics USED-FOR clustering the unlabelled images. model USED-FOR clustering the unlabelled images. labelled and unlabelled subsets of the data USED-FOR joint objective function. labeled data USED-FOR image representation. joint objective function USED-FOR data representation. classification benchmarks EVALUATE-FOR methods. approach COMPARE methods. methods COMPARE approach. methods USED-FOR novel category discovery. classification benchmarks EVALUATE-FOR novel category discovery. approach USED-FOR novel category discovery. classification benchmarks EVALUATE-FOR approach. OtherScientificTerm is labelled images. Method is general - purpose clustering model. Generic is latter. Material are unlabelled data, and labelled and unlabelled data. ","This paper proposes a general-purpose clustering model for the task of novel category discovery. The proposed method is based on self-supervised learning and self-clustering. The key idea is to learn a joint objective function between the labelled and unlabelled subsets of the data, which is then used to compute the rank statistics of the unlabelling subsets. The authors show that the proposed method outperforms existing methods on several benchmark datasets.","This paper proposes a general-purpose clustering model for the task of novel category discovery. The proposed method is based on self-supervised learning and self-clustering. The key idea is to learn a joint objective function between the labelled and unlabelled subsets of the data, which is then used to compute the rank statistics of the unlabelling subsets. The authors show that the proposed method outperforms existing methods on several benchmark datasets."
12474,SP:068c4e93c135968aef2637d2bfcba727a3c0f001,"selfsupervised robot interaction USED-FOR images. images HYPONYM-OF dynamical system. VP algorithms USED-FOR robotic manipulation and navigation domains. data - driven perception and planning USED-FOR VP algorithms. approach USED-FOR VP. nodes PART-OF graph. connectivity PART-OF graph. image samples PART-OF graph. semiparametric topological memory ( SPTM ) method HYPONYM-OF approach. deep image classification USED-FOR connectivity. topological connectivity FEATURE-OF graph. graph search methods USED-FOR planning. loss function USED-FOR connectivity classifier. manual tuning USED-FOR loss function. loss function USED-FOR SPTM. discriminative classifier USED-FOR energy function. contrastive predictive coding USED-FOR discriminative classifier. contrastive predictive coding USED-FOR energy function. hallucinated samples USED-FOR connectivity graph. connectivity graph USED-FOR zero - shot generalization. domain changes FEATURE-OF zero - shot generalization. simulated domains EVALUATE-FOR HTM. SPTM CONJUNCTION visual foresight methods. visual foresight methods CONJUNCTION SPTM. simulated domains EVALUATE-FOR visual foresight methods. HTM COMPARE SPTM. SPTM COMPARE HTM. HTM COMPARE visual foresight methods. visual foresight methods COMPARE HTM. simulated domains EVALUATE-FOR SPTM. visual foresight methods USED-FOR long - horizon planning. long - horizon planning EVALUATE-FOR HTM. plan quality EVALUATE-FOR visual foresight methods. plan quality EVALUATE-FOR HTM. Task is visual planning ( VP ). Method are Hallucinative Topological Memory ( HTM ), and conditional VAE model. OtherScientificTerm is context image of the domain. ","This paper proposes a new method for visual planning based on topological memory. The proposed method is based on the semiparametric topology memory (SPTM) method, which is an extension of the topological graph search method. The key idea is to learn a topological connectivity graph, which can then be used for planning. The authors show that the proposed method outperforms the state-of-the-art in terms of zero-shot generalization and long-horizon planning. ","This paper proposes a new method for visual planning based on topological memory. The proposed method is based on the semiparametric topology memory (SPTM) method, which is an extension of the topological graph search method. The key idea is to learn a topological connectivity graph, which can then be used for planning. The authors show that the proposed method outperforms the state-of-the-art in terms of zero-shot generalization and long-horizon planning. "
12483,SP:907d92896eda706e1526debb5a87b41bb1e978e0,"Large - scale benchmark datasets USED-FOR AI. benchmarks USED-FOR realistic problem distributions. spurious biases FEATURE-OF them. similar ( thus non - tail ) problems FEATURE-OF benchmarks. long - tail problems PART-OF real world problems. Adversarial Filters USED-FOR model - based reduction of dataset biases. AFLITE HYPONYM-OF iterative greedy algorithm. AFLITE USED-FOR reduced dataset. realistic problem distributions CONJUNCTION spurious biases. spurious biases CONJUNCTION realistic problem distributions. spurious biases FEATURE-OF reduced dataset. realistic problem distributions FEATURE-OF reduced dataset. AFOPTIMUM USED-FOR optimum bias reduction. AFLITE USED-FOR task. MNLI CONJUNCTION QNLI. QNLI CONJUNCTION MNLI. SNLI CONJUNCTION MNLI. MNLI CONJUNCTION SNLI. ImageNet CONJUNCTION SNLI. SNLI CONJUNCTION ImageNet. it USED-FOR benchmarks. SNLI CONJUNCTION QNLI. QNLI CONJUNCTION SNLI. ImageNet HYPONYM-OF benchmarks. QNLI HYPONYM-OF benchmarks. SNLI HYPONYM-OF benchmarks. MNLI HYPONYM-OF benchmarks. AFLITE USED-FOR measurable dataset biases. measurable dataset biases FEATURE-OF synthetic and real datasets. synthetic and real datasets EVALUATE-FOR AFLITE. K - nearest - neighbors USED-FOR dataset biases. Generic are filtered counterparts, and model. Metric is human performance. Task is bias reduction. ","This paper proposes an iterative greedy algorithm for bias reduction. The algorithm is based on Adversarial Filters (AFOPTIMUM), which is a model-based approach to reduce the bias of a dataset. The proposed algorithm is evaluated on synthetic and real-world datasets. The authors show that the proposed algorithm outperforms AFOPIMUM on a number of benchmark datasets.","This paper proposes an iterative greedy algorithm for bias reduction. The algorithm is based on Adversarial Filters (AFOPTIMUM), which is a model-based approach to reduce the bias of a dataset. The proposed algorithm is evaluated on synthetic and real-world datasets. The authors show that the proposed algorithm outperforms AFOPIMUM on a number of benchmark datasets."
12492,SP:82777947d2377efa897c6905261f5375b29a4c19,"decision metric USED-FOR models. matching networks CONJUNCTION prototypical networks. prototypical networks CONJUNCTION matching networks. prototypical networks PART-OF few - shot models. matching networks PART-OF few - shot models. softmax USED-FOR relative distance. batch normalization USED-FOR centering. Gaussian layer USED-FOR distance calculation. Gaussian layer USED-FOR prototypical network. support examples ’ distribution COMPARE centroid. centroid COMPARE support examples ’ distribution. distance calculation PART-OF prototypical network. support examples ’ distribution USED-FOR Gaussian layer. Method are Few - shot models, and prototypical few - shot models. Generic are They, and extension. OtherScientificTerm are class belongings, and null class ”. Material are Omniglot data set, matched test set, unmatched MNIST data, and MiniImageNet data set. Metric are classification accuracy, and test accuracy. ","This paper proposes a new decision metric for prototypical few-shot models. The proposed metric is based on the notion of centroid, which is a measure of the distance between the support examples and the centroid of the prototypical network. The centroid is defined as the difference between support examples’ distribution and that of the centroids of prototypical networks. The authors propose to use batch normalization to improve the accuracy of the proposed metric. They show that the proposed method outperforms existing methods on Omniglot and MiniImageNet datasets. ","This paper proposes a new decision metric for prototypical few-shot models. The proposed metric is based on the notion of centroid, which is a measure of the distance between the support examples and the centroid of the prototypical network. The centroid is defined as the difference between support examples’ distribution and that of the centroids of prototypical networks. The authors propose to use batch normalization to improve the accuracy of the proposed metric. They show that the proposed method outperforms existing methods on Omniglot and MiniImageNet datasets. "
12501,SP:4a6df2b39643f548dab806a0b128fe5a3ce4dadc,"Graph embedding techniques USED-FOR applications. learning on non - Euclidean data USED-FOR applications. node attribute information PART-OF graph embedding models. computational complexity CONJUNCTION memory usage. memory usage CONJUNCTION computational complexity. graph fusion USED-FOR graph. topology FEATURE-OF graph. topology CONJUNCTION node attribute information. node attribute information CONJUNCTION topology. GraphZoom HYPONYM-OF multi - level framework. graph CONJUNCTION node attribute information. node attribute information CONJUNCTION graph. graph fusion USED-FOR GraphZoom. it USED-FOR embeddings. embedding methods USED-FOR coarsened graph. GraphZoom USED-FOR embedding methods. graph datasets USED-FOR transductive and inductive tasks. transductive and inductive tasks EVALUATE-FOR approach. graph datasets EVALUATE-FOR approach. GraphZoom COMPARE unsupervised embedding methods. unsupervised embedding methods COMPARE GraphZoom. GraphZoom USED-FOR graph embedding process. graph embedding process COMPARE unsupervised embedding methods. unsupervised embedding methods COMPARE graph embedding process. classification accuracy EVALUATE-FOR GraphZoom. OtherScientificTerm is node attribute noise. Metric are accuracy, and scalability. Generic is them. ","This paper proposes a multi-level framework for learning graph embeddings on non-Euclidean graphs. The proposed method, GraphZoom, combines graph fusion and node attribute information to improve the performance of unsupervised graph embedding methods. The authors show that the proposed method is able to achieve better performance on several graph classification tasks. ","This paper proposes a multi-level framework for learning graph embeddings on non-Euclidean graphs. The proposed method, GraphZoom, combines graph fusion and node attribute information to improve the performance of unsupervised graph embedding methods. The authors show that the proposed method is able to achieve better performance on several graph classification tasks. "
12510,SP:e33a92e3a6acc668fa2022237e6d947b2eb8bd76,predictive coding USED-FOR image compression models. neural autoregressive image generation models USED-FOR absolute pixel intensities. prediction COMPARE absolute counterpart. absolute counterpart COMPARE prediction. model USED-FOR sharp transitions. model USED-FOR smooth transitions. absolute predictor USED-FOR model. relative predictor USED-FOR generating smooth transitions. mechanism COMPARE absolute prediction counterparts. absolute prediction counterparts COMPARE mechanism. unconditional image generation CONJUNCTION image colorization. image colorization CONJUNCTION unconditional image generation. image colorization CONJUNCTION super - resolution. super - resolution CONJUNCTION image colorization. benchmarks EVALUATE-FOR mechanism. benchmarks EVALUATE-FOR super - resolution. likelihood EVALUATE-FOR absolute prediction counterparts. benchmarks EVALUATE-FOR unconditional image generation. benchmarks EVALUATE-FOR absolute prediction counterparts. image colorization HYPONYM-OF benchmarks. likelihood EVALUATE-FOR mechanism. Material is natural images. Method is unified probabilistic model. ,"This paper proposes a new method for predicting the absolute pixel intensities of an image. The authors propose a unified probabilistic model that is able to generate smooth transitions between the absolute prediction and the relative predictor. The proposed method is evaluated on a variety of image generation tasks, including unconditional image generation, super-resolution, and image colorization. ","This paper proposes a new method for predicting the absolute pixel intensities of an image. The authors propose a unified probabilistic model that is able to generate smooth transitions between the absolute prediction and the relative predictor. The proposed method is evaluated on a variety of image generation tasks, including unconditional image generation, super-resolution, and image colorization. "
12519,SP:4224604c2650710cdf5be3ab8acc67c891944bed,"reinforcement learning CONJUNCTION Monte Carlo methods. Monte Carlo methods CONJUNCTION reinforcement learning. stationary distribution FEATURE-OF Markov chain. stationary distribution USED-FOR estimating quantities. Markov chain USED-FOR estimating quantities. estimation USED-FOR applications. variational divergence minimization USED-FOR constraint reformulations. off - line PageRank CONJUNCTION off - policy policy evaluation. off - policy policy evaluation CONJUNCTION off - line PageRank. off - policy policy evaluation HYPONYM-OF benchmark problems. off - line PageRank HYPONYM-OF benchmark problems. Task are real - world applications, and consistent estimation. OtherScientificTerm are transition operator, and stationary and empirical distributions. Generic are approach, and algorithm. Method are GenDICE, and error analysis. ","This paper studies the problem of estimating the transition operator of a Markov chain. The authors propose a variational divergence minimization (VDM) algorithm for this problem, which is based on the variational convergence of the stationary distribution and the empirical distribution. They show that the proposed method is able to achieve better convergence rates than existing methods. They also show that their method can be applied to off-policy optimization problems. ","This paper studies the problem of estimating the transition operator of a Markov chain. The authors propose a variational divergence minimization (VDM) algorithm for this problem, which is based on the variational convergence of the stationary distribution and the empirical distribution. They show that the proposed method is able to achieve better convergence rates than existing methods. They also show that their method can be applied to off-policy optimization problems. "
12528,SP:235998cafe7b558b6f6cf6c49b689ce84004af5d,"machine learning systems USED-FOR spurious patterns. statistical frameworks USED-FOR term. models USED-FOR spurious patterns. sentiment analysis CONJUNCTION natural language inference tasks. natural language inference tasks CONJUNCTION sentiment analysis. classifiers USED-FOR natural language inference tasks. classifiers USED-FOR sentiment analysis. classifiers COMPARE models. models COMPARE classifiers. classifiers USED-FOR spurious features. original or manipulated data USED-FOR classifiers. OtherScientificTerm are spurious associations, confounding, direct or indirect causal effects, and internal coherence. Task is natural language processing. Method is Classifiers. ","This paper studies the problem of classifying spurious patterns in natural language processing. The authors propose a new term, “internal coherence”, which is a measure of the coherence between the original and manipulated data. They show that the internal coherence can be defined as the difference between the classifier’s performance on the original or manipulated data and that of the model trained on the manipulated data, and that it can be used as a measure for classifiers’ ability to detect spurious patterns. They also show that classifiers are able to identify spurious patterns that are not present in the original data. ","This paper studies the problem of classifying spurious patterns in natural language processing. The authors propose a new term, “internal coherence”, which is a measure of the coherence between the original and manipulated data. They show that the internal coherence can be defined as the difference between the classifier’s performance on the original or manipulated data and that of the model trained on the manipulated data, and that it can be used as a measure for classifiers’ ability to detect spurious patterns. They also show that classifiers are able to identify spurious patterns that are not present in the original data. "
12537,SP:b720eb5b6e44473a9392cc572af89270019d4c42,super - resolution CONJUNCTION image restoration. image restoration CONJUNCTION super - resolution. CNN based image quality assessment CONJUNCTION super - resolution. super - resolution CONJUNCTION CNN based image quality assessment. full - reference perceptual quality features USED-FOR CNN based image quality assessment. image restoration CONJUNCTION imageto - image translation problems. imageto - image translation problems CONJUNCTION image restoration. CNN based image quality assessment CONJUNCTION image restoration. image restoration CONJUNCTION CNN based image quality assessment. frequency and orientation tuning of channels PART-OF trained image classification deep CNNs. spatial frequencies FEATURE-OF grating stimuli. VGG-16 HYPONYM-OF trained image classification deep CNNs. CNN channels USED-FOR human visual perception models. CNN channels USED-FOR spatial frequency and orientation selective filters. deep CNN representations USED-FOR perceptual quality features. contrast masking thresholds FEATURE-OF human visual perception. orientation selectivity FEATURE-OF deep CNN channels. perceptual quality features FEATURE-OF deep CNN channels. contrast masking thresholds FEATURE-OF spatial frequencies. ,This paper studies the effect of frequency and orientation tuning of channels in CNNs on the perceptual quality of deep CNNs. The authors show that the frequency-frequency and orientation-selectivity of CNN channels are highly correlated with human visual perception. They also show that these channels are more sensitive to grating stimuli than the full-reference perceptual quality features. ,This paper studies the effect of frequency and orientation tuning of channels in CNNs on the perceptual quality of deep CNNs. The authors show that the frequency-frequency and orientation-selectivity of CNN channels are highly correlated with human visual perception. They also show that these channels are more sensitive to grating stimuli than the full-reference perceptual quality features. 
12546,SP:e2c726a1c3e3ecbec198c4dd804a4298aacec3ad,drug development CONJUNCTION medical practice. medical practice CONJUNCTION drug development. graph neural networks USED-FOR task. nodes CONJUNCTION drug - drug interactions. drug - drug interactions CONJUNCTION nodes. drugs CONJUNCTION drug - drug interactions. drug - drug interactions CONJUNCTION drugs. drugs CONJUNCTION nodes. nodes CONJUNCTION drugs. graph neural networks USED-FOR DDI predictions. link prediction problems USED-FOR DDI predictions. graph energy neural network ( GENN ) USED-FOR link type correlations. structure prediction problem USED-FOR DDI prediction task. graph neural networks USED-FOR energy function. real world DDI datasets EVALUATE-FOR GENN. GENN COMPARE baselines. baselines COMPARE GENN. real world DDI datasets EVALUATE-FOR baselines. PR - AUC EVALUATE-FOR datasets. datasets EVALUATE-FOR GENN. PR - AUC EVALUATE-FOR GENN. GENN USED-FOR DDI correlations. GENN COMPARE baseline models. baseline models COMPARE GENN. Task is drug - drug interactions ( DDIs ). OtherScientificTerm is DDI types. Method is energy - based model. ,This paper proposes a graph energy neural network (GENN) for drug-drug interactions (DDI) prediction. GENN is based on graph neural networks (GNNs) and is able to predict link type correlations between two DDI types (drug interactions and drug interactions). The authors show that GENN outperforms baselines on several real-world DDI datasets. ,This paper proposes a graph energy neural network (GENN) for drug-drug interactions (DDI) prediction. GENN is based on graph neural networks (GNNs) and is able to predict link type correlations between two DDI types (drug interactions and drug interactions). The authors show that GENN outperforms baselines on several real-world DDI datasets. 
12555,SP:1f2a27579404aa165303789fdce9b3ed54f7b0c6,vq - wav2vec USED-FOR discrete representations of audio segments. online k - means clustering USED-FOR dense representations. Gumbel - Softmax CONJUNCTION online k - means clustering. online k - means clustering CONJUNCTION Gumbel - Softmax. algorithm USED-FOR dense representations. online k - means clustering USED-FOR algorithm. Gumbel - Softmax USED-FOR algorithm. Discretization USED-FOR algorithms. discrete inputs USED-FOR algorithms. TIMIT phoneme classification EVALUATE-FOR BERT pre - training. ,"This paper studies the problem of learning discrete representations of audio segments from discrete inputs. The authors propose two algorithms for this problem: online k-means clustering and Gumbel-softmax. The online k means clustering algorithm is based on the idea of online k mean clustering, which is a clustering technique for learning discrete embeddings. The proposed algorithm is evaluated on the TIMIT phoneme classification task. ","This paper studies the problem of learning discrete representations of audio segments from discrete inputs. The authors propose two algorithms for this problem: online k-means clustering and Gumbel-softmax. The online k means clustering algorithm is based on the idea of online k mean clustering, which is a clustering technique for learning discrete embeddings. The proposed algorithm is evaluated on the TIMIT phoneme classification task. "
12564,SP:f9c5b74b8bea5161d33676d9290d7b9d7e81d7b6,methods USED-FOR collaborative filtering models. methods USED-FOR ranking - based objective functions. actor - critic reinforcement learning USED-FOR methods. critic network USED-FOR ranking - based metrics. actor network USED-FOR metrics. critic - based method USED-FOR approximate ) ranking scores. critic - based method USED-FOR scoring process. learning - to - rank methods COMPARE critic - based method. critic - based method COMPARE learning - to - rank methods. neural network USED-FOR critic - based method. optimization procedure USED-FOR learning - to - rank methods. neural network USED-FOR scoring process. actor - critic COMPARE baselines. baselines COMPARE actor - critic. prediction models COMPARE baselines. baselines COMPARE prediction models. actor - critic USED-FOR prediction models. large - scale datasets EVALUATE-FOR actor - critic. large - scale datasets EVALUATE-FOR baselines. ,This paper proposes an actor-critic reinforcement learning method for collaborative filtering models. The proposed method is based on the actor network and the critic network. The critic network is used to learn the ranking-based objective functions. The actor network is then used to compute the approximate ranking scores. Experiments show that the proposed method outperforms the baselines. ,This paper proposes an actor-critic reinforcement learning method for collaborative filtering models. The proposed method is based on the actor network and the critic network. The critic network is used to learn the ranking-based objective functions. The actor network is then used to compute the approximate ranking scores. Experiments show that the proposed method outperforms the baselines. 
12573,SP:2444a83ae08181b125a325d893789f074d6db8ee,"off - policy reinforcement learning methods USED-FOR robot control. data - efficiency EVALUATE-FOR Deep Q - learning. multi - step TD - learning USED-FOR data - efficiency. Truncated Q - functions HYPONYM-OF TemporalDifference formulations. Shifted Q - functions USED-FOR farsighted return. Model - based Value Expansion CONJUNCTION TD3(∆ ). TD3(∆ ) CONJUNCTION Model - based Value Expansion. TD3 CONJUNCTION Model - based Value Expansion. Model - based Value Expansion CONJUNCTION TD3. approach COMPARE TD3. TD3 COMPARE approach. TD3(∆ ) HYPONYM-OF off - policy variant of TD(∆ ). approach COMPARE Model - based Value Expansion. Model - based Value Expansion COMPARE approach. approach USED-FOR function - approximation setting. function - approximation setting CONJUNCTION TD3. TD3 CONJUNCTION function - approximation setting. tabular case FEATURE-OF Composite Q - learning. simulated robot tasks EVALUATE-FOR Composite TD3. Composite TD3 COMPARE TD3. TD3 COMPARE Composite TD3. Composite TD3 COMPARE off - policy multi - step approaches. off - policy multi - step approaches COMPARE Composite TD3. simulated robot tasks EVALUATE-FOR off - policy multi - step approaches. simulated robot tasks EVALUATE-FOR TD3. TD3 COMPARE off - policy multi - step approaches. off - policy multi - step approaches COMPARE TD3. data - efficiency EVALUATE-FOR TD3. data - efficiency EVALUATE-FOR off - policy multi - step approaches. data - efficiency EVALUATE-FOR Composite TD3. Task is realworld applications. OtherScientificTerm are off - policy, target - policy rollout, truncated rollout, and shortand long - term predictions. Method is Composite Q - learning algorithm. ",This paper proposes a novel multi-step TD-learning algorithm for off-policy reinforcement learning. The proposed method is based on the truncated Q-function formulation of TD(∆). The authors show that the proposed method outperforms TD3 and TD3-∆ in the tabular case. The authors also show that their method can be applied to the function-approximation setting.,This paper proposes a novel multi-step TD-learning algorithm for off-policy reinforcement learning. The proposed method is based on the truncated Q-function formulation of TD(∆). The authors show that the proposed method outperforms TD3 and TD3-∆ in the tabular case. The authors also show that their method can be applied to the function-approximation setting.
12582,SP:64564b09bd68e7af17845019193825794f08e99b,"reinforcement learning USED-FOR real world robotics. dexterous manipulation USED-FOR system. manually designed resets USED-FOR learning. on - board perception USED-FOR manually designed resets. on - board perception USED-FOR learning. dexterous robotic manipulation tasks EVALUATE-FOR system. system USED-FOR vision - based skills. real - world three - fingered hand FEATURE-OF vision - based skills. Material is instrumented laboratory scenarios. Method are continuous learning, and robotic learning system. OtherScientificTerm are hand - engineered reward functions, and human intervention. Generic are solutions, and learning paradigm. ","This paper proposes a method for continuous reinforcement learning for dexterous robotic manipulation tasks. The proposed method is based on the idea of hand-engineered reward functions, which can be applied to any robotic learning system. The method is evaluated on a variety of tasks, including three-fingered robotic manipulation, and is shown to be able to achieve state-of-the-art performance. ","This paper proposes a method for continuous reinforcement learning for dexterous robotic manipulation tasks. The proposed method is based on the idea of hand-engineered reward functions, which can be applied to any robotic learning system. The method is evaluated on a variety of tasks, including three-fingered robotic manipulation, and is shown to be able to achieve state-of-the-art performance. "
12591,SP:ee4d59fa9487ecdcd663a4a7833689d1754aac7c,"adversarial examples USED-FOR Neural network robustness. labeled data USED-FOR adversarially robust generalization. perturbed test data EVALUATE-FOR networks. unlabeled data USED-FOR model. adversarially robust generalization FEATURE-OF model. classification accuracy EVALUATE-FOR accuracy part. accuracy part HYPONYM-OF parts. unlabeled data USED-FOR part. adversarially robust generalization COMPARE generalization. generalization COMPARE adversarially robust generalization. generalization PART-OF supervised learning. adversarially robust generalization USED-FOR Gaussian mixture problem. adversarial training algorithm USED-FOR adversarial robust generalization. MNIST CONJUNCTION Cifar-10. Cifar-10 CONJUNCTION MNIST. MNIST USED-FOR adversarial robust generalization. Cifar-10 USED-FOR adversarial robust generalization. unlabeled data USED-FOR adversarial training algorithm. Method is risk decomposition theorem. OtherScientificTerm are expected robust risk, prediction stability, perturbations, and label information. Metric is stability part. ","This paper studies the problem of adversarial robust generalization, which is an important problem in the context of supervised learning. The authors propose a novel adversarial training algorithm to improve the generalization performance of a model trained on unlabeled test data. The proposed method is based on the risk decomposition theorem, which shows that adversarial generalization is a function of the expected robust risk and the prediction stability of the model. The paper also provides a theoretical analysis of the proposed method. ","This paper studies the problem of adversarial robust generalization, which is an important problem in the context of supervised learning. The authors propose a novel adversarial training algorithm to improve the generalization performance of a model trained on unlabeled test data. The proposed method is based on the risk decomposition theorem, which shows that adversarial generalization is a function of the expected robust risk and the prediction stability of the model. The paper also provides a theoretical analysis of the proposed method. "
12600,SP:8d92aa968c590a352cb34c9fa1dbe77dff19519f,"estimation of advantage USED-FOR reinforcement learning algorithms. promotion focus CONJUNCTION prevention focus. prevention focus CONJUNCTION promotion focus. order statistics FEATURE-OF path ensemble. promotion focus FEATURE-OF learning process. prevention focus FEATURE-OF learning process. promotion focus COMPARE prevention focus. prevention focus COMPARE promotion focus. Terrain locomotion CONJUNCTION Atari games. Atari games CONJUNCTION Terrain locomotion. Atari games CONJUNCTION sparse - reward environments. sparse - reward environments CONJUNCTION Atari games. MuJoCo continuous control CONJUNCTION Terrain locomotion. Terrain locomotion CONJUNCTION MuJoCo continuous control. benchmarks EVALUATE-FOR schemes. schemes COMPARE mainstream methods. mainstream methods COMPARE schemes. benchmarks EVALUATE-FOR mainstream methods. MuJoCo continuous control HYPONYM-OF benchmarks. sparse - reward environments HYPONYM-OF benchmarks. Atari games HYPONYM-OF benchmarks. Terrain locomotion HYPONYM-OF benchmarks. Generic are it, and formulation. OtherScientificTerm are regulatory focuses, regulatory focus, and sparse rewards. ","This paper studies the problem of estimating the advantage of a policy in the context of reinforcement learning. The authors propose two approaches to this problem: (1) regulatory focus and (2) promotion focus. The proposed approach is based on the idea that the optimal path ensemble is a function of the order statistics of the paths in the path ensemble, and that the best path is the one with the lowest order statistics. They show that the proposed approach outperforms the state-of-the-art approaches on MuJoCo continuous control and Terrain locomotion.","This paper studies the problem of estimating the advantage of a policy in the context of reinforcement learning. The authors propose two approaches to this problem: (1) regulatory focus and (2) promotion focus. The proposed approach is based on the idea that the optimal path ensemble is a function of the order statistics of the paths in the path ensemble, and that the best path is the one with the lowest order statistics. They show that the proposed approach outperforms the state-of-the-art approaches on MuJoCo continuous control and Terrain locomotion."
12609,SP:2162408ce2a3267724b5f8f0abec41d4dc714220,"precision gating ( PG ) USED-FOR deep neural networks. approach USED-FOR DNN architectures. computational cost EVALUATE-FOR DNN execution. PG USED-FOR CNNs. PG USED-FOR statically compressed mobile - friendly networks. statically compressed mobile - friendly networks HYPONYM-OF CNNs. ShuffleNet HYPONYM-OF statically compressed mobile - friendly networks. prediction - based quantization schemes COMPARE PG. PG COMPARE prediction - based quantization schemes. compute EVALUATE-FOR PG. ImageNet EVALUATE-FOR PG. accuracy EVALUATE-FOR PG. PG USED-FOR RNNs. 8 - bit uniform quantization COMPARE PG. PG COMPARE 8 - bit uniform quantization. computational cost reduction EVALUATE-FOR LSTM. Penn Tree Bank dataset EVALUATE-FOR LSTM. computational cost reduction EVALUATE-FOR PG. perplexity EVALUATE-FOR PG. Metric are precision, and accuracy loss. ","This paper introduces Precision Gating (PG), a new quantization method for deep neural networks. The proposed method is based on the idea of precision gating, which aims to reduce the computational cost of training a deep neural network (DNN) while maintaining the accuracy. The authors show that the proposed method can be applied to a wide range of DNN architectures, including CNNs, RNNs, and LSTMs. They also show that PG can be used to improve the performance of existing quantization methods. ","This paper introduces Precision Gating (PG), a new quantization method for deep neural networks. The proposed method is based on the idea of precision gating, which aims to reduce the computational cost of training a deep neural network (DNN) while maintaining the accuracy. The authors show that the proposed method can be applied to a wide range of DNN architectures, including CNNs, RNNs, and LSTMs. They also show that PG can be used to improve the performance of existing quantization methods. "
12618,SP:0c2c9b80c087389168acdd42af15877fb499449b,"clean labeled data CONJUNCTION unlabeled data. unlabeled data CONJUNCTION clean labeled data. unlabeled data PART-OF TD. classifiers PART-OF unsupervised domain adaptation ( UDA ). unlabeled data USED-FOR classifiers. clean labeled data USED-FOR classifiers. noisy labeled data CONJUNCTION unlabeled data. unlabeled data CONJUNCTION noisy labeled data. unlabeled data PART-OF TD. unlabeled data USED-FOR classifiers. noisy labeled data USED-FOR classifiers. Butterfly framework USED-FOR WUDA. solution USED-FOR WUDA. label noise FEATURE-OF SD. Butterfly framework HYPONYM-OF solution. WUDA COMPARE UDA methods. UDA methods COMPARE WUDA. classification USED-FOR TD. models PART-OF Butterfly. deep networks HYPONYM-OF models. Butterfly USED-FOR WUDA. Butterfly COMPARE baseline methods. baseline methods COMPARE Butterfly. WUDA EVALUATE-FOR Butterfly. WUDA EVALUATE-FOR baseline methods. OtherScientificTerm are noisy - to - clean, and SD - to - TD - distributional. ","This paper proposes a method for unsupervised domain adaptation (UDA) based on the Butterfly framework. The proposed method is motivated by the observation that the label noise in the noisy-to-clean (SD) distribution is not always the same as that in the clean distribution. To address this issue, the authors propose to use a new classifier, called Butterfly, to learn a classifier that can be used in both the clean and noisy domain adaptation setting. The authors show that the proposed method can be applied to both clean and unlabeled data, and that it outperforms the existing UDA methods in terms of performance. ","This paper proposes a method for unsupervised domain adaptation (UDA) based on the Butterfly framework. The proposed method is motivated by the observation that the label noise in the noisy-to-clean (SD) distribution is not always the same as that in the clean distribution. To address this issue, the authors propose to use a new classifier, called Butterfly, to learn a classifier that can be used in both the clean and noisy domain adaptation setting. The authors show that the proposed method can be applied to both clean and unlabeled data, and that it outperforms the existing UDA methods in terms of performance. "
12627,SP:f3cc10ce2f77aeb2a6a3bae5631602452c14d403,"agent USED-FOR control tasks. high - dimensional images USED-FOR agent. model - free reinforcement learning ( RL ) USED-FOR agent. model - free reinforcement learning ( RL ) USED-FOR control tasks. high - dimensional images USED-FOR control tasks. control policy USED-FOR task. latent representation CONJUNCTION control policy. control policy CONJUNCTION latent representation. latent representation USED-FOR task. scarce reward signal USED-FOR high - capacity encoder. relevant features USED-FOR task. representation learning USED-FOR image - based RL. image reconstruction loss USED-FOR representation learning. auxiliary decoder USED-FOR end - to - end. auxiliary decoder USED-FOR off - policy actor - critic algorithm. control tasks EVALUATE-FOR model - free and model - based algorithms. OtherScientificTerm are suboptimal convergence, and latent features. Metric is sample efficiency. Method is off - policy algorithms. Task is image - based RL1. ",This paper proposes an off-policy actor-critic algorithm for image-based RL. The main idea of the algorithm is to use a high-capacity encoder to learn the latent representation of the task and a low-capacity decoder for the end-to-end decoder. The authors show that the proposed algorithm achieves suboptimal convergence in terms of sample efficiency and sample complexity. ,This paper proposes an off-policy actor-critic algorithm for image-based RL. The main idea of the algorithm is to use a high-capacity encoder to learn the latent representation of the task and a low-capacity decoder for the end-to-end decoder. The authors show that the proposed algorithm achieves suboptimal convergence in terms of sample efficiency and sample complexity. 
12636,SP:917bc9151a5829e97efd9bd0d0b2a3d1771b3265,"it USED-FOR tasks. Dropout HYPONYM-OF regularization technique. it USED-FOR DNNs. regularization technique USED-FOR generalization. regularization technique USED-FOR deep neural networks ( DNNs ). DNNs USED-FOR tasks. dropout USED-FOR training. dropout technique USED-FOR accelerating training. accelerating training CONJUNCTION generalization. generalization CONJUNCTION accelerating training. dropout technique USED-FOR generalization. multi - sample dropout HYPONYM-OF dropout technique. dropout USED-FOR generalization. multi - sample dropout USED-FOR dropout samples. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. ILSVRC 2012 ( ImageNet ) CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION ILSVRC 2012 ( ImageNet ). multi - sample dropout USED-FOR training. CIFAR-100 CONJUNCTION SVHN datasets. SVHN datasets CONJUNCTION CIFAR-100. multi - sample dropout USED-FOR image classification tasks. ILSVRC 2012 ( ImageNet ) CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION ILSVRC 2012 ( ImageNet ). CIFAR-10 USED-FOR image classification tasks. ILSVRC 2012 ( ImageNet ) USED-FOR image classification tasks. SVHN datasets USED-FOR image classification tasks. convolution layers CONJUNCTION dropout layer. dropout layer CONJUNCTION convolution layers. computation time FEATURE-OF convolution layers. training set CONJUNCTION validation set. validation set CONJUNCTION training set. error rates CONJUNCTION losses. losses CONJUNCTION error rates. validation set EVALUATE-FOR networks. losses EVALUATE-FOR networks. training set EVALUATE-FOR networks. error rates EVALUATE-FOR networks. multi - sample dropout USED-FOR networks. OtherScientificTerm are overfitting, dropout sample, sample losses, and fully connected layers. Metric is loss. Generic are technique, operator, and network. Method","This paper proposes a dropout technique for training deep neural networks. The dropout is a regularization technique that can be applied to any dropout layer in the network. The authors show that the dropout can be used to accelerate training and improve generalization. The proposed dropout method is evaluated on CIFAR-10, SVHN, and ImageNet datasets.","This paper proposes a dropout technique for training deep neural networks. The dropout is a regularization technique that can be applied to any dropout layer in the network. The authors show that the dropout can be used to accelerate training and improve generalization. The proposed dropout method is evaluated on CIFAR-10, SVHN, and ImageNet datasets."
12645,SP:5ad4b9e837e08d995b545b0b2734bc8fa4fafc43,"filters CONJUNCTION features. features CONJUNCTION filters. filter ambiguity FEATURE-OF CNNs. interpretability EVALUATE-FOR models. strategy USED-FOR interpretable CNNs. model USED-FOR disentangled filters. Label Sensitive Gate ( LSG ) structure USED-FOR model. supervised manner USED-FOR model. sparsity regularization USED-FOR LSG. LSG USED-FOR redundant filters. training strategy COMPARE model. model COMPARE training strategy. redundancy CONJUNCTION interpretability. interpretability CONJUNCTION redundancy. redundancy EVALUATE-FOR model. interpretability EVALUATE-FOR training strategy. interpretability EVALUATE-FOR model. Method is Convolutional neural networks ( CNNs ). Generic are pre - trained model, and method. OtherScientificTerm are redundant channels, and periodical shutdown. ","This paper proposes a new training strategy for interpretable CNNs. The proposed method is based on the Label Sensitive Gate (LSG) structure, which is used to disentangle disentangled filters. The authors show that the proposed method outperforms existing methods in terms of interpretability and redundancy. They also show that their method is more interpretable than existing methods.","This paper proposes a new training strategy for interpretable CNNs. The proposed method is based on the Label Sensitive Gate (LSG) structure, which is used to disentangle disentangled filters. The authors show that the proposed method outperforms existing methods in terms of interpretability and redundancy. They also show that their method is more interpretable than existing methods."
12654,SP:c9a512b6bc59aacbec2d5608284e29a7746172cf,"scalability CONJUNCTION non - stationarity. non - stationarity CONJUNCTION scalability. value function factorization learning USED-FOR collaborative multiagent systems. communication USED-FOR tasks. framework USED-FOR nearly decomposable Q - functions ( NDQ ). communication minimization USED-FOR framework. value function factorization learning CONJUNCTION communication learning. communication learning CONJUNCTION value function factorization learning. value function factorization learning USED-FOR framework. communication learning USED-FOR framework. information - theoretic regularizers USED-FOR framework. mutual information EVALUATE-FOR regularizers. regularizers COMPARE value function factorization methods. value function factorization methods COMPARE regularizers. QMIX HYPONYM-OF value function factorization methods. StarCraft unit micromanagement benchmark EVALUATE-FOR framework. framework COMPARE baseline methods. baseline methods COMPARE framework. StarCraft unit micromanagement benchmark EVALUATE-FOR baseline methods. Method is Reinforcement learning. Task is multi - agent settings. OtherScientificTerm are decentralized value functions, coordination, action selection, and communication messages. ",This paper proposes a new framework for value function factorization learning in multi-agent reinforcement learning. The proposed framework is based on the idea of nearly decomposable Q-functions (NDQ). The authors propose to use information theoretic regularizers to reduce the mutual information between the value function and the communication messages between agents. The authors show that the proposed method outperforms the baselines on the StarCraft unit micromanagement benchmark.,This paper proposes a new framework for value function factorization learning in multi-agent reinforcement learning. The proposed framework is based on the idea of nearly decomposable Q-functions (NDQ). The authors propose to use information theoretic regularizers to reduce the mutual information between the value function and the communication messages between agents. The authors show that the proposed method outperforms the baselines on the StarCraft unit micromanagement benchmark.
12663,SP:562f1a50f80d760a4be35095cd795cdb0f69a890,"AI USED-FOR complex programs. programming puzzles USED-FOR computers programming. programming puzzle HYPONYM-OF Boolean function. input - output pairs CONJUNCTION English problem descriptions. English problem descriptions CONJUNCTION input - output pairs. GAN - like algorithm USED-FOR puzzles. Troublemaker USED-FOR puzzles. Troublemaker HYPONYM-OF GAN - like algorithm. GAN - like algorithm USED-FOR automatic puzzle generation. Generic are problems, and it. OtherScientificTerm is Puzzles. Task is program synthesis. Method are puzzle - solver, and puzzle - solving techniques. ","This paper proposes a GAN-based algorithm for solving programming puzzles. The algorithm is based on the idea of solving a Boolean function that is defined as the sum of two Boolean functions. The problem is formulated as a program synthesis problem, and the authors propose to solve the problem by solving a sequence of programming puzzles that are generated by solving the Boolean function. The authors show that the proposed algorithm is able to solve more complex programming puzzles than existing methods. ","This paper proposes a GAN-based algorithm for solving programming puzzles. The algorithm is based on the idea of solving a Boolean function that is defined as the sum of two Boolean functions. The problem is formulated as a program synthesis problem, and the authors propose to solve the problem by solving a sequence of programming puzzles that are generated by solving the Boolean function. The authors show that the proposed algorithm is able to solve more complex programming puzzles than existing methods. "
12672,SP:627b515cc893ff33914dff255f5d6e136441d2e2,"structured decomposition of their behavior USED-FOR Reinforcement learning agents. lower - level primitives CONJUNCTION higher - level meta - policy. higher - level meta - policy CONJUNCTION lower - level primitives. lower - level primitives PART-OF policy. primitives PART-OF hierarchical reinforcement learning. primitives PART-OF policy design. information - theoretic mechanism USED-FOR decentralized decision. natural competition CONJUNCTION specialization. specialization CONJUNCTION natural competition. policy architecture COMPARE flat and hierarchical policies. flat and hierarchical policies COMPARE policy architecture. generalization EVALUATE-FOR policy architecture. Method is meta - policy. OtherScientificTerm are high - level meta - policy, and primitive. ",This paper proposes a hierarchical reinforcement learning method for learning policies that can be decomposed into lower-level primitives and higher-level meta-policies. The authors show that the proposed method is able to generalize better than flat and hierarchical policies. They also show that their method can generalize well to new environments. ,This paper proposes a hierarchical reinforcement learning method for learning policies that can be decomposed into lower-level primitives and higher-level meta-policies. The authors show that the proposed method is able to generalize better than flat and hierarchical policies. They also show that their method can generalize well to new environments. 
12681,SP:4d135a76ab151dd0adcf92c5ed8d3c717d256520,"models USED-FOR highdimensional state spaces. Model - based reinforcement learning methods USED-FOR models. rewards USED-FOR latent dynamics model. model - based planning framework USED-FOR latent reward prediction model. multi - step reward prediction USED-FOR planning. multi - step reward prediction USED-FOR latent representation. concise model - free representation USED-FOR framework. multi - pendulum and multi - cheetah environments FEATURE-OF framework. concise latent representation USED-FOR environments. method USED-FOR latent reward prediction model. method COMPARE model - based methods. model - based methods COMPARE method. sample efficiency EVALUATE-FOR model - free and model - based baselines. Planning COMPARE model - free and model - based baselines. model - free and model - based baselines COMPARE Planning. latent state - space FEATURE-OF Planning. sample efficiency EVALUATE-FOR Planning. Method are model - free reinforcement learning, and model - based algorithms. OtherScientificTerm are pendulums, and irrelevant information. Generic is them. ",This paper proposes a model-based planning framework for multi-pendulum and multi-cheetah environments. The proposed method is based on model-free reinforcement learning (MRL) and proposes to use a latent dynamics model to predict the future state of the pendulum and cheetah. The method is evaluated on the multi-Pendulum environment and shows that the proposed method outperforms the baselines.,This paper proposes a model-based planning framework for multi-pendulum and multi-cheetah environments. The proposed method is based on model-free reinforcement learning (MRL) and proposes to use a latent dynamics model to predict the future state of the pendulum and cheetah. The method is evaluated on the multi-Pendulum environment and shows that the proposed method outperforms the baselines.
12690,SP:03c61ba3d6fe01bd0bc3469cd408c370527d9d69,"GPU / TPU memory limitations CONJUNCTION training times. training times CONJUNCTION GPU / TPU memory limitations. parameterreduction techniques USED-FOR BERT. parameterreduction techniques USED-FOR memory consumption. training speed EVALUATE-FOR BERT. parameterreduction techniques USED-FOR training speed. methods USED-FOR models. models COMPARE BERT. BERT COMPARE models. it USED-FOR downstream tasks. self - supervised loss USED-FOR inter - sentence coherence. multi - sentence inputs USED-FOR it. multi - sentence inputs USED-FOR downstream tasks. model COMPARE BERT - large. BERT - large COMPARE model. GLUE, RACE, and SQuAD benchmarks EVALUATE-FOR model. OtherScientificTerm are model size, and parameters. Method are natural language representations, and pretrained models. ","This paper proposes a self-supervised loss to improve the performance of BERT-large on multi-sentence learning tasks. The proposed method is based on the idea that BERT can be used as a pre-trained model for downstream tasks. It is shown that the proposed method outperforms the baselines on GLUE, RACE, and SQuAD. ","This paper proposes a self-supervised loss to improve the performance of BERT-large on multi-sentence learning tasks. The proposed method is based on the idea that BERT can be used as a pre-trained model for downstream tasks. It is shown that the proposed method outperforms the baselines on GLUE, RACE, and SQuAD. "
12699,SP:5dd50f3e6cef6b82192a1d37b35469dc7fb443ce,"Transformer HYPONYM-OF neural network architecture. neural network architecture USED-FOR language understanding. Transformer USED-FOR language understanding. text CONJUNCTION videos. videos CONJUNCTION text. image CONJUNCTION text. text CONJUNCTION image. modalities FEATURE-OF tasks. image HYPONYM-OF modalities. text HYPONYM-OF modalities. videos HYPONYM-OF modalities. temporal input sequence FEATURE-OF hidden states. model USED-FOR tasks. architecture USED-FOR model. model USED-FOR asynchronous multi - task learning. multiple input modalities CONJUNCTION asynchronous multi - task learning. asynchronous multi - task learning CONJUNCTION multiple input modalities. model USED-FOR multiple input modalities. architecture USED-FOR tasks. tasks CONJUNCTION asynchronous multi - task learning. asynchronous multi - task learning CONJUNCTION tasks. multiple input modalities FEATURE-OF tasks. image captioning CONJUNCTION visual question answering. visual question answering CONJUNCTION image captioning. visual question answering CONJUNCTION video activity recognition. video activity recognition CONJUNCTION visual question answering. OmniNet USED-FOR tasks. part - of - speech tagging CONJUNCTION image captioning. image captioning CONJUNCTION part - of - speech tagging. OmniNet USED-FOR part - of - speech tagging. part - of - speech tagging CONJUNCTION visual question answering. visual question answering CONJUNCTION part - of - speech tagging. video activity recognition HYPONYM-OF tasks. part - of - speech tagging HYPONYM-OF tasks. visual question answering HYPONYM-OF tasks. image captioning HYPONYM-OF tasks. compressed model EVALUATE-FOR tasks. video captioning CONJUNCTION video question answering. video question answering CONJUNCTION video captioning. neural network USED-FOR tasks. modalities USED-FOR neural network. video question answering HYPONYM-OF tasks. video captioning HYPONYM-OF tasks. spatio - temporal cache PART-OF OmniNet. spatio - temporal cache USED-FOR self - attention mechanism. Method is spatio - temporal cache mechanism. Generic are it, and them. ","This paper proposes a Transformer-based model for language understanding. The proposed model is based on the Transformer architecture and is able to handle multiple input modalities, including video, text, image, and video activity recognition. The model is also able to learn a self-attention mechanism, which can be used for asynchronous multi-task learning. The authors also propose a spatio-temporal cache mechanism to store the hidden states of hidden states during training. Experiments show that the proposed model outperforms state-of-the-art models on a number of tasks.","This paper proposes a Transformer-based model for language understanding. The proposed model is based on the Transformer architecture and is able to handle multiple input modalities, including video, text, image, and video activity recognition. The model is also able to learn a self-attention mechanism, which can be used for asynchronous multi-task learning. The authors also propose a spatio-temporal cache mechanism to store the hidden states of hidden states during training. Experiments show that the proposed model outperforms state-of-the-art models on a number of tasks."
12708,SP:6c8e2dd1d6224dffb95dbf729b159f00bfb05721,"tasks EVALUATE-FOR Deep learning models. predictive accuracy EVALUATE-FOR Deep learning models. methods USED-FOR uncertainty quantification. Bayesian neural networks USED-FOR methods. discriminative accuracy EVALUATE-FOR approximate posterior inference. frequentist approach USED-FOR uncertainty quantification. formal inference procedure USED-FOR predictive confidence intervals. discriminative jackknife ( DJ ) HYPONYM-OF formal inference procedure. formal inference procedure USED-FOR regression models. higher - order influence functions ( HOIFs ) FEATURE-OF model parameters. higher - order influence functions ( HOIFs ) USED-FOR DJ procedure. loss gradients CONJUNCTION Hessian - vector products. Hessian - vector products CONJUNCTION loss gradients. oracle access USED-FOR loss gradients. DJ USED-FOR HOIFs. oracle access CONJUNCTION Hessian - vector products. Hessian - vector products CONJUNCTION oracle access. oracle access USED-FOR recursive formula. model accuracy EVALUATE-FOR it. recursive formula USED-FOR DJ. recursive formula USED-FOR HOIFs. DJ COMPARE Bayesian and non - Bayesian baselines. Bayesian and non - Bayesian baselines COMPARE DJ. OtherScientificTerm are predictive uncertainty, and Bayesian credible intervals. Material is highand low - confidence prediction instances. Method is Bayesian methods. Metric is frequentist coverage. Task is model training. ","This paper proposes a new method for estimating confidence intervals for Bayesian neural networks. The proposed method is based on the frequentist approach, which is used to approximate posterior inference. The authors show that the proposed method outperforms existing methods in terms of predictive uncertainty quantification. They also show that their method can be applied to a variety of Bayesian methods.","This paper proposes a new method for estimating confidence intervals for Bayesian neural networks. The proposed method is based on the frequentist approach, which is used to approximate posterior inference. The authors show that the proposed method outperforms existing methods in terms of predictive uncertainty quantification. They also show that their method can be applied to a variety of Bayesian methods."
12717,SP:057a035c4eeeb5fe985b20d0266126d66d9d243f,Generative Adversarial Networks USED-FOR video samples. complexity CONJUNCTION fidelity. fidelity CONJUNCTION complexity. complex Kinetics-600 dataset EVALUATE-FOR Generative Adversarial Networks. decomposition of its discriminator USED-FOR model. video synthesis CONJUNCTION video prediction. video prediction CONJUNCTION video synthesis. baseline USED-FOR synthesis. Fréchet Inception Distance USED-FOR prediction. Inception Score USED-FOR synthesis. Fréchet Inception Distance CONJUNCTION Inception Score. Inception Score CONJUNCTION Fréchet Inception Distance. prediction USED-FOR Kinetics600. UCF-101 dataset EVALUATE-FOR Inception Score. UCF-101 dataset EVALUATE-FOR synthesis. Kinetics-600 USED-FOR baseline. Kinetics-600 USED-FOR synthesis. Method is Generative models of natural images. Task is video modeling. ,This paper proposes a novel method for video synthesis and video prediction based on GANs. The method is based on the decomposition of the discriminator into two parts: a discriminator and an Inception Score. The discriminator is trained to predict the Inception Distance of the video samples and the prediction of the FID. The Inception score is computed by computing the Fréchet Inception distance between the predicted video and the original video. The paper shows that the proposed method outperforms baselines on the Kinetics-600 dataset. ,This paper proposes a novel method for video synthesis and video prediction based on GANs. The method is based on the decomposition of the discriminator into two parts: a discriminator and an Inception Score. The discriminator is trained to predict the Inception Distance of the video samples and the prediction of the FID. The Inception score is computed by computing the Fréchet Inception distance between the predicted video and the original video. The paper shows that the proposed method outperforms baselines on the Kinetics-600 dataset. 
12726,SP:a89ee8eb2f60d9d522993a57d656f0ef726d86d6,"few - shot classification benchmarks EVALUATE-FOR representation. base class data USED-FOR representation. stages USED-FOR representation. spatial attention map USED-FOR background clutter. pre - trained classifier USED-FOR spatial attention map. Method are Few - shot learning, pre - trained network, and meta - learning. OtherScientificTerm is prior knowledge. Material is large - scale dataset. Generic is network. ","This paper proposes a meta-learning method for few-shot learning. The method is based on meta-training a pre-trained classifier on a large-scale dataset. The authors show that the proposed method is able to learn a spatial attention map for the base class data, which is then used to learn the representation of the base classes. They also show that their method outperforms the baselines on a few benchmark datasets. ","This paper proposes a meta-learning method for few-shot learning. The method is based on meta-training a pre-trained classifier on a large-scale dataset. The authors show that the proposed method is able to learn a spatial attention map for the base class data, which is then used to learn the representation of the base classes. They also show that their method outperforms the baselines on a few benchmark datasets. "
12735,SP:d236f0b38414442af00b9be5e5d39e138f0069a2,"molecules CONJUNCTION game maps. game maps CONJUNCTION molecules. generative adversarial networks ( GANs ) USED-FOR structured objects. molecules HYPONYM-OF structured objects. game maps HYPONYM-OF structured objects. structural requirements FEATURE-OF objects. molecules HYPONYM-OF structural requirements. constraints PART-OF model. approach USED-FOR logical constraints. knowledge compilation techniques USED-FOR expected disagreement. knowledge compilation techniques USED-FOR approach. setup USED-FOR hybrid logical - neural constraints. hybrid logical - neural constraints USED-FOR complex requirements. setup USED-FOR complex requirements. graph reachability HYPONYM-OF complex requirements. constrained images CONJUNCTION molecules. molecules CONJUNCTION constrained images. molecules CONJUNCTION video game levels. video game levels CONJUNCTION molecules. Method are constrained adversarial networks ( CANs ), generator, unconstrained GANs, and CANs. ","This paper proposes a novel approach to learn constraints for constrained GANs. The proposed approach is based on the idea of hybrid logical-neural constraints, which can be applied to both the generator and the discriminator. The authors show that the proposed method is able to achieve better performance than the state-of-the-art on a variety of synthetic and real-world datasets. The method is evaluated on a number of synthetic datasets, including molecules, game maps, and constrained images.","This paper proposes a novel approach to learn constraints for constrained GANs. The proposed approach is based on the idea of hybrid logical-neural constraints, which can be applied to both the generator and the discriminator. The authors show that the proposed method is able to achieve better performance than the state-of-the-art on a variety of synthetic and real-world datasets. The method is evaluated on a number of synthetic datasets, including molecules, game maps, and constrained images."
12744,SP:110f0b86431f0a93cf48e08fe445e32172a37eae,"Deep neural networks COMPARE deep neural networks. deep neural networks COMPARE Deep neural networks. fixed activation functions USED-FOR deep neural networks. learnable activation functions USED-FOR Deep neural networks. adaptability of learnable activation functions USED-FOR model. expressive power FEATURE-OF model. expressive power FEATURE-OF adaptability of learnable activation functions. Adaptive Piecewise Linear units ( APL ) USED-FOR learnable activation function. stages PART-OF activation functions. gradient information PART-OF activation functions. Symmetric - APL activations USED-FOR deep neural networks. robustness EVALUATE-FOR deep neural networks. deep neural networks USED-FOR adversarial attacks. Network - in - Network CONJUNCTION ResNet-18. ResNet-18 CONJUNCTION Network - in - Network. Lenet CONJUNCTION Network - in - Network. Network - in - Network CONJUNCTION Lenet. activation functions COMPARE ReLUs. ReLUs COMPARE activation functions. activation functions USED-FOR architectures. ResNet-18 HYPONYM-OF architectures. activation functions USED-FOR adversarial fooling. Lenet HYPONYM-OF architectures. Network - in - Network HYPONYM-OF architectures. OtherScientificTerm are positive and negative halves, zero - centered continuous non - linearity, and SymmetricAPL function. Task is ablation studies. ",This paper proposes a new activation function for deep neural networks. The proposed function is based on adaptive piecewise linear units (APL). The authors show that the proposed function can be used to improve the robustness of neural networks against adversarial attacks. The authors also provide ablation studies to demonstrate the effectiveness of the proposed method.,This paper proposes a new activation function for deep neural networks. The proposed function is based on adaptive piecewise linear units (APL). The authors show that the proposed function can be used to improve the robustness of neural networks against adversarial attacks. The authors also provide ablation studies to demonstrate the effectiveness of the proposed method.
12753,SP:4787aff0fb84beb13cde0d40c32d3a743d8e4082,"models CONJUNCTION proprietary data. proprietary data CONJUNCTION models. third party components CONJUNCTION models. models CONJUNCTION third party components. application programming interfaces ( APIs ) USED-FOR pre - trained encapsulated models. wrapping deep learning model USED-FOR classification black - box. wrapping deep learning model USED-FOR measure of uncertainty. Dirichlet layer USED-FOR fusion layer. black - box classifier USED-FOR probabilistic neural network. Dirichlet layer USED-FOR probabilistic neural network. Dirichlet layer USED-FOR distribution. Dirichlet layer USED-FOR estimation of aleatoric uncertainty. multinomial output parameters USED-FOR distribution. uncertainty measure USED-FOR rejection system. NLP CONJUNCTION computer vision. computer vision CONJUNCTION NLP. technique CONJUNCTION methodology. methodology CONJUNCTION technique. wrapper USED-FOR uncertainty. Method are machine learning models, classifier, and simulated API based. Generic are external components, and components. Metric are auditability, and trustability. OtherScientificTerm are uncontrolled potential risks, black - box, aleatoric uncertainty, and misclassifications. ","This paper proposes a new method for black-box classification, which is based on the Dirichlet layer of a probabilistic neural network. The authors propose a method for detecting the black box uncertainty of a black box classifier, and propose a rejection system to detect misclassification. The proposed method is evaluated on a variety of datasets, and is shown to be effective in detecting misclassifications.","This paper proposes a new method for black-box classification, which is based on the Dirichlet layer of a probabilistic neural network. The authors propose a method for detecting the black box uncertainty of a black box classifier, and propose a rejection system to detect misclassification. The proposed method is evaluated on a variety of datasets, and is shown to be effective in detecting misclassifications."
12762,SP:1207bf6cf93737d63e1a7cc1ff3a99bf9d6098f9,"Stochastic methods USED-FOR deep neural networks. RMSprop CONJUNCTION Adam. Adam CONJUNCTION RMSprop. coordinate - wise adaptive stepsize FEATURE-OF Stochastic methods. Adam HYPONYM-OF coordinate - wise adaptive stepsize. RMSprop HYPONYM-OF coordinate - wise adaptive stepsize. Adam HYPONYM-OF Stochastic methods. RMSprop HYPONYM-OF Stochastic methods. they COMPARE stochastic gradient descent. stochastic gradient descent COMPARE they. blockwise adaptivity COMPARE adaptivity. adaptivity COMPARE blockwise adaptivity. adaptivity CONJUNCTION generalization. generalization CONJUNCTION adaptivity. blockwise adaptive gradient descent USED-FOR optimizing nonconvex objective. convergence rate FEATURE-OF optimizing nonconvex objective. blockwise adaptive gradient descent USED-FOR online convex learning. regret CONJUNCTION convergence rate. convergence rate CONJUNCTION regret. convergence rate EVALUATE-FOR blockwise adaptive gradient descent. regret EVALUATE-FOR blockwise adaptive gradient descent. blockwise adaptivity COMPARE coordinate - wise adaptivity. coordinate - wise adaptivity COMPARE blockwise adaptivity. generalization error EVALUATE-FOR coordinate - wise adaptivity. generalization error EVALUATE-FOR blockwise adaptivity. Nesterov ’s accelerated gradient CONJUNCTION Adam. Adam CONJUNCTION Nesterov ’s accelerated gradient. blockwise adaptive gradient descent COMPARE Adam. Adam COMPARE blockwise adaptive gradient descent. blockwise adaptive gradient descent COMPARE Nesterov ’s accelerated gradient. Nesterov ’s accelerated gradient COMPARE blockwise adaptive gradient descent. Method is Adagrad. OtherScientificTerm are network parameters, blockwise adaptive stepsize, and nonconvex objective. Metric is uniform stability. ",This paper studies the adaptive stepsize of stochastic gradient descent methods. The authors show that blockwise adaptive gradient descent (Adam) and RMSprop (RMSprop) converge to the same nonconvex objective. They also show that Adam is more stable than Nesterov’s accelerated gradient (Nesterov). ,This paper studies the adaptive stepsize of stochastic gradient descent methods. The authors show that blockwise adaptive gradient descent (Adam) and RMSprop (RMSprop) converge to the same nonconvex objective. They also show that Adam is more stable than Nesterov’s accelerated gradient (Nesterov). 
12771,SP:0334d79349e9fb8ca32751b7ad29f82e00a5381c,"large - scale supervised datasets USED-FOR deep features. datasets CONJUNCTION spatiotemporal models. spatiotemporal models CONJUNCTION datasets. scene and object structure FEATURE-OF implicit biases. implicit biases FEATURE-OF video datasets. observable and controllable object and scene bias FEATURE-OF video dataset. spatiotemporal understanding USED-FOR video dataset. 3D objects USED-FOR dataset. diagnostic tools USED-FOR spatiotemporal video architectures. CATER USED-FOR diagnostic tools. CATER HYPONYM-OF dataset. CATER USED-FOR deep video architectures. Task are Computer vision, static image analysis, and video understanding. Method are frame - by - frame classification methods, and long - term reasoning. OtherScientificTerm is temporal structure. ","This paper proposes a new dataset CATER for video understanding. CATER is a large-scale video dataset with 3D objects. The dataset is designed to study the implicit biases of video datasets. The authors show that the video dataset has observable and controllable object and scene bias, which can be used as a diagnostic tool for deep video models. They also show that CATER can be applied to a variety of video architectures.","This paper proposes a new dataset CATER for video understanding. CATER is a large-scale video dataset with 3D objects. The dataset is designed to study the implicit biases of video datasets. The authors show that the video dataset has observable and controllable object and scene bias, which can be used as a diagnostic tool for deep video models. They also show that CATER can be applied to a variety of video architectures."
12780,SP:b637c75acbe9d0152384b632f2e92a0d248cb720,"distribution USED-FOR synthetic data. Generative Adversarial Networks ( GANs ) HYPONYM-OF models. distribution USED-FOR models. image data USED-FOR visual applications. model compatibility problem HYPONYM-OF task. generating near - boundary data USED-FOR classifiers. generating ‘ easier ’ synthetic data USED-FOR GANs. pre - trained classifiers USED-FOR boundary information. Boundary - Calibration GANs ( BCGANs ) USED-FOR GAN. boundary information USED-FOR Boundary - Calibration GANs ( BCGANs ). pre - trained classifiers USED-FOR Boundary - Calibration GANs ( BCGANs ). GAN variants USED-FOR model compatibility. BC - loss CONJUNCTION GAN variants. GAN variants CONJUNCTION BC - loss. BC - loss USED-FOR model compatibility. BCGANs COMPARE GANs. GANs COMPARE BCGANs. model compatibility EVALUATE-FOR GANs. BCGANs USED-FOR realistic images. BCGANs COMPARE GANs. GANs COMPARE BCGANs. GANs USED-FOR realistic images. model compatibility EVALUATE-FOR BCGANs. Material is near - boundary data. Method is generator of GAN. OtherScientificTerm are posterior distributions, and boundaries of the pre - trained classifiers. ","This paper proposes Boundary-Calibration GANs (BCGANs), a method for generating near-boundary data to improve the model compatibility of GAN models. The method is based on the idea that the boundary information of the pre-trained classifiers can be used to improve model compatibility. The authors show that BCGANs outperform GAN variants in terms of model compatibility on synthetic and real-world datasets. ","This paper proposes Boundary-Calibration GANs (BCGANs), a method for generating near-boundary data to improve the model compatibility of GAN models. The method is based on the idea that the boundary information of the pre-trained classifiers can be used to improve model compatibility. The authors show that BCGANs outperform GAN variants in terms of model compatibility on synthetic and real-world datasets. "
12789,SP:93616e31fa1dc64d130c0c44cbb73c0412b24a97,approach USED-FOR robust neural networks. Adversarial training USED-FOR robust neural networks. Adversarial training USED-FOR approach. minimax robust optimization problem USED-FOR adversarial training. outer minimization USED-FOR robust classifier. inner maximization USED-FOR adversarial samples. outer minimization CONJUNCTION inner maximization. inner maximization CONJUNCTION outer minimization. hand - designed algorithms USED-FOR inner problem. convolutional neural network USED-FOR optimizer. robust classifier USED-FOR adversarial attack. optimizer USED-FOR adversarial attack. L2L COMPARE adversarial training methods. adversarial training methods COMPARE L2L. CIFAR-10 and CIFAR-100 datasets EVALUATE-FOR L2L. classification accuracy CONJUNCTION computational efficiency. computational efficiency CONJUNCTION classification accuracy. classification accuracy EVALUATE-FOR adversarial training methods. computational efficiency EVALUATE-FOR adversarial training methods. computational efficiency EVALUATE-FOR L2L. classification accuracy EVALUATE-FOR L2L. L2L framework USED-FOR generative adversarial imitation learning. Task is minimax problem. OtherScientificTerm is convex - concave structure. Method is adversarial training method. ,"This paper proposes a new adversarial training method, called L2L, which is based on the minimax robust optimization problem (MBO) framework. The main contribution of the paper is to propose a new minimax minimax optimization problem for adversarial learning, where the inner minimization problem is a convex-concave optimization problem, and the outer minimisation problem is an inner-maximization problem. The paper also proposes a generative adversarial imitation learning (GAD) framework, which uses a convolutional neural network to generate adversarial samples. The proposed method is evaluated on CIFAR-10 and Cifar-100 datasets, and compared to several baselines.","This paper proposes a new adversarial training method, called L2L, which is based on the minimax robust optimization problem (MBO) framework. The main contribution of the paper is to propose a new minimax minimax optimization problem for adversarial learning, where the inner minimization problem is a convex-concave optimization problem, and the outer minimisation problem is an inner-maximization problem. The paper also proposes a generative adversarial imitation learning (GAD) framework, which uses a convolutional neural network to generate adversarial samples. The proposed method is evaluated on CIFAR-10 and Cifar-100 datasets, and compared to several baselines."
12798,SP:0a66c3540383c76689258d2fffe0571ed944c1e7,"approaches USED-FOR Inverse Reinforcement Learning ( IRL ). reward function USED-FOR expert agent ’s policy. reward function USED-FOR control task. reward CONJUNCTION hard constraints. hard constraints CONJUNCTION reward. reward USED-FOR behavior. Markov Decision Processes ( MDPs ) USED-FOR IRL. Maximum Entropy IRL framework USED-FOR approach. algorithm USED-FOR Maximum Likelihood Constraint. algorithm USED-FOR observed behavior. Maximum Likelihood Constraint USED-FOR observed behavior. OtherScientificTerm are cumulative rewards, nominal reward function, and MDP. Generic is method. Material is simulated behavior. ",This paper proposes a new method for inverse reinforcement learning (IRL) based on the Maximum Entropy IRL framework. The main idea is to use the Maximum Likelihood Constraint (MCL) as the reward function for RL. The authors show that the proposed method is able to learn a reward function that maximizes the maximum entropy of the MCL. They also show that their method can be applied to MDPs. ,This paper proposes a new method for inverse reinforcement learning (IRL) based on the Maximum Entropy IRL framework. The main idea is to use the Maximum Likelihood Constraint (MCL) as the reward function for RL. The authors show that the proposed method is able to learn a reward function that maximizes the maximum entropy of the MCL. They also show that their method can be applied to MDPs. 
12807,SP:2a5fba69a6287b87a19bcd745d2e4326bbb723de,deep recurrent neural network architecture USED-FOR visual cortical circuits. architecture USED-FOR contour detection tasks. architecture COMPARE feedforward networks. feedforward networks COMPARE architecture. sample efficiency EVALUATE-FOR feedforward networks. γ - Net HYPONYM-OF architecture. sample efficiency EVALUATE-FOR architecture. orientation - tilt illusion HYPONYM-OF perceptual illusion. lowlevel edges COMPARE high - level object boundary contours. high - level object boundary contours COMPARE lowlevel edges. it USED-FOR lowlevel edges. γ - Net contour detection accuracy EVALUATE-FOR illusion. neural circuits USED-FOR biological visual systems. circuits PART-OF artificial neural networks. circuits USED-FOR computer vision. neural circuits USED-FOR contour detection. artificial neural networks USED-FOR computer vision. biological visual systems USED-FOR contour detection. neural circuits USED-FOR orientation - tilt illusion. ,This paper proposes a novel deep recurrent neural network architecture for contour detection tasks. The proposed architecture is based on neural circuits and is able to achieve state-of-the-art performance on object boundary contours and orientation-tilt illusions. The authors show that the proposed architecture can achieve better performance than feedforward networks on both high-level and low-level contours. ,This paper proposes a novel deep recurrent neural network architecture for contour detection tasks. The proposed architecture is based on neural circuits and is able to achieve state-of-the-art performance on object boundary contours and orientation-tilt illusions. The authors show that the proposed architecture can achieve better performance than feedforward networks on both high-level and low-level contours. 
12816,SP:4f094a3f7eeb302738c2b482fbaca56e34ac6a99,object detection methods USED-FOR detecting and classifying objects. deep convolutional neural networks ( CNNs ) USED-FOR object detection methods. semantics context constraints FEATURE-OF CNN - based object detector. conditional random field ( CRF ) model USED-FOR CNN. mean - field inference method USED-FOR CRF. context - aware module USED-FOR mean - field inference method. context - aware module PART-OF conCNN. stack of common CNN operations USED-FOR mean - field inference method. stack of common CNN operations USED-FOR conCNN. It PART-OF region - based object detection paradigm. average precision ( AP ) EVALUATE-FOR object detection. COCO datasets EVALUATE-FOR conCNN. conCNN USED-FOR object detection. average precision ( AP ) EVALUATE-FOR conCNN. Generic is methods. OtherScientificTerm is semantic context. ,This paper proposes a new object detection method based on conditional random field (CRF) model. The proposed method is based on a stack of common CNN operations and a context-aware module. Experiments show that the proposed method outperforms state-of-the-art methods.,This paper proposes a new object detection method based on conditional random field (CRF) model. The proposed method is based on a stack of common CNN operations and a context-aware module. Experiments show that the proposed method outperforms state-of-the-art methods.
12825,SP:74d63293d2f8a41a14743bfcd8939fca5e804fdb,"Batch Normalization ( BatchNorm ) USED-FOR deep neural networks. adversarial perturbations FEATURE-OF it. adversarial vulnerability FEATURE-OF BatchNorm. training CONJUNCTION inference. inference CONJUNCTION training. normalization statistics USED-FOR training. normalization statistics USED-FOR inference. normalization statistics USED-FOR adversarial vulnerability. adversarial vulnerability FEATURE-OF BatchNorm layer. mini - batch statistics HYPONYM-OF normalization statistics. Method are neural network architectures, and Robust Normalization ( RobustNorm ). OtherScientificTerm is adversarial perturbation. ","This paper studies the adversarial vulnerability of batch normalization (BatchNorm) in deep neural networks. The authors propose Robust Normalization (RobustNorm), which is a method to improve the robustness of BatchNorm to adversarial perturbations. RobustNorm is based on the observation that adversarial robustness is a function of the number of mini-batches and the size of the training batch. The main contribution of the paper is to show that RobustNormalization can be used as a regularizer for training and inference. The paper also provides a theoretical analysis of the robust normalization. ","This paper studies the adversarial vulnerability of batch normalization (BatchNorm) in deep neural networks. The authors propose Robust Normalization (RobustNorm), which is a method to improve the robustness of BatchNorm to adversarial perturbations. RobustNorm is based on the observation that adversarial robustness is a function of the number of mini-batches and the size of the training batch. The main contribution of the paper is to show that RobustNormalization can be used as a regularizer for training and inference. The paper also provides a theoretical analysis of the robust normalization. "
12834,SP:f16d3e61eda162dfee39396abbd594425f47f625,"Over - parameterized deep neural networks USED-FOR labeling of data. first - order methods USED-FOR Over - parameterized deep neural networks. over - fitting ability USED-FOR generalization. clean test data EVALUATE-FOR regularization methods. early - stopping HYPONYM-OF regularization methods. trainable auxiliary variable USED-FOR network output. regularization HYPONYM-OF regularization methods. generalization guarantee FEATURE-OF clean data distribution. gradient descent training USED-FOR generalization guarantee. methods USED-FOR gradient descent training. wide neural network CONJUNCTION neural tangent kernel ( NTK ). neural tangent kernel ( NTK ) CONJUNCTION wide neural network. noisily labeled datasets EVALUATE-FOR methods. OtherScientificTerm are network parameters, noisy labels, and label noise. Metric is generalization bound. ","This paper studies the generalization properties of over-parameterized deep neural networks. The authors show that under certain conditions, the over-fitting ability of the network can be reduced to zero, which is the case when the training data is clean and the label noise is low. They also provide a generalization bound on the generalizability of such networks. ","This paper studies the generalization properties of over-parameterized deep neural networks. The authors show that under certain conditions, the over-fitting ability of the network can be reduced to zero, which is the case when the training data is clean and the label noise is low. They also provide a generalization bound on the generalizability of such networks. "
12843,SP:67335658ec9de6ba3fa352ca4de073ac51f2f703,"convolutional neural networks ( CNNs ) USED-FOR regression. Convolutional Neural Tangent Kernel ( CNTK ) USED-FOR regression. algorithm USED-FOR CNTK. classification accuracy EVALUATE-FOR CNTK. CNTK COMPARE CNN architecture. CNN architecture COMPARE CNTK. classification accuracy EVALUATE-FOR CNN architecture. CIFAR-10 EVALUATE-FOR CNTK. Local Average Pooling ( LAP ) HYPONYM-OF operation. pixel shifts USED-FOR data augmentation. operation USED-FOR kernel. quadratic training cost FEATURE-OF kernel regression. CNN - GP CONJUNCTION CNTK. CNTK CONJUNCTION CNN - GP. full translation data augmentation USED-FOR CNTK. single convolutional layer USED-FOR pre - processing technique. random image patches PART-OF single convolutional layer. CIFAR-10 EVALUATE-FOR kernel. classifier COMPARE trained neural network. trained neural network COMPARE classifier. AlexNet COMPARE classifier. classifier COMPARE AlexNet. CNN - GP HYPONYM-OF kernel. horizontal flip data augmentation USED-FOR kernel. horizontal flip data augmentation USED-FOR CNN - GP. accuracy EVALUATE-FOR kernel. OtherScientificTerm are ` 2 loss, convolutional layers, and fixed kernel. Generic are layer, and kernels. Method are naive data augmentation, Global Average Pooling ( GAP ), and Fashion - MNIST. ","This paper proposes a new convolutional neural tangent kernel (CNTK) method for regression. The proposed method is based on the local average pooling (LAP) operation, which is a variant of Global Average Pooling (GAP). The authors show that the proposed method outperforms existing methods on CIFAR-10, Fashion-MNIST, and CNN-GP. ","This paper proposes a new convolutional neural tangent kernel (CNTK) method for regression. The proposed method is based on the local average pooling (LAP) operation, which is a variant of Global Average Pooling (GAP). The authors show that the proposed method outperforms existing methods on CIFAR-10, Fashion-MNIST, and CNN-GP. "
12852,SP:4ffab7f7f9fc09fdf59602228d231c6f6330fb98,"approach USED-FOR model - free Q - learning. MCTS USED-FOR state - action values. real experience USED-FOR prior. Q - estimates USED-FOR prior. Q - estimates CONJUNCTION real experience. real experience CONJUNCTION Q - estimates. model - free learning CONJUNCTION model - based search. model - based search CONJUNCTION model - free learning. MCTS USED-FOR value computation. physical reasoning tasks CONJUNCTION Atari. Atari CONJUNCTION physical reasoning tasks. agents USED-FOR physical reasoning tasks. agents USED-FOR Atari. it PART-OF agents. model USED-FOR Q - learning agent. Q - learning agent USED-FOR SAVE. SAVE COMPARE model - based search approaches. model - based search approaches COMPARE SAVE. training steps USED-FOR SAVE. real experience USED-FOR SAVE. model - free learning CONJUNCTION planning. planning CONJUNCTION model - free learning. Method is Search with Amortized Value Estimates. OtherScientificTerm are search budgets, and search. ","This paper proposes a method for model-free Q-learning with Amortized Value Estimates (SAVE) that combines model-based search with a Q-learned agent. The method is based on the MCTS framework, which is used to compute the state-action values of an agent, and the agent is trained on a set of Q-values. The authors show that SAVE outperforms the baselines in terms of performance on a number of tasks, including Atari and physical reasoning.","This paper proposes a method for model-free Q-learning with Amortized Value Estimates (SAVE) that combines model-based search with a Q-learned agent. The method is based on the MCTS framework, which is used to compute the state-action values of an agent, and the agent is trained on a set of Q-values. The authors show that SAVE outperforms the baselines in terms of performance on a number of tasks, including Atari and physical reasoning."
12861,SP:ab451cc0ec221864a5da532eceba0f021f30def4,"deep learning USED-FOR low - level vision tasks. it USED-FOR 3D visualization of the 2D input scenery. network architecture USED-FOR stereoscopic view synthesis. network architecture USED-FOR Deep 3D Pan. globally and locally adaptive dilations USED-FOR t - shaped ” adaptive kernels. local 3D geometries USED-FOR synthesis of naturally looking 3D panned views. globally and locally adaptive dilation FEATURE-OF t - shaped adaptive kernel. t - shaped adaptive kernel USED-FOR monster - net. t - shaped adaptive kernel USED-FOR network architecture. monster - net HYPONYM-OF network architecture. 2 - D input image USED-FOR synthesis of naturally looking 3D panned views. VICLAB STEREO indoors dataset EVALUATE-FOR method. PSNR CONJUNCTION SSIM. SSIM CONJUNCTION PSNR. RMSE CONJUNCTION PSNR. PSNR CONJUNCTION RMSE. PSNR EVALUATE-FOR monster - net. SSIM EVALUATE-FOR monster - net. RMSE EVALUATE-FOR monster - net. monster - net USED-FOR image structures. synthesized images FEATURE-OF image structures. coherent geometry FEATURE-OF synthesized images. coherent geometry FEATURE-OF image structures. SOTA USED-FOR unsupervised monocular depth estimation task. that USED-FOR unsupervised monocular depth estimation task. disparity information COMPARE that. that COMPARE disparity information. disparity information USED-FOR unsupervised monocular depth estimation task. disparity information COMPARE SOTA. SOTA COMPARE disparity information. t - shaped ” kernel COMPARE SOTA. SOTA COMPARE t - shaped ” kernel. SOTA USED-FOR that. t - shaped ” kernel USED-FOR disparity information. Task is single - image - based view synthesis. OtherScientificTerm are parallel camera views, arbitrary camera positions, X - axis, and global camera shift. Material is KITTI. ","This paper proposes a new network architecture for 3D panning. The proposed network is based on a t-shaped adaptive kernel, which can be used for both globally and locally adaptive dilation. The authors show that the proposed network outperforms the state-of-the-art baselines on the VICLAB STEREO indoors dataset. They also show that their network can be applied to unsupervised monocular depth estimation task.","This paper proposes a new network architecture for 3D panning. The proposed network is based on a t-shaped adaptive kernel, which can be used for both globally and locally adaptive dilation. The authors show that the proposed network outperforms the state-of-the-art baselines on the VICLAB STEREO indoors dataset. They also show that their network can be applied to unsupervised monocular depth estimation task."
12870,SP:e308cf28f7bd5d8e6c36517e2845298ccd401f5d,"representation CONJUNCTION generative model. generative model CONJUNCTION representation. Variational AutoEncoder ( VAE ) USED-FOR generative model. Variational AutoEncoder ( VAE ) USED-FOR representation. tasks PART-OF VAE. capacity FEATURE-OF variational network. information properties FEATURE-OF network. information properties USED-FOR tasks. network HYPONYM-OF information properties. generative model HYPONYM-OF information properties. theoretical objective USED-FOR maximal informative generative model. one HYPONYM-OF generative model. Capacity - Constrained InfoMax ( CCIM ) HYPONYM-OF generative model. one USED-FOR clustered and robust representation. one USED-FOR sharper samples. variational lower bound USED-FOR sharper samples. variational lower bound USED-FOR CCIM. variational lower bound USED-FOR one. one HYPONYM-OF VAE. OtherScientificTerm are informative one, and network capacity. Method is Variational InfoMax AutoEncoder ( VIMAE ). ","This paper studies the capacity-constrained InfoMax (CCIM) objective for variational auto-encoders (VAEs). The authors propose a variational variational autoencoder (VAE) that can be viewed as a variant of the InfoMax objective. The main contribution of the paper is the theoretical analysis of the variational capacity of the VAE. The authors show that the capacity of a VAE is a function of the number of samples and the information properties of the network. They also show that for any variational VAE, the maximum informative one is the one that maximizes the capacity. ","This paper studies the capacity-constrained InfoMax (CCIM) objective for variational auto-encoders (VAEs). The authors propose a variational variational autoencoder (VAE) that can be viewed as a variant of the InfoMax objective. The main contribution of the paper is the theoretical analysis of the variational capacity of the VAE. The authors show that the capacity of a VAE is a function of the number of samples and the information properties of the network. They also show that for any variational VAE, the maximum informative one is the one that maximizes the capacity. "
12879,SP:ef4a0c82cc364b797fba0ba86a91d9945b66a193,"approach COMPARE Skip - gram. Skip - gram COMPARE approach. disconnected networks USED-FOR latent feature identification. latent feature identification HYPONYM-OF applications. embeddings USED-FOR matrices of node - feature pointwise mutual information. algorithms COMPARE models. models COMPARE algorithms. social, web and citation network datasets EVALUATE-FOR models. social, web and citation network datasets EVALUATE-FOR algorithms. Method are network embedding algorithms, random walks, and multiscale approach ( MUSAE ). OtherScientificTerm are local distribution, and attribute - neighborhood relationships. ","This paper proposes a multiscale approach for latent feature identification in disconnected networks. The key idea is to learn a matrices of node-feature pointwise mutual information (MUSAE) for each node in the network, which is then used to train a network embedding algorithm. The authors show that the proposed method outperforms Skip-gram and random walks on a variety of datasets. ","This paper proposes a multiscale approach for latent feature identification in disconnected networks. The key idea is to learn a matrices of node-feature pointwise mutual information (MUSAE) for each node in the network, which is then used to train a network embedding algorithm. The authors show that the proposed method outperforms Skip-gram and random walks on a variety of datasets. "
12888,SP:70d92189aedeb4148b61b987d97a3c15898dd834,"multiple phase transitions FEATURE-OF IB objective. definition USED-FOR IB phase transitions. formula USED-FOR IB phase transitions. secondorder calculus of variations USED-FOR formula. Fisher information matrix USED-FOR parameterized models. canonical - correlation analysis ( CCA ) USED-FOR linear settings. maximum ( nonlinear ) correlation USED-FOR IB phase transition. theory USED-FOR algorithm. theory USED-FOR phase transitions. algorithm USED-FOR phase transitions. algorithm USED-FOR prominent phase transitions. theory CONJUNCTION algorithm. algorithm CONJUNCTION theory. theory USED-FOR prominent phase transitions. algorithm USED-FOR class difficulty. class difficulty FEATURE-OF MNIST. categorical datasets EVALUATE-FOR theory. categorical datasets USED-FOR phase transitions. CIFAR10 USED-FOR prominent phase transitions. Task is Information Bottleneck ( IB ). Generic are terms, dataset, and transitions. OtherScientificTerm are encoding distribution, β, and IB loss landscape. Metric is prediction accuracy. ","This paper studies the problem of learning information bottlenecks (IB) in the context of discrete phase transitions. The authors propose a new formulation of the IB objective, called Information Bottleneck (IB), which is defined as a function of the number of phase transitions in the data. The paper provides a second-order calculus of variations (CCA) to define the IB phase transition, which is then used to derive an algorithm for learning the IB loss landscape. The proposed algorithm is evaluated on MNIST and CIFAR-10, and is shown to outperform existing methods in terms of prediction accuracy.","This paper studies the problem of learning information bottlenecks (IB) in the context of discrete phase transitions. The authors propose a new formulation of the IB objective, called Information Bottleneck (IB), which is defined as a function of the number of phase transitions in the data. The paper provides a second-order calculus of variations (CCA) to define the IB phase transition, which is then used to derive an algorithm for learning the IB loss landscape. The proposed algorithm is evaluated on MNIST and CIFAR-10, and is shown to outperform existing methods in terms of prediction accuracy."
12897,SP:fecfd5e98540e2d146a726f94802d96472455111,"advantage function estimation methods USED-FOR reinforcement learning. independence property USED-FOR importance sampling advantage estimator. close - to - zero variance FEATURE-OF importance sampling advantage estimator. it COMPARE Monte - Carlo estimator. Monte - Carlo estimator COMPARE it. reward decomposition model USED-FOR Monte - Carlo estimator. method COMPARE advantage estimation methods. advantage estimation methods COMPARE method. sample efficiency EVALUATE-FOR advantage estimation methods. advantage estimation methods USED-FOR complex environments. complex environments EVALUATE-FOR method. sample efficiency EVALUATE-FOR method. OtherScientificTerm are time horizon, Monte - Carlo return signal, and estimation variance. Method is advantage estimation. Generic is estimator. ",This paper studies the importance sampling advantage estimator for reinforcement learning. The main idea is to use a reward decomposition model to estimate the variance of the advantage function. The authors show that this estimator has a close-to-zero variance and is independent of the time horizon. They also show that their estimator can be used to improve the sample efficiency of existing advantage estimation methods.,This paper studies the importance sampling advantage estimator for reinforcement learning. The main idea is to use a reward decomposition model to estimate the variance of the advantage function. The authors show that this estimator has a close-to-zero variance and is independent of the time horizon. They also show that their estimator can be used to improve the sample efficiency of existing advantage estimation methods.
12906,SP:f0d84396e0ede7969d3f3f55549d214f20daf1b0,"approach USED-FOR infinite horizon off - policy evaluation. value function USED-FOR accuracy. value function USED-FOR bias - reduced augmentation of their method. density ratio CONJUNCTION value function estimation. value function estimation CONJUNCTION density ratio. method COMPARE methods. methods COMPARE method. Task is Infinite horizon off - policy policy evaluation. OtherScientificTerm are stationary density ratio, biases, and bias. Method is density ratio estimation. Generic is them. ",This paper proposes a bias-reduced augmentation method for infinite horizon off-policy evaluation. The proposed method is based on density ratio estimation and value function estimation. The authors show that the proposed method outperforms existing methods in terms of accuracy and bias reduction. ,This paper proposes a bias-reduced augmentation method for infinite horizon off-policy evaluation. The proposed method is based on density ratio estimation and value function estimation. The authors show that the proposed method outperforms existing methods in terms of accuracy and bias reduction. 
12915,SP:73f8dddb09333a739c609cc324a1e813d29f8874,"large - scale base classes USED-FOR generalization of prior knowledge. base data USED-FOR task - agnostic feature. Metric - Softmax loss USED-FOR task - agnostic feature. Metric - Softmax loss USED-FOR discriminative feature. Metric - Softmax loss COMPARE episodic training. episodic training COMPARE Metric - Softmax loss. episodic training USED-FOR discriminative feature. task - adaptive transformation USED-FOR classifier. classifier USED-FOR few - shot setting. task - adaptive transformation USED-FOR few - shot setting. approach COMPARE state - of - the - arts. state - of - the - arts COMPARE approach. mini - ImageNet CONJUNCTION CUB-200 - 2011 benchmarks. CUB-200 - 2011 benchmarks CONJUNCTION mini - ImageNet. CUB-200 - 2011 benchmarks EVALUATE-FOR state - of - the - arts. mini - ImageNet EVALUATE-FOR state - of - the - arts. CUB-200 - 2011 benchmarks EVALUATE-FOR approach. mini - ImageNet EVALUATE-FOR approach. Task is Few - shot classification. Generic are task, and two - stage framework. Method are Metric - Softmax classifier, and fine - tuning scheme. ","This paper proposes a new method for few-shot classification. The proposed method is based on the Metric-Softmax classifier, which is a two-stage framework. The first stage is to train a classifier on the base class and then fine-tune the classifier using a task-adaptive transformation. The second stage is a fine-tuning step to improve the performance of the model. Experiments on CUB-200-2011 and mini-ImageNet show the effectiveness of the proposed method.","This paper proposes a new method for few-shot classification. The proposed method is based on the Metric-Softmax classifier, which is a two-stage framework. The first stage is to train a classifier on the base class and then fine-tune the classifier using a task-adaptive transformation. The second stage is a fine-tuning step to improve the performance of the model. Experiments on CUB-200-2011 and mini-ImageNet show the effectiveness of the proposed method."
12924,SP:cde2a84c463cdab9b19fcbdaf1cfe20d0187dcfa,fluid flow USED-FOR simulations of complex flow phenomena. fluid flow HYPONYM-OF problems. data - driven approach USED-FOR numerical solvers. accuracy EVALUATE-FOR numerical solvers. fine simulation resolution FEATURE-OF numerical scheme. numerical scheme USED-FOR method. neural network USED-FOR correction. fully supervised learning methods CONJUNCTION unsupervised learning method. unsupervised learning method CONJUNCTION fully supervised learning methods. naive and an optimized data acquisition USED-FOR fully supervised learning methods. differentiable Navier - Stokes solver USED-FOR unsupervised learning method. fully supervised learning methods HYPONYM-OF learning approaches. learning approaches USED-FOR targeted learning problem. unsupervised learning method HYPONYM-OF learning approaches. approach USED-FOR arbitrary partial differential equation models. accuracy EVALUATE-FOR fluid flow simulations. Method is numerical methods. Task is nonlinear simulation problems. ,This paper proposes a data-driven approach to learn numerical solvers for fluid flow problems. The proposed method is based on the Navier-Stokes solver (NSS) and a neural network. The authors show that the proposed method outperforms the state-of-the-art unsupervised and fully supervised learning methods on a variety of fluid flow simulation problems. They also show that their method can be applied to arbitrary partial differential equation models.,This paper proposes a data-driven approach to learn numerical solvers for fluid flow problems. The proposed method is based on the Navier-Stokes solver (NSS) and a neural network. The authors show that the proposed method outperforms the state-of-the-art unsupervised and fully supervised learning methods on a variety of fluid flow simulation problems. They also show that their method can be applied to arbitrary partial differential equation models.
12933,SP:e6534cd49bdc266dbeb111682ad37ef9d666e31e,"problem USED-FOR downstream online continual learning tasks. resource constrained data collection USED-FOR learning methods. discrete autoencoders PART-OF Quantization Modules ( SQM ). Quantization Modules ( SQM ) PART-OF architecture. discrete autoencoders PART-OF architecture. module USED-FOR latent space. latent space FEATURE-OF module. module USED-FOR module. methods COMPARE approach. approach COMPARE methods. method USED-FOR applications. episodic memory USED-FOR Experience Replay. episodic memory USED-FOR SQM. fixed memory budget USED-FOR continual learning benchmarks. method USED-FOR online compression of larger images. it USED-FOR modalities. Imagenet USED-FOR online compression of larger images. LiDAR data HYPONYM-OF modalities. Task is Online Continual Compression. Generic are learned representation, and modularity. OtherScientificTerm are moderate compressions, and pretraining. ","This paper proposes a new method for online continual learning. The authors propose to use discrete autoencoders (SQM) to learn the latent space of the learned representation, which is then used as an episodic memory. The proposed method is evaluated on several continual learning benchmarks, including Imagenet, Experience Replay, and LiDAR.","This paper proposes a new method for online continual learning. The authors propose to use discrete autoencoders (SQM) to learn the latent space of the learned representation, which is then used as an episodic memory. The proposed method is evaluated on several continual learning benchmarks, including Imagenet, Experience Replay, and LiDAR."
12942,SP:4ada8234990b4dbcdecb6bafeb6f509263661ae8,"framework USED-FOR multi - label problem. Metric Learning USED-FOR k nearest neighbor algorithms. k nearest neighbor algorithms USED-FOR multi - label problem. k nearest neighbor algorithms HYPONYM-OF framework. deep representation approach USED-FOR metric learning. neural networks USED-FOR feature data. raw image data USED-FOR neural networks. neural networks USED-FOR deep representation approach. deep convolutional networks USED-FOR image data. Bidirectional Representation learning USED-FOR network architecture. deep neural networks USED-FOR metric space. metric space FEATURE-OF testing data. deep neural networks USED-FOR model. approach COMPARE methods. methods COMPARE approach. multi - labels tasks EVALUATE-FOR approach. multi - labels tasks EVALUATE-FOR methods. systematic metric EVALUATE-FOR approach. systematic metric EVALUATE-FOR methods. Task is Multi - Label Learning task. Method are multiple - label metric learning, and multi - label metric learning. OtherScientificTerm are application restriction, and label dependency. Material is multi - label data set. ",This paper proposes a new framework for multi-label metric learning. The proposed method is based on Bidirectional Representation Learning (DRL) and uses deep neural networks to learn the metric space. The authors show that the proposed method outperforms the baselines on a variety of multi-labels tasks. ,This paper proposes a new framework for multi-label metric learning. The proposed method is based on Bidirectional Representation Learning (DRL) and uses deep neural networks to learn the metric space. The authors show that the proposed method outperforms the baselines on a variety of multi-labels tasks. 
12951,SP:d81a0edd94cc0b32734c42f1fb65d7070f963f86,large batch sizes CONJUNCTION data parallelism. data parallelism CONJUNCTION large batch sizes. large batch sizes USED-FOR neural networks training. convergence rate EVALUATE-FOR asynchronous methods. dynamical stability USED-FOR asynchronous training. asynchronous stochastic gradient descent algorithm USED-FOR minima. closed - form rules USED-FOR learning rate. momentum PART-OF analysis. momentum USED-FOR training stability. OtherScientificTerm is delay. Task is training. Metric is generalization. Generic is algorithm. ,"This paper considers the problem of training a neural network in an asynchronous setting. The authors propose an asynchronous stochastic gradient descent algorithm, which can be viewed as an extension of the existing asynchronous gradient descent algorithms. The main contribution of this paper is to provide theoretical analysis of the dynamics of the learning rate and the convergence rate of the proposed algorithm. They show that the algorithm converges to the minima of the training process in a closed form, and that the learning rates of the algorithm converge to minima when the number of training samples is small. They also provide theoretical results on the generalization properties of their algorithm. ","This paper considers the problem of training a neural network in an asynchronous setting. The authors propose an asynchronous stochastic gradient descent algorithm, which can be viewed as an extension of the existing asynchronous gradient descent algorithms. The main contribution of this paper is to provide theoretical analysis of the dynamics of the learning rate and the convergence rate of the proposed algorithm. They show that the algorithm converges to the minima of the training process in a closed form, and that the learning rates of the algorithm converge to minima when the number of training samples is small. They also provide theoretical results on the generalization properties of their algorithm. "
12960,SP:05587c2ba9ff9bf3574604a60f614dd807c95e22,"Value estimation PART-OF reinforcement learning ( RL ) paradigm. rich transition structure USED-FOR Model learning. weak scalar signal USED-FOR model - free methods. approach USED-FOR representation learning. representation learning USED-FOR RL. OtherScientificTerm are predictors, reward function, features, and value function. Generic are approaches, and task. Task are value prediction, and policy evaluation settings. Material is Atari 2600 games. ",This paper proposes a new method for value estimation in reinforcement learning. The key idea is to use a weak scalar signal to learn the transition structure between the reward function and the value function. The authors show that this transition structure can be used to improve the performance of model-free methods. They also show that the proposed method can be applied to the Atari 2600 games.,This paper proposes a new method for value estimation in reinforcement learning. The key idea is to use a weak scalar signal to learn the transition structure between the reward function and the value function. The authors show that this transition structure can be used to improve the performance of model-free methods. They also show that the proposed method can be applied to the Atari 2600 games.
12969,SP:6388fb91f2eaac02d9406672760a237f78735452,node classification CONJUNCTION graph classification. graph classification CONJUNCTION node classification. Graph Neural Networks ( GNNs ) USED-FOR graph related tasks. graph classification HYPONYM-OF graph related tasks. node classification HYPONYM-OF graph related tasks. graph neural networks USED-FOR adversarial attacks. graph rewiring operation USED-FOR graph. graph rewiring operation COMPARE operators. operators COMPARE graph rewiring operation. reinforcement learning USED-FOR attack strategy. rewiring operation USED-FOR reinforcement learning. rewiring operation USED-FOR attack strategy. real world graphs EVALUATE-FOR framework. perturbation FEATURE-OF graph structure. OtherScientificTerm is edges. ,"This paper proposes a novel adversarial attack method for graph neural networks. The proposed method is based on graph rewiring, which is a re-wiring operation that rewires the edges of a graph. The authors show that the proposed method can be used to improve the robustness of GNNs against adversarial attacks. The method is evaluated on several real-world graph classification and node classification tasks.","This paper proposes a novel adversarial attack method for graph neural networks. The proposed method is based on graph rewiring, which is a re-wiring operation that rewires the edges of a graph. The authors show that the proposed method can be used to improve the robustness of GNNs against adversarial attacks. The method is evaluated on several real-world graph classification and node classification tasks."
12978,SP:233b12d422d0ac40026efdf7aab9973181902d70,"selected data USED-FOR neural networks. Stein ’s unbiased risk estimator ( SURE ) USED-FOR denoising problems. divergence term PART-OF SURE. neural network framework USED-FOR divergence term. close form expression USED-FOR unbiased estimator. unbiased estimator USED-FOR prediction error. close form expression USED-FOR prediction error. piecewise linear representation USED-FOR encoderdecoder CNN. bootstrap and aggregation scheme USED-FOR neural network. close form representation USED-FOR bootstrap and aggregation scheme. neural network USED-FOR identity mapping. inverse problems EVALUATE-FOR algorithm. Generic are architectures, and it. ","This paper proposes a new unbiased risk estimator (SURE) for denoising problems. SURE is based on Stein’s unbiased risk estimation (STEIN) framework. The authors show that SURE can be decomposed into two parts: (1) a bootstrap and aggregation scheme, and (2) a piecewise linear representation of the denoised data. The bootstrap is used to compute the prediction error, and the aggregation is used for the identity mapping.","This paper proposes a new unbiased risk estimator (SURE) for denoising problems. SURE is based on Stein’s unbiased risk estimation (STEIN) framework. The authors show that SURE can be decomposed into two parts: (1) a bootstrap and aggregation scheme, and (2) a piecewise linear representation of the denoised data. The bootstrap is used to compute the prediction error, and the aggregation is used for the identity mapping."
12987,SP:978555e8eced096b92b37a91fc16b60f7d99c2b6,"meta - learning approaches USED-FOR few - shot classification. meta - knowledge CONJUNCTION task - specific learning. task - specific learning CONJUNCTION meta - knowledge. Bayesian inference framework USED-FOR objective. variational inference USED-FOR objective. variational inference USED-FOR it. it COMPARE meta - learning approaches. meta - learning approaches COMPARE it. balancing component CONJUNCTION Bayesian learning framework. Bayesian learning framework CONJUNCTION balancing component. OtherScientificTerm are distributional difference, task relatedness, and balancing variables. Method are meta - learning model, and meta - learning. Material is multiple realistic taskand class - imbalanced datasets. ",This paper proposes a meta-learning framework for few-shot classification. The proposed method is based on the idea of meta-knowledge and meta-balancing. The authors propose a variational inference framework for the meta-weighting objective and a Bayesian learning framework for balancing variables. Experiments show that the proposed method outperforms the baselines on both real-world and synthetic datasets.,This paper proposes a meta-learning framework for few-shot classification. The proposed method is based on the idea of meta-knowledge and meta-balancing. The authors propose a variational inference framework for the meta-weighting objective and a Bayesian learning framework for balancing variables. Experiments show that the proposed method outperforms the baselines on both real-world and synthetic datasets.
12996,SP:2395947721c4a337701a7c61cd4ba5c0e38fcc9b,"high - dimensional, continuous observations CONJUNCTION unknown dynamics. unknown dynamics CONJUNCTION high - dimensional, continuous observations. distribution shift USED-FOR Supervised learning methods. behavioral cloning ( BC ) USED-FOR Supervised learning methods. inverse RL CONJUNCTION generative adversarial imitation learning ( GAIL ). generative adversarial imitation learning ( GAIL ) CONJUNCTION inverse RL. generative adversarial imitation learning ( GAIL ) HYPONYM-OF methods. inverse RL HYPONYM-OF methods. reinforcement learning ( RL ) USED-FOR methods. generative adversarial imitation learning ( GAIL ) HYPONYM-OF reinforcement learning ( RL ). inverse RL HYPONYM-OF reinforcement learning ( RL ). methods USED-FOR reward function. reward function USED-FOR task. adversarial training USED-FOR complex and brittle approximation techniques. complex and brittle approximation techniques USED-FOR reward function. complex and brittle approximation techniques USED-FOR methods. RL USED-FOR alternative. sparsity prior USED-FOR long - horizon imitation. regularized variant of BC USED-FOR SQIL. sparsity prior USED-FOR regularized variant of BC. SQIL COMPARE GAIL. GAIL COMPARE SQIL. Atari CONJUNCTION MuJoCo. MuJoCo CONJUNCTION Atari. Box2D CONJUNCTION Atari. Atari CONJUNCTION Box2D. SQIL COMPARE BC. BC COMPARE SQIL. Atari FEATURE-OF image - based and low - dimensional tasks. Box2D FEATURE-OF image - based and low - dimensional tasks. MuJoCo FEATURE-OF image - based and low - dimensional tasks. image - based and low - dimensional tasks EVALUATE-FOR SQIL. image - based and low - dimensional tasks EVALUATE-FOR GAIL. imitation method COMPARE methods. methods COMPARE imitation method. constant rewards FEATURE-OF RL. constant rewards USED-FOR imitation method. RL USED-FOR imitation method. learned rewards USED-FOR methods. OtherScientificTerm are expert behavior, error accumulation, out - of - distribution states, and constant reward. Method are RL agent, and soft Q imitation learning ( SQIL ). Generic is method. ","This paper proposes a new reinforcement learning method called soft Q imitation learning (SQIL). SQIL is a variant of behavioral cloning (BC) that uses a sparsity prior for long-horizon imitation. The authors show that SQIL outperforms BC on MuJoCo, Atari, and Box2D tasks.","This paper proposes a new reinforcement learning method called soft Q imitation learning (SQIL). SQIL is a variant of behavioral cloning (BC) that uses a sparsity prior for long-horizon imitation. The authors show that SQIL outperforms BC on MuJoCo, Atari, and Box2D tasks."
13005,SP:37a4825aaeb899187b957d9ed9ae657617f4a055,"Point clouds HYPONYM-OF Lagrangian representation. deep - learning method USED-FOR stable and temporally coherent feature spaces. stable and temporally coherent feature spaces FEATURE-OF points clouds. temporal loss function USED-FOR mingling. higher time derivatives USED-FOR temporal loss function. super - resolution method CONJUNCTION truncation approach. truncation approach CONJUNCTION super - resolution method. techniques CONJUNCTION truncation approach. truncation approach CONJUNCTION techniques. techniques PART-OF super - resolution method. method USED-FOR large, deforming point sets. Generic are approaches, and approach. OtherScientificTerm are time dimension, flickering, undesirable local minima, halo structures, and halos. ","This paper proposes a deep learning method for learning stable and temporally coherent feature spaces for Lagrangian point clouds. The proposed method is based on the notion of mingling, which is an important problem in point clouds, and the authors propose to use a temporal loss function to reduce the variance of the temporal loss. The authors show that the proposed method can be applied to both super-resolution and truncation methods, and show that it can achieve better performance than existing methods. ","This paper proposes a deep learning method for learning stable and temporally coherent feature spaces for Lagrangian point clouds. The proposed method is based on the notion of mingling, which is an important problem in point clouds, and the authors propose to use a temporal loss function to reduce the variance of the temporal loss. The authors show that the proposed method can be applied to both super-resolution and truncation methods, and show that it can achieve better performance than existing methods. "
13014,SP:af54d04f219d381208c049b8a9c59b8cdd1783e0,"hand - crafted or Euclidean cost USED-FOR cost functions. side information USED-FOR cost function. side information USED-FOR subset correspondence. annotated cell type PART-OF single - cell data. side information USED-FOR cost function. marriage - matching CONJUNCTION single - cell RNA - seq. single - cell RNA - seq CONJUNCTION marriage - matching. images CONJUNCTION marriage - matching. marriage - matching CONJUNCTION images. images EVALUATE-FOR method. single - cell RNA - seq EVALUATE-FOR method. method COMPARE state - of - the - art benchmarks. state - of - the - art benchmarks COMPARE method. marriage - matching EVALUATE-FOR method. images CONJUNCTION single - cell RNA - seq. single - cell RNA - seq CONJUNCTION images. Generic is it. OtherScientificTerm is confounding. Method are Optimal transport ( OT ), OT, transport cost function, and Sinkhorn algorithm. ","This paper proposes Optimal Transport (OT), a method for learning a cost function for a subset of single-cell RNA-seq data. The proposed method is based on the Sinkhorn algorithm. The main idea is to use the side information of the cost function to learn a subset correspondence between the cell type and the subset correspondence. The authors show that the proposed method outperforms state-of-the-art methods on a number of benchmarks. ","This paper proposes Optimal Transport (OT), a method for learning a cost function for a subset of single-cell RNA-seq data. The proposed method is based on the Sinkhorn algorithm. The main idea is to use the side information of the cost function to learn a subset correspondence between the cell type and the subset correspondence. The authors show that the proposed method outperforms state-of-the-art methods on a number of benchmarks. "
13023,SP:fc86b06a367f6790c76b89ec3bfe4cb8627c540a,large datasets USED-FOR method. algorithm USED-FOR representation underlying normal data. clustering technique USED-FOR high dimensional data. clustering CONJUNCTION representation learning. representation learning CONJUNCTION clustering. clustering USED-FOR hypothesizing normal candidate subset. normal data subset USED-FOR autoencoder. representation learning USED-FOR hypothesizing normal candidate subset. reconstruction error USED-FOR scoring function. autoencoder USED-FOR reconstruction error. public benchmark datasets EVALUATE-FOR method. method COMPARE semi - supervised techniques. semi - supervised techniques COMPARE method. method COMPARE unsupervised techniques. unsupervised techniques COMPARE method. public benchmark datasets EVALUATE-FOR unsupervised techniques. unsupervised techniques COMPARE semi - supervised techniques. semi - supervised techniques COMPARE unsupervised techniques. Task is normal data selection. ,This paper proposes a new algorithm for normal data selection. The proposed method is based on the clustering technique for high dimensional data. The authors show that the proposed algorithm is able to learn the representation of the normal data subset from a small subset of the original data. They also show that their algorithm can be applied to semi-supervised and unsupervised data selection tasks. ,This paper proposes a new algorithm for normal data selection. The proposed method is based on the clustering technique for high dimensional data. The authors show that the proposed algorithm is able to learn the representation of the normal data subset from a small subset of the original data. They also show that their algorithm can be applied to semi-supervised and unsupervised data selection tasks. 
13032,SP:9e831d3595c15ca34cadb3c4a5b02230593b4ccb,"learning control policies USED-FOR reward function. local reward improvement update USED-FOR first step. L norm CONJUNCTION Kullback - Leibler divergence. Kullback - Leibler divergence CONJUNCTION L norm. convergence FEATURE-OF PCPO. metrics EVALUATE-FOR PCPO. L norm HYPONYM-OF metrics. Kullback - Leibler divergence HYPONYM-OF metrics. metrics USED-FOR convergence. control tasks EVALUATE-FOR PCPO. constraint violation CONJUNCTION reward. reward CONJUNCTION constraint violation. reward EVALUATE-FOR PCPO. OtherScientificTerm are constraints, fairness, policy, constraint set, and policy update. Generic are algorithm, and second step. Method is iterative method. Metric is reward improvement. ","This paper studies the problem of learning control policies with local reward improvement (PCPO) in the context of fairness. The authors propose a new algorithm PCPO, which is an iterative method for learning a policy that is robust to constraint violations and reward changes. The algorithm is based on the idea that the reward of a policy is a function of the constraint set, and the goal is to improve the fairness of the policy.  The authors show that PCPO converges to a convergence rate of the Kullback-Leibler divergence (KLD) and the L norm (L norm of the reward) in terms of both reward and KLD. They also show that the convergence rate for PCPO can be extended to the case of constraint violation and reward change. ","This paper studies the problem of learning control policies with local reward improvement (PCPO) in the context of fairness. The authors propose a new algorithm PCPO, which is an iterative method for learning a policy that is robust to constraint violations and reward changes. The algorithm is based on the idea that the reward of a policy is a function of the constraint set, and the goal is to improve the fairness of the policy.  The authors show that PCPO converges to a convergence rate of the Kullback-Leibler divergence (KLD) and the L norm (L norm of the reward) in terms of both reward and KLD. They also show that the convergence rate for PCPO can be extended to the case of constraint violation and reward change. "
13041,SP:068a0bb2497373acad5f70e66c61b71465b2de3d,"analogy structure FEATURE-OF embedding space. low rank transformation USED-FOR embedding. embedding transformation USED-FOR relative distances. α USED-FOR word embedding. Method is word embedding methods. Generic are inner mechanism, and method. OtherScientificTerm is word - context co - occurrence space. Material is real datasets. ","This paper studies the problem of word embedding in the context co-occurrence space. The authors propose a low-rank transformation for word embeddings, which is based on the idea that the embedding space is analogy structure, and that the inner mechanism can be viewed as a low rank transformation. They show that this low rank embedding transformation can be used to learn the relative distances between two words, and they show that it can be applied to any embedding method. ","This paper studies the problem of word embedding in the context co-occurrence space. The authors propose a low-rank transformation for word embeddings, which is based on the idea that the embedding space is analogy structure, and that the inner mechanism can be viewed as a low rank transformation. They show that this low rank embedding transformation can be used to learn the relative distances between two words, and they show that it can be applied to any embedding method. "
13050,SP:b2d099c78b48aab509ab64027ca49e9a47079fc0,"Similarity measurement USED-FOR data mining and machine learning tasks. approximate Random Projection Trees PART-OF X - Forest. RP Trees USED-FOR similarity measurement. layers PART-OF tree. randomness USED-FOR partition. real - world datasets EVALUATE-FOR model. Euclidean distance - based similarity metrics USED-FOR clustering tasks. model COMPARE RP Trees. RP Trees COMPARE model. X - Forest COMPARE RP Trees. RP Trees COMPARE X - Forest. X - Forest HYPONYM-OF model. efficiency EVALUATE-FOR model. accuracy EVALUATE-FOR RP Trees. Method is similarity measurement solution. Metric are speed, and exalted speed. OtherScientificTerm are prior knowledge, and projection vectors. Task is similarity measurements. ",This paper proposes an approximate Random Projection Trees (RP-Tree) model for clustering tasks. The proposed method is based on the idea of randomness in the projection trees. The authors show that the proposed method can be used to improve the efficiency of the clustering task. The method is evaluated on a number of real-world datasets. ,This paper proposes an approximate Random Projection Trees (RP-Tree) model for clustering tasks. The proposed method is based on the idea of randomness in the projection trees. The authors show that the proposed method can be used to improve the efficiency of the clustering task. The method is evaluated on a number of real-world datasets. 
13059,SP:77b8bed08af8be8af0c65a72a6e22cfb02645d02,"Statistical inference methods USED-FOR machine learning. Markov chain Monte Carlo ( MCMC ) CONJUNCTION variational inference ( VI ). variational inference ( VI ) CONJUNCTION Markov chain Monte Carlo ( MCMC ). Markov chain Monte Carlo ( MCMC ) USED-FOR inference algorithms. variational inference ( VI ) USED-FOR inference algorithms. MCMC CONJUNCTION VI. VI CONJUNCTION MCMC. simulation bias FEATURE-OF finite - length MCMC chains. hybrid method USED-FOR VI. gradient - based optimisation USED-FOR hybrid method. gradient - based optimisation USED-FOR simulation bias. method USED-FOR low - biased samples. approximation bias CONJUNCTION computational efficiency. computational efficiency CONJUNCTION approximation bias. method USED-FOR MCMC hyper - parameters. method COMPARE hybrid methods. hybrid methods COMPARE method. MCMC CONJUNCTION VI. VI CONJUNCTION MCMC. hybrid methods USED-FOR MCMC. hybrid methods USED-FOR VI. Generic is methods. Method are MCMC methods, and VI methods. OtherScientificTerm is MCMC simulation. ",This paper proposes a hybrid method for variational inference (VI) and Markov chain Monte Carlo (MCMC) based on gradient-based optimisation. The authors show that the proposed method is able to reduce the simulation bias of MCMC and VI in finite-length MCMC chains. They also show that their method can be used to improve the computational efficiency of VI and MCMC. ,This paper proposes a hybrid method for variational inference (VI) and Markov chain Monte Carlo (MCMC) based on gradient-based optimisation. The authors show that the proposed method is able to reduce the simulation bias of MCMC and VI in finite-length MCMC chains. They also show that their method can be used to improve the computational efficiency of VI and MCMC. 
13068,SP:64f2744e938bd62cd47c1066dc404a42134953da,"treatment USED-FOR Inferring causal effects. observational data USED-FOR Inferring causal effects. state - of - the - art methods USED-FOR causal inference. adapted unconfoundedness hypothesis USED-FOR they. variational autoencoders USED-FOR missing values. variational autoencoders USED-FOR latent confounders. them PART-OF multiple imputation strategy. methodology COMPARE competitors. competitors COMPARE methodology. methodology USED-FOR non - linear models. non - linear models COMPARE competitors. competitors COMPARE non - linear models. OtherScientificTerm are covariates, and Missing data. Task are real - world analyses, and causal inference procedures. Generic is They. ","This paper proposes a new method for causal inference based on the unconfoundedness hypothesis. The proposed method is based on variational autoencoders, where the latent confounders are assumed to be non-linear. The authors show that the proposed method outperforms the state-of-the-art methods on a number of real-world datasets. ","This paper proposes a new method for causal inference based on the unconfoundedness hypothesis. The proposed method is based on variational autoencoders, where the latent confounders are assumed to be non-linear. The authors show that the proposed method outperforms the state-of-the-art methods on a number of real-world datasets. "
13077,SP:971d0d94adf5113ee1bef8df9ea7dbd508cf4cbc,"neural architecture search algorithm USED-FOR compact reinforcement learning ( RL ) policies. ENAS CONJUNCTION ES. ES CONJUNCTION ENAS. combinatorial search space FEATURE-OF NAS. edge - partitionings USED-FOR compact architectures. vanilla policies COMPARE compact policies. compact policies COMPARE vanilla policies. compression EVALUATE-FOR vanilla policies. compression EVALUATE-FOR compact policies. colorings USED-FOR policies. colorings USED-FOR RL tasks. Toeplitz matrices USED-FOR compact policies. structured neural network architectures USED-FOR RL problems. approach USED-FOR structured neural network architectures. mobile robotics FEATURE-OF RL problems. limited storage and computational resources FEATURE-OF mobile robotics. OtherScientificTerm are edge - partitionings ( colorings ), same - weight classes, and weight parameters. ",This paper proposes a combinatorial search algorithm for compact reinforcement learning (RL) policies. The algorithm is based on edge-partitioning (e.g. Toeplitz matrices). The authors show that the proposed algorithm outperforms existing methods in terms of compression and memory usage.,This paper proposes a combinatorial search algorithm for compact reinforcement learning (RL) policies. The algorithm is based on edge-partitioning (e.g. Toeplitz matrices). The authors show that the proposed algorithm outperforms existing methods in terms of compression and memory usage.
13086,SP:18aaba3423e81e9437b509d1a5e24836ef5635f6,"time - series USED-FOR representation learning. Group Transform approach USED-FOR representation learning. framework USED-FOR time - frequency transformations. Wavelet Transform HYPONYM-OF time - frequency transformations. approach USED-FOR non - linear transformations. affine transformations of a mother filter USED-FOR Wavelet Transform filter - bank. transformations USED-FOR signal representations. maps USED-FOR signal representations. maps USED-FOR transformations. parameterization USED-FOR non - linear map. Deep Neural Network USED-FOR Group Transform. time - series datasets EVALUATE-FOR framework. Generic is representation. OtherScientificTerm are invertible maps, and strictly increasing and continuous functions. ","This paper proposes a novel framework for learning non-linear transformations of time-frequency transformations. The proposed framework is based on the idea of affine transformations of a mother filter, which can be viewed as an affine version of the Wavelet Transform filter-bank. The authors show that the proposed framework can be used to learn representations for time-frequencies that are invertible. They also show that their framework is able to generalize to nonlinear transformations. ","This paper proposes a novel framework for learning non-linear transformations of time-frequency transformations. The proposed framework is based on the idea of affine transformations of a mother filter, which can be viewed as an affine version of the Wavelet Transform filter-bank. The authors show that the proposed framework can be used to learn representations for time-frequencies that are invertible. They also show that their framework is able to generalize to nonlinear transformations. "
13095,SP:3058e6bc5e8c62af325c214c9e1436d6cdf09204,"inductive biases USED-FOR real - world data properties. scale - free HYPONYM-OF real - world data properties. hyperbolic or spherical HYPONYM-OF non - Euclidean spaces. Euclidean geometry CONJUNCTION vector space operations. vector space operations CONJUNCTION Euclidean geometry. Euclidean geometry USED-FOR graph neural networks. vector space operations USED-FOR graph neural networks. graph convolutional networks ( GCN ) USED-FOR ( products of ) constant curvature spaces. gyro - barycentric coordinates USED-FOR Euclidean concept of the center of mass. models USED-FOR Euclidean counterparts. node classification CONJUNCTION distortion minimization. distortion minimization CONJUNCTION node classification. Euclidean GCNs USED-FOR node classification. Euclidean GCNs USED-FOR distortion minimization. distortion minimization USED-FOR symbolic data. non - Euclidean behavior FEATURE-OF symbolic data. Method is unified formalism. OtherScientificTerm are geometries of constant curvature, curvature, and discrete curvature. ","This paper proposes a unified formalism for graph convolutional neural networks (GCN) based on Euclidean geometry and vector space operations. The authors show that GCN can be used to learn non-Euclidean geometries of constant curvature and discrete curvature. They also show that this can be applied to graph neural networks for node classification, distortion minimization, and node classification. ","This paper proposes a unified formalism for graph convolutional neural networks (GCN) based on Euclidean geometry and vector space operations. The authors show that GCN can be used to learn non-Euclidean geometries of constant curvature and discrete curvature. They also show that this can be applied to graph neural networks for node classification, distortion minimization, and node classification. "
