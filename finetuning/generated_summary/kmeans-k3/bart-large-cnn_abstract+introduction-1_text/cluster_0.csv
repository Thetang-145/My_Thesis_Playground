paper_id,summary
SP:b19df5243359791fbaad005d6f13d7e9fdb0ff63,"This paper presents a method for multi - agent agent learning based on role - based learning. The authors propose to decompose joint action spaces into restricted role action spaces by clustering actions according to their effects on the environment and other agents. They further integrate information about action effects into the role policies to boost learning efficiency and policy generalization. The proposed method, RODE, outperforms the current state - of - the - art MARL algorithm on 9 out of the 14 scenarios that comprise the challenging StarCraft II scenario setting. They also demonstrate that conditioning the role selector and role policies on action representations enables learned RODE policies to be transferred to tasks with different numbers of actions and agents, including tasks with three times as many agents."
SP:7deb61890d97422a0fe141ca807f968c70ab239a,"This paper studies the behaviour of the stochastic subgradient descent ( SSGD ) method applied to over - parametrized nonsmooth optimization problems that satisfy an interpolation condition. The authors leverage the composite structure of the empirical risk minimization problems to prove that SSGD converges, respectively, with rates O(1/ ) and O(log(1 / ) for convex and strongly - convex objectives when interpolation holds. These rates coincide with established rates for the SGD method, which is applied to smooth problems that also satisfy an extrapolation condition.   The analysis provides a partial explanation for the empirical observation that sometimes SGD and SSGD behave similarly for training smooth and nons mooth machine learning models. The analysis also proves that the rate O( 1 / ) is optimal for the sub - gradient method in the convex setting."
SP:c7e0b3fedc0d0409d662dd612b529fdacad2b03e,"This paper presents a set of experiments on machine translation and ( masked ) language modelling tasks that explore non - linear transformer - based approaches for improving the performance of Transformer - based models. The main contributions are : ( 1 ) Demonstrating that transformers can perform better when some of the transformer layers are not updated, and ( 2 ) that the backward pass can be entirely skipped by approximating top - layer gradients using an approach called backskipping. The paper also presents results on different types of reservoir layers, including convolutional and recurrent neural network - based ones.   The paper is well - written and well - motivated, and the experimental results are clear and convincing. However, I have a few concerns that need to be addressed."
SP:ba9f1d4738ec67a440346f3ac6c4cf35f7232077,This paper proposes a new approach ( FILTRA ) to establish steerability between features in different group representation in cyclic group CN and dihedral group DN. The authors claim that the kernel constructed by filter transform can also be interpreted in the group representation theory. This interpretation help complete the puzzle of steerable CNN theory and provides a novel and simple approach to implement steerable convolution operators. Experiments are executed on multiple datasets to verify the feasibility of the proposed approach. FilTRA is based on the transformation invariance or equivariance in the network architecture to enhance the the network robustness on geometry transformation of data and reduce overfitting.    The main contributions of this paper are as follows :   1. This paper proposes to use filter transform technique to construct steerable neural networks. 2. It shows that filter transformed kernels can be used to convolve input / output features in both group representation and group representation. 3. It provides a new analysis for steerable networks based on group representation based analysis.
SP:c1116fbb4d058eb6be195b5d13d19a55ba86b602,"This paper proposes a neural program synthesis method based on a top - down recurrent neural model, where the goal is to find a program that satisfies user - provided constraints while maximizing the program ’s score with respect to a neural model. Specifically, we focus on multimodal synthesis tasks in which the user intent is expressed using combination of natural language ( NL ) and input - output examples. At the core of the method, a recurrent neural network is proposed that places distributions over abstract syntax trees conditioned on the NL input. The method is evaluated on the STRUCTUREDREGEX dataset ( Ye et al. 2020a ) for synthesizing regular expressions from linguistically diverse natural language descriptions and positive/negative examples. The results show that the proposed method substantially outperforms prior state - of - the - art techniques in terms of accuracy and explores fewer states during search. In addition, the method also allows us to leverage automated program analysis techniques."
SP:55e02d79146bbb42f1ab6d4fafa2db5ddbe599b0,"This paper presents a novel method for the classification of the specificity landscape of protease enzymes. The specificity landscape comprises the set of all sequence motifs that are either recognized / cut by the enzyme or, more importantly, that are not recognized/cut by the enzymes. This paper proposes a protein graph convolutional neural network ( PGCN ) that uses a structure - based molecular interaction graph generated using the Rosetta energy function that describes the topology and energetic features, to determine substrate specificity. The proposed method is based on experimentally derived data with a single enzyme and a physically - intuitive structure-based molecular interaction energy graph to solve the classification problem for protease specificity.   The authors compare their method with previously used machine learning models and show that its performance in classification tasks is equivalent or better than those of the previous state - of - the - art methods. The model also readily lends itself to the design of novel enzymes with tailored specificity against disease targets. The main contributions of the paper are as follows :   1. The authors propose a new method for classifying the specificity of a class of enzymes, which they call Protein Convolutional Neural Networks. They show that their method is more interpretable and highlights critical sub - graph patterns responsible for observed specificity patterns. 2. The method is capable of both prospective prediction of specificity of chosen protease enzyme and generating novel designed enzymes for disease treatment targets."
SP:7727eeb7b17ad94ddfa0cf24e64a9626d83a8876,"This paper studies the underestimation bias in Double Q - Learning ( DQ - Learning ), a classical method for reducing the overestimation bias caused by taking maximum estimated values in the Bellman operation. The authors show that DQL suffers from multiple non - optimal fixed points under an approximate Bellman operator. To address the concerns of converging non - optimal stationary solutions, they propose a simple but effective approach as a partial fix. This approach leverages an approximate dynamic programming to bound the target value. They extensively evaluate their proposed method in the Atari benchmark tasks and demonstrate its significant improvement over baseline algorithms. The proposed method is easy to combine with other existing techniques such as clipped double Q - learning."
SP:1d630b69f95392a5ef3d7d580b523e077a3555a8,"This paper presents a novel method for training deep generative models ( DGMs ) of high - dimensional natural images. The proposed method, called NOT - SO - BAG - GAN ( NSB -GAN ), consists of two steps : 1 ) generate images in low - frequency bands by training a sampler in the wavelet domain and 2 ) train a decoder in the decoder domain. The decoder is trained in parallel with the sampler, which allows for training on much lower - dimensional images than end - to - end models. The paper also proposes a novel wavelet decoder network, Wavelet - based down - based decoder, which preserves more structural information than pixel - based methods, leading to significantly better generative quality of the low - resolution sampler.   The main contributions of the paper are as follows :   1 ) Developing a method to jointly train DGs and decoders in a manner similar to how DeepGAN and VQVAE - 2 are trained. The main difference between these two methods is that DGs require much more compute resources ( 512 TPU - v3 cores ), whereas the proposed method only requires half that ( 256 TPU-v3 cores per decoder ). This means that the training of DGs can be done much more efficiently. 2 ) Leverage of the Wavelet-based decoder for training DGs. The method is shown to outperform the baseline BigGAN and ESRGAN on ImageNet with a distance ( FID ) of 10.59, beating the baseline with half the compute."
SP:b943a73b1ec34867371325748dc3a91ff4011947,"This paper analyzes why supervised self - supervised learning ( SSL ) is suitable for few - shot learning ( FSL ). SSL is a method that aims at distilling transferable knowledge on existing classes with large - scale labeled data to cope with novel classes for which only a few labeled data are available. Due to the limited number of novel classes, the initial embedding network becomes an essential component and can largely affect the performance in practice.   The paper first summarized the supervised FSL methods and explained why SSL is suitable. Then, it further analyzed the main difference between supervised training and self - supervised training on FSL and obtained the bound for the gap between self -supervised loss and supervised loss. Finally, it proposed potential ways to improve the test accuracy under the proposed improved training setting."
SP:bd552f98e6a447cefa6b1a9bbdf40bc6539fb643,"This paper analyzes the convergence property of neural networks with finite widths under the assumption that global minima are always close to zero in the case of first order methods. In particular, the authors show that for two - layer teacher - student neural networks, the input weights of student neurons eventually align with one of the teacher neurons. This is in contrast to the case with finite - width networks, where the input of each neuron is independent of the other. The authors then extend their analysis to more general cases where the proof can be reduced to analyzing the properties of a special class of functions that the authors call angular distance ( AD ) function. They show that these properties can be easily verified numerically under certain conditions.   First, they show that the minimum determinant of the mat mat mat is always non - positive unless perfect alignment or the problem is degenerated. Second, they demonstrate that AD function behaves as a potential function in the sense that the further from being degenerated, the further away from being non - negative. Third, they provide experimental results showing that the AD function can be verified under certain settings."
SP:0f62846913ec10b44ed32845770da0565479dc75,"This paper introduces Deep Adaptive Semantic Logic ( DASL ), a framework for automating the generation of deep neural networks that incorporates user - provided formal knowledge to improve learning from data. The authors provide formal semantics that demonstrate that our knowledge representation captures all of first order logic and that finite sampling from infinite domains converges to correct truth values. D ASL ’s representation improves on prior neuro -symbolic work by avoiding vanishing gradients, allowing deeper logical structure, and enabling richer interactions between the knowledge and learning components. The paper demonstrates the effectiveness of DAS l through a toy problem in which we add structure to an image classification task and demonstrate that knowledge of that structure reduces data requirements by a factor of 1000 for MNIST toy problem. The same is true for visual relationship detection task where the addition of commonsense knowledge improves performance by 10.7% in conditions of data scarcity."
SP:2f19259d65fab904c1b771244da3dcb2f8aa0c26,"This paper analyzes the effect of regularization and gradient regularization on the convergence of iterative recurrent neural networks ( ResNet ). It builds on previous work that iterative methods approximate the computational flexibility of non - recurrent residual blocks. The authors show that while ResNet can express iterative solutions, it does not learn them when trained conventionally on computer vision tasks. They then introduce regularizations to encourage iterative convergent computation and test whether regularization provides a useful inductive bias for ResNets. The results suggest that an induction bias favoring an iterative solution does not outweigh an computational flexibility advantage of the regularized network."
SP:6c14506b8b2b06043409d912e6bf877651aaa665,"Normalization techniques are crucial in stabilizing and accelerating the training of deep neural networks. However, they are mainly designed for the independent and identically distributed ( IID ) data, not satisfying many real - world out - of - distribution ( OOD ) situations. This paper presents two normalization methods, SelfNorm and CrossNorm, to promote OOD generalization. SelfNorm uses attention to recalibrate statistics ( channel - wise mean and variance ) while CrossNorm exchanges the statistics between feature maps. Extensive experiments on different domains ( vision and language ), tasks ( classification and segmentation ), and settings (supervised and semi - supervised ) show their effectiveness."
SP:2774abdc11917321dd4994af0f0da1ff824bea03,"This paper presents an approach for augmenting a simple attention module in reinforcement learning ( RL ) with attention mechanisms in order to improve the sample efficiency and final performance of an agent trained in the DeepMind Control Suite ( DCTS ). The attention module consists of two modules : ( i ) extract - able task - relevant information such as agent locations and movements without the need for data augmentations or contrastive losses ; ( ii ) augment - able information that is relevant to the agent such as the location of the agent at time t and the agent's location at time z in the environment.   The main contribution of this paper is to propose and study the effectiveness of augmenting the attention module of the RL agent with the Attention Mechanism ( MAC ) in the reinforcement learning setting. The authors compare the performance of their approach with the state - of - the - art unsupervised pre - training and generative modeling approaches in three domains : vision, language and speech. The experiments show that their approach is more effective than the baselines in two domains ( vision and speech ) and that it is able to extract interpretable information in both domains. The paper also shows that the approach is effective in the case of the language domain where the agent is trained in a supervised manner, while it is not so well suited for the vision domain where it is assumed that the agent has access to all the relevant information."
SP:31a7051d08d19c01e11f1fac2f3041ed2fa28f15,"GradNorm is an extension of Rotograd ( Zhang et al., 2018 ), a widely used gradient - based approach for training multi - task neural networks, where different tasks share and thus compete during learning for the network parameters. GradNorm eases the fitting of all individual tasks by dynamically equalizing the contribution of each task to the overall gradient magnitude. However, it does not prevent the individual tasks ’ gradients from conflicting, i.e. pointing towards opposite directions, and thus resulting in a poor multitask performance. In this work, the authors propose an extension to GradNorm that addresses this problem by dynamically homogenizing not only the gradient magnitudes but also their directions across tasks. For this purpose, they propose to add a layer of task - specific rotation matrices that aligns all the task gradients.    The authors analyze the contributions of GradNorm ( and its predecessor ) through the lens of game theory, providing theoretical guarantees on the algorithm stability and convergence. They empirically show that results are comparably good to the usual way of applying GradNorm. They also provide some guarantees regarding the norm of the gradient with respect to all the shared parameters."
SP:ac9ebd027b92527d9a87b13ad11d002d99a2b0f6,"This paper proposes a new constraint for unsupervised geometry -invariant image translation, called minimal geometry - distortion constraint ( MGC ). The constraint is designed to guarantee the consistency of geometry structure of source and translated images, and thus reduce translation mismatch in the translation process. The authors observe that the pixel values before and after translation are highly correlated if the geometric structure is preserved because the color transformation is more regular within specific object regions. Based on this observation, the authors propose a mutual information ( MI ) based dependency measure that models the nonlinear relationships of pixel values in the source and translate images. To estimate MI from data, they propose the relative Squared - Loss Mutual Information ( rSMI ) which can be efficiently estimated in an analytic form. The experiments show that MGC can significantly reduce the geometry distortion by better preserving geometric structures. The quantitative and qualitative comparisons with baselines ( models without MGC and state - of - the - art methods on several datasets ) demonstrate the superiority of the proposed MGC constraint."
SP:92a38d7d18f07f68b8f93c61180e2cc1dddd21de,"This paper studies the sampling sensitivity of generator discriminators in the field of point cloud generative adversarial networks ( GANs ). It shows that sampling - sensitive discriminators ( e.g. PointNetMax, PointNet++, DGCNN, PointConv, KPConv ) produce point clustering artifacts, while sampling - over - sensitive ( PointNet - Mix ) generate shape point clouds. The paper proposes a sampling spectrum to depict the different sampling sensitivities of the different discriminators and further study how different evaluation metrics weigh the sampling pattern against the geometry and propose several metrics forming the sampling spectrum of metrics. Guided by the proposed sampling spectrum, the authors discover a middle - point sampling - aware baseline discriminator, which improves all existing point cloud generators by a large margin on sampling -related metrics and can supervise both shape generation and point density uniformity. Surprisingly, even the most naive fully - connected generator, coupled with PointNet-Mix, simply beats all the start - of - the - art generators. This discovery conveys an important message to the community that instead of focusing on the generator design, people should invest more time into the discriminator and seek for more powerful sampling -aware discriminators."
SP:16c4be3eb162bc81cb3343c2fc115eb8e926a5b5,"This paper investigates the robustness of Capsule Networks ( CapsNets ) against adversarial attacks. The authors propose a novel attack method, called vote attack, to attack the output capsules ( the generators of the Capsule Network ) in a detection - aware attack. They show that the vote attack is more effective than the traditional white - box attack and the adversarial attack. The main observation is that adversarial examples can manipulate the votes from primary capsules to attack other generators more effectively. They integrate the proposed vote attack into the detection aware attack paradigm, which bypasses the class - conditional reconstruction based detection method. Extensive experiments demonstrate the superior attack performance of the proposed attack method.   The main contributions of this paper are as follows :   1. An analysis of adversarial representations of the output of a CapsuleNet generator. This is done by comparing the representation produced by the generator when the generator is attacked with the representation generated by the output generator when not attacked. The analysis shows that the representations produced by both the generator and the generator output are significantly different. 2. Motivated by the findings, the authors propose an effective and efficient vote attack to circumvent the detection error of the detection algorithm. 3. They conduct extensive experiments to validate their proposed method."
SP:dbd093dff7a38ba8882bb8119c34623ddaaf4cc6,"Meta - reinforcement learning ( RM ) aims at finding a policy that is able to generalize to new environments. When facing a new environment, this policy must explore to identify its particular characteristics and then exploit this information for collecting reward. Even though policies based on recurrent neural networks can be used in this setting by training them on multiple environments, they often fail to model this trade - off, or solve it at a very high computational cost. In this paper, a new algorithm, IMPORT, is proposed that uses privileged information in the form of a task descriptor at train time to improve the learning of recurrent policies. IMPORT learns an informed policy ( i.e. a policy receiving as input the description of the current task ) that is used to both construct task embeddings from the descriptors, and to regularize the training of the recurrent policy through parameters sharing and an auxiliary objective. This approach significantly reduces the learning sample complexity without altering the representational power of RNNs, by focusing on the relevant characteristics of the task, and by exploiting them efficiently.   The authors evaluate IMPORT against the main approaches to online adaptation on environments that require sophisticated exploration / exploration strategies, and show that it outperforms vanilla RM, Thompson sampling and task - inference approaches to meta - RL."
SP:bd89d254fbf31db61db237d08ab42981e27c52df,"This paper presents Offline learning with Adaptive Policy in sequential recommendation systems ( OapRS ), a method to learn an RL policy from offline data in the real - world sequential recommendation system ( SRS ). The main idea is to learn a policy from an offline dataset, e.g., a simulator from the dataset, and train optimal policies in the simulator. However, due to the stochasticity of the real world and the unavailability of online sampling, the distortion of the simulator is inevitable. The authors propose to handle the distortion issue via learning to adapt to diverse simulators generated by the offline dataset. The adaptive policy is suitable for environments where the dynamics are changing and the dynamics of the offline setting are not available online. Experiments are conducted in synthetic environments and a real world ride - hailing platform. The results show that the method overcomes the distortion problem and produces robust recommendations in the unseen real world."
SP:1a166b28cf684e0d5759bd629f6a53370d2bf11c,"This paper proposes a method for learning goal - reaching policies from scratch, without the need for expert demonstrations or a value function. The method is based on the premise that any trajectory that is a successful demonstration for reaching the final state in that same trajectory can be turned into an optimal one using hindsight relabelling, and that imitation of these trajectories (provably ) enables an agent to (iteratively ) learn goal-reaching behaviors. The main contribution of this paper is GCSL, a simple goal -reaching RL algorithm that uses supervised learning to acquire goals from scratch without value - based methods, while retaining the benefits of off - policy learning.   The authors propose a simple algorithm in which an agent continually relabels and imitates the trajectories it generates to progressively learn goal - reachable behaviors from scratch. Each iteration, the agent collects new trajectories using the latest policy, and maximizes the likelihood of the actions along those trajectories under the goal that was actually reached, so as to improve the policy. The authors formally show that this iterated iterated procedure is robust to the performance bounds of supervised learning, and empirically demonstrate improved performance and robustness over current RL algorithms in several benchmark tasks. In addition, the method can leverage demonstrations ( if available ) to accelerate learning."
SP:c306530164d677e670554eeba8203c66bb3d9f7a,"This paper proposes a new autoregressive speech synthesis model, FastSpeech, which is based on a combination of two existing models, namely, the FasterSpeech model and the teacher - student model. The main contributions of the paper are as follows :   1 ) The proposed model is faster to train and inference speed is faster than the previous autore Progressive Speech Synthesis ( FastSpech ) model.   2 ) The teacher model is more accurate than the student model in predicting duration, pitch, and energy, and 3 ) the proposed model uses more variation information of speech ( e.g. pitch, energy and duration ) as conditional inputs as opposed to the teacher model which uses duration prediction as an input. The paper also proposes to directly generate speech waveform from text instead of using the waveform as a conditional input in training and use predicted values in inference. The experimental results show that the proposed method achieves a 3x training speed - up over the previous model and 2x faster inference speed over the first model."
SP:79e9fb20d383816f54738ce70d137131ebc10290,"The paper proposes a new formulation of the unsupervised dimension reduction problem ( UDR ) in the language of tempered distributions, i.e. a problem of approximating an empirical probability density function p(x ) by another tempered distribution q(x) whose support is in a k - dimensional subspace. The problem is reduced to the minimization of the distance between q and pemp, D(q, pemp ) over a set of generalized functions. This infinite - dimensional formulation allows to establish a connection with another classical problem of data science — sufficient dimension reduction ( SDR ), which is an optimization problem over ordinary functions. The alternating scheme is proposed to solve both UDR and SDR. An algorithm for UDR induces an algorithm for the second problem and vice versa. In SDR, a non - negative penalty function R(f ) is introduced in order to reduce the penalty space of the penalty function of the SDR to that of the ordinary function in UDR. The method proposed in this paper is based on the idea of two - step iterative computation, which briefly described as a ) an adaptation to real data and to fake data sampled around a k-dimensional subspace found at a previous iteration, b ) calculation of a new k - dimension subspace in the reformulate reformulate regime.   The paper presents experiments on 4 examples of the UDR problem and 1 SDR problem. The experiments are conducted on synthetic data and standard datasets."
SP:93e54522e6c2b805905d21fc968fc40866f2898b,"This paper proposes a novel feature contrastive learning ( FCL ) approach to improve the robustness of large network classification models. FCL introduces two new concepts : contextual feature utility ( SVU ) and contextual feature sensitivity ( SNS ). SVU encourages the model to be more sensitive to features that have higher contextual utility. SNS trains the model on features that are more likely to be seen by the user.    The main contributions of the paper are :   1. Developing a novel approach to balance the trade - off between robustness and sensitivity for classification of large networks. The authors claim that traditional robustness - based methods such as CIFAR-10, Cifar-100, and ImageNet are sensitive to noise in the presence of certain patterns, while FCL is sensitive to patterns that are present in a large fraction of the data. This is particularly true for datasets with high number of outliers, such as CNNs. The paper empirically shows that FCL leads to a better generalization of CNNs trained on synthetic data compared to non - FCL methods, and also leads to better robustness on real data."
SP:f03c50f15022c4f56ac2b3085354ffed38ad1145,"This paper introduces Disentangling Generative Adversarial Imitation Learning ( DisentanGAIL ), a method to learn from high dimensional observations of an expert performing a task by making use of adversarial learning with a latent representation inside the discriminator network. This latent representation is regularized through mutual information constraints to incentivize learning only features that encode information about the completion levels of the task being demonstrated. This allows to obtain a shared feature space to successfully perform imitation while disregarding the differences between the expert ’s and the agent’s domains.   The main contributions of the paper are as follows :   1. A new method that bypasses the need for access to a full set of optimal states and actions taken by the expert by the agent. 2. A latent representation of the agent's state space that is shared between the agent and the expert's point of view. 3. An algorithm that is able to efficiently imitate in a diverse range of control problems including balancing, manipulation and locomotive tasks, while being robust to various domain differences in terms of both environment appearance and agent embodiment."
SP:ef18f4188426bc01be309633b486884b0e7a81a4,"This paper characterizes the performance of training a pruned neural network by analyzing the geometric structure of the objective function and the sample complexity to achieve zero generalization error. The convex region near a desirable model with guaranteed generalization enlarges as the neural network model is pruned, indicating the structural importance of a winning ticket. Moreover, the authors theoretically show that the number of samples required for achieving zero generalisation error is proportional to the number   of the non - pruned weights in the hidden layer.   The authors provide experimental results to justify the implications of their theoretical results, while experimental results are also provided to justify some of the implications in pruning multi - layer neural networks."
SP:eed6cb2f8caed39f8295f4aeb6e044c2ac981c4d,"This paper proposes AutoLabel to automatically learn the labels for augmented data, based on the distance between the clean distribution and augmented distribution. AutoLabel is built on label smoothing and is guided by the calibration - performance over a hold - out validation set. The authors show that AutoLabel can be easily applied to existing data augmentation methods, including AugMix, mixup, and adversarial training. Experiments on CIFAR-10, CifAR-100 and ImageNet show that autoLabel improves the calibration of models ( and accuracy, although less dramatically ) on both clean and corrupted data. In addition, AutoLabel also helps bridge the gap between accuracy and robustness."
SP:0d5017e1a405bf86e3bac40e6e59886d4bf48450,"This paper presents a theoretical analysis of self - supervised representation learning ( SRL ) using a causal framework based on Invariant Causal Mechanisms ( RELIC ). The authors propose a novel SRL method that enforces invariance of proxy classifier predictions across augmentations through an invariance regularizer. They show how data augmentations can be more effectively utilized through explicit invariance constraints on the proxy classifiers employed during pretraining. Based on this, they propose a new SRL objective, Representation Learning via Invariance - Causative Mechanism, which enforces an invariant prediction of proxy target classifiers across augmentation through invariant regularizer which yields improved generalization guarantees. Further, using causality, they generalize contrastive learning, a particular kind of SRL, and provide an alternative theoretical explanation for the success of these methods. In the experiments, the authors compare the proposed SRL with two other SRL methods, namely ImageNet and Contrastive Learning. The results show that RELIC outperforms ImageNet on ImageNet while also significantly outperforming the other methods on Atari.  "
SP:8f80a6f79f78c6421857f392c9a5e98061d7eb60,"This paper introduces a new method for learning informative visual representation in navigation. The method, dubbed Visual Transformer Network ( VTNet ), is based on the Transformer encoder - decoder ( Tensor - transformer ) and the spatial location encoder ( location - aware encoder ). The encoder maps object and region features with their location cues as spatial - aware descriptors and then incorporates all the encoded descriptors through attention operations to achieve informative representation for navigation. A pre - training scheme is also introduced to associate the visual representations with navigation signals, and thus facilitate navigation policy learning. Experiments in the artificial environment show that VTNet significantly outperforms state - of - the - art methods in unseen testing environments.   The main contributions of the paper are as follows :   1. An effective method to learn informative visual representations of the observed scene in determining navigation actions. 2. A novel location encoding scheme to learn the spatial locations of objects and image regions so that directional navigation signals can be learned. 3. A well - designed and well - motivated method for training the navigation policy."
SP:3e7cbe3dff592ef371e48dd86be7719fc5343f17,"This paper presents a federated learning ( FLEX ) framework based on the secure aggregation method. It is shown that the proposed method reduces the communication / computational resources by a factor of n/log n relative to the existing secure aggregation solution without sacrificing data privacy, where n is the number of clients. The key idea behind the suggested suggested scheme is to design the topology of the secret - sharing nodes ( denoted by the assignment graph G ). The authors provide theoretical guarantees on the reliability / privacy of the proposed scheme and extensive real - world experiments. They demonstrate that their scheme, using only 50% of the resources required in the conventional scheme, maintains virtually the same levels of reliability and data privacy as the conventional method.   The authors first obtain a sufficient condition on G to guarantee reliable and private federated federatedlearning and then suggest using the Erdős - Rényi graph as G to the guarantee the reliability and privacy. Then, they demonstrate the reliability of their proposed method and show that it is robust against semi - honest and semi - malicious threats."
SP:00fae41e0eca0a1575cd7b2dcfabf0dc5c9c8b8a,"This paper presents an extension of a recent research direction in building neural network architectures to find optimal auctions by introducing a time - independent Lagrangian for the optimization procedure in Duetting et al. ( 2019 ). The authors propose two conceptual deviations from their approach which result in enhanced performance. First, the authors use recent results in theoretical auction design to introduce a time independent lagrangian which circumvents the need for an expensive hyper - parameter search ( as in prior work ), and also provides a single metric to compare the performance of two auctions ( absent from prior work in the paper ). Second, the optimization process in previous work uses an inner maximization loop to compute optimal misreports and the authors propose an additional neural network to circumvent this process through the introduction of an additional network.   The authors further imply a novel formulation of Auction Design as a two - player game with utility functions. They proposed architectures that are specific to single - bidder settings for finding auctions that satisfy incentive compatibility but are not necessarily competitive with each other. They demonstrate the effectiveness of their approach by learning competitive or strictly biased auctions and show that learning competitive auctions leads to improved performance."
SP:a0e8061beb5e9a6c631419861559d22b8d645cb4,"This paper proposes Bi - Tuning, a general learning approach to finetune both supervised and unsupervised pre - trained representations to downstream tasks. The proposed approach is based on two heads : a classifier head with an improved contrastive cross -entropy loss to better leverage label information in an instancecontrast way, and a projector head with a newly designed categorical contrastive learning loss to fully exploit the intrinsic structure of data in a category - consistent way. The authors conduct extensive experiments to validate the effectiveness of the proposed two - heads fine - tuning architecture with their novel loss functions. They show that bi - tuning achieves state - of - the - art results for fine -tuning tasks on CUB in low - data regime."
SP:87e5b552c13d73bd85249062a152c6c140e594a9,"This paper analyzes the problem of adversarial accuracy and adversarial training. The authors argue that standard adversarial tracking does not properly measure the robustness of classifiers because it has a tradeoff with standard accuracy even when it neglects generalization. They introduce a new measure for adversarial robustness called “ genuine adversarial accurate ”, which measures the adversarially perturbed samples without trading off accuracy on clean data and accuracy on perturbed data. They prove that a single nearest neighbor ( 1 - N ) classifier is the most robust classifier according to the new measure. They also show that using a norm - based distance metric when the class for each data point is within a certain distance from the ground truth classifier leads to more robust classifiers.    The main contributions of the paper are as follows :   1 ) The authors introduce a novel metric for measuring adversarial classifiers ’ robustness. The metric is based on the fact that standard training does not use invariance - based adversarial examples ( samples whose predicted classes are unchanged even if the perceptual classes are changed ). They show that the new metric does not favor a model with such examples. 2 ) They provide a new definition of the classifier ’s “ nearest neighbor ” and show that it is more robust than the one nearest classifier. 3 ) They use the metric to show that a classifier with a “ unique ” classifier ( i.e. classifier whose nearest neighbor is within 1 n’s percentage of a class but is independent of each other classifier in terms of the number of samples used to train the classifiers for the test set."
SP:2fda410b9281c5e253d385bc4382ec168bc161f3,"This paper proposes a new algorithm, FairAdj, to learn a fair adjacency matrix with proper graph structural constraints for fair link prediction. Specifically, the authors focus on dyadic fairness, which articulates a fairness concept that a predictive relationship between two instances should be independent of the sensitive attributes. Based on this, they theoretically relate the graph connections to link scores in neural network learning graph networks and reveal that regulating weights on existing edges in a graph contributes to Dyadic fairness conditionally. Subsequently, they propose their algorithm, which empirically learns a fair admissible matrix to empirically learn adjacencies. The authors conduct evaluations towards seven measurements of both utility and fairness - utility to demonstrate the effectiveness of the proposed method. Empirical validation demonstrates that fairAdj algorithm delivers effective predictions. Comparisons to other baseline methods ( Kipf & Welling, 2016b ; Grover & Leskovec, 2016 ; Rahman et al. 2019 ; Bose & Hamilton, 2019b ) consistently observe improvements from two aspects : 1 ) fairness metrics verify that our method can minimize the statistical gap between the predictions of intra and inter links, and 2 ) fair - utility enjoys a more favorable fairness - tradeoff ( same in fairness but less sacrifice in utility, and vice versa ) when compared to previous works."
SP:b614e9fbec58e9efa7722d2ec4a60fc93d210f92,"This paper proposes Disentangled Exploration Autoencoder ( DEAE ), a new method for generating synthetic data for autoencoders. DEAE is based on disentangled representation and regularization to guarantee validity of exploration in latent space and achieve controllable synthesis. The encoder of DEAE first turns the input sample into latent code, then explores the latent space through directed interpolation. To aid the interpolated code in successfully outputting a meaningful sample, after the interpolation, the encoder first regularizes the output by “ regularizing ” the latent code by disentangling it from the original representation to maintain perfect disentanglement. Experiments demonstrate that DEAE can improve performance of downstream tasks by synthesizing attribute - controlled augmented samples. Also, it is shown that it can eliminate dataset bias, which provides a solution for fairness problems never seen in training."
SP:c934adb14926a00ef9c73c9773cb0b3a2669921e,"This paper proposes a novel memory allocation scheme that aims to bridge the gap between episodic and semantic memory via a hierarchical latent variable model. Inspired by the traditional ( computer - science ) memory model of heap allocation, the authors propose a differentiable block allocated latent memory that learns to compress an episode of samples, referred to by the set of pointers {p1,..., p4 } in Figure 1, into a latent multi - dimensional memory ( K++ ). The K++ model infers a key distribution as a proxy to the pointers ( Marlow et al. 2008 ) and is able to embed similar samples to an overlapping latent representation space, thus enabling it to be more efficient on compressing input distributions. In this work, the memory is used to improve the memory model in the Kanerva Machine. The memory writing process is treated as a deterministic process, where the stochasticity of the read key distribution is leveraged to disperse information within the memory. The authors demonstrate that this allocation scheme improves performance in memory conditional image generation, resulting in new state - of - the - art conditional values on MNIST."
SP:e63d7d8c581019e17585fb9c0eac33d6836e187d,"This paper studies the sample complexity and loss landscape of attention - based neural networks by analyzing the local minimums of attention mechanisms. Theoretical analysis is based on two main components : ( 1 ) the loss landscape analysis of the attention mechanisms, and ( 2 ) the attention mechanism analysis. The attention mechanisms analysis focuses on the stationary points of the network. The authors show that under mild assumptions, every stationary point of an attention model achieves a low generalization error. They also show that attention models require lower sample complexity than models without attention. Experiments on various datasets validate their theoretical findings."
SP:f739d199fdee26f09994e3f9487aec1eab0f2e89,"This paper proposes a new perspective on active inference based on reinforcement learning ( RL ) and the expected free energy ( EFE ), which is a core quantity in active inference, and claims that EFE can be treated as a negative value function. Motivated by the concept of prior preference and a theoretical connection, the authors propose a novel inverse RL algorithm for designing EFE based rewards, by learning a prior preference from expert demonstrations, which can effectively handle the difference between local and global preferences. It will extend the scope of active inference to inverse RL problem. Experiments in Section 6 show that the optimal distribution over the first - step action induced from active inference can be interpreted using Q - learning, and it is shown that the proposed algorithm can be used to design EFE - based rewards in an inverse RL setting. This illustrates that the problem with inverse RL can be approached with a different perspective of active infra - inference."
SP:5592b79e49eba95c15103a3348f2bde57b60f2ab,"This paper proposes a data augmentation method to improve generalization in both adversarial and standard learning by using out - of - distribution ( OOD ) data that are devoid of the abovementioned issues. Theoretical analyses and experimental results on CIFAR-10, CifAR-100, and a subset of ImageNet suggest that OAT can help reduce the generalization gap in adversarial training and standard training. By applying OAT using various OOD datasets, it is shown that undesirable features are shared among diverse image datasets. It is also demonstrated that OOD can effectively extend training distribution by comparison with other data augmentations methods that can be employed in the absence of OOD data. The state - of the - art adversarial learning method using UID data is found to further improve by incorporating the proposed method of leveraging OODData. Furthermore, the authors demonstrate that proposed method can further improve the existing state-of - the-art adversarial trained method."
SP:3cac7a2c310165ed0de46d8e5546c3bfbd639158,"This paper introduces Fast Linearized Adaptive Policy ( FLAP ), a new meta - RL method that is able to extrapolate well to out - of - distribution tasks without the need to reuse data from training, and adapt almost instantaneously with the need of only a few samples during testing. FLAP builds upon the idea of learning a shared linear representation of the policy so that when adapting to a new task, it suffices to predict a set of linear weights. A separate adapter network is trained simultaneously with the policy such that during adaptation, we can directly use the adapter network to predict these linear weights instead of updating a meta - policy via gradient descent, such as in prior meta -RL methods like MAML, to obtain the new policy. The application of the separate feed - forward network not only speeds up the adaptation run - time significantly, but also generalizes extremely well to very different tasks that prior MetaRL methods would fail to generalize to."
SP:21a1bd4ada0723c96c0dbf7a142a2faf5defa4e3,"This paper proposes a federated version of the kernel k - means algorithm, where the goal is to find a distribution k such that the clustering quality of the federated kernels approaches that of the state - of - the - art. The main idea is to use distributed stochastic proximal gradient descent ( DSPGD ) to approximate the solution to the optimization problem of kernelk - means under federated settings. To tackle the second challenge, a communication efficient mech anism ( CEM ) is designed to reduce the communication cost. Experimental results show that the proposed algorithm achieves the best clustering with the reduced communication cost in most cases.    The main contribution of this paper is to develop a distributed algorithm that approximates the approximate solution of the problem of federated kernel k means. The authors provide theoretical analysis that shows that the convergence rate of the proposed method with CEM is O(1 / T ), where O(T ) is the iteration number of iterations. This is in contrast to the linearity of the communication rate of DSPGardian with the traditional CEM, which is linear in the dimension of the number of users."
SP:be568dd3fea51ce33a6d1e4b07dda5aee6342395,"The paper proposes CompOFA, a method to train several CNNs at once with a constant training cost that aims to reduce the search space and hence the training budget. To do so, it constrains search to models close to the accuracy - latency Pareto frontier. The paper proposes to incorporate insights of compound relationships between model dimensions to build CompO FA, a design space smaller by several orders of magnitude than that of Once - For - All ( OFA ). Experiments on ImageNet demonstrate that even with simple heuristics we can achieve a 2x reduction in training time and 2x speedup in model search /extraction time compared to the state of the art without the loss of PAREto optimality."
SP:04b84d26cf282dbb753cbf27f14c334f65d3f8ec,"This paper presents a meta - learning algorithm, ADversarial Meta - Learner ( ADML ), which leverages clean and adversarial samples to optimize the initialization of a learning model in an adversarial manner. The paper presents extensive experimental results on two image datasets, MiniImageNet and CIFAR100, showing that ADML outperforms several representative meta -learning algorithms in the cases involving adversarial data generated by different attack mechanisms.   The main contributions of the paper are as follows :   1. This paper proposes a new method, ADML ( adversarial meta - learner ), by combining MAML ( Finn et al. ( 2017 ) ) with adversarial training (e.g., Goodfellow et al ( 2015 ) ). However, the experimental results show that such an approach does not work well by their experimental results. 2. The authors propose a novel method, which utilizes antagonistic correlations between clean and adversary samples to let the inner gradient update arm - wrestle with the meta - update to obtain a good and robust initialization of model parameters. 3 ) This paper sheds light on tackling the cases with limited and even contaminated samples, which are common in real life. The experimental results justify the effectiveness and superiority of ADML in terms of both accuracy and robustness."
SP:dfbaa6b53c4e8328d52666ad4641fc917bf0c0b3,"This paper presents a data - driven framework for permutation selection for the decoding of the Bose - Chaudhuri - Hocquenghem ( BCH ) code. The key idea is to embed all the permutations of a code in a word - independent manner, by extracting relevant features from the embedding features of the code. This is done once before the test phase during a preprocess phase, where a trained NN accepts a corrupted word and the embedded permutations and predicts the probability for successful decoding for each permutation. Then, a set of either one, five or ten most - probable - to - decode permutations are chosen, and decoding is carried out on the permuted channel words rather than decoding an arbitrary dataset with all permutations, and empirically choosing the best subset of them. The method is evaluated on the renowned BCH code. To the best of my knowledge, this work is the first to leverage the benefits of self -attention networks in physical layer communication systems.   The main contribution of this paper is to combine domain knowledge with machine learning concepts such as node embedding and self - attention. It is shown that the bit error rate of the proposed method improves for the simulated BCH code as compared to the baseline decoders."
SP:c860a7b0952d708e7851c9bc4b63d246f64d1cba,"This paper proposes an approach to deal with the problem of fine - tuning BERT for a target text classification task with limited labeled examples. The authors propose to perform an unsupervised clustering task prior to finetuning on the target task and train BERT with or without MLM pretraining over the actual labeled data. The proposed approach is inspired by the use of clustering to obtain labels for training deep networks in computer vision ( Kolesnikov et al., 2019 ). The clustering is based on simple Bag Of Words ( BOW ) representations, to partition the training data into relatively homogeneous clusters of text. Next, the authors treat these clusters as labeled data for an intermediate text classified task, and train   BERT on predicting the cluster labels.   Experimental results demonstrate the practical value of this strategy on a variety of benchmark data, most prominently when the training sets are relatively small and the classification task is of topical nature. The results also analyze the results to gain insights as to when this approach would be most valuable and propose future directions to expand the present work."
SP:ea37f5882fd98dd4ce233077bb3069517d4ed4ea,"This paper studies the generative model - based reinforcement learning ( MBRL ) by comparing popular generative models using a fixed ( random - shooting ) control agent. The control agent is a random - armed predictive control agent, which is trained using an aggressive training schedule. The authors find that the deterministic models consistently outperform their probabilistic counterparts. They also found that heteroscedasticity at training time, perhaps acting as a regularizer, improves predictions at longer horizons. When multimodal multimodality is not required, they find that deterministic and mixture density nets outperform all other models by a large margin. The paper also proposes metrics and an experimental protocol to evaluate the performance of the models."
SP:4e25ba3714d78ba59a0d8efbb65e0ef5201702f8,"This paper proposes Affine Disentangled GAN ( ADIS - GAN ), which is a Generative Adversarial Network that can explicitly disentangle affine transformations in a self - supervised and rigorous manner. The objective is inspired by InfoGAN, where an additional affine regularizer acts as the inductive bias. The affine transformation is derived by decomposing the affine matrix into separate transformation matrices and inferring the transformation parameters by maximum likelihood estimation. Unlike the disentangled representations learned by existing approaches, the features learned by ADIS + GAN are axis - aligned and scalable. The scalability property makes it possible to make a trade - off between the compactness and expressivity of the learned representation."
SP:121f8420cfb49c6d80b5ebb4051e85947182594a,"This paper proposes CLSA, a contrastive learning method that leverages strongly augmented queries to improve the performance of unsupervised learning methods such as contrastive contrastive image classification and contrastive object detection. The main idea of the method is to learn from the distribution of augmented images over the representation bank rather than directly using the stronger augmentations. The paper shows that CLSA outperforms the supervised and self - supervised methods on both the transfer learning and object detection tasks on ImageNet, COCO, and VOCO. However, the paper also shows that the performance deteriorates when the augmented images are used for discrimination - based learning as the proposed method does not have the same generalizability as the supervised methods. This is argued to be because the distortions induced from the stronger augmented image structures can ridiculously change the image structures and thus the transformed images can not be viewed as the same as the original ones any more. To mitigate this problem, the authors propose to minimize the distribution divergence between the augmented and the distribution distributions of the representation over the representations. This avoids an overoptimistic assumption that could overfit the augmented queries containing distorted visual structures into the distorted images, while still being able to distinguish them from the negative samples by leveraging the distributions of weakly augmented counterparts. The proposed method achieves a top - 1 accuracy of 76.2 % for ImageNet with a standard ResNet-50 architecture with a single - layer classifier fine - tuned, which is almost as high as 76.5 % for a fully supervised ResNet - 50 backbone. It also achieves the competitive performances on several downstream tasks, including object detection and transfer learning."
SP:af54e542223097c315ecd677d0b968e9a0b2a1d4,"This paper proposes a method for the de - identification of magnetic resonance imagery ( MRI ) from a patient ’s MRI scan. The authors claim that existing methods for this task are either unable to effectively hide the identity of the patient or remove so much information that they adversely affect further analyses. To tackle this problem, they propose a conditional, multi - scale, 3D GAN architecture that takes a patient’sMRI scan as input and generates a 3D volume in which the brain is not modified but the face has been de - identified. Compared to the classical removal - based techniques, their deep learning framework preserves privacy more reliably without adversely affecting downstream medical analyses on the brain, including segmentation and age prediction.    The main contribution of this paper is to propose a new class of MRI de - identification techniques that remodel privacy - sensitive facial features as opposed to removing them. To accomplish this, the authors propose to use a conditional and non - adversarial approach to generate the 3D MRI scans. They claim that this approach is more effective than the traditional methods for de - identifying MRI scans because it does not require the patient to reveal much information about their identity."
SP:0ac3964bd2320341488476d60f57b75d2a79f92c,"This paper proposes a new graph pooling method based on multi - head attention based global pooling ( MHA ), which is a multi - headed attention based pooling approach for graph classification and link prediction tasks. The main idea of the proposed method is to first formulate the node pooling problem as a multiset encoding problem with auxiliary information about the graph structure, and then propose a Graph Multiset Transformer ( GMT ) method to solve this problem. The proposed method, based on MHA, is evaluated on graph classification benchmarks with high memory and time efficiency, and obtains even larger performance gain on graph reconstruction and generation tasks.    The main contributions of the paper are as follows :   1. The authors propose a new node representation pooling function that maps a set of node representations into a compact form based on a simple sum or average over all node representations, and consider all node features equally without consideration of their task relevance, and any structural dependencies among them. 2. They show that MHA may yield the same representation for two graphs that are distinguished by the Weisfeiler - Lehman graph isomorphism test, as they suboptimally preserve information from the node features. 3. They also show that their method can be easily extended to the previous node clustering approaches for hierarchical graph poolsing. 4. They provide experimental results on synthetic and real - world graphs, on which it largely outperforms MHA and other baselines."
SP:76848e7ac3e6709e92f6a6db60269cb5177495d1,"This paper proposes a new explanation for the long - range prediction error of graph neural networks ( GNNs ). The authors claim that the problem is caused by a bottleneck that prevents the GNN from aggregating messages across a long path. This bottleneck causes the over - squashing of exponentially growing information into fixed - size vectors, which in turn prevents the network from propagating messages originating from distant nodes and performing well when the prediction task depends on long - distance interaction. The paper empirically shows that popular GAT and GGNN are susceptible to this bottleneck, while popular GCN and GGIN are more resistant to it than GAT or GGNN. They also show that breaking the bottleneck using a simple fully adjacent layer reduces the error rate by 42 % in the QM9 dataset, by 12 % in ENZYMES, by 4.8 % in NCI1, and improves accuracy in VARMISUSE, without any additional tuning."
SP:90d8fa381446923902e42b259392e5e975e6caa1,"This paper proposes a method to adapt a cross - domain sentiment analysis method to a more generalizable classifier that is trained on both source and target data. The proposed method is based on learning a prototypical distribution for the source domain in a cross domain embedding space which is trained to be domain - agnostic. The source and the target data are obtained via minimizing the Sliced Wasserstein Distance ( SWD ) ( Lee et al., 2019 ). Theoretical and empirical analysis are provided to demonstrate that the method is effective in reducing the domain gap between the trained classifier and the one trained in the target domain. The main contribution is to develop a new domain adaptation algorithm for model adaptation by introducing large margins between different classes in the source distribution based on the notion of prototypical distributions in the embedding. The authors provide theoretical proof that the proposed method minimizes an upperbound for the expected error of the target classifier in the proposed domain adaptation method. Experimental results demonstrate that our algorithm outperforms state - of - the - art sentiment analysis algorithms."
SP:893fd7440b82f5da0d4c0944928810322eaee2f0,"This paper presents an empirical study of gender bias in NLI models trained on MNLI and SNLI data - sets. Three models ( BerBERT, RoBERTa, BART ) are used to train the models. They are trained on the MNLI dataset and the SNLI dataset with an additional dataset augmented with gender - balanced data. They compare the performance of the three models ( BERT, BART and BART - VAE ) on the task of pairing gender - neutral and gender - specific hypotheses. They find that the BART model is more biased than BERT and VAE, while the BERT model is less biased than BART. They also find that augmenting the training data with data from a gender - biased dataset can help reduce the bias of the tested models."
SP:a32ab755bd249c393b70938036ce8e810c0c439f,"This paper studies variational intrinsic control ( VIC ), an unsupervised reinforcement learning method for finding the largest set of intrinsic options available to an agent. In this paper, two variants of implicit VIC algorithms are considered : one that represents the options explicitly, and the other that does it implicitly. The authors show that the implicit reward used in the latter is subject to bias in stochastic environments, causing convergence to suboptimal solutions. To counter this bias, they propose two methods respectively based on the transitional probability model and Gaussian mixture model. They substantiate their claims through rigorous mathematical derivations and experimental analyses.   The main contribution of this paper is the following :   1. An analysis of the intrinsic reward bias of the implicit implicit reward in the VIC algorithm proposed in Gregor et al. ( 2016 ). They show that it is biased towards the case where the transition probabilities are higher than those where the options are lower. This bias is caused by the nature of the environment dynamics modeling in the implicit algorithm. They propose two modifications to this implicit reward to compensate for this bias. The first one is to use the transition probability model instead of the conditional probability model. The second one is a mixture model based on Gaussian probability. The experimental results show that this approach works better than the original implicit reward method."
SP:b4df2c4627a6d46c5100133e38c4bea20b296dd8,"This paper studies the problem of neural ensembling with a few labeled examples per class in the low - data regime of image classification. The authors propose to use an ensemble of relatively small deep neural networks to learn from a small set of labeled examples to improve the sample efficiency in this regime. The proposed method is based on a simple yet effective technique, which they call ensembled neural networks ( ENsembling for Classification ). They validate the effectiveness of their method on a number of datasets, including CIFAR-10, MNIST, and Fashion - MNIST. They show that their method outperforms the state - of - the - art approaches for learning from small datasets. They also provide empirical evidence of their advantage when compared to their deeper and wider competitors.   The main contributions of this paper are the following : ( a ) they study the use of ensembles in the small domain and show that they improve the state of the art in the art ; ( b ) they compare their method with that of their deep network competitors on a fixed computational budget and show their performance is better ; and ( c ) they provide a way of choosing the right ensemble configuration depending on the situation."
SP:4a0ee01f4897efa81659f37ef0468ee8195bbc4f,"This paper proposes Sparse Binary Neural Networks ( SBNN ), a method to introduce sparsity and quantization in quantized neural networks ( BNNs ). The main contribution of this paper is to propose a training scheme and a training algorithm to achieve sparsity in BNN with positive 0/1 binary weights instead of the - 1 / 1 weights used in state - of - the - art DNNs. The proposed method is validated on MNIST and CIFAR-10 datasets to show that SBNNs can achieve high compression rates and good generalization, while further reducing the operations of BNN.   The main contributions of the paper are as follows :   1. Introducing sparse BNN training and training algorithm. 2. Demonstrating that the proposed method, SBNN, can achieve a high compression factor and reduces the number of operations and parameters at time of inference. 3. Empirical results on linear and convolutional networks to show the effectiveness of SBNN. 4. Ablation results on Cifar-10 and MNIST datasets to verify the compression properties of the method."
SP:5be8539ad02595ad3c7a2d7afe8cbb3e9924467d,"This paper proposes a method for post hoc calibration of predictive uncertainty for out - of - distribution ( OOD ) machine learning models on corrupted data. The method is based on the Brier score ( Brier et al., 1983 ), which measures the difference between the predicted error rate and the actual error rate, as measured by collecting data into bins based on pmax = maxi p softmax i. The baseline for the proposed method for predictive uncertainty is simply use the softmax probabilities of the model, p.r.(f = softmax(f(f ) ) ), as a surrogate for class membership probabilities ( Hendrycks & Gimpel, 2017 ). The paper shows that this method significantly improves on benchmark results. The proposed method uses outlier exposure to properly calibrate the model probabilities to estimate the probability that a model prediction is correct.   The main contribution of this paper is to propose a method that combines endowing model predictions with endowing estimates of the probability of class membership with the model predictions. The main contributions of the paper are as follows :   1. Estimate the uncertainty between the model output and the true class membership probability. This is done by using the temperature scaling method ( Goyal et al, 2018 ). 2. Measure the error between the expected calibration error and the calibration error ( ECE ) of the baseline method. 3. Contain the calibration errors for temperature scaling, dropout, and VBI ( Blundell, 2015 ) as well as calibration error for calibration."
SP:ea503f67e38fce7dee9cc4996b55b8959911f030,"This paper studies the expressive power of graph neural networks and graph kernels from an empirical perspective. Specifically, it compares the graph representations and similarities produced by these algorithms against those generated by an intractable graph similarity function. The authors also investigate the impact of node attributes on the performance of the different models and kernels. The results reveal interesting findings. For instance, theoretically more powerful models do not necessarily yield higher - quality representations, while graph kernels are shown to be very competitive with neural networks."
SP:0cf7b7d929f50e0b7f4fda5e1f68e5ade2f7c29b,"This paper proposes a new image generation method for self - supervised image animation based on the First Order Motion Model ( FOMM ) and the VoxCeleb ( Nagrani et al., 2017 ) dataset. The proposed method is based on two main components : the occlusion mask and the background mask. The background mask is a data augmentation that cuts and mixes patches of different images, to regularize discriminator predictions on inpainting. The foreground image is augmented with the top - k percent occluded pixels of the foreground for consistency regularization. The experimental results show that the proposed method outperforms state - of - the - art image animation approaches in pixel - wise difference, low - level similarity, and keypoint distance, and feature embedding distance."
SP:60b535fc6cbc1a7a26ad53f706ebb17de346dc4f,"This paper proposes a method for learning disentangled representations of independent causal mechanisms ( ICM ) for data generation. ICM is a special case of disentanglement - based generative models that directly model multiple data generation processes ( mechanisms ) in a coarse granularity. The authors propose to learn a model that disentangles each mechanism and approximates the groundtruth mechanisms from observational data. They outline sufficient conditions under which the mechanisms can be learned using a self - supervised generative model with an unconventional mixture prior, simplifying previous methods. Moreover, they prove the identifiability of their model w.r.t the mechanisms in the self - supervised scenario. They compare their approach to disentangling representations on various downstream tasks, showing that their approach is more robust to intervention."
SP:44d4e24428d043a69b40013919cda0e8e7bff99c,"This paper presents a method for training a machine learning model for predicting molecular graph structure ( W ) given a 2D image of a chemical compound ( U ). The main idea is to learn f : U → W where we have a fully mediating representation V such that f factors into U → V → W. However, observing V requires detailed and expensive labels which are not available in the source domain. In this paper, the authors propose a graph aligning approach that generates rich or detailed labels given normal labels W. The proposed method is tested on the Maybridge compound data set and compared with a pretrained model on the source and target domains. The authors claim that the proposed method enables data efficient domain adaptation and reaches state - of - the - art performance on the data set."
SP:ad906dd9a176cffd283593321ff6b9ad19595528,"This paper proposes a method to solve the energy optimization problem in the setting of chiller power systems. The authors propose a neural network ( MNN ) based on the structure and loss design of deep learning to build a nonlinear model with lower redundancy function space. Specifically, the energy consumption estimation of most chillers can be physically viewed as an input - output monotonic problem. Thus, the authors propose to design a Neural Network ( NNN ) that mimics the physical behavior of the system to maximize the energy output.   The authors claim that using MNN for system identification can help the subsequent optimization step and improve 1.5% the performance of optimization compared to the state - of - the - art methods. They verify the proposed method in a cooling system of a data center, experimental results show the superiority of the framework in energy optimization."
SP:6cb65ee5d2926858570601eeeade24fe86c7f32f,"This paper proposes a multi - head attention transformer, named CausalTrans, for multi - horizon, multi - task and multi - target forecasting. The main idea of the Transformer is to replace the softmax in multi - headed attention with the Conditional Average Treatment Effect ( CATE ) in causal inference. The authors integrate the causal attention with CATE and propose a novel and fast multi - heads attention evolved from Taylor's expansion from Taylor ’s expansion. They further design a spatial graph fusion mechanism to significantly reduce the parameters ’ scale ’. They conduct a wide range of experiments to demonstrate the interpretability of causal attention and the effectiveness of various components of their Transformer. They show that their proposed Transformer can achieve up to 15% error reduction compared with various baseline methods."
SP:223980a1954d626d90ff54d8dc61b5d85a6b349c,"This paper proposes a multi - agent unsupervised learning framework, coupled mixture VAE ( cpl - mixVAE ), for the problem of identifying discrete and continuous factors of variability that can help unravel complex phenomena. The framework is composed of multiple interacting autoencoding agents, where individual agents operate on augmented copies of training samples to learn mixture representations, while being encouraged to reach consensus on the categorical assignments. The authors provide theoretical justification to motivate the use of such a framework, and formulate it as a variational inference problem. They benchmark their approach on MNIST and dSprites, achieving state - of - the - art categorical assignment while preserving interpretability of the continuous factors. They apply the method to a challenging single cell gene expression dataset for the population of neurons, and demonstrate that their method can be used to discover discrete categories referred to as neuronal types and type - specific genes regulating continuous within - type variability."
SP:c982610ad28662c3bd13132abe1f7307d1a61b68,"This paper studies the problem of equivariant convolutional networks ( GCNNs ) with additional symmetry priors, which can lead to a considerably improved performance compared to classical CNNs. Recent theoretical advances have shown that such models can be understood as performing convolutions with G - steerable kernels, that is, kernels that satisfy an equivariance constraint themselves. This paper attempts to derive such a characterization for practically relevant case of G being any compact group. To do so, it generalizes the famous Wigner - Eckart theorem for spherical tensor operators for the case of SO(3 ), considering both real and complex representations. Thereby, it demonstrates that the endomorphism bases, Clebsch - Gordan coefficients, and harmonic basis functions can usually be determined for theoretically relevant symmetry groups.   The main contributions of this paper are as follows : ( 1 ) A general characterization of the G - steerable kernel spaces, which is needed for the specific use cases discussed in [ 1 ] ; ( 2 ) A set of generalizations to the matrix of reduced matrix elements, and ( 3 ) A series of experiments to verify the correctness of the proposed method."
SP:7b2ea39069277ad0f4f79476a77ef84587a804d9,"This paper analyzes the effect of selective classification on accuracy in settings where the errors are costly but abstentions are manageable. Specifically, the authors consider the following settings :   1. A population of people living in a rural area.   2. A set of homogenous groups living in the same city. 3. A distribution of the population over the city, where each group is represented by a single classifier ( i.e., the group that is predicted to perform the best by the classifier ). 4. A mixture of the classifiers from each group. 5. The population as a whole. The authors find that selective classification can improve average accuracy, but can also magnify existing accuracy disparities between groups. This is particularly true in the presence of spurious correlations between the groups. They study this behavior consistently across five vision and NLP datasets. Surprisingly increasing abstention can even decrease accuracies on some groups. To better understand this phenomenon, they study the margin distribution, which captures the model ’s confidences over all predictions. For symmetric margin distributions, they prove that whether selective classification monotonically improves or worsens accuracy is fully determined by the accuracy at full coverage. They also study whether the distribution satisfies a property they call the left - log - sympathing property, which they call “ left-log - favorably ”."
SP:f1d57ee27e901daf7e4e2b84139019e945818911,"This paper proposes a hierarchical nonnegative CANDECOMP / PARAFAC decomposition ( NCPD ) method for topic modeling on large - scale data with complex multi - modal structure. The main contribution of this paper is the proposed hierarchical NCPD method, which is based on neural network architecture and backpropagation to mitigate error propagation. The authors also present related work on tensor decompositions and training methods. In Section 3, they test Neural NCPD on real and synthetic data, and offer some brief conclusions in Section 4.   The main contributions of the paper are as follows :   1. Hierarchical NCPD is a method to perform hierarchical topic modeling using neural networks. The proposed method uses neural networks to perform a hierarchical decomposition of the data. This decomposition is then used to train a neural network to perform topic modeling. The neural network is trained using a neural net architecture, where each layer is connected to a sub - network, and the sub - networks are connected via a back - propagation network. The sub - nets are connected by a back propagation network to each other, and each back - net is updated with a new sub - net every time the topic is updated. The training data is fed into the neural network and the topic model is updated using the new neural net. The results of the experiments on synthetic data and real data show that the proposed method outperforms the other methods."
SP:b6ddc3a560aa7155e7e927bf5360bedc36586597,"This paper proposes a new adversarial robustness certificate based on Graph Neural Networks ( GNN ). GNN is a classifier that simultaneously outputs multiple predictions based on a single input, i.e. a graph, image or document. The authors argue that existing robustness certificates consider each prediction independently and are overly pessimistic for such tasks. They assume that an adversary can use different perturbed inputs to attack different predictions, ignoring the fact that we have a single shared input.   The authors propose to fuse multiple single - node certificates into a drastically stronger collective certificate, which computes the number of predictions simultaneously guaranteed to remain stable under perturbation. They leverage their locality property to leverage the predictions in a close neighborhood. For example, on the Citeseer dataset, on node classification, the average number of certifiable feature is 7.7 ( compared to 351 for node classification in the original paper ). They also propose a multi - object detection scheme, where they still treat each detected object independently but they still leverage the detection property to fuse it into a single certificate."
SP:cc93dd2f68e415e2457166e78627865dc1b44697,"This paper proposes Quantile Regression GAN ( QRGAN ), a method for training generative adversarial networks ( GANs ) that is based on quantile regression. The main idea is to learn high - dimensional probability distributions by competitively training the generator and discriminator by minimizing the loss functions of the loss function of the discriminator. The authors compare QRGAN with two variants of GAN, namely, Least Squares GAN and Wasserstein GAN. LSGAN suffers from non - convergence problem and cause mode collapse while WGANs encounter with inefficient computation and slow training due to its constraints in the distance approximation. QRGAN is proposed to address both of these issues. Experiments on the mixture of gussian dataset and image generation experiments show that QRGAN outperforms WGAN and LSGAN in terms of the mode collapse and mode explosion problems."
SP:4ddb47ee77c374ae6c3e419412d92ca77260692e,"This paper investigates relevance metrics that can provide reasonable explanations to users. Specifically, three tests are adopted to evaluate whether the relevance metrics satisfy the minimal requirements for similarity - based explanation. The first test is the model randomization test originally proposed by Adebayo et al. ( 2018 ) for evaluating saliency - based methods. The other two tests are identical class test and identical subclass test, which are newly designed in this study. The experiments revealed that some relevance metrics demonstrated poor performances on identical class and identical class tests, indicating that their use should be deprecated for similarity explanation. Also, the authors analyzed the reasons behind the success and failure of metrics and expect these insights to help practitioners in selecting appropriate relevance metrics."
SP:6c2cbf2bc0f6dabe974e80ec1e82d2d12189906e,"This paper proposes a low - rank global attention ( LRGA ) module to Graph Neural Networks ( GNNs ) for improving their generalization power. LRGA is a computation and memory efficient variant of the dot - product attention ( DPA ) module. The paper theoretically analyzes the generalization properties of DPA and LRGA and focuses on a specific family of expressive graph neural networks. It shows that augmenting DPA with LRGA provides algorithmic alignment to a powerful graph isomorphism test, namely the 2 - WFLehman ( 2 - FWL ) algorithm. Experiments are performed on several benchmark datasets and show that LRGA improves state - of - the - art performance in most datasets, often with a significant margin. Ablation study in the random features framework is also performed to support the theoretical propositions."
SP:b4abdd28504b4c1de239eabd4e0e27d370efee71,"This paper proposes a new method to improve the calibration performance of Convolutional Neural Networks ( CNNs ). The main contribution is that the loss functions typically used to train CNNs do not penalize inability to localize an object, nor do they take into account an object’s relative size in the given image. During training on ImageNet - 1K almost all approaches use random crops on the images and this transformation provides the CNN with background only samples. This causes the classifiers to depend on context, which is harmful for safety - critical applications. To address this problem, the authors propose a method that combines the ideas of objectness and label smoothing during training to improve CNNs'calibration performance. The proposed method is based on the idea of relative object size within an image. This is done by computing a smoothing factor that is adaptive based on object size and the relative object being classified instead of relying on the context being classified. The method is evaluated on three popular datasets, with results showing that the method produces an average confidence that is an order of magnitude lower when compared to baselines for context - only images. Using transfer learning, they gain 0.021AP on MS COCO compared to the hard label approach."
SP:5254658923e594294b69d124a8d004166852822a,This paper proposes a new neural network architecture for inverse problem reconstruction based on convex duality. The proposed architecture is based on a two - layer ReLU denoising network with a weight decay regularization and a linear filter. The authors claim that the proposed architecture can be used for inverse reconstruction of MNIST and fast - MRI images. The main contributions of the paper are as follows :   - A new duality framework is proposed for training neural networks based on the duality of the ReLU network. The duality is a convex optimization method that allows for the training of convex neural networks with duality in addition to the regularization method. - The authors show that this duality approach can be applied to the reconstruction of MRI and MNIST images in a non - convex manner. - They provide experimental results that show the effectiveness of their proposed method for solving inverse problems for MRI reconstruction.  
SP:085cad6bc143c8713580bddfaa71f06496dac314,"This paper presents an end - to - end speech synthesis model that learns to synthesise speech from normalised text or phonemes. The authors focus on the task of generating high - fidelity audio through a combination of adversarial feedback and prediction losses constraining the generated audio to roughly match the ground truth in terms of its total duration and mel - spectrogram - based prediction loss. The proposed generator is feed - forward and thus efficient for both training and inference, using a differentiable alignment scheme based on token length prediction and soft dynamic time warping to capture temporal variation. The resulting model achieves a mean opinion score of 4.083, approaching the state - of - the - art from models trained using richer supervisory signals."
SP:01148cea55db606aa78d27e900818684a8bce9ab,"This paper proposes a non - parametric representation learning framework for node representation learning based on the Wasserstein graph diffusion process. The proposed method is based on embedding nodes into a discrete discrete distribution of node features in a lower - dimensional space, and then using graph diffusion to smooth the distribution representations of nodes with information from their local neighborhoods. This allows them to reduce the distortion caused by missing attributes and obtain integrated representations expressing information of both structure and attributes. To demonstrate the power of their representation method, they designed two algorithms based on on it for node classification ( with missing attributes ) and matrix completion respectively, and demonstrate their experiments in experiments."
SP:aeeb5909f7123ef631f569b469af9715205c881f,"This paper presents an approach for reinforcement learning ( RL ) in environments with sparse extrinsic rewards. The authors propose AMIGO, a method to train a goal - conditioned “ student ” policy in the absence of ( or alongside ) rewards in a procedurally generated environment. The method trains a teacher to propose increasingly challenging but achievable goals that the student learns to solve independent of the task to be solved. The teacher proposes the goals as a form of meta - learning, where the teacher learns how to interact with the environment and how to solve tasks that are too difficult for state - of - the - art reinforcement learning methods.   The authors provide extensive experiments on 6 challenging exploration tasks and show that the proposed method, called Adversarially Motivated Intrinsic GOals, is effective in training agents. They also provide extensive qualitative analysis and ablation study to evaluate the effectiveness of their method. The main contributions of the paper are as follows :   - An approach for learning a teacher that generates increasingly harder goals for the student. - A method for training the teacher to generate increasingly challenging yet achievable goals. - An analysis of the training data for the teacher and student."
SP:3d05bc7dca97681cb582298e318b9b973841eed3,"The paper considers the problem of information retrieval from a dataset of files stored on a single server under both a user distortion and a user privacy constraint. The proposed model can be seen as an extension of the well - known concept of private information retrieval by allowing for distortion in the retrieval process and relaxing the perfect privacy requirement. The authors evaluate the performance of the scheme on a synthetic Gaussian dataset as well as on the MNIST and CIFAR-10 datasets. They show that the optimal rate - distortion - leakage tradeoff is convex and that in the limit of large file sizes this allows for a concise information - theoretical formulation in terms of mutual information. Moreover, the authors propose a new data - driven framework by leveraging recent advancements in generative adversarial models which allows a user to learn efficient schemes of download rate from the data itself."
SP:3f9e2db00fc3dcd7a40588adcb638503ec10dc09,"Graph Neural Networks ( GNNs ) have become very popular for graph - related applications due to their superior performance. However, they have been shown to be computationally expensive in large scale settings, because their produced node embeddings have to be computed recursively, which scales exponentially with the number of layers. To address this issue, several sampling - based methods have recently been proposed to perform training on a subset of nodes while maintaining the fidelity of the trained model. In this work, we introduce a decoupled greedy learning method for GNNS ( DGL - GNN ) that, instead of sampling the input graph, decouples the GNN into smaller modules and associates each module with greedy auxiliary objectives. Our approach allows GNN layers to be updated during the training process without waiting for feedback from successor layers, thus making parallel GNN training possible. The authors evaluate their proposed training strategy thoroughly on benchmark datasets, and demonstrate it has superior efficiency while not sacrificing much performance. Further, we propose a lazy - update scheme during training to further improve its efficiency."
SP:5ecb1b288f7fc02aead4493f81640867bc349290,"This paper proposes a method for efficiently answering complex queries on incomplete Knowledge Graphs. The authors first translate each query into an end - to - end differentiable objective, where the truth value of each atom is computed by a pre - trained neural link predictor. They then analyse two solutions to the optimisation problem, including gradient - based and combinatorial search, and show that black - box neural models trained on millions of generated queries outperform state - of - the - art methods without training on a large and diverse set of complex queries. Finally, they demonstrate that it is possible to explain the outcome of their model in terms of the intermediate solutions identified for each of the complex query atoms.   The authors compare their method with other approaches in the literature such as Query2Box ( Ren et al. 2020 ), and find that the proposed framework achieves significantly better or equivalent predictive accuracy on a wide range of complex query types. The proposed framework is capable of out of distribution generalisation, since it is trained on simple queries only and evaluated on complex queries, and the intermediate results for its sub - queries and variable assignments can be used to explain any given answer."
SP:f04a522fd04c503754fdb8c52da68646d31271a4,"This paper presents a method for checking the robustness of neural networks with piecewise activation functions ( PAs ). The PAs are a special case of linear PAs that are commonly used in deep neural networks. The authors present a method that can be applied to PAs in feed - forward neural networks ( FNNs ) with linear activation functions. The key idea is to partition the input space into a set of convex polyhedral regions in which the network ’s behavior is linear. The goal is to ensure that the network classifies all the inputs within an `p - ball consistently, which precludes various forms of adversarial inputs. To this end, the authors propose a method based on an alternate parallel GPU implementation, parallelizing the analysis of the regions around a point using simple geometric projections. They compare their method with two existing methods that verify the PAs of deep convolutional networks. They find that their method is faster and more precise than both of them. However, they also find that the certified lower bounds of their method are not as tight as those of the other methods."
SP:5297651ff873f97c07b9c47ed3eff52251661844,"This paper proposes an approach to embed objects in an affordance space, where each dimension corresponds to an aspect of meaning shared by many actions, using text corpora. This embedding makes it possible to predict which verbs will be applicable to a given object, as captured in human judgments of affordance, better than a variety of alternative approaches. The dimensions learned are interpretable, and that they correspond to typical patterns of interaction with objects. Finally, the dimensions can be used to predict most dimensions of the SPoSE representation, in particular those that are categorical or functional functional. This suggests that affordance knowledge underlies much of the mental representation of objects, and in particular semantic categorization."
SP:72b4f3b40c6c6fa2eb53e95ed9a10a4077ffa049,"This paper proposes a method for the emergence of individuality ( EOI ) in multi - agent reinforcement learning ( MARL ). It builds on top of two popular MARL methods, MAAC ( Iqbal & Sha, 2019 ) and QMIX ( Rashid et al., 2018 ), and empirically demonstrates that EoI outperforms these methods in three scenarios where agents are preferred to take different roles, i.e. Pac - Men, Windy Maze, and Firefighters. It also shows that in a micro - task of StarCraft II, where the need for the division of labor is unknown, EOOI learns faster than existing methods. Empirically, the authors show that E - OI is compatible with centralized training and decentralized execution ( CTDE )."
SP:112509d6d3573a9d495d182fdfae6ec0327cddf5,"This paper presents a method for training classifiers based on randomized smoothing and ensembling. The proposed method, SWEEN ( Smoothed - Edge - Weighted Ensembling ), aims to improve the performance of randomized smoothed classifiers over their respective candidate models in terms of the approximated certified accuracy by a large margin. The main contribution of the paper is the introduction of an adaptive prediction algorithm to reduce the prediction and certification cost of the proposed method. Extensive experiments are conducted to validate the effectiveness of the method.   The main contributions of this paper are as follows :   1. An analysis of the robustness against adversarial attacks against randomized smoothing classifiers. The authors show that SWEen achieves state - of - the - art certified robustness. 2. A set of experiments is conducted to evaluate the effectiveness and the reduction in training time of their method. 3. An ablation study is performed to show that the method improves the performance on a small number of tasks."
SP:ea892e3d199ed6121279b20061a87f43afae8796,This paper proposes a method to learn subtask hierarchy by learning from demonstration. The proposed method is based on the Ordered Memory Policy Network ( OMPN ). The main idea is to use the inductive bias to learn a memory update rule that maintains a hierarchy among memories such that the higher - level memory can store longer - term information. The authors propose to use a bottom - up recurrence and a top - down recurrence to implement horizontal update and vertical expansion respectively. They also propose a differentiable stick - breaking process to train the model end - to - end. Experiments on both unsupervised and weakly supervised settings show that the proposed method achieves better task decomposition performance compared with strong baselines.   The authors also demonstrate the effectiveness of their approach with multi - task behavior cloning.
SP:cc6aa977ce561a2493ae74bb694205cd67c8d890,"This paper proposes a novel method for learning supervised learning from out - of - distribution ( OOD ) data, based on a Causal Semantic Generative Model ( CSG ). The model is based on the causal invariance principle, with a novel design in variational Bayes for both efficient learning and easy prediction. Theoretically, the authors prove that under certain conditions, CSG can identify the semantic factor by fitting training data, and this semantic - identifiability guarantees the boundedness of OOD generalization error and the success of adaptation adaptation.   The main contribution of the paper is to develop methods for OOD prediction from a single training domain, which is common and challenging. The methods are based on causal reasoning so that the two factors are modeled separately and developed methods for efficient learning. The authors also provide experimental results showing that CSG achieves better performance than other methods on real - world image classification."
SP:be3f34a59e5e61dcdbc7cb085f031ba4a5a5b758,"This paper studies adversarially corrupted rewards in an online learning setting with stochastic multi - armed bandits, linear contextual bandits, and Markov Decision Processes ( MDPs ). The main contributions of the paper are as follows :   1. The authors study the problem of designing robust algorithms with small regret over a period of time steps while the algorithm observes corrupted rewards, while maintaining robust estimates of the estimated rewards and transition probabilities. 2. They show that an extension of a UCB style exploration scheme achieves an optimal penalty of O( T ) with robust estimates by maintaining robust optimistic estimates of rewards at each state - action pair. 3. They extend this to the more general case where they modify the UCRL2 algorithm ( Auer et al. 2009 ) by maintaining $ \ell_2$-norm robust estimates. 4. Finally, they provide empirical evidence regarding the robustness of the proposed algorithms on synthetic and real datasets on which they claim to have near optimal regret."
SP:6d62a80aaebb2988df3953d4d7164e5a2fa1aa6d,"This paper proposes Rewriter - Evaluator ( REW ), a method to improve the performance of neural machine translation ( NMT ) by training a rewriter and an evaluator. The rewriter first translates a source sentence into a pre - written version, which is then translated into the final version by the evaluators. The authors propose to train the rewriter jointly with a prioritized gradient descent ( PGD ) method. They compare the proposed method with two widely used NMT models, RNNSearch ( Bahdanau et al., 2015 ) and Transformer ( Vaswani et al., 2017 ), and show that the proposed framework notably improves the performances of the two models and significantly outperforms previous baselines."
SP:9761fca8848868dfc9cacdab2537f8276ca76e0f,"This paper proposes a novel two - stage strategy for learning calibrated adversarial distribution for semantic segmentation. The main idea is to learn a distribution over predictions, where the empirical frequency of the sampled predictions closely reflects that of the corresponding labels in the training set. The method consists of two stages. In the first stage, the authors explicitly model the data with a categorical likelihood, and in the second stage, they train an adversarial network to sample from it an arbitrary number of coherent predictions. The model can be used independently or integrated into any black - box segmentation framework to facilitate learning of calibrated stochastic mappings. The authors demonstrate the utility and versatility of the approach by attaining state - of - the - art results on the LIDC dataset and a modified Cityscapes dataset. In addition, they use a toy regression dataset to show that their framework is not confined to segmentation and can be adapted to other tasks."
SP:ce965758f1b795a56c02f45d6a8d06cb8bdf29cb,"This paper proposes a new method to deal with error feedback induced by contractive compressors. The main idea is to replace the error feedback mechanism in the compressed communication framework of error feedback ( EF ) with a construction that transforms any contractive compressor into an induced unbiased compressor. The construction consists of two parts : ( 1 ) a transformer that turns any compressor into an unbiased compressor and ( 2 ) a modification of the compressors to make them unbiased.   The main contribution of this paper is the construction of a new error feedback framework that can deal with the error induced by compressors that are not unbiased, such as Top - K or PowerSGD. The authors show that this transformer can be applied to existing error feedback techniques such as EF, and show that it leads to vast improvements over the existing methods such as error feedback. They also show that their approach leads to a reduction in the error incurred by using EF compared to existing methods. The paper also proposes an experimental evaluation on an array of classification tasks with CIFAR10 dataset that corroborates their theoretical findings."
SP:4fd702490293e481c79614852ba27dd3ce9215a4,"This paper proposes a new research framework for hyperparameter transfer across adjustments ( HT - AA ) for machine learning ( ML ) algorithms. The main idea is to develop an algorithm that transfers knowledge from an existing hyper - parameter optimization ( HPO ) algorithm to a new one without making any adjustments to the hyperparameters or the search space of the existing HPO. The authors provide four baseline algorithms and 8 benchmarks that can be used to study various aspects of the ML algorithms, such as the hyper - parameter search space, the neural architectures used, and the search function. They also provide python packages for their baselines and benchmarks to facilitate future research on the algorithms.   The main contributions of the paper are as follows : - Developing a research framework that leverages existing knowledge from previous ML algorithms for faster development cycles and lower costs. - Providing a set of benchmarks that compare the performance of the new algorithm against the baseline and the one that was developed in the original setting."
SP:e8f99bae5853de525450fcb8facd23cf973fc161,"This paper proposes a new paradigm for image classification task by using speech as the supervised input. The proposed paradigm is based on audio spectrograms, Gaussian mixtures, and uniform random matrices of various dimensionalities. The authors experimentally compare the performance of the proposed paradigm against the state - of - the - art speech - based classification methods ( e.g., GANs, MNIST, and CIFAR10 ). They find that the proposed method achieves comparable performance to traditional categorical outputs ( i.e., text - based ) on the standard image classification tasks, but features learned through their label representations exhibit more robustness under various adversarial attacks and better effectiveness with a limited amount of training data. They also show that high - dimensional, high entropy label representations with high entropy produce more robust and data - efficient neural networks."
SP:4e8d924cba7367af0999b30d79250b4dc40413e1,"This paper proposes a new method to improve the robustness of ensemble neural networks by using a single model ( MIMO ) instead of multiple models ( multi - input multi - output ). The main idea is to train multiple subnetworks independent of each other to learn the task at hand and then use the predictions made by the sub - networks to improve model robustness without increasing the computational cost of forward passes. The authors compare their method with previous methods ( e.g., GAN - based methods ) on CIFAR-10, ImageNet, and ImageNet with and without out - of - distribution ( OOD ) variants and show that their method is more robust than those of the previous methods. They also show that they can improve the accuracy of their method compared to the other methods.   The main contributions of the paper are as follows : - Demonstrating that the benefits of using multiple models for forward passes can be achieved ‘ for free ’ under the single model ’s forward pass. - Showing that the performance of the proposed method is better than that of the methods that use multiple models under the same forward pass cost."
SP:d2f1c23b67c6744101034dc5e1c70765a733b169,"This paper proposes a method to distill knowledge from one Convolutional Neural Network ( CNN ) to another by utilizing sparse representation learning. The method, dubbed Sparse Representation Matching ( SRM ), first extracts sparse representations of the hidden features of the teacher CNN, which are then used to generate both pixellevel and image - level labels for training intermediate feature maps of the student network. The authors formulate SRM as a neural processing block, which can be efficiently optimized using stochastic gradient descent and integrated into any CNN in a plugand - play manner. Extensive experiments demonstrate that SRM is robust to architectural differences between the teacher and student networks, and outperforms other KD techniques across several datasets."
SP:e8c0f43bd5debf6544f588cd3442dc3dd62d0eee,"This paper proposes a novel approach to generalize policy - based reinforcement learning methods such as LQR and Jumping Task from pixels based on the sequential nature of reinforcement learning. This approach is orthogonal to recent approaches, which rarely exploit this structure explicitly. The authors introduce a theoretically motivated policy similarity metric ( PSM ) to measure behavioral similarity between states. They also present a contrastive representation learning procedure to embed any state similarity metric, which they instantiate with PSM to obtain policy similarity embeddings ( PSEs1 ). They demonstrate that PSEs improve generalization on diverse benchmarks, including LQRs with spurious correlations, a jumping task from pixels, and Distracting DM Control."
SP:92f3b4942da9075440dda618f561a85f8fde5a5c,"This paper presents a new approach to disentangle natural factors of variation in data ( e.g. object shape vs pose ) in machine learning. The proposed method, distributed equivariant operators, is based on the idea that each operator can act on a subspace of the latent space of a model's latent representation. The authors show that this approach introduces discontinuities in the encoder for a broad family of transformations acting on images, including simple affine transformations such as rotations and translations. Moreover, motivated by classical results from group representation theory, the authors propose an alternative, more flexible approach which relies on distribution of operators, potentially acting on the entire latent space.   The authors then theoretically and empirically demonstrate the capacity of the model equipped with the distributed operators in latent space to achieve a range of affine image transformations including translations, rotations, and combinations thereof thereof. They show that the distribution of equivariants improves the performance of the method."
SP:ef0f58c462bc5dd1c7b78f562c42a4e17f0f252b,"This paper proposes an EM algorithm for estimating the maximum posterior probability of a neural spike in neural spike recordings. The authors use the nonlinear Hawkes process, which they call SNMHP, to represent the excitatory - inhibitory interaction between neurons in the continuous - time regime of the neural spike process. They use three auxiliary latent variables : auxiliary Gamma variables, latent marked Poisson processes and sparsity variables. The auxiliary variables are augmented to make functional connection weights in a Gaussian form, which allows for a simple iterative algorithm with analytical updates. The EM algorithm is shown to be more efficient than the MLE - based expectationmaximization ( MLE ) algorithm in high dimensional cases. Experiments are conducted on synthetic and real neural recordings."
SP:1156d3deac022829bda930ffcb081947609d972b,"This paper studies the dynamics of the gradient descent ( GD ) algorithm for training two - layer neural network models. It is found that there are two distinctive phases in the GD dynamics in the under - parametrized regime : an early phase in which the dynamics follow closely that of the corresponding random feature model, followed by a late phase where the neurons are divided into two groups : a group of a few “ quenched ” neurons that dominate the dynamics and another group of neurons that support the continued activation and deactivation process. In particular, when the target function can be accurately approximated by a relatively small number of neurons, this quenching -activation process biases GD to picking sparse solutions. This neural network - like behavior is continued into the mildly over - parameterized regime, in which it undergoes a transition to a random featurelike behavior where the inner - parameters are effectively frozen during the training process. The authors also study 2LNNs under mean - field scaling where all neurons participate equally and the test performance is much more robust to the change of network width."
SP:9e81401a6f30c70d870a12cce0cf600557f92b80,"This paper proposes a practical method for solving practical reinforcement learning problems in the Markov Decision Process ( CMDP ) setting, where the agent has to maximize the expected return while satisfying a set of prescribed safety constraints. The authors decompose the CMDPs into a pair of MDPs ; reconnaissance MDP ( R - MDP ) and planning MDP, and train a reward - seeking policy and a fixed threat function for each MDP. They also present an efficient approximation method for the threat function that can greatly reduce the difficulty of solving R - RDP. In addition, they propose a method for P - PDP where they use a different reward and a different threat function to determine the safeness of each action. The experimental results show that the proposed method is more effective than the baselines used by the authors."
SP:f1d4ac7d5516dd0df742e224c8c09c721d0d0886,"This paper analyzes the relationship between square error and cross - entropy loss in training neural networks for various classification tasks. It is well - known that square error is better suited for image classification, while cross - entropy is more suited for NLP and speech recognition, but this paper argues that the empirical evidence supporting this belief may not be well - founded. The authors compare several neural networks and several loss functions across a range of standard benchmark datasets for classification, NLP, ASR and computer vision, and find that the square error loss generally performs better than the cross entropy loss on most tasks. Cross - entropy seems to have a slight edge on computer vision tasks, while square error seems to perform better on ASR. The paper suggests that training with square error needs to be part of best practices of modern deep learning on equal footing with cross - Entropy."
SP:915f1f0fc4850507c28c1d609239b41775863ebe,"This paper proposes Self - Predictive Representations ( SPR ), a method to augment self - supervised reinforcement learning with structure in the visual input and sequential interaction with the environment. SPR trains an agent to predict its own latent state representations multiple steps into the future using an encoder which is an exponential moving average of the agent ’s parameters and a learned transition model. SPR achieves a median score of 0.415 on Atari in a setting limited to 100 steps of environment interaction, which represents a 55% relative improvement over the previous state - of - the - art method. Even in this limited regime, SPR exceeds expert human scores on 7 out of 26 games. SPR further improves performance by adding data augmentation to the future prediction loss, which forces the agent’s representations to be consistent across multiple views of an observation."
SP:983f01c170909c8c67fd3be25f121bd61bdd8307,"This paper introduces InstantEmbedding, an efficient method for generating single - node representations using local PageRank computations. The authors theoretically prove that their approach produces globally consistent representations in sublinear time. They also show that their method produces high quality representations, demonstrating results that meet or exceed the state of the art for unsupervised representation learning on tasks like node classification and link prediction. They demonstrate this empirically by conducting extensive experiments on real - world datasets with over a billion edges. The experiments confirm that Instant Embedding requires drastically less computation time ( over 9,000 times faster ) and less memory ( by over 8,000 TIMES ) to produce a single node’s embedding than traditional methods including DeepWalk, node2vec, VERSE, and FastRP."
SP:d11037b8fe2b10aee672ba82f69410b40181f0f9,"Graph coarsening is a popular technique to reduce the size of a graph while maintaining essential properties. This paper proposes a data - driven method, GOREN, that combines graph neural networks ( GNN ) with graph neural network ( GN ) to improve the coarsened quality in an unsupervised way.   The authors propose a framework for measuring the quality of the proposed method, based on the goal, goal, operator, and goal - value. The goal is to carefully choose the Laplace operator on the coarse graph and the projection / lift operators on the graph. The authors observe that the current choice of edge weight for coarse graph may be suboptimal. Motivated by this observation, they parametrize the weight assignment map with graph Neural Network and train it to improve. Through extensive experiments on both synthetic and real networks, the authors demonstrate that their method significantly improves common graph coarsen methods under various metrics, reduction ratios, graph sizes, and graph types. The method generalizes to graphs of larger size ( 25 times larger than training graphs ) and scales to much larger graphs than previous work."
SP:0d680213339f0e2aedb0be4aeed51423706b8bf6,"This paper presents a method for computing the acoustic properties of 3D geometric objects based on discrete - laplacian and implicit encoders. The method is based on a point cloud approximation of each object, and each point is encoded in a high - dimensional latent space. The authors use a multi - layer network to estimate these acoustic properties for arbitrary topologies and takes less than 1ms per object on a NVIDIA GeForce GeForce RTX 2080 Ti GPU. They also prove that their learning method is permutation and rotation invariant, which is an important characteristic for accurate computation of acoustic scattering fields.   The authors also perform an ablation study to highlight the benefits of their approach. They highlight its application to generating environmental acoustic effects in dynamic environments."
SP:afc33a782c43e3d4c5c4fbf047d0b1108bc30bae,"This paper proposes Risk Extrapolation ( REx ), a method to mitigate distributional shift in machine learning models. The authors assume that the distributional shifts across training and test domains are representative of the variation we might encounter at test time, but also that shifts that are more extreme in magnitude may be present in the real world. They show that reducing differences in risk across training domains can reduce a model ’s sensitivity to a wide range of extreme distributional shifted, including the challenging setting where the input contains both causal and anti - causal elements. They motivate this approach as a form of robust optimization over a perturbation set of extrapolated domains ( MMREx ) and propose a penalty on the variance of training risks ( V - REx ) as a simpler variant. They prove that variants of REx can recover the causal mechanisms of the targets, while also providing some some robustness to changes in the input ( “ covariate shift ” ). They also show that Invariant Risk Minimization ( IRM ) is able to outperform alternative methods such as invariant risk minimization."
SP:411d5bcf7698d534ad60f581d479ff74849ba4de,"This paper introduces Fourier neural operators, a method for learning partial differential equations ( PDEs ) that is based on Fourier transformers ( ML - based methods ). It is claimed that this method is the first ML method to learn turbulent flows with zero - shot super - resolution ( compared to traditional PDE solvers which solve one instance of the equation per solution ). The authors argue that Fourier operators can approximate highly non - linear operators and thus are more expressive and efficient than the standard linear operators. They also propose three new methods : Burgers ’ equation, Darcy flow, and Navier - Stokes equation.   The main contributions of the paper are as follows :   1 ) The authors propose a new neural operator by parameterizing the kernel directly in Fourier space, allowing for an expressive and expressive architecture 2 ) They propose a method that learns the mapping from any functional parametric dependence to the solution of the PDE, in contrast to classical methods that only learn the mapping between function spaces 3 ) They show that the Fourier operator is able to learn an entire family of PDE ’s, unlike traditional methods which solve just one instance per PDE 4 ) They demonstrate that their method is faster and more expressive than the traditional linear operators in learning the mapping for turbulent flows."
SP:41d268d0eac9b4c84baa156fb641aa6d3060b5a4,"This paper studies the implicit bias of gradient flow ( gradient descent with infinitesimal step size ) on linear neural network training. The authors propose a tensor formulation of neural networks that includes fully - connected, diagonal, and convolutional networks as special cases, and investigate the linear version of the formulation called linear tensor networks. They show that gradient flow on separable classification finds a stationary point of the `2 / L max - margin problem in a “ transformed ” input space defined by the network in the paper. For underdetermined regression, they prove that   gradient flow finds a global minimum which minimizes a norm - like function that interpolates between weighted `1 and `2 norms in the transformed input space. The paper also provides experiments that corroborate their analysis."
SP:e27907ef4a4e6e0f5841618fcaa7e7e0db443f91,"This paper proposes PareCO, a method to jointly optimize the width - multipliers for different layers and the shared weights in a slimmable neural network. Specifically, the authors propose Pareto - aware Channel Optimization or Pare CO, a novel algorithm which approaches the intractable problem formulation in an approximate fashion using stochastic gradient descent. The training method proposed by Yu et al. ( 2019 ) is a special case of this problem. The proposed formulation is general and can be applied to objectives other than prediction error and FLOPs ( Yu & Huang, 2019 ). The authors perform extensive empirical analysis using 15 network and dataset combinations and two types of cost objectives to demonstrate the effectiveness of the proposed algorithm compared to existing alternatives. Quantitatively, improvements up to 1.7% and 8.8% in top - 1 accuracy on the ImageNet dataset can be attained for MobileNetV2 considering FLOPS and memory footprint, respectively. The results highlight the potential of optimizing the channel counts for the different layers jointly with the weights for slimpable networks."
SP:cf59403abb6ca89ccee4f8e77e9a33d99e6a00f5,"This paper proposes FedMatch, a federated learning method based on federated semi - supervised learning ( FSSL ). The authors study two scenarios of FSSL based on the location of labeled and unlabeled data. The first scenario considers a conventional case where clients have labeled data at both client and server and the second one considers a more challenging case where the labeled data is only available at the server. In both scenarios, the proposed FedMatch method is shown to outperform both local SSL and the naive combination of FL with SSL algorithms under the conventional labels - at - client scenario and the novel labels -at - server scenario, across multiple clients with both non - i.i.d and i.d.d data."
SP:9457b6d430a2cd864d526d7e90bf3e1ab13d6df4,"This paper proposes a new method, CoLES, for self - supervised learning on discrete event sequences generated by real - world users. In contrastive learning, the authors adopt contrastive methods previously used for audio and computer vision domains to the discrete event sequence domain in a self -supervised setting. Unlike most previous studies, they theoretically justify under mild conditions that the augmentation method underlying CoLES provides representative samples of discrete events sequences. They evaluated CoLES on several public datasets and showed that CoLES representations consistently outperform other methods on different downstream tasks. In the next section, they discuss related studies on self - supervised and contrastive learned methods adapted to event sequence datasets."
SP:385942a5bcee7384bb722a1669b541f2fac0cd36,"This paper proposes a new unsupervised language parsing method, StructFormer, that combines dependency grammar and language modeling. The main idea is to combine the dependency grammar that models one - to - one correspondences between words and the constituency grammar, that models the assembly of one or several corresponded words. To achieve this, the authors propose a new parsing framework that can jointly generates constituency tree and dependency graph. Then, they integrate the induced dependency relations into transformer, in a differentiable manner, through a novel dependency - constrained self - attention mechanism. Experimental results show that StructFormer can achieve strong results on unsuper supervised constituency parsing, un - supervised dependency parsing and masked language modeling, and can leverage the results to achieve strong performance on masked language model tasks."
SP:078966ff62775bba6031e47d374bda95f4a7dde3,"This paper proposes a method for learning grounding between scene graph nodes and visual objects under weak supervision. The proposed method leverages object bounding boxes and object feature embeddings to learn a metric between visual objects and scene graph node. The method is based on two components : object feature encoder and relational feature embedding. The object features encoder maps the nodes of the scene graph to a set of objects bounded by a relational embedding, which is then used to learn the metric between the two sets of objects and the graph nodes. This metric is used to train a neural network that learns the mapping between the nodes and the objects. The neural network is trained in two ways : ( 1 ) based on the visual embedding and ( 2 ) on object features and relational features.   Experimental results on scene graph parsing tasks show that the proposed method outperforms the state - of - the - art methods on the grounding task. Further experiments verify the grounding found by the grounding method can reinforce the performance of the existing method on another graph parsing task."
SP:4644dbf7466b6234d8abf69995fdfb357efcc119,"The paper proposes a new method to learn the distribution of data by minimizing a reconstruction loss together with a relational regularization on the latent space, named spherical sliced fused Gromov Wasserstein ( SSFG ). SSFG replaces the vMF distribution by a power spherical distribution to improve the sampling time in high dimension settings. The authors then apply the new discrepancies to the RAE framework to achieve its new variants, and conduct extensive experiments to show that the new proposed autoencoders have favorable performance in learning the latent manifold structure, image generation and reconstruction generation."
SP:5ae2c0af82cac89a65f1cc38c43e2d05ea298901,"This paper proposes a method to speed up training for deep linear networks with repeated layers, such as transformer networks, by sharing the weights across all the repeated layers till convergence, and then stop weight sharing and continue training until convergence. The untying point is automatically determined by monitoring gradient statistics. The adaptive untying criterion is obtained from a theoretic analysis over deep linear network. Empirical results show that this method is able to reduce the training time of BERT by 50 %. The authors also provide ablation studies on different ablation choices in implementing their algorithm."
SP:a51710551142316b67e2fccd969fea1ece35ba39,"This paper investigates adversarial perturbation methods to improve the adversarial transferability of perturbations. The authors use the interaction between adversarial agents inside perturbated states and their interactions with each other to establish a unified view of the problem. They show that there is a negative correlation between the transferability and the interaction inside perturbed states. The negative correlation is further verified through different DNNs with various inputs. This negative correlation can be regarded as a unified perspective to understand current transferability - boosting methods. To this end, they propose to directly penalize interactions during the attacking process, which significantly improves adversarial - transferability.   The paper proposes a new loss to penalize the interactions inside adversarialperturbations and enhance adversarial-boosting methods. The main idea is to introduce a penalization scheme that penalizes the interaction with the perturbed state during the attack process. This penalized interaction can be thought of as a kind of learning loss. The paper also proposes a way to use the penalized interactions to improve adversarial transfersability."
SP:f1565319075c1442c2cb52d96443facb492c06c2,"This paper presents a quantitative analysis of forgetting in deep learning models. The authors focus on the relationship between forgetting and task semantic similarity in neural network representations. They find that for tasks with intermediate similarity between the input and output, forgetting is maximal for sequences of tasks that have intermediate similarity. For tasks that are not intermediate in semantic similarity, the authors find that the forgetting is more prevalent in the deeper layers of the network. They show that this is due to the fact that the task representations in the hidden layers are more likely to be similar to each other than the output of the model. They also show that sequential training can lead to the forgetting occurring more frequently in the deep networks that are trained on intermediate similarity tasks. The paper also presents a number of methods to mitigate the forgetting of the deeper network representations that are based on different techniques."
SP:30d7532cdcf420bff3be6b92eea3d93bce59e6bd,"This paper presents a method for efficient pre - training and fine - tuning of large language models, such as BERT, XLNet and T5, in the early stage of BERT training. The proposed method is based on the structured lottery tickets developed by Liu et al. ( 2020a ) for early training of vision and language models in BERT. The authors propose to use network slimming to reduce the number of self - attention and fully - connected sub - layers inside a transformer in order to identify winning tickets in the BERT early training stage. The method is evaluated on GLUE and SQuAD tasks, and compared with BERT on both the GLUE task and the SQuad task. The results show that EarlyBERT achieves comparable performance to standard BERT with 35∼45 % less training time."
SP:c547f23ff6caaf5e9f35d258490b86ae0ac8ed03,"This paper analyzes the robustness of f - divergences when supervised learning with label noise. The main contribution of the paper is the derivation of a family of robust f - divergence measures for learning with noisy labels. These measures are based on the variational divergence between the predicted distribution on the clean distribution and a bias term introduced due to the noise in the supervised learning setting. The paper shows that these measures are robust with respect to a classifier ’s predictions when the supervised labels are clean and when the noise is added to the distribution.   The paper also proposes a number of experimental results to verify the effectiveness of these measures. The experimental results are presented in two main ways : ( 1 ) when maximizing a properly defined f - diversified fivergence measure, and ( 2 ) when minimizing the f - divergence of the supervised classifier when the label noise is present. In the first case, the paper empirically shows that the proposed measures are more robust than the supervised ones. The second case, however, is less certain."
SP:841888179dcdac901889c8d62cb5234311fe28f1,"This paper proposes SUNRISE, an ensemble - based method for stabilizing off - policy reinforcement learning ( RL ) in low - dimensional and high - dimensional environments. The main idea of the method is to use an ensemble of Bellman agents to predict the Q - values of the reward function in an ensemble based on uncertainty estimates from a Q - ensemble. The method is tested on continuous and discrete control tasks on OpenAI Gym and DeepMind Control Suite. The experiments show that the proposed method consistently stabilizes and improves the performance of existing off policy RL algorithms such as Soft Actor - Critic ( AC ) and Rainbow DQN. In addition, the paper also investigates the signal - to - noise ratio between the ensemble and standard Bellman in environments with noisy rewards and finds that the ensemble method performs better than the standard one in these environments."
SP:afc08f203562b841180811aef943bfb63a1659ea,This paper proposes a method for training a few - shot classification model based on task uncertainty. The paper proposes to measure the distributional mismatch between support and query sets via class - wise similarities in order to train the model with careful confidence. The method is algorithm - agnostic and can be applied to a wide range of meta - learning models. The main contribution of the paper is the training strategy for training the model to avoid being indiscriminately confident and thereby produce calibrated classification results without the loss of accuracy.   The main contributions of this paper are the following :   1. A method for measuring distributional mismatches between support examples and queries in a few shot classification framework. This is based on the premise that the random sampling of tasks can generate those in which it may be hard for the models to infer the queries from the support examples. 2. A novel training strategy is proposed that lets the model predict the query from the query examples with careful confident. 3. Extensive experiments are conducted to validate the effectiveness of the proposed training strategy and to show that the proposed method is able to learn well.
SP:12ae325ea3bce1e60195afac7d85895d2d20c29c,"This paper proposes a novel method to improve the learning performance of noise contrastive learning for video representation learning by leveraging a generative model that naturally pushes related samples together. Specifically, the authors propose a weighted combination of support samples ’ visual representations to be used as the caption for each text - to - video pair. The authors show that this method outperforms the state - of - the - art baseline by a significant margin. Moreover, they show that the proposed method improves upon the performance of their baseline by up to 30 % compared to the baseline."
SP:8a71d8fad25a126aff01431cacf348c05de75667,"This paper studies the problem of improving the performance of pre - trained language models ( PLMs ) for Chinese natural language processing ( NLP ) tasks. The main source of vocabulary for these Chinese PLMs remain to be the one provided by Google Chinese Bert Devlin et al. ( 2018 ), which is based on Chinese characters. This paper proposes a method, seg tok, to form the vocabulary of Chinese BERT, with the help of Chinese word segmentation ( CWS ) and subword tokenization. It also proposes three versions of multi - vocabulary pretraining ( MVP ) to improve the models expressiveness.   The main contributions of this paper are as follows :   1. The authors propose a method for pre - training a language model based on a single vocabulary, which limits the downstream performance of the language model. This method is called seg - tok. 2. They propose a multi - vocabulary pretraining method, where each vocabulary is segmented into sub - words using CWS and sub - word tokenization, and then the vocabulary is trained using a mixture of the segmented vocabulary and the tokenized vocabulary. 3. Finally, they propose three ways to pre - train language models, i.e., self - training, masked language training, and multi - language pretraining. The proposed methods are applied to three Chinese NLP tasks : 1. Self - training on sentence level tasks, where the goal is to learn a vocabulary that is representative of the one used by the masked language model ( e.g., BERT ) 2. The method is applied to the word - by - word task and shows that it is able to learn language models that are more expressive than the one that is learned by BERT. This shows that the proposed methods can learn languages that are expressive than those that are not expressive. The methods are also applied to two tasks that are non - expressive, where it is shown that the method does not only improve the performances of BERT but also that it can also improve the efficiency of the model. The third task is a sequence labeling task where the model is trained to predict which words should be used in a text and which ones should be left out."
SP:b93ec7bc02b48068073ffe705f71d2643e663d51,"Graph Convolutional Networks ( GCNs ) have emerged as the state - of - the - art model for graph - based learning tasks. However, it is still challenging to train GCNs at scale, limiting their applications to real - world large graphs and hindering the exploration of deeper and more sophisticated GCN architectures. This paper proposes a method, called BDS - GCN, that adopts unbiased boundary sampling strategy to enable efficient distributed GCN training while maintaining the full - graph accuracy. To this end, the authors propose BDS -GCN, a method that uses graph partitioning and distributed training for tackling the challenge of distributed training of GCNs. The proposed method consists of three components : ( 1 ) Graph partitioning : The authors propose to partition subgraphs of a given graph into several nodes of the same size and use the average distance between each node in each subgraph to estimate the distance between the next node in the subgraph and the previous node in a subgraph, ( 2 ) Graph Distillation : The author propose to use a technique similar to the one used in [ 1 ] to divide the graph subgraph into several smaller ones based on the number of edges and the distance to each edge. The authors claim that this method is more scalable and efficient than the previous distributed training method ( e.g., GraphDistillation ). ( 3 ) Graph Mutliplication : The method proposes to mutliplice subgraph of a graph of size 2.5 by dividing it into smaller chunks of size 1.5 and 1.6 by 2.0. The author claims that this approach is more efficient and more scalable than the other distributed training methods such as GraphDistillization and Graph Distillization.   The authors also claim that the proposed method is better suited for distributed training compared to the existing methods ( such as graphdistillation and graphconversion ) in terms of training performance and accuracy."
SP:2d4ba873d11e969ebd1fc31f9b5ab450c964d154,"This paper presents a neural network based on machine learning ( ML ) called ForceNet for quantum chemistry simulations. The motivation is to accelerate the discovery of new catalysts for renewable energy storage and other energy applications. The model is based on a graph neural network that uses surrounding 3D molecular structure to estimate per - atom forces. The key challenge is to accurately capture highly complex and non - linear quantum interactions of atoms in 3D space, on which forces are dependent. To this end, the authors adopt expressive message passing architecture, expressive activation functions, and model scaling in terms of network depth and width. The authors apply their model to the new large - scale quantum chemistry dataset OC20 (Anonymous, 2020 ) that contains 200+ million samples from atomic relaxations. They show that ForceNet is able to estimate the relaxed structures 4x more accurately than previous state - of - the - art, while being multiple - orders of magnitude faster than DFT. Finally, they use ForceNet as a surrogate to DFT to perform quantum chemistry simulation; specifically, calculating structure relaxations of complex systems."
SP:8bdcf4fe6abf4739d4732b7ea8538513135dcccc,"This paper presents a neural network generalisation bound based on the Rademacher complexity that uses the distance the weights have moved from their initial values. This bound has no direct dependence on the number of weights and compares favourably to other bounds when applied to convolutional networks. Inspired by this bound, the authors develop a simple yet effective fine - tuning algorithm that constrains the hypothesis class to a small sphere centred on the initial pre - trained weights, thus obtaining provably better generalisation performance than conventional transfer learning. One of these approaches is a penalty - based method that regularises the distance from initialisation according to the MARS - based distance metric, while the other two techniques make use of efficient projection functions to enforce constraints on the Euclidean and MARS distance. The experimental results demonstrate that projected sub - gradient methods improve performance over using penalty terms and that the widely used Euclidan metric is typically not the best choice of metric to measure distances in network parameter space."
SP:3a3249e97ef2345ea2264de5ed8287e16687838e,"This paper investigates a counterintuitive phenomenon that occurs in the context of model pruning where it is assumed that the best training configuration for evaluation is also the best configuration for mask discovery. To investigate this phenomenon, the authors decoupled the hyperparameters for finding ( Hfind ) and mask evaluation ( Hevalent ) and used unstructured magnitude pruning for vision classification tasks. They show that the decoupling results in a phenomenon where certain Hfind values lead to models which have lower performance but generate masks with substantially higher eventual performance compared to models that do not have these values. They also show that this phenomenon holds across a number of different models, datasets, configurations, and also for one - shot structured pruning. Finally, they demonstrate that different H find values yield masks with materially different layerwise pruning ratios."
SP:2d6f5d72b21675f74ff4cde4d16bfb36abd5795f,"This paper proposes a metric, m - coherence, to study alignment of per - example gradients in ResNet and EfficientNet models on ImageNet. The metric is defined as the number of examples in the sample that benefit from a small step along the gradient of any one example on average for a sample of size m. The authors show that compared to other commonly used metrics such as O(m ) and M(d ), the metric m -coherence is more interpretable, cheaper to compute, and is closely connected to gradient diversity, a quantity previously used in some theoretical bounds.   The authors experimentally study the evolution of alignment of each of the three models and find that in some cases m - Coherence reaches moderately high values during training. This indicates that over -parameterized neural networks find common patterns even in scenarios where generalization is not possible. A detailed analysis of this phenomenon provides both a deeper confirmation of CG, but at the same point puts into sharp relief what is missing from the theory in order to provide a complete explanation of generalization."
SP:e7c5de9a475d0ba71bc79580e8436024fb2c6f59,"This paper proposes a method for constructing statistics for implicit generative models where the evaluation of the likelihood function is intractable but sampling data from the model is possible. The authors frame the task of constructing sufficient statistics as learning mutual information maximizing representations of the data with the help of deep neural networks. The infomax learning procedure does not need to estimate any density or density ratio.   The authors apply their approach to both traditional approximate Bayesian computation and recent neural likelihood methods, boosting their performance on a range of tasks. They develop two new likelihood - free inference methods namely SMCABC+ and SNL+. Experiments on tasks with various types of data demonstrate their effectiveness."
SP:c5997bf2348e94949684f45fbd418661e85220c1,"This paper presents an unsupervised image - to - image translation model, TUNIT, that is trained without image - level ( i.e. pairs of pairs of images ) or set - level supervision. The authors propose to train the model in a semi - supervised manner, where they do n’t use paired images nor domain labels. Instead, they use separate image domains for each input image and domain labels for each output image. The model learns to separate the input images into the estimated domains and the domain labels into the separate image - domain pairs. It also learns to translate the output images from the estimated domain to the source images. Experimental results show that their model achieves comparable or even better performance than the set -level supervised model trained with full labels, and generalizes well on various datasets."
SP:0cd97e64e638cabbeea0fdef3e9c5b33f4000f72,"This paper studies the implicit bias of gradient descent for regression problems in wide neural networks. The main result is that the solution of training a width - n shallow ReLU network is within n−1/2 of the function which fits the training data and whose difference from initialization has smallest 2 - norm of the weighted second derivative with respect to the input. The curvature penalty function 1/ζ is expressed in terms of the probability distribution that is utilized to initialize the network parameters, and it is explicitly studied explicitly for various common initialization procedures, such as asymmetric initialization with a uniform distribution.   The main theory of the paper is as follows :   1. In Section 2, the authors provide settings and notation for training neural networks, and show that the training trajectories are captured by trajectories of spatially adaptive smoothing splines with decreasing regularization strength. 2. In Sections 3 and 4, the main results are presented, and the main theory is developed in Sections 4 and 5. 3. In section 4, technical proofs and extended discussions are deferred to appendices. 4."
SP:8b885142facbb3b8db41ec9d83822cee81324694,This paper studies the problem of weight decay in training deep neural networks. It shows that the L2 regularization and decoupled weight decay are unstable for adaptive gradient methods such as stochastic gradient descent ( SGD ) and Adaptive Momentum Estimation ( Adam ). The authors propose the stable weight decay ( SWD ) method to fix the unstable weight decay problem from a dynamical perspective. SWD uses a bias correction factor on the weight decay to make weight decay more stable during training. The proposed SWD method makes significant improvements over L2 Regularization and Decoupled Weight decay in the experiments.
SP:a3206dc71e32ba1830895bf442d3840f3331a532,"This paper proposes a novel method to combine the strengths of both Translational Memory ( TM ) and Neural Machine Translation ( NMT ). Specifically, the authors treat the matched sentence pair of TM as the additional signal and apply one encoder enhanced by the pre - trained language model ( PLM ) to encode the TM information and source sentence together. The authors extend the sentence level retrieval method to the n - gram retrieval method that is used in the previous work.   The authors validate their proposed methods on a mixed test set of multiple domains. Experiment results demonstrate that the proposed methods can significantly improve the translation quality and show strong adaptation for an unknown or new domain."
SP:72b43991a242872b2ceb1861e8ffbdf26c9f4818,"This paper studies the problem of learning deep convolutional neural networks from the principles of rate reduction and ( shift invariant ) classification. The authors show that the basic iterative gradient ascent scheme for maximizing the rate reduction of learned features naturally leads to a deep network, one iteration per layer. The architectures, operators ( linear or nonlinear ), and parameters of the network are all explicitly constructed layer - by - layer in a forward propagation fashion in this paper. They show that such a network can already learn a good discriminative deep representation without any back propagation training. Moreover, all linear operators of the so - derived network naturally become multi - channel convolutions when they enforce classification to be rigorously shift - invariant. The derivation also indicates that such an encoder - decoder network is significantly more efficient to learn and more likely to be transferrable to other networks."
SP:f8b02cf1b918b0956761829ec6ef9127596071ec,"This paper studies the implicit acceleration of gradient flow in two - layer linear models. The authors show that implicit acceleration emerges from a conservation law that constrains the dynamics to follow certain trajectories. More precisely, gradient flow preserves the difference of the Gramian matrices of the input and output weights and the amount of acceleration depends on both the magnitude of that difference ( which is fixed at initialization ) and the spectrum of the data. In Section 4, the authors make connections with Riccati differential equations, obtaining a more general characterization of the convergence rate and establishing an interesting link with explicit regularization."
SP:e5f086c806be88d50e461a782b5b00124f4656fb,"This paper presents CLIME, an explainable machine learning framework that aims to mitigate adversarial attacks and improve the reliability of LIME, a popular explainable ML framework that attempts to explain an opaque model ’s behavior by training a surrogate interpretable model to be locally faithful on perturbed instances. The paper proposes a theoretically sound framework based on the uniform sampling of user - defined subspaces that allows the end - user flexibility to delineate the precise subspace of the input domain to be explained. The main contributions of the paper are three - fold : ( 1 ) It proposes a framework that is model - agnostic and can be applied to any ML model, and extensive experiments demonstrate its versatility on real - world problems ; ( 2 ) It develops an estimation algorithm that is able to measure the true value of metrics such as fidelity up to any desired degree of accuracy, which can help in building trust in the generated explanations ; and ( 3 ) Zooming in and refining explanations for uncovering hidden biases is a key part of CLIME."
SP:b1d5ef15772e192eb8c8a0e65b3c21ee7c794295,"This paper proposes a novel pre - trained language model, called AMBERT ( A Multi - Grained BERT ), for the task of natural language understanding ( NLU ) in Chinese and English. It consists of two parts : ( 1 ) fine - grained tokenization and ( 2 ) coarse - granular tokenization. The fine - granulated tokenization is used in the sense that for languages like English, the tokens are words or sub - words, while for languages such as Chinese, they are characters. The authors argue that both fine grained and coarse grained tokens have advantages and disadvantages for learning in pre -trained language models. They propose to combine the two approaches in the following way : ( a ) after each tokenization, the encoder generates a sequence of contextualized representations of the words and phrases, ( b ) utilizes shared parameters between the two encoders for processing the sequence of the phrases, and ( c ) finally creates a text - based model. Experiments are conducted on benchmark datasets for CLUE, GLUE, SQuAD, CLUE and RACE in English and Chinese. The results show that the proposed model outperforms the existing best performing language models in almost all cases, particularly the improvements are significant for Chinese.    The authors also develop a version of their model, which performs equally well as the main one but uses about half of its inference time, and CLUE., RACE, and ClUE."
SP:fd1cfe80343d3789227d99d836a5674374a234f5,"This paper proposes a Transformer - based model for semantic parsing. The main idea is to incorporate Long Short - Term Memory ( LSTM ) into the Self - Attention mechanism of the original Transformer to capture more local context of phrases. Experimental results show that the proposed model captures the detailed meaning better than Transformer, raises local context awareness and achieves strong competitive performance on the Geo, MSParS and new SOTA datasets in the methods using Neural Network.   The paper is proposing a novel model based on Transformer architecture that is capable of a more detailed meaning representation by learning the phrase dependencies in the sentence. The authors also conduct experiments to confirm the awareness capacity of the model."
SP:2056a65a7500d79465685af883083cd706277c1f,"This paper proposes composite adversarial training ( CAT ), a method to improve the robustness of deep neural networks ( DNNs ) against adversarial perturbations. The authors focus on the adversarial defense against the compositions of pixel perturbation and spatial transformations, two of the most challenging adversarial attacks. They propose a method that integrates and optimizes multiple adversarial losses, leading to significant robustness improvement with respect to individual perturbedations as well as their “compositions ”. They evaluate the efficacy of CAT on benchmark datasets and models and compare it against other adversarial defenses. They show that CAT outperforms existing adversarial defence methods by large margins in defending against compositions of two major class of adversarial models, two major classes of spatial transformation models, while incurring limited impact on clean inputs. They also explore the optimization space of composite perturbated models, which could lead to several promising research directions."
SP:006e5b9ac9a8eb7223843731488bfefbd8eb09bd,"The paper proposes a new neural network architecture, called the Emergent Symbol Binding Network ( ESBN ), which learns abstract rules directly from high - dimensional sensory data given only a limited amount of training experience. The ESBN consists of a recurrent network augmented with an external memory that enables a form of variable - binding and indirection. The paper evaluates ESBN on a suite of tasks involving relationships among images that are governed by abstract rules, and shows that ESBN is capable of learning rules from a limited number of training examples and systematically generalizing these rules to novel entities. ESBN outperforms a number of other competitive neural network architectures on these tasks, but fails to generalize them successfully when trained on a limited set of problems involving a small number of entities.   The paper concludes from these results that a capacity for variable - bounding is a necessary component for human - like abstraction and generalization, and that the ESBN could be a promising candidate for how to incorporate such a capacity into neural network algorithms."
SP:4171ce45966ac499f51450a19fb233934c0847f0,"This paper proposes a method to translate between augmented natural language ( ARL ) and structured prediction language ( TANL ) tasks. The proposed method is based on the idea of treating ARL as a translation task between two natural language augmented languages, where the task - relevant information can be easily extracted from the augmented language.    The main contributions of the paper are as follows :   1. The authors propose a method for translating between ARL and natural language in structured language ( structured prediction ) tasks such as nested named entity recognition, relation extraction, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. The main idea is to use the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time ( multi - task learning ). 2. They show that their approach can match or outperform task - specific models on all tasks, and in particular, achieves new state - of - the - art results on joint entity and relation extraction ( CoNLL04, ADE, NYT, and ACE2005 datasets ) and relation classification ( FewRel and TACRED ). 3. They demonstrate that their method can also significantly improve the performance in a low - resource regime, thanks to better use of label semantics."
SP:8f1b2fc6829e0bdfcc981020b0dcf3e63a947910,"This paper proposes a method for dealing with the unlabeled entity problem, where the entities of a sentence may not be fully annotated. The authors propose a general approach, which can almost eliminate the misguidance brought by unlabeling entities, by using negative sampling. The key idea is to use negative sampling that, to a large extent, avoids training NER models with unlabeledly annotated entities. The method is competitive with the state - of - the - art method 1 - State - Of - The - Art method 1. This method is applied to both synthetic and real - world datasets. Experiments on synthetic datasets ( e.g., CoNLL-2003 ) show that the proposed method significantly outperforms prior baselines. On real world datasets, the proposed approach seems to outperform prior methods.   The main contributions of the paper are as follows :   1 ) Identification of two causes of performance degradation of NER model : 1 ) reduction of annotated entity pairs, and 2 ) use of pre - trained language models in training. The first cause is less impactful than the second one and can be mitigated by adopting language models. The second cause seriously misguides a model in training and greatly affects its performances. 2 ) Adopting negative sampling to avoid pre - training of language models is a good idea. The proposed method seems to work well."
SP:dd76ece8d92a8a230a8b43033d8cb2368c677a94,"This paper proposes a novel method for embedding speech or text of arbitrary length into a vector space of reduced dimensions by adapting stochastic neighbor embedding ( SNE ) to sequential inputs. The Euclidean distance between coordinates in the embedding space reflects the phonetic confusability between their corresponding sequences. Two encoder neural networks are trained : an acoustic encoder that accepts speech signals in the form of frame - wise subword probabilities obtained from an acoustic model and a text encoder trained to accept text transcriptions. Compared to a triplet loss criterion, the proposed method is shown to have more effective gradients for neural network training and also gives more accurate results with low - dimensional embeddings when the two encoder networks are used in tandem in a word ( name recognition ) recognition task, and when the encoder network is used standalone in an approximate phonetic matching task. In particular, in an isolated name recognition task depending solely on Euclideans, the recognition accuracy is identical to that of conventional finite state transducer(FST)-based decoding using test data with up to 1 million names in the vocabulary and 40 dimensions in embedding."
SP:9142189126b8612ac0acee6fe18a0cfcb70b6545,"This paper proposes a reinforcement learning algorithm for stationary mean - field games, where the goal is to learn a pair of mean - state and stationary policy that constitutes the Nash equilibrium. The authors propose a fictitious play algorithm that alternatively updates the mean -field state and the policy via gradient - descent and proximal policy optimization, respectively. The algorithm is in stark contrast with previous literature which solves each single - agent reinforcement learning problem induced by the iterative updates of both mean and policy to the same set of quantities. Furthermore, the authors prove that the proposed play algorithm converges to the Nash - equilibrium at a sublinear rate. To the best of the knowledge, this seems to be the first provably convergent reinforcement learning method.   The main contributions of the paper are as follows :   1. A novel approach to learning the mean state of a stationary policy by iteratively updating it via gradient descent. This approach is referred to as “ iterative updating ” in the paper. 2. A method to learn the policy by gradient descent in the form of proximal optimization, where each policy update is conditioned on the previous policy update and the current mean state. 3. A new way of thinking about the policy update, called “ proximal updates ”, which is similar to the gradient descent approach in the previous work. 4. A way of treating the policy updates as updates of mean states, where instead of updating the mean states directly, the updates are based on the current state of the policy and the mean of the previous state."
SP:c498f8a199da1818fe64ed88b0825c5aad688aec,"This paper proposes a method for approximate probabilistic inference on the joint distribution defined by a normalizing flow model. The authors first show that this task is computationally hard for a large class of flow models and propose a framework motivated by this hardness that trains a new generative model with the property that its composition with the given model approximates the target conditional distribution. By parametrizing this new distribution as another flow model, the authors can efficiently train it using variational inference and also handle conditioning under arbitrary differentiable transformations. Since the resulting approximate posterior remains a flow, it offers exact likelihood evaluation, inversion, and efficient sampling.   The authors provide an extensive empirical evidence showcasing the flexibility of the method on a variety of applications to inverse problems. They also experimentally demonstrate that their approach is comparable to simple MCMC baselines in terms of sample quality. The invertibility of the pre - trained model gives us a tractable algorithm for learning a distribution in the latent space whose samples approximately match the true conditional when fed into the pre-trained model."
SP:1d0f27f61c9d32911b8bd15d6b82ef5eec644f0f,"This paper presents a new dataset for the cell segmentation research, the Ultra - High - Resolution Image Segmentation ( U - RISC ) dataset. This is the largest EM ( Electron Microscopy ) dataset with multiple iterative annotations and uncompressed high - resolution raw data. During the analysis process, the authors found that the current popular segmentation evaluation criteria are inconsistent with human perception, and proposed a new evaluation criterion called Perceptual Hausdorff Distance ( PHD ) to resolve this inconsistency. The authors also conducted experiments to verify the superiority of PHD over the conventional evaluation criteria.    The main contribution of this paper lies in the following two parts : ( 1 ) it established the largest, original image resolution - based EM dataset for training and testing, and ( 2 ) it proposed a human - perception based evaluation criterion, PHD, and verified it with experiments. The dataset we contributed and the PHD criterion we proposed may help researchers to gain insights into the difference between human perception and conventional evaluation metrics, thus motivate the further design of the segmentation method to catch up with the human performance on original EM images."
SP:8ca7aff87c82be69c9542550c814f52c9419ab0a,"Continual Learning ( CL ) is an approach to learning algorithms that aims to overcome the inability of the learner to recall how to perform tasks observed in the past. To this end, the authors propose a modular architecture whose modules represent atomic skills that can be composed to perform a certain task. In this paper, they first propose a new suite of benchmarks to probe the effectiveness of the proposed modular CL algorithms across these new axes, and then they introduce a new modular architecture, whose modules are composed by the composition of a handful of neural modules which can be either borrowed from past tasks or freshly trained on the new task. The experiments in §5 show that this model performs at least as well as state - of - the - art methods on standard benchmarks, and much better on more challenging benchmarks, exhibiting better transfer performance and ability to scale to streams with a hundred tasks.    The key issue is how to efficiently select modules, as the search space grows exponentially in the number of modules, and the authors address this problem by leveraging a data driven prior over the space of possible architectures, which allows only local perturbations around the architecture of the previous task whose features best solve the current task (§4.2 ). In the experiments in   §5, which employ a stricter and more realistic evaluation protocol whereby data from each task can be played multiple times, it is shown that the proposed model performs better on this and more challenging benchmark and exhibiting better performance on the more challenging baseline."
SP:cc819c61f408e88f247eb87946187ccec3dad32e,"This paper presents a method for generating synthetic meta - tasks using generative generative models for the unsupervised learning of few - shot classification tasks in the meta - learning setting. The proposed method, LASIUM, is based on the idea of generating in - class and out - of - class samples from the latent space in a principled way in order to create synthetic classes for training and validation data of a meta - task.   The authors propose a family of algorithms, LATINUM, that they claim outperforms or is competitive with current unsuper supervised learning baselines on the most widely used benchmark datasets in the setting of meta - classification tasks. The main contributions of the paper are as follows :   1. An approach to generate synthetic generative classes for the training of supervised meta - learners. The authors claim that this approach is more principled than previous approaches such as random selection, clustering, and augmentation. 2. They compare the proposed method with the baselines of two other methods for generating generative latent space samples for training the supervised meta learner. 3. They show that the proposed methods outperform the other methods in terms of performance on the benchmark datasets of Meta - Learning with Few - Shot Classification ( MNCC ) and Metapath."
SP:b25771e5c214a352f74ba6196fbd88bca6c43c98,"This paper studies injectivity of generative models in two settings : inverse problems and compressed sensing with generative priors. In inverse problems, injectivity is a well - posed property that allows one to obtain a solution arbitrarily well to a certain problem. Injective generators can be trained with a sample complexity that is polynomial in the image dimension. This paper shows injectivity with iid Gaussian matrices, a commonly used tractable model, requires larger expansivity between 3.4 and 10.5. It also shows that inverting an injective network via worst - case Lipschitz constants of the inverse is stable under certain conditions. In compressed sensing, injective generators with a generative prior N and a possibly nonlinear forward operator A injective on the range of N, we seek a latent code z such that A(N(z ) is close to some measured y = A(x ), which is well posed only when N is injective.    The authors first establish sharp characterizations of injectivity for fully connected and convolutional ReLU layers and networks. First, through a layerwise analysis, they show that an expansivity factor of two is necessary and sufficient for injectivity by constructing appropriate weight matrices. They then use arguments from differential topology to study injectivity   of deep networks. Finally, using an argument based on an argument on random projections, the authors show that any injective generator can be approximated with an end - to - end map of the dimensionality that is at least as large as injective ReLU network."
SP:a95a153d3fe9bcf535ebf8514f51d00df483f210,"This paper proposes a new continuous conditional generative adversarial network ( CcGAN ) for image generation conditional on regression labels. This is in contrast to the existing conditional GANs, which are mainly designed for categorical conditions ( e.g. class labels ). The motivation for this work is that conditioning on the regression labels is mathematically distinct and raises two fundamental problems : ( 1 ) Since there may be very few ( even zero ) real images for some regression labels, minimizing existing empirical versions of cGAN losses ( a.k.a empirical empirical GAN losses ) often fails in practice ; ( 2 ) Since regression labels are scalar and infinitely many, conventional label input methods (e.g., combining a hidden map of the generator / discriminator with a one - hot encoded label ) are not applicable. The proposed method incorporates a novel method to incorporate regression labels into the generator and the discriminator, which leads to two novel empirical discriminator losses, termed HVDL and SVDL, and a novel empirical generator loss, which is derived under mild assumptions. A new benchmark dataset, RC-49, is also proposed for generative image modeling conditional on regressions, as few benchmark datasets are suitable for the studied continuous scenario. Experiments are conducted on the Circular 2 - D Gaussians, RC - 49, and UTKFace datasets, which show that the proposed method is able to generate diverse, high - quality samples from the image distribution conditional on the given regression label. Moreover, in these experiments, the proposed network substantially outperforms cGAN both visually and quantitatively."
SP:10dd09ab315870631d1451d200f2c87a023f8226,"This paper proposes a novel active learning ( AL ) algorithm that aims to reduce the sample complexity of active learning by querying unlabeled instances to be annotated by a human - in - the - loop. The authors argue that under relatively strict settings, both SSL and AL can theoretically achieve the same performance of fully - supervised learning ( SL ) using far less labeled samples than SL. The proposed method is inspired by recent developments in DNN theory, namely the neural tangent kernel ( NTK ) ( Jacobot et al., 2018 ). The main contribution of this paper is to propose a novel query strategy which naturally blends in with SSL, and show that the proposed AL algorithm can rapidly achieve the high performance of SL using fewer labeled data. The algorithm is a natural fit to ASSL, and I hope this work can catalyze research combining AL and SSL as opposed to an exclusion of either."
SP:7f3947c3fa5b09674507d8f3e10d9280376ecb94,"This paper proposes a federated learning method for training neural network models, where the server orchestrates cooperation between a subset of randomly chosen devices in each round. They view Federated Learning problem primarily from a communication perspective and allow more device level computations to save transmission costs. They point out a fundamental dilemma, in that the minima of the local - device level empirical loss are inconsistent with those of the global empirical loss. Different from recent prior works, that either attempt inexact minimization or devices for parallelizing gradient computation, they propose a dynamic regularizer for each device at each round, so that in the limit the global and device solutions are aligned. They demonstrate both through empirical results on real and synthetic data as well as analytical results that their scheme leads to efficient training, in both convex and non - convex settings."
SP:a3fbb073b0e2371b20d5d9df6ab829673f90354f,"This paper proposes a method to speed up the training of contrastive learning algorithms by introducing additional intermediate contrastive losses to reduce the computational cost of gradient descent updates. The proposed approach is based on the observation that the similarity between the intermediate layers is a good surrogate of the final similarity between two pairs of images, and the similarity introduced by the proposed approach can be used to select easy regions for each image to filter. The authors empirically show that their method can save the training time with almost no loss on the final performance of the downstream tasks, e.g. ImageNet linear classification, PASCAL VOC object detection and segmentation. The method largely reduces the training cost by over 30 % and can serve as an alternative to standard self - supervised learning training pipeline if the computation resources are limited."
SP:5b5e705ea1ee1b857e17e64d560a39052804949d,"This paper studies the actor - critic algorithm, which is one of the most popular families of reinforcement learning algorithms. The authors focus on the single - time - horizon setting, where the actor and critic are updated at the same time using the Bellman evaluation operator. They also consider two non - linear approximation settings, where both the actors and the critic are represented by linear or deep neural networks. They prove that the actor sequence converges to a globally optimal policy at a sublinear O(K−1 / 2 ) rate, where K is the number of iterations.    The authors establish the rate of convergence and global optimality of single - timestep single - temporal policy updates for the algorithm under the broader scope of policy optimization with nonlinear function approximation. They show that for the first time, under linear function approximation, the actor network with deep neural network finds the globally optimally performing policy. For the second time, the authors prove that actor network finds globally optimal policies with sublinear rate for first time. In the third time, they show that the policy converges with a sub - linear rate for actor network."
SP:26705a4dc305cce336f657c5937d1f5b4209548a,"This paper proposes a new representation for embedding numerical and textual information in log data. The main idea is to represent the data at three levels of abstraction : field level, log level, and log sequence level. Each level is represented in vector format and can be computed from the previous level. The authors use Transformer Networks ( NTNs ) to encode numerical and text information that is suitable for log embeddings. They show how a number of log processing applications can be readily solved with their representation.   After suitable anonymization, they plan to release the data set to the research community."
SP:165c51a16f17fb8726e968f8b34742b62011d60e,"This paper proposes a formalism for understanding deep convolutional neural networks ( CNNs ) that is motivated by the similarities between trained CNN kernels and oriented Gabor filters for addressing this problem. The core idea is to constrain the behavior of CNNs by splitting them into a succession of wavelet packet decompositions, which are modulated by freely - trained mixture weights. The authors evaluate their approach with three variants of the wavelet decomposition with the AlexNet architecture for image classification as an example. The first variant relies on the separable wavelets while the other two implement the 2D dual - tree real and complex wavelets. Experiments show that they achieve the accuracy rate of standard AlexNet, but with a significantly lower number of parameters.   Although our design is based upon a different CNN architecture, i.e., AlexNet i.i.d. i.e.. AlexNet - Real, the authors take advantage of their feature extraction properties such as directional selectivity and shift invariance to derive DT - CWPT ( which is similar to DTCWPT but lacks orientation properties )."
SP:d0a284da462584724ba6a3a48c9e986d391233f6,"This paper proposes an attention mechanism for both the players and the coach for dealing with dynamic multi - agent teams. The attention mechanism is based on the observation mechanism used in real - world team sports, where the players have a partial view of the environment while the coach has a complete view. The coach coordinates the players by distributing individual individual strategies. The proposed attention mechanism has three components : 1 ) incorporate a variational objective to regularize learning, 2 ) design an adaptive communication method to let the coach decide when to communicate with different players, and 3 ) design adaptive communication strategy to minimize communication from the coach to the agents.   The authors apply their methods on resource - collection tasks in heterogeneous agents, and evaluate zero - shot generalization for new team compositions at test time. Results show comparable or even better performance against methods where players have full observation but no coach. Moreover, there is almost no performance degradation even when the coach communicates as little as 13 % of the time with the players. These results demonstrate the significance of a coach to coordinate players in dynamic teams."
SP:4eb662b527d556758aaa1a0b589495fcc337fad0,"This paper studies the effect of influence functions in neural network models trained on datasets such as Iris, MNIST, CIFAR-10 and ImageNet on the interpretability and uncertainty of group influences in machine learning applications. The authors focus on the setting of neural networks trained with non - convex loss functions. They show that for shallow networks, influence functions are fairly accurate, while for deeper networks the estimates are often erroneous. They also show that training with weight - decay regularization is important to get high - quality influence estimates, and that the accuracy of influence estimates can vary significantly depending on the examined test points. These results suggest that in general influence function in deep learning are fragile and call for developing improved influence estimation methods to mitigate these issues."
SP:5fea74a2031d097a99dacf613bedcb054b0c3831,"This paper considers the problem of understanding why autoregressive language models, pretrained using large text corpora to do well on next word prediction, do so well even with zero - shot usage. The paper proposes to formalize the intuitive connection between the pretraining task and text classification by considering the following questions :   1 ) What is the intuitive link between the pre - trained language model and the task of text classification?   2 ) How can we formalize this connection and quantify the benefit of language modeling for the downstream downstream task of classification? The paper hypothesizes and empirically verifies empirically, that text classification tasks of interest can be reformulated as sentence completion tasks, thus making language modeling meaningful pretraining.   3 ) Theorem 4.2 shows that low - dimensional softmax models can be used to solve sentence - completion tasks by leveraging a new tool, conditional mean features ( definition 4.1 ), which is shown to be effective in practice. 4 ) Experiments in Section 6 verify the sentence completion reformulation idea and the good performance of conditional mean feature on standard benchmarks. 5 ) In Section 5.2, the paper presents a new mathematically motivated objective ( Quad ) that has formal guarantees that has a lower bound on the error rate for Quad."
SP:a67da438e9821010284416170c3699ae7ff96c99,"This paper proposes a new method for detecting if data samples used to train a neural network model are obtained from unauthorized sources. The proposed method is based on the idea that some images are easy to classify while others are difficult to classify, and that the reconstruction error used to distinguish between easy and difficult images in training is less effective than the one used to detect the source of the data. The method proposes to use a novel difficulty score that can be computed for each image, and its computation does not require a training set. The main contribution of the paper is to propose a method to define a membership error that subtracts the difficulty score from the reconstructed error. This method is shown to achieve high accuracy on an extensive number of benchmarks demonstrating its effectiveness compared to strong baseline methods.   The main contributions of this paper are as follows : ( 1 ) A new method is proposed to detect if the training data is used to generate images that are easy for training and hard for classification, and ( 2 ) The method is compared to an existing method that uses reconstruction error to detect training data samples that are likely to be used for generating easy images and hard images that were never seen before. The results show that the proposed method achieves high accuracy due to overfitting to the training images and low reconstruction error due to reconstruction error alone."
SP:6fe23ebe09f2a4e42a21598f8e9c79edeca99863,"This paper proposes a differentiable architecture search method, DrNAS, by formulating it into a distribution learning problem, treating the continuously relaxed architecture mixing weight as random variables, modeled by Dirichlet distribution. With recently developed pathwise derivatives, the Dirich let parameters can be easily optimized with gradient - based optimizer in an end - to - end manner. This formulation improves the generalization ability and induces stochasticity that naturally encourages exploration in the search space. Furthermore, to alleviate the large memory consumption of differentiable NAS, we propose a simple yet effective progressive learning scheme that enables searching directly on large - scale tasks, eliminating the gap between search and evaluation phases. Extensive experiments demonstrate the effectiveness of DrNAS. DrNAS achieves an average error rate of 2.46 % on CIFAR - 10, 23.7 % for ImageNet under the mobile setting, and achieves state - of - the - art results on all three datasets."
SP:c590d0ed2487b42480b53fc077546a4a0bc27a78,"This paper proposes a new method for approximating functions of low - dimensional but complex functions, such as representing images as function of pixel coordinates, or solving differential equations, or representing signed distance functions or neural radiance fields. The proposed method, multiplicative filter networks ( MFNs ), avoids traditional compositional depth altogether, and simply multiply together ( linear functions of ) sinusoidal or Gabor wavelet functions applied to the input. The authors compare their approach on networks with comparable numbers of parameters to the exact benchmarks proposed in the SIREN and Fourier features papers. They show that MFNs achieve better performance deltas when increasing the depth or width of the networks. However, MFNs retain some notable advantages over MFNs, including a bias towards smoother regions in the represented function and its gradients."
SP:f5be855300f63c185a006834302bd4b033b56258,"Gradient - based meta - learning is a popular method to learn meta - models by gradient descent. In this paper, the authors propose a novel method to explore the inner loop of the gradient descent algorithm to explore long horizons of task - specific models. The inner loop consists of a student network ( i.e., a teacher network ) that first optimizes task specific models by an inner loop and then backpropagates meta - gradients through the loop to update the meta - model. The number of inner - loop optimization steps has to be small ( e.g., one step ) to avoid high - order derivatives, big memory footprints, and the risk of vanishing meta - Gradients. The key idea of the proposed method is an intuitive teacher scheme to enable the student network to adequately explore the search space of task-specific models by the teacher network. The teacher then takes a “leap toward the regions probed by the student ” toward a high - quality model at a particular region of the graph. The proposed method works well when applied to four meta-learning algorithms over three tasks : few - shot learning, long - tail classification, and meta - attack. Extensive results provide an affirmative answer to the first question above that long - horizon exploration in the inner loops improves a meta - learner ’s performance. The authors expect that future work can facilitate future work to address the second question above."
SP:0361e02d56b7d121cb5ede1cb582284cc18fc599,This paper proposes a new offline RL algorithm based on behavior regularization to tackle limitations of standard off - policy RL algorithms. The proposed algorithm BRAC+ uses an analytical upper bound on KL divergence as the behavior regularizer to reduce variance associated with sample based estimations. It also introduces gradient penalty term to the policy evaluation objective to penalize the gradient of the Q value w.r.t the out - of - distribution actions. Experiments on the D4RL benchmark ( Fu et al. 2020 ) show that the proposed algorithm outperforms existing model - free and model - based offline RL algorithms in various datasets.
SP:b2cfb380aa2a21f72f508b453cf5949257a5b4ec,"This paper introduces Adjoined networks as a training approach that can regularize and compress any CNN - based neural architecture. The proposed approach trains both the original and the smaller network together in a one - shot learning paradigm, where the parameters of the larger network are shared across both the architectures. The authors provide theoretical guarantees on the regularization behavior of the adjoint training paradigm and provide extensive empirical evaluation of both the compression and regularisation behavior of adjoint networks. For resnet - 50 trained adjointly on Imagenet, they are able to achieve a 13.7x reduction in the number of parameters1 and a 3x improvement in the inference time without any significant drop in accuracy. For the same architecture on CIFAR - 100, the authors show that the bigger network can achieve a 99.7% reduction in parameters and a 5x better inference time."
SP:dba40073f79143e5355d194aa16db9eee0267a5d,"This paper proposes a new exploration method, - greedy, for reinforcement learning ( RL ). The main contribution of the paper is to propose an algorithm that retains the simplicity of - greedy while reducing its dithering. The authors claim that the main limitation of greedy exploration is its lack of temporal persistence, which limits its ability to escape local optima. They propose a temporally extended form of greedy that simply repeats the sampled action for a random duration. It turns out that, for many duration distributions, this suffices to improve exploration on a large set of domains. Interestingly, a class of distributions inspired by ecological models of animal foraging behaviour yields particularly strong performance."
SP:5efb581a368ace3bd085d48801a899559d6a43ef,"This paper studies the implicit regularization of gradient descent with infinitesimal matrix factorization. It builds upon the work of Gunasekar et al. ( 2017 ), who conjectured that gradient descent converges to a norm - minimization regularization when the depth - 2 matrix factorisation is applied to the initialization of rank minimization algorithms. In this work, the authors show that the infinite factorization is mathematically equivalent to Greedy Low - Rank Learning ( GLRL ), a generalization of Greedy Rank Minimization Learning ( GFRL ) under some reasonable assumptions. The authors also extend the results to deep matrix factorsization, where they show that GFRL is equivalent to a deep version of GLRL, at least in the early stage of the optimization process. They also use this result to confirm the intuition achieved on toy models ( Gissin et al. 2020 ) that rank minimisation is more likely to happen in practice when the initialization is done on a larger scale.   The paper provides theoretical and empirical evidence to support their theoretical findings. The theoretical results are supported by experiments in Appendix E. The empirical results support the theoretical results."
SP:7f997cf7a63a7330fc12fd525516080c91a3cb9b,"This paper proposes a two - stage model patching approach for improving the robustness of classifiers to spurious subgroup differences in skin cancer classification. The approach is based on two stages. In the first stage, the classifier is trained with data augmentations that deliberately manipulate subgroup features within a class. The second stage is to train with subgroup consistency loss and robustness objective.   The method is applied to two datasets, the Cervical Skin Cancer Dataset ( CSCD ) and the Skin Cancer Genome Browser ( SCB ). The authors compare the effectiveness of the proposed approach against the state - of - the - art in terms of robustness to spurious bandage differences between subgroups. They compare their approach with two baselines, the CycleGAN - based approach, and show that the proposed method is more robust than the baselines on both datasets. They also show that their approach is better than the SOTA approach on the SCB and SCB datasets."
SP:de6cea1e35a0555175e17546a93422e9a96a511e,"This paper proposes a new classifier, named Rule - based Representation Learner ( RRL ), that automatically learns interpretable nonfuzzy rules for data representation. To train the non - differentiable RRL effectively, they project it to a continuous space and propose a novel training method, called Gradient Grafting, that can directly optimize the discrete model using gradient descent. They conduct experiments on 9 small data sets and 4 large data sets to validate the advantages of their model over other representative classification models. They show that RRL outperforms the competitive approaches, has low complexity close to the simple decision trees and is rational for its main technical contributions."
SP:e36388a9452e557dd51bf0170bf2f9da22271a49,"This paper proposes a new regret minimization algorithm, RGM, which is based on the invariant risk minimization ( IRM ) approach. IRM is an approach that allows for simultaneous optimality in terms of predictive regret and structured environments. The authors extend IRM to structured environments by recasting the optimality condition as a multi - modal optimization problem. The proposed RGM algorithm is applied to three tasks : molecular property prediction, protein homology prediction, and stability prediction. The experiments show that RGM significantly outperforms previous state - of - the - art methods in all three tasks."
SP:cad3ed2fba57faf17a3e8899dc5a744d5358aa68,"This paper proposes a novel architecture for cross - modal retrieval based on cross - attention in text - vision BERT. The proposed method is based on the two - tower architecture, where the image features are concatenated with the vision probes and the query's features and fed into the vision tower to generate the attended vision probes. Then, the attention is applied to the attended probes and text probes in a series of layers, which are only used in the upper few layers of the proposed method. The authors claim that this method is more efficient than BERT based methods for large - scale document retrieval. Experiments on two public benchmarks demonstrate the effectiveness and efficiency of their proposed method on both text - image and document retrieval tasks."
SP:51fd82de525fcb738fdeaeeae20fbb2cdf975f0c,"This paper proposes a new type of actor, named forward - looking actor ( FORK ), for Actor - Critic algorithms. The proposed FORK can be easily integrated into a model - free ActorCritic algorithm. Experiments on six Box2D and MuJoCo environments with continuous state and action spaces demonstrate significant performance improvement for FORK compared to the state - of - the - art algorithms. In particular, TD3 - FORK performs the best among the all the all we tested. For Ant - v3, it improves the average cumulative reward by more than 50 % than TD3, and achieves TD3’s best performance using only 35 % of training samples. Furthermore, a variation of TD -FORK can solve BipedalWalkerHardcore, a well known difficult environment, with as few as four hours using a single GPU."
SP:6e730239e6e8b43c4988dd61dca30f15dc039ef7,"This paper proposes a method to aggregate local models into a global model when users have non - i.i.d. data. The proposed method is based on Bayesian inference from the perspective of sampling higher - quality global models and combining them via Bayesian model ensemble, leading to much robust aggregation. The authors show that an effective model distribution can be constructed by simply fitting a Gaussian or Dirichlet distribution to the local models. The method is compatible with existing FL algorithms that regularize clients ’ learning or leverage server momentum. The empirical studies validate FEDBE ’s superior performance, especially when users ’ data are not i.d and when the neural network architecture goes deeper."
SP:3ac5f437fc349a33810d0645664d1c448528af74,"This paper presents a paper under double - blind review of BERT. The paper focuses on the problem of classification in the era of deep learning and machine learning. The main contributions of the paper are as follows. First, the paper presents the paper under the assumption that BERT is a two - stage process.    - The paper first presents a summary of the key elements of the BERT framework. This includes the following : - The core idea of the method is to train a classifier that predicts the probability of a given question in a given text. This is done by training a large classifier trained on a large corpus of text. - The key idea of this paper is to use this classifier to predict the probability at each step of the training procedure. - Then the paper goes on to train the second classifier based on the previous classifier. The second set of trainings is based on a slightly modified version of the first set. - In the second set, the trainings are repeated until the classifier reaches its theoretical limit. - Finally, the test set is used for the third and final time for the paper to be submitted to the ACM for approval."
SP:efa2343ead47263a0d09e1c17f9aa044605b9650,"This paper provides a priori analysis of the settling time of deep neural networks based on the Lyapunov - based analysis of loss function. The setting is defined as a deterministic control problem where the weights of the network are control inputs and learning translates into a tracking problem. The authors formulate the supervised learning framework as a control problem with a single neuron case and a multi - layer perceptron case for regression. They derive an analytical formula for finite - time upper bound on settling time by means of the priori under the assumptions of boundedness of input. Finally, they prove that their loss function is robust against input perturbations.   The paper is organized as follows :   ( 1 ) In Section 1, the authors state the equations to compute upper bounds on the convergence time for training neural networks. In Section 2, the equations are applied to the case when bounded perturbation are admitted at the input and convergence guarantees are shown to hold true. Section 3, some numerical simulations are presented for both single neuron and multi - Layer perceptron cases for regression, and Section 4 collects conclusions and discusses future scope."
SP:7a0ded4b3b2d08d43765ff7b722da9b9863aabd6,"This paper considers the problem of disentanglement of representation learning in machine learning. It builds upon the work of Sorrenson et al. ( 2020 ), who showed that general incompressible flow networks ( GIN ) can recover the underlying latent variables that generate the data, and thus can provide a compact and disentangled representation. However, in this paper, they point out that the method taken by GIN for informative latent variables selection is not theoretically supported and can be disproved by experiments on synthetic data generated following the theoretical assumptions. They propose to use the mutual information between each learned latent variable and the auxiliary variable to correctly identify informative latent variable selection criteria for different informative variables selection criteria on classification, outlier detection, and defence against adversarial attacks. They also discuss some other interesting discoveries related to the framework of nonlinear ICA. And finally, they conclude their results and summarise some future working directions in the last section."
SP:0d9ba12bbf47b13a46c2225f9dc06878418daaea,"This paper proposes a new pooling method based on the Lifting Scheme from signal processing for image classification in convolutional neural networks. The proposed method consists of two pooling layers, namely LiftDownPool and LiftUpPool. The former decomposes a feature map into various downsized sub -bands, each of which contains information with different frequencies, and the latter is used to generate a refined upsampling feature map using the detail sub - bands. The method is evaluated on image classification and semantic segmentation with various ConvNet backbones. The results show that the proposed method outperforms baselines in terms of robustness to corruptions and perturbations of inputs."
SP:147239edceb17bade6ea5d3dca44e3a59998aa47,"This paper proposes a new embedding method for high - dimensional data. The embedding is based on the Johnson - Lindenstrauss ( JL ) lemma, which states that for every finite set T, there exists a mapping m = O( O(log(|T | ) ) such that all pairwise distances are preserved up to an -Lipschitz distortion. The authors show that Euclidean distances among the elements of T are approximated by the ` 1 norm on the images of {±1 } under a fast linear transformation. This contrasts with standard methods, where the Hamming distance is used instead of the norm in the embedding. The proposed method is both fast and memory efficient, with time complexity O(n log n ), which is compared to the time complexity of O(m ) with respect to the number of data points. The length of the binary codes required to achieve the desired accuracy is quite small, and we show it can even be compressed further without compromising the accuracy. To illustrate the results, the authors test the proposed method on natural images and show that it achieves strong performance."
SP:f65e229bca3904095743e7a501b1083cc60f1e22,"This paper proposes a biologically plausible explanation for the generalizability and robustness of artificial neural networks ( ANNs ). The authors hypothesize that the plasticity rules in the brains of neural networks can be learned by applying the Gradient Descent ( GD ) method. They argue that applying GD to learning rules is biologically plausible, in the sense that it is plausible that it can be learn over evolutionary time. They provide both empirical and theoretical evidence for this hypothesis. In their experiments, they show that plasticity rule for the synaptic weights of RNNs are learned through GD and are found to perform reasonably well ( with no backpropagation ). In the special case of the last layer of a classification network, the authors show analytically that GD recovers the Perceptron algorithm and the Multiplicative Weights ( or Winnow ) algorithm and improves upon the perceptron algorithm. Moreover, the classifiers learned with GD exhibit surprising levels of tolerance to adversarial perturbations.   The authors then experimentally show that learning more complex plasticityrules in a general RNN, establishing that learning Plasticity rules leads to performance that is quite good. Even though the performance is not at the same level as ANNs, our experiential result here is that the robustness appears to increase significantly with the depth ( number of rounds ) of the RNN."
SP:f435530146fa975cb27cd375a857df9bcbd87682,"This paper proposes a new method for visual question generation ( VQG ) based on Graph - to - Graph learning. The main idea of the method is to generate visual questions with answer - awareness and region - reference. In particular, they aim to ask the right visual question with Double Hints textual answers and visual regions of interest. To this end, they develop a simple methodology to self - learn the visual hints without introducing any additional human annotations. Furthermore, to capture these sophisticated relationships, they propose a new double - hint guided Graph -to - Sequence learning framework that first models them as a dynamic graph and learns the implicit topology, and then utilize a graph to - sequence model to generate the questions with double hints. The experiments on VQA2.0 and COCO - QA datasets demonstrate that their proposed model on this new setting can significantly outperform existing state - of - the - art baselines by a large margin."
SP:53a26ce11647866d3f6ba8b84ca9f13106197a8d,"This paper investigates the phenomenon of “ Double Descent ”, where neural networks exhibit non - monotonic test performance in quantities such as the sample size and the model size. Double descent is a phenomenon that occurs when two general networks, i.e., linear regression and non - linear regression to neural networks, behave differently in terms of the distribution of data and the number of neurons. This phenomenon is termed as “ double descent ” and has raised questions about generalization and whether we need to re - think our current understanding of generalization. This paper investigates whether optimally - tuned `2 regularization ’s can mitigate double descent for more general models, including neural networks. The authors prove empirically that optimally-tuned `2 ’ regularization can mitigate the double descent phenomenon for both linear and non linear networks. They also demonstrate empirically, empirically and theoretically, optimally tuning `2 can improve the performance of neural networks that do not exhibit double descent.   "
SP:c193ccc74b987beaf8d53a29a8529a0af5e87742,"The authors introduce a novel neural network for building image generators ( image generators ) and apply it to variational autoencoders ( VAEs ). The feature maps at each level of a deep neural net are computed in a spatially coherent way, using a sequential gating - based mechanism that distributes contextual information across 2 - D space. They show that augmenting the decoder of a hierarchical VAE by spatial dependency layers considerably improves density estimation over baseline convolutional architectures and the state - of - the - art among the models within the same class. Furthermore, they demonstrate that SDN can be applied to large images by synthesizing samples of high quality and coherence. In a vanilla VAE setting, they find that a powerful SDN decoder also improves learning disentangled representations. The authors also indicate indicating that neural architectures play an important role in this task."
SP:db91512a90e75675af03c2f197751c8526d6f5e9,"This paper presents EMaQ, an off - policy reinforcement learning method for offline RL, which is based on BCQ ( Fu et al., 2018a, 2018 ), a prior method in the offline RL literature. The main contribution of the paper is the introduction of a novel algorithm, Expected - Max - Q - learning ( EM aQ ), which aims to improve upon the performance of BCQ by restricting extracted policies to remain exactly within the support of a given behavior policy. To achieve this goal, the authors propose a new operator, Ex - Max Q - Learning ( ExQ - LE ), through which the learned policies are estimated to remain close to the given dataset of interactions. Empirical results show that EMaq outperforms the state - of - the - art in the D4RL benchmarks, and in the online RL setting, it is competitive with Soft Actor Critic ( SAC ). The key contributions are demonstrating the importance of the careful generative model design for estimating behavior policies, and an intuitive notion of complexity of offline RL problems."
SP:e2b80adeaa9208e0667a64a3f24661f77b48e487,"This paper proposes a batch - based optimizer for batch selection to improve model fairness in the context of data preprocessing and model training. The proposed algorithm, called FairBatch, is based on a bilevel optimization approach, where the inner optimizer is the standard batch selection algorithm and the outer optimizer optimizes the batch size. The authors propose three fairness measures : equal opportunity, equalized odds, and demographic parity. The empirical results show that the proposed algorithm outperforms other batch optimizers ( e.g., BatchOpt, BilevelOpt ) in terms of equal opportunity and equal odds. The paper also shows that the batch optimizer improves the performance of the pre - trained model compared to the state - of - the - arts model."
SP:72f26b850bb2258223c0fc71598e35ad07d690e6,"This paper studies the bounds on the Lipschitz constants of monDEQs, a recently proposed class of deep neural networks that can be viewed as representing an infinitely - deep network. The authors show that the bounds are bounded as a simple function of the strong monotonicity parameter of the network. They also highlight how to use these bounds to develop PAC - Bayes generalization bounds that do not depend on the depth of the neural network, and avoid the exponential depth dependence of comparable DNN bounds. The bounds are small relative to those of DNNs on MNIST and CIFAR-10, and are comparable with bounds on DNNS of around depth 5.    The main contributions of the paper are as follows :   1. This paper shows that the mon DEQs have bounds that are as tight as those of mon CNNs computed by AutoLip and SeqLip ( Virmaux & Scaman, 2018 ), which are the only existing methods for bounding CNN Lipshitz constants. 2. This shows that these bounds can be used to provide robustness guarantees, which is needed to characterize the smoothness of decision boundaries, and 3. These bounds are tight relative to the bounds of other methods for generalization of CNNs. 3. This is demonstrated by empirically demonstrating the robustness of the bounds to adversarial attacks on the MNIST dataset."
SP:bcfd4d7fd4590e3bc248a0a5422ce4b67db74a74,"This paper considers two settings : imitation learning and goal - conditioned reinforcement learning. In imitation learning, the objective is to match the expert ’s state - action distribution with that of the agent. This is achieved by sampling states that the agent is currently not likely to visit and using a value - function to guide the agent towards those states. The value function and density estimate are trained using self - supervised roll - out alone. The authors show in Sec. 4 that this minimizes the KL divergence between the expert’s distribution and the agent ‘s state action distributions and therefore provides an intuitive and principled imitation learning approach. In Sec. 5 they show that VDI uses demonstrations significantly more efficiently than the current state - of - the - art in common benchmarks. They also propose to extend UVD to match a distribution of expert states in the imitation learning setting."
SP:d57550b2f323b356d7e609acc35ee33039f376b4,"This paper proposes a variational multi - task learning framework, called VMTL, for the task - related learning problem. It is based on the idea that when training data is limited, it is important to leverage the relatedness between tasks in order to improve the overall performance of the model. To this end, the paper proposes to condition the prior of each task on related tasks by using Gumbel - softmax priors. Each prior is represented as a mixture of variational posteriors of other related tasks and the mixing weights are learned in a data - driven manner for each individual task. The posteriors over representations and classifiers are inferred jointly for all tasks and individual tasks are able to improve their performance by using the shared knowledge from other tasks. The paper provides extensive evaluation on four benchmark datasets, where it consistently outperforms previous methods in terms of the average accuracy of all tasks, and it achieves state - of - the - art performance."
SP:3ccdf8322f16c8a7bef82e32fad4c03969a510d1,"This paper proposes Long - Range Arena ( LRA ), a framework for evaluating the model quality of efficient long - range Transformer models. The authors claim that there is no well - established consensus on how to evaluate this class of models and that inconsistent benchmarking on a wide spectrum of datasets and datasets makes it difficult to assess relative model quality amongst many models. To this end, this paper proposes a systematic and unified benchmark, Long - range Arena, specifically focused on evaluating model quality under long - context scenarios. The proposed LRA consists of a suite of tasks consisting of sequences ranging from 1K to 16K tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural similarity, and visual - spatial reasoning.   The authors provide the following summary of their work :   ( 1 ) The authors propose a unified framework for enabling easy side - by - side comparisons of efficient Transformer model and broadly speaking, long range sequence models in general ; ( 2 ) The framework, which they plan to open source, is written in JAX/FLAX1 ; ( 3 ) The method is based on JAX / FLAX1. ( 4 ) It is claimed that the proposed framework, LRA, facilitates more research in this direction, and presents new challenging tasks to tackle, and that it is open source."
SP:e12e410c3335b76133ceda4c865b244fbbab8580,"This paper proposes a method for combining source code representation learning and code summarization based on the Structure and Context of source code. The source code is decomposed into source code, source code context, and source code structure, which are two complementary representations of the same computer program, i.e. source code source code and its parsed abstract syntax tree ( AST; Structure ), and its context and structure tree ( Context and Structure ; Structure + source code ). The authors train a single language - agnostic model on all five programming languages considered in this work, and jointly training on non - parallel data from multiple programming languages improves results on all individual languages, where the strongest gains are on low - resource languages. They also show that multilingual training only from Context does not lead to the same improvements, highlighting the benefits of combining structure and context learning on code."
SP:f46e98d48f90071831f1c0069bf74a7993be6db8,"This paper proposes a new approach for audio - visual navigation in 3D environments, where the goal is to find a sound source in an unmapped 3D space using both sights and sounds. The proposed approach is based on two components : ( 1 ) waypoints that are dynamically set and learned end - to - end within the navigation policy, and ( 2 ) an acoustic memory that provides a structured, spatially grounded record of what the agent has heard as it moves in the environment. The waypoints are learned via a reinforcement learning approach, and the acoustic memory is learned using an end to end learning policy. The authors also propose a waypoint - based approach to learn the waypoints. The experimental results show that the proposed approach outperforms the state - of - the - art on the Replica and Matterport3D datasets, and generalizes much better to the challenging cases of unheard sounds and noisy audio."
SP:23bfe317dcef00a91ea92389b3f39d9b93972454,"This paper studies the convergence of neural networks trained on the two - dimensional cellular automaton Conway's Game of Life. The authors find that networks trained for the task rarely converge to a solution based on the lottery ticket hypothesis, which states that neural networks rely on lucky random initial weights of subnetworks called “ lottery tickets ” that converge quickly to solutions based on lottery tickets. To investigate this hypothesis, the authors train CNNs on the Conway game and find that they require substantially more parameters to converge than other networks. They also find that the parameters that are sensitive to small perturbations are the same as those sensitive to a single sign change. Finally, they observe a critical value d0 such that training minimal networks with examples in which the cells are with probability d0 dramatically increases the chance of convergence to solution.    The main contributions of the paper are as follows :   1 ) The authors study the convergence rate of CNNs trained on two - dimensionality - based CNNs. They find that it does not converge as quickly as other networks on the task. 2 ) The parameters of the parameters sensitive to the perturbation are similar to those of other networks in that they tend to be more sensitive than the ones that do not have them. 3 ) The results are consistent with previous work that suggests that lottery tickets are lucky initial weights that lead to converging solutions."
SP:1b5ba618d3e28d48f9205c0780f8288a08fa5392,"This paper proposes a new objective function, BatchMeanTripleT loss, for Semi - Supervised Learning ( SSL ). The objective is based on the observation that the inputs having the same label should have the similar model outputs. Based on this observation, the authors propose a novel method, RankingMatch, that considers not only the perturbed inputs but also the similarity among the inputs that have same label. The authors claim that this new objective has the advantage of computational efficiency while taking into account all input samples. They also perform an ablation study to prove the efficacy of the proposed objective against existing versions of Triplet loss.   The main contributions of the paper are as follows :    1. Introducing a novel objective function to measure the consistency of the output of the semi - supervised learning method based on consistency regularization, which encourages the model to produce unchanged with perturbed input. 2. Empirically, the method achieves state - of - the - art performance across many standard SSL benchmarks with a variety of labeled data amounts, including 95.13% accuracy on CIFAR-10 with a labeled data amount of 95, 97.76 % accuracy on SVHN with 250 labels, and 97.77% accuracy with 1000 labels. 3. The method is simple yet effective, achieving high accuracy on many standardSSL benchmarks with various unlabeled data amounts."
SP:f3abccf4a2566ffbc821aba209fab15058639ad4,"This paper considers the problem of learning new tasks from a small fixed number of examples, by meta - learning across static data from a set of previous tasks, in an online incremental learning setting. The authors propose a meta - training algorithm that can adapt to variable amounts of data, and combine it with deep neural networks for effective online learning on challenging sequential problem settings. They find that their approach can outperform empirical risk minimization and a previous online meta learning method ( Finn et al. 2019 ) on two online image classification problems consisting of sequences of classification tasks and one online regression problem. Further, in the offline setting, their approach performs comparably to previous state - of - the - art algorithms in few - shot learning, and provides considerable gains in the variable - shot setting. These results suggest that meta -learning is an important ingredient for building learning systems that continuously learn and improve over a sequence of problems."
SP:95cb420d92ec42e12a4bbb0e66224f1c498a7161,"This paper presents a series of experiments designed to test the sensitivity of Transformer representations to syntactic structure in sentences. The authors use three different perturbations : ( 1 ) random permutation of random permutations of n -grams, ( 2 ) swapping two words in a sentence and comparing representations from perturbed sentences against the original, ( 3 ) swapping of two adjacent words which do or do not break apart a syntactic phrase, to test sensitivity to global phrase structure, and ( 4 ) testing sensitivity to local phrase structure. Results from three probes collectively suggest that Transformers build sensitivity to larger parts of the sentence along their layers, and that hierarchical phrase structure plays a role in this process. In particular, sensitivity to Local phrase structure increases along deeper layers. Based on an analysis of attention, the authors show that this is at least partly explained by generally larger attention weights between syntactically distant words. The interventional tools such as controlled input perturbation can be useful for analyzing deep networks.  "
SP:cb27b27a6fefc192ad1c2bd083d13eb9e51a5c44,"This paper presents a method for training GANs on few - shot image synthesis tasks with minimum computing cost. The authors propose a light - weight GAN structure that gains superior quality on high - resolution images. The model converges from scratch with just a few hours of training on a single GPU, and has a consistent performance even with less than 100 training samples. The proposed method consists of a skip - layer channel - wise excitation module and a self - supervised discriminator trained as a feature - encoder.   The authors compare their method with the state - of - the - art StyleGAN2, when data and computing budget are limited, on 13 datasets covering a wide variety of image domains 1. They show their model ’s superior performance compared to the state-of-the - art state GAN2, which has been shown to be an effective method to stabilize the GAN training 2. However, the auxiliary self - supervision tasks in prior works have limited using scenario and image domain. Moreover, prior works only studied on low resolution images (322 to 1282 ), and without a computing resource limitation."
SP:c0dbeb5d94b2388595cf7ad9675c55df0bac7f8e,"This paper proposes a novel dual - based method for neural network bounding. The authors argue that the current state - of - the - art dual - solver approaches for bounding neural networks are often too loose, making it difficult to verify more challenging properties of the network. They propose a novel method that recovers the strengths of the new relaxation in the dual space : tightness and a linear separation oracle. The proposed method shares the benefits of previous dual approaches for weaker relaxations and is competitive with primal approaches ( Anderson et al. 2020 ) and previous dual algorithms. The main contributions of the paper are as follows :   ( 1 ) The authors propose a dual solver that combines a linear LP relaxation with a tighter formulation. This allows for tighter bounds if the computational budget of the proposed method is larger than $ \ell_1 $ for piecewise linear activations. ( 2 ) They show that their method recovers the speed - accuracy trade - off of the looser dual solvers of the previous approaches ( Dvijotham et. al. 2018 ), which is crucial for efficient bounding in neural network bounds. ( 3 ) The method is flexible and can be applied to both linear and non - linear relaxations, which allows for more flexibility. ( 4 ) They compare their method with the baselines of prior works."
SP:56e3837417dbcce0d65338dc3aac4e1a20eb0df8,"This paper studies the problem of improving the performance of pre - trained language models ( PTLMs ) on NLU and NLG tasks. It is well - known that many tasks require relational commonsense knowledge about everyday concepts, which is crucial to many downstream tasks that need common sense to understand or generate or generate natural language. To address this problem, the authors propose two objectives : ( 1 ) generative ) objectives for learning common sense from text, and ( 2 ) contrastive ) objectives to learn contrastive concepts from text. The generative and contrastive objectives can be used as intermediate self - supervised learning tasks for incrementally pre - training PTLM before fine - tuning on downstream datasets. The authors develop a joint pre - pretraining framework to unify the generative / contrastive learning objectives and use them as intermediate learning tasks to improve the performance on both NLU & NLG. The proposed method, called Concept - Aware Language Model ( CALM ), is evaluated on four commonsense - related NLU datasets ( COMMONSENSEQA, OPENBOOKQA, PIQA and ANLI ) and a commonsense related NLG dataset ( COMMONGEN ), on which it consistently outperforms baseline methods by a consistent margin. The results and careful ablation studies demonstrate the potential of CALM to serve as a “ plug - and - play ” method for any pre -trained text - to - text-to - text transformer before fine-tuning on common sense - related tasks."
SP:7ec69bdee021af506293c87a3b75bce1c40a03d7,"This paper presents an unsupervised method for learning 2D segmentations of 2D objects from 2D video. The goal is to explore how physics, especially object interactions, facilitates disentangling of 3D geometry and position of objects from video. To do this, the paper proposes a new method, POD - Net, which uses both multi - scale pixel cues and physical motion cues to accurately segment observable and observable and partially observable objects of varying sizes, and infer properties of those objects. The method is tested on both synthetic and real scenes. The experiments show that the method is able to learn segmentations and representations that are useful for reasoning about physical events."
SP:66997bc19a3ba6548fcf21f114e748bea95cad1c,"This paper proposes a novel training method, Increasing Margin Adversarial ( IMA ) Training, to improve the robustness of deep neural networks ( DNNs ) against adversarial noises. The main idea is to increase the margins of training samples by moving the decision boundaries of the DNN model far away from the training samples to improve robustness. The IMA method is evaluated on six publicly available datasets ( including a COVID - 19 CT image dataset ) under strong 100 - PGD white - box adversarial attacks and the results show that the proposed method significantly improved classification accuracy on noisy data while keeping a relatively high accuracy on clean data."
SP:276ffd59fbf49e3ee02756da8920218102214917,"This paper proposes a new method for knowledge distillation, ProKT, which is based on the observation that a converged heavy teacher model is strongly constrained for learning a compact student network, which could make the optimization subject to poor local optima. The proposed method could be less sensitive with the quirks during optimization which could result in a better local optimization. The authors propose a new model -agnostic method by projecting the supervision signals of a teacher model into the student’s parameter space. Such projection is implemented by decomposing the training objective into local intermediate targets with approximate mirror descent technique. Experiments on both image and text datasets show that the proposed ProKT consistently achieves the state - of - the - art performance compared to several strong baselines."
SP:906dc21d6988953fcf57d63bbdd12973e5818d16,"This paper proposes a channel pruning method to solve the compression and acceleration of CNNs. The main idea is to use a hyper - structure network to generate the architecture of the main network, which can be optimized by regular backpropagation. The authors use a regularization term to specify the computational resource of the compact network. Experiments on CIFAR-10 and ImageNet show that the proposed method outperforms both conventional channel pruned methods and AutoML based pruning methods on ResNet and MobileNetV2."
SP:890fd9454596c051b0e9535baf73b1dd1fae67ca,"This paper proposes a method to train a theorem prover trained on a large knowledge base of proofs without learning from human proof. The method, dubbed DeepHOL Zero, is based on the exploration of premises based on a simple tf - idf ( term frequency - inverse document frequency ) based lookup in a deep reinforcement learning scenario. Experiments show that the method trained with this exploration mechanism but no human proofs outperforms provers that are trained only on human proofs.   The main contributions of the paper are as follows :   1. This paper proposes an exploration mechanism to train theorem provers without human proof in the presence of a large set of proofs. This is achieved by augmenting the exploration mechanism of a neural network with a simple lookup of premises using a reinforcement learning algorithm. The algorithm is trained using a combination of imitation and reinforcement learning. The authors perform multiple experiments to understand the importance of the underlying assumptions that make their exploration approach work, thus explaining their design choices. 2. They thereby solve one of the road blocks on the way to open - ended learning of mathematical reasoning in large theories. 3. They show that their method outperforms the state - of - the - art by a large margin."
SP:88209417a8ad07e6103084e41709be900303ce5f,"This paper proposes MODALS ( Modalityagnostic Automated Data Augmentation in Latent Space ), an approach to augment data for any modality in the latent space in a generic way. The idea is to fine - tune four universal data transformation operations to adapt the transform to data of different modalities. These operations are image processing, tabular processing, time - series, image modality adaptation, and latent space - based text augmentation. The method is tested on four datasets : MNIST, CIFAR-10, MNIST-2, and Fashion - MNIST. The experiments show that MODALS outperforms the other data augmentation methods on all four datasets."
SP:6d84670d321b0d584b097c630574bd748e85c9a2,"This paper studies the optimization of neural networks when there are more than two layers in the so - called mean field regime, a regime in which the number of layers in a neural network tends to infinity, and the learning dynamics tends to a nonlinear and nontrivial dynamical limit. This lends a way to study large - width neural networks via analyzing the mean field limit. Recent works have successfully applied such analysis to two - layer networks and provided global convergence guarantees. The extension to multilayer ones however has been a highly challenging puzzle, and little is known about the optimization efficiency in the mean - field regime when there is more than 2 layers. In this work, the authors prove a global convergence result for unregularized feedforward three - layer neural networks in the means field regime. To do so, they first develop a rigorous framework to establish the limit of 3 - layer network under stochastic gradient descent training. To that end, they propose the idea of a neuronal embedding, which comprises of a fixed probability space that comprises of neurons embedded in a fixed fixed network. The authors then used this embedding to prove global convergence guarantee under suitable regularity and convergence mode assumptions, which, unlike previous works, does not rely critically on convexity. Underlying the result is a universal approximation property, natural of neural network, which importantly is shown to hold at any finite training time ( not necessarily at convergence ) via an algebraic topology argument."
SP:b90f893f927db9c439595fd119a565cf43c971f4,"This paper proposes learning explanations of expert decisions by modeling their reward function in terms of preferences with respect to “ what if ” outcomes : Given the current history of observations, what would happen if we took a particular action? to learn these costbenefit tradeoffs associated with the expert ’s actions, the authors integrate counterfactual reasoning into batch inverse reinforcement learning. This offers a principled way of defining reward functions and explaining expert behavior, and also satisfies the constraints of real - world decision - making, where active experimentation is often impossible. The authors conduct experiments in both real and simulated medical environments, where they highlight the effectiveness of their batch - based learning approach in recovering accurate and interpretable descriptions of behavior. Additionally, they demonstrate the cold - start problem typical of conventional batch IRL solutions but also accommodates settings where the usual assumption of full observability fails to hold."
SP:c92916780418bfa7f0796fd9766b6d28b9eea5ef,"This paper presents a series of ablations on existing methods that use morphological information from graph neural networks ( GNNs ) to improve the performance of the proposed AMORPHEUS agent in continuous MTRL continuous control. Specifically, the authors focus on two settings : ( 1 ) environments where the state and action space dimensions of the agent are the same across tasks, and ( 2 ) environments in which the agent exhibits nontrivial behaviour such as cyclic attention patterns coordinated with gaits. In the first setting, they show that GNN - based methods ( e.g., Graph Neural Networks ) do not improve performance compared to the proposed method ( AMOR PHEUS ). In contrast, they propose a transformer based method ( transformer - based approach ) that uses limb features as node labels and edges to connect the nodes if their corresponded limbs are physically connected. The proposed method, called Transformer - based Approach to Graph - based Continual Learning ( TRACENUS ), is shown to outperform the GNN based methods in terms of sample efficiency and final performance. In addition, it is shown that it also outperforms GNN- based methods that do not use the information in the graph structure to define the message - passing scheme."
SP:2cf58f5cac20dccdc2034ef60e8e46b7988ebd7d,"This paper proposes a new method for visual counting based on modulated convolutions based on residual bottleneck. The method is named MoVie, short for Modulated Convolutional Bottlenecks, which aims to predict the number of occurrences given a natural image and a query ( e.g., a question or a category ). Unlike most prior works that use explicit symbolic models which can be computationally expensive and limited in generalization, the proposed method proposes a simple and effective alternative by revisiting modulated Convolutions that fuse the query and the image locally. The proposed method is validated on CLEVR, GQA, and COCO, where it is shown to outperform the state - of - the - art on counting - specific VQA tasks while being more efficient. It is also shown to be competitive on the more difficult benchmarks like GOCO and COUNTING, which are difficult benchmarks for visual reasoning. Finally, the authors also show evidence that Modulated Convolutional Probability Estimation ( MODE ) can be used for reasoning tasks beyond counting."
SP:c64e77507e562f236cb69361b22fb1a7951ffb22,"This paper presents a model - targeted poisoning attack that aims to induce a classifier that is closer to the target classifier than the one that is closest to the source. The authors provide a lower bound on the minimum number of poisoning points needed to reach the classifier induced by the proposed attack, which is based on online convex optimization. They prove that the poisoning attack converges to the model induced by training on the target model as the number of poison points increases given that the loss function is convex and proper regularization is adopted in training ( Theorem 4.1 ). Previous model - targeted attacks lack of such convergence guarantees, and the authors compare their attack to the state - of - the - art model - targeting attack ( Koh et al. 2018 ), which works in an online fashion and can incrementally find poisoning points that are nearly optimal. They find that their attack is better than the best known attack in terms of the success rate of the attack success rate and the intrinsic hardness of attacking different targets."
SP:a526023ec4cb839b83c574d31f59a9a67bc7af00,"This paper presents BiPointNet, the first model binarization approach for efficient deep learning on point clouds. The main motivation is to alleviate the resource constraint for real - time point cloud applications that run on edge devices. The paper introduces two methods : EMA and LSR to modulate the distribution before aggregation for maximum information entropy, and layer - wise scale recovery ( LSR ) to efficiently restore feature representation capacity. Extensive experiments show that the proposed method outperforms other binarized methods by convincing margins, at the level comparable with the full precision counterpart. Moreover, extensive experiments on multiple fundamental tasks, such as classification, semantic segmentation, and semantic segmentations, highlight that our method is task - agnostic."
SP:825b4d1db0c537a607655bb5b4bf221ec672c8af,"This paper presents three Transformer - based models, MemTransformer, MemCtrl and MemBottleneck, for the task of machine translation and language modelling respectively. The Transformer models are based on the Transformer baseline, which is a self - attention architecture that combines information from all elements of a sequence into context - aware representations. The authors argue that adding trainable memory to selectively store global as well as local representations of a sequences is a promising direction to improve the performance of these models. The paper proposes to augment the transformer baseline by adding memory tokens to store global information, ( 2 ) controlling memory update with dedicated layer and ( 3 ) controlling the attention pattern over the memory tokens. Experiments on GLUE benchmark show that the presence of memory positively correlates with the model performance for machine translation, language modelling and language modeling tasks. However, experiments on the masked language model show mixed results for memory tokens with memory update."
SP:f0fa1b7684bc605f6edd4813c44be20988fe8b4c,"This paper presents Prototypical Contrastive Learning ( PCL ), an unsupervised representation learning method that bridges contrastive learning with clustering. PCL learns low - level features for the task of instance discrimination, and more importantly, it encodes semantic structures discovered by clustering into the learned embedding space. Specifically, it introduces prototypes as latent variables to help find the maximum - likelihood estimation of the network parameters in an Expectation - Maximization framework.   The authors iteratively perform E - step as finding the distribution of prototypes via clustering and M -step as optimizing the network via contrastive training. They propose ProtoNCE loss, a generalized version of the InfoNCE Loss for contrastive learners, which encourages representations to be closer to their assigned prototypes. The authors also show that PCL also leads to better clustering results, and compare it to state - of - the - art methods on multiple benchmarks with substantial improvement in low - resource transfer learning."
SP:5342a5e1d87fd17b1a2efed967dbbfeafa440ee7,"The paper proposes a novel method to defend neural networks from adversarial attacks. The proposed method is based on the Orthogonal Multi - Path ( OMP ) block. The OMP block consists of a block containing multiple paths that are orthogonal with each other. The parameters of these paths are required to be orthogonally close to each other in order for the neural network to learn robust features. The paper shows that under white - box attacks, neural networks equipped with OMP blocks have accuracy over 80 %, and for black - box attack with accuracy over 50 %. The performance of OMP is compared with state - of - the - art adversarial defense methods. The main contributions of the paper are as follows :   ( 1 ) A new method for defending neural networks is proposed to overcome the problem of clean images with invisible perturbations that can fool deep neural networks. This new method is called OMP and it is similar to the previous SOTA method. However, it differs from the previous method in that it does not impose a constraint on the number of paths in each of the paths. Instead, it imposes a constraint that the parameters of all the paths in the OMP should be close to one another. This constraint is introduced in the paper to encourage neural networks to learn features that are appropriate for all paths and hence are expected to be robust. This is achieved by constructing a block that consists of multiple paths and imposing a constraint only on the first path in each time the network starts a new neural network. The authors show that this approach is better than the SOTA approach in terms of robustness and accuracy. ( 2 ) The authors conduct extensive experiments to show the effectiveness of the proposed method. They show that OMP works better than SOTA in two scenarios : 1 ) White - box PGD attacks are much more accurate than white box attacks and 2 ) Black box attacks are slightly less accurate but OMP seems to work better."
SP:776df66274ed12449fde8dcef873a593980f397c,"This paper proposes a self - supervised graph attention network, called SuperGAT, for dealing with noisy graphs. The main idea is to learn an attention mechanism to distinguish mislinked neighbors of nodes by using edge information. The authors analyze two graph characteristics that influence the effectiveness of attention forms and self - supervision : homophily and average degree. They find that GO is better at label - agreement prediction and DP at link prediction. They then propose a recipe to design graph attention that is based on these graph characteristics and test it on real - world datasets. The experimental results show that the proposed method outperforms GO and DP in most cases."
SP:80a05296d6b1e4c6e9e2df01938c73029ff8487d,"This paper proposes a new method for DSMAD, called Introspective Diagnosis System ( INS - DS ), which is based on two separate yet cooperative modules, i.e. an inquiry module for proposing symptoms and an introspective module for deciding when to inform a disease. The idea is inspired by the introspective decision - making process of human doctors, where the inquiry module first proposes the most valuable symptom inquiry, then the introspection module intervenes the potential responses of this inquiry and decides to inquire only if the diagnoses of these interventions vary. The authors also propose two evaluation metrics to validate the reliability and robustness of DSMAD methods, namely, reliability of internal trust ( Int ) and external trust ( Ext ). The reliability metric is to testify whether the diagnosis made by the model is insensitive to the task - irrelevant factors, e.g. the sampling noise and parameter initialization. The external trust is proposed to indicate the trust degree of a diagnosing process to users. According to the paper, the higher the Int. is, the less sensitive the diagnosis result is to the noise in the training process. The higher the Ext., the more likely the agent is likely to inquire about the overlap between the symptoms and diseases in the dataset. The experiments demonstrate that the proposed method achieves the new state - of - the - art under various experimental settings and possesses the advantages of reliability compared to other methods."
SP:10ae09d90d465125433a9b4f15b1405ab017920d,"This paper presents a novel method for dealing with the long - tailed and long - tail visual classification ( FGVC ) problems. The proposed method is based on the confusion energy - based framework proposed by Dubey et al. ( 2018 ). The confusion energy is modeled by a batch - wise matrix norm, termed as Batch Confusion Norm ( BCN ), which is used to solve the FGVC problem. The BCN is composed of two components : a fine - grained version for FGVC and an adaptive version for long tailed FGVC. In the adaptive version, the prediction matrix is constructed by taking into account the prediction results from all images within a batch, as well as an adaptive matrix to adjust class - specific weights. The experimental results show that the BCN achieves state - of - the - art performance on several benchmark FGVC datasets, and competitive performance on the popular natural world distribution dataset iNaturalist."
SP:90f1e0fe1e9678d1e9a4dcb519d4e8fd61098ce0,"This paper introduces a method for learning approximate variational reward distributions for inverse reinforcement learning ( IRL ), named AVRIL, for approximate Variational Variational Reward Imitation Learning. The main idea is to jointly learn an approximate posterior distribution over the reward that scales to arbitrarily complicated state spaces alongside an appropriate policy in a completely offline manner through a variational approach to said latent reward.   The main contribution of this paper is a method that allows for approximate reward inference using an arbitrarily flexible class of functions, in any environment, without costly inner - loop operations, and importantly entirely offline. This leads to an algorithm that is shown to outperform Bayesian IRL in real medical data and simulated control environments, notably in environments where it is now possible to achieve Bayesian reward inference."
SP:ccd251d95c0a2d8dc5ad2a148ec29955e105e71e,"This paper presents a method for learning policies in multi - agent environments based on an approximate auto - regressive counterfactual belief that is learned as a supervised task. The method, called Learned Belief Search ( LBS ), uses a novel public - private model architecture for underlying policies in order to efficiently evaluate these policies during rollouts in Hanabi. LBS is based on the fact that prior methods do not scale well with the amount of hidden information in the environment. The main contribution of this paper is to propose a method that attempts to address this limitation by using an approximate belief distribution that is used in lieu of an exact belief distribution. This approach is shown to be computationally efficient when applied to the benchmark environment of Hanabi, where it obtains around 60% of the benefit of exact search while reducing compute requirements by up to 35 %."
SP:db408e6bfe69a9b3984f3b27ca92b802aa37af42,"This paper presents a novel method for planning in large state spaces, called Shoot Tree Search ( STS ). It is an interpolation between two celebrated search mechanisms : MCTS and random shooting. The key idea of the method is to control the bias - variance trade - off between TD(n ) and STS in the tree search context. The paper presents experimental results showing that STS can get the best of both worlds consistently achieving higher scores.    The main contribution of this paper is the introduction of a novel algorithm that aims to control this trade -off more explicitly. The proposed method is based on a combination of two techniques : 1 ) a modified version of random shooting and 2 ) an extension of MCTs. The main novelty of the proposed method lies in the fact that it allows the user to control both the bias and the variance of the algorithm. This is the first time that this type of planning approach has been proposed in a large state space planning paper. I think this is a very important step forward in the direction of improving planning in state spaces. I hope that other researchers will follow suit and develop similar methods."
SP:5efc271ccc555fd9aa542548838170bd4c98e957,"This paper proposes a new method for learning inductive bias in neural networks, inspired by Peirce ’s view that deduction, induction, and abduction form an irreducible set of reasoning primitives, and design three synthetic tasks that are intended to require the model to have these three abilities to ensure that only the fundamental reasoning biases can be learned from these tasks. This defines a new pre - training methodology called “ LIME ” ( learning Inductive bias for Mathematical rEasoning ). Models trained with LIME significantly outperform vanilla transformers on three very different large mathematical reasoning benchmarks, compared to models trained with traditional methods. The main contributions of the paper are as follows :   ( 1 ) The authors propose a new way of training neural networks to learn inductive biases from synthetic datasets. This is in contrast to the traditional approach of training networks on a fixed number of experiments over a large number of benchmarks. The three synthetic datasets used in this work are the Bayesian Neural Network ( BNN ), Transformer ( transformer ), and Neural Networks ( NT ). These three datasets are designed to require only a small fraction of the computational cost of the typical downstream downstream tasks. The authors claim that this allows them to train networks that are flexible enough to learn induction bias from suitable generic tasks. ( 2 ) They also propose a way to train neural networks that is more general in nature, such that it is possible to train the network on a much larger set of datasets. ( 3 ) They show that their method outperforms other methods in terms of accuracy."
SP:bb8e0b554d3b3314fa343c902d9e60f1a141ea30,"This paper studies the inductive bias of weight normalization ( EWN ) on neural networks trained on exponential or cross - entropy loss. It is shown that EWN is equivalent to gradient flow on standard networks with adaptive learning rate, and hence causes the weights to be updated in a way that prefers asymptotic relative sparsity to hold for gradient descent via an appropriate learning rate. This is in contrast to the induction bias of standard weight normalisation ( SWN ) and unnormalized architectures, which demonstrate their implications on synthetic data sets and demonstrate their support for SGD even with sparse data sets.   The main contribution of this paper is the analysis of EWN, which encourages weight updates along the radial direction. It has been shown that gradient descent prefers simpler solutions over more ‘complex ’ solutions, where the notion of complexity is often problem /architecture specific. This paper shows that the gradient flow path with EWN can be extended to the adaptive setting. The convergence rate of the loss is given by 1 t(log t ) 2, and is independent of the depth of the network."
SP:c71f9d2a602516865a0b103028186e83b52e5f00,"This paper investigates the mode collapse problem in generative adversarial networks ( GANs ). The authors propose a method to mitigate mode collapse that occurs when the discriminator of a GAN is unable to maintain classification accuracy on previously seen samples. Motivated by this observation, they introduce a novel training procedure that dynamically spawns additional discriminators to remember previous modes of generation. They show that their training scheme can be plugged - in to existing GAN frameworks and improve standard metrics for GAN evaluation and coverage of the generated samples.   The main contribution of this paper is to propose a training scheme that can mitigate mode collapses in GAN framework. The method is based on the observation that mode collapse happens when discriminators are unable to keep track of the previous distribution of the target distribution. This is because some modes of the distribution are ignored by the generator. To mitigate this, the authors propose to add a feature that encourages discriminator to remember the distribution of previous distributions. The proposed training scheme is tested on two datasets."
SP:52c48198c95826e042f9e5a512ef3265daaff882,"This paper proposes a new regularization method for BERT, called AUBER, that leverages reinforcement learning to automatically prune attention heads from BERT. The method is based on the observation that BERT prunes its attention heads based on a proxy score for head importance, which is a promising direction to regularize BERT but heuristically based methods are suboptimal as they predetermine the order in which attention heads are pruned. To overcome this limitation, the proposed method leverages a policy - based pruning algorithm that learns a pruning policy that determines which attention head should or should not be pruned for regularization. The policy is learned using a combination of reinforcement learning and self - supervised learning. Experiments on MNIST, CIFAR-10, and MNIST show that the method achieves up to 9.39 % better accuracy."
SP:abcbbad146f1b0d5d579c215952c95e5499a378a,"This paper proposes a method to learn correspondence between two different domains : vision and morphology, and physics parameters ( mass and friction ), and morphology ( number of limbs ). The authors propose dynamics cycles that align dynamic robot behavior across two domains using a cycle - consistency constraint. Once this correspondence is found, they can directly transfer the policy trained on one domain to the other, without any additional fine - tuning on the second domain.   The method is applied both in simulation and with a real robot. In simulation, the authors adopt multiple tasks in the MuJoCo ( Todorov et al. 2012 ) physics engine, and show that their model can find correspondence and align two domains across different modalities, physical parameters, and morphologies. In real robot experiments, they use the xArm Robot ( Figure 1(a ) ). Given only uncalibrated monocular videos of the x arm performing random actions, their method learns correspondences between the real robot and simulated robot without any paired data. At test time, given a video of the robot arm executing a smooth trajectory, we can generate the same trajectory in simulation. At the same time, the algorithm learns a trajectory that is similar to the one generated by the robotic arm in the real world. This trajectory is then used in reinforcement learning ( RL ) policy trained in one domain directly to another domain without further optimizing the RL objective."
SP:006434d56992836ab9420d7d4215bc70664de304,"This paper studies the problem of explainability in the context of generative models using the Shapley framework for explainability. The main contribution of this paper is to develop two solutions to a problem of expressivity with respect to the data imputations of the model. The first one is based on generative modelling and provides flexible access to data imputation imputations, while the other directly learns the value function of the original Shapley value function. The authors show that “ off - manifold ” values can give rise to incorrect explanations, ( i ) model dependence on sensitive attributes, ( ii ) hide implicit model dependence, and ( iii ) lead to unintelligible explanations in higher - dimensional data.   The main contributions of the paper are as follows :   ( 1 ) This paper shows that the general implementations of Shapley explainability make an untenable assumption : that the model ’s features are uncorrelated. This assumption is proved mathematically and empirically to be wrong by the authors in two ways. ( 2 ) The authors demonstrate unambiguous drawbacks of this assumption and develop two solution to explainability that respect the data manifold. ( 3 ) The first solution provides performance and stability at the cost of flexibility while providing expressivity. ( 4 ) The second solution provides flexibility and expressivity while providing flexibility."
SP:7cda6bccf08887c7cef66d0ac3ccefdea8f5d7c8,This paper proposes a new method for opponent modelling based on variational autoencoders ( VAEs ). VAEs are an extension of deep reinforcement learning ( DRL ) based VAEs that use embeddings to augment the agent ’s decision - making policy via reinforcement learning. The authors provide a comprehensive evaluation and ablation study of VAEs in three multi - agent environments. They compare VAEs with a baseline that has access to the full trajectory of the opponent and a DRL based on the embedding policy. They show that VAEs can achieve comparable performance to DRL with full access to opponent trajectories. They also provide ablation studies on the different types of information used by VAEs.
SP:c239bc531bcf7293032748af29a1b786e9d893dd,"This paper proposes Consistent Contrastive Learning ( CO2 ), a method for contrastive learning with semi - supervised learning on unlabeled data. The main contribution of the paper is to introduce a consistency regularization term into the Contrastive learning framework to address the issue of heterogeneous similarity between query image crops and crops from other randomly sampled images. To address this issue, the authors propose to label crops from the same image as positives, and crop from other images from other samples as negatives. CO2 takes the corresponding similarity of a positive crop as a pseudo label, and encourages consistency between these two similarities. The authors evaluate the effectiveness of CO2 on various benchmarking protocols, including linear classification, semisupervised learning, image classification, object detection and semantic segmentation. They show that CO2 improves MoCo by 2.9% on top - 1 accuracy on ImageNet linear protocol, 3.8% and 1.1% top - 5 accuracy on 1% labeled semi - supervision settings, and 10% accuracy on PASCAL VOC. They also evaluate the transfer ability of the learned representations on three different downstream tasks."
SP:d18bab21790713e2facb053c47298fc9079ab783,"This paper studies the convergence rates of Optimistic Multiplicative Weight Update ( OMWU ) and Optimistic Gradient Descent Ascent ( OGDA ) for simple bilinear games over the simplex in constrained settings. The main contributions of the paper are as follows :   1. Under the assumption of uniqueness of the optimal solution, the authors show that linear last - iterate convergence can be achieved with a constant learning rate whose value depends only on the smoothness of the objective function. 2. The convergence rate of OGDA is shown to be exponentially fast even without the unique equilibrium assumption. 3. The condition of uniqueness is verified for matrix games and strongly - convex functions. 4. Experimental results are provided to support the theoretical results.   The main contribution of this paper is to introduce a sufficient condition under which OGDA exhibits concrete last -iterate convergence rates with a linear learning rate and to provide experimental results to further support the theory. In particular, it is shown that for strongly convex matrix games, OGDA converges exponentially fast with the learning rate of a constant constant, and for strongly -convex functions, the convergence rate is exponential fast with a learning rate that depends on a constant smoothness."
SP:bbc7f77308b298c332a39747f693bc396f00a89f,"This paper proposes Federated User Verification ( FedUV ), a framework for private training of UV models in federated setting. In FedUV, users jointly learn a set of vectors and maximize the correlation of their instance embeddings with a secret user - defined linear combination of those vectors. They show that choosing the linear combinations from the error - correcting code allows users to collaboratively train the model without revealing their embedding vectors. The experimental results show that FedUV performs on par with FedAwS, while not sharing the embedding vector with the server."
SP:40fa47cc0928e2925ef5ce6d808073f368ca2cd4,"This paper proposes a new method to measure the robustness of deep neural network classifiers based on the geometry of class manifolds ( CMs ). The margin is defined via the boundaries between these CMs. The authors propose a simple technique to estimate the effective dimension of CMs as well as boundaries between multiple CMs, by computing their intersection with random affine subspaces of varying dimension. They provide a theory for the technique and verify that their theoretical predictions agree with theoretical predictions on real neural networks. Through extensive experiments, they leverage this method to show deep connections between the geometry, generalization, and robustness. In particular, they investigate how CM dimension depends on : 1 ) the dataset, 2 ) architecture, 3 ) random initialization, 4 ) stage of training, 5 ) class, 6 ) ensemble size, 7 ) label randomization, 8 ) training set size, and 9 ) robustness to data. Together a picture emerges that well - performing robust models have higher dimensional CMs than worse performing models. Moreover, they offer a unique perspective on ensembling via intersections of CM dimension and margin."
SP:09bce202ac7a750c3700a8ef3cd92cfe8ed00c39,"This paper proposes a novel method to improve the trade - off between exploration and exploitation in reinforcement learning. The proposed method is based on the idea of Curiosity - Aware Entropy Temperature for SAC ( CAT - SAC ), which uses the state prediction error to model curiosity because an unfamiliar state has a large prediction error, while familiar states have a smaller prediction error. The authors argue that this encourages the agent to explore more in an unknown state, while less in a familiar state, so as to understand the environment more efficiently. To model curiosity for feature inputs, they propose a new curiosity model, X - RND, optimized by contrastive self - supervised learning. Experimental results on the MuJoCo benchmark show that the proposed method significantly improves the sample - efficiency on the difficult continuous control tasks."
SP:dce5eb20581a21c5de0a9fc07a8a79a1fbb28c71,"This paper presents a method for adapting meta - reinforcement learning algorithms for out - of - distribution meta - training, where the goal is to learn a set of skills without requiring extremely large amounts of data for meta - learning. The paper proposes a method called Experience Relabeling - based Meta - Reinforcement Learning ( MIER ) to adapt the learned model to the next state and reward on every transition, obtaining synthetic data to continue training the policy. This method outperforms prior meta - reinforcement learning methods in this setting, outperforming them on a meta - test - time basis.   The main contributions of the paper are as follows :   - A model identification and experience relabeling algorithm that is both efficient and extrapolates well when faced with out -of - distribution tasks at test time. - A method that adapts the model with gradient descent, which allows the authors to continue to improve the model using more gradient steps to continue improving the policy - A way to leverage all data collected from other tasks during meta -training, by using the learning model to relabel the next states and rewarding on every previously seen transition, obtained synthetic data for the new task. - An algorithm that enables MIER to adapt to tasks outside of the meta training distribution, which outperforms other meta - RL algorithms on a test time basis, which is compared to prior methods."
SP:34d78aa11f9d50baf75a9646a6f9128318c3389a,"This paper proposes a new method for tackling the few - shot learning ( FSL ) problem. The authors first cast the meta - overfitting problem as a gradient noise problem since few available samples cause meta - learner to overfit on existing examples ( clean or corrupted ) of an individual task at every gradient step. To address this problem, they propose Introspective Self - paced Learning ( ISPL ) that constructs a plurality of prior models to determine which sample should be abandoned at each step of FSL. They also propose Eigen - Reptile ( ER ) that updates the meta-parameters with the main direction of historical taskspecific parameters to alleviate gradient noise.   Experiments on different tasks demonstrate that the proposed methods outperform or achieve highly competitive performance compared with the state - of - the - art methods with or without noisy labels."
SP:a571bff9ffe4edafd7bc064c4d10609e6b981ce3,"This paper proposes AdvBN, a method to train deep neural networks that are robust to adversarial perturbations that occur naturally, such as changes in the distribution of input images or the style of images. The authors propose to model adversarial training by adversarially perturbing the mean and variance of deep image features, rather than image pixels. They show that AdvBN significantly improves the performance of ResNet-50 on ImageNet - C, Stylized - ImageNet, and ImageNet-Instagram over standard training practices. They also show how AdvBN can also improve generalization on semantic segmentation."
SP:6a9c46bd3cf854299f360bff136e1d79d3edb2e4,"This paper proposes Variance of Gradients ( VoG ), a metric that measures the degree to which data points with high VoG score are more difficult for the model to learn. VoG can be used to rank data by difficulty and to surface a tractable subset of the most challenging examples for human - in - the - loop auditing. The paper provides quantitative and qualitative support that VoG is a meaningful way to rank the data and a possible subset to aid human interpretability. The authors also provide empirical support for the effectiveness of VoG.   The paper proposes VoG as a metric for detecting outliers in the data distribution. The main contribution of the paper is to propose VoG, which can be computed using checkpoints stored over the course of training and is model agnostic, to be used as an unsupervised auditing tool at test time."
SP:074bfacc75837bb19049be8a2890e10de073dd8e,"This paper proposes a new method to improve the generation quality of samples from deep generative models. The proposed method, called Discriminator Gradient Gradient Low ( DGf low ), is based on the gradient flow of entropy - regularized f - divergences between the real and the generated data distributions. The gradient flow takes the form of a non - linear Fokker - Plank equation, which can be easily simulated by sampling from the equivalent McKean - Vlasov process. By refining inferior samples, the proposed method avoids wasteful sample rejection used by previous methods, such as DRS and MH - GAN. The main contribution of this paper is to propose a method that improves the generated samples via the gradient flows of entropy-regularized f and to use it in combination with two previous methods : DOT and DDLS.   The main contributions of the paper are as follows :   1. A new method for improving the quality of generated samples by penalizing the distribution of entropy between the generated and the real data distributions by a factor of 0.5. 2. A method to refine the sample rejection of DRS to avoid waste of samples. 3. Experiments on synthetic image and text datasets demonstrate that the proposed technique, as well as two other methods, can improve the sample quality for a number of popular generative modeling models, including VAEs and VAEs - VAE."
SP:74ecbc5a6d464bfa49337da9e0dd6a0fe714d4bb,"This paper presents a variable encoder - decoder ( VECO ) pre - training approach to unify the two mainstreams in both model architectures and cross - lingual tasks. The main idea of the approach is to split the standard Transformer block into several sub - modules trained with both inner sequence and cross language modeling, and correspondingly reorganize certain sub - module for understanding and generation tasks during inference. The paper also outperforms all existing cross lingual models and state - of - the - art Transformer variants on WMT14 English - to - German and English -to - French translation datasets with gains of up to 1.2 BLEU."
SP:3d177ad50727d1a2619b68ab8a897b79d8652beb,"This paper presents a novel approach for reinforcement learning ( RL ) that uses auditory event prediction as an intrinsic motivation to guide exploration in Atari - style games. The approach is based on two components. First, an agent collects a small amount of acoustic data and uses K - means to discover underlying auditory event clusters, which are then used to train a neural network to predict the auditory events and use the prediction errors as intrinsic rewards to guide RL exploration. Second, the neural network is used for audio - visual exploration using the Habitat simulator and the ThreeDWorld ( TDW ) simulator. Experimental results demonstrate the advantages of using audio signals over vision - based models as the intrinsic motivation for exploration in TDW."
SP:014f6118ebe55ece6be23c3a10f12e4591e444b1,"This paper studies the problem of novel category discovery on single and multimodal data with labels from different but relevant categories. The authors propose a generic, end - to - end framework to jointly learn a reliable representation and assign clusters to unlabelled data. To avoid over - fitting the learnt embedding to labelled data, they take inspiration from self - supervised representation learning by noise - contrastive estimation and extend it to jointly handle labelled and unlabeled data. In particular, they proposed using category discrimination on labelled data and cross - modal discrimination on multi - modular data to augment instance discrimination used in conventional contrastive learning approaches. They further employ Winner - Take - All ( WTA ) hashing algorithm on the shared representation space to generate pairwise pseudo labels for unlabelling data to better predict cluster assignments.   The main contributions of the paper can be summarized as follows :   1. A novel framework for learning representation that can be trained jointly on labeled and unlabelled data ;   2. A strategy to employ WTA hashing on the share representation space of both labelled data to generate additional ( pseudo ) supervision on unlabeling data ; and ( 3 ) a thoroughly evaluate   framework on challenging large scale multi - Modal video benchmarks and single - modAL image benchmarks, outperforming existing methods by a significant margin."
SP:4df640f502e88ddba2d7e183625231d70b083e82,"This paper proposes a new weakly supervised segmentation method based on the semi - supervised metric learning problem, where pixels of the same semantics need to be mapped to the same ( distinctive ) features in the feature space. The authors propose 4 types of contrastive relationships between pixels and segments in the image feature space, capturing low - level image similarity, semantic annotation, co -occurrence, and feature affinity. They also propose a conditional segmentation model, where conditional random fields are used to propagate sparse labels to the entire image. They compare their method with two previous methods, Pascal VOC and DensePose, and show consistent gains over the state - of - the - art ( SOTA ), especially for the sparsest keypoint supervision."
SP:f7d6099adb40a0ce2f8a3563dbd5207cf1fdea0f,"This paper proposes a simple but effective distillation strategy for unsupervised learning in the setting of self - supervised learning. The proposed method is called BINGO, which is short for Bag of InstaNces aGgregatiOn. It aims at transferring the relationship learned by the teacher to the student within a bag constructed by distillation within a compact compact representation over the student with respect to instances in a bag. The key idea is that the relationship among similar samples counts and can be seamlessly transferred by the student to boost the performance. The method achieves new state - of - the - art performance on small scale models, i.e., 65.5 % and 68.9 % top - 1 accuracies with linear evaluation with linear regression on ImageNet, using ResNet-18 and ResNet - 34 as backbone, respectively, surpassing baselines ( 52.5% and 57.4% top -1 accuracies ) by a significant margin.   The main contribution of this paper is that it provides a new paradigm for un - supervised distillation where knowledge between instances with high relation could be more effective than relation -agnostic ones. This may be inspiring for further explorations on knowledge transfer in unsuper supervised scenarios."
SP:328866aad6544c81ded8980934df31dc4472435f,"This paper proposes GATSBI, an adversarial approach to Simulation - based Inference ( SBI ) that reformulates the variational objective in a stochastic setting to learn implicit distributions. This approach is similar to generative adversarial networks ( GANs ) in that it does not require explicit likelihoods. The main difference is that unlike GGANs, SBI can be formulated as a variational inference problem with implicit priors. The proposed approach is evaluated on two SBI benchmark problems and two high - dimensional simulators. The results show that the approach is able to learn an implicit posterior over a large number of dimensions and can be extended to refine the estimates for specific observations, i.e. sequential posterior estimation. The limitations of the proposed approach are discussed and compared with common SBI algorithms."
SP:2915e82097eae4eb8546dc500f32b3ec37e3766f,"This paper proposes a method for identification and estimation of treatment effects ( TEs ) under limited overlap when subjects with certain features belong to a single treatment group. The identification is based on recovering prognostic scores from observed variables and a generative prognostic model. The generative model is learned as a new type of variational autoencoder ( VAE ), and is compared to state - of - the - art methods on ( semi - synthetic ) datasets. The main contributions are : 1 ) TE identification via prognostic score, which is obtained via an identifiable model. 2 ) bounds on individualized TE error, which justify the conditional BRL. 3 ) a new regularized VAE, called Beta - Intact - VAE. 4 ) experimental comparison to the current methods on Semi - synthetic datasets, realizing identification and conditional balance."
SP:ca358c9f36aac6e58ed1b3949c349d210c49a48e,"This paper proposes a framework for autonomous reinforcement learning ( ARL ) where the agent is not only responsible for its own experience but also contends with the lack of human supervision to reset between trials. The paper introduces a simulated world in which the agent does not have access to resets between trials, and introduces a benchmark for ARL, as well as formal definitions of two distinct ARL settings. The authors reformulate the learning tasks to reflect ARL constraints, such as the absence of explicitly available resets. They additionally evaluate a range of previously proposed algorithms on their benchmark, focusing on methods that explicitly tackle reset - free learning and other related scenarios. They find that both standard RL methods and methods designed for reset -free learning struggle to solve the problems in the benchmark and often get stuck in parts of the state space, underscoring the need for algorithms that can learn with greater autonomy and suggesting a path towards the development of such methods."
SP:abe51d4a9817c08f0abde5da0bb8e6ca4e02e7cf,"This paper investigates the reasoning capability of Graph Neural Networks ( GNNs ) for Question Answering ( QA ) in the knowledge - based model of QA. The authors claim that existing knowledge - aware GNN modules may only carry out some simple reasoning such as counting, and that it remains a challenging problem to build comprehensive reasoning modules for knowledge - powered QA systems. To investigate this problem, they dissect state - of - the - art GNN - based QA modules for QA and analyze their reasoning capability. They discover that even a very simple graph neural counter can not outperform all the existing GNN-based modules on CommonsenseQA and OpenBookQA, two popular QA benchmark datasets which heavily rely on knowledge -aware reasoning."
SP:3ea5a38e7fcd9111dcd299ad039b634e2781685f,"This paper proposes a compression method for Deep Neural Networks ( DNN ) inference. The main contribution is a three - stage compression scheme, called "" sequential compression "", which consists of two stages. The first stage compresses the representation of a DNN model into a block representation and a sub - DNN representation, and the second stage computes the representation into a vector representation, which is then fed into an inference pipeline. The authors claim that their method achieves near - optimal compression compared to Huffman compression and other state - of - the - art DNN inference methods. They also claim that the method is synergistic with other compression methods, such as pruning and quantization. The method is evaluated on AlexNet/VGG16 inference on CIFAR-10 and ImageNet-16 datasets."
SP:94c395afc794a9cc163e362078769ff83f3d20d0,"This paper proposes a new training method for improving the performance of tiny neural networks. The proposed method, NetAug, augments the network ( reverse dropout ) instead of inserting noise into the dataset or the network. It puts the tiny model into larger models and encourages it to work as a sub - model of larger models to get extra supervision, in addition to functioning as an independent model. The authors argue that training tiny models is different from training large models : rather than augmenting the data, we should augment the model, since tiny models tend to suffer from under - fitting rather than over - fitting due to limited capacity. They demonstrate the effectiveness of NetAug on image classification and object detection."
SP:9c24549b980e415616f818acbf4cf680ef8edb52,"This paper proposes a generative adversarial network ( GAN ) for dynamic point cloud sequences without requiring point correspondence annotation. The proposed method, Temporal Point cloud Upsampling GAN ( TPU -GAN ), can learn the underlying temporal coherence from point cloud sequence, which in turn guides the generator to produce temporally coherent output. In addition, the authors propose a learnable masking module to adapt upsampling ratio according to the point distribution of the generator. The quantitative and qualitative evaluation demonstrates the effectiveness of the method on the task as well as learning temporal co -herence from irregular point clouds sequences.   The authors conduct extensive experiments on point clouds from two different domains : particles in the fluid dynamical system and human action scanned data. They show that their method can be used for classification task and provides competitive performance."
SP:67efe60ad37807505369b7852bc0abed29ffdda8,"This paper proposes a new method for training object detection transformers, FP - DETR, that pre - trains an encoder - only transformer and finetunes it for object detection via a task adapter. The proposed method is inspired by the success of textual prompts in NLP, where query positional embeddings are used as visual prompts to help the model attend to the target area ( Prompting ) and recognize the object. The source code of the task adapter will be made publicly available. Experiments show that the proposed method achieves competitive performance on the challenging COCO dataset ( Lin et al., 2014 ) and a better trade - off between the number of parameters and detection accuracy. The method is more robust against common corruptions and can generalize well to small - size datasets like Cityscapes."
SP:a1f9897496303984fc7ad469222106b14b4a6233,"This paper presents FedPAGE, a new federated learning algorithm based on Federated SGD ( FedAvg ). FedAvg is an extension of Local SGD in which clients run multiple local SGD steps before communicating their update to an orchestrating server. This paper proposes to use Page - SGD to further reduce the communication complexity by utilizing the recent optimal PAGE method ( Li et al. 2021 ). The authors show that FedGEave uses much fewer communication rounds than previous local methods for both federated convex optimization and nonconvex optimization. They show that in both settings, the communication cost for each round is the same for both FedGEAveraging and SCAFFOLD. As a result, FedGEaveraging achieves new theoretical state - of - the - art results in terms of communication complexity for both convex and non - convex federated federated optimization."
SP:81e74765abc6524edd8fdf9a3ba107d7bddaa04b,"This paper studies the problem of designing neural networks that are robust to adversarial perturbations to the decision boundary of artificial neural network classifiers. To this end, the authors define adversarial subspaces, which are spanned by orthogonal directions of minimal perturbation to the boundary from any given input sample. They find that the distance to boundary within the boundary within one of these adversarial spaces increases as one moves further away from the adversarial examples in the training procedure. They further show that adversarial training increases the number of alternative label regions in adversarial space and changes the relative distribution of nearby alternate classes within each subspace. The authors conclude that the boundary which is more curved within the curvature of the boundary of one of the regions in the curved subspace within the curved adjacency space within the adjacencies of the classifier is also more curved than within the region within a random subspace of equal dimensionality within the general case.   The main contribution of this work is to further our understanding of the decision boundaries geometry of ANN classifiers by utilizing such adversarial attacks. To date, the most widely used method for defending against test - time adversarial attack is adversarial defense, where one incorporates adversarial data into the training process by either augmenting the dataset with pre - computed examples ( Ilyas et al. 2019 ) or incorporating an attack model by incorporating an adversarial dataset ( Goodfellow et al. 2018 ). The resulting analysis leads to a general explanation of classifier decision boundaries, as well as adversarial susceptibility, in the subspace where the boundary is closest to the input example. As a further demonstration of the utility of their approach, they investigate the defense method, so - called adversarial trained adversarial adversarial. The defense method requires including adversarial samples in the defense process by augmenting    ( I.e., augmenting dataset by incorporating pre - constructed examples into training procedure ), but the performance is still far from that on unperturbed images. They provide a new perspective on this defense by observing that it increases the distance of decision boundary in the entire adversarial subspace, even though the method utilizes only a single adversarial example per input. They additionally find that adversarially trained networks are more resistant to the distance increase in boundary distance within one region in the boundary space within one subspace, and the redistribution of proximal class labels and the decrease in boundary curvature is in the opposite direction."
SP:af5c25ecf38c5c3f3387720bdc80c2c54c5699fe,"This paper presents a two - stage weakly - supervised contrastive learning approach for learning visual representations using auxiliary information. The first stage is to cluster data according to its auxiliary information and the second is to learn similar representations within the same cluster and dissimilar representations from different clusters. The authors consider three datasets : UT - zappos50K, CUB - 200 - 2011, Wider Attribute, and ImageNet - 100. They consider two baselines that also leverage auxiliary information : i ) predicting the auxiliary - information - induced clusters with cross - entropy loss and ii ) adopting the contrastive multi - view coding ( CMC ) method when treating auxiliary information as another view of data. They show that the auxiliary information - infused representations, compared to conventional self - supervised representations, have a much better performance on downstream tasks. The second set of experiments focus on the analysis of Cl - InfoNCE to study how well it works with unsupervised constructed clusters ( K - means clusters ). They find it achieves better performance comparing to the clustering - based self - supervised learning approaches, such as the Prototypical Contrastive Learning ( PCL ) method, and better performance than the CMC method. The result suggests that the K -means method + Cl -InfoNCE can be a strong baseline for the conventional selfsupervised learning setting, which also leverages auxiliary data information."
SP:0a92fcc52970201de4a66b1e76c93dbea9dfd3f1,"This paper proposes PLISA ( Provable Learning - based Iterative Sparse Recovery Algorithm ), a method to learn algorithms automatically from data with sparse parameters. The main idea is to learn an iterative recovery algorithm by unrolling a classic path - following algorithm with some components being more flexible and learnable. The paper theoretically analyzes the recovery accuracy and Rademacher complexity of PLISA to characterize its generalization ability to solve new problems outside the training set. Furthermore, the paper also analyzes empirical properties of the algorithm to characterize generalization capabilities.   This paper contains novel theoretical contributions to the area of learning - based algorithms in the sense that ( i ) PLISA is generically applicable to a broad class of data, ( ii ) generalization of the method is possible, and ( iii ) general analysis has received less attention so far, which leads to tighter bound that can explain the empirical observations."
SP:5064eda9ba27060af15e81b2b317b2e4558b0ac4,"This paper proposes a method to learn a compact and decodable latent representation space for the original hybrid action space in the discrete - continuous action space for reinforcement learning ( RL ). The method, called Hybrid Action Representation ( HyAR ), is based on two components : ( 1 ) latent action embedding and ( 2 ) latent representation embedding. The former embedding maps the discrete action space to the latent space via an embedding table and embeds the dependence between discrete action and continuous action. The latter embedding is learned using an encoder - decoder setup. In the experiments, HyAR is compared with two baselines ( Auto - Encoder and Variational Encoder - VAE ) in three environments. The results demonstrate the superiority of HyAR when compared with previous baselines, especially for high - dimensional action spaces."
SP:5128bf712f6b197de113c7a371b4bec36f978eca,"This paper proposes SGEM, Stochastic Gradient with Energy and Momentum, to solve a large class of general non - convex stochastic optimization problems, based on the AEGD method that originated in the work of [ Sridharan et al., arXiv:10.05109 ]. SGEM incorporates both energy and momentum at the same time so as to inherit their dual advantages. The authors show that SGEM features an unconditional energy stability property, and derive energy dependent convergence rates in the general nonconvex setting, as well as a regret bound in the online convex setting. A lower threshold for the energy variable is also provided. Experiments are conducted to compare SGEM with AEG D and generalize SGDM in training some deep neural networks."
SP:11f49b0a975be87769be29e85d7e3924699cf2c9,"This paper proposes a Conditional Masked Language Model with Correction ( CMLMC ), a machine translation model that improves upon the state - of - the - art non - autoregressive machine translation ( NAR ) models. NAR models have achieved significant performance improvements, nearing human - level accuracy on some languages. This paper investigates the possible reasons behind this performance gap, namely, the indistinguishability of tokens, and mismatch between training and inference. The proposed model builds on the CMLM architecture and addresses the aforementioned problems. The authors modify the decoder structure by exposing the positional encodings and incorporating causal attention layers to differentiate adjacent tokens. They also propose a novel correction loss that teaches the model how to correct translation translation mistakes made in early decoding iterations from a fully masked sentence. With these improvements, the proposed model achieves new state of the art NAR results on multiple datasets."
SP:96f8ac3c6163e56d8ae1954a162bae01e6b58a0a,"This paper presents a neural network architecture inspired by the WaveNet architecture and proposes a spiking neural dynamics method to replace the dilated temporal convolutions commonly used in neural networks. The proposed method, WaveSense, is based on neural dynamics, fixed time - constants and a simple feed - forward architecture. WaveSense is evaluated on several datasets for keyword spotting. The results show that WaveSense outperforms the state - of - the - art performance of artificial neural networks such as CNNs and LSTMs. The paper also proposes a number of other avenues to explore that could take advantage of the work."
SP:7f20a2e4e95f857140b87b0730360b3ff2f371f4,"This paper introduces Shifty, an algorithm that aims to provide high - confidence behavioral fairness guarantees that hold under demographic shift. The algorithm is based on the fact that prior work has failed to recognize the phenomenon of demographic shift, where certain subgroups of the population become more or less probable in deployment ( a phenomenon we call demographic shift ), and hence prior work ’s fairness assurances are often invalid. The paper proposes a class of algorithms, called Shifty algorithms, that provide behavioral guarantees that provide high-confidence behavioral guarantees for the guarantee to hold under such a phenomenon. The authors also propose a method for creating such algorithms, and an evaluation on real - world data, and ( 4 ) an open - source Shifty implementation and a release of all the data. The experiments demonstrate that Shifty is an effective tool for training models that are fair when demographic shift occurs, and that the algorithm is capable of providing high - confident behavioral guarantees in practice."
SP:94f097921bee5fdc10ec2e7c901b2ddb876d9d41,"This paper proposes a neural - based method for solving stochastic multi - stage optimization problems, named Neural Stochastic Dual Dynamic Programming ( N - SDDP ). The main idea is to extend the state - of - the - art method for multi - state stochastically optimization, known as stochaastic dual dynamic programming, by introducing a neural component that learns to map problem instances to a piece - wise linear value function within a low - dimensional space. The neural component is trained using supervised learning ( SL ) and reinforcement learning ( RL ) for MSSO problems in Section 4. The authors compare the proposed approach with existing algorithms that also exploit supervised learning and RL in Section 5. In Section 5, they conduct an empirical comparison on synthetic and real - world problems and find that N -SDDP is able to effectively exploit successful problem solving experiences to greatly accelerate the planning process while maintaining the quality of the solutions found.   The main contributions of the paper are as follows : - Motivated by the difficulty of SDDP and shortcomings of existing learning - based approaches, the authors propose to extend SDDP by solving successive problems in a non - linear fashion using a neural network and learning a linear function to map each instance to a different dimension. This allows the neural network to interact with a base SDDP solver that is trained to solve the problem at each step. - An empirical investigation demonstrates that the proposed method, called N - NSDDP, can significantly reduce problem solving cost without sacrificing solution quality over competitors such as SDP."
SP:3d9f5132f9ec3807dbca78462a459fd123a09b24,"This paper proposes a new protocol, called SUBMIX, for next - token prediction based on relaxation of group differentially private prediction. The goal is to prevent privacy violations by language models that were fine - tuned on a private corpus after pre - training on a public corpus. The proposed method is based on Relaxation of Group Differentially Private prediction ( LDP ), which is a relaxation of a group private prediction to a group that is not private to any individual user. The authors claim that this allows them to prevent samples that are unique to any user from being generated by the proposed method. SubMIX also admits a tight, data - dependent privacy accounting mechanism, which allows it to thwart existing data - extraction attacks while maintaining the utility of the language model.   The main contributions of this paper are as follows :   1. This paper introduces a new method, which claims to be the first protocol that maintains privacy even when publicly releasing tens of thousands of next -token predictions made by large transformer - based models such as GPT - 2. 2. This work also shows that subMIX can effectively prevent existing data extraction attacks against GPT-2, with privacy leakage as small as = 2 %. 3. The main contribution of this work is that it proposes a method to prevent data - extracting attacks against language models while maintaining privacy."
SP:7f524d186ea939309c7eeb843c62b6a4b4cfbc8a,"This paper proposes an unsupervised method to detect out - of - distribution ( OOD ) samples using a k - N density estimate with respect to a classification model’s intermediate activations on indistribution samples. The authors leverage a recent insight about label smoothing, which they call the Label Smoothed Embedding Hypothesis, and show that one of the implications is that the k -NN density estimator performs better as an OOD detection method both theoretically and empirically when the model is trained with label smoothed. The proposed method outperforms many OOD baselines and the authors provide new finite - sample high - probability statistical results."
SP:aafbd6ada14cc59a272fe4bf95fac71fa18e57ab,"This paper proposes a novel method for training non - adversarial generative generative models based on denoising score matching. In contrast to GANs and VAEs, which learn representations by directly transforming latent codes to data samples, this paper proposes to learn an infinite - dimensional latent code that can be used for representation learning without any supervised training. The proposed method is based on a new formulation of the denoised score matching objective and thus encodes information needed for denoisation. The authors evaluate the effect of the initial noise scale on the sampling speed and the improvement of state - of - the - art models on semi - semi - supervised image classification on a stochastic differential equations on a continuous time domain. They show how adversarial training in diffusion - based models can improve sample quality and improve sampling speed using a new approximation of the prior approximation."
SP:8cfc837d5c10d539bbd098df7134c42e4830ba25,"This paper presents C - Planning, a method for learning goal - conditioned reinforcement learning ( RL ) policies that can be applied to a wide range of domains, including navigation and robotic manipulation. The paper proposes a method to learn a curriculum of intermediate states for RL policies by planning at training time to automatically generate intermediate states. The method consists of three steps : E - Step planning a sequence of waypoints using graph planning, M - Step learning a goal - conditioned policy to reach those waypoints, and D - Step training a policy to predict the probability of reaching the waypoints. C - Planning is evaluated on 2D navigation tasks and 18D robotic manipulation tasks. The results show that C - planning is more efficient than other RL algorithms and achieves better performance than prior methods. The authors also evaluate the cost of planning during training and not testing."
SP:ef3193842e06d4a6edb8a6a86ea5bc97ee5eaa4a,"This paper studies the problem of training deep neural networks with mixup, which is a regularization technique that improves generalization and adversarial robustness. The authors propose to extend mixup to the k - mixup case by perturbing training data in the direction of other randomly - chosen instances in the training set to leverage the structure of the data. This is done using displacement interpolation, i.e. interpolation under the Wasserstein metric. They theoretically demonstrate theoretically in simulations that k - Mixup preserves cluster and manifold structures, and they extend theory studying the efficacy of standard mixup   to the   k - mixingup case. The empirical results show that training with k -Mixup further improves generalizability and robustness across several network architectures and benchmark datasets of differing modalities."
SP:0fe6a9848026e5f6436a380199e27a9ad26cffed,"This paper proposes a nonlinear embedding learning method for deep neural networks. The motivation is that conventional deep networks almost universally use a linear classifier on the learned embeddings. This could be suboptimal for a network with a limited - capacity backbone since better nonlinear classifiers could exist in the same embedding vector space. The authors advocate a non - linear classification layer for deep networks to tackle this problem. They theoretically show that their classification layer optimizes over all possible radial kernel functions on the space of embedding to learn an optimal non linear classifiers. They then demonstrate the usefulness of this layer in learning more model - efficient classifiers in a number of computer vision and natural language processing tasks. In addition, they empirically show that the kernelized classification layer is a viable alternative to using a larger backbone to improve classification accuracy."
SP:01ee8ec81619784788eb0ce9785098e437d17a7c,"This paper theoretically analyzes the source of bias in Graph Neural Networks ( GNNs ) and proposes a set of data augmentation methods to reduce the intrinsic bias in GNN - based learning. Specifically, the paper proposes to adapt input graph topology and nodal features adaptively, in order to reduce corresponding terms in the analysis that lead to bias in the obtained GNN representations. Theoretical analysis is performed on both the feature masking and graph structure to identify the sources of bias. Based on the analysis, the authors propose two new methods to enhance the fairness of GNN-based learning mechanisms : node classification and link prediction. Experiments are carried out on real networks in the context of graph contrastive learning, and the proposed strategies are compared with state - of - the - art GNN based learning methods. The proposed strategies incur low additional computation complexity compared to non - adaptive counterparts, and are compatible to operate in conjunction with various GNN   learning frameworks, including other fairness enhancement methods."
SP:7739dc9e37f7f1384f87d2e60281e5bb27fece99,"This paper considers the problem of estimating treatment effects from observational data in the presence of unmeasured confounders. A popular way to address this challenge is to utilize an instrumental variable ( IV ) for two - stage regression, i.e. 2SLS and variants, but they need to assume additive separability of noise and are limited to the linear setting. Recently, many nonlinear IV regression variants were proposed by regressing the treatment with confounder and the predicted treatment in the first stage, leading to confounding bias between predicted treatment and outcome in the second stage. In this paper, the authors propose a Confounder - balanced IV Regression ( CB - IV ) algorithm to jointly remove the bias from the observed data with IV regression and achieve better bias - variance trade - off for treatment effect estimation. Experiments on both synthetic and real - world datasets demonstrate that the proposed algorithm outperforms the state - of - the - art methods, including IV regression - based methods, for treatment effects estimation. The authors theoretically prove that CB -IV algorithm is also effective under the multiplicative assumption rather than the additive separable assumption."
SP:fdb68c39fce254b73310a3101b2fe97ba47e69fe,"This paper analyzes the adaptability of model - agnostic meta - learning ( MAML ) in linear and nonlinear settings. It is well known that NAL is a non - adaptive learning method, but it is not well studied in terms of its adaptability to new tasks. This paper tries to address this issue in a linear regression setting consisting of a mixture of easy and hard tasks, where the hardness of the tasks is related to the rate that gradient descent converges on the task. It shows that in order to achieve substantial gain over NAL, ( i ) there must be some discrepancy in hardness among the tasks, and ( ii ) the optimal solutions of the hard tasks must be closely packed with the center far from the center solutions in the easy tasks. The paper also highlights the importance of task hardness and task geography to the success of MAMM.    The main contributions of this paper are as follows : 1. The authors provide a few image classification experiments that support their insights for when and how to train a model agnostic model for hard tasks in practice. 2. They provide an image classification experiment that supports the insights of their theoretical analysis. 3. They conduct some experiments to show that the adaptivity of the proposed method is better than that of NAL."
SP:e8143c7880c16ee9ce7a544e0fd80f001b1b4f9f,"This paper proposes a method to learn both the hyperparameters and the variables of the linearized optimal minimization algorithm ( PALM ) based on unrolled source separation methods ( unfolding / unrolling ). The proposed method, LPALM, is based on the unrolled method of unfolding source separation ( unfolding and unrolling source separation ), which assumes a fixed dictionary of source separation matrices during the training and testing phases. In contrast to most existing unrolled methods, which assume a fixed known dictionary during the testing and training phases, this article further emphasizes on the ability to deal with variable mixing matrices ( a.k.a. kappa - dictionaries ).   The main contributions of the paper are as follows :   1. The authors propose a method for learning the hyperparmeters of the Linearized Optimized Minimization algorithm by learning both the Parameter and Variable Variables of the PALM algorithm. This is done by using the learned variables and Parameter Variance Matrix ( PDM ). 2. They show that the learned PDM can be used to learn the parameters of the algorithm without the need for hyperparameter selection. 3. They compare the performance of the proposed method with the state - of - the - art unrolled sources ( unrolled and unrolled ) in the semi - blind setting. 4. They also show that their method outperforms the other methods in the sparse source separation setting."
SP:7716315001949ab88c8a216302fe51bae872fc87,"This paper proposes a new method for improving the performance of transformers on the task of language modeling. The authors claim that transformers exhibit impressive scaling but their performance hinges on processing large amounts of data, and their computational and memory requirements grow quadratically with sequence length. Motivated by these considerations, the authors introduce a novel attention module called implicit self - attention and construct a Legendre Memory Unit based model that exhibits an O(n ) and O( n lnn ) ( or better ) dependency for memory and computation respectively. They show that for the same amount of training, their model improves the loss over transformers about as much as transformers improve over LSTMs. Additionally, they demonstrate that adding global self - attentions to the architecture of their architecture and the augmented model improves performance even further."
SP:832f422b3554e89702e13c8c5690ee26f2289e3b,"This paper presents a new method for generating images using generative adversarial networks ( GANs ) based on keypoint embeddings, LatentKeypointGAN. The key is a two - stage GAN that is trained end - to - end on the classical GAN objective with internal conditioning on a set of space keypoints. These keypoints are associated with an appearance embedding that respectively control the position and style of the generated objects and their parts. The paper shows that the key provides an interpretable latent space that can be used to re -arrange the generated images by re - positioning and exchanging keypoint embedded images. The authors also provide a new methodology for keypoint detection that contests established autoencoder methods.   The main contributions of the paper are as follows :   1. A new method, GAN - based method for unsupervised key point detection. This method is based on the key point embedding and is trained with two stages. The training scheme is disentangling the image into spatial and appearance factors without domain knowledge and supervision signals. 2. A set of images are generated using the GAN and the training scheme and the key points are used to train a new generation of keypoints and matching images."
SP:9206ae6e31077569313838504ef6daa89ad3b59c,"This paper studies the phenomenon of representation shrinkage and gradient explosion in fully - connected neural networks with layer normalization using the mean field formalism. The authors show that increasing the depth of the neural network leads to a gradient explosion or representation shrinking, and that the appearance of these phenomena is not restricted to a specific initialization scheme or a choice of activation function, but rather is an inherent property of the fully connected architecture itself. They also show that many popular normalization techniques fail to mitigate these problems.   The main contribution of this paper is to provide a non - perturbative analysis of signal propagation in neural networks by means of mean field normalization. They find that the maximum and variance of its singular values grow with depth even if the mean remains O(1 ). This is an interesting and interesting result that could be useful for the future development of deep neural networks."
SP:2177be818b5843c580c787f1b2d725154846feb6,"This paper proposes a line search approach for finding optimal step sizes for stochastic gradient descent in deep learning. The authors claim that classical line search approaches, designed for losses without inherent noise, are usually not applicable for deep learning because they do not scale to the full batch loss. They argue that recent empirical findings suggest that the full - batch loss behaves locally parabolically in the direction of noisy update step directions and that the trend of the optimal update step size changes slowly. Based on these findings, the authors propose a line - search method that approximates the fullbatch loss with a parabola estimated over several mini - batches. The learning rates are derived from such parabolas during training. In the experiments conducted, their approach mostly outperforms outperforms SGD tuned with a piece - wise constant learning rate schedule.   The authors also analyze the performance of their approach across datasets, models, and gradient noise levels. In particular, a comprehensive hyper - parameter, runtime, and memory consumption analysis is performed."
SP:62233782f9046c85617d9ccfe8427eae7d1c9da7,"This paper proposes a new variant of the Noise - Contrastive Estimation ( NCE ) method, called eNCE, which replaces the log loss in NCE with an exponential loss, and shows that the resulting condition is polynomial in the dimension and the parameter distance between P ∗ and Q when they belong to an exponential family. To address this, the authors propose a normalized gradient descent algorithm for NCE, for which the first solution is the first one that provides a provable polynomials rate for learning the parameters of the ground truth distribution. The authors claim this to be a simple and effective fix to the flatness of the loss landscape in many settings, as evidenced by experimental results on synthetic and MNIST datasets.   The main contribution of this paper is to provide a formal analysis of the reasons for the NCE ’s poor performance when an inappropriate noise distribution is used. Namely, they prove that these challenges arise due to an ill - behaved ( more precisely, flat ) loss landscape, and introduce a variant of NCE called e NCE which uses an exponential Loss to address this. Theoretically, both NCE and e.NCE can potentially suffer from numerical issues during optimization when P \in \mathbb{P } and Q are far ( this is an interesting direction for future work ). Nonetheless, the proposed solution seems to be the only one that addresses this problem."
SP:ceba6c1421b2d03863007fdaf029b8b946519c1b,"This paper studies the convergence of distributed SGD with Byzantine resilience ( BR ) and privacy - aware workers ( DP ) under the assumption that a fraction of the workers are malicious and the other fraction are honest and providing noisy information to the server to ensure privacy. The authors show that the integration of standard practices in DP and BR is not straightforward and show that many existing results that rely on Byzantine resilience are invalid under Byzantine faults, especially those relying on ( α, f ). To circumvent this shortcoming, the authors revisit the theory of ( re ) tuning SGD by re - parameterizing the learning algorithm. They show that by carefully re -tuning SGD, one can obtain reasonable learning accuracy while simultaneously guaranteeing DP and Byzantine resilience. To quantify this convergence guarantee, they present ( in Section 3.4 ) necessary and sufficient conditions for the ηapproximated VN condition to hold."
SP:bc783f0c829f90931535e63687d13172879631b3,"This paper considers the problem of source code editing with few exemplars. The editing exemplar, containing the original and modified support code snippets, showcases a certain editorial pattern, and code editing adapts the common pattern derived from few support exemplars to a query code snippet. In this work, the authors propose a novel deep learning approach to solve this code editing problem automatically. Their learning approach combines edit representations extracted from support exemplar and compositionally generalizes them to the query code editing via multi - extended similarities ensemble. Specifically, we parse the support and query code snippets using language - specific grammar into abstract syntax trees. We apply the similarities measurement in multiple extents from individual nodes to collective tree representations for query and support sample matching, and ensemble the matching results through a similarity - ranking error estimator. The authors evaluate the proposed method on C# and Python datasets, and show up to 8.6% absolute accuracy improvements compared to the baseline methods by 8.0 - 10.9% in terms of absolute accuracy."
SP:ca0c4bdb02f7d939fb6de38b6b446ced4b5984a0,"This paper presents a generative model for generating sequence data based on the relational constraints between different subcomponents of an example ( e.g. lines of a poem or measures of music ). The model consists of two parts : ( i ) a model to generate a set of constraints and ( ii ) a second model to produce data satisfying these constraints.   For model i, the authors propose a program synthesis algorithm that infers the constraints present in the training data, and then learns a Generative Model ( GMD ) based on GMD and constraint data. For model ii, they propose a GNN that learns a GMD based on constraint data and then generates data satisfying the constraints of the GNN. The authors experimentally show that their approach significantly improves over state - of - the - art methods for generating data based generative models for music and sequence data."
SP:692ae0c583a1585eff1a7d9c0d3b51b7879611cc,"This paper addresses the problem of set - to - hypergraph prediction, where the goal is to infer the set of relations for a given set of entities. This is a common abstraction for applications in particle physics, biological systems and combinatorial optimization. The authors address two common scaling problems encountered in such tasks : the exponentially growing number of hyperedges and the run - time complexity, both leading to higher memory requirements. They propose to predict and supervise the positive edges only, which changes the asymptotic memory scaling from exponential to linear. Second, they introduce a training method that encourages iterative refinement of the predicted hypergraph, which allows us to skip iterations in the backward pass for improved efficiency and constant memory usage. Third, they combine both contributions in a single set -to - hyper graph model that enables us to address problems with larger input set sizes.   The authors provide an in - depth ablation on each of their technical contributions and compare their model against prior work on common set - To - Hypergraph benchmarks. They show that their model outperforms the state - of - the - art, especially for larger sets."
SP:e3481fb6d8d1aa45d6ed4a454e781f5a2c30c57e,"This paper proposes a post - processing method to mitigate bias in state - of - the - art deep learning models. It is based on learning a shallow neural network, called the Ethical Module, which transforms the embeddings of a pre - trained model to give more representation power to the discriminated subgroups. The training is supervised by the von Mises - Fisher loss, whose hyperparameters allow to control the space allocated to each subgroup in the latent space. Besides being very simple, the resulting methodology is more stable and faster than most current methods of bias mitigation that are known to be unstable and computationally expensive."
SP:3fb5dcc8b8fb731e09c14b16480cada1c7ccfaa7,"This paper proposes a new method for learning from placebos in class - incremental learning ( CIL ) called pseudo - pseudo CIL, which leverages unlabeled placebo data from a free image stream to improve the KD effect for both logit distillation ( KD ) and feature distillation methods. The proposed method is based on a reinforcement learning - based algorithm that learns a policy to adaptively produce phase - specific functions to evaluate the quality of placebos. It is shown to significantly improve a number of top - performing CIL methods, in particular on higher - resolution benchmarks, e.g. ImageNet-1k and ImageNet - Subset, and with a lower memory budget for old class exemplars per class. The paper also provides extensive comparisons and visualizations for the method in three CIL benchmarks."
SP:506e0a888c03a955b708464eed3670c04baf4912,"This paper presents a method for learning energy - based models ( EBM ) based on auxiliary algorithms for sampling, inference, and learning on various discrete EBMs. Specifically, the authors propose an auxiliary algorithm for sampling the energy function of a neighborhood based on a composition of local moves to efficiently explore large neighborhoods. They also give a fast version of their algorithm that only queries the evaluation of energy function twice for each proposal via linearization of the proposed energy function. Empirically, they show that their path auxiliary algorithms considerably outperform other generic samplers on   discrete EBM models. They demonstrate that their methods significantly improve the efficiency on the energy - weighted parity model, weighted permutation model, Restricted Boltzmann Machine, and Factorized Hidden Markov Model. Their method can also learn competitive deep EBMs on discrete image data.   The main contributions of this paper are the following :   1. A new sampling method for sampling EBM based on MCMC with informed proposal is proposed. This is in contrast to previous work that only allows local updates as it requires evaluating all energy changes in the neighborhood. 2. An auxiliary algorithm is proposed for sampling EBMs based on an informed proposal that uses the composition of the local moves. 3. An algorithm for learning EBMs is proposed that uses a linearized version of linearized energy function to learn."
SP:4b466277aa5561a80c48d5e72559de4ce95f228b,"This paper presents Variational Predictive Routing ( VPR ), a neural probabilistic inference system that organizes latent representations of video features in a temporal hierarchy, based on their rates of change, thus modeling data as a hierarchical renewal process. By employing an event detection mechanism that relies solely on the system ’s latent representations ( without the need of a separate model ), VPR is able to dynamically adjust its internal state following changes in the observed features, promoting an optimal organisation of representations across the levels of the model's latent hierarchy. Experiments are conducted on several video datasets, showing the ability of VPR to discover changes, be cost effective, adjust representations to the dataset’s temporal factors of variation and produce farsighted and diverse rollouts in the environment, all of which are key advantages for agent learning and planning."
SP:459ef2e6bd7638020955dbb4d8ae1098619f7b95,This paper presents an end - to - end and single - stage method for image extraction based on global and attention - based local feature matching ( UGALR ). The method is based on two components : global feature extraction and local feature learning. Global feature extraction is done by searching for global features and applying attention to local features. Local feature learning is used to learn more accurate and semantic information about the global features. The proposed method is trained using a single stage that removes the re - ranking process and learning local features in the second stage. Experiments on two datasets ( Oxford and Paris ) show that the proposed method achieves state - of - the - art performance compared to other popular methods.    The main contributions of the paper are as follows : ( 1 ) It proposes a method that learns more accurate global and semantic local feature information through combining spatial and channel attention with the aid of intermediate supervision supervision. ( 2 ) It applies the learned global feature information to the image extraction process by removing the re-ranking process and learns more semantic information through the learning of local information.
SP:487cc308a1e8ee078c54b2158bcae47e920e73f8,"This paper proposes a new method, RotoGrad, to tackle the problem of negative transfer in multitask learning. In particular, the authors focus on homogenizing the gradient magnitude across tasks, or greedily change the gradient directions, in order to avoid future conflicts. To this end, they propose an algorithm that jointly homogenizes gradient magnitudes and directions, while ensuring training convergence. Empirically, they run extensive experiments to empirically demonstrate that the proposed method outperforms competing methods in several tasks, including multi - label classification in CelebA and CIFAR-10 and CelebA, as well as in computer vision tasks using the NYUv2 dataset."
SP:050cd8319d84a1bd8c2ccb930ba69b33c8fb6e60,"This paper proposes a model fusion framework for fusing heterogeneous neural networks, namely, unequal width and unequal depth neural networks. The proposed method is called Cross - Layer Alignment Fusion ( CLAFusion ) and it consists of three parts : cross - layer alignment, layer balancing method, and layer - wise model fusion method.   The authors first formulate the cross layer alignment as an assignment problem using layer representation and layer similarity, then propose a dynamic programming - based algorithm to solve it. Next, the authors propose two natural and fast methods to balance the number of layers between two networks. Finally, the experiments demonstrate the efficiency of CLAF Fusion on three different setups. In skill transfer, the framework successfully fuses two neural networks trained on heterogeneous data and improves the performance over the individual networks without retraining. In addition, the fused model from CLAF fusion serves as an efficient initialization when training residual networks. Furthermore, the proposed method shows potential applications for model compression and knowledge distillation in the teacher - student setting."
SP:f764eae15cd083fdb4eb2af09ac64c2d878a454f,"This paper studies the implicit regularization effect of SGD seen in supervised learning in the offline RL setting. Theoretical analysis shows that SGD with temporal difference learning ( TTL ) regularizer favors degenerate solutions compared to SGD without TTL. Empirically, the paper shows that offline RL with TTL regularizer results in feature co - adaptation, which causes performance degradation. Based on this observation, the authors propose explicit regularizer, DR3, which counteracts the undesirable effects of this implicit regularizer. DR3 can be combined with offline RL methods, such as offline value - based RL methods such as REM, CQL, and BRAC, and empirical results show that DR3 improves performance and stability in offline RL and robotic manipulation from images."
SP:6fd793b27123bf80504e2ad5957455b7ec311612,"This paper proposes HyperDQN ( Hyper - Q - Value Variance Variance ), a method for generating posterior samples of randomized least - square value iteration ( RLSVI ) under the assumption that a good feature of the Q - value function is known in advance and is fixed during the training. The main contribution of this paper is to propose a hyper - model, which is a modified version of the meta - model ( meta - RL ) that outputs the parameters of the base model. The hyper model has three main advantages over the meta model : First, it can generate approximate posterior samples regarding the parameter of the function. Second, the posterior samples can be obtained in a more efficient way than the existing methods, and third, the changing feature does not affect the efficiency of the algorithm. The method is evaluated on the Atari suite and compared against DQN, OPIQ, OB2I, BootDQ, and NoisyNet on the SuperMarioBros benchmark. The experimental results show that the proposed method is more efficient than the other exploration methods."
SP:b428383660928374c953f659ea1e05852dbdcd6e,"This paper proposes to learn causal representation from observational data by regularizing the learning procedure with mutual information measures according to a hypothetical causal graph. The optimization involves a counterfactual loss, based on which the authors deduce a theoretical guarantee that the causality - inspired learning is with reduced sample complexity and better generalization ability. Extensive experiments show that the models trained on causal representations learned by our approach is robust under adversarial attacks and the distribution shift."
SP:1258c05a80a17949b50e6dae13deea1d2235f456,"This paper proposes ProgFed, the first progressive learning framework for efficient and effective distributed learning for federated learning. It aims to reduce the computation and two - way communication costs while maintaining the strong performance of the final models. The authors theoretically prove that Prog Fed converges at the same asymptotic rate as standard training on full models. They conduct extensive experiments on various datasets ( CIFAR-10/100, EMNIST and BraTS ) and architectures ( VGG, ResNet, ConvNets, 3D - Unet ) to show that their method can save around 25% computation cost, up to 32% two - ways communication costs in federated classification, and 63% in Federated segmentation without sacrificing performance. The experiments also show that our method complements classical compression and appears robust against compression errors. It may motivate more advanced compression schemes based on progressive learning. With these combined techniques, they are able to reduce communication of up to 50× at only 0.1% loss in utility."
SP:8cdaa6e0dafd750ebdb5d7a4c1987a042400662f,"This paper studies the generalization of adversarial training through the lens of the adversarial Rademacher complexity of deep neural networks. The authors provide a method to overcome this issue and provide upper bounds of the maximum adversarial complexity of neural networks that can be used to generalize well to adversarial examples on the test set.   The main contribution of this paper is to provide the analysis of the upper bounds on the maximum Adversarial Rademaacher complexity for deep neural network training. The upper bounds are based on the fact that the neural network weights are bound by the product of the weight norms of the trained and the adversarially - trained model, and the authors show that the larger the weight norm, the more likely it is that the trained model generalizes well to the examples that are adversarial to the training data. The method is applied to two - layer neural networks, and it is shown that it is possible to remove the second layer from the analysis using the same techniques as the first layer analysis for standard training. However, it is not shown that this method is applicable to the third layer neural network setting, which is the one that is more commonly used in adversarial settings. In addition, the authors also provide experiments to analyze the relationship between the upper and lower bounds."
SP:925d6bb051e9b384669fb695085b678c11f7c11a,"This paper proposes a new estimator for differential entropy based on kernel - based estimators of entropy. The estimator is based on a kernel based kernel estimator that is parametrized and parameterized. The method is flexible enough to be applied to both continuous and discrete entropy. This allows the estimator to be used as a conditional estimator on either continuous or discrete entropy, as well as mutual information. The authors empirically validate their method on high - dimensional synthetic data and further apply it to guide the training of neural networks for real - world tasks. Experiments on a large variety of tasks, including visual domain adaptation, textual fair classification, and textual fine - tuning demonstrate the effectiveness of KNIFE - based estimation."
SP:d2f3beac855f0d72c13552fecb2bdb9d42195df3,"The paper introduces resmax, a variant of the soft - greedy - greedy operator that is similar to softmax in the sense that it assigns distinct probabilities to actions based on the estimated action - values. However, unlike softmax, the probability for taking each action is determined using its suboptimality gap, which is inversely proportional to the difference between the approximated value of the greedy action and the given action, and avoids the use of the exponential that causes softmax to overemphasize actions that appear high - valued during learning. The authors prove that resmax is a non - expansion operator, and so combines well with generalized value iteration algorithms. They show that it ensures a minimal probability on each action regardless of the action-values, ensuring all actions are explored. They conclude with an empirical study, across a variety of hard and easy exploration problems, with tabular and deep function approximation. They find that resMax outperforms softmax and especially when softmax suffers from overestimation."
SP:792ae8808aa6902758146aef1548c975492b833c,"This paper presents a method for controlling the model ’s learnability on a specific dataset with a special key. The key is an adversarial invertible transformation that can be viewed as a mapping from image to image, to slightly modify data samples so that they become “unlearnable ” by machine learning models with negligible loss of visual features. Meanwhile, one can unlock the learnability of the dataset and train models using the corresponding corresponding key.   The proposed learnability lock leverages class-wise class - wise perturbation that applies a universal transformation function on data samples of the same label. This ensures that the model can be easily restored with a simple inverse transformation while remaining difficult to be detected or reverse - engineered. The authors empirically demonstrate the success and practicability of their method on visual classification tasks. They demonstrate that their pipeline works well on common image datasets and is more robust to defensive techniques such as adversarial training."
SP:9af10703605e620e563241e2602a50b629f3d37a,"This paper presents a general approach for handling missing features in graph machine learning applications that is based on minimization of the Dirichlet energy and leads to a diffusion - type differential equation on the graph. The discretization of this equation produces a simple, fast and scalable algorithm which is called Feature Propagation. The authors experimentally show that the proposed approach outperforms previous methods on seven common node - classification benchmarks and can withstand surprisingly high rates of missing features.    The main contribution of this paper is to propose an approach to handle missing features of Graph Neural Networks ( GNNs ). The proposed approach is similar to the one proposed in [ 1 ] and [ 2 ], but it is different in the sense that it does not rely on the availability of the node or edge features of the graph, but rather on the minimisation of the energy minimizer. The main difference between these two approaches is that the former minimizes the energy and the latter minimises the energy with respect to the graph distribution. This allows the former to be discretized into a simple and fast algorithm, which is then used to solve the diffusion equation. The algorithm is shown to outperform the previous methods in terms of accuracy and reconstruction time. The major contribution of the paper is that it proposes an approach that is fast, scalable and relatively easy to implement. The method is tested on two datasets, GCNMF and PaGNN, and it is shown that it outperforms the other methods on both of them."
SP:cbaa3f1379fa99159899d79ccb479c0187403aca,This paper proposes a novel active learning strategy for training a model with limited labeled data by selecting a core subset of an unlabeled data set to label in an active learning regime where less than 1 % of the data set is labeled. The authors propose a Generalized Benders Decomposition ( GBC ) algorithm for active learning by minimizing the discrete Wasserstein distance between the labeled data set and the unlabeling data set. They show that this problem can be tractably solved with the GBC algorithm using high - quality latent features that can be obtained by unsupervised learning on the unlabelled data set using the full data set versus a subset via a discrete subset via the Wassersteins distance. Numerical results on several data sets show that their optimization approach is competitive with baselines and outperforms them in the low budget regime.   The main contributions of this paper are as follows :   1 ) The authors introduce an integer optimization problem for selecting the core set that minimizes the discrete distance between labeled data and the unlabeled data. 2 ) They derive a new deterministic bound on the difference in the difference between the difference on the labeled and unlabelED data set for the active learning problem. 3 ) They develop a globally convergent customized algorithm for this problem using GBC and show that it outperforms baselines. 4 ) They provide a low budget active learning method for image classification and domain adaptation where up to 400 images can be labeled.
SP:4c72923f78ca6590dc11e10d1a2403076a583718,"This paper proposes a method to solve the de novo genome assembly problem by applying geometric deep learning to a graph convolutional network trained on a dataset generated from human genomic data to reconstruct the genome by finding a path through the assembly graph by learning a score from the lengths of the overlaps between the sequences and the graph topology which is computed with a greedy algorithm, when traversing with the greedy search over the overlap lengths only. The authors show that their method reconstructs the correct path in the fraction of the time required for the state - of - the - art de - novo assemblers. This favourable result paves the way for the development of powerful graph learning algorithms that can solve the graph machine learning problem much quicker and possibly more accurately than human handcrafted techniques, the authors argue.   The main contribution of this paper is the introduction of a previously untaken path of applying geometricDeep learning to the central part of the genome assembly — untangling a large assembly graph from which a genomic sequence needs to be reconstructed. This is achieved by training a neural network that learns a score for each graph in the dataset from which the reconstructed genomic sequence is drawn. The network is trained using a dataset that consists of a mixture of synthetic and human genomic sequences. The model is trained on the synthetic data and the dataset is used to train the neural network. The experiments are conducted to validate the effectiveness of the proposed method."
SP:24de906e4289c9073b6c55c747b0913b8df5e053,"This paper proposes a method to incorporate experience replay ( ER ) into the meta - training procedure of OML Javed & white ( OML ), a state - of - the - art meta - learning algorithm for continual learning. The idea is that ER can be used for meta - testing but not for optimally meta - trained training. The paper proposes to store the samples ’ representations, instead of the samples themselves, into the replay buffer and use it to select the samples that can best help overcome catastrophic forgetting. Experimental results on a number of real - world benchmark data sets demonstrate that the proposed method outperforms the current state of the art in terms of generalization and discriminative performance."
SP:3c78454f053f74930979a8054cd7c8a34b6fe63d,"This paper proposes a new method for multi - agent joint Q - learning based on deep neural networks, called Explicit Credit Assignment Joint Q - Learning ( ECAQ ). It is based on the observation that most of the existing methods implicitly learn the credit assignment just by ensuring that the joint Q-value satisfies the Bellman optimality equation. In contrast, the authors formulate an explicit credit assignment problem where each agent gives its suggestion about how to weight individual Q - values to maximize the joint one, in addition to guaranteeing the optimality of the joint value. Theoretically, it is shown that the optimal Q - value can be found in the gradient ascent solution for this problem. Empirically, they approximate the core idea with deep neural network and propose Explicit Credit - Assigned - Joint - Q learning. Experiments are conducted on several challenging tasks to demonstrate the effectiveness of the proposed method. The results demonstrate that the method achieves interpretable credit assignment and superior performance compared to advanced baselines."
SP:0d2b225ac697679d10df25f371b2a718d4949b42,"This paper proposes Greedy Model Space Attack ( GMSA ), an attack framework that can serve as a new baseline for evaluating transductive learning based defenses for evaluating adversarial robustness. The main contribution of this paper is to analyze the robustness of various defenses based on adversarial training and threat models. Theoretically, attacking these defenses reduces to solving a bilevel optimization problem, which poses difficulty in crafting adaptive attacks. In this paper, the authors formulate and analyze threat models for Transductive - Learning based defenses, and point out important subtleties in the defense mechanisms. They propose the principle of attacking model space and present GMSA that can break instantiations of GMSA.   The authors perform a systematic empirical study on various defenses. They show that even weak instantiations that are not protected by GMSA can break respective defenses. Specifically, for defenses that are adversarially training, they reduce the robust accuracy to that of adversarial learning alone. On the positive side, they report a somewhat surprising empirical result of “ trans - ductive adversarial trained ” : Adversarially retraining the model using fresh randomness at the test time gives a significant increase in robustness against attacks that we consider."
SP:e7024cae196fc5eb6a62d289a95d76b532b6a36c,"This paper studies the problem of batch normalization of neural network training. It is well known that normalization methods have been effective in speeding up the training of neural networks. However, the reason for its success is not well understood. The authors cast normalization as an approximation of the limiting case where the entire dataset is normalized jointly, and explore other ways to approximate the gradient from this limiting case. They demonstrate an approximation that removes the need to keep more than one example in memory at any given time, at the cost of a small factor increase in the training step computation. They further use their insights to improve batch renormalization for very small minibatches.   The main contribution of this paper is to develop a method for performing per - example normalization in a way that does not modify the inference - time architecture of the inference model. The method consists of two parts : ( 1 ) a per - sample per - gradient computation, which removes the extra extra information from the training procedure, ( 2 ) an aggregation step that joins the information from multiple examples similar to the way that the gradients with respect to the model parameters are commonly averaged over the minibatch in SGD, and ( 3 ) a method that performs well in the absence of identity shortcuts."
SP:4aa42984fcb0fd66936d668477b2719ef5c427d4,"This paper proposes a method to reduce the number of parameters that the pre - trained Transformer model has to work with when fine - tuning. The method, called Low - Rank Adaptation ( LoRA ) freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, thereby greatly reducing the total number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine - tuned with Adam, LoRA, when combined with the memory requirement by 3 times, can reduce the size of the parameters by 10,000 times. The authors also provide an empirical investigation into rank - deficiency in language model adaptation, which sheds light on the efficacy of LoRA and shows that LoRA performs on par or better than the state - of - the - art."
SP:b77a00beb0802f47810b03d3c4aa24d92781414f,"This paper proposes a new method for regular - constrained conditional random fields ( CRF ) based on regular language constraints. The authors show that their method, RegCCRF, can be used as a drop - in replacement for standard CRF and outperforms them empirically in semantic role labeling on a standard dataset. They also show that the proposed method is distinct from approaches that enforce constraints at decoding time, and that it better approximates the true data distribution.   The main contributions of the paper are as follows :   1. This paper proposes an approach to regularize the Markov assumption in CRF based on a regular language constraint. This is done by treating the space of possible output structures of a CRF as a set of regular language L and using regular expressions to specify the constraints. 2. This approach is tested empirically on a dataset where it outperforms the state - of - the - art in semantic labeling. 3. It is shown that the method is able to learn non - local dependencies in the output distribution, which is important for structured prediction."
SP:74c186a96c12adff178264aa84ace8d04dc7d725,"This paper proposes two neural networks for camera - based physiological measurement. The first network is a convolutional backbone network, which is a combination of a visual transformer and a neural architecture. The second network is an end - to - end neural network, where each layer consists of several convolutions. The authors evaluate the performance of the proposed network on three popular benchmark datasets : CIFAR-10, MNIST and Fashion - MNIST. They show that the proposed networks achieve state - of - the - art performance. They also evaluate the latency of the networks and show that their network achieves a 33% improvement in efficiency."
SP:3003bab6e3f7e2e21cd6cf27ee7d483d877d9fb3,"This paper proposes Hardware - Aware Latency Pruning ( HALP ), a pruning method that considers network pruning as a global resource allocation optimization problem, aiming at maximizing the accuracy while constraining the latency under a predefined budget. The authors compare HALP on both classification and detection tasks, over varying networks, on ImageNet and VOC datasets, and demonstrate that their method yields consistent latency and accuracy improvements over state - of - the - art methods. They also demonstrate that ImageNet pruning results present a viable 1.6× to 1.9× speedup while preserving very similar original accuracy of the ResNets. For SSD pruning on VOC, HALP improves throughput by 1.9494× with only a 0.56 mAP drop."
SP:c44d676c09c8e5a70d73b21b507b41a422fec809,"This paper proposes GraphEBM, a molecular graph generation method via energy - based models ( EBMs ). It is an exploratory work to perform permutation invariant and multi - objective molecule generation. The energy function is learned by contrastive divergence and generate samples by Langevin dynamics. To generate molecules with a specific desirable property, the authors propose a simple yet effective learning strategy, which pushes down energies with flexible degrees according to the properties of corresponding molecules. The authors empirically demonstrate that their approach is effective via random, single - objective and multiobjective molecules generation."
SP:70e60fa5deef3e3ba77d05d0c3e0e7fbf396aa1d,"This paper proposes a neural - based program synthesis method, CROSSBEAM, which uses a neural model to learn a hands - on search policy to combine previously explored programs into new programs, taking into account the search history and partial program executions. The model is trained on - policy using data extracted from its own bottom - up searches on training tasks. The method is evaluated on two very different domains, string manipulation and logic programming, and the authors observe that CROSS BEAM achieves nearly 100 % success rate on tasks with large enough solutions that prior state - of - the - art has a 0% success rate.   The main contributions of the paper are as follows :   1. A neural model is used to learn the search policy for program synthesis, instead of relying on a combinatorial search algorithm. The neural model learns to explore the search space efficiently, exploring much smaller portions of the program space compared to the state -of - art methods. 2. The approach is evaluated in the string manipulation domain, where it is shown to solve 62% more tasks within 50K candidate expressions than BUSTLE ( Odena et al. 2021 ) on the same test sets used in the BUSTle paper. 3. In the inductive logic programming domain, the approach is compared with the state of the art methods on two different domains."
SP:daa044ffefe80bae16b014f60061d941ed8c2ba6,"This paper proposes a method to replace the standard squared Bellman error regularizer in Deep Reinforcement Learning ( DQL ) with a more stable alternative that relies on target networks to stabilize training. The proposed method is based on replacing the standard regularizer with an explicit regularizer that allows to use up - to - date parameters as well as control the regularization. The method is validated on four environments from the Four Rooms environment ( Bellemare et al., 1999 ) with DNNs function approximation and shows that it can approximate the true value function and learn quickly compared to the standard DQL method. It is also shown that DQL methods outperform popular algorithms based on the squared Bell man error on a subset of the Atari suite. The main contributions of the paper are as follows :   1. The authors propose a new regularizer for the squared bellman error that replaces the standard one used in DQL. The regularizer is explicit, which allows for more stable training and faster adaptation to fast - changing target Q - values. 2. They show that their method outperforms the standard method on four Atari environments."
SP:dd174014d056a7d2bc86ee99119841eafa62ed52,"This paper proposes a new perspective on Graph Neural Networks ( GNNs ) based on GraphSNN, a novel neural model based on graph structure and isomorphism theory. Theoretically, the authors show that the graph structure of graphs is more expressive than the Weisfeiler Lehman test in distinguishing graph structures than the Lehman - based methods. The authors then propose to use this graph structure as the basis for a message - passing aggregation scheme, where the message is passed through a hierarchy of graph subgraphs based on isomorphicity theory. They show that their graph structure based model outperforms Lehman based methods on three benchmark graph learning tasks."
SP:beb9ba0261e176bfc50e9bf5bed2b6169d388285,"This paper proposes a new method for uncertainty quantification ( PIs ) based on linear combinations of neural networks ( NNs ), named PI3NN. It addresses three major issues with the state - of - the - art PI methods, i.e., existing PI methods require retraining of NNs for every given confidence level and suffer from the crossing issue in calculating multiple PIs. Also, they usually underestimate uncertainties of OOD samples leading to over - confident PIs, which is problematic. The proposed method calculates PIs from linear combination of three NNs, each of which is independently trained using the standard mean squared error loss. The coefficients of the linear combinations are computed using root - finding algorithms to ensure tight PIs for a given confidence levels. The method is evaluated on several benchmarks and real - world experiments. The results show that the proposed method outperforms several previous methods in terms of uncertainty quality, robustness, and OOD sample identification."
SP:4b44a834e2212bacb4c2d9408a81f1efc76a670b,"This paper proposes Fully Online Meta - Learning ( FOML ), a meta - learning method that does not require any ground truth knowledge about the task boundaries and stays fully online without resetting the online parameters back to the meta - parameters between tasks, instead updating the parameters continually in a fully online fashion. The method is compared with several state - of - the - art methods in two datasets, Rainbow - MNIST and CIFAR - 100, and compared with a buffer of previously seen data based method. The results show that the proposed method is able to learn new tasks faster and achieves lower error rates, both on a simple sequential image classification task from prior work and a more complex benchmark that is proposed based on the Cifar100 dataset, with a sequence of 1200 tasks.   The main contribution of this paper is an online meta - Learning algorithm that continually updates its online parameters with each new datapoint or batch of datapolits, while simultaneously performing meta - gradient updates on a separate set of meta - parameterized data. The main contributions of the paper are the following : 1. Demonstrating that FOMM can learn new task boundaries faster than the state of the art online learning methods, 2. Comparisons with the buffer based method, and 3. Showing that the new method achieves better error rates than the baseline method."
SP:fbae35cb171b3a3eb7c5d4bc83881ed7c4a70aae,This paper proposes a differentiable scaffolding tree ( DST ) for molecular optimization. The DST consists of a graph neural network ( GNN ) and a learned knowledge network ( learned graph parameters ). The GNN is used to backpropagate the derivatives from the target properties to the derivatives of the learned network. The learned parameters are then used to perform gradient - based optimization on a chemical graph structure by back - propagating the derivatives through the GNN. The authors claim that the DST is both sample - efficient and effective. They also provide an explanation that helps domain experts understand the model.
SP:61b59899cf6ae442d9f8f5226e79708a4280cfb2,"This paper proposes a method to predict the lab test response of patients based on drug - lab interactions and diagnosis - lab interaction graphs. The method is based on the knowledge of drug interactions between drugs, co - morbidities, and lab test results and uses a knowledge - augmented approach to predict patients ’ response for a target lab result. It also takes into consideration patients’ past lab responses to personalize the prediction. Experiments on real - world datasets demonstrate the effectiveness of the proposed solution in reducing prediction errors by a significant margin. Case studies show that the identified factors for influencing the predicted results are consistent with clinicians ’ understanding."
SP:8623cebb515c4a736427449b46ad2cdf8b806b77,"This paper proposes a new method for the open - set single domain generalization ( OS - SDG ) setting, where only one source domain is available to train the model. The proposed method is based on the adversarial data augmentation strategy, which complements the diversity of source domain to learn a robust model. In this paper, the authors propose CrossMatch to generate auxiliary samples that are potentially out of the source label space during training. The auxiliary samples may not belong to the actual unknown classes in the target domains, but they could still assist in identifying whether a sample belongs to known classes or not. The authors also adopt a consistency regularization on generated auxiliary samples between multibinary classifiers and the model trained by SDG methods, to improve the model ’s capability on unknown class identification. Experimental results on benchmark datasets prove the effectiveness of CrossMatch on enhancing the performance of the proposed method in the OS -SDG setting."
SP:126f8ffb855aa22eda4d681a499953879ed3679e,"This paper proposes two extensions of Wasserstein policy optimization ( WPO ) and Sinkhorn policy optimization in reinforcement learning ( SPO ) based on Lagrangian duality. Theoretically, they show that WPO guarantees a monotonic performance improvement, and SPO provably converges to WPO as the entropic regularizer diminishes. Experiments across tabular domains and robotic locomotion tasks further demonstrate the performance improvement of both approaches, more robustness of WPO to sample insufficiency, and faster convergence of SPO.   The authors also point out that a recent concurrent work by Wang et al. ( 2021a ) studied DRO using the DRO formulation of the Lagrangians. They show that the similarity shared in the duality formulations is fundamentally different from constrained policy optimization from DRO problems, and that DRO is a different problem."
SP:999eacf6500c87205584a3256d7ca45b3016fb1c,"The paper proposes a forgetting - and - re - learning paradigm for training neural networks. The forgetting step selectively removes undesirable information from the model, and the relearning step reinforces features that are consistently useful under different conditions.   The forgetting operation can be leveraged to improve upon existing algorithms by designing more targeted forgetting operations. For example, the forgetting operation could be used in Section 5 to improve the performance of ReLU by retraining the model in a way that leads to parameter values with better generalization properties. The proposed forgetting operation has the potential to unify many existing iterative training algorithms in the image classification and language emergence literature, and allows to understand the success of these algorithms in terms of the disproportionate forgetting of undesirable information."
SP:2789859517b6624730b14a7e010444a72d3dd3ed,"This paper studies the problem of training batch agents. The authors argue that the performance of batch agents is limited in both theory and practice without strong assumptions on the data - collection process e.g. sufficient coverage or a good policy. To address this problem, the authors propose to train agents in an offline - online setting where the agent has access to a batch of data to train on but is also allowed to learn during the evaluation phase in an online manner. This is an extension to batch RL, allowing the agent to adapt to new situations without having to precommit to a policy. In the experiments, they find that standard RL agents trained in this way outperform agents trained only offline or online, sometimes by a large margin, highlighting the potential of this new setting. They also find that different amounts of training are best for different agents in each environment. Taken together, these experiments highlight the difficulty of batch RL."
SP:76625a25e770415599a34122110d61cb3b7e614c,"This paper proposes a method to tackle the problem of domain generalization ( DG ) via learning to reduce domain shift with an episodic training procedure. In particular, it learns to optimize Y - discrepancy between the unseen target domain and source domains only using source - domain samples. The theoretical analyses show that there is a tradeoff between classification performance and computational complexity for discrepancy - optimal meta - learning. Theoretical results also shed light on a bilevel optimization algorithm for DG. Empirically, the authors conduct experiments on DomainBed ( Gulrajani & Lopez -Paz, 2020 ) and evaluate on two DG benchmarks. Results show that their method is highly effective and achieves state - of - the - art performances."
SP:6421a9759c766641fd8c128a249f1a9c5699d19c,"This paper studies the problem of best - first search ( BFS ) for PSPACE planning in Sokoban, SAT, CSP, and Monte Carlo tree search with Deep Neural Networks ( DNN ). It is well known that PSPACE is a NP - complete problem that is hard for specialized solvers to solve due to the exponential combinatorial search space. This paper proposes two approaches to tackle this problem. The first is a DNN - based approach where a policy network is used to predict the best way to search for the optimal solution. The second approach is a combination of DNNs and a heuristic approach, where a random restart strategy is used on top of the policy network to prevent the search space from becoming too large. The authors study the effectiveness of both the policy and value networks in solving BFS and CSP. They show that the value network is more effective than the policy in solving the BFS problem. They also show the importance of restarting the search more often to prevent left and right heavy tails in DNN search. They provide extensive experiment data to support their findings."
SP:84c415bc0f120d1997289f91661ff74e7297d3bd,"Meta - Imitation Learning is a promising technique for the robot to learn a new task from observing one or a few human demonstrations. However, it usually requires a significant number of demonstrations both from humans and robots during the meta - training phase, which is a laborious and hard work for data collection, especially in recording the actions and specifying the correspondence between human and robot. In this work, the authors present an approach of meta - imitation learning by watching video demonstrations from humans. In comparison to prior works, their approach is able to translate human videos into practical robot demonstrations and train the meta-policy with adaptive loss based on the quality of the translated data.   The authors propose an approach with existing meta - meta - learning methods which learns meta - policy with both human and robotic demonstrations on a set of challenging tasks. They show that their method achieves comparable results to the baselines in one - shot new task learning with the same human video demonstrations, while their method achieved higher performance to the baseline on the set of vision - based tasks through watching."
SP:fedf5c75e83d6ab41ef9d5daa9054ffe4e424ec2,"This paper studies the generalization performance of deep neural networks trained with gradient - based optimizers. It is well known that adaptive optimizers lag behind SGD in terms of generalization, mainly in the image classification domain. The authors argue that this is due to the following factors : ( 1 ) output scores and network weights are too large, causing training loss to become too small and the network to lose its adaptivity ( ability to move around and escape regions of poor generalization ) in the weight space ; ( 2 ) Adaptive optimizers like Adam, being aggressive at optimizing the train loss, are particularly affected by this ; ( 3 ) even with weight decay (WD ) and normal hyper - parameter tuning, adaptive optimizer like Adam still lag behind with SGD a lot. To address this issue, the authors propose to fine - tune parameters like learning rate and batch normalization to improve the performance of the network. The main contributions of the paper are as follows : ( i ) The authors identify the problem of over - parameterized deep networks : without appropriately tuned regularization, such networks have the tendency to make output scores ( logits ) and network weight ( weights ) large, which in turn causes generalization to be poor. ( ii ) They propose to fix this problem by carefully tuning parameters such as learning rate, batch normalisation and batch size.   The authors also propose to train large - batch models by speeding up the training process by leveraging parallel parallel GPUs. They argue that large batch sizes are useful for speeding up training process."
SP:819df8d847a99f13ed5efdcabae8b464c12b464b,"This paper proposes a new method for learning equivariance - based neural network architectures, called partial G - CNNs. The idea is to learn a network that is equivariant with respect to the chosen symmetries of the data, but only when they are present in the data. The authors argue that this is beneficial when the data comes from a discrete group ( e.g. rotated MNIST ), but harmful when they do not. For example, when the distribution of data is based on a subset of a group rather than the group as a whole, a model that respects the chosen symmetry is better suited to represent the data than one that does not. The paper empirically shows that the proposed method is able to match the performance of conventional G -CNNs for tasks where full equivariancy is necessary, and outperform them when it is not.   The authors also show that the method is applicable to discrete groups, continuous groups, and combinations thereof. The main contributions of the paper are the following :   ( 1 ) The authors propose a method to learn equivariances for the data using a family of networks, which they call the Equivariant Convolutional Neural Networks ( ENNs ). They show that this method is equivalent to learning a full ENN and a partial ENN, and show that it can outperform other methods such as Natural Image Classification ( NIC ) and Natural Image Synthesis ( NIS ). The method also shows that it is possible to learn an ENN that is a partial and full Equivariance when necessary, which outperforms other methods like NIC and NIS, and vice versa. The methods are tested on a variety of tasks such as image classification, rotation, MNIST, and face classification."
SP:0c0ca9df96f1fa2eb8b83a47d0d5964590fef290,"This paper proposes a new method to improve the scalability of Langevin dynamics by replacing the datapoint - wise iterations of MCMC with updates of an inference model that maps observations into latent variables. The proposed method, called amortized Langevin Dynamics ( ALD ), is based on a new deep latent variable model named the Langevin autoencoder ( LAE ). Based on the new model, the authors show that ALD can obtain samples from distributions in both conditional and unconditional cases, and that it converges significantly faster than traditional LD. The authors also show that LAE can generate better samples in terms of the Fréchet Inception Distance ( FID ) compared to VAE - based methods."
SP:5631097031c7e599bdeae64366ffa6e4558837c6,"This paper studies the problem of hypergraph reasoning, i.e. predicting the relationship between several entities based on the input facts. The authors observe that logical rules ( e.g., my parent’s parent is my grandparent ) usually apply locally, and sparsely in the real world. Inspired by these observations, the authors propose Sparse and Local Neural Neural Logic Machines ( SpaLoc ), a structured neural network for hyper graph reasoning. To leverage the sparsity in hypergraph neural networks, SpaLoc represents the grounding of relationships such as parent and grandparent as sparse tensors and uses neural networks and finite - domain quantification operations to infer new facts.   The paper shows superior accuracy and efficiency on synthetic datasets compared with prior art and achieves state - of - the - art performance on several real - world knowledge graph reasoning benchmarks. The paper also shows that training via sub - graph sampling and label calibration enables the model to learn relational rules in real world knowledge graphs with more than 10K nodes, whereas the original NLM can be barely applied to graphs with over 100 nodes."
SP:9657121b01c51f78c00d06b47d3e8d678dd85d54,"This paper proposes to relax the assumption that k is a positive integer, and proposes to draw k from a probability distribution for training. The authors combine this with recent advances in differentiable sorting and ranking to propose a new family of differentiable top - k cross - entropy classification losses. They find that relaxing k does not only produce better top - 5 accuracies, but also makes models more robust, which leads to top - 1 accuracy improvements. They achieve new state - of - the - art results ( for publicly available models ) on ImageNet1K with an 88.37% top -1 accuracy and a 98.68 % top -5 accuracy. They also propose splitter selection nets, which require fewer layers than existing selection nets.   The main contributions of the paper are as follows :   - The authors derive a novel family of top -k cross - k losses and relax the assumed assumption of a fixed k in the assumption of the probability distribution. - They show that the scalability of their scalability method is more than twice as good as that of 10,000 classes, and they demonstrate its scalability to more than 10 classes."
SP:cb3188f435c54a365890e20e4d582c250d919833,"This paper proposes a new method for solving optimal transport ( OT ) problems based on the Douglas - Rachford splitting ( DR ) method. The proposed method is based on solving the original OT problem directly instead of solving an approximate regularized problem, as many state - of - the - art techniques do. This allows the method to provide sparse transport plans and avoid numerical issues of methods that use entropic regularization. The algorithm has the same cost per iteration as the popular Sinkhorn method, and each iteration can be executed efficiently, in parallel, in the same order as the previous iterations. In addition, the authors establish a linear convergence rate for the formulation of the OT problem. The authors also detail an efficient GPU implementation of the proposed method that maintains a primal - dual stopping criterion at the cost of no extra extra cost."
SP:9a087cc734a3e7f3ab848bef5e2eff37fe40f303,"This paper presents a systematic study of generalization in federated learning ( FL ), focusing on six tasks. The main contributions are as follows :   1 ) The authors propose a semantic synthesis strategy that enables realistic simulation without naturally - partitioned data. 2 ) They observe and explain differences in behavior across natural and synthetic federated datasets, indicating that dataset synthesis strategy can be important for realistic simulation. 3 ) Informed by their findings, they call out community suggestions for future FL works."
SP:da0e8c89f343abfe500eb4c1968e418c2fb52ef6,"This paper studies the few - shot setting of prompt - based language models ( PLMs ) for the GLUE language understanding task. The authors claim that the performance of PLMs under the zero shot setting is comparable to that of BERT - trained language models on the widely - used IMDB and Amazon datasets. However, they observe some limitations of using PLMs for zero - shot tasks, particularly for the language understanding tasks ( e.g. GLUE ). To address these limitations, the authors propose two strategies : ( 1 ) test some basic models adapted from the few shot setting and ( 2 ) propose two new strategies for zero shots setting in Section 3, following with a coarseto - fine study in Section 4. The experimental results show that the proposed strategies can yield very promising results on a few widely -used datasets.   The paper also presents some preliminaries about the utilization of the PLMs in Section 2, and it is suggested that the BERT family should be used for the zero - shots setting. The rest of the paper is organized as follows :   1 ) Preliminary analysis of the state - of - the - art BERT language model. This paper analyzes the accuracy of the model on the IMDB dataset, and 86.22 % accuracy on the Amazon dataset, which outperforms manually created prompts without engineering in achieving much better and stable performance. 2 ) The accuracy of 74.06 % (±13.04 % ) on Amazon dataset. 3 ) Experiments on the Multi - Null Prompting dataset, where the accuracy is achieved by using two different strategies."
SP:9817dccb1a121058b23a2ef825ed339cf8b53674,"This paper proposes a new attention mechanism, Attention Mechanism ( MAC ), that aims to align relevant parts of the encoded image with the target output. The authors claim that the existing attention methods fail to build clear alignment because the aligned parts are unable to well represent the target. They propose to use an attention module that deliberately locates the target in an image region and refines representation to be target - specific. They show that the alignment and interpretability of attention can be significantly improved by using this new attention method. Experiments on synthetic handwritten 8 digit as well as real - world scene text recognition datasets show that their approach outperforms the mainstream ones such as soft and hard attention."
SP:3913ed3b3cf6494368e3be6cacb637ff85f80ee6,This paper presents an approach to solving the vehicle routing problem using reinforcement learning and supervised learning. The authors propose a supervised learning framework that constructs a complete tour plan from scratch while respecting an apriori fixed number of available vehicles. They compare their approach against several state - of - the - art deep learning and OR solvers through a thorough and unified experimental evaluation protocol that ensures comparability amongst different ML - based approaches. They show that their approach delivers competitive results in comparison to approaches that work only for unbounded fleet sizes and outperforms these methods when considering fixed vehicle costs. They also shed some light on existent inconsistencies in the experimentation protocols of the related work related to their work.
SP:594a813c0d0baa66738b9c8331370f861ad3c416,"This paper proposes a novel method to learn the causal relationship between two variables : the observed graph structure ( e.g., clustering effect ) and the existence of link between a pair of nodes. The authors leverage causal models considering the information of the node pair ( i.e. learned graph representations ) as context, global graph structural properties as treatment, and link existence as outcome. The proposed method, called CFLP, trains GNN based link predictors to predict both factual and counterfactual links.   The authors compare the performance of the proposed method with the state - of - the - art methods on several benchmark datasets. The results show that CFLP outperforms competitive baseline methods on both benchmark datasets, especially when the choice of treatment variable is chosen carefully. This work sheds insights for improving graph machine learning with causal analysis, which has not been extensively studied yet."
SP:48a7e50451b887f55be17b2662aa11ce18791cc1,"This paper proposes a two - stage unsupervised feature selection method based on knowledge contrastive disTillation ( SOFT ) model. In the first stage, the authors learn a sparse attention matrix that can represent second - order relations between features. Then in the second stage, they build a graph based on the learned attention matrix and perform graph segmentation, which groups high -correlated features together. Experimental results on 12 public datasets show that SOFT outperforms classical and recent state - of - the - art methods which demonstrates the effectiveness of our proposed method."
SP:14bcae11aeede63f28d1b80c05ed18a01d3e3f3c,"This paper proposes a new multimodal multi - modal model, the Mutually - supervised Multimodal VAE ( MEME ), that avoids explicit combinations between data and modalities by implicitly combining information between modalities implicitly through mutual supervision. This formulation naturally allows learning from partially - observed data where some modalities can be entirely missing, which is something that most existing approaches either cannot handle, or do so to a limited extent. To address this issue, the authors propose MEME, which avoids such explicit combinations by repurposing semisupervised semiautomatic VAEs to combine information via mutual supervision implicitly through semantically similar data. The authors also investigate the capability of MEME to capture the ‘ relatedness ’, a notion of semantic similarity, between modality in the latent representation ; in this setting, they show that MEME outperforms prior work considerably. They also contrast the quality of the representations learnt by mutual supervision learnt by MEME against standard approaches and observe interesting trends in its ability to capture relatedness between data."
SP:e834a52cadebe5f125ce491273b4ad1146beae3f,"This paper proposes revising Explore Options within the Deep Reinforcement Learning paradigm to tackle complex visual problems. In this paper, the authors first introduce J - PER, a new transition - selection algorithm based on the interest of multiple agents to generate directed behaviors in an environment. The authors then provide experiments in Atari following the benchmarking study to ensure fairness. They show that Deep Explore Options can learn from multiple intrinsic rewards, ignore harmful intrinsic rewards and learn to balance exploration, but also isolate exploitative or exploratory behaviors. Next, they propose to consider reward learning as an auxiliary task with a resulting architecture achieving a faster wall - clock speed and building a stronger, shared - shared - representation representation. They test Deep Explore options on hard and easy exploration games of the Atari Suite and show that they are a very strong alternative to a weighted sum of rewards, convincingly beating the baselines in 4 of the 6 tested environments, and with comparable performances in the other 2."
SP:41578dd1a4bdb043b3d68afa5f9cebb3e14f3907,This paper proposes a new method for learning Hamiltonian dynamical systems from data based on stiffness - aware neural network. The method splits the training data into stiff and non - stiff portions based on a stiffness aware index. This is a simple yet effective metric to quantify the stiffness of the dynamical system. This classification along with a resampling technique allows the proposed method to apply different time integration strategies such as step size adaptation to better capture the dynamic characteristics of the data. The authors validate the SANN method with complex Hamiltonian dynamics including a three - body problem and billiard model. Extensive numerical results show that SANN can accurately predict the dynamics and significantly outperform other methods.
SP:bfb0a059eeb6f40a18fbd20c0eec5037a64ca09e,"This paper studies the ability of large pre - trained language models to perform multi - step computations on tasks that can be done “in one pass ”, such as generating realistic text or synthesizing computer programs. The authors find that the language models are able to perform complex multistep computations when asked to perform the operation “step by step, showing the results of intermediate computations, even in the few - shot regime. They show that training Transformers to emit full program traces line by line annotated with local variables dramatically improves their ability to predict the result of executing a given computer program on a particular input. This application in some sense subsumes the others in the paper.   The authors first show that language models perform reasonably well on some tasks that are easy to do “ in one pass, ” such as adding integers or generating text. However, they struggle with tasks that require unbounded multi-step computation such as synthesizing programs or executing programs. They then show that the same models can perform more complex computations if they are trained to emit intermediate computation steps into a “ scratchpad ”. This is done by training the model to generate intermediate steps in the form of a line - by - line approximation. They find that this approach significantly improves the performance of the models compared to training the models to generate text or execute programs in a single pass. They also show that it is possible to train Transformers to generate programs by generating intermediate steps by training them to emit steps in a series of increasingly complex tasks ranging from long addition to the execution of arbitrary programs. Finally, the authors show that in some cases it is better for Transformers to be trained to generate program traces than to generate a program."
SP:e6c1a8b4bba287455dc9cf145b6bd1f04e2148a9,"This paper presents a novel method for generating adversarial attacks on deep neural networks based on image - based generative adversarial perturbations. The authors claim that the proposed method is more interpretable and physically - realizable than the existing adversarial attack methods ( e.g., adversarial image generation and adversarial adversarial training ). The proposed method, called feature fool attacks, is based on the fact that conventional adversarial methods fail to produce interpretable attacks that are intelligible to humans and pose limited threats in the physical world. The key idea of the feature fool attack is to generate feature - level attacks at the ImageNet scale that are simultaneously interpretable, universal to any source image, and physically realizable. These attacks can also reveal spurious feature associations that can be exploited by novel combinations of natural objects. Based on these findings, the authors emphasize the importance of cautious deployment for vision networks and their fortification against these types of feature fooling attacks. The following sections contain related work, methods, and experiments, and a discussion."
SP:873618263dc4246a39c44d0abfecfb5f688817e3,"This paper studies the problem of solving stochastic annealing ( SA ), a global optimisation technique that is applicable to a wide range of discrete and continuous variable problems. The authors focus on two components of SA : neighbour proposal distribution ( PD ) and temperature annealed schedule ( HAN ). PD and HAN - based on a reinforcement learning approach where the goal is to learn a policy that can be optimised for higher solution quality given a fixed computational budget. In this paper, the authors propose to apply a learned PD policy to the problem setting and show that it outperforms the state - of - the - art on Rosenbrock ’s function, the Knapsack problem, the Binpacking problem, and the Travelling Salesperson problem. They also show that Neural SA scales well to large problems while again outperforming popular off - line optimisation methods such as finetuning."
SP:cae31f7436920eb3946e3f5bca0ac88a73d7c3ec,"This paper addresses the problem of non - stationarity in cooperative multi - agent reinforcement learning. The authors propose a novel metric to measure the divergence between the divergence of consecutive joint policies in order to control the dynamic regret of each agent. They propose TRD - Net, a method based on message passing and mirror descent to estimate the joint policy divergence more accurately. They also propose a trust - region decomposition network to decompose the joint policies and impose trust -region constraints on the factorized policies, simple policy factorization like mean - field approximation will lead to more considerable divergence, which can be considered as a good approximation to the trust - regional decomposition dilemma dilemma.    The main contributions of the paper are as follows :   1 ) A novel metric for measuring the divergence in joint policies ’ divergence, based on the KL divergence between consecutive policy divergences, is introduced. This is a straightforward but non - trivial way to measure joint policies’ divergence which is difficult to estimate accurately. 2 ) A new algorithm, MAMT, is proposed to adjust the trust region of the local policies adaptively in an end - to - end manner to constrain the divergence. 3 ) The proposed algorithm is shown to outperform baselines in multiple cooperative tasks of different complexity, and stable performance improvement."
SP:989b58167a15ae4fafbe27ff534d327991b6c4d7,"This paper presents a self - supervised representation learning framework for audio - visual speech, called Audio - Visual Hidden Unit BERT ( AV - HuBERT ), for the task of audio - only speech recognition. The key idea is to pre - train a multi - stream audio and visual representation learning model, where only the audio input is available, and only the visual input is used for automatic speech recognition ( A - VHU ). The authors show that their model achieves better performance than the state - of - the - art in terms of WER on the largest public lip - reading benchmark LRS3 ( 433 hours ) trained with only 30 hours of labeled data. They also show that the pre - trained model is complementary to the self - training model and can be used as a pre - training baseline for audio / speech - only recognition.    The main contributions of the paper are as follows :   1. The introduction of a new multi - modal hidden unit learning framework, named Audio - visual hidden unit BERT, which learns an audio representation for the speaker based on the speaker ’s lip movements and the produced sound. This is in contrast to the unimodal BERT model, which pre - trains on audio clusters and only learns the visual representations. 2. The application of the learned audio representation to the lipreading task is demonstrated by comparing the performance of the proposed method with that of the state of the art A - HUBERT model trained on 31,000 hours of transcribed video. 3. The performance of AV - HBT is compared to that of A - BERT trained on the same amount of labeled video ( 1,000 hours )."
SP:7c9eb8aa4a4dcb5965157d860e812d81654e3aa7,"This paper proposes a novel algorithm, ECORD, to tackle the scalability issue of graph neural network ( GNN ) based reinforcement learning ( RL ) algorithms. The main idea is to pre - process a graph using GNNs in a pre - processing step, before entering a fast - acting exploratory phase directed by a recurrent unit. The authors claim that the proposed method ECORD achieves a new SOTA for RL algorithms on the Maximum Cut problem, while also providing orders of magnitude improvement in speed and scalability. ECORD reduces the gap by up to 73 % on graphs with a decreased wall - clock time compared to the previous state - of - the - art GNN - based methods, while maintaining the performance of GNN based methods up to 500 vertices."
SP:f741d980c9c560a21298e947f1605dcbab7ceeac,"This paper investigates the problem of training VAEs with discrete latents. The authors sidestep absolute standard VAE mechanisms such as sampling approximation, reparameterization trick and amortization in order to train discrete VAEs similarly to conventional ones. To do so, they use evolutionary algorithms in conjunction with a variational setting using truncated posteriors. They first show how a discrete variational method ( A. ties into gradient ascent for network weights and B. uses decoder network to select states ) can be more efficient than conventional amortized training for training VAE. Then, they show that direct optimization can be efficiently scalable to hundreds of latent variables using smaller networks. More importantly, they find the effectiveness of direct optimization to be highly competitive in ‘ zero - shot ’ learning ( where high effectiveness for small networks is required ). Finally, they investigate VAE training on a single image without previous training on clean data or training on large image datasets."
SP:deb189d37bd51b92762ce259a106d9a9e9d81ea4,"Controlled Effect Network ( CEN ) is an unsupervised method based on counterfactual measures of blame to identify effects on the environment controlled by the agent. CEN is evaluated in a wide range of environments showing that it can accurately identify controlled effects. Additionally, the authors evaluate CEN as an intrinsic motivator by integrating it in the state - of - the - art exploration method, achieving substantially better performance than action - prediction models."
SP:ea18d57904e25fd09ed0f6c9972029d78779a8a6,"This paper proposes a new method, structure - regularized pruning ( SRP ), for training image super - resolution ( SR ) networks. The main idea is to regularize the pruned structure of SR networks so that the locations of pruned filters are aligned across different layers of the network. To do this, the authors propose to select the filters of the same indices as unimportant filters for each of the layers connected by the same residual and apply L2 regularization to drive the weights towards zero so that their expressive power does not cause performance degradation. The authors conduct extensive comparisons with both lightweight and larger image SR networks, resulting in a lightweight network SRPN - L and a very deep one SRPN. They achieve superior performance gains over recent methods quantitatively and visually."
SP:0dee45001ae9600f485614dfe6874a516ac01db5,"This paper proposes a new method for few - shot learning, called ConFeSS ( Contrastive Learning and Feature Selection System ), that tackles large domain shift between base and novel categories. The method trains a feature extracting backbone with contrastive loss on the base category data and a feature masking module to select relevant features that are more suited to target domain classification. The paper also proposes a fine - tuned classifier along with the backbone backbone such that the backbone produces features similar to the relevant ones. To validate the effectiveness of the proposed method, the paper presents results on a recently introduced cross - domain few - shots learning benchmark, the CDFSL benchmark. The results demonstrate that the method outperforms all meta - learning approaches and produces competitive results against recent cross - domains methods. The main contributions of the paper can be summarized as follows : ( i ) Learning a feature extractor module with appropriate constraints to select features with which to generalize to target samples ; ( ii ) Fine - tuning the backbone by regularizing it with the selected relevant features ; ( iii ) A feature selection module that is able to learn features from both the base and target domains."
SP:92aa611d71a8da597358330d84fddbb90de2cf4f,"This paper considers the question of whether the generalisation of neural networks is due to the functions returned by gradient descent, or bias already present in the network architecture? This paper finds that while typical networks that fit the training data already generalise fairly well, gradient descent can further improve generalisation by selecting networks with a large margin. This conclusion is based on a careful study of the behaviour of infinite width networks trained by Bayesian inference and finite width neural networks trained with gradient descent. To measure the implicit bias of the architecture, new technical tools are developed to both analytically bound and consistently estimate the average test error of the neural network–Gaussian process ( NGP ). This error is found to be already better than chance, corroborating the findings of Valle - Pérez et al. ( 2019 ), underscoring the importance of architecture. Further, a new theoretical tool ( Theorem 3 ) is developed to enable consistent estimation of the average error of NNGP posterior on a given holdout set. This highlights a curious fact : minimum a posteriori functions can generalise best, and gradient ascent can select for those functions."
SP:a0e3cf719a95bbc5aad2f663ba5a3169c316ee9b,"This paper proposes a cross - lingual manifold mixup ( X - Mixup ) method to improve cross - language transfer performance. The authors claim that the performance gap between source and target languages is strongly associated with the representation discrepancy, and that the best way to bridge the gap is to adaptively calibrate the discrepancy and give representations that are compromised for target languages. To this end, the authors propose the X - mixup method, which is based on two approaches : 1 ) calibrating representation discrepancy and 2 ) giving representations for source languages. Experiments on the XTREME benchmark show X -Mixup achieves 1.8% performance gains on multiple text understanding tasks, compared with strong baselines, and reduces the cross - langual representation discrepancy significantly, as Figure 1 shows."
SP:19f8cd8f0c274b6141ba097d2ebb6d18af0986fd,"This paper studies the problem of Byzantine robust distributed or federated learning, where a central server trains a machine learning model over data distributed across multiple workers. In this setting, a fraction of these workers may deviate from the prescribed algorithm and send arbitrary messages to the server. While this problem has received significant attention recently, most current defenses assume that the workers have identical data. For realistic cases when the data across workers are heterogeneous ( non - iid ), the authors propose a simple bucketing scheme that adapts existing robust algorithms to heterogeneous datasets at a negligible computational cost. They also theoretically and experimentally validate their approach, showing that combining bucketing with existing robust algorithm is effective against challenging attacks. The paper is the first to establish guaranteed convergence for the non - Byzantine robust problem under realistic assumptions."
SP:4d63513b9a1b9b9fc44a69b3d5679a8f48eb95e7,"This paper investigates the relationship between disentanglement and multi - task learning based on hard parameter sharing in the setting of neural networks trained on automatically generated supervised tasks. The authors use a set of standard metrics to investigate the effect of the shared parameter sharing on the latent representations obtained by neural networks training on the task. They find that in a hard - parameter sharing scenario, disentangled representations have a clear positive impact on the models performance, as measured by the results in this matter vary for different datasets. However, it is inconclusive whether disentangling representations have an impact on model performance on the supervised tasks, as the authors do not provide any empirical evidence to support this conclusion."
SP:9851adb72e2918780f661f83f7da06eb866787be,"This paper presents a framework for certifying the robustness of reinforcement learning policies under adversarial state perturbations in reinforcement learning games. The main contributions are two - fold :    1. A state - level robustness certification for cumulative rewards. This is done by using a policy derived from Q - functions smoothed with Gaussian noise over each encountered state to guarantee that the actions taken along this trajectory are robust to adversarial attacks. 2. A global smoothing algorithm is proposed to ensure the lower bound of the cumulative rewards bound. 3. A local smoothing approach is proposed that makes use of adaptive search in order to obtain tight bounds for the reward bounds.   The authors evaluate their methods on three Atari games ( RegPGD, RegCVX, and RadialRL ), and show that their algorithms achieve high certified robustness among these games. They also demonstrate that their certifications are often tight."
SP:78da3c97182ec1baf6a131740bf7c91a9afb2fd2,"This paper proposes a new approach to conformal prediction in which we aim to output a precise set of promising prediction candidates that is guaranteed to contain a limited number of incorrect answers. This is particularly relevant to large - scale settings where the cost (monetary or otherwise ) of false positives is substantial, such as in - silico screening for drug discovery, where any positively identified molecular compound is then manufactured and tested. The authors propose to trade coverage for precision by enforcing that the presence of incorrect candidates in the predicted conformal sets ( i.e. true positive rate ) is bounded according to a user - specified tolerance. Subject to this constraint, their algorithm then optimizes for a generalized notion of set coverage ( GSP ) that allows for any number of true answers for a given query ( including zero ). They demonstrate the effectiveness of this approach across a number of classification tasks in natural language processing, computer vision, and computational chemistry."
SP:b126d2f3c397633745c8833e22ace93a2470e963,"This paper analyzes the expected length distortion of ReLU networks with standard random initialization. The authors show that the distortion does not grow with depth, and indeed shrinks slightly. They generalize this result by proving upper bounds both for higher moments of the length distortion and for the distortion of higher - dimensional volumes. The theoretical results are corroborated by their experiments.   The authors also consider the case where the weights and biases of a ReLU network are multiplicated by C, and show that there exist settings of the weights for which the distortion grows exponentially with depth. They also show that this is the correct initialization for the weight variance that must be used if the outputs ( Hanin & Rolnick, 2018 ) and gradients are to remain well - controlled at init. The value of this paper comes in analyzing the behavior of the behavior at He ( specifically at He = 2 )."
SP:b3b6d0512edfca461ea295ee8665f7f226c45d57,"This paper proposes SAFER, a behavioral prior - based policy learning algorithm that learns safe policies for reinforcement learning ( RL ) under the constraints of safety constraints and sparse rewards. The goal is to tackle the problem of RL in settings where it is difficult to specify the constraints and rewards and it is hard to quickly and safely acquire successful policies. To this end, the authors propose to use contrastive training on safe and unsafe data to learn a policy that is safe and useful for downstream RL. To do this, SAFER first learns to extract a safety variable from offline data that encodes safety requirements, as well as the safe primitive skills over abstract actions in different scenarios. Then, the proposed algorithm is trained to extract the safety variable through contrastive learning on safe data and safe - unsafe data, and then applies the learned policy to downstream RL tasks. The proposed method is evaluated on several tasks inspired by the game Operation,1 in which it is shown that SAFER outperforms baseline methods in learning successful policies and enforcing safety. In addition, it is also shown that the safe policies learned by SAFER have a higher success rate and less safety violations."
SP:a5dadb3ecc3caed3b9d9a68eda0d48a53c2d1ce2,"This paper proposes a neural network architecture for image restoration based on the Human Visual System ( HVS ). The architecture is called CMFNet and it is based on three types of image restoration tasks : image dehazing, deraindrop, deblurring, and image smoothing. The source code and pretrained models of three restoration tasks are available online. The experiments show that the proposed architecture has competitive performance results on four datasets, including four image degradation types, including image de - hazing, deburring and image smoothness. The performances of the proposed model are closer to the human sense sense than the one used in the original HVS paper."
SP:263b386beee44b0b45b6f6dc3cf80d020500be62,"This paper proposes a federated learning method, IT - PFL - HN, where clients participate in the training process and labeled data is used for training, while novel clients do not contribute their data to training. The authors propose a new learning setup, Inference - Time PFL, where a model trained on a set of clients, needs to be later evaluated on novel unlabeled clients at inference time. They propose a novel approach to this problem, which is based on a hypernetwork module and an encoder network that learns a representation for a client for a given dataset for evaluation. They evaluate on four benchmark datasets, CIFAR10, Cifar100, iNaturalist and Landmarks, showing that it generalizes better than current FL and PFL methods, especially when the novel client has a large domain shift. They also analyze the generalization error for the novel clients and show how it can be bounded using results from multi - task learning and domain adaptation."
SP:960d0a63a82593f6e72275b65f0501f0469d1924,"This paper presents a conditional diffusion based generative model ( RCDM ) to visualize representations learned with self - supervised learning ( SSL ) and supervised training ( SSL - supervised ). It is shown that SSL representations are more robust to adversarial attacks than supervised ones, while supervised ones are more susceptible to data augmentation attacks than SSL ones. The authors also show that the RCDM model is able to learn representations on par with state - of - the - art generative models while being faithful to the representation used as conditioning.    The main contribution of the paper is the use of RCDM to analyze the representations learned by SSL and supervised learning with the help of conditioning samples from a conditioning representation. The conditioning samples are obtained by sampling from the same conditioning representation repeatedly to identify which aspects of the representation are common to all samples, thus identifying what is encoded in the representation, while the aspects that vary greatly show what was not retained, the authors argue in the paper. To this end, they use the following observations : ( 1 ) SSL embeddings appear most invariant, followed by supervised - trained representation and last SSL representations2 ( Fig. 3 ) SSL - trained representations retain more detailed information on the content of the background and object style while supervised -trained representations appear oblivious to these, much like supervised ones. ( 2 ) SSL representations appear to retain information on object scale, grayscale vs. color, and color palette of background, much   like supervised representations. ( 3 ) despite their invariant training criteria, SSL representations seem to be more susceptible than supervised models to the adversarial attack. ( 4 ) SSL models appear to be able to exploit structure inside SSL representations leading to meaningful manipulation of image content (s.a splitting representation in foreground/background components to allow background substitution ), much as supervised ones do. ( 5 )"
SP:398899e6c86b4a2a17dfa5c2f4478811f4331c1d,"The paper presents Fp sketch, a well - celebrated streaming algorithm for frequency - moments estimation ( Fp ) that is differentially private as is when p is 0, 1. The paper analyzes Fp and shows that it is exponentially better than existing DP baselines and only worse than the optimal non - private baseline by a logarithmic factor. The evaluation shows that Fp Sketch can achieve reasonable accuracy with differential privacy guarantee. The conjecture is proved in Theorem 3 and is doubly confirmed by numerically simulated plots in Figure 2 and Theorem 4."
SP:3253b13851b5a3b5e3c8c6e24891db05903a4e57,"This paper proposes RSPO ( Reward - Switching Policy Optimization ), a method to find policies that are both locally optimal and sufficiently different from existing policies in RL environments. The motivation is to encourage the learning policy to consistently converge towards a previously undiscovered local optimum by introducing extrinsic and intrinsic rewards via novelty measurement during the optimization process. Experiments are conducted in a variety of domains ranging from single agent particle - world tasks and MuJoCo continuous control to multi - agent stag - hunt games and StarCraft II challenges. The main contributions of the paper are as follows :   1. The paper proposes a method for learning policy optimizers that iteratively iteratively find novel policies by iteratively finding novel policies, i.e., policy optimization, that is designed to converge to a locally optimal policy that is different from the existing policies. 2. For trajectories with high likelihood under existing policies, the authors propose to utilize an intrinsic diversity reward to promote exploration. 3. The authors conduct extensive experiments to validate the effectiveness of their method.    The main contribution of this paper is to develop a method that is able to discover diverse strategies in complex RL environments that are not necessarily the same as the policies found in the existing literature. This is achieved by introducing a novel reward function that encourages exploration in addition to the intrinsic rewards that are provided by the existing reward function. The novelty is measured using a trajectory - based measure of how likely it is that a given trajectory is likely to be encountered by the current policy or a novel policy. This novelty measure is used to encourage exploration in order to encourage policy optimization."
SP:e3ab3aa87ab023bd9949b99a17d4b6e26c1473c0,"Diffusion models have emerged as an expressive family of generative models rivaling GANs in sample quality and autoregressive models in likelihood scores. Standard diffusion models typically require hundreds of forward passes through the model to generate a single high - fidelity sample. This paper proposes a method that optimizes fast samplers for any pre - trained diffusion model by differentiating through sample quality scores. The method is named Differentiable Diffusion Sampler Search ( DDSS ). The authors identify a parametric family of non - Markovian diffusion models, called Generalized Gaussian Diffusion Models ( GGDM ), that admits high fidelity samples. They show that optimizing the degrees of freedom of the GGDM sampler via gradient descent leads to improved sample quality. The optimization procedure backpropagates through the sampling process using the reparametrization trick and gradient rematerialization."
SP:7a7506f2b5500a573c0cfb8b0822e5ea725c886a,"This paper proposes P - Adapters : lightweight models that sit between the embedding layer and first attention layer of Large Language Models ( LLMs ). They take LLM embeddings as input and output continuous prompts that are used to query the LLM. Additionally, they investigate Mixture of Experts ( MoE ) models that learn a set of continuous prompts and select one of them to query. They require a separate classifier trained on human - trained on natural language data to convert the natural language prompts to the ones used in the continuous ones. The authors compare the performance of the proposed P -Adapters with that of the MoE models. They find that the PAdapters show between 12 - 26 % absolute improvement in precision and 36 - 50 % improvement in consistency over a baseline of only using natural language queries. They also investigate what makes a P - adapter successful and conclude that access to the LLMs’s embeddINGS of the original natural language prompt is a significant factor."
SP:35cdf71f027cc5168b55cc34c64bfb2f3087d6f5,"This paper proposes Continuous Classification of Time Series ( CCTS ), a method for continuous classification of time series. The main idea is to classify time series in a multi - distribution fashion, where each time series is represented as a series of one - shot classifications, and each classifier is trained to be able to classify one of them at every time. The method is based on the idea that time series always evolves dynamically, changing features introduce the multi distribution form, which is different from the existing single - shot classification. However, most models are hard to achieve it due to their independent identically distributed premise. Two main problems are catastrophic forgetting and overfitting. To overcome these two main problems, the authors propose a novel Adaptive model training policy ACCTS. ACCTS trains a model based on data distributions adaptive to the time series evolution and the model change. Instead of reviewing all old distributions, ACCTS only replays the important samples adaptive to contribution of data to the model. Experiments on four real - world datasets show that ACCTS can classify more accurately than all baselines at each time. Both polices are dynamic rather than static to trade off the forgetting and the overfitting, which are the main reasons why the proposed method is more accurate."
SP:d9b74b749aa465496763d3a3a9bf3a53e800587e,"This paper proposes an extension of the language model to external memory in order to be able to read and memorize new data at inference time, thus acquiring new knowledge immediately. The authors propose to use an approximate kNN lookup into the memory of a language model into the internal representations of past inputs. They show that this extension of external memory improves language modeling across various benchmarks and tasks, including generic webtext ( C4 ), math papers ( arXiv ), books ( PG - 19 ), code ( Github ), as well as formal theorems ( Isabelle ). They also show that the performance steadily improves when we increase the size of memory up to 131k tokens. Finally, they show that their models are actually using external memory   in the way that we had hoped, e.g. by looking up the definitions of lemmas in a theorem proving corpus."
SP:7a1bbf86c3fdb8738aa826ca330493e857d050ba,"This paper proposes a tractable sampling scheme based on the Metropolis–Hastings Monte Carlo algorithm to generate sequences from masked language modeling ( MLM ). The paper proposes to interpret MLMs as energy - based sequence models and propose two energy parametrizations derivable from the trained MLMs. In order to draw samples correctly from these models, the authors propose to use the same masked conditionals used for training the masked language models, and they are accepted or rejected based on their energy values according to the target distribution. The authors claim that their tractable sampler generates higher quality samples than other recently proposed undirected generation approaches. The experiments with the proposed sampler provide evidence that masked language modelling objectives result in implicit training of an energy network, and these findings suggest that further exploration in future work, of variants of these objectives for explicit training of large scale energy networks is a worthwhile endeavour."
SP:011626ba4fafee13d4a30e3f13c1df5b7071a7f1,"This paper studies the problem of data augmentation for improving the performance of deep neural networks for various NLP tasks. The paper proposes a policy - based augmentation method, called Data Augmentation Policy Discovery ( DND ), which learns a good augmentation policy for each task by finding the best augmentation strategy that maximizes the diversity of augmentation data while minimizing the risk of losing the semantics of the original data. DND is evaluated on six different text classification datasets and GLUE benchmark, where it consistently improves over the recent state - of - the - art augmentation schemes by successfully discovering the effective augmentation methods ( Figure 1(c ) ). The method is more effective on the challenging low - data and class - imbalance regimes, and the learned augmentation policies are easily transferable to the different tasks and models. This implies the broad applicability of the augmentation scheme."
SP:69d41a862ea189f72d4e8af2854e27b95a91fa41,"This paper proposes a meta - learning framework for offline reinforcement learning ( RL ) based on attention mechanism, intra - task attention mechanism and inter - task contrastive learning objectives to achieve robust task inference in the offline RL setting. The proposed framework is based on the SOTA OMRL algorithm, FOCAL. The authors propose to replace the task - level representation learning objective of Momentum Contrast ( MoCo ) with a matrix - form objective to capture the correlation within the transition ( state, action, reward ) dimensions of MoCo. To this end, they propose to use sequence - wise self -attention ( Vaswani et al., 2017b ) to better capture correlation within transition ( VASWAN ) dimensions. They also propose a matrix form objective of the MoCo objective for task representation learning, by replacing its queue with a meta-batch sampled on the fly. They provide theoretical analyses showing that their objective serves as a better surrogate than naive contrastive loss for task inference and the proposed attention mechanism on top can also reduce the variance of task representation. Experiments are conducted to demonstrate the superior performance and robustness of their end - to - end framework compared to prior algorithms across multiple meta - RL benchmarks."
SP:ed86c60850d5c8302dcf1c2167db303e778fe681,"This paper proposes a parametric sequential generative modeling method called belief fine - tuning ( BFT ), which leverages approximate dynamic programming to determine the model parameters at each time step. It is shown that BFT can improve the accuracy of the belief model at test time because it specializes the capacity of the model to the space of local observations. Furthermore, in cases in which tracking the exact belief state is intractable, performing search on top of BFT yields substantial improvements over not performing search. This is the first instance of successful approximate belief state - based search in a multi - agent setting in which computing an exact belief - state is difficult.   BFT enables approximate public belief state search in imperfect - information games for the first time, where the number of possible information states is too large to track tabularly. The experimental results are shown on large - scale variants of the benchmark game Hanabi."
SP:6150725599c10f0e26f0d7cb1fc04b5b227a4456,"This paper presents a new method, Pixelated Butterfly, to improve the training speed and accuracy of sparse neural network models ( ViT, MLP - Mixer, and ViT - MLPMixer ) without the loss of quality compared to their baselines. The main contribution of the paper is to propose two variants of butterfly matrices, one for block - based and one for flat - based, to take advantage of modern hardware capabilities. The paper empirically shows that the proposed method can train faster than the baselines without loss of accuracy on CIFAR-10/100 classification and WikiText-103 language modeling tasks, while maintaining the same accuracy.   The paper also provides ablation studies that highlight the importance of each of the components : ( 1 ) the proposed Butterfly matrices are not hardware efficient, and ( 2 ) the balanced compute budget allocation brings 2× speedup compared to baselines that only sparsify attention. The authors empirically validate that their method is better than ViT and MLP-Mixer on a wide range of benchmarks."
SP:136e31054a55abca840f6478491972023c2296cb,"This paper presents a novel method for generating conditional diffusion probabilistic models based on score - based generative models. Inspired by the class clustering phenomenon, the authors propose a method to explicitly model the class center in the forward and reverse process of the Markov chain diffusion process, and make an elegant modification to the original formulation, which enables controllable generation and gets interpretability. The authors also provide another direction for faster sampling and more analysis of their method. They conduct extensive experiments on multiple tasks, and achieve competitive results compared with the state - of - the - art methods ( conditional image generation on CIFAR-10 with an inception score of 9.58 and FID score of 3.3 ). They name the conditional DDPM as ST -DDPM and conduct experiments on the task of conditional image Generation, free - form image inpainting, and attribute - to - image synthesis."
SP:fc2196f1f4ecd864398fed6640ff3f8b19870763,"This paper proposes a new method for domain generalization ( DG ) based on the assumption that source and target domain features are invariant. The authors argue that this assumption could be overly strict and suboptimal when the source domains share little information with the target domain and leverages information from selective source domains in a compositional way instead of relying on a unique invariant hypothesis across all source domains. To address this issue, the authors propose a LASSO method that explores diverse latent sub - spaces and learns individual hypotheses on those sub - spacings. The method is based on a two - stage approach. First, a hypothesis is constructed for each source domain by constructing a hypothesis shared among domains. Second, the hypothesis is tested on a set of source domains and target domains. Third, the method is evaluated on several well - known DG benchmarks, where it achieves state - of - the - art results.    The main contributions of the paper are as follows :   1. A new method is proposed to tackle the problem of generalizing from source domain features to target domain characteristics. The key idea is to use the label - informative features of the source domain to learn a hypothesis for each sub - domain. This is different from existing methods that rely on the invariant features of source domain. The main idea of the proposed method is to explore sub - domains and learn the hypothesis of each sub domain based on their label - based features. This allows the method to generalize to source domain characteristics without relying on the domain - based invariant feature. The proposed method has two stages. The first stage is to construct a hypothesis of a source domain and a target domain. Then, the stage two is to test the hypothesis on the source and the target domains, and the proposed hypothesis is used to learn the target hypothesis. The second stage is the final stage where the target and source hypothesis are tested on target domains and source domains, respectively. This stage is similar to the first stage except that instead of constructing a single hypothesis across source domains it constructs a hypothesis across domains. The hypothesis of target domain is then tested on source domains only. The model is compared with two existing methods, one based on domain invariance and another based on source domain similarity. The experimental results show that the proposed approach outperforms the other two methods."
SP:6e8e5bdeb77e3cafe1975da8411fb65118955d14,"This paper improves upon the original KT algorithm in two ways. First, the authors show that applying KT directly to the target RKHS yields tighter dimension - free guarantees for any kernel, any distribution, and any fixed function in the Hilbert space. Second, they show that for kernels like Gaussian, inverse multiquadric, and sinc, target KT admits maximum mean discrepancy ( MMD ) guarantees comparable to or better than those of square - root KT without making explicit use of a square root kernel. Third, they prove that KT with a fractional power kernel yields better - than -Monte - Carlo MMD guarantees for non - symplectic kernels, like Laplacian MMD. Fourth, they establish that KT applied to a sum of the target and power kernels simultaneously inherits the improved MMD guarantee of power KT and tighter individual function guarantees of target KT. Finally, they demonstrate significant improvements in integration error even in 100 dimensions when compressing challenging differential equations."
SP:645c3f1864aa843d4899fc2406f694b5aab8460d,"This paper presents an open - source benchmark suite for the NP - hard MaxIMUM INDEPENDENT SET problem, in both its weighted and unweighted variants. The benchmark suite compares state - of - the - art traditional and machine learning - based solvers for solving the problem. The paper also conducts an in - depth analysis of the popular guided tree search algorithm by Li et al., testing various configurations on small and large synthetic and real - world graphs. The analysis shows that classical algorithmic solvers often are faster, while providing solutions of similar quality to a recent solver based on reinforcement learning and observe that for this solver, the GNN is responsible for the competitive solution quality. Finally, the paper shows that LEARNING WHAT TO DEFER seems to be able to find good results very quickly, indicating that unsupervised reinforcement learning for combinatorial problems is a promising direction for future research."
SP:155ecd17d264a084b014abdfd0362146d8fb07e0,"This paper proposes Wavelet Compressed Convolution ( WCC ), a novel approach for activation maps compression for 1 × 1 convolutions ( the workhorse of modern CNNs ). WCC achieves compression ratios and computational savings that are equivalent to low quantization rates at a relatively minimal loss of accuracy. To this end, they use a hardware - friendly Haar - wavelet transform, known for its effectiveness in image compression, and define the convolution on the compressed activation map. They show that using WCC dramatically improves the results over aggressive quantization for the same compression rates while retaining the baseline network architecture. By combining WCC with light quantization, they show that we achieve compression rates equal to equal to 2 - bit and 1 -bit with minimal degradation in image - to - image tasks."
SP:004865e6affad32403b7965493a53c8a7ffdda0a,"This paper presents a no - regret learning algorithm for correlated and coarse correlated equilibria in extensive - form games, which is a much more challenging setting for learning in general - sum games. The algorithm is based on the Ellipsoid algorithm, which can be applied to both normal - form and correlated - form problems. The main contributions are two - fold :    ( 1 ) Developing a faster no -regret learning dynamics for correlated correlated equilibrium ( EFCE ) for extensive form games ; and ( 2 ) Connecting predictive ( that is, optimistic ) regret minimization with the framework of -reg - rex - minimization ( ERM ).   In the second part of the paper, the authors show that their algorithm achieves better performance than the best prior rate of O(T 1/2 ) in multiplayer general sum - sum - imperfect - information games. This is in contrast to the case for Nash equilibrium in normal form games where the distribution of play is estimated from a fixed distribution. The authors also show that the performance of their algorithm is better than that of Nash equilibrium when the distribution is correlated. They also provide experimental results on two standard benchmarks, where their algorithm outperforms Nash equilibrium by a large margin."
SP:ee545ff83df4d7ff256ac61fbe0eb0765f52f1d5,"This paper proposes a method to learn a discretization of a continuous action space by leveraging the priors of demonstrations. This discretisation strategy is based on the fact that the actions faced by the agent are in a finite number but plausible in light of the demonstrator ’s behavior. By discretizing the action space we can apply any discrete action deep RL algorithm to the continuous control problem. The authors evaluate the proposed method on three different setups : RL with demonstrations, RL with play data, and Imitation Learning. They find that AQuaDem consistently outperforms state - of - the - art continuous control methods, both in terms of performance and sample efficiency on a variety of hard manipulation tasks."
SP:4b39279b98d6aa311bb49dd1384925f9d6f66c2d,"This paper considers the problem of generalization in semantic segmentation, which aims to learn a robust model using only labeled synthetic ( source ) data and unseen real data ( target ) domains. The authors find that image style variation can largely influence the model ’s performance and the style features can be well represented by the channelwise mean and standard deviation of images. Inspired by this, they propose a novel adversarial style augmentation ( AdvStyle ) approach, which can dynamically generate hard stylized images during training and thus can effectively prevent the model from overfitting on the source domain. Specifically, AdvStyle regards the style feature as a learnable parameter and updates it by adversarial training to construct an adversarial image for robust model training. AdvStyle is easy to implement and can be readily applied to different models. Experiments on two synthetic datasets demonstrate that AdvStyle can significantly improve the model performance on unseen real domains and show that it can achieve state - of - the - art accuracy on the datasets considered. Moreover, Adv style can be employed to domain generalized image classification and produces clear improvement on the considered datasets."
SP:4a2e6d70b383e4941e0bc44e7e82972b22e26792,"This paper presents a novel method for learning natural - looking mid - air gesture recognition systems. The main idea is to use a neuromorphic gesture analysis system that encodes event - based gesture data at high temporal resolution into a latent space representation that can be used to compute the similarity of mid air gesture data. The method is based on the Hybrid Guided VAE ( HVAE ), which is an encoder - guided Variational Autoencoder ( VAE ) with a Dynamic Vision Sensor ( DVS ) encoder. The DVS encoder maps the input gesture data into an interpretable latent representation that is visualized through T - SNE plots. The goal of the method is to train a system that is able to learn the similarity between the input data and the DVS data without the need of rigid gestures.   The main contributions of the paper are as follows :   1. A novel method to learn natural looking mid air gestures. The authors propose a method that uses a neural encoder to encode the input to a latent representation of the input. The encoder is guided by an actor - critic algorithm. This encoder encodes the input into an event encoder which is then guided by a neural network. The neural network is trained using a combination of supervised learning and self - supervised learning. 2. An encoder component is added to the neural network which is used to decode the input from the latent representation into a disentangled latent representation. 3. A neural network encoder and encoder module are used to train the model on the ground truth DVS dataset. The experimental results show that the proposed method achieves 87% classification accuracy."
SP:2e66468a6b94177e54b0052b97713ee63902c278,"This paper presents a method for sparse hierarchical table ensembling ( S - HTE ) for deep learning for tabular data. The method is based on the Sparse Hierarchical Table Ensemble ( SHTE ), a neural network that learns a sparse representation of the input data and uses an annealing mechanism to reduce the computational complexity of the network. SHTE is different from ferns in two ways : firstly, it replaces the decision trees in SHTE with an oblivious decision tree, and secondly it augments the representations of the decision tree with a sparsified version. The authors show that their method outperforms the state - of - the - art fern - based methods on a standard classification and regression benchmark. The experimental results show that S -HTE is more accurate than fern in terms of classification accuracy than other methods."
SP:b238db9252d83a13438bb747d70e635bb9945958,"This paper proposes Latent Action Q - learning ( LAQ ), an offline RL method that learns value functions from state - only tuples. The paper theoretically analyzes the applicability of tabular Q learning to discrete Markov decision processes ( MDPs ) and shows that it can learn the same value function under any arbitrary refinement of the action space. This theoretical result motivates the design of LAQ as an offline learning method that can learn effective value functions. In the experiments, the authors show that LAQ outperforms imitation learning methods, oracle, and other methods that learn from observation - only data. The experiments in 5 environments ranging from 2D grid world to 3D visual navigation in realistic 3D environments demonstrate the benefits of the proposed method over simpler alternatives, imitation learning oracles, and competing methods."
SP:108ebe9045a9e2b8b5aba8352733782462db8a81,"This paper proposes SWARM Parallelism1, a method to parallelize training of large models trained in dedicated GPU clusters that can be expensive to deploy and operate. The authors identify two challenges that make it difficult to train large models with conventional model parallelism : high latency and high bandwidth. To overcome these challenges, the authors propose SWARM parallelism with two approaches : preemptible instances, and pooling resources from multiple regions.   The first approach is SWARM, which is a method for parallelizing training of poorly connected, heterogeneous nodes that are rebuffed. The second approach is compression - aware, which proposes to alternate between parallelizing the training of two parallelized versions of the same model. The proposed method is applied to train a large Transformer language model with 1.1B shared parameters on a swarm of preemptible T4 GPUs with less than 400 Mb/s network throughput. The experimental results demonstrate that it is possible to train the model with high training throughput on preemptible low - power T4 GPU and < 400Mb /s network. The main contributions of the paper are as follows :   1 ) The authors propose a method called SWARM that parallelizes training of a large model trained in a dedicated GPU cluster. 2 ) They propose to alternate the parallelization of two versions of SWARM. 3 ) They show that SWARM achieves better performance than the other approaches."
SP:91d2f094d5481651b554f58aecc2a6207057a47c,"This paper proposes a method to bridge the gap between offline and online RL in multi - agent reinforcement learning ( MARL ). The authors claim that due to the discrepancy between the behavior policy and the learned policy, the transition dynamics in offline RL are biased compared to the dynamics in online RL. To overcome this bias, the authors propose online transition correction ( OTC ) to implicitly correct the biased dynamics by modifying the sampling probabilities of transition dynamics. The proposed OTC is simple yet effective to increase data efficiency and improve agent policies in online tuning. Experimental results show that OTC outperforms baselines in a variety of tasks, and ablation studies demonstrate the effectiveness of the distance measures, the practicability of rank - based prioritization, and the improvement of adaptive prioritization."
SP:d0e650d568214481b07a0452ec606ccbf6d05410,"This paper proposes a method for quantizing the forward and backward gradients of neural networks in order to reduce the computational footprint of deep neural network training. The forward phase of the training process is usually quantized in the form of logarithmic gradient quantization ( LQ ), where each of the weights and activations of the network are quantized with respect to the output of intermediate neural layers. The authors argue that this quantization of the forward phase is crucial for unbiased training of DNNs. Based on this argument, the authors propose a quantization method ( LUQ ) that quantizes the forward / backward phase of training in a log - arithmic manner. The proposed method is based on the idea that LQ is better than other quantization methods in terms of the degradation achieved by training the network compared to other methods. The main idea of the method is to avoid multiplications during the second and third phase ( i.e., the multiplicative step ) of training by minimizing the area affected by the multiplier. The method is tested on ResNet50 on ImageNet and achieves state - of - the - art results in 4 - bit training."
SP:f2862d1f987164ed6c3c375cd8962e57c369373b,"This paper studies the problem of meta - learning with polythetic classifications based on shared patterns of features that are common in the natural world. It shows that Prototypical Networks, such as Matching Networks, require an embedding dimension that is exponential in the number of task - relevant features to emulate these functions. In contrast, attentional classifiers such as Attentional Attentional Classifiers ( ACL ), which are monothetic classifiers over a set of features, do not require such a linear embedding. The authors propose a selfattention feature - selection mechanism that adaptively dilutes non - discriminative features to improve the performance of the meta - learner. They also show that in the presence of task irrelevant features, Attentional models are susceptible to misclassification."
SP:e1e513fef25d29e17cdadd1b36d932a8ad8897cd,"This paper proposes an environment and training methodology to explore emergent language between agents using reinforcement learning and continuous communication. The paper proposes a simple messaging environment where a speaker and a listener are trained to communicate via a continuous waveform. The goal is to explore whether and how language emerges when using RL to train agents that communicate via continuous acoustic signals. To this end, the paper proposes to use Q - learning as a learning tool to learn a symbolic representation of the speaker's concept to the listener's receptive field using a combination of discrete symbols and noise - based reinforcement learning. The experiments show that the learned representation is expressive and that the speaker and the listener are able to communicate with each other in a language that is similar to each other's. They also show that it is possible to ground the emergent communication by introducing a caregiver predisposed to “hearing ” or “speaking ” English. Finally, they describe how their platform serves as a starting point for future work that uses reinforcement learning, multi - agent systems, and deep reinforcement learning to study the questions of language learning and emergence."
SP:0e6ff65ba4a3df35947d1b6f4d438612088d90a0,"Natural Language Processing ( NLP ) models have been shown to be vulnerable to backdoor attacks, where a pre - defined trigger word in the input text causes model misprediction. Previous NLP backdoor attacks mainly focus on some specific tasks, which makes those attacks less general and applicable to other kinds of NLP models and tasks. In this work, the authors propose BadPre, the first task - agnostic backdoor attack against the pre - trained NLP model. The key feature of the attack is that the adversary does not need prior information about the downstream tasks when implanting the backdoor to the pre-trained model. When this malicious model is released, any downstream models transferred from it will also inherit the backdoor. The authors further design a simple and effective strategy to bypass a state - of - the - art defense strategy to circumvent the backdoor and show that their approach can compromise a wide range of downstream NLP tasks in an effective and stealthy way. They perform extensive experiments over 10 different types of downstream tasks and demonstrate that BadPre can achieve performance drop for up to 100 %, while the backdoored downstream models can still preserve their original functionality completely."
SP:58d3ecb4a1906251e79ad883aa97cc2502642658,"This paper proposes a new method for skill discovery, DISk, which learns skills one after another in an incremental fashion in a static or evolving environment. The idea is that traditional skill pre - training methods assume stationary environments during training, which makes it difficult for them to adapt quickly to changes in the environment and to not forget skills learned before. On the other hand, in an evolving or expanding environment, skill learning must be able to adapt fast to new environment situations while not forgetting previously learned skills.   DISk proposes to learn skills in stages, one skill at a time, one in each stage, one after the other. This incremental nature of DISk lets agents discover diverse skills in either static or expanding environments. Even in static environments, the authors demonstrate that DISk can learn a diverse set of controllable skills on continuous control tasks that outperform current state - of - the - art methods on skill quality, as well as in sample and computational efficiency. Finally, they demonstrate that our learned skills can be used without any modifications in the hierarchical setting to efficiently solve downstream long - horizon tasks."
SP:2c6595408f5ec95537eaf555e5fe3d992b58c222,"This paper proposes a novel convolution method called log - polar space convolution ( LPSC ) to replace conventional convolutional neural networks ( CNNs ). The idea is that the convolution kernel lies in the log polar space of the feature map, and that the receptive fields in the lower layers of the network correspond to the logarithmic distances between the kernel and the ground truth. The authors propose to divide the receptive field into different regions according to the relative directions of the kernel, and then apply the method to a variety of neural network architectures, including AlexNet, VGGNet, ResNet, DeepLabv3+ and CE - Net. They show that the proposed method can be combined with conventional methods and achieves better performance than the conventional methods on various benchmarks."
SP:7791f96b1eef277a9133975507a750d9e7c6b8ff,"This paper studies the problem of understanding the generalization ability of neural networks ( NNs ) based on the PAC - Bayes generalization based on information compression of weights ( IIW ). The main contributions are four - fold : ( 1 ) they propose a new information bottleneck under the umbrella of PACBayes generalisation guarantee, namely PAC - bayes Information Bottleneck ( PIB ) ; ( 2 ) they derive an approximation of the intractable IIW ; ( 3 ) they design a Bayesian inference algorithm grounded on stochastic gradient Langevin dynamics ( SGLD ) for sampling from the optimal weight posterior specified by PIB ; and ( 4 ) they demonstrate that their new information measure covers the wide ground of NN ’s behavior.   The main contribution of this paper is the introduction of PIB, which is an information bottleneck on the trade - off between accuracy and information complexity of NNs, namely PIB - based information bottleneck. The authors empirically identify the fitting to compressing phase transition during NNs ’ training and the concrete connection between IIW compression and generalization. Besides, they verify that IIW is able to explain NNs in broad cases, e.g., varying batch sizes, overparameterization, and noisy labels."
SP:a733847ade77ffbf38760fc79da17893dea8d53f,"This paper investigates the effect of linear separability of the perturbations used in data poisoning attacks. The authors find that the attacks are linear separable when assigned with the target labels of the corresponding samples. This is an important population property for various perturbation that were not unveiled before. Moreover, the authors confirm that linear separation is indeed the workhorse for recent attacks. Synthetic attacks are as powerful as the deliberately crafted attacks. This finding also suggests that the shortcut learning problem is more serious than previously believed as deep models heavily relies on shortcuts even if they are of an imperceptible scale."
SP:7b50be406138ad01db3ee112899f622637896fe9,"This paper proposes POELA, an algorithm for optimization of the importance weighted return ( i.e., the weighted average of the expected return of the policy with respect to the initial state under which it is assumed that the policy will be adopted. The authors provide a theoretical justification of the proposed algorithm through a better per - state - neighbourhood normalization condition. They further test the algorithm in a healthcare - inspired simulator and a logged dataset collected from real hospitals. The experiments show the proposed method with less overfitting and better test performance compared with state - of - the - art reinforcement learning algorithms."
SP:c976752a55b9ff47dc63c95a9fd7b51a81e8a42e,"This paper presents CoLLIE, a simple, yet effective model for continual learning of how language is grounded in vision. Given a pre - trained multimodal embedding model, where language and images are projected in the same semantic space ( in this case CLIP by OpenAI ), the paper learns a transformation function that adjusts the language embeddings when needed to accommodate new language use. Unlike traditional few - shot learning, the model does not just learn new classes and labels, but can also generalize to similar language use, as well. The paper verifies the model’s performance on two different tasks of continual learning. First, it shows that it can efficiently learn and generalize from only a few examples, with little interference with the model ’s original zero - shot performance. Second, it demonstrates that the model can perform equally well on tasks it could do well on before."
SP:d3371b322acfc321ee79a2e1b438d82644872fa4,"This paper proposes VLAF2, a method for learning image captioning models for describing novel objects or visual concepts which are unseen in the training captions. The main contribution of the paper is to propose a method which utilizes the language knowledge from existing visual / language models to describe novel visual data with fluent and natural language expression. The method is based on two main components : 1 ) the language model that produces the captions, and 2 ) the linguistic model that predicts the visual content of the image.   1 ) The language model predicts the linguistic content of an image. 2 ) The visual content is predicted from the visual embeddings. 3 ) The linguistic model is trained to predict the object content from the image, and 4 ) the object captions are trained to be language - fluent. The authors claim that their method is better than the state - of - the - art in terms of fidelity, fluency, and fidelity - fluency. They also claim that the model improves the fidelity and fluency of captions compared to existing methods. They conduct extensive experiments on the nocaps dataset to validate the effectiveness of their framework. They compare their method with several existing methods and show that it outperforms them on all metrics."
SP:9f3b6486662d80350d77a4b060d4a5b8b22a6130,"This paper studies the effect of neural collapse on the transfer learning ability of foundation models in the few - shot learning setting. Specifically, the authors focus on the problem of learning representations for classification that are transferable to new, unseen classes. Recent results in the literature show that representations learned by a single classifier over many classes are competitive with representations learned from special - purpose algorithms designed for few - shots learning. The authors propose to study neural collapse by analyzing the phenomenon that the features learned by overparameterized classification networks show an interesting clustering property, called neural collapse, which generalizes to new samples from the training classes, and, more importantly, to new classes as well. They demonstrate that neural collapse allows foundation models to provide feature maps that work well in transfer learning and, specifically, in the many - shot setting.   The main contributions of this paper are as follows :   1. An analysis of the neural collapse phenomenon, which is theoretically and empirically shown to generalize neural collapse generalizes new samples, and – more importantly – to new class as well, allows the authors to demonstrate that the feature maps provided by foundation models can transfer well to transfer learning. 2. An explanation for the phenomenon based on the recently observed phenomenon. 3. Some additional related related work is also discussed. 4. Experiments are conducted to validate the theoretical analysis."
SP:624c95d9ce1ee4b66274e858e2da22bef6b052c7,"This paper proposes a method to jointly solve the denoise, densify, and complete inaccurate point cloud reconstruction tasks. To this end, the authors propose a deep point - cloud reconstruction network consisting of two stages : 1 ) a sparse stacked - hourglass network as for the initial densification and denoising, 2 ) a refinement via transformers converting discrete voxels into 3D points, and 3 ) a module called amplified positional encoding which is designed to differently amplify the magnitude of positional encoding vectors based on the points ’ distances for adaptive refinements. Extensive experiments demonstrate that our network achieves state - of - the - art performance among the recent studies in the recent papers in the ICLR - NUIM and ShapeNet datasets."
SP:34a81ca65131576d4c14332a4e9eb3a4c344cab7,"Graph Convolutional Networks ( GCNs ) are the state - of - the - art method for learning graph - structured data, and training large - scale GCNs requires distributed training across multiple accelerators such that each accelerator is able to hold a partitioned subgraph. However, distributed GCN training incurs prohibitive overhead of communicating node features and feature gradients among partitions for every GCN layer in each training iteration, limiting the achievable training efficiency and model scalability. To this end, this paper proposes PipeGCN, a simple - yet - effective scheme that hides the communication overhead by pipelining inter - partition communication with intra - partition computation. This work not only provides a theoretical convergence guarantee but also finds the convergence rate of pipeGCN to be close to that of the vanilla distributed distributedGCN training without staleness."
SP:8302d49558ee0f16392d623d4e604e92db10d041,"This paper proposes a method for test - time adaptation, i.e. using the test input to improve model robustness. The authors propose marginal entropy minimization with ensembled augmentations ( MEME ), a method that can be used in any test setting where the model is probabilistic and adaptable. In MEME, when presented with a test example, perform different data augmentations on the data point, and then adapt ( all of ) the model parameters by minimizing the entropy. Experiments are conducted on two baseline ResNet models, two robust ResNet-50 models, and a robust vision transformer model, and the authors demonstrate that this approach achieves accuracy gains of 1 - 8 % over standard model evaluation and also generally outperforms prior augmentation and adaptation strategies. For the setting in which only one test point is available, the authors achieve state - of - the - art results on the ImageNet-C, ImageNet - R, and, among ResNet - 50 models, distribution shift benchmarks, demonstrating that MEME is more broadly applicable on a wide range of distribution shifts."
SP:a985de5e940ff3a4160b378201b8c02f68d1914a,"This paper proposes a method to jointly optimize the model and policy in reinforcement learning ( RL ). The main contribution is an algorithm, Mismatched No More ( MnM ), for model - based RL that provably maximizes a lower bound on expected reward. This bound becomes tight at optimality under certain assumptions. It is claimed that it is the first RL method that jointly optimizes the policy and the model in the same objective. The method is competitive with prior state - of - the - art methods on benchmark tasks. On certain hard exploration tasks, the method outperforms prior methods based on maximum likelihood estimation.   The main contributions of this paper are as follows :   1. Mismatch No More is a method for jointly optimizing the model ( i.e., jointly training the model ) and the policy ( ii. optimizing the policy ) in RL. This joint optimization mends the objective mismatch mismatch in prior work 2. The resulting algorithm is similar to a GAN ( GAN is a classifier that distinguishes between real and fake transitions, and 3. the policy is updated to avoid states where the model predictions are unrealistic 3. The experimental results demonstrate that the proposed algorithm is able to outperform prior methods on some benchmark tasks 4."
SP:a469fbcdc20b11dff4085b6fbc384e77f33cd37d,"This paper proposes a simple method to combine the advantages of behavioral cloning from observation history and observation history with a single observation ( BC - SO ). The observation history has access to all required information, but it manifests the “ copycat problem ”, where it relies excessively on extrapolating past actions and fails to attend to important visual cues. The behavioral cloning method on the other hand, uses a coarse action based on the instantaneous observation, and then refine it into a final action using historical information. The proposed method outperforms all baselines on CARLA autonomous driving from images and various MuJoCo continuous control tasks. The authors also conduct extensive ablation studies to verify their hypothesis for why their approach works."
SP:95c4533b5d1a865c4cc6a54615e7ad6357bdaad1,"This paper proposes a meta - learning method called DyAd that generalizes across heterogeneous domains by partitioning them into different tasks. DyAd has two parts : an encoder which infers the time - invariant hidden features of the task with weak supervision, and a forecaster which learns the shared dynamics of the entire domain. The encoder adapts and controls the forecaster during inference using adaptive instance normalization and adaptive padding. Theoretically, the authors prove that the generalization error of DyAd is related to the task relatedness in the source domain, as well as the domain differences between source and target. Experiments are conducted to show that DyAd outperforms state - of - the - art approaches on two tasks."
SP:ec70553cb0c27e5349c1b8cce6bcaa96a83bf050,"This paper proposes a novel method for weakly supervised monocular 3D object detection, named as WeakM3D, which is based on the generated 2D boxes and corresponding RoI LiDAR points as the weak supervision. The method is evaluated on the KITTI benchmark, which outperforms some existing fully supervised methods which use massive 3D box labels. The main challenges of the proposed method are geometric alignment loss, ray tracing loss, loss balancing, and learning disentanglement. The authors propose four effective strategies to resolve them, including geometric aligning loss, geometric trace loss, learning disenanglement, and loss balancing. Experiments are conducted to validate the effectiveness of the method."
SP:34217c6a8ca43b8eeb9ddc83d6f1f0af05918984,"This paper proposes a state - of - the - art model architecture for natural language processing based on deep transformer based models. The proposed architecture is based on a soft gradient based subword tokenization module ( GBST ) that automatically learns latent subword representations from characters in a data - driven fashion. The authors additionally introduce CHARFORMER, a deep Transformer model that integrates GBST and integrates it into the transformer architecture and operates on the byte level. Experiments on English GLUE, multilingual GLUE and noisy text datasets show that the proposed architecture outperforms a series of competitive byte - level baselines while maintaining competitive quality. Additionally, the authors demonstrate via visualization that the latent subwords learned by the proposed model are interpretable to some extent."
SP:d26d25f2ef23a89a2c139d0dd87c4c86fddcff5e,"This paper proposes a new method for detecting backdoor attacks in deep neural networks ( DNNs ). The authors claim that the existing methods do not provide access to the original training data or the parameters of the target DNN, making them impractical in many real - world applications, e.g., black - box hard - label backdoor detection. To address this problem, the authors propose AEVA, an adversarial extreme value analysis ( AEVA ) to detect backdoors in neural networks. AEVA is based on an extreme - value analysis of the adversarial map, computed from the monte - carlo gradient estimation. Experiments are conducted on three widely - adopted tasks with backdoor trigger implementations and two variants of the backdoor attack, where only the output label of the black box DNN is available. The experiments show that AEVA can detect backdoor attacks under both the white - box and hard - box backdoor attacks. The main contributions of the paper are as follows : ( 1 ) The authors show that the objective of backdoor detection can be bounded by an objective of adversarial objective. ( 2 ) They show that this objective leads to a solution with highly skewed distribution. ( 3 ) They demonstrate that the AEVA approach is effective in detecting backdoors."
SP:c6dbca0ed0799b7fec21777606f6f809eb2d8c48,"This paper proposes a new uncertainty measure, KLoS, for in - distribution and out - of - distribution ( OOD ) uncertainty in classifier uncertainty. The proposed measure is based on a Kullback - Leibler divergence ( KL divergence ) between a predicted Dirichlet distribution and a specifically designed class - wise prototype Dirichlets distribution. The authors leverage the second - order uncertainty representation provided by evidential models to represent the KL divergence. The paper also proposes to learn an auxiliary neural network, KloSNet, to learn a refined criterion directly aligned with the evidential training objective for training samples and to improve uncertainty estimation. Experiments on image datasets and model architectures show that the proposed measure performs better than existing measures when training with OOD data. In presence of OOD training data, whereas previous measures may perform poorly, the new measure performs more robust."
SP:8b4f3916dca4e627931558e14836749bd4a6792f,"This paper analyzes a semi - supervised method for learning convolutional neural networks ( CNNs ) from unlabeled images. The method is based on two assumptions : ( 1 ) that patches in the input images have low - dimensional structure ( e.g., when the patches are sampled from a low - dimension manifold ) and ( 2 ) that the distribution of patches is similar to that of the input. The authors show that under these assumptions, the method provably learns CNNs. Specifically, they show that, under the assumption that the patch structure in the images has low -dimensional structure, the algorithm can learn a linear classifier over the produced representation. They also show that the dependence of the algorithm on the dimension of the patch distribution is essentially optimal.   The main contribution of this paper is the following :   1 ) Under the distributional assumption that patches are distributed in a similar way to the one under which CNNs are learned in practice, the authors prove that their algorithm can efficiently learn CNNs under the distribution assumption. 2 ) The authors provide lower bounds on the run - time and sample complexity of their algorithm for learning CNNs, and show that it is optimal for the covering number of patches to depend on in the same way that CNNs depend on."
SP:7f2f354d5cc1030bd97bd716aea8fe1d3af86b25,"This paper proposes a new algorithm for face clustering based on Graph Convolutional Networks ( GCN ). The proposed method Ada - NETS is based on the fact that the previous methods based on kNN relations in the feature space may lead to a lot of noise edges connecting two faces of different classes, thus degrading the performance of GCNs. The main idea is to cluster faces by constructing clean graphs for GCNs with a proper number of edges connecting to each face image. In Ada -NETS, each face is transformed to a new structure space, obtaining robust features by considering face features of the neighbour images. Then, an adaptive neighbour discovery strategy is proposed to determine a proper amount of edges to reduce the noise edges while maintaining the good ones to build a graph with clean yet rich yet rich edges for cluster faces. Experiments on multiple public clustering datasets show that Ada - NetS significantly outperforms current state - of - the - art methods, proving its superiority and generalization."
SP:a3bc8e26f55e78f07de081ca85865afd52b6ae4a,"This paper proposes a new method, Unit DRO ( DRO minimizes the loss over a reweighted dataset of possible data distributions ( the “ uncertainty set ” ) without demographics. The authors argue that the convex condition of KL DRO may not hold for overparameterized neural networks, and applying KLDRO fails to generalize under distribution shifts in real scenarios. Instead, they propose a simple yet analytical solution, change - of - measure technique and the analytical solution of KL - DRO. The proposed method is empirically evaluated on large - scale DG ReID and cross - domain ReID benchmarks compared to standard baselines such as CD and DRO, and it is shown to be able to retrieve valuable samples or subgroups without the demographic information."
SP:62c1f734b7f6c6e7d5114da6f37c9e3cdda73a23,"Graph Neural Networks ( GNNs ) have been proven effective across a wide range of molecular property prediction and structured learning problems. However, their efficiency is known to be hindered by practical challenges such as oversmoothing and overfitting. To tackle these challenges, this paper introduces “ Noisy Nodes ”, a very simple technique for improved training of GNNes, in which we corrupt the input graph with noise, and add a noise correcting node - level loss. The paper claims that this noise correction helps ameliorate oversmoothhing by encouraging diverse node latents.   The main contribution of this paper is to introduce a simple regulariser which allows even generic architectures not designed for quantum chemistry to achieve state - of - the - art results. This regulariser applies well -studied methods in simple, straightforward ways. It is not clear from the paper how much this regulariser helps with the overfitting problem, but using it seems to be the only way to mitigate the noise correction problem. This paper also proposes a non - spatial GNN architecture which is shown to outperform the GNN on Open Graph Benchmark ( OGBN - Arxiv )."
SP:24a1b44f37f8eedbab2047fb84600a322d289f3b,"This paper tackles the set2vec problem, which is the task of extracting a vector representation from an input set of feature vectors, by incorporating a fixed number of learnable queries in attention. In this paper, the input set elements are considered as i.i.d samples from a mixture distribution, and the set embedding feed - forward network is defined as the maximum - a - posterior ( MAP ) estimate of the mixture distribution which is approximately attained by a few ExpectationMaximization ( EM ) steps. The whole MAP - EM steps are differentiable with a Fixed number of steps, allowing efficient auto - diff diff backpropagation for any given downstream task. Furthermore, the proposed mixture set data fitting framework allows unsupervised set representation learning via marginal likelihood maximization aka the empirical Bayes Bayes. The authors evaluate their approach on various tasks demonstrating improved performance over the state - of - the - art on various sets, bioinformatics and NLP. They also find that OTKE can be seen as a special case of their framework, specifically a single - step EM with extra balanced assignment constraints on the E - step. Compared to OTKE, our approach provides more flexible set embeddings as well as prior - induced model regularization."
SP:b4f7b660b84fe7702fbcc8a96c192abc3a64f045,"This paper proposes a method for performing unsupervised feature selection in contrastive analysis ( CA ) setting. The goal is to select a small number of informative features for use in unknown downstream tasks. The definition of “ informative ” is subjective and dependent on the specifics of the problem domain. The proposed method, Contrastive Feature Selection ( CFS ), works on the semi - synthetic dataset and four real - world biomedical datasets, and it consistently outperforms previous state - of - the - art methods designed for standard CA settings. The authors validate their approach through extensive experiments ( Section 5 ). They begin by applying CFS to a semi-synthetic dataset to better understand when it may succeed as well as potential failure cases. Then apply it to four real-world biomedical datasets and find that CFS consistently outperform standard unsuper supervised feature selection algorithms."
SP:bc4f69f23aba2034cbf14cb31bdc7a991806bbf6,"This paper studies the effect of early stopping on the generalization of linear regression models when the model dimension of the model exceeds the number of features arising from the training data. It is well known that early stopping can mitigate the phenomenon of “ double descent ”, where the model predicts two distinct phases of the training process ( i.e., the first time the model is stopped early and the second time the data is collected ). This paper proposes to study the relationship between early stopping time and model dimension as well as the sample size of the dataset in order to better understand this phenomenon. Theoretical results suggest that optimal early stopping corresponds to training deep neural networks, and empirical results suggest early stopping helps mitigate double descent in various settings.   The main contributions of this paper are the following :   1. An analysis of the effect on generalization when early stopping at the optimal early - stopping time is compared to stopping early at the early - stop time with the data collected after the training of the neural network. 2. A theoretical analysis on the difference between the effects of stopping early and stopping late on the same dataset. 3. An empirical analysis on early stopping and stopping early on a different dataset. 4. A discussion on the role of the data augmentation technique."
SP:ede87b50cd9c4a6533f17e3e5ddfaaeaaac71dcf,"This paper proposes a quasi - Newton method for the policy gradient algorithm with entropy regularization for Shannon entropy. The authors provide a simple proof that the proposed method enjoys faster convergence than Newton - type quadratic convergence near the optimal policy. The proposed method is shown to converge in single - digit iterations, often orders of magnitude faster than other state - of - the - art algorithms such as the Newton method. The paper also provides some variants of the method, including the PGPG method with regularization and some of its variants. In the experiments, the authors show that the convergence rate can be improved to O(e^{(e ) }.   The main contribution of this paper is to provide a proof that all these algorithms enjoy faster convergence compared to the Newton methods. The proof is based on synthetic and industrial - scale examples from reinforcement learning problems."
SP:3535504f7599b1f39239f7cd8e09acd40fa8fdf0,This paper proposes a general method inspired by case - based reasoning to train agents and generalize out of the training distribution. The main idea is to collect instances of positive experiences from the agent ’s interaction with the world in the past and later reuses the collected experiences to act efficiently. The method can be applied in conjunction with any existing on - policy neural agent in the literature for TBGs. The proposed method is applied on the TextWorld Commonsense dataset and achieves better or comparable scores on 24 of the 33 games in the Jericho suite compared to previous work. The authors also show that CBR agents are resilient to domain shifts and suffer only marginal drops in performance ( 6%) on out - of - distribution settings when compared to their counterparts.
SP:9a5dd0148a15dc5b4d2bc6762dfe8a8991f8866c,"This paper proposes a method to distill multiple word senses from a pre - trained contextual language model ( BERT ) by using attention over the senses of a word in a context and transferring this sense information to fit multi - sense embeddings in a skip - gram - like framework. The authors propose a two - stage approach to train the sense disambiguation mechanism in their model with a distribution over word senses extracted from the output layer embedding of BERT. Experiments on the contextual word similarity and sense induction tasks show that this method is superior to the non - contextual word embedding method in terms of performance on the proposed tasks.   The authors also conduct experiments to evaluate the benefits of using this multi -sense embedding in a downstream application, namely embedded topic modeling ( EBM )."
SP:e4cdba0fc7cd7f440d4436219f3959d8d5e2ad28,"This paper proposes to use the same architecture and pretrained weights of a neural net model to understand both images and point - clouds. Specifically, they transfer the image - pretrained model to a point -cloud model by inflating 2D convolutional filters to 3D convolutionsal filters and finetuning the inflated imagepretrained models ( FIP ). They find that models with minimal finetuned efforts — only on input, output, and batch normalization — can achieve competitive performance on 3D point-cloud classification. Meanwhile, FIP improves data efficiency, reaching up to 10.0 points top - 1 accuracy gain on few - shot classification. It also speeds up training of point - clouds models by up to 11.1x for a target accuracy."
SP:dc99c307931ae9c5d4a1b998dc94cfc6ac78d11f,"This paper proposes a method for training an autoregressive generative model based on an energy - based learning objective ( EBM ). The motivation is that EBM suffers from two intrinsic flaws, i.e., exposure bias and lack of long - range coherence, which limits its ability to model distributions properly. To alleviate these flaws, the authors propose a joint distribution constraint which fits joint distributions at each time step. The joint distribution is then estimated using the EBM objective. The authors show that their method is capable of alleviating the exposure bias problem and increase temporal coherence by imposing a constraint. Besides, unlike former energy based models, they estimate energy scores based on the underlying autore progressive network itself, which does not require any extra network. Finally, thanks to importance sampling, they can train the entire model efficiently without an MCMC process.   The authors demonstrate how to optimize their model constructed from a single network, using wake - sleep algorithms without MCMC ; iv ) in a number of applications, such as language modeling, neural machine translation, and image generation."
SP:51e748c55bd4134047098559577fa3f37aa7433a,"This paper studies adversarial training ( AT ) methods to improve the robustness of deep neural networks ( DNNs ). It is well - known that adversarial attacks expose a severe fragility of deep learning systems, which is why AT methods incorporate adversarial examples during training. This paper proposes a unified framework that connects Wasserstein distributional robustness ( DR ) methods with current state - of - the - art AT methods. The main contributions of this paper are as follows : 1 ) Theoretically, the authors propose a general framework that bridges DR robustness and standard robustness achieved by AT methods, which encompasses DR versions of the SOTA AT methods and generalizes DR counterparts. 2 ) Practically, motivated by theoretical study, they develop a novel family of algorithms that generalize the AT methods in the standard robusts setting, which have better generalization capacity. 3 ) Empirically, they conduct extensive experiments on benchmark datasets, which show that the proposed AT methods achieve better performance than standard AT methods on some of them."
SP:f192046ea8ad61bfc8e05a0ddb90a8bd15b4640b,"This paper presents a novel method for unsupervised representation learning for multivariate time series. The authors claim that the existing works mainly adopt the framework of contrastive learning and involve the data augmentation techniques to sample positives and negatives for contrastive training. However, their designs of representation learning framework have two drawbacks : ( 1 ) they mostly use segment level augmentation derived from time slicing, which may bring about bias and incorrect optimization with false negatives due to the loss of global context, and ( 2 ) they all pay no attention to incorporate the spectral information and temporal - spectral relations in feature representation. To address these problems, the authors propose a novel framework, namely Bilinear Temporal - Spectral Fusion ( BTSF ). It consists of two modules : S2T and T2S Aggregation, which are used to aggregate information from time - frequency pairs and iteratively refine representations of time series through cross - domain interactions with Spectrum - to - time and Timeto - Spectrum Aggregation modules. Experiments are conducted on three major practical tasks such as classification, forecasting and anomaly detection, which is the first to evaluate on all three tasks. Results shows that BTS F achieves the superiority over the state - of - the - art methods and surpasses them by a large margin across downstream tasks."
SP:ef54840009afb095c67bbbc29a7824c20a375ee8,"This paper proposes an algorithm for automatically adjusting the learning rate during gradient descent in deep neural networks. The proposed method is based on a second - order gradient derivation and analysis, inspired by Newton - Raphson analysis of neural network structure. It is shown to be robust to the initial learning rate and batch size, making it ideal for an off - the - shelf optimizing scheme. The authors also show that the scheme can be extended to accommodate for different learning rates per layer. Extensive experimental evaluation is conducted to validate the effectiveness of the proposed method."
SP:263c787361cd6d4443ce516d389c694d0fe44b28,"This paper proposes a meta - reinforcement learning algorithm for the sequential multi - task learning setting, where the agent can not revisit previous tasks to collect data during training. The proposed algorithm, called continual meta - policy search ( CoMPS ), repeats two subroutines : learning a new task using RL and using the experience from RL to perform offline meta - learning to prepare for the next task in the sequence. The authors compare the performance of the proposed method with several baseline meta - RL algorithms, such as DQN, LQR, and PPO, as well as a few others that are commonly used in reinforcement learning research. The experiments show that the proposed algorithm achieves a higher average reward with fewer samples on average over each of the tasks, and generalizes better to a set of held - out tasks."
SP:2bd729b7aa045bf74e31229c9e76e57af36e804b,"This paper proposes a new threat model for poisoning classifiers, where a third party aims to gain control of the poisoned classifiers without access to the original trigger. Under this threat model, the authors propose a test - time, human - in - the - loop attack method to generate multiple effective alternative triggers that can be used to bypass the access to both the backdoor and the training data. The authors compare their approach to previous work on modeling trigger distributions and find that their method are more scalable and efficient in generating effective triggers than those of previous works. The paper also includes a user study which demonstrates that users easily determine the existence of such backdoors in such classifiers.   The authors argue that there is no such thing as a secret backdoor in poisoned classifier : poisoning a classifier invites attacks not just by the party that possesses the trigger, but from anyone with access to classifier. The main contributions of this paper are as follows :    1 ) they consider a new thread model of poisoned classesifiers, in which : ( 1 ) a party that wishes to control the classifier can do so by first visualizing smoothed adversarial examples and then using human inspection to construct effective triggers. ( 2 ) they propose a interpretable, human in theloop attack method under this model to generate alternative triggers by first generating adversarial example for a smoothed version of the smoothed classifier by first creating a set of examples for each classifier class. ( 3 ) they demonstrate the effectiveness of their approach on constructing alternative backdoor triggers in high - resolution datasets such as ImageNet and TrojAI. ( 4 ) they compare their method to other methods and show promising results."
SP:e58ab0e3cff6b18013145a1a99cfa9da0a3d872f,"This paper proposes a novel method for distilling the output of StyleGAN2, an unconditional GAN based on the StyleGAN architecture. The authors claim that the output discrepancy between the teacher and student model is the main challenge of GANs. They identify that the style module is the determining factor to ensure output consistency and propose a novel initialization strategy for the student model, which can ensure the output consistency to the maximum extent possible. They also propose a latent - direction - based distillation loss to employ the rich relational knowledge between different images, which outperforms the existing state - of - the - art CAGAN ( Liu et al. 2021 ) by a large margin. Extensive experiments demonstrate the effectiveness of their approach in distilling StyleGAN 2, outperforming existing GAN distillation methods by a Large margin."
SP:2c2231743fa33b95828c6615263954ce1c05f95d,"This paper proposes a method for generating online approximations of offline algorithms based on graph - based learning. The method is based on a multi - task learning model, which is trained on synthetic data and historical stock market data. The model is trained to generate labels based on the behavioral structure of the offline algorithms, and the labels are then used to predict the offline algorithm's predictions. The paper claims that this approach is the first general and end - to - end differentiable approach for generating offline algorithms.   The main contributions of this paper are as follows. First, it formalizes the notion of behavioral structures in characterizing offline algorithms by using them to generate descriptive labels and then formulate the machine learning problem in machine learning terms. Second, it introduces the multi-task ML model that is used in experiments on synthetic and real - world data. Third, it shows that the method can be equally applied to any offline algorithm on time series data. Finally, it discusses the directions for future research."
SP:ee3a21d2fb8a073099aa200129a53c31f3b6561d,"This paper proposes a new method for improving the scalability of Gaussian Processes ( GP ), a nonparametric Bayesian model based on variational approximations. The proposed method is based on sparse variational GPs, where the number of inducing points is limited to a fixed number of $ \sqrt(N^2)$. The main idea is to use a neural network to compute the parameters of the posterior of the variational distribution $ q$ for each input data point, and then use the output of the neural network as the inducing points. The method is evaluated on several datasets, and shows that it is able to perform similar or better than the state - of - the - art variational GP methods. However, the training and prediction times of the proposed method are much better than those of other methods."
SP:f20c99b441545047a16ae524cc2e317b2c3787a2,"This paper proposes a protocol for secure ( Byzantinetolerant ) decentralized training that emphasizes communication efficiency in the presence of malicious peers. The protocol is based on two components : a communication protocol and a training protocol. The communication protocol is designed to be able to handle both Byzantine and Sybil attacks. The training protocol consists of two components. The first component, the communication protocol, is designed for distributed training. The second component is the training protocol, the Sybil protocol.   The authors conduct two experiments to evaluate the effectiveness of the proposed communication protocol. First, the authors test the effectiveness on ResNet-18 for CIFAR-10 classification and pretraining ALBERT-large in a setup where almost half of all peers are malicious. They show that it has a marginal communication overhead overhead in terms of the bounds for its resistance against Byzantine attacks and theoretical bounds for the resistance against Sybil attack. Second, they conduct controlled experiments on image classification and language modeling and modeling in presence of Byzantine attackers. The results show that the proposed protocol is more effective than the previous communication protocol for handling Byzantine attacks."
SP:93894f20ab2593e5237b6972fef9fe63e96af89a,"This paper presents a method for learning a physics - informed Lagrangian model of turbulence, based on SPH, to solve inverse problems in astrophysics and engineering. SPH is a mesh - free method for obtaining approximate numerical solutions of the equations of fluid dynamics, which has been widely applied to weakly and strongly compressible turbulence. The authors present a learning algorithm that learns a hierarchy of parameterized and “ physics - explainable ” SPH informed fluid simulators using both physics based parameters and Neural Networks as universal function approximators. The learning algorithm develops a mixed mode approach, mixing a forward and reverse mode automatic differentiation with forward and adjoint based sensitivity analyses to efficiently perform gradient based optimization. They show that their learning method is capable of solving inverse problems over the physically interpretable space of Neural Network parameters, as well as over the space of Lagrangeian trajectory based trajectories. They also analyze this hierarchy of models and address how adding more of the known physical ( SPH ) structure affects the ability of the models to generalize to longer times and larger Reynolds numbers. Finally, in section 6, they draw conclusions on the progress made so far along with providing a discussion of future work."
SP:d11b81f9ab414fcf430a03cd70c2d3246b678474,"This paper proposes an approach to regularize a single deterministic neural network ( DE ) based on a data - dependent regularizer that maximizes the entropy of the embedding space between the class clusters. This is achieved by synthetically generating between - cluster samples via the convex combination of two images from different classes and maximizing the entropy on these samples. The authors claim that their approach is more robust to the superficial input perturbations compared to the recently proposed SNGP and DUQ ( van Amersfoort et al. 2020 ) approaches. They compare their approach with ResNet and Wide - ResNet architectures using real - world datasets ( CIFAR-10 and CifAR-100 ). They demonstrate that the proposed approach consistently provides much improved classification accuracy, better calibrated probabilities in the case of domain shift and reliable uncertainty estimates when exposed to situations involving domain - shift and out - of - distribution samples.   The main contributions of the paper are as follows :   1. The approach is simple and does not require any modifications to the architectures of the DE. This means that, as opposed to the extremely competitive DE, it is an extremely efficient model. 2. The proposed approach is data - independent, which means that the authors do not need to modify the architecture in order to obtain better accuracy. 3. The experimental results are very promising."
SP:365490b872464f00634dc7a50d024fceaf0a61ee,"This paper presents a new method for image animation based on deep generative models. The authors propose a method called Latent Image Animator ( LIA ), which is a self - supervised auto - encoder - autoencoder model based on GANs. The key idea is to learn a set of orthogonal motion directions in the latent space of the source image, and use their linear combination, in order to represent any displacement of the latent code in the space of images. The proposed method is evaluated on the VoxCeleb dataset, where it is shown to be able to transfer motion from a driving video onto two still images of Marilyn Monroe and Emmanuel Macron. The experimental results show that the proposed method significantly outperforms the state - of - the - art methods on both the raw and generated quality of the images."
SP:86f9f89f84e117c86478b9afaf087f65524f5472,"This paper proposes a meta - learning method called task interpolation ( MLTI ) to generate additional meta - training tasks by randomly sampling a pair of tasks and interpolating the corresponding features and labels.   Theoretically, the authors prove that MLTI corresponds to a data - adaptedive meta - regularization and further improves the generalization ability of the proposed method. The authors conduct extensive experiments on 8 datasets from diverse domains including image recognition, pose prediction, molecule prediction, and classification. They find that the proposed framework is compatible with six representative meta - learner algorithms and consistently outperforms other state - of - the - art strategies."
SP:73d577e9c4f4af5e11a9e5bdb583ee0f50a315f5,"This paper proposes a new method to guarantee fairness of adversarial downstream predictors in representation learning. The authors claim that recent work has shown that strong adversarial predictors can still exhibit unfairness by recovering sensitive attributes from learned representations. The proposed method, called Fair Normalizing Flows ( FNF ), trains an encoder to minimize the statistical distance between the latent representations of different groups. The encoder is trained as a normalizing flow trained to maximize the distance between each latent representation of the two groups. This allows the authors to guarantee the maximum unfairness of any potentially adversarially biased downstream predictor. The main advantage of FNF is that its exact likelihood computation allows us to obtain guarantees on the maximum fairness computation. Experiments are conducted on a variety of datasets to demonstrate the effectiveness of the proposed method."
SP:404d5643327f60f0f06f820033a56081f9e01900,"This paper proposes a novel graph neural network ( GNN ) model, COUNT - GNN, for the task of subgraph isomorphism counting for graph subgraphs. The main idea of the model is to learn a low - dimensional representation for both the query and the input graph in order to predict the number of sub - graph isomorphisms. The proposed model is based on a message passing mechanism, where messages are propagated to all edges and aggregated based on the edge adjacency. To encode fine - grained structural information for structure matching, the authors adopt an edge - centric message passing scheme and modulate the graph representation conditioned on the query to improve structure matching. Experiments on several benchmark datasets demonstrate that the proposed model significantly outperforms the state - of - the - art GNN - based models on sub graph isomorphicism counting.   The main contributions of the paper are three - fold : ( 1 ) The model capitalizes on edge - centered aggregation to encode structure information, which improves structure matching between the queries and input graph from the edge perspective, and ( 2 ) The authors also design an query - conditioned graph modulation in GNN to adapt structure matching to different queries from the graph perspective."
SP:5a94f18156ab2949c86de45fcf0de2e16977eebb,This paper proposes a federated learning method named Agnostic Personalized Federated Federated Learning ( APFL ). APFL is based on two components : Similarity Matching and Kernel Factorization ( SimFed ). SimFed measures task - level similarity based on locally learned knowledge and matches the relevant ones for personalized knowledge reflection. The authors also factorize their model parameters into two basis vectors and the highly sparse masks to significantly reduce the dimensionlaity of parameter space for alleviating knowledge collapse and information loss when reflecting the heterogeneous knowledge. They extensively validate their method on both single and multi - domain datasets and show that it outperforms the current state - of - the - art baselines.
SP:97f30bea31eccef6c770fbce1e14fd6d2493a178,"This paper presents a method to learn object - centric representations of video scenes by distilling explicit object dynamic representations ( e.g. velocity ) from raw video input. The method is based on Object Dynamics Distillation Network ( ODDN ), a method that distills explicit object dynamics representations ( velocity, object - pair interactions ) from video input into a relation module that calculates object - Pair interactions and applies it to the corresponding dynamic representations of objects in the scene. The paper also presents a relation model that allows the model to obtain better scene decomposition and reconstruction quality in segmentation and reconstruction. Experiments are conducted on tasks of video events reasoning and video prediction, which are two important evaluations for video understanding. The results show that visual representations of the proposed method perform better in answering reasoning questions around physical events in a video."
SP:ba8e50d1fa9cb824fa3f76c0c691997cd151d760,"Graph neural networks ( GNN ) have shown advantages in many graph - based learning tasks but often fail to predict accurately for a task based on sets of nodes such as link /motif prediction and so on. Many works have recently proposed to address this problem by using random node features or node distance features but they suffer from either slow convergence, inaccurate prediction or high complexity. In this work, the authors revisit GNNs that allow using positional features of nodes given by positional encoding ( PE ) techniques such as Laplacian Eigenmap, Deepwalk, etc. The authors study these issues in a principled way and propose a provable solution, a class of GNN layers termed PEG with rigorous mathematical analysis. PEG uses separate channels to update the original node features and positional features simultaneously. Extensive link prediction experiments over 8 real - world networks demonstrate the advantages of PEG in generalization and scalability."
SP:cf448479f68c3194c1a9e11729bf70d7cc2ae8fd,"This paper proposes LaMer, a text style transfer framework based on large - scale language models. The main idea is to leverage the intrinsic parallelism within the data that exists between the two parallel datasets Dsrc and Dtgt without considering the self - parallelism between D - srt and D - tgt. The model first mines roughly parallel expressions in the non - parallel datasets with scene graphs, and then employs MLE training, followed by imitation learning refinement, to leverage parallelism in the data. On two benchmark datasets (sentiment & formality transfer ) and a newly proposed challenging task ( political stance transfer ), LaMer achieves qualitative advances in transfer accuracy, content preservation, and human evaluations. Experiments demonstrate that our model not only makes training more efficient but also generates more readable and diverse expressions than previous models."
SP:8f7b2d1020d9e527118b8fb816760c13b0d0bfcb,"This paper proposes a method to answer hyper - relational queries in Graph Neural Networks ( GNNs ) based on qualifier pairs. Qualifier pairs are pairs of key - value pairs that correspond to nodes in a graph that are selected by a neural network based on a multi - hop logical reasoning algorithm. The proposed method is based on the fact that existing algorithms operate on classical triple - based graphs, whereas modern KGs often employ a hyper - relational modeling paradigm. In this paradigm, typed edges may have several key - values pairs known as qualifiers that provide fine - grained context for facts. In queries, this context modifies the meaning of relations and usually reduces the answer set. Besides that, the authors propose a method called Qualifier - based QA to answer such queries and demonstrate in their experiments that qualifiers improve query answering ( QA ) accuracy on a diverse set of query patterns."
SP:5f8b58424a1a8eeb72217e75189d6f773a298a7a,"This paper presents DYHPO ( Dynamic Hyperparameter Optimization of Gray Boxes ), a method for finding hyperparameter configurations that outperform state - of - the - art methods for fine - tuning hyperparameters in the gray - box setting. The method is based on Bayesian optimization with a surrogate for Gaussian Processes setup. The authors propose a new surrogate for the Gaussian processes that embeds the learning curve dynamics and a new acquisition function that incorporates multi - budget information. The proposed method is compared with the state of the art methods in two large - scale experiments on three datasets. The main contributions of the paper are as follows :   1. Demonstrating that the proposed method outperforms the state-of - the-art methods in terms of performance on the three datasets ( Tabular, Image and NLP ). 2. Showing that the method is able to learn to find the configuration that performs the best across all the datasets. 3. Providing empirical evidence on the performance of the method on the tabular dataset and the neural network on the NLP dataset."
SP:99d3d94e3af5d2dc7b92c00ac1345d1d2dd0d15b,"This paper proposes a new method to improve the performance of learned image compression by making the model inference integer - arithmetic - only, which is much simpler than the existing training and fine - tuning based approaches yet still keeps the superior rate - distortion performance of learning image compression. The authors also propose to extend the deterministic inference to fit Gaussian mixture models and improve the discretization of the entropy parameters. The proposed method is shown to improve upon the state - of - the - art image compression models that can infer in a cross - platform consistent manner, which makes the further development of the developed quantization method more promising."
SP:85d0df515e9e555f3ea1c21d607304dfaeae69c0,"This paper proposes a noise reconstruction and removal network for denoising FIB - SEM images. The proposed method is based on a neural network architecture inspired by gated recurrent units, which reconstructs and removes the noise by synthesizing the sequential data. The network consists of two modules : Noise reconstruction module and Noise2Noise loss module. The noise reconstruction module is designed to be fully unsupervised. The training objective is to train the network to distinguish true signal from noise.   The paper presents three main contributions : ( 1 ) A new noise reconstruction method, which uses recurrent units to reconstruct and remove the noise from 3D electron microscopy images. ( 2 ) A detailed performance analysis using numerical as well as empirical metrics. ( 3 ) An improvement in the performance of noise reconstruction over the noise2noise loss method."
SP:e6275b0b103fa90dcebcdd3d3c14c830c3402972,"This paper studies the label propagation strategy in graph neural networks ( GNNs ) and the stochastic label trick. The former is typically based on stacked message - passing layers that share neighborhood information to transform node features into predictive embeddings, while the latter involves spreading label information to unlabeled nodes via a parameter - free diffusion process, but operates independently of the node features. Given that the material difference is merely whether features or labels are smoothed across the graph, it is natural to consider combinations of the two for improving performance. In this regard, it has recently been proposed to use a randomly - selected portion of the training labels as GNN inputs, concatenated with the original node features for making predictions on the remaining labels. This so -called label trick accommodates the parallel use of features and labels, and is foundational to many of the top - ranked submissions on the Open Graph Benchmark ( OGB ). The authors argue that under certain simplifying assumptions, the label trick can be reduced to an interpretable, deterministic training objective composed of two factors : ( 1 ) a data - fitting term that naturally resolves potential label leakage issues, and ( 2 ) regularization factor conditioned on graph structure that adapts to graph size and connectivity.   The authors firstly argue that the main contribution of the paper is to provide a perspective to motivate a broader range of label trick use cases, and provide experiments to verify the efficacy of these extensions. Later, the authors leverage this perspective to encourage the use of label propagation as a regularizer in a range of other than node property prediction tasks."
SP:b6cbc3661f9c440687c3dd01ee35a118c87db377,"This paper presents a multi - agent agent model for understanding theory of mind ( ToM ). The authors propose to model machine theory of mind in a more flexible and symmetric scenario where all agents can speak, listen, see other agents, and move freely through a grid world. They show that the best agents fail to achieve performance comparable to other agents with access to the gold - standard mental state of other agents. An effective strategy to solve SymmToM requires developing theory ofMind to maximize each agent ’s rewards. They also show that even maintaining the simple rules of the environment, modifying its parameters results in much more difficult challenges for models, even for models where we artificially introduce perfect information.   They discuss examples where different levels of theory of Mind are required to solve the task, and possible metrics, such as reward for learning."
SP:f8ce83805eee46c6c196e8477bf10d8d7f7e0f46,"This paper presents a method for zero - shot object detection for training vision systems for manufacturing robots. The method is based on the YCB Video Dataset ( YCB ), which contains 21 objects in various categories. The main contribution is a novel neural network structure that based on YOLOv5 and able to perform generalized zero -shot detection. The output bounding boxes can be further combined with other gZSL algorithm to achieve full zero - shots object detection and recognition. The authors also propose a novel attribute labelling method for objects in YCB video dataset, which represents colour and shape information of an object for the neural network to learn.   The main contributions in this paper are in three folds : 1. A novel splitting method for YCB dataset that splits the dataset by seen and unseen objects. This splitting can be used for both gZSD and gZ SL research that related to daily objects. 2. A new class label for object detection, which is a combination of 16 attributes to represent colour and Shape information. 3. A novel method for training the vision system for training with YCB datasets."
SP:aa1dcd9217270010f16a00004facede942efea17,"This paper investigates how to train an autoregressive latent video prediction model capable of generating high - fidelity future frames with minimal modification to existing models, and produce high - resolution ( 256x256 ) videos. Specifically, the authors scale up prior models by employing a high - fidelity image generator ( VQ - GAN ) with a causal transformer model, and introduce additional techniques of top - k sampling and augmentation to further improve video prediction. The authors show that the proposed method achieves competitive performance to state - of - the - art approaches on standard video prediction benchmarks with fewer parameters, and enables high resolution video prediction on complex and large - scale datasets. They also show that pre - trained representations of HARP can be useful for learning multi - task imitation learning agent on Meta - World MT50 benchmark ( Yu et al. 2020 )."
SP:7f57896afd63bc869d2db6ddf7abbeaa71daae11,"This paper proposes to integrate the ViT architecture into generative adversarial networks ( GANs ) to improve image generation performance. To this end, it introduces ViTGAN, a GAN architecture composed of ViT discriminators and generators. The authors claim that existing GAN regularization methods interact poorly with self - attention, causing instability during training. To resolve this issue, they introduce several novel regularization techniques for training GANS with ViTs. For ViT generators, they examine architectural choices for latent and pixel mapping layers to ensure convergence of faciliate convergence. Empirically, their approach achieves comparable performance to the leading CNN based GAN models on three datasets : CIFAR-10, CelebA, and LSUN - 10. However, it is not our intention to claim that it is superior to the best - performing CNN - based models. Instead, it aims to close the performance gap between the conventional CNN-based GAN architectures and the novel ViT - based architecture, composed of vanilla ViT layers."
SP:bbae3afcaea0a2e54904cb8daaed7df4fe37da6e,"This paper proposes a two - stage training method for image generative models based on variational autoencoders. The authors hypothesize that the high - dimensional nature of image data distributions poses an intrinsic challenge to good generative modeling. They show that the entropy in these natural image distributions is attributable to visually imperceptible information, which dominates the training objective, giving models an easy way to achieve competitive likelihoods without successful modeling of the visually perceptible bits. Based on this hypothesis, the authors propose a secondary high - rate model to be trained on top of the low - rate one in the ELBO framework. The secondary model is restricted to modeling visually imperCEPTIBLE information, and the authors show that it can improve the sample quality achieved by the initial low -rate model with minimal impact on the initial ELBO.    The authors further propose a second stage training process that trains a second high - rank model to capture the information that is the bulk of the likelihood signal. The second stage is similar to the one proposed in [ 1 ]. However, low - rank models have much worse ELBOs due to poor modeling of information that the secondary model does not capture."
SP:bfed56018134ec66cde9a7e958df964d4cca3164,"Diffusion probabilistic models ( DPMs ) represent a class of powerful generative models. However, due to the large number of timesteps required to train them, it is difficult to estimate the variance in each timestep of the reverse process. In this paper, the authors propose Analytic - DPM, a training - free inference framework that estimates the analytic forms of the variance and KL divergence using the Monte Carlo method and a pretrained score - based model. Theoretically, they show that both the optimal reverse variance and the corresponding optimal KL divergence of a DPM have analytic forms w.r.t its score function. Further, they derive both lower and upper bounds of the optimal variance and clip the estimate for a better result. Empirically, their method consistently improves the log - likelihood of these DPM and meanwhile enjoys a 20× to 40× speed up."
SP:3f935ba5784c3e86db72421426bc479061af1a4b,"This paper investigates whether it is feasible to switch to vision transformers ( ViTs ) as the de facto approach to automated medical image diagnosis, pushing the state - of - the - art in classification, detection and segmentation tasks, while at the same time potentially gaining from other properties of ViTs such as built - in explainability. The authors conducted a series of experiments on several standard medical image benchmark datasets and tasks, and compared ViTs with CNNs on natural and synthetic images. They found that, while CNNs perform better if trained from scratch, off - The - shelf vision transformer can perform on par with and sometimes better than CNNs when pretrained on image, both in a supervised and unsupervised setting. They also found that ViTs can perform better on image classification tasks if the classification task is performed by a combination of two methods, i.e., one based on the natural image and another based on synthetic image.   The authors also conducted some ablation studies to see if it is possible to switch from CNNs to ViTs."
SP:a64e0535f268901e38fd51e027c612ebcdbae1a4,"This paper studies the problem of pre - training neural language models ( NLMs ) with natural language understanding ( NLUs ). The authors show that the pre - trained NLMs tend to over - generalize, while the fine - tuned NLMs underperform. The main reason for this is that the pretrained NLMs learn to generalize only between text segments that appear in the same training example, but not between different text segments seen in different training examples. This leads to a bias towards overgeneralization. To address this problem, the authors propose two methods. The first one is to pre - train the NLMs with the same input as the natural language learner, but only between two sentences that are never shown to each other in the training example. Then, the second method is to fine - tune the NLM on two separate sets of sentences, one that is shown to be semantically related to the other, and the other is not.   The authors prove two main results. First, the first shows that pre - pretraining NLMs is more general than fine - tuning NLMs in terms of the number of layers required to model dependencies between two text segments. The second shows that the depth of the network is much smaller for pre - pre - finetuned NLMs than for fine - fined NLMs. This indicates that there is room for further improvement in the way NLMs are trained. The author's second contribution is to propose a new training scheme for NLUs."
SP:59066956fa2e423d5f2d2ea4f91c4ddf6afd4683,"This paper proposes a new method for learning to optimize ( L2O ) models, based on a combination of meta - training and symbolic regression. The main idea is to replace the neural network representation of the optimization rules with a symbolic representation in order to reduce the memory and interpretability overhead of the neural networks. The authors claim that the proposed method is better suited for large - scale optimization problems, as it is able to scale up to a ResNet-50 with 23.5 million parameters, and achieves state - of - the - art performance when applied to training larger ( ResNet -152, 58.2 million parameters ) or very different deep models ( MobileNet v2, Sandler et al., 2018 ).   The main contributions of the paper are as follows :   1. Introducing the symbolic regression to the optimization rule generation process of L 2O models for the first time. This allows the authors to avoid two common pitfalls of using neural networks in the optimization process : ( 1 ) scalability : the numerical rules represented by neural networks create extra memory overhead, and limits their applicability to optimizing larger tasks ; ( 2 ) interpretability : it is unclear what each L2 O model has learned in its black - box optimization rule, nor is it straightforward to compare different models in an explainable way. To address these issues, the authors introduce a lightweight symbolic representation and trainable re - parameterization to enable fine - tuning of the rules. The proposed method outperforms existing manually designed and tuned optimizers on various datasets. The paper also proposes a lightweight re - parametrization method to enable L2o models to be trained on larger datasets."
SP:54dfeb363beee9959aecc9e0853ff06e43bd94e4,"This paper presents a method for defending against adversarial attacks using deep neural networks ( DNNs ). The main contribution is to prove an adaptive version of the Neyman -Pearson Lemma, which is a provable version of smoothing - based robustness certificates for smoothing based RL. The key idea of the method is to introduce noise to the adversarial perturbation at each time step so that the adversary can infer the defense strategy used by the victim agent by observing the states, actions, etc. from previous time - steps and adapt itself to produce stronger attacks in future steps. The authors show that their method provides nontrivial certificates on Cartpole, Pong, Freeway and Mountain Car environments that can yield meaningful robustness guarantees in practice.   The main theoretical contribution of the paper is to propose a method to guarantee that the final total reward obtained by policy smoothing remains above a certain threshold, even though the actions at intermediate time -steps may change under the attack. The method is evaluated on four standard Reinforcement Learning benchmark tasks to evaluate the effectiveness of the defense and the significance of the theoretical results. The experiments show that the method can outperform the average performance of undefended agents under a practical attack in at least two of the tasks."
SP:e0f9add5fde18eaab0eeb2b10b14928acc8ec5b8,"This paper proposes a method for predicting the target performance of a model trained on labeled source data and unlabeled target data. The proposed method, ATC, learns a threshold on the model ’s confidence and uses it to predict the accuracy of the model. The method is based on previous methods across several model architectures, types of distribution shifts ( e.g. due to synthetic corruptions, dataset reproduction, or novel subpopulations ) and datasets ( WILDS, ImageNet, BREEDS, CIFAR, and MNIST ). In experiments, the authors show that ATC estimates target performance 2.4 % more accurately than prior methods. They also explore the theoretical foundations of the problem, proving that in general, identifying the accuracy is just as hard as identifying the optimal predictor and thus, the efficacy of any method rests upon ( perhaps unstated ) assumptions on the nature of the shift. Finally, analyzing their method on some toy distributions, they provide insights concerning when it works and when it fails."
SP:e748bf6ee653087cae825df32a8546f9ccebfcf1,"This paper proposes a method to solve the partial distribution matching ( PDM ) problem of point set registration, where the goal is to recover a transformation that matches one set to the other. The proposed method, called Partial Wasserstein - 1 ( PWAN ), is based on the partial discrepancy theory, which theoretically derives the Kantorovich - rubinstein duality for the discrepancy and shows its gradient can be explicitly computed. Based on this theory, the authors propose a method called PWAN, which approximates the PW discrepancy by a neural network, which learns the transformation and learns the adversarially adversarial with the network, and also incorporates an efficient coherence regularizer for non -rigid transformations to avoid unrealistic deformations. The authors evaluate the method on a number of real - world data sets, and show that the proposed method is robust against outliers, the unknown non - rigid deformations, and the large sizes of point sets."
SP:f94f77696d100b2638fa2a6d82c8df47db3b6a36,"This paper proposes a transfer learning method for hyperparameter optimization ( HPO ) based on Deep Kernel Gaussian Process surrogate with Landmark Meta - Features ( DKLM ). DKLM is a neural network that can be jointly meta - trained on a set of source tasks and then transferred efficiently on a new ( unseen ) target task. The main idea is to capture the similarity between hyper - parameter configurations with an end - to - end meta - feature network that embeds the set of evaluated configurations and their respective performance. As a result, the proposed method can learn contextualized dataset - specific similarity representations for hyper - parameter configurations. The authors experimentally validate the performance of DKLM in a wide range of HPO meta - datasets from OpenML and demonstrate the empirical superiority of our method against a series of state - of - the - art baselines."
SP:e3c57f3589e8ab674644d900c14b3473cd71a23f,"This paper proposes a new method for fingerprinting generative models for deep fake detection and attribution. The key idea of the method is to train a generative model with a specific set of fingerprints that can be used to identify the source of a given dataset. The method is based on the fingerprinting method developed in the field of deep generative adversarial networks ( DANs ). The authors claim that this method is more efficient than other methods for generating fake data as it is able to detect the difference between real data and generated data more accurately. The proposed method uses a 128 - bit fingerprinting operation to generate a large population of models with distinct fingerprints. Each model is trained to generate 10 fingerprints, which are then used to generate samples that contain the fingerprints of a specific source. The paper claims that this allows for more accurate detection of fakes and more accurate attribution of source data.   The authors also claim that the proposed method is the first one of its kind for generating samples that contains more than 10 fingerprints. This is achieved by training 10 models with the same number of fingerprints and using only one of them for each of the 10 instances of fake data. They claim that it is the only method in the literature that uses this many fingerprints for such a small number of samples. They further claim that their method is also the only one that uses more than one of these 10 fingerprints to generate fake data for each instance of real data. In the experiments, they show that the method outperforms several other methods in terms of accuracy and accuracy."
SP:73bffd1a0856b80d29f7a2b2b68be57882531f07,"This paper proposes two methods for explaining post - hoc local explanations for similarity learners for tabular and text data. The first method, analogies, is based on finding analogous pairs of examples that share the same level of similarity as the input pair and provides insight into ( latent ) factors underlying the model ’s prediction. The second method, feature attributions, provides a set of attributions to explain the similarity between a pair of inputs as determined by a black box similarity learner. The authors compare their methods on the Semantic Textual Similarity ( STS ) dataset ( CER et al., 2017 ) and patients in terms of their healthcare utilization using Medical Expenditure Panel Survey ( MEPS ) data, and IRIS ( IRIS ) iris species. The proposed methods outperform feature and exemplar - based baselines in both quantitative evaluation and human user study. They also present examples of feature - and analogy - based explanations and illustrate specific insights."
SP:6a3c4ae05d582f8896840483b08c735ced2976bc,"This paper analyzes the robustness of ensemble ML models under the model - smoothness assumption. The authors first prove that ensemble models are more robust than a single model in terms of certified robustness, while standard ensemble models only achieve marginal improvement compared to single model. Then, the authors propose to train ensemble models with diversity regularized training ( DRT ) to train certifiably robust ensemble models. Experiments on MNIST, CIFAR-10 and ImageNet datasets show that DRT enhanced ensembles can achieve higher certification robustness than existing single and ensemble models, demonstrating the state - of - the - art certified L2 - robustness.    The main contributions of the paper are the following :   1. Demonstrating that under mild conditions, ensemble models can achieve certification of robustness higher than single models. 2. Proving that diversified gradient and large confidence margin are sufficient and necessary conditions for certifiable robustness under model -smoothness assumption, and 3. Providing analysis based on the proposed Ensemble - before - Smoothing strategy. 3. Proposing a suitable smoothing strategy to improve the smoothing performance of ensembled models."
SP:3002b29c27709780238876d8c3f81bbd6a0f8112,"Graph Neural Networks ( GNNs ) have become increasingly popular architectures for learning with graphs. Recent works have revealed important shortcomings in their expressive power. In response, several higher - order GNN have been proposed that substantially increase the expressive power, albeit at a large computational cost. Motivated by this gap, this paper explores alternative strategies and lower bounds. In particular, it analyzes a new recursive pooling technique of local neighborhoods that allows different tradeoffs of computational cost and power. First, we prove that this model can count subgraphs of size k, and thereby overcomes a known limitation of low - orders GNN. Second, we show how recursively pooling can exploit sparsity to reduce the computational complexity. More generally, we provide a matching information - theoretic lower bound for the number of graph representations that pool over time."
SP:5d0cbd84336caf5f31e1f98e11f6733230e4d792,"This paper studies the problem of knowledge integration in pre - trained language models. It is well - known that language models are not very good at capturing factual knowledge. This has led to the development of knowledge - enhanced language models ( KI ) which aim to incorporate external knowledge into pretrained LMs. However, the efficacy and limitations of these methods are not well -understood. In this paper, the authors revisit the KI process in an information - theoretic view and show that KI could be interpreted using a graph convolution operation. Based on that, they propose a simple probe model called Graph Convolution Simulator ( GCS ) for interpreting KI methods. They show that GCS can correctly simulate and interpret the process for two popular knowledge - enriched language models : K - adapter ( Wang et al., 2021 ) and ERNIE ( Zhang et al, 2019 ). They find that only a small amount of factual knowledge is captured in these models during integration. They also break down the analysis of KI in terms of type of relations and the popularity of entities, and find that simple relational knowledge is often catastrophically forgotten, while complex relational knowledge often gets overlooked. The authors conduct experiments to verify that our proposed GCS model can indeed be used to correctly interpret KI processes, and use it to analyze two typical knowledge - analyze two different knowledge - analytically analyzed LMs : K- adapter and ER - NIE."
SP:7e73948421e98307fceb69a316d8a4e7c4926cda,This paper studies the effect of the adaptation learning rate in meta - learning with mixed linear regression. The authors propose a principled way to estimate the optimal adaptation learning rates that minimize the population risk of MAML. They show that ERM outperforms empirical risk minimization ( ERM ) in terms of the initialization distance to the task optima. They also provide empirical results that show that the learning rate that is optimal in meta learning is also optimal in practice.   The authors also provide theoretical results that suggest that the optimal learning rate is closer to the optimal initialization distance in practice than optimal in theory.
SP:effbc85d89b1197d9c2abcaf5ff13864135dd6e1,"This paper proposes a new method for source - free domain adaptation ( SFDA ) based on feature extraction from unlabeled data in a source domain to unlabelled data in the target domain without access to source data during adaptation. The proposed method, called Feature Restoration ( BUFR ), is a bottom - up training scheme for SFDA which boosts performance by preserving learnt structure in the later layers of a network.   The main contributions of the paper are as follows :   ( 1 ) The authors propose a method to extract the same features from the source domain data as were extracted from the target data rather than extracting new ones as in the case of SFDA. The source data is kept in the same way as in SFDA, i.e., it is only used to estimate the feature distribution under the source data. The target data is used to compute the approximate feature distribution that is used in the feature extractor that is applied in the adaptation of the model to the source. The model is trained with the same parameters as the source model, but with the assumption that the source features are available to the model. The authors argue that this ensures that the model does not lose the calibration that is important for the calibration of the source models. The method is tested on real and synthetic data and compared with several baselines. The results show that BUFR outperforms existing SFDA methods in terms of accuracy, calibration and data efficiency, while being less reliant on the performance of the models in the source domains."
SP:7d63034ec7e6a4f178681ff2a49feb485cd47116,"This paper proposes Federated Robust BatchNormalization ( FedRBN ), a method for propagating adversarial robustness among distributed learners in Federated Learning ( FL ) setting. The proposed method is based on batch normalization ( BN ) statistics, inspired by the strong connection between model robustness and statistic parameters in the BN layer ( Schneider et al. 2020 ). Experiments are conducted to show that the proposed method yields robustness competitive with the best all - AT - user baseline ( FATBN ) by only a 2% drop ( out of 59%) on robust accuracy compared to the baselines of AT - baseline ( FATBN ). The experimental results also show that even when only 20% of non - iid users used AT during learning, the proposed FedRNN yields comparable performance to the best baselines.   The main contributions of the paper are as follows :   1. The authors propose a simple yet effective method to propagate robustness through batch - normalization statistics among FL users without requiring raw data to be shared ; 2. The method is communication efficient as it only incurs an one - time additional communication after training. 3. The experiments demonstrate the feasibility and effectiveness of their method."
SP:42c7a79e58b6a9f776fa6ae928bd89c194f9303f,"This paper proposes a transformer - like architecture for inferring network structure from observed game outcomes ( equilibrium actions ) without explicit knowledge of the utility function. The proposed method is applied to three network games, where a player ’s payoff depends not only on their own actions but also on those of their neighbors ’ actions.   The main contribution of the paper is to propose a novel transformer-like architecture which correctly accounts for the symmetries of the problem and learns a mapping from the equilibrium actions to the network structure of the game without explicit utility function knowledge. The authors test their method on three different types of network games using both synthetic and real - world data, and demonstrate its effectiveness in network structure inference and superior performance over existing methods. The main contributions of this paper are as follows. First, the authors propose a model based on a novel permutation - invariant transformer architecture that can be used to infer network structure behind the games without explicitly knowing the utility functions. Second, to our knowledge, the framework is one of the first to first to infer the network structures of the three games using only observed games, and the authors compare their method with two existing methods, one based on data - driven structural inference and the other based on the Taylor Expansion. The experiments show that their method outperforms the other two methods."
SP:1c7b9157cf8c06ca771da78895fc3af969b0fb85,"This paper presents GraphANGEL, a novel method for relation prediction in heterogeneous graphs based on subgraphs of observed nodes and relations. The main idea is to predict the relations between each node pair by checking whether the subGraphs containing the pair are similar to other sub -graphs considered in the considered relation. The authors claim that the inductive bias of their method, which is based on the fact that each graph pattern represents a specific logical rule, can generalize to unseen relation types and leads to more explainable predictive models. The method is applied to heterogeneous graph based recommendation as well as knowledge graph completion tasks with the state - of - the - art methods. It is shown to outperform existing models for both transductive and inductive settings. Extensive experimental comparisons on these benchmarks demonstrate the superiority of the method over other methods."
SP:26ed25a7b42da2cf11b76a727102d8aa36d76657,"This paper proposes to study few - shot learning in histology images by incorporating contrastive learning ( CL ) with latent augmentation ( LA ) to build a few - gains - gains system. CL learns useful representations without manual labels, while LA transfers semantic variations of the base dataset in an unsupervised way. The authors set up three cross - domain tasks that simulate real clinics problems :   ( 1 ) Learning few - shots in a histology dataset. The goal is to learn a model that generalizes better than supervised learning for such data. The main contribution of this paper is to propose a CL - based learning method that leverages both the advantages of CL and LA.   The main contributions of the paper are as follows : ( i ) The authors propose to incorporate CL and La augmentation into a few shots learning system. The idea is that CL can exploit unlabeled training data and can scale gracefully to other label - hungry problems. ( ii ) In experiments, the authors show that models learned by CL generalize better than those learned by LA. ( iii ) They analyze and provide empirical explanations for this observation, which they believe could contribute to understanding how model generalizes in the context of representation learning and histology image analysis."
SP:badbe687258cd5c282ca167b1f6fbfc6b5400dbf,"This paper studies the problem of learning long - term dependencies in irregularly - sampled time series using recurrent neural networks ( RNNs ). The authors propose a novel method, called mixed - memory - recurrent neural network ( MMRNN ), to encode a continuous - time dynamical flow within the RNN that allows it to respond to inputs arriving at arbitrary time - lags while ensuring a constant error propagation through the memory path.   The authors first theoretically prove that the class of ODE-RNNs suffers from the exploding and vanishing gradient problem, making them unable to learn long -term dependencies efficiently. Then, they show that learning learning ODE - RNN by the adjoint method ( Chen et al. 2018 ) does not help with this problem. As a solution, they propose a continuous time recurrent model, called continuous - recurrent MHRNNs, which is a RNN with a memory compartment separated from its timecontinuous state. This way, the memory can be used to encode continuous time - continuous flow within RNN, while the dynamical flows can be encoded in the continuous time RNN. Finally, the authors provide experimental results showing that the proposed MIRMNRNNs outperform the proposed RNN-based counterparts on non - sampled data."
SP:4efd22f9122fa5856a9f4302eb6875fa0c414912,"This paper presents a method for optimizing the binarized BERT model for natural language processing ( NLP ) tasks. The authors propose BiBERT, an accurate and efficient method to optimize BERT models that is based on minimizing the information degradation and the direction mismatch in the forward and backward propagation of information in BERT. To this end, the authors propose a bi - attention structure for maximizing representation information statistically and a DirectionMatching ( DMD ) scheme to optimize the optimization direction of the model.    The main contributions of the paper are as follows :   1. An analysis of the performance of BERT on the synthetic and real - world NLP tasks, showing that the performance drops significantly when the model is fully binarised. This paper is the first one of its kind to perform such an analysis. 2. An explanation for the performance drop is given for why BERT performs so poorly. 3. A solution is proposed to address this problem, which is a combination of using the DMD ( Direction Matching - based Optimization of Distillation ) scheme and the Bi - Attention Structure ( Bi - Attention ) to maximize the information information in the representation of the BERT embedding. 4. Experiments on synthetic NLP datasets show the effectiveness of the proposed method, showing significant FLOPs and model size savings."
SP:619bd742e92bea6241852f5a9d2b7bacf13b393a,"This paper proposes a new method to solve keypoint detection and instance association by using Transformer. Specifically, the authors argue that the naive attention patterns of Transformer are still not subjectively controlled, so there is no guarantee that the keypoints will always attend to the instances to which they belong. To address this issue, they propose a novel approach of supervising self - attention for multi - person key point detection and instances association, by using instance masks to supervise self - Attention to be instance - aware, and assign the detected keypoints to their instances based on the pairwise attention scores. The experiments on the COCO key - point detection challenge and person instance segmentation task demonstrate the effectiveness and simplicity of the proposed method, and show a promising way to control self -attention behavior for specific purposes. The results show that the method can achieve the expected instance-discriminative characteristics without affecting the standard forward propagation of the attention - based grouping algorithm."
SP:14750819593136fc9ef4efd032ab6f94dc5f6a02,"This paper studies reinforcement learning ( RL ) for sequential decision making under uncertainty in the setting of mean - variance ( MV ) trade - offs in reinforcement learning. It aims to obtain MV - efficient policies that achieve Pareto efficiency regarding the MV trade -off. To achieve this purpose, it trains an agent to maximize the expected quadratic utility function, in which the maximizer corresponds to the Pare to efficient policy. The authors formulate the problem setting in Section 3 and propose the main algorithms in Section 4. Then, they investigate the empirical effectiveness of their proposed methods in Section 6.    The main contributions of this paper are as follows :   1. A new method is proposed to obtain the expected variance estimate of the variance term of the optimal policy maximizer. This is done by training an agent with a policy that maximizes the expected utility function of the policy under the assumption that the variance is close to zero. The main idea is to train the agent with an agent that maximises the expected vector in the vector space, such that the policy with the maximized vector is the one that is closest to the optimal value maximizer, i.e., the policy that minimizes the vector with the lowest variance is equivalent to the one with the highest value maximiser. The paper proposes to train this agent with the following algorithm in Section 2. This algorithm is based on the following steps : 1. Train the agent by minimizing the expected value of the vector in $ \mathbb{R}$. 2. Train an agent by maximizing the vector $ \theta$. 3. Train another agent by maximising the vector by minimizing a lower bound on the variance of the lower bound of the upper bound. 4. Train two agents with the same policy in each of these three steps. The second agent is trained with the policy maximized by the second algorithm in each step. The third agent is the policy optimizer trained in the third step with the third algorithm in the fourth step. In Section 5, the authors conduct experiments to evaluate the effectiveness of the proposed methods."
SP:f675b564b3a9c8626ce7944d752fa3e0d868428e,"This paper proposes a method for adapting an autoencoder network without modifying the encoder and decoder neural networks, and adapting only the generative generative MDN channel model. The channel is generatively - modeled using a mixture density network ( MDN ). The authors propose a method that utilizes feature transformations at the decoder to compensate for changes in the channel distribution, and effectively present decoder samples close to the source distribution. Experimental evaluation on simulated datasets and real mmWave wireless channels demonstrate that the proposed method can adapt the MDN model using very limited number of samples, and improve or maintain the error rate of the auto - encoder under changing channel conditions.   The authors make the following contributions : 1 ) They propose a fast and sample - efficient method for adapt a generativeMDN ( used for modeling the channel ) based on the properties of Gaussian mixtures, and 2 ) Based on theMDN adaptation, the authors propose efficient input - transformation methods at   decoder that compensate    for changes   in the class - conditional channel distribution, and decrease   the   error rate."
SP:77dc92137ea490d3e1b4b8ee1630dbe2ee0bddfa,"This paper proposes a new approach for the natural language inference task of learning the most plausible explanation between the cause and the event in the αNLI task. The authors argue that the existing methods model the relation between each candidate hypothesis separately and penalize the inference network uniformly. They argue that it is unnecessary to distinguish the reasoning abilities among correct hypotheses and similarly, all wrong hypotheses contribute the same when explaining the reasons of the observations. Therefore, they propose to group instead of ranking the hypotheses and design a structural loss called “ joint softmax focal loss ” in this paper. Based on the observation that the hypotheses are generally semantically related, they have designed a novel interactive language model aiming at exploiting the rich interaction among competing hypotheses. The experimental results show that their IMSL method has achieved state - of - the - art results in ACC and AUC on both the validation set and test set. Impressive abductive reasoning performance is achieved when tested using RoBERTa as the pretrained language model. The best language model DeBERTa ( He et al., 2021 ) is not tested due to the constraint by our limited GPU resources ( 4 - piece RXT 2080Ti )."
SP:17cd72df5fc19398f582d27516fd742b073f79e3,"This paper proposes a method for out - of - distribution ( OOD ) detection based on deep neural networks ( DNNs ). OOD detection is a critical problem in machine learning as it is known to produce overconfident predictions on OOD data. The authors propose a method that combines a certifiable OOD detector with a standard classifier from first principles into an OOD aware classifier. This way they achieve the best of two worlds : certifiably adversarially robust detection, even for OOD samples close to the in - distribution distribution, without loss in either prediction accuracy or detection performance for non - manipulated OOD.   The main contributions of the paper are as follows :   1. Introducing a method to train a classifier that is certifiable on out of distribution data and a detector that is trained to be non - confident in detection of OODs. The detector is trained on the same set of data as the classifier, and the two are trained together. The classifier is trained in the same way as the detector, with the exception that the detector is only trained to detect OOD - sensitive data. This allows the detector to be trained with data that is not subject to adversarial manipulations. 2. The method is applied to two types of data : OOD and non - OOD datasets. The detection results are reported for both. 3. The main contribution of the method is the proposed method for classifier training. The proposed method is evaluated on two datasets, one for out-of - distribution detection and one for classification. The experimental results show that the proposed by the authors are better than those of the baselines."
SP:9c3756f13932236aff3e8104f4fa193dcc8fde2f,"This paper proposes a novel method called Image Classification Eraser ( ICE ) to erase classification information for any image encountered from arbitrary datasets. The authors claim that previous transfer attack papers assumed that the surrogate models of the attacker and the black - box victim models are trained on the same dataset, but this assumption is usually unrealistic as the attacker may not know the dataset used by the victim model and further, the attacker needs to attack any randomly encountered images that may not come from same dataset. To tackle this challenge, the authors propose a generalized attacker by a meta - learning framework, which builds a surrogate model for the attacker using a meta learning framework. Experiments on Cifar-10 ( Krizhevsky et al., 2009 ), CIFAR-100, and TieredImageNet ( Ren et al, 2018 ) demonstrate that the proposed ICE outperforms the modified transfer attack methods on the GTA problem. Furthermore, the proposed method can be modified to extend existing transfer attacks to tackle the GTA, but with significantly worse performance compared with ICE. In particular, given the source dataset CifAR-10 and the source models ResNet-18 and MobileNet-V1 trained on CfAr-10, the average attack success rate on the proposed attack, compared with existing attack methods, is about 17.0%."
SP:2e0447c741a3f09be1095633d870200355211260,"This paper studies the problem of false negative predictions in pre - trained discriminative language models ( PrLMs ). It is well - known that training language models on true negatives can be more efficient and robust than training on false positives, but it is surprising that this problem is kept out of the research scope of PrLM until this work to the best of the authors'knowledge. To address the issue of misconceived false negatives, the authors propose an enhanced pre - training approach to counteract misconceived negatives and fine - tune it on top of the ELECTRA architecture ( Clark et al., 2019 ). Experiments on GLUE and SQuAD benchmarks show that the proposed methods indeed bring about better performance together with stronger robustness to a great extent."
SP:281bc59d639aa76d84921b3ec4ce1ee8f1ba5b51,"This paper proposes ORCA, a novel open - world semi - supervised learning setting where unlabeled test data contains only classes previously encountered in labeled training data. The authors argue that this assumption rarely holds for data in the wild, where instances belonging to novel classes may appear at testing time. To tackle this challenging problem, ORCA proposes an end - to - end approach that assigns instances to previously seen classes or forms novel classes by grouping similar instances without assuming any prior knowledge. The key idea in ORCA is to utilize uncertainty adaptive margin to circumvent the bias towards seen classes caused by learning seen classes faster than the novel classes. Experiments on image classification datasets and a single - cell dataset demonstrate that ORCA consistently outperforms alternative baselines, achieving 25 % improvement on seen and 96% improvement on novel classes of the ImageNet dataset.   The goal is to solve the class distribution mismatch mismatch between labeled and unlabelled data, where at the test time every input instance either needs to be classified into one of the existing classes or a new unseen class needs to is initialized and the instance assigned to it. In this novel setting, there are no existing methods that can operate under the open world SSL setting. To address this challenge, the authors extend existing state - of - the - art SSL, open - set recognition and novel class discovery methods to the open - worlds SSL setting and then compare them to ORCA. The experimental results demonstrate that   ORCA effectively addresses the challenges of open-world SSL and consistently outperform all baselines by a large margin. Specifically, or ORCA achieves 25% and 96 % improvements on seen - and novel - class improvements on ImageNet and single cell datasets."
SP:6c572c4c21b01a0cf3fd9ef97fbb348ef4e405ae,"This paper proposes SLIM - QN, a second - order method for training large - scale deep neural networks ( DNNs ). The main idea is to use momentum in Hessian updates together with an adaptive damping mechanism to achieve stable convergence. The authors provide rigorous theoretical results on the convergence of the proposed method on the benchmark datasets ImageNet and ResNet-50 and compare it to SGD. They show that SLIM-QN achieves faster convergence than SGD and achieves better accuracy on ImageNet. They also evaluate the performance of their method on Transformers and Vision Transformer models."
SP:4bffce00ebb02d2e676eec897647ac14c3344deb,"This paper proposes Locality Sensitive Pruning ( LSP ) for graph pruning for graph - related tasks. LSP is based on the observation that large graphs often contain redundant components that can be removed without compromising the performance of graph neural networks ( GNNs ) without compromising their accuracy. The authors argue that sparsification of the input graph before pruning can lead to similar environments in the resulting sparsified graph that are similar to the environments of the original graph while dissimilar environments result in dissimilar outcomes in the sparsifying graph with high probability. To justify the application of LSP, the authors demonstrate the advantage of applying pruning based on locality sensitive hashing ( LSH ) over other pruning strategies in various scenarios on synthetic and real - world datasets. They argue that LSP removes a significant amount of edges from large graphs without compromising on the performance, accompanied by a considerable acceleration in the speed up of the algorithm."
SP:c5e024f4e2079586298519ca868630efd7579eca,"This paper proposes IDAA ( identity - disentangled adversarial augmentation ), a data augmentation method for contrastive self - supervised learning ( SSL ) whose goal is to distinguish positives from negatives without distorting the key information about the original identities of the samples. The paper proposes to decompose a sample x to be its variational auto - encoder ( VAE ) reconstruction G(x ) plus the residual R(x) = x, where R retains most identitydistinctive information due to an information - theoretic interpretation of the VAE objective. The authors then apply IDAA to different SSL methods, and show that IDAA consistently improves both efficiency and generalization performance on multiple benchmark datasets. They also show that the IDAA learned on a dataset can be transferred to other datasets and can be applied to a pretrained VAE model and does not require any labeled data."
SP:0991bc5f213bd8ab7572e2fed309e1b57a35835b,"This paper proposes a warning mechanism for detecting distribution shifts in training data distribution that would allow continuous monitoring of model performance without increasing the false alarm rate. The proposed method is based on two components : ( 1 ) a warning signal that detects distribution shifts while ignoring benign ones, and ( 2 ) a pre - trained model that tracks the difference between source ( training ) and target ( test ) distributions leading to a significant increase in a risk function of interest in a particular dataset. The main idea of the proposed system is to train a model on the data distribution and then use the warning signal to detect distribution shifts that are harmful to the training model while ignoring those that are benign to the test model.   The main contributions of the paper are as follows : ( a ) The proposed system has been validated on both simulated ( Section 3.1 ) and real data (Section 3.2 ), illustrating its promising empirical performance ; ( b ) the method is robust to traditional losses and generalizations of Brier score ( Brier, 1950 ) to multiclass classification ; and ( c ) it is possible to use the same number of labels for both training and pre - training."
SP:1c7b954273e3a9cda333385b15a3e8ed3bf8178a,"This paper presents an approach to reconstruct physical parameters from a single real - world pendulum motion image using a neural ODE - based appearance model. The physical parameters of the pendulum are estimated using an embedding of an underlying ODE based physical model, which allows for interpretability and long - term predictions. The embeddings of the ODE are represented by neural implicit representations, which are used to synthesize high - resolution and photo - realistic imagery. The authors also propose a per - scene model, where only a single short video clip that depicts the physical phenomenon is used to train the model. They show that their method is able to recover the metric length from the monocular video ( relative error to true length is less than 2.5 % )."
SP:51efd1451343f4994d857daa5490e299b812bc2d,"This paper proposes a new context - dependent reinforcement learning algorithm for the Markovian context evolution setting, which is characterized by an unknown finite number of not directly observable contexts and abrupt context changes during an episode. The authors argue that this setting is often met in applications and tackle it using a Bayesian approach and variational inference. They adapt a Hierarchical Dirichlet Process ( HDP ) prior for model learning, which they argue is arguably best - suited for Markov process modeling. They then derive a context distillation procedure, which identifies and removes spurious contexts in an unsupervised fashion. They argue that the combination of these two components allows to infer the number of contexts from data thus dealing with the context cardinality assumption. They show that the optimal policy depends on the context belief ( context posterior probability given past observations ) and derive another theoretical result, which shows performance improvement bounds for the fully observable context case. Equipped with these results, they experimentally demonstrate that they can infer the true context cardinalities from data. Further, they demonstrate that the context distillations procedure can be used during training as a regularizer and can also be used to merge similar contexts, where the measure of similarity is only implicitly defined through the learning loss. They also show that their algorithm appears to provide an optimization profile with fewer local maxima and minima than the state - of - the - art algorithms of different frameworks ( such as continual RL and Partially - Observable Markov Decision Processes ( POMDPs ) that fail to solve C - MDPs in their setting. They elaborate on the reasons for such failures and elaborate on potential reasons why this is the case."
SP:ea167b126212b2092bc1190d7f8376bf7c54a888,"This paper presents a framework to train knowledge based multilingual language models ( KMLMs ). The authors first generate a large amount of code -switched synthetic sentences and reasoning - based training data using the Wikidata knowledge graphs. Then based on the intra - and inter - sentence structures of the generated data, they design pretraining tasks to facilitate knowledge learning, which allows the language models to not only memorize the factual knowledge but also learn useful logical patterns. The proposed framework is evaluated on a wide range of knowledge - intensive cross - lingual NLP tasks, including named entity recognition, factual knowledge retrieval, relation classification, and logic reasoning. The novel logic reasoning task is designed by us to test the reasoning capability of the models. Our KMLM achieves consistent and significant improvements on all knowledge -intensive tasks, meanwhile it does not sacrifice the performance on general NLP benchmarks."
SP:6c11cf29c90f923346372ba6f11452c36e69ad6d,"This paper proposes an approach to train agents to behave altruistically towards others by rewarding them for benefiting other agents in a task - agnostic manner. The authors assume that other agents ’ goals are known so that the altruistic agent can cooperate in achieving those goals. They show that explicit knowledge of other agents’ goals is often difficult to acquire in the case of human agents, their goals and preferences may be difficult to express fully, may be ambiguous or even contradictory. Thus, it is beneficial to develop agents that do not depend on external supervision and can learn altruistic behaviour in an unbiased manner. They evaluate their approach on three different multi - agent environments where another agent’s success depends on the altruistically agent ’s behaviour. Finally, they show that their unsupervised agents can perform comparably to agents explicitly trained to work cooperatively, in some cases even outperforming them."
SP:5dbc54201ba184266c5054f0d2944bd197bc307a,"This paper studies the phenomenon of Hessian double descent in neural networks. Double descent is a phenomenon that occurs when the Hessian of a parametric estimator with maximum - likelihood - type loss behaves differently from that of a linear or kernel regression model. The authors argue that the existing theoretical understanding of the phenomenon is primarily based on linear and kernel regression models — with informal parallels to neural networks via the Neural Tangent Kernel — and that these analyses do not adequately capture the mechanisms behind the phenomenon. The main contribution of this paper is to propose a new analysis based on leveraging of the population loss and its lower bound, while imposing minimal assumptions on the form of the parametric model. This approach allows the authors to understand the effect of the loss functions used to train neural networks — which, as they will see, clearly influences the nature of double descent — and reveals novel insights into the properties of neural networks near the interpolation threshold.   The main contributions of the paper are as follows : ( 1 ) The authors propose a novel and theoretically sound analysis of the double descent phenomenon in finite - width neural networks and their Hessian. ( 2 ) They show that the lower bound of their derived bounds bear an intimate connection with the spectrum of the optimum Hessian at the optimum, and importantly, exhibit a double descent behaviour at the $ \epsilon$ threshold. ( 3 ) They further investigate how the loss function affects double descent."
SP:b485114712055f39a7afb951dbc3db482ff523fd,"Deep Graph Neural Tangent Kernel ( GNTK ) is a well - known graph neural network ( GCN ) formulation that allows for expressivity and trainability. However, it is well known that deep GCNs suffer from the over - smoothing problem, where node representations tend to be indistinguishable as more layers are stacked up. This paper proposes a method, called Critical DropEdge, to overcome the exponential decay of trainability in GCNs. The proposed method is a graph - adaptive and connectivity - aware method that is easy to implement in both finite - width and infinite - width GNNs. Experiments show that using the proposed method can outperform counterparts in the large depth setting.    The main contributions of this paper are as follows :   1. A theoretical analysis of the dynamics of the graph neural tangent kernel under gradient descent for the optimization of deep GCN. Theoretically, the authors identify the asymptotic behavior of GNTk in the deep GCNNs and formulate a theoretical framework that enables them to reveal the dropping trainability of wide GCN at a given depth. 2. An empirical study is conducted to validate the effectiveness of the method."
SP:25a92b3583afdc6892e59f1e769125d52c8011af,"This paper studies the problem of estimating the second derivative of the first - order dynamics of the heart rate in video - based medical data. This work is motivated by the fact that most of the previous work in this field has focused on extracting statistics for the first order dynamics, while less emphasis has been put on the accuracy of waveform morphology, which is necessary for many clinically impactful scenarios, such as the estimation of the second - order pulse dynamics in the case of blood pressure and arterial disease. The authors propose a method to incorporate both the second and third - order derivatives of the data into the loss function of the neural networks in order to better estimate the second order dynamics. The method is based on the observation that in many cases the properties of interest are subtle variations in higher - order changes such as acceleration, which can be better estimated by neural networks. This is true in the cardiac pulse case, where the second second derivative can be used as an indicator of blood pressures and disease.    The authors provide three main contributions : ( 1 ) They provide evidence that higher - ordered dynamics are better estimated in neural networks when explicitly optimized for in the loss functions. ( 2 ) They show that adding second - derivative inputs additionally improves performance, and 3 ) They describe a novel deep learning architecture that incorporates second derivative input frames and target signals and evaluate it against clinical - grade contact sensor measurements."
SP:0a88d2fcbdfab3e196bf6b9c75adb1006ab87536,"This paper proposes a method to train agents to communicate in a symmetric and compositional language in complex settings like dialog games. Inspired by the theory that language may evolve from simple tasks to difficult tasks, the authors propose a novel architecture called symbolic mapping as a basic component of the communication system of agent learning. They find that symbolic mapping learned in simple referential games can notably promote language learning in difficult tasks. Further, they explore vocabulary expansion, and show that with the help of symbolic mapping, agents can easily learn to use new symbols when the environment becomes more complex. All in all, they probe into how symbolic mapping helps language learning and find that a process from simplicity to complexity can serve as a natural way to help multi - agent language learning."
SP:89575be04cb33b41d7a0a7b62f9496c2838a1317,"This paper proposes a hierarchical approach to learn an agent that navigates and interacts with an environment by using three levels of hierarchical learning. The first level is composed of a navigation policy, a interaction policy, and a manipulation policy. The navigation policy is conditioned on the language instructions of the high - level policy composition controller ( PCC ). The interaction policy alternates between the navigation policy and the interaction policy with the corresponding object masks. The goal of the paper is to learn a human - interpretable sequence of sub - goals that can be executed by the agent based on language instructions given by the PCC. The proposed approach achieves state - of - the - art performance on the ALFRED benchmark.    The main contributions of this paper are as follows :   1. A hierarchical approach for learning an agent ’s navigation policy. This approach is based on a three - level hierarchical approach. The policy at the highest level, PCC, first computes a sequence of language instructions to guide the agent to navigate the environment. The lower level, interaction policy and navigation policy are then conditioned on these instructions. The upper level, manipulation policy and object mask policy are used to interact with the environment and the corresponding objects. The approach is applied to three different environments, two of which are synthetic environments and two are natural ones. The results show that the proposed approach is able to learn the navigation and interaction policies more efficiently than competing approaches. 2. A method for learning the interaction policies. This method is called HACR ( Hierarchical Approach for Compositional Reasoning ). It consists of three steps. First, it computes the sequence of actions that the agent should take when interacting with a given environment. This sequence is divided into subgoals. The subgoal subgoal corresponds to the action that should be taken by the corresponding interaction policy. Next, the subgoal is combined with the action taken by an interaction policy to obtain the final subgoal. This process is repeated until the environment is fully explored. This procedure is referred to as “ compositional reasoning ” in the paper. 3. Finally, the agent is trained to use the corresponding subgoal actions with the appropriate interaction policy in order to achieve desired results."
SP:e2c8efe00db7baba2368f4f6a37815809b9e235e,"The paper considers the problem of classifying animals in MNIST images under the nuisance - label relationship, where the background of the image is the nuisance and the label is correlated with the covariates. The paper proposes a method, called Nuisance - Randomized Distillation ( NURD ), to learn a predictive model that works well regardless of the relationship between the background and covariates in the family of distributions that are known as the "" nuisance - varying family "". The family consists of three distributions : ( 1 ) a distribution where the label and the nuisance are independent of each other, ( 2 ) an uncorrelating distribution, and ( 3 ) a set of representations of covariates that are conditioned on the distribution under which the nuisance is independent of the label. The main contributions of the paper are the following :   1 ) The paper defines the family as distributions that differ only in the nuisance-label relationship, and proposes a way to train a model that does well on every member of the family regardless of this relationship in that member.   2 ) In section 2, the paper introduces the idea of nuisance - randomization, which is a form of conditioning on the covariate distribution that is obtained by randomizing the distribution of the generative models that are trained to produce nuisance - randomlyized data. 3 ) The second step is distillation, which maximizes the information a representation has with the label on the nusiance - randomized data over the set of uncorrelated representations. The authors prove that the representation achieves the highest performance within the set on every distribution in the model is trained on. 4 ) The authors conduct experiments on two tasks : detecting waterbirds and chest X - ray classification. In the detecting waterbird task, using non - lung patches as the nuisance as the background, the authors show that under strong spurious correlations the proposed model performs poorly compared to the one trained under a different nuisance distribution."
SP:c75998b76f4e0510fc719d25959a10fc07db1c40,"This paper presents a zero - shot classification method, OTTER ( Optimal TransporT distillation for Efficient Zero - shot Recognition ), which uses online entropic optimal transport to find a soft image - text match as labels for contrastive learning. Previous works, such as CLIP, use InfoNCE loss to train a model to predict the pairing between images and text captions, but this method is data hungry and requires more than 400 M image-text pairs for training. The inefficiency can be partially attributed to the fact that the image -text pairs are noisy. To address this, this paper proposes OTTER, which is based on a method to train an encoder - decoder model with a pre - trained pair of text encoders and an image encoder. OTTER outperforms ( 35 ) or ties ( 2 ) all baselines in 37 of the 37 datasets. The authors also propose a quantitative vision - language compositionality benchmark and show comparable results to CLIP."
SP:e83cd70377542b5d187998e2e4a7ac070f453ed6,"This paper presents Pix2Seq, a method for object detection based on language modeling. The authors cast object detection as a language modeling task conditioned on the observed pixel inputs. Object descriptions ( e.g. bounding boxes and class labels ) are expressed as sequences of discrete tokens, and they train a neural net to perceive the image and generate the desired sequence. The approach is based mainly on the intuition that if a neural network knows about where and what the objects are, we just need to teach it how to read them out. Beyond the use of task - specific data augmentations, the approach achieves competitive results on the challenging COCO dataset, compared to highly specialized and well optimized object detection algorithms."
SP:abc9315f61929cc1c54dfef8ff83d7eac56ec2f2,"This paper presents a method for distilling the learned policy from the CNN policy network into the interpretable symbolic policy, which is composed from geometric and numerical symbols and operators and a policy regression algorithm called RoundTourMix. The symbolic policy can be treated as discrete and abstracted representations of the policy network, but are found to be more interpretable, robust and transferable. The proposed symbolic distillation approach is experimentally demonstrated to maintain the performance and “ denoise ” the policy on six specific environments, and the distilled symbolic policy achieves comparable or higher scores with interpretable actions while generalizing better on the new environments than their teacher policy networks.   The main contributions of the paper are as follows :   1. The paper proposes an end - to - end learning pipeline that follows a stage - wise approach that features hierarchical reasoning. Specifically, the approach progressively converts a policy network   into the interpreted symbolic policy via a distillation method. The distillation algorithm is proposed to distill the symbolic rules as teacher - student, where the teacher learns to draw auxiliary lines and measure geometric relations and the student learns to condition action on distance 1. 2. The method is evaluated on seven challenging visual RL environments and compared with the state - of - the - art methods. The results show that the proposed distillation methods achieve comparable or even stronger effectiveness in seven highly challenging visual environments."
SP:04e7e181aeb1244ae1c4837ad416aef93ea3ea32,"This paper proposes a new model for image - to - image translation, named PIVQGAN, based on StyleGAN2. The main idea is to learn the coarse - level object arrangements ( posture ) and the fine - grained level styling ( identity ) of the generated image from two exemplar sources. To achieve this goal, the authors propose a joint - training scheme with self - supervision methods for the GANInversion encoder and the generator. The encoder learns to encode the shaping and composition information from the commonly shared objects inside the training - set images. The generator automatically learns the pose - related representations. The resulting model, which the authors call VQSN, outperforms existing baselines in terms of visual quality and translation accuracy. The experiments conducted on various datasets show better synthesis image quality and disentangling scores of the model. Moreover, the paper present model applications beyond pose - identity translation beyond image translation thanks to the latent - space reducing feature of the leveraged VQ SNS module."
SP:e51a7f45493064972585109f203a867e9828eb15,"This paper proposes a simple yet effective neural model for extracting information from speech signals, based on MLP architecture. The model splits feature channels into non - overlapped chunks and processes each chunk individually. These chunks are then merged together and further processed to consolidate the output. By setting different numbers of chunks and focusing on different window sizes, speech - MLP learns multiscale local temporal dependency. The proposed model is successfully evaluated on two tasks : keyword spotting and speech enhancement. On the KWS and SE tasks, it demonstrated that the simple model can achieve performance comparable to or better than transformers with less parameters and inference time. Such results indicate that more complex models, such as transformers, are oftentimes not necessary for speech processing tasks, the authors argue."
SP:d708d3886f4abd4552d8ccb2096df7361c803b13,"This paper studies the problem of reducing the amount of labeled data required for transfer learning in binary classification problems by deriving a novel lower bound on the generalization error that can be achieved by any transfer learning algorithm ( i.e., a generalization algorithm that utilizes the data of a related but different source task to compensate for the lack of data in a target task where there are few labeled training data ).   While there has been many recent algorithmic advances in this domain, a fundamental understanding of when and how much one can transfer knowledge from a related source to reduce training data is far from understood. The main contribution of this paper is to provide a precise answer to this question for binary classification problem. The authors also consider a more general setting where there is more than one source domain for knowledge transfer to the target task and develop new bounds on generalisation error in this setting to establish a new upper bound. The lower bound is based on a notion of distance that is indicative of the difficulty of knowledge transfer between different pairs of source/target tasks, and it applies to any arbitrary source / target data distributions and requires minimal assumptions that enables it application to a broad range of problems. The paper also provides a lower bound that is comparable to the upper bounds achieved by transfer learning base - lines that utilize weighted empirical risk minimization on the combination of source(s ) and target data sets."
SP:f7511ba9ccad03233b34b1bf41bbac7361d20a57,"This paper proposes a probabilistic method to complete the geometry of large - scale 3D scenes. The proposed method is based on the Generative Cellular Automata ( GCA ), a generative model that learns the multi - modal distribution and transform the formulation to process large scale continuous geometry. The continuous geometry is incrementally generated as a sparse voxel embedding, which contains the latent code for each occupied cell. The authors derive that the training objective for the sparse embedding maximizes the variational lower bound of the complete shape distribution and therefore their progressive generation constitutes a valid generative models. They demonstrate that cGCA can faithfully generate multiple plausible solutions of shape completion on input scans with a significant amount of missing data as shown in Figure 1. They also demonstrate that their approach outperforms other deterministic models even in less ambiguous cases."
SP:d22d8f074adbe8fb0f25fb8f8d96201b3159bf6b,"This paper proposes temporal priors as a non - Markovian generalization of behavioral priors that can be used for exploration in reinforcement learning. The authors focus on state - independent priors, which exploit the idea of temporal consistency and are generally applicable and capable of transferring across a wide range of tasks. They show how dynamically sampling actions from a probabilistic mixture of policy and temporal prior can accelerate off - policy reinforcement learning in unseen downstream tasks, and provide empirical evidence that their approach improves upon strong baselines in long - horizon continuous control tasks under sparse reward settings. They also introduce a setting in Section 3 that is described in Section 4, while while while describing the method in Section 5."
SP:25e06c022ae8b3cbbb8db413d7b534a1a5c92391,"This paper proposes a new method for learning rate scheduling in deep neural networks, based on Graph - Network - based Scheduler ( GNS ). The method is based on a directed graph message passing network, which is used to represent the current dynamics of the learning rate scheduler and to train an agent to control the rate via reinforcement learning. The authors compare the proposed method with two existing methods, namely GNS and GTS, and show that GNS outperforms both of them in terms of accuracy and generalization to different datasets and network structures.   The main contributions of the paper are as follows :   ( 1 ) A new method based on graph message - passing network to represent learning rate schedule ; ( 2 ) An efficient reward collection procedure is designed to speed up the training of the agent ; ( 3 ) The proposed method is able to generalize to problems of different datasets ( from CIFAR10 to CIFar100 ) and model structures ( from ResNet to VGG and from RoBERTa - base to RoBERTA - large )."
SP:d73cb0471c1770607ad3e4621cfc5f170683dd8e,"This paper tackles the problem of deep object - centric learning from a point cloud, which is crucial for high - level relational reasoning and scalable machine intelligence. The authors introduce a framework, SPAIR3D, to factorize a 3D point cloud into a spatial mixture model where each component corresponds to one object. They derive the Chamfer Mixture Loss, which fits naturally into their variational training pipeline, and adopt an object - specification scheme that describes each object ’s location relative to its local voxel grid cell. The experimental results demonstrate that the proposed method has strong scalability and is capable of detecting and segmenting an unknown number of objects."
SP:3c57e921c1bf23e482551ceb71702931a7f07439,"This paper investigates the possibility of grounding high - level tasks, expressed in natural language, to a chosen set of actionable steps. While prior work focused on learning from explicit step - by - step examples of how to act, the authors surprisingly find that if pre - trained LMs are large enough and prompted appropriately, they can effectively decompose high -level tasks into low - level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions, so the authors propose a procedure that conditions on existing demonstrations and semantically translates the plans to Admissible actions. The authors conduct a human evaluation of multiple techniques and models and report on the tradeoffs between executability and semantic correctness."
SP:e0159d1c9df2e657892a3a0c77549df4698d9a1a,"This paper proposes a new interpretation of Variational Autoencoders ( VAEs ) based on the fact that VAEs naturally reveal a Riemannian structure of the learned latent space. They show that using these geometrical considerations can significantly improve the generation from the vanilla VAE which can now compete with more advanced VAE models on four benchmark datasets. In particular, they propose a new way to generate samples consisting in sampling from the uniform distribution deriving intrinsically from the manifold learned by a VAE. They also stress the proposed method’s robustness in the low data regime which is known as very challenging for deep generative models. Finally, they validate the method on a complex neuroimaging dataset combining both high dimensional data and low sample sizes."
SP:b4b8e1727f8617894f10f20365cb68de79f0e650,"This paper presents Transformer - MGK, a novel architecture that replaces redundant heads in transformers with a mixture of keys at each head. The key distribution is based on a Gaussian mixture model. Compared to its conventional transformer counterpart, the proposed model accelerates training and inference, has fewer parameters, and requires less FLOPs to compute while achieving comparable or better accuracy across tasks.   The paper presents a novel model that replaces the keys in the heads of a Transformer with a transformer with a Mixture of Gaussian Keys ( transformer - gmk ). The main idea is that for each token, self - attention computes a weighted average of the feature representations of other tokens where the weight is proportional to a similarity score between each pair of tokens. This mechanism allows a token to pay attention to other tokens in the sequence and attain a contextual representation. It has been shown that the learned knowledge from a pre - trained model can be transferred to different data modalities and has limited supervision."
SP:82731dcce233e748f63382e09b6224a513fe9689,"This paper presents a novel approach to the problem of spatial navigation in a two - dimensional continuous environment. The authors propose to use a direct - inverse model of the environment dynamics to fuse image and action related signals, allowing reconstruction of the action relating the two successive images, as well as prediction of the new image from its current value and the action. They also propose a minimalistic recurrent architecture, called Resetting Path Integrator ( RPI ), that can be trained to keep track of its position relative to its starting point during a sequence of movements. RPI updates its internal state using the ( possibly noisy ) self - motion signal and the possibly noisy ( possibly non - noisy ) motion - related signal and it when the image signal is present. The internal state of this minimal model exhibits strong correlation with position in the environment due to the direct - inverse models, is stable across long trajectories through resetting, and allows for disambiguation of visually confusing positions through integration of past movement. The architecture is compared to off - the - shelf LSTM networks on identical tasks and consistently shows better performance while also offering interpretable internal dynamics and higher - quality representations."
SP:1a27c397d1e73def5e724c5c6f25548975ba50fa,"This paper studies the problem of learning representations of the input data with effective features for prediction, which is believed to be a key factor to their superior empirical performance. The authors consider learning problems motivated by practical data, where the labels are determined by a set of class relevant patterns and the inputs are generated from these along with some background patterns. They prove that neural networks trained by gradient descent can succeed on these problems. The success relies on the emergence and improvement of effective features, which are learned among exponentially many candidates efficiently by exploiting the data. In contrast, no linear models on data - independent features of polynomial sizes can learn to as good errors. Furthermore, if the specific input structure is removed, then no polynomials algorithm in the Statistical Query model can learn even weakly. These results provide theoretical evidence showing that feature learning in neural networks depends strongly on the input structure and leads to the superior performance."
SP:8ada73ed7eade9ebdeef376485e849c42575bc5f,"This paper studies the problem of understanding the robustness of machine learning models in the presence of adversarial examples generated by test - time adversaries. The main contribution of this paper is to develop a methodology to analyze the robusts of fixed feature extractors, which in turn provides bounds on the robustnesses of any classifier trained on top of it. The method relies on the effectiveness of the method used to find collisions between pairs of perturbed examples at deeper layers of the feature extractor, which is defined as the ratio of the number of times a pair of classes interact with each other in the input space when perturbed. The lower the density of the conflict graph, the tighter the lower bound of the bound on the loss of the classifier.   The main contributions of the paper are as follows :   ( 1 ) The authors develop a method to identify the layers of robustly trained models that contribute the most to a lack of robustness, as well as compare the same layer across different training methods to provide a quantitative comparison of their relative robustness ; ( 2 ) They develop a solution based on the iterative solution of a convex program that proposes a bespoke algorithm to find the most robust layer in a given training set. This solution is applied to both linear and non - linear features extractors. In particular, for linear features, the authors propose a closed - form expression for collision finding while for arbitrary features, they propose an algorithm based on a bsugaru algorithm. The authors also provide a set of open questions that can be asked to further improve the analysis of the bounds."
SP:874b5fa51924cbcceed490d98a0ea80f74586b32,"This paper presents a novel offline reinforcement learning method, Value - based Episodic Memory ( VEM ), which learns the V - function instead of the Q - function to naturally keep the learning procedure within the offline dataset. The authors propose Expectile V - learning ( EVL ), a method which smoothly interpolates between the value learning and behavior cloning and introduces implicit planning along offline trajectories to enhance learned V - values and accelerate convergence. They provide theoretical analysis for the convergence properties of their proposed VEM method, and empirical results in the D4RL benchmark show that VEM achieves superior performance compared to other baselines."
SP:34f08d92681504490c2f739b0d08f79f9764b2f5,"This paper proposes a method for training neural network classifiers to defend against adversarial perturbations through adversarial training, while balancing the trade - off between robust accuracy and standard accuracy. The method proposes to reweight the loss associated with individual training samples based on a notion of class - conditioned margin, with the goal of improving robust generalization. Inspired by MAML - based approaches, the authors formulate weighted adversarial learning as a bilevel optimization problem where the upper - level task corresponds to learning a robust classifier, and the lower - level is learning a parametric function that maps from a sample ’s multi - class margin to an importance weight. Extensive experiments demonstrate that their approach improves both clean and robust accuracy compared to related techniques and state - of - the - art techniques."
SP:3ad36be6b6900aabe43da043461cf178ce977082,"This paper proposes a method to incorporate geometric and physical information in both the message and update functions of the Steerable Equivariant Graph Neural Networks ( SEGNNs ), a generalisation of equivariant graph neural networks ( EGNs ). Specifically, the authors introduce a new class of activation functions for general use with steerable feature fields in the form of steerable MLPs. The main contribution of the paper is the introduction of the steerable node attributes, which are defined using the spherical harmonic embedding ( Eq. 4 ) of relative positions and can additionally include node force, spin or velocities, as the authors do in the N - body experiment. Experiments are conducted on two datasets : 1 ) non - linear message aggregation, where it is shown that the proposed method improves upon classic linear ( linear ) message aggregation ; and 2 ) the effectiveness of the proposed methods on the equivariance based graph neural network ( GENN ) task.   The main contributions of this paper are as follows :   1 ) Introducing a new type of MLP, called steerable E(3 ) MLPs, which generalise equivariants in graph networks, such that node and edge attributes are not restricted to invariant scalars, but can contain covariant information, such as vectors or tensors. This model is composed of two components : a message encoder and an encoder. The encoder encoder first encodes the message as a vector, and the decoder is a linear combination of the vector and the edge embedding. The decoder then encoder as a tensor encoder of the embedding of the edge and the node. The authors claim that the more geometric information is incorporated in the encoder, the better the performance of the model improves. 2 ) Experiments on the geomagnetic and physical properties of the node attributes are conducted to show that the effectiveness and efficiency of the approach is enhanced."
SP:8928aa83f7ebd4e310f4fe1d01ff0eb0c96e4d2b,"This paper proposes a differentiable physics model for differentiable materials based on differentiable fabrics. The main contribution is to develop a physics model that can be used to learn dynamics, solve inverse problems and design new materials. The method is based on three main components : ( 1 ) Differentiable materials, ( 2 ) differentiable structures and forces, and ( 3 ) yarn - to - yarn interactions. The model is trained using Bayesian optimization on inverse problems, and is compared with Bayesian Optimization of Inverse Problems ( BOI ) and Reinforcement Learning ( RL ). The results show that the proposed method is more accurate than BOI and RL on the inverse problem and better than Reinforcement learning on the control learning task. On the data efficiency side, the authors show that their model generates more accurate predictions and faster in control learning compared to BOI."
SP:2c8358c095b10981d3015b9f6c75765419a9480d,"This paper proposes a method for transfer learning in reinforcement learning ( RL ), where an agent is given a set of base tasks and a new set of tasks from an unknown distribution, and must decide whether to learn a task - specific skill or to use existing skills to solve the new task, or to combine existing skills and learn a new skill based on the existing task. The method is based on two main theoretical results : 1 ) bounds on the number of tasks that need to be learned throughout an agent’s lifetime to generalise over a distribution 2 ) bounds the performance of the transferred policy on a new task when applied to the new tasks.   The main contribution of the paper is the following :   1 ) It gives bounds on how many new tasks an agent needs to learn to generalize over the distribution of the base tasks ; and 2 ) it provides a way for the agent to generate an estimate of the optimal transfer learning policy for each new task by using the learned base tasks as a starting point. The paper empirically shows that the proposed method can be applied to transfer learning both after learning base tasks, and after learning an arbitrary set of new tasks from the unknown distribution ; and 3 ) it is able to achieve good performance on new tasks before training even starts. In addition, the paper also shows that it is possible to apply the method to learning new tasks when the distribution is unknown."
SP:c85d71d05164d019cc32bf423e4c4fe20c169f41,"This paper presents a distributed solution for the multivariate time series classification ( MTSC ) problem based on a 2.5 % version of the popular ROCKET method. The main contributions of the paper are as follows :   1. An analysis of the theoretical background of ROCKET and the limitations of its features ; 2. A discussion of the advantages and limitations of using such a large number of features in ROCKET ; 3. A practical solution based on LightWaveS, which is fast both during training and inference.   The paper presents three versions of the algorithm and their results on training time, accuracy, inference speedup and scalability. The method is shown to achieve speedup ranging from 9x to 65x compared to ROCKET during inference during inference on an edge device, on datasets with comparable accuracy, and a reduction in the number of channels in the network."
SP:db43614ca016280a79448f44a97c81c8ff5ba981,"This paper presents a method for training adversarial text encoders based on auxiliary masked language models ( MLMs ). The main idea is to jointly train multiple MLMs of different sizes to provide training signals at various levels of difficulty. Different from ELECTRA which trains one MLM as the generator, this paper jointly trains multiple MLM of different size and different difficulty levels. The authors propose to learn mixture weights over the auxiliary MLMs’ outputs to maximize the discriminator loss by backpropagating the gradient from the discriminators via Gumbel - Softmax. The experimental results show that the proposed method works better than the baselines tested by the authors."
SP:db3825633ab5d0671340390b23ab655838cc38b2,"This paper proposes an adaptive fine - tuning approach to fine - tune language models for relational fact extraction from large pre - trained language models. The paper proposes to train language models on a small training dataset of existing facts from a knowledge graph, while training on a restricted set of relations to show that even using fewer training relations are needed to achieve high knowledge extraction quality. The authors also analyze the transfer learning capabilities of this adapted language model by training it on a limited set of relation and show that some relations profit from the training of semantically related relationships.   The paper is well - written and well - motivated. However, there are a few major flaws in the paper. First, the authors fail to clearly state the motivation for the proposed approach. It is not clear what the motivation is behind the proposed method. It seems to me that the motivation comes from the fact that language models can be thought of as knowledge graphs and the task of extracting relational facts from them is similar to extracting facts from graph knowledge. Second, there is a lack of discussion about the differences between the methods used to train the language models in this paper and the ones used in prior work. This is not surprising since the paper does not clearly state which of the two methods are superior in terms of the amount of training data and the quality of the learned relations. Third, the paper also fails to mention that there is no clear explanation of the difference between the techniques used in the previous work and this one."
SP:ae25d32714b2b9f7e02cc20f4a36252e20e78e4f,"This paper presents a knowledge graph based learning framework for understanding the properties of entities in the knowledge graph. The authors propose to learn the knowledge base embeddings in different geometric spaces and apply manifold alignment to align the shared entities. The hyperbolic space characterizes the transitivity naturally because of its tree - like properties while the Euclidean space reveals its weakness for other relations.   The authors evaluate the proposed learning framework on the out - of - taxonomy entity typing task, where they aim to predict the types of the entities from the Knowledge graph. Experimental results on two datasets based on YAGO3 demonstrate that the proposed approach has significantly good performances."
SP:9ab3bc525ee4a9c96518c43e4c43082655a7674f,"This paper proposes a low - shot learning framework for link prediction in temporal knowledge graphs. The authors propose a self - attention mechanism to effectively encode temporal interactions between entities, and a network to compute a similarity score between a given query and a ( one - shot ) example. They also propose a temporal neighborhood encoder with self -attention mechanism that effectively extracts the temporally -resolved neighborhood information for each entity. They conduct experiments on two real - world datasets and demonstrate the superiority of the proposed model over state - of - the - art baselines on both datasets. They construct two new publicly - available benchmarks for one - shots learning over TKGs."
SP:91f92a40e12afd0702f07ae7f4175ecce57b7007,"This paper proposes a neural model for learning visual reasoning tasks. The main idea is to represent a solver for each task as a neural module that calls existing modules ( solvers for simpler tasks ) in a functional program - like manner. Lower modules are a black box to the calling module, and communicate only via a query and an output via a text - to - text exchange. The authors propose a module for each new task that combines existing modules and composes their outputs in order to produce its own output. The proposed model effectively combines previous skill - sets, does not suffer from forgetting, and is fully differentiable.   The authors further analyze the interpretability of PMN ’s reasoning process with human judges. They show that PMN outperform baselines without module composition on all tasks, and demonstrate improved performances in all tasks by learning progressively by learning on top of previously acquired knowledge."
SP:de33b02e7f2faec5bcae9a5516721aa1ef190572,"This paper presents SCU, a novel method for improving the parameter efficiency of deep convolutional neural networks ( CNNs ). SCU is based on the idea that the information - preserving nature of identity connection can preserve the channel - selectivity in CNNs. In particular, SCU gradually learns the channel selectivity via pruning unimportant channels, and rewiring the pruned parameters to important channels. Theoretical results show that SCU generally achieves both model compression and accuracy improvement compared to the baselines, consistently for all tested architectures.    The main contribution of this paper is the proposed SCU that improves parameter efficiency and channel - selectsivity of various modern CNNs with bottlenecks. The experimental results demonstrate that the SCU - based models without any postprocessing generally achieve the desired state - of - the - art results."
SP:2d80fa4bc440061be2234b5070503d3fa056baed,"This paper considers the problem of learning a binary classifier only from positive data and unlabeled data ( PU learning ). The authors propose a method to partially identify the classifier. The proposed algorithm learns a scoring function that preserves the order induced by the class posterior under mild assumptions, which can be used as a classifier by setting an appropriate threshold. Through experiments, the method outperforms previous methods for PU learning on various real - world datasets. The experimental results show that the proposed method is appropriate for real world applications compared to existing approaches for PU - learning."
SP:5f312626b0613d2e07c59214c5f00db208a98717,"This paper proposes a new approach to measure the similarity between the gradients of the main task and auxiliary tasks in neural networks. The main idea is to use the cosine similarity between gradients as an adaptive weight to detect when an auxiliary loss is helpful to the main loss and when it could be hurting. The proposed approach is based on a simple yet effective approach for learning. The authors propose to use multi - task supervised learning on subsets of ImageNet, gridworld, and reinforcement learning on Atari games. They show that their approach is guaranteed to converge to critical points in the main tasks and demonstrate the practical usefulness of the proposed algorithm in a few domains.   The main contributions of the paper are as follows :   1. An adaptive weighting approach for measuring the similarity of auxiliary losses between the main and auxiliary losses in neural network training. This is a simple but effective way to detect the difference between auxiliary losses and the main losses in training. 2. A method for learning auxiliary losses is proposed to learn auxiliary losses. 3. The method is tested on the gridworld task and the reinforcement learning task."
SP:e270ae3eeb7ab4fa91ba37d4d68ce10f2fa0a3b5,"This paper proposes a geometric framework to analyze the geometry of high - dimensional geometry of adversarial examples in order to better understand the phenomenon of misclassification in machine learning models. The main contribution of the paper is to propose a geometric analysis of the high - dimensionality of the data manifold in which adversarial example are constructed. The geometric analysis is based on the manifold reconstruction literature and is motivated by the observation that for low - dimensional data embedded in high dimensional space, there are many directions off the manifold off which it is not possible to construct robust examples. This is a natural consequence of learning a decision boundary that classifies the low -dimensional data manifold well, but classifies points near the manifold incorrectly.    The main contributions of this paper are three - fold :   1. The analysis highlights the importance of codimension in the classification algorithms for low dimensional data on high dimensional data manifolds. 2. It shows that different classification algorithms are less sensitive to changes in the direction of the boundary in which the data is located. 3. It provides experimental evidence on synthetic datasets and MNIST that support the theoretical results. In particular, it shows that adversarial training in balls around the training data is sample inefficient."
SP:e07d948a79d478ecd23a0a4406d4ddd3ac5e3be3,"This paper proposes a new representation learning method for high - dimensional time series data. The authors claim that most representation learning algorithms for time - series data are difficult to interpret due to non - intuitive mappings from data features to salient properties of the representation and non - smoothness over time. To address this problem, they propose a gradient - based version of the traditional self - organizing map algorithm that is more performant than the original. Furthermore, they integrate a Markov model uncovers the temporal transition structure, which improves clustering performance even further and provides additional explanatory insights as well as a natural representation of uncertainty.   The authors evaluate their model on static MNIST data, a time series of linearly interpolated ( Fashion - MNIST images ), a chaotic Lorenz attractor system with two macro states, and a challenging real world medical time series application on the eICU data set. They compare favorably with competitor methods and facilitate downstream tasks on the real world data."
SP:5915ee71ea58dbdbafa31c1ad291d1e5940a0cf4,"This paper studies the distribution mismatch between linear interpolations between two random latent vectors in implicit generative models. The linear interpolation is defined as the difference between the probability distribution of a generator and the latent distribution of the generator. The distribution mismatch can be eliminated by using non - linear interpolating between the two latent vectors. The paper proposes a new distribution, the multidimensional Cauchy distribution, which is defined on the zero - centred hypercube. The authors show that there is a trade - off between the linear distribution being linear and the distribution having even the most basic properties required for stable training, such as finite mean and variance. They also provide a general method of creating nonlinear interpolations that is easily applicable to a large family of commonly used latent distributions.   The main contribution of this paper is to investigate the properties of the distribution of probability distributions in the context of latent space prior distributions. This is a very interesting and important research topic. The main contributions of the paper are :   1. Identification of distribution mismatch in the latent space distribution between the generator and its latent distribution. 2. Demonstrating that distribution mismatch is eliminated completely by a proper choice of the latent probability distribution. 3. Providing empirical evidence that the proposed distribution can be used to identify regions of the generative model distribution that are more stable than the distribution."
SP:19b63ca635712f1509ca6e0141303c192f2709e0,"This paper proposes a method to learn hyperbolic embeddings for the ubiquitous attention mechanism of attention in deep neural networks. This is in contrast to the previous approaches which only imposed the geometry of embedding of object representations on the activations of deep networks. The main idea is to use the embedding space more efficiently without increasing the number of parameters of the model. The proposed method is based on the hyperboloid model and the geometry imposed on the parameters of shallow networks. It is shown to achieve better generalization on machine translation, learning on graphs, and visual question answering tasks while keeping the representations compact."
SP:f6049e9f80a63c9306c1cebcb6b229aa6da44ddc,"This paper presents the first in - depth security analysis of DNN fingerprinting attacks that exploit cache side - channel attacks to extract information about the architecture of deep neural networks ( DNNs ). The authors propose two new attacks, DeepRecon and DeepFusion, which are based on the Flush+Reload technique. The first attack is based on a co - located process on the host machine where the attacker observes the accesses of the target functions in the shared framework of VGG16 and ResNet50. The second one is a meta - model that fingerprints the architecture and family of the pretrained model in a transfer learning setting. Based on the extracted attributes, the authors demonstrate how an attacker can reconstruct the architectures of two common networks, VGG 16 and VGG50 ( Simonyan & Zisserman, 2014, 2016 ), as proof of concept. They also demonstrate a useful example of DeepRecon through model fingerprinting in a Transfer learning attack. Finally, they propose countermeasures to obfuscate an attacker from extracting the correct attributes and sequences using observation attacks like DeepReason and show that these defenses significantly increase the errors in the extracted extracted attributes and can be implemented in various DL frameworks without hardware."
SP:6a3dd89db6c24a1f98e8866ef0a4c1c2c1ec6635,"This paper proposes a hierarchical network model, called Hierarchical Prediction Network ( HPNet ), to understand how spatiotemporal memories might be learned and encoded in a representational hierarchy for predicting future video frames. The model is inspired by the feedforward, feedback and lateral recurrent circuits in the mammalian hierarchical visual system. It assumes that spatiotemeporal memories are encoded in the recurrent connections within each level and between different levels of the hierarchy.   The model contains a feed - forward path that computes and encodes the features of successive complexity and a feedback path that projects interpretation from a higher level to the level below. Within each level, the feed forward path and the feedback path intersect in a recurrent gated circuit that integrates their signals as well as the circuit’s internal memory states to generate a prediction of the incoming signals. The network learns by comparing the incoming signal with its prediction and updating its internal model of the world by minimizing the internal errors at each level in the hierarchy in the style of predictive self - supervised learning. In this paper, the authors first demonstrate HPNet ’s effectiveness in predictive learning and its competency in long range video prediction in benchmark datasets. Then, they provide neurophysiological evidence showing that neurons in the early visual cortex of the primate visual system exhibit the same sensitivity to memories of global movement patterns as units in the lowest modules of the HPNet. Finally, the findings suggest that predictive self supervised learning might be an important principle for representational learning in the visual cortex."
SP:fb74e57f35666742caf651e6da33b5defcf259a8,"This paper proposes a method to compute continuous embeddings for kmers from raw raw RNA -seq data, in a reference - free fashion. The proposed method is based on a continuous representation that will account for both gene expression as well as mutations and chromosomal rearrangements.    The authors consider the problem of including the rich patient - specific sequence information from RNA -Seq data via their proposed continuous representation. To this end, they propose a model which learns gene - like representations from the raw raw data from acute myeloid leukemia patients. The model is trained to capture information of both DNA sequence similarity and DNA sequence abundance in the embedding latent space. They report that the latent space recovers exon information from raw data and that the representation space allows detection of genomic abnormalities such as translocations and patient-specific mutations, making this representation space both useful for visualization and analysis. The authors show that their model handles situations that are standard in cancer genomics but considered edge case in standard pipelines. In particular, they consider the following situations : 1 ) in the case of leukemia, where the patient suffers from an aggressive form of leukaemia where the prognosis is poor and the progesterone levels are high. The patient is admitted to hospital and treated with chemotherapy and radiotherapy. The primary outcome is that the patient is alive and recovering well. 2 ) the patient deteriorates rapidly and dies within a few months. The prognosis deteriorates further and the patient becomes seriously ill. 3 ) The patient goes on to develop serious health problems."
SP:03aca6ff6a7f0ad2d5ccbcb15ed9536e305a9880,"This paper proposes a new compression method based on architecture compression, where instead of operating on the weight or filter space of the network like classical model compression methods, the approach operates on the architecture space. A 1 - D CNN encoder / decoder is trained to learn a mapping from discrete architecture space to a continuous embedding and back. This embedding is jointly trained to regress accuracy and parameter count in order to incorporate information about the architecture ’s effectiveness on the dataset. During the compression phase, we first encode the network and then perform gradient descent in continuous space to optimize a compression objective function that maximizes accuracy and minimizes parameter count. The final continuous feature is then mapped to a discrete architecture using the decoder. The authors compare this approach to conventional compression methods such as pruning, distillation and reinforcement learning and show that their architectures are smaller and faster than those produced by the baseline methods."
SP:0511b5d10a90e3fe814e2d35208b4a987894ea62,"This paper proposes a framework for planning and learning in the offline setting where an agent with an internal model needs to continually act and learn in the world. The proposed framework builds on the synergistic relationship between local model - based control, global value function learning, and exploration. Specifically, the authors study how local trajectory optimization can cope with approximation errors in the value function and how approximate value functions can help reduce the planning horizon and allow for better policies beyond local solutions. The authors also demonstrate how trajectory optimization and time - coordinated exploration can be used to perform temporally coordinated exploration in conjunction with estimating uncertainty in value function approximation. This exploration is critical for fast and stable learning of the value functions. Combining these components enable solutions to complex control tasks, like humanoid locomotion and dexterous in - hand manipulation, in the equivalent of a few minutes of experience in the real world.   The authors propose a “plan online and learn offline ” framework for the setting where a nominal model and computational resources are available to an agent, and can accelerate learning of novel task instances. In particular, they study the case where the internal nominal dynamics model used by the agent is accurate, and the goal is to learn the internal dynamics model based on knowledge of physics ( Todorov et al., 2012 ), or through learning ( Ljung, 1987 ). In this work, they use the following setting :   1. Nominal dynamics models based on physics. 2. Substantially accurate. 3. Qualified. 4. Quantitative. 5. Qualitative results."
SP:771494fda4702cd8c7efbf225b19028f91b449b9,"This paper presents a zero - shot dual machine translation ( ZMT ) approach that combines the previously proposed LSTM - based and Transformers - based unsupervised approaches for zero - shots NMTs. The proposed approach is based on two components : ( 1 ) reinforcement learning, where the goal is to learn a language pair that is similar enough to be able to be translated by a single machine and ( 2 ) dual learning, which is a special case of this approach. The former relies on a pre - trained language pair, while the latter relies on the language pairs of the target language pair to be learned. The authors compare the proposed ZMT with the LSTMs and Transformers-based methods on the UN corpus, and show that the proposed approach outperforms them by large margins. They also evaluate on the newstest 2014 dataset on the Spanish - French - Spanish ( both directions ) and on the New Zealand dataset ( English - Spanish - English ). The results show that ZMT - dual outperforms the two other methods."
SP:1558dc03f99670f9ddccdca9c223a2baf962d438,"This paper proposes a generative adversarial network based on the framework for Information - Retrieval ( IR ). IR is a framework that leverages the conditional probability distribution p(d|q ) over the documents ( d, q ) given the query ( q ). The authors claim that optimizing their minimax loss function will result in a generator which can learn the distribution, but their setup and baseline term steer the model away from an exact adversarial formulation, and this work attempts to point out certain inaccuracies in their formulation.   The authors propose a co - training like setup where two models are trained in a co-operative rather than an adversarial fashion, where one model is trained to be adversarial and the other one to be neutral. The goal of the paper is to draw conclusions from the loss curves of the two models trained in the same way as the previous work, i.e., by drawing conclusions by drawing the same by drawing a line along the line drawn from one model's curve to the other model ’s curve. The main contribution of this paper is that it proposes a method for generating adversarial networks based on IR that is different from the previous works in that it draws conclusions from its loss curves rather than the loss functions of the other two works. This is done by using the following steps :    1. Draw the loss curve of the first model trained in IR. 2. Estimate the probability distribution of the loss function of the second model. 3. Approximate the loss of the third model. 4. Measure the difference of the difference between the first and second model in terms of the number of adversarial adversarial cases. 5. Applege the difference in terms between the second and third adversarial case and use this information to estimate the true adversarial loss for the generator."
SP:6a13dda852ab075a3c0fb691476d6dc57919c729,"This paper proposes a new approach to approximate inference in generative generative models, based on variational auto - encoders and sparse coding. The main idea is to model sparsity in the space of a VAE with a Spike and Slab prior distribution, and derive the evidence lower bound using a discrete mixture recognition function thereby making approximate posterior inference as computational efficient as in the standard VAE case. With the new approach, the authors are able to infer truly sparse representations with generally intractable non - linear probabilistic models. They show that these sparse representations are advantageous over standard VAEs on two benchmark classification tasks ( MNIST and Fashion - MNIST ). Furthermore, they demonstrate qualitatively that the sparse elements capture subjectively understandable sources of variation that can recover sparse, informative and interpretable representations regardless of the predefined number of latent dimensions."
SP:06a22143186fa2948fbe324ccae96a62ff12064e,"This paper presents a novel approach for training generative models, called Generative Feature Matching Networks ( GFMN ), which leverages pre - trained neural networks such as autoencoders and ConvNet classifiers to perform feature extraction. The main contributions of this work can be summarized as follows : ( 1 ) The authors propose a non - adversarial feature matching - based approach to train a generative model that does not use adversarial learning, have stable training and achieves state - of - the - art results for challenging benchmarks such as CIFAR10 and STL10. ( 2 ) They propose an ADAM - based moving average method that allows effective training with small minibatches ; ( 3 ) They show that the same feature extractor is effective across different datasets. ( 4 ) They conduct extensive experiments with different challenging datasets, including ImageNet."
SP:2d7cf2f07a27d6c8e304a1b47c25387ad2e4432d,"Graph Neural Networks ( GNNs ) are an effective framework for representation learning of graphs. However, there is limited understanding of their representational properties and limitations.   This paper presents a theoretical framework for analyzing the expressive power of GNNS to capture different graph structures. The main idea is to follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and achieved state - of - the - art results on both node and graph classification tasks. This paper characterizes the discriminative power of popular GNN variant, such as GraphSAGE, and shows that they can not learn to distinguish certain simple graph structure. Then, the authors develop a simple architecture that is provably the most expressive among the class of Gnns and is as powerful as the Weisfeiler graph isomorphism test. The authors validate their theoretical findings on a number of graph classification benchmarks, and demonstrate that their model achieves state -of - The - art performance."
SP:51126f2dd37ce57d2614c9044ede1e43627f0829,"This paper proposes a framework for interpretable continual learning ( ICL ) based on variational continual learning. The main idea is to generate a good explanation of a previously performed task, which can then be used to improve performance on a new task that is being learned. The paper proposes to use saliency maps to provide explanations of previously performed tasks and propose a new metric to assess the quality of the explanations. Experiments show that ICL achieves state - of - the - art results in terms of overall continual learning performance as measured by average classification accuracy, and also in terms   of its explanations, which are assessed qualitatively and quantitatively using the proposed metric. Although we focus on performing within the continual learning framework ( N Nguyen et al., 2018 ), our proposed methodology is flexible and can be deployed with other continual learning frameworks, such as saliency detection methods.   The main contribution of this paper is to develop a framework that can be applied to many continual learning approaches. The ICL idea is general and may be applied in many ways to many different continual learning methods. ICL is an example of one of those methods. It is interesting to see that the authors do not compare the ICL method with other methods that focus on interpretability, but rather one of interpretability of interpretable methods that focuses on interpretable explanations."
SP:27a565b3e5442b93d208652784051e640b0c1bfe,"This paper proposes a new evaluation framework for adversarial attacks on neural sequence - to - sequence ( seq2seq ) models, taking meaning preservation into account and demonstrate that existing methods may not preserve meaning in general. Based on these findings, the authors propose new constraints for attacks on word - based MT systems and show, via human and automatic evaluation, that they produce more semantically similar adversarial inputs than existing methods. Furthermore, the paper shows that performing adversarial training with meaning - preserving attacks is beneficial to the model in terms of robustness without hurting performance."
SP:54ddd8132bf9e4259d2c2d72b348d2bb5f9e227c,"This paper presents a method for improving the performance of RL policies by combining the reinforcement learning algorithms using inverse ( negative ) rewards and original rewards. The inverse rewards are competitive with the original rewards, and help the original policies correct their mis - actions. The proposed method is based on three algorithms : deep Q - learning, double Q - Learning, and on - policy actor - critic. The experiments for some games in OpenAI gym show that the hybrid polices obtain rewards up to 54.7 % more than the original algorithms. The improved polices are more stable than the originally policies as well.   The main contribution of this paper is to develop a method to improve the performance and stability of RL algorithms by using inverse rewards and the policies using original rewards and inverse rewards."
SP:89a732b57934d08b937c93560f391b7758e54f8a,"This paper presents a new method for learning object parts, their hierarchical structure, and dynamics for predicting future motion of object parts from unlabeled videos. The method is called Parts, Structure, and Dynamics ( PSD ). It consists of three parts : 1 ) recognize object parts via a layered image representation, 2 ) predict hierarchy via a structural descriptor that composes low - level concepts into a hierarchical structure ; and 3 ) model the system dynamics by predicting the future by watching how each object part moves in the future. The model is trained on two datasets, one for training and one for testing. Experiments on both real - world and synthetic data show that the model works well on all three tasks."
SP:bb2a655d67bed9da43f0b8ec7d888b89c217d12e,"This paper proposes Deep Determinantal Generative Classifier ( DDGC ), a generative classifier that is trained on top of the discriminative deep neural network ( DenseNet ) and trained for image classification tasks such as CIFAR ( Krizhevsky & Hinton, 2009 ) and SVHN ( Netzer et al., 2011 ). It is well - known that deep neural networks poorly generalize from noisy training datasets, and DDGC is an attempt to address this problem by using the minimum covariance determinant estimator. The main idea is to estimate the parameters of the generative model using the parametrization of the hidden feature spaces. The paper shows that DDGC significantly improves the classification accuracy, with neither re - training of the deep model nor changing its architectures. In addition, DDGC can be used to further improve various prior training methods, such as the state - of - the - art noise - handling training method and adversarial attacks. Finally, the paper proposes the ensemble version of DDGC to improve its performance, by investigating the investigating layer - wise characteristics of the classifier."
SP:0fa525cc708470b757a60117cb608bb2feaa2c50,"This paper presents a method for solving the problem of subgoal discovery in RL using incremental unsupervised learning over a small memory of the agent's experiences in the environment. The method is based on a model - free approach that does not require the acquisition of a model of the environment for learning action selection policies at multiple levels of temporal abstraction. The authors claim that this approach is more suitable for large - scale applications with sparse delayed feedback and large number of state spaces.   The main contributions of the paper are the following :   1. A method for learning subgoal policies and skills to achieve those subgoals in RL. This method combines the learning of state - level representations of states, automatic sub - goal discovery, intrinsic motivation learning of skills, and subgoal selection by a “ meta - controller ’s ” learning. The meta controller learns a state representation, a subgoal policy, and a set of skills corresponding to the subgoal. This state representation can be used to learn an action policy and skills policy to achieve subgoal goals. The skill policy can be learned by using the learned state representation and the skills policy. The algorithm is evaluated on two RL problems : a variant of the rooms environment and the ATARI 2600 game. The experiments show that the proposed method is more effective than the baselines on both of these problems. 2. An approach for integrating subgoal and skill learning. This approach consists of two steps. First, the meta controller is trained to identify subgoal states and skills. Then the meta - controllers are trained to discover subgoal actions and skills using the state - space representations of the states. This is done by using a combination of state space representations, subgoal state representations, and skills policies. The proposed approach is shown to be more effective on both the rooms and ATARI problems than baselines."
SP:e5861538bc8bb9165cb33299bbf12dd875abf976,This paper proposes a neural framework that can learn to solve the Circuit Satisfiability problem. The framework is built upon two fundamental contributions : a rich embedding architecture that encodes the problem structure and an end - to - end differentiable training procedure that mimics Reinforcement Learning and trains the model directly toward solving the SAT problem. Experimental results show the superior out - of - sample generalization performance of the framework compared to the recently developed NeuroSAT method.
SP:ff3e5d44619df3825632b0b1a943add081364861,"This paper proposes a new method combining the cross - entropy method ( CEM - RL ) and deep reinforcement learning ( DDPG ) algorithms for policy search. The authors argue that combining the two methods together is a good way to combine the best of both worlds. They compare the performance of the proposed method with that of two other methods, namely TD3 and TD2 and argue that CEM is better than TD3 in terms of both performance and sample efficiency. They also argue that there is still a lot of unexplored potential in combining evolutionary and deep RL methods.   The main contributions of the paper are as follows :   1. An analysis of the empirical relationship between CEM and TD3. They show that TD3 is a better choice for off - policy deep RL algorithms than CEM. 2. A comparison is made with TD2, TD3, TD2 - TD and TD4. 3. A discussion is given about the advantages and disadvantages of each of them."
SP:78b2eb326695da0b0cc4ba39a9206d11644a5e32,"This paper presents an interpretable multi - variable LSTM recurrent neural network ( RNN ) based on a hidden state matrix and updating process. The authors also propose a mixture attention mechanism and associated summarization methods to quantify the temporal and variable importance in data. Experimental results on real datasets demonstrate the superior performance and interpretability of the proposed RNN compared to a variety of baselines including statistical, machine learning and neural network baselines.   The contribution of this paper is fourfold. First, the authors propose the interpretable RNN based on the hidden state matrices and updating scheme. Second, based on these variable - wise hidden states, they develop a novel mixture and variable attention mechanism. Third, attention values are further summarized to quantify variable-wise temporal importance and overall variable importance. Lastly, extensive experiments are performed to demonstrate the interpretability and superior performance of IMVLSTM."
SP:1c26660569b579f060f7b4a31e321c6d2356b928,"This paper proposes a new data augmentation method, feature smoothing, for defense against adversarial attacks on MNIST and CIFAR-10 datasets. The method trains a neural network on virtual training data as an interpolation of features from a pair of samples, with the new label remaining the same as the dominant data point. The paper compares the proposed method with other “ efficient ” methods, such as label smoothing and logit squeezing, weight decay, mix up, and features smoothing. The experiments show that all methods produce an unbiased estimation of the decision boundary with smaller estimated variance than the other methods except weight decay. The theoretical analysis of the connections between different methods is conducted in Section 4. The last section concludes with a comparison of the performance of different methods."
SP:88d652f9e411dd3a2e9ad651d9011e579653c6aa,"This paper proposes a theoretical framework for deep neural networks with ReLU nonlinearity. The framework bridges data distribution with gradient descent rules, favors disentangled representations and is compatible with common regularization techniques such as Batch Norm.   The framework is built upon teacher - student setting, by projecting the student ’s forward / backward pass onto the teacher’s computational graph. The authors do not impose unrealistic assumptions ( e.g. Gaussian inputs, independence of activation, etc ). Our framework could help facilitate theoretical analysis of many practical issues."
SP:7842bbe0e2324cfd732db8745550733ccc3dfcdc,"This paper presents a method for learning behavioral modules ( BMs ) from pre - conditioned pre - frontal cortex ( PFC ), inspired by human behavior formation and behavioral modular neural networks. The authors propose a modular architecture of neural networks with a Behavioral Module ( BM ) and corresponding end - to - end training strategy. This approach allows efficient learning of behaviors and preferences representation which is particularly useful for user modeling ( as for dialog agents ) and recommendation tasks, as well as allows learning personalized representations of different user states. The experiments show that the proposed method allows separation of main task’s objectives and behaviors between different BMs and enables network extendability through independent learning of new behavior patterns. Moreover, the authors demonstrate a strategy for an efficient transfer of newly learned BMs to unseen tasks. The experimental results show the effectiveness of their approach on video games domain."
SP:300c391ff644b6889cd9ae27cf0d162dfcdd4451,"This paper proposes a new neural network training method based on differentiable neuromodulation of plasticity. The proposed method is based on the differentiable plasticity framework proposed in Miconi et al. ( 2017 ), which allows for differentiable, non - modulated plasticity to be incorporated into neural networks. The method is called backpropamine in reference to its ability to emulate the effects of natural, self - modulating dopamine in artificial neural networks trained by backpropagation. The experimental results establish that the proposed method can outperform both non - plastic and non - Modulated plastic networks, both on simple reinforcement learning tasks and on a complex language modeling task involving a multi - million parameter network. The authors also show that the back Propamine framework can be trained with gradient descent to achieve better performance than the state - of - the - art methods."
SP:1ab5d94d31e99351433436c026799c8aa597bf73,"This paper presents a method to train deep neural networks ( DNNs ) using binary quantization. The authors propose a non - intrusive quantization technique based on re - training the full precision model, followed by directly optimizing the corresponding binary model. The quantization training process takes no longer than the original training process, and the authors also propose a new loss function to regularize the weights, resulting in reduced quantization error. The proposed method is shown to achieve full precision accuracy on CIFAR - C and WikiText - 2 using 2 - bit quantization respectively. Comparable results are also shown for ImageNet.   The authors also present a 1.5 bits hybrid model for WikiText-2, which outperforms the performance of TWN LSTM."
SP:0876b1d9a6d664808ca1ab15865679fbf638267e,"This paper proposes a method for open - ended style - content recombination ( STOC ). STOC is a style encoder - style decomposition followed by a content encoder, where the style representation is extracted from the latent space of the style embedding and the content representation is obtained from the training data. The method is applied to image classification tasks where the class label specifies the content of an image, and the class - irrelevant properties that differentiate instances constitute the style.   The method starts by constructing a content embedding using an existing deep metric - learning technique, which is incorporated into a trained - to - be - trained style - encoder ( VAE ). The VAE reconstruction loss alone is inadequate to ensure a decomposition of the latent representation into style and content. The proposed method thus includes an auxiliary loss, leakage filtering, which ensures that no style information is used for reconstruction and vice versa for vice versa. Theoretical results show that STOC can transfer existing STOC style labels to novel content class labels, and that the style labels do not need to be explicit. In contrast, previous work assumes that the content classes in testing are the same as those in training. The approach is general in nature and can be applied to any domain."
SP:d37e15cde7765fca87595a242f0a4511b3346d46,"This paper proposes a method to speed up deep reinforcement learning ( deep RL ) training for problems that have the property of state - action permissibility ( SAP ).   The paper identifies a special property SAP in a class of RL problems that can be leveraged to cut down the exploration space to markedly improve the RL training efficiency. It proposes a novel approach to using the SAP property, i.e. building a binary predictive model to predict whether an action in a state is permissible or not ahead of time. The proposed approach incorporates the proposed SAP property into two state - of - the - art deep RL algorithms to guide their state -action exploration. Experimental results show that the proposed approach can result in a huge speedup in RL training."
SP:20015d8b60e13300586b67c281858cbe28825c48,"This paper analyzes the behavior of deep autoencoders under the assumption of random weights in a multilayer autoencoder model. The authors provide quantitative answers and insights to three questions that have not been fully understood in the literature : 1 ) How the deep encoder performs approximate inference and its connection to reversibility considered by several theoretical studies. 2 ) Deep encoders display a higher degree of sensitivity to perturbations in the parameters, distinct from the shallow counterparts. 3 ) It is experimentally shown experimentally that it is possible to train a deep decoder without resorting to layer - wise techniques such as batch normalization or pre - training.   The main contributions of this paper are as follows :   1 ) The analysis reveals interesting phase transition phenomena when the depth becomes large. This, in particular, provides quantitative answers for three questions yet fully understood by the literature. The first question is how the random deep weight - tied encoder model performs “ approximating inference ” as posed by Scellier et al. ( 2018 ), and the second is how it performs reversibility. The analysis is not specific to any depths or any Lipschitz activations, and their analytical techniques may have broader applicability. The third is how to avoid pitfalls in training initialization practice."
SP:91764f80dbe2401ade38b35a8253ba05f0f86386,"This paper proposes a new algorithm for adversarial image attacks based on the simple iterative principle of DCT. The goal is to find an imperceptibly modified image that is misclassified by the target model. The proposed method can be used for both targeted and untargeted attacks. The method requires a median of 600 black - box model queries ( ResNet - 50 ) to produce an adversarial ImageNet image. The authors compare the proposed method with the state - of - the - art adversarial attack algorithms ( Guo et al., 2019 ) and argue that the proposed algorithm should serve as a strong baseline for future adversarial attacks because it is extremely fast and can be implemented in less than 20 lines of PyTorch code."
SP:fc20ae0fbf57a1ce489c04b85c7c2f4c93dc2450,This paper proposes a new method of option discovery and reinforcement learning based on the options framework. The goal is to learn a set of state - actionable options that are transferable across different tasks. The method is based on a landmark state and sub - goal setting. The key idea is to use the landmark state as a starting point for exploration and the sub - goals as starting points for learning the intra - option policies. The authors also propose a pseudo - reward for learning intra - options that extends to function approximators. The proposed method is tested on grid worlds and high dimensional environments like Deepmind - Lab.
SP:12a172c1e2892d016b37932acfc48dcb56874a89,"This paper proposes a domain division method for zero - shot learning ( ZSL ). The proposed method is based on the bootstrapping and Kolmogorov - Smirnov method. The idea is to split the training instances into known, unknown and uncertain domains, and then conduct recognition tasks in each domain. The uncertainty domain is introduced to adopt those instances whose domain labels cannot be predicted confidently. The authors extensively evaluate the importance of domain division on several zero shot learning benchmarks and achieved significant improvement over existing ZSL approaches. Extensive experiments demonstrate that their approach achieved the state - of - the - art performance on OSL and G - ZSL benchmarks."
SP:28bcf7c6a4673e9ec2b4ebed09839d85188e0b2a,"This paper proposes a neural network for classification and regression, without the need to learn layout structures in the output space. Standard solutions such as softmax cross - entropy and mean squared error are effective but parametric, meaning that known inductive structures such as maximum margin separation and simplicity need to be learned for the task at hand. Instead, the authors propose polar prototype networks, a class of networks that explicitly states the structure, i.e., the layout of the output, of the class. The structure is defined by polar prototypes, points on the hypersphere of the input space. For classification, each class is described by a single polar prototype and they are a priori distributed with maximal separation and equal shares. For regression, classes are assigned to prototypes randomly or based on semantic priors and training becomes a matter of minimizing angular distances between examples and their class prototypes."
SP:d1034342785d133cf8372b8624897963cc2ee83a,"This paper proposes a method for RL ( reinforcement learning ) based on RLSP ( Reward Learning by Simulating the Past ). RLSP is based on the observation that the initial state of the world at the start of the agent's training is a good source of information about human preferences, which can then be used to learn implicit preferences that require active action. The paper presents experiments on a suite of proof - of - concept environments designed to evaluate RLSP's ability to learn from initial state. The experiments show that RLSP can infer both side effects that should be avoided as well as preferences for how the environment should be organized.   The paper also presents an algorithm for RLSP, which infers reward from initial states based on a Maximum Causal Entropy ( MCE ) model of human behavior. The algorithm is evaluated on a set of environments designed by Ziebart et al. ( 2010 ), where the goal is to learn a policy that maximizes the expected reward for actions taken and minimises the expected risk for actions not taken. The experiment results show that the RLSP algorithm is able to learn both implicit preferences for actions and implicit preferences to avoid side effects, which are hard to learn in an agent that is designed to maximize rewards."
SP:417a4e0acee699b3e004ad30d0ecf533a9ed987e,"This paper proposes a method for learning the dependency structure between latent variables in deep latent variable models. The motivation is that the existing methods for learning such dependency structures are limited to graphs of a particular form, or difficult to integrate with the gradient - based optimization techniques of modern neural networks. The authors propose a method that combines the strengths of deep generative models and probabilistic graphical models. In particular, they express the latent variable space of a variational autoencoder ( VAE ) in terms of a Bayesian network with a learned, flexible dependency structure. The network parameters, variational parameters as well as the latent topology are optimized simultaneously with a single objective. Inference is formulated via a sampling procedure that produces expectations over latent variable structures and incorporates top - down and bottom - up reasoning over latent variables. They validate their framework in extensive experiments on MNIST, Omniglot, and CIFAR - 10. Comparisons are made to state - of - the - art structured VAE models."
SP:976dedab53e69610692a563382ada1dbb82c1e9d,"This paper presents a dynamical neural network ( DLN ) based on top - down feedback and contrastive learning for solving dictionary learning problems. The authors show that the proposed DLN can be used to solve the 1 - minimizing dictionary learning problem, and the true gradients for learning are provably computable by individual neurons. They also show that using spiking neurons to construct a learning process, they can solve the learning problem for the Boltzmann machine.   The main contributions of the paper are as follows :   1. A new dynamical neuron model is proposed. The activation function of the neuron model corresponds to the bounded sigmoid - like function in Hopfield networks or Boltzmanni machines, and a special network topology where connection weights have dependency on the weights of the connections. 2. The learning process is similar to that of the Hopfield network in that the network minimizes the gradient information of a dictionary learning objective function and uses it to maintain weight dependency for the network to maintain gradient information. 3. The main difference is that instead of running the network in two configurations, the authors run it in two. The difference in states after a long - enough evolution, called limiting states in short, is shown to hold. 4. The method is compared to a Hopfield neuron model and a BoltZmann machine, and it is shown that it is able to achieve better performance than both of them in terms of the number of neurons and the speed up."
SP:f45117a6beaeb86a70b1380b4fac3cfba37fb892,"Convolutional neural networks ( CNNs ) show great results on both high - level and low - level features representations, however, the capability has not been fully embodied for lane detection task. In this paper, the three main contributions are as follows :   1. Multiple encoder - decoder module in end - to - end ways to improve the performance of CNNs for detection.   2. Different configurations of multiple encoder-decoders module in different ways to show the promising results of lane detection. 3. Different methods to rethink evaluation methods to improve lane detection for the limitation of popular methods based on IoS."
SP:68b0a10ca06df74612d0753cc3f3ddddde806035,"This paper studies the problem of learning from historical and off - policy feedback in contextual bandit learning, where one only has access to a collection of logged feedback from the actions taken by a historical policy in order to learn a policy that takes good actions in possibly unseen contexts. The paper proposes a method called Maximum Likelihood Inverse Propensity Scoring ( MLIPS ) to estimate a maximum likelihood surrogate policy based on the logged action - context pairs, and then use this surrogate policy as the surrogate policy to obtain the inverse propensity weights in the off - policies estimator, as if the logs are generated by this approximate policy. Experiments on multi - label classification problems and a large - scale dataset demonstrate the empirical effectiveness of MLIPS. Furthermore, the proposed surrogate policy technique is complementary to existing error reduction techniques, and when combined is able to consistently boost the performance of several widely used approaches, such as inverse propensity scoring ( IPS ) and policy optimization for exponential models ( POEM )."
SP:8e0ed65c5dded23b34798499b2436b24422fd729,"This paper proposes a meta - learning approach to learn how to create individualized feature embedding specific to a given query image for better classifying. Given a query image, the authors propose to train a kernel generator to generate convolutional kernels based on the features in the query image to be classified. The kernel generator is trained on two standard few - shot classification data sets, including Omniglot Lake et al. ( 2011 ) and miniImageNet Vinyals et al ( 2016 ). The authors propose two approaches to learn the feature embeddings for each query image. The first approach is to use a feature extractor and the second is a feature generator.   The authors claim that this approach is more flexible than the meta - learner approach in that it does not rely on the similarity metric between query images and feature extractors. However, it is not clear from the paper that this is the case in the case of the kernel generator approach. The author's claim is that the feature generator can be used to generate kernels for different query images during training, which can be generalize to unseen categories without fine - tuning. The main contribution of this paper is to propose a method to learn an individualized embedding for each individual query image based on its characteristics. This is done by training a feature kernel generator and then using the learned feature kernel to generate individualized features for each image."
SP:faa3f7ffdcfb6e3b8ec0421193dae3d9987b015c,"This paper studies the evolution of deep neural networks ( DNNs ) via evolutionary strategies ( ES ) and population - based genetic algorithms ( GA ). It shows that ES can be considered a gradient - based algorithm because it performs stochastic gradient descent via an operation similar to a finite - difference approximation of the gradient. It also shows that GA can evolve the weights of a DNN with a simple, gradient - free, population based algorithm, and it performs well on hard deep RL problems, including Atari and humanoid locomotion. The unexpectedly competitive performance of GA ( and random search ) suggests that the structure of the search space in some of these domains is not amenable to gradient based search. That realization opens up new research directions on when / how to exploit the regions where a gradientfree search might be more appropriate and motivates research into new kinds of hybrid algorithms.   The paper proposes to combine ES with novelty search, which encourages exploration on tasks with deceptive or sparse reward functions, and the largest neural networks ever evolved with a traditional evolutionary algorithm, the Deep GA. These results ( 1 ) expand our understanding of the scale at which GAs can operate ( 2 ) suggest intriguingly that in some cases following the gradient is not the best choice for optimizing performance, and ( 3 ) make immediately available the multitude of neuroevolution techniques that improve performance."
SP:dfdbe3267a8160f24746884cdf5297993e424231,"This paper presents a curiosity method for reinforcement learning ( RL ) based on the novelty bonus. The novelty bonus is based on how many steps it takes to reach the observation from the observation in memory. The paper compares the proposed method with the state - of - the - art curiosity method ICM ( Pathak et al., 2017 ) under the same budget of environment interactions in three visually rich 3D 3D environments : VizDoom, DMLab and MuJoCo. They show that their method outperforms the baseline ICM in all three environments. They also show that an ant equipped with their curiosity module learns locomotion out of the first - person - view curiosity only. The main contributions of the paper are as follows :   1. They propose a new curiosity method which uses episodic memory to form novelty bonus to determine the bonus, the current observation is compared with the observations in memory, which incorporates rich information about environment dynamics. 2. They compare the performance of their method with that of ICM on the three visual 3D tasks. 3. They demonstrate that the method is more robust to spurious behaviours than the baseline. 4. In the complete absence of rewards, their method covers at least 4 times more area ( measured in discrete ( x, y ) coordinate cells ) than ICM."
SP:1e58a1c5344d1b5b7c8a40210a243700bd933d65,"This paper proposes a method for learning transition models in the relational setting, where the goal is to learn a distribution over the properties of the objects in the resulting state given their properties in the previous state. The proposed method is based on two main components. First, an iterative greedy algorithm is used to construct a set of deictic references that determine which objects are relevant in any given state. Second, feed - forward neural networks are used to learn the transition distribution on the relevant objects ’ properties ’. This strategy is demonstrated to be both more versatile and more sample efficient than learning a monolithic transition model in a simulated domain in which a robot pushes stacks of objects on a cluttered table."
SP:8ce00a3fedbf54a7f2c1ff414511cbb7d59b4597,"This paper proposes a new instance - wise feature selection method, named INVASE, which consists of 3 neural networks, a selector network, a predictor network, and a baseline network. The selector network is trained iteratively by minimizing a Kullback - Leibler ( KL ) divergence between the full conditional distribution and the selected - only conditional distribution of the outcome. The predictor network is used to train the selector network using the actor - critic methodology. The baseline network is the one that is used for subset sampling. The experiments show that INVASE significantly outperforms the state - of - the - art in terms of true positive rates, false discovery rates, and several prediction metrics."
SP:b91d6c33349df0bb6cb7e1c5e9433f0d4744b4da,"This paper proposes a method for semantic segmentation based on patch - level adversarial learning and domain adaptation. The proposed method is based on two modules : label - level alignment module and the adversarial patch learning module. The module uses the label histogram of patches to guide the learning of the discriminative feature representations of patches based on label histograms in the source domain. The method uses a disentangled space to learn the feature representations in target patches to the closer distributions in source ones, and then uses an adversarial training scheme to adjust the feature representation in the target patches with the one obtained from the source data. The authors compare the proposed method against several baselines and state - of - the - art methods on semantic segmentations and show that the proposed adaptation method performs favorably against various baselines."
SP:00922af13a21464cbc4cd7b34c196dd4f86c9247,"This paper proposes two optimistic algorithms for training deep neural nets based on the observation that the mini - batch of stochastic gradients in consecutive iterations do not change drastically and consequently may be predictable. Inspired by the similar setting in online learning literature called OPTIMISTIC ONLINE LEARNING, the authors propose two new optimistic algorithm for AMSGrad and Adam, respectively, by exploiting the predictability of gradients. The new algorithms combine the idea of momentum method, adaptive gradient method, and algorithms in the Optimistic Online Learning ( OLE ) paper, which leads to speed up in training neural nets in practice, where gt is the gradient of loss function in round t and mt is the “guess ” of gt before seeing the loss function before getting gt. This kind of regret can be much much smaller than O( T ) when one has a good guess of the next gradient. The authors evaluate their algorithms with ( Kingma & Ba ( 2015 ), ( Reddi et al. ( 2018 ), and Daskalakis et al ( 2019 ) ). Experiments show that their optimistic algorithms are faster than the baselines. They also provide theoretical analysis of the proposed algorithms."
SP:52228b48f2776d57dd422edb33b82e247f056b75,"This paper establishes benchmarks for image classifier robustness. The first benchmark, IMAGENET - C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety - critical applications. Then, the authors propose a new dataset called IMAGenET - P which enables researchers to benchmark a classifier’s robustness to common perturbations. This benchmark evaluates performance on common corruptions and perturbation not worst - case adversarial perturbedations.   The authors find that there are negligible changes in relative corruption robusts from AlexNet classifiers to ResNet classifier. Afterward, they discover ways to enhance corruption - robustness, and afterward, afterward they discover that a bypassed adversarial defense provides substantial common perturbed robusts. Together, these benchmarks may aid future work toward networks that robustly generalize."
SP:20358ea0f769e6ea9222d8e35159d711ee1b20b2,"This paper studies the problem of language modelling with deterministic dropout in a family of conditional models whose objectives are lower bounded than the usual stochastic dropout objective. This allows them to pick any model from this family after training, which leads to a substantial improvement in regularisation - heavy language modelling. The family includes models that compute a power mean over the sampled dropout masks, and their less stochastically subvariants with tighter and higher bounds than the fully stochastically drop out objective. The deterministic subvariant ’s bound is equal to its objective, and the highest amongst these models. It also exhibits the best model fit in their experiments. Together, these results suggest that the predominant view of deterministic models as a good approximation to MC averaging is misleading. Rather, deterministic deterministic model with low bound on dropout is the best available approximation."
SP:ac1b950ad29429ae045bb5e53279014a6a0b9d2b,"This paper proposes a new pruning method based on cumulative saliency based soft filter pruning ( GSFP ) to prune redundant filters in Convolutional Neural Networks ( CNNs ). The proposed method combines soft pruning with robust pruning, which measures the global redundancy of the filter in the whole model by using the soft prune strategy. In addition, in the model recovery process after pruning the model, the authors use cumulative salient strategy to improve the accuracy of pruning. The authors claim that the proposed method has two advantages over previous works : ( 1 ) More accurate pruning guidance. For a pre - trained CNN model, accumulating the saliency of each filter over the entire data set can provide more accurate guidance for pruning than using local pruning alone. On the other hand, pruning from a global perspective is more accurate than local prune.   The authors also propose a more robust method to recover the model from the pruning error. They propose a reasonable normalization formula to prevent certain layers of filters in the network from being completely clipped due to excessive pruning rate."
SP:621e41d4199e333ec7f9d0936d4e34c918f39c11,"This paper proposes a cross - lingual document classification framework ( CACO ) for dealing with limited training data in related language pairs. The authors use a character - based embedder and a word - based classifier to jointly train the embedder. The embedder is trained on the source language and the target language, and the classifier is trained to predict the word based on the vector representations derived from the embeddings.   The authors compare the performance of the proposed method with the state - of - the - art CLWE - based method on two datasets CLWE-2 and CLDC-9. The results show that the proposed approach outperforms the CLWE method on both datasets. The main contributions of the paper are as follows :   1. A better transfer learning scheme that exploits cross - linguistic subword similarity between source and target language words. 2. A multi - task objective that can further improve the model if additional cross -lingual or monolingual resources are available."
SP:544e421f9c747640d949f433e3091763508b7237,"This paper proposes a new method for weakly supervised temporal action localization based on marginalizing the dominant response of the most salient regions. The proposed method, called marginalized average attentional network ( MAAN ), is based on the marginalized average aggregation ( MAA ) module, which learns a set of latent discriminative probabilities in an end - to - end fashion to sample multiple subsets from the video snippet features according to a set   latent discrim inative probabilities and takes the expectation over all the averaged subset features. The authors provide theoretical analysis of the properties of MAA and an explanation of the reasons why MAAN alleviates the issue raised by the domination of theMost salient regions, ( 3 ) a fast iterative algorithm that can effectively reduce the computational complexity of the MAA module, and ( 4 ) a superior performance on two benchmark video datasets, THUMOS14 and ActivityNet1.3.   The main contributions of the paper are as follows :   1 ) A new method to marginalize the dominant responses of the salient regions of the video : MAAN. 2 ) A fast algorithm to reduce the complexity of constructing MAA from O(2 ) to O(T ). 3 ) Extensive experiments on two large - scale video datasets show that our MAAN achieves better performance than the state - of - the - art."
SP:9f98c9bac99003741dd14e093b54d692c0b0e8d8,"This paper introduces Holographic Reduced Representation ( HRR ), a framework for constructing structured representations of natural language models based on reduced - form representations of language representations. The main idea of the paper is to use the reduced form language representations as a way of constructing compositional models for Natural Language Processing ( NLP ) models. The proposed HRR framework is based on the idea that the role of language models is a division between syntax and semantics. The authors propose two models, one based on VSA and the other based on HRR, and they compare their proposed models with the state - of - the - art NLP models. They show that the proposed models are able to discover crude linguistic roles, which roughly resembles a classic division between syntactic and semantic roles. They also show that their proposed model is able to predict the distribution of words more accurately than existing models.    The main contributions of this paper are as follows :   1. Introducing a structured language representation based on a reduced form of Natural Language Representation. This is a novel approach to construct compositional language models. It builds on top of the structureless distributed representations of NLP that are already present in the literature. The structureless representations are a result of the fact that the language models have access to all the information in the language representations, which is not available in the structured representations. This allows the authors to construct structured language representations that are more representative of the linguistic structure than the structured representation of the language. The structured representations are then used to train a set of neural models for NLP. These neural models are trained with the following parameters : 1. The word level, where the representation of a word is composed of the word, the word - level, and the chunk - level. 2. The chunk level representation is the sum of all the words in a word, and it is composed by the word level and chunk level of each word. 3. In the case of the chunk level representations, the representations are composed by all the tokens in the chunk, and then the tokens are concatenated with the word. The representations of the chunks are then fed into the VSA model and the language model. The models are then trained using the learned word level representation and the learned chunk representation."
SP:5908b6acfed0e7c51e203c72eba907e6635e6c60,"This paper proposes a novel method to integrate the perception and planning components of Partially Observable Partial Decision Processes ( POMDPs ). The authors claim that due to the combinatorial nature of the selection process, it is intractable for the planning and perception components to be integrated in the same action space. To address this problem, the authors propose a greedy strategy for observation selection that aims to minimize the uncertainty in state - state observations. The proposed method is based on point - based value iteration algorithm that incorporates the greedy strategy to achieve near - optimal uncertainty reduction for sampled belief points. This in turn enables the solver to efficiently approximate the reachable subspace of belief simplex by essentially separating the perception related to planning from the planning related to perception from planning.   The authors also propose a method to simultaneously learn a selection strategy and a planning strategy and demonstrate its performance and computational advantage in a range of robotic scenarios."
SP:0adec4abec17b3aab0c6eb69d11925dc20544950,This paper proposes a new algorithm for improving the performance of deep neural networks by adapting the distribution of training examples from easy to hard. The authors claim that the distribution shifting in weight of top layers affects the training of preceding layers during the backward pass. They call this phenomenon inverse “ internal covariate shift ” and propose adaptive weight that assigns small values to hard examples to reduce the influence of noisy gradients. The less - weighted hard sample receives the proposed representation loss while the less - weightsed hard samples receive the representation loss for training. The proposed loss is easy to combine with existing stochastic algorithms like SGD. Experimental results showed that the proposed algorithm brings consistent improvement and can accelerate convergence over several benchmark datasets.
SP:8b555b9f24044bc68c204169d6a37e262361d706,"This paper proposes a method to learn heuristics for combinatorial optimization problems by training a model based on attention layers and training with REINFORCE. Specifically, the authors propose to train the attention layer model using a deterministic greedy rollout, which they find to be more efficient than using a value function. The method is tested on TSP, VRP, OOP, and CTSP, and the authors show that the attention layers model is better than the Pointer Network for solving these problems.    The main contribution of this paper is to propose a method for training the attention model and to show how to train this model with a simple but effective greedy rollout baseline. The goal of the method is not to outperform a nonlearned specialized TSP algorithm such as Concorde ( Applegate et al., 2006 ), but rather to learn a model that is capable of solving problems of reasonable size, with a single set of hyperparameters. This is important progress towards the situation where we can learn strong heuristic to solve a wide range of practical problems for which no good heuristically exist. The authors focus on the Travelling Salesman Problem ( TSP ) and the Vehicle Routing Problem ( VRP ) and show that their approach is able to achieve good results for TSP and VRP. However, they need better models and better ways of training to improve their model."
SP:efb76bcf1dbd9a9cf6b5db74b5d4256a9f9e9e73,"This paper proposes a novel neural architecture search ( DNAS ) framework to efficiently explore its exponential search space with gradient - based optimization. Experiments show they surpass the state - of - the - art compression of ResNet on CIFAR-10 and ImageNet by quantizing different layers with different bit - widths. The DNAS pipeline is very fast, taking less than 5 hours on 8 V100 GPUs to complete a search on ResNet18 for ImageNet, while previous NAS algorithms ( such as Zoph & Le ( 2016 ) typically take a few hundred GPUs for several days. DNAS is a general architecture search framework that can be applied to other problems such as efficient efficient neural structure discovery."
SP:ea4173f8265bc50296de51c4ee7ecb6b8f78bec0,This paper proposes a new attention architecture based on the posterior attention model. The authors claim that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence. The proposed posterior attention models yield better BLEU score and alignment accuracy than existing attention models. This is a significant finding that challenges the current practice of computing attention distribution without considering the output token. The running time overhead of posterior attention is only 40 % over existing soft - attention.   The main contributions of this paper are as follows :   1. This paper proposes an attention distribution for the next decoding stage that is conditioned on the output of the previous stage. This allows the attention to be marginalized from the input to the output stage. 2. The attention propagated to the next stage is a posterior attention distribution that is independent of the output. 3. This enables the attention distribution to be more entropy efficient than the distribution of soft attention in the first stage of the decoding stage.
SP:987e2c14abc091d4d3ef9b48fb2046408eb1f59e,"This paper introduces a new method for image - to - image translation, called HarmonicGAN, to learn bi - directional translations between the source and the target domains. The main idea is to introduce a smoothness term over the sample graph to enforce consistent mappings during the translation process. The authors show that this method outperforms the state - of - the - art methods in a number of applications including medical imaging, object transfiguration, semantic labeling and semantic labeling. They also show that their method turns CycleGAN from a failure to a success, halving the mean - squared error and generating images that radiologists prefer over competing methods in 95% of cases."
SP:885a69003bad0e79cb2872a4e5c772191ad7e34f,"This paper proposes a stochastic algorithm ( h - detach ) to address the vanishing gradient problem ( EVGP ) in LSTM optimization. EVGP is a problem that occurs when gradient components through the linear path ( cell state ) in the L STM computational graph get suppressed, which prevents important gradient components from being back - propagated adequately over a large number of steps. This paper proposes to address this problem by introducing h -detach, which is a simple algorithm that is specifically designed for addressing EVGP in the case of LSTMs. The authors hypothesize that the suppressed gradients carry information about long - term dependencies, and that suppressing these gradients is a way to ensure that L STMs do not capture them. Based on this hypothesis, the authors propose a simple method to back - propagate gradients along the path of least squares ( H - sqrt(LSTM ) ). The method is based on the back - propagation theory, which states that if the gradient components along the linear paths are suppressed, then these gradient components will flow through the paths that are close to the long - range dependencies, which will lead to better optimization.   The authors empirically show that their method works well on two benchmark datasets, CIFAR-10 and Cifar-100. They also show that the method works better on CIFARS-100 than CIFar-10, and the method is also more accurate than the SOTA method on the MNIST dataset."
SP:9aaff3777321347d1194884af5690b0b5185eff9,"This paper proposes a Bayesian deep learning method for training real binary weight networks without layer wise scaling factors. The method is based on a nested neural network, where the policy network is trained with a reinforcement learning scheme and the binary weights are generated in a burn - after - reading style. The posterior distribution of the weights is parameterized as a function of the learned policy network. The paper evaluates the proposed method on ImageNet, CIFAR-10, ImageNet-50, and MNIST. Results show that the method performs better than other Bayesian methods for training binary weight neural networks without filter wise scaling factor.   The main contributions of the paper are as follows :   1. A deep neural network with nested parameterization is trained. This is done by training a policy network and a neural network that learns the parameters of the nested parameterized neural network. This neural network is then used to generate binary weights on - the - fly, which are sampled from the learned neural network during the training phase. 2. The authors show that this method is better at training binary weights than other methods that are based on point estimation. 3. They compare the performance of their method with three other methods based on Bayesian approaches."
SP:29d1f6d0661a51e56c59bbb106da56700fc22d9a,"This paper proposes a Bayesian nonparametric framework for federated learning with neural networks. Each data server is assumed to train local neural network weights, which are modeled through the framework. Then, an inference approach is developed to synthesize a more expressive global network without additional supervision or data pooling. The proposed framework is validated on two popular image classification datasets, MNIST and CIFAR-10."
SP:ab1f2bd216635d63450688866c729a501bd7e9d0,"This paper proposes Stable Opponent Shaping ( SOS ), a new method that interpolates between LOLA and a stable variant named LookAhead. The paper claims that SOS inherits the theoretical guarantees of the multi - agent GAN ( GAN ) while also shaping the learning of opponents and consistently either matching or outperforming LOLA. In the experiments, the authors show that SOS plays tit - for - tat in the IPD on par with LOLA while all other methods mostly defect. They also implement a more involved GAN setup, testing for mode collapse and mode hopping when learning Gaussian mixture distributions."
SP:bdafb5fca09a775a8c92d2826d5dc977d28091c2,"This paper proposes an alarm system that sets off an alarm when the segmentation result of a medical segmentation algorithm is not good enough. The method uses a Variational Auto - Encoder ( VAE ) that is trained using only the ground truth masks, therefore the bad segmentation results with bad shapes become the rare events for VAE and will result in large value in the loss function. The paper also proposes a low - dimensional feature space that is used to learn classifiers and regressors in the feature space to predict the qualities of segmentation. The shape feature is captured using the value of loss function when segmentation is tested using VAE.   The paper compares the proposed alarm system on several segmentation algorithms that perform differently on different datasets, but the system consistently provides reliable prediction on the qualities. The main contribution of the paper is to develop a feature space using shape feature which is a strong prior information shared among different data, so it is capable to predict good and bad segmentations given different segmentsation algorithms on different dataset."
SP:60738395d9efe2b3fe3a00c542ebb4261e54386c,"This paper proposes a simple image compression method based on deep neural networks called the "" deep decoder "". The decoder is a simple architecture with no convolutions and fewer weight parameters than the output dimensionality, which is used to compress images into a concise set of network weights, which can then be used for inverse problems such as denoising and inpainting. The proposed method is simple in the sense that each layer has an identical structure that consists of only one upsampling unit, pixel - wise linear combination of channels, ReLU activation, and channelwise normalization. This simplicity makes the network amenable to theoretical analysis and it sheds light on aspects of neural networks that enable them to be effective signal representations.   The main contributions of this paper are the following :   1. Introducing a simple deep neural network that can generate natural images from very few weight parameters. 2. Demonstrating the performance of the proposed method, the deep decoders, on a set of inverse problems, denoise, inpainte, and reconstruction from few and noisy measurements. 3. A theoretical analysis of how the network's architecture and training procedure can improve the performance. 4. A theory and explanation on what makes the deep network different from other methods."
SP:1c9bad3bd4d670172f65aa0304e9837ecafc6b3d,"This paper presents an end - to - end neural network architecture for program synthesis from natural language ( NL ) programs. The proposed method, called SAPS, is trained using a pretrained word embedding and a pre - trained language embedding, and is trained on abstract syntax trees, combined with a bi - directional multi - layer LSTM for processing of word sequences. The decoder features a doubly - recurrent L STM, for which the authors propose novel signal propagation schemes and soft attention mechanism. SAPS performs on par with or better than the method proposed in a previous study, and it does not require post - processing of the resulting programs."
SP:d2ec231bb6153a303e5110e671dea14c2721e636,"This paper presents a method for evaluating the robustness of deep neural networks against adversarial perturbations on the MNIST dataset. The authors show that even the state - of - the - art Madry et al's L∞ defense has lower L0 robustness than undefended networks and is still highly susceptible to L2 perturbation, ( 2 ) performs not much better than simple input binarization, and ( 4 ) features perturbed features that make little sense to humans. They present a novel robust classification model that performs analysis by synthesis using learned learned class-conditional data distributions. They also develop a new attack that exploits the structure of their defended model by devising a novel decision - based attack that seeks to minimize the number of perturbed pixels ( L0 ). The results suggest that our approach yields higher state-of - the-art robustness on MNIST against L0 and L2 and that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class. They will release the model architecture and trained weights as a friendly invitation to fellow researchers to evaluate their model independently."
SP:91a24e7f4b952c37441feab4a7e8555014c856a4,"This paper proposes a new framework for training GANs, motivated by recent results that controlling spectra of weight matrices in discriminator can improve training of GAN. Specifically, the authors propose a reparameterization approach for the weight matrix of the discriminator in GAN, which allows them to directly manipulate the spectra through various regularizers and constraints, without intensively computing singular value decompositions. The proposed method is capable of generating images with competitive quality by utilizing spectral normalization and encouraging the slow singular value decay decay. The experiments on CIFAR-10, STL - 10, and Imagenet confirm that compared to other methods, this proposed method, while not as good as some of the other methods ( e.g., GAN - based methods ), is able to generate images with comparable quality.   The main contributions of the paper are as follows : ( 1 ) Motivated by recent works ( Brock et al., 2016 ; Miyato et. al.., 2018 ) that suggest that controlling Spectra of Weight matrices of discriminators can improve GAN training, and ( 2 ) that the spectrum control improves the generalization ability of the proposed method."
SP:8115fd9b681198d62100c36794926fb57dc0a4f5,"This paper introduces the Anderson accelerated value iteration ( A2VI ) method and applies it to value iteration in reinforcement learning. The main contribution of this paper is the introduction of the Anderson acceleration technique into the value iteration, which can be viewed as an approximation of the policy evaluation by interpolating on historical data, which is a classical approximate method for policy evaluation. The authors further apply their method to the Deep Q - learning algorithm, resulting in the Deep Anderson Accelerated Q - Learning ( DA2Q ) algorithm. They give theoretical analysis of their algorithm and conduct experiments on both toy problems and Atari games. Both the theoretical analysis and empirical results show the effectiveness of our algorithm."
SP:bd79b0c0af778a36008a0c0cf2fb6393fd2789d4,"This paper proposes a novel method, SupportNet, to solve the catastrophic forgetting problem in the class incremental learning scenario. The proposed method combines the strength of deep learning and support vector machine ( SVM ), where SVM is used to identify the support data from the old data, which are fed to the deep learning model together with the new data for further training so that the model can review the essential information of the former data when learning the new information. Two powerful consolidation regularizers are applied to stabilize the learned representation and ensure the robustness of the learned model. The authors validate their method with comprehensive experiments on various tasks, which show that SupportNet drastically outperforms the state - of - the - art incremental learning methods and even reaches similar performance as the deep deep learning method."
SP:d228d213f79716774043cea253305fecece659ec,"This paper analyzes four measures of unit selectivity in neural network ( NNs ), namely precision ( Bowers et al., 2014 ), CCMAS, class - conditional mean activity selectivity ( CCMAS ), and a new measure called top - class selectivity Morcos et al, 2018, and compare with previous work on recurrent neural networks ( RNNs ). They show that the precision and CCMAS measures provide a much higher level of selectivity than is warranted, with the most selective hidden units only responding strongly to a small minority of images from within a category. They also compare these selectivity measures to a state - of - the - art activation maximization ( AM ) method for visualizing single - unit representations in CNNs ( Nguyen et al., 2017 ). AM images are generated to strongly activate individual units, and some of them are interpretable by humans. For the first time, they evaluate the interpretability of the AM images in an on - line experiment and compare these ratings with the selectivity ratings for corresponding units in the corresponding CNN images. The results show that hidden units with interpretable AM images, but not with AM images with corresponding interpretable CNN images are not highly selective. Furthermore, the interpretable images in the hidden layers were not associated with highly selective units. These findings highlight the problem with current selectivity metrics and show that new measures are required in order to provide a better assessment of learned representations in NNs."
SP:b9deae0392e0160b400d76c549d382e235196f8c,"This paper presents a novel family of Graph Neural Networks ( GNNs ) for solving community detection problems in a supervised learning setting. The authors first compare the performance of GNN with the state - of - the - art belief propagation algorithm on binary and multiclass stochastic block models. They show that GNN can match or even surpass the performance ( in terms of the signal - to - noise ratio ) of belief propagation on binary block models, which is believed to reach the computational threshold in these cases. They also provide an upper bound on the energy gap controlling the energy difference between the local and global minima ( or minimum ), which they assume is benign on large enough graphs.   The authors then provide an analysis of the optimization landscape of using ( linear ) GNNS to solve community detection problem, showing that under certain simplifications and assumptions, the loss value at any local minimum is close to the global minimum /minima under certain assumptions. In particular, the authors propose to augment GNNWith with the non - backtracking operator operator defined on the line graph of edge of adjacencies defined in the paper. The analysis is based on the spectral methods or posterior inference under certain probabilistic graphical models."
SP:a9ed31090e55f6152fc31c7512af5d634cc7225a,"This paper proposes a dictionary learning algorithm that recovers both the dictionary and the coefficients of a linear combination of a few columns of a matrix. The problem is formulated as follows : given a set of data, one can estimate the sparse weights forming the linear combination, which are known as coefficients, from a dictionary. The linear combination is then used to learn a linear model based on the dictionary. However, the dictionary is not provably recoverable, which means that the corresponding optimization is inherently non - convex. This was a major challenge until recently, when provable algorithms for dictionary learning were proposed. These provide guarantees only on the recovery of the dictionary, without explicit recovery guarantees on the coefficients. Moreover, any estimation error in the dictionary adversely impacts the ability to successfully localize and estimate the coefficient. This potentially limits the utility of existing provable dictionary learning methods in applications where coefficient recovery is of interest. To this end, the authors develop NOODL : a simple Neurally plausible alternating Optimization - based Online Dictionary Learning algorithm, which recovers both dictionary and coefficients exactly at a geometric rate, when initialized appropriately. The proposed algorithm is also amenable for large scale distributed implementations in neural architectures, by which they mean that it only involves simple linear and non - linear operations. Finally, they corroborate these theoretical results via experimental evaluation of the proposed algorithm."
SP:85232b72a2643d6dc81cf952ccbb95192032b7c5,"This paper presents a novel method for learning binary hash codes with any differentiable model and similarity function. The authors propose to use log likelihood loss on top of an accurate approximation for the probability that two inputs fall within a Hamming distance target. Their training scheme obtains a good estimate of the true gradient by better sampling inputs and evaluating loss terms between all pairs of inputs in each minibatch. To fully leverage the resulting hashes, they use multi - indexing. They demonstrate that these techniques provide large improvements to a similarity search task. They report the best results to date on competitive information retrieval tasks for ImageNet and SIFT 1M, improving MAP from 73% to 85% and reducing query cost by a factor of 2-8, respectively."
SP:3bd4ccff7f48380d2db8dff2c4ca515894a7f1db,"This paper presents Graph HyperNetwork ( GHN ), a method for finding the best neural network topology using neural network architecture search ( NAS ). NAS is a method that automatically finds the best task - specific neural network that outperforms many manual architecture designs. However, it is expensive as the search requires training thousands of different networks, while each can last for hours. In this work, the authors propose GHN to amortize the search cost : given an architecture, it directly generates the weights by running inference on a graph neural network and therefore can predict the topology of an architecture more accurately than regular hypernetworks and premature early stopping. To perform NAS, they randomly sample architectures and use validation accuracy of networks with GHN generated weights as the surrogate search signal. GHN is fast – they can search nearly 10 times faster than other random search methods on CIFAR-10 and ImageNet."
SP:65ccf43cd4e033d22239069057f5200d49f33724,"This paper proposes a method to leverage non - expert demonstrations to improve generative adversarial imitation learning ( GAIL ). The key idea of the paper is to perform multiclass classification to learn discriminator functions where the expert demonstrations are regarded as being drawn from an extra class. The authors show that their method learns better policies than GAIL when the number of expert demonstrations is small compared to GAIL on continuous control tasks. They also show that the minimax formulation commonly used in GAIL does not guarantee the optimality of policies for their method, and propose a modified optimization procedure which provides such a guarantee.    The main contributions of this paper are as follows :   1 ) The authors propose a method that leverages both expert and non expert demonstrations in the discriminator learning objective, and this leads to a better feature representation of discriminator function. 2 ) They show that mix non expert and expert demonstrations with expert demonstrations or agent ’s trajectories only learn a mixture policy and does not learn the expert policy. 3 ) Their method is shown to outperform GAIL especially when only a small number of agent demonstrations is available."
SP:e8427949a98effbd37ce7604fa11f240e2342196,"This paper proposes a new type of neural network, the Invertible Neural Networks ( INNs ), for the inverse problem of finding hidden parameters from a set of measurements. In contrast to classical neural networks, INNs focus on learning the forward process, using additional latent output variables to capture the information otherwise lost. The authors argue that INNs are a powerful analysis tool to find multi - modalities in parameter space, uncover parameter correlations, and identify unrecoverable parameters.   The main contributions of the paper are as follows :   1. An analysis of the distribution of the latent variables in the INN network, conditioned on an observed measurement. This is done by learning the inverse process using the learned forward process and the latent variable distribution. 2. A combination of forward training and unsupervised backward training is used to learn the inverse through Bayesian computation ( ABC ) and conditional VAEs. This enables identifying parameter correlations and multimodalities. 3. In the experiments, the authors show that the INNs outperform other neural networks in terms of accuracy on a small number of experiments."
SP:75c9bb53bac29bdb390f9ba5707caee4ab1f5925,"This paper proposes a new method for quantifying the uncertainty of deep neural networks ( NNs ). It builds on a previous work that showed that using an ensemble of NNs trained with a proper scoring rule leads to results competitive to those of Bayesian NNs. This ensemble method can be understood as finite mixture model with uniform mixing weights. This paper proposes to replace the fixed mixing weights by an adaptive, input - dependent distribution represented by an NN, and by considering uncountably many mixture components. The resulting model can be seen as the continuous counterpart to mixture density networks and is therefore referred to as compound density networks. The authors empirically show that the proposed model results in better uncertainty estimates and is more robust to adversarial examples than previous approaches."
SP:e1e38289285c1b8fdb318e4f6d37a198a08787a2,"This paper proposes a compression method for training deep neural networks based on the KullbackLeibler divergence between the sampled variational distribution and the encoding distribution. The authors argue that relaxing weight determinism allows for more efficient coding schemes and consequently higher compression rates. In particular, following the classical bits - back argument, the authors encode the network weights using a random sample, requiring only a number of bits corresponding to the divergence of the distribution of the variational and encoding distributions. The proposed method is evaluated on the benchmarks LeNet-5/MNIST and VGG-16/CIFAR-10 and yields the best test performance for a fixed memory budget, while optimizing the expected loss on the training set."
SP:ad70d8cf3a4558aab0d3b7155594464a3debd912,"This paper presents a new method for neural architecture search ( NAS ) that directly learns the architectures for large - scale target tasks ( e.g. ImageNet ) and hardware with direct hardware metrics ( i.e., latency ) instead of with pretrained models. The proposed method, called ProxylessNAS, is inspired by recent works ( Liu et al., 2018 ) and inspired by the following ways : 1 ) remove the restriction of repeating blocks in previous NAS works and allow all of the learned and specified blocks to be used in the search. 2 ) remove all restrictions on the size of the candidate set and the number of parameters in each block. 3 ) provide insights for efficient CNN architecture design for hardware metrics such as latency.   Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. Benefiting from previous proxy - based approaches, Proxyless NAS achieves better results than previous proxy-based approaches, with 200x fewer GPU hours training compared to MnasNet, with the same accuracy level of top - 1 accuracy while being 1.4x faster. The paper also provides insights on how to improve the performance of CNNs on mobile devices."
SP:e5b70d43d301d1980fae02623ea711976b429c14,"This paper considers the problem of penalizing players in two - player min - max games, where the objective is to train a classifier that is fair to all players in both the first and second - order versions of the same game. The paper proposes to replace the linear penalties in the Lagrangian dual problem with additive penalties in order to avoid instability and instability in non - convex settings where the two players do not play each other. The authors argue that the use of second order penalties allows training the penalized objective with a fixed value of the penalty coefficient, thus avoiding instability and potential lack of convergence associated with two - players min - min games. They also derive a method for efficiently computing the gradients associated with the second - ordered penalties in stochastic mini - batch settings.   The authors evaluate the performance of their algorithm in a number of different settings, and show that their algorithm is able to adequately optimize the desired constraints, such as encouraging feature orthonormality in deep image autoencoders and imposing predictive fairness across protected data groups."
SP:e4720b8e4efdb222c45eafd47fd8a7fbf15d881d,"This paper studies the problem of learning deep generative models with discrete latent variables. The authors focus on the reweighted wake - up - sleep ( RWS ) algorithm and evaluate it against several state - of - the - art methods, including control - variable based methods and continuous - relaxation based methods. They show that RWS is a competitive alternative to these methods and outperforms them on a number of benchmarks. They also provide a pedagogical example of a weakness in RWS that can be fixed by using defensive importance sampling."
SP:7459ae5b1d886e68930c4c9e21df508bc8ab3c9a,"This paper presents a method for training structured prediction energy networks ( SPENs ) based on a scalar reward function, which is typically used to evaluate predictions when the true output of the network is unknown. The paper proposes to use an efficient truncated randomized search in this reward function to train structured prediction networks, which provide efficient test - time inference using gradient - based search on a smooth, learned representation of the score landscape, and have previously yielded state - of - the - art results in structured prediction tasks. The authors claim that this method yields previously unknown local improvements, providing effective supervision to SPENS, and avoiding their traditional need for labeled training data."
SP:638c1bc09992029b78bd83f0127594dcccb96c06,"This paper proposes an active learning based framework, EffAcTS, to selectively choose model parameters for this purpose so as to collect only as much data as necessary to select such a subset of trajectories for active learning. The authors apply this framework to an existing method, namely EPOpt, and experimentally validate the gains in sample efficiency and the performance of their approach on standard continuous control tasks. They also explore connections to Multi - Task Learning that are revealed upon casting Robust Policy Search as a Multi -Task Learning problem and discuss its relation to existing work in the area."
SP:491c239713a6489f0b1790ca26db54a1813c67ae,"This paper proposes a nonlinear temporal difference learning ( TTN ) architecture for nonlinear value function approximations that is compatible with linear algorithms such as least - squares ( or batch ) methods, data - efficient least - squared methods, eligibility traces, and the recently developed linear policy evaluation algorithms, to provide non - linear value estimates without the need for a fixed basis or fixed representation. The authors show that the convergence of the proposed method, TTN, is equivalent to that of the linear algorithms developed for the linear setting. They also show that TTN can be used to leverage the benefits of linear algorithms while retaining the flexibility of nonlinear functions. They empirically demonstrate that TTNs can be effective for control with neural networks, enabling the use of fitted Q - iteration within TTN as an alternative to target networks."
SP:327d606cf3813b00a009a7785e08ef9e11f89493,"This paper proposes LEArning and Planning with Semantics ( LEAPS ), a method for planning and planning with a learned semantic model in man - made environments that are visually diverse but contain intrinsic semantic regularities. The approach consists of a multi - target sub - policy that acts on visual inputs and a Bayesian model over semantic structures, and a semantic model placed in an unseen environment, where the agent first plans with the semantic model to make high - level decisions, proposes the next sub - target for the sub -policy to execute, and updates the model based on new observations. The paper performs experiments in visual navigation tasks using House3D, a 3D environment that contains diverse human - designed indoor scenes with real - world objects. The experiments show that LEAPS outperforms the state - of - the - art model - free approaches, even when the semantic signals are not given as input to the policy, and that the relative improvements of LEAPS over baselines become more significant when the targets are further away from the agent."
SP:d7c26f43bc68d160095b1f50447528843d79edbd,"This paper proposes a new method to tackle the problem of generalization and accident explanation in deep learning for driving models. The authors claim that the current deep learning models fail to generalize well to unobserved driving environments due to lack of diversity of training data and lack of accident explanation. To tackle these two problems, the authors propose a method that consists of two modules : perception module and driving module. The perception module is used for learning easier driving - related perception knowledge which is referred to as ability of pixel level understanding of input including what & where and how far knowledge. The module is trained with segmentation map and depth map first while the former serves as what & what where knowledge and the latter serves as how far information. Then, after the perception module was trained, the weights of the module are used to train the driving module with driving dataset. Experiments show that the proposed method is effective in improving generalization of the proposed model.   The main contributions of the paper are as follows :   1. Developing a method to train a module for learning easy - to - understand perception of easy driving tasks while training the module for difficult driving tasks. This is based on the belief that knowledge of associated easy task is benificial for addressing difficult task, and trained it with multi - task perception - related basic knowledge and driving knowledge stepwisely. 2. Training the module to generate final control commands for difficult task before training the model for the difficult driving task. 3. Demonstrating the effectiveness of multitask perception knowledge for better generalization."
SP:b6bd98cc70fab97e1245cbb63a42ef89ab7e7ed5,"This paper studies the relationship between adversarial robustness and standard generalization in the context of generative adversarial networks ( GANs ). The authors show that there is a trade - off between the standard accuracy of a model and its robustness to adversarial perturbations, and argue that training robust models may lead to a reduction in standard accuracy. They also show that adversarially robust classifiers learn fundamentally different feature representations than standard classifiers. However, they find that the features learned by robust models tend to align better with salient data characteristics and human perception. The feature embeddings learnt by the robust models yield also clean inter - class interpolations, similar to those found byGANs and other generative models."
SP:9c9275d75cd95b1b82e0cbb1421e3d3ade1ce33a,"This paper proposes a new method for training deep neural networks without backpropagation. The proposed method is based on the Equilibrium Propagation ( EPR ) method proposed by Scellier & Bengio ( 2017 ), which is a method for gradient - based training of neural networks using only local learning rules and does not rely on neurons having a mechanism for back - propagating an error gradient. Equilibrium propagation, however, has a major practical limitation : inference involves doing an iterative optimization of neural activations to find a fixed - point, and the number of steps required to closely approximate this fixed point scales poorly with the depth of the network. In response to this problem, this paper proposes Initialized Equilibrium - Propagated Feed - Forward Network, which uses a feed - forward network to initialize a neural network for inference, resulting in a learned feedforward network. Experiments show that this network appears to work as well or better than the original version of EPR which requires fewer steps to converge."
SP:ac9ea91eb465517de495477cf67bc94d5ed1b0cb,"This paper studies the convergence rate of ZO - signSGD, a new algorithm for stochastic optimization ( ZO ) based on gradient estimators. SignSGD is a special case of SGD in that it is gradient - free and uses gradient estimates to estimate the sign of the gradient of the solution. The authors compare the convergence of the new algorithm with SGD and SGD - based methods in terms of the rate of convergence of O( d / T ), where d is the number of optimization variables and T is number of iterations in the algorithm. The convergence rate is obtained under some mild conditions where d and T are the optimization variables in the case of convex and nonconvex optimization, respectively.   The authors also analyze the effect of different types of gradient estimates on the convergence rates of the ZO algorithm. In particular, the authors show that the gradient estimator used for the sign estimator is more sensitive to the gradient sign than the one used for SGD. They also propose several variants of their algorithm, one of which is called ZO-signSGD with O(\sqrt{d / T } ) convergence rate. The experiments focus on generating adversarial examples of black - box adversarial attacks in MNIST and CIFAR-10. They compare the performance of the proposed algorithm with other ZO algorithms on the generation of these examples."
SP:5f79b11777f6ef1d70c85418bfc2e4616dd7d960,"This paper proposes a method to reduce the computation efforts of convolutional neural networks. The method takes advantage of the fact that some convolutionsal operations are actually wasteful since their outputs are pruned by the following activation or pooling layers : 1 ) A filter conducts a series of multiply - accumulate ( MAC ) operations, 2 ) A checkpoint is set in the MAC process to determine whether a filter could terminate early based on the intermediate result, 3 ) A fine - tuning process is conducted to recover the accuracy drop due to the applied checkpoints. The experimental results show that the proposed method can save approximately 50% MAC operations with less than 1% accuracy drop for CIFAR-10 example and CifAR-100 dataset."
SP:7801e9c854ad7d960c0d24fda15597af6994c23f,"This paper presents a novel approach for mitigating adversarial inputs to the automatic speech recognition ( ASR ) models. The authors focus on two types of attacks for generating audio adversarial examples : Speech - to - Label attack and Speech - To - Text attack. The first attack is based on a genetic algorithm ( Alzantot et al., 2018 ) and the second one is a probabilistic loss function ( Cisse et al, 2017 ).   The authors show that ( i ) input transformation developed from image - based adversarial defense provides limited robustness improvement and is subtle to advanced attacks ; ( ii ) temporal dependency can be exploited to gain discriminative power against adversarial example and is resistant to adaptive attacks considered in the experiments considered in their experiments. The results show promising means of improving the robustness of ASR systems but also offer novel insights in exploiting domain - specific data properties to mitigate the negative effects of adversarial input. In particular, the results reveal the importance of using the temporal dependency in audio data to gain discriminate power against adversary examples."
SP:51830b811a8e39b4f0a5b7609df719e026fac6a1,"This paper proposes a new generative model for generating multi - object images based on the composition of individual objects and their relation to background and background background to generate images. The key idea is to learn the relation between each of the individual objects in an image by composing them with the background and then generating a set of images that are representative of the relationship between the objects and the background. The method is based on Wasserstein and Non - Saturating ( WNS ), which are two well - established generative models. The authors compare the proposed method with WNS and a baseline of popular GANs and show that the generated images are more faithful to the reference distribution. They also show that their proposed method outperforms the baseline in generating better images."
SP:fb59990b8da0e95d8202383478a456667de60449,"This paper proposes a supervised learning method for learning disentangled representations from unlabeled images, where the only source of supervision is an auxiliary reference set of images that contains all the factors of interest in the training set. The proposed method, called reference - based variational autoencoders, is a deep generative model designed to exploit the weak supervisory signal provided by the reference set. In the experiments, the authors show that the proposed method is able to naturally address three tasks such as feature learning, conditional image generation, and attribute transfer.   The main contributions of the paper are as follows :   1. The introduction of a learning setting called “ reference based disentangling ” where the goal is to learn a representation where a set of target factors are disenangled from others. The authors propose a variational inference framework where adversarial learning is used to minimize the objective of addressing the learning objective. 2. The use of the VAR in the learning setting allows the authors to test the effectiveness of their method on three tasks : Feature Learning, conditional Image Generation, and Attribute Transfer. 3. The experiments evaluate the performance of the method on image classification, feature learning and image generation."
SP:dbc1983d9b9d72aa14f8e8515d793d2bbde26c9c,"This paper proposes a method for continual online learning from an incoming stream of data, using deep neural network models. The proposed method, called MOLe, is based on a meta - RL approach to model - based reinforcement learning, where models are instantiated for task changes and old models are recalled when previously seen tasks are encountered again. The authors combine an online learning procedure that uses stochastic gradient descent to update model parameters, and an expectation maximization algorithm with a Chinese restaurant process prior to the task distribution to develop and maintain a mixture of models to handle non - stationary task distributions. This allows for all models to be adapted as necessary, with new models instantiated with new tasks encountered, and existing models recalled once the new task is encountered. The method is evaluated on a suite of challenging simulated robotic tasks including disturbances, environmental changes, and simulated motor failures. It is shown that MOLe outperforms a state - of - the - art prior method, as well as natural baselines such as continuous gradient updates for adaptation and online learning without meta - training. Furthermore, the authors observe that meta - learning can be used to meta - train a model such that this direct online adaptation with SGD is effective, which is not the case otherwise."
SP:5665e5f006f84927beb0440e145f476e02538077,"This paper presents a method for training distributed RL agents with experience replay distributed DQN ( RNN ). The method is based on two main contributions. First, the authors study the effect of parameter lag and recurrent state staleness on the training of RNNs. They show that parameter lag leads to representational drift and recurrent staleness, which are exacerbated by the presence of experience replay. Second, they show that using a fixed set of hyperparameters, they can train an agent that achieves state - of - the - art performance on Atari - 57, and matches the state of the art on DMLab - 30.   The second contribution is an empirical study into the effects of the network architecture used to train the agent. The authors show that the fixed network architecture leads to a reduction in the number of parameters used by the agent, which leads to an increase in the performance. They also show that training with a fixed network allows the agent to achieve better performance than using a single network architecture."
SP:47ace37f31a46d5ee85c283e62ddb71a12f2c5c4,"This paper studies the problem of training sequential generative models for capturing coordinated multi - agent trajectory behavior, such as offensive basketball gameplay. It is often beneficial to design hierarchical models that can capture long - term coordination using intermediate variables, and these intermediate variables should capture interesting high - level behavioral semantics in an interpretable and manipulatable way. The authors present a hierarchical framework that can effectively learn such sequential models, inspired by recent work on leveraging programmatically produced weak labels, which we extend to the spatiotemporal regime. In addition to synthetic settings, the authors show how to instantiate their framework to effectively model complex interactions between basketball players and generate realistic multi -agent trajectories of basketball gameplay over long time periods. They validate their approach using both quantitative and qualitative evaluations, including a user study comparison conducted with professional sports analysts, and show significant improvements over standard baselines."
SP:1a90cdf028068528b0559e7d44bf26dda20310bd,"Graph - VRNN is an end - to - end state estimation method that learns to integrate information from a learned dynamics model with ambiguous visual information, in the context of interacting agents. It is based on a graph - structured variational recurrent neural network, which is trained end to end to infer the current state of the ( partially observed ) world, as well as to forecast future states. The authors show that their method outperforms various baselines on two sports datasets, one based on real basketball trajectories, and one generated by a soccer game engine. In summary, the main contribution is a unified way to do state estimation and future forecasting at the level of objects and relations directly from pixels directly from Graph -VRNN."
SP:8392f04b7265f665ba6d44d297bca245d44b4708,"This paper proposes a method for end - to - end training of a base neural network that integrates calls to existing blackbox functions. The method approximates the blackbox functionality with a differentiable neural network in a way that drives the base network to comply with the black - box function interface during the end -to - end optimization process. At inference time, we replace the differentiable estimator with its external blackbox counterpart. The experimental results show that the integrated model generalizes better than a fully differentiable model, and learns more efficiently compared to RL - based methods."
SP:13fb86de763a0b34ac6fa34ea9dfbd1c476ce43e,"This paper proposes a method for learning - to - learn ( LTL ) based on meta - learning, where the goal is to learn a model based on data - driven inductive bias rather than a parametric one. The authors propose a method based on Generalizing the Model - Agnostic Metal - Earning algorithm ( MAML ) to jointly estimate parameter initializations for gradient descent as well as a latent assignment of tasks to initializations. This approach better captures the diversity of training tasks as opposed to consolidating inductive biases into a single set of hyperparameters, the authors argue. They also propose to extend their latent variable model to the non - parametric setting and leverage stochastic point estimation for scalable inference in a Dirichlet process mixture model ( DPMM ). The experiments demonstrate better performance on the standard benchmark for 1 - shot classification on a set of few - shot regression tasks."
SP:a410144dbe19713a06c63da87d9fb58b999a7492,"This paper proposes Meta Auxiliary Learning ( MAXL ), a method to automatically learn auxiliary tasks based on sub - class labels generated from the primary task. The primary task is image classification, and the auxiliary tasks are generated by training a model on the principal task and then using the labels generated by the model to select auxiliary tasks that are similar enough to the principal that they are equivalent to each other and can be learned by a single agent. MAXL is trained on three image classification datasets, CIFAR-10, Cifar-100, and ImageNet, and compared with two baselines, one based on domain knowledge and another based on human knowledge. The authors claim that MAXL outperforms the other baselines in terms of generalisation and scalability, and that the method is scalable and general enough to be applied to other tasks."
SP:76248e1c914c60ce69de244fe7ec62488d01e161,This paper presents a neural network based representation for addressing the open set recognition problem. The main idea is to learn a representation that allows instances from the same class to be close to each other and instances from different classes to be further apart. The paper presents three approaches for training the representation. The first approach is to use a loss function that enables to use the same distance function both when training and when an outlier score is used. The second approach is an approach for learning a representation for learning the distance function. The third approach is a combination of the first two approaches.   The paper compares the proposed approaches on three datasets from two different domains. The results show statistically significant improvement when compared to other approaches on two datasets.
SP:d4ee856bbf2dfb6390e5247086fec2e52dcb6858,"This paper studies the problem of energy and area efficiency of embedded deep convolutional neural networks in large - scale applications. The authors propose two approaches to reduce the solution distance by starting with pretrained fp32 precision baseline networks and fine - tuning, and find that the noise due to quantization during training during training increases with reduced precision, and seek ways to overcome this noise with stochastic gradient descent. They also demonstrate ResNet - 18, ResNet-34, and 4 - bit models that match the accuracy of the full - precision networks – the highest scores to date – on the ImageNet classification benchmark. Surprisingly, the weights of the low - precision network are very close ( at 8 - bits ) to the corresponding weights of corresponding baseline networks, making training from scratch unnecessary.    The authors find that both gradient noise and the number of iterations required to overcome noise increases with the reduction in precision. They show that ResNet152, VGG16bn, and VGG-16bn with 8 - bit precision can match or exceed accuracy of full precision networks after finetuning."
SP:6bfdc37b346e6ddfa049e0414647f4beda8ede3f,"This paper presents an end - to - end model for predicting post - bounce trajectories of real - world collision trajectories based on physics - based methods. The model consists of two modules : a Physics Inference Module ( PIM ) and a Visual Inference module ( VIM ). The PIM learns to model physical parameters for locations in a scene given a single still image, while the VIM is trained to model the model physical interactions for the prediction task given physical parameters and observed pre - collision 3D trajectories. The authors also introduce a dataset of 5K videos of bouncing trajectories from real world collision scenarios. They show that their model outperforms the state - of - the - art in terms of trajectory prediction and physics based methods for bouncing restitution and collision normals."
SP:010bd055310c363d3cb0fbe0e11546de58220e15,"This paper analyzes the relationship between adversarial vulnerability and the gradient norm of the training objective in neural networks. Specifically, the authors show that the network's gradient norm increases as the square root of the input size increases, and that the adversarial sensitivity of the network increases with the larger the input dimension. The authors provide theoretical and empirical support for their theoretical results by extensive experiments.    The main contributions of the paper are as follows :   1. The first order analysis of the gradients of neural networks based on the weight statistics at the network ’s weight distribution at initialization. This analysis is based on two main components : ( 1 ) the weight distribution of the output dimension of the neural network and ( 2 ) the distance between the output and the output of the classifier. The second part of the analysis uses the first - order analysis to show that there is a monotonic relationship between the gradient of the objective and the network’s vulnerability to adversarial noise. This relationship is shown to be monotonically increasing as the size of the inputs increases. This observation is supported by experiments on MNIST, CIFAR-10, and Fashion - MNIST. The experiments confirm that the results of this paper hold for the majority of the networks tested. 2. The paper also provides theoretical support for the existence of the monotonicity of the relationship by showing that the number of gradients in the network decreases as the image dimension increases. 3. The conclusion is empirically supported by a series of experiments on a variety of image classification tasks."
SP:5fa3ae057e55be6b71cc94a7dbfe31e54e1c536f,"This paper presents an agent modeling approach that encourages agents to interact with the environment and a target agent to maximize the change in the observed behaviors of that agent. The probing agent is trained using imitation learning and pure curiosity - driven reinforcement learning. The learning process consists of two learning processes : i ) imitation learning for an approximated agent model and ii ) pure curiosity and reinforcement learning to discover new behaviors that otherwise can not be observed. The experimental results suggest that the agent learned by our approach i ) generalizes better in novel scenarios than the ones learned by passive observation, random probing, and other curiosity driven approaches do, and ii) can be used for enhancing performance in multiple applications including distilling optimal planning to a policy net, collaboration, and competition."
SP:3af184a5529d6ec2a0862efd1af80ef5b50d2952,"This paper proposes a modification to the activation function of artificial neural networks ( ANNs ) inspired by biological neuromodulators ( NUs ) to enable them to adjust their activation in run - time based on their input patterns. Specifically, the authors propose modulators, which mimic the function of biological NUs and enable the slope of activation function to be context dependent. This modification produces statistically significant improvements in comparison with traditional ANN nodes in the context of Convolutional Neural Networks and Long Short - term Memory networks. Experimentation shows that the modulator mechanism in the proposed method works better than the traditional activation function.    The main contributions of the paper are as follows :   1. The authors introduce a new type of ANN nodes, which are termed modulators. The idea is to enable the activation of the ANN nodes to be adjusted based on the input patterns of other ANN nodes. This is in contrast to traditional ANNs which are activated based on a fixed set of parameters, such as the number of neurons and the order of connections. 2. The modulators are connected to each other via a shared pool of neurons. 3. The output of the modulators is shared across all the other ANNs. Experiments show that the proposed modulator method outperforms the other methods in terms of test stability and test accuracy."
SP:287a577834fd2820a939a1113b39146a22727491,"This paper presents a neural analysis and synthesis ( NANSY ) framework that can manipulate voice, pitch, and speed of an arbitrary speech signal. The main contribution is a novel training strategy based on information perturbation. The idea is to perturb information in the original input signal ( e.g., formant, pitch, and frequency response ) thereby letting synthesis networks selectively take essential attributes to reconstruct the input signal. Because NansY does not need any bottleneck structures, it enjoys both high reconstruction quality and controllability. The experiments show that NANSY can achieve significant improvement in performance in several applications such as zero - shot voice conversion, pitch shift, and time - scale modification. The authors also propose a new set of analysis features, i.e., pitch feature and Yingram, which allows for fully self - supervised training."
SP:90f35ad1ec0c38b0817f5678ee2a5c4f0e08fb38,This paper presents a theoretical analysis of the generalization properties of the gradient - based bilevel programming framework. The main contribution of the paper is an expectation bound on the stability of the validation set of the classical cross - validation algorithm based on the assumption of uniform stability. The authors show that the gradient based algorithms can be better than cross - validation algorithms under certain conditions. They also show that regularization terms in both the outer and inner terms can help alleviate the overfitting problem of gradient based methods. The paper also presents experiments on feature learning and data reweighting for noisy labels to corroborate their theoretical findings.
SP:42f52aec3a776d87daa5fd72b8e6325d12c88d63,"This paper proposes a knowledge distillation approach to facilitate the transfer of dark knowledge from a teacher to a student. The key idea is to learn the teacher models that are friendly to students and, consequently, more appropriate for knowledge transfer. The main goal of the approach lies in training teacher models and the subsequent knowledge distilling procedure is straightforward. The experimental results demonstrate that the proposed approach can improve the performance of diverse student models in terms of accuracy and convergence speed. The proposed algorithm demonstrates outstanding accuracy in several well - known knowledge distillations techniques with various combinations of teacher and student models. In Section 5, the experimental results with in - depth analyses are presented in Section 4, and the conclusion in Section 5 is the conclusion of the paper."
SP:e15a1c21229233fd97dc1dfa0a4ef48b69dc9f95,"This paper studies the problem of generalization to out - of - distribution ( OOD ) data, which is one of the central problems in modern machine learning. Theoretical understanding of what kind of invariance can guarantee generalization of OOD generalization is still limited. In this paper, the authors introduce a new concept of expansion function, which characterizes to what extent the variance is amplified in the test domains over the training domains, and therefore give a quantitative meaning of invariant features. Based on these definitions, they prove that generalization largely depends on the expansion function and model selection. Extensive experiments on benchmark OOD datasets demonstrate that their model selection criterion has a significant advantage over baselines."
SP:37b04b9068d39bcf0a581eb8181d13cf1a8926bf,"This paper proposes a Variational Continual Bayesian Meta - Learning ( VC - ML ) algorithm that addresses the problem of catastrophic forgetting in streaming low - resource online tasks, where the tasks follow a non - stationary distribution. The proposed algorithm is based on a variational continuous Bayesian meta - learning framework, where meta - parameters are assumed to follow Gaussian distributions, and task - specific parameters follow their respective distributions. To approximate the posterior distributions of interest, the authors propose a structured variational inference method. Experiments on tasks from non - stationary distributions show that the proposed algorithm outperforms the state - of - the - art baselines on four benchmark datasets. It has been shown that the Bayesian formulation of the algorithm can alleviate negative transfer among dissimilar tasks and prevent dramatic parameter changes."
SP:776d5b02b8d3a8bbcc1f52706f3887c384cb149e,"This paper proposes a probabilistic algorithm for the problem of boundary value problems ( BVPs ), which are ordinary differential equations subject to boundary conditions. In contrast to previous work, the authors introduce a Gauss–Markov prior and tailor it specifically to BVP problems, which allows computing a posterior distribution over the solution in linear time, at a quality comparable to that of well established, non - Probabilistic methods. The proposed method is compatible with other statistical modelling tool - chain in the statistical model - chain. The main advantage of the proposed algorithm is that it is practically usable for solving BVP solvers. The authors provide three examples, all of which are depicted in Figure 1. In particular, recovering the trajectory of a pendulum between two positions amounts to solving the ODE, and recovering the position of a person in an infectious disease can be made up for by available counts of infected people at the final time - point of the integration domain. To solve the BVP problem, one needs to compute the vector field f : R → R, as well as L, as in the case of L - R, where L is the dimension of the input space.   The authors propose to solve the problem by iteratively conditionatively computing a Gaussian process on approximately “ solving ” a grid of grid points. Each iteration requires solving a generic least - squares problem of size equal to the number of employed grid points, which has a cubic complexity of $ \sqrt(t )$. The authors show that the proposed method can be solved in approximately $ t$ times faster than the state - of - the - art non - Gaussian method. The method is shown to converge to the optimal solution of the problem with high polynomial complexity, and it is also shown to be compatible with non - probabilistically compatible methods."
SP:86aac0c6b75fdc12f84bba342934865616f866d4,"This paper considers the problem of learning a near optimal policy for two reward - mixing mixing - mixing MDPs ( RM - MDP ), where a reward function is drawn from one of multiple possible reward models at the beginning of every episode, but the identity of the chosen reward model is not revealed to the agent. The latent state space, for which the dynamics are Markovian, is not given to the agents. The authors make no assumptions on the dynamics, and study the problem in full generality. They provide the first efficient algorithm that does not require any assumptions in partially observed environments where the observation space is smaller than the latent space space. This is the first work that provides efficient exploration techniques without any assumptions.    The main contribution of this paper is the exploration algorithm that finds an optimal policy in an adversarial MDP setting where only the reward model differs across different tasks. However, with no further assumptions, even for two switching reward - models, the problem requires several new ideas beyond existing algorithmic and analysis techniques for efficient exploration. The main contributions of the paper are the following :   1. The exploration algorithm finds an efficient policy after exploring 2 episodes, where the number of states and actions respectively is time - horizon and S.H.O.P.A.2. 2. The algorithm finds a policy in the POMDP2 episode, where H is the time - horizon and T is the total number of episodes. 3. The reward function of the agent is chosen randomly from the set of reward models. 4. The goal of the algorithm is to find an optimal reward function in the second episode, even if the policy is not optimal in the first episode."
SP:1a3c70ae9cf2a806d603f4b9e7ca6e10b720a956,"This paper proposes Single - Cause Perturbation ( SCP ), a method to estimate the effect of conditional single - cause interventions on the treatment effect of a single variable.   The main idea of the work is to use the multi - cause CATE estimator, which estimates the impact of interventions that affect multiple variables at the same time. However, CATE has confounding bias because it estimates the effects of interventions on a large number of variables, each with a different cause combination. To overcome the confounding bias, the paper proposes to augment observational data with the estimated potential outcomes of interventions, and then perform covariate adjustment on the augmented data to obtain the one - cause estimator. The paper shows that the proposed method is valid under the following assumptions :   1. The number of interventions is limited to one per single cause ; 2. The intervention is conditional on a set of variables ( i.e., only one outcome per variable ) ; 3. The interventions affect all the variables in the same way. 4. The treatment effect depends on the combination of the intervention and the single cause. 5. The procedure is valid with respect to the following additional assumptions : ( a ) the number of intervention variables is limited and ( b ) the interventions are conditional on one or more variables in each of the multiple variables ; and ( c ) the intervention takes place at a time when all the other variables are available for the intervention at the time of intervention ( e.g., at time t, t+1 ). The main contributions of the paper are as follows : ( 1 ) It proposes a novel method for estimating the conditional single cause treatment effect, ( 2 ) it introduces a covariate based estimator to overcome confounding bias and ( 3 ) it demonstrates the performance gain of its proposed method."
SP:247bc6675cce89d51558537daf63dadb0c4307f8,"The paper proposes a neural operator learning method based on the multi - wavelet transform, which is a compressive operator ’s kernel that compresses the associated operator ‘s kernel using fine - grained wavelets. By explicitly embedding the inverse multiwavelet filters, the proposed method is able to learn the projection of the kernel onto fixed multi wavelet polynomial bases. The projected kernel is trained at multiple scales derived from repeated computation of multi-wavelet transform. This allows learning complex dependencies at various scales and results in a resolution - independent scheme. The proposed method exploits the fundamental properties of the operator’s kernels which enable numerically efficient representation. The experiments on the Korteweg - de Vries ( KdV ) equation, Burgers ’ equation, and the Darcy flow equation show that the proposed architecture outperforms the existing neural operators."
SP:1153785e6a016cfee2644952a772aa08927299b6,"This paper proposes a new method for training binary neural networks ( BNNs ) based on frequency domain approximation ( FDA ). The authors propose to estimate the gradient of the sign function in the Fourier frequency domain using the combination of sine functions for training the neural network, namely DA - BNN. The proposed approach does not affect the low - frequency information of the original sign function which occupies most of the overall energy, and high - frequency coefficients will be ignored to avoid the huge computational overhead. In addition, the authors embed a noise adaptation module into the training phase to compensate the approximation error. The experiments on several benchmark datasets and neural architectures illustrate that the binary network learned using our method achieves state - of - the - art accuracy."
SP:33b95ea8da4d30b8e8f9d3fe3acca023d4b8d831,"This paper presents a multi - area neural network model for the Checkerboard Task, a perceptual decision - making task that requires the coordination of multiple parts of the brain. The authors propose to study the distributed computations of multiple areas of the human brain using a model based on recurrent neural networks ( RNNs ) with neuroscience - inspired architecture constraints. They show that incorporating multiple areas and incorporating Dale's Law is critical for biasing the networks to learn biologically plausible solutions. They also show that output - relevant information is preferentially propagated between areas and that the RNN ’s output area resembles PMd in terms of unit statistics and neural population activity, and that it only retains the “ output relevant ” signals. They compare their results to monkey neuron recordings from the dorsal premotor cortex ( PMd ) and find that, when incorporating Dale’s law and anatomically - informed levels of feedforward inhibition into training, PMd dynamics emerged in multi - Area RNNes. More broadly, their results suggest that constrained multi - region networks are less able to generate sufficient representations of task information."
SP:db3ced65d67e3373fb3936ec50f41c8ef010bbbe,"This paper proposes a new approach to visualize the saliency maps used in image classification. The authors argue that a single saliency map provides an incomplete understanding of the decision making process of CNNs. They argue that there are often many other maps that can explain a classification equally well. They introduce structured attention graphs ( SAGs ), which compactly represent sets of attention maps for an image by visualizing how different combinations of image regions impact the confidence of a classifier. They conduct a user study to evaluate the effectiveness of SAGs in helping users gain a deeper understanding of CNN’s decision making. They show that user accuracy is increased significantly when presented with SAGs compared to standard saliency mapping baselines."
SP:f2b385bfd9ada0e26aa8829214b424f58582d9f7,"This paper studies how the choice of training objective affects the transferability of the hidden representations of convolutional neural networks trained on ImageNet. Specifically, the authors consider the following questions :   1. Is there a trade - off between learning invariant features for the original task and features relevant for transfer tasks?   2. How do the different objectives affect the quality of the features on downstream tasks? The authors answer this question by first comparing the performance of the original neural network with the representations obtained by different objectives and then comparing these representations with the ones obtained by the proposed loss functions.   The authors find that the objectives that lead to statistically significant improvements in ImageNet accuracy over vanilla softmax cross - entropy, but the resulting fixed feature extractors transfer substantially worse to downstream tasks, and that choice of loss has little effect when networks are fully fine - tuned on the new tasks. The authors also find that differences among loss functions are apparent only in the last few layers of the network and that they collapse within - class variability in representations."
SP:b66b5e24f68563e2e200eda660f0dbaff53efeff,"This paper presents a novel neural network training strategy, selective backpropagation through time ( SBTT ), which enables learning of deep generative models of latent dynamics from data in which the set of observed variables changes at each time step. The resulting models are able to infer activity for missing samples by combining observations with learned latent dynamics. The authors test SBTT applied to sequential autoencoders and demonstrate more efficient and higher - fidelity characterization of neural population dynamics in electrophysiological and calcium imaging data. They further demonstrate that performance could be further improved by using limited, highbandwidth sampling to pretrain dynamics models, and then using SBTT to adapt these models for sparsely -sampled data."
SP:3513a83806e71006b86d60b779d8bd6bb87c3546,"This paper proposes a hierarchical approach to sequence - to - sequence learning with neural networks, where each node in the target tree is transduced by a node in a source tree, and both the source and target trees are treated as latent and induced during training. The source tree is treated as a mixture of conditioned and latent, and the source tree and target tree are treated jointly. The authors also propose a neural parameterization of the grammar which enables parameter sharing over the combinatorial space of derivation rules without the need for manual feature engineering. They apply this latent neural grammar to various tasks in sequence prediction, including a diagnostic language navigation task designed to test for compositional generalization, and small - scale English - French machine translation. They find that it performs respectably compared to baseline approaches and outperforms the baselines."
SP:d06fc251f2a9287f7a2236a188349628d8f39d9a,"This paper proposes a novel method for solving the Group Elastic Net problem in the form of a function - on - scalar regression framework. The proposed method is based on the Augmented Lagrangian framework, which exploits the sparsity in the structure of the Lagrangians to reduce computational burden. The method is applied to the problem of feature selection, where a scalar feature is selected from a large set of scalar predictions, and functional response is modeled against a very large number of potential scalar predictors.   The main contributions of the paper are as follows :   1. The authors propose a new algorithm to solve the Group Elastic Net problem. The main idea of the algorithm is to use the Sparsity structure in the Augmentation Lagrangeian framework to reduce the computational burden of the proposed method. 2. The algorithm can be applied to both feature selection and functional data analysis. 3. Theoretical results and empirical results are presented to demonstrate the performance of the method on simulated data and data from a Genome Wide Association study on childhood obesity."
SP:e0b53f76f3a6b756fedd09926f9cf034f89f4a5a,"This paper presents a method for learning point process models from structured data. The proposed method is based on a mixture model of multi - level marked point processes, where each level is represented by a log - gaussian Cox process, and the data points are clustered in a matrix of log - Gaussian processes. The authors propose to use ES and FPCA to estimate the likelihood of each point process in the matrix. ES is a semi - parametric solution method that is applied to a fixed set of points, while FPC is a functional principal analysis of point processes.    The main contributions of the paper are as follows :   1. A new point process model is proposed based on the mixture model from the proposed method. The key idea is to use the log gaussian process matrix as the basis for estimating the likelihoods of the points in each cluster. This is done by computing the expected solution of each cluster using the ES algorithm, and then using the functional principal component analysis of the point process. 2. The method is tested on simulated data and real data from the Neural Information Processing Systems ( NIPS 2021 ) conference. 3. It is shown that the method outperforms the baselines in terms of accuracy and variance."
SP:3aa213076f3e9f9838ac654517df2fe1fca33499,"This paper presents Online Meta - Adaptive Control ( OMAC ), a multi - task learning approach for adaptive nonlinear control. OMAC is motivated by robot control, where a robotic system encounters a sequence of new environmental conditions that it must quickly adapt to, and a key emphasis is to integrate online representation learning with established methods from control theory in order to arrive at a unified framework that yields both control - theoretically and learning - theoretically guarantees. The authors provide instantiations of OMAC under varying assumptions and conditions, leading to the first non -asymmptotic end - to - end convergence guarantee for multi -task non - linear control. They also show how to integrate OMAC with deep representation learning, which further improves empirical performance."
SP:cb274c93a169b199ea09120ca02105a3f16b31c5,"This paper studies the problem of training neural networks with certifiable robustness guarantees. It is well - known that the state - of - the - art ( SOTA ) methods including interval bound propagation ( IBP ) and CROWN - IBP have per - batch training complexity similar to standard neural network training, which is why they usually use a long warmup schedule with hundreds or thousands epochs to reach SOTA performance and are thus still costly. In this paper, the authors identify two important issues in existing methods, namely exploded bounds at initialization and imbalance in ReLU activation states and improve IBP training. The two issues make certified training difficult and unstable, and thereby long warm - up schedules were needed in prior works. To mitigate these issues and conduct faster certified training with shorter warm - ups, the paper proposes three improvements : 1 ) derive a new weight - based initialization method for faster training of certified networks. 2 ) We propose to fully add Batch Normalization ( BN ) to each layer in the model, since we find BN can reduce the imbalance in the imbalance between certified and uncertified activation states. 3 ) We also design regularization to explicitly tighten certified bounds and balance ReLUactivation states during training."
SP:18ffeb199a670fb2b1f4417b8653479001944dab,"This paper studies the problem of change point detection in an adversarial setting, where the goal is to prevent spurious change points from appearing in the data that would otherwise be detected by the standard detection methods. The authors propose a method based on the Huber contamination framework, which allows the contamination distributions to be different at each time point. The proposed method, called ARC, is a variant of the robust univariate mean estimator ( RUME ) that is trained to detect change points without the presence of any change points. It is shown that under certain conditions, ARC achieves better performance than the state - of - the - art in terms of the minimax lower bound of the error rate. The paper also presents a variant, called automatic ARC ( ARC - A ), which adapts to the contamination proportion."
SP:d03617b5fc446768809cf015c9234b0c9386a690,"This paper studies the relationship between the minibatch size and the precision of SGD and GD in simulating learning with statistical queries ( SQ ). It is shown that when the mini - batch size b is small enough, SGD can go beyond SQ learning and simulate any sample - based learning algorithm, and thus its learning power is equivalent to that of PAC learning. Similarly, with fine enough precision relative to the sample size m, GD can also simulate any samples. The analysis relies on introduction of a variant of SQ learning which is called mini -batch statistical Queries ( bSQ ), and introduces a full - batch variant, fbSQ. In this variant, which is related to the Honest - SQ model [ 20, 21 ], statistical queries are answered using a mini - batches of samples drawn from the source distribution, up to some precision. The paper first shows that bSGD methods can always be simulated by b SGD, by constructing a differentiable model where at each step the derivatives with respect to some of the parameters contain the answers to the statistical queries. Then, based on the relationship of the precision m ( for GD ) and the minibatch size b ( for SGD ), the paper shows that fbGD can not be used to simulate arbitrary sample based methods, and simulating arbitrary PAC methods is provably not possible. In order to simulate PAC, a novel “ sample extraction ” procedure is introduced, where a single sample is used to extract one of the samples in each minibatch from the distribution. This procedure might be of interest, perhaps also in studying privacy, where such an extraction is not desirable."
SP:1de2864fe2f53e25596a9bd2c61e2048e79296f6,"This paper studies the convergence of the Lloyd - type algorithm for generating discrete data from a distribution over a set of points. The authors show that in most cases, a suitably adjusted version of Lloyd ’s algorithm — in which Voronoi cells are replaced by Power cells — leads to configurations with small Wasserstein error. This is surprising because, again, of the non - convex nature of the problem, as well as the existence of spurious critical points. They provide explicit estimates for convergence of this algorithm, starting from a cloud of points that are sufficiently far from each other in the distribution that is evenly distributed in the ambient space that is within the bounds of the corresponding gradient descent. They then establish a Polyak -Łojasiewicz - type inequality ( Corollary 6 ) for the function FN : Y 7→ 12W 2 2 2( ρ, \�Y ) introduced in ( 3 ), and they study convergence of a gradient descent algorithm for FN ( Theorem 7 ). Finally, in Section 4, they report numerical results on optimal uniform quantization in dimension d = 2.   The main contribution of this paper is to provide error estimates for one step of Lloyd-type algorithm in deterministic and probabilistic settings. The main result is that the algorithm converges to a uniform distribution over the set of N points with an error term depending on the distances between Dirac masses in the discrete distribution, and the error term depends on the distance with which the distance is estimated."
SP:c3d364aeee55230a436c3ce4e8dc8310ee73959e,"The paper proposes a new dynamic feature transform, relational self - attention ( RSA ), which leverages rich structures of spatio - temporal relations in videos by dynamically generating relational kernels and aggregating relational contexts. The authors claim that the proposed RSA network substantially outperforms convolution and self - Attention counterparts, achieving the state - of - the - art on the standard motion - based benchmarks for video action recognition, such as Something - V1&V2, Diving48, and FineGym."
SP:2c2530069d5cab485629090243da464d107feadd,"This paper studies the mean field theory of multilayer neural networks in the setting of infinite - width scaling, in which the learning dynamics is shown to be closely tracked by the mean - field limit. The authors first study the random fluctuation around this limit in the case of shallow networks. This fluctuation is expected from a large - width expansion to the next order. The fluctuation can be studied at any network depth. To study the fluctuation, the authors derive a system of dynamical equations based on the neuronal embedding framework introduced by Nguyen and Pham ( 2017 ). The second - order mean field formulation captures the limiting fluctuation and captures the distribution distribution.   The authors then demonstrate through the framework the complex interaction among neurons in this second-order mean field limit, the interaction with stochasticity with cross - layer dependency and the non - linear time evolution inherent in the limit - based fluctuation. A limit theorem is proven to relate quantitatively this limit to the fluctuations realized by large -width networks, and the authors apply the result to show a stability property of gradient descent mean field training. In particular, they show that along the training trajectory, the network progressively biases towards a solution with “ minimal fluctuation ” ( in fact, vanishing fluctuation ) in the learned output function, even after the network has been initialized at or has converged ( sufficiently fast ) to a global optimum. This extends a similar phenomenon previously shown only for shallow networks with a squared loss in the empirical risk minimization setting, to multilayers networks. The loss function that is used in this setting is not necessarily convex in a more general setting."
SP:a3d927854d9d7fd39b8d05a79666810d585d5062,"This paper proposes a framework for learning irreversible dynamics with unknown a priori model form for a metriplectic dynamical system. The framework is based on the Poisson brackets of Hamiltonian/Lagrangian mechanics, which can be cast in a metrizlectic setting. The authors propose to learn generalized Casimirs for energy and entropy guaranteed to be conserved and nondecreasing, respectively. They provide benchmarks for dissipative systems demonstrating learned dynamics are more robust and generalize better than either "" black - box "" or penalty - based approaches. They also provide theoretical guarantees for the existence of a fluctuation - dissipation theorem, ensuring thermodynamic consistency. Finally, they note that this work is an important first step toward handling more complicated dissipative chaotic systems ubiquitous to science and engineering problems."
SP:32e8e83e06b1e9a4dad761334d5947c91bfd1853,"This paper proposes a sample selection - based algorithm for fair and robust training of Trustworthy AI models. The proposed algorithm is based on a combinatorial optimization problem for the unbiased selection of samples in the presence of data corruption. To solve this optimization problem, the authors propose a greedy algorithm that is efficient and effective in practice. Experiments show that the proposed algorithm obtains fairness and robustness that are better than or comparable to the state - of - the - art technique, both on synthetic and benchmark real datasets."
SP:991127729bf067fe27fdd7ed360aab39e4df5921,"This paper studies the relationship between the prior on the network weights and stationary Gaussian process priors in Bayesian neural networks. The motivation is that existing neural network models are known to reinforce hidden data biases, making them unreliable and difficult to interpret. This paper proposes to circumvent this problem by introducing inductive biases in the function space of the neural network. Specifically, the authors propose to use periodic activation functions ( GPPs ) of the limiting stationary gaussian process to approximate the prior of the hidden layer on the weights of the network. They show that this corresponds to a prior that is invariant to the translation - invariant priors of stationary Gaussians. They also show that there is a correspondence between this prior and the spectral density of the weights in the hidden layers of the Bayesian network. This correspondence is extended to triangular wave activations and periodic ReLU activation functions. Experiments are conducted on in - domain and out - of - domain detection and show that the proposed method is able to capture sensitivity to perturbed inputs in deep neural networks for detection."
SP:d61a2aecfea4612c473b4e6fd41f3dc2fcbb04a1,"This paper presents a method to train an agent to generate feedback on interactive programs by classifying Markov Decision Processes ( MDPs ). The goal is to learn about the dynamics and reward model of the input MDP to determine if the agent ’s MDP should be categorized as correct or broken. The paper proposes to train the agent on a dataset of 7,274 student submissions to a single assignment with hand -coded bug labels to support future research. The method is based on a cooperative objective between an agent and an autoregressive model, where the agent is given a MDP and a classifier is given an assignment to determine whether the MDP is correct or not.   The paper shows that the method is able to sample differential trajectories from a set of MDP that allows the classifier to determine membership : Play - to - Grade. The main contribution of the paper is to develop a method that enables an automatic feedback system for developing interactive programs as a task of classifying the dynamics of the inputs MDP under reasonable generalization. The authors also show that their method can be used to train a modeler that learns to learn from the output of the agent."
SP:daf99ad91613d6e11b13315ccbd1bbe25094ae4b,"This paper presents a novel approach to interpret the importance of low - level input features in Deep Reinforcement Learning ( DRL ) models. The authors propose Represent And Mimic ( RAMi ) framework for training an identifiable representation to capture the independent factors of variation for the objects and a mimic tree that extracts the causal impact of the latent features on DRL action values. To jointly optimize both the fidelity and the simplicity of the mimic tree, the authors derive a novel Minimum Description Length ( MDL ) objective based on the Information Bottleneck (IB ) principle. Based on this objective, they describe a Monte Carlo Regression Tree Search ( MCRTS ) algorithm that explores different splits to find the optimal mimic tree. They derive an information - theoretic IB -MDL objective that incorporates both fidelity and simplicity for mimic tree learning. They demonstrate the interpretability of their mimic tree by showing latent traversals, decision rules, causal impacts, and human evaluation results."
SP:84560de78af979354fff83d1370d8675c1e9191f,"The paper proposes a Bayesian framework based on Gaussian latent information martingale ( GLIM ) to model the evolution of weather and political predictions over time.   The method is based on the fact that the probability of rain on a specific day changes by the hour as new information becomes available, and the authors propose to model this through a Gaussian Latent Information Martingale. The authors propose three different metrics to measure the volatility of the GLIM model. The first metric measures the variance of the posterior probability path distributions. The second metric measures how likely it is for the posterior distribution to be Gaussian. The third metric measures whether the posterior is close to the true distribution or not, and this is measured by a logistic regression measure. The paper shows that GLIM outperforms the other two baselines in terms of the variance and the likelihood of posterior distributions. In addition, the paper also shows that the method preserves important properties of the Gaussian, i.e., that it is able to estimate posterior distributions with appropriate amount of volatility and better quantifies future uncertainties."
SP:0c4bfb44e0a353256692d5e5ae96f65c1a14363d,"This paper studies the problem of active pure exploration with fixed confidence in generic stochastic bandit environments. The goal of the learner is to answer a query about the environment with a given level of certainty while minimizing her sampling budget. For this problem, instance - specific lower bounds on the expected sample complexity reveal the optimal proportions of arm draws an Oracle algorithm would apply. This is an optimization problem whose tractability strongly depends on the structural properties of the environment, but may be instrumental in the design of efficient learning algorithms.   The authors devise Frank - Wolfe - based Sampling ( FWS ), a simple algorithm whose sample complexity matches the lower bounds for a wide class of pure exploration problems. The algorithm is computationally efficient as, to learn and track the optimal proportion of arm draw, it relies on a single iteration of Frank -Wolfe algorithm applied to the lower - bound optimization problem. The authors confirm the asymptotic optimality of FWS, as well as its empirical superiority, not only for the case of best arm identification in unstructured bandits as predicted by [ 13 ], but also for a wider class of purely exploration problems. The authors also provide an interesting solution to the three important obstacles they needed to devise and analyze a FW - type sampling rule : ( i ) the objective function in ( 1 ) is not smooth ; (ii ) its curvature becomes infinite in general close to the boundary of Σ ; and (iii ) the estimate of the estimate µ́(t ) is evolving and might be far from the limit."
SP:0947a0f08fba53d3c8af9b78dd64e6e10fc73e32,"This paper proposes a new method, LADDER, to improve surrogate modeling for black - box optimization ( BO ) in combinatorial spaces. The proposed method is based on a reduction to BO over continuous spaces by learning a latent representation of structures using deep generative models ( DGM ). The key idea is to define a novel structure - coupled kernel that explicitly integrates the information from decoded structures with the learned space representation for better surrogate modeling. Experiments are conducted on real - world benchmarks to show the efficacy of the proposed method over BO over latent space method and state - of - the - art methods.   The main contributions of the paper are as follows :   ( 1 ) A new surrogate modeling method is proposed to improve the surrogate modeling in BO. This method is referred to as LADder. The main idea of the method is to learn a learned representation of continuous spaces using DGM. This representation is used to approximate the target black box function for the surrogate model. The surrogate model is trained to learn the input from the continuous space and the output from the latent space representation. This surrogate model can be used to select the inputs with high utility guided by a learned surrogate model to perform the black box evaluation. The paper proposes to use the learned representation from DGM to select high - utility inputs guided by learned surrogate models to perform black box evaluations. The method is evaluated on a set of synthetic and real world problems. The results show that the proposed approach performs better than the baselines of BO and SOTA."
SP:37adabdc6615c5199a481553c8ccc06d57363614,"This paper studies the problem of regret minimization in finite horizon Markov Decision Processes ( MDPs ) with linear structure. The authors first derive a necessary condition on the representation, called universally spanning optimal features ( UN ISOFT ), to achieve constant regret in any MDP with linear reward function. This condition generalizes to linear mixture MDP and linear contextual bandits and it requires that the features observed along trajectories generated by the optimal actions provide information on the whole feature space ( see Asm 4 ).   The authors then demonstrate that this condition is sufficient for these classes of problems by deriving a constant regret bound for two optimistic algorithms ( LSVI - UCB and ELEANOR ), and then propose an algorithm for representation selection and prove that it achieves constant regret when one of the given representations, or a suitable combination of them, satisfies the UNISOFT condition."
SP:92566b664ab2f6ee9b73f29327aeef85d14ecf60,"This paper proposes a differentiable contact model for learning dynamics from data. The contact model is based on a three - stage differential equation, which includes contact mechanics ( frictionless / restricted / elastic contact ) and contact mechanics with and without elasticity ( with or without elan ). In particular, the contact model can be viewed as a combination of the Lagrangian and Hamiltonian dynamics encoder and the inductive bias of neural networks. The proposed contact model allows simultaneous learning of both the contact and the elan dynamics, and is differentiable with respect to the joint angle and restitution. It is shown to be able to learn the energy conservation during the smooth part of the trajectories and the energy - conservation during the frictionless and elastic contact. This model is tested on a series of physical systems with different coefficients of restitution and friction, as well as two differentiable physics simulators ( a 2D and a 3D contact model ). The experimental results show that the proposed model is able to achieve better performance than the state - of - the - art in terms of energy conservation, restitution and elasticity."
SP:82d59a3609dfd458f90f23d4e477c8b497e9dc18,"This paper analyzes the effect of stochastic training on the Lipschitz constant of the neural network ( NN ) by relating it to the training dynamics of the network. Specifically, the authors show that the NN whose 1st layer bias is trained more steadily ( i.e. slowly and with little variation ) have bounded complexity even in regions of the input space that are far from any training point. The authors also show that steady training with Dropout implies a training that grows poly - logarithmically with the number of parameters. Overall, the results support the intuition that good training behavior can be a useful bias towards good generalization."
SP:9b329c915fa8d4045c167c9df37a49ee314d190e,"This paper proposes a distribution - independent PAC learning algorithm for learning PAC learning of halfspaces in the Massart noise model with strongly polynomial sample complexity independent of the bit complexity of the examples. The authors show that any distribution can be efficiently decomposed as a disjoint mixture of few distributions for which a Forster transform exists and can be computed efficiently. The main application of this result is to obtain the first Polynomial - time algorithm for distribution - dependent PAC learning in the noise model of Massart.   The algorithm of [ DGT19 ] requires n = poly(d, b, 1 / ) labeled examples, runs in time poly(n, b ) and achieves misclassification error $ \�+$. The dependence on b in the runtime is likely to be inherent, in the sense that the sample upper bound argument does not yield a sub - exponential time learning algorithm. On the other hand, there is no a priori reason to believe that the poly(b ) dependence is needed in the sample complexity, even though it is known [ MN06 ] that it is information - theoretically sufficient to achieve optimal mis classification error. This sample complexity bound is non - constructive, and the main result of this paper provides an affirmative answer to this question. The guarantees of the algorithm are stated in more detail ( the proper algorithm of the [ CKMY20 ] algorithm builds on the same ideas )."
SP:e5229305af00067ae2dbabd903e585964aec8928,"This paper proposes a new method for attacking graph neural networks ( GNNs ) with Bayesian optimisation ( BO ) based attack method, GRABNEL1, which is a black - box attack method that is query - efficient and parsimonious. The method is based on Bayesian optimization and Bayesian perturbation. The authors claim that this is the first work to use BO for adversarial attacks on graph data. The main contributions of the paper are as follows :   1. Introducing a new attack method to attack graph neural network ( graph - neural - network ). This is a very important work as the majority of the literature focuses on node - level classification tasks, and little effort has been dedicated to analysing the vulnerability of graph neural - networks with respect to the classification task. The few existing methods often require unrealistic setups, such as access to internal information of the victim models, or an impractically large number of queries. The proposed method does not require policy training on a separate labelled dataset to effectively attack a new sample. It can be easily adapted to perform various modes of attacks such as deleting or rewiring edges and node injection.   2. An analysis is performed to link the generated adversarial examples to the topological properties of the perturbed graph, an important step towards interpretable adversarial example. 3. Empirically, the authors validate the effectiveness and flexibility of the proposed method on a wide range of graph - based learning tasks involving varying graph properties, constraints and modes of attack."
SP:4999e5664383066fdacd14be6242c7b83f85f3dd,"This paper studies the problem of online learning adaptation to label shift in the online setting, where the test - time label distribution is continually changing and the model must dynamically adapt to it without observing the true label. The authors propose two adaptation algorithms inspired by classical online learning techniques such as Follow The Leader ( FTL ) and Online Gradient Descent ( OGD ) and derive their regret bounds. They empirically verify their findings under both simulated and real world label distribution shifts and show that OGD is particularly effective and robust to a variety of challenging label shift scenarios. They also evaluate their adaptation algorithms on CIFAR - 10 and evaluate the performance of FTH and OGD."
SP:806515ae07fb1c9d02773592005d53d4158ef102,"This paper proposes a method for detecting and localizing gradual changes in the distribution of a sequence of time - ordered observations. The proposed method is based on the nonparametric setting where the data distribution does not depend on a specific data generating model, and does not require prior knowledge about which features of the distribution are subject to change.   Theoretically, the authors provide guarantees on detection ( false positive rate, power ) and localization ( consistency ). Theoretical guarantees are provided for both detection and localization. The main contribution of the paper is the formulation of a general method to detect and localize gradual changes that does not rely on prior domain knowledge. The method is applied to two settings : the simple abrupt setting, where the distribution is assumed to follow a discontinuity jump in distribution and is unrealistic for some applied settings, and the more complicated case in which the distribution follows a more complex and continuous structure. The setting in the latter case assumes that the data does not follow an abrupt transition, and is instead structured as a series of discrete discontinuity jumps in distribution followed by a gradual change in distribution. The first setting is the simpler abrupt setting which is assumed that the distribution does n’t follow a continuous transition. The second setting is considered to be the more complex case where the distributions follow a continuum. The authors provide theoretical guarantees for detection of the first setting and the second setting for the latter setting. The guarantees are based on two main components : ( 1 ) detection of false positive rates and ( 2 ) localization of the localization."
SP:7a3c8a7b17ecab19361d36e1d3d73fa35b71214c,"This paper proposes a biologically plausible neural network ( NN ) based on independent component analysis ( ICA ) that can be used as an alternative to the standard linear BSS algorithm in signal processing. To be considered as biologically plausible, the NN must satisfy at least the following requirements :   1. The algorithm must operate in the online setting where data samples are streamed one at a time without storing any significant fraction of the data in the memory ; 2. The synaptic weight update is local, i.e., it depends only on the biophysical variables present in the vicinity of a synapse.   The authors propose a novel objective function for ICA that extends more conventional Hebbian learning rules by a time - varying modulating factor, which is a function of the total output activity and modulates the overall plasticity rules by overall output activity. The proposed method is based on the single - layer NN that can separate independent sources without pre - processing. The authors also propose a local learning rule that is applied to update the weight of the weight update using local learning rules, which are applied to both the neural architecture and the synaptic learning rules. Interestingly, their algorithm relies on modulating the plasticity rule by the total activity by the time - changing factor, suggesting a role of extracellular environment on plasticity."
SP:22f8b517a3df65144412938f5891c463d7bae0ab,"The paper proposes a method to characterize the space of solutions associated with various tasks. The authors first apply this approach to a simple two - neuron network and demonstrate how distinct solutions arise. They then study three tasks inspired by the neuroscience literature : interval reproduction ( 20 ), delayed discrimination ( 21 ) and interval discrimination ( 22 ). They show that different networks with identical hyperparameters find qualitatively different solutions. Furthermore, the diversity revealed with these challenging inputs corresponds to different computations performed by the network. To chart this space, the authors introduce a tool that reduces the dynamics of a network into a graph that captures the essence of the computation performed. Applying it to all networks partitions the space into a handful of possible reduced dynamics reduced dynamics. Additionally, these classes can be partially predicted using experimentally accessible neural activity obtained only in response to trained stimuli. The results shed light on the concept of the Space of Solutions and its uses both in Machine learning and in Neuroscience."
SP:9b08a0f547ead3b59077a43b1052c6d46a0730f6,"This paper proposes a new method, Arbitrary Conditioning with Energy ( ACE ), for the problem of arbitrary conditional density estimation, which aims to model any possible conditional distribution over a set of covariates. The authors propose a method that can simultaneously estimate the distribution p(xu | xo ) for all possible subsets of unobserved features xu and observed features xo, while avoiding unnecessary bias and complexity. They specify densities with a highly expressive energy function and reduce the problem to only learning one - dimensional conditionals. They empirically demonstrate that ACE is state - of - the - art for arbitrary conditional distribution estimation and data imputation. They find that complicated prior approaches can be easily outperformed with a simple scheme that uses mixtures of Gaussians and fully - connected networks."
SP:f2b14f5854e6aa6922795d1d2051b7402486cef6,"This paper proposes a new loss function for the standard SISR solution, UDL - SR, which is based on the idea that texture and edge pixels with high uncertainty are more important visual information than smooth areas in a high - resolution image. The loss function treats every pixel equally with the assumption that the importance of all pixels is the same. The paper proposes to use variance estimation characterizing the uncertainty on a pixel - by - pixel basis into the loss function in order to adaptively train deep networks for PSNR in high - uncertainty situations. The proposed loss function is applied to three different networks, and experimental results on three popular networks show that the proposed uncertainty - driven loss function achieves better PSNR performance than traditional MSE or L1 loss."
SP:9997583f40fa648adf57bb4fc34228f357be0cf1,"This paper proposes a generalization of the PAC - Bayesian framework for adversarial robustness that is based on the PACBayesian framework. The main idea is to estimate how robust a model is to perturbations in the input at test time based on a worst - case analysis of the risk of a hypothesis over all the possible perturbation scenarios. The approach is similar to the one used in Bayesian Equation Theory ( Yao et al., 2017 ), except that instead of deriving a worst case analysis for each case, the authors use a PAC - bayesian framework to bound the risk for majority votes over the whole class of hypotheses. The authors argue that this approach has the advantage to provide general bounds ( i ) that are valid for any kind of attacks ( i.i.d. adversarial attacks ), ( ii) that are tight thanks to the PAC-bayesian framework, and ( iii ) that can be directly minimized during the learning phase to obtain a robust model on different attacks at the test time.   The main contributions of the paper are as follows :   ( 1 ) An improvement in the robustness of adversarial adversarial data under the assumptions made by PAC-Bayesian methods. This improvement is due to the fact that the authors do not have to deal with the adversarial attack in the worst case scenario, which is usually the case in the Bayesian literature. ( 2 ) The authors also argue that it is important to provide a general bound on the number of attacks that a model can be expected to be invariant to under certain assumptions. ( 3 ) A new adversarial setting is proposed to allow for robustness to be measured that is more general than the one that is used in the previous work. The proposed setting is more stable than the previous one."
SP:90b72e8dc41584e38f25dff9fb2853f5b11dc8fa,"This paper proposes a probabilistic entity representation model ( PERM ) for Knowledge Graphs ( KGs ) for logical reasoning over queries. The proposed model is based on the Gaussian representation of entities, which is a weighted combination of Gaussian density and mean and covariance. The authors define the operations of projection, intersection, union, and closed logical operations of PERM. They also provide the formulation for building the reasoning chains for complex queries. Experiments are conducted on several benchmark datasets for KGs and compare the performance of the proposed model with other state - of - the - art methods. They evaluate the performance on a COVID - 19 drug - free case study and show that the proposed method performs better than the other methods."
SP:b6184c9732dbb7eba7c20cae8869d975c428efe4,"Gradient - based hyperparameter optimization ( HPO ) is a popular technique in few - shot meta - learning. However, HPO with its large number of gradient steps poses memory scaling and gradient degradation issues. To tackle these issues, the authors propose forward - mode differentiation with sharing ( FDS ) algorithm. FDS shares hyperparameters that are contiguous in time to prevent gradient degradation and memory scaling issues. The authors provide theoretical guarantees about the noise reduction properties of their algorithm. Empirically, they show that FDS outperforms the state - of - the - art gradient - based methods on the CIFAR-10 benchmark."
SP:9c3a326e5ee4e862923d3bf9415f32a077db8534,"This paper presents a dual - system model that attempts to bridge the gap between two systems : intuitive and associative ( “ System 1 ” ) and the deliberative and logical ( ‘ System 2 ’ ). The authors focus on the problem of inconsistent and incoherent logical reasoning between generations of neural sequence models. They propose two approaches for bridging the gap : 1 ) candidate generations from a neural sequence model are examined for logical consistency by a symbolic reasoning module, which can either accept or reject the generations. 2 ) neural inference is used to mediate between the neural System 1 and the logical System 2.   The authors test their model on the gSCAN grounded compositional challenge in two text generation domains : robust story generation and grounded instruction - following. The results show that the proposed dual system model requires much less data to train than previous models, and achieves higher accuracy and stronger generalization. They also test their approach also on instruction following, showing how goalprediction models and execution models can easily be combined to achieve improved performance in low - data regimes. Overall, the findings indicate that neuro - symbolic, dual process models are a promising means of addressing longstanding problems of robustness and consistency in neural sequences models."
SP:d77d046095e4c8336c0c76ac48cb046923230753,"This paper proposes a novel method for estimating the mean outcome of off - policy evaluation ( OPE ) in continuous treatment settings, such as personalized dose - finding. The key ingredient of the method lies in adaptively discretizing the treatment space using deep discretization, by integrating deep learning [ 28 ], multi - scale change point detection [ 35 ], and the doubly - robo - aware value estimators in discrete domains. Theoretically, the convergence rate of the proposed method, Deep Jump Learning ( DJL ), is shown to be faster than the rate of convergence of kernel - based OPE methods under the piecewise model assumption. Empirical results are provided for simulations and a real data application to warfarin - dosing, showing that DJL outperforms the state - of - the - art methods."
SP:4d085e57286fdd36143108a002d16914222c239a,"This paper proposes a continuous - time variational inference framework for discrete - event systems based on Markov jump processes modulating a subordinated diffusion process. The authors propose to combine Gaussian process approximation on the diffusion level with posterior inference for Markov - jump processes in order to recover MJP approximations of the diffusion and MDPs. They provide the exact evolution equations for the prior and posterior marginal densities, the direct solutions of which are computationally intractable.   The authors then proceed to develop a new continuous time inference algorithm, which recovers the MJP of a hybrid system by minimizing the path - wise Kullback - Leibler divergence and the Bayesian latent state estimates for arbitrary points on the real axis and ( i ) point estimates of the unknown system parameters, utilizing variational expectation maximization. They also provide a generalized VI framework for hybrid systems which recovers existing diffusion approximated by MJP as special cases. The main contributions of the paper are as follows :   1. A continuous time model based on a Markov-jump process modulated a subordination diffusion process is proposed. 2. An inference algorithm is developed based on the posterior inference of a variational method. 3. An empirical evaluation of the proposed method is provided."
SP:d1f396e691f9d331adfb7b694a99c50e8004331f,"This paper studies the effect of the spikiness of the sensing matrices on the performance of expectation propagation algorithms, i.e. the expectation propagation algorithm ( GLM - EP ). Specifically, the authors consider the following nonlinear inverse problems : 1 ) compressed sensing, 2 ) phase retrieval, and 3 ) finite - sum recovery.    For compressed sensing problem, they use the following framework : the Lorenz partial order of the nonlinear mapping f ( as well as the sampling ratio ). They show that spikier spectrums are better for EP, while in 1 - bit compressed sensing problems, less spiky ( flatter ) spectrums offer better recoveries. They also show that when the number of measurements required for perfect recovery approaches the lower bound of the information theoretical lower bound. For phase retrieval problems, they show that the monotonicity of a function, which is related to the scalar minimum mean square error, is more important than the spikyness of the matrix spectrum."
SP:ee66604d4da9fd04826e90ccbb94f0499eba4c63,"This paper presents a novel method to address the domain shift problem, i.e. confusion between seen and unseen categories, by progressively improving cross - domain transferability and category discriminability of visual representations. The method, named Dual Progressive Prototype Network ( DPPN ), constructs two prototypes that record prototypical visual patterns for attributes and categories, respectively. It alternately searches attribute - related local regions and updates corresponding attribute prototypes to progressively explore accurate attribute - region correspondence. Experiments are conducted on four benchmarks : zero - shot learning ( GZSL ), image classification ( CIFAR-10 ), semantic classification ( SGML ), object classification ( GPT ), and semantic clustering ( C4A ). Results show that the proposed method achieves state - of - the - art performance on all four benchmarks."
SP:61eb6297568c3f6869fbb03eaf6a21260de5466c,"This paper presents an end - to - end deep learning approach for removing defocus blur from a single image. The main idea is to use a pixel - wise Gaussian kernel mixture ( GKM ) model, which is proposed for representing spatially variant blur kernels in an efficient linear parametric form, with higher accuracy than existing models. Then, a deep neural network called G - KMNet is developed by unrolling a fixed - point iteration of the GKKM - based deblurring. The G - MNet is built on a lightweight scale - Recurrent architecture, with a scale - recurrent attention module for estimating the mixing coefficients in GKMs. Extensive experiments show that the proposed method outperforms existing methods, but also has its advantages in terms of model complexity and computational efficiency."
SP:18bf447c90935c373e5ec4cdfbbf8f2a273d2edb,"This paper presents a new method for self - supervised video representation learning ( SSVRL ) based on cross - guidance contrastive learning ( CG ). The main contributions are three - fold : ( 1 ) They propose an efficient and effective framework called MVCGC that can learn representations from compressed videos directly, i.e., directly decode RGB frames and motion vectors ( that resemble low - resolution optical flows ) from a compressed video on - the - fly ; ( 2 ) They design a novel CG - based learning algorithm to capture mutual information between RGB frame and motion vector from the compressed video, and ( 3 ) They apply the learned video representations on two downstream tasks ( action recognition and action retrieval ) across different benchmarks to demonstrate that their method is more efficient than its counterparts.   The main contribution of this paper is the novel CG- based Cross Guidance Contrastive learning approach that combines the two objectives of capturing mutual information and learning representations from two input streams simultaneously. This is the first time that two seemingly contradictory goals are simultaneously achieved by exploiting compressed videos and capturing information between two input stream. The experimental results show that the proposed method yields new state - of -the - art performance on both downstream tasks. In addition, the learned features of motion vectors are as representative as those of optical flows."
SP:8c7b1d976d9758cd534c565ec31a23f97892e503,"This paper proposes a method to mitigate the asymptotic overconfidence of Bayesian neural networks ( BNNs ) in multi - class classification settings. It is shown that the output variance of a BNN with finitely many features is quadratic in the distance from the data region, while a Bayesian linear model with ReLU features converge, in the infinite - width limit, to a particular Gaussian process ( GP uncertain ) with a variance that grows cubically so that no overconfidence can occur. This paper extends the finite - width method of Meinke and Hein [ 8 ] to infinite - feature - space ReLU neural networks with infinite features, and shows that the resulting model is more robust to overconfidence than non - non - linear models. Moreover, it shows that it can be applied post - training to any pre - trained ReLU BNN at a low cost.   The main contribution of this paper is the theoretical analysis. They show that their method ( i ) models the uncertainty, thus ensuring that the surrounding output variance ( i.e., the distance to the training data ) of the non - Gaussian model with infinite ReLU network does not grow cubically, and ultimately yields uniform overconfidence. They empirically confirm the effectiveness of their method and show that it is complementary to the method of the Meinkel and Hein paper."
SP:e77276f61626e896f6a985296f1d832129242cdf,"This paper considers the problem of estimating a causal quantity of interest among a set of available formulas. The goal is to estimate the formula with lowest asymptotic variance in as few samples as possible in a sequential setting. The setting is assumed to be sequential where each formula is considered as one arm of a multi - armed bandit, and the goal of learning the arm with the lowest variance is replaced with the goal to learn the arm that will produce the best estimate. The authors adapt well - known bandit algorithms to this goal by introducing finite - sample confidence bounds on the asymetrical variance, and adapt the best - arm - identification algorithms of LUCB and Successive Elimination bandit to use these bounds. The main result is that even without Neyman orthogonality, estimation of O(n−1/2 ) estimation of $ \tau$ is possible whenever $ \nablaO(n)$ is estimated at rate O(tau )$."
SP:471361588bfc6c6033631509d1e43e77fd9721ce,"This paper studies the convergence rate of ErrorCompensatedX, an algorithm that compresses the gradient during training in order to reduce the communication cost between the student and the teacher. The authors show that this approach decelerates the convergence of the algorithm. They also show that adding back the previous step’s compression error does not fully compensate the magnitude of the compression error. To address this problem, the authors propose a new algorithm that uses the compressed error from the previous two steps. They show that the algorithm converges faster than the one that does not use it. The convergence rate depends only on the ratio of the error compensation error with respect to the training error.   The main contribution of this paper is to provide a unified theoretical framework for this class of variance reduced algorithms, where the variance of the stochastic gradient is reduced by taking a moving average over all history gradients. The main result of the paper is that, for any compressed algorithm in the form of ( 2 ), it can achieve the same asymptotic convergence rate with training as with the training without compression. This is the first general result for error compensation in distributed learning."
SP:3b7ff0dc668cac2191d95fcc4dc6e0335dec3206,"This paper studies the problem of explainability of graph neural networks ( GNNs ). The authors focus on two aspects of GNN explainability : ( 1 ) local explainability, which explains each instance independently, thus hardly exhibits the class - wise patterns, and ( 2 ) global explainability which systematizes the globally important patterns, but might be trivial in the local context. This dichotomy limits the flexibility and effectiveness of explainers greatly. To address this problem, the authors propose ReFine, a multi - granularity explainer that attempts to capture both the global and local aspects of the graph neural network. ReFine is evaluated on synthetic and real - world datasets, and it is shown to outperform the state - of - the - art in terms of AUC on explaining graph classification over the leading baselines."
SP:9b5a62d3a2b27bc60da28980e9fb0ecdff1215c0,"Graph Neural Networks ( GNNs ) are a popular tool for generating counterfactual explanations of predictions made by graph neural networks ( GNNs ). Traditionally, explanations are generated by finding a subgraph of an input graph that has a strong correlation with the prediction. These explanations are not robust to noise because independently optimizing the correlation for a single input can easily overfit noise. The authors propose a novel method to generate robust explanations by explicitly modelling the common decision logic of GNNs on similar input graphs.    The main contribution of this paper is to develop a method that generates explanations that are robust to the noise and that align well with human intuition. Most existing methods generate explanations by identifying subgraphs that have a correlation with a prediction but not the other way around. This is problematic because removing an identified subgraph from an input does not necessarily change the prediction result. In this paper, the authors propose an alternative approach that explicitly models the decision boundaries of a GNN that govern the predictions of many similar input graph. This allows them to generate explanations that can be robust to both noise and uncertainty. The main contributions of the paper are as follows :   1. A novel method for generating robust explanations of GNN predictions. This approach is based on a novel algorithm that explicitly modifies a graph neural network ( graph - neural network ) based on the decision boundary of the GNN. This modifies the graph network by the common logic of the many different GNNs. 2. A set of edges identified by an explanation from the input graph is added to the graph and these edges are used to generate counterfactually different graphs. 3. The predictions of the proposed by the proposed method are compared to those of the state - of the art methods on several public datasets. The experimental results demonstrate the superior performance of this method on fidelity, robustness, accuracy and efficiency."
SP:4edb870786c9cea2c6075359cb4e79b02a8e2f5f,"This paper presents a method for improving the transfer of voice style and content from source speech to target speech using self - supervised learning and adversarial feedback. The method is based on the idea that the gap between the converted voice and the target voice is due to the insufficient decomposition of content and voice style from the source speech. To tackle this problem, the authors propose a new information bottleneck that can decompose the content and style with only a small loss of content information. The discriminator is decomposed into content discriminator and style discriminator with self -supervision. The experimental results show the superiority of the proposed method over other methods in terms of transfer performance in both many - to - many and zero - shot voice style transfer scenarios."
SP:9fbb0c6beb3f8f88972f13dcf0e1fe7db03233c7,"This paper presents a method for 3D object tracking in sparse 3D point clouds based on a shape - aware feature learning network and a voxel - to - BEV target localization network. The key idea of the network is to capture 3D shape information of the object to learn discriminative features of the background so that the potential target from the background in sparse point clouds can be identified. To this end, they first perform template feature embedding to embed the template’s feature into the 3D space of the target and generate a 2D shape to characterize the shape information. Then, they generate a dense BEV feature map, where the regression of the 2D center and the z - axis center can be performed more effectively. Extensive evaluation on the KITTI and nuScenes datasets shows that the proposed method significantly outperforms the current state - of - the - art methods by a large margin."
SP:8b788c78680a54c453a04f4551436763ee57585e,"This paper proposes a positional encoding method based on learnable Fourier features for multi - dimensional positions. Instead of hard - coding each position as a token or a vector, the authors represent each position, which can be multi - dimensionally, as a trainable encoding based on a learnable feature mapping, modulated with a multi - layer perceptron. The method is parameter - efficient, in the sense that the number of parameters do not grow with sequence length. The authors evaluate their method on a number of tasks where Transformer - based models have been used for image generation, object detection, and image classification, which all involve 2D positions (vertical and horizontal in images ). The experiments show that the proposed method consistently outperforms existing methods by both improving accuracy and accelerating learning."
SP:d2ac1b6381315bce4449f09bd519f33a2a42d714,"This paper considers the problem of learning the causal structure of a system from observational data in the presence of latent variables and selection bias. Constraint - based methods are one of the main approaches for solving this problem, but the existing methods are either computationally impractical when dealing with large graphs or lacking completeness guarantees. This paper proposes a novel computationally efficient recursive constraint - based method that is sound and complete. The key idea of the approach is that at each iteration a specific type of variable is identified and removed. This allows us to learn the structure efficiently and recursively, as this technique reduces both the number of required conditional independence ( CI ) tests and the size of the conditioning sets. The former substantially reduces the computational complexity, while the latter results in more reliable CI tests. To the best of our knowledge, this is the tightest bound in the literature. We further provide a lower bound on the required number of tests required by any constraint -based method. The upper bound of our proposed approach and the lower bound at most differ by a factor equal to the total number of variables in the worst case. We provide experimental results to compare the proposed approach with the state of the art on both synthetic and real - world structures."
SP:49a4912ce457f5f5ec62c44fa10444af8075fabf,This paper introduces a novel Thompson Sampling framework for stochastic multi - arm bandit and linear contextual bandit decision making problems in online decision making. The authors propose a dynamic batch policy to balance exploration - exploitation trade - off in order to reduce the number of interactions with the environment from T to O(log T ). The proposed policy is based on dynamic batch allocation mechanism that dynamically determines the duration of each batch based on an offline estimation of the regret accumulated during that phase. The paper empirically shows that the proposed policy achieves the same ( asymptotic ) regret bound of a fully sequential one while carrying out only O( log T ) batch queries. It also shows empirically that batch Thompson Samplings methods with a fixed batch size but equal number of batches incur higher regrets.
SP:653a519e3c799c25e0d0b4240322642040b121a3,"This paper presents a theoretical analysis of the trade - off between learning domain - invariant representations vs. domain - invariant representations in the multi - source domain adaptation ( MSDA ) and domain generalization ( DG ) settings. The main contribution of this paper is to develop a novel upper - bound for the target general loss for learning two kinds of domain - Invariant representation, i.e., one for each source DA and one for the multiple source DG setting, which is more complicated due to the presence of multiple source domains and the potential unavailability of target domains during training. The upper - bounds developed in Theorem 8 are used to study the pros and cons of enforcing learning each of the source DA representation and the trade off between enforcing and learning the domain invariant representation in MSDA and DG setting. The authors also conduct experiments to inspect the trade-off of these representations for offering practical hints regarding how to use them in practice and explore other interesting properties of their developed theory."
SP:2a7bee950cd07494d59dfee60ac2e86cc0e481b1,"This paper proposes a new model compression method, aligned structured sparsity learning ( ASSLN ), for light - weight image super - resolution ( SR ) networks. The proposed method is based on a weight normalization layer and a sparsity penalty term, which aims to align the sparsity parameters across different layers of the network. The method is named as ASSLN, with smaller model size and lower computation than state - of - the - art methods. The authors conducted extensive comparisons with lightweight SR networks and compared with SOTA lightweight image SR methods. They show that the proposed method achieves superior performance gains over recent methods quantitatively and visually.    The main contributions of the paper are as follows :   - A new weight normalisation layer is proposed for SR networks to reduce the number of parameters in the model. - The proposed L2 regularization is applied to the scale parameters for sparsity. - An alignment penalty term is proposed to align sparsity parameter locations across the network layers. - A detailed pruning process is presented, with detailed visualization for analysis."
SP:e9830bb9e7d3ddc3bd1c2994590fdb5d8f3668be,"This paper introduces Episodic Multi - agent Reinforcement learning ( EMC ), a method that combines multi - agent reinforcement learning with Curiosity - driven exploration. The authors leverage an insight from factorized MARL algorithms that the individual utility functions used for local execution are the embeddings of local actionobservation histories, and can capture the interaction between agents due to reward backpropagation during centralized training. Therefore, they use prediction errors of individual Q - values as intrinsic rewards for coordinated exploration and utilize episodic memory to exploit explored informative experience to boost policy training. They evaluate EMC in didactic examples and a broad set of StarCraft II micromanagement benchmark tasks. The results on more complicated StarCraft II tasks show that EMC significantly outperforms other multi-agent state - of - the - art baselines."
SP:c7e33d479575c88e22282ee6fd4f978bcd3c06ed,"The authors study the problem of list - controllable linear regression, where an adversary can corrupt a majority of the examples in a linear regression model with Gaussian covariates. The main result is a statistical query ( SQ ) lower bound of d for this problem, which is a lower bound than the upper bounds of previous state - of - the - art SQ algorithms. The class of SQ algorithms is rather broad and includes a range of known supervised learning algorithms, as well as several known algorithmic techniques in machine learning that are known to be implementable using SQs, such as spectral techniques, local search and expectation Maximization.   The main contribution of the paper is the introduction of the SQ lower bound, which provides evidence that current upper bounds for this task are nearly the best possible. The lower bound qualitatively matches the performance of previously developed algorithms, providing evidence that this current upper bound is the best that is currently possible."
SP:7b258252a9063514348f5fa8d9c85afd85748747,"This paper proposes a new method for hybridizing neural differential equations ( ODEs ) and pharmacological models for predicting patient health trajectories. The motivation is that the pharmacological model can only describe a limited set of variables, while the neural models can capture the evolution of these variables over time, which is difficult to capture in a clinical setting. The authors propose a method that combines the knowledge from the pharmacology model with machine learning to learn a larger latent variable, which they call the latent hybridisation model ( LHM ). The LHM is based on the idea that we can learn the relationship between a set of latent variables, the expert variables, and the latent variables of the model. The model is trained using a combination of supervised learning and stochastic gradient descent. The experimental results show that the LHM outperforms the state - of - the - art methods in terms of accuracy and predictive utility."
SP:3ea9e86e5755ef84d28e3163c60531ace5d62e3a,"This paper proposes a theoretical framework for analyzing a MAML - like algorithm, assuming all available tasks require approximately the same representation. The authors provide risk bounds on predictors found by finetuning via gradient descent, demonstrating that the method provably leverages the shared structure. They also illustrate these bounds in the logistic regression and neural network settings. In Section 5, they establish settings where learning one representation for all tasks ( i.e. using a “ frozen representation ” objective ) fails to outperform directly learning the target task with no other information, in the worst case. This separation underscores the benefit of meta - learning over “ representation - based “ objectives in few - shot learning ”."
SP:8ba5a2ac80f7c53f81ad008e96c033ecad14ac0d,"This paper presents G2L2, a neural - symbolic model for language learning based on lexicon - based representations. The model is based on a neural network embedding, which maps each word to a syntactic type and a semantic program. The embedding is trained using a combination of lexicon entries and a neuro - symbolic embedding. To facilitate learning in an exponentially growing compositional space, the authors introduce a joint parsing and expected execution algorithm, which does local marginalization over derivations to reduce the training time. The authors evaluate the model on two domains : visual reasoning and language - driven navigation. Results show that the model generalize from small amounts of data to novel compositions of words."
SP:16c458651815813efdcbe8ba1205bbddbe3e4e68,"This paper proposes and analyzes a stochastic Newton algorithm, FEDSN - LITE, which is based on the convergence of convergence guarantees for quasi - self - concordant objectives ( e.g. logistic regression ) under the condition that each machine has access to an unbiased estimator of the Hessian of the population objective with arbitrary vectors. The authors show that this method can reduce the number, and frequency, of required communication rounds compared to existing methods without hurting performance, by proving that convergence is guaranteed for objectives such as quasi - Self - Concordance. They also provide empirical evidence to support the main guarantees of their method.   The main contributions of the paper are as follows :   1. Theorem 1 : Theorem states that the convergence guarantees of the proposed method are guaranteed under the conditions that the objective is non - differentiable and that the communication rounds do not have to be longer than the number of required rounds. 2. Section 2 : In Section 3, the authors prove that under these conditions, the communication round length does not increase as much as expected. 3. Section 4 and 5 show how communication rounds can be reduced for some regimes in terms of terms of MDPs under certain conditions. 4. Section 6 and 7 compare a more practical version of FED SN - LIte against the other methods, showing that communication can be significantly reduced in some regimes."
SP:d7e479d59f82d4c55372a68ca7b4516f2871f346,"This paper proposes Density - aware Chamfer Distance ( DCD ), a new metric for measuring the similarity between two point sets. The authors claim that EMD and CD do not have the same sensitivity to mismatched local density, while EMD is dominated by global distribution, while CD is more sensitive to detailed structures. To tackle these issues, the authors propose DCD, which is derived from the Chamfer Distance metric ( CD ). The paper claims that DCD is more computationally efficient than CD and is sensitive to more detailed structures than EMD. Experiments on PCN and VRCNet show that the proposed DCD reduces the EMD metric and surprisingly reduces CD as well compared with the network trained with CD.   The authors also propose a novel point discriminator module that estimates the priority for another guided downsampling step, and it achieves noticeable improvements under DCD together with competitive results for both CD and EMD under the new metric."
SP:e4b302009520770814ff2c096020b779a9fc38fe,"This paper studies the effect of knowledge distillation on student generalization when applied to a small teacher model compared to a larger teacher model, such as an ensemble of networks. The authors show that the performance of the student does not always match that of the teacher, even when the student has the capacity to match the teacher perfectly. They also show how the details of the dataset used for distillation play a role in how closely the student matches the teacher — and that more closely matching the teacher paradoxically does not necessarily lead to better student generalisation. They identify difficulties in optimization as a key reason for why the student is unable to match its teacher.    The authors then show how distillation does not typically work as it is commonly understood : there often remains a surprisingly large discrepancy between the predictive distributions of the teachers and the student, even in cases where the student can perfectly match their teacher. They then show that distillation is not guaranteed to work."
SP:895c7e03f9e4dadb94be1f39d61bf0b5e1533f4f,"This paper proposes a new algorithm for learning a k - decision tree ( k - tree ), which is a recursive partition of a matrix ( 2D - signal ) into 1 block matrices ( where each rectangle is assigned a real label ). The main motivation for this paper is the following challenges : sub - optimality of k - trees, scalability, computational time, and computational complexity. The authors propose a small summarization that provably approximates this loss to every such tree, up to a multiplicative factor of 1 / \sqrt(1 + \� ). This is the first algorithm that provides the first output that outputs such a ( 1 + 1 /\sqrt{1 } ) approximation to the optimal k -tree of D of D. The size of the core is the size |C| of the error parameter for every such matrix D, and its construction takes O(Nk ) time. Experimental results on sklearn and lightGBM show that applying our coresets on real - world data-sets boosts the computation time of random forests and their parameter tuning by up to x10, while keeping similar accuracy.   The main contribution of the paper is by forging a link between decision trees from machine learning – to partition trees in computational geometry – to divide trees in a computational geometry geometry geometry. It is shown that applying the first outputs of the proposed algorithm, D of N entries ( labels ) to a set of real data - sets boosts the computational time of the random forests by as much as 1. The authors provide experimental results on Sklearn and LightGBM as well as some real world data sets to show the benefits of their algorithm."
SP:f3ece96b15ec06d703925df2061ed9694ec3bca5,"This paper studies Top - m identification ( m identification of arms with largest means under a fixed error rate under misspecified linear bandit models ), where the data inevitably deviates from linearity. This problem is motivated by practical applications, especially in medicine and recommendation systems, where linear models are popular due to their simplicity and the existence of efficient algorithms. The authors first derive a tractable lower bound on the sample complexity of any correct algorithm for the general Top - M identification problem, and then describe the first algorithm for this setting, which is both practical and adapts to the amount of misspecification. They show that the upper bound matches the lower bound when the complexity of the algorithm is upper than its sample complexity. Finally, they describe an algorithm on synthetic data showing both synthetic and real - world data, showing competitive performance with respect to existing baselines."
SP:e71c5e39b8d8d1640d6de2352ac51ddd52eea89d,"This paper proposes a new method for learning disentangled graph representations with self - supervised learning for graph neural networks ( GNNs ). The proposed method is based on the Disentangled Graph Contrastive Learning ( DGCL ) approach. The key idea is to first identify the latent factors of the input graph and derive its factorized representations, and then propose a novel novel factor - wise discrimination objective in a contrastive learning manner, which can force the representations to independently reflect the expressive information from different latent factors. The method is evaluated on both synthetic and real - world datasets, and it is shown to outperform several state - of - the - art baselines."
SP:0a7edbbdabab11273689c40c517001eb46491113,This paper presents a method for assessing the robustness of networks trained on a large panel of statistical models. It is based on a stochastic simulation inspired by the field of Statistical Reliability Engineering. The robustness assessment is cast as a statistical hypothesis test : the network is deemed as locally robust if the estimated probability of failure is lower than a critical level. The authors derive theoretical guarantees that are nonasymptotic w.r.t. the sample size. They outline the efficiency of their method making a low number of calls to the network function. This work presents a scalable and efficient procedure assessing corruption robustness. It provides completeness and theoretical guarantees on the lack of soundness.
SP:c1db485ff1ff9573daa421e167225654babb55ac,"This paper introduces a general framework, called CoPE, that enables a polynomial expansion of two input variables, i.e., the noise variable and the conditional variable, to be used for conditional generation of images. The authors show how CoPE can be trivially augmented to accept an arbitrary number of input variables and show how the cross - relation between two variables can be captured. CoPE is evaluated in five tasks ( class - conditional generation, super - resolution, translation, image - to - image translation, attribute - guided generation ) involving eight datasets. The experiments show that CoPE performs well on all five tasks, and that it can be used to generate high - quality images on a variety of conditional generation tasks, e.g., by defining task - specific recursive formulations. To facilitate reproducibility, the authors also provide source code for CoPE."
SP:5a75bc7a3ea0ce971cfceebbc1c2434e3aa2584d,"This paper proposes a new method for computing MMD statistics based on neural tangent kernel ( NTK ) and MMD. The main idea is to compute the MMD statistic based on NTK and perform NTK based two - sample tests to address the long - standing challenge of memory and computational complexity of MMD, which is essential for online implementation to assimilate new samples.   Theoretically, the authors show that there is a connection between the NTK test statistic and the kernel MMD and prove that the latter is more computationally efficient and memory - efficient than the former. Theoretical experiments on synthetic and real - world datasets validate the theory and demonstrate the effectiveness of the proposed NTK-MMD statistic. The proposed method can also be used for other applications of NTK - MMD statistics, such as hypothesis tests and testing the hypothesis of neural networks."
SP:1df2ffbbe56b8018067820980b93af2a8b57f891,"This paper proposes a method to train an autoencoder G(x ) that is class - dependent via a trade - off between reconstructing x by G ( x ) and classifying x by D(x − G( x ), where the former competes with the latter in decomposing x so the latter retains only necessary information for classification in x − G (x ). The method is applied to both clean images and adversarial images and the authors claim that this simple approach substantially improves the detection and defense against different types of adversarial attacks. The authors conduct extensive experiments on both CIFAR and ImageNet dataset and additionally provide novel interpretations to the results. Inspired by these observations, the authors propose to conduct adversarial detection and adversarial defense respectively on x, where they consistently outperform the results on the original x. In experiments, they also discover that the perturbations generated by adversarial attack are mainly generated by G(X ) and that the predictions made by the defense based on G(y ) are significantly better than those made by G by D."
SP:2789874561620ba7894c4672f935056bb911e919,"The authors introduce DP - FTS - DE, a federated Thompson sampling algorithm for Bayesian Optimization ( BO ) in the federated learning setting of the FL setting. The proposed algorithm is based on the differential privacy ( DP ) framework proposed in [ 1 ], which provides a general framework for adding DP to iterative algorithms. The authors leverage this framework to improve the utility of distributed exploration ( DE ) to further improve the privacy of Bayesian Thompson Sampling ( FTS ) algorithm. They provide theoretical guarantees for both the privacy and utility of the proposed algorithm. Theoretical guarantees are provided for both privacy loss and utility loss. Experimental results are provided to show that the proposed method achieves high utility ( competitive performance ) with a strong privacy guarantee ( small privacy loss ) and induces a trade - off between privacy and privacy - utility in real - world applications."
SP:be7d6b81736a2c3f89abd8771b41b18802e88832,"This paper proposes a new active learning method for multi - label active learning ( AL ) based on Gaussian Process - Bayesian Bernoulli Mixture model ( GP - BM ). The main idea is to combine Gaussian process ( GP ) and Bayesian bias ( BM ) to quantify a data sample ’s overall contribution to a correlated label space and choose the most informative samples for cost - effective annotation. The model encodes label correlations using a Bayesian mixture of label clusters, where each mixture component corresponds to a global pattern of label correlations. The number of mixture components is dynamically adjusted during inference so that the model complexity is automatically calibrated according to the size of training data, which is critical for AL. The proposed GP - B2M achieves the state - of - the - art active learning performance in the proposed model. Experiments on real - world multi -label datasets demonstrate that the learned mixture components accurately capture complex label correlations that are critical for active data sampling. They are interpretable, which can help to unveil important relationships among labels."
SP:2b7270b0370c193300bcbbb5fb0a4101b3329d99,"This paper proposes a new method to improve the performance of the closed - form lidar method. The proposed method is based on the fact that previous work has shown that the end - to - end latency of lidar methods can be reduced significantly when operating on wedge - shaped point cloud sectors rather then the full point cloud. However, due to use of cartesian coordinate systems these methods represent the sectors as rectangular regions, wasting memory and compute. In this work, the authors propose to use a polar coordinate system and make two key improvements on this design. First, they increase the spatial context by using multi - scale padding from neighboring sectors : preceding sector from the current scan and/or the following sectors from the past scan. Second, they improve the core polar convolutional architecture by introducing feature undistortion and range stratified convolutions. Experimental results on the nuScenes dataset show significant improvements over other streaming based methods."
SP:7ae2c5b7d9c8a6c8f4a353606aa419929c47f31b,"This paper proposes a framework to define distributions over structured latent variables in deep learning. In contrast to the standard approach, which defines a latent variable as a perturbed algorithm output and uses a differentiable surrogate for training, this paper proposes to use the Gumbel - Max trick to define the differentiable surrogates by leveraging the score function estimators for optimization. In particular, the authors highlight a family of recursive algorithms with a common property, called stochastic invariant, which allows them to construct reliable gradient estimates and control variates without additional constraints on the model. In the experimental section, they consider various structured latent variable models and achieve results competitive with relaxation - based counterparts.   The main contribution of this paper is the introduction of a framework, along with a low - variance score estimator, to allow training models that do not admit relaxed variables and improve optimization by alleviating the bias of the relaxed estimators."
SP:415d363c66a6967c1daca9dc02001b85bf7f0752,"This paper proposes GainTuning, a method for adapting CNN denoisers trained on large datasets to a single noisy test image. To avoid overfitting, it optimizes a single multiplicative scaling parameter ( Gain ) of each channel in the convolutional layers of the CNNs. The method is tested on standard image - denoising benchmarks, boosting their performance on nearly every image in a held - out test set. It is also tested on transmission - electronmicroscope data at extremely low signal - to - noise ratios, where it is shown to faithfully reconstruct the structure of catalytic nanoparticles from these data at a low signal to noise ratio.   The main contributions of the paper are : 1. Gain tuning is a novel method for adaptively adapting CNN models pre - trained on a large dataset on which the distribution of the test images deviates systematically from the training distribution. 2. It improves state - of - the - art CNNs on standard imagedenoising benchmarks. 3. It provides a method to adaptively adjust the Gain parameter in each channel of CNNs in order to reduce overfitting."
SP:90afa1102683b456bc72a54abef466326827546a,"This paper proposes a new method for solving the combinatorial optimization problem of panoptic segmentation ( COPS ). The proposed method is based on a convolutional neural network ( CNN ) and an asymmetric multi - way cut problem solver. The goal is to find a solution that maximizes a smooth surrogate of the quality metric of segmentation by backpropagating the gradient through the optimization problem w.r.t. the objective function. The method is evaluated on two datasets, Cityscapes and COCO, and compared with two baselines. The results show that the proposed method outperforms the baselines on both datasets.   The main contributions of the paper are as follows : ( 1 ) A new method is proposed for solving combinatorially optimization of COPS. This is a very interesting and important problem. It is an important problem that needs to be solved and the method proposed addresses it well. ( 2 ) This paper is a good starting point for future work in this problem. The paper is well written and well motivated. The main weakness is that it does not provide clear direction on how to choose the CNN or the cut. It does not clearly state what the goal is of the method. The authors should clearly state the goal and give a clear explanation of how the method is differentiable. ( 3 ) It is important to clearly state that the goal of this paper is not to solve the problem because it is not an optimization problem. ( 4 ) The main contribution of this work is to propose a new way of solving COPS that is different from the existing methods. ( 5 ) This work is well motivated and well written."
SP:1952e174d9ec7b83ad1d394ece7fe77ea1f6d78d,"This paper proposes Recursive Bayesian Networks ( RBNs ), which generalise and unify PCFGs and DBNs, combining their strengths and containing both as special cases. The main challenge lies in performing joint inference over the exponential number of possible structures and the variables. The authors provide two solutions : ( 1 ) For arbitrary DBN, they generalise PCFG to the mixed discrete - continuous case, which allows for maximum posterior estimates of the continuous latent variables via gradient descent, while marginalising over network structures ; ( 2 ) For Gaussian data, they additionally derive an analytic approximation of the marginal data likelihood ( evidence ) and marginal posterior distribution, allowing for robust parameter optimisation and Bayesian inference. The experiments are conducted on synthetic data and an application to the challenging task of hierarchical music analysis.   The main contributions of the paper are as follows :   1 ) The authors propose a new type of Bayesian network, which they call Recursive Networks. They define a joint distribution over tree -structured Bayesian networks with discrete or continuous latent variable. This allows for segmentation and tree induction from noisy sequences, compared to change point detection and hierarchical clustering. In addition, they approach the unsolved problem of musical data analysis from the raw note level and compare their results to expert annotations."
SP:5f29b169d3e4bbaeeec85e1aeebe2094fae4be6e,"This paper proposes a constrained backpropagation ( CBP ) algorithm based on the pseudo - range multiplier method to obtain the optimal set of weights that satisfy a given set of constraints. It is claimed that the Lagrangian function ( loss function plus constraint function ) is the defining characteristic of the proposed CBP algorithm, and that it is the only one that utilises Lagrangians as its objective function. The authors evaluate the performance of CBP applied to AlexNet, ResNet-18, ResNet - 50, and GoogLeNet ( pre - trained with full - precision weights ) with four different constraints ( binary, ternary, one - bit shift, and two - bits shift weight constraints ) on ImageNet. The results highlight the classification accuracy outperforming the previous state - of - the - art results. This highlights CBP as a learning algorithm to address diverse constraints with the minimal performance loss by employing appropriate constraint functions. The code for CBP is publicly available at GitHub."
SP:3ddf8e2e108fb261bb23aec8a27a25aba7523dc1,"This paper proposes a novel active learning strategy for Gaussian process classification ( GPC ) based on estimated error reduction ( EER ) for active learning. The authors argue that existing active learning strategies that maximize the EER aim at reducing the classification error after training with the new acquired instance in a onestep - look - ahead manner are computationally prohibitive as it requires retraining the GPC with every new query. To overcome these limitations, the authors develop computationally efficient algorithms for EER - based active learning with GPC based on the joint predictive distribution of label pairs as a one - dimensional integral, as a result of which the computation of the acquisition function avoids retraining GPC for each new query, remarkably reducing the computational overhead. They also derive the gradient chain rule to efficiently calculate the gradient of acquisition function, which leads to the first query synthesis active learning algorithm implementing EER-based strategies. The experiments clearly demonstrate the computational efficiency of the proposed algorithms, which show superior performance in terms of sampling efficiency compared to the existing state - of - the - art algorithms."
SP:fa1fac04cd4ccb1f3eaf80807db09f9683ce6b50,"This paper studies the problem of unbounded gradients in continuous autoencoder - based autoencoders ( VAE ). The authors show that under certain assumptions on the energy function of VAE, the gradients tend to infinity, which they claim is due to over - regularization of the model's energy function. They show that this energy function can be replaced with an energy function with infinite gradients if the objective is to avoid over - normalization and to simultaneously achieve bounded gradients around optimal representations. However, if the goal is to achieve both of these goals simultaneously, the authors find that under these conditions, the number of gradients will be arbitrarily small and unbounded. In other words, VAE models with unbounded energy function tend to have latent representations that are worse than those with regularized energy function, i.e., better than the ones obtained by good local minimizers.   The authors then show how to deal with this problem by considering the case when the input is a low - dimensional manifold ( e.g., a set of pixels on the surface of a 2D image ) and the output is a high dimensional manifold of the same dimension. They find that the output of a VAE model with bounded energy function will be better than that of a model with no energy function at all. This is true for both high dimensional and low dimensional data. They go on to show how under regularization, the gradient of the input will tend to be smaller than the output. This in turn leads to suboptimal feature selection, which can lead to poor generated samples. They argue that this happens because VAE energy function is not properly regularized. Finally, they show how this can be reversed, which leads to under - regularized gradients."
SP:2611cfd6e0696a57d061687993cef1fe5c95999d,"This paper studies the min - max regret bound for the bandit problem with feedback graphs. The problem is modeled by a directed graph G = ( V, E ), where V is the collection of bandit arms and E is the number of incident arms of each bandit arm. The paper proposes two ways to study the bound, i.e., the fractional weak domination number and the k - packing number capturing upper bound, and the lower bound for regret respectively. They show that the two notions are inherently connected via aligning them with the linear program of the weakly dominating set and its dual. Based on this connection, they utilize the strong duality theorem to prove a general regret upper bound. They also show that for several special families of graphs, they can get rid of the ( log | V | ) 1 3 factor and establish optimal regret bounds."
SP:e50dec57af337839cbde4b65fb7b431785fda44d,"This paper studies the interpretability of neighbourhood Shapley values ( SHAP ) for the case when the feature absence is simulated using a global population. The authors argue that the use of global population can lead to potentially misleading results when local model behaviour is of interest. To address this issue, they propose neighbourhood reference distributions that improve the interpretation of SHAP. The formulation of the neighbourhood reference distribution is based on the Nadaraya - Watson estimator, a well - studied kernel regressor that can be expressed as a self - normalised importance sampling estimator. Empirically, they observe that Neighbourhood Shapley value identify meaningful sparse feature relevance attributions that provide insight into local model behavior, complimenting conventional Shapley analysis. They also increase on - manifold explainability and robustness to the construction of adversarial classifiers. They consider how smoothing can also be used to stabilise SHAP values."
SP:35bdeb78f9fe74e754177fb54b48e7399dc8590d,"This paper proposes a novel method, PlayVirtual, which augments cycle - consistent virtual trajectories to enhance the data efficiency for feature representation learning in deep reinforcement learning ( RL ). The main idea is to use a trajectory cycle to generate a large amount of virtual state - action sequences, which can be used for better feature learning for un - experienced or less - experienced RL agents. PlayVirtual consists of two components. First, it predicts future states in a latent space based on the current state and action by a dynamics model and then predicts the previous states by a backward dynamics model, which forms a trajectories cycle. Second, based on this, it augments the actions with a trajectory to meet the cycle consistency constraint. The method is validated on discrete control benchmark Atari [ 2 ] and continuous control benchmark DMControl Suite [ 43 ], where it is shown to achieve the best performance on both benchmarks."
SP:ca09e472cbcf2ac8c8c9b192a87df2ed59218210,"This paper investigates how network architecture affects the robustness of neural networks to noisy labels in two domains : target and noise. Specifically, the authors propose a formal framework that connects the predictive power of a neural network to the alignments between its architecture and the noise function. This framework is based on a linear model trained on a small set of clean labels and a linear network trained on the learned representations of the clean labels. The authors hypothesize that a network is more robust to noise labels if its architecture is more aligned with the target function than the noise. To support their hypothesis, they provide both theoretical and empirical evidence across various neural network architectures and different domains. They also find that when the network is well - aligned with target function, its predictive power in representations could improve upon state - of - the - art methods in terms of test accuracy."
SP:903727fe028684623a8ccadec210e641ecffc685,"This paper proposes a novel method to learn a reward function by using data to replace the reward function in reinforcement learning ( RL ). The paper proposes to learn the value function from transitions and successful outcomes, without learning an intermediate reward function. The key idea of the algorithm is to directly learn to predict whether the task will be solved in the future via classification without using separate reward learning and policy search procedures.   The main contribution of the paper is an algorithm for off - policy example - based control. The authors show that their method outperforms state - of - the - art imitation learning methods ( AIRL [ 7 ], DAC [ 17 ] and SQIL [ 28 ] ) and recent methods that learn reward functions ( ORIL [ 41 ], PURL [ 38 ], VICE [ 8 ] ). They show that the method satisfies a new data - driven Bellman equation, where examples take the place of the typical reward function term in the equation. Experiments show that our approach outperforms prior methods that learned explicit reward functions. Empirically, our method significantly outperforms SQIL and ORIL."
SP:39ccbd5909a1d7ed212fe92d8d6843c2c70dfe1f,"This paper studies differentially private stochastic optimization in convex and non - convex settings. For the convex case, the authors focus on the family of non - smooth generalized linear losses ( GLLs ). They provide an algorithm for the l2 setting that achieves optimal excess population risk in near - linear time, while the best known differentially public algorithms for general convex losses run in super -linear time. The algorithm for l1 setting has nearly - optimal excess risk and circumvents the dimension dependent lower bound of [ AFKT21 ] for general non - Smooth convex loss. In the nonconvex setting, they provide several new algorithms for approximating stationary points of the population risk. For l2 - case with smooth losses, they obtain a linear - time algorithm with rate Õ ( 1 n1/3 + d 1/5 ( nε ) 2 / 5 ). Finally, for   l2-case with polyhedral constraint,   the authors provide the first method for non- Smooth weakly convex optimization.   The settings investigated are also novel in the DP literature and are also not well known in the mainstream literature. Some of the settings are novel and some of them are well known. The goal of this work is to provide faster and more accurate methods for DP -SO."
SP:99a476f71e6901aefe281f11fb72ff78265a5b6e,"This paper studies cooperative bandit learning under three real - world communication scenarios, i.e., message - passing over stochastic time - varying networks, instantaneous reward - sharing over a network with random delays, and adversarially corrupted reward - passing with adversarial corruptions. The authors propose decentralized algorithms that achieve competitive performance in each of these environments, along with near - optimal guarantees on the incurred group regret as well as an improved delayed - update algorithm that outperforms the existing state - of - the - art on various network topologies. The proposed algorithms are straightforward to implement and obtain competitive empirical performance.    In the setting with perfect communication, the authors provide an improved version of their algorithm, which is based on a gossip - based communication protocol for cooperative multi - armed bandits. However, there are several differences in the setup considered in Chawla et al compared to our setup, such as the fact that their regret depends on the difficulty of the first arms rather than the total number of arms. In contrast, our algorithm will always provide a speed up of order ↵(G )N regardless of the arms themselves, and when we run our algorithm by setting the delay parameter = d?(G) (diameter of the graph G ), we obtain an O( 1N ) speedup regardless of   the sparsity of G, based on the graph structure, which can dominate the log T term when we have a large number of agents present. Furthermore, we can see that the authors obtain a constant between O(K + ( log N ) and O(N ) for some 1. In the case of some 1, the constant is O( K )."
SP:d3e896a65470f2439bc7753b4f66e152306b2d6f,"This paper proposes a quantization method for post - training quantization in vision transformers to reduce the memory storage and computational costs. The proposed method introduces a ranking loss into the conventional quantization objective that aims to keep the relative order of the self - attention results after quantization. Moreover, the authors explore a mixed - precision quantization scheme by exploiting the nuclear norm of each attention map and output feature calculated by the attention map. Experimental results on several benchmarks demonstrate the effectiveness of their algorithm for achieving better performance over the state - of - the - art quantization approaches.   The quantized process in the transformer is formulated as an optimization problem for finding the optimal quantization intervals. Specially, their goal is to maximize the similarity between the full - precision and quantized outputs. The authors also analyze the relationship between quantization loss of different layers and the feature diversity. The effectiveness of the proposed method is verified on several benchmark datasets."
SP:aa6b1328585b5916267a3ff4f9119e7aa4ce2bb5,"This paper studies the convergence rate of synchronous and asynchronous Double Q - learning ( DQ - learning ), where the former is a constant - time learning rate and the latter is a finite - time learner. The authors claim that synchronous Dq - learning converges to a global optimum with a time complexity of $ \mathbb R^\mathbb{R}^D$ while the asynchronous algorithm converges at a complexity $ \rhoR^L$. The authors argue that this is due to the fact that the learning rate is polynomial in nature, and that asynchronous algorithms tend to prefer a slower learning rate due to their higher complexity. They also propose novel analysis techniques and provide affirmative answers to the above questions.   The main contributions of the paper are as follows :   1. Developing new analysis techniques that improve the existing convergence rate by the order of magnitude in terms of its dependence on all major parameters ( 1., 1 − \�,D,L ). 2. Introducing a new learning rate strategy, i.e., the discount factor, which is a parameter related to the sampling strategy for synchronous double - q - learning. 3. Providing experimental results that show that the learned rate converges faster than the one used in the previous analysis."
SP:04fd4d83717c4f7e1a4b5651a59200151f33411d,"This paper presents an approach for semi - supervised ( SSL ) OOD detection, where the goal is to detect out - of - distribution ( OOD ) samples during training. The paper proposes a new method, called Structure - keep - unzipping ( STU ), to learn a new representation space in which OOD samples could be separated well. The method is tested on four benchmark datasets, including the CIFAR-10, MNIST, and Fashion MNIST datasets.    The main contributions of the paper are as follows :   1 ) The paper presents a novel approach for learning OOD - detectable samples from unlabeled data. This is in contrast to previous approaches that rely on labeled data and in - distribution data, which are assumed to be the same distribution as labeled data. The main challenge of this paper is that the labeled data are not available, and thus, the proposed STU could not be used to train the algorithm. 2 ) The training data used for training the STU algorithm is not available for testing. Therefore, the authors propose to use data from an unknown distribution to train their algorithm instead. The proposed method outperforms the other methods by a large margin. 3 ) The experimental results show that STU outperforms other methods and achieves remarkable detection performance on several benchmarks. 4 ) The authors further discuss the advantages of the proposed method over other methods in terms of accuracy and cost."
SP:6bf8b94483b26033795b0eda9649518027f5e1c2,"This paper proposes a transformer - based architecture for the task of referring expression comprehension ( REC ) and segmentation ( RES ). The authors leverage a simple one - stage transformer based encoder - decoder, where the encoder is trained to generate contextualized lingual queries which are then used to directly regress the bounding box and produce a segmentation mask for the corresponding referred regions. The paper shows that the proposed model outperforms the state - of - the - art on both tasks by significant margins ( up to 19.4 % for RES and 8.5 % for REC ). Experiments on a simple pre - trained dataset ( an external dataset ) are also conducted to validate the effectiveness of the proposed architecture."
SP:29b552b36696c9bda72f3ab4f31605d98880fd6b,"This paper studies the problem of weak - learner - based boosting, where the weak learner is assumed to belong to an easy - to - learn base class and the booster attempts to learn a combination of weak hypotheses by repeatedly calling the learner. The authors focus on an especially natural formulation in which the weak hypotheses belong to a class that is easy to learn. They show that the resources required by the booster depend on the number of classes k and the amount of calls made to the weak learners. They also prove a trade - off between number of oracle calls and resources required of the weaklearner.    The main contribution of the paper is a thorough study of a variety of natural weak learning assumptions for multiclass boosting, and showed that each of those assumptions implies the plurality vote assumption ( i ), ii ), iii ), and iv ). Finally, the authors show that weighted votes are extremely expressive and can approximate arbitrarily complex concepts by weighted votes."
SP:f63b050773871338c48b778c362172e4b72477a4,"This paper presents a novel method for unsupervised learning of object representations without supervision in the form of GENESIS - V2, a model that learns to segment objects in images without supervision and that uses an autoregressive prior to generate scenes in an interpretable, object - centric fashion. The method is based on the embedding - based approach in which embeddings of pixels are clustered in a differentiable fashion using a stochastic stick - breaking process. This clustering procedure also leads to randomly ordered object representations, but without the need of initialising a fixed number of clusters a priori. The authors benchmarked the method on two synthetic datasets ( ObjectsRoom and ShapeStacks ) and two real - world datasets ( Sketchy and the MIT - Princeton Amazon Picking Challenge ) and compared it to several recent baselines ( RNNs, iterative refinement and prior art ). The results show that the proposed method outperforms the baselines in terms of segmentation and object segmentation as well as scene generation."
SP:408deb9e5577ee7118b836fee77135df641fe545,"This paper develops adaptive conformal inference ( ACI ), a method for forming prediction sets that are robust to changes in the marginal distribution of the data in the data - generating process. The authors model the distribution shift as a learning problem in a single parameter whose optimal value is varying over time and must be continuously re -estimated. ACI can be combined with any black box method that produces point predictions of the unseen label or estimated quantiles of its distribution. They show that ACI achieves the desired coverage frequency over long - time intervals irrespective of the true data generating process and that it can obtain approximate marginal coverage at most time steps. They test ACI on two real world datasets and find that its predictions are strong to visible and significant distribution shifts."
SP:e6e5b1e2428abcf1a163ec1cce15cd299f9a544f,"This paper proposes a novel method for pose estimation in multi - person scenes, based on the Part - based Pose Generation ( PPG ) and Pose Refinement ( PPR ) modules of OPECNet. The PPG module is used to infer multiple coarse poses for each person from his/her body parts, refined by incorporating pose priors, and finally fused in the Pose Fusion module. The refined coarse poses are then used to generate the global pose for a person. The global pose is then used in the PINet module to infer the complete pose cues for that person from all body parts. PINet is trained using the OCH - MANU dataset. The performance of the proposed method is compared against the state - of - the - art methods ( OPECNet and DPLI ), as well as two other methods ( PINet and DPLI ). The results show that PINet outperforms the other methods in most of the cases."
SP:e76f048c3dccffcb8bcc6a66f6165fc19d175610,"This paper proposes a method for solving S - rectangular robust Markov decision processes ( RMDPs ) with L - concained rectangular ambiguity sets. The authors propose a method that combines a homotopy continuation method with a bisection method to solve S - rectangular ambiguity in quasi - linear time in the number of states and actions. The proposed method improves on the cubic time required by leading general linear programming methods. The experimental results confirm the practical viability of the proposed method and show that it outperforms a leading commercial optimization package by several orders of magnitude.   The authors claim that the method could be applied to a wide range of applications. The main contributions of the paper are as follows :   1. A method for computing the Bellman operator for S - Rectangular Markov Decision Processes. This is a very interesting problem that has been a long time problem for RL researchers. It is well known that Bellman is the most important operator in RMDP solvers. However, it has been proved that it is difficult to find an exact solution for this problem. This paper proposes to solve this problem by combining the homotopic continous method and the bisected method. The method is based on the fact that the bisections of the problem are easier to find than the concatenation of the rectangles of the decision processes. The homotopically bisected problem can be solved much faster than the one solved by the linear programming method. This leads to a reduction in the time required to solve the RMDP. The paper claims that this could be done in a fraction of the time. 2. A set of experiments are conducted to validate the effectiveness of the method."
SP:c4af66a64a5c2bd58ca2e29dbc4b27d5bf4b63b8,"This paper studies the problem of online knapsack prediction in the form of the generalized one - way trading and two - stage trading setting. The main contribution of the paper is to study the competitive ratio between the most competitive and least competitive online algorithms for fixed frequency predictions of the problem. The competitive ratio is defined in terms of the ratio of the expected return of the algorithm compared to the expected loss of the optimal algorithm. The paper shows that even seemingly weak predictions can be utilized effectively to provably improve the performance of online algorithms.    The main contributions of this paper are as follows :   1. The authors study the problem by obtaining upper and lower bounds for the number of items of each value $ V, $ V$, and vmin = min(V ), $ vmax = max(V, respectively. They show that for any value $ 2 V, let sv = P i|vi=v si denote the total size of items with value v. 2. For each value v 2, the frequency predictions P provided to the algorithm are a lower bound `v and an upper bound uv such that `v   v2V `, where `v ` is the upper bound of the set of all possible values that items may take. 3. The algorithm has a competitive ratio of ↵ for KNAPSACK - FP with prediction P if the algorithm ’s profit is at least ↵ times the optimum for all inputs respecting the given frequency prediction P. 4. The goal is to design an online algorithm with the best competitive ratio for this prediction model. 5. The method is applied to the one way trading setting and two stage trading settings and shows that it achieves competitive ratio only marginally worse than the best algorithm."
SP:1d478d4fa3f5df0ded963ef164325667fd744dbb,"This paper proposes a new model - based episodic memory for reinforcement learning. The memory estimates trajectory values, guiding the agent towards good policies. Based on the memory, the authors construct a complementary learning model via a dynamic hybrid control model that unifies model based, episodic and habitual learning into a single architecture. Experiments demonstrate that the proposed model allows significantly faster and better learning than other strong reinforcement learning agents across a variety of environments including stochastic and non - Markovian settings. The authors also interpret model behavior and provide analytical studies to validate their empirical results.   Contributions :   ( 1 ) The authors propose a new episodic control method that addresses limitations of current episodic RL through a Dynamic Hybrid Control method. The method is based on the idea that the best way to learn from past experiences is to recall past experiences and use them to build a model of trajectories. This is similar to what is done in prior work. The main difference between the proposed method and prior work is that the authors propose to use a different type of memory for each trajectory. The previous work used a memory for trajectories and used it to estimate trajectory values. The proposed method uses a different kind of memory that estimates trajectories based on past experiences. This allows the authors to build on top of the memory from previous work and use it to build complementary learning models. The advantages of this approach over the previous one is that it does not require the trajectories to be drawn from a fixed set of states. The disadvantage is that there is no guarantee that trajectories drawn from the fixed from the past will be close to the current state. This means that the agent may end up with trajectories that are closer to the ground zero states than the current one. This in itself is a limitation of the current method. This paper addresses this limitation by proposing a new method that does not rely on the past trajectories as input to the model. The paper also proposes a dynamic control method to address the limitations of the episodic method."
SP:551174c1266b5f4b6aaf5432a4c713386f90898c,"This paper proposes a semi - supervised learning method, DP - SSL, for unlabeled data, that uses data programming to generate probabilistic labels. The proposed method is different from the existing SSL methods that rely on human experts to provide initial labeling functions ( LFs ). Instead, the proposed method uses a multiple - choice learning ( MCL ) based approach to automatically generate LFs from scratch in SSL style. The paper also proposes a label model to resolve the conflict and overlap among the noisy labels produced by the LFs. The experiments on four standard benchmarks show that DP -SSL outperforms the state - of - the - art methods, especially when only a small number of labeled samples are available."
SP:d1d6a40a8bde62a21da4fc18a076e344c84ab0d0,"This paper presents a multi - view pose transformer ( MVPT ) for estimating multi - person 3D poses from multi - views images. The proposed approach is based on directly regressing the 3D joint locations of the multi - viewed body parts from the input images. To represent the joint locations, the authors propose a hierarchical scheme to concisely represent query embeddings of multi -person skeleton joints and introduces an inputdependent query adaptation approach to improve the accuracy. The authors also propose a novel projective attention module along with a RayConv operation for fusing multi -view information effectively. The experimental results show that the proposed approach outperforms the state - of - the - art methods on several benchmarks while being much more efficient."
SP:2e147bd5321e25bb27d2531fd58c46460a1e5320,"This paper addresses two learning problems : 1 ) Support recovery for a family of sparse vectors, where each vector has at most k non - zero elements, and 2 ) designing queries such that all sparse vectors from the family can be approximately reconstructed based on the error - free responses of the learning model introduced in the work of Gandikota et al. 2020. These problems can be seen as generalizations of support recovery and approximate recovery problems, well - studied under the framework of 1 - bit compressed sensing. The main contribution of the paper is to prove the existence of learning algorithms for the first problem which work without any assumptions. Under a mild structural assumption on the unknown vectors, it is also shown how to learn with high probability the supports of all unknown vectors without any assumption. The authors also show how to improve the query complexity for the second problem by using tensor decomposition techniques and constructions of union - free families."
SP:e3388e479a825be429f3a878e2c4d8b05903ff10,"This paper considers the problem of detecting abrupt temporal change in temporal behavior patterns through the use of a suite of sensors for detecting abrupt changes in behavior patterns. The authors propose a computationally efficient online sensing scheme, which seamlessly balances the need for exploration of different sensing options with exploitation of querying informative actions. They derive expected delay bounds for the proposed scheme and show that these bounds match the information - theoretic lower bounds at low false alarm rates. They then perform a number of experiments on synthetic and real datasets on which they demonstrate the effectiveness of their proposed method.   The main contributions of the paper are as follows :   1 ) They propose an information - theoretic lower bound on the detection delay for a general class of finitely parameterized probability distributions, which they then propose to use for the bandit - quickest changepoint detection problem, where sensing actions ( or sensors ) are sequentially chosen, and only measurements corresponding to chosen actions are observed. 2 ) They show that this lower bound matches the bounds of their information - theoretical lower bound for the expected delay for detection delay in the case of high false alarm rate detection. 3 ) They demonstrate that the lower bound of their lower bound is in fact the upper bound of the upper bounds of the lower bounds for expected delay in this case."
SP:268260e9452ba2bc57e50a6b7b3328233137ac9b,"This paper unifies several SGD - type updates for stochastic nested problems into a single SGD approach that they term ALternating Stochastic gradient dEscenT ( ALSET ) method. The main goal of this paper is to study the efficiency of running the vanilla alternating SGD ( 4 ) for the nested problem ( 1 ), and its implications on the special problem classes ( 2 ) and ( 3 ). By leveraging the hidden smoothness of the problem, this paper presents a tighter analysis of ALSET for solving stochastically nested problems. Under the new analysis, it is possible to achieve an - -stationary point of the goal in each of the four nested problems, it requires O(2 ) samples in total under certain conditions.   The main contribution of the paper is applying this analysis to the nested bilevel, min - max, and reinforcement learning problems. The three problems share a nested structure, and existing works often treat them separately, thus developing problem - specific algorithms and analyses for solving them. In this paper, the authors combine these three problems into one nested structure and study their convergence rates. The convergence rate of the proposed method is faster than that of SGD for non - nested problems and slower than for SGD in nested problem. However, the paper also shows that SGD convergence rate for nested problems is slower than SGD / SGD update rate in the nested case. This paper shows that the proposed ALSET method can be used to solve nested problems with faster convergence rate. The paper also provides an analysis of why SGD and SGD updates do not work well in practice."
SP:82ad52361bc5b2c421f1dc6b76e1a5520570fc6c,"The paper proposes Siamese Sampling and Reasoning ( SiaSamRea ), a method to generate similar clips from the same video using siamese sampling, and a reasoning strategy for integrating the interdependent knowledge between contextual clips into the network for video question answering ( VideoQA ). The method is based on fine - tuning each clip - text pair independently on the pretrained transformer - based model via supervised learning. The reasoning strategy consists of two modules : ( 1 ) siamesese knowledge generation generation to learn the inter -relationship among clips ; ( 2 ) siaamese reasoning to produce the refined label by propagating the weights of inter - relationship to the predicted candidates of all clips. Experiments demonstrate that the proposed method achieves state - of - the - art performance on five VideoQ a benchmarks, e.g. +2.1% gain on MSVTT - QA, +1.9% on MSVD - Q A, + 1.8% on How2QA and +4.3% ( action ) on TGIF -QA."
SP:160022e2cd61159da92f92e85520b7062a337a8d,"This paper presents a novel approach to learning structured probabilistic representations from observed data for a variety of tasks, such as language modeling, unsupervised grammar induction, and video modeling. The authors propose a matrix - vector based inference method based on a low - rank constraint and a rank - based constraint to approximate the expressivity and speed of inference of structured models such as Hidden Markov Models ( HMMs ), Probabilistic Context - Free Grammars ( PCFGs ), and HSMMs. The central idea of the approach is to view the inference step as a matrix where each state is represented as a vector and the constraints are the sum of the rank of each state in each vector. The paper then shows that this matrix - based inference can be applied to a number of different structured models including HMMs, PCFGS, and HMMs as well as hidden semi - universal Markov models ( HSMMs ). The main contributions of the paper are as follows :   1. Demonstrating a simple approach to reduce the computational complexity and memory complexity of a large class of structured model such as HMMs. 2. Showing that applying a low rank constraint to the central inference step of HMMs allows to approximate expressivity of the model in terms of the number of states in each state. 3. Providing a method for applying low rank constraints to the high - rank constraints of the PCFG model.   The paper is well written and well presented, and the method is well motivated. However, I have some major concerns that I have not addressed in the paper. I will be happy to raise my score to a 6th or 7th place. I hope that the authors will clarify my concerns."
SP:238592ad73927194cdf0c0cf9ae2e48ca86e182c,"This paper introduces Sample Average Uncertainty ( SAU ), a simple uncertainty measure for contextual bandits that is claimed to be more efficient than Thompson Sampling and other exploration strategies like Bayesian exploration. The uncertainty measure is based on the distribution of the parameters of the action - value function, which is the outcome model of the environment. The authors show theoretically that the uncertainty measure estimated by SAU asymptotically matches the uncertainty measured by the approximate posterior methods that have been shown to underperform in the deep bandit scenario. Because of its simplicity SAU can be seamlessly applied to deep contextual bandits as a very scalable drop - in replacement for epsilongreedy exploration. They empirically show that SAU - based exploration outperforms current state - of - the - art deep Bayesian bandit methods at modest computation cost."
SP:ffc5b18f7e18607b2934e5aa199e7542005d79f4,"This paper presents a method for generating interpretable behavioral videos based on disentangled behavioral embedding. The method is based on the Disentangled Behavior Embedding ( DBE ) method. DBE disentangles the dynamic behavioral factors ( DBF ) from time - to - time embeddings by taking in the time - derivative of each dynamic behavioral factor and distancing it from the embedding space. The authors propose to combine DBE with a stochastic temporal model to learn continuous and discrete latent representations for simultaneous embedding and segmentation within the same model. The proposed method, VDBE, is end - to-end trainable in an unsupervised fashion, alleviating the need to train a second post - hoc model applied to latent embedding in the same way as DBE and DBE. Experiments are conducted on two tasks : behavioral motif generation and behavior decoding. The results show that the proposed method outperforms DBE on both of these tasks."
SP:bf78a450e4aad6b87fdeb8ec0d68adaaff7b595b,"This paper proposes DMTET, a deep 3D conditional generative model that can synthesize high - resolution 3D shapes using simple user guides such as coarse voxels. It marries the merits of implicit and explicit 3D representations by leveraging a novel hybrid 3D representation. Compared to the current implicit approaches, which are trained to regress the signed distance values of the reconstructed surface, the proposed approach directly optimizes for the reconstructed surfaces, which enables them to synthesize finer geometric details with fewer artifacts. The core of the proposed method includes a deformable tetrahedral grid that encodes a discretized signed distance function and a differentiable marching tetrahedra layer that converts the implicit signed distance representation to the explicit surface mesh representation. This combination allows joint optimization of the surface geometry and the geometry of the topology.   The paper proposes a new method for training the proposed model, which is trained on a dataset of complex 3D animal shapes. The proposed method is based on Deep 3D generative models that directly generate explicit representations such as meshes, while the model can synthesise shapes with arbitrary topology and thus achieves better reconstruction quality than state - of - the - art methods while requiring a lower computation cost."
SP:2bc0bd6aa2a12691b16145f0d23542c4c86e3a44,"This paper proposes a new measure of mutual information ( MI ), sliced MI ( SMI ), as a surrogate measure of dependence. SMI is defined as an average of MI terms between one - dimensional random projections and two - dimension random projections. The authors claim that SMI preserves many of the structural properties of classic MI while gaining scalable computation and efficient estimation from samples. Furthermore, and in contrast to classic MI, SMI can grow as a result of deterministic transformations, which enables leveraging SMI for feature extraction by optimizing it over processing functions of raw data to identify useful representations thereof. This is supported by numerical studies of independence testing and feature extraction, which demonstrate the potential gains of SMI."
SP:e220b348901b476c2afd95f97630fb5400582f40,"This paper proposes a new method for two - step constrained Bayesian optimization ( BO ) of non - myopic acquisition functions. The proposed method is based on the reparameterization trick for more efficient derivative - based optimization of unconstrained BO. The authors argue that traditional myopic methods, such as average approximation and infinitesimal perturbation analysis, do not extend to constrained settings because constraints introduce discontinuities in the sampled acquisition function surface. Moreover, they argue that being non -myopic is even more important in constrained problems because fear of violating constraints pushes myopic method away from the limit.   The main contributions of the paper are as follows :   1. A new method is proposed to estimate the gradient of the two - steps optimal acquisition function of the 2 - O - POTT - C method. The main idea is to use the same likelihood ratio method that is used in importance sampling. The difference is that instead of using a measure of importance sampling, the authors use a measure that is the same type as the one used in the importance sampling trick. This means that the estimator of the gradient is unbiased. 2. The method is tested on two different settings of BO, sequential and batch settings. The experimental results show that the proposed method generally outperforms the state - of - the - art in terms of query efficiency by 2x or more over previous methods, and in some cases by as much as 10x more than previous methods."
SP:51fbd861422647912f275b48861ea3c4812afdc8,"This paper proposes Multi - dimensional Distributional DQN ( MD3QN ), which extends distributional RL to model the joint return distribution from multiple reward sources in reinforcement learning ( RL ). The main contributions of the paper are as follows :    1. This paper proposes a new method to model joint distribution between return distribution and reward distribution in RL. The authors claim that the joint distribution is more informative than the scalar value function in RL as it captures more information about the correlation between the randomness of different sources of reward and the distribution of rewards.   2. This work builds upon the idea of hybrid reward architectures ( HRA ) to model source - specific value functions for each source of reward, which is also shown to be beneficial in performance. The paper proposes to combine the benefits of HRA and distributional algorithms by minimizing the Maximum Mean Discrepancy ( MDE ) between joint return distributions and its Bellman target. 3. In experiments, the authors show that their method outperforms previous RL methods utilizing multi - dimensional reward functions in environments with richly correlated reward functions."
SP:1f85c93d6bbfd65bf497c92c9cd534d799753097,"This paper introduces a new method for the task of cortical surface reconstruction based on the flow - based method of Ordinary Differential Equation ( ODE ). The proposed method, called CorticalFlow, is based on a new geometric deep - learning model that learns to deform a reference template towards a targeted object using a set of diffeomorphic transformations. To reduce the number of topological errors introduced by its discrete resolution, numerical conditions are introduced to improve the manifoldness of the predicted triangle mesh. Experiments are conducted to evaluate the performance of the proposed method and compare it to state - of - the - art methods in terms of Chamfer distance reduction across all cortical surfaces compared to DeepCSR ( the second - best performing method in this criteria ). In terms of surface regularity, it surpasses NMF or Voxel2Mesh with an average reduction of at least 32.58% of self - intersecting faces while handling template meshes with many more vertices. It is also faster and more memory - efficient than all of these competitors."
SP:2f31d9cf4ad17ad08344439ca0aef7ec91944545,"This paper proposes a new algorithm for the non - convex setting of data - based un - training, where the goal is to remove the influence of deleted data points from trained models from the trained models at a cheaper computational cost than fully retraining those models. The main contribution of the paper is to propose an algorithm that provides a general reduction from deletion guarantees for adaptive sequences of deletions to deletion guarantees against non - adaptive sequences. The algorithm uses differential privacy and its connection to max information to ensure that the deletion guarantees are valid for sequences that are chosen independently of the models that are published. The paper also proposes a method to design a practical attack against the SISA algorithm of Bourtoule et al., which is based on CIFAR-10, MNIST, Fashion - MNIST and Fashion -MNIST."
SP:7150006590e268ab732c9be6c9048f67a377f956,This paper proposes a method for optimising the conditional value at risk ( CVaR ) of the total return in Bayes - Adaptive Markov Decision Processes ( MDPs ) in reinforcement learning. The authors show that a policy based on Bayesian optimisation and Monte Carlo tree search is risk - averse to the uncertainty of the prior distribution and the stochasticity of the Markov decision process. They reformulate the problem as a two - player game and propose an approximate algorithm based on Monte Carlo Tree Search and Bayesian Optimisation. Experimental results show that their algorithm significantly outperforms the baseline methods on two domains.   The main contribution of this paper is to propose a method to optimise a risk metric in model - based Bayesian RL to simultaneously address both the epistemic and the aleatoric uncertainty in the MDP decision making process.
SP:a94f39406f73d7483ddd744ed2f03c78b8bc5d44,"This paper studies the problem of learning deep neural networks trained with the logistic loss via gradient descent on binary classification data where the underlying data distribution is general, and the ( optimal ) Bayes risk is not necessarily zero. In this setting, it is shown that gradient descent with early stopping achieves population risk arbitrarily close to optimal in terms of logistic and misclassification losses, as well as calibration, meaning the sigmoid mapping of its outputs approximates the true underlying conditional distribution arbitrarily finely. The necessary iteration, sample, and architectural complexities of this analysis all scale naturally with a certain complexity measure of the true conditional model. While it is not shown that early stopping is necessary, the paper shows that any univariate classifier satisfying a local interpolation property is inconsistent with optimal test error for noisy data.   The main contribution of this paper is to provide a mathematical basis for this good performance on arbitrary binary classification problems, considering the simplest possible networks : shallow ReLU networks where only the inner ( inputfacing ) weights are trained via vanilla gradient descent. The central contributions are as follows : 1. The required number of data samples, network nodes, and gradient descent iterations all shrink if the distribution satisfies a natural notion of simplicity, which is approximated well by a low - complexity infinite - width random feature model. 2. For data with a finite number of samples, the required number and iterations of the network iterations also shrink naturally with the simplicity of the conditional model, which approximates well. 3. While competing with R - risk with R may seem a strenuous goal, in fact it simplifies many aspects of the learning task due to the fact that it is a universal approximation to the universal approximation of a universal model. 4. The paper will be highlighted in the main result below via the calibration theory via the Bartlett et al. ( 2004 ). The main result in the calibration section is highlighted as the main contribution. The analysis is backed by a number of lemmas that could be useful elsewhere, such as multiplicative error property of logistics loss, and separately a technique to control the effects of large network width over not just finite sample, but over the entire sphere."
SP:a9c786cbb61e1f10f3542161b13e43a1a68ab34d,"This paper proposes a coordination detection framework incorporating neural temporal point process with prior knowledge such as temporal logic or pre - defined filtering functions. The proposed method jointly learns a Gibbs distribution of group assignment based on how consistent an assignment is to ( 1 ) the account embedding space and ( 2 ) the prior knowledge. To address the challenge that the distribution is hard to be efficiently computed and sampled from, the authors design a theoretically guaranteed inference approach to learn a mean - field approximation for it. Experimental results on a real world dataset show the effectiveness of our proposed method compared to state - of - the - art model in both unsupervised and semi - supervised settings. Further apply our model on a COVID - 19 Vaccine Tweets dataset and the detection result suggests presence of suspicious coordinated efforts on spreading misinformation about COVID-19 vaccines."
SP:b5c6e967a26a02861db2ecd620e9061db0c03e59,"This paper studies the generalization properties of deep neural networks trained with neural tangent kernels ( NTK ) for structured data with low - dimensional nonlinear structure. The data consists of two disjoint smooth curves on the unit sphere, and the problem is to classify data drawn from one of the smooth curves. The authors prove that when the network depth is large relative to certain geometric properties that set the difficulty of the problem, randomly - initialized gradient descent quickly learns to correctly classify all points on the two curves with high probability. To their knowledge, this is the first generalization guarantee for deep networks with nonlinear data that depends only on intrinsic data properties. The existence of certificates ( and more generally, the conditions under which practically - trained neural networks can fit structured data ) is open, except for a few very simple geometries which they will review below.   In particular, via the fine - grained control of the decay decay properties of the NTK, the authors demonstrate that, if the network is sufficiently deep, NTK can be locally approximated by a translationally invariant operator on the manifolds and stably inverted over smooth functions, which guarantees convergence and generalization. This leads in turn to a novel perspective on the role of network depth as a fitting resource in the classification problem, which is inaccessible to shallow networks."
SP:8f6bee3be43df6b6e80804974014caaafe08c49e,"This paper proposes a new method for training cGAN, ReACGAN, which is an auxiliary classifier GAN with softmax cross entropy loss ( ACGAN ). It is well known that training ACGAN is challenging as the number of classes in the dataset increases, and ACGAN also tends to generate easily classifiable samples with a lack of diversity. To address these issues, the paper introduces two cures for ACGAN : first, it identifies that gradient exploding in the classifier can cause an undesirable collapse in early training, and projecting input vectors onto a unit hypersphere can resolve the problem. Second, it proposes the Data - to - data Cross - Entropy loss ( D2D - CE ) to exploit relational information in class - labeled dataset. On this foundation, the authors propose the Rebooted Auxiliary Classifier Classifier Adversarial Network ( RACGAN ) to generate realistic images by incorporating class information into GAN. Experiments are conducted on CIFAR10, Tiny - ImageNet, CUB200, and ImageNet datasets to validate the effectiveness of the proposed method.   The experimental results show that ReACAN achieves state - of - the - art results on the four datasets, and that it achieves better performance than the state of the art in terms of Fréchet Inception Distance ( FID ) and consistency regularization. The authors also verify the differentiable augmentations and the harmonization with StyleGAN2."
SP:080e80746a87228b156408ff649ab7a17f44e92d,"This paper presents an extension of the policy space response oracle ( PSRO ) algorithm for two - player zero - sum games, which has been empirically shown to find approximate Nash equilibria in large games. Although PSRO is guaranteed to converge to an approximate Nash equilibrium and can handle continuous actions, it may take an exponential number of iterations as the number of information states ( infostates ) grows. To address this issue, the authors propose Extensive - Form Double Oracle ( XDO ), an extensive - form double - oracle algorithm for zero sum games that can handle both continuous and sequential actions. Unlike PSRO, which mixes best responses at the root of the game, XDO mixes best response at every infostate. The authors also introduce Neural XDO ( NXDO ) where the best response is learned through deep RL. Experiments on modified Leduc poker game and Oshi - Zumo show that XDO achieves a lower exploitability than PSRO with the same amount of computation. They also find that NXDO outperforms PSRO and NFSP on a sequential continuous - action game."
SP:bda04facef4f34679fc4e17b8ea1aae74c3d649f,"This paper proposes a graph - level unsupervised representation learning method based on permutation - invariant variational autoencoder for graph structured data. The main idea is to train an encoder and decoder network that learns to match the node order of input and output graph, without imposing a particular node order or performing expensive graph matching. The problem is that most neural network architectures are by design not invariant to the order of their inputs. To address this issue, the authors propose an additional permutation model that assigns to each input graph a permutation matrix to align the input graph node order with that of the reconstructed graph. The authors demonstrate the effectiveness of their proposed model for graph reconstruction loss in three different settings : graph classification, graph reconstruction, generation and regression."
SP:e17ea6aeba78c9dfc25596d8b35a2a4f1f1f6763,"Graph Neural Networks ( GNNs ) have limited scalability with respect to the graph and model sizes. On large graphs, increasing the model depth often means exponential expansion of the scope. Beyond just a few layers, two fundamental challenges emerge : 1. degraded expressivity due to oversmoothing, and 2. expensive computation due to neighborhood explosion. This paper proposes a design principle to decouple the depth and scope of the representation of a target entity ( i.e. a node or edge ). First, a localized subgraph is extracted as the bounded - size scope, and then a GNN of arbitrary depth is applied on top of the subgraph. Empirically, on seven benchmarks ( including the largest ogbn - 100M graph with 111M nodes ), SHADOW - GNN achieves significant accuracy gains compared to the original models. Meanwhile, the computation and hardware costs are reduced by orders of magnitude."
SP:4890f251db559a0a572afc66e0c1f899b577d9ff,"Normalizing flows are a widely used class of latent - variable generative models with a tractable likelihood. Affine coupling models are a particularly common type of normalizing flows, for which the Jacobian of the latent - observable - variable transformation is triangular, allowing the likelihood to be computed in linear time. Despite the widespread usage of affine couplings, the special structure of the architecture makes understanding their representational power challenging. The question of universal approximation was only recently resolved by three parallel papers, who showed reasonably regular distributions can be approximated arbitrarily well using affine coupling networks, albeit with networks with a nearly -singular Jacobian. In this paper, we show that any log -concave distribution, even with ill - conditioned Jacobians, can be approximation approximated using well - conditioned universal approximation. In terms of proof techniques, we uncover and leverage deep connections between affine - coupling architectures, underdamped Langevin dynamics, and Hénon maps. Our results also inform the practice of training affine Coupling : we approximate a padded version of the input distribution with iid Gaussians, a strategy which Koehler et al. empirically observed to result in better - conditioned flows, but had hitherto no theoretical grounding."
SP:5ffa81488ed1092deb89bd5e150fa146325057ce,"This paper proposes a method to solve the Lagrangian problem of coupons allocation in the e -commerce market, where the goal is to allocate coupons within a fixed budget while maximizing users ’ retention on the platform. The method consists of three components : 1 ) a budget constrained offline reinforcement learning and evaluation with λ - generalization ( BCORLE(λ ) framework ), 2 ) an offline RL algorithm called R - BCQ for policy learning, 3 ) a model - free off - policy evaluation method called REME ( Random Ensemble Mixup Evaluator ) for policy evaluation, and 4 ) a new offline RL method based on batch - constrained Q - learning ( R -BCQ ).   Experiments on a simulation platform and a real - world e - commerce market validate the effectiveness of the proposed methods."
SP:6b04cc7b4e45b9e65a1d34c15e3f75a2ef27d601,"This paper proposes a method for source - free domain adaptation ( SFDA ), where the source pretrained model is adapted to the target domain in the absence of source data. The method is based on the observation that target data, which might no longer align with the source domain classifier, still forms clear clusters. The proposed SFDA method captures this intrinsic structure by defining local affinity of the target data and encourage label consistency among data with high local affinity. The authors observe that higher affinity should be assigned to reciprocal neighbors, and propose a self - regularization loss to decrease the negative impact of noisy neighbors. The experiments results on three 2D image datasets and one 3D point cloud dataset show that the method achieves state - of - the - art performance compared with related methods."
SP:ac1bf04ff782e5892a0bc5fe5949848ca8e731c2,"This paper proposes a method for pooling features from a set of features into a fixed - dimensional representation, called Sliced - Wasserstein Embedding ( PSWE ), for learning representations from set - structured data. PSWE is motivated by the fact that existing pooling methods, such as mean - pooling and featurewise sort pool, do not scale well with the number of features in the set, while the Euclidean distance between features in a set equals the distance between the samples in the pool. To overcome this, PSWE introduces a new pooling mechanism, which is based on the squared squared sliced - wasserstein distance ( SW distance ). The SW distance is defined as the average of the distances between two samples that are sampled from the same probability distribution. The authors show that PSWE can be used to learn representations from sets of different sizes effectively, by pooling the features of a set using a permutation - invariant pooling method. The method is evaluated on three data modalities, namely : point cloud classification, graph classification, and image recognition. The results show that the PSWE method outperforms the state - of - the - art methods on all three tasks, and the performance of PSWE on the graph learning and object detection tasks is slightly better than the other methods."
SP:6cb2f0cbc076f8680cb00411790629f8e1478053,"This paper proposes a family of RNNs, namely SBO - RNN, that can be formulated using stochastic bilevel optimization ( SBO ). The proposed family consists of feedforward and backpropagation, which solve the lower and upper - level optimization for learning hidden states and their hyperparameters, respectively. They prove that under mild conditions there is no vanishing or exploding gradient in training SBO RNN. Empirically, they demonstrate their approach with superior performance on several benchmark datasets, with fewer parameters, less training data, and much faster convergence. They also prove that our networks manage to obtain good training stability by selecting proper learning rates."
SP:d3a4300e21ca215334f256f0467a428470548fe4,"This paper proposes a new algorithm for minimizing power consumption in systems with multiple power - saving states. The proposed algorithm is based on a learning - augmented version of the classical online algorithm for the ski rental problem in the learning augmented setting. The main idea is to predict the length of idle periods and then make decisions based on ( potentially inaccurate ) predictions of the predicted lengths of the idle periods. The algorithm is applied to two different types of power saving states, one based on the expected wake - up cost of the current state and the other based on expected energy savings of the next state after the idle period. It is shown that the performance of the proposed algorithm degrades gracefully with increasing prediction error, with a worst - case guarantee almost identical to the classical classical algorithm.   The main contribution of this paper is to develop an algorithm that makes decisions about which of the two states to save based on prediction error and learning loss. This is a work of theoretical nature and the authors are not aware of the potential negative societal impact of their proposed algorithm."
SP:22aba6284123af0ecd6605ee4e89b351bd7e10a3,"This paper presents a theoretical framework for quantifying the transferability of multi - source transfer learning problems, with both the task similarities and the sample complexity of learning models taken into account. In particular, they consider the setup where the models learned from different tasks are linearly combined for learning the target task, and use the optimal combining coefficients to measure the transferable capacity. Then they demonstrate the analytical expression of this transferability measure, characterized by the sample sizes, model complexity, and the similarities between source and target tasks, which provides fundamental insights of the knowledge transferring mechanism and the guidance for algorithm designs. In addition, they develop an alternating alternating alternating iterative algorithm to implement their theoretical results for training neural networks in multi - neural network transfer learning tasks. Finally, experiments on image classification tasks show that their approach outperforms existing transfer learning algorithms in both multi -source and few - shot scenarios."
SP:0fb8dcf15e0d43547d566fdba7bc70b3bb600005,"This paper proposes a neural network - based method for understanding the polarity of search asymmetry in visual search. The authors propose a target - dependent top - down cue - dependent visual recognition model that produces a sequence of eye movements until the target is found. The eye movements are conditioned on the distance between the target and the search image. The model is trained on a pre - trained version of ImageNet and is not trained with the target or search images, or with human visual search data. The experiments show that the model spontaneously reveals search asymmeteries that are qualitatively consistent with human behavior.   The authors test the model on six paradigmatic search tasks that show polarity in humans. They also test whether asymmetry arises from the natural statistics seen during object classification tasks or is modified by training on augmented versions of the ImageNet with altered stimulus statistics e.g. by rotating the images by 90 degrees. The empirical results show that, when trained on an augmented version of the original ImageNet without any training data, search polarity appears to disappear or is altered depending on the training protocol. The results are consistent with, and build bridges between, psychophysics observations in the field of visual search studies [ 35 ] and analyses of behavioral biases of deep networks [ 28 ]."
SP:f0cc968ea9da4884dcdaf6d0c75ea9f1511bdfc3,"This paper studies the problem of training certifiable robust models against adversarial examples in the setting of linear relaxation - based training. The authors claim that the current state - of - the - art method often has a landscape with favorable optimization properties that is difficult to train robust models on. They show that Interval Bound Propagation ( IBP ) training uses much looser bounds but outperforms other models that use tighter bounds. They also identify another key factor that influences the performance of certifiable training : smoothness of the loss landscape. They find significant differences in the loss landscapes across many linear relaxation-based methods, and that the state of the art method, while having favorable landscape, often has landscape with unfavorable optimization properties. To verify their claims, they propose a new training method with tighter bounds and a favorable loss landscape that achieves decent performance under a wide range of perturbations, while others with only one of the two factors can perform well only for a specific range.   The paper also proposes a new method to test the claims of the authors, which is based on the proposed method with the desired properties. With the tightness and the smoothness, the proposed proposed method achieves a decent performance."
SP:a158f8772a9dada059ffd1d6d7838ed40d8483da,"This paper considers the problem of online linear regression in the stochastic setting. It derives high probability regret bounds for online ridge regression and the forward algorithm. This enables it to compare online regression algorithms more accurately and eliminate assumptions of bounded observations and predictions.   The authors advocate for the use of forward algorithm in lieu of ridge due to its enhanced bounds and robustness to the regularization parameter. Moreover, they explain how to integrate it in algorithms involving linear function approximation to remove a boundedness assumption without deteriorating theoretical bounds. They showcase this modification in linear bandit settings where it yields improved regret bounds. Last, they provide numerical experiments to illustrate their results and endorse their intuitions forward algorithm instead of ridge."
SP:17ff9a2133aebf2d1b1787e8efc49d709389c0e7,"This paper presents two variants of the extragradient ( EG ) method for solving nonconvex nonconcave optimization problems. The first is a two - time - scale variant of the EG method with a slow O(1/k ) rate on the squared gradient norm, where k denotes the number of iterations. The second is a variant of EG with an anchoring technique, named extra - anchored gradient ( EAG ), which is studied under a smooth convex - concave setting. The stochastic analysis of FEG is also provided. This paper proposes a backtracking line - search version, named FEG - A, for the case where the problem parameters are not available. The corresponding saddle - gradient operator satisfies the negative comonotonicity condition."
SP:4e38973033de24fc183c6112e1146f8eef0ddaea,"This paper studies uniformity testing for ranking data, where the alternative class is restricted to Mallows models. The authors first consider uniform distribution testing with simple pairwise statistics, which allows them to test uniformity using only two samples, if m is large enough. They also consider uniformity test with central and local differential privacy ( DP ) constraints. They present a central DP algorithm that requires O(max{1 / \mathbb{1 } }, 1 / p m ), where $ O$ is the privacy budget and $ 0 $ is the number of samples required. They show uniform distribution can be distinguished from Mallows model with O(m 1 / 2 ) samples based on simple Pairwise statistics. Interestingly, they apply the central DP to apply to the local DP scenario, since it works with binary statistics that is extracted from the ranking data.   The authors then show uniformity tests with central DP and local DP constraints, and they show that for large m very small deviation form the uniform can be detected with high confidence based on two samples."
SP:99a835191a3ba8372e391b6d3316e9b68e543295,"Greedy algorithms have long been a workhorse for learning graphical models, and more broadly for learning statistical models with sparse structure. In the context of learning directed acyclic graphs, greedy algorithms are popular despite their worst - case exponential runtime. In practice, however, they are very efficient. This paper provides new insight into this phenomenon by studying a general greedy scorebased algorithm for learning DAGs. Unlike edge - greedy algorithms such as the popular GES and hill - climbing algorithms, our approach is approach is vertex - greedy and requires at most a polynomial number of score evaluations. The authors then show how recent polynomials - time algorithms are a special case of this algorithm, thereby illustrating how order - based algorithms can be rigorously interpreted as score - based algorithm. This observation suggests new score functions and optimality conditions based on duality between Bregman divergences and exponential families, which we explore in detail in detail. Finally, we provide extensive experiments suggesting that this algorithm indeed optimizes the score in a variety of settings."
SP:b60989706296b963b6671c01f22384978a334be1,This paper proposes a neural architecture dilation for adversarial robustness ( NADAR ) framework for improving the accuracy - robustness trade - off between standard accuracy and robustness. The main contribution of this paper is to propose a dilation framework that allows the architecture to be optimized for both standard and adversarial classification while maintaining a minimal accuracy drop.   The main contributions of the paper are as follows :   1. An analysis of the standard error bounds of adversarial training for CNNs. This paper shows that the standard accuracy bounds of CNNs are bounded by the architecture of the backbone network. 2. A FLOP - aware architecture is proposed to mitigate the effect of increasing the computational cost of the network by minimizing the number of FLOPs. 3. Experimental results on benchmark datasets demonstrate the effectiveness of the proposed algorithm to balance the accuracy and the robustness of the neural network.
SP:77ed765e911a4e5f2bfba13cbd2403500a5d05e6,"This paper studies the problem of reward - free reinforcement learning with linear function approximation for episodic Markov decision processes ( MDPs ). The authors propose a new provably efficient algorithm, UCRL - RFE under the linear Mixture MDP assumption, where the transition probability kernel of the MDP can be parameterized by a linear function over certain feature mappings defined on the triplet of state, action, and next state. They show that to obtain an ε - optimal policy for arbitrary reward function, the algorithm needs to sample at most â‚¬(H5d2d2 ) episodes during the planning phase during the exploration phase and at least the length of the episode d ( d is the dimension of the feature mapping ). They also propose a Bernstein - type variant of UCRL-RFE using Bernstein -type bonus and show that it needs to samples at least d(Hd(H + d ) = 0.2 to achieve an $ \epsilon$-optimal policy. The upper bound matches the lower bound in terms of the dependence on $ \alpha$ and $ d$."
SP:28563ba0975f56ddb662cd46e85de78bb6024d36,"This paper proposes a method for forecasting future events based on shifting seasonal patterns in a data stream of events. The method is based on the Shifting Seasonal Matrix Factorization ( SSMF ) approach. SSMF is an algorithm that learns multiple seasonal patterns ( called regimes ) and is able to switch between them as the data stream changes over time. The main idea is to predict future events by detecting regime shifts in seasonal patterns as data stream evolves.    The method has the following properties :   ( 1 ) it works in an online setting, i.e. processes each observation in constant time and memory ; ( 2 ) it can detect regime shifts without human intervention by using a lossless data compression scheme ; ( 3 ) it effectively realizes regime - shift - aware regime - aware data compression by using it on three real - world data streams, and the method outperforms state - of - the - art methods by accurately forecasting upcoming events on three data streams."
SP:e4bb07033001be4d04695ef058f426d49fe440be,"This paper proposes a neural network architecture, WeaveNet, for solving non - linear assignment problems. The architecture is based on a feature - based architecture, where each feature is connected to another feature based network via a feature sharing module and a feature communication module. The authors compare the performance of the proposed architecture against the state - of - the - art algorithmic method for solving real - world assignment problems, based on the learning - based method. The main contributions of the paper are as follows :   1. It proposes a novel architecture for solving assignment problems based on neural networks. This is a very interesting and important direction to explore in the field of informatics.   2. The proposed architecture is a bit different from the usual neural network architectures, in that it does not rely on a central processing unit as in the case of traditional neural networks, but instead relies on several modules. These modules are the feature weaving layer, the feature sharing layer, and the feature communication layer. The feature weaving module is the one that is most likely to be used for solving the assignment problem. 3. The communication layer is the module that is used to transfer information from one feature to another in order to increase the likelihood of finding a solution. 4. This module is also the only one that relies on the number of elements in the problem to determine the solution. The other two modules are used to predict the final solution. 5. The experimental results show that the proposed method generally performs better than the baselines in most of the cases. However, the experimental results do not compare favourably to the state of the art in the real world."
SP:8a559e21d45661eef427b310e5fe8488d5749137,"This paper studies adversarial robustness of self - supervised learning proxy tasks for 3D point cloud recognition with adversarial training in the context of autonomous driving. Specifically, the authors study three types of models : PointNet, Convolution - based ( DGCNN ), and Transformer - based 3D architectures ( PCT 3D ). They compare the performance of PointNet vs. ConvNN, PCT vs. PointNet and fine - tuning vs. ensemble learning. They find that fine - tuned models from different pre - training tasks have different vulnerabilities, and adversarial examples generated by attacking them do not transfer well among each other. They further leverage ensemble methods to boost the robustness by a substantial margin."
SP:657c5a1114c0d054b9e767d85990bbbb0492912d,"This paper considers the problem of iterative projections of close - by points over widely - prevalent submodular base polytopes in convex optimization over base - based convex function minimization. The main contribution of the paper is an algorithm called A2FW ( Adaptive Frank Wolf Variations of Online Mirror Descent ), which is a variant of the Frank - Wolfe algorithm ( FW ). Theoretical results show that it is able to find the optimal solution after crossing a polytope with a radius of convergence of $ O(n^2)$, where $ n$ is a parametric sub - modular function and $ \nabla(n)$ is the size of the sub - set for which the solution can be found. The method is based on the fact that there exists a set of sub - sets of size $ n^2$ such that if $ n = 1 $, then there exists an active set of size n^3 $ such that $ n \leq n/2 $ with $ n=1 $ is the solution to the problem.   Theoretically, it is shown that this set of subsets can be discovered by computing the projection of the active set with a factor of $ n/1 $, which reduces the runtime of computing the Bregman projections of the FISTA algorithm by a factor $ \log(n/2)$. However, in practice, computing this projection is computationally expensive, and the authors propose to use a toolkit to speed up computation of the computation of projections using both discrete and continuous perspectives. The proposed in the paper. The experimental results suggest that the proposed method is more effective than the baseline AFW algorithm in some cases."
SP:8dae43d6b5cebb7ef6c39437d997b390c2380536,"This paper considers the problem of learning the natural parameters of a k - parameter in the exponential family of minimal truncated exponential families, a class of exponential families where the support and support of the natural parameter $ \mathbb R^\mathbb{R}$ are bounded and the support of $ \nabla_i$ is also bounded. While the traditional maximum likelihood estimator for this class is consistent and asymptotically normal, evaluating it computationally is computationally computationally hard. This paper proposes a computationally efficient estimator that is consistent as well as normal as possible under mild conditions.   The main contribution of this paper is to provide finite sample guarantees to achieve an ( 2 ) error of $ O(poly(k / \alpha ) ) in the parameter estimation with sample complexity $ O(\sqrt{O})$ and computational complexity $ \sqrt(O(poly(\k / α ) )$. The authors show that, at the population level, our method can be viewed as the maximum likelihood estimation of a re - parametrized distribution of the same class of class $ r$ of exponential family $ x$. They also show that learning a distribution in this exponential family is equivalent to learning the corresponding natural parameter of the corresponding class $ x^2 $. The authors also provide experimental results showing that their estimator can be used to learn the corresponding parameter of a different exponential family for a different population."
SP:4f9ddb697e86356fb293ef34a69ca3702c4e8164,"This paper proposes a differentiable renderer for inverse graphics based on differentiable rasterization and ray tracing. The renderer is based on DIBR++, which is a learning - based renderer that combines physics - based differentiability with ray tracing in order to capture the photorealistic effects of environmental lighting and material models. The paper compares the performance of DIB - R++ with state - of - the - art differentiability based renderers on synthetic data and real data, and demonstrates several artistic applications, including material editing and scene relighting.   The main contributions of the paper are as follows :   1. A new renderer based on path tracing and differentiable ray tracing is proposed for the inverse graphics problem. 2. The method is validated on synthetic and real images. 3. DIB-R++ shows superior performance on synthetic images and lighting configurations compared to prior rasterisation - based methods."
SP:6ac1c8556e7131939cc582f513bc9921470e1b09,"This paper proposes a differentiable training method to replace the conventional soft - argmax method for training neural networks. The proposed method is based on a continuous formulation of the output distribution of the probability distribution, which is then used to approximate the expectation of the average error of the samples drawn from that distribution. The expectation can be approximated by computing the average of the total number of samples across the distribution and minimizing the difference between the average and the expected error. The method is tested on a variety of localization tasks, where it is shown to be more effective than the conventional method."
SP:478c05c90090f9d80b72ac352c488073b45a5d8b,"Graph Contrastive Learning ( GCL ) is a method to learn generalizable representations from contrastive views. This paper addresses the following issues : 1 ) changing the graph structure through data augmentation to generate contrastive view may mislead the message passing scheme, as such graph changing action deprives the intrinsic graph structural information, especially the directional structure in directed graphs ; 2 ) since GCL usually uses predefined contrastive viewing with hand - picking parameters, it does not take full advantage of the contrastive information provided by data augmentation, resulting in incomplete structure information for models learning. To address these concerns, the paper proposes a directed graph data augmentation method called Laplacian perturbation and theoretically analyzes how it provides contrastive data without changing the directed graph structure. The paper also proposes a new learning framework, directed graph contrastive learning ( DiGCL ), which dynamically learns from all possible contrastive points and learns from multiple easy - to - learn ( E - E - Diff ) views generated by the proposed method. Experiments on various benchmarks reveal the dominance of the presented method over the state - of - the - art approaches, including unsupervised and supervised learning."
SP:85b383d2f722f7bff438840e423f5cb4c67d5980,"This paper proposes Symbolic Interactive Language Grounding benchmark ( SILG ), which unifies a collection of diverse grounded language learning environments under a common interface. SILG consists of grid - world environments that require generalization to new dynamics, entities, and partially observed worlds ( RTFM, Messenger, NetHack ), as well as symbolic counterparts of worlds that require interpreting rich language with respect to complex scenes ( ALFWorld, Touchdown ). Together, these environments provide diverse grounding challenges in richness of observation space, action space, language complexity, and plan complexity.   The authors propose the first shared model architecture for RL on these environments, and evaluate recent advances such as egocentric local convolution, recurrent state - tracking, entity - centric attention, and pretrained LM using SILG. They find that many recent modelling advances do not result in significant gains on environments other than the one they were designed for. This highlights the need for a multi - environment benchmark. Finally, the best models significantly underperform humans on SILG, which suggests ample room for future work."
SP:23c8db56f59f778fe812a5dd161f7a1f21c3cdba,"This paper presents a method for scaling up MoE models for computer vision with few - shot fine - tuning. The authors propose a method called Vision MoE ( V - MoE ) that is a sparse version of the Vision Transformer, which they claim is scalable and competitive with the largest dense networks. They also propose an extension to the routing algorithm that can prioritize subsets of each input across the entire batch, leading to adaptive per - image compute. This allows V -MoE to trade - off performance and compute smoothly at test - time.   The authors also demonstrate the potential of training models with up to 24 MoE layers, 32 experts per layer, and almost 15 B parameters. They show that these models can be stably trained, seamlessly used for transfer, and successfully fine - tuned with as few as 1 000 datapoints. Moreover, their largest model achieves 90 % test accuracy on ImageNet when fine -tuned. The paper also provides some analysis of the routing decisions, revealing patterns and conclusions which may further improve understanding in the field."
SP:c5235f41dfb8b5cc478f11c5d5e0ab0b8676871e,"This paper studies the expressivity of hidden - layer neural networks with fewer than n neurons when the activation is smooth. The authors claim that as long as the width m > 2n/d ( where d is the input dimension ), there exists at least one global minimizer with zero training loss. Second, they identify a nice local region with no local - min or saddle points, and prove that every KKT point is a nearly global minimiser. Third, they show that projected gradient methods on this constrained formulation significantly outperform SGD for training narrow neural nets. In real - data experiments, their proposed training regime can significantly outperforms SGD."
SP:0be529f5254afd59dcfa6b34a359c7037e7a8323,"This paper proposes a continuous covariance - based continuous learning algorithm for risk - aware multi - armed bandit learning that explicitly takes into account option correlation between reward and risk. The proposed algorithm is based on a continuous neural network that learns a weight vector for each of the given options and observes random feedback according to the decisions made by the learner. The objective of the agent is to achieve the best trade - off between reward - risk, measured with option covariance. The paper considers three feedback settings, i.e., full information, semi - bandit and full bandit feedback, to capture different reward observation scenarios in practice. The experimental results demonstrate the superiority of the proposed method.   The authors also propose novel algorithms with optimal regrets ( with optimal regret ) and provide matching lower bounds to validate their optimalities. This is the first work that considers option correlation in risk-aware bandits and explicitly quantifies how arbitrary covariance structures impact the learning performance. The novel analytical techniques developed for exploiting the estimated covariance to build concentration and bounding the risk of selected actions based on sampling strategy properties can likely find applications in other bandit analysis and be of independent interests."
SP:472a90bb175b0286765c5a47b040e1a58f594a05,"This paper proposes a noncommutative extension of Lee - Seung ’s Matrix Multiplicative Update ( MMU ) algorithm for computing positive semidefinite ( PSD ) factorization of data, which is a generalization of the nonnegative nonnegative matrix factorization ( NMF ) problem. The authors show that the squared loss objective of MMU is non - increasing, fixed points correspond to critical points, and the analysis relies on Lieb’s Concavity Theorem. The paper also shows that the MMU algorithm can be also be used as a primitive to calculate blockdiagonal PSD factorizations and tensor PSD factors. The experimental results demonstrate the utility of the method on real and synthetic data.   The main contribution of the paper is the introduction of a non - commutative update method for computing PSD with the spectral decomposition of PSD matrices. The method is based on the multiplicative update algorithm developed by Lee and Seung ( Lee & Seung, 1982 ) in which non - negativistic updates are preserved by scaling with positive diagonal matrices, and it retains the simplicity of implementation and scalability of multiplicative update algorithms for NMF."
SP:83abd6d149d88cc6e96cbc4d488e4fe9dc2a4fcb,"This paper proposes a meta - learning framework for domain generalization ( DG ) based on domain specific domain invariance ( DS ) and domain specific - domain invariant ( DSI ). DSDI is a meta learning framework that extends the invariance view beyond the invariant view to capture the usefulness of domain specific information. The authors propose to disentangle features in the latent space while jointly learning both domain - invariant and domainspecific features in a unified framework. The domain - specific representation is optimized through the meta -learning framework to adapt from source domains to target domains, targeting robust generalization on unseen domains.   The authors extensively evaluate mDSDI on several state - of - the - art DG benchmark datasets, including Colored - MNIST, VLCS, PACS, Office - Home, Terra Incognita, DomainNet in addition to a newly created Background - Colored MNIST for ablation study to examine the behavior of our mDS DI framework. They show competitive results with state of the art techniques in DG."
SP:4191474c75e2fedf514f0f3001a67a047eb74c30,"This paper presents a new generative generative model for image synthesis, based on a combination of diffusion and classifier models. The diffusion model is based on the work of Nichol and Dhariwal ( Nichol et al., 2018 ), and the classifier model was developed by Song et al, 2018. The authors show that the diffusion model can achieve image sample quality superior to the current state - of - the - art generative models on unconditional image synthesis by finding a better architecture through a series of ablations. They further improve sample quality with classifier guidance : a simple, compute - efficient method for trading off diversity for fidelity using gradients from a classifier. They achieve an FID of 2.97 on ImageNet 128 ⇥128, 4.59 on Image Net 256 ⁥256, and 7.72 on Image net 512⇥512, and match BigGAN - deep even with as few as 25 forward passes per sample. Finally, they find that classifiers guidance combines well with upsampling diffusion models, further improving FID to 3.94 on imageNet 256."
SP:fe3cab08596cde4c14ecf6fca8d0f95b02bab229,"This paper proposes a method to leverage out - of - distribution samples ( i.e. unlabeled samples coming from outside target classes ) for improving few - shot learning. The key idea is to use the distance between prototypes and samples from outside the distribution to drive the classifier to avoid irrelevant features by maximizing the distance from prototypes to out of distribution samples while minimizing that to samples from in - distribution data. The method is simple to implement, agnostic to feature extractors, lightweight without any additional cost for pre - training, and applicable to both inductive and transductive settings. Extensive experiments on various standard benchmarks demonstrate that the proposed method consistently improves the performance of pretrained networks with different architectures."
SP:b1f65724926f136979829b7a6c870bc31f38f591,"This paper proposes ReMERN and ReMERT, two methods to improve the performance of off - policy RL algorithms with replay buffer. The main contribution of the paper is to provide theoretical justifications for TD error, recentness and corrective feedback. The authors also propose two new methods to compute the prioritization weight. ReERN learns an error network and uses it to learn the temporal ordering of states, while ReERT exploits the temporal order of states. The experiments show that ReERN improves performance of standard off -policy RL methods in various benchmarks, including MuJoCo and Meta - Co."
SP:601ebf30b3c6aa35fcef49633aa8eb0acd0f2c66,"This paper addresses sequential prediction with expert advice in a nonstationary environment with long - term memory. The authors propose a linear - time algorithm that improves on the best known regret bounds [ 27 ]. This algorithm incorporates a relative entropy projection step that is advantageous over previous weight - sharing approaches in that weight updates may come with implicit costs as in for example portfolio optimization. They also give an algorithm to compute this projection step in linear time, which may be of independent interest to some readers.   The paper is organized as follows :   1. Introducing the model and discussing related work, giving a detailed overview of the previous results on which they improve. 2. In Section 3 they give main results, a regret bound which holds for two algorithms, and a method to compute relative entropy projections with non - uniform lower - box constraints. 3. In Section 4 they derive a new method for deriving new predictions. 4. Finally, they give a few concluding remarks in Section 5."
SP:b2439973063e827b3cbe92306a2fdee3286b6b44,"This paper considers the problem of contextual linear bandits, which is motivated by routing applications in navigational engines and recommendation systems. In this problem, the learner is presented with a set of possible actions and is asked to choose one of them. The goal is to maximize the utility of the selected action over the set of all other actions. To achieve this goal, the authors propose two algorithms : regret O(d log T ) and exp(O(D log d ) ). The regret algorithm is based on the fact that the distance between the true point and the hyperplane in convex geometry is lower than the total distance between true and hyperplane. The exp - plane algorithm relies on a variant of Steiner ’s formula for the centroid of a convex set, which may be of independent interest. The authors also consider the case where the user is allowed to provide a list of several recommendations, and give an algorithm with an O(log d ) regret and a list size polyd. Finally, they construct nearly tight algorithms for a weaker variant of this problem where the student only learns the identity of an action that is better than the recommendation. The results for local contextual recommendation are included in the Supplementary Material."
SP:abe83c7e0bcf4829742609d709637e2f84d8a4d9,"This paper presents Lale, an open - source sklearn - compatible AutoML library that allows users to access a library of source code for AutoML tools without having access to the source code. The main contribution of Lale is a set of orthogonal combinators for composing machinelearning operators into pipelines, which are then used to search through hyperparameter schemas to find optimizers. The paper compares Lale with the state - of - the - art tools and evaluates it with a user study.   The main contributions of the paper are as follows :   1. Lale introduces a small set of combinators that can be used for composing operators for the AutoML pipeline. 2. It describes a translation scheme from pipelines to search spaces for optimizers, and uses schemas for search spaces and type - checking. 3. It compares the proposed combinators to the combinators of prior works and shows that Lale outperforms them."
SP:0d7f1cae577ed598048b64617e85ca6bd5c6d7fa,"This paper studies the problem of learning neural network weights that generalize well from small datasets with meta - learning. The authors propose that the learning algorithm should be allowed to make its own weight updates, i.e., should not be forced by the weight updates of the meta - learner, but should instead be guided by learning where to learn. They find that this leads to patterned sparsity and less interference in generalization in a range of few - shot and continual learning problems. Moreover, they find that sparse learning also emerges in a more expressive model where learning rates are meta - learned. The paper also shows that performance improves when meta - learners are allowed to use binary gradient masks only. This is done by adapting a version of MAML adapted for online learning."
SP:05037e1850003a725a466b64d3e32aa2aed458fb,"The paper considers the problem of shared response modeling, a multi - view learning problem where one wants to identify common components from multiple datasets or views. The authors propose a new method, ShICA, that models each view as a linear transform of shared independent components contaminated by additive Gaussian noise. They show that this model is identifiable if the components are either non - Gaussian or have enough diversity in noise variances. They then show that in some cases multi - set canonical correlation analysis can recover the correct unmixing matrices, but that even a small amount of noise makes Multiset CCA fail. To solve this problem, they propose to use joint diagonalization after multiset correlation analysis, leading to a new approach called ShICA - J. Experiments on fMRI and MEG data demonstrate that the proposed method outperforms existing GroupICA and IVA methods. Further, the authors provide a maximum likelihood estimator of ShICA that models non -Gaussian components using a Gaussian mixture model."
SP:44dd1faa1813c433fd7581d05cae3df440bfb93e,"This paper studies the problem of how to train agents that collaborate well with human partners without using human data. The authors argue that the crux of the problem is to produce a diverse set of training partners that can adapt rapidly to the agent's individual strengths and weaknesses. Unfortunately, most standard multi - agent reinforcement learning techniques, such as self - play ( SP ) or population play ( PP ) produce agents that overfit to their training partners and do not generalize well to humans. To tackle this problem, the authors propose a method they call Fictitious Co - Play ( FCP ). FCP is based on a two - player collaborative cooking simulator that has recently been proposed as a challenge problem for coordination with humans. They find that FCP agents score significantly higher than SP, PP, and BCP when paired with novel agent and human partners. Furthermore, humans also report a strong subjective preference to partnering with FCP - trained agents over all baselines."
SP:21c84bd720b1e90ea0f88fbf8fd24dbcb49b547c,"This paper proposes FACMAC, a method for cooperative multi - agent reinforcement learning in both discrete and continuous action spaces. FACMAC learns a centralised but factored critic, which combines per - agent utilities into the joint action - value function via a non - linear function, as in QMIX, a popular multi -agent Q - learning algorithm. The authors evaluate FACMAC on variants of the multi - agents particle environments, a benchmark for MuJoCo, and a challenging set of StarCraft II micromanagement tasks. Empirical results demonstrate FACMAC ’s superior performance over MADDPG and other baselines on all three domains on which the number of agents and/or actions is large."
SP:1c8351b8a6cdf1212840388e19a596729b3bfda4,"The paper proposes a biologically plausible plasticity - based model of long - term memory based on Hebbian plasticity and three - factor rules. The model is based on the Hopfield network, which is biologically plausible because of its attractor - based plasticity. The authors compare the proposed model to classical Hopfield networks on autoassociative memory, continual recall, and sequence learning tasks. They show that their model is on par with the classical model on auto - recall and can be extended to hetero - associative memory. They also show how the feed - forward structure of their model allows it to be applied to more biologically - motivated memory tasks."
SP:7ad6da2c63859d64970e9b35326e9ceab48add47,"This paper proposes simple stochastic and online gradient descent methods for pairwise learning. The main difference from the existing studies is that they only pair the current instance with the previous one in building a gradient direction, which is efficient in both the storage and computational complexity. They develop novel stability results, optimization, and generalization error bounds for both convex and nonconvex problems, as well as both smooth and nonsmooth problems. They also extend their algorithms and stability analysis to develop differentially private SGD algorithms that significantly improves the existing results. The paper is concluded with experimental validation of theoretical findings."
SP:cb11dacc930d71a616ee2fbe4acfae030f9dca59,"This paper proposes REDO, a class - agnostic framework to reconstruct dynamic objects from RGBD or calibrated videos. The main idea is to combine the implicit 4D implicit function, which is pixel - aligned with aggregated temporal visual cues, with a 4D transformation module which captures object dynamics to support temporal propagation and aggregation. The authors conduct extensive experiments on two synthetic RGBD video datasets, SAIL - VOS 3D and DeformingThings4D++, and on the real - world calibrated video dataset 3DPW [ 86 ]. They find that REDO generalizes well and consistently outperforms prior 4D reconstruction methods by a large margin. They provide a comprehensive analysis to validate the effectiveness of each of the introduced components."
SP:8ae97752e74b4395774575009031abcb6ba5cea7,"This paper provides a non - asymptotic analysis of linear stochastic approximation ( LSA ) algorithms with fixed stepsize. This family of methods arises in many machine learning tasks and is used to obtain approximate solutions of a linear system, which can only be accessed through random estimates { ( An,bn ) : n 2 N 2 N⇤ }. The analysis is based on new results regarding moments and high probability bounds for products of matrices which are shown to be tight and tight. The authors derive high probability bound on the performance of LSA under weaker conditions on the sequence { ( an,bn : n,bn 2 N ⇤ ) than previous works. However, in contrast, they establish polynomial concentration bounds with order depending on the stepsize in contrast to previous works, in which the bounds are polynomials. They show that the conclusions of their analysis can not be improved without additional assumptions such as the presence of the covariance matrices appearing in the central limit theorems.   The main contributions of this paper are as follows :   1. This paper provides an analysis of the probability bounds of the LSA algorithm with fixed step - size and fixed number of iterations. The main contribution is to provide a new analysis based on the results of the moment - based and moment - free moments of the linear system. The second contribution is a set of probability measures on ( R2d, B2R2 ) with marginals. These probability measures are the set of all probability measures ( R^2 ) on Rd with finite second moment, and are obtained by Denote by P2(Rd ). These measures are used to estimate the probability of the solution of the case when the number of steps is less than a certain threshold. This is the case where the matrices are covariance matrix and the marginals are covariant. The first set of probabilities is obtained by computing the difference between the expected value of the product of two matrices ( R and B ) and the expected values of marginals of the second set ( R - B ). This set is used as the basis for the second part of the analysis. It is shown that the probability measures of the first set are the same for both sets of probabilities. This means that there is no need for additional assumptions in order to improve the analysis of this work. 2. In particular, the authors show that no additional assumptions ( e.g., that no Gaussian or exponential high probability can be added to the moments ) are necessary for the analysis to be able to obtain better results."
SP:86c1e937755e35efafecc09dfe2606ffb1653a41,"This paper extends the options framework for temporal abstraction in reinforcement learning from discounted Markov decision processes ( MDPs ) to average - reward MDPS. The main contributions include general convergent off - policy inter - option learning algorithms, intra - option algorithms for learning values and models, as well as sample - based variants of learning algorithms. The convergence of the proposed algorithms and convergence of their convergence proofs extend those recently developed by Wan, Naik, and Sutton ( 2017 ). They also extend the notion of option - interrupting behavior from the discounted to the average -reward formulation. They show the efficacy of their proposed algorithms with experiments on a continuing version of the Four - Room domain.   The authors also introduced an algorithm to improve an agent ’s behavior given estimated option values. Instead of letting an option execute to termination, this algorithm involves potentially interrupting an option’s execution to check if starting a new option might yield a better expected outcome. The authors show that if so, then the currently -executing option is terminated and the new option is executed, and the old option is not executed, a better outcome can be obtained."
SP:7e4e1e20e7c253d02c6ae58457fb30029f130f0c,"This paper proposes a new method for training visual transformers ( VTs ), an architectural paradigm alternative to Convolutional Networks ( CNNs ). The authors empirically analyse different VTs, comparing their robustness in a small training set regime, and show that, despite having a comparable accuracy when trained on ImageNet, their performance on smaller datasets can be largely different. Moreover, the authors propose an auxiliary self - supervised task which can extract additional information from images with only a negligible computational overhead, which is used jointly with the standard ( supervised ) training and it does not depend on specific architectural choices, thus it can be easily plugged in the existing VTs. The experiments show that this task is beneficial to speed - up training and improve the generalization ability of different VTs, independently of their specific architectural design or application task.   The authors claim that some local properties of the visual domain which are embedded in the CNN architectural design, in fact, should be learned from samples, in particular, in the case of some common CNNs, in order to improve generalization performance."
SP:0132ef17585e293b23e9dc45189c0989d829b52a,"Hyperbolic Procrustes Analysis ( HPA ) is a new method for label - free alignment of hierarchical data. It is based on the Lorentz model of hyperbolic spaces. HPA consists of three components : translation, scaling, and rotation. The authors analyze the proposed components, highlighting their useful properties for alignment. The efficacy of HPA, its theoretical properties, stability and computational efficiency are demonstrated in simulations."
SP:3580ac64f09e3021de5d4c92411bcc0f3c5d10f3,"This paper studies the trade - off between accuracy for sum queries and point queries in differentially private micro - data - protected query answering systems. The paper shows that under certain assumptions on the population of interest, the accuracy of the sum query is worse than the accuracy for its component sub - populations. This is because of the uncertainty principle, which states that there is a trade -off between the error penalty for each sub - population depending on the number of queries and the amount of data produced by the query answering system. This uncertainty principle is well motivated and motivated by the fact that in the case of pure differential privacy, one can provide noisy answers to sum query and all point queries while guaranteeing that each answer has squared error O(1 / d ). To mitigate this, the paper proposes two mitigation strategies : 1 ) create a collection of benchmark datasets that can be used for public study of this problem, and 2 ) propose some algorithms, inspired by the lower and upper bound proofs, for mitigating the effects of this uncertainty principle. Limitations : empirically, these algorithms perform well on the benchmarks but they do not have theoretical proofs of performance."
SP:c0e64dc8acfaed3e4d7745af12fd34003d0e5017,"This paper proposes a novel method for combining planning and goal - conditioned reinforcement learning ( RL ) for long - horizon navigation and continuous control tasks. The method, called CO - PILOT, is based on the idea that planning and RL can learn from each other to overcome their own drawbacks : Planning can find the shortest path to a distant goal that provides dense reward / guidance but is inaccurate without a precise environment model ; planning can find a high - level goal that is easy to reach but hard to reach without a detailed environment model. To overcome these drawbacks, the authors propose to train a planner and an RL agent on each other on a curriculum of tree - structure - based long horizon tasks. To do this, the planner decomposes a long horizon task to a tree of sub - tasks, whose layers construct coarse - to - fine sub - task sequences as plans to complete the original task. The planning policy is trained to minimize the RL agent ’s cost of completing the sequence in each layer from top to bottom, which gradually increases the sub - tasks and thus forms an easy - forms curriculum for the planner. Next, a bottom - up traversal of the tree is used to train the RL agents from easier sub - tasked with denser rewards on the bottom layers to harder ones on top layers and collects its cost on each sub -task in the next episode. The authors compare the proposed method with SAC, HER, PPO, RRT, NEXT, SGT, and their combination ( SoRB ) on long horizon navigation tasks. Compared to existing RL, planning and combining them, CO - PLOT significantly improves the sample efficiency and the final success rate for long horizon exploration."
SP:9911693a04a300b5a93634fb0267ef83e5489d77,"This paper proposes a Bayesian framework for generating local explanations for black box models. The motivation is that state - of - the - art black box explanation techniques are inconsistent, unstable, and require significant hyper - parameter tuning. The paper proposes to address these challenges by developing a novel framework that combines their associated uncertainty with their explanation intervals. The framework is based on Bayesian versions of LIME and KernelSHAP. The main contributions are as follows :   1. The authors propose a novel Bayesian explanation framework based on LIME / SHAP.   2. They use it to approximate the uncertainty of the two popular explanation methods, LIME & SHAP, by estimating the number of perturbations needed to generate explanations with a desired level of uncertainty. 3. They show that their uncertainty sampling technique speeds up the process of generating explanations by up to a factor of 2 relative to random sampling of explanations. 4. They evaluate the efficacy of the proposed framework on a variety of datasets including COMPAS, German Credit, ImageNet, and MNIST."
SP:5efb4b81bd37c70640e8768e9dfb5bba14a0cfb8,"This paper presents a method for tackling the property difference due to similarity between features in Adder neural networks ( ANNs ) and CNNs. The authors propose to pre - define the feature distributions in order to model the heavy -tailedness in ANNs and instead make use of a mixture of Multivariate Skew Laplace Distributions ( MLE ). They show that MLE improves classification accuracy by 0.7 % on both CIFAR-100 and ImageNet with ResNet-18 compared to vanilla ANN with only a modification to the ANN classifier head. The main idea is to embed this mixture of skew Laplace into the loss function through substituting the distribution parameters for the head of the classifier in the pre - defined distribution head. A likelihood regularization comes naturally for fitting ANN features to pre-defined distributions to achieve good classification performance. However, the authors introduce an angle - based constraint on the feature distribution based on their locations, covariance and skewness, which drives the distribution tails to different angle regions for disentanglement. Experiments conducted on several benchmarks and comparison with other distributions demonstrate the effectiveness of proposed approach for boosting the performance of ANNs."
SP:cbccb65457564992d534504c0d060da44cafce8c,"Gradient starvation arises when cross entropy loss is minimized in training neural networks with over - parametrized weights. Gradient descent is a term coined by the authors to refer to a phenomenon that occurs when the weights of a neural network are over - parameterized, i.e., they capture only a subset of features relevant for the task, while ignoring other predictive features that are present in the training data. The authors provide a theoretical explanation of why this happens and a formalization of the properties of the learning dynamics during gradient descent that lead to this imbalance, and prove that such a situation can be expected given certain statistical structure. Based on this formalism, the authors develop guarantees for a novel but simple regularization method aimed at decoupling feature learning dynamics, improving accuracy and robustness in cases hindered by gradient starvation. They illustrate their findings with simple and real world experiments."
SP:8f6fe37cb0a332b66e10cc00261a44622841c8c6,"This paper investigates the relationship between human and AI in Hanabi, a cooperative game in which humans and agents work in teams of humans and AI agents. The authors evaluate both rule - based and learning - based agents in the following ways : ( 1 ) on the objective metric of the game score, ( 2 ) subjective metrics of the human ’s perceived performance, teamwork, interpretability, trust, and overall preference of AI teammate, ( 3 ) across nearly all subjective metrics, and ( 4 ) across the entire game, they find that humans have a clear preference toward a rule based AI agent ( SmartBot ) over a state - of - the - art learning based agent ( Other - Play ).    The paper is interesting in highlighting the need to incorporate subjective metrics into the evaluation of human - AI teaming rather than a singular focus on objective task performance. However, I do have some concerns about the authors ’ claims that this paper is “ the first quantified study of subjective human preferences toward such AI ”. My main concerns are as follows : ( a ) the authors do not compare their results with the state of the art collaborative RL agents ( e.g., Deep RL ) in the setting of Hanabi and ( b ) their results do not support their hypothesis that collaborative RL can create a superior AI agent over rule based agents. I also have concerns that the authors did not provide sufficient explanation of their results to support their claim that the current collaborative RL agent is superior."
SP:2a05e333fc1a14057515ef3addde9a40152373db,"This paper presents a novel approach for the task of visual question generation ( VQG ), which aims to generate human - like neural questions from an image and potentially other side information ( e.g., answer type or the answer itself ). Existing works often suffer from the severe one image to many questions mapping problem, which generates uninformative and non - referential questions. The key rationale is that the salient visual regions of interest can be viewed as a constraint to improve the quality of the generated questions. To this end, the authors propose a novel learning approach for learning a learning model for the problem : a Double - Hints guided Generative Adversarial Networks ( DH - GAN ) consisting of a question generator and a question - answer - aware discriminator. The proposed method outperforms the state - of - the - art approaches by a large margin on two benchmark datasets, including both automatic machine metrics and human evaluation."
SP:15756d6ef47b39ded404acea2135c93bd5ee1062,"This paper proposes Generalized Data Weighting ( GDW ), a method to mitigate label noise and class imbalance by manipulating gradients at the class level. The main contribution of the paper is to propose a two - stage scheme embedded in a bi - level optimization framework which does not introduce any extra computational cost to obtain class - level weights. The authors propose to unroll the loss gradient to class-level gradients by the chain rule and reweights the flow of each gradient separately. In this way, GDW achieves remarkable performance improvement on both issues. GDW outperforms state - of - the - art methods by 2.56 % under the 60% uniform noise setting in CIFAR10."
SP:7a8f56a01bec51ebf70d9ff689005a62cccfe5c6,"This paper proposes a new language grounding task for embodied agents. The goal is to learn the meaning of spatio - temporal descriptions of behavioral traces of an embodied agent. This is achieved by training a truth function that predicts if a description matches a given history of observations. The descriptions involve time - extended predicates in past and present tense as well as spatio-temporal references to objects in the scene. To study the role of architectural biases in this task, the authors train several models including multimodal Transformer architectures and the latter implement different attention mechanisms. The authors observe that maintaining object identity in the attention computation of the Transformers is instrumental to achieving good performance on generalization and that summarizing object traces in a single token has little influence on performance. They then discuss how this opens new perspectives for language - guided autonomous embodied agents and propose a code under open - source license to encourage the wider community to build upon and extend this work in the future."
SP:3d4a9d439bc84c3b0e6600f6985a23bdf95cd67f,"This paper presents a method for multiple object tracking and segmentation based on Prototypical Cross - Attention Network ( PCAN ). PCAN first distills a space - time memory into a set of prototypes and then employs cross - attention to retrieve rich information from the past frames. To segment each object, PCAN adopts a prototypical appearance module to learn contrastive foreground and background prototypes, which are then propagated over time. The authors also develop a MOTS approach that employs PCAN on frame and instance - level. Experiments are conducted on both the BDD100K dataset and the YouTube - VIS dataset, and show that PCAN outperforms other methods."
SP:1175ad16382b349ab1a39895150172d266abe571,"This paper studies the question of whether gradient descent represents gradient descent in deep learning. The main contribution of the paper is a theoretical analysis of the degree of approximation of gradient descent based on the curvature around the trajectory of the gradient flow trajectory. The authors show that gradient descent is approximated by a degree that depends on the radius of curvature of the trajectory. They show that this approximation holds for deep neural networks with homogeneous activations. They also show that the approximation holds even for gradient flow over deep linear neural networks.   The authors conduct experiments to verify their theoretical analysis through experiments with basic deep learning settings, which demonstrate that reducing the step size of gradients descent often leads to only slight changes in its trajectory. This confirms that in basic settings, central aspects of deep neural network optimization may indeed be captured by gradient flow."
SP:b8412e9ce82ce92125fe7cd3aff7bea8b906d16e,"This paper considers a stochastic multi - armed bandit ( MAB ) problem with delayed impact of actions. In this setting, actions taken in the past impact the arm rewards in the subsequent future, which is prevalent in the real world. For example, the capability to pay back a loan for people in a certain social group might depend on historically how frequently that group has been approved for loan applications. The authors propose an algorithm that achieves a regret of â‚¬3.3 and shows a matching regret lower bound of ⌦(KT 2 / 3 ), where K is the number of arms and T is the learning horizon. They generalize the bandit setting to encode the dependency of this “ bias ” due to the action history during learning.   The goal is to maximize the collected utilities over time while taking into account the dynamics created by the delayed impact created by MAB. The proposed algorithm is based on the Markovian [ 39 ] and combinatorial bandits [ 13 ] in that they also assume the Lipschitz reward structure and consider combinatorially action space. The results of the proposed algorithm are similar to those of previous works that have formulated delayed action impact in bandit learning [ 56, 38 ]."
SP:9c1d678dff5f609197dc3cfb67b841827f4a439a,"This paper proposes a new method for video instance segmentation ( VI ) based on transformers. It builds on the per - clip approach for video segmentation, which has been shown to provide superior performance over per - frame methods leveraging richer information from multiple frames. In this work, the authors propose to utilize memory tokens as a means of conveying information as well as summarizing each frame scene. The features of each frame are enriched and correlated with other frames through exchange of information between the precisely encoded memory tokens. The authors validate their method on the latest benchmark sets and achieved state - of - the - art performance ( 42.6 % on YouTube - VI ). The method can also be applied to near - online inference for processing a video in real - time with only a small delay."
SP:6c922eaa358f6fb9771690b1240e4f6f08a35b69,"This paper proposes a general graph embedding method, residual2vec, that can mitigate the bias of random walks in embedding based on structural properties of graphs. Specifically, the authors show that random walks are biased by the degree of each node, where a node is sampled proportionally to its degree of presence in the context of each context. They also show that the embedding process word2vec has an implicit bias arising from the optimization algorithm, skip - gram negative sampling ( SGNS ), which happens to negate the bias due to the friendship paradox. To leverage this feature further, they propose a more general framework, residual 2vec, which can also compensate for other systematic biases in random walks, such as bias of degree of edges and bias of clustering. The authors demonstrate that the proposed method performs better than conventional embedding methods in link prediction and community detection tasks, using a citation graph of 260k journals. Moreover, the debiasing allows them to explicitly model salient features of graphs that are not explicitly modeled in graph embeddings."
SP:851eac96135b577a5014166edcb43db6a190cf4b,"This paper studies the problem of estimating non - linear functionals of discrete distributions in the context of local differential privacy. The initial data are supposed to be distributed according to an unknown discrete distribution p = ( p1,..., pK ). Only α - locally differentially private ( LDP ) samples are publicly available, where the term ‘ local ’ means that each individual attribute xi is produced using one individual attribute. We exhibit privacy mechanisms ( PM ) that are either sequentially interactive ( i.e. they are allowed to use already published confidential data ) or non - interactive. The authors describe the behavior of the quadratic risk for estimating the power sum functional F(\kappa ), which is similar to the MLE analyzed by Jiao et al [ 18 ] in the multinomial model. However, due to the privacy constraint, the rates we attain are slower and similar to those obtained in the Gaussian model by Collier et al. [ 9 ]. In the noninteractive case, we study two plug - in type estimators, one for estimators of F(\alpha ) and the other for estimator of F(\beta ). We give lower bounds results over all α - LDP mechanisms and all estimators using the private samples.   The authors stress the fact that testing is a different problem from learning functional functions. For example, when the distance between two probability distributions is evaluated by a discrepancy or a distance, functionals Fγ naturally appear in their expression. Note however that testing rates may differ from the estimation rates of the discrepancy as is the case for the L1 distance."
SP:a0408b54f88a26479f33f36bb27e0a675f637ccd,"This paper proposes GAPPLETRON, a surrogate regret bounding algorithm for online multi - class classification, where the learner ’s feedback is determined by an arbitrary directed graph. The paper considers the problem of online multiclass classification in a setting where the learners ’ feedback is given by a graph - theoretic directed graph, which includes bandit feedback as a special case. The main contribution of the paper is to propose an algorithm that works with arbitrary graph - based feedback graphs, which has the potential to be used for a variety of downstream applications, including label efficient classification and filtering.    The main contributions of this paper are as follows :   1. A new algorithm based on an arbitrary graph based feedback graph, called APPLETron, is proposed to solve the surrogate regret problem. 2. A set of surrogate regret bounds are established for the proposed algorithm, which are shown to hold for a large class of surrogate losses, both in expectation and with high probability. 3. Experiments on synthetic data show that for various feedback graphs our algorithm is competitive against known baselines, such as GAN - GAN, in achieving a constant surrogate regret of order B in the full information case. 4. Further, experiments show that the bounds of the bounds are also valid for surrogate regret in the case of synthetic data."
SP:490262589efce6fb10b913431ec6db8d4e5b2dec,"This paper studies the problem of explainable clustering in the setting of k - clustering, a setting first formalized by Dasgupta, Frost, Moshkovitz, and Rashtchian ( ICML 2020 ). The authors give an algorithm that outputs an optimal ( not necessarily explainable ) clustering for k - median objective and k - means objective, and a factor of O(k log k ) for the k - meant objective. They also give a lower bound showing that an Ω(log k ) loss is unavoidable for both objective and objective - combined with upper bounds that generalize to objectives given by higher `p - norms. The algorithm is simple, simple simple, and simple simple. In particular, it is given an initial not - necessarily - explainable ( R ) initial for k- median objective in R, which is oblivious to the number of data points and runs in time O(dk log k, independent of the number   n ) n, n. Similarly, in R - Means objective, the algorithm outputs an output that outputs an output with an output similar to that of R - Median objective, but with an upper bound that is better than the previous best upper bounds.   The main contribution of this paper is the following :    1. A new lower bound on the upper bound of the lower bound for the variance in the variance of the objective output of the algorithm. This lower bound matches the previous upper bound for objective output. 2. An algorithm that produces an output for explainable k - mean objective and explainable mean objective. 3. A factorization of the output of this algorithm. 4. An explanation algorithm for the loss incurred by this lower bound. 5. A proof that the algorithm's upper and lower bounds are better than previous upper bounds for objective and mean."
SP:6a9e47be710ddaf386bffc54d003d7dc2b67fdc3,"This paper proposes a multilingual language model ( PrLM ) that combines implicit language modeling and universal dependency parsing. Syntax in terms of universal dependency parse serves as not only pre - training objective but also learned representation in the PrLM. The authors claim that the proposed PrLM provides interpretability and convenience in downstream language processing tasks, in which multilingual PrLM takes advantage of language universality to alleviate the issue of limited resources for low - resource languages. Experiments on cross - lingual NLU benchmarks : XNLI and XQuAD, and linguistic structure parsing datasets : UD2 v2.7, SPMRL’14, and the English Penn Treebank ( PTB ) show that universal structure knowledge learnt and integrated can indeed help the mult bilingual PrLM obtain better universal linguistic word representations and outperform m -BERT and XLM - R baselines in all the above tasks."
SP:94f4b65214a648cbc84f13beba45a825e2e9901a,"This paper presents a method for learning transformer based improvement models for solving vehicle routing problems ( VRPs ). The authors claim that the relative PE method is not suitable for representing VRP solutions due to the noise and the lack of symmetry of the solutions. To address this problem, the authors propose a method called Dual - aspect Collaborative Transformer ( DACT ) to learn embeddings for node and positional features separately, instead of fusing them together as done in existing ones, so as to avoid potential noises and incompatible correlations. The positional features are embedded through a novel cyclic positional encoding ( CPE ) method to allow Transformer to effectively capture the circularity and symmetry of VRP solution ( i.e. cyclic sequences ).   The authors also train DACT using Proximal Policy Optimization ( PPO ) and design a curriculum learning strategy for better sample efficiency. They apply DACT to solve the traveling salesman problem ( TSP ) and the capacitated vehicle routing problem ( CVRP ). They show that DACT achieves better performance than the baselines in both synthetic and benchmark settings."
SP:e5c8680d8da9e7548fcb9bb5c073848eb80e1dd0,"The paper considers the Bayes error, a quantity that measures the optimal classification error theoretically achievable for a given data distribution. While generally intractable, the paper shows that it is possible to obtain an exact estimate of the error of generative generative models learned using normalizing flows. The method relies on a fundamental result, which states that the error is invariant under invertible transformation. The paper then proposes to use this result to compute the exact error of the learned flow models by computing it for Gaussian base distributions, which can be done efficiently using Holmes - Diaconis - Ross integration. Moreover, the method is able to evaluate the intrinsic "" hardness "" of standard benchmark datasets, which is defined as the hardness caused by the internal data distribution p. The focus of this work is about the latter, and it is shown that even if p. is known to be a particular value EBayes, it may be highly unlikely that this error is achievable given a model trained on only N samples from p.    The paper also shows that, depending on the temperature of the learning process, some synthetic datasets can be generated that closely resemble standard benchmarks. However, with almost any temperature adjustment, the synthetic datasets are capable of obtaining accuracy very near optimal. Finally, the authors also show that, under certain settings, some of these synthetic datasets may be able to achieve accuracy close to the optimal."
SP:2896679f0472522bc3334178cd7574494cf12b7b,"This paper presents GradInit, an automated method for initializing neural networks. GradInit is based on a simple heuristic ; the norm of each network layer is adjusted so that a single step of SGD or Adam with prescribed hyperparameters results in the smallest possible loss value. This adjustment is done by introducing a scalar multiplier variable in front of each parameter block, and then optimizing these variables using a simple numerical scheme. Grad Init accelerates the convergence and test performance of many convolutional architectures, both with or without skip connections, and even without normalization layers. It also improves the stability of the original Transformer architecture for machine translation, enabling training it without learning rate warmup using either Adam or SGD under a wide range of learning rates and momentum coefficients. The experiments show that Grad Init is a useful tool for identifying potential causes for instability at initialization, such as those imposed by normalisation layers, and we summarize interesting scale patterns learned by Grad Init that can be helpful for designing better initialization rules."
SP:f69731403592fa5bdd4ca327708582d615aa131c,"Linear mixed - effect models provide a natural baseline for estimating disease progression using longitudinal data. They provide interpretable models at the cost of modeling assumptions on the progression profiles and their variability across subjects. A significant improvement is to embed the data in a Riemannian manifold and learn patient - specific trajectories distributed around a central geodesic. The authors extend this approach by learning the metric from the data allowing more flexibility while keeping the interpretability. Specifically, they learn the metric as the push - forward of the Euclidean metric by a diffeomorphism, which is estimated iteratively as the composition of radial basis functions belonging to a reproducible kernel Hilbert space. The metric update allows us to improve the forecasting of imaging and clinical biomarkers in the Alzheimer’s Disease Neuroimaging Initiative ( ADNI ) cohort. Our results compare favorably to the methods in the benchmarking in the TADPOLE challenge."
SP:438e906f52c4c0538956b51a2270b3ac498b27a8,"This paper proposes a method for constructing parallel convolutional neural networks, named routing - by - memory ( PU ), for extracting semantic features from CNNs. PU consists of two parts : a memory head and a procedure head. The memory head maintains a summary of a type of features, and the procedure head applies the corresponding procedure to the corresponding intermediate features in a divide - and - conquer fashion. The method is applied to VGGNet, ResNet, and EfficientNet and achieve significant improvements in the accuracies on Tiny ImageNet, Imagenet, and CIFAR-100 benchmarks.   The main contributions of the paper are as follows : ( 1 ) It proposes a PU mechanism for existing CNN architectures, and introduces parallel PUs for each stage of the network. ( 2 ) It applies a four - step training strategy to train the PUs. ( 3 ) Experimental results show that the proposed PU improves the performance of VGGNets and ResNets, and marginally improves EfficientNets. ( 4 ) It also applies the PU mechanism to TinyImageNet and TinyImagenet."
SP:d240173080cd3647dbaa5173a6422396f226775b,"This paper studies the equivariance of certain fundamental symmetries in classical physics. The authors show that it is simple to parameterize universally approximating polynomial functions that are equivariant under these symmetsries, or under the Euclidean, Lorentz, and Poincaré groups, at any dimensionality d. They show that the scalar - based method is simple, efficient, and scalable. They also note that the symmetris considered in this work are all global symmetrie, as they act on all points in the same way. However, they believe their model could be made general enough to encompass gauge symmetry if we replace the global metric by any position - dependent metric, and this can be done with parallel transport. The parallel transport operations would also have to obey our invariance characterization, but they believe this is possible to do."
SP:72c0f47566904deb27d8157da30807ec1d6b5685,"This paper introduces a new family of bounding box regression loss functions based on the intersection over union ( IoU ) loss and its variants, which are the most commonly used loss functions for bbox regression. The authors generalize existing IoU - based losses to a family of losses that have a power IoU term and an additional power regularization term with a single power parameter α, which they call the α - IoU losses. They analyze properties such as order preservingness and loss / gradient reweighting. Experiments on multiple object detection benchmarks and models demonstrate that the new losses can surpass IoU-based losses by a noticeable performance margin. They also show that the α-IoU losses are more robust to small datasets and noisy bboxes."
SP:397125177d7007316d67194ec00d5dc57b44ac79,"This paper considers the imitation learning problem of learning a policy in a Markov Decision Process ( MDP ) setting where the reward function is not given, but demonstrations from experts are available. Finding a policy that is distributionally robust against noisy demonstrations based on an adversarial construction potentially solves this problem by avoiding optimistic generalizations of the demonstrated data. This paper studies Distributionally Robust Imitation Learning ( DROIL ) and establishes a close connection between DROIL and Maximum Entropy Inverse Reinforcement Learning ( MIRL ). DROIL can be seen as a framework that maximizes a generalized concept of entropy.   The authors develop a novel approach to transform the objective function into a convex optimization problem over a polynomial number of variables for a class of functions that are additive over state and action. This approach, called Our approach lets us optimize both stationary and non - stationary policies and, unlike prevalent prevalent previous methods, it does not require repeatedly solving an inner reinforcement learning problem. The authors experimentally show the significant benefits of DROIL’s new optimization method on synthetic data and a highway driving environment."
SP:58f220bbbed8d3e0633b408fca3b6838c4ad323d,"This paper proposes a general post - processing algorithm for individual fairness ( IF ), where the learner only has access to the predictions of the original model and a similarity graph between individuals guiding the desired fairness constraints. The main appeal of postprocessing is that it avoids expensive retraining. The authors cast the IF postprocessing problem as a graph smoothing problem corresponding to graph Laplacian regularization that preserves the desired “ similar individuals similarly ” interpretation. They empirically demonstrate the connection of the new objective function to a local relaxation of the objective function in the original individual fairness objective. Empirically, their post -processing algorithms correct individual biases in large - scale NLP models such as BERT while preserving accuracy."
SP:ef791aa29decd839e7e583c9d1f71e8309ca87ef,"This paper proposes a unified encoding method for cross - domain Text - to - SQL ( T2S ), where the authors adopt the graph structure to provide a unified model for both the natural language question and database schema. Based on the proposed unified modeling method, the authors devise a structure - aware aggregation method to learn the mapping between the question - graph and the database - graph. The proposed method is called Structure - Aware Dual Graph Aggregation Network ( SADGA ). The authors conduct extensive experiments to study the effectiveness of their proposed method on the benchmark, the Spider benchmark. The experimental results show that the proposed method outperforms the baseline methods."
SP:a2fa25a4539a38af61a0993f65ecc14339f26c2e,"This paper studies the problem of training end - to - end learnable discrete - continuous models for computation graphs with more than one discrete probability distribution in each execution path. The authors show that it is challenging to optimize the parameters of these models, mainly due to small gradients and local minima. They propose two strategies to overcome these challenges : 1 ) increasing the scale parameter of the Gumbel noise perturbations during training and 2 ) proposing dropout residual connections for discrete - continuous computation graphs. They show empirically that the proposed methods are required for training and outperform state - of - the - art approaches on several benchmark datasets."
SP:bb3ec363e90269db4a2ba99d8107cb56f86e68f0,"This paper studies the problem of covariate shift in Bayesian inference for neural networks. It shows that Bayesian neural networks ( BNNs ) with full - batch Hamiltonian Monte Carlo achieve poor generalization under covariate shifts, even underperforming classical estimation. The authors show how a Bayesian model average can in fact be problematic under covariates shift, particularly in cases where linear dependencies in the input features cause a lack of posterior contraction. They additionally show why the same issue does not affect approximate inference procedures, or classical maximum a - posteriori ( MAP ) training. Finally, they propose novel priors that improve the robustness of BNN with respect to many sources of covariates shifted data."
SP:f86ec7042e9b73ae071704a6d3ed17d7e3da1b75,"This paper categorizes meta - learning evaluation into two settings : in - distribution ( ID ), in which the train and test tasks are sampled iid from the same underlying task distribution, and out - of distribution ( OOD ) in which they are not. While most metalearning theory and some FSL applications follow the ID setting, the authors identify that most existing few - shot classification benchmarks instead reflect OOD evaluation, as they use disjoint sets of train ( base ) and test ( novel ) classes for task generation. The authors provide realistic examples of the ID scenario and show that the performance of popular meta -learning methods can drastically differ in ID vs. OOD scenarios, and provide suggestions on how to construct FSL benchmarks to allow for ID evaluation as well as more reliable OOD evaluations."
SP:371f77148b4f00a929f7c118b1bb7c5a6238d264,"This paper proposes an open rule induction method, Orion ( ORF ), which automatically inducts open rules from LMs without manually annotating them. This is in contrast to the traditional method of rule induction based on KB - based methods, which inducts rules by discovering data commonalities with the knowledge base. The authors argue that the current methods are “ learning rules from rules ”, while the proposed method is based on language model ( LM ) based rule induction. The main idea of the proposed system is to combine the expressive power of open rules with the inductive power of the LM - based method to generate rules that are expressive enough to be applied to the real world. The proposed method has been empirically validated on a number of downstream tasks, where it is shown to outperform the manually annotated open rules."
SP:8be2e0ea4a83fe32a4859f456007a829e5e9270a,"This paper introduces Implicit Constraint Q - Learning ( ICQ ), a novel offline RL algorithm for multi - agent learning. The main idea is to learn from a dataset of state - action pairs of a single agent and a joint policy, and then use the value from the value of the action pairs to estimate the extrapolation error of the joint policy.    The paper considers the setting where a large number of agents cooperatively complete a task in StarCraft II. The authors show that the single agent version of ICQ outperforms D4RL, a standard single - agent offline benchmark. They extend ICQ to multi - agents tasks by decomposing the joint - policy under the implicit constraint. They further show that ICQ achieves the state - of - the - art performance in the challenging multi -agent offline tasks."
SP:1939b24b68970c33ca16ce238deed257f76d009e,"This paper presents a method for generating non - uniform adversarial examples ( AEs ) for adversarial training of machine learning models. It is well known that uniform norm - bounded perturbations do not yield AEs that are realistic for real - world adversaries. This paper proposes a method to overcome this issue by taking into account the characteristics of the empirical data distribution, the dependence of the features on each other, and the importance of features on the training data. The method is applied to generate AEs for the following applications : malware detection, credit risk prediction, and spam detection. The AEs generated by the proposed method outperform uniform AEs in domains such as finance, finance, and social networks.   The key idea of the proposed approach1 is to enable non uniform perturbation that can adequately represent the feature dependencies during adversarial learning. This is achieved by using the following three components : ( 1 ) the distribution of the data, ( 2 ) the correlation between the features, and ( 3 ) the dependence on the features that the perturbed images have on training the model. The experiments show that the method outperforms uniform perturbed AEs by a large margin."
SP:417b30930b245667d777e5d90ee80dd41546760e,"The authors extend the spectral filtering theory of Marteau-ferey et al. [ 1 ] to generalized self - concordant loss functions ( GSC ), which contain the logistic loss. They show that spectral filtering regularization can be used to obtain faster convergence rates of the excess risk than Tikhonov regularization for GSC under certain assumptions. These assumptions are the source and capacity conditions, which characterize the difficulty of the learning task, and are related to the proximal point method in optimization, and overcomes the limitation of the classical Tikhonenov regularisation. The main result is a probabilistic upper bound on the maximum excess risk, which is optimal given usual source - capacity assumptions.   The authors also show that the iterated version of spectral filtering can be applied to GSC in a similar way to regularize the residuals of proximal - point regularization. This approach is equivalent to performing a few steps of the proximally - fitting regularization in the context of least squares. The authors claim that this approach is suboptimal in practice because it suffers from a “ saturation ” effect, which occurs when the learning tasks become simpler, and the learning rate stops improving."
SP:1caeee4f00b52fe356ff4e5dd004d0203e838370,"This paper proposes a new neural network architecture named Deformable butterfly ( DeBut ) that generalizes the conventional butterfly matrices and can be adapted to various input - output dimensions. It inherits the fine - to - coarse - grained learnable hierarchy of traditional butterflies and when deployed to neural networks, the prominent structures and sparsity in a DeBut layer constitutes a new way for network compression. The authors apply DeBut as a drop - in replacement of standard fully connected and convolutional layers and demonstrate its superiority in homogenizing a neural network and rendering it favorable properties such as light weight and low inference complexity, without compromising accuracy. The natural complexity - accuracy tradeoff arising from the myriad deformations of a deBut layer also opens up new rooms for analytical and practical research.   The authors introduce a new kind of linear transform named DeBut product chain ( or simply DeBut chain ) that can be sized to adapt to different input -output dimensions. For one thing, it does not limit itself to PoT blocks as in Fastfood Transform or Butterfly matrices [ 20, 34, 5, 6 ]. Moreover, the intermediate matrix dimensions in a Delbut chain can either shrink or grow to permit a variable tradeoff between number of parameters and representation power. The flexibility of tuning the dense matrix sub - blocks in DeBut also permits an interpretation similar to the CNN receptive field for exploiting locality and correlation in data. In fact, as will be shown in experiments, a DeToB chain does not distinguish CONV or FC layers, and can even be used as a substitute of both while maintaining a high output accuracy. To our knowledge, the DeBut linear transform is proposed for the first time, and this work is a starter to showcase its use in DNNs which we hope can provoke further theoretical and practical insights."
SP:d345ce1d7afc367ee1a9fb68d50ff1b2219f02cb,"This paper proposes MetA Reusable Knowledge ( MARK ), a method to address the problem of catastrophic forgetfulness in artificial neural networks. The authors propose a method that maintains a set of shared weights among tasks that can be used for weight reusability when learning a new task. Specifically, the authors propose to use the shared weights as a common Knowledge Base ( KB ) that is enriched with new knowledge as the model learns new tasks. To this end, a metalearning approach is used to incrementally enrich the KB with new information. The method is evaluated on the 20 - split Mini - ImageNet dataset and compared with several baselines. The results show that MARK outperforms the other methods in terms of average accuracy while achieving almost zero forgetfulness using 55 % of the number of parameters."
SP:722c52467e384058f8fdffa254d0e8db47440a64,"This paper proposes a data - driven framework for scheduling heuristics in an exact MIP solver. The main idea is to learn from data describing the performance of primal heuristic on two classes of challenging MIP instances. Based on this data, the authors propose a problem - specific schedule of heuristic scheduling for each instance. The schedule is designed to be flexible enough to adapt to changing the nature of the challenging instances.    The main contributions of the paper are as follows :   1. The authors propose an algorithm that learns from data to schedule the heuristic planning based on the parameters of the primal integral. The algorithm is designed based on two types of data. The first is that of a set of MDP solvers. The second is based on an exact set of matching MDPs. The algorithms are designed for each of these two classes.  2. The method is tested on two sets of challenging instances, where it is shown that it is able to reduce the average primal integral by up to 49 % on one class and up to 50 % on the other class. The methods are validated on a number of benchmark datasets and compared with several baselines."
SP:5a21f0a49731dcb1d68deb06a75138e8e9d514d5,"This paper presents a theory of reinforcement learning ( RL ) in which the learner receives binary feedback only once at the end of each episode. The goal is to show that learning is possible in this more challenging setting where trajectory labels are generated by an unknown parametric model, and provide a statistically and computationally efficient algorithm that achieves sub - linear regret.    The main contribution of this paper is to provide an algorithm that provides binary feedback at each step of an episode to a parametric learner that is trained to predict trajectories for a given set of episodes. The algorithm is based on the following assumptions : ( 1 ) it is possible to learn trajectories that are close to the true trajectories of the model, ( 2 ) the model is capable of producing trajectories close to a true trajectory, ( 3 ) the learning algorithm is computationally and statistically efficient, and ( 4 ) the reward function provided by the algorithm is close to that of the true trajectory. The paper presents two main results : ( a ) under an explorability assumption, it is shown that the proposed algorithm achieves sublinear regret, ( b ) the algorithm achieves better performance than the state - of - the - art in terms of accuracy, ( c ) it provides a statistically significant reduction in the number of false positives, ( d ) it demonstrates that the algorithm can learn to predict a trajectory that is closer to the truth than the true one, ( e.g., it achieves a better accuracy than the best known RL algorithm ). The second main result is more difficult to evaluate as it is not known whether the trajectory is “ good ” or “ bad ”, but harder to evaluate is whether the algorithm “ learns ” to predict it. It is shown to perform better than the SOTA algorithm and the baseline RL algorithm."
SP:e66bd9582058ba0f6091bb1042ce2ecfdaae1515,"This paper proposes a novel edge representation learning framework based on Dual Hypergraph Transformation ( DHT ), which transforms the edges of a graph into the nodes of a hypergraph. This dual hypergraph construction allows the authors to apply message - passing techniques for node representations to edges. After obtaining edge representations from the hypergraphs, the authors then propose to either cluster or drop edges to obtain graph - level representations of the nodes. The proposed method, called Edge Representation Learning ( ERL ), outperforms state - of - the - art graph representation learning and graph pooling methods on graph classification, not only because of its accurate edge representation but also due to its lossless compression of nodes and removal of irrelevant edges."
SP:e398873e29b05176e1d52dc6f86a59a4f405e6fd,"This paper studies the problem of learning representations of data that are sufficient for reinforcement learning ( RL ) in the context of Mutual Information Maximization ( MI ), a theoretical formulation of a state representation for learning representing the optimal policy. The paper proposes to study several objectives of MI : 1. Representation learning objectives based on the maximization of mutual information ( MMI ). This paper is motivated by the fact that it is well known that MMI can be used to learn representations of high - dimensional observations, which can accelerate learning by discarding irrelevant and redundant information, while retaining the information necessary for control. 2. Representations of high dimensional observations that are representative enough to be used for the objectives of MDPs that are commonly used in RL.   The paper first provides a theoretical analysis of two objectives of the paper, namely, Representation Learning for Policy Optimization ( RLP ) and State Representation for Policy Discovery ( SRD ). The analysis is based on two main assumptions : ( 1 ) that the objective of SRD is to learn a representation of the optimal state policy, and ( 2 ) that it should be able to estimate the maximum information value ( MV ) from the observed data points. Based on the analysis of the theoretical analysis, the paper finds that the first of these two objectives is not sufficient for RL. The second and third objectives are sufficient, but only if one of them is assumed to be the maximum MV. The authors then experimentally show that SRD does not yield sufficient representations for the first and second objectives, and that the third one is sufficient for the third and fourth objectives. The experimental results verify the theoretical findings."
SP:50181f740910195d3a50dd7d7f8cbb1c476d730b,"This paper presents a novel method for 3D pose estimation based on steerable convolution, named Sparse Steerable Convolution ( SS - Conv ). The proposed method is based on a sparse convolution with sparse tensors, while strictly preserving the property of SE(3)-equivariance. The authors propose a general pipeline based on SS - Conv, which decodes object poses directly from the learned convolution and a novel Feature - Steering module to support iterative pose refinement. They conduct thorough experiments on three tasks of pose - related, 3D object semantic analysis, including instance - level 6 - D pose estimation, category-level 6D pose and size estimation, and category - level six - d pose tracking. Their proposed pipeline outperforms existing methods on almost all the metrics evaluated by the three tasks. Ablation studies also show the superiority of their SS -Conv over alternative convolutions in terms of both accuracy and efficiency."
SP:d746bfb200577c980d92727bb0b1a3c23e7bfdc5,"This paper proposes a dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input. Specifically, the authors devise a lightweight prediction module to estimate the importance score of each token given the current features. To optimize the prediction module in an end - to - end manner, they propose an attention masking strategy to differentiably prune a token by blocking its interactions with other tokens. The experimental results demonstrate the competitive trade -off between speed and accuracy. The authors demonstrate the possibility of exploiting the sparsity in space for the acceleration of transformer - like models. In particular, by hierarchically pruning 66 % of the input tokens, they can greatly reduce 31% GFLOPs and improve the throughput by over 40 %. The drop of accuracy is within 0.5 % for all different vision transformers."
SP:d0b6cde42b1cba5e6e3c7c5131426fd84adbd3d7,"This paper studies the problem of obtaining distribution - free guarantees for predictive inference in continuous regression, where the features X are continuously distributed. In particular, the authors focus on the setting where the confidence intervals of the underlying regression function E [ Y | X ] must be non - vanishing width, even as the sample size tends to infinity, and where there are only a small number of possible values of X under certain assumptions. The authors claim that existing methods such as holdout methods, conformal prediction, and cross - validation methods are able to provide such guarantees, but that inference for the underlying function is more challenging. They study the problem in settings in between the finite setting and the continuous setting, where vanishing - width confidence intervals are achievable if and only if the effective support size of the distribution of X is smaller than the square of the diameter of the sample in question. They find that there are several distinct regimes in between these two extremes.    The main contributions of the paper are the following :   1. An analysis of the relationship between confidence intervals and the expected values of features X under the continuous distribution of features, and the distributional assumptions under which these features are obtained. This is done by considering the case where the feature X is continuous and the features Y are discrete, and assuming that the intervals between features Y and Z are at least as large as those between features A and Z, and that they do not have to be larger than the average of the squares of the features across the features in question 1. 2. A discussion of the conditions under which it is possible to obtain distributional guarantees for underlying regression functions under continuous distributional distributions, and which are not possible under finite distributional settings. 3. 4. An explanation of why it is difficult to obtain such guarantees under finite setting, and how to choose between the two settings."
SP:123952325765c040c3078fc7dca2b6d370e55590,"This paper proposes Representation Neutralization for Fairness ( RNF ), a method to mitigate bias in learning by debiasing only the task - specific classification head of DNN models. The key idea of RNF is to discourage the classification head from capturing biased information from the encoder. To this end, they leverage samples with the same ground - truth label but different sensitive attributes, and use their neutralized representations to train the classification heads of the DNN model. Experimental results on several benchmark datasets demonstrate the effectiveness of the RNF framework. Additionally, they show RNF to be complementary to existing methods that learn debiased encoders and can be further improved within their framework."
SP:210eb2c811f966bb1ac53932cacabbad9bb608fe,"This paper proposes a new convolutional layer, B - CNNs, that is invariant to translations and rotations. It is based on Bessel's rotation - invariant convolutions, which are a special case of rotational invariance in CNNs. The key idea is to take advantage of Bessels'rotation angles and learn a set of possible rotation angles by design. To achieve this, the authors propose a Bessel - CNN that takes into account all possible rotational angles in the continuous set of rotation angles.   The main contribution of the paper is the formulation of a new method, the Bessel method, to obtain this invariance. The paper also proposes a number of other related works, including a new CNN layer, a CNN layer with Bessel rotation angles, and a CNN + Bessel layer with a different rotation angle."
SP:ee51ecbd476d5b65903c942a62be89ff5d91698b,"This paper introduces ParK, a new method for solving kernel ridge regression problems. The proposed method combines partitioning with random projections and iterative optimization to reduce space and time complexity while provably maintaining the same statistical accuracy. In particular, constructing suitable partitions directly in the feature space rather than in the input space, promotes orthogonality between the local estimators, thus ensuring that key quantities such as local effective dimension and bias remain under control. The authors characterize the statistical -computational tradeoff of their model, and demonstrate the effectiveness of their method by numerical experiments on large - scale datasets.   The paper is organized as follows :   1. State the problem and recall the basics of Kernel Ridges Regression ( KR ). 2. Introduce ParK. 3. In Section 3, illustrate the algorithm, in Section 4, analyze the prediction error of the method, and in Section 5, present the results of our numerical experiments. 4. Draw the conclusions and report the main limitations of the work."
SP:1f096d6fabd5b1fde43d06c552d46d87cd35cb4a,"This paper proposes a novel method for learning to communicate via discrete tokens within a learned continuous space. The authors claim that the standard practice of using one - hot vectors as discrete communication tokens prevents agents from acquiring more desirable aspects of communication such as zero - shot understanding. The proposed method is inspired by word embedding techniques from natural language processing and uses discrete tokens derived from a learned, continuous space derived from the learned communication space. They show in a decision theoretic framework that their technique optimizes communication over a wide range of scenarios, whereas one hot tokens are only optimal under restrictive assumptions. In self - play experiments, they validate that our trained agents learn to cluster tokens in semantically - meaningful ways, allowing them communicate in noisy environments where other techniques fail. Lastly, they demonstrate that agents using their method can effectively respond to novel human communication and that humans can interpret unlabeled emergent communication tokens."
SP:8630ccc627534f9033bced04e2137a897ffef701,"This paper proposes a new model architecture called CoAtNets, which combines the strengths from two key insights : ( 1 ) that self - attention and attention can be naturally unified via simple relative attention ; ( 2 ) vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency ; and ( 3 ) that stacking layers of attention layers vertically stacking can improve generalization and efficiency.   The main contributions of the paper are as follows :   1. Introducing a new architecture that combines strengths from both ConvNets and Transformer models. The main contribution of this paper is to propose a family of hybrid models built from two of the key insights from Convolutional Neural Networks : ( i ) relative attention is a natural way to unite attention and convolutional attention, and ( ii ) stacking attention layers is an effective way to improve generalizability. 2. Combining the strengths of the two insights into a single model is a novel and interesting idea. 3. The proposed architecture achieves state - of - the - art performance under different resource constraints across various datasets. The experiments show that CoAtNet achieves SOTA performances under comparable resource constraints under different data sizes."
SP:d3ecbeeffa5ab365743ba8653c6739f24742ee31,"This paper proposes a new oracle bound based on a parametric form of the Chebyshev - Cantelli inequality, which is amenable to efficient minimization. The oracle is based on the second order Markov inequality introduced by Masegosa et al. [ 2020 ]. The authors also derive a new concentration of measure inequality which they name PAC - Bayes - Bennett inequality, since it combines PAC - bayesian bounding with Bennett ’s inequality. They use it for empirical estimation of the expected risk of a weighted majority vote in the case of PAC-Bayes - Bernstein. They provide an empirical evaluation demonstrating that the new bounds can improve on the work of Masego et al [ 2020]."
SP:5bac542a6532d43cf100e085398b4a4783719814,"This paper proposes a method for weakly - supervised audio - visual video parsing based on cross - modality and cross - video cross - supervision. The authors claim that the proposed method can be applied to the task of audio - video parsing. The proposed method is based on the following components : 1. Cross - video co - occurrence of events across different videos and modalities. This allows the authors to localize segments of target events while excluding irrelevant ones. 2. The discovery of supervisory signals across videos that can be used to train the method. 3. The ability of the method to discriminate between real and imagined events.   The method is trained on a dataset of over 1,000 videos. The training is done using the following steps :   1. First, the authors generate a corpus of videos and train a few rounds of the training. They use the generated corpus as a training data set to train their proposed method. This is done in two stages : during the first stage, the training is split into two stages. During the second stage, they train a subset of the videos and use the learned data to train a final version of their method. The second stage of training uses the learned corpus to train an encoder - decoder model. The encoder and decoder are trained on the same set of videos, but the encoder is only trained on one video and the decoder on the other one. The decoder is trained to distinguish real from imagined events in both videos and imagenet with the help of the learned from the imagined ones. They also train a decoder to distinguish between real vs imagined events using the imagined and imagined versions of the imagined events and vice versa. They compare their method with the one they trained and show that their method outperforms it."
SP:8fd6a03c1794afa524328d45f4232eacf6f86693,"This paper proposes QuPeD, a generalized federated learning algorithm that learns a generalized model via knowledge distillation ( KD ) among clients who have access to heterogeneous data and resources. The authors first propose an algorithm for learning quantized models through a relaxed optimization problem, where quantization values are also optimized over. When each client participating in the ( federated ) learning process has different requirements for the compressed model ( both in model dimension and precision ), the authors formulate a compressed personalization framework by introducing a Knowledge Distillation ( KD ) framework.   The authors develop an alternating proximal gradient update for solving this compressed personalisation problem, and analyze its convergence properties. The convergence results for the main convergence properties for centralized and personalized settings, respectively, are provided. The paper also provides extensive numerical results."
SP:fca8b4f1e765cf1724a37f0ae9a7dac1cb79c8b1,"This paper proposes a new method for constrained clustering based on variational auto - encoder ( VAE ). VAE is an extension of the Auto - Encoder framework developed in the context of deep generative models ( GNNs ). The main difference is that VAE incorporates instance - level clustering, while DC - GMM does not. The key idea of the proposed method is to use pairwise constraints to guide the clustering process towards a desirable partition of the data by indicating which samples should or should not belong to the same cluster. The authors provide extensive experiments to demonstrate the effectiveness of their method and compare it to state - of - the - art methods on a wide range of data sets.   The main contributions of the paper are as follows :   ( 1 ) The authors propose a new framework for constrained clustered clustering that is intuitive, interpretable, and can be trained efficiently in the framework of stochastic gradient variational inference ( SVI ). This is a novel and important direction to take in the era of deep learning. The proposed method can leverage prior information on a growing amount of only partially labeled data. It is an important step forward in the direction of unsupervised data clustering. ( 2 ) It is important to explicitly integrate domain knowledge in the form of probabilistic relations, which the authors explicitly integrate in the proposed model. ( 3 ) The experiments on two challenging real - world applications demonstrate the usefulness of their model and the potential of their approach."
SP:84379c0c881b7390ecc22fb398edfaf66d1af1ff,"This paper presents a kernel approximation method for large - scale learning based on the Neural Tangent Kernel ( NTK ) method. The authors propose a near - input - sparse time approximation method based on sketching the polynomial expansions of arc - cosine kernels for the convolutional version of NTK, which they call CNTK. They show that their sketch for NTK can transform any image using a linear runtime in the number of pixels. Furthermore, they prove a spectral approximation guarantee for the NTK matrix, by combining random features ( based on leverage score sampling ) of the arc - cosmicine kernels with a sketching algorithm. They benchmarked their methods on various tasks and show that a linear regressor trained on their method matches the one trained on the benchmarking dataset on CIFAR-10 while achieving 150× speedup."
SP:fa2668083ff3bb592c29a4c6822ae96ff54d0dbe,"This paper proposes a multi - person 3D motion prediction framework based on multi - range transformers. The key observation is that a human’s action and behaviors may highly depend on the other persons around, so the proposed framework introduces a Multi - Range Transformers model which contains of a local - range encoder for individual motion and a global - range encoding for social interactions. The Transformer decoder then performs prediction for each person by taking a corresponding pose as a query which attends to both local and global encoder features. The proposed method achieves significant improvement over state - of - the - art approaches for long - term motion prediction in 3D ( with 2 - 3 persons ). The gain enlarges as the future prediction time steps are increased from 1 second to 3 seconds. Qualitatively, the authors demonstrate that their method can predict interesting behaviors and interactions between different persons while previous approaches will repeat the same poses as it goes to further steps in the future. More interestingly, they extend the task to perform prediction with 9 - 15 persons by mixing the CMU -Mocap and Panoptic datasets."
SP:0a0e07af37c8fe8580639b1df62d27b6f63f8dee,"This paper proposes a new approach for guiding reinforcement learning in the long - horizon planning setting, where the goal is to learn a policy that is robust to unobserved and partially observed regions of the environment. The proposed approach, called predictive program synthesis ( MPPS ), trains a generative model to predict the unobserved portions of the world, and then synthesizes a program based on samples from this model to guide the policy. MPPS is evaluated on a set of challenging benchmarks, including a 2D Minecraft - inspired environment where the agent must complete a complex sequence of subtasks to achieve its goal, and achieves a similar performance as using handcrafted programs to guide   the agent. The experimental results demonstrate that MPPS can obtain the benefits of program - guided reinforcement learning without requiring the user to provide a new guiding program for every new task. In contrast, we are not directly synthesizing the policy, but a program synthesis, where a program is used to generate the guiding programs."
SP:5bb42b178b0d27da271bfa60e633fdac718638c4,"This paper investigates the problem of causal imitation learning in sequential settings, where the imitator must make multiple decisions per episode. The authors propose an efficient algorithm to determine imitability and to find the policy for each action that leads to proper imitation. They prove that the proposed criterion is complete ( i.e. both necessary and sufficient ). Finally, they verify that their approach compares favorably with existing methods in contexts where a demonstrator has access to latent variables through simulations. Due to space constraints, proofs are provided in the technical report."
SP:85bd81f0c5b6ccbc421ebbaf6f5c72164bc70b7f,"This paper presents an object - based transition model that decomposes a scene into objects, aligns them ( with respect to a slot - wise object memory ) to maintain a consistent order across time, and predicts how those objects evolve over successive frames. The model is trained end - to - end without supervision using transition losses at the level of the object - structured representation rather than pixels. The authors show that the combination of an objectlevel loss and correct object alignment over time enables the model to outperform a state - of - the - art baseline, and allows it to deal well with object occlusion and re -appearance in partially observable environments.   The main contributions of the paper are as follows :   1. Introducing a novel alignment module, which allows to deal properly with two issues that are not handled satisfactorily by other transition models, namely object persistence and object identity. 2. Providing object level loss over object - level representations instead of over pixels. 3. Improving the performance of the model on object identity and persistence."
SP:f32eddbb5c33a8422c075579ff08aa9833338d44,"This paper studies the problem of risk minimization ( ERM ) with respect to the exploration rate in the data. The main contribution of this paper is to provide a generalization algorithm based on the importance sampling weighted ERM that leverages the strong convexity of squared - error loss to obtain fast rates for regression and regret guarantees for policy learning.   The main contributions of the paper are as follows :   ( 1 ) This paper provides a generic algorithm for the exploration and regret of ERM for the context sensitive case where the data is collected using contextual bandit. The algorithm is applied to both Donsker - like and non - donsker entropy conditions and is compared to standard ERM as well as fast rates when a variance bound applies. It is shown that under these conditions the rate of exploration is fast for regression, while it is slow and regret - based rates are fast and regret based. The paper also shows that regret based rates can be used to obtain lower bounds for the lower bound of the upper bound in the literature. ( 2 ) The paper then applies the generic algorithm to the case when the data comes from a dataset that is more sensitive to the context than the data from which the ERM is not trained. This is shown to lead to faster regret rates and regret bound lower bounds. ( 3 ) This algorithm is then used for the policy learning problem, where the regret bound upper bound is obtained. ( 4 ) Finally, the paper also provides an empirical study of ISWERM and shows that it is able to close an open gap between the lower and upper bounds of the literature ( 5 ) in the case where policy learning is used."
SP:f549a0c231b71bae0acbed6e3afb41890ee89cd9,This paper proposes reparametrizing the training samples of kernel - reweighted regression with a doubly non - negative matrix in order to mitigate adversarial perturbations that can occur in the case of low - sample - size and covariate perturbation. The authors propose to use either the log - determinant divergence or the Bures - Wasserstein distance as the weighting metric in the kernel - weighted regression model. They show that the adversarially re - weighted estimate can be solved efficiently using first - order methods when the uncertainty is confined in an uncertainty set using either of the two weighting metrics. They also show that reweighting the training training samples can be effective in mitigating adversariially reweighing. They conduct extensive experiments on several datasets and show that their reweighting strategy is effective on some of them.
SP:fe12e13602925b9400fd596a987755beb10aa3d1,"This paper introduces a novel estimator based on importance sampling and statistical couplings for the categorical setting of gradient estimators for training models with discrete latent variables. The authors claim that the continuous relaxation of a continuous gradient estimator is not always available or tractable for the discrete case, and hence introduce a novel derivation of their estimator which is based on the construction of a stick - breaking coupling and statistical sampling. They also consider estimators based on reparameterization of categorical variables as sequences of binary variables and Rao - Blackwellization. They show that their proposed estimators outperform the state - of - the - art unbiased estimators across a range of problems without requiring more computation."
SP:e16fdf963ec2f9c0d79fa404e47e7862a5d6e922,"This paper proposes a new approach for finding the best architecture predicted by neural architecture search ( NAS ) predictors. The proposed approach is based on two key steps : sampling some architecture - performance pairs and fitting a proxy accuracy predictor. Given limited samples, these predictors, however, are far from accurate to locate top architectures due to the difficulty of fitting the huge search space. The authors propose a paradigm shift from fitting the whole architecture space using one strong predictor, to progressively fitting a search path towards the high - performance sub - space through a set of weaker predictors that are predicted to be more accurate than the strong predictor. The main idea is to sample a few well - performing architectures guided by the previously learned predictor and estimate a new better weak predictor to gradually refine the ranking of the sampling space. Experiments demonstrate that WeakNAS costs fewer samples to find top - performance architectures on NAS-Bench-101 and NAS - Bench -201. Compared to state - of - the - art ( SOTA ) predictor - based NAS methods, WeakNAS outperforms all with notable margins, e.g., requiring at least 7.5x less samples.   The paper reflects on a simple yet crucial question : if our final goal is to find the best architectures, do we really need to model the whole space well? This paper proposes to address this question by firstly combining the strong and weak predictors and then progressively changing the way they are used."
SP:8f74abb04037ba2e59dcf8320dc555b149f68ed8,"This paper presents Entropic Desired Dynamics for Intrinsic ConTrol ( EDDICT ), a method for learning state - action trajectories using latent codes. The method is based on the idea that agents can learn trajectories by maximizing the number of latent codes that can be discriminated from future states under some short - time horizon. The authors provide a simple instantiation of this idea, which assumes fixed additive latent dynamics, which results in tractable learning and an interpretable latent space. They then show that this latent space is globally consistent and can be used to construct a coordinate system that allows agents to reach more states in the long - term while still optimizing a local objective. They also show that their method is more exploratory than prior methods in terms of state coverage and unsupervised performance on hard - exploration games such as Montezuma's Revenge."
SP:c731a78c3e7f98ccd0253b51a0d42bf8deeb71f9,"This paper proposes a novel method for generating pharmacochemically acceptable molecules with large docking scores. The method is based on fragment - based generative RL with Explorative Experience replay for Drug design ( FREED ), which constrains the generated molecules to a realistic and realistic and qualified chemical space and effectively explores the space to find drugs by coupling them to a novel generation method and a novel error - free experience replay ( PER ). The docking score optimization is a difficult exploration problem that involves many local optima and less smooth surfaces with respect to molecular structure. The authors show that their method produces molecules of higher quality compared to existing methods while achieving state - of - the - art performance on two of three targets. They further show that predictive error - PER significantly improves the model performance and propose a novel explorative algorithms based on PER and show that they significantly improve model performance."
SP:b938bca513e7de1231212064caf8877a78d8b612,"This paper studies the problem of learning directed acyclic graphical models from observational data in settings without specific distributional assumptions. The authors propose an information - theoretic approach that uses a local Markov boundary search procedure in order to recursively construct ancestral sets in the underlying graphical model. They show that for certain graph ensembles, a simple forward greedy search algorithm ( i.e. without a backward pruning phase ) suffices to learn the Markov boundaries of each node. This substantially improves the sample complexity, which we show is at most polynomial in the number of nodes. This is then applied to learning the entire graph under a novel identifiability condition that generalizes existing conditions from the literature. As a matter of independent interest, the authors establish finite - sample guarantees for the problem. Moreover, they apply their results to the special case of the special - case of polytrees, where for which the assumptions simplify, and provide explicit conditions under which polytree are identifiable and learnable in time. They further illustrate the performance of the algorithm which is easy to implement, in a simulation study."
SP:af08109d4c45dc9401efb0e63c22167e9da28adb,"This paper studies differential privacy ( DP ) learning in the setting where each user holds m samples and the privacy is enforced at the level of each user ’s data. The authors show that, as long as each user receives sufficiently many samples, they can learn any privately learnable class via an (, )DP algorithm using only O(log(1 / 1 ) / ) users. For the local model, where d is the probabilistic representation dimension, the authors show a nearly - matching lower bound on the number of users required. A crucial component of the results is a generalization of global stability that allows the use of public randomness [ BLM20 ] that allows us to show that the global stability can be boosted to be arbitrarily close to one, at a polynomial expense in the amount of samples required.    The main contribution of this paper is a novel connection between correlated sampling ( CS ) and DP learning. CS is a tool from sketching and approximation algorithms [ Bro97, KT02, Cha02 ], which allows correlated sampling to be used for DP learning without the need for user access to the entire set of data. CS allows the correlated sampling strategy to be applied to learnable classes via a relatively small number of samples. This allows the authors to obtain results that are similar to those of [ 1 ] and [ 2 ]. The main difference between these two works is that the authors use CS to learn DP instead of O(d ) users in the case of local models, where D is the dimension of the representation dimension. The difference between the two works in the local models and the CS setting is that CS is used for learning DP algorithms, while D is used to learn private classes."
SP:da4f21d107a7f442c4d3e3ec13bdb44b041e07cf,"This paper provides a theoretical analysis of the convergence rate of gradient descent methods for learning transition and reward models of a latent Markov decision process based on value iteration networks ( VINs ). In particular, the authors show that for a linear parametrization, gradient descent with the implicit parameterization converges to global optima despite nonlinearity and non - convexity introduced by the implicit representation. The authors then use these results to discuss conditions under which end - to - end methods might be preferable. The paper also provides some empirical results in simple illustrative problems which serve to demonstrate properties derived from the analysis."
SP:992aa07d4f815d1c81f967374590eece933833b1,"This paper presents a method for cleaning up noise in Knowledge Graphs ( KGs ) through KG refinement task. The authors propose a method called IterefinE which iteratively combines two techniques : PSL - KGI and PSL / MLN for KG completion. PSL is an extension of Probabilistic Soft Logic ( PSL ) and MLN is a variant of Markov Logic Network ( MLN ). They show that the proposed method is able to reject noisy facts from KG and at the same time infer higher quality new facts. Experiments on a range of KG benchmarks show that this method can achieve a 9 % improvement of overall weighted F1 score.   The authors also show that their method can also perform longer chains of reasoning ( Implicitly ) by using PSL embeddings which perform longer chain of reasoning. The method operates in a co - training mode, which results in explicit type - supervised embedding of the refined KG from PSL from the authors'method."
SP:676fc4a3041af22e8f20ccba7daa2a0b1f5d6af5,"This paper proposes a new evaluation paradigm based on knowledge base completion ( KB ) methods. The authors argue that binary predictions are not essential to reflect the actual KBC quality, and propose a novel evaluation paradigm, designed to provide more transparent model selection criteria for a realistic scenario. They construct the data set FB14k - QAQ with an alternative evaluation data structure, where instead of single facts, they use KB queries, i.e., facts where one entity is replaced with a variable, and construct corresponding sets of entities that are correct answers. They remove some of these correct answers from the set, and simulating the realistic scenario of real - world entities missing from a KB, they can explicitly measure a model ’s ability to handle queries that have more correct answers in the real world than in the KB, including the special case of queries without any valid answer. They evaluate a number of state - of - the - art KB embeddings models on their new benchmark. They observe that good performance on the ranking task does not necessarily translate to good performances on the actual completion task. The results motivate future work on KB embedding models with better prediction separability and, as a first step, they propose a simple variant of TransE that encourages thresholding and achieves a significant improvement in classification F1 score relative to the original TransE."
SP:83fe0a496a79bcf97ccba1c6d34b7d11e7d5c330,"This paper proposes a new dialog system model, the Alternating Roles Dialog Model ( ARDM ), which is based on two pre - trained language models, BERT and GPT-2. BERT is a language model pretrained on a large set of human annotated data, while GPT - 2 is a more general language model trained on a much smaller set of data. The main novelty of the proposed ARDM is that it is able to work on two datasets, CamRest and MultiWOZ, without the need of extensive human supervision. The authors compare the performance of ARDM with state - of - the - art methods on both the popular task - oriented and non - task oriented datasets. ARDM performs better than the state of the art on the non - challenging task of persuading people to donate to a charity. On the other hand, the performance on the task of persuasion appears to be slightly worse."
SP:b11c06b7c4ef1aa43c59f808a679425e302d158e,"This paper proposes a new method to measure confidence in the probability that a class label predicted by a deep neural network is in the top 5 on the test set. It is well known that the softmax values of the network are not estimates of the probabilities of class labels, but there is a misconception that these values are not informative. The authors define the notion of implied loss and prove that if an uncertainty measure is an implied loss, then low uncertainty means high probability of correct ( or top k ) classification. The method is simple to use on existing networks. It proposed confidence measures for Top k which can be evaluated by binning values in the same way as binning confidence for methods which are quite accurate, as is the case for top 1 or top 5 uncertainty for image classification."
SP:ab9666e15f2a0113d96cb4b47b1cbb30fa1f7982,"This paper studies the generalization properties of neural networks as a function of their architecture and hyperparameters. In particular, the authors focus on neural networks with wide neural networks at large depths where the situation simplifies considerably. They show that in the large depth limit of training, random networks before training are Gaussian Processes governed by a kernel known as the Neural Network GP ( NNGP ) kernel, which is related to the Neural Tangent Kernel ( NTK ). NTK is a kernel that describes the gradient descent training of deep neural networks. In this paper, they show that the spectrum of the NTK kernel simplifies significantly and becomes “ weakly data - dependent ”. They also show that there are large regions of hyperparameter space where networks can only memorize the training set in the sense they reach perfect training accuracy but completely fail to generalize outside the training data set, in contrast with several recent results. By analyzing this spectrum, they arrive at a precise characterization of trainability and a necessary condition for generalization across a range of architectures including Fully Connected Networks ( FCNs ) and Convolutional Neural Networks ( CNNs ). The authors also perform a thorough empirical investigation of these theoretical results and find excellent agreement on real datasets."
SP:d3470c35aae48bf92439a55fdb98ccf07100e567,"This paper presents Graph Convolutional Networks ( GRAPHQA ), a graph - based method to estimate the quality of protein models. The main contribution of the paper is to propose a graph based method for the identification of the structure of protein molecules based on their 3D molecular geometry. This is in contrast to the traditional hand - engineered or representation - based methods for protein identification. The proposed method is based on Graph - based representation learning, which is a type of graph neural network. The authors claim that this method has several desirable properties such as explicit modeling of both sequential and 3D structure, geometric invariance and computational efficiency. The paper presents extensive experiments that compare the current state - of - the - art for both hand - engineered and representation - learning methods, as well as carefully evaluating the individual contributions of each of the four components of the proposed method. The experimental results show that the current method performs better than previous methods in terms of both the number of folds and the overall quality of the models."
SP:5188280131b58a35d3deda126a0754aea8fa6e58,"This paper revisits and extends the literature on the loss function of linear neural networks for the determinantal variety, where the functional space is either a set of linear maps with bounded rank or a linear map of input to output. The authors make a distinction between pure critical points, which depend only on the function space, and spurious critical points which arise from the parameterization of this space by the network ’s weights. For this type of network, they use geometric properties of determinantial varieties to derive new results on the landscape of linear networks with different loss functions and different parameterizations. The analysis clearly illustrates that the absence of “ bad local minima ” in the loss landscape of the linear networks is due to two factors. First, smooth convex losses may lead to landscapes with many bad minima. Second, deep networks with “ no bottlenecks ” have no bad local Minima for arbitrary smooth loss functions. In particular, the paper shows that deep networks without “ deep layer ” has the same property as shallow linear networks.   The authors also make a connection between the results obtained in the previous work of Lu & Kawaguchi ( 2017 ) and Zhang ( 2019 ), and the results of Bah et al. ( 2019, 2019 )."
SP:ee71597ceab23eb4db1d6608f15f80ad51f7ff6d,"This paper proposes a general framework SEED ( Sampling, Encoding, and Embedding Distributions ) for inductive and unsupervised representation learning on graph structured objects. The main idea is to use graph similarity evaluation as a surrogate for learning the representation for a given input graph by sampling a number of subgraphs whose reconstruction errors could be efficiently evaluated, encodes the subgraph samples into a collection of sub - graph vectors, and employs the embedding of the sub graph vector distribution as the output vector representation for the input graph. The authors claim that the proposed SEED framework is able to achieve up to 10 % improvement compared with competitive baseline methods in terms of the quality of representations obtained when a reasonable number of small subgraph are sampled. By adjusting sample size, the authors are able to make trade - off between effectiveness and efficiency."
SP:d9406fdf0a180a5efc6f15ba8739524665f0f9d2,"This paper studies the problem of counterfactual regret minimization ( CFR ) in zero - sum combinatorial games. It shows that the regret of the proposed Lazy - CFR algorithm is almost the same as that of the vanilla CFR algorithm. The main contribution of the paper is that it proposes a lazy update strategy to avoid traversing the whole game tree in each round, which is time consuming in large - scale games. The paper empirically evaluates the proposed lazy - CFR on four benchmark games : MC - CFR, MC - CFR+ ( Bowling et al. 2017 ), LazyCFR and Lazy-CFR+ on the standard benchmarks, Leduc Hold’em ( Southey et. al., 2005 ) and heads - up flop hold ’em poker ( Brown et.al., 2019 ). The results show that the proposed lazier version of the lazy CFR algorithm ( Lazy CFR ) is provably faster than the vanilla vanilla CFR and only needs to visit a small portion of the game tree compared to CFR, which traverses the whole tree in a round. The same idea is then applied to CFR+ and it is shown that it works well in practice as suggested by theory."
SP:023aa3dca1cf7992b22993a7088e8a74c92bb47e,"This paper proposes a novel method to learn transferable features by minimizing the feature distribution discrepancy between the source and target domains in Unsupervised Domain Adaptation ( UDA ). The authors use explicit feature distribution modeling to model the deep features from each domain as Gaussian mixture distributions. The proposed method, Distribution Matching Prototypical Network ( DMPN ), trains a deep network over the source domain data to generate features following a gaussian mixture distribution. The network is then used to assign pseudo labels to the unlabeled target data to learn both discriminative and domain invariant features. The hyper - parameter sensitivity analysis shows that the approach is robust w.r.t hyper -parameter changes. Extensive experiments on Digits Image transfer tasks and synthetic - to - real image transfer task demonstrate that the proposed method can provide superior results than state - of - the - art approaches."
SP:40be996e8bb86e887077b762b87c7c34a786ac98,"This paper proposes a new method for conditioning conditional conditional normalizing normalizing flows ( CNFs ), named InfoCNF, that partitions the latent space into a class - specific supervised code and an unsupervised code, and uses gating networks to learn the error tolerances of its ordinary differential equation ( ODE ). The latent space is partitioned into two classes : one for labeled code and one for high dimensional latent code. The labeled code is shared among all classes for efficient use of labeled information. The highdimensional latent code is generated by the conditional CNF, which needs to be of the same size as the input data. The authors propose a partitioning strategy ( slightly ) that slightly increases the number of function evaluations ( NFEs ). They also propose a gating network that uses the error tolerance learned by ODE to help reduce the error of the learned ODE. Experiments on CIFAR-10 show that the proposed method achieves similar performance as the one proposed by the authors without gating, but reduces the NFE by more than 21 % when trained with large batches, and achieves a reduction of 5 % in test error and a small improvement in NLLs.   The authors also confirm the benefits of their gating approach on unconditional CNF and observe that it helps reduce the NNEs by 15 % while preserving the NLL by 15%."
SP:97764e3393216106ff2ac3f550845acf4636119f,"This paper studies the convergence of the TD learning algorithm in the lazy training regime for nonlinear neural networks trained with the Temporal - Difference ( TD ) learning algorithm. The authors prove convergence of TD learning in both the under - and over - parametrized regime to local and global minima, respectively, of a natural, weighted error function ( the projected TD error ), and illustrate such convergence through numerical examples. To obtain the result summarized above, the authors adapt the contraction conditions developed in the framework of linear function approximations to a nonlinear, differential geometric setting. Furthermore, they extend some existing results on the convergence in the convergence regime of nonlinear models trained by gradient descent in the supervised learning framework to the world of reinforcement learning. This requires a generalization to non - gradient fields such as the ones encountered in the gradient flow setting. This paper proves that TD learning for value approximation in reinforcement learning, when the model is a non - linear function of its parameters, is convergent ( convergent, i.e., with probability one )."
SP:c518e4030f12b0f59ad1d7c0fc0ebd313c68ef95,"This paper presents a method to train an agent that can generate observations that can help predict whether a hypothesis about the dynamics of the world is true or false. The agent is trained end - to - end with the reward function, where it is observed that agents trained with no reward fail to learn how to solve the problem of hypothesis verification. The method is based on the fact that the majority of hypotheses can be formulated as triplets ( pre - condition, action sequence, post - condition ), which can then be fine - tuned to verify more general hypotheses.   The main contribution of this paper is to propose a method for training an agent to generate hypotheses about the world by generating and testing hypotheses about its environment. The main idea is to train the agent as follows :   1. In the training phase, the agent is given a hypothesis and given an environment, it is asked to generate observations which can be used to test the hypothesis. 2. In order to generate the observations, it computes a set of probability measures of the environment. These measures are then used to predict the probability of the observed observations. 3. Then the agent takes an action based on these probabilities, and is rewarded based on whether it is better than the observed observation. The proposed method is shown to outperform other methods for hypothesis verification, and it is shown that it is able to generate more accurate observations than other methods."
SP:6fa2f842b1bc993ed8024a3ce13dbd91529c61be,"This paper studies the feasibility of approximate reasoning in a fixed dimensional latent space using neural networks. Specifically, the authors train a neural network to map mathematical formulas into a latent space of fixed dimension, and measure the embedding quality of the resulting formulas with respect to their predicted embedding embeddings. The authors also train neural networks to predict the latent representation of the formula generated by the resulting formula. The neural networks are trained to be able to make non - trivial predictions about the rewrite - success of statements. The experiments show that graph neural networks can not perform the reasoning in the latent space, even when they are trained for several steps purely in latent space.   The authors claim that this is an important question that needs to be addressed in the literature."
SP:a77ab500a5e7d4ea8430871d1e603941e92974fd,"This paper proposes a new method for learning depth estimation from very sparse ground truth measurements. The method is based on a global neural network architecture that learns to estimate depth using only a few pixels of ground truth per image. The network is trained using a combination of supervised and unsupervised learning. The supervised part of the training is done by training the network on a large set of labeled data and a much smaller set of unlabeled data, which is used to train the global network. The authors compare the proposed method with three other methods : deep neural networks, classic geometry methods, and standard convolutional networks. In the sparse data regime, the proposed approach outperforms all baselines thanks to its ability to train with sparse labels and its robustness to variations in the camera parameters."
SP:2afba5e24478da4e9d493887c7cf00e288cc0deb,"This paper proposes a new method to solve the problem of predicting hash tokens in a Bloom filter. The proposed method, called Superbloom, is similar to the ECOC method in that it uses hash functions to map each hash token to multiple hash tokens, similarly to a binary classification problem. However, unlike ECOC, SuperBloom does not reduce the problem all the way down to binary predictions. The key observation is that it is important to use a multi - layer Transformer for Bloom filter digests to remove ambiguity in the hashed input. The authors argue that this provides an alternative method to solving problems with large vocabulary size. They also extend the idea of word pieces in natural language models to machine learning tasks on opaque ids."
SP:745dd86d7f7bba79a02d27922003b764b620f83e,"This paper proposes a learning based agglomerative clustering framework for discovering 3D parts for objects in unseen categories. The key idea is to restrict the local context for extracting part - level features, which encourages the generalizability to unseen categories without seeing any annotated samples. The proposed method is applied to PartNet, a fine - grained 3D part dataset. It is shown that it can transfer knowledge of parts learned from 3 training categories to 21 unseen testing categories. It also shows that the method can achieve the state - of - the - art performance against four shape segmentation baselines."
SP:868fc6df740b04963442d5abcfe2f4845585cfc8,"This paper proposes a method for editing neural networks to learn a particular transformation in a latent space. The method is based on the fact that it is not possible to obtain a pair of source and target datasets that can be used to train a generative model on a specific set of data without also training on a second source dataset that is not the target dataset. To overcome this problem, the authors propose a technique called neuron editing that learns how to edit a particular neuron in a particular space by defining an editing transformation on those neurons and generate transformed data by performing the transformation in the trained space.   The method has the advantage of being generally applicable to a wide variety of data domains, and applications, and can be demonstrated on image transformations and then move to two main applications in biology : removal of batch artifacts representing unwanted noise and modeling the effect of drug treatments to predict synergy between drugs. In the following section, they detail the neuron editing method to solve the extrapolation problem by trying to perform natural image domain transfer on the canonical CelebA dataset ( Liu et al., 2018 ), and predicting the combined effects of multiple drug treatments ( binatorial drug effects )."
SP:6dee6932e64fe47bb44dd42fc242fa9d89b8d89c,"This paper proposes a meta - learning method for initializing neural networks for few - shot and many - shot image segmentation. The authors propose an extension of FOMAML ( FOM - L and Reptile - L ) to image segmentations to address the problem of generalization to many - shots settings. The proposed method is based on first - order initialization of initializations for deep neural networks that must produce dense, structured predictions given an arbitrary amount of training data for a new task.   The authors show that the proposed method outperforms the state - of - the - art methods on the FSS - 1000 dataset, while only requiring one forward pass through a single model at a time. They also show empirically that a fixed update routine is needed for adapting to new tasks and that meta - learned initializations are sensitive to changes in the update routine ’s hyperparameters ( Figure 2 ). Finally, they address the question of competitive performance by showing that our proposed methods are competitive with ImageNet - trained initializations with FP - k dataset."
SP:ec6f390f6d45fb79c33ae5d9c8a24cadb96fbd60,"This paper proposes a new method for semi - supervised few - shot learning ( SS - FSL ) based on Prototypical Networks ( PN ). The proposed method is based on the idea of “ random walk ”, which is a graph - based method for learning representations that are compact and well - separated between classes. This paper proposes to combine this idea with PN, and proposes to train a prototypical network on top of PN. The network is trained with a fraction of the labeled data, and is compared with a fully supervised network trained on all the labels. The experimental results show that the proposed method outperforms PN trained with all the labelled data and fully supervised networks trained on 100% of the labels, even outperforming it in the 1 - shot mini -agenet case with 50.89% to 49.4% accuracy. The authors also show that their model is resistant to distractors, unlabeled data that does not belong to any of the training classes, and hence reflects robustness to labelled/unlabelled class distribution mismatch."
SP:d12e687bd2ee9fa60554312e644bb0a6487974f1,"This paper proposes Contrastive Sensor Fusion ( CSF ), a semi - supervised learning method for learning representations of multi - sensor combinations. CSF is trained on a dataset of over 20 million scenes, each consisting of four bands for each of three different sensors. The encoder is trained without fine - tuning or stacking additional models. The experimental results show that CSF performs better than other methods on a number of classification tasks. The authors claim that the performance of CSF improves as more sensors are fused."
SP:4d8e054f07006b4f896721b5c24da805727d2c22,"This paper compares the fine - tuning - based method for neural network pruning with weight - rewinding - based methods for retraining. Fine - tuning trains the unpruned weights from their final trained values using a small fixed learning rate, while weight re - winding trains the weights from the original values using the same learning rate schedule. The authors compare the performance of the proposed re - winding method against fine - tuning and the state - of - the - art method by Ortiz et al. ( 2020 ). They show that re - winds outperform fine -tuning in terms of accuracy while fine - Tuning outperforms the proposed method. They also show that the proposed methods are more stable than fine - tuned methods."
SP:3bb1c79f9482e09828eda45fbb2e654f37219365,"This paper studies the relationship between output margin and generalization in deep neural networks. It is well known that a large output margin implies good generalization, but it is less clear for deep networks. This paper proposes a new metric, the all - layer margin, to study this relationship. The margin is defined as the difference between the output margin of each layer of a neural network and the output of all the layers of the network after that layer. The output margin depends on the depth of the layer and the Jacobian of the neural network, and the margin of the hidden layer is the sum of the average Jacobian and the norm of hidden layer norms. The authors show that the margin for deep neural nets is bounded by an all - Layer Margin that is independent of the depth and Jacobian. They also show that training neural nets with a larger margin leads to better generalization bounds.   The main contributions of this paper are as follows :   1 ) The authors propose a training algorithm to encourage neural nets to expand the margin in order to improve the generalization of neural nets. This is done by training the network on the CIFAR-10 dataset and the WideResNet dataset. The algorithm is trained on both clean and adversarially robust classification. The results show that this encourages the network to expand its margin. 2 ) The proposed training algorithm is applied to two sets of experiments. The first set of experiments compares the performance of the proposed training method with that of a standard training method on the clean dataset and one on the robust dataset. It shows that the proposed method outperforms both. The second set outperforms the standard method on both the clean and the robust datasets."
SP:3d44f27468087280e85dfb1fc7291db05179fe6d,"This paper proposes a low - resource dialogue generation model based on a disentangled response decoder, which is trained with unstructured dialogues and documents. The method is based on the assumption that there are only limited training examples of grounded dialogues, and that the majority of the training data comes from ungrounded dialogues. The decoder is designed to be able to isolate parameters that depend on the knowledge - grounded dialogue generation from the entire generation model, i.e., the major part of the model can be learned from a large number of un - grounded dialogued and un - structured documents, while the remaining small parameters can be well fitted using the limited training example set. The proposed method is evaluated on two benchmarks, where it is shown to achieve state - of - the - art performance with only 1 - 8 training data."
SP:9b555f7fe743f5effdbdc8701ed519ce3159c4b0,"This paper proposes a new model architecture for non - parallel bilingual data, mirror - generative neural machine translation models ( MGNMT ). The main idea is to combine the source to target translation model, the target to source translation model and two language models, and two latent semantic space models, which are learned jointly by mirroring the source and target translation. The paper conducts extensive experiments on a variety of language pairs and scenarios, including resource - rich and low - resource situations. They show that the proposed model outperforms existing approaches in most of the cases. They also demonstrate that the architecture is architecture - free which can be applied to any neural sequence model such as Transformer and RNN."
SP:d7a530a0ec4112095a58cef4cda9646f8ca6449d,"This paper studies the relationship between entropy and performance of maximum entropy reinforcement learning ( MRL ) algorithms. In particular, this paper focuses on the case of Soft Actor Critic ( SAC ), a popular algorithm for maximum entropy RL. The authors argue that the bounded nature of the action spaces in SAC is the primary reason why it is able to achieve superior performance compared to other MRL algorithms. Based on this insight, the authors propose two ways to improve the performance of SAC. First, they propose a streamlined algorithm that does not employ entropy maximization but nevertheless matches the sampling efficiency and robustness performance of the SAC algorithm for the Mujoco benchmarks. Second, they combine their streamlined algorithms with a simple non - uniform sampling scheme to achieve state - of - the - art performance for the benchmarks.   The contributions of this paper are thus thus threefold :   1. Uncovering the primary contribution of the entropy term of the maximum entropy algorithms when the environments have bounded action spaces. 2. Proposing a non - uniform sampling method for selecting transitions from the replay buffer during training. 3. Providing source code for reproducibility analysis."
SP:545e8da553fcb47d84eaa044d8a4947d3cd3230e,"This paper presents a proof - of - concept attack on two neural net based copyright detection systems, namely AudioTag and YouTube's Content ID system. The authors first describe a well - known music identification method and implement this system in the form of a neural net. They then attack this system using simple gradient methods using adversarial music created this way successfully fooling both the AudioTag copyright detector and YouTube ’s ContentID system.   The authors then highlight the importance of hardening the detection systems to adversarial attacks and highlight some of the vulnerabilities of these systems. The paper is well - written and well - motivated. It is well presented. However, there are a few flaws in the paper that make it difficult to understand. The main weakness of the paper is that it does not clearly state the source code of the neural net used for the attacks. This is not clear to me how the authors justify the use of this neural net as a source code for the adversarial attack. I would like to see the authors clearly state what kind of source code is used in the attacks and the reason for the vulnerability of these neural net systems. Also, the authors should clearly state that the method they use is not the only one that is used for adversarial defense."
SP:b511822850da3bf1079a36ed6f5ad4db80fbc424,"This paper proposes a decomposition method for understanding the similarity between two input images by decomposing the final activation maps of deep metric learning. The decomposition is based on the fact that the activation map of a neural network is only a linear combination of the features of the input image and the feature maps of the two neighboring images. Instead, the decomposition generates a point - to - point activation map between the two images, which is then used for cross - view pattern discovery and interactive retrieval. The proposed decomposition can be directly applied to a large range of metric learning applications and provides valuable information about understanding the model. Experiments are conducted on CIFAR-10 and Cifar-100 datasets to validate the effectiveness of the decomposable activation maps."
SP:67bf71219fe6bedec5f5525200e734638e4a6ca2,"This paper proposes a new algorithm combining model - based planning ( AOP ) and model - free planning ( MLP ) for lifelong reinforcement learning. The goal of the paper is to address the challenge of learning in an online lifelong learning setting where the underlying dynamics of the environment may change over the course of a lifetime, and traditional model free policy learning methods struggle to adapt quickly to these changes. The main contributions are the following :   1. A new algorithm, AOP, that combines model based planning and model free planning.   2. Empirical results show that the proposed algorithm AOP is able to call upon more extensive planning only when necessary, leading to reduced computation times. 3. Experiments show that AOP can adapt more adaptively to novel situations, adapting behaviors and policies effectively in the face of unpredictable changes."
SP:11159cb878a436a5d4fc6edb4132f2cc3c1b3f72,"This paper proposes a new attention mechanism, TVMAX, to replace the traditional softmax attention mechanism in image captioning models. The attention mechanism is replaced by two alternative sparsity - promoting transformations, sparsemax and total - variation sparse attention ( TVMAX ). The paper presents results in the Microsoft COCO and Flickr30k datasets, obtaining gains in comparison to softmax. The experimental results show that TVMAX outperforms the other compared attention mechanisms in terms of human - rated caption quality and attention relevance."
SP:fb0c3ce3db6ad674ddc615bdc6203cdcbe42c804,"This paper proposes a generative model to predict the evolution of dynamic graphs, i.e., the topology of graphs that are generated from structured data. This is a challenging problem since most real - world networks are dynamic graphs and topology tends to change over time. This paper proposes to use a graph neural network along with a recurrent architecture to capture the temporal evolution patterns of dynamic graph generation. The authors propose to use the following approaches :    1. Generative model : - First, a graph topology model is constructed from the structured data and a recurrent graph network. - Then, the authors use the predicted topology from the graph model to construct a graph instance that corresponds to that topology. - The authors then apply the generated graph instance to several datasets to test the effectiveness of the proposed model.   2. Experimental results are provided showing that the proposed method outperforms the baselines on three tasks : - Generative graph generation, - Graph generation of structured data, - Predictive graph generation of dynamic data, and - Prediction of dynamics of evolving graph dynamics."
SP:ff722957a1765c0568426ed88dd910a6b74054ef,"This paper proposes Generative Imputation and Stochastic Prediction ( GI ) for imputing missing features and estimating the distribution of target assignments given incomplete data. The method is based on a simple and effective generator network to generate imputations that a discriminator network is tasked to distinguish. Following this, a predictor network is trained using the imputed samples from the generator to capture the classification uncertainties and make predictions accordingly. The proposed method is evaluated on CIFAR-10 image dataset as well as three real - world tabular classification datasets, under different missingness rates and structures. The experimental results show the effectiveness of the proposed method in generating generating imputations and generating class predictions."
SP:c051b0fe779d9e4131016970b7ba469b596f3009,"This paper proposes a new method for estimating the importance of long - horizon off - policy estimation for long - range problems. The main idea is to use Reproducing Kernel Hilbert Spaces ( RKHSs ), a method developed by Liu et al. ( 2018 ) to estimate the fixed point of a certain operator ( i.e., the stationary distribution of a known behavior policy ) that can be drawn from stationary data sources ( e.g., high - fidelity simulators ).   The main contribution of this paper is to develop a new estimator that computes importance ratios of stationary distributions, without knowledge of how the off -policy data are collected. In particular, the authors propose a novel approach that eliminates the limitations of generalization generalization that eliminates such limitations such limitations. To this end, they formulate the problem as solving for the fixed starting point of an operator that can draw data from stationary distribution. The authors then propose to use the following methods :   1. An importance - sampling method based on a weighted average of a fixed number of importance - samplings per horizon, where each horizon is sampled from a fixed distribution. 2. A weighted sum of all importance - sampled horizon horizon horizon horizons, where the average of the horizon horizon variances per horizon is computed using a weighted sum over the horizon varials. 3. A distribution over horizon varients that is independent of the distribution of stationary data. 4. A set of stochastic gradient measures that measure the difference between the average ratio of the average and the expected value of each horizon horizon measured by each horizon. These measures are then used to compute the expected cost of the estimated importance of a horizon horizon. The method is applied to two datasets, one of which is based on the average value of stationary distribution varials and the other on the expected costs of stationary and non - stationary distributions. The results show that the proposed method outperforms other importance sampling methods on average cost and expected cost."
SP:065c900843011a71b70ed35357a2f71fe83872a7,"This paper introduces Gaussian Mixture Modeling Model ( GMM ), a framework that allows to define a dataset containing K different distribution modes. In a traditional GMM, it is straightforward to compute the conditional probability p(x|k|k, \� ) which describes the distribution index corresponding to the data, and computing the responsibility p(k|x, θ ). In this paper, the authors use the Generative Adversarial Network ( GAN ) framework to compute these probabilities at the data’s latent space z instead of x, where z is the corresponding latent representation of the data. The authors also introduce an additional classification network which is trained with the GAN in an “ end - to - end ” fashion. These techniques allow the authors to discover interesting properties of an unsupervised dataset, including dataset segments as well as generating new “ outdistribution ” data by smooth linear interpolation across any combinations of the modes."
SP:2da1608209058d214f8671062cc9eb0833ba4831,"This paper proposes a method to train large neural networks with significantly improved accuracy and lower computational cost. It achieves this by gating the deep - learning architecture on a fine - grained - level. Individual convolutional maps are turned on/off conditionally on features in the network. To achieve this, the authors introduce a new residual block architecture that gates convolutionian channels in a fine-grained manner. They also introduce a generally applicable tool batch - shaping that matches the marginal aggregate posteriors of features in a neural network to a pre - specified prior distribution. They use this novel technique to force gates to be more conditional on the data. They show that the resulting networks automatically learn to use more features for difficult examples and fewer features for simple examples. They compare state - of - the - art results compared to other conditional computing architectures such as Convnet - AIG (Veit & Belongie, 2018 ), SkipNet (Wang et al. 2018a ), Dynamic Channel Pruning(Gao et al., 2018, and soft - guided adaptively - dropped neural network )."
SP:f90e9f0eb53f92601bdfa3f7bf86f71d037aad30,"This paper proposes a probabilistic importance inference approach for pruning deep neural networks ( DNNs ). Specifically, the authors propose to test the significance of a connection in a DNN to the DNN ’s outputs using a nonparemetric scoring test and keep only those significant ones. Experimental results show that the proposed approach achieves better lossless compression rates than existing techniques such as PCII.   The main contributions of this paper are as follows :   1. This paper introduces the PCII method and algorithm. The main idea of the algorithm is to use a neural network connection between the input of a given DNN and the output of another DNN in order to perform pruning. Theoretical properties are established for using the method to infer dependence between a network connection and DNN output. Theorem 4.1 establishes theoretical properties for using    PCII algorithm to estimate the importance of the connection between DNN connection and the outputs of the other DNN. 2. The authors provide experimental results to demonstrate the superiority of the proposed method over PCII and other existing pruning methods. 3. The algorithm is tested on a small set of DNN examples and compared with PCII, a standard pruning method. Results indicate that the method outperforms the other methods in terms of compression. 4. The method is applied to several DNN pruning tasks, and the results indicate that it achieves better results than other pruning techniques."
SP:64cbbb6a2f6847ef71cd5a23ba3e4cc5c815a56e,"This paper proposes a method for hierarchical reinforcement learning ( RL ) based on iterative convolutional sparse coding and compression. The method is based on the idea that the problem of RL with long - range dependencies and nested hierarchical structure is equivalent to finding a minimum - length program that can generate the data — that is, finding a program with a minimum of complexity $ \mathcal{K}$ that can be used to approximate the minimum - complexity code of a sequence of action trajectories. To this end, the authors propose a method that iteratively compresses action sequences to learn nested behavioral hierarchies of arbitrary depth, with actions of arbitrary length. The learned temporally extended actions provide new action primitives that can participate in deeper hierarchies as the agent learns. The authors demonstrate the relevance of this approach for tasks with non - trivial hierarchical structure and show that the approach can be use to accelerate learning in recursively more complex tasks through transfer learning."
SP:e1ccfb3a684aef8a0fb36194eb16af1667811e81,"The paper proposes a new generative model, Hierarchical Bayes Autoencoder ( HBAE ), for generating complex image sets. The model is based on variational autoencoders ( VAE ) and uses a multi - modal energy based decoder ( EBM ) instead of the unimodal distribution used in VAE. The EBM decoder is trained using adversarial approximation where a conditional generator is trained to match the distribution of the input to the decoder. The decoder can also be trained using variational inference to recover latent codes conditioned on inputs.   The paper proposes to extend the training procedure to generate sets of inputs, by inferring a latent code for a set of examples, and sampling set members through the multimodal decoder and using the EBM to generate unconditional samples. In both single image and set - shot cases, the model is able to generate plausible variations consistent with the input data and generates realistic unconditional samples, and is capable of few - shot classification of sets. To the best of our knowledge, this paper is the first model to model image sets, and it is an extension of the HBAe formulation to model set of inputs."
SP:1130a391afa30d1e0fddadedd2a3aaa70a4cb751,"This paper studies cross - normalization in deep off - policy reinforcement learning ( TD ). The authors propose a new normalization method based on cross - distribution normalization ( C - N ), which is an extension of batch normalization that re - centers data for two different distributions, as present in TD. They show that the proposed normalization improves over the state of the art in DDPG and TD3 across a range of MuJoCo benchmark tasks. They also show that well - designed normalization can improve stability and remove the necessity of target networks in TD algorithms."
SP:f9cafaa5131176290fa069e6d24046c079cd9eea,"This paper presents a method for learning features that are unbiased to bias and that are uncorrelated with the bias or confounder variables. The authors propose a method based on the adversarial training strategy to learn discriminative features unbiased and invariant to the confounders. The proposed method is based on a new adversarial loss function that encourages a vanished correlation between the learned and learned features. The method is applied to a synthetic data, medical images, and a gender classification ( Gender Shades Pilot Parliaments Benchmark ) dataset. It is shown that the learned features by the method not only result in superior prediction performance but also are not biased with bias or confounding variables."
SP:783049ff463edd1283c058c6106a3e1f9a033df4,"This paper proposes a new transformer architecture, Group - Transformer, for character - level language modeling. The architecture is based on the group strategy, where each group in a transformer is represented by a subgroup, and the subgroup is computed using a group embedding operator. The authors compare the proposed architecture with LSTM - based and transformer - based models on two benchmark datasets, enwik8 and text8, and show that the proposed approach outperforms them both. Further analysis is performed to identify the contributions of each of the proposed modules in detail."
SP:946c26d371297c88d0ac246257104099b4585edc,"This paper presents a method for training generative models with hierarchical - latent hierarchical - variable structures based on Optimal Transport ( OT ), which is an alternative to Variational Autoencoders ( VAE ) for training such models. The authors argue that VAE does not leverage the deep latent structure of the generative model to its full potential, and propose a method called STACKEDWAE to train such models with a hierarchical structure based on the Maximum Mean Discrepancy ( MMD ) regulariser. The MMD regulariser is used to regularise the latent with the maximum mean deviation between distributions, which allows for easier convergence between distributions.   The authors first show that the method is able to train models with the hierarchical structure without the need for highly bespoke models and inference networks. Then, they show that STACKedWAE performs significantly better when training hierarchical - Latent models than the original WAE framework, and that in - so - doing, it is more effective than the WAE baseline."
SP:309b47441d227ffa33f96f9f16f2addc607e5bb0,"This paper presents a simple autoregressive video generation model based on a three - dimensional self - attention mechanism for generating continuations of high fidelity and realism. The authors compare the proposed model with state - of - the - art video generation models based on neural network architectures, latent variable models, adversarial training and a range of other methods to generate continuations outside of narrow domains and often struggle with fidelity and fidelity. They show that their proposed model is conceptually simple and achieves competitive results across multiple metrics on popular benchmark datasets such as CIFAR-10, Cifar-100, and the Kinetics dataset. They also present results from training their models on Kinetics, a large scale action recognition dataset comprised of YouTube videos exhibiting phenomena such as camera movement, complex object interactions, and diverse human movement. They hope that their results, which include realistic continuations encourage further research on comparatively complex and large scale datasets like Kinetics such as Kinetics to further improve the quality of video generation."
SP:ad8fcdbc47a50dd2bf58aba2bc6cfe199e84dd4d,"This paper proposes an adversarial generative model for zero - shot classification of ICD codes, where the goal is to improve the prediction on codes that have no labeled data without compromising the performance on seen codes. The proposed method, called AGMC - HTS, is based on the hierarchical ICD code structure and a novel pseudo cycle generation architecture to guarantee the semantic consistency between the synthetic and real features. Experiments on the MIMIC - III dataset show that the proposed method can improve the F1 score from nearly 0 to 20.91% for the zero - shots codes and the AUC score by 3% ( absolute improvement ) from previous state of the art."
SP:3ce82ae297e5759ab957babe9927062e7a71b0ba,"This paper proposes a method to improve the sample efficiency of model - free RL by simultaneously learning embeddings of states and action sequences. The goal is to learn a forward prediction objective that captures the structure of the environment’s dynamics, enabling efficient policy learning. The paper considers the problem of continuous control from pixel observations in a high - dimensional continuous environment. The authors propose a method that combines state and action embedding to learn both a state representation and an action representation. The state representation is learned by sampling from the learned state representation, and the action representation is learnt by sampling action sequences from learned state embedding. The method is applied to continuous control in the TD3 continuous environment and compared with the state - of - the - art methods in two environments. The results show that TD3 with TD2 and TD3 without TD2 can achieve better performance than TD2 with TD3 on a simple environment, and better performance on a more complex environment."
SP:11ce1616e721340eea9e80dad7460c77355ac7d1,"This paper proposes a meta - learning framework, called ARML, that automatically constructs the meta - knowledge graph for each task based on the existing meta - learner ’s knowledge from previous tasks and the task - specific meta - learners ’ knowledge from the previous tasks. The authors claim that the proposed framework addresses the challenge of task heterogeneity by a learned meta-knowledge graph that captures the relationship among tasks and improves the interpretability of the algorithms.   The proposed framework is based on two main components : ( 1 ) the automatically constructed meta - graph to facilitate learning a new task, and ( 2 ) the task specific graph that is trained to capture the relationship between the tasks. In the experiments, the authors conducted extensive experiments on 2D toy regression and few - shot image classification and the results demonstrate the superiority of the proposed ARML over state - of - the - art meta-learning algorithms."
SP:37c209cd1c628b5c2f2b282fbeaf4bbf437c7670,"This paper presents a novel method for controlling the controllable language generation capability of large transformer - based language models ( LM ). The authors show that controlling attributes of the generated language ( e.g. switching topic or sentiment ) is difficult without modifying the model architecture or fine - tuning on attribute - specific data. They propose a simple alternative : the Plug and Play Language Model ( PPLM ), which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario, the attribute models are simple classifiers consisting of a user - specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM, and the LM is used to generate text. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM ’s hidden activations and thus guide the generation of the text from the learned layer. They show that the approach can be used to detoxify instances where generation of toxic content is likely by following the negative gradient of a model trained to detect toxicity ( Section 4.4 ). They also show how PPLMs are flexible in that any combination of differentiable attribute models may be used in steer text generation, which will allow for diverse and creative applications."
SP:12d0980bfea2de880905a0b87b40856969bb1c58,"This paper proposes a new unsupervised learning approach for learning data representations with unlabeled data. The proposed approach is based on a denoising autoencoder, where the noisy input data is generated by corrupting clean data in the gradient domain with the objective of learning a Laplacian pyramid representation of the input data. This can be naturally generalized to span multiple scales with the proposed approach. Experiments on several visual benchmarks demonstrate that better representations can be learned with this proposed approach compared to its counterpart with single - scale corruption compared to the one proposed by the authors. The authors also demonstrate that the learned representations perform well when transferring to other vision tasks.   The main contributions of the paper are as follows :   - A novel approach to learn data representations for unlabeling data with a novel method to learn a pyramid based learning algorithm. The method proposed in this paper can be easily generalized to other domains and is shown to perform well on several benchmarks. - The proposed method is superior to the conventional DAE and achieves state - of - the - art performance on the benchmarks."
SP:12afc1b259e51a31cbeb72366d2b93fbee1aafaa,"This paper tackles the problem of under - sensitivity in natural language inference. The authors propose a novel method to verify the robustness of natural language models by using Interval bound propagation ( IBP ) approach. This approach is based on the decomposable attention mechanism of the NeurIPS model. They show that IBP can efficiently prove, given a model, whether a particular sample is free from the under - sensitive problem using standard training methods. They compare different training methods to address the problem, and show that IBP training leads to a significantly improved verified accuracy compared to standard training."
SP:14257af9fe83522c6e5b5d6b0d68945b944e30fb,"This paper presents a novel method for solving the continuous - state - action - transition problem in deep reinforcement learning ( DQN ). The authors propose a graph - based method based on a replay memory, which they call QGRAPH. They show that the Q - value for each transition in their simplified MDP is a lower bound of the lower bound for the same transition in the original continuous Q - learning problem. By using these lower bounds in TD learning, the authors show that their method is less prone to soft divergence and exhibits increased sample efficiency while being more robust to hyperparameters. They also show that this method preserves information from transitions that have already been overwritten in the replay memory.    The authors first propose to analyze the structure of the replay - memory on the data graph and show that specific types of structures are linked to soft divergences. Then, they construct a simple Markov Decision Process ( MDP ) for which exact Q - values can be computed efficiently as more data comes in – resulting in a resulting a Q - resulting in Q - Graph. This MDP can then be used to solve the continuous state and action - space continuous Q-learning problem. In particular, the transitions are represented in a data graph, and the authors link these transitions in their MDP to a specific subgraph. They then show that a subgraph with a favorable structure is more likely to be a good candidate for soft divergence. They derive their method from these insights and show it prevents many cases of soft divergence from occurring."
SP:c92c97e47d8b218dfd009bbf61f5b3547b395f91,"This paper studies the effect of the embedding complexity on generalization to the target domain of unsupervised domain adaptation ( UDA ). Specifically, the authors study the following :   1. The impact of the layer - dependent complexity of the neural networks used to learn the source and target domain embeddings.   2. The upper bound on the risk associated with learning the source domain embedding. 3. The sensitivity sensitivity of the network to layer - dependant networks. 4. The generalization performance of the proposed algorithm."
SP:f3f3c6fbae757836551b3f1ee54a7d1e040132b8,"This paper addresses the problem of generalization error bounds for stochastic gradient Langevin dynamics ( SGLD ) and other noisy gradient methods for learning general non - convex objectives, which has attracted significant attention in recent years. The authors develop a new framework, termed Bayes - Stability, for proving algorithm - dependent generalisation error bounds. The framework combines ideas from both the PAC - Bayesian theory and the notion of algorithmic stability. The main contribution of the paper is to develop a data - dependent bounding framework that combines the ideas of Bayes-Stability and the Bayes method. The bounds obtained in this paper can distinguish randomly labelled data from normal data, which provides an explanation to the intriguing phenomena observed in Zhang et al. ( 2017a ). They also study the setting where the total loss is the sum of a bounded loss and an additional `2 regularization term. They obtain new bounds for the continuous Langevin dynamic in this setting by developing a new Log - Sobolev inequality for the parameter distribution at any time."
SP:a82fcd1d3196ddf078cfe8f4bc6f445d9d2bdc11,"This paper investigates the role of the hippocampus in continual learning of spatial navigation strategies in the context of continual reinforcement learning. The authors first analyse the population - level activity of the hippocampal CA1 neurons in the hippocampus with demixed principal component analysis ( dPCA ). They find representations that are directly related to how the neural population encodes not only multiple tasks, but also reward signals, such as reward prediction errors. They also compare this hippocampal features with standard reinforcement learning algorithms, highlighting similarities and differences. Finally, they demonstrate that a standard deep reinforcement learning model achieves similar average performance when compared to animal learning, but fails to mimic animals during task switching."
SP:51acf1f8108683dce543a1fb4a61fbd593f9b4cc,"In this paper, the authors introduce TPO, a tree search based policy optimization method for continuous environments in the continuous action space setting of the Humanoid benchmark [ 9 ]. The authors propose a limiting tree search branching factor by drawing only a few action samples from the policy distribution and defining a new loss function based on the trajectories ’ mean and standard deviations. In the experiments, TPO significantly improves the policy on nearly all the environments. For example, in complex environments such as Humanoid with a 17 dimensional action space, they achieve a 2.5x improvement over the baseline algorithm 3.3. However, they observed that bootstrapping tree search with a pre - trained policy allows us to achieve high results with a low MCTS branching factor and few simulations."
SP:1ce3bc4d31712886f7dcada5b5ae67c3c376819a,"This paper studies the problem of finding good training tickets for deep neural networks trained alone on ImageNet. The authors claim that the lottery ticket hypothesis that deep networks trained on CIFAR-10 are able to match the performance of the full network when trained in isolation is not well supported by the empirical literature. This paper aims to answer the following open questions :   1. Can we find good winning tickets with few data samples or few labels? Can we even obtain “ good ” tickets without supervision?   2. What is the property of winning tickets that makes them so special? The authors show that winning tickets generated on the full ImageNet dataset when evaluated on the ImageNet classification task are comparable to those generated by training on a subset of the data.   3. How do we know if we can trust the generated winning tickets if we only have access to the subset of data or if we have access only to the full dataset? This paper tries to answer these questions by using a combination of self - supervised learning with and without training. The experiments show that under certain conditions, training with supervised learning can yield better results than training with only training data."
SP:dbcebe5b73486885d9f4478b258047c02f8481a2,"This paper studies the problem of “ excessive prediction undersensitivity ” in the context of neural reading comprehension ( NPC ) models. The paper proposes a novel adversarial attack on NPC models that aims to prevent them from producing the same answer to the same question as the one they are trained on when the question is adversarially generated. To do this, the authors propose a noisy attack on the semantic variations of NPC questions that are generated by the adversarial question generator. The authors then show that SQuAD2.0 and NewsQA models are vulnerable to this attack and commit a substantial fraction of errors on adversarically generated questions. They also show that robust models generalise better in a biased data setting with a train / evaluation distribution mismatch. Finally, they demonstrate that robust NPCs are less prone to overly rely on predictive cues only present in the training set."
SP:5da870060778de460c1abe91562d6f3e707efef4,"This paper proposes a model - based approach to ensuring the safety of the RL agent while making sure that the agent does not cause any unnecessary disruptions to its environment. The current approaches to this problem, such as manually constraining the agent or adding a safety penalty to the reward function, can introduce bad incentives in complex domains, as they require the agent to know all the possible unsafe scenarios an agent could encounter. The proposed approach is to learn the transition dynamics of the environment and generate a directed graph called the imaginative module. This graph encapsulates all possible trajectories that can be followed by the agent, allowing the agent efficiently to efficiently traverse through the imagined environment without ever taking any action in reality. The imagination graph is used to predict whether the current actions of the agent can cause it to end up in dangerous states in the future, and if so, how likely it is that this will happen. The method is compatible with any existing RL algorithm and any task with discrete action space. It is designed to be able to act safely while learning to solve the task. The authors experimentally validate their proposal on two gridworld environments and a self - driving car simulator, demonstrating that their approach to safety visits unsafe states significantly less frequently than a baseline, and run experiments to confirm that their proposed method improves safety throughout learning and at convergence."
SP:c2796f28fb067138303df8d424d646f4ada31558,"This paper proposes a novel method to learn finite differences inspired by physics equations, which leverages data - driven end - to - end learning to discover underlying dynamical relations between the spatial and temporal differences in given sequential observations. The authors propose a novel architecture, Physics - aware Difference Graph Networks ( PA - DGN ) to handle physics - governing observations on an unstructured grid. They demonstrate the superiority of their method over DGN in the approximation of directional derivatives and the prediction of graph signals on the synthetic data and the real - world climate observations from weather stations.   The authors conduct extensive experiments to conduct exhaustive experiments to predict climate observations and demonstrate that PA -DGN outperforms other baselines."
SP:db8ed4f4fc3967f5dd4d208d5d029730eb99e840,"This paper considers the problem of training structured neural networks with nonsmooth regularization ( e.g. `1 - norm ) and constraints (e.g. interval constraints ). The authors formulate training as a constrained nons - smooth non - convex optimization problem, and propose a convergent proximal - type stochastic gradient descent ( ProxSGD ) algorithm. They show that under properly selected learning rates, with probability 1, every limit point of the sequence generated by the proposed Prox SGD algorithm is a stationary point. To support the theoretical analysis and demonstrate the flexibility of the proposed algorithm, the authors conduct extensive numerical tests to show that the algorithm can be used to train either sparse or binary neural networks through an adequate selection of the regularization function and constraint set."
SP:2ca1f4da9faee79768764cda5d09d949cc942acc,"This paper presents an approach to train an end - to - end compression algorithm based on a non - deterministic compression method. The encoder maps an input image to a discrete code using an encoder from which the decoder can reconstruct the original image. The quantization step of this method is non - differentiable, which means that approximate methods are used to train the encoder and decoder. This paper proposes a method to circumvent this problem by mapping the input image into a distribution in continuous space from which a sample can be encoded. The expected code length is the relative entropy to the encoding distribution, i.e., it is bitsback efficient. The method is trained straight - forwardly using gradient - based optimizers. The authors test their method on the CLIC dataset and show that the rate - distortion curve of their method is competitive with the state - of - the - art on the Kodak dataset ( Eastman Kodak Company, 1999 ) on low bitrates."
SP:788fd2b6956dd69bf7752d39ea21883947128c8a,"This paper proposes a new super resolution ( SR ) method based on compressed JPG images. The proposed method is based on two components : the two - layer SR structure and cycle loss. The two - level SR structure consists of two components, one each for LR and LR images and one for C - JPG. The cycle loss consists of a linear combination of the two components. The authors also propose an integrated SR model training pipeline. The experimental results demonstrate that the proposed method can surpass traditional SR models in terms of quality."
SP:18dd92f2f55020be4f5a089b3b251327e47886f4,"This paper proposes a neural network architecture based on convolutional learning to estimate a full surface of pass probabilities from single - location labels derived from high frequency spatio - temporal data of professional soccer matches. The network is able to perform remarkably well from low - level inputs by learning a feature hierarchy that produces predictions at different sampling levels that are merged together to preserve both coarse and fine detail. The proposed deep learning architecture can be easily adapted to solve many other related problems in sports analytics. The authors demonstrate this by extending the network to estimate pass - probability estimate to learn to estimate the estimate of pass - selection likelihood while also properly estimating the complete probability map. In the following sections, the authors describe related work, provide a detailed explanation of the architecture and design considerations of the model, and present experimental results on a broad set of 208, 489 passing events from pro soccer matches from a dataset of 208, 489 matches."
SP:1ae31baf383fc520687b255d9cac14c3b040e253,"This paper proposes an inductive matrix completion method based on graph - based embedding learning. The authors show that their method, IGMC, outperforms state - of - the - art transductive baselines on 5 benchmark datasets ( MovieLens-1M, Douban-100K, CIFAR-10M, IMDB-100M, FIVA-50M, RMSE - RMSE, and PPO ) without using any side information other than the matrix to complete. They also show that IGMC is inductive - it can generalize to users / items unseen during the training ( given that their interactions exist ), and can even transfer to new tasks. They show that an IGMC model trained on the MovieLens - 100K dataset can be directly used to predict Douban movie ratings and even outperforms some baselines trained specifically on Douban. Finally, their visualization confirms that local enclosing subgraphs are indeed strong predictors of ratings.   The authors also analyze IGMC’s behavior on sparse rating matrices, showing that it is more robust than Transductive methods on sparse matrices."
SP:c5cb1b50e17a69e88d5ae28848e265215162da1e,"This paper proposes SMTP, a momentum version of the stochastic three - point method ( STP ). The authors consider the problem of unconstrained minimization of a smooth objective function in the setting where only function evaluations are possible. They show new complexity results for non - convex, convex and strongly convex functions and compare against STP, other state - of - the - art derivative - free optimization algorithms and against policy gradient methods. SMTP significantly outperforms STP and all other methods that the authors considered in numerical experiments.    The main contribution of this paper is SMTP with importance sampling which they call SMTP - with - importance. They provide analysis of this method for the setting of learning to control tasks on MuJoCo Todorov et al. ( 2012 ) environments with varying difficulty."
SP:a216cfc29937eb398ea98cb1aea3481c9aed8240,"This paper proposes Action Semantics Network ( ASN ) for multiagent coordination in multi - agent systems ( MASs ). In this setting, each agent makes individual decisions but all of them contribute globally to the system evolution. Thus, learning in MASs is difficult since each agent’s selection of actions must take place in the presence of other co - learning agents. The environmental stochasticity and uncertainties increase exponentially with the increase in the number of agents. Previous works borrow various multi agent coordination mechanisms into deep learning architecture to facilitate multi - agents coordination. However, none of them explicitly consider action semantics between agents that different actions have different influence on other agents. To this end, this paper proposes a novel network architecture, named ASN, to characterize such action semantics for more efficient multi - Agent coordination. ASN characterizes different actions ’ influence on each other agents using neural networks based on the action semantics. The main contributions of this paper can be summarized as follows : 1 ) to the best of our knowledge, this is the first work to explicitly considered action semantics and design a new network to extract it to facilitate learning inMASs ; 2 ) ASN can be easily combined with existing DRL algorithms to boost learning performance ; 3 ) experimental results on StarCraft II micromanagement and Neural MMO show that ASN significantly improves the performance of state - of - the - art DRL approaches compared with several network architectures."
SP:efaf3a440dc17e05177832083ffbc23760ed7c97,"This paper proposes a method to exploit the structure of the low - rank structure in the state - action value function, i.e., Q function, for both planning and deep RL. In particular, the authors propose to use ME ( Matrix Estimation ) techniques to leverage the low rank structure of Q function in order to improve the planning performance of value - based methods. The authors also propose a scheme ( SV - RL ) to extend their scheme to deep RL, which is naturally applicable for Value - based techniques. Experiments on Atari games show that SV -RL can consistently improve the performance of Value - Based methods, achieving higher scores for tasks when low -rank structures are confirmed to exist."
SP:430336893b247b7bd45687d78b0d0511a7369e87,"This paper proposes a new algorithm, Best - Action Imitation Learning ( BAIL ), for batch reinforcement learning ( DRL ) in the Ant environment. BAIL does not involve maximizing Q functions over the action space, instead it selects from the batch the actions it believes to be high - performing actions for their corresponding states, then uses those state - action pairs to train a policy network using imitation learning. Although BAIL is simple, the authors demonstrate that BAIL achieves state - of - the - art performance on the Mujoco benchmark. They also provide an anonymized code for reproducibility."
SP:94078964876667e8a5d9ae7728d779d5b91a576e,This paper proposes a new deep extreme multi - label learning algorithm for short text documents. The proposed DeepXML algorithm is based on the idea of splitting training of head and tail labels and learning word embeddings on head labels and transferring them through a novel residual connection to data impoverished tail labels. The authors also propose a negative sub - sampling technique to increase the amount of negative training data available by extending the state - of - the - art negative subsampling techniques.   The main contributions of the paper are the following :   1. A new training algorithm for deep extreme classifiers is proposed that is an order of magnitude more scalable than the current state of the art classifiers. 2. An efficient training algorithm is proposed to reduce the number of labels that need to be predicted by the classifier by re - ranking the set of predicted labels to eliminate the hardest negatives for the original classifier. 3. Slice algorithm for highly scalable and efficient embedding learning is proposed. 4. The method is empirically shown to be better at matching user queries to advertiser bid phrases as compared to leading techniques in production on a popular web search engine.
SP:b1b1252d82fa1bea18309e0b0b894e0f28f48bc9,"This paper proposes a new method for improving the efficiency of hash - based collaborative filtering ( HBC ). The proposed method is based on the idea of self - masking, where the user's hash code acts as a mask on the items, such that it learns to encode which bits are important to the user, rather than the user ’s preference towards the underlying item property that the bits represent. This allows a binary vector representation ( vector vector representations ( vector representations ) of users and items to be learned so that recommendations can be computed efficiently using the Hamming distance, which is simply the sum of differing bits between two hash codes. The authors evaluate their approach against state - of - the - art baselines on 4 datasets and obtain significant gains of up to 12 % in NDCG. They also make available an efficient implementation of self-masking, which experimentally yields less than 4 % runtime overhead compared to the standard Hashing distance."
SP:80898d0f2b2c8dc3388fa9164e529eae36aa1b21,"This paper analyzes the mode collapse of GANs. The authors propose two methods to calibrate the GAN ’s learned distribution without access to the training data or the current model parameters. The first method is based on latent space reshaping via Gaussian mixture models and importance sampling. They are observed to alleviate mode collapse without re - training training data, nor even needing access to model parameters in the original training data. The second method is a combination of black - box approaches, i.e., latent space resampling via Gaussians and importance samplers. The mode collapse analysis is conducted on several GAN models. The results show that the black box method is effective in alleviating mode collapse and that the importance sampling seems to work best."
SP:e5b5dda2f024cfda10526e744aa035e0165af58a,"This paper proposes a novel method for training neural networks governed by neural tangent kernels ( NTKs ) that are beyond the linearized regime of the NTK and are governed by the Taylor expansion of the network. The authors propose randomizing the neural networks, which allows them to escape their NTK regime and couple with quadratic models. They show that the optimization landscape of randomized two - layer networks is nice and amenable to escaping - saddle algorithms. They also establish results on the generalization and expressive power of such randomized neural nets. These results lead to sample complexity bounds that are advantageous when mild isotropic assumptions on the feature are present."
SP:cef7ea513eb3e42be4edf40e4ee1701a969bcbea,"Graph Neural Networks ( GNNs ) have recently received a lot of attention due to their power in handling graph data for different downstream tasks across different application domains. Many GNN models have been proposed, which mainly differ in their graph filter design. However, most of these models believe there is a best filter for all the graph data. This paper attempts to address three questions for the semi - supervised node classification task : 1 ) Whether there exists an optimal filter that performs the best on all graph data ; 2 ) Which graph properties should be considered for finding the best graph filter ; and 3 ) How to design appropriate filters that adapt to a given graph. The paper proposes a novel assessment tool, Graph Filter Discriminant Score ( GFD Score ), to analyze the effectiveness of graph convolutional filters. Using the tool, the authors find out that there is no single filter as a “ single best filter ”, and graphs with different properties are in favor of different graph filter choices.   Based on these findings, they develop Adaptive Filter Graph Neural Network ( AFGNN ), a simple but powerful model that can adaptively learn data - specific filters. For a graph, AFGnn leverages graph filter assessment as an extra loss term and learns to combine a set of base filters. Experiments on both synthetic and real - world benchmark datasets have demonstrated that our proposed model has the flexibility in learning an appropriate filter and consistently provides state - of - the - art performance."
SP:3c5ec9dbcf914c8901e4e35f3c2a7df4707422ab,"This paper studies the problem of training neural networks overparameterized neural networks with distributionally robust optimization ( DRO ). The authors find that applying group DRO to over - parametrized neural networks consistently fails to fit the training data. They find that any model with vanishing average training loss also has vanishing worst - case training loss. Instead, the poor worst - cases performance arises from poor generalization on some groups. To address this problem, the authors introduce a stochastic optimization algorithm, with convergence guarantees, to efficiently train a group of DRO models to minimize the worst - possible training loss over a set of pre - defined groups. They show that with this algorithm, they achieve substantially higher worst - worst - group accuracies, with a 10.2 % higher percentage point improvement on average on an iid of the test set compared to early stopping."
SP:eb1ee2e0f7d8466a04b58508ecb3da7b667eecdf,"This paper proposes a new method for explaining the behavior of black - box classifiers. The proposed method uses a neural network to predict the relevance scores of features according to their contributions. The relevance scores are obtained by computing a weighted sum of the contributions of each feature and then computing a KL divergence. The paper proposes to use the distribution controllers to guide the distribution of relevance scores. The distribution controllers can be thought of as a set of hyperparameters where the parameters of the hyperparameter can be easily set.   The paper also proposes a classification loss to train the proposed model to avoid the non - parameter tuning on ad hoc constraints. The experimental results demonstrate that the proposed method outperforms other local explanation methods in terms of faithfulness and explainability. In addition, the method also provides discriminative masks for intuitive explanation."
SP:32ea7cbc47cbdb1f703f4e07c31ce90abe083424,"The paper proposes a method to train deep neural networks for image reconstruction and classification tasks without any supervision. The proposed method is based on the non - differentiable top - K selection process, where the top patches extracted from the dataset are fed to a task - specific network, e.g. auto - encoder or classifier, to solve a domain specific problem. The method is able to learn to detect recurring structures in the training dataset by learning to reconstruct images. It can also learn to localize structures when only knowledge on the occurrence of the object is provided, and in doing so it outperforms the state - of - the - art.    The main contributions of the paper are as follows :   - A deep neural network that can be trained to tackle image reconstruction / classification problems that involve detection of multiple object instances, without supervision regarding their whereabouts. - A method that learns to extract the most significant K patches, and feeds these patches, which are the most relevant patches, into a task specific network to solve the problem. - An approach to train such a network by treating the result of top - k selection as a slack variable, resulting in a simple and effective multi - stage training."
SP:da1c5f6351d531482e90b86c3cceb52850c520de,"This paper introduces AutoAssemblet, a neural program synthesis algorithm for 32 - bit x86 processors and RAM. The main idea is to generate a chunk of code that can be executed to match a state change inside the CPU and RAM, and the algorithm is based on self - learning via reinforcement learning and policy networks and value networks. The authors also propose an effective multi - entropy policy sampling technique to alleviate online update correlations. The algorithm is applied to basic programming tasks and shows significant higher success rates compared to several competing baselines."
SP:0d4687fc36c02e27d1b95d532a3947589f92b1da,"This paper studies the impact of model architecture on the speed of training in the context of gradient descent optimization. The authors use the ideas from prior work that shows gradient descent can be modeled as a first - order ODE ’s matrix H to characterize the convergence rate. They introduce a simple analysis technique that enumerates H in terms of all possible “paths ” in the network. They show that changes in model architecture parameters reflect as changes in the number of paths and the properties of each path, which jointly control the speed - of - convergence."
SP:3e3bc8f617df742a395e7d315ec3810a42071294,"This paper studies the relationship between neural networks ( NNs ) and interpolating kernel methods ( Kernels ) in order to better understand the convergence and generalization behaviors of NNs. The authors make explicit the bias of initialization on strongly overparametrized NNs under gradient descent. They show that the test error of wide ReLU - NNs trained with squared loss are essentially a sum of two parts : The first is the minimum complexity solution of an interpolating kernels method, while the second is the initialization. This decomposition has two consequences : ( a ) the second part becomes negligible in the regime of small initialization variance, which allows us to transfer generalization bounds from minimum complexity Kernels to NNs ; ( b ) in the opposite regime, the test errors of wide NNs increases significantly with the increase in the variance of initialization.    The paper first makes explicit the connection between the initialization bias of Kernels and the bias towards strongly over parameterized neural networks. This is done by showing that the first part of the decomposition is negligible when the training data is small and depends heavily on the initialization, and that it is non - negligible when training with large initialization variance. Then, the authors show that under certain assumptions on the training and initialization, the generalization behavior of neural networks trained with Kernels is bounded by the bounds of the bound of minimum complexity of the Kernels. This bounds can be broken down into two terms : ( 1 ) the bound on the number of initialization parameters, and ( 2 ) the bounds on the generalizability of the network. The second term refers to the degree to which the network generalizes according to the first bound of the second bound. The paper then goes on to discuss some of the assumptions of the first and second bound, as well as some empirical results."
SP:b15ea009a36a0a76728dfc103d668d6781a8a99a,"This paper presents a new method for 3D object detection based on pseudo - LiDAR. The proposed method is based on a modified version of pseudo - LIADAR with improved stereo depth estimation. The main contributions are :   1 ) Improving the accuracy of depth estimation for faraway objects, which is the primary weakness of the current state - of - the - art method. 2 ) Leveraging the proposed depthpropagation algorithm to leverage cheaper but extremely sparse Li - DAR sensors, which alone provide insufficient information for 3 - D detection, to de - bias our depth estimation method. 3 ) Developing a new detection algorithm based on the KITTI object detection benchmark.   The main contribution of the paper is the proposed method for stereo - based 3D detection. This method outperforms the previous method for detection by 40 %."
SP:983d84502264633f3385d426c1d4601a0744ea9a,"This paper proposes a method for adversarial example detection based on GAT - Generative - Adversarial - Training ( GAT ). The method is based on the one - against - the - rest classification framework, where the adversarial examples are perturbed from other classes. The authors propose to train K - base detectors where the i - th detector is trained to discriminate natural data of class i from adversarial samples of class k, and the k - base detector uses the predicted label of the input to determine whether the input is a natural sample ( class k ) or one that is likely to belong to class k under the proposed detection scheme.   The authors provide a comprehensive evaluation of the above adversarial - example detection / classification methods, and demonstrate their competitive performances and compelling properties. They also provide a generative approach to detecting / classifying adversarial instances by interpreting each base detector as an unnormalized density model of the classconditional data."
SP:461e9308d050bc3dc7b35233452668bb31f5d491,"This paper proposes Rewarding Impact - Driven Exploration ( RIDE ), a novel intrinsic reward for exploration in RL that encourages the agent to take actions which result in impactful changes to its representation of the environment state. RIDE is designed for procedurally - generated environments, where an agent is unlikely to visit a state more than once and exploration is more sample efficient. The authors compare RIDE against state - of - the - art intrinsic reward methods on singleton environments with high - dimensional observations, as well as on hard - exploration tasks in procedurallygenerated grid - world environments. Their experiments show that RIDE outperforms state -of -the - art exploration methods, particularly in Procedurally -generated environments. Furthermore, a qualitative analysis demonstrates that, in contrast to prior work, RIDE does not suffer from diminishing intrinsic rewards during training and encourages agents substantially more to interact with objects that they can control."
SP:c002c20b5e8696588e029c0f65e88860418826c4,"This paper presents a comprehensive study on the embedding - based retrieval models for large - scale query - document retrieval problems. The authors focus on the two - tower retrieval model, which is based on the Transformer model. The retrieval algorithm is trained using a set of paragraph - level pre - training tasks, where it is asked to select a subset of relevant documents from a large corpus of documents, and then to score the documents based on their similarity with the relevant answer. The paper shows that the proposed retrieval algorithm outperforms the baselines in terms of both query recall and document re - ranking. Experiments are conducted on three different embedding models : BFS, Wiki Link Prediction ( WLP ), and the combination of all three. The experiments show that BFS performs better than other baselines and that the WLP model is more efficient than the other two baselines."
SP:4e161e08a624f87633dfb49dfd46bd1665e15189,"This paper proposes a novel graph neural network architecture, BiGraphNet, based on a parametric graph convolutional network. The main idea is to replace graph pooling in hierarchical networks with a single parametric bipartite operation, which is based on the fact that the input graph and output graph of a graph network must be parametrized in the same way, i.e., input and output graphs must have the same graph structure. The authors argue that the proposed method is more flexible than early graph neural networks, such as skip connections and graph autoencoders, which are more expensive to compute and store. The proposed method does not require the graph structure of input graph to be parametricized, which means that it is possible to parameterize the output graph without changing the representation of the graph network.   The main contributions of the paper are as follows :   1. Introducing a new method for parametricizing graph convolutions, which replaces the pooling operation with a bi - arithmically parametricised one. 2. Demonstrating that this new method can be applied to a variety of graph architectures, the authors show that it can be combined with skip connections to create architectures that are both flexible and efficient. 3. Empirically, they show that their method outperforms early methods such as graph skip connections, which don't require parametricization."
SP:9b9b6ee9014e5538442ba76d6059ed01f59ec8fb,"This paper proposes a few - shot classification method based on feature - wise transformation layers to generalize better to unseen domain under the domain generalization setting. The proposed method is based on MatchingNet ( Vinyals et al., 2016 ) and three metric - based methods ( RelationNet, RNN, and Graph Neural Networks ( GANs ). The authors compare the performance of the proposed method with that of two existing methods ( RNN and MetricNet ) on five datasets ( mini -ImageNet, CUB and CUB - C and Plantae ) and four image datasets ( CUB, C - Image, Tiny - Image and ImageNet ). They show that the proposed methods perform better than the other methods on the mini - image and C - image datasets and on the Plantae dataset when the feature distribution is different for each dataset. They also show that their feature wise transformation layer outperforms the other two methods when generalizing to unseen domains."
SP:df46627cb984a56bba36d510bfc52e00751e9107,"This paper proposes a new type of convolutional neural network for Lagrangian fluid simulation. Unlike previous approaches, the authors do not build an explicit graph structure to connect the particles but use spatial convolutions as the main differentiable operation that relates particles to their neighbors. To this end, they present a simple and effective extension of N - D convolutions to the continuous domain. They show that their network architecture can simulate different materials, generalizes to arbitrary arbitrary collision geometries, and can be used for inverse problems. In addition, they demonstrate that their continuous convolutions outperform prior formulations in terms of accuracy and speeda learned."
SP:3e17f333cf07183969c02bb66afdd3ccbf25bb19,"This paper proposes BatchEnsemble1, an ensemble method whose computational and memory costs are significantly lower than typical ensembles. It achieves this by defining each weight matrix to be the Hadamard product of a shared weight among all ensemble members and a rank - one matrix per member. It is parallelizable across devices, where one device trains one member of the ensemble and another is updated simultaneously for a given mini - batch.   The paper shows that on CIFAR-10/100 classification with ResNet32 ( He et al., 2016 ) and WMT14 EN-DE/EN-FR machine translation with Transformer ( Vaswani et al, 2017 ), which involves 100 sequential learning tasks, the proposed method yields comparable performance to progressive neural networks while having a much lower computational cost and memory cost. The paper also applies the proposed approach to lifelong learning on Split - ImageNet and shows that it is effective in calibrated prediction on out - of - distribution datasets and uncertainty evaluation on contextual bandits."
SP:a123a425ef3eb6188833d5a42e851bc3fa59df65,"This paper proposes a neural network - based method for solving partial differential equations ( PDEs ) based on partial differential equation solver for forward and inverse problems. The solver is grid - free, mesh - free and shape - free with an unsupervised approach, and the solution is approximated by a neural networks. The proposed method is a unified formulation of both forward - and inverse - PDE solvers, where the optimized loss function consists of a few elements : L2 and L2 norm. The authors provide experimental results on two free shape 2D second order systems with application to Electrical Impedance Tomography ( EIT ) and three other arbitrary domains. They additionally solve the inverse problem of diffusion and wave equations using a second order elliptic equation.   The main contributions of the paper are as follows :   1. The formulation of the PDE is based on a novel neural network-based method. The input to the neural network is a points set in an arbitrary domain and the output is the set of the corresponding function values. The network is trained to minimize deviations of the learned function from the strong PDE solution and satisfy the boundary conditions. 2. The resulting solution in turn is an explicit smooth differentiable function with a known analytical form. Unlike other numerical methods such as finite differences and finite elements, the derivatives of the desired function can be analytically calculated to any order. This framework enables the solution of high order non - linear PDES. 3. The algorithm is tested on the EIT application on a circular domain."
SP:973d0ad0faadcf7298300f2758de9154205e7113,"This paper studies the problem of training large neural networks for verification and probabilistic reasoning with SAT solvers. The authors propose a modified training procedure and architecture for training binarized neural networks ( BNNs ) that they claim improves the performance of verification and quantitative queries.    The main contributions of the paper are as follows :   1. They propose a modification to the training procedure of BNN training to make it easier for logic - based verification tools, like SAT solver, to reason about the network without sacrificing accuracy on the primary task of verifying the accuracy of the network. The proposed procedure is based on two parts : ( a ) a separation between training and verification, by not committing to a certain property during training and ( b ) a reduction in the number of perturbations to the network during training. They compare their proposed method with two existing methods and demonstrate significant performance gains over previous work ( Narodytska et. al. 2018 ; Khalil et al. 2019 ; Baluta et al. 2019 ). They get more than 10x-20x improvements on tested benchmarks for both verification and quantitative queries, e.g., finding the probability that a perturbation yields an adversarial example. The main contribution of this paper is that it proposes a new training procedure that makes the resulting network easier for verification tools like SAT - solvers, like the one proposed in [ 1 ]. The two main components of the proposed training procedure are the separation of training and verifying. The training procedure is similar to that used in [ 2 ], except that the verification part is done during training instead of during verification. The difference between the two is that the former does not require the network to satisfy a certain condition during training, while the latter requires it to satisfy the condition in order to verify the network's accuracy. The experiments show that this approach scales better on larger neural networks compared to existing work for verification. It is also shown that the proposed method is better on quantitative queries compared to the existing work."
SP:ca985e758f195bd04fb9f24b290a83974d6d308b,"This paper studies the expressive power of graph neural networks falling within the message - passing framework ( GNNmp ). Two results are presented : First, GNNMP are shown to be Turing universal under sufficient conditions on their depth, width, node attributes, and layer expressiveness. Second, it is discovered that it can lose a significant portion of their power when their depth and width is restricted. The proposed impossibility statements stem from a new technique that enables the repurposing of seminal results from distributed computing and leads to lower bounds for an array of decision, optimization, and estimation problems involving graphs. Strikingly, several of these problems are deemed impossible unless the product of a graph neural network ’s width and depth exceeds a polynomial of the graph size. This dependence remains significant even for tasks that appear simple or when considering approximation. It might be easier to obtain insights about models by studying their limitations. After all, the knowledge of what cannot be computed ( and thus learned ) by a network of specific characteristics applies independently of the training procedure. Further, by helping us comprehend the difficulty of a task in relation to a model, impossibility results can yield practical advice on how to select model hyperparameters."
SP:a98ae70a91850bbe624c307ba61d3daeb2494b82,"This paper proposes a generative model based on localised generative flows ( LGF ). The generative models are composed of continuous mixtures of bijections, which enables each bijection to learn a local region of the target rather than its entirety. The method is a generalisation of existing flow - based methods, which can be used without modification as the basis for an LGF model. Unlike normalising flows, LGF do not permit exact computation of log likelihoods, but instead propose a simple variational scheme that performs well in practice. The authors show empirically that LGF yields improved performance across a variety of density estimation tasks."
SP:3adc341dece170f428195e4dccadfb5f5daddf2d,"This paper studies the environment bias of vision - language - language ( VLN ) and language - guided navigation ( L2L ) in training agents in unseen training environments. The authors propose two ways to overcome the bias : environment re - splitting and feature replacement. The environment bias is studied by using ImageNet labels as semantic features and three kinds of semantic features : ( 1 ) detected object labels, ( 2 ) ground truth semantic views, and ( 3 ) learned semantic view features. They show that the semantic features significantly reduce the performance gap between seen and unseen environments on multiple datasets and achieve competitive unseen results to previous state - of - the - art models. They also provide a discussion on how to eliminate environment bias by employing advanced high - level semantic features which are more rational for the domain.   The authors observe that neither the language nor the underlying navigational graph, but the low - level visual appearance conveyed by ResNet features directly affects the agent and contributes to this bias in results in results. According to this observation, several kinds of representations which contain less low -level visual information, hence the agent learned with these features could be better generalized to unseen testing environments and achieve strong results in testing unseen environments."
SP:298e0043e99f586d314fbd9d16fdc6ae885e1ebb,"This paper proposes a method for using human feedback to accelerate the training of deep reinforcement learning agents. The paper proposes to use event - related electric potentials ( EFPs ) as auxiliary reward functions to a DRL algorithm with the intent of accelerating the agent's learning of the game in an environment in which humans are not involved in the interaction with the environment but observe the agent. The proposed method is based on the idea that the human ’s intrinsic reactions to the agent’s behavior is sensed as implicit feedback by placing electrodes on the human scalp and monitoring what are known as Event - related Electric Potentials. The implicit feedback is then used to augment the agent's learning in the RL tasks. The main contributions of the paper are the following :   ( 1 ) They argue that the definition of error - potentials is generalizable across different environments ; specifically they show that error -potentials of an observer can be learned for a specific game and the definition used as - is for another game without requiring re - training of the error - potentials. ( 2 ) They propose two different frameworks to combine recent advances in DRL into ErrP based feedback system, the first framework allows humans to provide implicit feedback while training in the loop, taking advantage of recent approaches in learning from imperfect demonstrations, while in the second framework, the implicit human feedback is obtained prior to the RL agent. ( 3 ) Finally, they scale the implicit feedback ( via ErrP ) based RL to reasonably complex environments ( games ) and demonstrate the significance of their approach through synthetic and real user experiments."
SP:a8395f8b877e1eebaef9ff2e8b4e488d55a74ef4,"This paper proposes a new method for comparing the performance of image classifiers, based on the notion of entropy, that aims to minimise the amount of information ( entropy ) required in individual test images to maintain correct classification. The goal is to compute an approximate minimal - entropy positive image for which the classifier provides a correct classification, becoming incorrect upon any further reduction. The paper proposes two complementary frameworks for computing entropy : 1 ) entropy metric that allows to combine and compare the effects of various types of reductions ( e.g. crop, colour reduction, resolution reduction ) on classification performance, in turn generalising similar methods explored in previous works 2 ) laconic classification framework for computing minimal entropy positive images of both human and machine classifiers. Experiments on the ILSVRC test - set show that machine classifier is more sensitive to reduced resolution resolution than human classifier in the evaluated setting, and that humans classify images of machine models with higher precision than machines classify those of humans.   The paper concludes with open challenges regarding the laconsic classification of images using DNNs."
SP:81cec8f907d8fa0653b5bc08af1f59bfefd49619,"This paper analyzes the robustness of adversarial defenses for deep neural networks based on the observation that the adversarial examples are not robust and small perturbations to the attacking input often recover the desired prediction. While the intuition is simple, a detailed understanding of this phenomenon is missing from the research literature, and the authors identify a family of defense techniques that are based on instability assumption that are similar across the literature. The defenses include deterministic lossy compression algorithms and randomized perturbation to the input that all lead to similar gains in robustness. The authors conduct extensive experimental analysis to identify the mechanisms that explain the effectiveness ( or ineffectiveness ) of these defenses. They show that adaptive attacks designed on the channel based on additive Laplace noise are often successful against other defenses. This result implies that for many defense strategies, the attacker need not be fully adaptive, i.e., they do not need to know exactly what kind of transformation is used to defend the network.   The authors argue that the optimization procedure in the attacker should find the smallest distance from the original image that closes the recovery window that can find the optimal attack strategy. In fact, they can devise a generic attacker that attacks a particularly strong lossy channel, and adaptive attacks based on this channel, such as adaptive attack designed on adaptive noise, can be more robust than other defenses that attack a channel that is not adaptive. The main contribution of this paper is a detailed analysis of why these defenses work and some potential mechanisms that could explain their effectiveness."
SP:a136b98e0ed478144ce9dd26e2b6d611543124e8,"This paper proposes a neural 3D mapping network that learns 3D object detection using 2.5D video, 3D feature maps, and 3D imagenet modules for object detection. The method is based on the view prediction framework proposed in view prediction paper [ 1 ]. View prediction is the task of predicting what a scene would look like from an alternative viewpoint. In this paper, the authors propose a neural network that takes 2D video as input and transforms it into 3D features maps by disentangling the scene content from the motion of the camera. The feature maps are then used to train an object detector that predicts the position of objects in 3D space. The object detectors are trained in a semi - supervised manner, where they are trained on the feature maps generated by the neural mapping network. The experiments show that the proposed method achieves better performance than the previous state - of - the - art in terms of object detection accuracy and feature detection accuracy.    The main contributions of this paper are as follows :   1. The authors propose an approach to learn 3D visual representations for the object detection task. They use a 3D bottlenecked object detection network, which consists of an object detection module, an egomotion module, and an imagination flow module. They show that their method outperforms the previous work on object detection, feature detection, and feature mapping. 2. They also show that an unsupervised learning approach improves the performance of their method. 3. They test their method on a set of static and dynamic videos, and compare their method with the prior work."
SP:6fd61604a2eeb8a2cbbda6c40807cebef6d40f2f,"This paper studies the problem of unsupervised domain translation ( UDT ), which consists in finding meaningful correspondences between two domains, without access to explicit pairings between them. The paper proposes a theoretical formulation of UDT in Optimal Transport ( OT ) framework, which makes the implicit bias towards low - energy transformations explicit, leading to cast UDT into an optimal transport framework. This allows them to provide theoretical guarantees for existing UDT methods, as well as solve UDT problems where previous methods fail. They also propose a simple approach to solve the UDT problem by learning a mapping from one domain to another, linking these paired elements together.   Previous work in this direction has been the CycleGAN model proposed in Zhu et al. ( 2017 ) which has led to extensions for many applications and has given impressive results. The starting point of this work is to understand and study this successful approach as there remains little theoretical understanding of why these models work. The main contributions are the following : Assuming the outputs uniquely define the weights up to invariant, which is not currently shown in the general case, the paper shows that the optimal transport formulation of OT can be used to regularize UDT models in this context and its dynamical formulation allows to build a model which overcomes the shortcomings of CycleGAN - like models, such as the shortcomings in the empirical results. In addition, the dynamic formulation also provides a link between OT and CycleGAN, and the authors propose a robustly converges model robustly toward mappings which give satisfactory results."
SP:8bb3ce11ad773685f6e41d90db3e7a5481e5ba47,"This paper proposes a rotation based regularization method, RotationOut, for training neural networks. It differs from Dropout in that it considers the input layer as an entire vector and introduces regularization by randomly rotating the vector. The proposed method can also be used in convolutional layers and recurrent layers with small modifications. The authors use a noise analysis method to interpret the difference between rotation and dropout in co - adaptation reduction. They also show how to use rotation / dropout together with Batch Normalization. Extensive experiments in vision and language tasks are conducted to show the effectiveness of the proposed method."
SP:37620ae8dc5683eb2843792e0aa4cbe6cba366f7,"This paper proposes a method to approximate adversarial adversarial perturbations ( UAPs ) in a data - free setting, where the original training data is not available for adversarial derivation. The proposed approach is based on the dilate loss formulation, which maximizes the Euclidean norm of the output before non - linearity at any layer. The authors show that the adversarial generation with full training data can be approximated to the adversary formulation without full data by a sequential optimization of the perturbation with the proposed dilate - loss. The main idea is to constrain the ReLU activation function at every layer to act roughly linear for data points and thus eliminate the dependency on data for crafting UAPS. Extensive experiments demonstrate that the proposed method not only has theoretical support, but achieves higher fooling rate than the existing data -free work. Furthermore, the authors provide evidence of improvement in limited data cases."
SP:2fd7d5507a8727db743dc89379a6f021d31ed39a,"This paper proposes a method for transferable Neural Architecture Search ( NAS ) based on meta - learning to learn an architecture suitable for a new set of tasks. The main idea of the method is to learn a meta - architecture that can be used to adapt to a new task quickly through a few gradient steps. To do so, the authors propose a method called T - NAS, which consists of three components : ( 1 ) meta - search, ( 2 ) adaptation step, and ( 3 ) gradient descent.    - Meta - search :   The first part of the proposed method is based on the idea that the goal of NAS is to find an architecture that is suitable for the task at hand and adapt it to the new task via gradient descent on top of the meta - learned architecture. The adaptation step is similar to the one used in the original NAS paper. The difference is that instead of adapting a single task, it adapts the architecture across multiple datasets or tasks. This is done by adapting the architecture of the neural network that is used for the original task to the task on which it is being transferred. The method is applied to several tasks on CIFAR-10 and ImageNet and compared with the state - of - the - art methods in supervised learning and few - shot learning. The results show that the method outperforms the other methods in most of the cases. However, the method performs worse in the supervised learning setting."
SP:1314a79ba12474adb33ff31b3cb22bed25b94fb7,"This paper proposes a new stochastic neural network ( SNN ) architecture for discriminative learning based on activation uncertainty and encouraging high activation variability. Compared to existing SNNs, the proposed SE - SNN is simpler to implement and faster to train, and produces state - of - the - art results on network compression by pruning, adversarial defense, and learning with label noise. The proposed architecture is based on a simple yet effective stochastically - based architecture.   The main contribution of this paper is to propose a new SNN architecture that directly models activation uncertainty by directly modeling activation uncertainty. This is in contrast to the existing activation uncertainty - based approaches that rely on a priori activation activation models. The activation model in the proposed SNN can be thought of as a mixture of activation - based and activation - free activation. This allows the activation model to be more flexible in terms of the number of activations and the amount of activation noise produced by the network compared to the activation models in the literature. The paper also proposes a method for training the SNN based on the activation uncertainty model. This method is referred to as simple and effective sto chastic Neural Network ( S - SNN ). The experimental results show that the proposed method is effective on a variety of tasks, including adversarial attacks and label noise robustness."
SP:bd4935d4fcf33f60f22e0f2fd9f7dc8ddfab6d17,"This paper proposes a meta - learning algorithm for generating curious behavior in reinforcement learning. The motivation is that curiosity is a mechanism found by evolution that encourages meaningful exploration early in an agent’s life in order to expose it to experiences that enable it to obtain high rewards over the course of its lifetime. The authors formulate the problem of generating such curious behavior as one of meta learning : an outer loop will search over a space of curiosity mechanisms that dynamically adapt the agent ’s reward signal, and an inner loop will perform standard reinforcement learning using the adapted reward signal. However, current meta - RL methods based on transferring neural network weights have only generalized between very similar tasks. To broaden the generalization, the authors propose to meta - learn algorithms : pieces of code similar to those designed by humans in ML papers. The proposed algorithm combines neural networks with other building blocks such as buffers, nearest - neighbor modules and custom loss functions. They demonstrate the effectiveness of the approach empirically, finding two novel novel algorithms that perform on par or better than human - designed published curiosity algorithms."
SP:6dff0f3a84809ae0ba9f58f36303597f1ba6dcc5,"This paper proposes a new method for generating code for the Any - Code - to - Code Generation ( AnyC2C ) problem. The proposed method, called Structural Language Modeling Model ( SLM ), is based on the tree - based approach of the previous state - of - the - art Seq2seq - based method, which is a sequence to sequence approach to generating code. In contrast, the proposed approach leverages the syntax of programming languages to model a code snippet as a tree, and decomposes the probability of the program ’s abstract syntax tree (AST ) by decomposing it into a product of conditional probabilities over its nodes. The authors present a neural model that computes these conditional probabilities by considering all AST paths leading to a target node in the tree.    The authors first discuss the theoretical advantages of SLMs, and show how they generalize many previous structural approaches for code generation, such as the SEQ - based approaches, in terms of the kinds of expressions that can be generated in this task. Then, the authors discuss theoretical advantages and limitations of their approach. Finally, they discuss the proposed method and compare it to several existing structured approaches in generating Java and C# code."
SP:7fc60d6fd1cfcc135c34f9664d172d3fd1c0ae0a,"This paper proposes a new method for solving non - convex optimization problems in learning large - scale neural networks ( NNs ). The authors introduce a new tool called canonical space to represent the objective functions in learning NNs. The canonical space is based on a linear transformation between the original NN model space and the canonical space, which are related by a pointwise linear transformation, which is represented by the so - called disparity matrix.   The authors prove that the gradient descent methods surely converge to a global minimum of zero loss provided that the disparity matrices maintain full rank. If this full - rank condition holds, then the learning of NNs behaves in the same way as normal convex optimisation. In particular, the authors show that the chance to have singular disparity matrixes is extremely slim in the case of large NNs, and that simple gradient descent algorithms are likely to converge to zero loss."
SP:78a536138570fe9b5d88350e4b16d598a7db1fe0,"This paper proposes two interactive graph - based segmentation algorithms that enforce connectivity between pixels of the same label. The first is an instance - aware heuristic based on a discrete Potts model and the second is a class - aware integer linear programming formulation that ensures global optimum. Both algorithms can take RGB, or utilize the feature maps from any DCNN, whether trained on the target dataset or not, as input. The authors present competitive semantic ( and panoptic ) segmentation results on the PASCAL VOC 2012 and Cityscapes dataset given initial scribbles. They also demonstrate that their interactive approach can reach 90.6% mIoU on the VOC validation set with an overhead of just 3 correction scribbles on the ground truth data set."
SP:2eb90879ddbc39b6b5c05152784d6044d1940513,"This paper studies the problem of detecting adversarial images generated by adversarial perturbations in the form of image classifiers. The perturbation can cause a shift in the salient features of an image, which often results in misclassification. Previous work has suggested that these salient features could be used as a defense, arguing that with saliency tools we could successfully detect adversarial examples. This paper argues that prior work which used gradient - based saliency models is ineffective as an adversarial defense, and proposes a simple baseline which uses the same model but with the saliency map removed. To remedy this, this paper proposes to learn a saliency model that captures the shifts in saliency due to adversarial attacks while also having a low computational cost. Based on this learnt model, the authors propose a novel defense : a CNN that distinguishes between adversarial and natural images using salient pixels as its input. The defense is tested on adversarial as well as white - box attacks, and it is shown that the proposed defense generalizes well (detecting stronger attacks when trained on weaker defenses )."
SP:fe5510d05ff091a5f133f2dbcd1b23d8d58d2c3e,"This paper considers the problem of computing the probability that a trained machine learning model is susceptible to adversarial attacks from any input from the unknown distribution ( unknown to the authors ). The authors consider the problem in terms of global robustness, i.e. probability that its prediction at any point sampled from the ( unknown ) input distribution from the training set is robust against adversarial attack.   The authors first provide a set of empirical results on MNIST, Fashion - MNIST and CIFAR-10 training data, showing that their method is within the bounds of the generalisation error upper bounded by upper - bounded by, for any 0 > 0 selected a priori model. They then provide a statistically sound analysis of the robustness / accuracy trade - off for a variety of neural networks architectures and training methods, including stochastic gradient descent and iterative pruning techniques. They empirically observe that robustness and accuracy tend to be negatively correlated for networks trained via Stochastic Gradient Descent ( SDP ), while a positive trend is observed for models trained with iterative gradient descent. They also show that concentration inequalities can be employed to compute global robusts with estimation error upper - upper - lower bounds, which are statistically sound."
SP:8f5616a1480b68c04b496ed498d237d5a7e87794,"This paper proposes a novel method of robust reinforcement learning based on the Wasserstein distance to measure the disturbance to the reference transition kernel in the Cartpole environment. The authors show that the proposed method is able to reduce an infinite - dimensional optimization problem to a finite - dimensional risk - aware problem. They then propose a robust actor - critic algorithm based on a modified version of the moderated robust Bellman equation. The method is tested on the CartPole environment and compared with two baselines. The proposed method outperforms the baselines in terms of robustness and sensitivity to perturbations.   The main contributions of the paper are the following :   1. A novel method for measuring the disturbance of the reference kernel to the transition kernel. This is done by using the distance between the reference reference kernel and the state disturbance. 2. A risk aware optimal bellman equation is derived. 3. Through this equation, the authors show the existence of optimal robust policies and provide a sensitivity analysis for the perturbation. 4. The sensitivity analysis is used to design a novel robust learning algorithm. 5. The effectiveness of the proposed algorithm is verified using experiments on Cartpole."
SP:d85963f5f0f6b20cf08f2a7c169ae33a45db7de2,"This paper proposes a method to approximate Nash equilibrium in multi - player games. The authors use the Pushforward measure technique to represent a mixed strategy in continuous strategy spaces. The proposed method generalizes the Gradient - based Nikaido - Isoda ( GNI ) algorithm to measure the distance between the players ’ joint strategy profile and a Nash equilibrium. The method is shown to converge to a stationary Nash equilibrium under the convex assumption on the payoff functions, the same popular setting as in previous studies. Experiments are conducted on three games : quadratic games, general blotto games, and GAMUT games. Results show that the proposed method outperforms other baselines."
SP:280d85cd8164a268f9d496ae5f17189c50f30dc1,"This paper proposes a method to augment training data for text classification using natural language explanations ( NL explanations ). The main contributions are two - fold : ( 1 ) NL explanations are unstructured and inherently compositional, which asks for a modularized model to represent their semantics, and ( 2 ) NL explanation often have large numbers of linguistic variants, resulting in low recall and limited generalization ability. To overcome these challenges, the authors propose a novel NExT framework to utilize NL explanations for augmenting model training.   The main idea is to transform NL explanations into logical forms by semantic parsing, which are then converted into different types of actions by the logical forms for labeling data. The authors conduct extensive experiments on two representative tasks ( relation extraction and sentiment analysis ) to demonstrate the superiority of their method over various baselines. Experiments on two benchmark datasets ( correlation extraction and multi - hop question answering ) demonstrate its superiority over baseline methods."
SP:a9b5f7257dedd719cfe341fca275776734af1d98,"This paper extends the verified training method for machine learning models to ( 1 ) recurrent neural network architectures and ( 2 ) complex specifications that go beyond simple adversarial robustness, particularly specifications that capture temporal properties like requiring that a robot periodically visits a charging station or that a language model always produces sentences of bounded length. Experiments show that while models trained using standard training often violate desired specifications, this paper produces models that both perform well ( in terms of test error or reward ) and can be shown to be provably consistent with specifications."
SP:3903680e07b676409e3cf6a1044b67291fe38630,"This paper proposes a method for training reinforcement learning agents in a toy gridworld environment, where the agent is trained on a few variations of the environment, and its learned state representations are regularized during training to minimize the constant variance in the learned policies. The authors formalize the problem as a visual domain randomization problem, and show that minimizing the policy ’s Lipschitz constant with respect to the randomization parameters leads to low variance in learned policies across domains.   The authors then propose a regularization method for regularizing the learned representations during training, and conduct experiments that demonstrate that this technique leads to more efficient and robust learning than standard randomization methods in the gridworld setting. Finally, the authors compare their method with standard domain randomisation and other regularization techniques in complex visual environments in order to evaluate the effectiveness of their method."
SP:c79046dc56b9ee9c926f87386046422ea134ae8d,"This paper proposes a simple and effective framework to tackle the imbalanced data pairs problem in deep learning. The authors cast DML as a pairwise binary classification problem that classifies a pair of examples as similar or dissimilar, and propose a simple framework to sample pairs in a batch of data for updating the model.   The key to this framework is to define a robust loss for all pairs over a mini -batch of data, which is formulated by distributionally robust optimization ( DRO ). The flexibility in constructing the uncertainty decision set of the dual variable allows the authors to recover state - of - the - art complicated losses and also to induce novel variants of the DRO framework. Experimental results on several benchmark datasets show that the proposed variants of DRO can outperform state - based methods and hard example mining methods."
SP:38420928e40ef80c0136ad607b9275f9ab1e0769,"This paper presents a stochastic trust region ( STR ) algorithm for finding the minimum in non - convex finite - sum minimization problem. The algorithm is based on the trust region method with inexact gradient and Hessian estimation. The authors prove that the proposed algorithm achieves a convergence rate of order O(1/k ) as long as the differential estimations are sufficiently accurate. Based on this result, the authors propose a sample - efficient STR algorithm which finds an estimator of the local minimum within the Hessian oracle query. This algorithm improves the state - of - the - art result by a factor of O(n ). Finally, they also develop Hessian - free STR algorithms which achieve the lowest runtime complexity. Experiments verify theoretical conclusions and the efficiency of the proposed algorithms.   The main contribution of this paper is the formulation of a formulation in which we make explicit control of the step size in the Trust region method. This idea is leveraged to develop two efficient, but theoretically similar, algorithms, STR and STR - free. The first method, STR1, achieves convergence in O(min{1/2, \sqrt{n/1.5 } ) for the oracle queries, which is lower than existing results for solving the problem ( 1 ). The second method STR2, which uses the gradient estimator in STR1 with a novel Hessian estimator to approximate the gradient, is theoretically closer to the convergence of the first method."
SP:28a35b70b5e6915af28cacebc4ea50690c9534af,"This paper proposes Farkas, a method for training deep neural networks without batch normalization or weight initialization. It is motivated by the observation that training neural networks with ReLU activation results in a reduction in training capacity in the presence of non - active neurons. The authors propose to replace the hyperplane in a neural network by a non - dead hyperplane that is added to the network at each layer to ensure that at least one neuron is active at a given layer. The method is based on linear programming with the goal of maximizing the number of active neurons per layer.    The paper presents results on three datasets : CIFAR-10, ImageNet, MNIST, and MNIST - CUB. The results show that the proposed method outperforms the baselines in terms of training capacity on all three datasets. The main contributions are as follows :   1. Introducing a method to replace hyperplanes in neural networks by a geometrically motivated method. The hyperplane can be added to neural networks at any layer of the network and is counted as one of the active neurons in the network. This allows the authors to remove the need for batch normalisation and weight initialization in neural network training. 2. A new layer is added at each stage of training to each of the residual networks. This adds a new layer to each residual network, and the authors show that it increases the training capacity of the networks. 3. The paper also shows that the method improves the performance on MNIST and ImageNet."
SP:1d325b148e3efe407241c1f1cbe8d17400499741,"This paper presents a method for computing computationally efficient robustness certificates for deep classifiers with differentiable activation functions. The authors show that if the eigenvalues of the Hessian of the network are bounded, they can compute a robustness certificate in the l2 norm efficiently using convex optimization. They also derive a computationallyefficient differentiable upper bound on the curvature of a deep network by means of curvature bounds as a regularizer during training.   The authors compare their method with CROWN ( Zhang et al., 2019 ) and CROWN - based training. They show that their method outperforms CROWN significantly while taking less time to compute. In addition, they also show that CRT is better than CROWN when trained with PGD and TRADES."
SP:33f6f5aa0d4655e5d75fe612e0eff05e579d45c5,"This paper proposes a method for compressed sensing recovery using untrained deep generative models. The method is based on the recently proposed Deep Image Prior ( DIP ), wherein the convolutional weights of the network are optimized to match the observed measurements. They show that this approach can be applied to solve any differentiable linear inverse problem, outperforming previous unlearned methods. Unlike various learned approaches based on generative model, their method does not require pre - training over large datasets, which reduces reconstruction error, especially for noisy measurements. Finally, they prove that, using the DIP optimization approach, moderately overparameterized single - layer networks trained can perfectly fit any signal despite the nonconvex nature of the fitting problem. This theoretical result provides justification for early stopping and regularization methods."
SP:23c0f621e6041003b59bf0532130760694cf6a4a,"This paper proposes TAIC, a method for applying reinforcement learning ( RL ) to the temporal abstraction problem of learning a latent representation of action sequences. The authors formulate the problem as learning a temporal abstraction of the long action sequences and present a novel approach of regularizing the latent space by adding information - theoretic constraints. They show that TAIC learns temporal abstraction from past experience and expert demonstrations without task - specific knowledge. They also show in the experiments that their learned temporal abstraction conveys meaningful information and benefit the RL training. In addition, the proposed framework provides an efficient tool for transferring knowledge between tasks."
SP:4e54c9196ba1eb2b6a0b0eee41e4a6f3a9de72dd,"This paper presents a novel method to sample nodes from a Graph Convolutional Network ( GCN ) based on a bi - directional diffusion based sampling method. The method is based on two components : ( 1 ) the influence of parent nodes and ( 2 ) the reverse influence of candidate nodes on the higher layer and the lower one on the candidate nodes of the lower layer. The authors claim that this approach addresses the problem of “ over - expanding ” the number of layers in a GCN and the “ neighbor explosion ” problem, as well as the sparsity issue suffered in current layerwise sampling algorithms.   The authors first show that a desirable layer - wise sampler should take into account both parent influence and reverse influence, and then propose a novel sampling strategy, which samples the nodes layer by layer conditionally based on the factors of the bi -directional diffusion between layers. Then, the authors apply the self - attention mechanism as the aggregator to learn suitable weights for different - hop neighbors during the training, which allows the model to incorporate both the first - order and higher - order proximities during a single layer propagation process without extra recursive propagation or skip connection. Experiments on three large benchmark graphs demonstrate the effectiveness and efficiency of the proposed model. The experimental results demonstrate that the proposed method outperforms the comparative state - of - the - art baselines and exhibit that our model can be efficiently and effectively scaled to very deep layers on very large graphs."
SP:bb0af9c011ef982c34fcadb545f6b5771818e7fa,"This paper presents a state - space model for videos, called STOVE, which is a combination of an image model and a dynamics model. The dynamics model is trained using joint inference and regularizing training. The image model is used to learn the position and velocities of objects in a state space, and the dynamics model to model the interactions between objects in the state space of the image model.   The model is designed for unsupervised learning from videos, where the goal is to be able to predict the physical behavior of an object in a video given only a few timesteps of data. To this end, the authors combine an image - based state space model with a dynamics - based dynamics model in a compositional manner. The authors claim that this approach improves on previous work by reusing the dynamics models for inference, accelerating and regularising training. They then present their experimental evaluation, where they show that their model outperforms the state - of - the - art supervised baselines in terms of video prediction performance. They also compare the performance of their model with other state space state space models and a control model."
SP:e67b463bc0aec2345925d609fa521ea49df57fd9,"This paper proposes a generative autoencoding model that combines the best properties of variational autoencoders ( VAE ) and generative adversarial networks ( GAN ). It is known that GAN can produce very realistic samples while VAE does not suffer from mode collapsing problem. To tackle this problem, the authors propose a novel approach to train the VAE model with an implicit likelihood by an adversarially trained discriminator. They call their method as Implicit \�Jeffreys Autoencoder ( \lambda - JAE ). They evaluate the generation and reconstruction ability of their model on CIFAR10 ( Krizhevsky et al., 2009 ) and TinyImagenet datasets. They show that their model achieves the state - of - the - art trade - off between generation quality and reconstruction quality. They also demonstrate how they can balance between mode - seeking and mass - covering behaviour of our model by adjusting the weight in our objective."
SP:87056d0147ddcaf5d78f6888b05161fbdbb3346c,"This paper presents an analysis of adversarial attacks on CNN classifiers that can be used to attack the Bayes - optimal classifier for certain class distributions, while for others the classifier is robust to such attacks. The authors present analytical results showing conditions on the data distribution under which all points can be made arbitrarily close to the optimal decision boundary and show that this can happen even when the classes are easy to separate, when the ideal classifier has a smooth decision surface and when the data lies in low dimensions. They find that standard CNN training consistently finds a vulnerable classifier with optimal classifiers while large - margin methods often find robust classifiers with the exact same training data. The results suggest that adversarial vulnerability is not an unavoidable consequence of machine learning in high dimensions, and may be the result of the specific distributions of commonly used datasets or of suboptimal training methods used in current practice."
SP:a7b3a35e6a79084bdfd1e4a963dfa081279cd8bb,"This paper studies the effect of pruning in terms of top - 1 accuracy for neural network classification with respect to the sparsity introduced by the introduction of PIE ( pruning identified exemplars ) in the Neural Network Classification ( NNC ) model. PIE is defined as a subset of images that are hard - to - generalize - to images, and the authors find that removing PIE images from the test - set significantly improves the performance of the NNC model for both sparse and non - sparse non - parseshpearse models. However, they find that certain examples, such as pruned examples of atypical examples, are more likely to be mislabelled and of lower image quality than other examples. The authors also show that certain classes of images are more affected than others by sparsity, and that certain classifications are more sensitive than others to pruning. They conclude that PIE could be useful for tasks where it is important to choose not to classify certain examples when the model is uncertain about how to classify them."
SP:4b17edaa7ec6201891433320d85f9a415656b763,"This paper presents KG - A2C1, an agent that learns to explore combinatorially large text - based action spaces using a knowledge graph and a template based action space. The main contributions of the paper are as follows :   1. The agent learns to reason about the game state using the knowledge graph while exploring the action space using the template - based template.   2. The knowledge graph is used to predict the state of the game and the template is used for action generation. This is done by constructing a dynamic knowledge graph based on the knowledge of the current state and the action taken by the agent. 3. The action is generated using a graph based action template. 4. The method is evaluated on a wide variety of popular text based IF games. The results show that the proposed method outperforms state - of - the - art agents in most cases. The authors claim that this is due to the fact that the agent is able to explore large action spaces while also being able to build a dynamic graph based knowledge graph."
SP:b1784ecbb8f36eef9cae33d61ce60d80c2f9c38d,"This paper proposes a method to augment the maximum likelihood estimation ( MLE ) used in language generation tasks with an additional data - dependent Gaussian prior ( D2GPo ) to alleviate the issue of negative diversity ignorance. The main drawback of MLE is that it treats all incorrect predictions as being equally incorrect, which unfairly downplays the nuance of these detailed token - wise structure. To counter this drawback, the authors propose an extra Kullback - Leibler divergence term derived by comparing a data - independent Gaussians prior and the detailed model training prediction, which is then injected into the final MLE loss through a KL divergence term. Experimental results show that the proposed method makes effective use of a more detailed prior in the data and has improved performance in typical language generation task, including supervised and unsupervised machine translation, text summarization, storytelling, and image captioning."
SP:7c29cb5a32b14e1392408dc5daba4cd35848bea9,"This paper proposes a new calibration method for Deep Neural Networks ( DNNs ) based on temperature scaling and focal loss. Focal loss is the most commonly used method for calibrating cross entropy loss in cross entropy neural networks ( CE ). The authors argue that the proposed calibration method improves upon the state - of - the - art temperature scaling method while preserving the confidence of the model. The proposed method is based on two components. First, the authors propose to replace the commonly used CE loss with a method based on focal loss and temperature scaling. The second component is temperature scaling with the proposed method. Experiments are conducted on two datasets ( CIFAR-10/100 and NLP - 20 ) and four different network architectures. The experiments show that in almost all cases, DNN trained with focal loss are more calibrated than those trained with cross - entropy loss, MMCE, and Brier loss Brier."
SP:cd6b8417ec8bcb773c78cff677bb0a76d6b3f6f3,"This paper introduces LiPopt, a polynomial optimization framework for computing increasingly tighter upper bounds on the Lipschitz constant of neural networks. The underlying optimization problems boil down to either linear ( LP ) or semidefinite ( SDP ) programming, and the authors propose to use sparse connectivity of a network to significantly reduce the complexity of computation. This is specially useful for convolutional as well as pruned neural networks, and they conduct experiments on networks with random weights and networks trained on MNIST. They show that in the particular case of the $ \ell_1 $ case, their approach yields superior estimates, compared to baselines available in the literature. In general, their proposed approach can be readily extended with minor modifications to any directed acyclic computation."
SP:31c9dc0dd8806daddc9cb48c56ec819577fe46cd,"This paper proposes a self - supervised learning approach for video features that results in significantly improved performance on downstream tasks ( such as video classification, captioning and segmentation ) compared to existing methods. The method extends the BERT model for text sequences to the case of sequences of real - valued feature vectors, by replacing the softmax loss with noise contrastive estimation ( NCE ). The authors also show how to learn representations from sequences of visual features and sequences of words derived from ASR ( automatic speech recognition ), and show that such cross - modal training ( when possible ) helps even more."
SP:0f24424d10f1201dd25e8c56354e10afc9b2b11c,"This paper proposes a method to transfer data between clients and servers during the inference phase of training neural networks. The authors propose a simple yet effective framework that allows to select certain parts of the input data needed for the subsequent application of a given neural network. The data is stored on a public storage server without the possibility for the users to directly execute code and apply machine learning models. Depending on the available bandwidth, this data transfer can become a major bottleneck. To alleviate this problem, the authors propose to train a neural network with associated selection masks as well as the associated server. The server and the neural network are trained simultaneously such that a good model performance is achieved while, at the same time, only a minimal amount of data is selected. During inference phase, only the parts selected by the masks have to be transferred between the server and client. Experiments indicate that it is often possible to significantly reduce the amount of the data needed during inference phase without affecting the model performance much.   The authors claim that the individual selection criteria can be adapted to the specific needs of the task at hand. For example, in the case of image classification, individual mask could be used to select only a fraction of data needed to achieve a model performance that is comparable with one that is obtained on all the data."
SP:aa4fcf5b2cae05c5c6a903c24e4992b56655dee2,"This paper proposes a method for training neural networks that can detect out - of - distribution ( OOD ) examples without compromising much of its classification accuracy on the test examples from known classes. Based on the Outlier Exposure ( OE ) technique, the authors propose a novel loss function that achieves OOD detection with Mahalanobis distance - based classifier. The proposed method is evaluated on image classification and text classification tasks. The experiments show that the proposed method achieves better results than the state of the art in both image and text detection with OE on both tasks.   The main contribution of the paper is that it proposes a methodology for training a neural network that is able to detect OOD examples from novel class distributions while maintaining its accuracy on known class distributions. This is in contrast to the existing methods that rely on the assumption that all classes are known prior to the training stage. The main contributions of this paper are as follows :   1. The authors propose an approach to train neural networks to detect out of distribution samples from novel classes without compromising on the accuracy of the known classes 2. The method is tested on two tasks : Image classification and Text classification."
SP:89bc528ef801182365ac279e8963803afccb391d,"This paper proposes an end - to - end deep learning model, called E2Efold, for RNA secondary structure prediction which can effectively take into account the inherent constraints in the problem. The key idea is to directly predict the RNA base - pairing matrix, and use an unrolled algorithm for constrained programming as the template for deep architectures to enforce constraints. With comprehensive experiments on benchmark datasets, the authors demonstrate the superior performance of the proposed model compared to previous SOTA ( especially for pseudoknotted structures ) while being as efficient as the fastest algorithms in terms of inference time."
SP:b68560cce8c64ebe0ca5e6534b3732c775d36452,This paper proposes a new method of training agents to simulate each other ’s internal representations of the environment. The idea is to create a virtual environment in which all agents participate and interact with their own biased representations. Each agent asynchronously internalises their own predictive model and forms a virtual simulation within which the agent plays trials of the episodes in entirety. The goal is to develop a policy trained solely inside the agents ’ simulations that can then be transferred to the real - world environment.   The authors propose to train a policy by taking turns to visit the internal simulations of the other agents and developing policies that complement one another’s shortcomings. The experimental results show that the proposed method outperforms the state - of - the - art methods.
SP:bd1dc08b4fd9a5cc78d26d7eb7f05dbb4a629ab1,"This paper proposes a new approach for open - domain dialog generation based on semantic latent space learning. The model learns a latent space from the prompt and responses extracted from the dataset. The latent space is learned by maximizing the pair relationship between the prompts and responses as a regression task on the latent space, instead of classification on the vocabulary using MLE loss. An additional autoencoder is trained, for recovering the full sentence from the full latent space. Experimental results show that the proposed model eliminates the generic response problem, while achieving comparable or better coherence compared to the baselines of previous approaches."
SP:ef0d5fd333ed60feb3946d24002e9a90642aea66,"This paper proposes Gaussian light and shadow ( GLAS ) method for fine - grained classification. GLAS is based on Gaussian mask perturbation inspired by shadowing and light. It is claimed that the scalability of the mask allows it to be applied to many different types of classification tasks. The main contributions of the paper are as follows :   ( 1 ) It introduces a novel notion of salient explanation which is critical in explaining the fine-grained classification tasks ; ( 2 ) It proposes a simple yet efficient black - box method, which provides an easy way to perturb an input image based on the Gaussian lighting and shadowing ; ( 3 ) It shows the broad applicability of GLAS to various other tasks : object localization and visual captioning. Quantitative comparisons show that GRAS is superior to conventional methods."
SP:d17ca20cc527c28ab7358cb5b14954e5fb56409f,"This paper proposes a method to train neural networks with deconvolutional kernels by removing pixel - wise and channel - wise correlations before the data is fed into each layer of the network. The method is based on the observation that the training of convolutional neural networks is computationally expensive because of the strong correlations in real - world image data. The paper shows that the method can be applied to 10 modern neural network models by replacing batch normalization within each of them with the proposed method. Experiments are conducted on the CIFAR-10 dataset, MNIST dataset, FashionMNIST dataset and ImageNet dataset. The proposed method is shown to outperform the baselines on all of these datasets in terms of accuracy and convergence."
SP:e1b0de9a36bf8359df368b7a55a7f23e99d88db7,"This paper presents a novel quantization method for quantizing weights in generative adversarial neural networks ( GANs ) based on EM algorithms, named as QGAN. The method is based on applying existing successful CNN quantization methods to quantize weights in GAN models to extreme low bits. The authors claim that none of them generates samples with reasonable quality because of the underrepresentation of quantized weights in models, and the generator and discriminator networks show different sensitivities upon the quantization precision. Based on observations from the sensitivity study, the authors develop a multi - precision quantization algorithm, named QGAN, which provides the precision as low as possible to satisfy the quality requirement of generated samples."
SP:58c4905f59f04a50b30d27c99521126a6455d38a,"This paper studies the problem of last - iterate convergence of gradient descent algorithms in settings where the input is not linear or strongly convex. It is shown that the gradient descent algorithm of Hamiltonian gradient descent ( HGD ) converges to the squared norm of the gradient norm under certain conditions, including the bilinear and strongly concave settings, as well as the nonconvex - concave setting. The authors introduce a novel condition for the convergence of HGD, the “ sufficiently bilINear ” condition, which is distinct from the previously known conditions such as the Polyak -Łojasiewicz ( PL ) condition or pure bilinearlyity. They also show that a related algorithm known as Consensus Optimization ( CO ) can effectively train GANs in a variety of settings, including on CIFAR-10 and celebA. They show that CO can be viewed as a perturbation of the HGD algorithm, which implies that for some parameter settings, CO converges at the same rate as HGD. Finally, the authors show that stochastic versions of both HGD and CO are equivalent to each other."
SP:d8556b52272321a1415ac2d85bb12e88b51ee73a,"This paper studies the stability of the forward / backward process of learning ResNet. Specifically, the authors consider the case where the number of residual blocks in a ResNet block is $ \sqrt{L}$ and the activation function $ \tau(L)$ is a scalar with ReLU activation. They show that under certain assumptions on the scalar $ \mathbb R^\mathbb L$, the forward process of ResNet is stable for a finite number of blocks and explodes for larger blocks. They also show that the over - parameterization requirement of the ResNet only weakly depends on the depth, which corroborates the advantage of using ResNet over vanilla feedforward network. Empirically, they show that deep ResNet can be easily trained even without normalization layer and that adding 1/\sqrt L on top of normalization can improve performance compared to ResNet with normalization."
SP:cf70dc496825ece2f28fdf4f1a6f4316c69e0e48,"This paper proposes a method to train sparse neural networks with a fixed parameter count and a fixed computational cost throughout training, without sacrificing accuracy relative to existing dense - to - sparse training methods. The method updates the topology of the network during training by using parameter magnitudes and infrequent gradient calculations. The authors show that this allows the optimization to escape local minima where it would otherwise become trapped if the sparsity pattern were to remain static. They also show that if the full gradient information is needed less than every 11 - sparsity iterations, then the overall work remains proportional to the model sparsity."
SP:d2d2b892518d54d0e63e26a056f2298be3be2610,"This paper proposes a new method to study the interpretability of the latent space of deep generative models. The key idea is to use a learned generative model such as GANs or GAN - VAE to generate an image from a latent space that can be manipulated in a number of ways such as the position of the image or the scale of the object in the image. To this end, the paper proposes to use GAN ’s latent space encoder, which is an extension of the GAN encoder and decoder. The encoder encoder is a linear combination of GAN and GAN-VAE encoder with a few modifications.    The main contributions of the paper are as follows :   ( 1 ) A new latent space interpretation of the generative process. This is done by using an encoder that encodes a set of directions that correspond to the learned representation of an image. These directions can then be used to control specific properties of the generated image, e.g., position of object in image or scale of object to be controlled in image. The paper shows that this can be done by simply changing the direction of the encoder in the space of possible directions of the learned representations. This can lead to some interesting results. ( 2 ) An analysis of the impact of disentanglement between the learned learned representations and the generated images. This analysis is done using two main results. First, it is shown that disentangling can have a negative impact on the ability to control the image in latent space. Second, this paper shows how disentangler can have an positive impact when controlling the position in image and the scale in image, and shows that the effect is positive when the position is controlled."
SP:1c63389e972d4652fac831e9d11609cd3c3c371a,"This paper proposes a physics - as - inverse - graphics approach for unsupervised physical scene understanding of systems from video, where the differential equations governing the scene dynamics are known, but labeled states or objects are not available. The proposed approach combines differentiable physics engines, enabling objects and explicit state and velocity representations to be discovered. This framework allows them to perform long term extrapolative video prediction, as well as vision - based model - predictive control. They evaluate their approach on 4 datasets with different non - linear interactions and visual difficulty. They show that their approach significantly outperforms related un - supervised methods in long - term future frame prediction of systems with interacting objects, due to its ability to build dynamics into the model as an inductive bias. They also show that the controller ’s interpretability provides unique capabilities in goal - driven control and physical reasoning for zero - data adaptation."
SP:c6b8b682bf3087a65cb2379700b8a0183853c2af,"Graph Convolutional Networks ( GCN ) are used to predict class relevance of noisy examples. For each class, the GCN is treated as a binary classifier learning to discriminate clean from noisy examples using a weighted binary cross - entropy loss function, and then the clean “ probability ” is exploited as a relevance measure. Each noisy example is weighted by its relevance when learning a classifier for the end task. Experimental results show that the proposed GCN - based cleaning process significantly improves the classification accuracy over the standard classification method, while outperforming the method by Douze et al ( 2018 ) using the same large - scale collection of data."
SP:dd9c9a5dccbba5dd15b03ca6b314a9e153e95548,"Graph Neural Networks ( GNNs ) follow the scheme that the representation vector of each node is updated recursively using the message from neighbor nodes, where the message of a neighbor is usually pre - processed with a parameterized transform matrix. To make better use of edge features, the authors propose Edge Information Maximized Graph Neural Network ( EIGNN ) that maximizes the Mutual Information ( MI ) between edge features and message passing channels. The MI is reformulated as a differentiable objective via a variational approach. The authors theoretically show that the newly introduced objective enables the model to preserve edge information, and empirically corroborate the enhanced performance of MI - maximized models across a broad range of learning tasks."
SP:f1cf63d728da51b4f83eb50ef69e3788b3a5ed74,"This paper presents a new method for verifying the non - trivial properties of generative networks. The goal of the paper is to verify that the output of a generative network is consistent with a classifier that is robust to different amounts of “ bald ” or “ robust ” classifier. To do so, one needs to capture sufficient non - convexity so as to be able to produce precise bounds on the output. The paper proposes a deterministic verifier, APPROXLINE, that performs both deterministic and probabilistic interpretation. The main contributions are : ( 1 ) Demonstrating deterministic verification of certain visible, highly non - trivial properties of certain generative neural networks ( e.g., classifier robustness to the classifier “ nA ” ), and ( 2 ) Developing and testing a method that can verify the non trivial property of the outputs of the generative models.   The main contribution of this paper is the introduction of the deterministic method, which is based on two components. First, the authors propose a method for deterministic interpretation of the output from generative nets. The second part of the contribution is to develop and test the verifier that verifies the nonrivial property of outputs. The verifier consists of two parts. The first part is a set of outputs for which the output is guaranteed to be consistent, and the second part is an extension of this set. The method is tested on two sets of outputs, one for the classifiers and one for a second set of output classifiers. The experimental results show that the method verifies that the outputs are consistent with the specifications."
SP:2b0887dcf09249e8cee30d38163aeb9ef1e92b27,"This paper studies the problem of suspended animation in graph residual learning for existing graph neural networks ( GNNs ). Specifically, this paper focuses on the spectral graph convolutional operator ( GCN ), which has been criticized for its performance degradation, especially for the models with deep architectures. In this paper, the authors propose GRESNET ( Graph Residual Network ) framework, which creates extensively connected highways to involve nodes ’ raw features or intermediate representations throughout the graph for all the model layers. Different from the other learning settings, the extensive connections in the graph graph data will render the existing simple residual learning methods to fail to work. The authors prove the effectiveness of the new graph residual terms from the norm preservation perspective, which will help avoid dramatic changes to the node ’s representations between sequential layers. Detailed studies about the GRES NET framework for many existing GNNS, including GCN, GAT and LOOPYNET, will be reported in the paper with extensive empirical experiments on real - world benchmark datasets. The main contribution of this paper is to identify the suspended animation problem with the existing graph residual networks. In Section 2, we will introduce related work of the related work, such as the graph residual operator and suspended animation limit.    In Section 3, the suspendedAnimation limit will be analyzed in Section 4, and Graph residual learning will be introduced in Section 5, whose effectiveness will be tested in Section 6. Finally, the paper will conclude this paper in Section 7."
SP:dc436ade4d04072de35a90e5e4a1bfebfddb04e9,"The paper proposes a nonlinear parametric 3D face reconstruction method based on CNN - based convolutional neural networks trained on a dataset generated from a linear 3D morphable model ( 3DMM ). The authors claim that the proposed method is superior to state - of - the - art methods and is robust to various expression, pose, and lighting conditions. The proposed method consists of three components : ( 1 ) a CNN based model that regresses the face shape and texture directly, ( 2 ) a center loss that combines the identity and expression representations of the CNN model, and ( 3 ) an adversarial loss to train the model with adversarial losses in a semi - supervised manner.   The first part of the paper introduces the CNN based CNN model and its training data, and the second part introduces the center loss. The CNN model is trained with a dataset of face images from a single person, and is trained to be able to generalize well to different ages, ethnicities, expressions, poses, and lightings. The method is evaluated on a set of 5 facial images from the same person and 5 different people of different ages and locations. The model is compared with CNN based, 3DNN based, and 2DMM based methods in terms of reconstruction accuracy and reconstruction quality on the 5 images. The main contributions of this paper are as follows :   ( a ) A nonlinear 3D parametric model that generalizes well to images generated from different people and different locations, ( b ) An adversarial training method that trains the model to learn the center of mass loss of the expression and identity representations, ( c ) A novel method for training the expression representations, where the model trains the CNN and center loss separately, and uses the CNN network to train them."
SP:f7bc06697b09e2d59ec06b2cbcf3c0828ece32ae,"This paper proposes a method for solving imitation problems using reinforcement learning when only partial knowledge about the transition kernel is available. The method, dubbed Expert Induced Markov Decision Process ( eMDP ), is a formulation of Reinforcement Learning ( RL ) with a synthetic kernel, which is used to simulate the transition of state components for which the kernel is known ( s, r ) and extract from demonstrations the state component for which kernel is unknown ( s - r ), and the next state is then stitched from the two components : s = { s } and s - u. The authors describe in detail the recipe for building an e - MDP and analyze the errors caused by its synthetic kernel. The experiments include imitation tasks in multiplayer games, where the agent has to imitate one expert in the presence of other experts for whom they do n’t have full knowledge. They show that combining a policy gradient algorithm with their model achieves superior performance compared to the simulation - free alternative. They also show that solving it amounts to solving an imitation problem that seeks to match the state densities at each step. Finally, the authors show empirical results that stress the benefits of using eM DP when the transition kernels are not known and model - based approaches are not applicable."
SP:82cce92821e8168ab4a6fd67573b66c1d17673b8,"This paper proposes a self - supervised reinforcement learning method, MISC, for learning to control states of interest without any external reward function. The authors formulate the intrinsic objective as rewarding the skills that maximize the mutual information between the context state and the state of interest. They evaluate their approach for different simulated robotic manipulation tasks from OpenAI Gym and a navigation task in the Gazebo simulator. They demonstrate that MISC enables the agent to learn skills, such as reaching, pushing, picking up, and sliding the object without rewards. They also show that the pre - trained policy can be quickly adapted to the specific tasks with external sparse reward signal. Fourthly, the pretrained mutual information discriminator also improves the learning process of the agent."
SP:5db63d39cfd8132bec832ab64b8fbd403b3b8df0,"This paper proposes a trojaning attack method for large neural network ( NN ) models, which outperforms existing studies in capability, generality, and stealthiness. The authors argue that the attack is programmable that the malicious misclassification target is not fixed and can be generated on demand even after the victim ’s deployment.   The authors claim that the trojaned attack is not limited in a small domain, but can affect applications of different domains that reuses its general features. The trojan is applied to the Flower dataset and Caltech-256 dataset and attacks the victim model for the two smaller datasets. It is shown that the same trojan used for the smaller dataset can affect victims using any other dataset. The author also proposes a more powerful trojan for the larger dataset."
SP:35ea626ee4dd1a7a368a660eb852192924966b7f,This paper presents a few - shot regression ( FSR ) algorithm for drug discovery. It is based on the fact that there are relatively few FSR methods available that can be applied to real - world drug discovery tasks. The authors propose an algorithm that learns a kernel function and a differentiable learning algorithm for FSR. The choice of the kernel is critical and the algorithm learns to find the appropriate kernel for each task during inference. The algorithm is applied to both toy and novel drug discovery datasets. It outperforms state - of - the - art FSR algorithms on both toy - world benchmarks and benchmarks derived from biological assays.
SP:91ca4c3ee07617356250bae9f4ef9799b3b134ff,"Graph Neural Networks ( GNNs ) perform well on many tasks, but less structured networks fail. Theoretically, there is limited understanding of why and when a network structure generalizes better than others. In this paper, the authors develop a framework to characterize which reasoning tasks a network can learn well, by studying how well its computation structure aligns with the algorithmic structure of the relevant reasoning process. They formally define this algorithmic alignment and derive a sample complexity bound that decreases with better alignment. This framework offers an explanation for the empirical success of popular reasoning models, and suggests their limitations."
SP:a52aee8da5cf5acd2baf3c2a62cb679e13b18bd5,"This paper proposes a new metric, Fréchet Joint Distance ( FJD ), to evaluate conditional generative adversarial networks ( cGANs ). FJD is defined as the distance between the joint distributions of image generation and conditioning and is based on existing metrics, namely FID and joint distribution embedding ( FID ). The authors claim that FJD can capture image quality, conditional consistency, and intra - conditioning diversity, which are desirable properties of conditional generation. Experiments on a controllable synthetic dataset ( dSprite ) are conducted to demonstrate the benefits of FJD when compared to other metrics, and also to compare the newly introduced metric to existing cGAN - based models for a variety of conditioning modalities ( e.g. class labels, bounding boxes, object masks, and text )."
SP:fa822e8472efae17c7dfde8258057898383ecbbb,"This paper proposes a method to identify ‘ decision states ’, a parsimonious set of states where decisions meaningfully affect the future states an agent can reach in an environment. The authors utilize the VIC framework ( Gregor et al., 2016 ) which maximizes an agent’s ‘empowerment ’ and formulate a sandwich bound on the empowerment objective that allows identification of decision states. Unlike previous work ( Goyal et al., 2019 ), the decision states are discovered without extrinsic rewards – simply by interacting with the world.   The authors show that their decision state identification mechanism is transferable and leads to improved sample - efficiency on goal - driven tasks in novel environments. On a challenging grid - world navigation task, the method outperforms ( a re -implementation of ) Goyal and proves that it can identify decision states in a task - free manner."
SP:a19a51df7e28a5d3380be4fba13842efbfe3efec,This paper proposes a method called SEFT ( Set Functions for Time Series ) for dealing with irregularly sampled time series in healthcare data. The proposed method is based on differentiable set function learning and is parallelizable. It scales well to very large datasets and online monitoring scenarios. It is evaluated on multiple healthcare time series datasets and shows that it performs competitively with the state - of - the - art while significantly reducing runtime.
SP:4ae89d64460b08749acc192004545c1fa8b7553b,"This paper investigates the inductive bias of neural networks for audio signal priors. The authors empirically show that current network architectures for audio processing do not show strong evidence in capturing priors for natural image priors and propose an operation called Harmonic Convolution to train deep neural networks to model priors by explicitly utilizing the harmonic structure in harmonic series. The proposed operation is done by engineering the kernels to be supported by sets of harmonic series, instead of by local neighborhoods as convolutional kernels. Experiments on unsupervised audio restoration tasks show that networks trained with the proposed method achieve state - of - the - art performance on supervised tasks."
SP:c81a2b3fd1c56b9b18e4a358e3ff8b40aea5256a,"This paper introduces data echoing, a method to speed up training by reusing intermediate outputs from earlier stages of neural network training. Data echoing reuses ( or “ “echoes ” ) intermediate output from earlier pipeline stages in order to reclaim idle capacity from accelerators. The authors investigate the behavior of different data echoing algorithms on various workloads, for various amounts of echoing, and for various batch sizes. They find that in all settings, at least one data echoing algorithm can match the baseline performance of a baseline algorithm using less - predictive performance using less upstream computation."
SP:b4cf56d3fa7d65cacde33f17cd04bd5bbc52dd71,"This paper proposes Variational Intrinsic Successor FeatuRes ( VISR ), a novel algorithm that learns controllable successor features in successor features ( SF ). The main contribution is to address the generalization and slow inference problem by making use of another recent advance in RL, successor features. In this paper, SF enables fast transfer learning between tasks that differ only in their reward function, which is assumed to be linear in some features. To do so, the authors propose a novel method, Variational intrinsics successor fattusRes ( VIR ), which learns the features required by SF. The authors show that VIR outperforms the state - of - the - art in the unsupervised setting where the rewards are only exposed briefly after a long period of supervised learning. In contrast, in the supervised setting, VIR is shown to outperform all baselines, which includes algorithms that operate in three regimes : strictly unsuper supervised, supervised with limited data, and both. The authors also augment the popular 57 - game Atari suite with the proposed method, and show that VISR achieves human - level performance on 12 games."
SP:83500230586a9134f910ad067b7233dc563dc1ba,"This paper studies the generalization properties of deep neural networks ( DNNs ). The authors show that generalization results from smoothness of the functional approximation combined with a flat initial approximation. This smoothness increases with number of units, explaining why massively overparameterized networks continue to generalize well. Generalization is due to flat initialization in the over - parametrized FC ReLu nets. The impact of global, rather than local, impact of breakpoints and delta - slopes helps regularize the approximating function in the large gaps between training data, resulting in their smoothness.   The authors also study the effect of standard initializations and the value of depth, the underlying loss surface, and the origins of generalization."
SP:7225825e353b711a7d023f706fafe5e17e4e2fb2,"This paper proposes a new method to tackle the image - to - image translation problem. The proposed method is based on the Generative Adversarial Network ( GAN ), which is a GAN - based method for generating images. The main idea is to use an attention mechanism to estimate the probability that the discriminator's input is real, and then use it to draw an attention map that highlights the critical features for such prediction. The attention map is then used to guide the generator to produce more plausible and realistic images.   The proposed approach is evaluated on a number of image transfer tasks. The experimental results demonstrate the superiority of the proposed method over other methods that have been used in the past."
SP:41c089ba65393174dae1dc136f79030a0a4fc532,"This paper proposes a new perspective on the role of multiplicative interaction layers in neural networks. The authors claim that the presence of such primitive operations in neural network architecture is under - appreciated, and argue that they should be considered in many situations where multiple compute or information paths need to be combined, in place of the simple and oft -used concatenation operation. They also conjecture that multiplicative interactions offer a particularly powerful inductive bias when fusing multiple streams of information or when conditional computation is required.   The main contributions of the paper are thus : ( i ) to re - establish the connection between hypernetworks, gating, multiplicative RNNs and their design principles ; ( ii ) to aid the community community’s understanding of other models, such as hyper - nets and gating networks, through them ; ( iii ) to show their efficacy at representing certain solutions ; and ( iv ) to empirically apply them to large scale sequence modeling and reinforcement learning problems, where they demonstrate state - of - the - art results."
SP:5144391584e6d3825e12684b7c053e4e282cff2b,"This paper presents a new algorithm for active learning, called Batch Active Learning by Diverse Gradient Embeddings ( BADGE ), which samples groups of points that are disparate and high magnitude when represented in a hallucinated gradient space. The goal is to incorporate both predictive uncertainty and sample diversity into every selected batch. Crucially, BADGE trades off between uncertainty and diversity without requiring any hand - tuning hyperparameters. The authors show that BADGE is robust to architecture choice, batch size, and dataset, and generally performs as well as or better than the best baseline across all of the environmental conditions.   The main contributions of this paper are as follows :   1. Introducing the notation and setting of the BADGE algorithm in Section 3 and experiments in Section 4. 2. Demonstrating the robustness of BADGE across all the aforementioned environmental conditions in Section 5. 3. Providing empirical evidence that the proposed algorithm is applicable to real world active learning problems in Section 6."
SP:ce6023b1e6bf45b071a6f5457b2575425ae03366,This paper proposes a self - explanation model for deep neural networks. The authors claim that deep neural network ( DNNs ) are hard to interpret because each hidden layer carries a mix of low level features and high level features. They propose a novel feature leveling architecture that isolates low level feature from high level feature on a per - layer basis to better utilize the GLM layer in the proposed architecture for interpretation. The proposed method is based on two main components : ( 1 ) feature extraction and ( 2 ) feature leveling.   The first part of the paper introduces the idea of self - explaining models and introduces a toy example to illustrate the effect of the mixture effect of features in hidden layers. The second part gives a detailed account of the feature leveling network that could effectively isolate different levels of features. The author also provides a high level introduction to some related works that motivated their architectural design. The experimental results show that the proposed method achieves competitive performance on various real world datasets and improves interpretability.
SP:b70ceead1bf6c7dc684c74501716e7012b891022,"This paper proposes a method for training a classifier over a large number of classes, known as ‘ extreme classification ’, which has become a topic of major interest with applications in technology, science, and e -commerce. Traditional softmax regression induces a gradient cost proportional to the number of class classes C, which often is prohibitively expensive. A popular scalable softmax approximation relies on uniform negative sampling, which suffers from slow convergence due to a poor signal - to - noise ratio. In this paper, the authors propose a simple training method for drastically enhancing the gradient signal by drawing negative samples from an adversarial model that mimics the data distribution.   The paper proposes three main contributions : ( i ) An adversarial sampling mechanism that produces negative samples at a cost only logarithmic in C, thus still resulting in cheap gradient updates ; ( ii ) a mathematical proof that this adversarial sampler minimizes the gradient variance while any bias due to non - non - un - uni - norm is eliminated ; and ( iii ) experimental results on large scale data sets that show a reduction of the training time by an order of magnitude relative to several competitive baselines."
SP:29b52fee83309268d9864f3b1fc3617948577d41,"This paper presents a method for learning a low dimensional encoding of the environment learned with a combination of model based and model - free objectives. The method leverages intrinsic rewards that are based on a weighted distance of nearest neighbors in the low dimensional representational space to gauge novelty. The authors leverage these intrinsic rewards for sample - efficient exploration with planning routines in representational spaces. One key element of their approach is that they perform more gradient steps in - between every environment step in order to ensure the model accuracy is high ( hence ensure an accurate and novelty heuristic ). They test their approach on a number of maze tasks, as well as a control problem and show that their exploration approach is more sample -efficient compared to strong baselines."
SP:257c98dc1a9f3efcbf9544d9ee2ff524b000543d,"This paper studies few - shot classification and out - of - distribution detection in the setting where only a few labeled examples are available for each class. The authors propose two new methods for the detection task, which are based on the popular few - shots classification dataset and a few benchmark datasets. The first method is based on standard methods for detection of out of distribution ( OOD ), i.e., if a classifier has access to a large set of examples, it should be able to detect OOD, even if the examples do not belong to the class that it is trained to classify. The second method, based on a neural network, is a bit different from the standard OOD detection method, in that the neural network is not trained to predict which examples belong to which class, but rather to predict the probability that an example belongs to a class that does not appear in the dataset. The neural network must be trained to discriminate between two classes based on their similarities and differences in similarities of the examples. The proposed methods are applied to four popular few shot classification datasets, and the authors compare their methods with the standard detection methods and two new ones. The results show that the proposed methods perform better than the existing detection methods."
SP:a3632b773143dfb3a8f104c6b658dfa1167d155b,"This paper proposes a generalized model of sequence generation that unifies decoding in directed and evaluating undirected sequence models. The proposed framework models the process of generation rather than a resulting sequence, and under this framework, various neural sequence models are derived as special cases, such as autoregressive, semi - autorocentric, and refinement - based non -autoregressive models. This unification enables the proposed approach to adapt decoding algorithms originally developed for directed sequence models to undirectED models to generate sequences directly from them. The authors demonstrate this by various decoding strategies for a cross -lingual masked translation ( Lample and Conneau, 2019 ), and show that generation from the proposed model, under their framework, is competitive with the state of the art on WMT’14 English - German translation. The experiments reveal that the proposed method can do constant - time translation with the budget as low as 20 iterations ( equivalent to generating a sentence of length 20 in the conventional approach ) while achieving similar score to linear - timetranslation from the same masked translation model."
SP:eca5e2be9831dfb79c4f5e633cbfadcfd2e00eb1,"This paper proposes a two - stage method for generating mathematical expressions ( MEs ) recognition using LaTeX sequences. The first stage is based on object detection, where an object detection algorithm is used to generate an image of the input image, and the second stage uses an attention mechanism to translate the math symbols into LaTeX with position information. The authors compare the performance of the proposed method with the end - to - end method and a baseline method based on a neutral network. The experimental results show that the proposed methods perform better than the baseline method in terms of accuracy and generalization ability."
SP:923fee8623da1569a7f54a57b4b326f29440b4c0,"This paper proposes a quantization method for reducing the memory footprint of convolutional neural network architectures. The main contribution is a vector quantization approach that aims at preserving the quality of the reconstruction of the network outputs rather than its weights. The method only requires a set of unlabelled data at quantization time and allows for efficient inference on CPU by using bytealigned codebooks to store the compressed weights. They validate their approach by quantizing a high performing ResNet-50 model to a memory size of 5 MB ( 20× compression factor ) while preserving a top - 1 accuracy of 76.1 % on ImageNet object classification and by compressing a Mask R - CNN with a 26× factor. They show that applying their approach to the semi - supervised ResNet - 50 of Yalniz et al. ( 2019 ) leads to a 5 MB memory footprint and a 76 % top -1 accuracy on image classification. Moreover, their approach generalizes to other tasks such as image detection."
SP:74850ad70241948f93fed95ba1f0ac11360437c1,"This paper presents a Transformer - based model for solving the Mathematics Dataset. The model is based on the Tensor Product Transformer ( TP - Transformer ). The authors propose a novel attention mechanism, called TP - Attention, which explicitly encodes the relations between each Transformer cell and the other cells from which values have been retrieved by attention. The paper shows theoretically how the proposed attention mechanism avoids the ambiguity introduced by multiple layers of standard attention in the mathematics dataset ( Sec. 6 ). In Sec. 7, it is shown theoretically how multiple layer attention suffers from the binding problem, and it is theoretically demonstrated theoretically how TP - attention avoids such ambiguity ( Sec. 8 ).   The paper concludes with a discussion of related work, and a conclusion (Sec. 9 )."
SP:d319df820c6630c409fab32097652a083e8f53ea,"The paper considers the problem of generalization between training and inference on identically distributed training and test sets. The training data may not be representative of the empirical data set, which consists of real - world input samples. To address this problem, the authors first reformulate a learning algorithm as a procedure for searching for a source code that maps input features to classes. They then derive a necessary and sufficient condition for generalization using a universal cognitive similarity metric, namely information distance, based on Kolmogorov complexity. Using this condition, they formulate an optimization problem to learn a more general classification function. To achieve this end, they extend the input features by concatenating them with encodings of the classifier. The authors then conduct extensive experiments to demonstrate that a model trained on this encoder is more robust to adversarial perturbations and Gaussian noise than one trained on uncoded input features."
SP:b8e86f5e89330d81ba4967a7ed2dbfb56375d8a0,"This paper proposes a new graph pooling method based on compressive Haar transforms. The proposed method, HaarPooling, is based on following a chain of sequential clusterings of the input graph. The input of each pooling layer is transformed by the compressed Haar basis of the corresponding clustering layer and operates in the frequency domain by the synthesis of nodes in the same cluster and filters out fine detail information by applying the Haar transform. The authors provide experimental results on benchmark graph classification tasks and compare the proposed Haar Pooling method with existing graph - based pooling methods.   The paper is organized as follows :   1. A summary of the paper. 2. The main contributions. 3. The experimental results."
SP:17bea301d6718ef5f28864dd2445552b3cf65eeb,"This paper presents a point cloud decoder architecture based on sample - based decoders that map a shape representation to a point feature distribution, allowing an arbitrary number of sampled features to be transformed into individual output points. The main contribution of this paper is to study how to adapt PointNet encoder networks to match the semantics of their input point clouds, which has been shown to improve their effectiveness over naive feedforward alternatives. The vast majority of work on point cloud encoders are still based on fully connected networks that map shape representations to a fixed number of output points, whereas this paper proposes to use sample based decoder architectures based on a fully connected network that maps a feature distribution to a set of sampled points.   The paper develops three sample based encoder architectures and compares their performance to each other and show their improved effectiveness over feedforward architectures. In addition, the authors investigate the learned distributions to gain insight into the output transformation of the decoder to obtain a novel decoder algorithm, which they call the ‘NoiseLearn ’ algorithm. This paper is available as an extensible platform to reproduce these results and serve as a baseline for future work."
SP:51d826ead5d1d9cb89d493ce4c39728651bbc57b,"This paper presents a comprehensive study of deep learning with real - world noisy data across a variety of noise levels, architectures, methods, and training settings. The main contributions are as follows :   ( 1 ) Deep Neural Networks ( DNNs ) generalize much better on real world noise than synthetic data. Specifically, the paper shows that when networks are fine - tuned, they are able to generalize better than when networks without fine - tuning are used. ( 2 ) Deep neural networks may not learn patterns first on real-world noisy data, and vice versa. ( 3 ) Real - world noise appears to be less harmful, yet it is more difficult for robust DNN methods to improve. ( 4 ) Robust learning methods that work well on synthetic noise may not work as well on real - World noise, or vice versa ; ( 5 ) Real world noise seems to be more harmful than synthetic noise, but it is not as harmful as the synthetic noise.   The authors conduct a large - scale study across all the noise levels and types, and across several different learning methods. They conduct a benchmarking study with 10 controlled noise levels to establish a benchmark of real world noisy labels at 10 controlled levels. They also conduct a study with different network architectures, training methods and settings to establish their findings. They compare their findings with the state - of - the - art in terms of generalization and robustness."
SP:9873f78fb2821afdbb5551700e6ab6a0e8bcb9f0,"This paper proposes a method for collecting human supervision for supervised learning that combines rule - level and instance - level supervision. The supervision is composed of clean and noisy supervision, where the clean supervision is in the form of a set of rules and the instance level supervision is a combination of instance labels. The authors propose a training algorithm that jointly denoises rules via latent coverage variables, and trains the model through a soft implication loss over the coverage and label variables. The denoised rules and trained model are used jointly for inference. Empirical evaluation on five different tasks shows that ( 1 ) the algorithm is more accurate than several existing methods of learning from a mix of clean supervision and supervision, and ( 2 ) the coupled rule - ex - exemplar supervision is effective in denoising rules."
SP:6f2c656dbb7629f652a4291d6971625184d8118b,"Graph neural networks ( GNNs ) are a class of deep models that operate on data with arbitrary topology represented as graphs. This paper proposes an efficient memory layer for GNN that can jointly learn node representations and coarsen the graph. Based on this proposed memory layer, two new networks are introduced : memory - based GNN ( MemGNN ) and graph memory network ( GMN ). The experimental results show that the proposed models achieve state - of - the - art results in 8 out of 9 graph classification and regression benchmarks. The learned representations could correspond to chemical features in the molecule data."
SP:81bc52d734c86975d741b6482d65ca71a9d81620,"This paper analyzes the effect of initial initialization in deep linear networks on the convergence of neural networks. The authors prove for the first time that orthogonal group speeds up convergence relative to the standard Gaussian initializations when the initialization weight scales linearly in the depth of the network. They show that for deep networks, the width needed for efficient convergence to a global minimum with Orthogonal initializations is independent of the depth, whereas the width of initializations with Gaussian generalization is linearly to the depth. They also show that Gaussian initialization leads to exponentially long convergence time if the width is too small compared with the depth in deep networks. They perform experiments to support their theoretical results and demonstrate the benefits of a good initialization."
SP:9f5d95fc89c2f0d59d04838aa180f3db67997dfa,"This paper proposes a method to optimize the bit allocation of weights and activations for compression of deep neural networks. The authors claim that previous methods assign equal bit rate to quantize weights in all layers, which is not reasonable in the case of high rate compression ( such as 2 - bit quantization ), as some layers in deep neural network are sensitive to quantization and performing coarse quantization on these layers can hurt the accuracy. The main contribution of this paper is to address an important problem of how to optimize bit allocation for deep CNNs compression.   The authors first explore the additivity of output error caused by quantization in CNNs. They find that additivity property holds for neural networks which are continuously differentiable in the layers. Based on this observation, the authors formulate the optimal bit allocation problem of weight and activation bit allocation. They develop their bit allocation framework and report experimental results. Section 5 discusses the bit inference rate and Section 6 discusses the impact of inference rate of quantization."
SP:7191d7b217a12b1bf9c47d790896a8227c14cc3d,"This paper proposes a novel inference framework for inference of generative adversarial networks ( GANs ) based on variational inference to fuse variational auto - encoders and variational generative networks ( VAEs ). The main contribution of this paper is to establish the generalization error bound of iWGANs and to provide a rigorous probabilistic interpretation of the model under the framework of maximum likelihood estimation.    The main contributions of the paper are as follows :   1. Establishes a principled framework to fuse the variational variational encoder and generative generative network, i.e., the iterative primal dual optimization process, to jointly learn an encoder network and a generative model. 2. Develops a competitive and stable performance with state - of - the - art results on two benchmark datasets. 3. Demonstrates that the proposed method is able to match the performance of VAEs with a clear stopping criteria, which has many advantages over other autoencoder - based methods. 4. Extensive experimental results are presented to demonstrate the effectiveness of the method."
SP:cca6ae14fd0dd12352855e594acf7f3263bb1f24,"This paper proposes a new model of annotation based on MPA ( Multi - Approached Private Annotation ), which is based on a partially pooled structure ( based on stick breaking process ). The proposed model is designed to alleviate the effects of sparsity inherent in some crowdsourcing environments. Specifically, the proposed model uses a nonparametric partially pooled structure, which fits jointly with the ability of the annotators hierarchical community profiles. The individual estimates can thus be improved using information about the community about the data when the data data is scarce. The evaluation is conducted on the Phrase Detectives 2 corpus, in various levels of the sparsity, assessing the accuracy of the inferred mention pairs, the quality of the post - hoc constructed silver chains, and the viability of using silver chains as an alternative to expert - annotated chains when training a state - of - the - art coreference system. The paper also provides insights into the inferred community profiles of a few known spammers and honest players of the game used to collect data."
SP:4295cae4a56a02eb21c486408c1bf37a7483cb49,"This paper proposes a new method for sparse reward reinforcement learning ( DRL ) based on successor feature control ( SFC ). SFC is a general type of reward that is general and not task - specific. It takes into account statistics over complete trajectories and thus differs from previous methods that only use local information to evaluate intrinsic motivation. This paper proposes to learn separate intrinsic and extrinsic task policies and schedule between these different drives to accelerate exploration and stabilize learning. The experimental results show that SFC and the hierarchical exploration framework SID can be incorporated into existing DRL methods with minimal computation overhead.   The authors evaluate their proposed method in three different environments with pure visual inputs : VizDoom, DeepMind Lab and DeepMind Control. The results show a substantially improved exploration efficiency with SFC."
SP:9fa22eb03a79bce0fc1c8e84ae8640e010701eca,"This paper presents a method for weakly - supervised video retrieval based on a multi - level attention mechanism to learn richer multimodal representations. The method is based on two components : a frame - by - word interaction module and a word - conditioned visual graph ( WCVG ) module. The module is composed of a Frame - By - Word interaction module as well as a Word - Conditioned Visual Graph. The authors also incorporate a novel application of positional encodings, commonly used in Transformers to learn visual - semantic representations that contain contextual information of their relative positions in the temporal sequence through iterative message - passing. The proposed method is evaluated on the DiDeMo dataset and CharadesSTA dataset. The results show that the proposed method outperforms the state - of - the - art weakly supervised method by a significant margin but also obtains an improvement of 10 % for the Recall@1 accuracy metric over strongly - supervised methods."
SP:27ac670353f34ee7a23bb7622f80c1dfbc0985e0,"This paper proposes a method to learn an object - specific deep neural network to synthesize the view - dependent appearance of an object using an RGB video of the object. The goal is to generate photo - realistic re - rendering of reconstructed objects for virtual and augmented reality applications. The method is a hybrid between image - based rendering and GAN - based image synthesis. The source image is a 3D stereo view of the target object and the output is a composite of multiple reprojected images. The composition of the reprojected views to a final output image without the need of hand - crafted blending schemes is enabled using a network called CompositionNet.   The core contribution is the explicit handling of view-dependent effects in the source and the target views using EffectsNet that can be learned in a self - supervised fashion. The network does not have to allocate capacity on “remembering ” object appearance, instead it learns how to combine the appearance of captured images. This is similar to the approach used in ImageNet. The main difference is that the source image can be converted to diffuse images which can be projected into other views. This warping assumes diffuse surfaces, in case of view - dependence effects, in the case of target view - independent effects, the pipeline reinserts the new views into the target view. To learn the composition network, the authors learn a composition network that outputs photo - realistic results. The authors demonstrate the effectiveness of their approach both qualitatively and quantitatively on synthetic data."
SP:257d124367b1da9a595dc11a9df750d6bade298e,"This paper presents a method for estimating uncertainty for deep neural networks ( DNNs ) that relies on an inverse formulation of the Multivariate Normal Distribution ( MND ), which is an information form. The authors show that the model uncertainty can be estimated in this form using a scalable Laplace Approximation scheme, which involves a diagonal correction of the Kronecker - factorized eigenbasis. As this makes the inversion of the information matrix intractable an operation that is required for a full Bayesian analysis, the authors devise a novel low - rank approximation of this eigenbase that exploits spectral sparsity of DNN. The methods are provided that develops into a memory - wise tractable sampling computations. Empirical evaluations over various benchmarks show the superiority of this approach over existing methods."
SP:2e03ceba4004b82f86f8349352a8ee4520e9c35d,"This paper proposes a new method, called AHash, to solve the min - wise hash problem. The authors claim that AHash is more efficient than Minwise Hashing ( OPH ) and other methods for computing MinHash values. The main reason for this is that OPH has the drawback that the load of the bins in a MinHash is not balanced, which leads to the existence of empty bins and false similarity computation. The proposed method AHash aims to balance the load so that the number of elements in a bin can be balanced so that there are as few empty bins as possible. To this end, the authors propose a method, Amortization Hashing, which can generate as many empty bins in each MinHash as possible without hurting runtime efficiency compared with OPH and densification strategies.   The authors compare AHash with two existing methods, Minwise Hash and OPH, and two new methods, AHash - D and AHash + D. They compare the performance of AHash on real datasets on MNIST, CIFAR-10, and Fashion MNIST. They also compare their method with two new datasets, MNIST and Fashion - MNIST - COCO. The experiments show that their method outperforms the other methods."
SP:d73827ab98b0ff6bd92abfefea43a5f88ea40de2,This paper proposes a method for extracting features from phase shift data by adding a graph structure to each data point and constructing a suitable machine learning architecture for graph data with a neural network. The proposed method is based on the cyclic permutation to a graph neural network ( CNN ) and the authors claim that it is able to identify the order in which the data points are collected. The method is tested on simulated data and experimental data obtained from a test setup. The experimental results show that the proposed method performs better than the baselines.
SP:0df5ad333eb4ff9cca7f2d117909e2ce533a65d8,"Neural conditional text generation systems have achieved significant progress in recent years, showing the ability to produce highly fluent text. However, the inherent lack of controllability in these systems allows them to hallucinate factually incorrect phrases that are unfaithful to the source, making them often unsuitable for many real world systems that require high degrees of precision. In this work, the authors propose a novel confidence oriented decoder that assigns a confidence score to each target position, and this score is learned in training using a variational Bayes objective, and can be leveraged at inference time using a calibration technique to promote more faithful generation. Experiments on a structured data - to - text dataset – WikiBio ( Lebret et al., 2016 ) – show that our approach is more faithful to source than existing state - of - the - art approaches, according to both automatic metrics and human evaluation."
SP:03307deac29173b2968fbd08f95fc77eb1f82410,"This paper proposes a new pruning method based on the observation that magnitude - based pruning minimizes the Frobenius distortion of a linear operator corresponding to a single layer. Based on this observation, the authors propose to extend the single layer optimization to a multi - layer optimization. The proposed method, called lookahead pruning, is tested on various networks, including VGG and ResNet, particularly in the high - sparsity regime. The authors also introduce a functional approximation perspective toward MP and its variants as a generalization of MP."
SP:dc80fdc75bc14ae19fe4ba9b85c35ce00b12856f,"This paper introduces Moniqua, a method for decentralized stochastic gradient descent ( SGD ) based on graph - based communication between parallel workers. The authors prove theoretically that the method achieves the same asymptotic convergence rate as the original SGD with full - precision communication. They show empirically that the proposed method does not require any additional memory for non - convex objectives and does not support biased or linear quantizers. They also show that the communication is robust to very low bit - depth and very high bit - radius.   The main contributions of the paper are as follows :   1. The paper proposes a method that allows decentralized SGD to use quantized communication. The method is based on Graph - based Graph Communication ( GBC ), where parallel workers are connected to form a graph and communicate adjacently. In contrast to SGD, where workers are not connected to each other but only to a fixed number of bits, the authors propose to use a technique that allows them to communicate a provably bounded number of bit per iteration. This allows the communication to be more robust to bias and linear quantization. 2. They provide experimental results that show that their method is able to achieve the same convergence rate with respect to wall - clock time as SGD without any extra memory. 3. They compare their method with the original GBC method and show that it achieves similar or slightly better results."
SP:86c61a658d07ab86e2d84cef7e480bf7a06e4ddb,"This paper introduces a family of partial models that are provably correct, yet remain fast because they do not need to fully model future observations. The authors claim that partial models can be causally incorrect because they are confused by the observations they don’t model, and can therefore lead to incorrect planning. To address this, they introduce a general family of models that is provably causally correct but remains fast because it is not required to model all of the observations.   The authors first propose two partial models for reinforcement learning. The first is based on MDPs learned in illustrative environments, and the authors empirically investigate their effects on models learned in two environments. In the first environment, they learn a model of the environment and use it to jointly model the rewards and rewards of the agent ’s previous actions. The second environment, where the agent has access to only the observed data, they show that the proposed partial models are more accurate than the other partial models. The experiments are conducted on two environments, one in which the agent is only given the observations and the other in which it is given the full set of observations, and they compare the performance of the proposed models."
SP:c70479b2096a52584b242de58272ca8d8565feea,"This paper proposes a new variational autoencoder ( VAE ) model for distributed simulation ( DS ) and channel synthesis ( CS ) problems, where the objective is to learn a common representation of two correlated data variables ( e.g., a shared concept ) and capture the remaining randomness in respective data variables by imposing the mutual information between the data variables and the common representation as a regularization term. The proposed model is based on the Wyner VAE model, which is a variant of the VAE with additional encoder components introduced by the authors. The encoder is designed to decompose a data vector into the common representations and the local representation, which can be used for sampling with style manipulation. The experimental results show that learning a succinct common representation achieves better generative performance in DS and CS tasks compared to existing VAE variants."
