paper_id,summary
SP:b19df5243359791fbaad005d6f13d7e9fdb0ff63,This paper proposes a method to decompose joint action spaces into restricted role action spaces by clustering actions according to their effects on the environment and other agents. The authors also integrate information about action effects into the role policies to boost learning efficiency and policy generalization. The proposed method outperforms the current state-of-the-art MARL algorithms on 9 of the 14 scenarios that comprise the challenging StarCraft II micromanagement benchmark and achieves rapid transfer to new environments with three times the number of agents.
SP:7deb61890d97422a0fe141ca807f968c70ab239a,"This paper studies the behavior of the stochastic subgradient descent (SSGD) method applied to over-parameterized nonsmooth optimization problems that satisfy an interpolation condition. By leveraging the composite structure of the empirical risk minimization problems, the authors prove that SSGD converges, respectively, with rates O(1/ ) and O(log(\1/)) for convex and strongly-convex objectives when interpolation holds. These rates coincide with established rates for the smooth problems. The analysis provides a partial explanation for the empirical observation that sometimes SGD and SSGD behave similarly for training smooth and nonsmoothened machine learning models."
SP:c7e0b3fedc0d0409d662dd612b529fdacad2b03e,"This paper investigates the effect of randomly initialized layers in transformers on the performance of the model. The authors introduce the idea of “reservoir layers”, i.e., random layers that are interspersed with regular transformer layers, and show that freezing these random layers can improve the performance and reduce the wall-clock time until convergence. The experiments are conducted on machine translation (MT) and masked language modeling tasks."
SP:ba9f1d4738ec67a440346f3ac6c4cf35f7232077,"This paper proposes a new approach (filtrra) to construct steerable convolutional networks based on group representation theory. In particular, the authors show that kernel constructed by filter transform can be interpreted in the group representations theory. They then show that filter transformed kernels can be used to convolve input/output features in different group representations. This interpretation provides a novel and simple approach to implement steerable CNN operators. The experimental results show the feasibility of the proposed approach on classification and regression tasks."
SP:c1116fbb4d058eb6be195b5d13d19a55ba86b602,"This paper proposes an optimal neural synthesis approach for multimodal program synthesis, where the goal is to find a program that satisfies user-provided constraints while also maximizing the program’s score with respect to a neural model. Specifically, the user intent is expressed using combination of natural language (NL) and input-output examples. At the core of the method is a top-down recurrent neural model that places distributions over abstract syntax trees conditioned on the NL input. This model not only allows for efficient search over the space of syntactically valid programs, but it also allows us to leverage automated program analysis techniques for pruning the search space based on infeasibility of partial programs. As a result, our synthesizer can more aggressively search more aggressively than prior work and significantly speed up search."
SP:55e02d79146bbb42f1ab6d4fafa2db5ddbe599b0,"This paper presents a protein graph convolutional neural network (PGCN) that uses a structure-based molecular interaction graph generated using the Rosetta energy function that describes the topology and energetic features, to determine the substrate specificity. The proposed method is able to recapitulate and predict the specificity of the NS3/4 protease from the Hepatitic C virus. The model is capable of both prospective prediction of chosen protease enzymes and generating novel designed enzymes with tailored specificity against disease targets."
SP:7727eeb7b17ad94ddfa0cf24e64a9626d83a8876,"This paper studies the underestimation bias in double Q-learning, a classical method for reducing overestimation in deep Q learning. The authors show that underestimation may lead to multiple non-optimal fixed points under an approximate Bellman operator. To address this issue, the authors propose a simple but effective approach as a partial fix. The approach leverages an approximate dynamic programming to bound the target value. Experiments on the Atari benchmark tasks demonstrate its significant improvement over baseline algorithms."
SP:1d630b69f95392a5ef3d7d580b523e077a3555a8,"This paper proposes a two-step training framework for deep generative models (DGMs) of high-dimensional natural images. First, it generates images in low-frequency bands by training a sampler in the wavelet domain. Then, it super-resolves these images back to the pixel-space with a novel wavelet super-resolution decoder network. The proposed method achieves a FID of 10.59 on ImageNet."
SP:b943a73b1ec34867371325748dc3a91ff4011947,"This paper provides a theoretical analysis of self-supervised learning (SSL) for few-shot learning (FSL). In particular, the authors analyze the performance of SSL-based methods on FSL tasks. They show that the loss function of SSL is an upper bound of the supervised metric loss function for FSL. They further show the main difference between the supervised training and self-training loss functions, and provide a bound for the gap between the two losses. Finally, they propose potential ways to improve the test accuracy under the setting of FSL, such as increasing the batch size of the key-value dictionary and the contrastive loss."
SP:bd552f98e6a447cefa6b1a9bbdf40bc6539fb643,"This paper studies the convergence properties of two-layer teacher-student neural networks with finite width. In particular, the authors show that under the most basic settings, all student neurons must align with the teacher neuron at any local minima. The methodology is extendable to more general cases, where the proof can be reduced to analyzing the properties of a special class of functions that we call angular distance (AD) function. The authors demonstrate that these properties can be easily verified numerically."
SP:0f62846913ec10b44ed32845770da0565479dc75,This paper proposes a method to combine knowledge representation and reasoning in neuro-symbolic computing with deep neural networks. The key idea is to represent declarative knowledge as assertions in first order logic. The relations and functions that make up the vocabulary of the domain are implemented by neural networks that can have arbitrary structure. The logical connectives in the assertions compose these networks into a single deep network that is trained to maximize their truth. The method is evaluated on a toy problem and a visual relationship detection task.
SP:2f19259d65fab904c1b771244da3dcb2f8aa0c26,"This paper studies the effect of regularization and gradient regularization on the learning of iterative solutions in ResNets. The authors propose two regularizations to encourage iterative convergent computation and test whether this provides a useful inductive bias. To make the networks more iterative, they manipulate the degree of weight sharing across layers using soft gradient coupling. They also impose a Lipschitz constraint on the residual functions using spectral normalization. However, neither of these regularizations improve classification accuracy on standard visual recognition tasks."
SP:6c14506b8b2b06043409d912e6bf877651aaa665,"This paper proposes two normalization techniques for improving out-of-distribution (OOD) generalization of deep neural networks. The first technique, SelfNorm, uses attention to recalibrate the channel-wise mean and variance of feature maps, while the second technique, CrossNorm, exchanges the statistics between feature maps. Experiments on different domains (vision and language), tasks (classification and segmentation), and settings (supervised and semi-supervised) show the effectiveness of the proposed methods."
SP:2774abdc11917321dd4994af0f0da1ff824bea03,"This paper proposes a simple yet effective architecture for encoding image pixels in vision-based RL. Specifically, the authors propose R-LAtte: Reinforcement Learning with Attention module, which augments a simple attention module in the convolutional encoder of an RL agent. The proposed module can extract interpretable task-relevant information such as agent locations and movements without the need for data augmentations or contrastive losses, and significantly improve the sampleefficiency and final performance of the agents."
SP:31a7051d08d19c01e11f1fac2f3041ed2fa28f15,"This paper proposes an extension of GradNorm, a widely used gradient-based approach for training multi-task neural networks, by dynamically homogenizing not only the gradient magnitudes but also their directions across tasks. Specifically, it adds a layer of task-specific rotation matrices that aligns all the task gradients. Importantly, it also analyzes GradNorm through the lens of game theory, providing theoretical guarantees on the algorithm stability and convergence. The experiments on several real-world datasets and network architectures show that the proposed method outperforms previous approaches for multitask learning."
SP:ac9ebd027b92527d9a87b13ad11d002d99a2b0f6,"This paper proposes a geometry-distortion constraint for unsupervised image-to-image translation. The proposed constraint promotes the consistency of geometry structures and reduces the unwanted distortions in translation by reducing the randomness of color transformation in the translation process. To facilitate estimation and maximization of MGC, the authors propose an approximate representation of mutual information called relative Squared-loss Mutual Information (rSMI) that can be efficiently estimated analytically. The effectiveness of the proposed constraint is demonstrated on several benchmark datasets."
SP:92a38d7d18f07f68b8f93c61180e2cc1dddd21de,"This paper studies the effect of point sampling patterns in point cloud GANs. It shows that sampling-insensitive discriminators (e.g. PointNet-Max) produce shape point clouds with point clustering artifacts, while sampling-oversensitive discriminator fail to guide valid shape generation. The authors propose the concept of sampling spectrum to depict the different sampling sensitivities of discriminators. They further study how different evaluation metrics weigh the sampling pattern against the geometry and propose several perceptual metrics forming a sampling spectrum of metrics. Guided by the proposed sampling spectrum, they discover a middle-point sampling-aware baseline discriminator, which improves all existing point cloud generators by a large margin on sampling-related metrics."
SP:16c4be3eb162bc81cb3343c2fc115eb8e926a5b5,"This paper investigates the adversarial robustness of CapsNets (CapsNets), a recently proposed neural network architecture that is shown to be more robust to white-box attacks than CNNs under popular attack protocols. The authors propose a novel vote attack where they attack the output capsules directly and integrate the attack into the detection-aware attack paradigm, which can successfully bypass the class-conditional reconstruction based detection method. Extensive experiments demonstrate the superior attack performance of the proposed attack method."
SP:dbd093dff7a38ba8882bb8119c34623ddaaf4cc6,"This paper studies the problem of meta-reinforcement learning in the online adaptation setting, where a single policy is trained for a fixed family of tasks and the policy must balance exploration (or probing), to reduce the uncertainty about the current task, and exploitation to maximize the cumulative reward of the task. The paper proposes a new algorithm that uses privileged information in the form of a task descriptor at train time to improve the learning of recurrent policies. This approach significantly reduces the learning sample complexity without altering the representational power of RNNs, by focusing on the relevant characteristics of the tasks and by exploiting them efficiently. The authors evaluate their algorithm in a variety of environments that require sophisticated exploration/exploitation strategies and show that it outperforms vanilla RNN, Thompson sampling and the task-inference approaches to meta reinforcement learning."
SP:bd89d254fbf31db61db237d08ab42981e27c52df,"This paper proposes an adaptive policy learning method for offline reinforcement learning in sequential recommendation systems (SRS). In particular, the authors propose to learn an RL policy that can adapt to diverse simulators generated by the offline dataset. The adaptive policy is suitable to real-world environments where dynamics are changing and have stochasticity in the offline setting. Experiments are conducted in synthetic environments and a real world ride-hailing platform. The results show that the method overcomes the distortion problem and produces robust recommendations in the unseen real world."
SP:1a166b28cf684e0d5759bd629f6a53370d2bf11c,"This paper proposes a method for learning goal-reaching policies from scratch, without the need for expert demonstrations or a value function. The authors leverage the property that any trajectory is a successful demonstration for reaching the final state in that trajectory. They propose a simple algorithm in which an agent continually relabels and imitates the trajectories it generates to progressively learn goal- reaching behaviors from scratch. Each iteration, the agent collects new trajectories using the latest policy, and maximizes the likelihood of the actions along these trajectories under the goal that was actually reached, so as to improve the policy. They formally show that this iterated supervised learning procedure optimizes a bound on the RL objective, derive performance bounds of the learned policy and empirically demonstrate improved performance and robustness over current RL algorithms in several benchmark tasks."
SP:c306530164d677e670554eeba8203c66bb3d9f7a,"This paper proposes two improvements to FastSpeech, a non-autoregressive text-to-speech (TTS) model that uses an autoregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in output). The proposed improvements are: (1) directly training the model with ground-truth target instead of the simplified output from teacher, and (2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Experimental results show that the proposed improvements achieve a 3x training speed-up and faster inference speed."
SP:79e9fb20d383816f54738ce70d137131ebc10290,"This paper studies the unsupervised dimension reduction (UDR) problem in the language of tempered distributions, i.e. approximating an empirical probability density function pemp(x) by another tempered distribution q(x), whose support is in a k-dimensional subspace. The authors propose a nonnegative penalty function R(f) that “forces” the support of f to be k-dim. Then they present an algorithm for minimization of I(q) + \lambdaR(f), based on the idea of two-step iterative computation. They demonstrate the method on 4 examples (3 UDR and 1 SDR)."
SP:93e54522e6c2b805905d21fc968fc40866f2898b,This paper studies the trade-off between robustness and sensitivity in deep neural networks. The authors propose Feature Contrastive Learning (FCL) that encourages the model to be more sensitive to the features that have higher contextual utility. The utility is defined as the change in the loss function when we perturb a specific input feature. The sensitivity and utility are context dependent and change from one image to another. FCL is shown to achieve a better balance of robustness/sensitivity.
SP:f03c50f15022c4f56ac2b3085354ffed38ad1145,"This paper proposes a method for imitation learning from high dimensional observations of an expert performing a task. The method is based on adversarial learning with a latent representation inside the discriminator network. The latent representation is regularized through mutual information constraints to incentivize learning only features that encode information about the completion levels of the task being demonstrated. This allows to obtain a shared feature space to successfully perform imitation while disregarding the differences between the expert’s and the agent's domains. Empirically, the method is able to efficiently imitate in a diverse range of control problems including balancing, manipulation and locomotive tasks."
SP:ef18f4188426bc01be309633b486884b0e7a81a4,"This paper studies the performance of training a pruned neural network with accelerated gradient descent (AGD) in the lottery ticket hypothesis (LTH) setting. The authors analyze the geometric structure of the objective function and the sample complexity to achieve zero generalization error. They show that the convex region near a desirable model with guaranteed generalization enlarges as the neural network model is pruned, indicating the structural importance of a winning ticket. Moreover, when the algorithm is specified as an (accelerated) stochastic gradient descent algorithm, the number of samples required for achieving zero generalisation error is proportional to number of non-pruned weights in the hidden layer. "
SP:eed6cb2f8caed39f8295f4aeb6e044c2ac981c4d,"This paper proposes a method for label smoothing for data augmentation. The authors propose a distance-based approach where the training labels are smoothed to different extents based on the distance between the augmented data and the clean data. The proposed method is evaluated on CIFAR-10, Cifar-100 and ImageNet datasets and shows improvements in accuracy and calibration performance."
SP:0d5017e1a405bf86e3bac40e6e59886d4bf48450,"This paper studies self-supervised representation learning from a causal perspective. The authors propose a novel objective, Representation Learning via Invariant Causal Mechanisms (RELIC), that enforces invariant prediction of proxy targets across augmentations through an invariance regularizer which yields improved generalization guarantees. The RELIC significantly outperforms competing methods in terms of robustness and out-of-distribution generalization on ImageNet and Atari."
SP:8f80a6f79f78c6421857f392c9a5e98061d7eb60,"This paper proposes a visual transformer network (VTNet) for target-driven visual navigation. The main idea is to extract spatial-aware descriptors from visual observations and then decode visual representations of the observed scenes. A pre-training scheme is proposed to associate the visual representations with navigation signals, and thus facilitate navigation policy learning. The proposed method is evaluated on the artificial environment AI2-Thor and achieves state-of-the-art performance."
SP:3e7cbe3dff592ef371e48dd86be7719fc5343f17,"This paper proposes a communication-computation efficient secure aggregation method for federated learning. The key idea is to design the topology of the secret-sharing nodes as sparse random graphs instead of the complete graph corresponding to the existing solution. Theoretical guarantees on the reliability/privacy of the proposed scheme are provided, and extensive real-world experiments demonstrate that the proposed method can achieve the same levels of reliability and data privacy as the existing secure aggregation methods. "
SP:00fae41e0eca0a1575cd7b2dcfabf0dc5c9c8b8a,This paper proposes a new neural network architecture for finding truthful auctions. The proposed approach builds on top of the work of Duetting et al. (2019) by introducing a time-independent Lagrangian and introducing an additional neural network to improve the performance of RegretNet. The paper also proposes a novel formulation of Auction Design as a two-player game with stationary utility functions. Experiments are conducted to demonstrate the effectiveness of the proposed approach.
SP:a0e8061beb5e9a6c631419861559d22b8d645cb4,"This paper proposes a general learning approach to fine-tune both supervised and unsupervised pre-trained representations to downstream tasks. The proposed approach, called bi-tuning, integrates two heads upon the backbone of pretrained representations: a classifier head with an improved contrastive cross-entropy loss to better leverage the label information in an instancecontrast way, and a projector head with a newly-designed categorical contrastive learning loss to fully exploit the intrinsic structure of data in a category-consistent way. Extensive experiments on CUB and ImageNet datasets show that the proposed approach achieves state-of-the-art results."
SP:87e5b552c13d73bd85249062a152c6c140e594a9,"This paper proposes a new measure for the robustness of classifiers, called “genuine adversarial accuracy”, which measures the adversarial robustness without trading off accuracy on clean data and accuracy on adversarially perturbed samples. The authors argue that standard adversarial accuracies (i.e., the cross-entropy loss of the classifier) is not a good measure of robustness, as it does not favor invariance-based adversarial examples, samples whose predicted classes are unchanged even if the perceptual classes are changed. They prove that a single nearest neighbor (1-NN) classifier is the most robust classifier according to genuine robustness for given data."
SP:2fda410b9281c5e253d385bc4382ec168bc161f3,"This paper studies the problem of link prediction in homogeneous graphs, where the dyadic fairness criterion expects the predictions to be statistically independent of the sensitive attributes from the given two vertices. The authors theoretically analyze the relationship between dyadic fairness and the graph connections, and show that regulating weights on existing edges in a graph can contribute to the fairness conditionally. Then, the authors propose an algorithm, FairAdj, to empirically learn a fair adjacency matrix with proper graph structural constraints for fair link prediction, and in the meanwhile preserve predictive accuracy as much as possible."
SP:b614e9fbec58e9efa7722d2ec4a60fc93d210f92,"This paper proposes a new method for generating controllable attribute-controllable augmented samples in autoencoders. The proposed method is based on disentangled representation and regularization to guarantee the validity of exploration in latent space. The encoder first turns the input sample into a disentanglement latent code, then explores the latent code space through directed interpolation. To aid the interpolated latent code in successfully outputting a meaningful sample, after the decoder, the authors regularize the output by ’reusing’ the encoder to force the obtained latent representation to maintain perfect disentangling. Experiments demonstrate that the proposed method can improve the performance of downstream tasks by synthesizing attribute-controlled augmented samples."
SP:c934adb14926a00ef9c73c9773cb0b3a2669921e,"This paper proposes a new Bayesian memory allocation scheme that bridges the gap between episodic and semantic memory via a hierarchical latent variable model. The proposed method is based on the idea of locally contiguous memory and extends the Kanerva Machine, enabling a novel differentiable block allocated latent memory. The authors demonstrate that this allocation scheme improves performance in memory conditional image generation, resulting in new state-of-the-art conditional likelihood values."
SP:e63d7d8c581019e17585fb9c0eac33d6836e187d,"This paper studies the sample complexity and loss landscape of attention-based neural networks. Theoretical results show that, under mild assumptions, every local minimum of the attention model has low prediction error, and attention models require lower sample complexity than models without attention. Besides, the theoretical results also provide guidelines for designing future attention models."
SP:f739d199fdee26f09994e3f9487aec1eab0f2e89,"This paper extends the concept of expected free energy (EFE), which is a core quantity in active inference, and claims that EFE can be treated as a negative value function. Then, the authors propose a method for learning a prior preference from expert demonstrations, which can effectively handle the difference between local preferences and global preferences, and extend the scope of active inference to an inverse RL problem. The proposed method, called prior preference learning (PPL), is a simple and novel method to learn a prior-preference of an active inference from an expert simulation. The experiments show the applicability of the proposed method to inverse RL problems."
SP:5592b79e49eba95c15103a3348f2bde57b60f2ab,"This paper proposes a data augmentation method to improve generalization in both adversarial and standard learning by using out-of-distribution (OOD) data that are devoid of the abovementioned issues. The authors show how to improve the generalization theoretically using OOD data in each learning scenario and complement the theoretical analysis with experiments on CIFAR-10, CifAR-100, and a subset of ImageNet. The results indicate that undesirable features are shared even among image data that seem to have little correlation from a human point of view. They also present the advantages of the proposed method through comparison with other data augmentations methods, which can be used in the absence of unlabeled in distribution (UID) data."
SP:3cac7a2c310165ed0de46d8e5546c3bfbd639158,"This paper proposes a new meta-RL method that learns a shared linear representation of the policy so that when adapting to a new task, it suffices to predict a set of linear weights. A separate adapter network is trained simultaneously with the policy such that during adaptation, we can directly use the adapter network to predict these linear weights instead of updating a meta-policy via gradient descent to obtain the new policy. The application of the separate feed-forward network not only speeds up the adaptation run-time significantly, but also generalizes extremely well to very different tasks that prior MetaRL methods fail to generalize to. Experiments on standard continuous-control metaRL benchmarks show FLAP presenting significantly stronger performance on out-of-distribution tasks with up to double the average return and up to 8X faster adaptation run time speeds."
SP:21a1bd4ada0723c96c0dbf7a142a2faf5defa4e3,"This paper proposes a federated learning algorithm for kernel k-means, where the goal is to compute the top $k$ eigenvalues of the kernel matrix $K$ in a distributed manner. The authors propose a distributed stochastic proximal gradient descent (DSPGD) algorithm to solve the optimization problem and a communication efficient mech anism (CEM) to reduce the communication cost. Theoretical analysis shows that DSPGD with CEM converges with an O(1/T ) rate, where T is the number of iterations. The clustering quality of the proposed algorithm is also shown to be comparable to the standard kernel k means."
SP:be568dd3fea51ce33a6d1e4b07dda5aee6342395,"This paper proposes CompOFA, a method to reduce the search space of Once-For-All (OFA) by constraining search to models close to the accuracy-latency Pareto frontier. The authors leverage insights of compound relationships between model dimensions to build a design space smaller by several orders of magnitude. They show that this smaller design space is dense enough to support equally accurate models for a similar diversity of hardware and latency targets, while also reducing the complexity of the training and subsequent extraction algorithms."
SP:04b84d26cf282dbb753cbf27f14c334f65d3f8ec,"This paper proposes a new meta-learning algorithm, ADML (ADversarial Meta-Learner), which leverages clean and adversarial samples to optimize the initialization of a learning model in an adversarial manner. The authors show that the proposed method is robust to adversarial attacks, i.e., it only leads to a minor performance degradation when there are adversarial examples. It also sheds light on tackling the cases with limited and even contaminated samples. Experiments are conducted on two widely-used image datasets, MiniImageNet and CIFAR100, in terms of both accuracy and robustness."
SP:dfbaa6b53c4e8328d52666ad4641fc917bf0c0b3,"This paper presents a data-driven framework for permutation selection for decoding error correction codes. The authors propose to use a self-attention model (Vaswani et al. 2017) to embed the permutations of a code in a pre-process during a test phase. At test time, a NN accepts a corrupted word as input and outputs the permutation of the decoder. The proposed method is evaluated on a simulated Bose Chaudhuri Hocquenghem (BCH) code and shows consistent improvements in the bit error rate."
SP:c860a7b0952d708e7851c9bc4b63d246f64d1cba,"This paper proposes to use an unsupervised text clustering task as an intermediate task for fine-tuning a pretrained language model for text classification. Specifically, the authors propose to use a Bag of words (BOW) clustering technique to partition the unlabeled training data into relatively homogeneous clusters of text instances. Next, they treat these clusters as labeled data for an intermediate text classification task, and train BERT – with or without additional MLM pretraining – with respect to multi-class classification prior to the actual target task labeled data. Extensive experiments demonstrate the practical value of this strategy on a variety of benchmark data, most prominently when the training data available for the target task is of topical nature."
SP:ea37f5882fd98dd4ce233077bb3069517d4ed4ea,"This paper compares several probabilistic and deterministic models for micro-data model-based reinforcement learning (MBRL) on the Acrobot environment. The authors show that the mixture density nets outperform all other models by a large margin when multimodal posterior predictives are required. They also show that heteroscedasticity at training time, perhaps acting as a regularizer, improves predictions at longer horizons."
SP:4e25ba3714d78ba59a0d8efbb65e0ef5201702f8,"This paper proposes an affine disentangled GAN, which is a generative adversarial network that explicitly disentangles affine transformations in a self-supervised and rigorous manner. Unlike the disentanglement representations learned by existing approaches, the features learned by ADIS-GAN are axis-aligned and scalable, where transformations such as rotation, horizontal and vertical zoom, horizontal/horizontal and vertical skew, horizontal & vertical translation can be explicitly selected and learned. Experiments on MNIST, CelebA and dSprites datasets demonstrate the effectiveness of the proposed method."
SP:121f8420cfb49c6d80b5ebb4051e85947182594a,"This paper proposes a new contrastive learning method for self-supervised learning with stronger augmentation. The main idea is to minimize the distribution divergence between the weakly and strongly augmented images over the representation bank to supervise the retrieval of stronger queries from a pool of candidates. This avoids an overoptimistic assumption that could overfit the strongly augmented queries containing distorted visual structures into the positive targets, while still being able to distinguish them from the negative samples by leveraging the distributions of weakly augmented counterparts. The proposed method achieves top-1 accuracy of 76.2% on ImageNet with a standard ResNet-50 architecture with a single-layer classifier fine-tuned."
SP:af54e542223097c315ecd677d0b968e9a0b2a1d4,"This paper proposes a method for de-identification of magnetic resonance images (MRI) that preserves privacy by remodeling privacy-sensitive facial features rather than removing them. To achieve this, the authors propose a conditional, multi-scale, 3D GAN architecture that takes a patient’s MRI scan as input and generates a 3D volume in which the brain is not modified but the face has been de-identified. Compared to the classical removal-based techniques, the proposed method preserves privacy more reliably without adversely affecting downstream medical analyses on the brain, including segmentation and age prediction."
SP:0ac3964bd2320341488476d60f57b75d2a79f92c,"This paper proposes a multi-head attention based global pooling layer to capture the interaction between nodes according to their structural dependencies. The authors first formulate the graph pooling problem as a multiset encoding problem with auxiliary information about the graph structure, and propose a Graph Multiset Transformer (GMT) to solve the problem. The GMT satisfies both injectiveness and permutation invariance, such that it is at most as powerful as the Weisfeiler-Lehman graph isomorphism test. The experimental results show that GMT significantly outperforms state-of-the-art node clustering methods on graph classification benchmarks with high memory and time efficiency, and obtains even larger performance gain on graph reconstruction and generation tasks."
SP:76848e7ac3e6709e92f6a6db60269cb5177495d1,"This paper proposes a new explanation for the poor performance of GNNs on long-range tasks. The authors claim that there is a bottleneck at every layer, which causes the message propagation to be compressed into fixed-size vectors, which results in the over-squashing of exponentially growing information. The paper shows that GAT, GGNN, GCN, and GIN are susceptible to this bottleneck, and that they fail to propagate messages originating from distant nodes and perform poorly when the prediction task depends on long range interaction. They also show that prior work, which extensively tuned GNN models of long range problems, also suffer from this bottleneck and that breaking the bottleneck improves their performance."
SP:90d8fa381446923902e42b259392e5e975e6caa1,"This paper proposes a domain adaptation method for cross-domain sentiment analysis. The main idea is to learn a prototypical distribution for the source domain, which is trained to be domain-agnostic by minimizing the Sliced Wasserstein Distance (SWD) between the source and the target distributions, and then use it to align the distribution of the target in the source embedding space. Theoretical and empirical analysis are provided to demonstrate the effectiveness of the proposed method."
SP:893fd7440b82f5da0d4c0944928810322eaee2f0,"This paper proposes an evaluation methodology to measure the presence of gender bias in natural language understanding through inference. The authors construct a challenge task which involves pairing gender neutral premise against gender-specific hypothesis. They evaluate state-of-the-art NLI models on the existence of gender stereotypes using occupations. Their findings suggest that three models (BERT, RoBERTa, BART) trained on MNLI and SNLI data-sets are significantly prone to gender-induced prediction errors. They also find that debiasing techniques such as augmenting the training dataset to ensure a gender-balanced dataset can help reduce such bias."
SP:a32ab755bd249c393b70938036ce8e810c0c439f,"This paper studies the variational intrinsic control (VIC), an unsupervised reinforcement learning method for finding the largest set of intrinsic options available to an agent. The main idea of VIC is to maximize the mutual information between the set of options and final states, called empowerment. The authors show that the intrinsic reward in implicit VIC suffers from variational bias in stochastic environments, causing convergence to suboptimal solutions. To correct this behavior and achieve the maximal empowerment, the authors propose two methods respectively based on the transitional probability model and Gaussian mixture model."
SP:b4df2c4627a6d46c5100133e38c4bea20b296dd8,"This paper studies the use of neural ensembles for few-shot image classification. The authors propose to use an ensemble of relatively small convolutional neural networks (CNNs) to tackle the problem of learning from a small sample and show the superiority of the proposed method over the state-of-the-art approaches. The proposed method is motivated by the nice variance-reduction properties of ensemble of CNNs and the computational cost of training them. The experiments are conducted on CIFAR-10, CUB-100 and ImageNet datasets."
SP:4a0ee01f4897efa81659f37ef0468ee8195bbc4f,"This paper proposes a method for sparsity and compression of binary neural networks (BNNs). Specifically, the authors propose to use positive 0/1 binary weights instead of the -1/1 weights used by state-of-the-art binary networks. The proposed method is able to achieve a high compression factor and reduces the number of operations and parameters at inference time. Experiments on linear and convolutional networks over MNIST and CIFAR-10 datasets show that SBNNs can achieve high compression rates and good generalization."
SP:5be8539ad02595ad3c7a2d7afe8cbb3e9924467d,"This paper proposes a simple post hoc calibration method for predictive uncertainty. The method uses outlier exposure to properly calibrate the model probabilities. The proposed method is simple and easy to implement. The authors show that the proposed method outperforms the baseline method (Ovadia et al., 2019) on corrupted data."
SP:ea503f67e38fce7dee9cc4996b55b8959911f030,"This paper studies the expressive power of graph neural networks and graph kernels from an empirical perspective. Specifically, it compares the graph representations and similarities produced by these algorithms against those generated by a well-accepted, but intractable graph similarity function. The authors also investigate the impact of node attributes on the performance of the different models and kernels."
SP:0cf7b7d929f50e0b7f4fda5e1f68e5ade2f7c29b,"This paper proposes a method for self-supervised image animation. The authors propose to use CutMix, a data augmentation method, to improve the quality of the generated images. The main idea is to use the top-k percent occluded pixels of the foreground to regularize the image generation process. The proposed method is evaluated on the VoxCeleb and Tai-HD datasets. The results show that the proposed method outperforms the baseline methods. "
SP:60b535fc6cbc1a7a26ad53f706ebb17de346dc4f,"This paper proposes a self-supervised approach to learn independent causal mechanisms (ICM), which directly model multiple data generation processes (mechanisms) in a coarse granularity. The authors aim to learn a model that disentangles each mechanism and approximates the groundtruth mechanisms from observational data. They outline sufficient conditions under which the mechanisms can be learned using a single self supervised generative model with an unconventional mixture prior, simplifying previous methods. Moreover, they prove the identifiability of their model w.r.t. the mechanisms in the self supervised scenario. They compare their approach to disentangled representations on various downstream tasks, showing that their approach is more robust to intervention, covariant shift, and noise due to the disentanglement between the data generation process."
SP:44d4e24428d043a69b40013919cda0e8e7bff99c,"This paper proposes a graph aligning approach that generates rich or detailed labels given normal labels W for predicting molecular graph structures (W) given a 2D image of a chemical compound (U). The proposed approach is based on the idea of domain adaptation from the source domain where we have access to the expensive labels V to the target domain. The authors show that, using only 4000 data points, the proposed approach achieves up to 4x improvement of performance after domain adaptation to target domain compared to pretrained model only on source domain. "
SP:ad906dd9a176cffd283593321ff6b9ad19595528,This paper proposes a monotonic neural network (MNN) to model the energy consumption of a chiller power system. The main idea is to constrain the input-output of the MNN to conform to physical laws and provide accurate function space about chiller plants. Experiments on a cooling system of a data center show the superiority of the proposed method over existing ones.
SP:6cb65ee5d2926858570601eeeade24fe86c7f32f,"This paper proposes a method to predict the causal effects of predictors on multiple forecasting targets, such as supply and demand in ride-sharing platforms. The authors integrate the causal attention with the Conditional Average Treatment Effect (CATE) estimation method in causal inference. They propose a novel and fast multi-head attention evolved from Taylor’s expansion instead of softmax, reducing time complexity from O(V) to O(\V) where V is the number of nodes in a graph. They further design a spatial graph fusion mechanism to significantly reduce the parameters’ scale. They conduct a wide range of experiments to demonstrate the interpretability of the proposed method."
SP:223980a1954d626d90ff54d8dc61b5d85a6b349c,"This paper proposes a multi-agent VAE framework to jointly learn a mixture representation for a single data modality. The proposed method is based on the idea of collective decision making, where each agent receives an augmented copy of the given sample with the same class label and is encouraged to reach consensus on the categorical assignments. The method is evaluated on two datasets, MNIST and dSprites, where it is shown to achieve state-of-the-art categorical assignment while preserving the interpretability of the continuous factors. It is also evaluated on a single-cell gene expression dataset profiling over 100 cortical neuron types."
SP:c982610ad28662c3bd13132abe1f7307d1a61b68,"This paper provides a general characterization of steerable kernel spaces for group equivariant convolutional networks (G-steerable kernels) for any compact group $G$. The work is motivated by a striking analogy between the constraints underlying steerable kernels on the one hand and spherical tensor operators from quantum mechanics on the other hand. By generalizing the famous Wigner-Eckart theorem for spherical tensors, the authors prove that steerable Kernel spaces are fully understood and parameterized in terms of generalized reduced matrix elements, Clebsch-Gordan coefficients, and harmonic basis functions on homogeneous spaces. "
SP:7b2ea39069277ad0f4f79476a77ef84587a804d9,"This paper analyzes the effect of selective classification on accuracy disparities between groups. It shows that increasing the abstention rate can even decrease accuracies on groups that have lower accuracy at full coverage. The authors analyze the margin distribution, which captures the model’s confidences across all predictions and determines which examples it abstains on at each threshold. The margin distribution is analyzed for symmetric margin distributions and shows that whether selective classification monotonically improves or worsens accuracy is fully determined by the full coverage accuracy and whether the distribution satisfies a property we call left-log-concavity. Motivated by the analysis, the authors train distributionally robust models that achieve similar full-coverage accuracies across groups and show that selective classification uniformly improves each group on these models."
SP:f1d57ee27e901daf7e4e2b84139019e945818911,"This paper proposes a hierarchical nonnegative nonnegative (NCPD) decomposition (Hierarchical NCPD) model and a training method, Neural NCPD, for performing hierarchical topic modeling on multi-modal tensor data. The proposed method utilizes a neural network architecture and backpropagation to mitigate error propagation through hierarchical NCPD. This approach allows to explore the topics learned at different ranks simultaneously, and illustrate the hierarchical relationship of topics."
SP:b6ddc3a560aa7155e7e927bf5360bedc36586597,"This paper proposes a collective adversarial robustness certificate for multi-output classifiers. The proposed approach is based on the locality property of graph neural networks and leverage their locality property to fuse multiple single-node certificates into a drastically stronger collective certificate. For example, on the Citeseer dataset our collective certificate for node classification increases the average number of certifiable feature perturbations from 7 to 351 to defend against adversarial attacks."
SP:cc93dd2f68e415e2457166e78627865dc1b44697,This paper proposes Quantile Regression GAN (QRGAN) to improve the performance of GANs. The main idea is to use quantile regression to minimize 1-Wasserstein distance between real and generated data distribution as a novel approach in modification of loss functions for the improvement of generative adversarial networks (GANs). The proposed method is evaluated on CIFAR-10 and ImageNet datasets and compared with several existing GAN variants. The results show that QRGAN is more robust to mode collapse and less prone to gradient explosion.
SP:4ddb47ee77c374ae6c3e419412d92ca77260692e,"This paper proposes three tests to evaluate relevance metrics for similarity-based explanations, i.e., the cosine similarity of the gradients of the loss function, the class-test and subclass-test similarity tests, and the two-class and subclass similarity tests. The proposed three tests are designed to evaluate whether the relevance metrics satisfy the minimal requirements for similarity based explanations. The experiments show that cosine similarities of gradients performs best, which would be a recommended choice in practice. The authors also show that some metrics perform poorly in the tests and analyze the reasons of their failure."
SP:6c2cbf2bc0f6dabe974e80ec1e82d2d12189906e,This paper proposes a low-rank global attention (LRGA) module to improve the generalization of graph neural networks (GNNs). The authors show that the proposed LRGA module aligns with the 2-FWL update step via polynomial kernels and bound the sample complexity of the kernel’s feature map when learned with a randomly initialized two-layer MLP. Experiments are conducted to verify the effectiveness of the proposed method.
SP:b4abdd28504b4c1de239eabd4e0e27d370efee71,"This paper proposes a label smoothing method to improve the calibration performance of convolutional neural networks (CNNs). The proposed method is based on the idea of objectness, which quantifies the likelihood that an image window contains an object belonging to an image class. The authors propose a smoothing factor that is adaptive based on relative object size within an image. Experiments on classification and transfer learning tasks demonstrate the effectiveness of the proposed method."
SP:5254658923e594294b69d124a8d004166852822a,"This paper proposes a two-layer fully-convolutional ReLU denoising network with weight decay regularization (WDC) as a convex duality framework that makes it amenable to convex optimization. In particular, it implies training neural networks with WDC induces path sparsity while the prediction is piecewise linear filtering. Experiments with MNIST and fastMRI datasets confirm the efficacy of the dual network optimization problem."
SP:085cad6bc143c8713580bddfaa71f06496dac314,This paper proposes an end-to-end approach to synthesize speech from text or phonemes. The authors propose a differentiable alignment scheme based on token length prediction and adversarial feedback. The proposed generator is feed-forward and thus efficient for both training and inference. The model achieves a mean opinion score exceeding 4 on a 5 point scale.
SP:01148cea55db606aa78d27e900818684a8bce9ab,"This paper proposes a non-parametric method for node representation learning. The proposed method is based on the Wasserstein graph diffusion to smooth the distribution representations of nodes with information from their local neighbors. The authors also propose two algorithms based on it for node classification (with missing attributes) and matrix completion respectively, and demonstrate their effectiveness in experiments. "
SP:aeeb5909f7123ef631f569b469af9715205c881f,"This paper proposes a meta-learning approach for intrinsic motivation for RL agents. In particular, the agent is composed of a goal-conditioned teacher and a student policy. The teacher learns to propose increasingly challenging goals for the student policy to learn general skills for acting in a new environment, independent of the task to be solved. The proposed approach is evaluated on procedurally-generated environments. "
SP:3d05bc7dca97681cb582298e318b9b973841eed3,"This paper considers the problem of private information retrieval (PIR) from a dataset of files stored on a single server under both a user distortion and a user privacy constraint. The authors show that the optimal rate-distortion-leakage tradeoff is convex (see Lemma 1) and that it allows for a concise information-theoretical formulation in terms of mutual information in the limit of large file sizes. Moreover, they propose a new data-driven framework by leveraging recent advancements in generative adversarial models (GAN) which allows a user to learn efficient PIR schemes. The proposed model can be seen as an extension of the well-known concept of privacy retrieval by allowing for distortion in the retrieval process and relaxing the perfect privacy requirement."
SP:3f9e2db00fc3dcd7a40588adcb638503ec10dc09,"This paper proposes a decoupled greedy learning method for GNNs (DGL-GNN) that, instead of sampling the input graph, decouples the GNN into smaller modules and associates each module with greedy auxiliary objectives. This allows GNN layers to be updated during the training process without waiting for feedback from successor layers, thus making parallel GNN training possible. The method achieves improved efficiency without significantly compromising model performances, which would be important for time or memory limited applications. Further, a lazy-update scheme during training to further improve its efficiency."
SP:5ecb1b288f7fc02aead4493f81640867bc349290,"This paper proposes a method for answering First-Order Logic Queries on incomplete Knowledge Graphs. The authors translate each query into an end-to-end differentiable objective, where the truth value of each atom is computed by a pre-trained neural link predictor. They then propose two solutions to the optimisation problem, including gradient-based and combinatorial search. In experiments, the proposed approach produces more accurate results than state-of-the-art methods."
SP:f04a522fd04c503754fdb8c52da68646d31271a4,"This paper proposes a method for verifying the local robustness of neural networks with piecewise-linear activation functions for the l_2 norm. Local robustness is the property that a model classifies all inputs within an l_p-ball consistently, which precludes various forms of adversarial inputs. In this paper, the authors propose a simple geometric projection-based method for computing the decision boundaries around a given input, which is sufficient for assessing the robustness. The proposed method is shown to be more precise than many approximate verification approaches, while performing multiple orders of magnitude faster than complete verifiers and scaling to much deeper networks."
SP:5297651ff873f97c07b9c47ed3eff52251661844,"This paper proposes an approach to predict the set of verbs that are most likely to be applicable to a given object based on human judgements of object similarity. The approach is based on SPoSE (Hebart et al., 2020), a model of the mental representations of objects. The model is trained using a dataset of 1.5M Amazon Mechanical Turk (AMT) judgments of objects, where subjects were asked which of a random triplet of objects was the odd one out. The authors show that the dimensions learned are interpretable, and that they correspond to typical patterns of interaction with objects."
SP:72b4f3b40c6c6fa2eb53e95ed9a10a4077ffa049,"This paper proposes a method for the emergence of individuality (EOI) in multi-agent reinforcement learning (MARL). EOI learns a probabilistic classifier that predicts a probability distribution over agents given their observation and gives each agent an intrinsic reward of being correctly predicted by the classifier. The intrinsic reward encourages the agents to visit their own familiar observations, and learning the classifiers by such observations makes the intrinsic reward signals stronger and in turn makes the agents more identifiable. Two regularizers are proposed to increase the discriminability and enhance the feedback. The proposed method is compatible with centralized training and decentralized execution (CTDE). Empirically, the proposed method outperforms existing methods in a variety of MARL scenarios."
SP:112509d6d3573a9d495d182fdfae6ec0327cddf5,"This paper proposes a randomized smoothing method for certified robustness against l2-norm adversarial attacks. The proposed method, called Smoothed WEighted Ensembling (SWEEN), is based on the idea of ensemble of base classifiers. Theoretical analysis shows that the optimal SWEEN model can be obtained from training under mild assumptions. An adaptive prediction algorithm is also developed to reduce the prediction and certification cost. Extensive experiments are conducted to demonstrate the effectiveness of the proposed method."
SP:ea892e3d199ed6121279b20061a87f43afae8796,"This paper proposes a method to discover subtask hierarchy by learning from demonstration. The proposed method is based on ordering memory policy network (OMPN), which is able to identify subtask boundaries in an unstructured demonstration. Experiments on Craft and Dial demonstrate that the proposed method can achieve higher task decomposition performance under both unsupervised and weakly supervised settings, comparing with strong baselines."
SP:cc6aa977ce561a2493ae74bb694205cd67c8d890,"This paper proposes a Causal Semantic Generative model (CSG) for out-of-distribution (OOD) generalization and domain adaptation. Theoretically, the authors prove that under certain conditions, CSG can identify the semantic factor, and the test-domain prior is identifiable which leads to an accurate prediction. The proposed method is based on the causal invariance principle, with a novel design in variational Bayes for both efficient learning and easy prediction."
SP:be3f34a59e5e61dcdbc7cb085f031ba4a5a5b758,"This paper studies the problem of online learning algorithms that are robust to adversarial corruptions of the rewards. In particular, the authors consider the setting where the reward at each time step can be corrupted with probability $\mathcal{O}(\frac{1}{\sqrt{T}^2}{\epsilon^{-2})$, where $T$ is the number of experts, $T$, is the time step, and $T_i$ is a set of actions. The authors propose algorithms with near-optimal regret (in terms of the regret with respect to the true uncorrupted reward distribution) in three settings: stochastic multi-armed bandits, linear contextual bandits, and finite state finite state MDPs. "
SP:6d62a80aaebb2988df3953d4d7164e5a2fa1aa6d,"This paper proposes Rewriter-Evaluator, a framework for neural machine translation (NMT) that consists of a rewriter and an evaluator. At every pass, the rewriter produces a new translation to improve the past translation, and the evaluation is based on the translation quality to decide whether to terminate the rewriting process. The authors also propose a prioritized gradient descent (PGD) method to improve training efficiency. Extensive experiments on two translation tasks, Chinese-English and English-German, show that the proposed framework significantly improves the performance of NMT models and significantly outperforms prior methods."
SP:9761fca8848868dfc9cacdab2537f8276ca76e0f,"This paper proposes a two-stage approach to learn a multimodal predictive distribution for semantic segmentation. In the first stage, the authors explicitly model the data with a categorical likelihood, and in the second stage, they train an adversarial network to sample from it an arbitrary number of coherent predictions. The model can be used independently or integrated into any black-box segmentation framework to facilitate learning of calibrated stochastic mappings. The authors demonstrate the utility and versatility of the approach on the multigrader LIDC dataset and a modified Cityscapes dataset."
SP:ce965758f1b795a56c02f45d6a8d06cb8bdf29cb,"This paper proposes a new compressed communication with error feedback (EF) method for dealing with contractive compressors in distributed optimization. In particular, they propose a construction which can transform any contractive compressor into an induced unbiased compressor. They show that their approach leads to vast improvements over EF, including reduced memory requirements, better communication complexity guarantees and fewer assumptions. They also extend their results to federated learning with partial participation following an arbitrary distribution over the nodes."
SP:4fd702490293e481c79614852ba27dd3ce9215a4,"This paper proposes a new research framework for hyperparameter transfer across adjustments (HT-AA) for machine learning (ML) algorithms. The authors provide four simple HT-AA baseline algorithms and eight benchmarks changing various aspects of ML algorithms, such as hyper-parameter search spaces, neural architectures, and the neural architectures used. They also provide python packages for the baselines and benchmarks. They empirically demonstrate the need for well-vetted algorithms for HTAA-based methods."
SP:e8f99bae5853de525450fcb8facd23cf973fc161,"This paper investigates the role of label representations in the training of deep neural networks for image classification. Specifically, the authors propose a new supervised task using the image classification task as the supervised signal, where the labels are represented as audio spectrograms. They show that high dimensional, high entropy label representations are generally more useful because they provide a stronger error signal. They test their hypothesis with evidence from various label representations including constant matrices, spectrogram, shuffled spectrogram, Gaussian mixtures, and uniform random matrices of various dimensionalities."
SP:4e8d924cba7367af0999b30d79250b4dc40413e1,"This paper proposes a multi-input multi-output (MIMO) configuration to train multiple subnetworks that independently learn the task at hand. By ensembling the predictions made by the sub-networks, the proposed method improves model robustness without increasing compute. The proposed method achieves a significant improvement in negative log-likelihood, accuracy, and calibration error on CIFAR10, CifAR100, and ImageNet."
SP:d2f1c23b67c6744101034dc5e1c70765a733b169,"This paper proposes a method to transfer intermediate knowledge obtained from one Convolutional Neural Network (CNN) to another by utilizing sparse representation learning. The proposed method first extracts sparse representations of the hidden features of the teacher CNN, which are then used to generate both pixel-level and image-level labels for training intermediate feature maps of the student network. The authors formulate SRM as a neural processing block, which can be efficiently optimized using stochastic gradient descent and integrated into any CNN in a plug-and-play manner. The experiments demonstrate that SRM is robust to architectural differences between the teacher and student networks, and outperforms other KD techniques across several datasets."
SP:e8c0f43bd5debf6544f588cd3442dc3dd62d0eee,"This paper proposes a new policy similarity metric (PSM) for measuring behavioral similarity between states. The PSM assigns high similarity to states for which the optimal policies in those states as well as in future states are similar. The authors also present a contrastive representation learning procedure to embed any state similarity metric, which they instantiate with PSM to obtain policy similarity embeddings (PSEs). The authors demonstrate that PSEs improve generalization on LQR with spurious correlations, a jumping task from pixels, and Distracting DM Control Suite."
SP:92f3b4942da9075440dda618f561a85f8fde5a5c,"This paper studies the problem of disentanglement, i.e. disentangling natural factors of variation in data (e.g. object shape vs pose). The authors propose a new approach based on distributed equivariant operators, which relies on classical results from group representation theory, to disentangle simple affine transformations such as rotations and translations. The authors show that this approach introduces discontinuities in the encoder for a broad family of transformations acting on images —encompassing translations, rotations, and translations —and propose an alternative, more flexible approach that relies on distributed Equivariant Operators (EoE) to solve the problem. Theoretical and empirical results are provided to demonstrate the effectiveness of the approach."
SP:ef0f58c462bc5dd1c7b78f562c42a4e17f0f252b,"This paper proposes an efficient expectationmaximization (EM) algorithm to estimate the maximum a posteriori (MAP) of spike trains in neural recordings. The authors propose to use three auxiliary latent variables (Pólya-Gamma variables, latent marked Poisson processes and sparsity variables) are augmented to make functional connection weights in a Gaussian form, which allows for a simple iterative algorithm with analytical updates. They demonstrate the accuracy and efficiency performance of the algorithm on synthetic and real data."
SP:1156d3deac022829bda930ffcb081947609d972b,"This paper studies the dynamics of gradient descent (GD) for training two-layer neural network models in the under-parameterized regime and the mildly over-parametrized regime. It is shown that GD has two phases: an early phase in which the dynamics follows closely that of the corresponding random feature model, followed by a late phase where the neurons are divided into two groups: a group of a few (maybe none) “activated” neurons that dominate the dynamics and another group of quenched neurons that support the continued activation and deactivation process. In particular, when the target function can be accurately approximated by a relatively small number of neurons, this quenching-activation process biases GD to picking sparse solutions. This neural network-like behavior is continued into the mildly-overparametrization regime, where it undergoes a transition to a random featurelike behavior where the inner-layer parameters are effectively frozen during the training process."
SP:9e81401a6f30c70d870a12cce0cf600557f92b80,"This paper proposes a method to solve CMDP problems by decomposing the CMDP into reconnaissance MDP and planning MDPs. In R-MDP, the authors propose to train the threat function, the Q-function analogue of danger that can determine whether a given state-action pair is safe or not, by preferentially sampling rare dangerous events. The authors also present an efficient approximation method for the risk function that can greatly reduce the difficulty of solving R-mDP. The proposed method is evaluated on a variety of benchmark tasks."
SP:f1d4ac7d5516dd0df742e224c8c09c721d0d0886,"This paper provides a systematic comparison between the cross-entropy and square losses for classification tasks across a range of NLP, ASR and CIFAR-10 datasets. The authors show that the square loss outperforms the cross entropy loss in most cases, even when the number of training epochs is the same as the optimal number of epochs for cross entropy. The paper also shows that the performance on nearly all non-vision tasks can be improved, sometimes significantly, by switching to square loss."
SP:915f1f0fc4850507c28c1d609239b41775863ebe,"This paper proposes a self-supervised representation learning method for reinforcement learning (RL) with limited data. The proposed method, Self-Predictive Representations (SPR), trains an agent to predict its own latent state representations multiple steps into the future. The encoder is an exponential moving average of the agent’s parameters, and the transition model is a learned transition model. The future prediction objective outperforms prior methods for sample-efficient deep RL from pixels. SPR also improves performance by adding data augmentation to the future prediction task, which enforces consistency across different views of each observation, and achieves a median human-normalized score of 0.415 on Atari."
SP:983f01c170909c8c67fd3be25f121bd61bdd8307,"This paper proposes a local node embedding method based on Personalized PageRank (PPR) to generate node representations in sublinear time. The proposed method is evaluated on node classification and link prediction tasks and compared with state-of-the-art methods such as DeepWalk, node2vec, VERSE, and FastRP. The paper provides theoretical guarantees on the locality of the computation, and the proof of the global consistency of the generated representations."
SP:d11037b8fe2b10aee672ba82f69410b40181f0f9,This paper proposes a data-driven approach for graph coarsening. The main idea is to leverage the recent progress of deep learning on graphs to develop a framework to learn the edge weights in an unsupervised manner from a small collection of graphs. This learned weight assignment map can then be applied to new graphs (of potentially much larger sizes) and can be used to improve the quality of the resulting graphs. The proposed method is evaluated on both synthetic and real-world graphs.
SP:0d680213339f0e2aedb0be4aeed51423706b8bf6,"This paper proposes a geometric deep learning method to estimate the acoustic scattering properties of 3D objects at interactive rates. The method uses a point cloud approximation of each object, and each point is encoded in a high-dimensional latent space. The proposed method can accurately estimate these acoustic properties for arbitrary topologies and takes less than 1ms per object on a NVIDIA GeForce 2080 Ti GPU. The paper also proves that the learning method is permutation and rotation invariant and demonstrate high accuracy on objects quite different from the training data."
SP:afc33a782c43e3d4c5c4fbf047d0b1108bc30bae,"This paper proposes Risk Extrapolation (REx) as a form of robust optimization over a perturbation set of extrapolated domains (MMREx), and propose a penalty on the variance of training risks (V-REx). The authors prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution (covariate shift). By appropriately trading-off robustness and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur."
SP:411d5bcf7698d534ad60f581d479ff74849ba4de,"This paper proposes a new method for solving complex partial differential equations (PDEs) using neural networks. The proposed method is based on the Fourier neural operator (FNO), which parameterizes the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. Experiments on Burgers’ equation, Darcy flow, and Navier-Stokes equation show that the proposed method achieves superior accuracy compared to previous learning-based solvers under fixed resolution. "
SP:41d268d0eac9b4c84baa156fb641aa6d3060b5a4,"This paper studies the implicit bias of gradient flow (GD) on linear neural network training. The authors propose a tensor formulation of neural networks that includes fully-connected, diagonal, and convolutional networks as special cases, and investigate the linear version of the formulation called linear tensor networks. With this formulation, the convergence direction of the network parameters are characterized as singular vectors of a neural network defined by the network. For separable classification, the authors show that gradient flow finds a stationary point of the $\ell_2$/L max-margin problem in a “transformed” input space. For underdetermined regression, they prove that GD finds a global minimum which minimizes a norm-like function that interpolates between weighted `1 and `2 norms in the transformed input space, which subsumes existing results in the literature while removing standard convergence assumptions."
SP:e27907ef4a4e6e0f5841618fcaa7e7e0db443f91,"This paper studies the problem of optimizing slimmable networks from a multi-objective optimization lens, which leads to a novel algorithm for optimizing both the shared weights and the width-multipliers for the sub-networks. The proposed method is evaluated on 15 network and dataset combinations and two types of cost objectives, i.e., FLOPs and memory footprint, to demonstrate the effectiveness of the proposed method compared to existing alternatives. The results highlight the potential of optimizing the channel counts for different layers jointly with the weights for slimmability networks."
SP:cf59403abb6ca89ccee4f8e77e9a33d99e6a00f5,"This paper studies federated semi-supervised learning (FSSL) in the setting where the labeled data is only available at the server and the unlabeled data at the client can be either partially or completely labeled. The authors propose FedMatch, a method that combines the idea of federated learning (FL) and semi supervised learning (SSL) to improve the performance of the proposed method. The proposed method is based on two components: (1) an inter-client consistency loss that enforces the consistency between the predictions made across multiple clients, and (2) a sparse parameter decomposition that ensures that training on labeled data are effectively separated and minimizing the interference between labeled and unlabelled data. Experimental results show that FedMatch outperforms both local SSL and the naive combination of FL and SSL. "
SP:9457b6d430a2cd864d526d7e90bf3e1ab13d6df4,"This paper proposes a self-supervised learning method for discrete event sequences. The proposed method is based on contrastive learning, which is previously used for audio and computer vision domains. Unlike most previous studies, the authors theoretically justify under mild conditions that the augmentation method underlying CoLES provides representative samples of discrete events sequences. They evaluated CoLES on several public datasets and showed that CoLES representations consistently outperform other methods on different downstream tasks."
SP:385942a5bcee7384bb722a1669b541f2fac0cd36,"This paper proposes a method for unsupervised dependency parsing and constituency parsing. The main idea is to use a distance-based parsing mechanism that predicts a sequence of syntactic distances and syntactic heights to represent dependency and constituency trees at the same time. The authors propose a dependency-constrained self-attention layer to replace the multi-head attention layer in the standard transformer model. The proposed method is evaluated on three tasks: masked language modeling, dependency parsing, and dependency parsing. "
SP:078966ff62775bba6031e47d374bda95f4a7dde3,This paper proposes a method for learning the mapping between scene graph nodes and visual objects under weak supervision. The proposed method learns a metric among visual objects and scene graphs by incorporating information from both object features and relational features. Extensive experiments on Visual Genome (VG) and Visual Relation Detection (VRD) datasets verify that the proposed method post an improvement on scene graph grounding task over current state-of-the-art approaches. Further experiments on the scene graph parsing task verify the grounding found by our model can reinforce the performance of the existing method.
SP:4644dbf7466b6234d8abf69995fdfb357efcc119,"This paper proposes two variants of sliced fused Gromov Wasserstein (SFG) for relational regularized autoencoders (RAEs). The first variant, MSSFG, replaces the vMF distribution by a mixture of von Mises-Fisher distributions to capture multiple important areas of directions that are far from each other. The second variant, PSSFG, replaces vMF with a power spherical distribution to improve the sampling time in high-dimensional settings. Experiments on latent manifold structure, image generation, and reconstruction demonstrate the effectiveness of the proposed methods."
SP:5ae2c0af82cac89a65f1cc38c43e2d05ea298901,"This paper proposes an approach to speed up training for a particular kind of deep networks which contain repeated structures, such as the transformer module. In this paper, the authors first train such a deep network with the weights shared across all the repeated layers till some point, then stop weight sharing and continue training until convergence. The untying point is automatically determined by monitoring gradient statistics. The adaptive untying criterion is obtained from a theoretic analysis over deep linear networks. Empirical results show that our method is able to reduce the training time of BERT by 50%."
SP:a51710551142316b67e2fccd969fea1ece35ba39,"This paper investigates the role of the interaction inside adversarial perturbations to explain and boost the adversarial transferability of deep neural networks (DNNs). In particular, the authors show that there is a clear negative correlation between the transferability and the interaction between the perturbation units (i.e., the importance of the i-th unit to the attacking) in adversarial attacks. They further show that adversarial attack with lower transferability tend to exhibit larger interactions between perturbated units. Based on this, they propose to directly penalize interactions during the attacking process, which significantly improves the adversarially transferability."
SP:f1565319075c1442c2cb52d96443facb492c06c2,"This paper studies the role of higher layers in catastrophic forgetting in deep neural networks. It finds that higher layers are disproportionately responsible for forgetting, with sequential training resulting in an erasure of earlier task representational subspaces. It also investigates different methods for mitigating forgetting, and finds that while all stabilize higher layer representations, some methods encourage greater feature reuse in higher layers, while others store task representations as orthogonal subspace, preventing interference. The paper also studies the connection between forgetting and task semantics, finding that semantic similarity between subsequent tasks consistently controls the degree of forgetting."
SP:30d7532cdcf420bff3be6b92eea3d93bce59e6bd,"This paper proposes a general efficient training algorithm based on structured lottery tickets for large-scale language models. The proposed method, called EarlyBERT, is applied to both pre-training and fine-tuning of BERT. The authors propose to slim down the self-attention and fully-connected sub-layers inside a transformer and identify structured winning tickets in the early stage of training. They apply those tickets towards efficient BERT training, and conduct comprehensive pre- training and finetuning experiments on GLUE and SQuAD downstream tasks. The results show that the proposed method achieves comparable performance to standard BERT, with 35%-45% less training time."
SP:c547f23ff6caaf5e9f35d258490b86ae0ac8ed03,"This paper proposes a family of f-divergence measures that are robust to label noise. The authors analyze the robustness of these measures and identify conditions under which they are robust. They also propose methods to make them more robust. Finally, the authors empirically verify the effectiveness of the proposed methods."
SP:841888179dcdac901889c8d62cb5234311fe28f1,"This paper proposes ensemble-based weighted Bellman backups, which re-weights target Q-values based on uncertainty estimates from a Q-ensemble. The authors empirically observe that the proposed method stabilizes and improves learning on both continuous and discrete control benchmarks. They also investigate the signal-to-noise aspect by studying environments with noisy rewards. Finally, they present a unified framework, coined SUNRISE, that combines the weighted bellman backups with an inference method that selects actions using highest upper confidence bounds (UCB) for efficient exploration."
SP:afc08f203562b841180811aef943bfb63a1659ea,"This paper proposes a method for calibrating the model for few-shot classification. The method is based on class-wise similarity between the support and query sets, which is used to measure the distributional mismatch between the two sets. The proposed method is algorithm-agnostic and can be easily extended to a range of meta-learning models. Experiments are conducted on CIFAR-10 and ImageNet to show the effectiveness of the method."
SP:12ae325ea3bce1e60195afac7d85895d2d20c29c,"This paper proposes a new method for video-text representation learning. The main idea is to use a generative model to naturally push these related samples together: each sample’s caption must be reconstructed as a weighted combination of other support samples’ visual representations. This simple idea ensures that representations are not overly-specialized to individual samples, are reusable across the dataset, and results in representations that explicitly encode semantics shared between samples. The proposed method outperforms others by a large margin on MSR-VTT, VATEX, ActivityNet, and MSVD."
SP:8a71d8fad25a126aff01431cacf348c05de75667,"This paper proposes a novel method, seg tok, to form the vocabulary of Chinese BERT, with the help of Chinese word segmentation (CWS) and subword tokenization. The authors also propose three versions of multi-vocabulary pretraining (MVP) to improve the models expressiveness. Experiments show that compared with char based vocab, the proposed method can improve the performances of Chinese PLMs on sentence level tasks, it can also improve efficiency; and it can improve PLMs’ downstream performance."
SP:b93ec7bc02b48068073ffe705f71d2643e663d51,"This paper proposes an unbiased boundary sampling strategy to enable efficient and scalable distributed GCN training while maintaining the full-graph accuracy. The proposed method is evaluated on several node classification and link prediction tasks. The authors claim that the proposed method can boost the throughput by up to 500% and reduce the memory usage by up-to 58% for distributed training, while achieving the same accuracy."
SP:2d4ba873d11e969ebd1fc31f9b5ab450c964d154,"This paper proposes a graph neural network (GNN) based quantum chemistry model for atomic simulations. The proposed model, called ForceNet, is based on graph convolutional network (GCN) to predict the per-atom forces of atoms in 3D molecular structure. The model is evaluated on the OC20 benchmark dataset for quantum chemistry simulations, where it is shown to achieve a 4x higher success rate than existing ML-based quantum chemistry models."
SP:8bdcf4fe6abf4739d4732b7ea8538513135dcccc,"This paper studies the impact of different regularization strategies when fine-tuning a pre-trained network for a new task. The authors provide a neural network generalization bound based on Rademacher complexity that uses the distance the weights have moved from their initial values. This bound has no direct dependence on the number of weights and compares favourably to other bounds when applied to convolutional networks. Inspired by this, the authors develop a simple yet effective fine tuning algorithm that constrains the hypothesis class to a small sphere centred on the initial pre- trained weights, thus obtaining provably better generalization performance than conventional transfer learning."
SP:3a3249e97ef2345ea2264de5ed8287e16687838e,This paper investigates the phenomenon of decoupling the hyperparameters for mask discovery and mask evaluation in unstructured magnitude pruning on vision classification tasks. The authors show that the best hyperparameter to train the regular model or evaluate the mask (Hfind) are often different from the best one (Heval) for finding the mask. They also show that different Hfind values yield masks with different layerwise pruning ratios and that the decoupled find-eval phenomenon is causally mediated by these ratios.
SP:2d6f5d72b21675f74ff4cde4d16bfb36abd5795f,"This paper proposes a new metric, called m-coherence, to study the alignment of per-example gradients during training. The metric is based on the Coherent Gradients (CG) theory, which provides a simple, unified explanation for memorization and generalization in neural networks. This paper shows the evolution of alignment of each example gradients in ResNet and EfficientNet models on ImageNet and several variants with label noise. The most surprising result is that memorization is not as strong as one might expect when training with completely random labels."
SP:e7c5de9a475d0ba71bc79580e8436024fb2c6f59,"This paper proposes a method to learn the summary statistics for implicit generative models (IGMs) where the evaluation of the likelihood function is intractable but sampling data from the model is possible. The authors frame the task of constructing sufficient statistics as learning mutual information maximizing representations of the data with the help of deep neural networks. They apply their approach to both traditional approximate Bayesian computation and recent neural likelihood methods, boosting their performance on a range of tasks."
SP:c5997bf2348e94949684f45fbd418661e85220c1,"This paper proposes an unsupervised image-to-image translation model (TUNIT) that simultaneously learns to separate image domains and translate input images into the estimated domains. TUNIT uses a differentiable clustering method based on mutual information maximization for estimating domain labels, a contrastive loss for embedding style codes, and a generative adversarial networks (GAN) to learn the image translation functions across various domains. Experimental results show that the proposed model achieves comparable or better performance than the set-level supervised model trained with full labels, generalizes well on various datasets, and is robust against the choice of hyperparameters."
SP:0cd97e64e638cabbeea0fdef3e9c5b33f4000f72,"This paper studies the implicit bias of gradient descent training of wide neural networks and the corresponding implicit bias in function space. The authors show that the solution of training a width-n shallow ReLU network is within n-1/2 of the function which fits the training data and whose difference from initialization has smallest 2-norm of the weighted second derivative with respect to the input. The curvature penalty function 1/ζ is expressed in terms of the probability distribution that is utilized to initialize the network parameters, and the authors compute it explicitly for various common initialization procedures. The analysis clarifies important details and allows them to obtain significant generalizations."
SP:8b885142facbb3b8db41ec9d83822cee81324694,"This paper studies the problem of weight decay in the context of adaptive gradient methods, such as Adam and Adam with decoupled weight decay. The authors claim that the L2 regularization is unstable weight decay for all optimizers that use momentum, and that decoupling weight decay is highly unstable for all adaptive gradients. They further propose the Stable Weight Decay (SWD) method to fix the instability problem from a dynamical perspective. The proposed SWD method makes significant improvements over the existing L2 and Decoupled Weight Decay methods. "
SP:a3206dc71e32ba1830895bf442d3840f3331a532,"This paper proposes a method to combine the strengths of both Translation Memory (TM) and Neural Machine Translation (NMT) by treating the matched sentence pair of TM as the additional signal and apply one encoder enhanced by the pre-trained language model (PLM) to encode the TM information and source sentence together. The authors extend the sentence level retrieval method to the n-gram retrieval method so that they don’t need to calculate the similarity score. Further, they explore new methods to manipulate the information flow from TM to the NMT decoder. The proposed methods are evaluated on a mixed test set of multiple domains."
SP:72b43991a242872b2ceb1861e8ffbdf26c9f4818,"This paper attempts to interpret modern deep (convolutional) networks from the principles of rate reduction and (shift) invariant classification. It shows that the basic iterative gradient ascent scheme for maximizing the rate reduction of learned features naturally leads to a deep network, one iteration per layer. The architectures, operators (linear or nonlinear), and parameters of the network are all explicitly constructed layer-by-layer in a forward propagation fashion. All components of this “white box” network have precise optimization, statistical, and geometric interpretation. The preliminary experiments indicate that such a network can already learn a good discriminative representation without any back propagation training."
SP:f8b02cf1b918b0956761829ec6ef9127596071ec,"This paper studies the implicit acceleration of gradient flow in over-parameterized two-layer linear models. The authors show that implicit acceleration emerges from a conservation law that constrains the dynamics to follow certain trajectories. More precisely, gradient flow preserves the difference of the Gramian matrices of the input and output weights and the amount of acceleration depends on both the magnitude of that difference (which is fixed at initialization) and the spectrum of the data. "
SP:e5f086c806be88d50e461a782b5b00124f4656fb,"This paper studies the problem of generating interpretable explanations for black-box machine learning models. The authors propose a new approach called CLIME, which is based on uniform sampling of subspaces of the input domain, which allows the user to specify the exact subspace to be sampled from. They also propose an efficient estimation algorithm to measure the true value of metrics such as fidelity up to any desired degree of accuracy, which can help in building trust in the generated explanations. Experiments demonstrate the effectiveness of the proposed approach."
SP:b1d5ef15772e192eb8c8a0e65b3c21ee7c794295,"This paper proposes AMBERT (A Multi-grained BERT), a pre-trained language model, on the basis of both fine-graged (i.e., words or sub-words in English and characters in Chinese) and coarse (coarse) tokenizations. The proposed model takes both the sequence of words (finegrained tokens) and the sequences of phrases as input after tokenization, employs one encoder for processing the words and the other encoder to process the phrases, and utilizes shared parameters between the two encoders. Experiments have been conducted on benchmark datasets for Chinese and English, including CLUE, GLUE, SQuAD and RACE, and the results show the model outperforms the existing best performing models in almost all cases."
SP:fd1cfe80343d3789227d99d836a5674374a234f5,"This paper proposes a Transformer-based semantic parsing model for semantic parsing. The main idea is to incorporate Long Short-Term Memory (LSTM) into the Self-Attention mechanism of the original Transformer to capture more local context of phrases. Experimental results show that the proposed model captures the detailed meaning better than Transformer, raises local context awareness and achieves strong performance on Geo, MSParS and Atis datasets."
SP:2056a65a7500d79465685af883083cd706277c1f,"This paper proposes a new adversarial training method, called CAT, to improve the robustness of deep neural networks (DNNs) against the combinations of multiple perturbations. The authors propose a new class of adversarial perturbation models, called composite adversarial models, which are composed of pixel and spatial transformations. The proposed method is able to flexibly integrate and optimize multiple adversarial robustness losses, which leads to DNNs robust with respect to multiple individual perturbated models as well as their “compositions”. "
SP:006e5b9ac9a8eb7223843731488bfefbd8eb09bd,"This paper proposes a method for learning abstract rules from a limited number of training examples and generalizing these rules to novel entities. The proposed method is based on a recurrent network augmented with an external memory that enables a form of variable-binding and indirection. This binding mechanism allows symbol-like representations to emerge through the learning process without the need to explicitly incorporate symbol-processing machinery, enabling the model to learn rules in a manner that is abstracted away from the particular entities to which those rules apply. The authors show that the proposed method outperforms a number of other competitive neural network architectures on a suite of tasks involving relationships among images governed by abstract rules."
SP:4171ce45966ac499f51450a19fb233934c0847f0,"This paper proposes a new framework, Translation between Augmented Natural Language (TANL), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of tackling the problem by training task-specific discriminative classifiers, the authors frame it as a translation task between augmented natural languages, from which the task-relevant information can be easily extracted. The proposed approach can match or outperform task- specific models on all tasks, and in particular, achieves new state-of-the-art results on CoNLL04, ADE, NYT, and ACE2005 datasets. It achieves this while using the same architecture and hyperparameters for all tasks."
SP:8f1b2fc6829e0bdfcc981020b0dcf3e63a947910,"This paper studies the problem of unlabeled entity problem in NER, where the entities of a sentence may not be fully annotated. The authors claim that two causes of performance degradation are identified: (1) reduction of annotated entities, and (2) treating unlabelled entities as negative instances. Based on the above observations, the authors propose a general approach, which can almost eliminate the misguidance brought by unllabelled entities. The key idea is to use negative sampling that, to a large extent, avoids training NER models with unlabeling entities. Experiments on synthetic datasets and real-world datasets show that the proposed approach is robust to unlabelED entity problem and surpasses prior baselines."
SP:dd76ece8d92a8a230a8b43033d8cb2368c677a94,"This paper proposes a novel acoustic word embedding called Acoustic Neighbor Embeddings where speech or text of arbitrary length are mapped to a vector space of fixed, reduced dimensions by adapting stochastic neighbor embedding (SNE) to sequential inputs. The Euclidean distance between coordinates in the embedding space reflects the phonetic confusability between their corresponding sequences. Two encoder neural networks are trained: an acoustic encoder that accepts speech signals in the form of frame-wise subword posterior probabilities obtained from an acoustic model and a text encoders that accepts text in the forms of subword transcriptions. Compared to a triplet loss criterion, the proposed method is shown to have more effective gradients for neural network training. It also gives more accurate results with low-dimensional embeddings when the two encoder networks are used in tandem in a word (name) recognition task and when the text encoder network is used standalone in an approximate phonetic matching task."
SP:9142189126b8612ac0acee6fe18a0cfcb70b6545,"This paper considers the problem of finding the Nash equilibrium of stationary mean-field games (MFG) with infinite number of agents via reinforcement learning. In particular, the goal is to learn a pair of mean field state and stationary policy that constitutes the Nash equilibria. The paper proposes a fictitious play algorithm that alternatively updates the state and the policy via gradient descent and proximal policy optimization, respectively. Theoretical results show that the proposed algorithm converges to the Nash Equilibrium at a sublinear rate."
SP:c498f8a199da1818fe64ed88b0825c5aad688aec,"This paper proposes a framework for approximate probabilistic inference on the joint distribution defined by a normalizing flow model. Specifically, the authors train a new generative model with the property that its composition with the given model approximates the target conditional distribution. By parametrizing this new distribution as another flow model, they can efficiently train it using variational inference and handle conditioning under arbitrary differentiable transformations. The resulting approximate posterior remains a flow, it offers exact likelihood evaluation, inversion, and efficient sampling."
SP:1d0f27f61c9d32911b8bd15d6b82ef5eec644f0f,"This paper introduces a new ultra-high resolution image segmentation dataset for the cell membrane, called U-RISC, which is the largest annotated Electron Microscopy (EM) dataset with multiple iterative annotations and uncompressed high-resolution raw data. The authors also propose a new evaluation criterion called Perceptual Hausdorff Distance (PHD) to measure the quality of cell membrane segmentation results. Experiments on human subjects show that PHD is more similar to human perception than the popular segmentation methods."
SP:8ca7aff87c82be69c9542550c814f52c9419ab0a,"This paper proposes a new benchmark for continual learning (CL) that aims to evaluate the ability to transfer knowledge from previous tasks and to scale memory and compute sub-linearly with the number of tasks. The authors also propose a new modular architecture, whose modules represent atomic skills that can be composed to perform a certain task. The learning algorithm leverages a task-driven prior over the exponential search space of all possible ways to combine modules, enabling efficient learning on long streams of tasks and better transfer and ability to scale to a hundred tasks."
SP:cc819c61f408e88f247eb87946187ccec3dad32e,"This paper proposes a generative approach for generating synthetic training data for few-shot meta-learning. The idea is to interpolate between in-class and out-of-class pairs in the latent space of the generative model to generate synthetic data for meta-training. The proposed method is evaluated on CIFAR-10/100 and ImageNet, and compared with several baselines. The results show that the proposed method outperforms or is competitive with the baselines in most cases. "
SP:b25771e5c214a352f74ba6196fbd88bca6c43c98,"This paper studies injectivity of fully-connected and convolutional ReLU layers and networks. The authors show that global injectivity with iid Gaussian matrices, a commonly used tractable model, requires larger expansivity between 3.4 and 10.5. They also characterize the stability of inverting an injective network via worst-case Lipschitz constants of the inverse. Finally, they show that an end-to-end—rather than layerwise—doubling of the dimension suffices for injectivity."
SP:a95a153d3fe9bcf535ebf8514f51d00df483f210,"This paper proposes the continuous conditional generative adversarial network (CcGAN), which is the first generative model for image generation conditional on continuous, scalar conditions (termed regression labels). The regression labels are scalar and infinitely many, and conventional label input methods (e.g., combining a hidden map of the generator/discriminator with a one-hot encoded label) are not applicable. The proposed CcGAN solves the above problems by (S1) reformulating existing empirical cGAN losses to be appropriate for the continuous scenario; and (S2) proposing a novel method to incorporate regression labels into the generator and the discriminator. "
SP:10dd09ab315870631d1451d200f2c87a023f8226,"This paper proposes to combine semi-supervised learning (SSL) and active learning (AL) to reduce the sample complexity of supervised deep learning (DL). In particular, the authors argue that the annotation efficiency brought by AL algorithms that seek diversity on labeled samples can be improved upon when using SSL as the training scheme. The authors propose an AL algorithm that focuses on controlling the convergence rate of a classification network by actively querying instances to improve the rate of convergence upon inclusion to the labeled set. The experiments show that a deep neural network trained using a combination of the proposed method and SSL algorithm can achieve high performance using far less labeled samples than SL."
SP:7f3947c3fa5b09674507d8f3e10d9280376ecb94,"This paper studies federated learning (FL) from a communication perspective, and proposes a dynamic regularizer for each device at each round, so that in the limit the global and device solutions are aligned. Specifically, in each round the device objective is dynamically modified with a penalty term so that, when model parameters converge, they do so to stationary points of global empirical loss. Theoretical analysis of the convergence of the proposed algorithm is provided. Empirical results on real and synthetic data show that the proposed method leads to efficient training, in both convex and non-convex settings, while being fully agnostic to device heterogeneity and robust to large number of devices."
SP:a3fbb073b0e2371b20d5d9df6ab829673f90354f,"This paper focuses on accelerating contrastive learning algorithms with little or even no loss of accuracy. The authors claim that the similarity on the intermediate layers is a good surrogate of the final similarity, and exploit this observation by introducing additional intermediate contrastive losses. They truncate the back-propagation and update only a part of the parameters for each gradient descent update. They also do selection based on intermediate losses to filter easy regions for each image, which further reduces the computational cost. They apply their method to recently-proposed MOCO (He et al., 2020), SimCLR (Chen et al. 2020a), SwAV (Caron et al, 2020b), and MocO V2 (C Chen et al 2020b)."
SP:5b5e705ea1ee1b857e17e64d560a39052804949d,"This paper studies the global convergence and global optimality of actor-critic in the single-time-scale setting, where the actor and critic are updated simultaneously. Specifically, the critic update is obtained by applying the Bellman evaluation operator only once while the actor is updated in the policy gradient direction computed using the critic. The authors consider two function approximation settings where both the agent and the critic are represented by linear or deep neural networks. For both cases, they prove that the actor sequence converges to a globally optimal policy at a sublinear O(K-1/2) rate, where K is the number of iterations. Moreover, under the broader scope of policy optimization with nonlinear function approximation, the authors prove that actorcritic with deep neural network finds the global optimal policy for the first time."
SP:26705a4dc305cce336f657c5937d1f5b4209548a,"This paper proposes to represent logs at a few levels of abstraction including field level, log level, and log sequence level. The representation is in vector format and serve as interfaces to downstream applications. The authors use a version of Transformer Networks (TNs) to encode numerical and textual information that is suitable for log embeddings. They show how a number of log processing applications can be readily solved with the representation."
SP:165c51a16f17fb8726e968f8b34742b62011d60e,"This paper proposes a method to constrain the behavior of convolutional layers by splitting them into a succession of wavelet packet decompositions, which are modulated by freely-trained mixture weights. The proposed method is evaluated on the AlexNet architecture for image classification. The experiments show that it achieves the accuracy rate of standard AlexNet, but with a significantly lower number of parameters, and an interpretation of the properties."
SP:d0a284da462584724ba6a3a48c9e986d391233f6,"This paper proposes an attention mechanism for both the players and the coach in multi-agent reinforcement learning (MARL) with variable number of agents. Specifically, the players have a partial view of the environment, while the coach has a complete view. The coach coordinates the players by distributing individual strategies. The authors also design an adaptive communication method to let the coach decide when to communicate with different players. The proposed method is evaluated on resource collection tasks in the Multi-agent particle environment. "
SP:4eb662b527d556758aaa1a0b589495fcc337fad0,"This paper provides a comprehensive empirical study of influence functions in deep neural network models. The authors show that the network architecture, its depth and width, as well as the extent of model parameterization and regularization techniques have strong effects on the accuracy of the influence functions. In particular, they show that (i) influence estimates are fairly accurate for shallow networks, while for deeper networks the estimates are often erroneous; (ii) for certain network architectures and datasets, training with weight-decay regularization is important to get high-quality influence estimates; and (iii) the accuracy can vary significantly depending on the examined test points."
SP:5fea74a2031d097a99dacf613bedcb054b0c3831,"This paper studies the connection between the pretraining task of next word prediction and text classification. The authors hypothesize and verify empirically that classification tasks of interest can be reformulated as sentence completion tasks, thus making language modeling a meaningful pre-training task. Theoretical results show that language models that are -optimal in cross-entropy (log-perplexity) learn features that can linearly solve such classification tasks with $O(\sqrt{\epsilon^2)$ error, thus demonstrating that doing well on language modeling can be beneficial for downstream tasks. "
SP:a67da438e9821010284416170c3699ae7ff96c99,"This paper proposes a new approach for membership inference attacks (MIA) for conditional image generation models (e.g. image translation). The proposed approach is based on the reconstruction error, which is computed on the pixel-wise reconstruction error of the model. The reconstruction error is used to distinguish between easy and hard images. The difficulty score is computed using the accuracy of a linear predictor computed over a given image, predicting pixel values from deep features of that image. The proposed method is evaluated on an extensive number of benchmarks."
SP:6fe23ebe09f2a4e42a21598f8e9c79edeca99863,"This paper proposes a Dirichlet Neural Architecture Search (DrNAS) method for neural architecture search (NAS) by formulating it into a distribution learning problem. The proposed method treats the continuously relaxed architecture mixing weight as random variables, modeled by Dirichlett distribution, which can be optimized with gradient-based optimizer in an end-to-end manner. This formulation improves the generalization ability and induces stochasticity that naturally encourages exploration in the search space. To alleviate the large memory consumption of differentiable NAS, the authors propose a simple yet effective progressive learning scheme that enables searching directly on large-scale tasks, eliminating the gap between search and evaluation phases. Extensive experiments demonstrate the effectiveness of DrNAS."
SP:c590d0ed2487b42480b53fc077546a4a0bc27a78,"This paper proposes a multiplicative filter network (MFN) for representing low-dimensional but complex functions. The authors argue that MFNs are a simpler class of function approximators than SIREN and Fourier feature networks, and that they are as good as or better than them on the tasks of 3D signed distance fields and neural radiance fields. The main advantage of MFNs is that the entire function can be viewed as a linear function over an exponential number of Fourier or Gabor basis functions, respectively."
SP:f5be855300f63c185a006834302bd4b033b56258,"This paper proposes a teacher-student scheme to enable gradient-based meta-learning algorithms to explore long horizons by the inner loop. The key idea is to employ a student network to adequately explore the search space of task-specific models (e.g., by more than ten steps), and a teacher network then takes a “leap” toward the regions probed by the student. The teacher not only arrives at a high-performing model but also defines a lightweight computation graph for the outer loop for the meta-gradients. The proposed approach is generic; it performs well when applied to four algorithms over three tasks: few-shot learning, long-tailed classification and meta-attack."
SP:0361e02d56b7d121cb5ede1cb582284cc18fc599,"This paper proposes a variant of the behavior regularized actor critic (BRAC) algorithm for offline reinforcement learning (RL). In particular, the authors propose to use state-dependent Lagrange multipliers for the regularization term to avoid distributing KL divergence penalty across all states of the sampled batch. They also propose a gradient penalty term to the policy evaluation objective to penalize the gradient of the Q value w.r.t. the out-of-distribution actions. The proposed method outperforms the baselines on several benchmark datasets."
SP:b2cfb380aa2a21f72f508b453cf5949257a5b4ec,This paper proposes Adjoined networks as a training approach that can regularize and compress any CNN-based neural architecture. The one-shot learning paradigm trains both the original and the smaller networks together. The parameters of the smaller network are shared across both the architectures. The authors prove strong theoretical guarantees on the regularization behavior of the adjoint training paradigm. They complement their theoretical analysis by an extensive empirical evaluation of both the compression and regularization behaviour of adjoint networks.
SP:dba40073f79143e5355d194aa16db9eee0267a5d,"This paper proposes a simple extension of the greedy exploration algorithm for reinforcement learning (RL). The authors claim that the main limitation of greedy exploration is its lack of temporal persistence, which limits its ability to escape local optima. They propose a temporally extended form of greedy that simply repeats the sampled action for a random duration. The authors show that, for many duration distributions, this extension improves exploration on a large set of domains."
SP:5efb581a368ace3bd085d48801a899559d6a43ef,"This paper studies the implicit regularization of gradient flow (GD) in matrix factorization. In particular, the authors show that GD with infinitesimal initialization is equivalent to Greedy Low-Rank Learning (GLR), a rank-minimization algorithm, under some reasonable assumptions. The authors also extend the results to the case where depth > 3, and show that the benefit of being deeper is that the above convergence has a much weaker dependence over initialization magnitude. "
SP:7f997cf7a63a7330fc12fd525516080c91a3cb9b,"This paper proposes CAMEL, a two-stage framework for improving robustness to subgroup differences. The first stage models subgroup features within a class and learns semantic transformations between them, and then trains a classifier with data augmentations that deliberately manipulate the subgroups features. The second stage is to use the transformations to remove the classifier’s dependence on subgroup-specific features. CAMEL uses CycleGAN to learn the intra-class, inter-subgroup augmentations, and balances subgroup performance using a theoretically-motivated subgroup consistency regularizer, accompanied by a new robust objective. The effectiveness of CAMEL is demonstrated on 3 benchmark datasets."
SP:de6cea1e35a0555175e17546a93422e9a96a511e,"This paper proposes a new classifier, named Rule-based Representation Learner (RRL), that automatically learns interpretable nonfuzzy rules for data representation. To train the non-differentiable RRL effectively, the authors project it to a continuous space and propose a novel training method, called Gradient Grafting, that directly optimize the discrete model using gradient descent. An improved design of logical activation functions is also devised to increase the scalability of RRL and enable it to discretize the continuous features end-to-end. Experiments on 9 small and 4 large data sets show that RRL outperforms the competitive approaches."
SP:e36388a9452e557dd51bf0170bf2f9da22271a49,"This paper proposes a regret minimization (RGM) algorithm and its extension for structured environments. RGM builds from invariant risk minimization by recasting simultaneous optimality condition in terms of predictive regret, finding a representation that enables the predictor to compete against an oracle with hindsight access to held-out environments. The structured extension adaptively highlights variation due to complex environments via specialized domain perturbations. The authors evaluate the method on multiple applications: molecular property prediction, protein homology and stability prediction and show that RGM significantly outperforms existing baselines."
SP:cad3ed2fba57faf17a3e8899dc5a744d5358aa68,"This paper proposes a cross-probe BERT architecture for large-scale cross-modal text-to-image retrieval. The proposed method relies on devised text and vision probes, and the cross-Modal attentions are conducted on text and image probes. It takes lightweight computation cost, and meanwhile effectively exploits crossmodal attention. The experiments conducted on two public benchmarks demonstrate state-of-the-art effectiveness and efficiency."
SP:51fd82de525fcb738fdeaeeae20fbb2cdf975f0c,"This paper proposes a new type of Actor, named forward-looking Actor or FORK, for Actor-Critic algorithms. It can be easily integrated into a model-free ActorCritic algorithm. The experiments on six Box2D and MuJoCo environments with continuous state and action spaces demonstrate significant performance improvement FORK can bring to the state-of-the-art algorithms."
SP:6e730239e6e8b43c4988dd61dca30f15dc039ef7,"This paper proposes a new aggregation method for federated learning (FL) based on Bayesian inference. In particular, the authors propose to sample higher-quality global models and combine them via Bayesian model Ensemble, leading to much robust aggregation. The authors show that an effective model distribution can be constructed by simply fitting a Gaussian or Dirichlet distribution to the local models. The empirical studies validate FEDBE’s superior performance, especially when users’ data are not i.i.d. and when the neural networks go deeper."
SP:3ac5f437fc349a33810d0645664d1c448528af74,"This paper investigates the role of sub-components of BERT in the contextualization of grammatical concepts such as subject-verb agreements (SVA) and reflexive anaphora (RA) in NLP models such as BERT. In particular, the authors propose a graph-based method to analyze the impact of an input concept (e.g. subject’s number) on the output concept (i.e. corresponding verb) through a collection of paths passing through a sequence of model nodes and/or edges. They also propose a search procedure for finding patterns representative of concept-critical paths that let them selectively explore the importance of chosen aspects of a model. They further extend the experimental framework to integrate impacts of multiple words towards a given concept (as opposed to impact of a single word)."
SP:efa2343ead47263a0d09e1c17f9aa044605b9650,"This paper studies the problem of finite-time convergence of deep neural networks. The authors formulate the supervised learning framework as a control problem where weights of the network are control inputs, and learning translates into a tracking problem. They show that the loss function is a Lyapunov function of the weights, and derive an upper bound on the settling time of the deep neural network. They also show that their loss function can be robust against input perturbations."
SP:7a0ded4b3b2d08d43765ff7b722da9b9863aabd6,"This paper studies disentanglement in generative models. The authors point out that the method taken by GIN for informative latent variables selection is not theoretically supported and can be disproved by experiments. Instead, they propose to use the mutual information between each learned latent variables and the auxiliary variable to correctly identify the “informative latent variables”. They show the improvement brought by their method in experiments on synthetic data."
SP:0d9ba12bbf47b13a46c2225f9dc06878418daaea,"This paper proposes two new bidirectional pooling operations, namely LiftDownPool and LiftUpPool, for improving the robustness of convolutional neural networks to perturbations and corruptions. The proposed methods are based on the classical lifting scheme, which decomposes a feature map into various downsized sub-bands, each of which contains information with different frequencies. As the pooling function is perfectly invertible, a corresponding up-pooling layer is able to generate a refined upsampled feature map using the detail sub-band, which is useful for image-to-image translation challenges. Experiments show the proposed methods achieve better results on image classification and semantic segmentation tasks."
SP:147239edceb17bade6ea5d3dca44e3a59998aa47,"This paper proposes a fast, distance-preserving, binary embedding algorithm to transform a high-dimensional dataset T into binary sequences in the cube {±1}. When T consists of well-spread (i.e., non-sparse) vectors, the proposed embedding method applies a stable noise-shaping quantization scheme to Ax where A is a sparse Gaussian random matrix. The authors also show that Euclidean distances among the elements of T are approximated by the l_1 norm on the images of the cube. The proposed method is both fast and memory efficient, with time complexity $m$ and space complexity $O(m)$ on well- spread data."
SP:f65e229bca3904095743e7a501b1083cc60f1e22,"This paper proposes to use plasticity rules as a proxy for Gradient Descent (GD) in order to improve the generalization and robustness of RNNs. Plasticity rules are rules that control the strength of a synapse based on the firing history as seen at the post-synaptic neuron. The authors propose to learn the parameters of the plasticity rule by GD on the rule parameters and show that this process can be learned efficiently. They also argue that applying GD to learning rules is biologically plausible, in the sense that it is learned over evolutionary time. "
SP:f435530146fa975cb27cd375a857df9bcbd87682,"This paper proposes a new approach for visual question generation (VQG) task, which aims to generate human-like questions from an image and potentially other side information (e.g. answer type or the answer itself). The authors propose a novel learning paradigm to generate visual questions with answer-awareness and region-reference. In particular, they aim to ask the right visual questions using Double Hints textual answers and visual regions of interests, effectively mitigating the existing one-to-many mapping issue. They develop a simple methodology to self-learn the visual hints without introducing any additional human annotations. Furthermore, they propose a new double-hints guided Graph to Sequence (GTS) learning framework that first models them as a dynamic graph and learns the implicit topology. The proposed method outperforms existing state-of-the-art baselines on VQA2.0 and COCO-QA datasets."
SP:53a26ce11647866d3f6ba8b84ca9f13106197a8d,"This paper studies the effect of optimally tuned regularization on the generalization properties of linear regression with isotropic data distribution. Theoretically, the authors show that optimally-tuned l2 regularization achieves monotonic test performance as we grow either the sample size or the model size. Empirically, they demonstrate empirically that the optimally tunable l2 can mitigate double descent for more general models, including neural networks."
SP:c193ccc74b987beaf8d53a29a8529a0af5e87742,"This paper introduces a spatial dependency network (SDN) for building image generators and applies it to variational autoencoders (VAEs). The authors show that augmenting the decoder of a hierarchical VAE by spatial dependency layers considerably improves density estimation over baseline convolutional architectures and the state-of-the-art among the models within the same class. In a vanilla VAE setting, the authors also show that a powerful SDN decoder also improves learning disentangled representations."
SP:db91512a90e75675af03c2f197751c8526d6f5e9,"This paper proposes a simplified version of BCQ (Fujimoto et al., 2018a) for offline RL, which removes a heuristic design choice and naturally restricts extracted policies to remain exactly within the support of a given behavior policy. The authors derive this simplified algorithm through the introduction of a novel backup operator, Expected-Max Q-Learning (EMaQ), which is more closely related to the resulting practical algorithm. Specifically, in addition to the distribution support, EMaQ explicitly considers the number of samples and the proposal distribution, allowing them to derive new sub-optimality bounds which can serve as a novel measure of complexity for offlineRL problems. In the offline RL setting – the main focus of this work – EMaq matches and outperforms prior state-of-the-art in the D4RL benchmarks, and in the online RL setting, it is competitive with Soft Actor Critic (SAC)."
SP:e2b80adeaa9208e0667a64a3f24661f77b48e487,"This paper proposes a batch selection algorithm for improving the fairness of a pre-trained model by adjusting the batch size for sensitive groups based on the fairness measure of an intermediate model, measured in the current epoch. The proposed algorithm, which the authors call FairBatch, implements this optimization and supports prominent fairness measures: equal opportunity, equalized odds, and demographic parity. The main contribution of this paper is to develop an algorithm (FairBatch) that implements the optimization via adjusting batch sizes w.r.t. sensitive groups. Experiments conducted both on synthetic and benchmark real data demonstrate that the proposed algorithm can provide such functionalities while achieving comparable (or even greater) performances against the state of the arts."
SP:72f26b850bb2258223c0fc71598e35ad07d690e6,"This paper studies the Lipschitz constants of monotone deep equilibrium (monDEQ) models, a recently proposed subclass of DEQs. The authors show that monDEQs have Lipschtiz constants that can be bounded as a simple function of the strong monotonicity parameter of the network. They also show how to use these bounds to develop PAC-Bayes generalization bounds that do not depend on any depth of the networks and avoid the exponential depth-dependence of comparable DNN bounds."
SP:bcfd4d7fd4590e3bc248a0a5422ce4b67db74a74,"This paper proposes a unified approach for goal-conditioned reinforcement learning and imitation learning. The main idea is to use density estimation to estimate the likelihood of reaching a state conditioned on a goal, and then use this estimate to train an imitation learning algorithm that is able to learn from minimal amounts of expert data using self-supervised environment interactions. The proposed approach is evaluated on Mujoco tasks, where it is shown to be able to match the performance of Hindsight Experience Replay (HER) in deterministic domains and outperform it in stochastic domains."
SP:d57550b2f323b356d7e609acc35ee33039f376b4,"This paper proposes a variational multi-task learning (VMTL) method for multi-tasks learning. The proposed method is based on variational inference for feature representations and classifiers jointly. In particular, the proposed method uses Gumbel-softmax priors to condition the prior of each task on related tasks. The mixing weights are learned in a data-driven manner for each individual task. Experiments on four benchmark datasets demonstrate the effectiveness of the proposed VMTL."
SP:3ccdf8322f16c8a7bef82e32fad4c03969a510d1,"This paper proposes a systematic and unified benchmark, Long-Range Arena, specifically focused on evaluating model quality under long-context scenarios. The benchmark is a suite of tasks consisting of sequences ranging from 1K to 16K tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. The authors systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers,. Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on the newly proposed benchmark suite."
SP:e12e410c3335b76133ceda4c865b244fbbab8580,"This paper proposes a code summarization model that combines the representation learning on both the abstract syntax tree (AST) and the source code (context) of the program. The authors also propose a multilingual version of the model, which is trained on five different programming languages. The proposed model is evaluated on the monolingual and multi-lingual versions of the five programming languages, and the results show that the proposed model outperforms the state-of-the-art on all tasks."
SP:f46e98d48f90071831f1c0069bf74a7993be6db8,"This paper proposes a reinforcement learning approach for the task of audio-visual navigation, in which an agent navigates through a complex, unmapped 3D environment using both sights and sounds to find a sound source (e.g., a phone ringing in another room). The proposed approach consists of two key elements: (1) waypoints that are dynamically set and learned end-to-end within the navigation policy, and (2) an acoustic memory that provides a structured, spatially grounded record of what the agent has heard as it moves. The approach is evaluated on two challenging datasets of real-world 3D scenes, Replica and Matterport3D."
SP:23bfe317dcef00a91ea92389b3f39d9b93972454,"This paper studies the lottery ticket hypothesis, which suggests that neural networks rely on lucky random initial weights of subnetworks called “lottery tickets” that converge quickly to a solution (Frankle & Carbin, 2018). To investigate how weight initializations affect performance, they examine small convolutional networks that are trained to predict n steps of the two-dimensional cellular automaton Conway’s Game of Life, the update rules of which can be implemented efficiently in a small CNN. They find that networks of this architecture trained on this task rarely converge. Rather, networks require substantially more parameters to consistently converge. Furthermore, they find that the initialization parameters that gradient descent converges to the solution are sensitive to small perturbations, such as a single sign change. They observe a critical value d0 such that training minimal networks with examples in which cells are alive with probability d0 dramatically increases the chance of convergence."
SP:1b5ba618d3e28d48f9205c0780f8288a08fa5392,"This paper proposes a new semi-supervised learning method for image classification. The proposed method is based on the FixMatch SSL method (Sohn et al., 2020), which utilizes pseudo-labeling and consistency regularization to produce artificial labels for unlabeled data. The authors propose a new objective function, dubbed BatchMean Triplet loss, which has the advantage of computational efficiency while taking into account all input samples. Experiments on CIFAR-10 and SVHN datasets show the effectiveness of the proposed method."
SP:f3abccf4a2566ffbc821aba209fab15058639ad4,"This paper studies the problem of meta-learning in the online incremental learning setting, where the goal is to learn a sequence of tasks, and datapoints from each task are received sequentially. In this setting, the authors propose an algorithm that can generalize from many-shot to zero-shot learning at the beginning of the learning process, and from the middle of learning to the end of learning. The authors show that the proposed algorithm is able to learn the full task set with fewer overall labels and achieves greater cumulative performance compared to standard supervised methods. "
SP:95cb420d92ec42e12a4bbb0e66224f1c498a7161,"This paper proposes a series of probes to analyze the representations of pretrained Transformer models in terms of their sensitivity to syntactic structure. The probes are based on the notion of representational invariance, which means that the representations are invariant to small changes in the input sentence structure. Three types of perturbations are used: random permutations of n-grams of varying width, swapping of two spans which do or do not form a syntactic phrase, and swapping of adjacent words that do or don't break apart the syntactic sentence. The results suggest that the Transformer model is sensitive to larger parts of the sentence along its layers and that hierarchical phrase structure plays a role in this process."
SP:cb27b27a6fefc192ad1c2bd083d13eb9e51a5c44,This paper proposes a method to train a GAN on high-resolution images with low computational cost and few training samples. The authors propose a skip-layer channel-wise excitation module and a self-supervised discriminator trained as a feature-encoder with an extra decoder. The proposed method is evaluated on a variety of image datasets and achieves state-of-the-art performance.
SP:c0dbeb5d94b2388595cf7ad9675c55df0bac7f8e,"This paper proposes a specialised dual solver for neural network bounding. The main contribution is a novel dual algorithm that realises the full potential of the new relaxation by operating on a small active set of dual variables. The method shares the benefits of previous dual approaches for weaker relaxations: massive parallelism, GPU implementation, low cost per iteration and valid bounds at any time. The proposed method achieves better bounds than off-the-shelf solvers in only a fraction of their running time."
SP:56e3837417dbcce0d65338dc3aac4e1a20eb0df8,"This paper proposes a method for augmenting pretrained language models with concept-centric commonsense knowledge for NLU and NLG tasks. The authors propose two kinds of self-supervised pre-training objectives: (1) concept-to-concept (C2S) and (2) concept order (COR) to encourage the pre-trained model to compose sentences and perform compositional reasoning, and (3) generative and contrastive objectives for learning common sense from the text. The proposed method, called concept-aware language model (CALM), is evaluated on NLU/NLG tasks and shows better performance than the baselines."
SP:7ec69bdee021af506293c87a3b75bce1c40a03d7,"This paper proposes a self-supervised method for unsupervised physical object discovery. The proposed method uses multi-scale pixel cues and physical motion cues to accurately segment observable and partially occluded objects of varying sizes, and infer properties of those objects. The discovered object properties can also be used to reason about physical events. The model is evaluated on both synthetic and real-world scenes."
SP:66997bc19a3ba6548fcf21f114e748bea95cad1c,"This paper proposes a method to improve the robustness of deep neural networks (DNNs) against adversarial attacks. Specifically, the authors propose to increase the margins of training samples by moving the decision boundaries of the DNN model far away from the training samples to improve robustness. The proposed method is evaluated on six publicly available datasets (including a COVID-19 CT image dataset) under strong 100-PGD white-box attacks. The results show that the proposed method significantly improved classification accuracy on noisy data while keeping a relatively high accuracy on clean data."
SP:276ffd59fbf49e3ee02756da8920218102214917,"This paper proposes ProKT, a new model-agnostic method for knowledge distillation by projecting the supervision signals of a teacher model into the student’s parameter space. The projection is implemented by decomposing the training objective into local intermediate targets with approximate mirror descent technique. The proposed method could be less sensitive with the quirks during optimization which could result in a better local optima. Experiments on both image and text datasets show that the proposed ProKT consistently achieves the state-of-the-art performance."
SP:906dc21d6988953fcf57d63bbdd12973e5818d16,"This paper proposes a channel pruning method for compression and acceleration of convolutional neural networks. The proposed method is inspired by hypernet, where a hyper-structure network is used to generate the architecture of the main network, and a regularization term is added to specify the computational resource of the compact network. This paper further introduces learnable layer-wise scaling factors to balance the gradients from different terms, and they can be optimized by hyper-gradient descent. Extensive experimental results on CIFAR-10 and ImageNet show that the method is competitive with state-of-the-art methods."
SP:890fd9454596c051b0e9535baf73b1dd1fae67ca,"This paper proposes a method to train a theorem prover to prove higher-order logic theorems in the presence of a large knowledge base of potential premises without learning from human proofs. The main idea is to augment the exploration of premises based on a simple tf-idf (term frequency-inverse document frequency) based lookup in a deep reinforcement learning scenario. The proposed method, dubbed DeepHOL Zero, outperforms provers that are trained only on human proofs and approaches the performance of a prover trained by a combination of imitation and reinforcement learning."
SP:88209417a8ad07e6103084e41709be900303ce5f,"This paper proposes an automated data augmentation approach called MODALS (Modalityagnostic Automated Data Augmentation in the Latent Space) to augment data for any modality in a generic way. MODALS exploits automated data augmentations to fine-tune four universal data transformation operations in the latent space to adapt the transform to data of different modalities. Through comprehensive experiments, the authors demonstrate the effectiveness of MODALS on multiple datasets for text, tabular, time-series and image modalities for various machine learning tasks."
SP:6d84670d321b0d584b097c630574bd748e85c9a2,"This paper studies the global convergence of three-layer neural networks in the mean field regime. The main contribution of this paper is a global convergence result for unregularized feed-forward networks with arbitrary width and depth. The global convergence results are based on the universal approximation property, which is shown to hold at any finite training time (not necessarily at convergence) via an algebraic topology argument."
SP:b90f893f927db9c439595fd119a565cf43c971f4,"This paper proposes a method for learning explanations of expert decisions by modeling their reward function in terms of preferences with respect to “what if” outcomes: Given the current history of observations, what would happen if we took a particular action? To learn these costbenefit tradeoffs associated with the expert’s actions, the authors integrate counterfactual reasoning into batch inverse reinforcement learning (IRL) to learn these reward functions and explain expert behavior. The proposed method is evaluated in both simulated and real-world medical environments."
SP:c92916780418bfa7f0796fd9766b6d28b9eea5ef,"This paper investigates the effect of using morphological information in graph neural networks (GNNs) in the context of multi-task reinforcement learning (MTRL). In particular, the authors show that GNNs do not improve the performance of existing MTRL methods that use such information. They also propose a new GNN-based method, AMORPHEUS, that uses transformers to replace the information encoded in the GNN. The authors claim that the benefit of using such information is outweighed by the difficulty of message-passing due to sparsely connected graphs."
SP:2cf58f5cac20dccdc2034ef60e8e46b7988ebd7d,"This paper proposes a method for visual counting, which aims to predict the number of occurrences given a natural image and a query (e.g. a question or a category). Unlike most prior works that use explicit, symbolic models which can be computationally expensive and limited in generalization, the authors propose a simple and effective alternative by revisiting modulated convolutions that fuse the query and the image locally. The proposed method is called MoVie, short for Modulated ConVolutional Bottlenecks, and it is shown to be able to improve the state-of-the-art on counting-specific VQA tasks while being more efficient."
SP:c64e77507e562f236cb69361b22fb1a7951ffb22,This paper proposes a model-targeted poisoning attack that can target a desired model based on online convex optimization. The attack comes with provable convergence to any achievable target classifier. The distance from the induced classifier to the target model is inversely proportional to the square root of the number of poisoning points. The authors also provide a lower bound on the minimum number of poisonings needed to achieve a given target classifiers.
SP:a526023ec4cb839b83c574d31f59a9a67bc7af00,"This paper proposes a method to improve the efficiency of binarized deep learning models for real-time point cloud applications. The authors claim that the performance drop mainly stems from two challenges: aggregation-induced feature homogenization that leads to a degradation of information entropy, and scale distortion that hinders optimization and invalidates scale-sensitive structures. To tackle these challenges, the authors introduce Entropy-Maximizing Aggregation (EMA) to modulate the distribution before aggregation for the maximum information entropy and Layer-wise Scale Recovery (LSR) to restore feature representation capacity. Extensive experiments show that the proposed method outperforms existing binarization methods by convincing margins, at the level even comparable with the full precision counterpart."
SP:825b4d1db0c537a607655bb5b4bf221ec672c8af,"This paper proposes and studies ways to improve the performance of Transformer-based models by adding trainable memory to selectively store local and global representations of a sequence. Specifically, the authors propose to add memory tokens to store non-local representations, creating memory bottleneck for the global information, and controlling memory update with a dedicated layer. Experiments on GLUE benchmark show that the proposed method improves the performance."
SP:f0fa1b7684bc605f6edd4813c44be20988fe8b4c,"This paper proposes a method for unsupervised representation learning that bridges contrastive learning with clustering. Specifically, the authors introduce prototypes as latent variables to help find the maximum-likelihood estimation of the network parameters in an Expectation-Maximization framework, and iteratively perform E-step to find the distribution of prototypes via clustering and M-step as optimizing the network via contrastive loss. The authors propose ProtoNCE, a generalized version of the InfoNCE loss, which encourages representations to be closer to their assigned prototypes. The proposed method outperforms state-of-the-art methods on multiple benchmarks."
SP:5342a5e1d87fd17b1a2efed967dbbfeafa440ee7,"This paper proposes a method to improve the robustness of deep neural networks (DNNs) against adversarial attacks. Specifically, the authors propose a block containing multiple paths to learn robust features and the parameters of these paths are required to be orthogonal with each other. Via forward learning and backward correction, one OMP block makes the neural networks learn features that are appropriate for all the paths and hence are expected to be robust. The proposed method can be posed in any layer of a neural network. The performance under both white-box and black-box attacks is better than the existing state-of-the-art adversarial defenders."
SP:776df66274ed12449fde8dcef873a593980f397c,"This paper proposes a self-supervised graph attention network (SuperGAT) for noisy graphs. Specifically, the authors exploit two attention forms compatible with a self supervised task to predict edges, whose presence and absence contain the inherent information about the importance of the relationships between nodes. By encoding edges, SuperGAT learns more expressive attention in distinguishing mislinked neighbors. Experiments on 17 real-world datasets demonstrates that the recipe generalizes across 15 datasets of them."
SP:80a05296d6b1e4c6e9e2df01938c73029ff8487d,"This paper proposes a method for DSMAD (dialogue system for medical automatic diagnosis) that is able to produce reliable and convincing diagnosing processes and also is robust in making diagnosis facing noisy interaction with patients. The authors propose a novel DSMAD agent, INS-DS (Introspective Diagnosis System), consisting of two separate yet cooperative modules, i.e., an inquiry module for proposing symptom-inquiries and an introspective module for deciding when to inform a disease. They also propose two evaluation metrics to validate the reliability and robustness of DSMAD methods. Extensive experimental results demonstrate the effectiveness of the proposed method."
SP:10ae09d90d465125433a9b4f15b1405ab017920d,"This paper proposes a batch-wise regularization based on the proposed Batch Confusion Norm (BCN) to flexibly address the natural world distribution which usually involves fine-grained and long-tailed properties at the same time. The BCN term can alleviate possible overfitting due to exploring image features of fine details. More importantly, BCN can learn to exert proper distribution of confusion strength over tailed and head categories to improve classification performance. While the resulting FGVC model by the BCN technique is effective, the performance can be consistently boosted by incorporating extra attention mechanism."
SP:90f1e0fe1e9678d1e9a4dcb519d4e8fd61098ce0,This paper proposes a method for inverse reinforcement learning (IRL) that learns an approximate posterior distribution over the reward that scales to arbitrarily complicated state spaces. This is achieved by using a variational approach to the latent reward. The proposed method is evaluated on real medical data and classic control simulations. 
SP:ccd251d95c0a2d8dc5ad2a148ec29955e105e71e,"This paper proposes a method to improve the performance of agents in partially observable POMDPs by sampling from an approximate auto-regressive counterfactual belief model that is learned as a supervised task. The proposed method, called Learned Belief Search (LBS), is based on the SPARTA algorithm, which estimates the expected return for every possible action in a given action-observation history (AOH). LBS is applied to the Hanabi benchmark, where it is shown to outperform the state-of-the-art methods in Hanabi. "
SP:db408e6bfe69a9b3984f3b27ca92b802aa37af42,"This paper proposes a method called Shoot Tree Search (STS) to control the depth and breadth of the search for planning in large state spaces. STS is an interpolation between two celebrated search mechanisms: MCTS and random shooting. It also lets the user control the bias-variance trade-off, akin to TD(n), but in the tree search context. The experiments on challenging domains show that STS can get the best of both worlds consistently achieving higher scores."
SP:5efc271ccc555fd9aa542548838170bd4c98e957,"This paper proposes a method for pre-training Transformer-based models on mathematical reasoning tasks. The main idea is to design synthetic tasks that are devoid of mathematical knowledge to ensure that only the fundamental reasoning biases can be learned from these tasks. Three synthetic tasks are designed to require the model to have deduction, induction, and abduction abilities. The proposed method is called LIME (Learning Inductive bias for Mathematical rEasoning) and is evaluated on three different mathematical reasoning benchmarks."
SP:bb8e0b554d3b3314fa343c902d9e60f1a141ea30,"This paper analyzes the inductive bias of gradient descent for weight normalized homogeneous neural nets, when trained on exponential or cross-entropy loss. The analysis focuses on exponential weight normalization (EWN), which encourages weight updates along the radial direction. This paper shows that the gradient flow path with EWN is equivalent to gradient flow on standard networks with an adaptive learning rate, and hence causes the weights to be updated in a way that prefers asymptotic relative sparsity. These results can be extended to hold for gradient descent via an appropriate adaptive learning rates."
SP:c71f9d2a602516865a0b103028186e83b52e5f00,"This paper proposes a method to tackle the mode collapse problem in GANs. The authors claim that mode collapse is caused by the discriminator’s inability to maintain classification accuracy on previously seen samples. Motivated by this observation, the authors propose a training procedure that dynamically spawns additional discriminators to remember previous modes of generation. The proposed method can be plugged into any existing GAN frameworks to mitigate mode collapse and improve standard metrics for GAN evaluation."
SP:52c48198c95826e042f9e5a512ef3265daaff882,"This paper proposes an approach to regularize BERT by pruning attention heads based on a proxy score for head importance. The proposed approach leverages reinforcement learning to automatically prune attention heads from BERT. Instead of relying on heuristics or rule-based policies, the proposed approach learns a pruning policy that determines which attention heads should or should not be pruned for regularization. Experimental results show that AUBER outperforms existing pruning methods by achieving up to 9.39% better accuracy."
SP:abcbbad146f1b0d5d579c215952c95e5499a378a,"This paper proposes a method for learning correspondence between two domains (i.e., simulation and real-world) by learning to translate between them with unpaired data. The proposed method is based on dynamics cycles that align dynamic robot behavior across two domains using a cycle-consistency constraint. Once this correspondence is found, the proposed method can directly transfer the policy trained on one domain to the other without any additional fine-tuning on the second domain. The method is evaluated on a variety of problem domains, both in simulation and on real robot."
SP:006434d56992836ab9420d7d4215bc70664de304,"This paper focuses on the problem of model-agnostic Shapley explainability, i.e., the attribution of a model’s predictions to its input features in a mathematically principled and model agnostic way. The authors propose two solutions to the problem: (1) generative modeling, which provides flexible access to data imputations; (2) directly learning the Shapley value-function, providing performance and stability at the cost of flexibility. The main contributions are twofold: 1) The authors show that off-manifold explanations are often incorrect, (ii) hide implicit model dependence on sensitive attributes, and (iii) lead to unintelligible explanations in higher-dimensional data. 2) On-Manifold explainability overcomes these problems."
SP:7cda6bccf08887c7cef66d0ac3ccefdea8f5d7c8,"This paper proposes a VAE-based method for opponent modelling in multi-agent reinforcement learning. The proposed method is based on the VAE encoder-decoder architecture, which is trained to reconstruct the local actions and observations of the opponent based on embeddings that depend only on the local observations. The learned embedding condition the policy of the controlled agent in addition to its local observation, and the policy and the embedding model are optimised concurrently during the RL learning process. The method is evaluated in two benchmark environments, the multiagent particle environment and level-based foraging. The results show that the proposed method achieves comparable performance to an ideal baseline which has full access to the opponent’s information, and significantly higher returns than a baseline method which does not use the learned embeddeddings."
SP:c239bc531bcf7293032748af29a1b786e9d893dd,"This paper proposes a consistency regularization method for contrastive learning on semi-supervised learning on unlabeled data. The proposed method, called Consistent Contrast (CO2), takes the similarity of the query crop to each crop from other images as a pseudo label, and encourages consistency between these two similarities. CO2 improves Momentum Contrastive Learning (MoCo) by 2.9% top-1 accuracy on ImageNet linear protocol, 3.8% and 1.1% accuracy on 1% and 10% labeled settings, and transfers to image classification, object detection, and semantic segmentation on PASCAL VOC."
SP:d18bab21790713e2facb053c47298fc9079ab783,"This paper studies the last-iterate convergence of Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weights Update (OMWU) for saddle-point optimization in bilinear games over the probability simplex. The authors show that when the equilibrium is unique, OGDA converges with a learning rate whose value is set to a universal constant, improving the result of (Daskalakis & Panageas, 2019b) under the same assumption. They also extend the results to more general objectives and feasible sets for the projected OGDA algorithm, by introducing a sufficient condition on the smoothness of the objective function."
SP:bbc7f77308b298c332a39747f693bc396f00a89f,"This paper proposes FedUV, a federated learning framework for training user verification (UV) models in the federated setup, where each user has access to the data of only one class and user embeddings cannot be shared with the server or other users. The proposed method is based on a secret user-defined linear combination of the embedding vectors. The authors show that choosing the linear combinations from the codewords of an error-correcting code allows users to collaboratively train the model without revealing their embedding vector. The experimental results for user verification with voice, face, and handwriting data show that FedUV is on par with existing approaches."
SP:40fa47cc0928e2925ef5ce6d808073f368ca2cd4,"This paper studies the geometry of the class manifolds (CMs) of deep neural network classifiers. The authors propose a simple technique to estimate the effective dimension of CMs as well as boundaries between multiple CMs, by computing their intersection with random affine subspaces of varying dimension. They provide a theory for the technique and verify that their theoretical predictions agree with measurements on real neural networks. Through extensive experiments, they leverage this method to show deep connections between the geometry, generalization, and robustness. Together a picture emerges that well-performing, robust models have higher dimensional CMs than worse performing models."
SP:09bce202ac7a750c3700a8ef3cd92cfe8ed00c39,"This paper proposes a novel approach to the exploration-exploitation trade-off in deep reinforcement learning (RL). The authors argue that the agent should explore more in an unfamiliar state, while less in a familiar state, so as to understand the environment more efficiently. To this end, they propose Curiosity-Aware entropy Temperature for SAC (CAT-SAC), which utilizes the curiosity mechanism in developing an instance-level entropy temperature. The curiosity is added to the target entropy to increase the entropy temperature for unfamiliar states and decrease the entropy for familiar states. By tuning the entropy specifically and adaptively, the agent is encouraged to explore when its curiosity is large, and vice versa."
SP:dce5eb20581a21c5de0a9fc07a8a79a1fbb28c71,"This paper proposes a meta-reinforcement learning (meta-RL) algorithm that is both efficient and extrapolates well when faced with out-of-distribution tasks at test time. The proposed method is based on the observation that dynamics models can be adapted efficiently and consistently with off-policy data, more easily than policies and value functions. The dynamics models are then used to generate synthetic experience for the new tasks without using meta-RL at all."
SP:34d78aa11f9d50baf75a9646a6f9128318c3389a,"This paper studies the problem of meta-learning with few-shot learning (FSL) in the presence of label noise. In particular, the authors cast the meta-overfitting problem (overfitting on sampling and label noise) as a gradient noise problem since few available samples cause meta-learner to overfit on existing examples (clean or corrupted) of an individual task at every gradient step. The authors propose Eigen-Reptile (ER) that updates the meta parameters with the main direction of historical taskspecific parameters to alleviate gradient noise. Furthermore, they propose Introspective Self-paced Learning (ISPL) that constructs a plurality of prior models to determine which sample should be abandoned. Theoretical and empirical results show the effectiveness of the proposed methods."
SP:a571bff9ffe4edafd7bc064c4d10609e6b981ce3,"This paper proposes Adversarial Batch Normalization (AdvBN) to make neural networks robust to changes in image style and appearance, rather than small perturbations at the pixel level. The authors formulate a min-max game in which an adversary chooses adversarial feature statistics, and network parameters are then updated to resist these changes in feature space that correspond to appearance differences of input images. AdvBN significantly improves the performance of ResNet-50 on ImageNet-C, Stylized-ImageNet, and ImageNetInstagram over standard training practices."
SP:6a9c46bd3cf854299f360bff136e1d79d3edb2e4,"This paper proposes a metric called Variance of Gradients (VoG) as a proxy metric for detecting outliers in the data distribution. The authors claim that it is a meaningful way to rank data by difficulty and to surface a tractable subset of the most challenging examples for human-in-the-loop auditing. Experiments are conducted on Cifar-10, Krizhevsky et al. (2009) and ImageNet (Russakovsky et al., 2015)."
SP:074bfacc75837bb19049be8a2890e10de073dd8e,"This paper proposes a method to improve the sample quality of deep generative models (DGMs) by refining inferior samples using the gradient flow of entropy-regularized f-divergences between the real and the generated data distributions. The gradient flow takes the form of a non-linear Fokker-Plank equation, which can be easily simulated by sampling from the equivalent McKean-Vlasov process. Compared to existing works that focus on specific GAN variants, this paper shows that the proposed method can be applied to GANs with vector-valued critics and can be extended to VAEs and Normalizing Flows. Empirical results on multiple synthetic, image, and text datasets demonstrate the effectiveness of the method."
SP:74ecbc5a6d464bfa49337da9e0dd6a0fe714d4bb,"This paper presents a variable encoder-decoder (VECO) pretraining approach to unify the two mainstreams in both model architectures and pre-training tasks. VECO splits the standard Transformer block into several sub-modules trained with both inner-sequence and cross-sequence masked language modeling, and correspondingly reorganizes certain sub- modules for understanding and generation tasks during inference. The proposed approach achieves new state-of-the-art results on various cross-lingual understanding tasks of the XTREME benchmark covering text classification, sequence labeling, question answering, and sentence retrieval."
SP:3d177ad50727d1a2619b68ab8a897b79d8652beb,"This paper proposes an intrinsic reward for deep reinforcement learning (RL) based on the prediction error of auditory events. Specifically, the authors propose to use the prediction errors as intrinsic rewards to encourage the agent to understand the causal effect of its actions through auditory event prediction. The authors first cluster the sound data using K-means and then train a neural network to predict the auditory events conditioned on the embedding of visual events. The prediction errors are then used to reward the agent for visiting states where the prediction is wrong and encouraged to be visited more often. "
SP:014f6118ebe55ece6be23c3a10f12e4591e444b1,"This paper studies the problem of novel category discovery on single-and-multi-modal data with labels from different but relevant categories. The authors propose a generic, end-to-end framework to jointly learn a reliable representation and assign clusters to unlabelled data. To avoid over-fitting the learnt embedding to labelled data, the authors extend the conventional contrastive learning by noise-contrastive estimation and extend it to jointly handle labelled and unlabeled data. In particular, the proposed method uses category discrimination on labelled data and cross-moda discrimination on multimodal data to augment the instance discrimination used in conventional Contrastive learning approaches. The proposed method also employs the WTA hashing algorithm on the shared representation space to generate pairwise pseudo labels for unlabeled data to better predict cluster assignments."
SP:4df640f502e88ddba2d7e183625231d70b083e82,"This paper proposes a method for weakly supervised semantic segmentation. The authors formulate the task as a semi-supervised metric learning problem, where pixels of the same semantics need to be mapped to the same (distinctive) features. They propose 4 types of contrastive relationships between pixels and segments in the feature space, capturing low-level image similarity, semantic annotation, co-occurrence, and feature affinity, which act as priors; the pixel-wise feature can be learned from training images with any partial annotations in a data-driven fashion. The proposed method is evaluated on Pascal VOC and DensePose."
SP:f7d6099adb40a0ce2f8a3563dbd5207cf1fdea0f,"This paper proposes a simple but effective distillation strategy for unsupervised learning. The method, termed as BINGO, targets at transferring the relationship learned by the teacher to the student. The goal of distillation is to aggregate compact representations over the student with respect to instances in a bag. The proposed method achieves new state-of-the-art performance on small scale models."
SP:328866aad6544c81ded8980934df31dc4472435f,"This paper proposes GATSBI, an adversarial approach to simulation-based inference (SBI) for high-dimensional data. The authors propose to reformulate the variational objective in the adversarial setting to learn implicit posterior distributions. The proposed approach is amortized across observations, works in high dimensional posterior spaces, and supports implicit priors. Experiments are conducted on two SBI benchmark problems and on two high dimensional simulators."
SP:2915e82097eae4eb8546dc500f32b3ec37e3766f,"This paper studies the identification and estimation of individualized treatment effects (TEs) under limited overlap, i.e., when subjects with certain features belong to a single treatment group. The authors propose a generative prognostic model to model a prognostic score which is widely used in biostatistics and sufficient for TEs. The model is learned as a beta-Intact-VAE, which is a new type of variational autoencoder (VAE) that enables representations balanced for treatment groups conditioned on individualized features. The proposed method is compared with recent methods using (semi-)synthetic datasets."
SP:ca358c9f36aac6e58ed1b3949c349d210c49a48e,"This paper proposes a framework for Autonomous Reinforcement Learning (ARL): reinforcement learning where the agent not only learns through its own experience, but also contends with lack of human supervision to reset between trials. They introduce a simulated benchmark EARL1 around this framework, containing a set of diverse and challenging simulated tasks reflective of the hurdles introduced to learning when only a minimal reliance on extrinsic intervention can be assumed. They show that standard approaches to episodic RL and existing approaches struggle as interventions are minimized."
SP:abe51d4a9817c08f0abde5da0bb8e6ca4e02e7cf,This paper analyzes the reasoning capabilities of the existing knowledge-aware graph neural networks (GNNs) for question answering (QA) tasks. The paper shows that existing GNNs for QA may only be able to perform simple reasoning tasks such as counting. The authors then propose a simple yet effective graph-based neural counter that achieves better performance on CommonsenseQA and OpenBookQA.
SP:3ea5a38e7fcd9111dcd299ad039b634e2781685f,"This paper proposes a three-stage framework to enable DNN inference with near-optimal compression and much better performance during inference runtime. The key insight of the method leverages the concept of Succinct Data Structures, which supports fast queries directly on compressed representation without decompression. The method first transforms DNN models as our proposed formulations in either Element-wise or Block-wise manner, then compresses the transformed DNNs using the proposed data structures. Finally, the method exploits the specialized execution pipelines for different model formulations, to retrieve relevant data for inference. The experimental results show that, our method keeps near-optimality compression, and achieves at least 8.7X/11.5X speedup on AlexNet/VGG-16 inference, compared with Huffman Coding."
SP:94c395afc794a9cc163e362078769ff83f3d20d0,"This paper proposes a new training method for improving the performance of tiny neural networks. The authors argue that training tiny models is different from training large models: rather than augmenting the data, we should augment the model, since tiny models tend to suffer from under-fitting rather than over-fitting due to limited capacity. To alleviate this issue, NetAug augments the network (reverse dropout) instead of inserting noise into the dataset or the network. It puts the tiny model into larger models and encourages it to work as a sub-model of larger models to get extra supervision, in addition to functioning as an independent model."
SP:9c24549b980e415616f818acbf4cf680ef8edb52,"This paper proposes a generative adversarial network (GAN) for super-resolution of dynamic point cloud sequences without requiring point correspondence annotation. The proposed model, Temporal Point cloud Upsampling GAN (TPU-GAN), can implicitly learn the underlying temporal coherence from point cloud sequence, which in turn guides the generator to produce temporally coherent output. In addition, a learnable masking module is proposed to adapt upsampling ratio according to the point distribution. Extensive experiments on two different domains: particles in the fluid dynamical system and human action scanned data show the effectiveness of the proposed method."
SP:67efe60ad37807505369b7852bc0abed29ffdda8,This paper proposes a new method for fully pre-training an encoder-only transformer and fine-tuning it for object detection via a task adapter. The proposed method is inspired by the success of textual prompts in NLP and treats query positional embeddings as visual prompts to help the model attend to the target area (prompting) and recognize the object. Experiments on the challenging COCO dataset demonstrate that the proposed method achieves competitive performance.
SP:a1f9897496303984fc7ad469222106b14b4a6233,"This paper proposes a new federated learning algorithm, FedPAGE, which further reduces the communication complexity by utilizing the recent optimal optimal PAGE method (Li et al., 2021). The authors show that FedPAAGE uses much fewer communication rounds than previous local methods for both federated convex and nonconvex optimization. Specifically, in the convex setting, the number of communication rounds is O(3/4 S), improving the best-known result O(N S ) by a factor of N, where N is the total number of clients, S is the sampled subset of clients in each communication round, and is the target error. In the non convex case, the communication rounds are O(sqrt(N+S S^2) for convex optimization, improving the state-of-the-art result $O(N^2/3 S^3/3)$ of SCAFFOLD."
SP:81e74765abc6524edd8fdf9a3ba107d7bddaa04b,"This paper proposes a method to study the decision boundary geometry of ANN classifiers by utilizing adversarial perturbations. The authors define adversarial subspaces, which are spanned by orthogonal directions of minimal perturbation to the decision boundaries from any given input sample, and measure the distance to the boundary in a large subspace, where the distance grows smoothly and sub-linearly as one increases the dimensionality of the subspace. The geometry of the boundary is characterized in terms of curvature, shape, and proximity to the input. The paper provides new insight into the consequences of adversarial training by quantifying the increase in boundary distance within adversarial subsets, the redistribution of proximal class labels, and the curvature."
SP:af5c25ecf38c5c3f3387720bdc80c2c54c5699fe,"This paper presents a two-stage weakly-supervised contrastive learning approach. The first stage is to cluster data according to its auxiliary information, and the second stage learns similar representations within the same cluster and dissimilar representations for data from different clusters. The proposed method is evaluated on three datasets: UT-K, CUB-200-2011, Wider Attribute, and ImageNet-100. The results show that the proposed method outperforms the existing methods in most cases."
SP:0a92fcc52970201de4a66b1e76c93dbea9dfd3f1,"This paper studies the problem of learning to recover sparse parameters from observational data. The authors propose a method called PLISA (Provable Learning-based Iterative Sparse recovery Algorithm) to learn algorithms automatically from data. PLISA is designed by unrolling a path-following algorithm, with some components being more flexible and learnable. With this structure, the authors theoretically show the improved recovery accuracy achievable by PLISA. Furthermore, they analyze the empirical Rademacher complexity of PLISA to characterize its generalization ability to solve new problems outside the training set."
SP:5064eda9ba27060af15e81b2b317b2e4558b0ac4,"This paper proposes a hybrid action representation (HyAR) to learn a compact and decodable latent representation space for the original hybrid action space. Specifically, HyAR constructs the latent space and embeds the dependence between discrete action and continuous parameter via an embedding table and conditional Variational Auto-Encoder (VAE). To further improve the effectiveness, the action representation is trained to be semantically smooth through unsupervised environmental dynamics prediction. Finally, the agent then learns its policy with conventional DRL algorithms in the learned representation space and interacts with the environment by decoding the hybrid action embeddings to the original action space with the help of conventional RL algorithms. "
SP:5128bf712f6b197de113c7a371b4bec36f978eca,"This paper proposes SGEM, Stochastic Gradient with Energy and Momentum to solve general non-convex stochastic optimization problems, which is based on the AEGD method that originated in the work [AEGD: Adaptive Gradient Descent with Energy. arXiv: 2010.05109]. The authors show that SGEM features an unconditional energy stability property, and derive energy-dependent convergence rates in the general nonconveX stochastically setting, as well as a regret bound in the online convex setting. A lower threshold for the energy variable is also provided. The experimental results show that the proposed method converges faster than SGDM and generalizes better or at least as good as SGDM in training deep neural networks."
SP:11f49b0a975be87769be29e85d7e3924699cf2c9,"This paper proposes a Conditional Masked Language Model with Correction (CMLMC) to improve the performance of non-autoregressive (NAR) Transformer-based machine translation (MT) models. The authors identify two shortcomings of CMLM, i.e., the indistinguishability of tokens, and the mismatch between training and inference procedures, and propose CMLMC to address these problems. The proposed method achieves state-of-the-art NAR performance when trained on raw data without distillation and approaches AR performance on multiple datasets."
SP:96f8ac3c6163e56d8ae1954a162bae01e6b58a0a,"This paper proposes a spiking neural network (SNN) for local signal processing. The proposed SNN is inspired by the WaveNet architecture and uses simple neural dynamics, fixed time-constants and a simple feed-forward architecture and hence is particularly well suited for a neuromorphic implementation. The authors test the capabilities of this model on several datasets for keyword-spotting. The results show that the proposed network beats the state-of-the-art in terms of accuracy."
SP:7f20a2e4e95f857140b87b0730360b3ff2f371f4,"This paper proposes a class of algorithms, called Shifty algorithms, that provide high-confidence behavioral guarantees that hold under demographic shift. The authors evaluate Shifty using a real-world dataset of university entrance exams and subsequent student success. They show that the learned models avoid bias under demographic shifts, unlike existing methods."
SP:94f097921bee5fdc10ec2e7c901b2ddb876d9d41,"This paper proposes a meta-learning approach for multi-stage stochastic optimization (MSSO) that learns to map problem instances to a piece-wise linear value function within intrinsic low-dimension space, which is architected specifically to interact with a base SDDP solver, so that can accelerate optimization performance on new instances. The proposed Neural Stochastic Dual Dynamic Programming (ν-SDDP) continually self-improves by solving successive problems. An empirical investigation demonstrates that the proposed approach can significantly reduce problem solving cost without sacrificing solution quality across a range of synthetic and real-world process optimization problems."
SP:3d9f5132f9ec3807dbca78462a459fd123a09b24,"This paper proposes a protocol for private next-token prediction to prevent privacy violations by language models that were fine-tuned on a private corpus after pre-training on a public corpus. The proposed method is based on a relaxation of group differentially private prediction. The authors also propose a tight, data-dependent privacy accounting mechanism, which allows it to thwart existing data-extraction attacks while maintaining the utility of the language model."
SP:7f524d186ea939309c7eeb843c62b6a4b4cfbc8a,"This paper proposes an unsupervised method to detect out-of-distribution (OOD) samples using a k-NN density estimate with respect to a classification model’s intermediate activations on indistribution samples. The authors leverage a recent insight about label smoothing, which they call the Label Smoothed Embedding Hypothesis, and show that the proposed method performs better as an OOD detection method both theoretically and empirically. The proposed method outperforms many OOD baselines and provides new finite-sample high-probability statistical results."
SP:aafbd6ada14cc59a272fe4bf95fac71fa18e57ab,"This paper proposes a new approach to learn representations for semi-supervised image classification. The proposed approach is based on a new formulation of the denoising score matching objective, which can be seen as a multi-scale denoiser autoencoders. The paper also shows that adversarial training in diffusion-based generative models can improve sample quality and sampling speed. "
SP:8cfc837d5c10d539bbd098df7134c42e4830ba25,"This paper proposes a method for goal-conditioned reinforcement learning, where the goal is to learn to reach distant goals, i.e. to reach a goal that is further away than the current state. The proposed method, C-Planning, is based on expectation maximization (EM) and graph-planning (GP). In particular, the E-step is for planning a sequence of waypoints, while the M-step aims to learn a policy to reach those waypoints. The method is evaluated on 2D navigation tasks and 18D manipulation tasks, where it is shown to outperform the baselines."
SP:ef3193842e06d4a6edb8a6a86ea5bc97ee5eaa4a,"This paper proposes to extend mixup to k-mixup, i.e., perturbing k-batches of training data in the direction of other k-sample instances in the training set. This is done using displacement interpolation under the Wasserstein metric. The authors demonstrate theoretically and in simulations that k-Mixup preserves cluster and manifold structures, and extend theory studying the efficacy of standard Mixup to the k=1 case. The empirical results show that training with k- mixup further improves generalization and robustness across several network architectures and benchmark datasets of differing modalities."
SP:0fe6a9848026e5f6436a380199e27a9ad26cffed,This paper proposes a kernelized classification layer for deep neural networks. The classification layer finds an optimal nonlinear classifier for the embeddings by mapping them into a Reproducing Kernel Hilbert Hilbert Space (RKHS) that optimally separates them into different classes. The authors theoretically show that the classification layer optimizes over all possible radial kernel functions on the space of embedding to learn an optimal classifier. The proposed method is evaluated on a number of computer vision and natural language processing tasks.
SP:01ee8ec81619784788eb0ce9785098e437d17a7c,"This paper investigates the sources of bias in node representations obtained via Graph Neural Networks (GNNs). The analysis reveals that both nodal features and graph structure lead to bias in the obtained representations. Based on the analysis, fairness-aware data augmentation frameworks are developed to reduce the intrinsic bias. Extensive experiments on node classification and link prediction are carried out over real networks in the context of graph contrastive learning."
SP:7739dc9e37f7f1384f87d2e60281e5bb27fece99,"This paper considers the problem of estimating treatment effects from observational data in the presence of unmeasured confounders. The authors propose a Confounder Balanced IV Regression (CB-IV) algorithm to jointly remove the bias from the unmeasureable confounder with IV regression and achieve better bias-variance trade-off in imbalanced treatment distributions due to the observed confoundering by balancing for treatment effect estimation. The proposed algorithm consists of three main modules: (1) treatment regression: regressing the treatment with IVs and confoundings like previous nonlinear IV methods for removing the confounding; (2) confounding balancing: learning a balanced representation to eliminate the bias induced by the observed data; and (3) outcome regression: Regressing the outcome with the predicted treatment and the balanced representation for treatment effects estimation. Extensive experiments demonstrate that CB-IV algorithm outperforms the state-of-the-art methods, including IV regression methods, for treating effect estimation, for outcome regression, and for treatment-effect estimation."
SP:fdb68c39fce254b73310a3101b2fe97ba47e69fe,"This paper analyzes the performance of MAML in the setting of linear regression with a mixture of easy and hard tasks, where the hardness is related to the rate that gradient descent converges on the task. The authors show that the optimal solutions of the hard tasks must be closely packed with the center far from the center of the easy tasks optimal solutions. They also give numerical and analytical results suggesting that these insights apply to two-layer neural networks."
SP:e8143c7880c16ee9ce7a544e0fd80f001b1b4f9f,"This paper proposes a method for sparse blind source separation (BSS) that leverages the data-driven knowledge stemming from realistic simulations or ground-truth data by learning both the hyperparameters and the variable mixing matrices. The proposed LPALM algorithm enables to perform semi-blind source separation, which is key to increase the generalization of the learnt model in real-world applications. The authors demonstrate the relevance of the proposed method in astrophysical multispectral imaging: the algorithm not only needs up to 10-10 times fewer iterations but also improves the separation quality."
SP:7716315001949ab88c8a216302fe51bae872fc87,"This paper proposes a new transformer-based model for autoregressive language modeling. The proposed model is based on the Legendre Memory Unit (LMU), which projects a sliding window of the input sequence onto Legendre polynomials to provide a temporal representation and compression of input signal. The authors also introduce a novel attention module called implicit self-attention, which operates only on the output of the LMU at each time step, and captures information about the past tokens. Experiments show that the proposed model has a smooth power-law relationship with respect to the model size, and loss that scales better than transformers."
SP:832f422b3554e89702e13c8c5690ee26f2289e3b,"This paper proposes a GAN-based approach for disentangling the image into spatial and appearance factors without domain knowledge and supervision signals. The proposed approach, LatentKeypointGAN, is trained end-to-end on the classical GAN objective with internal conditioning on a set of space keypoints. These keypoints have associated appearance embeddings that respectively control the position and style of the generated objects and their parts. The generated keypoints are mapped to spatial heatmaps of increasing resolution. The heatmaps define the position of the keypoints and their support sets the influence range of their respective encodings. In the second step, a SPADE-like image generator turns the sampled values sampled from a normal distribution into 2D keypoint locations and their associated encoding."
SP:9206ae6e31077569313838504ef6daa89ad3b59c,"This paper studies the properties of deep fully-connected neural networks with layer normalization using the mean field formalism, and carry out a non-perturbative analysis of signal propagation. The authors demonstrate that increasing the depth leads to gradient explosion or representation shrinkage. They also show that many popular normalization techniques fail to mitigate these problems. The method can also be applied to residual networks to guide the choice of initialization variances."
SP:2177be818b5843c580c787f1b2d725154846feb6,"This paper proposes a line search method for learning rate schedules for stochastic gradient descent (SGD) in deep learning. The proposed method is based on recent empirical findings that the full-batch loss behaves locally parabolically in the direction of noisy update step directions and that the trend of the optimal update step size changes slowly. Based on these findings, the authors propose a line-search method that approximates the full batch loss with a parabola estimated over several mini-batches. The learning rates are derived from such parabolas during training. The experiments show that the proposed method outperforms SGD tuned with a piece-wise constant learning rate schedule and other line search approaches for Deep Learning across models, datasets, and batch sizes on validation and test accuracy."
SP:62233782f9046c85617d9ccfe8427eae7d1c9da7,"This paper provides a theoretical analysis of noise-contrastive estimation (NCE) in the setting where the target and noise distributions belong to an exponential family. The authors show that when the noise distribution is poorly chosen, the loss landscape can become extremely flat. This poses challenges for standard first order and second order optimization methods, forcing them to take an exponential number of steps to converge to a good parameter estimate. To address this issue, the authors propose a variant of NCE called eNCE, which replaces the log loss in NCE with an exponential loss and for which normalized gradient descent (NGD) is used."
SP:ceba6c1421b2d03863007fdaf029b8b946519c1b,"This paper studies the problem of combining differential privacy and Byzantine resilience in distributed machine learning. In particular, the authors consider the setting where a fraction of the workers are malicious (Byzantine) and the other fraction are honest. They show that the integration of standard practices in DP and BR is not straightforward and that many existing results on the convergence of distributed SGD under Byzantine faults, especially those relying on Byzantine resilience, are rendered invalid when honest workers enforce DP. To circumvent this shortcoming, they revisit the theory of (alpha, f)-BR to obtain an approximate convergence guarantee. They also provide insights on how to improve this guarantee through hyperparameter optimization."
SP:bc783f0c829f90931535e63687d13172879631b3,"This paper considers the problem of code editing with few exemplars. The authors propose a method that combines edit representations extracted from support exemplars and compositionally generalizes them to the query code snippet editing via multi-extent similarities ensemble. Specifically, they parse the support and query code snippets using language-specific grammar into abstract syntax trees. They apply the similarities measurement in multiple extents from individual nodes to collective tree representations for query and support sample matching, and ensemble the matching results through a similarity-ranking error estimator. They evaluate the proposed method on C# and Python datasets and show up to 8.6% absolute accuracy improvements compared to non-composition baselines."
SP:ca0c4bdb02f7d939fb6de38b6b446ced4b5984a0,"This paper proposes a generative model for generating high-level structure in the form of relational constraints between different subcomponents of an example (e.g., lines of a poem or measures of music). The authors propose a program synthesis algorithm that infers the relational constraints present in the training data, and then learn a generation model based on the resulting constraint data. They show that their approach significantly improves over the state-of-the-art in terms of capturing high level structures in the data, while performing comparably or better on low-level structures."
SP:692ae0c583a1585eff1a7d9c0d3b51b7879611cc,"This paper studies the problem of set-to-hypergraph prediction, where the goal is to infer the set of relations for a given set of entities. The paper addresses two common scaling problems encountered in set to hypergraph tasks that limit the size of the input set: the exponentially growing number of hyperedges and the run-time complexity, both leading to higher memory requirements. The authors make three contributions. First, they propose to predict and supervise the positive edges only, which changes the asymptotic memory scaling from exponential to linear. Second, they introduce a training method that encourages iterative refinement of the predicted hypergraph, which allows us to skip iterations in the backward pass for improved efficiency and constant memory usage. Third, they combine both contributions in a single model that enables them to address problems with larger input set sizes."
SP:e3481fb6d8d1aa45d6ed4a454e781f5a2c30c57e,"This paper proposes a post-processing method to mitigate bias of state-of-the-art models. It consists in learning a shallow neural network, called the Ethical Module, which transforms the deep embeddings of a pre-trained model to give more representation power to the discriminated subgroups. The training is supervised by the von Mises-Fisher loss, whose hyperparameters allow to control the space allocated to each subgroup in the latent space. Besides being very simple, the resulting methodology is more stable and faster than most current methods of bias mitigation."
SP:3fb5dcc8b8fb731e09c14b16480cada1c7ccfaa7,"This paper proposes a novel approach to perform knowledge distillation in the context of class-incremental learning (CIL). In particular, the authors propose to compute the KD loss using placebo data chosen from a free image stream (e.g., Google Images), which is both simple and surprisingly effective even when there is no class overlap between the placebos and the old data. When the image stream is available, they use an evaluation function to quickly judge the quality of candidate images (good or bad placebos) and collect good ones. For training this function, they sample pseudo CIL tasks from the data in the 0-th phase and design a reinforcement learning algorithm. Their method does not require any additional supervision or memory budget, and can significantly improve a number of top-performing CIL methods, in particular on higher-resolution benchmarks."
SP:506e0a888c03a955b708464eed3670c04baf4912,"This paper proposes a path auxiliary algorithm for sampling from discrete energy-based models (EBMs) using MCMC with a composition of local moves to efficiently explore large neighborhoods. It also gives a fast version of the algorithm that only queries the evaluation of energy function twice for each proposal via linearization of the energy function. Empirically, it is shown that the proposed algorithms outperform other generic samplers on various discrete models for sampling, inference, and learning."
SP:4b466277aa5561a80c48d5e72559de4ce95f228b,"This paper proposes Variational Predictive Routing (VPR), an event-based hierarchical generative model that is able to dynamically adjust its internal state following changes in the observed features, promoting an optimal organisation of representations across the levels of the model’s latent hierarchy. In particular, VPR uses a neural probabilistic inference system that organizes latent representations of video features in a temporal hierarchy, based on their rates of change, thus modeling continuous data as a hierarchical renewal process. Experiments on several video datasets show that VPR can detect event boundaries, disentangle spatiotemporal features across its hierarchy, adapt to the dynamics of the data, and produce accurate timeagnostic rollouts of the future."
SP:459ef2e6bd7638020955dbb4d8ae1098619f7b95,"This paper proposes a method for image retrieval that combines global and local features. The proposed method is an end-to-end and single-stage pipeline that uses convolutional neural networks to learn local feature matching and CNN-based homography transformation and information fusion to fuse the global features with the local features, which reduces time and space costs caused by local features matching, storing local features and re-ranking. Experiments on the Revisited Oxford and Paris datasets validate the effectiveness of the proposed method."
SP:487cc308a1e8ee078c54b2158bcae47e920e73f8,"This paper proposes RotoGrad, a multi-task learning algorithm that aims to jointly homogenize gradient magnitudes and directions across tasks. The main idea is to re-weight the task gradients at each step of the learning process, while encouraging learning those tasks that have converged the least thus far. The paper also proposes to smoothly rotate the shared feature space differently for each task, seamlessly aligning gradients in the long run. Theoretical analysis shows that the cooperation between gradient magnitude and direction-homogenization ensures the stability of the overall learning process. Empirical results show that the proposed method leads to stable (convergent) learning, and scales up to complex network architectures."
SP:050cd8319d84a1bd8c2ccb930ba69b33c8fb6e60,This paper proposes a method to fuse heterogeneous neural networks via cross-layer alignment. The proposed method is based on a dynamic programming-based algorithm to balance the number of layers between networks. Experiments are conducted on CIFAR-10 and ImageNet datasets to demonstrate the effectiveness of the proposed method. 
SP:f764eae15cd083fdb4eb2af09ac64c2d878a454f,"This paper analyzes the implicit regularization effect of SGD in deep reinforcement learning (RL) and shows that it can lead to poor generalization and degenerate feature representations in the offline deep RL setting. Theoretical analysis shows that when existing models of implicit regularizer are applied to temporal difference learning, the resulting derived regularizer favors degenerate solutions with excessive “aliasing”, in stark contrast to the supervised learning case. The paper also empirically shows that feature representations learned by a deep network value function trained via bootstrapping can indeed become degenerate, aliasing the representations for state-action pairs that appear on either side of the Bellman backup. To address this issue, the paper proposes a simple and effective explicit regularizer, called DR3, that counteracts the issues discussed above."
SP:6fd793b27123bf80504e2ad5957455b7ec311612,"This paper proposes HyperDQN, an extension of randomized least-square value iteration (RLSVI) to deep reinforcement learning (RL). The main idea is to use a probabilistic hyper-network (i.e., meta-network) to predict the parameter of the Q-value function, which is then used to sample the action sequences from the posterior distribution. The proposed method is evaluated on the Atari game SuperMarioBros, where it outperforms DQN and other randomized exploration methods."
SP:b428383660928374c953f659ea1e05852dbdcd6e,"This paper proposes to learn causal representation from observational data by regularizing the learning procedure with mutual information measures according to our hypothetical causal graph. The optimization involves a counterfactual loss, based on which the authors deduce a theoretical guarantee that the causality-inspired learning is with reduced sample complexity and better generalization ability. Extensive experiments show that the models trained on causal representations learned by our approach is robust under adversarial attacks and distribution shift."
SP:1258c05a80a17949b50e6dae13deea1d2235f456,"This paper proposes a progressive training framework for efficient and effective federated learning. The proposed ProgFed reduces computation and two-way communication costs while maintaining the strong performance of the final models. Theoretically, the authors prove that the proposed method converges at the same asymptotic rate as standard training on full models. Extensive results on a broad range of architectures, including CNNs, ResNets, and U-nets, and diverse tasks from simple classification to medical image segmentation show that the highly effective training approach saves up to 20% computation and up to 63% communication costs for converged models."
SP:8cdaa6e0dafd750ebdb5d7a4c1987a042400662f,"This paper studies the generalization of adversarial training through the lens of the adversarial Rademacher complexity of deep neural networks. The authors provide an upper bound on the adversarially trained weight norms, which includes the product of the Frobenius norm and the norm of the weight matrices. They provide experiments to show that the weight norms are larger than the standard trained norms, thus providing an explanation for the bad generalization performance of Adversarial training."
SP:925d6bb051e9b384669fb695085b678c11f7c11a,"This paper proposes a kernel-based differential entropy estimator (KNIFE) that is parameterized, differentiable, and can be applied to both conditional and mutual information (MI) estimation. Theoretical analysis is provided to show that the proposed estimator satisfies the asymptotic properties of existing DE and MI estimators. Experiments on synthetic and real-world data demonstrate the effectiveness of the proposed method. "
SP:d2f3beac855f0d72c13552fecb2bdb9d42195df3,"This paper proposes a new soft-greedy operator, called resmax, which takes actions proportionally to their suboptimality gap: the residual to the estimated maximal value. The authors prove that resmax is a non-expansion for any fixed exploration hyperparameter, unlike the softmax policy which requires a state-action specific temperature to obtain a nonexpansion (called mellowmax). The authors empirically validate the effectiveness of resmax in tabular and deep RL."
SP:792ae8808aa6902758146aef1548c975492b833c,"This paper proposes a method to control the model’s learnability on a specific dataset with a special key. In particular, it proposes adversarial invertible transformation to slightly modify data samples so that they become “unlearnable” by machine learning models with negligible loss of visual features. Meanwhile, one can unlock the learnability of the dataset and train models normally using the corresponding key. The proposed learnability lock leverages class-wise perturbation that applies a universal transformation function on data samples of the same label. This ensures that the learningability can be easily restored with a simple inverse transformation while remaining difficult to be detected or reverse-engineered."
SP:9af10703605e620e563241e2602a50b629f3d37a,"This paper proposes a general approach for handling missing features in graph machine learning applications that is based on minimization of the Dirichlet energy and leads to a diffusion-type differential equation on the graph. The discretization of this equation produces a simple, fast and scalable algorithm which the authors call Feature Propagation. The proposed approach outperforms state-of-the-art methods on standard node-classification benchmarks and can withstand surprisingly high rates of missing features."
SP:cbaa3f1379fa99159899d79ccb479c0187403aca,"This paper proposes an integer optimization method for selecting a core set that minimizes the discrete Wasserstein distance between the unlabeled set and the set to be labeled. The optimization problem admits a Generalized Benders Decomposition solution algorithm that solves sub-problems that are orders of magnitude smaller (Geoffrion, 1972). The algorithm guarantees convergence to a globally optimal solution and can be further accelerated with customized constraints. Numerical results on several data sets show that the optimization approach is competitive with baselines and outperforms them in the low-budget regime."
SP:4c72923f78ca6590dc11e10d1a2403076a583718,"This paper proposes a method for de novo genome assembly based on graph convolutional neural networks. The authors propose to use a greedy search algorithm to find a path through the assembly graph, which is based on the lengths of the overlaps between the sequences and the graph topology. They show that the proposed method outperforms the greedy search over the overlap lengths only. They also show that their method reconstructs the correct path in the fraction of time required for the state-of-the-art genome assembly algorithms."
SP:24de906e4289c9073b6c55c747b0913b8df5e053,"This paper proposes a method to improve the performance of online-aware meta-learning (OML) by incorporating experience replay (ER) into its meta-testing procedure. The authors propose to store the samples’ representations, instead of the samples themselves, into the replay buffer. Moreover, they introduce a meta-learned Predictive Sample Selection to replace the widely used reservoir sampling to populate the replay buffers. Experimental results on a number of real-world meta-continual learning benchmark data sets demonstrate that the proposed method outperforms the state-of-the-art."
SP:3c78454f053f74930979a8054cd7c8a34b6fe63d,"This paper proposes a method for multi-agent joint Q-learning based on centralized training with decentralized execution (CTDE). The authors formulate an explicit credit assignment problem where each agent gives its suggestion about how to weight individual Q-values to explicitly maximize the joint Q value. Theoretically, they give a gradient ascent solution for this problem. The authors instantiate the core idea with deep neural networks and propose Explicit Credit Assignment Joint Q-Learning (ECAQ) to improve the performance of IGM-based methods. Extensive experiments justify the effectiveness of ECAQ."
SP:0d2b225ac697679d10df25f371b2a718d4949b42,"This paper studies adversarial attacks against transductive learning-based adversarial defenses. The authors formulate and analyze threat models for these defenses, and point out important subtleties in the modeling that were unclear or overlooked in previous work. They propose the principle of attacking model space for solving bilevel attack objectives, and present Greedy Model Space Attack (GMSA), an attack framework that can serve as a new baseline for evaluating transductionive learning based defenses. Through systematic evaluation, they show that GMSA, even with weak instantiations, can break some existing adversarial defense methods."
SP:e7024cae196fc5eb6a62d289a95d76b532b6a36c,"This paper studies the problem of batch normalization (BN) in the context of neural network training. The authors cast BN as an approximation of the limiting case where the entire dataset is normalized jointly, and explore other ways to approximate the gradient from this limiting case. They demonstrate an approximation that removes the need to keep more than one example in memory at any given time, at the cost of a small factor increase in the training step computation, and a fully per-example training procedure, which removes the extra computation at the expense of a drop in the final model accuracy. They further use their insights to improve batch renormalization for very small minibatches."
SP:4aa42984fcb0fd66936d668477b2719ef5c427d4,"This paper proposes a low-rank adaptation method to reduce the number of trainable parameters for fine-tuning large pre-trained language models. The proposed method freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture. Compared to GPT-3 with Adam, the proposed method can reduce the total number of parameters by 10,000 times and the GPU memory requirement by 3 times. "
SP:b77a00beb0802f47810b03d3c4aa24d92781414f,"This paper proposes a regular-constrained linear-chain conditional random field (RegCCRF) that can enforce a broad class of constraints, including non-local ones, by specifying the space of possible output structures as a regular language L. The resulting RegCCRF has the same formal properties as a standard CRF, but assigns zero probability to all label sequences not in L. Notably, it can incorporate their constraints during training, while related models only enforce constraints during decoding. The authors prove that constrained training is never worse than constrained decoding, and show empirically that it can be substantially better in practice."
SP:74c186a96c12adff178264aa84ace8d04dc7d725,"This paper proposes two neural models for camera-based physiological measurement, EfficientPhys and AddPhys, that remove the need for face detection, segmentation, normalization, color space transformation, and other preprocessing steps. The proposed models achieve state-of-the-art accuracy on three public datasets. The latency of the proposed networks also achieves a 33% improvement in efficiency."
SP:3003bab6e3f7e2e21cd6cf27ee7d483d877d9fb3,"This paper proposes Hardware-Aware Latency Pruning (HALP) that formulates structural pruning as a global resource allocation optimization problem, aiming at maximizing the accuracy while constraining latency under a predefined budget. For filter importance ranking, HALP leverages latency lookup table to track latency reduction potential and global saliency score to gauge accuracy drop. Both metrics can be evaluated very efficiently during pruning, allowing the authors to reformulate global structural structure pruning under a reward maximization problem given target constraint. This makes the problem solvable via an augmented knapsack solver, enabling HALP to surpass prior work in pruning efficacy and accuracy-efficiency trade-off. HALP is evaluated on both classification and detection tasks on ImageNet and VOC datasets."
SP:c44d676c09c8e5a70d73b21b507b41a422fec809,"This paper proposes a molecular graph generation method via energy-based models (EBMs) to perform permutation invariant and multi-objective molecule generation. The authors propose a permutation-invariant energy function and learn the energy function by contrastive divergence and generate samples by Langevin dynamics. To generate molecules with a specific desirable property, they propose a simple yet effective learning strategy, which pushes down energies with flexible degrees according to the properties of corresponding molecules. Further, they explore to use their GraphEBM for generating molecules towards multiple objectives via compositional generation, which is practically desired in drug discovery."
SP:70e60fa5deef3e3ba77d05d0c3e0e7fbf396aa1d,This paper proposes a neural-network-based approach for bottom-up program synthesis. The approach is based on the idea that the neural network should take into account the search history and partial program executions during the search process. The proposed approach is evaluated on two tasks: string manipulation and logic programming. The results show that the proposed approach outperforms the state-of-the-art.
SP:daa044ffefe80bae16b014f60061d941ed8c2ba6,"This paper proposes a method to stabilize deep Q-learning by augmenting the squared Bellman error with a functional regularizer. Unlike target networks, the regularization we propose here is explicit and enables us to use up-to-date parameters as well as control the regularisation. This leads to a faster yet more stable training method. The proposed method outperforms the target-network based methods in terms of sample efficiency and performance."
SP:dd174014d056a7d2bc86ee99119841eafa62ed52,This paper proposes a method to improve the expressive power of message-passing GNNs by introducing structural properties of graphs into the message passing aggregation scheme. Theoretical results show that the proposed method is strictly more expressive than the Weisfeiler-Lehman test in distinguishing graph structures. The proposed method can also capture different classes of local structures w.r.t. different graph learning tasks. Experiments on node classification and link prediction tasks demonstrate the effectiveness of the proposed approach.
SP:beb9ba0261e176bfc50e9bf5bed2b6169d388285,"This paper proposes a novel prediction interval (PI) method for uncertainty quantification, which addresses three major issues with the state-of-the-art PI methods. The proposed method calculates PIs from linear combinations of three NNs, each of which is independently trained using the standard mean squared error loss. The coefficients of the linear combinations are computed using root-finding algorithms to ensure tight PIs for a given confidence level. The authors theoretically prove that PI3NN can calculate PI for a series of confidence levels without retraining NNs and it completely avoids the crossing issue. Moreover, it does not introduce any unusual hyperparameters resulting in a stable performance."
SP:4b44a834e2212bacb4c2d9408a81f1efc76a670b,"This paper proposes a meta-learning method for online continual adaptation. The proposed method is based on meta-training and meta-adaptation, where the meta-network is trained on a set of tasks and then used to meta-train on the new tasks. The authors argue that the proposed approach is better suited to online settings where the task boundaries are not known and the task distribution is not discrete. They propose an algorithm called Fully Online Meta-Learning (FOML) which does not require any ground truth knowledge about task boundaries and stays fully online without resetting back to pretrained weights. They show that FOML is able to learn new tasks faster than the state-of-the-art online learning methods on the Rainbow-MNIST and CIFAR100 datasets."
SP:fbae35cb171b3a3eb7c5d4bc83881ed7c4a70aae,This paper proposes a differentiable scaffolding tree (DST) to convert discrete chemical structures to locally differentiable ones. DST enables a gradient-based optimization on a chemical graph structure by back-propagating the derivatives from the target properties through a graph neural network (GNN). The empirical studies show that DST is both effective and sample efficient.
SP:61b59899cf6ae442d9f8f5226e79708a4280cfb2,"This paper proposes a knowledge-augmented approach to predict patients’ response for a target lab result. The authors use a transformer encoder (Vaswani et al. 2017) to capture the patient information, while the modified graph attention network (GATv)(Brody et al., 2021) is used to combine the outputs from both the transformer and the GATv2 to obtain a strong latent patient representation so as to accurately predict the target lab test result. Experiments on real-world datasets demonstrate the effectiveness of the proposed solution in reducing prediction errors by a significant margin."
SP:8623cebb515c4a736427449b46ad2cdf8b806b77,"This paper studies the problem of open-set single domain generalization (OS-SDG), where the source and target domains do not have the same label space. The authors propose a CrossMatch approach to improve the performance of SDG methods on identifying unknown classes by leveraging a multi-binary classifier. CrossMatch generates auxiliary samples out of source label space by using an adversarial data augmentation strategy. It also adopts a consistency regularization on generated auxiliary samples between multibinary classifiers and the model trained by the proposed method. Experimental results on benchmark datasets prove the effectiveness of CrossMatch."
SP:126f8ffb855aa22eda4d681a499953879ed3679e,"This paper studies the Wasserstein policy optimization (WPO) and Sinkhorn trust region optimization (SPO) methods for policy optimization in reinforcement learning. The main idea of WPO and SPO is to directly optimize the policy distribution and derive their close-form policy updates based on the Lagrangian duality. Theoretically, the authors show that WPO guarantees a monotonic performance improvement and that SPO provably converges to WPO as the entropic regularizer diminishes. Experiments across tabular domains and robotic locomotion tasks demonstrate the performance improvement of both approaches."
SP:999eacf6500c87205584a3256d7ca45b3016fb1c,"This paper proposes a new paradigm for neural network training called forget-and-relearn, which unifies many existing iterative training algorithms in the image classification and language emergence literature, and allows us to understand the success of these algorithms in terms of the disproportionate forgetting of undesirable information. The forgetting step selectively removes undesirable information from the model, and the relearning step reinforces features that are consistently useful under different conditions. The authors leverage this understanding to improve upon existing algorithms by designing more targeted forgetting operations."
SP:2789859517b6624730b14a7e010444a72d3dd3ed,"This paper studies the offline-online setting, where the agent has access to a batch of data to train on but is also allowed to learn during the evaluation phase in an online manner. This is an extension to batch RL, allowing the agent to adapt to new situations without having to precommit to a policy. The authors show that standard RL agents trained in an offline-Online manner can outperform agents trained only offline or online, sometimes by a large margin."
SP:76625a25e770415599a34122110d61cb3b7e614c,"This paper studies the problem of domain generalization (DG) via learning to reduce domain shift with an episodic training procedure. In particular, the authors propose to learn to optimize Y-discrepancy between the unseen target domain and source domains only using source-domain samples. Theoretically, they give a PAC-style generalization bound for discrepancy-optimal meta-learning and further make comparisons with other DG bounds including ERM and domain-invariant learning. The theoretical analyses mainly show that there is a tradeoff between generalization performance and computational complexity for the proposed method. Empirically, the algorithm achieves state-of-the-art results on two DG benchmarks."
SP:6421a9759c766641fd8c128a249f1a9c5699d19c,"This paper studies the role of policy and value networks in DNN-based best-first search on the Sokoban domain. The authors show the effectiveness of the policy network, further enhanced by the value network, as a guiding heuristic for the search. They also show the existence of left heavy tails and propose an abstract tree model that can empirically explain the appearance of these tails."
SP:84c415bc0f120d1997289f91661ff74e7297d3bd,"This paper proposes a method for meta-imitation learning from videos of human demonstrations. The main idea is to translate human videos into practical robot demonstrations and train the meta-policy with adaptive loss based on the quality of the translated data. The approach relies only on human videos and does not require robot demonstration, which facilitates data collection and is more in line with human imitation behavior. Experiments show that the proposed method achieves comparable performance to the baseline on fast learning a set of vision-based tasks through watching a single video demonstration."
SP:fedf5c75e83d6ab41ef9d5daa9054ffe4e424ec2,"This paper proposes a new training method for over-parameterized deep networks trained using gradient-based optimizers for classification and ranking problems. The authors claim that without appropriately tuned regularization, such networks have the tendency to make output scores (logits) and network weights large, causing training loss to become too small and the network to lose its adaptivity (ability to move around and escape regions of poor generalization) in the weight space. Adaptive optimizers like Adam, being aggressive at optimizing the train loss, are particularly affected by this. It is well known that adaptive optimizers lag behind SGD a lot in terms of generalization performance, mainly in the image classification domain. The proposed training method closes the gap with SGD in this paper."
SP:819df8d847a99f13ed5efdcabae8b464c12b464b,"This paper proposes a new approach to group equivariant convolutional neural networks (G-CNNs) that is able to learn both partial and full equivariance at every layer. The approach is based on the idea that the distribution of the data can be better represented by a subset of a group than by the group as a whole, and that a model that respects equivariancy partially is better suited to represent the data. The proposed approach is evaluated on MNIST and CIFAR-10 datasets, and it is shown that the proposed approach performs on par with G-CNN and outperforms them."
SP:0c0ca9df96f1fa2eb8b83a47d0d5964590fef290,"This paper proposes a new method for training deep latent variable models (DLVMs). The proposed method is based on the amortized Langevin dynamics (ALD), which replaces the datapoint-wise MCMC iterations with updates of an inference model that maps observations into latent variables, which enables scalable inference from large-scale datasets. The authors prove that ALD has the target posterior as a stationary distribution under some assumptions, and extend ALD to sampling from an unconditional distribution such as an energy-based model, enabling more flexible generative modeling by applying it to the prior distribution of the latent variable. Based on ALD, the authors construct a new deep latent variables model named the Langevin autoencoder (LAE), which uses ALD for posterior inference and sampling from the latent space EBM."
SP:5631097031c7e599bdeae64366ffa6e4558837c6,"This paper proposes a method for hypergraph reasoning, i.e., predicting the relationship between several entities based on the input facts. The authors claim that logical rules (e.g., my parent’s parent is my grandparent) usually apply locally and sparsely. Inspired by these observations, the authors propose Sparse and Local Neural Logic Machines (SpaLoc) to leverage the sparsity in hypergraph neural networks, which represents the grounding of relationships such as parent and grandparent as sparse tensors and uses neural networks and finite-domain quantification operations to infer new facts. "
SP:9657121b01c51f78c00d06b47d3e8d678dd85d54,"This paper proposes a new family of differentiable top-k cross-entropy classification losses for k > 1, where k is drawn from a probability distribution. The authors show that relaxing k does not only produce better top-5 accuracies, but also makes models more robust, which leads to top-1 accuracy improvements. The proposed method achieves a new state-of-the-art on ImageNet for publicly available models with an 88.37% top1 and a 98.68% top5 accuracy."
SP:cb3188f435c54a365890e20e4d582c250d919833,"This paper proposes a method for solving large-scale optimal transport (OT) problems at an unprecedented combination of speed and accuracy. The proposed method builds on the celebrated Douglas-Rachford splitting technique and tackles the original OT problem directly instead of solving an approximate regularized problem, as many state-of-the-art techniques do. This allows to provide sparse transport plans and avoid numerical issues of methods that use entropic regularization. The algorithm has the same cost per iteration as the popular Sinkhorn method, and each iteration can be executed efficiently, in parallel. In addition, the authors establish a linear convergence rate for the formulation of the OT problem."
SP:9a087cc734a3e7f3ab848bef5e2eff37fe40f303,"This paper proposes a framework for disentangling performance gaps in federated learning (FL) by separating performance gaps from unseen client data (out-of-sample gap) from performance gaps (from unseen client distributions) in FL. The authors also propose a semantic synthesis strategy that enables realistic simulation without naturally-partitioned data. In addition, the authors call out community suggestions for future FL works."
SP:da0e8c89f343abfe500eb4c1968e418c2fb52ef6,"This paper studies the few-shot setting of prompt-based language models (PLMs) for zero-shot tasks. The authors propose a simple multi-null prompting strategy to improve the performance of PLMs for the zero shot setting. They explore three different strategies: hand-crafted prompts, continuous prompts, and various answer engineering strategies. They show that a simple Multi-Null Prompting (without manually/automatically created prompts) strategy can yield very promising results on a few widely-used datasets, e.g., IMDB dataset and Amazon dataset. "
SP:9817dccb1a121058b23a2ef825ed339cf8b53674,"This paper proposes a method to improve the performance of the attention mechanism in image recognition tasks. The proposed method is based on a sharpener module, which aims to align the relevant parts of the encoded image with the target output. The authors claim that the alignment and interpretability of attention can be significantly improved. Experiments on synthetic handwritten digits and real-world scene text recognition datasets show that the proposed method outperforms the mainstream ones such as soft and hard attention."
SP:3913ed3b3cf6494368e3be6cacb637ff85f80ee6,"This paper proposes a method for learning to construct a complete tour plan for the vehicle routing problem (VRP). The proposed method is based on the Permutation Invariant Pooling Model (Kaempfer & Wolf, 2018), which was originally developed for the mTSP problem. The main contribution of this paper is to extend this model to the VRP problem and to use a supervised learning framework to construct the tour plan from scratch. The proposed approach is shown to be much faster and easier to train and achieves competitive results that incorporate the practical aspect of vehicle costs."
SP:594a813c0d0baa66738b9c8331370f861ad3c416,"This paper proposes a method for link prediction based on counterfactual inference. In particular, the authors propose a method to predict the existence of edges between two pairs of nodes based on the structure of the graph. The proposed method is based on two components: (1) generating counterfactually generated links, and (2) learning representations from both the observed and counterfactuality links. Experiments on benchmark datasets show that the proposed method achieves state-of-the-art performance on link prediction. "
SP:48a7e50451b887f55be17b2662aa11ce18791cc1,"This paper proposes a two-stage Second-Order unsupervised feature selection via knowledge contrastive disTillation (SOFT) model that incorporates the second-order covariance matrix with the first-order data matrix for feature selection. In the first stage, SOFT learns a sparse attention matrix that can represent second order relations between features, and in the second stage, it builds a relational graph based on the learned attention matrix and perform graph segmentation. Experimental results on 12 public datasets show that SOFT outperforms classical and recent state-of-the-art methods."
SP:14bcae11aeede63f28d1b80c05ed18a01d3e3f3c,"This paper proposes a semi-supervised multi-modal variational autoencoder (MEME) that combines information between modalities implicitly through mutual supervision. This formulation naturally allows learning from partially-observed data where some modalities can be entirely missing, which is something that most existing approaches either cannot handle, or do so to a limited extent. The authors demonstrate that MEME outperforms baselines on standard metrics across both partial and complete observation schemes on the MNIST-SVHN (image-image) and CUB (image–text) datasets. They also demonstrate the quality of the representations learnt by mutual supervision against standard approaches."
SP:e834a52cadebe5f125ce491273b4ad1146beae3f,"This paper proposes Deep Explore Options, a method for combining intrinsic and extrinsic rewards in deep reinforcement learning (RL). Intrinsic Motivation (IM) is an alternative reward signal used to spur curiosity and entice behavior exploration in RL. Explore Options (EO, Bagot et al. (2020)) have been proposed as an alternative to the wide-spread weighted-sum (WS) scheme rt = rte + betar t i + r_t i in the tabular setting, which allows the agent to explore in a directed way, i.e. explicitly looking for new knowledge and experiences. In this paper, the authors propose to extend EO to high-dimensional spaces and propose to use intrinsic reward learning as an auxiliary task, with a new architecture achieving 50% faster wall-clock speed and building a stronger, shared representation. The proposed method is evaluated on hard and easy exploration games of the Atari Suite."
SP:41578dd1a4bdb043b3d68afa5f9cebb3e14f3907,"This paper proposes a new method for learning Hamiltonian dynamical systems from data. The proposed method is based on the stiffness-aware neural network (SANN), which identifies and splits the training data into stiff and non-stiff portions based on a stiffness aware index (SAI) to quantify the stiffness of the dynamical system. This classification along with a resampling technique allows them to apply different time integration strategies such as step size adaptation to better capture the dynamic characteristics of the Hamiltonian vector fields. The authors evaluate SANN on complex physical systems including a three-body problem and billiard model and show that SANN is more stable and can better preserve energy when compared with the state-of-the-art methods, leading to significant improvement in accuracy."
SP:bfb0a059eeb6f40a18fbd20c0eec5037a64ca09e,"This paper proposes a method to improve the performance of Transformer-based language models on multi-step computations. The main idea is to allow the model to produce an arbitrary sequence of intermediate tokens, which it calls a scratchpad, before producing the final answer. The scratchpad contains the intermediate results from a standard long addition algorithm (see Figure 2). To train the model, the intermediate steps of the algorithm are encoded as text and use standard supervised training to make them better at complex discrete computations without modifying the underlying architecture. The authors also show that scratchpads help Transformers learn to perform long addition in the fine-tuning regime, and in particular that they improve out-of-distribution generalization generalization to larger problem instances."
SP:e6c1a8b4bba287455dc9cf145b6bd1f04e2148a9,"This paper proposes feature-level adversarial perturbations using deep image generators and a novel optimization objective to generate targeted feature-based attacks at the ImageNet scale that are simultaneously interpretable, universal to any source image, and physically-realizable. These attacks can also reveal spurious, semantically-describable feature/class associations that can be exploited by novel combinations of natural objects. The authors also use them to guide the design of “copy/paste” adversaries in which one natural image is pasted into another to cause a targeted misclassification."
SP:873618263dc4246a39c44d0abfecfb5f688817e3,"This paper proposes a reinforcement learning approach for simulated annealing (SA) problems. The main idea is to learn the neighborhood proposal distribution and temperature schedule for SA. The proposal distribution is learned using reinforcement learning, and the temperature schedule is learned via a policy optimization approach. The proposed approach is evaluated on four tasks: Rosenbrock’s function, bin packing, TSP, resource allocation, and bin packing."
SP:cae31f7436920eb3946e3f5bca0ac88a73d7c3ec,"This paper studies the non-stationarity problem in cooperative multi-agent reinforcement learning (MARL). In particular, the authors propose a novel notion, the $\delta$-stability, which is defined as the divergence between the KL-divergence of consecutive joint policies. The authors then propose a trust-region decomposition network (TRD-Net) based on message passing to estimate the joint policy divergence more accurately. The proposed method is evaluated on several MARL tasks, and it is shown that the proposed method can achieve better performance compared with baselines."
SP:989b58167a15ae4fafbe27ff534d327991b6c4d7,"This paper proposes a self-supervised representation learning framework for audio-visual speech, which masks multi-stream video input and predicts automatically discovered and iteratively refined multimodal hidden units. The proposed method achieves state-of-the-art performance on the lip-reading task with only 30 hours of labeled data, outperforming the previous SOTA model (Makino et al., 2019) trained with a thousand times more transcribed video data (31K hours). The lip reading WER is further reduced to 26.9% when using all the labeled data from LRS3 and combined with self-training. "
SP:7c9eb8aa4a4dcb5965157d860e812d81654e3aa7,This paper proposes a reinforcement learning (RL) approach for solving max-cut combinatorial optimization problems. The main idea is to use a single GNN to pre-process the max cut problem and then use a recurrent unit to explore the solution space in an exploratory phase. Theoretical results show that the proposed approach is faster than the existing state-of-the-art approaches on max cut problems. Experiments are conducted on 500-5000 vertices and 10k vertices.
SP:f741d980c9c560a21298e947f1605dcbab7ceeac,"This paper proposes a new approach to train VAEs with discrete latent variables. The authors propose to use evolutionary algorithms to optimize the parameters of the variational autoencoder (VAE) using truncated posteriors. The proposed approach is evaluated on zero-shot denoising, where the authors show that the proposed approach outperforms amortized training and sampling-based methods. "
SP:deb189d37bd51b92762ce259a106d9a9e9d81ea4,"This paper proposes a method to identify controlled effects of an agent’s actions on the environment using counterfactual measures of blame. The proposed method, called Controlled Effect Network (CEN), is an unsupervised method that is based on the human notion of “blame”. CEN is evaluated in a wide range of environments showing that it can accurately identify the controlled effects. Moreover, it is evaluated as an intrinsic motivator by integrating it in the state-of-the-art exploration method, achieving substantially better performance than action-prediction models."
SP:ea18d57904e25fd09ed0f6c9972029d78779a8a6,"This paper proposes a structure-regularized pruning (SRP) method to improve the performance of SRPN-L and SRPN, a lightweight network and a very deep one. The main idea is to prune the weights of the residual blocks of the SRPN network, and then use L2 regularization to drive the weights towards zero so that eventually their absence will cause minimal performance degradation. Experiments on CIFAR-10 and ImageNet show that the proposed method outperforms the existing methods."
SP:0dee45001ae9600f485614dfe6874a516ac01db5,"This paper proposes a method for cross-domain few-shot learning (CDFSL) that tackles large domain shift between base and novel categories. The method consists of three steps: (1) pretraining a backbone network on a single-source dataset, (2) learning a masking module on the target dataset, and (3) fine-tuning the backbone network and feature selection module. The backbone network is trained in an unsupervised fashion, where a self-supervised contrastive learning approach is considered with the contrastive loss (Chen et al., 2020). This is in contrast to meta-learning approaches, which use supervision during the pre-training stage. Experimental results demonstrate that the proposed method outperforms all meta-training approaches and produces competitive results against recent cross- domain methods."
SP:92aa611d71a8da597358330d84fddbb90de2cf4f,This paper studies the generalization properties of neural networks trained by gradient descent (GD) and by Bayesian inference (BI). The authors show that GD can improve generalization by selecting networks with a large margin. They also provide a PAC-Bayes bound on the average test error of the neural network–Gaussian process (NNGP) for binary classification. The paper is well-written and easy to follow. 
SP:a0e3cf719a95bbc5aad2f663ba5a3169c316ee9b,"This paper proposes a cross-lingual manifold mixup (X-Mixup) method, which adaptively calibrates the representation discrepancy and gives compromised representations for target languages. Experiments on the XTREME benchmark show X-MixUp achieves 1.8% performance gains on multiple text understanding tasks, compared with strong baselines, and reduces the performance discrepancy significantly. "
SP:19f8cd8f0c274b6141ba097d2ebb6d18af0986fd,"This paper studies the problem of Byzantine robust distributed or federated learning, where a central server trains a machine learning model over data distributed across multiple workers. In this setting, a fraction of the workers may deviate from the prescribed algorithm and send arbitrary messages to the central server, leading to a significant loss of performance. The authors propose a simple bucketing scheme that adapts existing robust algorithms to heterogeneous datasets at a negligible computational cost. They also theoretically and experimentally validate their approach, showing that combining bucketing with existing robust methods is effective against challenging attacks."
SP:4d63513b9a1b9b9fc44a69b3d5679a8f48eb95e7,"This paper investigates the role of disentanglement in the representation learned by multi-task learning in the context of hard-parameter sharing. The authors propose a set of metrics to evaluate the disentangledness of the representations obtained by neural networks trained on automatically generated supervised tasks. They show that disentangling is achieved naturally during the training process of the models. They also show that in a setting with hard parameter sharing, the disenanglement is achieved in the latent representation of the model."
SP:9851adb72e2918780f661f83f7da06eb866787be,"This paper presents a framework of certifying robust policies (CROP) for reinforcement learning against adversarial state perturbations, which provides state level robustness certification and the first certification for cumulative rewards. Specifically, the authors develop a local smoothing algorithm that uses a policy derived from Q-functions smoothed with Gaussian noise over each encountered state to guarantee the robustness of actions taken along this trajectory. The authors also develop a global smoothing algorithms for certifying the strength of the lower bound of a finite-horizon cumulative reward under adversarial attacks.  The authors evaluate the proposed method on three representative Atari games. "
SP:78da3c97182ec1baf6a131740bf7c91a9afb2fd2,"This paper proposes a new approach to conformal prediction in which they aim to output a precise set of promising prediction candidates that is guaranteed to contain a limited number of incorrect answers. Conformal prediction provides the ability to adapt to model uncertainty by constructing a calibrated candidate set in place of a single prediction, with guarantees that the set contains the correct answer with high probability. In this paper, the authors propose to trade coverage for precision by enforcing that the presence of incorrect candidates in the predicted conformal sets (i.e., the total number of false positives) is bounded according to a user-specified tolerance. Subject to this constraint, their algorithm optimizes for a generalized notion of set coverage that allows for any number of true answers for a given query (including zero). They demonstrate the effectiveness of this approach across a number of classification tasks in natural language processing, computer vision, and computational chemistry."
SP:b126d2f3c397633745c8833e22ace93a2470e963,"This paper studies the complexity of ReLU networks with random initialization. The authors show that the expected length distortion does not grow with depth, and indeed shrinks slightly. They also generalize this result by proving upper bounds both for higher moments of the length distortion and for the distortion of higher-dimensional volumes. These theoretical results are corroborated by their experiments."
SP:b3b6d0512edfca461ea295ee8665f7f226c45d57,"This paper proposes SAFER, a safe reinforcement learning algorithm for learning policies in settings where safety constraints are difficult to specify and sparse rewards. The proposed algorithm is based on the idea of behavioral prior learning, which extracts useful policy primitives for learning from offline datasets. The authors claim that current behavioral priors may not be wellequipped for safe policy learning, and in some settings, may promote unsafe behavior, due to their tendency to ignore data from undesirable behaviors. To overcome these issues, SAFER learns to extract a safety variable from offline data that encodes safety requirements, as well as the safe primitive skills over abstract actions in different scenarios. The inference stage composes a safe and successful policy from the safety skills according to the inferred safety variable and abstract action. In the experiments, the authors demonstrate its effectiveness on several complex safety-critical robotic grasping tasks inspired by the game Operation."
SP:a5dadb3ecc3caed3b9d9a68eda0d48a53c2d1ce2,"This paper proposes a multi-branch architecture for image restoration. The main idea is to separate different attention features from stacking multiple complicated blocks to multiple simple block with simple block architecture. The proposed method is evaluated on three image restoration tasks: deblurring, dehazing and deraining. The results show that the proposed method outperforms the baseline methods."
SP:263b386beee44b0b45b6f6dc3cf80d020500be62,"This paper proposes an approach for personalized federated learning (PFL) with novel clients at inference time, i.e., after a model has been trained on a set of clients, it needs to be evaluated on novel unlabeled clients. The proposed approach is based on a hypernetwork module and an encoder module, where the encoder learns a representation for a client given its own unlabelled data, and the hypernetwork generates a personalized model for that client. The authors show that the proposed approach generalizes better than existing FL and PFL methods, especially when the novel client has a large domain shift. They also analyze the generalization error for the novel clients."
SP:960d0a63a82593f6e72275b65f0501f0469d1924,"This paper proposes a conditional diffusion based generative model (RCDM) to visualize representations learned by self-supervised learning (SSL) methods. The authors show that the representation learned by RCDM is more robust to adversarial perturbations and can be used for image manipulation. In addition, the authors demonstrate the model's generation quality is on par with state-of-the-art generative models."
SP:398899e6c86b4a2a17dfa5c2f4478811f4331c1d,"This paper studies the problem of differentially private fractional frequency moments estimation, i.e. estimating the pth frequency moment of a given set. The authors propose a new algorithm, Fp sketch, which is based on the well-known sketching algorithm of Indyk, 2006. The main contribution of this paper is to show that the algorithm is DP-guaranteed when the cardinality of the set $p$ is $p \in (0,1$ and the space complexity is $O(1/\sqrt{log}(n/\epsilon^2)$. This is exponentially better than existing DP baselines and only worse than the optimal non-private baseline by a logarithmic factor."
SP:3253b13851b5a3b5e3c8c6e24891db05903a4e57,"This paper proposes a method for learning policies that are both locally optimal and sufficiently different from existing ones. The proposed method, Reward-Switching Policy Optimization (RSPO), switches between extrinsic and intrinsic rewards via a trajectory-based novelty measurement during the optimization process to encourage the learning policy to consistently converge towards a previously undiscovered local optimum. RSPO is able to discover a wide spectrum of strategies in a variety of domains, ranging from single-agent particle-world tasks and MuJoCo continuous control to multi-agent stag-hunt games and StarCraftII challenges."
SP:e3ab3aa87ab023bd9949b99a17d4b6e26c1473c0,This paper proposes a differentiable sampler search method for diffusion models. The main idea is to optimize the degrees of freedom of diffusion models by maximizing sample quality scores via gradient descent. The authors propose Generalized Gaussian Diffusion Models (GGDM) as a family of flexible non-Markovian samplers. The optimization procedure backpropagates through the sampling process using the reparametrization trick and gradient rematerialization. The proposed method achieves strong results on unconditional image generation across various datasets with only 10 inference steps.
SP:7a7506f2b5500a573c0cfb8b0822e5ea725c886a,This paper proposes an approach to improve the robustness of large language models (LLMs) by adapting natural language prompts into continuous representations that allow LLMs to accurately predict factual information. The authors propose a lightweight model called P-Adapters that takes LLM embeddings as input and output continuous prompts that are used to query the LLM. The approach is evaluated on the BERT and RoBERTa datasets and compared with the more complex MoE models. 
SP:35cdf71f027cc5168b55cc34c64bfb2f3087d6f5,"This paper proposes a method to classify time series data at every time. The authors define the problem of continual classification of time series as a continual learning task with the unclear distribution division. They propose a novel Adaptive model training policy ACCTS to overcome two main problems and finally achieve CCTS. The adaptability represents in two aspects: (1) adaptive multi-distribution extraction policy. Instead of the fixed rules and the prior knowledge, ACCTS extracts data distributions adaptive to the time series evolution and the model change; (2) adaptive importance-based replay policy. Experiments on four real-world datasets show that the proposed method can classify more accurately than all baselines at each time."
SP:d9b74b749aa465496763d3a3a9bf3a53e800587e,"This paper proposes a method to increase the size of the external memory of self-attention layers in transformers by using approximate k-nearest-neighbor (kNN) search into a large external memory. The kNN-augmented layer stores the (key, value) pairs in the memory, which are then processed by the local attention layer. The authors demonstrate that kNN can be used to improve the performance of transformers across various benchmarks and tasks, including generic webtext, math papers, books, code, and formal theorems."
SP:7a1bbf86c3fdb8738aa826ca330493e857d050ba,"This paper proposes to interpret masked language modeling (MLM) as energy-based sequence models and propose two energy parametrizations derived from the trained MLMs. Then, the authors develop a tractable sampling scheme based on the Metropolis-Hastings Monte Carlo algorithm. The samples are proposed from the same masked conditionals used for training the masked language models, and they are accepted or rejected based on their energy values according to the target distribution. Experiments on open-ended unconditional generation and a conditional generation task of machine translation verify the effectiveness of the proposed method."
SP:011626ba4fafee13d4a30e3f13c1df5b7071a7f1,"This paper proposes a learning-based data augmentation method for NLP tasks. The authors propose a novel reward function for training the augmentation policy to construct difficult but not too different samples (DND). The proposed method jointly optimizes a data-augmentation policy while training the model, to construct the augmented samples with low confidence but a high semantic similarity with original ones. In addition, a sample re-weighting scheme is introduced to focus on difficult augmented samples after the original ones are learned confidently for more effective learning from the augmented ones. Experiments on various text classification tasks and GLUE benchmark demonstrate the effectiveness of the proposed method."
SP:69d41a862ea189f72d4e8af2854e27b95a91fa41,"This paper studies the context-based offline reinforcement learning (COMRL), an understudied problem with tremendous potential impact by enabling RL algorithms in many real-world applications. The authors propose to improve upon one of the SOTA OMRL algorithms, FOCAL, by incorporating intra-task attention mechanism and inter-task contrastive learning objectives, to robustify task representation learning against sparse reward and distribution shift. Theoretical analysis and experiments are presented to demonstrate the superior performance and robustness of the end-to-end and model-free framework compared to prior algorithms."
SP:ed86c60850d5c8302dcf1c2167db303e778fe681,This paper proposes an inference-time improvement framework for parametric sequential generative modeling methods called belief fine-tuning (BFT). BFT leverages approximate dynamic programming to determine the model parameters at each time step. It can improve the accuracy of the belief model at test time because it specializes the capacity of the model to the space of local observations. BFT enables approximate public belief state search in imperfect-information games where the number of possible information states is too large to track tabularly.
SP:6150725599c10f0e26f0d7cb1fc04b5b227a4456,"This paper proposes Pixelated Butterfly, a method for sparse training of overparameterized neural networks. The main idea is to optimize over a continuous superset of sparse matrices with a fixed structure known as products of butterfly matrices. The paper also proposes simple variants of butterfly (block and flat) to take advantage of modern hardware. The method (Pixelated Butterfly) uses a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers (e.g., attention, MLP)."
SP:136e31054a55abca840f6478491972023c2296cb,"This paper proposes a novel conditional diffusion probabilistic model (DDPM) for conditional image generation, which explicitly models the class center in the forward and reverse process, and make an elegant modification to the original formulation, which enables controllable generation and gets interpretability. The authors also provide another direction for faster sampling and more analysis of the method. Extensive experiments on multiple tasks, and achieve competitive results compared with the state-of-the-art methods."
SP:fc2196f1f4ecd864398fed6640ff3f8b19870763,"This paper proposes a novel approach for domain generalization (DG) based on decomposing the high-dimensional latent feature space to lower-dimensional sub-spaces, after which an individual hypothesis is learned (optimally) for each latent sub-space. The label-informative features are learned from source latent representations, which are then used to project the target examples onto appropriate sub-paces. The authors also provide a rigorous theoretical analysis to explain how their approach can learn meaningful sub-samples that helps to reduce the domain shift. Finally, they empirically demonstrate that the proposed method can achieve favorable results on several benchmark datasets."
SP:6e8e5bdeb77e3cafe1975da8411fb65118955d14,"This paper improves upon the kernel thinning (KT) algorithm of Dwivedi and Mackey (2021) by targeting a reproducing kernel Hilbert space (RKHS) and leveraging a less smooth squareroot kernel. The authors show that KT applied directly to the target RKHS yields tighter, dimension-free guarantees for any kernel, any distribution, and any fixed function in the RKTHS. They also show that, for analytic kernels like Gaussian, inverse multiquadric, and sinc, target KT admits maximum mean discrepancy (MMD) guarantees comparable to or better than those of square-root KT without making explicit use of a square root kernel. Finally, they establish that KT+ inherits the improved MMD guarantees of power KT and the tighter individual function guarantees of target KT. In experiments with target KT and KT+, they witness significant improvements in integration error even in 100 dimensions and when compressing challenging differential equation posteriors."
SP:645c3f1864aa843d4899fc2406f694b5aab8460d,"This paper presents an open-source benchmark suite for the max-min independence set problem. The suite offers a unified interface to various state-of-the-art traditional and machine learning-based solvers. The paper also conducts an in-depth analysis of the popular guided tree search algorithm by Li et al. (2018) and shows that the graph convolution network used in the tree search does not learn a meaningful representation of the solution structure, and can in fact be replaced by random values."
SP:155ecd17d264a084b014abdfd0362146d8fb07e0,"This paper proposes Wavelet Compressed Convolution (WCC) to compress the activation maps compression for 1x1 convolution layers. WCC uses a hardware-friendly Haar-wavelet transform, known for its effectiveness in image compression, and define the convolution on the compressed activation map. By combining WCC with light quantization, WCC achieves compression rates equal to 2-bit and 1-bit with minimal degradation in image-to-image tasks."
SP:004865e6affad32403b7965493a53c8a7ffdda0a,"This paper studies the problem of learning in extensive form games, which is a generalization of normal form games to games with general-sum utilities and imperfect information. The authors propose a new algorithm for learning in this setting, which they call “extensive form correlated equilibrium (EFCE)”. The algorithm is based on the counterfactual regret minimization (CFR) framework, and it is shown that it converges to an approximate EFCE at a rate of $O(T^3/4)$ where $T$ is the number of players. This is faster than the previous best rate of $\tilde{O}(T^{1/2)$, which is the worst-case convergence rate for two-player zero-sum games. "
SP:ee545ff83df4d7ff256ac61fbe0eb0765f52f1d5,"This paper proposes a method to learn a discretization of continuous action spaces by leveraging the priors of demonstrations. By discretizing the action space, the proposed method can be applied to any discrete action deep RL algorithm to the continuous control problem. The method is evaluated on three different setups: RL with demonstrations, RL with play data, and Imitation Learning. The proposed method consistently outperforms state-of-the-art continuous control methods."
SP:4b39279b98d6aa311bb49dd1384925f9d6f66c2d,"This paper proposes AdvStyle, a method for domain generalization in semantic segmentation based on adversarial style augmentation. The idea is to generate hard stylized images during training and thus prevent the model from overfitting on the source domain. The proposed method is easy to implement and can be easily applied to different models. Experiments on two synthetic-to-real semantic segmentations benchmarks demonstrate that AdvStyle can significantly improve the model performance on unseen real domains."
SP:4a2e6d70b383e4941e0bc44e7e82972b22e26792,"This paper proposes a novel approach for mid-air gesture recognition. The proposed approach consists of an event-based guided Variational Autoencoder (VAE) and a convolutional neural network (SNN) to encode spatio-temporal event streams of events recorded by a Neuromorphic Dynamic Vision Sensor (DVS) into a latent space representation suitable to compute the similarity of midair gesture data. The hybrid GuidedVAE achieves 87% classification accuracy on the DVSGesture dataset and it can encode the sparse, noisy inputs into an interpretable latent space representations, visualized through T-SNE plots. The authors also implement the encoder component of the model on neuromorphic hardware and discuss the potential for the algorithm to enable real-time, self-supervised learning of natural mid- air gestures."
SP:2e66468a6b94177e54b0052b97713ee63902c278,"This paper proposes to use ferns (oblivious decision trees) instead of neurons for deep learning for tabular data. The authors propose a Sparse Hierarchical Table Ensemble (S-HTE) to enable deep learning capabilities by constructing a fern-based neural network, which is dense at the beginning of the training process and becomes sparse using annealing mechanism, leading to an efficient final predictor. The paper shows its accuracy is comparable to alternatives, while having an order of magnitude lower computational complexity."
SP:b238db9252d83a13438bb747d70e635bb9945958,"This paper tackles the problem of learning value functions from undirected state-only experience (i.e. state transitions without action labels). The authors first theoretically characterize the applicability of Q-learning in this setting. They show that tabular Q learning in discrete Markov decision processes (MDPs) learns the same value function under any arbitrary refinement of the action space. This theoretical result motivates the design of Latent Action Q-Learning (LAQ), an offline RL method that can learn effective value functions. LAQ learns value functions on discrete latent actions obtained through a latent-variable future prediction model. The experiments in 5 environments ranging from 2D grid world to 3D visual navigation in realistic environments demonstrate the benefits of LAQ over simpler alternatives."
SP:108ebe9045a9e2b8b5aba8352733782462db8a81,"This paper proposes SWARM, a model parallelism algorithm for training large models on swarms of heterogeneous unreliable devices. SWARM creates temporary randomized pipelines between available nodes that are rebalanced in case of failure. To further reduce the network usage of the approach, the authors develop several compression-aware architecture modifications and evaluate their tradeoffs. Finally, they combine their insights to train a large Transformer language model on a swarm of preemptible T4 GPUs with less than 400Mb/s network throughput."
SP:91d2f094d5481651b554f58aecc2a6207057a47c,"This paper proposes a method to correct the transition dynamics of offline data for online tuning in decentralized multi-agent reinforcement learning (MARL). The transition dynamics in offline data do not accord with the dynamics in online execution, which creates severe errors in value estimates, leading to uncoordinated and sub-optimal policies. The authors propose a method called online transition correction (OTC) to implicitly correct the biased transition dynamics by modifying sampling probabilities. They design two types of distances, i.e., embedding-based and value-based distance, to measure the similarity between transitions, and propose an adaptive rank-based prioritization to sample transitions according to the transition similarity. OTC is simple yet effective to increase data efficiency and improve agent policies in online tuning."
SP:d0e650d568214481b07a0452ec606ccbf6d05410,"This paper proposes a logarithmic unbiased quantization (LUQ) method to quantize both the forward and backward phase to 4-bit, achieving state-of-the-art results in 4-bits training. The authors analyze different types of stochastic rounding schemes to explain the importance of having an unbiased rounding scheme for the neural gradients. The proposed method is based on the [1,3,0] FP4 format for the gradients to convert the standard FP4 to the INT4 format. The forward phase quantization is combined with the forward quantization of INT4 to form the forward phase with INT4 quantization. The method is evaluated on ResNet-50 on ImageNet and CIFAR-10, and achieves the state of the art results."
SP:f2862d1f987164ed6c3c375cd8962e57c369373b,"This paper studies the problem of polythetic classification in the context of few-shot learning. Specifically, the authors show that threshold meta-learners, such as Prototypical Networks, require an embedding dimension that is exponential in the number of task-relevant features to emulate these functions. In contrast, attentional classifiers such as Matching Networks, are able to solve these problems with a linear embeddings dimension. The authors propose a self-attention feature-selection mechanism that adaptively dilutes non-discriminative features. They demonstrate the effectiveness of their approach in meta-learning Boolean functions, and synthetic and real-world tasks."
SP:e1e513fef25d29e17cdadd1b36d932a8ad8897cd,"This paper proposes a multi-agent reinforcement learning (MARL) framework to study the emergence of language between agents using a continuous communication channel trained through reinforcement learning. The authors propose a simple messaging environment where a speaker agent needs to convey a concept to a listener agent, and the listener needs to map the continuous signal to the concept. The speaker agent is equipped with a vocoder that maps symbols to a continuous waveform, and this is passed over a lossy continuous channel. The listener agent is trained using deep Q-learning. The paper shows that basic compositionality emerges in the learned language representations. It also shows that noise is essential in the communication channel when conveying unseen concept combinations. "
SP:0e6ff65ba4a3df35947d1b6f4d438612088d90a0,"This paper proposes BadPre, a method for backdoor attack against pre-trained NLP models. BadPre embeds the backdoors into the pretrained language models, which can be transferred to the downstream language tasks. The key feature of BadPre is that the adversary does not need prior information about the downstream tasks when implanting the backdoor to the pret-trained model. When this malicious model is released, any downstream models transferred from it will inherit the backdoor, even after the extensive transfer learning process. Experimental results show that BadPre can compromise a wide range of downstream NLP tasks in an effective and stealthy way."
SP:58d3ecb4a1906251e79ad883aa97cc2502642658,"This paper proposes a reward-free method for skill discovery, where skills are learned one after another in an incremental fashion. The main idea is to treat each individual skill as an independent neural network policy without parameter sharing, which further decouples the learned skills into a set of previously learned skills. The new skills are optimized to have high entropy with respect to previously learned state, and the old skills are fixed to ensure the agent doesn’t forget a learned skill. The proposed method is evaluated on a series of continuous control tasks, where it is shown to outperform the existing state-of-the-art skill discovery methods on both skill quality and the ability to solve downstream tasks."
SP:2c6595408f5ec95537eaf555e5fe3d992b58c222,"This paper proposes a new convolutional layer, called log-polar space convolution (LPSC), where the convolution kernel is elliptical and adaptively divides its local receptive field into different regions according to the relative directions and logarithmic distances. The receptive field grows exponentially with the number of distance levels. The authors show that LPSC can be implemented with conventional convolution via log-Polar space pooling and can be applied in any network architecture to replace conventional convolutions. Experiments on different tasks and datasets demonstrate the effectiveness of the proposed method."
SP:7791f96b1eef277a9133975507a750d9e7c6b8ff,"This paper studies the role of information compression in the generalization properties of neural networks (NNs). In particular, the authors propose a new information bottleneck (IB) based on the trade-off between accuracy and information complexity of NNs, namely PIB, which is an approximation of the Information Bottleneck (IB). The authors also propose an MCMC-based algorithm to sample from the optimal weight posterior characterized by PIB. Empirically, PIB is able to identify the fitting to compressing phase transition during NNs’ training and the concrete connection between the IIW compression and the generalisation."
SP:a733847ade77ffbf38760fc79da17893dea8d53f,"This paper investigates the properties of data poisoning attacks, which add imperceptible perturbations to training data to maximize the test error. The authors find that the perturbation of advanced attacks are almost linear separable when assigned with the target labels of the corresponding samples. Moreover, the authors further confirm that linear separability is indeed the workhorse for recent attacks. The paper also shows that the adversarial attacks are as powerful as the deliberately crafted attacks."
SP:7b50be406138ad01db3ee112899f622637896fe9,"Offline policy optimization is a critical problem in many real-world decision-making problems, as online learning is costly and concerning in many applications. In this paper, the authors propose an algorithm to avoid the overfitting phenomenon in optimizing the importance weighted return, and propose a better per-state-neighborhood normalization condition to avoid this overfitting issue. The authors provide a theoretical justification of the proposed algorithm through a new constraint on policy named eligible actions (EA), and show the limitation of previous attempts to this approach through an illustrative example. The experiments show the proposed method with less overfitting and better test performance compared with state-of-the-art batch RL algorithms."
SP:c976752a55b9ff47dc63c95a9fd7b51a81e8a42e,"This paper presents a method for continual learning of how language is grounded in vision. In particular, the authors propose CoLLIE, a simple yet effective model that learns a transformation function that adjusts the language embeddings when needed to accommodate new language use. The model does not just learn new classes and labels, but can also generalize to similar language use, as opposed to traditional few-shot learning. The authors verify the model's performance on two different tasks of continual learning and show that it can efficiently learn and generalize from only a few examples, with little interference with the original zero-shot performance."
SP:d3371b322acfc321ee79a2e1b438d82644872fa4,"This paper proposes a novel method for novel object captioning (NOC) that leverages language knowledge from BERT and CLIP. The proposed method, VLAF2, is able to achieve state-of-the-art results in terms of fidelity, fluency, and fluency. Experiments on the nocaps dataset show that the proposed method outperforms the existing methods."
SP:9f3b6486662d80350d77a4b060d4a5b8b22a6130,"This paper studies the problem of transfer learning in the setting of few-shot learning, where the goal is to learn representations for classification that are transferable to new, unseen classes. In this setting, the authors propose a new perspective based on the recently discovered phenomenon of neural collapse (Papyan et al., 2020), which identifies the training dynamics of deep networks for standard classification tasks where the features output of the penultimate layer (the output of feature maps) are concentrated around the same class. The authors show that neural collapse generalizes to new samples from the training classes, and – more importantly – to new classes as well, allowing foundation models to provide feature maps that work well in transfer learning and, specifically, in the few shot setting."
SP:624c95d9ce1ee4b66274e858e2da22bef6b052c7,"This paper proposes a deep point cloud reconstruction network consisting of two stages: 1) a 3D sparse stacked-hourglass network as for the initial densification and denoising, 2) a refinement via transformers converting the discrete voxels into 3D points. The paper further improves the performance of transformer by a newly proposed module called amplified positional encoding. Extensive experiments demonstrate that the network achieves state-of-the-art performance among the recent studies in the ScanNet, ICL-NUIM, and ShapeNetPart datasets."
SP:34a81ca65131576d4c14332a4e9eb3a4c344cab7,"This paper proposes a new method for distributed GCN training, called PipeGCN, which aims to achieve a full-graph accuracy boosted training efficiency. The main idea is to pipeline inter-partition communication to hide the communication overhead by pipelining node features and feature gradients among partitions for every GCN layer in each training iteration. The paper also provides a theoretical convergence guarantee and shows the convergence rate is close to that of the vanilla distributed GCNs training without staleness. Extensive experiments show the effectiveness of the proposed method."
SP:8302d49558ee0f16392d623d4e604e92db10d041,"This paper proposes a method for test-time robustification, i.e., using the test input to improve model robustness. The authors propose a simple approach that can be used in any test setting where the model is probabilistic and adaptable: when presented with a test example, perform different data augmentations on the data point, and then adapt (all of) the model parameters by minimizing the entropy of the model’s average, or marginal, output distribution across the augmentations. This objective encourages the model to make the same prediction across different augmentations, thus enforcing the invariances encoded in these augmentation, while also maintaining confidence in its predictions."
SP:a985de5e940ff3a4160b378201b8c02f68d1914a,"This paper proposes a model-based reinforcement learning algorithm that jointly optimizes a lower bound on the expected return of the dynamics model and the policy. The lower bound is a global lower bound, and this bound becomes tight under certain assumptions. The proposed algorithm resembles a generative adversarial network (GAN) in that the model is trained using a discriminator that distinguishes between real and fake transitions, and the same discriminator is included in the objective for the policy, and both the model and policy are jointly trained to maximize reward and minimize discriminator accuracy. The authors demonstrate that their algorithm is competitive with prior state-of-the-art methods on benchmark tasks."
SP:a469fbcdc20b11dff4085b6fbc384e77f33cd37d,"This paper proposes a coarse-to-fine approach to combine the advantages of behavioral cloning (BC) and observation history (OH) methods for imitation learning. The main idea is to first compute a coarse action based on the instantaneous observation, and then refine it into a final action using historical information. The proposed method is evaluated on CARLA and MuJoCo continuous control tasks, and it outperforms BC-SO and BC-OH."
SP:95c4533b5d1a865c4cc6a54615e7ad6357bdaad1,"This paper proposes a model-based meta-learning method (DyAd) for dynamic forecasting in large-scale heterogeneous domains. The encoder maps different dynamical systems to time-invariant hidden features representing constants of motion, boundary conditions, and external forces which characterize the system. The forecaster takes the hidden representations and the past system states to forecast the future system state. Theoretically, the generalization error is related to the task relatedness in the source domain, as well as the domain differences between source and target. Experiments are conducted on turbulent flow and real-world ocean data forecasting tasks."
SP:ec70553cb0c27e5349c1b8cce6bcaa96a83bf050,"This paper proposes a method for weakly supervised monocular 3D object detection. The proposed method first detects 2D boxes on the image. Then, it adopts the generated 2d boxes to select corresponding RoI LiDAR points as the weak supervision. A network is trained to predict 3D boxes which can tightly align with associated RoI points. This network is learned by minimizing the newly-proposed 3D alignment loss between the 3D box estimates and the corresponding object-LiDAR point."
SP:34217c6a8ca43b8eeb9ddc83d6f1f0af05918984,"This paper proposes a soft gradient-based subword tokenization module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion. GBST enumerates candidate subword blocks and learns to score them in a position-wise fashion using a block scoring network. The authors also introduce a deep Transformer model that integrates GBST and operates on the byte level. Experiments on English GLUE, multilingual, and noisy text datasets show that GBST outperforms a series of competitive byte-level baselines while generally performing on par and sometimes outperforming subword-based models."
SP:d26d25f2ef23a89a2c139d0dd87c4c86fddcff5e,"This paper studies the problem of detecting backdoors in black-box deep neural networks (DNNs). In this setting, the DNN is a fully black box and only its final output label is accessible. The authors first show that the objective of backdoor detection is bounded by an adversarial objective, which can be optimized using Monte Carlo gradient estimation in this setting. Then, the authors propose the adversarial extreme value analysis (AEVA) algorithm to detect backdoors. The AEVA algorithm is based on an extreme-value analysis computed from the monte-carlo gradient estimation. Experimental results show that AEVA is effective in detecting backdoor attacks under the hard-label scenarios."
SP:c6dbca0ed0799b7fec21777606f6f809eb2d8c48,"This paper proposes a new class-wise uncertainty measure, called Kullback-Leibler divergence criterion (KLoS), which is a KL divergence criterion on the class-probability simplex. The authors claim that KLoS captures class confusion and lack of evidence in a single score and does not require out-of-distribution (OOD) training data. They also design an auxiliary neural network, KloSNet, to learn a refined criterion directly aligned with the evidential training objective. Experiments are conducted on CIFAR-10 and Fashion MNIST datasets to demonstrate the effectiveness of the proposed method."
SP:8b4f3916dca4e627931558e14836749bd4a6792f,"This paper studies the problem of learning convolutional neural networks (CNNs) under some natural distributional assumptions. Specifically, the authors assume that the distribution of patches in the input images has low-dimensional structure (e.g., when the patches are sampled from a low dimensional manifold). Under this assumption, they study a semi-supervised algorithm, that learns a linear classifier over datadependent features which were obtained from unlabeled data. They show that the algorithm provably learns CNNs, and that the dependence of the algorithm on the dimension of the patch distribution is essentially optimal."
SP:7f2f354d5cc1030bd97bd716aea8fe1d3af86b25,"This paper proposes a novel algorithm named Ada-NETS to cluster faces by constructing clean graphs for GCNs. First, each face is transformed to a new structure space, obtaining robust features by considering face features of the neighbour images. Then, an adaptive neighbour discovery strategy is proposed to determine a proper number of edges connecting to each face image. Experiments on multiple public clustering datasets show that Ada-NetS significantly outperforms current state-of-the-art methods, proving its superiority and generalization."
SP:a3bc8e26f55e78f07de081ca85865afd52b6ae4a,"This paper proposes a new method for out-of-distribution generalizable person re-identification (DG) based on distributionally robust optimization (DRO). Specifically, the authors propose a new loss function that optimizes the loss over reweighted dataset where important samples (i.e., samples on which models perform poorly) will be upweighted and others will be downweighted. The proposed method is evaluated on DG ReID and cross-domain ReID benchmarks."
SP:62c1f734b7f6c6e7d5114da6f37c9e3cdda73a23,This paper proposes a simple technique to improve the performance of GNNs by corrupting the input graph with noise and adding a noise correcting node-level loss. The authors claim that the noise correction loss helps ameliorate oversmoothing by encouraging diverse node latents. Experiments are conducted on 3D molecular property prediction tasks and non-spatial datasets.
SP:24a1b44f37f8eedbab2047fb84600a322d289f3b,"This paper proposes a method for learning a vector representation for the set2vec problem, which is the task of extracting vector representation from an input set consisting of a variable number of feature vectors. The authors consider the input set as i.i.d. samples from a mixture distribution and define the set embedding feed-forward network as the maximum-a-posterior (MAP) estimate of the mixture which is approximately attained by a few ExpectationMaximization (EM) steps. The whole MAP-EM steps are differentiable operations with a fixed number of mixture parameters, allowing efficient auto-diff back-propagation for any given downstream task. Furthermore, the proposed mixture set data fitting framework allows unsupervised representation learning naturally via marginal likelihood maximization aka the empirical Bayes."
SP:b4f7b660b84fe7702fbcc8a96c192abc3a64f045,"This paper proposes a method for feature selection in contrastive analysis (CA) setting, where the goal is to select a small number of informative features for use in unknown downstream tasks. The proposed method, called CFS (Contrastive Feature Selection), is based on the idea of selecting a small set of features that are useful for downstream tasks, i.e., select features that best reflect the variations of interest. CFS is evaluated on a semi-synthetic dataset and four real-world biomedical datasets, and it consistently outperforms previous state-of-the-art methods designed for standard feature selection scenarios."
SP:bc4f69f23aba2034cbf14cb31bdc7a991806bbf6,"This paper studies the effect of early stopping (i.e. stopping the training process at test time) on the generalization properties of linear regression models. The authors propose a new overparametrized setting where the label is generated by a low-dimensional transformation of the input data. They show that the optimal early stopping time depends on the model size and the number of samples. They also show that early stopping can help mitigate ""double descent"" in various settings."
SP:ede87b50cd9c4a6533f17e3e5ddfaaeaaac71dcf,"This paper proposes a quasi-Newton method for the policy gradient algorithm with entropy regularization. In the case of Shannon entropy, the resulting algorithm reproduces the natural policy gradient (NPG) algorithm. For other entropy functions, this method results in brand new policy gradient algorithms. The authors provide a simple proof that all these algorithms enjoy the Newton-type quadratic convergence near the optimal policy. The proposed method typically converges in single-digit iterations, often orders of magnitude faster than other state-of-the-art algorithms."
SP:3535504f7599b1f39239f7cd8e09acd40fa8fdf0,"This paper proposes a general method inspired by case-based reasoning to train agents and generalize out of the training distribution. The method can be applied in conjunction with any existing on-policy neural agent in the literature for text-based games. The experiments show that the proposed approach consistently improves existing methods, obtains good out-of-distribution generalization, and achieves new state of the art results on widely used environments."
SP:9a5dd0148a15dc5b4d2bc6762dfe8a8991f8866c,"This paper proposes a two-stage method to distill multiple word senses from a pre-trained language model (BERT) by using attention over the senses of a word in a context and transferring this sense information to fit multi-sense embeddings in a skip-gram-like framework. The authors demonstrate an effective approach to training the sense disambiguation mechanism in our model with a distribution over word senses extracted from the output layer embedding of BERT. Experiments on the contextual word similarity and sense induction tasks show that this method is superior to or competitive with state-of-the-art multi-contextual word embedding models on multiple benchmark data sets, and experiments with an embedding-based topic model (ETM) demonstrates the benefits of using this multi-concept embedding in a downstream application."
SP:e4cdba0fc7cd7f440d4436219f3959d8d5e2ad28,"This paper proposes to transfer the 2D convolutional filters of a 2D image-pretrained model to a 3D point-cloud model by inflating the filters and finetuning the inflated imagepretrained models (FIP). The authors show that FIP-IO (finetuning only input, output and batch normalization layers) can achieve competitive performance on the 3D Point-cloud classification task. The authors also show that the FIP improves data efficiency, reaching up to 10.0 points top-1 accuracy gain on few-shot classification."
SP:dc99c307931ae9c5d4a1b998dc94cfc6ac78d11f,"This paper proposes a method for training autoregressive generative models (ARGMs) with energy-based models (E-ARM). The main idea is to use an energy based learning objective to force ARGMs to fit the joint distribution along with the conditional one at each time step. The proposed method is evaluated on language modeling, neural machine translation, and image generation tasks."
SP:51e748c55bd4134047098559577fa3f37aa7433a,"This paper presents a unified framework that connects Wasserstein distributional robustness (DR) with adversarial training (AT) methods. In particular, the authors introduce a new cost function and a new series of risk functions, with which they show that standard AT methods are special cases of their counterparts in the framework. This connection leads to an intuitive relaxation and generalization of existing AT methods and facilitates the development of a new family of distributional adversarial robustness AT-based algorithms. Extensive experiments show that the proposed algorithms robustify further their standard AT counterparts in various settings."
SP:f192046ea8ad61bfc8e05a0ddb90a8bd15b4640b,"This paper proposes a method for unsupervised representation learning for multivariate time-series data. The authors argue that the existing contrastive learning methods mostly use segment-level data augmentation techniques to sample positives and negatives for contrastive training, which may bring about sampling bias and incorrect optimization with false negatives due to the loss of global context. To address these problems, the authors propose a novel framework, namely Bilinear Temporal-Spectral Fusion (BTSF), which utilizes the instance-level augmentation by simply applying dropout on the entire time series for better preserving global context and capturing long-term dependencies. Extensive experiments are conducted on three major practical tasks for time series such as classification, forecasting and anomaly detection, which is the first to evaluate on all three tasks."
SP:ef54840009afb095c67bbbc29a7824c20a375ee8,"This paper proposes an algorithm for automatically adjusting the learning rate during gradient descent. The learning rate is optimized via a simple extra gradient descent step. The authors formulate first and second-order gradients with respect to learning rate as functions of consecutive weight gradients, leading to a cost-effective implementation. Extensive experimental evaluation is conducted, validating the effectiveness of the proposed method for a plethora of different settings."
SP:263c787361cd6d4443ce516d389c694d0fe44b28,"This paper proposes a meta-learning method for sequential multi-task learning, where the goal is to learn to learn, using their experience on previous tasks to learn new tasks more quickly. The proposed method, continual meta-policy search (CoMPS), meta-training in an incremental fashion, over each task in a sequence, without revisiting prior tasks. CoMPS continuously repeats two subroutines: learning a new task using RL and using the experience from RL to perform completely offline meta learning to prepare for subsequent task learning. The method is evaluated on several continuous control tasks."
SP:2bd729b7aa045bf74e31229c9e76e57af36e804b,"This paper proposes a new threat model for poisoned classifiers, where one without knowledge of the original trigger, would want to control the poisoned classifier. Under this threat model, they propose a test-time, human-in-the-loop attack method to generate multiple effective alternative triggers without access to the initial backdoor and the training data. They construct these alternative triggers by first generating adversarial examples for a smoothed version of the classifier, created with a procedure called Denoised Smoothing, and then extracting colors or cropped portions of adversarial images with human interaction. They demonstrate the effectiveness of our attack through extensive experiments on high-resolution datasets: ImageNet and TrojAI."
SP:e58ab0e3cff6b18013145a1a99cfa9da0a3d872f,"This paper studies the problem of knowledge distillation for unconditional GANs, especially StyleGAN2. The authors claim that the main challenge of unconditional distillation lies in the output discrepancy issue, where the teacher and student model yield different outputs given the same input latent code. To address this issue, the authors propose a novel initialization strategy for the student model, which can ensure the output consistency to the maximum extent, and a latent-direction-based distillation loss that preserves the semantic relations in latent space. Extensive experiments demonstrate that the proposed method outperforms existing GAN distillation methods."
SP:2c2231743fa33b95828c6615263954ce1c05f95d,"This paper proposes a method for approximating offline algorithms in online settings by encoding the behavior of offline algorithms into graphs, and then training a multi-task learning model to simultaneously detect behavioral structures which have already occurred and predict those that may come next. The authors demonstrate the methodology on both synthetic data and historical stock market data, where the contrast between explanation and prediction is particularly stark. Taken together, this work represents the first general and end-to-end differentiable approach for generating online approximations of offline algorithm."
SP:ee3a21d2fb8a073099aa200129a53c31f3b6561d,"This paper proposes a method to improve the scalability of Gaussian Processes (GPs) by amortizing the computation of inducing points and the parameters of the variational posterior approximation. Specifically, the authors propose to use a neural network that receives the observed data as an input and outputs the inducing points locations and the posterior parameters. They evaluate their method in several experiments, showing that it performs similar or better than other state-of-the-art sparse variational GP approaches. "
SP:f20c99b441545047a16ae524cc2e317b2c3787a2,"This paper proposes a protocol for secure (Byzantinetolerant) decentralized training that emphasizes communication efficiency. The authors propose a novel strategy for decentralized Byzantine-tolerant training on all participants, where the extra communication cost does not depend on the number of parameters. They also provide theoretical bounds for its resistance against Byzantine and Sybil attacks and show that it has a marginal communication overhead. They conduct large-scale experiments on image classification and language modeling."
SP:93894f20ab2593e5237b6972fef9fe63e96af89a,"This paper presents a physics-informed machine learning approach for smoothed particle hydrodynamics (SPH), a mesh-free Lagrangian method for obtaining approximate numerical solutions of the equations of fluid dynamics, which has been widely applied to weakly and strongly compressible turbulence in astrophysics and engineering applications. The authors present a learnable hierarchy of parameterized and “physics-explainable” SPH informed fluid simulators using both physics based parameters and Neural Networks as universal function approximators. The learning algorithm develops a mixed mode approach, mixing forward and reverse mode automatic differentiation with forward and adjoint based sensitivity analyses to efficiently perform gradient based optimization. They show that their physics informed learning method is capable of: (a) solving inverse problems over the physically interpretable parameter space, as well as over the space of Neural Network parameters; (b) learning Lagrangians statistics of turbulence; (c) combining Lagrangeian trajectory based, probabilistic, and Eulerian field based loss functions; and (d) extrapolating beyond training sets into more complex regimes of interest."
SP:d11b81f9ab414fcf430a03cd70c2d3246b678474,This paper proposes an entropy maximization regularizer (Mix-MaxEnt) to regularize a single deterministic neural network to obtain improved accuracy and reliable uncertainty estimates. The proposed method generates between-cluster samples via the convex combination of two images from different classes and maximizes the entropy on these samples. Such a data-dependent regularization guides the maximum likelihood estimation to prefer a solution that maps out-of-distribution samples to high entropy regions (creating an entropy barrier); and (2) is more robust to the superficial input perturbations.
SP:365490b872464f00634dc7a50d024fceaf0a61ee,"This paper proposes a method for self-supervised image transfer based on linear navigation in the latent space of a generative auto-encoder. Specifically, the proposed method learns a set of orthogonal motion directions and uses their linear combination to represent any displacement in latent space. The proposed method is evaluated on three datasets (VoxCeleb, Taichi and TED-talk) and shows superior performance compared to the existing methods."
SP:86f9f89f84e117c86478b9afaf087f65524f5472,This paper proposes a meta-learning method called task interpolation (MLTI) that generates additional tasks by randomly sampling a pair of tasks and interpolating the corresponding features and labels. The authors show that the proposed method is compatible with six representative meta learning algorithms and consistently outperforms other state-of-the-art strategies in eight real-world datasets from various domains. Theoretical analysis shows that MLTI corresponds to a data-adaptive meta-regularization and further improves the generalization.
SP:73d577e9c4f4af5e11a9e5bdb583ee0f50a315f5,"This paper proposes a new approach for fair representation learning based on normalizing flows. In particular, the encoder is trained to minimize the statistical distance between the latent representations of different groups. The main advantage of FNF is that its exact likelihood computation allows us to obtain guarantees on the maximum unfairness of any potentially adversarial downstream predictors. Experiments are conducted to show the effectiveness of the proposed approach."
SP:404d5643327f60f0f06f820033a56081f9e01900,"This paper proposes a graph neural network (GNN) for subgraph isomorphism counting. The proposed method is based on the edge-centric message passing scheme, where messages on edges are propagated and aggregated based on edge adjacency. At the graph level, the authors modulate the input graph representation conditioned on the query, so that the graph can be adapted to each query individually to improve their matching. The experimental results show that the proposed method achieves superior performance in comparison to the state-of-the-art baselines."
SP:5a94f18156ab2949c86de45fcf0de2e16977eebb,"This paper studies the problem of agnostic personalized federated learning (PFL), where each client can have their own personalized labels, which might not be compatible with others (even for the same class), and can be also possibly from a variety of multiple domains. The authors propose a novel method, namely Similarity Matching and Kernel Factorization (SimFed), which measures task-level similarity based on locally learned knowledge and matches the relevant ones for personalized knowledge reflection. SimFed factorizes the model parameters into two basis vectors and the highly sparse masks to significantly reduce the dimensionlaity of parameter space for alleviating knowledge collapse and information loss when reflecting the heterogeneous knowledge. Experiments on both single-domain and multi-domain datasets show that the proposed method outperforms the current state-of-the-art approaches."
SP:97f30bea31eccef6c770fbce1e14fd6d2493a178,"This paper proposes an object dynamics distillation network (ODDN) that distills explicit object dynamic representations (e.g., velocity) from raw video input. The authors build a relation module that calculates object-pair interactions and applies it to the corresponding dynamic representations of objects. They verify the approach on tasks of video events reasoning and video prediction. The results show that visual representations of ODDN perform better in answering reasoning questions around physical events in a video compared to representaions of previous scene representation methods."
SP:ba8e50d1fa9cb824fa3f76c0c691997cd151d760,"This paper proposes a new GNN layer called PEG for link prediction. PEG is based on positional encoding (PE) techniques such as Laplacian Eigenmap, Deepwalk, etc. Theoretical analysis shows that PEG has permutation equivariance w.r.t. the original node features and rotation equivariant positional features. Extensive experiments over 8 real-world networks demonstrate the advantages of PEG in generalization and scalability."
SP:cf448479f68c3194c1a9e11729bf70d7cc2ae8fd,"This paper proposes a novel text style transfer framework based on large-scale language models. The proposed LaMer first mines the roughly parallel expressions in the non-parallel datasets with scene graphs, and then employs MLE training, followed by imitation learning refinement, to leverage the intrinsic parallelism within the data. On two benchmark tasks (sentiment & formality transfer) and a newly proposed challenging task (political stance transfer), the model achieves qualitative advances in transfer accuracy, content preservation, and fluency. The model not only makes training more efficient, but also generates more readable and diverse expressions than previous models."
SP:8f7b2d1020d9e527118b8fb816760c13b0d0bfcb,"This paper proposes a method for approximate query answering (QA) on hyper-relational knowledge graphs (KGs). In particular, the authors focus on logical queries that use conjunctions (∧) and existential quantifiers (∃) where the function symbols are parameterized with the qualifiers of the relation. This parameterization enables the QA model to handle higher-order logical queries, which are often observed in real-world KGs applications. The authors also propose a method to answer such queries and demonstrate in their experiments that qualifiers improve QA on a diverse set of query patterns."
SP:5f8b58424a1a8eeb72217e75189d6f773a298a7a,"This paper proposes a method for hyperparameter optimization (HPO) for deep neural networks. The proposed method is based on Bayesian optimization (BO) with a new surrogate for Gaussian Processes that embeds the learning curve dynamics and a new acquisition function that incorporates multi-budget information. The authors also extend Expected Improvement (Jones et al., 1998) to the multi-fidelity case and propose a deep kernel GP that captures the learning dynamics (learning curve until the evaluated budget). The joint effect modeling of a GP kernel across budgets together with a dedicated acquisition function leads to significant empirical gain for the proposed method."
SP:99d3d94e3af5d2dc7b92c00ac1345d1d2dd0d15b,"This paper proposes to improve the performance of learned image compression methods by introducing post-training quantization and making the model inference integer-arithmetic-only, which is much simpler than the existing training and fine-tuning based approaches. Based on that, the authors further improve the discretization of the entropy parameters and extend the deterministic inference to fit Gaussian mixture models. With these proposed methods, the current state-ofthe-art image compression models can infer in a cross-platform consistent manner, which makes the further development and practice of learned images compression more promising."
SP:85d0df515e9e555f3ea1c21d607304dfaeae69c0,"This paper proposes an unsupervised noise reconstruction and removal network for denoising 3D electron microscopy images. The proposed method is inspired by gated recurrent units, which reconstructs and removes the noise by synthesizing the sequential data. The network uses a triplet of images as input and is trained to map one noise realization to the other, using a modified Noise2 loss function. The paper provides detailed performance analysis using numerical as well as empirical metrics."
SP:e6275b0b103fa90dcebcdd3d3c14c830c3402972,"This paper provides a theoretical analysis of the label trick for node property prediction. The authors show that the stochastic label trick can be reduced to an interpretable, deterministic training objective composed of two factors. The first is a data-fitting term that resolves potential label leakage issues, while the second is a regularization factor conditioned on graph structure that adapts to graph size and connectivity. The paper also provides a broader range of label trick use cases, and provides experiments to verify the efficacy of these extensions."
SP:b6cbc3661f9c440687c3dd01ee35a118c87db377,"This paper proposes a new multi-agent reinforcement learning task called SymmToM, where all agents can speak, listen, see, and move freely through a grid world. To solve this task, the authors propose to model machine theory of mind in a more flexible and symmetric scenario; a multiagent environment where agents are able to speak, hear, and see each other agents. The authors show that the proposed task can be solved by either using well-known deep reinforcement learning (RL) models, nor by tailoring models to the simple rules of the environment. They also show that their best agents fail to achieve performance comparable to agents with access to the gold-standard mental state of other agents, demonstrating that the modeling of theory-of-mind in multiagent scenarios is still an open challenge."
SP:f8ce83805eee46c6c196e8477bf10d8d7f7e0f46,"This paper proposes a zero-shot object detection method for industrial robots. The proposed method is based on the idea of zeroshot learning (ZSL), which is a subset of unsupervised learning, and it aims to detect novel objects in the image with the knowledge learned from and only from seen objects. In this paper, the authors propose to use the YCB Video Dataset (YCB) as the dataset for the zero shot object detection, which contains 21 objects in various categories. "
SP:aa1dcd9217270010f16a00004facede942efea17,"This paper proposes an autoregressive latent video prediction method that is capable of predicting high-fidelity future frames with minimal modification to existing models, and produce high-resolution (256x256) videos. Specifically, it scales up prior models by employing a high-Fidelity image generator (VQ-GAN) with a causal transformer model, and introduces additional techniques of top-k sampling and data augmentation to further improve video prediction quality. The proposed method achieves competitive performance to state-of-the-art approaches on standard video prediction benchmarks with fewer parameters, and enables high resolution video prediction on complex and large-scale datasets."
SP:7f57896afd63bc869d2db6ddf7abbeaa71daae11,"This paper studies the problem of training generative adversarial networks with vision transformers (ViTs). The authors propose several modifications to the ViT-based GANs to stabilize the training dynamics and facilitate the convergence. The experiments are conducted on three public image synthesis benchmarks: CIFAR-10, CelebA, and LSUN bedroom. "
SP:bbae3afcaea0a2e54904cb8daaed7df4fe37da6e,"This paper studies the problem of image generative modeling with variational autoencoders (VAEs) and proposes a two-step approach: 1) prioritize the modeling of visually perceptible information to achieve good sample quality, and 2) model the majority of the likelihood signal to achieve a good likelihood. The authors propose to use a secondary high-rate model on top of the low-rate VAE to improve the ELBO. Experiments on MNIST and CIFAR-10 show the effectiveness of the proposed approach."
SP:bfed56018134ec66cde9a7e958df964d4cca3164,"This paper proposes a training-free inference framework for diffusion probabilistic models (DPMs) that estimates the analytic forms of the variance and KL divergence of a DPM using the Monte Carlo method and a pretrained score-based model. The authors also derive both lower and upper bounds of the optimal variance and clip the estimate for a better result. Empirically, the analytic-DPM improves the log-likelihood of various DPMs, produces high-quality samples, and meanwhile enjoys a 20x to 80x speed up."
SP:3f935ba5784c3e86db72421426bc479061af1a4b,"This paper investigates whether it is feasible to switch to transformer-based models for medical image classification as well, or if we should keep working with CNNs – can we trivially replace CNNs with transformers? The authors consider this question in a series of experiments on several standard medical image benchmark datasets and tasks. The findings show that, while CNNs perform better if trained from scratch, off-the-shelf vision transformers can perform on par with convolutional neural networks when pretrained on ImageNet, both in a supervised and self-supervised setting."
SP:a64e0535f268901e38fd51e027c612ebcdbae1a4,"This paper analyzes the expressivity of pretraining neural language models (NLMs) in terms of their ability to model dependencies between sentences that appear in the same training example. The authors show that pretraining NLMs can model stronger dependencies between text segments that appeared in same training examples than they can between sentences appearing in different training examples. This result has two main implications: 1) it formalizes the motivation behind a broad line of recent successful NLM training heuristics, proposed for the pretraining and fine-tuning stages, and 2) it indicates further improvements to be made in NLM pretraining for the benefit of natural language understanding tasks. "
SP:59066956fa2e423d5f2d2ea4f91c4ddf6afd4683,This paper proposes a new symbolic representation and interpretation framework for learning to optimize (L2O) methods. The authors propose to convert a numerical L2O predictor into a symbolic form that preserves the same problem-specific symbolic form and can be made tunable and subject to further testing by a lightweight re-parameterization. They further propose a lightweight model that can be meta-trained on large-scale problems and outperform human-designed and tuned optimizers.
SP:54dfeb363beee9959aecc9e0853ff06e43bd94e4,"This paper studies the problem of adversarial robustness in reinforcement learning (RL). The authors propose a randomized smoothing-based approach to defend against an adaptive RL adversary, which can directly certify the total reward without requiring the policy to be robust at each time-step. The main contribution is to prove an adaptive version of the Neyman-Pearson Lemma, where the adversarial perturbation at a particular time can be a stochastic function of current and previous observations and states as well as previous actions. Based on this result, the authors propose policy smoothing where the agent adds a Gaussian noise to its observation at each step before passing it through the policy function. The authors show that the certificates are tight by constructing a worst-case scenario that achieves the bounds derived in the analysis. "
SP:e0f9add5fde18eaab0eeb2b10b14928acc8ec5b8,"This paper studies the problem of estimating the target-domain accuracy of a given classifier under distribution shift. The authors propose a method called Average Thresholded Confidence (ATC) that learns a threshold on the model’s confidence, predicting accuracy as the fraction of unlabeled examples for which model confidence exceeds that threshold. ATC outperforms previous methods across several model architectures, types of distribution shifts (e.g., due to synthetic corruptions, dataset reproduction, or novel subpopulations), and datasets (WILDS, ImageNet, BREEDS, CIFAR, and MNIST). In the experiments, ATC estimates target performance 2-4ˆ more accurately than prior methods. "
SP:e748bf6ee653087cae825df32a8546f9ccebfcf1,"This paper proposes a method for the partial distribution matching (PDM) problem, where point sets are regarded as discrete distributions and the goal is to partially match one set to the other. To handle the large-scale PDM problem, the authors propose a method by utilizing the partial Wasserstein-1 (PW) discrepancy, which they show can be efficiently optimized. Specifically, they theoretically derive the Kantorovich–Rubinstein duality for the PW discrepancy, and show its gradient can be explicitly computed. Based on these theoretical results, they propose a PWAN, which approximates the potential by a neural network, and learns the transformation adversarially with the network. It also incorporates an efficient coherence regularizer for non-rigid transformations to avoid unrealistic deformations."
SP:f94f77696d100b2638fa2a6d82c8df47db3b6a36,"This paper proposes a transfer learning method for hyperparameter optimization (HPO) based on Deep Kernel Gaussian Process (DKLM) surrogate with Landmark Meta-features that can be jointly meta-trained on a set of source tasks and then transferred efficiently on a new (unseen) target task. The authors design DKLM to capture the similarity between hyper-parameter configurations with an end-to-end meta-feature network that embeds the set of evaluated configurations and their respective performance. As a result, DKLM can learn contextualized dataset-specific similarity representations for hyper parameter configurations. The proposed method is evaluated on a large-scale benchmark that involves 16 search spaces and 101 datasets from OpenML for a total of 3.4 million hyper-parmeter evaluations."
SP:e3c57f3589e8ab674644d900c14b3473cd71a23f,"This paper proposes a fingerprinting mechanism to enable responsible release and regulation of generative models. In particular, given a GAN backbone with a generator and a discriminator, the fingerprint auto-encoder is used to modulate the fingerprint embedding from each convolutional filter of the generator and try to decode the fingerprint from the generated images. As a result, the generated samples contain fingerprints that can be accurately detected and attributed to their sources. Experiments show that the proposed method achieves effectiveness in deep fake detection and attribution."
SP:73bffd1a0856b80d29f7a2b2b68be57882531f07,"This paper proposes two methods to provide model-agnostic local explanations for similarity learners applicable to tabular and text data. The first method provides feature attributions to explain the similarity between a pair of inputs as determined by a black-box similarity learner. The second method proposes analogies as a new form of explanation in machine learning. The goal is to identify diverse analogous pairs of examples that share the same level of similarity as the input pair and provide insight into (latent) factors underlying the model’s prediction. The selection of analogies can leverage feature attribution, thus connecting the two forms of explanation."
SP:6a3c4ae05d582f8896840483b08c735ced2976bc,"This paper studies the certified robustness of ensembles of deep neural networks (DNNs) against adversarial attacks. The authors show that standard ensemble models only achieve marginal improvement over a single model in terms of certified accuracy, and then analyze the necessary and sufficient conditions for the certifiably robustness. They show that diversified gradients and large confidence margins are sufficient and necessary conditions for certifiable robustness, and provide the bounded model-smoothness analysis based on the proposed Ensemble-before-Smoothing strategy. They also prove that an ensemble model can always achieve higher certified accuracy than a single base model under mild conditions. Based on the theoretical findings, they propose the lightweight Diversity Regularized Training (DRT) to train certified robust ensemble ML models. Extensive experiments show that the proposed DRT enhanced ensemble models can consistently achieve high certified accuracy on MNIST, CIFAR-10 and ImageNet."
SP:3002b29c27709780238876d8c3f81bbd6a0f8112,"This paper studies the expressive power of higher-order message passing graph neural networks (GNNs). In particular, the authors analyze a new recursive pooling technique of local neighborhoods that allows different tradeoffs of computational cost and expressive power. First, they prove that this model can count subgraphs of size k, and thereby overcomes a known limitation of low-order GNNs. Second, they show how the sparsity can exploit sparsity to reduce the computational complexity compared to the existing higher order GNN. More generally, they provide a (near) matching information-theoretic lower bound for counting subgraph with graph representations that pool over representations of derived (sub-)graphs."
SP:5d0cbd84336caf5f31e1f98e11f6733230e4d792,"This paper proposes a probe model for understanding the integration of external knowledge into language models (LMs). The proposed probe model is based on a graph convolutional operation (GCS) and is able to interpret the integration process of knowledge-enhanced LMs and expose what kind of knowledge is integrated into these models. The authors conduct experiments to verify that our GCS model can indeed be used to correctly interpret the KI process, and use it to analyze two typical knowledge-driven LMs: K-Adapter and ERNIE. They find that only a small amount of factual knowledge is captured in these models during integration. They further find that while K- adapter struggles to integrate time-related knowledge, it successfully integrates knowledge of unpopular entities."
SP:7e73948421e98307fceb69a316d8a4e7c4926cda,"This paper studies the effect of the adaptation learning rate in meta-learning with mixed linear regression. First, the authors present a principled way to estimate optimal adaptation learning rates that minimize the population risk of MAML. Second, they interpret the underlying dependence between the optimal learning rate and the input data. Finally, they prove that compared with empirical risk minimization (ERM), MAMl produces an initialization with a smaller average distance to the task optima, consistent with previous practical findings."
SP:effbc85d89b1197d9c2abcaf5ff13864135dd6e1,"This paper proposes a method for source-free domain adaptation (SFDA) which aims to adapt a model trained on labelled data in a source domain to unlabelled data in the target domain without access to the source-domain data during adaptation. The authors propose a method called Feature Restoration (FR) to address the issue of measurement shift (measurement shift) which is characterized by a change in measurement system and is pervasive in real-world machine learning systems. The proposed method is based on the idea that the source features can be restored by restoring the original source features to the target feature space. In addition, a bottom-up training scheme (BUFR) is proposed to improve the performance of FR by preserving learnt structure in the later layers of a network. The experimental results show that the proposed method outperforms existing SFDA methods in terms of accuracy, calibration, and data efficiency."
SP:7d63034ec7e6a4f178681ff2a49feb485cd47116,"This paper studies the problem of federated adversarial robustness propagation (FRP), i.e., the propagation of adversarial training (AT) from high-resource users that can afford AT, to those low resource users that cannot afford it, during the FL process. The authors propose a simple yet effective propagation approach that transfers robustness through carefully designed batch-normalization statistics. The proposed method is shown to grant FL remarkable robustness even when only a small portion of users afford AT during learning."
SP:42c7a79e58b6a9f776fa6ae928bd89c194f9303f,This paper studies the problem of inferring the structure of the interaction network from observed equilibrium actions of a network game. The authors propose a novel transformer-like architecture which correctly accounts for the symmetries of the problem and learns a mapping from the equilibrium actions to the network structure without explicit knowledge of the utility function. They test their method on three different types of network games using both synthetic and real-world data and demonstrate its effectiveness in network structure inference and superior performance over existing methods.
SP:1c7b9157cf8c06ca771da78895fc3af969b0fb85,"This paper proposes GraphANGEL, a relation prediction framework that predicts relations between each node pair by checking whether the subgraphs containing the pair are similar to other subgraph containing the considered relation. Each graph pattern explicitly represents a specific logical rule, which contributes to an inductive bias that facilitates generalization to unseen relation types and leads to more explainable predictive models. The model consistently outperforms existing models in terms of heterogeneous graph based recommendation as well as knowledge graph completion."
SP:26ed25a7b42da2cf11b76a727102d8aa36d76657,"This paper proposes a contrastive learning (CL) with latent augmentation (LA) method for few-shot learning in histology images. The idea is simple and straightforward: CL learns useful representations without manual labels, while LA transfers semantic variations of the base dataset in an unsupervised way. Experiments show that models learned by CL generalize better than supervised learning for histology image in unseen classes, and LA brings consistent gains over baselines."
SP:badbe687258cd5c282ca167b1f6fbfc6b5400dbf,"This paper proposes a continuous-time recurrent neural network (RNN) model for learning long-term dependencies in irregularly sampled time-series. The authors first show that ODE-RNNs suffer from the vanishing and exploding gradient problem, which is similar to standard RNNs, and the underlying reason for this issue is the vanishing or exploding of the gradient during training. Then, the authors propose a solution by equipping arbitrary continuous time networks with a memory compartment separated from its time-continuous state. This way, the model is able to respond to inputs arriving at arbitrary time-lags while ensuring a constant error propagation through the memory path. The proposed model is called mixed-memory RNN (mmRNN)."
SP:4efd22f9122fa5856a9f4302eb6875fa0c414912,"This paper proposes a method to perform full binarization of BERT, i.e., 1-bit weight, embedding, and activation. The authors claim that the performance drop can be mainly attributed to the information degradation and optimization direction mismatch respectively in the forward and backward propagation, and propose a method called BiBERT to eliminate the performance bottlenecks. The proposed method introduces an efficient Bi-Attention structure for maximizing representation information statistically and a DirectionMatching Distillation (DMD) scheme to optimize the binarized BERT accurately. Extensive experiments show the effectiveness of the proposed method."
SP:619bd742e92bea6241852f5a9d2b7bacf13b393a,This paper proposes a transformer-based approach for multi-person pose detection and instance segmentation. The key idea is to use the self-attention module of the Transformer to predict the location of keypoints and assign them to instances based on the pairwise attention scores. The proposed method is evaluated on COCO pose detection challenge and the person-instance segmentation task. 
SP:14750819593136fc9ef4efd032ab6f94dc5f6a02,"This paper proposes a new method for minimizing the mean-variance (MV) trade-off in the context of reinforcement learning. The proposed method, called direct expected quadratic utility maximization (EQUMRL), trains an agent to maximize the expected utility function, in which the maximizer corresponds to the Pareto efficient policy. The main advantages of the proposed method are (i) it does not suffer from the double sampling issue by avoiding the explicit variance estimation; (ii) it is able to learn MV-efficient policies and has plenty of interpretations from various perspectives; and (iii) it can avoid the double-sampling issue. "
SP:f675b564b3a9c8626ce7944d752fa3e0d868428e,"This paper studies the problem of test-time domain adaptation for an autoencoder system whose channel is generatively-modeled using a mixture density network (MDN). Different from the setting of conventional training-time (unsupervised or semi-supervised) domain adaptation, here we have a fully-trained channel model and a source domain, that we would like to adapt to a target domain using only a small labeled dataset (and no unlabeled data). Moreover, the distribution of the channel is expected to change frequently (e.g., a wireless link), making it challenging to collect sufficient data for frequent retraining. To address this, the authors propose a fast and sample-efficient method for adapting the auto-encoder without modifying the encoder and decoder neural networks, and adapting only the MDN channel model. The method utilizes feature transformations at the decoder to compensate for changes in the channel distribution, and effectively present to the decoders samples close to the source distribution. Experimental evaluation on simulated datasets and real mmWave wireless channels demonstrate that the proposed method can adapt the MDNs channel using very limited number of samples and improve or maintain the error rate of the autoencoders under changing channel conditions."
SP:77dc92137ea490d3e1b4b8ee1630dbe2ee0bddfa,"This paper proposes a new approach to the abductive natural language inference task (alphaNLI), which aims to infer the most plausible explanation between the cause and the event. The authors argue that it is unnecessary to distinguish the reasoning abilities among correct hypotheses; and similarly, all wrong hypotheses contribute the same when explaining the reasons of the observations. Therefore, they propose to group instead of ranking the hypotheses and design a structural loss called “joint softmax focal loss” in this paper. The experimental results show that our IMSL has achieved the highest performance on the RoBERTa-large pretrained model, with ACC and AUC results increased by about 1% and 5% respectively."
SP:17cd72df5fc19398f582d27516fd742b073f79e3,"This paper proposes a method for adversarially robust out-of-distribution (OOD) detection. The proposed method combines a certifiable OOD detector with a standard classifier from first principles into an OOD aware classifier to achieve the best of two worlds: certifiably robust OOD detection, even for OOD samples close to the in distribution, without loss in either prediction accuracy or detection performance for non-manipulated OOD data. Moreover, due to the particular construction our classifier provably avoids the asymptotic overconfidence problem of standard neural networks."
SP:9c3756f13932236aff3e8104f4fa193dcc8fde2f,"This paper studies transfer attacks in the query-free black-box setting, where the attacker does not know the dataset used by the victim model, and further, the attacker needs to attack any randomly encountered images that may not come from the same dataset. The authors define a new Generalized Transferable Attack (GTA) problem where they assume the attacker has a set of surrogate models trained on different datasets (with different label sets and image sizes), and none of them is equal to the dataset of the target dataset. They then propose a novel method called Image Classification Eraser (ICE) to erase classification information for any encountered images from arbitrary dataset. Extensive experiments on Cifar-10, CIFar-100, and TieredImageNet demonstrate the effectiveness of the proposed ICE on the GTA problem. Furthermore, they show that existing transfer attack methods can be modified to tackle the problem, but with significantly worse performance compared with ICE."
SP:2e0447c741a3f09be1095633d870200355211260,"This paper proposes a method to improve the robustness and efficiency of masked language modeling (MLM) pretrained language models (PrLMs) by addressing the issue of false-negative in the discriminative PrLMs. In particular, the paper claims that the existing MLM pretraining methods simply treat all corrupted texts as equal negative without any examination, which leads to the resulting model inevitably suffer from the false negative issue where training is carried out on wrong data and leads to less efficiency and less robustness in the resulting pretraining. To address this issue, this paper proposes enhanced pre-training methods to counteract false negative predictions and encourage pretraining language models on true negatives, by correcting the harmful gradient updates subject to false negative prediction. Experimental results on GLUE and SQuAD benchmarks show that the proposed methods indeed bring about better performance together with stronger robustness."
SP:281bc59d639aa76d84921b3ec4ce1ee8f1ba5b51,"This paper proposes an open-world semi-supervised learning setting, which formalizes the notion that novel classes may appear in the unlabeled test data. The authors propose ORCA, an end-to-end approach that assigns instances to previously seen classes or forms novel classes by grouping similar instances without assuming any prior knowledge. The key idea in ORCA is to utilize uncertainty adaptive margin to circumvent the bias towards seen classes caused by learning seen classes faster than the novel classes. In this way, ORCA gradually increases the discriminability of the model during the training and reduces the gap between intra-class variance of seen with respect to novel classes during training."
SP:6c572c4c21b01a0cf3fd9ef97fbb348ef4e405ae,"This paper proposes a second-order quasi-Newton optimization method for training large-scale deep neural networks (DNNs). The proposed method, SLIM-QN, uses the BFGS update rule that directly approximates the Hessian inverse using past parameters and gradients, without explicitly constructing the matrix and computing its inverse. To achieve stable convergence, the authors introduce momentum in Hessian updates together with an adaptive damping mechanism. The authors provide rigorous theoretical results on the convergence of the proposed method in a stochastic setting and demonstrate that it has much less compute and memory overhead compared to existing second order methods. "
SP:4bffce00ebb02d2e676eec897647ac14c3344deb,"This paper proposes a method called Locality Sensitive Pruning (LSP) for graph pruning based on Locality-Sensitive Hashing. LSP aims to sparsify a graph so that similar local environments of the original graph result in similar environments in the resulting sparsified graph, which is an essential feature for graph-related tasks. Extensive experiments on synthetic and real-world datasets demonstrate the superiority of LSP."
SP:c5e024f4e2079586298519ca868630efd7579eca,"This paper studies the problem of data augmentation for contrastive self-supervised learning. The authors propose a simple adversarial augmentation method that can modify training data to be hard positives/negatives without distorting the key information about their original identities. In particular, they decompose a sample x to be its variational auto-encoder (VAE) reconstruction plus the residual residual R(x) = x-G(x), where the residual retains most identity-distinctive information due to an information-theoretic interpretation of the VAE objective. Then, they adversarially perturb G(x)-G(\x) in the bottleneck space and add it back to the original residual. They apply this “identity-disentangled adversarial augmentation (IDAA)” to different self-Supervised learning methods and demonstrate its efficiency and generalization performance."
SP:0991bc5f213bd8ab7572e2fed309e1b57a35835b,"This paper studies the problem of detecting distribution shifts in machine learning models. The authors propose a method that can detect harmful shifts while ignoring benign ones, and allow continuous monitoring of model performance without increasing the false alarm rate. The proposed method is applicable in settings where (some) true labels are revealed after the prediction is performed, or when batches of labels become available in a delayed fashion. The method is evaluated on both simulated and real datasets."
SP:1c7b954273e3a9cda333385b15a3e8ed3bf8178a,"This paper proposes a method to infer physical parameters from a single video. The authors use neural implicit representations for appearance modeling with neural ordinary differential equations (ODEs) in order to obtain interpretable physical models directly from visual observations. The proposed method is able to recover the pendulum length from the monocular video (relative error to true length is less than 2.5%). The authors also propose a per-scene model, so that only a single short video clip that depicts the physical phenomenon is necessary."
SP:51efd1451343f4994d857daa5490e299b812bc2d,"This paper considers the context-dependent Reinforcement Learning (RL) setting, which is characterized by: a) an unknown finite number of not directly observable contexts; b) abrupt (discontinuous) context changes occurring during an episode; and c) Markovian context evolution. This challenging setting is often met in applications and the authors tackle it using a Bayesian approach and variational inference. They adapt a sticky Hierarchical Dirichlet Process (HDP) prior for model learning, and derive a context distillation procedure, which identifies and removes spurious contexts in an unsupervised fashion. They then find the representation of the optimal policy enabling efficient policy learning using off-the-shelf RL algorithms. Finally, they demonstrate empirically (using gym environments cart-pole swing-up, drone, intersection) that their approach succeeds where other methods fail."
SP:ea167b126212b2092bc1190d7f8376bf7c54a888,"This paper proposes a framework to pretrain knowledge-based multilingual language models (KMLMs). The authors first generate a large amount of code-switched synthetic sentences and reasoning based multilingual training data using the Wikidata knowledge graphs. Then based on the intra-and inter-sentence structures of the generated data, they design pretraining tasks to facilitate knowledge learning, which allows the language models to not only memorize the factual knowledge but also learn useful logical patterns. The pretrained KMLMs demonstrate significant performance improvements on a wide range of knowledge-intensive cross-lingual NLP tasks, including named entity recognition, factual knowledge retrieval, relation classification, and a new task designed by the authors, namely, logic reasoning."
SP:6c11cf29c90f923346372ba6f11452c36e69ad6d,This paper proposes an approach to train agents to support other agents by maximizing the number of states that the other agent can reach in its future. The authors propose an agent that learns to increase the choices another agent has by preferring to maximize the number states that it can reach. The proposed approach is evaluated on three different multi-agent environments where another agent’s success depends on the altruistic agent's behaviour. The results show that the proposed approach outperforms the baselines.
SP:5dbc54201ba184266c5054f0d2944bd197bc307a,"This paper studies the double descent (DD) phenomenon in finite-width neural networks. The main contribution of this paper is a lower bound of the population risk of a neural network in the regime of under/over-parameterized. The lower bound is derived by using influence functions and the spectrum of the Hessian at the optimum, which is shown to exhibit double descent behavior at the interpolation threshold. The authors further investigate how the loss function affects double descent, and thus uncover interesting properties of neural networks and their Hessian spectra near the threshold."
SP:b485114712055f39a7afb951dbc3db482ff523fd,"This paper studies the asymptotic behavior of the trainability of GNNs with respect to depth and depth depth. In particular, the authors study the infinite-width and infinite-depth versions of the infinitely-wide graph neural network (GNN). Theoretically, they show that the training dynamics of GCNs with infinite depth and infinite width can be characterized by the graph neural tangent kernel (GNTK), which is a kernel that governs the optimization trajectory under gradient descent for wide GCNs and deep GCNs. They also show the exponential decay of trainability as the depth goes to infinity. Finally, they propose Critical DropEdge, a connectivity-aware and graph-adaptive sampling method, inspired by their theoretical insights on trainability. "
SP:25a92b3583afdc6892e59f1e769125d52c8011af,"This paper studies the problem of recovering the second derivative of the cardiac blood volume pulse (BVP) signal from a video. The authors propose to use a multi-derivative learning objective to improve the performance of the first-order dynamics of the PPG signal. In particular, the authors argue that the second-order signal highlights subtle features that can be difficult to discern from those in the lower derivatives. The proposed approach is evaluated on the video of the left ventricle ejection time (LVET) intervals of the heart."
SP:0a88d2fcbdfab3e196bf6b9c75adb1006ab87536,"This paper investigates how symbolic mapping can help agents learn compositional and symmetric language in complex settings like dialog games. The authors propose a novel architecture called symbolic mapping as a basic component of the communication system of agent. They show that symbolic mapping learned in simple referential games can notably promote language learning in difficult tasks. Further, they explore vocabulary expansion, and show that agents can easily learn to use new symbols when the environment becomes more complex."
SP:89575be04cb33b41d7a0a7b62f9496c2838a1317,"This paper proposes a hierarchical modular approach to learn agents that navigate and manipulate objects in a divide-and-conquer manner for the diverse nature of the entailed tasks. Specifically, it first infer a sequence of subgoals to be executed based on language instructions by high-level policy composition controller (PCC). Then discriminatively control the agent’s navigation by a master policy by alternating between navigation policy and various independent interaction policies. Finally, it infer manipulation actions with the corresponding object masks using the appropriate interaction policy. The proposed method achieves state-of-the-art performance on the ALFRED benchmark."
SP:e2c8efe00db7baba2368f4f6a37815809b9e235e,"This paper proposes Nuisance-randomized distillation (NURD), a method to learn representations that perform well regardless of the nuisance-label relationship in a nuisance-varying family of distributions. The authors define the set of representations such that conditioning on any member of the family, the nuisance and the label remain independent, and prove that the representations in this set always perform better than chance, while representations outside of this set may not. NURD finds a representation from this set that is most informative of the label under the nuisance randomized distribution, and proves that this representation achieves the highest performance within the set on every distribution in the nuisance changing family. The proposed method is evaluated on chest X-ray classification and pneumonia classification."
SP:c75998b76f4e0510fc719d25959a10fc07db1c40,"This paper proposes OTTER (Optimal TransporT distillation for Efficient zero-shot recognition), which uses online entropic optimal transport to find a soft image-text match as labels for contrastive learning. The proposed method is based on pretrained image and text encoders and achieves strong performance with only 3M image text pairs. OTTER consistently outperforms existing methods on Google Open Images (19,958 classes) and ImageNet 10K (10032 classes) from Tencent ML-Images."
SP:e83cd70377542b5d187998e2e4a7ac070f453ed6,"This paper presents Pix2Seq, a simple and generic framework for object detection. Unlike existing approaches that explicitly integrate prior knowledge about the task, this paper simply casts object detection as a language modeling task conditioned on the observed pixel inputs. Object descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete tokens, and we train a neural net to perceive the image and generate the desired sequence. The proposed method achieves competitive results on the challenging COCO dataset."
SP:abc9315f61929cc1c54dfef8ff83d7eac56ec2f2,"This paper proposes a symbolic policy distillation method for visual reinforcement learning (RL). The main idea is to distill the symbolic knowledge of the policy network into the symbolic policy, which is composed of geometric and numerical symbols and operators. The symbolic policy can be treated as discrete and abstracted representations, which are found to be more interpretable, robust and transferable. The proposed symbolic distillation approach is experimentally demonstrated to maintain the performance and “denoise” the CNN policy."
SP:04e7e181aeb1244ae1c4837ad416aef93ea3ea32,"This paper proposes a method for unsupervised image-to-image translation, where the goal is to disentangle the coarse-level object arrangements (posture) and the fine-grained level styling (identity) of the generated image from two exemplar sources. The authors propose a Vector-Quantized Spatial Normalization (VQSN) module for the generator for better pose-identity disentanglement. The VQSN module automatically learns to encode the shaping and composition information from the commonly shared objects inside the training-set images. Second, the authors design a joint-training scheme with self-supervision methods for the GANInversion encoder and the generator. Specifically, they let the encoders and generators reconstruct images from two differently augmented variants of the original ones, one defining the pose and the other for identity. Comprehensive experiments conducted on various datasets show better synthesis image quality and disentangling scores of the proposed method."
SP:e51a7f45493064972585109f203a867e9828eb15,"This paper proposes a simple MLP-based model for keyword spotting and speech enhancement tasks. The proposed model is based on the idea of splitting channels into chunks and processing each chunk individually. The chunks are then merged together and further processed to consolidate the output. The model is evaluated on two tasks: keyword spotting (V2-35 and LibriWords) and speech enhancing (VoiceBank). In all experiments, the proposed model outperforms the transformer-based solutions with fewer parameters lower GFLOPS."
SP:d708d3886f4abd4552d8ccb2096df7361c803b13,This paper provides a lower bound on the generalization error of any transfer learning algorithm as a function of the amount of source and target samples. The lower bound depends on a natural notion of distance that can be easily computed on real world data sets. It applies to any arbitrary source/target data distributions and requires minimal assumptions that enables it application to a broad range of problems. The paper also considers a more general setting where there are more than one source domains for knowledge transfer to the target task and develops new bounds on generalisation error in this setting.
SP:f7511ba9ccad03233b34b1bf41bbac7361d20a57,"This paper proposes a probabilistic shape completion method extended to the continuous geometry of large-scale 3D scenes. The local continuous shape is incrementally generated as a sparse voxel embedding, which contains the latent code for each occupied cell. Experiments show that the model successfully generates diverse plausible plausible scenes faithful to the input, especially when the input suffers from a significant amount of missing data."
SP:d22d8f074adbe8fb0f25fb8f8d96201b3159bf6b,"This paper proposes temporal priors as a non-Markovian generalization of behavioral priors for guiding exploration in reinforcement learning. The authors focus on state-independent priors, which exploit the idea of temporal consistency, and show how dynamically sampling actions from a probabilistic mixture of policy and temporal prior can accelerate off-policy reinforcement learning in unseen downstream tasks. They provide empirical evidence that their approach improves upon strong baselines in long-horizon continuous control tasks under sparse reward settings."
SP:25e06c022ae8b3cbbb8db413d7b534a1a5c92391,"This paper proposes a learning-to-learn method for learning rate scheduling in training deep neural networks. The proposed method is based on constructing a directed graph for the underlying neural network of the target problem, which encodes current dynamics with a graph message passing network and trains an agent to control the learning rate accordingly via reinforcement learning. The authors evaluate their framework on benchmarking datasets, Fashion-MNIST and CIFAR10 for image classification, and GLUE for language understanding."
SP:d73cb0471c1770607ad3e4621cfc5f170683dd8e,"This paper proposes a method for unsupervised object-centric learning from a point cloud. The proposed method, SPAIR3D, is a VAE-based generative model that generates spatial mixture distributions on point clouds to discover 3D objects in static scenes. The authors propose a Chamfer Mixture Loss function tailored for learning mixture models over point cloud data with a novel graph neural network that can be used to model and generate a variable number of 3D points. Experimental results demonstrate that the proposed method has strong scalability and is capable of detecting and segmenting an unknown number of objects."
SP:3c57e921c1bf23e482551ceb71702931a7f07439,"This paper investigates the use of large language models (LLMs) to generate high-level instructions (e.g., ""open the fridge"") that can be used to perform low-level actions (i.e., open the fridge) in an embodied environment (VirtualHome). The authors find that large LLMs trained on a single task description and its associated sequence of actions can produce very plausible action plans that reflect the information already stored in the model – no model fine-tuning is involved. Unfortunately, despite their semantic correctness, the produced action plans are often not executable in the environment. The authors propose several tools to improve executability of the model’s outputs. First, they enumerate all admissible action phrases and map the model's output action to the most semantically-similar admissible actions. Second, they use the model to autoregressively generate actions in a plan by conditioning past actions that have been admissible via the above techniques. Third, they provide weak task supervision to the model by prompting the model with a known example similar to the query task. This is somewhat reminiscent of prompt tuning approaches, but does not require access to gradients or the model internals."
SP:e0159d1c9df2e657892a3a0c77549df4698d9a1a,"This paper proposes a new sampling scheme to generate samples from the latent space of a VAE by sampling from the uniform distribution deriving intrinsically from the Riemannian manifold learned by the VAE. The authors show that VAEs naturally unveil naturally a latent space with a structure that can be modeled as a Riemmanian manifold through the learned covariance matrices in the posterior distributions. They propose a natural sampling scheme consisting in sampling from a uniform distribution defined on the learned manifold and given by the metric. They show that this procedure improves significantly the generation process from a vanilla VAE and makes it able to perform as well as more advanced VAE models in terms of Frechet Inception Distance (Heusel et al., 2017) and Precision and Recall (Sajjadi et al. 2019) scores on four benchmark datasets."
SP:b4b8e1727f8617894f10f20365cb68de79f0e650,"This paper proposes Transformer with a Mixture of Gaussian Keys (Transformer-MGK), a novel transformer architecture that replaces redundant heads in transformers with a mixture of keys at each head. These mixtures of keys follow a Gaussian mixture model and allow each attention head to focus on different parts of the input sequence efficiently. Compared to its conventional transformer counterpart, Transformer- MGK accelerates training and inference, has fewer parameters, and requires less FLOPs to compute while achieving comparable or better accuracy across tasks."
SP:82731dcce233e748f63382e09b6224a513fe9689,"This paper proposes a method to combine image and proprioceptive information for navigation in a two-dimensional continuous environment. The proposed method is based on direct-inverse model of environment dynamics to fuse image and action related signals, allowing reconstruction of the action relating the two successive images, as well as prediction of the new image from its current value and the action. The authors propose a minimalistic recurrent architecture, called Resetting Path Integrator (RPI), that can easily and reliably be trained to keep track of its position relative to its starting point during a sequence of movements. RPI updates its internal state using the (possibly noisy) self-motion signal, and occasionally resets it when the image signal is present. The internal state of this minimal model exhibits strong correlation with position in the environment due to the direct-Inverse models, is stable across long trajectories through resetting, and allows for disambiguation of visually confusing positions through integration of past movement."
SP:1a27c397d1e73def5e724c5c6f25548975ba50fa,"This paper studies the role of the structure of the input distribution in the feature learning ability of neural networks. Specifically, the authors consider the setting where the labels are determined by a set of class-relevant patterns and the inputs are generated from these along with some background patterns. The authors prove that neural networks trained by gradient descent can succeed on these problems. The success relies on the emergence and improvement of effective features, which are learned among exponentially many candidates efficiently by exploiting the data (in particular, structure of input distribution). In contrast, no linear models on data-independent features of polynomial sizes can learn to as good errors. Furthermore, if the specific input structure is removed, then no Polynomial algorithm in the Statistical Query model can learn even weakly. These results show that feature learning in neural networks depends strongly on the input structure and leads to the superior performance."
SP:8ada73ed7eade9ebdeef376485e849c42575bc5f,"This paper studies the robustness of deep neural networks to adversarial perturbations. In particular, the authors consider the setting where the feature extractor is fixed and the classifier is trained on top of it. The main contribution of this paper is the derivation of lower bounds on the minimum loss incurred by any classifier trained on the features extracted by the model. The lower bounds are based on the effectiveness of the method used to find collisions between pairs of perturbed examples at deeper layers. For linear feature extractors, the paper provides closed-form expressions for collision finding, while for arbitrary features extractors the paper proposes a bespoke algorithm. The bounds are then used to identify the layers of robustly trained models that contribute the most to a lack of robustness."
SP:874b5fa51924cbcceed490d98a0ea80f74586b32,"This paper proposes Expectile V-Learning (EVL), a method for offline reinforcement learning (RL) that learns the V-function instead of the Q-function to keep the learning procedure within the offline dataset. The authors also introduce implicit planning along offline trajectories to enhance learned V-values and accelerate convergence. The paper provides theoretical analysis for the convergence properties of the proposed method, and empirical results in the D4RL benchmark show that the method achieves superior performance in most tasks."
SP:34f08d92681504490c2f739b0d08f79f9764b2f5,"This paper proposes a method to improve the robustness of adversarial training by reweighting the loss associated with individual training samples based on a notion of class-conditioned margin, with the goal of improving robust generalization. The authors formulate weighted adversarial learning as a bilevel optimization problem, where the upper-level task corresponds to learning a robust classifier, and the lower-level function maps from a sample’s multi-class margin to an importance weight. Extensive experiments demonstrate that the proposed method improves both clean and robust accuracy compared to related techniques and state-of-the-art baselines."
SP:3ad36be6b6900aabe43da043461cf178ce977082,This paper proposes a non-linear message passing layers for equivariant graph neural networks. The message passing layer is composed of steerable MLPs that incorporate geometric and physical information in both the message and update functions. The authors define steerable node attributes and provide a new class of activation functions for general use with steerable feature fields. Experiments on several tasks in computational physics and chemistry demonstrate the effectiveness of the proposed method.
SP:8928aa83f7ebd4e310f4fe1d01ff0eb0c96e4d2b,"This paper proposes a new differentiable physics model for composite materials such as cloths, where the granularity of yarns and model individual yarn physics and yarn-to-yarn interactions. The authors propose several differentiable forces, whose counterparts in empirical physics are indifferentiable, to facilitate gradient-based learning. These forces are applied to cloths and are ubiquitous in various physical systems. Experiments demonstrate the model’s explicability in learning meaningful physical parameters, versatility in incorporating complex physical structures and heterogeneous materials, data-efficiency in learning, and high-fidelity in capturing subtle dynamics."
SP:2c8358c095b10981d3015b9f6c75765419a9480d,"This paper proposes a method for lifelong reinforcement learning (RL) based on logical composition. The authors extend the work of Nangue Tasse et al. (2020) to the discounted and stochastic tasks, and provide bounds on the performance of the transferred policy on a new task and the number of tasks that need to be learned throughout an agent’s lifetime to generalize over a distribution. The proposed method is evaluated in two settings: (1) pretraining on a set of base tasks provided by the Boolean algebra framework, and (2) when the pretrained tasks are not tasks and the agent is able to achieve good performance on new tasks before training even starts."
SP:c85d71d05164d019cc32bf423e4c4fe20c169f41,"This paper proposes a distributed solution for multivariate time series classification (MTSC) based on a wavelet scattering transformation of the time series and distributed feature selection. The proposed method, LightWaveS, employs just 2,5% of the ROCKET features, while achieving accuracy comparable to recent deep learning solutions. It also scales well with more nodes and large numbers of channels."
SP:db43614ca016280a79448f44a97c81c8ff5ba981,"This paper proposes a new approach for pretraining text encoders with an adversarial learning curriculum via a Mixture of Signals from multiple auxiliary masked language models (MLMs). The main encoder is trained as a discriminator to detect replaced tokens generated by auxiliary MLMs. Different from ELECTRA which trains one MLM as the generator, this paper jointly trains multiple MLMs of different sizes to provide training signals at various levels of difficulty. The authors also learn mixture weights over the auxiliary MLM’s outputs to maximize the discriminator loss by backpropagating the gradient from the main discriminator via Gumbel-Softmax. Experiments on the GLUE and SQuAD benchmark demonstrate the effectiveness of the proposed approach."
SP:db3825633ab5d0671340390b23ab655838cc38b2,"This paper proposes a simple adaptive fine-tuning method for extracting relational knowledge from large pre-trained language models by a clozestyle sentence serving as a query. The proposed method, called BERTriple, uses a small training dataset of existing facts from a knowledge graph to perform the prompt tuning on the standard fill-mask task using a few training triples from the knowledge graph. Experiments show that the proposed method outperforms all baselines, even by using significantly fewer training facts."
SP:ae25d32714b2b9f7e02cc20f4a36252e20e78e4f,"This paper proposes to learn knowledge base embeddings in different geometric spaces and apply manifold alignment to align the shared entities. The proposed approach is evaluated on the out-of-taxonomy entity typing task, where the goal is to predict the types of the entities from the knowledge graph. Experimental results on two datasets based on YAGO3 demonstrate that the approach has significantly good performances, especially in low dimensions and on small training rates."
SP:9ab3bc525ee4a9c96518c43e4c43082655a7674f,"This paper proposes a one-shot learning framework for link prediction in temporal knowledge graphs. The proposed method employs a self-attention mechanism to effectively encode temporal interactions between entities, and a network to compute a similarity score between a given query and a (one-shot) example. The experiments show that the proposed algorithm outperforms the state of the art baselines for two well-studied benchmarks while achieving significantly better performance for sparse relations."
SP:91f92a40e12afd0702f07ae7f4175ecce57b7007,"This paper proposes Progressive Module Networks (PMN), a framework for multi-task learning by progressively designing modules on top of existing modules. Each module is a neural network that can query modules for lower-level tasks. The modules communicate by learning to query other modules and process their outputs, while the internal module processes are a blackbox. This is similar to a computer program that uses available available libraries without having to know their internal operations. PMN’s modules are task-level modules, and they are compositional, i.e. modules build on modules which build on module. The proposed method is evaluated on a set of visual reasoning tasks, and demonstrates improved performance in all tasks by learning progressively."
SP:de33b02e7f2faec5bcae9a5516721aa1ef190572,"This paper proposes a method to improve the efficiency of convolutional neural networks (CNNs) by pruning and rewiring channels. The proposed method is based on the idea that identity connections (i.e., residual connections) are useful for preserving information from the previous layer. The authors propose two methods to de-allocate the channels and re-allocation the channels of high importance to other channels. Experiments on CIFAR-10 and ImageNet show that the proposed method can improve the model compression and accuracy."
SP:2d80fa4bc440061be2234b5070503d3fa056baed,"This paper considers the problem of learning a binary classifier only from positive data and unlabeled data (PU learning). The authors propose a scoring function that preserves the order induced by the class posterior under mild assumptions, which can be used as a classifier by setting an appropriate threshold. The method outperforms previous methods for PU learning on various real-world datasets."
SP:5f312626b0613d2e07c59214c5f00db208a98717,"This paper proposes to use cosine similarity between gradients of tasks as an adaptive weight to detect when an auxiliary loss is helpful to the main loss. The authors show that their approach is guaranteed to converge to critical points of the main task and demonstrate the practical usefulness of the proposed algorithm in a few domains: multi-task supervised learning on subsets of ImageNet, reinforcement learning on gridworld, and Atari games."
SP:e270ae3eeb7ab4fa91ba37d4d68ce10f2fa0a3b5,"This paper proposes a geometric framework to analyze the high-dimensional geometry of adversarial examples. In particular, the authors highlight the importance of codimension: for low-dimensional data manifolds embedded in high dimensional space there are many directions off the manifold in which to construct adversarial perturbations. Adversarial examples are a natural consequence of learning a decision boundary that classifies the data manifold well, but classifies many points near the manifold incorrectly. The authors show that a tradeoff between robustness under different norms, (2) that adversarial training in balls around the data is sample inefficient, and (3) sufficient sampling conditions under which nearest neighbor classifiers and ball-based adversarial learning are robust."
SP:e07d948a79d478ecd23a0a4406d4ddd3ac5e3be3,"This paper proposes a method for learning discrete representations of time-series data that are interpretable. The proposed method is built on top of the self-organizing map (SOM) framework, which allows to map states from an uninterpretable continuous space to a lower-dimensional space with a predefined topologically interpretable structure, such as an easily visualizable two-dimensional grid. To overcome the non-differentiability in discrete representation learning, the authors propose a gradient-based algorithm that is more performant than the original. Moreover, to allow for a probabilistic interpretation of the method, they integrate a Markov model in the representation space. This model uncovers the temporal transition structure, improves clustering performance even further and provides additional explanatory insights."
SP:5915ee71ea58dbdbafa31c1ad291d1e5940a0cf4,"This paper studies the properties of multidimensional probability distributions in the context of latent space prior distributions of implicit generative models. Specifically, the authors study the distribution mismatch arising while decoding linear interpolations between two random latent vectors. The distribution mismatch can be eliminated completely by a proper choice of the latent probability distribution or using non-linear interpolations. The authors also prove that there is a trade off between the interpolation being linear, and the latent distribution having even the most basic properties required for stable training, such as finite mean."
SP:19b63ca635712f1509ca6e0141303c192f2709e0,"This paper proposes to use hyperbolic geometry in the embeddings used to compute the attention mechanisms for different neural networks architectures. The main idea is that the number of objects grows exponentially for any semantic distance from the query, so the embedding space can encode those objects without having any interference. The proposed method shows improvements in generalization on neural machine translation, learning on graphs, and visual question answering tasks while keeping the neural representations compact."
SP:f6049e9f80a63c9306c1cebcb6b229aa6da44ddc,"This paper studies the problem of DNN fingerprinting attacks that exploit cache side-channels. The authors define a threat model for these attacks: the adversary does not need the ability to query the victim model; instead, she runs a co-located process on the host machine where the victim’s deep learning (DL) system is running and passively monitors the accesses of the target functions in the shared framework. They introduce DeepRecon, an attack that reconstructs the architecture of the victim network using the internal information extracted via Flush+Reload. They demonstrate that an attacker can accurately reconstruct two complex networks (VGG19 and ResNet50) having observed only one forward propagation. Based on the extracted architecture attributes, the attacker can build a meta-model that accurately fingerprints the architecture and family of the pretrained model in a transfer learning setting. Finally, they propose and evaluate new framework-level defense techniques that obfuscate our attacker's observations."
SP:6a3dd89db6c24a1f98e8866ef0a4c1c2c1ec6635,"This paper proposes a hierarchical network model for video prediction. The model is inspired by the feedforward, feedback and lateral recurrent circuits in the mammalian hierarchical visual system. It assumes that spatiotemporal memories are encoded in the recurrent connections within each level and between different levels. The network learns by comparing the incoming signals with its prediction, updating its internal model of the world by minimizing the prediction errors at each level of the hierarchy in the style of predictive self-supervised learning. It processes data in blocks of video frames rather than a frame-to-frame basis. "
SP:fb74e57f35666742caf651e6da33b5defcf259a8,This paper proposes a method to learn embeddings for RNA-seq data in a reference-free fashion. The authors claim that the model captures information of both DNA sequence similarity as well as DNA sequence abundance in the embedding latent space. They also show that the latent space recovers exon information from raw RNA-Seq data from acute myeloid leukemia patients.
SP:03aca6ff6a7f0ad2d5ccbcb15ed9536e305a9880,"This paper proposes a method for model compression based on the architecture space, i.e. a mapping from discrete architecture space to a continuous space that encodes the structure of architecture for a specific dataset. A 1-D CNN encoder/decoder is trained to learn the mapping from the discrete space to the continuous embedding and back. This embedding is jointly trained to regress accuracy and parameter count in order to incorporate information about the architecture’s effectiveness on the dataset. During the compression phase, we first encode the network and then perform gradient descent in continuous space to optimize a compression objective function that maximizes accuracy and minimizes parameter count. The final continuous feature is then mapped to a discrete architecture using the decoder. The proposed method is evaluated on CIFAR-10/100, FMNIST and SVHN."
SP:0511b5d10a90e3fe814e2d35208b4a987894ea62,"This paper studies the problem of continual learning in the presence of a nominal dynamics model. The authors propose a framework called POLO, which is a combination of local trajectory optimization, global value function learning, and exploration. In particular, the authors show how trajectory optimization can cope with approximation errors in the value function and can stabilize and accelerate value learning. They also show how approximate value functions can help reduce the planning horizon and allow for better policies beyond local solutions. Finally, they demonstrate how trajectories optimization can be used to perform temporally coordinated exploration in conjunction with estimating uncertainty in value function approximation."
SP:771494fda4702cd8c7efbf225b19028f91b449b9,This paper proposes an approach for zero-shot neural machine translation (NMT) that combines the dual learning approach of He et al. (2016) and the reinforcement learning approach proposed by Lample et al (2018) for unsupervised NMT. The proposed approach is evaluated on the UN corpus and the newstest 2014 dataset. The results show that the proposed approach outperforms the Lample-based approach on the en-to-en task and the Transformer-based one on the f-toen task. 
SP:1558dc03f99670f9ddccdca9c223a2baf962d438,"This paper analyzes the IRGAN framework, a generative model for information retrieval. The authors point out some inaccuracies in the formulation of the loss function and propose a co-training scheme to improve the performance of the proposed method. The proposed method is evaluated on three different tasks, where it is shown to outperform the original IRGAN model. "
SP:6a13dda852ab075a3c0fb691476d6dc57919c729,"This paper proposes a VAE-based approach for learning sparse representations. The authors propose a Spike and Slab prior to model sparsity in the latent space of the VAE, and use a discrete mixture recognition function to map observations to sparse latent vectors. The proposed approach is evaluated on two classification tasks (MNIST and Fashion-MNIST) and shows improved classification accuracy and robustness to the number of latent dimensions. "
SP:06a22143186fa2948fbe324ccae96a62ff12064e,"This paper proposes a non-adversarial feature matching-based approach to train generative models. The approach, Generative Feature Matching Networks (GFMN), leverages pretrained neural networks such as autoencoders and ConvNet classifiers to perform feature extraction. The experimental results demonstrate that, due to the expressiveness of the features from pretrained ImageNet classifier, the approach can achieve state-of-the-art results for challenging benchmarks such as CIFAR10 and STL10."
SP:2d7cf2f07a27d6c8e304a1b47c25387ad2e4432d,"This paper provides a theoretical analysis of the expressive power of GNNs to capture different graph structures. In particular, the paper analyzes the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and shows that they cannot learn to distinguish certain simple graphs structures. The paper then develops a simple architecture that is provably the most expressive among the class of Graph Neural Networks (GNNs) and is as powerful as the Weisfeiler-Lehman graph isomorphism test. Empirically, the authors validate the theoretical findings on a number of graph classification benchmarks and demonstrate that their model achieves state-of-the-art performance."
SP:51126f2dd37ce57d2614c9044ede1e43627f0829,This paper proposes a framework for interpretable continual learning (ICL) that uses saliency maps to provide explanations of previously performed tasks and proposes a new metric to assess the quality of the explanations. The proposed method is based on the variational continual learning framework to take advantage of its flexibility and efficacy in overcoming catastrophic forgetting. Experiments show that ICL achieves state-of-the-art results in terms of overall continual learning performance as measured by average classification accuracy.
SP:27a565b3e5442b93d208652784051e640b0c1bfe,"This paper proposes a new evaluation framework for adversarial attacks on seq2seq models taking meaning preservation into account. The authors also propose new constraints for attacks on word-based MT systems and show, via human and automatic evaluation, that they produce more semantically similar adversarial inputs. Finally, the authors show that performing adversarial training with meaning-preserving attacks is beneficial to the model in terms of adversarial robustness without hurting test performance."
SP:54ddd8132bf9e4259d2c2d72b348d2bb5f9e227c,"This paper proposes a method to improve the performance of Q-learning, double-Q-learning and on-policy actor-critic by using inverse (negative) rewards to measure the badness of a state and using it to fix the Q-risk. The authors prove the convergence of the inverse policies and show that the policies using inverse rewards are competitive with original policies and help the original policies correct their mis-actions. The experiments for some games in OpenAI gym show the hybrid polices based on the proposed method obtain the rewards up to 63.8%, 97.8% and 54.7% more than the original algorithms."
SP:89a732b57934d08b937c93560f391b7758e54f8a,"This paper proposes a method for learning object segmentation, hierarchical structure and dynamics from videos. The proposed method is based on the idea that human perception is holistic and generative, explaining scenes as a whole instead of in isolation. The method is evaluated on three tasks: segmenting object parts, building their hierarchical structure, and capturing their motion distributions. Experiments show that the proposed method works well on all three tasks."
SP:bb2a655d67bed9da43f0b8ec7d888b89c217d12e,"This paper proposes a generative classifier that can be applied to any softmax neural classifier pre-trained on noisy datasets. The proposed method is based on the minimization of the minimum covariance determinant (MCD) of the feature space of the discriminative deep model. The authors show that the proposed method can improve the classification accuracy, with no re-training of the deep model nor changing its architectures. In addition, the authors also show that DDGC is robust against adversarial perturbations due to its large margin property."
SP:0fa525cc708470b757a60117cb608bb2feaa2c50,"This paper proposes a method for hierarchical reinforcement learning (HRL) that learns subgoals, i.e., states that are useful to attain, and skills that allow the agent to efficiently reach those states. The proposed method is based on incremental unsupervised learning over a small memory of the most recent experiences of the agent. The agent is encouraged to reach the subgoal states using intrinsic motivation. The method is evaluated on two RL problems: a variant of the rooms environment and the ATARI 2600 game."
SP:e5861538bc8bb9165cb33299bbf12dd875abf976,"This paper proposes a neural framework that can learn to solve the Circuit Satisfiability problem. The framework is built upon two fundamental contributions: a rich embedding architecture that encodes the problem structure, and an end-to-end differentiable training procedure that mimics Reinforcement Learning and trains the model directly toward solving the SAT problem. Experimental results show the superior out-of-sample generalization performance of the framework compared to the NeuroSAT method."
SP:ff3e5d44619df3825632b0b1a943add081364861,"This paper proposes to combine the cross-entropy method (CEM) and Twin Delayed Deep Deterministic Policy Gradient (TD3) to improve the sample efficiency of DDPG, a sample efficient off-policy deep RL algorithm. CEM-RL is a combination of CEM and TD3, where TD3 is a variant of TD2. The authors evaluate the proposed method on Mujoco tasks, and show that it is able to achieve a better trade-off between performance and sample efficiency."
SP:78b2eb326695da0b0cc4ba39a9206d11644a5e32,"This paper proposes an interpretable multi-variable LSTM recurrent neural network (IMV-LSTM) for multi-variate forecasting and knowledge extraction. The proposed model is equipped with hidden state matrix and update process, so as to learn variableswise hidden states. The authors also develop a mixture attention mechanism and associated summarization methods to quantify the temporal and variable importance in data. Extensive experiments using real datasets demonstrate the prediction performance and interpretability of the proposed model."
SP:1c26660569b579f060f7b4a31e321c6d2356b928,"This paper proposes an efficient data augmentation method for improving the adversarial robustness of deep neural networks. The proposed method, called feature smoothing, trains a neural network on virtual training data as an interpolation of features from a pair of samples, with the new label remaining the same as the dominant data point. The paper also proposes a unified framework to understand the connections and differences among different efficient methods by analyzing the biases and variances of decision boundary. The authors show that under some symmetrical assumptions, label smoothing (label smoothing), logit squeezing, weight decay, mix up, logit compression, and weight decay all produce an unbiased estimation of the decision boundary with smaller estimated variance."
SP:88d652f9e411dd3a2e9ad651d9011e579653c6aa,"This paper proposes a theoretical framework for deep and locally connected ReLU networks. The framework is built upon teacher-student setting, by projecting the student's forward/backward pass onto the teacher's computational graph. This framework could help facilitate theoretical analysis of many practical issues, e.g. disentangled representations in deep networks. "
SP:7842bbe0e2324cfd732db8745550733ccc3dfcdc,"This paper proposes a modular architecture of neural networks with a Behavioral Module (BM) and corresponding end-to-end training strategy. This approach allows efficient learning of behaviors and preferences representation. This property is particularly useful for user modeling (as for dialog agents) and recommendation tasks, as allows learning personalized representations of different user states. The experiments also show network extendability through independent learning of new behavior patterns. Moreover, it allows efficient transfer of newly learned BMs to unseen tasks."
SP:300c391ff644b6889cd9ae27cf0d162dfcdd4451,"This paper proposes a differentiable formulation for the neuromodulation of plasticity, which can be used to train neural networks with gradient descent. It extends previous work on differentiable Hebbian plasticity (Miconi et al., 2017; Miconi, 2017; Ellefsen et al. 2015; Velez & Clune, 2017) to implement backprop reference in its ability to emulate the effects of neurommodulation in natural neural networks. The paper shows that the proposed framework can improve the performance of neural networks on both reinforcement learning and supervised learning tasks."
SP:1ab5d94d31e99351433436c026799c8aa597bf73,"This paper proposes a method for training neural networks with binary quantization. The proposed method consists of two phases: phase1 trains the full precision model with quantization and phase2 trains the binary model constructed by phase1. The authors also propose a new loss function to regularize the weights, resulting in reduced quantization error. Experiments on CIFAR and ImageNet show the effectiveness of the proposed method."
SP:0876b1d9a6d664808ca1ab15865679fbf638267e,"This paper proposes a method for open-ended style transfer, i.e., the method can generalize to style and content not present in the training data. The method starts by constructing a content embedding using an existing deep metric-learning technique. This trained content encoder is incorporated into a variational autoencoder (VAE), paired with a to-be-trained style encoder. The VAE reconstruction loss alone is inadequate to ensure a decomposition of the latent representation into style andcontent. The authors propose an auxiliary loss, leakage filtering, which ensures that no style information remaining in the content representation is used for reconstruction and vice versa. The synthesized images are synthesized by decoding the style representation obtained from one image with the content representations from another. The proposed method achieves state-of-the-art performance on few-shot learning tasks."
SP:d37e15cde7765fca87595a242f0a4511b3346d46,"This paper proposes a method to speed up deep reinforcement learning (deep RL) training for problems that have the property of state-action permissibility (SAP). The proposed method is based on the idea that after an action at is performed in a state and the agent reaches the new state, the agent can decide whether the action is permissible or not permissible in that new state. The authors propose two types of permissability: (1) if the action can never lead to an optimal solution and thus should not be tried, and (2) even without performing the action at in state st, an agent can already decide whether at is permitted or not permitted in st. The method is applied to two state-of-the-art deep RL algorithms to guide their state-actions exploration."
SP:20015d8b60e13300586b67c281858cbe28825c48,"This paper studies the behavior of random deep weight-tied autoencoders under the assumption of random weights. The authors provide a precise characterization in the limit of large dimensions, which reveals interesting phase transition phenomena when the depth becomes large. They also provide insights on pitfalls in training initialization practice, and demonstrate experimentally that it is possible to train a deep autoencoder, even with the tanh activation and a depth as large as 200 layers."
SP:91764f80dbe2401ade38b35a8253ba05f0f86386,"This paper proposes a new black-box adversarial attack method that is based on a simple intuition: if the distance to a decision boundary is very small, we don’t have to be too careful about the exact direction along which we traverse, we can perturb the image by adding or subtracting a low-frequency component of the discrete cosine transform (DCT). The proposed method can be used for both targeted and untargeted attacks, resulting in previously unprecedented query efficiency in both settings. The proposed algorithm can be implemented in less than 20 lines of PyTorch code, and it is extremely fast and easy to implement."
SP:fc20ae0fbf57a1ce489c04b85c7c2f4c93dc2450,"This paper proposes an option discovery method for hierarchical reinforcement learning (HRL) based on successor representations (SRs). The key idea is to discover landmark sub-goals which are prototypical states of well-connected regions. These are points from which a densely connected set of states are easily accessible. The authors propose a new model called Successor options that leverages Successor representations to achieve the same. They also design a novel pseudo-reward for learning the intra-option policies. Finally, they describe an incremental option discovery algorithm that iteratively builds options and explores in environments where exploration through primitive actions is inadequate to form the successor representations."
SP:12a172c1e2892d016b37932acfc48dcb56874a89,"This paper studies the problem of domain division, which aims to segment instances drawn from different probabilistic distributions. The authors propose a domain division algorithm to split the testing instances into known, unknown and uncertain domains, and then conduct recognition tasks in each domain. Two statistical tools, namely, bootstrapping and KolmogorovSmirnov (K-S) Test, for the first time, are introduced to uncover and fine-tune the decision boundary of each domain, and the uncertain domain is newly introduced in the framework to adopt those instances whose domain labels cannot be predicted confidently. Extensive experiments demonstrate that the approach achieved the state-of-the-art performance on OSL and G-ZSL benchmarks."
SP:28bcf7c6a4673e9ec2b4ebed09839d85188e0b2a,"This paper proposes polar prototype networks, a class of networks that explicitly states the structure, i.e., the layout, of the output. The structure is defined by polar prototypes, points on the hypersphere, which are described by a single polar prototype for classification and equal shares for classification. The paper also proposes a method for regression based on polar prototypes for higher-dimensional outputs. Experiments are conducted on CIFAR-10 and ImageNet."
SP:d1034342785d133cf8372b8624897963cc2ee83a,This paper proposes a reward learning algorithm for deep reinforcement learning that uses the initial state of the world as a source of information about human preferences. The authors claim that this initial state can be used to infer both side effects that should be avoided as well as preferences for how the environment should be organized. They propose an algorithm based on Maximum Causal Entropy IRL and evaluate the idea in a suite of proof-of-concept environments designed to show its properties.
SP:417a4e0acee699b3e004ad30d0ecf533a9ed987e,"This paper proposes a method for learning the dependency structure between latent variables in deep latent variable models. The proposed method combines the complementary strengths of deep generative models and probabilistic graphical models. In particular, the latent variable space of a variational autoencoder (VAE) is expressed in terms of a Bayesian network with a learned, flexible dependency structure. The network parameters, variational parameters and the latent topology are optimized simultaneously with a single objective. Inference is formulated via a sampling procedure that produces expectations over latent variable structures and incorporates top-down and bottom-up reasoning over latent variables values. Experiments on MNIST, Omniglot, and CIFAR-10 show that the learned dependency structures more accurately model the distribution, outperforming several common common latent dependency structures."
SP:976dedab53e69610692a563382ada1dbb82c1e9d,"This paper proposes to use dynamical neural networks (DNNs) to solve dictionary learning problems such as l1-minimizing sparse coding and dictionary learning. The main idea is to use spiking neurons to construct the dynamical network. The authors show that the true gradients for learning are provably computable by individual neurons using only local information. They also provide a learning process, its rigorous mathematical analysis, and numerical results on several dictionary learning tasks."
SP:f45117a6beaeb86a70b1380b4fac3cfba37fb892,This paper proposes an end-to-end convolutional neural network for the task of lane detection. The main idea is to use multiple encoder-decoders modules in an end to end fashion. The proposed method is evaluated on the road segmentation and lane detection tasks. The results show that the proposed method achieves better results than the baselines.
SP:68b0a10ca06df74612d0753cc3f3ddddde806035,"This paper proposes a new approach for batch contextual bandit learning based on Maximum Likelihood Inverse Propensity Scoring (MLIPS), which estimates a maximum likelihood surrogate policy based on the logged action-context pairs, and then uses this surrogate policy as the proposal in inverse propensity weights. Theoretical analysis shows that MLIPS is asymptotically unbiased, and moreover, has a smaller nonasymptotic mean squared error than the IPS estimator. Empirical results on multi-label classification problems and a large-scale ad placement dataset demonstrate the empirical effectiveness of MLIPS."
SP:8e0ed65c5dded23b34798499b2436b24422fd729,"This paper proposes a meta-learning method for few-shot image classification. The proposed method learns how to create an individualized feature embedding specific to a given query image for better classifying. Specifically, the authors introduce a kernel generator as meta-learner to learn to construct feature embeddings for query images. The kernel generator acquires meta-knowledge of generating adequate convolutional kernels for different query images during training, which can generalize to unseen categories without fine-tuning. The experiments on Omniglot and miniImageNet show the proposed method achieves highly competitive performance."
SP:faa3f7ffdcfb6e3b8ec0421193dae3d9987b015c,"This paper proposes a gradient-free, population-based genetic algorithm (GA) for deep reinforcement learning (RL). The authors show that the proposed method, called Deep GA, outperforms ES, DQN, A3C, and other gradient-based RL methods on Atari and humanoid locomotion tasks. The authors also show that Deep GA is faster than ES, and can be used to combine DNNs with novelty search, which encourages exploration on tasks with deceptive rewards."
SP:dfdbe3267a8160f24746884cdf5297993e424231,"This paper proposes a new curiosity method which uses episodic memory to form the novelty bonus. To determine the bonus, the current observation is compared with the observations in memory. The comparison is done based on how many environment steps it takes to reach the current observations from those in memory — which incorporates rich information about environment dynamics. This allows the proposed method to overcome the known “couch-potato” issues of prior work — when the agent finds a way to instantly gratify itself by exploiting actions which lead to hardly predictable consequences. The proposed method is evaluated on VizDoom, DMLab and MuJoCo."
SP:1e58a1c5344d1b5b7c8a40210a243700bd933d65,"This paper presents a method for modeling transition models in uncertain domains using relational rules. For any action, a rule selects a set of relevant objects and computes a distribution over properties of just those objects in the resulting state given their properties in the previous state. Feed-forward neural networks are used to learn the transition distribution on the relevant objects’ properties. This strategy is demonstrated to be both more versatile and more sample efficient than learning a monolithic transition model in a simulated domain."
SP:8ce00a3fedbf54a7f2c1ff414511cbb7d59b4597,"This paper proposes a novel feature selection method, called INVASE, which attempts to learn which subset of the features is most relevant for each sample. The proposed method consists of 3 neural networks, a selector network, a predictor network, and a baseline network, which are used to train the selector network using the actor-critic methodology. The selector network is trained to minimize a KL divergence between the full conditional distribution and the selected-features-only conditional distribution of the outcome. Experiments on both synthetic and real-word data show that INVASE significantly outperforms state-of-the-art benchmarks."
SP:b91d6c33349df0bb6cb7e1c5e9433f0d4744b4da,"This paper proposes a domain adaptation method for pixel-level semantic segmentation. The authors propose to learn discriminative feature representations of patches based on label histograms in the source domain, through the construction of a disentangled space. They then use an adversarial learning scheme to push the feature representations in target patches to the closer distributions in source ones. Extensive ablation studies and experiments are conducted on various benchmark datasets with various settings."
SP:00922af13a21464cbc4cd7b34c196dd4f86c9247,"This paper proposes two algorithms for training deep neural networks. The first algorithm, called New-OPTIMISTIC-AMSGrad, is based on the observation that mini-batch of stochastic gradients in consecutive iterations do not change drastically and consequently may be predictable. The second algorithm is called ADAM-DISZANTS, which combines the idea of momentum method, adaptive gradient method, and Optimistic Online Learning. Theoretical analysis is provided for both algorithms. Experiments on CIFAR-10 and ImageNet show that both algorithms are faster than the baselines."
SP:52228b48f2776d57dd422edb33b82e247f056b75,"This paper proposes two benchmark datasets for robustness to corruption and perturbation. The first dataset, called ImageNet-C, is for corruption robustness, and the second one, called IMAGENET-P, is designed to evaluate the robustness of a classifier to perturbations. The authors also propose three methods and architectures to improve corruption and adversarial robustness. "
SP:20358ea0f769e6ea9222d8e35159d711ee1b20b2,"This paper shows that dropout training can be understood as performing MAP estimation concurrently for an entire family of conditional models whose objectives are lower bounded by the dropout objective. The family includes models that compute a power mean over the sampled dropout masks, and their less stochastic subvariants with tighter and higher lower bounds than the fully-stochastic dropout. The deterministic subvariant’s bound is equal to its objective, and the highest among these models. It also exhibits the best model fit in the experiments."
SP:ac1b950ad29429ae045bb5e53279014a6a0b9d2b,"This paper proposes a global soft filter pruning (GSFP) scheme to prune redundant filters of Convolutional Neural Networks (CNNs). Specifically, the GSFP adopts a robust pruning method, which measures the global redundancy of the filter in the whole model by using the soft pruning strategy. In addition, in the model recovery process after pruning, the authors use the cumulative saliency strategy to improve the accuracy of pruning. The authors also propose a reasonable normalization formula to prevent certain layers of filters from being completely clipped due to excessive pruning rate."
SP:621e41d4199e333ec7f9d0936d4e34c918f39c11,"This paper proposes a cross-lingual document classification framework (CACO) between related language pairs. CACO leverages the subword similarity by jointly training a character-based embedder and a word-based classifier. The embedder derives vector representations for input words from their written forms, and the classifier makes predictions based on the word vectors. The authors use a joint character representation for both the source language and the target language, which allows the embedder to generalize knowledge about source language words to target language words with similar forms. They also propose a multi-task objective that can further improve the model if additional data is available."
SP:544e421f9c747640d949f433e3091763508b7237,"This paper proposes a marginalized average attentional network (MAAN) for weakly-supervised temporal action localization. The MAAN samples multiple subsets from the video snippet features according to a set of latent discriminative probabilities and takes the expectation over all the averaged subset features. Theoretically, the authors prove that the MAA module successfully reduces the difference in responses between the most salient regions and the others. Moreover, they propose a fast algorithm to reduce the complexity of constructing MAA from O(2^T) to O(T^2). Extensive experiments on two large-scale video datasets show that our MAAN achieves a superior performance."
SP:9f98c9bac99003741dd14e093b54d692c0b0e8d8,"This paper proposes to use Holographic Reduced Representation (HRR) for disentangled representations for language modeling. HRR is a generalization of the Vector Symbolic Architecture (VSA) family, which is used to represent and manipulate structures. The authors propose a language model with HRR that explicitly encodes the underlying structure as role-filler pairs on both word-level and chunk-level representations, and show that HRR provides an inductive bias towards the learning of decomposed representations. The proposed model can effectively separate certain aspects of word or chunk representation, which roughly corresponds to a division between syntax and semantics."
SP:5908b6acfed0e7c51e203c72eba907e6635e6c60,This paper studies the problem of partial-observable Markov decision processes (POMDPs) where the agent has an active role in its perception by selecting which observations to receive. The authors propose a greedy strategy for observation selection that aims to minimize the uncertainty in state. They develop a novel point-based value iteration algorithm that incorporates the greedy strategy to achieve near-optimal uncertainty reduction for sampled belief points. This enables the solver to efficiently approximate the reachable subspace of belief simplex by essentially separating computations related to perception from planning. The proposed solver is evaluated in a range of robotic scenarios where the robot simultaneously performs active perception and planning.
SP:0adec4abec17b3aab0c6eb69d11925dc20544950,"This paper proposes a new curriculum loss for training deep neural networks. The authors claim that training hard examples aggravates the distribution shifting and damages the training. To address this problem, the authors introduce a curriculum loss that consists of two parts: a) an adaptive weight that mitigates large early punishment; b) an additional representation loss for low-weighted samples. Experiments on CIFAR-10 and ImageNet show that the proposed curriculum loss is effective in improving the performance."
SP:8b555b9f24044bc68c204169d6a37e262361d706,"This paper proposes a method to learn heuristics for combinatorial optimization problems. The authors propose a model based on attention layers with benefits over the Pointer Network and show how to train this model using REINFORCE with a simple baseline based on a deterministic greedy rollout, which they find is more efficient than using a value function. They significantly improve over recent learned heuristic for the Travelling Salesman Problem (TSP), getting close to optimal results for problems up to 100 nodes. They also learn strong heuristic for two variants of the Vehicle Routing Problem (VRP), the Orienteering Problem (OP) and (a stochastic variant of) the Prize Collecting TSP (PCTSP)."
SP:efb76bcf1dbd9a9cf6b5db74b5d4256a9f9e9e73,"This paper proposes a differentiable neural architecture search (DNAS) framework to efficiently explore its exponential search space with gradient-based optimization. The proposed framework is based on constructing a super net whose macro architecture (number of layers, filter size of each layer, etc.) is the same as the target network. Each layer of super net contains edges representing convolution operators with quantized weights and activations with different precisions. Experiments show the quantized models with 21.1x smaller model size or 103.9x lower computational cost can still outperform baseline quantized or even full precision models."
SP:ea4173f8265bc50296de51c4ee7ecb6b8f78bec0,"This paper proposes a new attention mechanism for sequence-to-sequence learning. The main idea is to decompose the joint distribution of the output and attention variables into a posterior distribution conditioned on the output, which is then propagated to the next decoding stage. The proposed method is evaluated on 5 translation and 2 morphological inflection tasks and shows better performance than existing soft-attention and hard attention."
SP:987e2c14abc091d4d3ef9b48fb2046408eb1f59e,"This paper proposes a method for unsupervised image-to-image translation. The proposed method is based on CycleGAN, which is a GAN-based method that is able to learn a mapping from a source domain to a target domain in an unpaired setting. The main idea of this paper is to learn bi-directional translations between the source and the target domains by introducing a smoothness term over the sample graph to attain harmonic functions to enforce consistent mappings during the translation. Experiments are conducted on medical imaging, object transfiguration, and semantic labeling tasks."
SP:885a69003bad0e79cb2872a4e5c772191ad7e34f,"This paper studies the exploding and vanishing gradient problem (EVGP) of LSTM. The authors propose a simple stochastic algorithm (h-detach) to address this problem. Specifically, they show that when the weights are large, the gradient components through the linear path (cell state) get suppressed. Based on the hypothesis that these components carry information about long term dependencies (which they show empirically), their suppression can prevent LSTMs from capturing them. Their algorithm prevents gradients flowing through this path from getting suppressed, thus allowing the LstM to capture such dependencies better. They show significant improvements over vanilla RNN gradient based training."
SP:9aaff3777321347d1194884af5690b0b5185eff9,"This paper proposes a method for training binary weight networks (BWNs) from scratch under the Bayesian deep learning perspective, where the final objective is to approximate the posterior distribution of binary weights rather than reach a point estimation. The posterior distribution is parameterized as a policy network trained with a reinforcement learning scheme. During the training phase, it generates binary weights on-the-fly since what we actually maintain is the policy network, and all the binary weights are used in a burn-after-reading style. At the testing phase, we can sample binary weight instances for a given recognition architecture from the learnt policy network. Experiments conducted on the widely used image classification datasets show that SnapQuant has better performance in comparison to related probabilistic methods."
SP:29d1f6d0661a51e56c59bbb106da56700fc22d9a,"This paper proposes a probabilistic framework for federated learning with neural networks. The main idea is to train local models for each data source, in parallel, and then match the estimated local model parameters across data sources to construct a global network. The matching is governed by the posterior of a Beta-Bernoulli process (BBP), a Bayesian nonparametric model that allows the local parameters to either match existing global ones or create a new global parameter if existing ones are poor matches. This construction allows the size of the global network to flexibly grow or shrink as needed to best explain the observed data."
SP:ab1f2bd216635d63450688866c729a501bd7e9d0,"This paper studies the problem of learning in non-convex games. The authors propose a new algorithm called Stable Opponent Shaping (SOS) that interpolates between LOLA and a stable variant called LookAhead. Theoretically, the authors prove that SOS converges locally to equilibria and avoids strict saddles in all differentiable games. Experiments are conducted to show that SOS outperforms LOLA."
SP:bdafb5fca09a775a8c92d2826d5dc977d28091c2,"This paper proposes an alarm system to detect the failure of segmentation algorithms. The idea is to project the segmentation results into a low dimensional feature space, and then learn classifiers/regressors in the feature space to predict the qualities of the results. The feature space is formed using shape feature, which is a strong prior information shared among different data. The shape feature is captured using the value of loss function when the result is tested using a Variational Auto-Encoder (VAE). The VAE is trained using only the ground truth masks, therefore the bad segmentation result with bad shapes become the rare events for VAE and will result in large loss value. By utilizing this fact, the VAE can detect all kinds of shapes that are out of the distribution of normal shapes in ground truth (GT). Finally, the representation in the one-dimensional feature space can predict the quality of the segmentations results."
SP:60738395d9efe2b3fe3a00c542ebb4261e54386c,"This paper proposes an untrained simple image model, called the deep decoder, which is a deep neural network that can generate natural images from very few weight parameters. The network is simple in the sense that each layer has an identical structure that consists of only one upsampling unit, pixel-wise linear combination of channels, ReLU activation, and channelwise normalization. This simplicity makes the network amenable to theoretical analysis, and it sheds light on the aspects of neural networks that enable them to form effective signal representations."
SP:1c9bad3bd4d670172f65aa0304e9837ecafc6b3d,"This paper presents an end-to-end approach to program synthesis from natural language (NL) specifications. The main idea is to use a pretrained word embedding and a bi-directional multi-layer LSTM for processing of word sequences. The decoder features a doubly-recurrent LSTMs, for which the authors propose novel signal propagation schemes and soft attention mechanism. The proposed method is evaluated on a large dataset of problems proposed in a previous study, and produces correct programs in over 92% of cases."
SP:d2ec231bb6153a303e5110e671dea14c2721e636,"This paper studies the adversarial robustness of deep neural networks (DNNs) against adversarial perturbations on the MNIST dataset. The authors show that even the widely recognized and by far most successful L∞ defense by Madry et al. (1) has lower L0 robustness than undefended networks and is still highly susceptible to L2 perturbation, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization, and (4) features adversarial examples that make little sense to humans. Based on these observations, the authors propose a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. They derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks."
SP:91a24e7f4b952c37441feab4a7e8555014c856a4,"This paper proposes a new framework for training GANs, which allows more flexible spectrum control (e.g., making the weight matrices of the discriminator have slow singular value decay). Specifically, the authors propose a new reparameterization approach for the weights matrices in the discriminative network. Theoretically, they show that the spectrum control improves the generalization ability of GAN. The experiments on CIFAR-10, STL-10 and ImgaeNet datasets confirm that compared to other methods, the proposed method is capable of generating images with competitive quality."
SP:8115fd9b681198d62100c36794926fb57dc0a4f5,"This paper proposes an accelerated value iteration method for reinforcement learning. The proposed method is based on the Anderson acceleration technique, which is applied to the classical value iteration and policy iteration methods. The paper also extends the proposed method to the deep Q-learning algorithm, resulting in the Deep Anderson Accelerated Q-Learning (DA2Q) algorithm. Theoretical analysis and empirical results show the effectiveness of the proposed methods."
SP:bd79b0c0af778a36008a0c0cf2fb6393fd2789d4,"This paper proposes a novel method to tackle the catastrophic forgetting problem in the class incremental learning scenario. The proposed method combines the strength of deep learning and support vector machine (SVM), where SVM is used to identify the support data from the old data, which are fed to the deep learning model together with the new data for further training. Two powerful consolidation regularizers are applied to stabilize the learned representation and ensure the robustness of the learned model. The experimental results show that the proposed method outperforms the state-of-the-art incremental learning methods."
SP:d228d213f79716774043cea253305fecece659ec,"This paper compares different measures of unit selectivity in AlexNet, a well-studied network trained on a single task, with respect to four measures: localist selectivity, precision, class-conditional mean activity selectivity and precision, CCMAS, and a new measure called top-class selectivity. The authors claim that the precision and CCMAS measures provide a much higher level of selectivity than is warranted, with the most selective hidden units only responding strongly to a small minority of images from within a category. They also generated activation maximization (AM) images that maximally activated individual units and found that under (5%) of units in fc6 and conv5 produced interpretable images of objects, whereas fc8 produced over 50% interpretable image."
SP:b9deae0392e0160b400d76c549d382e235196f8c,"This paper proposes a line graph neural network (LGNN) for solving node-wise community detection problems in a supervised learning setting. In particular, the authors propose to augment GNNs with the non-backtracking operator defined on the line graph of edge adjacencies. They show that, in a data-driven manner and without access to the underlying generative models, they can match or even surpass the performance of the belief propagation algorithm on binary and multiclass stochastic block models, which is believed to reach the computational threshold in these cases."
SP:a9ed31090e55f6152fc31c7512af5d634cc7225a,"This paper studies the problem of online dictionary learning, where the goal is to model the given data as a linear combination of a few columns of a matrix known as a dictionary, and where the sparse weights forming the linear combination are known as coefficients. The authors propose a simple Neurally plausible alternating Optimization-based Online Dictionary Learning algorithm, which recovers both the dictionary and coefficients exactly at a geometric rate, when initialized appropriately. The algorithm is scalable and amenable for large scale distributed implementations in neural architectures, by which they mean that it only involves simple linear and nonlinear operations."
SP:85232b72a2643d6dc81cf952ccbb95192032b7c5,This paper proposes a new loss function and training scheme for learning binary hash codes with any differentiable model and similarity function. The loss function improves over prior methods by using log likelihood loss on top of an accurate approximation for the probability that two inputs fall within a Hamming distance target. The training scheme obtains a good estimate of the true gradient by better sampling inputs and evaluating loss terms between all pairs of inputs in each minibatch. The authors demonstrate that these techniques provide large improvements to a similarity search tasks.
SP:3bd4ccff7f48380d2db8dff2c4ca515894a7f1db,"This paper proposes a method for neural architecture search (NAS) by learning a hypernetwork that predicts the parameters of unseen networks by directly operating on their computational graph representations. Given an architecture, it directly generates the weights by running inference on a graph neural network and predicts network performance. To perform NAS, they randomly sample architectures and use the validation accuracy of networks with GHN generated weights as the surrogate search signal. They can search nearly 10x faster than other random search methods on CIFAR-10 and ImageNet. They also extend to the anytime prediction setting, where they have found networks with better speed-accuracy tradeoff than the state-of-the-art manual designs."
SP:65ccf43cd4e033d22239069057f5200d49f33724,This paper proposes a method to improve generative adversarial imitation learning (GAIL) by using additional information from non-expert demonstrations which are easier to obtain. The key idea of the method is to perform multiclass classification to learn discriminator functions where expert demonstrations are regarded as being drawn from an extra class. Experiments on continuous control tasks demonstrate that the proposed method learns better policies when the number of expert demonstrations is small.
SP:e8427949a98effbd37ce7604fa11f240e2342196,"This paper studies the problem of inferring the posterior distribution of the parameters of a neural network from a set of measurements. The authors propose an approach called Invertible Neural Networks (INNs) to solve the inverse problem, where the forward process is well-defined, but the inverse is ambiguous. The INN learns to associate hidden parameter values x with unique pairs [y, z] of measurements and latent variables. Given a specific measurement and the distribution of latent variables, the inverse pass of the INN provides the full posterior over parameter space. Experiments on synthetic data and real-world problems from medicine and astrophysics show that INNs are a powerful analysis tool to find multi-modalities in parameter space, uncover parameter correlations, and identify unrecoverable parameters."
SP:75c9bb53bac29bdb390f9ba5707caee4ab1f5925,"This paper proposes a method for uncertainty quantification of deep neural networks (NNs) based on mixture distributions. The authors propose to replace the fixed mixing weights of a finite mixture model with an adaptive, input-dependent distribution represented by an NN, and by considering uncountably many mixture components. The resulting model can be seen as the continuous counterpart to mixture density networks and is therefore referred to as compound density networks (CDNs). The authors empirically show that the proposed model results in better uncertainty estimates and is more robust to adversarial examples than previous approaches."
SP:e1e38289285c1b8fdb318e4f6d37a198a08787a2,"This paper proposes a new method for neural network compression. The main idea is to use a full variational distribution over weights instead of using deterministic weights, which allows for more efficient coding schemes and consequently higher compression rates. The proposed method is based on the classical bits-back argument, which states that the effective coding cost of the network weights is KL(q||p) = Eq[log qp], where q is a deterministic set of weights. The authors propose a scheme which produces an approximate sample from the variational posterior and derives a random weights message encoded with this random sample, while minimizing the resulting code length, which is called Minimal Random Code Learning (MIRLE). "
SP:ad70d8cf3a4558aab0d3b7155594464a3debd912,"This paper proposes ProxylessNAS, a proxy-less approach to neural architecture search (NAS) that can directly learn the architectures for large-scale target tasks and target hardware platforms. The main contributions of this paper are the following: (1) The paper proposes to remove the restriction of repeating blocks in previous NAS works (Zoph et al. 2018c; Liu et al., 2018c) and allow all of the blocks to be learned and specified blocks can be learned. (2) It proposes to use differentiable NAS to reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization."
SP:e5b70d43d301d1980fae02623ea711976b429c14,"This paper proposes to modify the linear penalties to second-order ones, and argues that this results in a more practical training procedure in non-convex, large-data settings. For one, the use of second order penalties allows training the penalized objective with a fixed value of the penalty coefficient, thus avoiding the instability and potential lack of convergence associated with two-player min-max games. Secondly, the authors derive a method for efficiently computing the gradients associated with the second- order penalties in stochastic mini-batch settings. The resulting algorithm performs well empirically on a number of standard benchmarks."
SP:e4720b8e4efdb222c45eafd47fd8a7fbf15d881d,This paper proposes a reweighted wake-sleep (RWS) algorithm for learning generative models with discrete latent variables. The authors show that RWS outperforms the state-of-the-art methods in learning discrete latent-variable models. The main contributions of this paper are two folds: (1) RWS is a competitive alternative to the importance weighted autoencoder (IWAE) and (2) it is able to learn better models and inference networks with increasing number of particles.
SP:7459ae5b1d886e68930c4c9e21df508bc8ab3c9a,"This paper proposes to use truncated randomized search to train structured prediction energy networks (SPENs), which provide efficient test-time inference using gradient-based search on a smooth, learned representation of the score landscape, and have previously yielded state-of-the-art results in structured prediction tasks. In particular, this paper shows that this method yields previously unknown local improvements, providing effective supervision to SPENs, avoiding their traditional need for labeled training data. "
SP:638c1bc09992029b78bd83f0127594dcccb96c06,"This paper proposes an active learning approach for robust policy search (RPS). In particular, the authors propose a framework, EffAcTS, to selectively choose model parameters for this purpose so as to collect only as much data as necessary to select such a subset. They apply this framework to an existing method, namely EPOpt, and experimentally validate the gains in sample efficiency and the performance of our approach on standard continuous control tasks. They also present a Multi-Task Learning perspective to the problem of RPS, and draw connections from our proposed framework to existing work on multi-task learning."
SP:491c239713a6489f0b1790ca26db54a1813c67ae,"This paper proposes a two-timescale network (TTN) architecture that enables linear methods to be used to learn values, with a nonlinear representation learned at a slower timescale. The approach facilitates the use of algorithms developed for the linear setting, such as data-efficient least-squares methods, eligibility traces and the myriad of recently developed linear policy evaluation algorithms, to provide nonlinear value estimates. The authors prove convergence for TTNs, with particular care given to ensure convergence of the fast linear component under potentially dependent features provided by the learned representation."
SP:327d606cf3813b00a009a7785e08ef9e11f89493,"This paper proposes a hybrid model-based and model-free approach, LEArning and Planning with Semantics (LEAPS), consisting of a multi-target sub-policy that acts on visual inputs, and a Bayesian model over semantic structures. When placed in an unseen environment, the agent plans with the semantic model to make high-level decisions, proposes the next sub-target to execute, and updates the model based on new observations. The proposed approach outperforms strong baselines that do not explicitly plan using the semantic content."
SP:d7c26f43bc68d160095b1f50447528843d79edbd,"This paper proposes a method to improve the generalization ability of end-to-end deep learning-based autonomous driving models. The proposed method consists of two modules: a perception module and a driving module. The perception module is used for learning easier driving-related perception knowledge, i.e., ability of pixel level understanding of input including what & where and how far knowledge. The driving module is trained to generate final control commands for difficult driving tasks. Experiments are conducted to show the effectiveness of the proposed method."
SP:b6bd98cc70fab97e1245cbb63a42ef89ab7e7ed5,"This paper studies the trade-off between adversarial robustness and standard generalization in deep learning models. The authors show that adversarially robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. They further argue that this phenomenon is a consequence of robust classifiers learning fundamentally different feature representations than standard classifiers. In particular, the features learned by robust models tend to align better with salient data characteristics and human perception."
SP:9c9275d75cd95b1b82e0cbb1421e3d3ade1ce33a,"This paper proposes a method for training deep neural networks without back-propagation. It builds on the Equilibrium Propagation method proposed by Scellier & Bengio (2017), which uses only local learning rules and does not rely on neurons having a mechanism for backpropagating an error gradient. The main limitation of the proposed method is that inference involves doing an iterative optimization of neural activations to find a fixed-point, and the number of steps required to approximate this fixed point scales poorly with the depth of the network. The proposed method trains a feedforward network to approximate the state of the fixed-points using a local learning rule. After training, it can simply use this initializing network for inference, resulting in a learned feed-forward network."
SP:ac9ea91eb465517de495477cf67bc94d5ed1b0cb,"This paper proposes a zeroth-order (ZO) stochastic optimization algorithm, ZO-signSGD, which enjoys dual advantages of gradient-free operations and signSGD. The latter requires only the sign information of gradient estimates but is able to achieve a comparable or better convergence speed than SGD-type algorithms. The authors show that ZO sign SGD requires $\sqrt{\delta}$ times more iterations than signSGd, leading to a convergence rate of $O(d/\sqrt{d}$ under some mild conditions, where $d$ is the number of optimization variables and $T$ is number of iterations. In addition, the authors analyze the effects of different types of gradient estimators on the convergence rate, and propose several variants of ZO SignSGD with $O(\sqrt d/T)$ convergence rate. Finally, the application side of the paper explores the connection between the proposed algorithm and black-box adversarial attacks in robust deep learning."
SP:5f79b11777f6ef1d70c85418bfc2e4616dd7d960,"This paper proposes a new optimization method to reduce the computation efforts of convolutional neural networks. The method takes advantage of the fact that some convolution operations are actually wasteful since their outputs are pruned by the following activation or pooling layers. The authors propose to set a checkpoint in the MAC process to determine whether a filter could terminate early based on the intermediate result. Furthermore, a fine-tuning process is conducted to recover the accuracy drop due to the applied checkpoints. The experimental results show that the proposed method can save approximately 50% MAC operations with less than 1% accuracy drop for CIFAR-10 example model and Network in Network on the Cifar-10 and CifAR-100 datasets."
SP:7801e9c854ad7d960c0d24fda15597af6994c23f,This paper studies adversarial robustness of ASR systems against input transformation attacks. The authors show that the input transformation developed from image adversarial defense provides limited robustness improvement and is subtle to advanced attacks. They also show that temporal dependency can be exploited to gain discriminative power against audio adversarial examples and is resistant to adaptive attacks. 
SP:51830b811a8e39b4f0a5b7609df719e026fac6a1,"This paper proposes a generative model for multi-object image generation that explicitly considers object and their relations explicitly, and generate images by means of composition. The authors propose to structure the generator of a GAN to consider objects and their relation explicitly, which provides a way to efficiently learn a more accurate generative models of real-world images, and serves as an initial step towards learning corresponding object representations. The generator learns to identify and disentangle information corresponding to different objects at a representational level. They evaluate their approach on several multi- object image datasets, and find that the generator learns about the individual objects and the background of a scene without prior access to this information."
SP:fb59990b8da0e95d8202383478a456667de60449,"This paper proposes a method for learning disentangled representations from visual data, where different high-level generative factors are independently encoded. The authors propose a learning setting which they refer to as “reference-based disentangling”: given a pool of unlabelled images, the goal is to learn a representation where a set of target factors are disentanglement from others. The only supervision comes from an auxiliary reference set that contains images where the factors of interest are constant. The proposed method, called reference-based variational autoencoders, is a novel deep generative model designed to exploit the weak supervisory signal provided by the reference set. During training, the authors use the variational inference framework where adversarial learning is used to minimize the objective function."
SP:dbc1983d9b9d72aa14f8e8515d793d2bbde26c9c,"This paper proposes a meta-learning for online learning (MOLe) approach to model-based reinforcement learning, where adapting the predictive model is critical for control. The authors propose an online learning procedure that uses stochastic gradient descent to update model parameters, and an expectation maximization algorithm with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distributions. This allows for all models to be adapted as necessary, with new models instantiated for task changes and old models recalled when previously seen tasks are encountered again. Furthermore, the authors observe that meta-training a model such that direct online adaptation with SGD is effective, which is otherwise not the case for large function approximators."
SP:5665e5f006f84927beb0440e145f476e02538077,"This paper studies the problem of training RNN-based RL agents from distributed prioritized experience replay. The authors demonstrate the effects of parameter lag resulting in representational drift and recurrent state staleness and empirically derive an improved training strategy. The resulting agent, Recurrent Replay Distributed DQN (R2D2), quadruples the previous state of the art on Atari-57 and matches the state-of-the-art on DMLab-30."
SP:47ace37f31a46d5ee85c283e62ddb71a12f2c5c4,"This paper proposes a hierarchical generative model for capturing coordinated multi-agent trajectory behavior, such as offensive basketball gameplay. The proposed approach is inspired by recent work on leveraging programmatically produced weak labels, which is extended to the spatiotemporal regime. In addition to synthetic settings, the authors instantiate the framework to effectively model complex interactions between basketball players and generate realistic multiagent trajectories of basketball gameplay over long time periods. The authors validate the approach using both quantitative and qualitative evaluations, including a user study comparison conducted with professional sports analysts."
SP:1a90cdf028068528b0559e7d44bf26dda20310bd,"This paper proposes a graph-structured variational recurrent neural network (Graph-VRNN) that is trained end-to-end to infer the current state of the (partially observed) world and to forecast future states. The proposed method is evaluated on two sports datasets, one based on real basketball trajectories, and one generated by a soccer game engine. The results show that the proposed method outperforms various baselines."
SP:8392f04b7265f665ba6d44d297bca245d44b4708,"This paper proposes a method for end-to-end training of a base neural network that integrates calls to existing black-box functions. The base network is trained with a differentiable neural network, and at inference time, it replaces the differentiable estimator with its external blackbox non-differentiable counterpart such that the base network output matches the input arguments of the black box function. The proposed method is evaluated on semantic parsing and question answering tasks, and it is shown that the integrated model generalizes better than a fully differentiable model."
SP:13fb86de763a0b34ac6fa34ea9dfbd1c476ce43e,This paper proposes a meta-learning algorithm that learns a mixture of hierarchical Bayesian models over the parameters of an arbitrary function approximator such as a neural network. The authors propose a stochastic expectation maximization procedure to jointly estimate parameter initializations for gradient descent as well as a latent assignment of tasks to initializations. This approach better captures the diversity of training tasks as opposed to consolidating inductive biases into a single set of hyperparameters. The experiments demonstrate better generalization performance on the standard miniImageNet benchmark for 1-shot classification and few-shot regression.
SP:a410144dbe19713a06c63da87d9fb58b999a7492,"This paper proposes a self-supervised approach to auxiliary learning for image classification, where the auxiliary task is hierarchical sub-class image classification. The proposed method, called Meta Auxiliary Learning (MAXL), learns a multi-task evaluator to determine the target labels for auxiliary tasks to improve the generalization performance on the principal task. Experiments on three different CIFAR datasets show that MAXL outperforms baseline auxiliary learning methods, and is competitive even with a method which uses a human-defined class hierarchy."
SP:76248e1c914c60ce69de244fe7ec62488d01e161,"This paper proposes a neural network-based representation for open set recognition. In particular, the authors propose to use a distance-based loss function to learn a representation of the open set. The proposed approach is evaluated on three datasets from two different domains. The results show that the proposed approach outperforms the baselines. "
SP:d4ee856bbf2dfb6390e5247086fec2e52dcb6858,"This paper studies the problem of training low-precision convolutional neural networks. In particular, this paper focuses on 8-bit and 4-bit networks. The authors first show that the accuracy of the low precision networks is close to the full precision networks after one epoch of fine-tuning. Then, the authors propose two methods to improve the performance of the networks: (1) using pretrained fp32 baseline networks, and (2) using larger batches and matched learning rate annealing."
SP:6bfdc37b346e6ddfa049e0414647f4beda8ede3f,This paper proposes a method to predict post-bounce trajectories and infer physical properties of objects in a scene. The method consists of two modules: Physics Inference Module (PIM) and Visual Inference module (VIM). PIM learns to model physical interactions for the prediction task given physical parameters and observed pre-collision 3D trajectories. VIM is used to infer physical parameters for locations in the scene given a single still image. The model learns from the collected dataset of real-world bounces and is bootstrapped with additional information from simple physics simulations.
SP:010bd055310c363d3cb0fbe0e11546de58220e15,"This paper studies the relationship between adversarial vulnerability and the norm of the gradients of the training objective of a differentiable classifier. The authors show that the norm grows as the square root of the input dimension, which implies that the network becomes more vulnerable to adversarial perturbations as the input size increases. They also provide some theoretical results to support their findings. "
SP:5fa3ae057e55be6b71cc94a7dbfe31e54e1c536f,"This paper proposes a method for interactive agent modeling by encouraging an agent to learn to probe. In particular, the probing agent learns to interact with the environment and with a target agent (i.e., a demonstrator) to maximize the change in the observed behaviors of that agent. Through probing, rich behaviors can be observed and are used for enhancing the agent modeling to learn a more accurate mind model of the target agent. The framework consists of two learning processes: i) imitation learning for an approximated agent model and ii) pure curiosity-driven RL for an efficient probing policy to discover new behaviors that otherwise can not be observed."
SP:3af184a5529d6ec2a0862efd1af80ef5b50d2952,"This paper proposes a modification to ANNs that mimics the function of biological neuromodulators and are termed modulators, which enable other traditional ANN nodes to adjust their activation sensitivities in run-time based on their input patterns. In this manner, it enables the slope of the activation function to be context dependent. This modification produces statistically significant improvements in the context of convolutional neural networks and LSTM cells."
SP:287a577834fd2820a939a1113b39146a22727491,"This paper presents a neural analysis and synthesis (NANSY) framework that can manipulate voice, pitch, and speed of an arbitrary speech signal. NANSY does not require any labels associated with speech data such as text and speaker information, but rather uses a new set of analysis features, i.e., wav2vec feature and newly proposed pitch feature, Yingram, which allows for fully self-supervised training. Experiments on zero-shot voice conversion, pitch shift, and time-scale modification demonstrate the effectiveness of the proposed method."
SP:90f35ad1ec0c38b0817f5678ee2a5c4f0e08fb38,"This paper studies the generalization properties of gradient-based unrolled differentiation (UD) methods for hyperparameter optimization. In particular, it provides an expectation bound on the validation set of the bilevel programming (BP) based on uniform stability. The authors also provide a generalization bound for the classical cross-validation (CV) method. Theoretical results suggest that gradient based methods are better than CV under certain conditions, and regularization terms in both the outer and inner levels can relieve the overfitting problem in gradient based algorithms. "
SP:42f52aec3a776d87daa5fd72b8e6325d12c88d63,"This paper proposes a new approach to transfer knowledge from a teacher to a student via knowledge distillation. The main idea is to train a student-friendly teacher network (SFTN) that aims to make the representations of the teacher model friendly to the student model. The SFTN consists of two steps: student-aware teacher learning (i.e., it learns the student branches jointly to obtain the student friendly representations) and teacher-friendly distillation (ii) that learns the teacher models that are friendly to students and, consequently, more appropriate for knowledge transfer. The proposed approach is applicable to diverse architectures of teacher and student while it can be incorporated into various knowledge distillations algorithms. The experimental results with in-depth analyses are presented in Section 4."
SP:e15a1c21229233fd97dc1dfa0a4ef48b69dc9f95,"This paper studies domain generalization to out-of-distribution (OOD) data. In particular, the authors introduce a new concept of expansion function, which characterizes to what extent the variance is amplified in the test domains over the training domains, and therefore give a quantitative meaning of invariant features. It turns out that OOD generalization largely depends on the expansion function. As recently pointed out by Gulrajani and Lopez-Pazazaz, any OOD learning algorithm without a model selection module is incomplete."
SP:37b04b9068d39bcf0a581eb8181d13cf1a8926bf,"This paper proposes a variational continual Bayesian meta-learning (VC-BML) algorithm for non-stationary online tasks. The authors propose a Dynamic Gaussian Mixture Model (DGMM) for meta-parameters, which is a mixture of task-specific parameters. The number of component distributions is determined by a Chinese Restaurant Process (CPR). The authors also propose a structured variational inference (SVI) method to approximate the posterior distributions of interest by deriving the parameter distributions and quantify the uncertainty. The experimental results show that the proposed method is superior in transferring knowledge among diverse tasks and alleviating catastrophic forgetting."
SP:776d5b02b8d3a8bbcc1f52706f3887c384cb149e,"This paper proposes a probabilistic method for solving ODE boundary value problems (BVPs), which are ordinary differential equations subject to boundary conditions. The authors introduce a Gauss–Markov prior and tailor it specifically to BVPs, which allows computing a posterior distribution over the solution in linear time, at a quality and cost comparable to that of well established, non-probabilistic methods. The model further delivers uncertainty quantification, mesh refinement, and hyperparameter adaptation. The proposed method is compatible with other statistical modelling tool-chain."
SP:86aac0c6b75fdc12f84bba342934865616f866d4,"This paper considers the reward-mixing Markov decision process (RM-MDP) setting, where the transition kernel and initial state distribution are defined as in standard MDPs, while the reward function is randomly chosen from one of M-reward models at the beginning of every episode (see the formal definition 1). The authors provide the first polynomial-time algorithm that finds an -optimal policy after exploring $\tilde{O}(\poly(H, -1)^2)$ episodes, where H is time-horizon and S, A are the number of states and actions respectively. This is the first efficient algorithm that does not require any assumptions in partially observed environments where the observation space is smaller than the latent state space."
SP:1a3c70ae9cf2a806d603f4b9e7ca6e10b720a956,"This paper proposes a single-cause perturbation procedure to estimate the multi-cause conditional average treatment effect (CATE) from observational data. In particular, the authors propose a two-step procedure, Single-cause Perturbation (SCP), which augments the observational dataset with the estimated potential outcomes under single-causal interventions and performs covariate adjustment on the augmented dataset to obtain the estimator. The authors show that the procedure is valid under standard assumptions in causal inference. Experimental results on extensive synthetic and semi-synthetic data demonstrate the effectiveness of the proposed method."
SP:247bc6675cce89d51558537daf63dadb0c4307f8,"This paper proposes a multi-wavelet-based neural operator learning scheme that compresses the associated operator’s kernel using fine-grained wavelets. The projected kernel is trained at multiple scales derived from using repeated computation of multiwavelet transform. This allows learning the complex dependencies at various scales and results in a resolution-independent scheme. Compared with the existing neural operator approaches, the proposed model shows significantly higher accuracy and achieves state-of-the-art in a range of datasets."
SP:1153785e6a016cfee2644952a772aa08927299b6,"This paper proposes a method for training binary neural networks (BNNs) by approximating the gradient of sign function in the Fourier frequency domain using the combination of sine functions for training BNNs, namely frequency domain approximation (FDA). The proposed approach does not affect the low-frequency information of the original sign function which occupies most of the overall energy, and high-frequency coefficients will be ignored to avoid the huge computational overhead. In addition, the authors embed a noise adaptation module into the training phase to compensate the approximation error. The experiments on several benchmark datasets and neural architectures illustrate that the binary network learned using our method achieves the state-of-the-art accuracy."
SP:33b95ea8da4d30b8e8f9d3fe3acca023d4b8d831,"This paper proposes to use multi-area RNNs with neuroscience-inspired architecture constraints to learn biologically plausible solutions to a perceptual decision-making task. They show that incorporating multiple areas and Dale’s Law is critical for biasing the networks, and show that output-relevant information is preferentially propagated between areas. The results suggest that cortex uses modular computation to generate minimal sufficient representations of task information."
SP:db3ced65d67e3373fb3936ec50f41c8ef010bbbe,"This paper proposes structured attention graphs (SAGs) to visualize multiple localized saliency maps for image classification. The proposed SAGs are composed of disjunctions of conjunctions of features represented by local regions, which are connected based on containment relationships between the regions, and each map is accompanied with the prediction confidence of the classification based on the map (see Fig. 2 for an example). The paper also proposes a diverse sampling approach for SAG construction and visualization, which allows users to view information from a diverse set of maps which serves as a novel type of explanation for new users to efficiently view and interpret the local maps. The paper conducts a user study comparing the use of SAG to traditional saliency map for answering counterfactual questions about image classifications."
SP:f2b385bfd9ada0e26aa8829214b424f58582d9f7,"This paper studies how the choice of training objective affects the transferability of the hidden representations of convolutional neural networks trained on ImageNet. They show that many objectives lead to statistically significant improvements in ImageNet accuracy over vanilla softmax cross-entropy, but the resulting fixed feature extractors transfer substantially worse to downstream tasks. The authors also show that differences among loss functions are apparent only in the last few layers of the network. They also delve deeper into representations of the penultimate layer, finding that different objectives and hyperparameter combinations lead to dramatically different levels of class separation."
SP:b66b5e24f68563e2e200eda660f0dbaff53efeff,"This paper proposes a method to obtain spatio-temporal super-resolution in neuronal time series by exploiting relationships among neurons, embedded in latent low-dimensional population dynamics. The authors propose a neural network training strategy, selective backpropagation through time (SBTT), enables learning of deep generative models of latent dynamics from data in which the set of observed variables changes at each time step. The resulting models are able to infer activity for missing samples by combining observations with learned latent dynamics. They test SBTT applied to sequential autoencoders and demonstrate more efficient and higher-fidelity characterization of neural population dynamics in electrophysiological and calcium imaging data."
SP:3513a83806e71006b86d60b779d8bd6bb87c3546,"This paper proposes a hierarchical approach to sequence-to-sequence learning with quasi-synchronous grammars. The source and target trees are treated as latent and induced during training. The authors develop a neural parameterization of the grammar which enables parameter sharing over the combinatorial space of derivation rules without the need for manual feature engineering. They apply this latent neural grammar to various domains—a diagnostic language navigation task designed to test for compositional generalization (SCAN), style transfer, and small-scale machine translation—and find that it performs respectably compared to baseline approaches."
SP:d06fc251f2a9287f7a2236a188349628d8f39d9a,"This paper proposes a new algorithm to perform Group Elastic Net with application to function-on-scalar feature selection, where a functional response is modeled against a very large number of scalar predictors. The proposed algorithm is based on the sparsity structure of the Augmented Lagrangian to greatly reduce the computational burden. The authors also extend the algorithm to the function on scalar regression framework. Experiments on simulations demonstrate the CPU time gains afforded by the approach compared to existing competitors."
SP:e0b53f76f3a6b756fedd09926f9cf034f89f4a5a,"This paper proposes a mixture model of multi-level marked point processes for identifying potential heterogeneity in the observed data. Specifically, the authors study a matrix whose entries are marked log-Gaussian Cox processes and cluster rows of such a matrix. An efficient semi-parametric Expectation-Solution (ES) algorithm combined with functional principal component analysis (FPCA) of point processes is proposed for model estimation. The effectiveness of the proposed framework is demonstrated through simulation studies and real data analyses."
SP:3aa213076f3e9f9838ac654517df2fe1fca33499,"This paper proposes a multi-task learning approach for adaptive nonlinear control, which they call Online Meta-Adaptive Control (OMAC). The goal is to control a nonlinear system subject to adversarial disturbance and unknown environment-dependent nonlinear dynamics, under the assumption that the environment dependent dynamics can be well captured with some shared representation. The authors provide instantiations of their approach under varying conditions, leading to the first non-asymptotic end-to-end convergence guarantee for multi- task non linear control. They also show how to integrate OMAC with deep representation learning, which improves empirical performance."
SP:cb274c93a169b199ea09120ca02105a3f16b31c5,"This paper focuses on the certified robust training of deep neural networks. The authors identify two issues in existing methods, namely exploded bounds at initialization, and the imbalance in ReLU activation states and improve IBP training. To mitigate these issues and conduct faster certified training with shorter warmup, the authors propose three improvements based on interval bound propagation (IBP) training: 1) derive a new weight initialization method, 2) add Batch Normalization (BN) to each layer in the model, and 3) design regularization to explicitly tighten certified bounds and balance ReLU activations during warmup."
SP:18ffeb199a670fb2b1f4417b8653479001944dab,"This paper studies the problem of change point detection in the presence of adversarial perturbations to the data. The authors propose to use the Huber-contamination framework, which allows the contamination distributions to be different at each time point. They show that the detection boundary is a function of the contamination proportion and derive the minimax-rate optimal localisation error rate, quantifying the cost of accuracy in terms of contamination proportion. They also propose a computationally-feasible method, matching the lower bound under certain conditions, saving for logarithmic factors."
SP:d03617b5fc446768809cf015c9234b0c9386a690,"This paper studies the power of learning via mini-batch stochastic gradient descent (SGD) and batch Gradient Descent (GD) on the empirical loss of a differentiable model or neural network. The authors show that SGD and GD can always simulate learning with statistical queries (SQ), but their ability to go beyond that depends on the precision of the gradient calculations relative to the minibatch size b and sample size m. When the precision is small enough, SGD can go beyond SQ learning and simulate any sample-based learning algorithm and thus its learning power is equivalent to that of PAC learning; this extends prior work that achieved this result for b = 1. Similarly, with polynomially many bits of precision, GD can simulate PAC learning regardless of the batch size. On the other hand, when b^{-1} is large enough, the learning power of SGD is equivalent of that of SQ learning. "
SP:1de2864fe2f53e25596a9bd2c61e2048e79296f6,This paper considers the problem of estimating the Wasserstein distance between two probability distributions over a metric space. The authors propose a modified version of the standard Lloyd’s algorithm in which Voronoi cells are replaced by Power cells. They show that the algorithm converges to a point cloud that is sufficiently far from each other. Similar bounds can be deduced for the corresponding gradient descent.
SP:c3d364aeee55230a436c3ce4e8dc8310ee73959e,This paper proposes a new dynamic feature transform for video understanding. The proposed RSA leverages rich structures of spatio-temporal relations in videos by dynamically generating relational kernels and aggregating relational contexts. The RSA network substantially outperforms convolution and self-attention counterparts on the standard motion-centric benchmarks for video action recognition. 
SP:2c2530069d5cab485629090243da464d107feadd,"This paper studies the fluctuation in the multilayer case of the mean field theory of multi-layer neural networks. The main contribution is the derivation of the second-order mean field limit, which captures the limiting fluctuation distribution. Theoretical results are also provided to show the stability property of gradient descent mean field training. "
SP:a3d927854d9d7fd39b8d05a79666810d585d5062,"This paper proposes to learn generalized Casimirs for energy and entropy in a metriplectic dynamical system, which is a generalization of the Poisson brackets of Hamiltonian/Lagrangian mechanics. The authors propose a new parameterization of dissipative brackets that is appropriate for learning irreversible dynamics with unknown a priori model form. The proposed method is able to preserve the fluctuation-dissipation theorem, ensuring thermodynamic consistency, and the authors show that the learned dynamics are more robust and generalize better than either black-box or penalty-based approaches."
SP:32e8e83e06b1e9a4dad761334d5947c91bfd1853,"This paper proposes a sample selection-based algorithm for fair and robust training. The authors formulate a combinatorial optimization problem for the unbiased selection of samples in the presence of data corruption, and propose a greedy algorithm that is efficient and effective in practice. Experiments show that the proposed algorithm obtains fairness and robustness that are better than or comparable to the state-of-the-art technique, both on synthetic and benchmark real datasets. The algorithm can be used by modifying the sampling step in batch selection without changing the training algorithm or leveraging additional clean data."
SP:991127729bf067fe27fdd7ed360aab39e4df5921,"This paper studies the relationship between the activation functions of Bayesian neural networks (BNNs) and stationary Gaussian process (GP) priors. The authors show that periodic activation functions in BNNs establish a connection between the prior on the network weights and translation-invariant, stationary GP priors and show that this link goes beyond sinusoidal (Fourier) activations by also covering triangular wave and periodic ReLU activation functions. They also show that placing a Student-t-prior on the weights of the hidden layer corresponds to a prior on function space with Matérn class of kernels. Finally, they show in a range of experiments that periodic activations have comparable performance for in-domain data and capture sensitivity to perturbed inputs in deep neural networks for out of domain detection."
SP:d61a2aecfea4612c473b4e6fd41f3dc2fcbb04a1,This paper proposes a method to automatically assign a score to a student’s interactive code assignments by learning to classify Markov Decision Processes (MDPs). The authors propose a cooperative approach where an autoregressive model is used to sample differential trajectories from the input MDPs that allow a classifier to determine membership: Play to Grade. The method is evaluated on a real-world Code.org dataset with hand-coded bug labels.
SP:daf99ad91613d6e11b13315ccbd1bbe25094ae4b,"This paper proposes a method for interpretable deep reinforcement learning (DRL) based on mimic learning. The method is based on an identifiable multi-object network (IMONet) that detects objects from high-dimensional input space and learns an object representation to embed the objects with a limited number of latent variables. This representation is well-disentangled, allowing the mimic tree to uniquely extract the underlying causal relation from DRL models. A Monte Carlo Regression Tree Search (MCRTS) algorithm explores different splits to find the IB-optimal mimic trees. Experiments show that the mimic trees achieves strong approximation performance with significantly fewer nodes than baseline models."
SP:84560de78af979354fff83d1370d8675c1e9191f,"This paper introduces a Bayesian framework for modeling the structure of dynamic predictions over time. The authors model these trajectories by assuming predictions update according to a latent process of information flow, which is inferred from historical data. This approach preserves important properties of probability paths such as the martingale structure and appropriate amount of volatility and better quantifies future uncertainties around probability paths.  The authors show that GLIM outperforms three popular baseline methods, producing better estimated posterior probability path distributions measured by three different metrics."
SP:0c4bfb44e0a353256692d5e5ae96f65c1a14363d,"This paper studies the problem of active pure exploration with fixed confidence in stochastic bandit environments. The goal is to answer a query about the environment with a given level of certainty while minimizing her sampling budget. For this problem, instance-specific lower bounds on the expected sample complexity reveal the optimal proportions of arm draws an Oracle algorithm would apply. The authors devise Frank-Wolfe-based Sampling (FWS), a simple algorithm whose sample complexity matches the lower bounds for a wide class of pure exploration problems. The algorithm is computationally efficient as, to learn and track the optimal proportion of arms draws, it relies on a single iteration of Frank-wolfe algorithm applied to the lower-bound optimization problem. FWS is applied to various pure exploration tasks, including best arm identification in unstructured, thresholded, linear, and Lipschitz bandits."
SP:0947a0f08fba53d3c8af9b78dd64e6e10fc73e32,This paper proposes a new method for Bayesian optimization (BO) for combinatorial spaces. The proposed method is based on deep generative models (DGMs) to learn a latent space representation of structures using encoder-decoder style generative model (DGM) and a surrogate model over this latent space to perform BO. The key idea is to extend a kernel over the latent space (with its hyper-parameters estimated on the evaluated points) to non-evaluated points in the space by utilizing the features of structured kernels defined by string kernels and graph kernels. The experiments on real-world benchmarks show that LADDER significantly improves over the BO over latent space method and performs better or similar to state-of-the-art methods.
SP:37adabdc6615c5199a481553c8ccc06d57363614,"This paper studies the role of representation of state-action value functions in regret minimization in finite-horizon Markov Decision Processes (MDPs) with linear structure. They first derive a necessary condition on the representation, called universally spanning optimal features (UNISOFT), to achieve constant regret in any MDP with linear reward function. This result encompasses the well-known settings of low-rank MDPs and, more generally, zero inherent Bellman error (also known as the Bellman closure assumption). They then demonstrate that this condition is also sufficient for these classes of problems by deriving a constant regret bound for two optimistic algorithms (LSVI-UCB and ELEANOR). Finally, they propose an algorithm for representation selection and prove that it achieves constant regret when one of the given representations, or a suitable combination of them, satisfies the UNISOFT condition."
SP:92566b664ab2f6ee9b73f29327aeef85d14ecf60,"This paper proposes a differentiable contact model, which can capture contact mechanics: frictionless/frictional, as well as elastic/inelastic contacts. The proposed contact model extends the scope of Lagrangian and Hamiltonian neural networks by allowing simultaneous learning of contact and system properties. The authors demonstrate this framework on a series of challenging 2D and 3D physical systems with different coefficients of restitution and friction. The learned dynamics can be used as a simulator for downstream gradient-based optimization tasks."
SP:82d59a3609dfd458f90f23d4e477c8b497e9dc18,"This paper investigates the Benevolent training hypothesis (BTH) which argues that the complexity of the function a deep neural network (NN) is learning can be deduced by its training dynamics. The paper provides evidence for BTH by relating the NN’s Lipschitz constant at different regions of the input space with the behavior of the stochastic training procedure. The main results are: 1) NNs whose 1st layer bias is trained more steadily (i.e., slowly and with little variation) have bounded complexity even in regions of input space that are far from any training point. 2) The steady training with Dropout implies a training-and-datadependency generalization bound that grows poly-logarithmically with the number of parameters."
SP:9b329c915fa8d4045c167c9df37a49ee314d190e,"This paper studies the problem of distribution-independent PAC learning of halfspaces in the Massart noise model with strongly polynomial sample complexity, i.e., independent of the bit complexity of the examples. In particular, the authors show that any distribution can be efficiently decomposed as a disjoint mixture of few distributions for which a Forster transform exists and can be computed efficiently. The main application of this result is the distribution-dependent halfspace learning problem, where the goal is to learn a binary classification function from random labeled examples with noisy labels. The sample complexity of [DGT19] scales polynomially with the bit-complexity, which is an artifact of the algorithmic approach. Information-theoretically, no such dependence is needed — alas, the standard VC-dimension-based sample upper bound is nonconstructive. "
SP:e5229305af00067ae2dbabd903e585964aec8928,"This paper proposes a black-box adversarial attack method for graph neural networks. The proposed method is query-efficient and parsimonious with respect to the perturbation applied. Experiments are conducted to validate the effectiveness and flexibility of the proposed method on a wide range of graph classification tasks involving varying graph properties, constraints and modes of attack. An open-source implementation is also provided."
SP:4999e5664383066fdacd14be6242c7b83f85f3dd,"This paper studies the problem of label shift adaptation in the online setting, where the test-time label distribution is continually changing and the model must dynamically adapt to it without observing the true label. The authors propose two adaptation algorithms inspired by classical online learning techniques such as Follow The Leader (FTL) and Online Gradient Descent (OGD) and derive their regret bounds. They empirically verify their findings under both simulated and real world label distribution shifts and show that OGD is particularly effective and robust to a variety of challenging label shift scenarios."
SP:806515ae07fb1c9d02773592005d53d4158ef102,"This paper proposes a method for detecting and localizing gradual changes in the distribution of a sequence of time-ordered observations. The proposed method does not require a specific data generating model, a particular data type, or prior knowledge about which features of the distribution are subject to change. Theoretical guarantees for both detection and localization are provided. "
SP:7a3c8a7b17ecab19361d36e1d3d73fa35b71214c,"This paper proposes a biologically plausible blind source separation (BSS) algorithm for signal processing based on kurtosis-based ICA methods. The proposed algorithm is based on a novel objective function for ICA, which is a min-max optimization problem with geometric interpretation, and solve it by stochastic gradient optimization. The authors show that the proposed algorithm performs well on synthetic and real-world datasets. "
SP:22f8b517a3df65144412938f5891c463d7bae0ab,"This paper proposes a method to characterize the space of solutions associated with various tasks in RNNs. The authors first study a simple two-neuron network on a task that leads to multiple solutions. They trace the nature of the final solution back to the network’s initial connectivity and identify discrete dynamical regimes that underlie this diversity. They then examine three neuroscience-inspired tasks: Delayed discrimination, Interval discrimination, and Time Reproduction. For each task, they find a rich set of solutions."
SP:9b08a0f547ead3b59077a43b1052c6d46a0730f6,"This paper proposes a method for estimating the conditional distribution p(x_u|x_o) of unobserved features x_u and observed variables x_o over a set of covariates. The proposed method, called Arbitrary Conditioning with Energy (ACE), is a one-layer neural network-based approach that can simultaneously estimate the conditional distributions of all possible subsets of the unobserved variables and the observed variables. The main idea is to use an energy-based energy function to model the one-dimensional conditional densities and reduce the problem to only learning one-dimension conditionals (from which more complex distributions can be recovered during inference). This results in a simpler and higher-performing approach that is both simpler and simpler than prior methods. "
SP:f2b14f5854e6aa6922795d1d2051b7402486cef6,"This paper proposes a new adaptive weighted loss for single-image super-resolution (SISR) to train deep networks focusing on challenging situations such as textured and edge pixels with high uncertainty. Specifically, the authors introduce variance estimation characterizing the uncertainty on a pixel-by-pixel basis into SISR solutions so the targeted pixels in a high-resolution image (mean) and their corresponding uncertainty (variance) can be learned simultaneously. Moreover, uncertainty estimation allows us to leverage conventional wisdom such as sparsity prior for regularizing SisR solutions. The authors demonstrate that such uncertainty-driven loss can achieve better results than MSE or L1 loss for a wide range of network architectures."
SP:9997583f40fa648adf57bb4fc34228f357be0cf1,"This paper studies the problem of adversarial robustness, i.e., the ability of a model to be robust or invariant to imperceptible perturbations in the input. The authors propose a general PAC-Bayesian generalization bound that can be used to estimate, at test time, how much a model will be invariance to adversarial examples. The main contribution of this paper is to derive the PAC-PAC-Bayes framework to bound the averaged risk on the perturbation for majority votes (over the whole class of hypotheses), which has the advantage to provide general bounds that are valid for any kind of attacks (i.e. adversarial attacks), tight thanks to the PACBayesian framework, and can be directly minimized during the learning phase to obtain a robust model on different attacks."
SP:90b72e8dc41584e38f25dff9fb2853f5b11dc8fa,"This paper proposes a probabilistic entity representation model (PERM) for logical reasoning over Knowledge Graphs (KGs). The proposed model encodes entities as a multivariate Gaussian density with mean and covariance parameters to capture its semantic position and smooth decision boundary, respectively. The model also defines the closed logical operations of projection, intersection, and union that can be aggregated using an end-to-end objective function. The proposed PERM significantly outperforms the state-of-the-art methods on various public benchmark KG datasets on standard evaluation metrics."
SP:b6184c9732dbb7eba7c20cae8869d975c428efe4,"This paper proposes a gradient-based hyperparameter optimization method for few-shot meta-learning. The proposed method is based on forward-mode differentiation with sharing (FDS), which tackles memory scaling issues with forward mode differentiation, and gradient degradation issues by sharing hyperparameters that are contiguous in time. The authors provide theoretical guarantees about the noise reduction properties of the algorithm, and demonstrate its efficiency empirically by differentiating through 10 gradient steps of unrolled optimization. "
SP:9c3a326e5ee4e862923d3bf9415f32a077db8534,"This paper proposes a method to improve the performance of neural sequence models by adding logical reasoning. In particular, the authors propose a symbolic reasoning module that can either accept or reject generations generated by a neural sequence model. The proposed method is evaluated on two tasks: robust story generation and grounded instruction-following. Results show that this approach can increase the coherence and accuracy of neurally-based generations."
SP:d77d046095e4c8336c0c76ac48cb046923230753,"This paper considers the problem of estimating the mean outcome of a treatment decision under a new treatment decision using historical data generated by a different decision rule. The authors propose a novel estimation method for OPE using deep jump learning. The key ingredient of the method lies in adaptively discretizing the treatment space using deep discretization, by leveraging deep learning and multiscale change point detection. The method is further justified by theoretical results, simulations, and a real application to Warfarin Dosing."
SP:4d085e57286fdd36143108a002d16914222c239a,"This paper proposes a variational inference framework for continuous-time hybrid dynamical systems. The model is based on a Markov jump process modulating a subordinated diffusion process. The authors provide the exact evolution equations for the prior and posterior marginal densities, the direct solutions of which are however computationally intractable. Therefore, the authors develop a new method that combines a Gaussian process approximation on the diffusion level with posterior inference for Markov Jump processes. By minimizing the path-wise Kullback-Leibler divergence, they obtain Bayesian latent state estimates for arbitrary points on the real axis and point estimates of unknown system parameters, utilizing variational expectation maximization."
SP:d1f396e691f9d331adfb7b694a99c50e8004331f,"This paper studies the impact of the spectrum of the sensing matrices on the performance of expectation propagation (EP) algorithms. The authors define a notion of “spikiness” and show the importance of the spikiness in terms of EP’s performance. In particular, they show that for phase-retrieval problems, matrices with spikier spectrums are better for EP, while in 1-bit compressed sensing problems, less spiky (flatter) spectrums offer better recoveries. "
SP:ee66604d4da9fd04826e90ccbb94f0499eba4c63,"This paper proposes a method for generalized zero-shot learning (GZSL) by progressively improving cross-domain transferability and category discriminability of visual representations. The proposed method constructs two types of prototypes: attribute prototypes and category prototypes, which record prototypical visual patterns for attributes and categories, respectively. With attribute prototypes, the proposed method alternately searches attribute-related local regions and updates corresponding attribute prototypes to progressively explore accurate attribute-region correspondence. The category prototypes are projected into multiple spaces to enlarge category margins, which further strengthens the category discrimination. Experiments on four benchmarks demonstrate the effectiveness of the proposed approach."
SP:61eb6297568c3f6869fbb03eaf6a21260de5466c,"This paper presents an end-to-end deep learning approach for removing defocus blur from a single image, so as to have an all-in-focus image for consequent vision tasks. A pixel-wise Gaussian kernel mixture (GKM) model is proposed for representing spatially variant defocus kernels in an efficient linear parametric form, with higher accuracy than existing models. A deep neural network is developed by unrolling a fixed-point iteration of the GKM-based deblurring. The proposed method is built on a lightweight scale-recurrent architecture, with a scale-Recurrent attention module for estimating the mixing coefficients in GKKM for defocus debLurring. Extensive experiments show that the proposed method outperforms existing defocusing methods, but also has its advantages in terms of model complexity and computational efficiency."
SP:18bf447c90935c373e5ec4cdfbbf8f2a273d2edb,"This paper proposes a method for self-supervised video representation learning (SSVRL) that is both storage- and computation-efficient and effective. In particular, the authors propose to directly decode RGB frames and motion vectors from compressed videos on-the-fly to improve the storage and computation efficiency. The authors also propose a cross guidance contrastive learning algorithm based on multi-instance InfoNCE loss to enhance the representation ability of the motion vectors, hence the effectiveness of the method. Comprehensive experiments on two downstream tasks show that the proposed method yields new SOTA performance compared with the existing methods."
SP:8c7b1d976d9758cd534c565ec31a23f97892e503,This paper proposes an extension of the cubic spline kernel for Bayesian neural networks (BNNs) to the infinite-width limit. The authors show that the resulting model is asymptotically maximally uncertain far away from the data while the BNNs’ predictive power is unaffected near the data. The paper also shows that the extended model approximates a full GP posterior and can be applied post-hoc to any pre-trained ReLU BNN at a low cost.
SP:e77276f61626e896f6a985296f1d832129242cdf,"This paper considers the problem of identifying the best causal effect estimator among a set of possible estimators of the causal effect of some exposure on an outcome of interest. The authors propose to use the best-arm-identification bandit framework, where the goal is to identify the estimator with the lowest asymptotic variance in as few samples as possible. They introduce new tools for constructing finite-sample confidence bounds on estimates of the variance that account for the estimation of potentially complex nuisance functions, and adapt the algorithms of LUCB and Successive Elimination to use these bounds. They validate their method by providing upper bounds on the sample complexity and an empirical study on artificially generated data."
SP:471361588bfc6c6033631509d1e43e77fd9721ce,"This paper proposes ErrorCompensatedX, a general method for error compensation for variance-reduced stochastic gradient descent (SGD). Theoretical analysis shows that adding the previous step’s compression error, as done in existing work, does not fully compensate the compression error. The proposed method uses the compressed error from the previous two steps to achieve the same asymptotic convergence rate with the training without compression. Moreover, it provides a unified theoretical analysis framework for this class of variance reduced algorithms."
SP:3b7ff0dc668cac2191d95fcc4dc6e0335dec3206,"This paper proposes a method to generate multi-grained explanations for graph neural network (GNN) models. The main idea is to combine the contrastive learning and class-wise generative probabilistic models. Specifically, the pre-training phase accounts for the contrastivity among different classes. Then, the fine-tuning phase adapts the explanations in the local context. Experiments on both synthetic and real-world datasets show the superiority of the explainer."
SP:9b5a62d3a2b27bc60da28980e9fb0ecdff1215c0,This paper proposes a method to generate robust counterfactual explanations on GNNs by explicitly modelling the common decision logic of GNN on similar input graphs. The explanations are naturally robust to noise because they are produced from common decision boundaries of a GNN that govern the predictions of many similar inputs. They are also counterfactually because removing the set of edges identified by an explanation from the input graph changes the prediction significantly. The method is evaluated on several public datasets.
SP:4edb870786c9cea2c6075359cb4e79b02a8e2f5f,"This paper proposes a self-supervised representation learning method for voice style transfer (VST), which can decompose and transfer voice style through a novel information bottleneck and adversarial feedback. Specifically, the proposed information bottleneck decomposes the content and style with only a small loss of content information. The discriminator is decomposed into a content discriminator and a style discriminator, which enable the model to achieve better generalization to the voice style of the converted speech. The experimental results show the superiority of the proposed method in disentanglement and transfer performance."
SP:9fbb0c6beb3f8f88972f13dcf0e1fe7db03233c7,"This paper proposes a Siamese voxel-to-BEV tracker to improve the tracking performance in sparse 3D point clouds. Specifically, it consists of a shape-aware feature learning network and a target localization network. It compresses the point cloud along z-axis through max pooling to obtain a dense BEV feature map, where the regression of the 2D center and the z- axis center can be performed more effectively. Extensive evaluation on the KITTI and nuScenes datasets shows that the method significantly outperforms the current state-of-the-art methods by a large margin."
SP:8b788c78680a54c453a04f4551436763ee57585e,"This paper proposes a novel positional encoding method for Transformer-based deep models. The proposed method is based on learnable Fourier feature mapping, which is modulated with a multi-layer perceptron. The representation is particularly advantageous for spatial multi-dimensional positions, e.g., pixel positions on an image, where L2 distances or more complex positional relationships need to be captured. The experiments show that the proposed method outperforms existing methods by both improving the accuracy and allowing faster convergence."
SP:d2ac1b6381315bce4449f09bd519f33a2a42d714,"This paper considers the problem of learning the causal MAG of a system from observational data in the presence of latent variables and selection bias. Constraint-based methods are one of the main approaches for solving this problem, but the existing methods are either computationally impractical when dealing with large graphs or lacking completeness guarantees. This paper proposes a novel computationally efficient recursive constraint-based method that is sound and complete. The key idea of the approach is that at each iteration a specific type of variable is identified and removed, which allows us to learn the structure efficiently and recursively, as this technique reduces both the number of required conditional independence tests and the size of the conditioning sets. The former substantially reduces the computational complexity, while the latter results in more reliable CI tests."
SP:49a4912ce457f5f5ec62c44fa10444af8075fabf,"This paper proposes a batch Thompson Sampling (Batch-TS) algorithm for stochastic multi-arm bandit and linear contextual bandit problems. The authors show that the proposed algorithm achieves the same (asymptotic) regret bound of a fully sequential one while carrying out only $O(log T)$ number of batch queries. To achieve this exponential reduction, the authors dynamically decide the duration of each batch in order to balance the exploration-exploitation trade-off."
SP:653a519e3c799c25e0d0b4240322642040b121a3,"This paper studies the problem of representation learning for multiple source domain adaptation (MSDA) and domain generalization (DG) problems. The authors develop upper-bounds for the general target loss in the MSDA and DG settings, whose proofs can be found in Appendix A, and define two kinds of representation: general representation and compressed representation. They further study the pros and cons of enforcing learning each representation, and conduct experiments to inspect the trade-off of these representations. "
SP:2a7bee950cd07494d59dfee60ac2e86cc0e481b1,"This paper proposes aligned structured sparsity learning (ASSL) to further reduce the redundancy of network parameters in SR networks. ASSL introduces a weight normalization layer and applies L2 regularization to the scale parameters for sparsity. To align the pruned filter locations across different layers, the authors propose a sparsity structure alignment penalty term, which minimizes the norm of soft mask gram matrix. The proposed ASSL can train efficient image SR network, named as ASSLN, with smaller model size and lower computation than state-of-the-art methods."
SP:e9830bb9e7d3ddc3bd1c2994590fdb5d8f3668be,"This paper proposes an intrinsic reward based on the prediction errors of individual Q-values as intrinsic rewards for coordinated exploration in deep cooperative multi-agent reinforcement learning (MARL). The authors leverage the insight of factorized MARL algorithms that the individual utility functions used for local execution are the embeddings of local action-observation histories, and can capture the interaction between agents due to reward backpropagation during centralized training. Therefore, the authors utilize episodic memory to exploit explored informative experience to boost policy training. The authors illustrate the advantages of their method by didactic examples, and demonstrate its significant outperformance over state-of-the-art MARL baselines on challenging tasks in the StarCraft II micromanagement benchmark."
SP:c7e33d479575c88e22282ee6fd4f978bcd3c06ed,"This paper studies the problem of list-decodable linear regression, where an adversary can corrupt a majority of the examples. The goal is to output a small list of hypothesis vectors such that at least one of them is close to the target regression vector. The main result is a Statistical Query (SQ) lower bound of d for this problem. The lower bound qualitatively matches the performance of previously developed algorithms, providing evidence that current upper bounds for this task are nearly best possible."
SP:7b258252a9063514348f5fa8d9c85afd85748747,"This paper proposes a hybrid model for predicting the patient health status and disease progression over time, where a wealth of domain knowledge is available from pharmacology. It integrates a system of expert-designed ODEs with machine-learned Neural ODE to fully describe the dynamics of the system and to link the expert and latent variables to observable quantities. The approach is evaluated on synthetic data and real-world intensive care data of COVID-19 patients."
SP:3ea9e86e5755ef84d28e3163c60531ace5d62e3a,"This paper proposes a theoretical framework for analyzing a MAML-like algorithm, assuming all available tasks require approximately the same representation. It provides risk bounds on predictors found by finetuning via gradient descent, demonstrating that the method provably leverages the shared structure. In contrast, it establishes settings where learning one representation for all tasks (i.e. using a “frozen representation” objective) fails."
SP:8ba5a2ac80f7c53f81ad008e96c033ecad14ac0d,"This paper presents Grammar-Based Grounded Lexicon Learning (G2L2), a lexicalist approach toward learning a compositional and grounded meaning representation of language from grounded data, such as paired images and texts. At the core of the approach is a collection of lexicon entries, which map each word to a syntactic type and a neuro-symbolic semantic program. Given an input sentence, the proposed approach derives the meaning of the sentence by composing lexical meanings based on syntax. The recovered meaning programs can be executed on grounded inputs. To facilitate learning in an exponentially growing compositional space, the authors introduce a joint parsing and expected execution algorithm, which does local marginalization over derivations to reduce the training time."
SP:16c458651815813efdcbe8ba1205bbddbe3e4e68,"This paper proposes a distributed stochastic Newton algorithm for homogeneous distributed convex optimization, where each machine can calculate the gradients of the same population objective and the Hessian-vector products. The authors show that their method can reduce the number, and frequency, of required communication rounds compared to existing methods without hurting performance, by proving convergence guarantees for quasi-self-concordant objectives. "
SP:d7e479d59f82d4c55372a68ca7b4516f2871f346,"This paper proposes a new similarity measure named Density-aware Chamfer Distance (DCD) to measure the similarity between two point sets. It is derived from Chamfer distance (CD) and can detect disparity of density distributions and is more computationally efficient than EMD. The authors also propose a novel point discriminator module that estimates the priority for another guided downsampling step, and it achieves noticeable improvements under DCD together with competitive results for both CD and EMD on the point cloud completion task."
SP:e4b302009520770814ff2c096020b779a9fc38fe,"This paper studies the problem of knowledge distillation, a popular technique for training a small student network to emulate a larger teacher model, such as an ensemble of networks. The authors show that, while distillation can improve student generalization, it does not typically work as it is commonly understood: there often remains a surprisingly large discrepancy between the predictive distributions of the teacher and the student, even in cases when the student has the capacity to perfectly match the teacher. They identify difficulties in optimization as a key reason for why the student is unable to match the larger teacher, and also show how the details of the dataset used for distillation play a role in how closely the student matches the teacher — and that more closely matching the teacher paradoxically does not always lead to better student generalisation."
SP:895c7e03f9e4dadb94be1f39d61bf0b5e1533f4f,"This paper proposes a coreset for k-decision trees, which is a (k, \epsilon)-coreset that approximates the optimal k-tree of a given matrix $D$ and a given number $k$ of leaves. The coreset size is polynomial in $k$, and its construction takes $O(Nk)$ time. The paper also proposes a greedy algorithm for the construction of the coreset. Experimental results on sklearn and lightGBM show that the proposed coreset can improve the computation time of random forests and their parameter tuning by up to $x10$ while keeping similar accuracy."
SP:f3ece96b15ec06d703925df2061ed9694ec3bca5,"This paper studies the problem of top-m identification for misspecified linear bandit models, which is motivated by practical applications, especially in medicine and recommendation systems, where linear models are popular due to their simplicity and the existence of efficient algorithms, but in which data inevitably deviates from linearity. The authors first derive a tractable lower bound on the sample complexity of any $\delta$-correct algorithm for the general Top- m identification problem. They then describe the first algorithm for this setting, which can be both practical and adapts to the amount of misspecification. They derive an upper bound to its sample complexity which confirms this adaptivity and that matches the lower bound when \delta > 0. Finally, they evaluate their algorithm on both synthetic and real-world data, showing competitive performance with respect to existing baselines."
SP:e71c5e39b8d8d1640d6de2352ac51ddd52eea89d,"This paper proposes a method for learning disentangled graph representations with self-supervised learning. The authors first identify the latent factors of the input graph and derive its factorized representations. Then they propose a novel factor-wise discrimination objective in a contrastive learning manner, which can force the representations to independently reflect the expressive information from different latent factors. Extensive experiments on both synthetic and real-world datasets demonstrate the superiority of the proposed method against several state-of-the-art baselines."
SP:0a7edbbdabab11273689c40c517001eb46491113,This paper proposes a method to assess the robustness of deep neural networks (DNNs) to adversarial and random perturbations. The method is based on a stochastic simulation inspired by the field of Statistical Reliability Engineering (SRE). The robustness assessment is cast as a statistical hypothesis test: the network is deemed as locally robust if the estimated probability of failure is lower than a critical level. The authors derive theoretical guarantees that are nonasymptotic w.r.t. sample size. Experiments tackling large scale networks outline the efficiency of the method making a low number of calls to the network function.
SP:c1db485ff1ff9573daa421e167225654babb55ac,"This paper proposes a framework for conditional image generation. The main idea is to use multivariate polynomials to capture the higher-order auto- and cross-correlations between the two input variables, i.e., the noise variable and the conditional variable. The proposed framework is evaluated on class-conditional generation, inverse problems, edges-to-image translation, image to image translation, and attribute-guided generation."
SP:5a75bc7a3ea0ce971cfceebbc1c2434e3aa2584d,"This paper proposes a new neural network Maximum Mean Discrepancy (MMD) statistic by identifying a new connection between neural tangent kernel (NTK) and MMD. This connection enables the authors to develop a computationally efficient and memory-efficient approach to compute the MMD statistic and perform NTK based two-sample tests towards addressing the long-standing challenge of memory and computational complexity. Theoretically, such a connection allows us to understand the NTK test statistic properties, such as the Type-I error and testing power, by adapting existing theories for kernel MMD and NTK. Numerical experiments on synthetic and real-world datasets validate the theory and demonstrate the effectiveness of the proposed NTK-MMD statistic."
SP:1df2ffbbe56b8018067820980b93af2a8b57f891,"This paper proposes a variational autoencoder (VAE) based approach to extract class-dependent information as x-G(x) via a trade-off between reconstructing x and classifying x, where the former competes with the latter in decomposing x so that the latter retains only necessary information for classification in x - G(x). The authors apply it to both clean images and adversarial images and discover that the perturbations generated by adversarial attacks mainly lie in the class-independent part of x. The decomposition results also provide novel interpretations to classification and attack models. Inspired by these observations, the authors propose to conduct adversarial detection and defense respectively on x-GG(x), which outperforms the results on the original x."
SP:2789874561620ba7894c4672f935056bb911e919,"This paper proposes a federated learning (FL) algorithm based on the federated Thompson sampling (FTS) algorithm, which has promising applications such as federated hyperparameter tuning. However, FTS is not equipped with a rigorous privacy guarantee which is an important consideration in FL. This paper integrates differential privacy (DP) into FTS to preserve user-level privacy and further improves the utility of the algorithm through distributed exploration (DE). The resulting differentially private FTS with DE (DP-FTS-DE) algorithm is endowed with theoretical guarantees for both the privacy and utility and is amenable to interesting theoretical insights about the privacy-utility trade-off."
SP:be7d6b81736a2c3f89abd8771b41b18802e88832,"This paper proposes a Gaussian process-Bayesian Bernoulli Mixture model (GP-BM) for multi-label active learning (ML-AL) to capture the label correlations and choose the most informative samples for cost-effective data annotation. The BM encodes label correlations using a BayesianBernoulli mixture of label clusters, where each mixture component corresponds to a global pattern of label correlations. To tackle highly sparse labels under AL, the BM is further integrated with a predictive GP to connect data features as an effective inductive bias and achieve a feature-component-label mapping. The predictive GP predicts coefficients of mixture components that help to recover the final set of labels of a data sample. The model also outputs a predictive distribution that provides both the label prediction and their correlations in the form of a label covariance matrix. A principled sampling function is designed accordingly to capture both the feature uncertainty (through GP) and label covariances (through BM) for effective data sampling. Experiments on real-world datasets demonstrate the state-of-the-art AL performance of the proposed model."
SP:2b7270b0370c193300bcbbb5fb0a4101b3329d99,This paper proposes to use polar coordinate system to improve the end-to-end latency of lidar perception models by operating on wedge-shaped point cloud sectors rather than the full point cloud. The proposed method uses multi-scale padding from neighboring sectors: preceding sector from the current scan and/or the following sector from past scan. It also improves the core polar convolutional architecture by introducing feature undistortion and range stratified convolutions. Experimental results on the nuScenes dataset show significant improvements over other streaming based methods.
SP:7ae2c5b7d9c8a6c8f4a353606aa419929c47f31b,"This paper proposes a framework for learning structured latent variables. The authors extend the Gumbel-Max trick to define distributions over structured domains, and leverage the score function estimators for optimization. In particular, they highlight a family of recursive algorithms with a common feature they call stochastic invariant. The feature allows them to construct reliable gradient estimates and control variates without additional constraints on the model. In the experiments, they consider various structured latent variable models and achieve results competitive with relaxation-based counterparts."
SP:415d363c66a6967c1daca9dc02001b85bf7f0752,"This paper proposes a method for adapting CNN denoisers trained on large datasets to a single test image. To avoid overfitting, the proposed method optimizes a single multiplicative scaling parameter (the “Gain”) of each channel in the convolutional layers of the CNN. The authors show that GainTuning improves state-of-the-art CNNs on standard image-denoising benchmarks, boosting their denoising performance on nearly every image in a held-out test set. The adaptive improvements are even more substantial for test images differing systematically from the training data, either in noise level or image type. Finally, the method is applied to a scientific application to transmission-electronmicroscope images."
SP:90afa1102683b456bc72a54abef466326827546a,"This paper proposes a differentiable architecture for panoptic segmentation, which consists of a convolutional neural network and an asymmetric multi-way cut problem solver. The latter solves a combinatorial optimization problem that combines semantic and boundary predictions to produce a panoptical labeling. The formulation allows to directly maximize a smooth surrogate of the panopticity quality metric by backpropagating the gradient through the optimization problem. Experimental results on Cityscapes and COCO datasets show the effectiveness of the proposed approach."
SP:1952e174d9ec7b83ad1d394ece7fe77ea1f6d78d,"This paper proposes Recursive Bayesian Networks (RBNs), which generalizes and unifies PCFGs and DBNs, combining their strengths and containing both as special cases. RBNs define a joint distribution over tree-structured Bayesian networks with discrete or continuous latent variables. The main challenge lies in performing joint inference over the exponential number of possible structures and the continuous variables. This paper provides two solutions: 1) For arbitrary RBN, it generalizes inside and outside probabilities from PCFG to the mixed discrete-continuous case, which allows for maximum posterior estimates of the continuous latent variable via gradient descent, while marginalising over network structures. 2) For Gaussian RBNS, it additionally derive an analytic approximation of the marginal data likelihood (evidence) and marginal posterior distribution, allowing for robust parameter optimisation and Bayesian inference. Experiments on synthetic data and an application to the challenging task of hierarchical music analysis demonstrate the effectiveness of the proposed method."
SP:5f29b169d3e4bbaeeec85e1aeebe2094fae4be6e,"This paper proposes a constrained backpropagation (CBP) algorithm based on the pseudo-Lagrange multiplier method (LMM) to obtain the optimal set of weights that satisfy a given set of constraints. The proposed method is compatible with conventional deep learning frameworks and can be applied as a post-training method. The authors considered various types of constraints, including binary, ternary, one-bit shift, and two-bit shifts, and applied CBP to AlexNet, ResNet, and ResNet-50 on ImageNet. "
SP:3ddf8e2e108fb261bb23aec8a27a25aba7523dc1,"This paper studies the problem of active learning for Gaussian process classification (GPC). The authors propose two scenarios: discrete (pool-based) and continuous (query-synthesis) active learning scenarios. In the discrete scenario, the learner sequentially selects the best instance for labeling by optimizing an acquisition function to enhance data/label efficiency. The computation of EER-based acquisition functions is typically prohibitive as it requires retraining the GPC with every new query. Moreover, as the EER is not smooth, it can not be combined with gradient-based optimization techniques to efficiently explore the continuous instance space for query synthesis. To overcome these critical limitations, the authors develop computationally efficient algorithms for EER based active learning with GPC. The authors also derive the gradient chain rule to efficiently calculate the gradient of the acquisition function, which leads to the first query synthesis active learning algorithm implementing EER strategies."
SP:fa1fac04cd4ccb1f3eaf80807db09f9683ce6b50,"This paper studies the effect of unbounded gradients in variational autoencoders (VAEs) on over- and under-regularization and reconstruction errors, respectively. The authors show that if the ultimate goal is to avoid over-regularisation (high reconstruction errors) and underregularization (excessive latent dimensions are not pruned from the model), then an autoencoder-based energy function with infinite gradients around optimal representations is provably required per a certain technical sense. This result suggests that heuristic modifications to or constraints on the energy function may at times be ill-advised and large gradients should be accommodated to the extent possible."
SP:2611cfd6e0696a57d061687993cef1fe5c95999d,"This paper studies the regret bounds for the multi-armed bandit problem with graph feedback. The authors propose two notions of the fractional weak domination number and the k-packing independence number, which capture upper bound and lower bound for the regret respectively. They show that the two notions are inherently connected via aligning them with the linear program of the weakly dominating set and its dual. Based on this connection, they utilize the strong duality theorem to prove a general regret upper bound $O(\delta^2/\alpha)$ and a lower bound $\Omega(\log(V|) 1 3 T^2^3)$, where $\alpha$ is the integrality gap of the dual linear program. The upper bound is tight up to a $O(V\times \log^1/3)$ factor for the vertex packing problem including trees and graphs with bounded degree. Moreover, they show that for several special families of graphs, they can get rid of the (log|V|1 3) factor and establish optimal regret."
SP:e50dec57af337839cbde4b65fb7b431785fda44d,This paper proposes a neighborhood SHAP method to improve the interpretability of SHAP. The authors show that the Nadaraya-Watson estimator at x can be interpreted as an importance sampling estimator where the expectation is taken over the proposed neighbourhood. They also show that greater locality increases the number of model evaluations on the data manifold and with this the robustness of the attributions against adversarial attacks. 
SP:35bdeb78f9fe74e754177fb54b48e7399dc8590d,"This paper proposes a novel method, dubbed PlayVirtual, which augments cycle-consistent virtual trajectories to enhance the data efficiency for RL feature representation learning. Specifically, it predicts future states in a latent space based on the current state and action by a dynamics model and then predicts the previous states by a backward dynamics model, which forms a trajectory cycle. Based on this, it augments the actions to generate a large amount of virtual state-action trajectories. The proposed method is evaluated on the Atari and Deepmind Control Suite benchmarks and achieves the state-of-the-art performance."
SP:ca09e472cbcf2ac8c8c9b192a87df2ed59218210,This paper investigates how the network’s architecture impacts its robustness to noisy labels. The authors provide a formal framework connecting the robustness of a network to the alignments between its architecture and target/noise functions. The framework measures a network's robustness via the predictive power in its representations — the test performance of a linear model trained on the learned representations using a small set of clean labels. They hypothesize that a network is more robust to noise labels if its architecture is more aligned with the target function than the noise. They provide both theoretical and empirical evidence across various neural network architectures and different domains.
SP:903727fe028684623a8ccadec210e641ecffc685,"This paper proposes a data-driven reinforcement learning algorithm that learns to predict future success from transitions and success examples, without learning a reward function. The authors propose a method for classifying future events using a variant of temporal difference learning that they call recursive classification. This method satisfies a new Bellman equation, where success examples are used in place of the standard reward function term. Theoretical analysis shows that the robust example-based control objective is equivalent to minimizing the squared Hellinger distance (an fivergence). Experiments show that the proposed method outperforms prior methods that learn explicit reward functions."
SP:39ccbd5909a1d7ed212fe92d8d6843c2c70dfe1f,"This paper studies differentially private stochastic optimization (DP-SO) in convex and non-convex settings. In the convex setting, the authors provide algorithms for the l1-case with smooth losses and polyhedral constraint. For the constrained l2-case, they obtain a linear-time algorithm with rate $\mathcal{O}(\frac{1}{n1/3} + d 1/5 (nε)^2/5 )$, which matches the best existing non-private algorithm when d = O(\sqrt{n})$. They also extend their results to the lp setting, with only polylogarithmic overhead in the rates."
SP:99a476f71e6901aefe281f11fb72ff78265a5b6e,"This paper studies cooperative multi-agent bandit learning under stochastic time-varying networks, adversarial corruptions and delays. The authors propose decentralized algorithms that achieve competitive performance, along with near-optimal guarantees on the incurred group regret as well. The proposed algorithms are straightforward to implement and obtain competitive empirical performance. "
SP:d3e896a65470f2439bc7753b4f66e152306b2d6f,"This paper proposes a post-training quantization method for vision transformers to reduce the memory storage and computational costs of the model. To preserve the functionality of the attention mechanism, the authors introduce a ranking loss into the conventional quantization objective that aims to keep the relative order of the self-attention results after quantization. Moreover, they explore a mixed-precision quantization scheme by exploiting the nuclear norm of each attention map and output feature. The effectiveness of the proposed method is verified on several benchmark models and datasets, which outperforms the state-of-the-art post-train quantization algorithms."
SP:aa6b1328585b5916267a3ff4f9119e7aa4ce2bb5,"This paper studies the convergence rate of double Q-learning with a constant learning rate, which improves the previous analysis of Xiong et al. (2020) by an order of magnitude in terms of the dependence on all major parameters (1-\gamma, 1-\delta, D,L). The main contribution of this paper is to show that synchronous double Q learning attains an accurate global optimum with a time complexity of $\tilde{O}(\sqrt{L}(1-gamma)^7^2)$, where $\gamma$ is the discount factor, and $\delta$ is a parameter related to the sampling strategy for asynchronous double Qlearning. "
SP:04fd4d83717c4f7e1a4b5651a59200151f33411d,This paper studies semi-supervised out-of-distribution (OOD) detection in the setting where only a subset of labeled data and unlabeled data is available. The authors propose a new approach called STEP to improve OOD detection performance by introducing a new technique: Structure-Keep Unzipping (STU) that learns a new representation space in which OOD samples could be separated well. Theoretical results show that STU outperforms other methods by a large margin on several benchmarks.
SP:6bf8b94483b26033795b0eda9649518027f5e1c2,"This paper proposes a simple one-stage multi-task framework for visual grounding tasks. Specifically, the authors leverage a transformer architecture, where two modalities are fused in a visual-lingual encoder. In the decoder, the model learns to generate contextualized lingual queries which are then decoded and used to directly regress the bounding box and produce a segmentation mask for the corresponding referred regions. The proposed method outperforms state-of-the-art methods by a large margin on both REC and RES tasks."
SP:29b552b36696c9bda72f3ab4f31605d98880fd6b,"This paper studies the problem of multiclass boosting, in which the weak learner is assumed to belong to an easy-to-learn base class and is an agnostic PAC learner for that class with respect to the standard classification loss. The goal of the overall boosting algorithm is to learn a combination of weak hypotheses by repeatedly calling the weak learners. The authors study the resources required for boosting, especially how they depend on the number of classes k, for both the booster and weak learners, and show that the boosting algorithm itself requires O(log k) samples, as well as analyzing a variant of AdaBoost for our setting."
SP:f63b050773871338c48b778c362172e4b72477a4,"This paper proposes an embedding-based approach for unsupervised object segmentation and object-centric scene generation. The proposed method is based on embeddings of pixels are clustered in a differentiable fashion using a stochastic stick-breaking process. Similar to iterative refinement, this clustering procedure also leads to randomly ordered object representations, but without the need of initializing a fixed number of clusters a priori. This is used to develop a new model, GENESIS-V2, which can infer a variable number of object representations without using RNNs or iterative refining. Experiments on synthetic and real-world datasets demonstrate the effectiveness of the proposed method."
SP:408deb9e5577ee7118b836fee77135df641fe545,"This paper proposes a method for online prediction sets that are robust to changes in the marginal distribution of the data. The authors build on ideas from conformal inference to provide a general wrapper that can be combined with any black box method that produces point predictions of the unseen label or estimated quantiles of its distribution. The approach is simple, in that it requires only the tracking of a single parameter that models the shift, and general as it can be used with any modern machine learning algorithm that produces points predictions or estimated response quantiles for the response. The paper shows that over long time intervals ACI achieves the target coverage frequency without any assumptions on the data-generating distribution. Moreover, when the distribution shift is small and the prediction algorithm takes a certain simple form, ACI additionally obtain approximate marginal coverage at most time steps."
SP:e6e5b1e2428abcf1a163ec1cce15cd299f9a544f,"This paper proposes a method for multi-person pose estimation in crowded scenes. The main idea is to use a Pose-level Inference Network (PINet) to infer the complete pose cues for a person from his/her visible body parts. The PINet first applies the Part-based Pose Generation (PPG) to generate multiple coarse poses for each person, refined by the Pose Refinement module through incorporating pose priors, and finally fused in the Pose Fusion module. Experiments on several crowded scenes pose estimation benchmarks demonstrate the superiority of PINet."
SP:e76f048c3dccffcb8bcc6a66f6165fc19d175610,This paper proposes a new algorithm for solving robust Markov decision processes (RMDPs) with L∞-constrained rectangular ambiguity sets. The proposed algorithm combines a novel homotopy continuation method with a bisection method to solve S-rectangular ambiguity in quasi-linear time in the number of states and actions. The algorithm improves on the cubic time required by leading general linear programming methods. The experimental results confirm the practical viability of the method and show that it outperforms a leading commercial optimization package by several orders of magnitude.
SP:c4af66a64a5c2bd58ca2e29dbc4b27d5bf4b63b8,"This paper studies the problem of learning-augmented online algorithms with weak predictions for the online knapsack problem, where the prediction is in the form of knowing upper and lower bound for the number of items of each value. The paper systematically derives online algorithms that attain the best possible competitive ratio for any fixed prediction; they also extend the results to more general settings such as generalized one-way trading and two-stage onlineknapsack. The results show that even seemingly weak predictions can be utilized effectively to provably improve the performance of online algorithms. "
SP:1d478d4fa3f5df0ded963ef164325667fd744dbb,"This paper proposes a new model-based episodic memory of trajectories to address the limitations of episodic control. The memory estimates trajectory values, guiding the agent towards good policies. The authors also construct a complementary learning model via a dynamic hybrid control, unifying model based, episodic and habitual learning into a single architecture. Experiments demonstrate that the proposed model allows significantly faster and better learning than other strong reinforcement learning agents across a variety of environments."
SP:551174c1266b5f4b6aaf5432a4c713386f90898c,"This paper proposes a new semi-supervised learning (SSL) method for unlabeled data. The proposed method is based on data programming (DP) scheme to generate probabilistic labels for unlabelled data. Different from existing DP methods that rely on human experts to provide initial labeling functions (LFs), the authors develop a multiple-choice learning (MCL) based approach to automatically generate LFs from scratch in SSL style. With the noisy labels produced by the LFs, the authors design a label model to resolve the conflict and overlap among the noise labels, and finally infer the labels for the unlabeling data. Extensive experiments on four standard SSL benchmarks show the effectiveness of the proposed method."
SP:d1d6a40a8bde62a21da4fc18a076e344c84ab0d0,"This paper proposes a multi-view pose transformer (MVPT) for estimating multi-person 3D poses from multi-views images. The key idea is to represent skeleton joints as learnable query embeddings and let the model progressively attend to and reason over the input images to directly regress the actual 3D joint locations. To improve the accuracy of such a simple pipeline, MVP presents a hierarchical scheme to concisely represent query embedding of multi-people skeleton joints and introduces an inputdependent query adaptation approach. Also, a novel geometrically guided attention mechanism, called projective attention, is proposed to more precisely fuse the cross-view information for each joint. "
SP:2e147bd5321e25bb27d2531fd58c46460a1e5320,"This paper studies the problem of learning the support of unknown sparse vectors from noisy measurements. The authors propose a learning model that is a generalization of support recovery and approximate recovery problems, well-studied under the framework of 1-bit compressed sensing. The main contribution of the paper is to prove the existence of learning algorithms for the first problem which work without any assumptions. Under a mild structural assumption on the unknown vectors, the authors also show the existence and the query complexity of the second problem. "
SP:e3388e479a825be429f3a878e2c4d8b05903ff10,"This paper studies the bandit quickest changepoint detection (BQCD) problem, where the goal is to detect abrupt statistical changes in a data stream. The authors derive an information-theoretic lower bound on the detection delay for a general class of finitely parameterized probability distributions. They then propose a computationally efficient online sensing scheme, which seamlessly balances the need for exploration of different sensing options with exploitation of querying informative actions. They derive expected delay bounds for the proposed scheme and show that these bounds match the lower bounds at low false alarm rates, establishing optimality of the proposed method."
SP:268260e9452ba2bc57e50a6b7b3328233137ac9b,"This paper considers the problem of stochastic nested optimization, which is a generalization of the non-nested problems. The authors propose an algorithm that unifies several SGD-type updates into a single SGD approach that they term ALternating Stochastic Gradient dEscenT (ALSET) method. By leveraging the hidden smoothness of the problem, this paper presents a tighter analysis of ALSET for stochastically nested problems. Under the new analysis, to achieve an -stationary point of the nested problem, it requires O(\epsilon^{-2}) samples in total. Under certain regularity conditions, applying the proposed algorithm to the min-max and reinforcement learning problems either improves or matches the best-known sample complexity in the respective cases."
SP:82ad52361bc5b2c421f1dc6b76e1a5520570fc6c,"This paper proposes a Siamese Sampling and Reasoning (SiaSamRea) approach for video question answering (VQA), which consists of a sampling mechanism to generate sparse and similar clips (i.e., siamese clips) from the same video, and a novel reasoning strategy for integrating the interdependent knowledge between contextual clips into the network. The reasoning strategy consists of two modules: (1) siamesese knowledge generation to learn the inter-relationship among clips; (2) knowledge reasoning to produce the refined soft labels. Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance on five VideoQA benchmarks."
SP:160022e2cd61159da92f92e85520b7062a337a8d,"This paper proposes a method to reduce the computational and memory complexity of a large class of structured models by introducing a low-rank constraint. The central inference step is viewed as a matrix-vector product and using a low rank constraint, we can trade off model expressivity and speed via the rank. Experiments on language modeling, polyphonic music modeling, unsupervised grammar induction, and video modeling show that our approach matches the accuracy of standard models at large state spaces while providing practical speedups."
SP:238592ad73927194cdf0c0cf9ae2e48ca86e182c,"This paper proposes a novel uncertainty measure for exploration in deep contextual bandits. The uncertainty measure is a frequentist quantity that only depends on the value prediction of each action. The authors show theoretically that the uncertainty measure estimated by SAU matches the uncertainty provided by Thompson Sampling, as well as its regret bounds. They also show empirically that SAU-based exploration outperforms current state-of-the-art deep Bayesian bandit methods on several real-world datasets at modest computation cost. "
SP:ffc5b18f7e18607b2934e5aa199e7542005d79f4,"This paper proposes a method to disentangle the dynamic behavioral factors (pose) from time-invariant, non-behavioral nuisance factors (context) in a deep autoencoder, and exploit the temporal structures of pose dynamics. The authors also combine DBE with a stochastic temporal model to propose Variational Disentangled Behavior Embedding (VDBE), an end-to-end approach that learns meaningful discrete behavior representations and generates interpretable behavioral videos. Compared to competing approaches, DBE and VDBE enjoy superior performance on downstream tasks like fine-grained behavioral motif generation and behavior decoding."
SP:bf78a450e4aad6b87fdeb8ec0d68adaaff7b595b,"This paper introduces DMTET, a deep 3D conditional generative model that can synthesize high-resolution 3D shapes using simple user guides such as coarse voxels. It marries the merits of implicit and explicit 3D representations by leveraging a novel hybrid 3D representation. Unlike the current implicit approaches, which are trained to regress the signed distance values, the proposed model directly optimizes for the reconstructed surface, which enables the synthesizer to synthesize finer geometric details with fewer artifacts. The core of the model includes a deformable tetrahedral grid that encodes a discretized signed distance function and a differentiable marching tetrahedra layer that converts the implicit signed distance representation to the explicit surface mesh representation. This combination allows joint optimization of surface geometry and topology as well as generation of the hierarchy of subdivisions using reconstruction and adversarial losses defined explicitly on the surface mesh."
SP:2bc0bd6aa2a12691b16145f0d23542c4c86e3a44,"This paper proposes sliced mutual information (SMI) as a surrogate measure of dependence. SMI is defined as an average of MI terms between one-dimensional random projections. The authors show that it preserves many of the structural properties of classic MI, while gaining scalable computation and efficient estimation from samples. Furthermore, SMI can grow as a result of deterministic transformations. This enables leveraging SMI for feature extraction by optimizing it over processing functions of raw data to identify useful representations thereof."
SP:e220b348901b476c2afd95f97630fb5400582f40,"This paper proposes a computationally efficient two-step lookahead constrained Bayesian optimization acquisition function (2-OPT-C) for solving constrained black-box optimization problems. The proposed method is based on a likelihood-ratio-based unbiased estimator of the gradient of the optimal acquisition function. In numerical experiments, the proposed method achieves improved query efficiency by 2x or more over previous methods. "
SP:51fbd861422647912f275b48861ea3c4812afdc8,"This paper introduces Multi-Dimensional Distributional DQN (MD3QN), which extends distributional RL to model the joint return distribution from multiple reward sources. The authors prove the convergence for the joint distributional Bellman operator and build an empirical algorithm by minimizing the Maximum Mean Discrepancy (MMD) loss over joint return distributions and its Bellman target. In experiments, the authors show that the proposed method accurately models the joint returns in environments with richly correlated reward functions and outperforms previous RL methods utilizing multi-dimensional reward functions in the control setting."
SP:1f85c93d6bbfd65bf497c92c9cd534d799753097,"This paper proposes a method for 3D surface reconstruction from volumetric images. The method is based on a flow-based model that predicts a dense 3D flow field from a 3D image and uses a flow ODE to compute diffeomorphic mapping for each vertex in the template mesh. The proposed method is evaluated on the task of brain surface reconstruction, which is an essential step for brain morphometry in neurodegenerative diseases and disorders. "
SP:2f31d9cf4ad17ad08344439ca0aef7ec91944545,"This paper studies the problem of machine unlearning, i.e. the task of removing the influence of deleted data points from trained models at a cheaper computational cost than fully retraining those models. In this paper, the authors give a general reduction from deleting guarantees against adaptive sequences to deletion guarantees against non-adaptive sequences, using differential privacy and its connection to max information. They show in theory how prior work for non-convex models fails against adaptive deletion sequences, and use this intuition to design a practical attack against the SISA algorithm of Bourtoule et al. on CIFAR-10, MNIST, Fashion-MNIST."
SP:7150006590e268ab732c9be6c9048f67a377f956,This paper studies risk-averse Bayes-adaptive reinforcement learning (BARL). The authors propose to optimize the conditional value at risk (CVaR) of the total return in Bayes adaptive Markov decision processes (MDPs) in order to mitigate epistemic and aleatoric uncertainty. The authors reformulate the problem as a two-player stochastic game and propose an approximate algorithm based on Monte Carlo tree search and Bayesian optimisation. The proposed algorithm outperforms two baseline methods on two domains.
SP:a94f39406f73d7483ddd744ed2f03c78b8bc5d44,"This paper studies the behavior of shallow ReLU networks trained with the logistic loss via gradient descent on binary classification data where the underlying data distribution is general, and the (optimal) Bayes risk is not necessarily zero. In this setting, it is shown that gradient descent with early stopping achieves population risk arbitrarily close to optimal in terms of not just logistic and misclassification losses, but also the calibration, meaning the sigmoid mapping of its outputs approximates the true underlying conditional distribution arbitrarily finely. Moreover, the necessary iteration, sample, and architectural complexities of this analysis all scale naturally with a certain complexity measure of the true conditional model. "
SP:a9c786cbb61e1f10f3542161b13e43a1a68ab34d,This paper proposes a method to detect coordinated accounts on social media based on neural temporal point processes (NTPs). The authors propose a variational inference approach to learn a Gibbs distribution of group assignment based on how consistent an assignment is to (1) the account embedding space and (2) the prior knowledge. The proposed method is evaluated on the COVID-19 Vaccine Tweets dataset and the detection result suggests presence of suspicious coordinated efforts.
SP:b5c6e967a26a02861db2ecd620e9061db0c03e59,"This paper studies the problem of binary classification on disjoint smooth manifolds with low-dimensional structure, where the goal is to correctly classify every point on each of the sub-manifolds. The main contribution is the analysis of this problem for a nontrivial class of manifolds: one-dimensional smooth curves that are non-intersecting, cusp-free, and without antipodal of points. The analysis proceeds by a reduction to dynamics in the neural tangent kernel (NTK) regime where the network depth plays the role of a fitting resource in solving the classification problem. In particular, the paper shows that when the network is sufficiently deep, the NTK can be locally approximated by a translationally invariant operator on the manifolds and stably inverted over smooth functions, which guarantees convergence and generalization."
SP:8f6bee3be43df6b6e80804974014caaafe08c49e,"This paper proposes a new auxiliary classifier generator for conditional generative adversarial networks (cGANs). The authors identify that ACGAN training is prone to collapse at the early stage of training as the number of classes increases and propose two remedies: (1) projecting input vectors onto a unit hypersphere and (2) data-to-data cross entropy loss (D2D-CE) to exploit relational information in the class-labeled dataset. The proposed ReACGAN achieves state-of-the-art generation results on CIFAR10, Tiny-ImageNet, CUB200, and ImageNet datasets."
SP:080e80746a87228b156408ff649ab7a17f44e92d,"This paper proposes an extension of the policy space response oracles (PSRO) algorithm for two-player zero-sum games, which is guaranteed to converge to an approximate Nash equilibrium linearly in the number of infostates. Unlike PSRO, which mixes best responses at the root of the game, XDO mixes best response at every infostate. The authors also introduce Neural XDO (NXDO), where the best response is learned through deep RL. In tabular experiments on Leduc poker and Oshi-Zumo, the authors show that tabular XDO achieves a lower exploitability than PSRO with the same amount of computation."
SP:bda04facef4f34679fc4e17b8ea1aae74c3d649f,"This paper proposes a permutation-invariant variational autoencoder for graph structured data. The proposed model indirectly learns to match the node order of input and output graph, without imposing a particular node order or performing expensive graph matching. The authors demonstrate the effectiveness of the proposed model for graph reconstruction, generation and interpolation and evaluate the expressive power of extracted representations for downstream graph-level classification and regression tasks."
SP:e17ea6aeba78c9dfc25596d8b35a2a4f1f1f6763,"This paper proposes to decouple the depth and scope of GNNs – to generate representation of a target entity (i.e., a node or an edge), we first extract a localized subgraph as the bounded-size scope, and then apply a GNN of arbitrary depth on top of the subgraph. Theoretically, the proposed approach improves the GNN expressive power from the perspectives of graph signal processing (GCN), function approximation (GraphSAGE), and topological learning (GIN). Empirically, on seven graphs (with up to 110M nodes) and six backbone GNN architectures, the design achieves significant accuracy improvement with orders of magnitude reduction in computation and hardware cost."
SP:4890f251db559a0a572afc66e0c1f899b577d9ff,"This paper shows that any log-concave distribution can be approximated using well-conditioned affine-coupling flows. In particular, the authors show that the Jacobian of each affine block is triangular, so that the determinant can be calculated in linear time. The paper also provides theoretical evidence for the benefits of Gaussian padding when training normalizing flows. The results also inform the practice of training affine couplings."
SP:5ffa81488ed1092deb89bd5e150fa146325057ce,"This paper proposes a method to solve the problem of coupons allocation within a fixed budget while maximizing users’ retention on the e-commerce platform. Specifically, the authors propose a budget constrained offline reinforcement learning and evaluation with λ-generalization (BCORLE(lambda)) framework. The proposed method can help enterprises develop a coupons allocation policy which greatly improves user’s retention rate on the platform while ensuring the cost does not exceed the budget. "
SP:6b04cc7b4e45b9e65a1d34c15e3f75a2ef27d601,"This paper proposes a method for source-free domain adaptation (SFDA), where the source pretrained model is adapted to the target domain in the absence of source data. The method is based on the observation that target data, which might no longer align with the source domain classifier, still forms clear clusters. To capture this intrinsic structure, the authors define local affinity of the target data and encourage label consistency among data with high local affinity. The authors observe that higher affinity should be assigned to reciprocal neighbors, and propose a self regularization loss to decrease the negative impact of noisy neighbors. Furthermore, to aggregate information with more context, they consider expanded neighborhoods with small affinity values."
SP:ac1bf04ff782e5892a0bc5fe5949848ca8e731c2,"This paper proposes a method for learning representations from sets. The proposed method is based on an end-to-end trainable Euclidean embedding for sliced-Wasserstein distance to learn from set-structured data effectively. The method is evaluated on a variety of set-based tasks, including point-cloud, graph, and image classification tasks, and shows superior performance over existing set representation learning approaches."
SP:6cb2f0cbc076f8680cb00411790629f8e1478053,"This paper proposes a family of RNNs that can be formulated using stochastic bilevel optimization (SBO). The authors convert the SBO problem into an RNN where the feedforward and backpropagation solve the lower and upper-level optimization for learning hidden states and their hyperparameters, respectively. The authors prove that under mild conditions there is no vanishing or exploding gradient in training SBO-RNN. Empirically, the authors demonstrate the approach with superior performance on several benchmark datasets."
SP:d3a4300e21ca215334f256f0467a428470548fe4,"This paper studies the problem of dynamic power management (DPM) in the context of the ski rental problem, i.e., the online problem of deciding whether to continue renting skis for one more day or to buy them for the next day. The authors propose a learning-augmented online algorithm that makes decisions based on (potentially inaccurate) predicted lengths of the idle periods. The algorithm’s performance is near-optimal when predictions are accurate and degrades gracefully with increasing prediction error, with a worst-case guarantee almost identical to the optimal classical online algorithm for the problem. "
SP:22aba6284123af0ecd6605ee4e89b351bd7e10a3,"This paper proposes a transferability measure for multi-source transfer learning problems with both the task similarities and the sample complexity of learning models taken into account. The authors consider the setup where the models learned from different tasks are linearly combined for learning the target task, and use the optimal combining coefficients to measure the transferability. Then, they demonstrate the analytical expression of this measure, characterized by the sample sizes, model complexity, and the similarities between source and target tasks, which provides fundamental insights of the knowledge transferring mechanism and the guidance for algorithm designs. In addition, they develop an alternating iterative algorithm to implement their theoretical results for training deep neural networks in multi- source transfer learning tasks."
SP:0fb8dcf15e0d43547d566fdba7bc70b3bb600005,This paper proposes an image-computation model (eccNET) for visual search. The model takes a target and a search image as inputs and produces a sequence of eye movements until the target is found. The proposed model integrates eccentricity-dependent visual recognition with target-dependent top-down cues. The authors compared the model against human behavior in six paradigmatic search tasks that show asymmetry in humans. They hypothesized that the polarity of search asymmetry arises from experience with the natural environment. They tested this hypothesis by training the model on augmented versions of ImageNet where the biases of natural images were either removed or reversed.
SP:f0cc968ea9da4884dcdaf6d0c75ea9f1511bdfc3,"This paper studies the problem of training certifiable robust models against adversarial examples. The authors identify that the smoothness of the loss landscape is important for building certifiably robust models, in addition to the tightness of upper bound on the worst-case loss over the allowed perturbation. They also propose a new certifiable training method with tighter bounds and a favorable loss landscape. The proposed method achieves a decent performance under a wide range of perturbations."
SP:a158f8772a9dada059ffd1d6d7838ed40d8483da,"This paper considers the problem of online linear regression in the stochastic setting. The authors derive high probability regret bounds for online ridge regression and the forward algorithm. This enables them to compare online regression algorithms more accurately and eliminate assumptions of bounded observations and predictions. The study advocates for the use of the forward algorithms in lieu of ridge due to its enhanced bounds and robustness to the regularization parameter. Moreover, the authors explain how to integrate it in algorithms involving linear function approximation to remove a boundedness assumption without deteriorating theoretical bounds. This modification in linear bandit settings yields improved regret bounds."
SP:17ff9a2133aebf2d1b1787e8efc49d709389c0e7,"This paper proposes a fast extragradient method for nonconvex-nonconcave minimax problems. The proposed method is built on top of two existing methods, namely EG+ and EAG, which have been shown to converge at a rate of $O(1/k)$ on the squared gradient norm. The main contribution of this paper is that it proposes a two-time-scale variant of EG with anchoring that has a fast $O(\frac{1}{\sqrt{k}})$ rate on the gradient norm, where $k$ denotes the number of iterations. This paper further develops its backtracking line-search version, named FEG-A, for the case where the problem parameters are not available. "
SP:4e38973033de24fc183c6112e1146f8eef0ddaea,"This paper studies uniformity testing for statistical data that consists of rankings over m items, where the alternative class is restricted to Mallows models. It shows that uniform distribution can be distinguished from Mallows model with O(m 1/2) samples based on simple pairwise statistics, which allows us to test uniformity using only two samples, if the number of items is large enough. The paper also considers uniformity tests with central and local differential privacy (DP) constraints. The central DP algorithm requires O(max{1/\�0, 1/p m}), where $\�0 is the privacy budget parameter. The local DP algorithm is straightforward to apply to the local DP scenario, since it works with binary statistics that are extracted from the ranking data."
SP:99a835191a3ba8372e391b6d3316e9b68e543295,"This paper proposes a general greedy score-based algorithm for learning directed acyclic graphs (DAGs). Unlike edge-greedy algorithms such as the popular GES and hill-climbing algorithms, the proposed approach is vertex greedy and requires at most a polynomial number of score evaluations. Theoretical analysis is provided to show the connection between the proposed algorithm and existing order-based algorithms for learning DAGs. Finally, extensive experiments are provided to support the theoretical results."
SP:b60989706296b963b6671c01f22384978a334be1,This paper proposes neural architecture dilation for adversarial robustness (NADAR) to improve the robustness of convolutional neural networks (CNNs) against adversarial attacks. The main idea of the paper is to optimize the architecture from increasing robustness and decreasing accuracy in order to achieve a trade-off between accuracy and robustness. Theoretical analyses on the standard and adversarial error bounds are provided and the proposed method is evaluated on CIFAR-10/100 and ImageNet datasets.
SP:77ed765e911a4e5f2bfba13cbd2403500a5d05e6,"This paper studies model-based reward-free reinforcement learning with linear function approximation for episodic Markov decision processes (MDPs), where the transition probability kernel of the MDP can be parameterized by a linear function over certain feature mappings defined on the triplet of state, action, and next state. The authors propose a new provably efficient algorithm, called UCRL-RFE under the Linear Mixture MDP assumption, where the exploration phase consists of two phases: exploration phase and planning phase. They show that to obtain an $\epsilon-optimal policy for arbitrary reward function, UCRLRFE needs to sample at most $\tilde{O}(H^5d^2\eps^2)$ episodes during exploration phase, where H is the length of the episode, d is the dimension of the feature mapping, and the horizon is the number of episodes. The upper bound matches the lower bound in terms of the dependence on $H$ and $d$. The authors also propose a variant of UCRL RFE using Bernstein-type bonus and show that it needs $O(Hd(H + d)^2$ episodes to achieve an $\eps$ optimal policy."
SP:28563ba0975f56ddb662cd46e85de78bb6024d36,"This paper proposes a method to predict future events in a streaming data stream of events with seasonal patterns that evolve over time. The proposed method, called Shifting Seasonal Matrix Factorization (SSMF), learns multiple seasonal patterns (called regimes) and is able to detect regime shifts in seasonal patterns as the data stream evolves. The method works in an online setting, i.e., processes each observation in constant time and memory; and it effectively realizes regime shifts without human intervention by using a lossless data compression scheme. The authors demonstrate that the proposed method outperforms state-of-the-art baseline methods by accurately forecasting upcoming events on three real-world data streams."
SP:e4bb07033001be4d04695ef058f426d49fe440be,"This paper proposes a novel neural network architecture, WeaveNet, for solving assignment problems. The core module, feature weaving layer, is stacked to model frequent communication between elements in a parameter-efficient way for solving the combinatorial problem of assignment. The experimental results showed its impressive performance among the learning-based baselines."
SP:8a559e21d45661eef427b310e5fe8488d5749137,"This paper systematically studies the impact of various self-supervised learning proxy tasks on different architectures and threat models for 3D point clouds with adversarial training. Specifically, it studies MLP-based, convolution-based (DGCNN), and transformer-based 3D architectures. Experiments demonstrate that appropriate applications of self supervision can significantly enhance the robustness in 3d point cloud recognition. The analysis reveals that local feature learning is desirable for adversarial robustness since it limits the adversarial propagation between the point-level input perturbations and the model’s final output. This insight also explains the success of DGCNN and the jigsaw proxy task in achieving stronger 3D adversarial attacks."
SP:657c5a1114c0d054b9e767d85990bbbb0492912d,"This paper proposes to speed up the computation of projections of submodular base polytopes by reusing structural information from previous minimizers. Specifically, the authors propose to use the away-step Frank-Wolfe algorithm to use this information and enable early termination. Theoretical results show that the proposed method can improve the runtime of computing certain Bregman projections by a factor of $\Omega(n/log(n))$ for cardinality-based submodularity. "
SP:8dae43d6b5cebb7ef6c39437d997b390c2380536,"This paper considers the problem of learning the natural parameters of a k-parameter minimal exponential family from i.i.d. samples in a computationally and statistically efficient manner. The authors focus on the setting where the support and natural parameters are appropriately bounded. They provide finite sample guarantees to achieve an (`2) error of $\alpha$ in the parameter estimation with sample complexity $poly(k/\alpha)$ and computational complexity $\mathcal{O}(\frac{k}{\alpha}(k)$. They also show that, at the population level, their method can be viewed as the maximum likelihood estimation of a re-parametrized distribution belonging to the same class of exponential family."
SP:4f9ddb697e86356fb293ef34a69ca3702c4e8164,"This paper proposes a hybrid differentiable renderer that combines rasterization and ray-tracing through an efficient deferred rendering framework. The framework builds on top of DIB-R and integrates physics-based lighting and material models to capture challenging non-Lambertian reflectance under unknown poses and illumination. The method is versatile and supports both single-bounce ray tracing and a spherical Gaussian representation for a compact approximation of direct illumination, allowing to adapt and tune the shading model based on the radiometric complexity of the scene. The authors validate the technique on both synthetic and real images and demonstrate superior performance on reconstructing realistic materials BRDFs and lighting configurations."
SP:6ac1c8556e7131939cc582f513bc9921470e1b09,"This paper proposes a differentiable training method to train neural networks with soft-argmax, which imposes implicit constraints to the shape of the probability map by minimizing the expectation of the localization error. The expectation can be approximated by calculating the average error of all samples drawn from the output distribution. Samples can be drawn from categorical distributions, which can be solved by differentiable sampling from continuous distributions. The proposed method is evaluated on various localization tasks."
SP:478c05c90090f9d80b72ac352c488073b45a5d8b,"This paper proposes a directed graph data augmentation method called Laplacian perturbation to generate contrastive views for graph contrastive learning. The proposed method is based on the approximate directed graph Laplancian where a teleport probability is introduced to control the degree of approximation. Moreover, the proposed method dynamically learns from all possible contrastive view generated by the perturbed graph. Then, the model is trained using multi-task curriculum learning to progressively learn from multiple easy-to-difficult views. Experiments on various benchmarks demonstrate the effectiveness of the proposed approach."
SP:85b383d2f722f7bff438840e423f5cb4c67d5980,"This paper proposes a multi-environment Symbolic Interactive Language Grounding benchmark (SILG), which unifies a collection of diverse grounded language learning environments under a common interface. The authors propose the first shared model architecture for RL on these environments, and evaluate recent advances such as egocentric local convolution, recurrent state-tracking, entity-centric attention, and pretrained LM using SILG. The shared architecture achieves comparable performance to environment-specific architectures. Moreover, the best models significantly underperform humans on the proposed benchmark."
SP:23c8db56f59f778fe812a5dd161f7a1f21c3cdba,"This paper proposes a sparse version of the Vision Transformer (ViT) that is scalable and competitive with the largest dense networks while requiring as little as half of the compute at inference time. The authors also propose an extension to the routing algorithm that can prioritize subsets of each input across the entire batch, leading to adaptive per-image compute. Finally, the authors demonstrate the potential of V-MoE to scale vision models, and train a 15B parameter model that attains 90.35% on ImageNet."
SP:c5235f41dfb8b5cc478f11c5d5e0ab0b8676871e,"This paper studies the optimization landscape of training narrow neural networks. The authors prove that as long as the width m > 2n/d, there exists at least one global minimizer with zero training loss, and identify a nice local region with no local-min or saddle points, but it is not clear whether gradient descent can stay in this nice region. The paper also considers a constrained optimization formulation where the feasible region is the nice region, and proves that every KKT point is a near-global minimizer. It is expected that projected gradient methods converge to KKT points under mild technical conditions."
SP:0be529f5254afd59dcfa6b34a359c7037e7a8323,"This paper proposes a continuous mean-covariance bandit (CMCB) model to explicitly take into account option correlation. Specifically, in CMCB, there is a learner who sequentially chooses weight vectors on given options and observes random feedback according to the decisions. The agent’s objective is to achieve the best trade-off between reward and risk, measured with option covariance. To capture different reward observation scenarios in practice, the authors consider three feedback settings, i.e., full-information, semi-bandit and full bandit feedback. They propose novel algorithms with optimal regrets (within logarithmic factors), and provide matching lower bounds to validate their optimalities. The experimental results also demonstrate the superiority of the algorithms."
SP:472a90bb175b0286765c5a47b040e1a58f594a05,"This paper proposes an extension of the Multiplicative Update (MMU) algorithm for computing Positive Semidefinite (PSD) factorization of a matrix, which is a generalization of the Nonnegative Matrix Factorization (NMF) problem in which we seek a collection of r-dimensional non-negative vectors satisfying the condition Xij = tr(AiBj) for all i, j in [m,n]. The authors show that the MMU algorithm ensures that updates remain PSD by congruence scaling with the matrix geometric mean of appropriate PSD matrices, and it retains the simplicity of implementation that the multiplicative update algorithm for NMF enjoys. They show that under their update scheme the squared loss objective is non-increasing and fixed points correspond to critical points. The analysis relies on Lieb’s Concavity Theorem. Beyond PSD factorizations, the authors also use the algorithm as a primitive to calculate blockdiagonal PSD and tensor PSD."
SP:83abd6d149d88cc6e96cbc4d488e4fe9dc2a4fcb,"This paper proposes a meta-learning framework for domain generalization (DG) that extends beyond the invariance view to further capture the usefulness of domain-specific information. The key insight is to disentangle features in the latent space while jointly learning both domain-invariant and domainspecific features in a unified framework. The proposed method, called mDSDI, optimizes the domain specific representation to adapt from source domains, targeting a robust generalization on unseen domains. The authors empirically show that the proposed method outperforms the state-of-the-art techniques in DG."
SP:4191474c75e2fedf514f0f3001a67a047eb74c30,This paper proposes to improve the performance of diffusion models (DMs) by improving their architecture and by using classifier guidance to improve their sampling quality. The proposed method achieves state-of-the-art performance on unconditional image synthesis and conditional image synthesis tasks. The authors also show that the proposed method is able to match the quality of GANs with as few as 25 forward passes.
SP:fe3cab08596cde4c14ecf6fca8d0f95b02bab229,"This paper proposes a method for few-shot learning that leverages out-of-distribution data to improve the performance of the model. The proposed method is based on the idea of counter-examples, i.e., unlabeled samples coming from outside the target classes, to drive the classifier to avoid irrelevant features by maximizing the distance from prototypes to out of distribution samples while minimizing that to in-dist distribution samples. The method is simple to implement, agnostic to feature extractors, lightweight without any additional cost for pre-training, and applicable to both inductive and transductive settings. Extensive experiments on various standard benchmarks demonstrate that the proposed method consistently improves the performance."
SP:b1f65724926f136979829b7a6c870bc31f38f591,"This paper studies the problem of prioritized sampling in experience replay for off-policy reinforcement learning. The authors propose two methods to compute the prioritization weight, namely ReMERN and ReMERT. Theoretically, the authors show that there is an optimal prioritization strategy for Bellman update that can directly maximize the return of the policy. They also provide theoretical justifications for previous criteria, such as TD error, recentness and corrective feedback, and propose two new methods to calculate the prioritisation weight. "
SP:601ebf30b3c6aa35fcef49633aa8eb0acd0f2c66,This paper considers the problem of sequential prediction with expert advice in a nonstationary environment with long-term memory guarantees in the sense of Bousquet and Warmuth [4]. The authors give a linear-time algorithm that improves on the best known regret bounds. This algorithm incorporates a relative entropy projection step. This projection is advantageous over previous weight-sharing approaches in that weight updates may come with implicit costs as in for example portfolio optimization. 
SP:b2439973063e827b3cbe92306a2fdee3286b6b44,"This paper considers the problem of contextual linear bandits, which is motivated by routing applications in navigational engines and recommendation systems. In this setting, the learner wishes to learn a hidden d-dimensional value $w^*$ from a subset of actions $X_t$ of possible actions, and the goal is to minimize the total distance between the true point w^* and the hyperplanes the separation oracle returns. The authors design algorithms for this problem which achieve regret $O(d \log T)$ and $O(\log T^2/\log d)$, respectively. They also consider the variant where we are allowed to provide a list of several recommendations, and they give an algorithm with regret $\log d$ and list size poly(d)."
SP:abe83c7e0bcf4829742609d709637e2f84d8a4d9,"This paper proposes a method for gradual automated machine learning (AutoML) by introducing a set of combinators for composing machine learning operators into pipelines. It describes a translation scheme from pipelines and associated hyperparameter schemas to search spaces for AutoML optimizers. The paper presents Lale, an open-source sklearn-compatible AutoML library, and evaluates it with a user study."
SP:0d7f1cae577ed598048b64617e85ca6bd5c6d7fa,"This paper proposes a meta-learning approach that learns a weight initialization such that a small number of weight changes results in low generalization error. The authors show that this form of meta learning can be improved by letting the learning algorithm decide which weights to change, i.e., by learning where to learn. They find that patterned sparsity emerges from this process, with the pattern of sparsity varying on a problem-by-problem basis. This selective sparsity results in better generalization and less interference in a range of few-shot and continual learning problems. Moreover, sparse learning also emerges in a more expressive model where learning rates are meta-learned."
SP:05037e1850003a725a466b64d3e32aa2aed458fb,"This paper proposes a new approach to identify the shared components of neural responses in a multi-view learning setting. The proposed approach models each view as a linear transform of shared independent components contaminated by additive Gaussian noise. The authors show that this model is identifiable if the components are either non-Gaussian or have enough diversity in noise variances. They then show that in some cases multi-set canonical correlation analysis can recover the correct unmixing matrices, but that even a small amount of sampling noise makes Multiset CCA fail. To solve this problem, the authors propose to use joint diagonalization after MultisET CCA, leading to a new method called ShICA-J, which is based on second-order statistics, and a maximum-likelihood method that is both more accurate and more costly. In addition, ShICA comes with a principled method for shared components estimation."
SP:44dd1faa1813c433fd7581d05cae3df440bfb93e,"This paper studies the problem of training agents that can collaborate well with human partners in common-payoff games without using human data. The authors argue that the crux of the problem is to produce a diverse set of training partners. To this end, the authors propose to train an agent partner as the best response to a population of self-play agents and their past checkpoints taken throughout training, a method they call Fictitious Co-Play (FCP). The experiments focus on a two-player collaborative cooking simulator that has recently been proposed as a challenge problem for coordination with humans. They find that FCP agents score significantly higher than SP, PP, and BCP when paired with novel agent and human partners."
SP:21c84bd720b1e90ea0f88fbf8fd24dbcb49b547c,"This paper proposes a new actor-critic method for cooperative multi-agent reinforcement learning (MARL). The main idea is to use a centralised but factored critic, which combines per-agent utilities into the joint action-value function via a non-linear monotonic function, as in QMIX. The centralised policy gradient estimator is used to optimize over the entire joint action space, rather than optimising over each agent’s action space individually as in MADDPG. This allows for more coordinated policy changes and fully reaps the benefits of a centralized critic."
SP:1c8351b8a6cdf1212840388e19a596729b3bfda4,"This paper proposes a biologically plausible key-value memory network that stores inputs using a combination of biologically plausible three-factor plasticity rules. The network performs on par with classical Hopfield networks on auto-associative memory tasks and can be naturally extended to continual recall, hetero-associationative memory, and sequence learning. The results suggest a compelling alternative to the classical hopfield network as a model of biological long-term memory."
SP:7ad6da2c63859d64970e9b35326e9ceab48add47,"This paper studies the problem of pairwise learning, where the loss function depends on a pair of instances. The authors propose simple stochastic and online gradient descent methods for the problem. The main idea is to only pair the current instance with the previous one in building a gradient direction, which is efficient in both the storage and computational complexity. They develop novel stability results, optimization, and generalization error bounds for both convex and nonconvex as well as both smooth and nonsmooth problems."
SP:cb11dacc930d71a616ee2fbe4acfae030f9dca59,"This paper presents REDO, a class-agnostic framework to REconstruct the Dynamic Objects from RGBD or calibrated videos. Compared to prior work, the problem setting is more realistic yet more challenging for three reasons: 1) due to occlusion or camera settings an object of interest may never be entirely visible, but REDO aims to reconstruct the complete shape; 2) it is able to handle different object dynamics including rigid motion, non-rigid motion, and articulation; 3) it can reconstruct different categories of objects with one unified framework. To address these challenges, the authors develop two novel modules. First, they introduce a canonical 4D implicit function which is pixel-aligned with aggregated temporal visual cues. Second, they develop a 4D transformation module which captures object dynamics to support temporal propagation and aggregation. The efficacy of REDO is evaluated on synthetic RGBD video datasets SAIL-VOS 3D and DeformingThings4D++, and on real-world video data 3DPW."
SP:8ae97752e74b4395774575009031abcb6ba5cea7,"This paper provides a non-asymptotic analysis of linear stochastic approximation (LSA) algorithms with fixed stepsize. The analysis is based on new results regarding moments and high probability bounds for products of matrices which are shown to be tight. The authors derive high-probability bounds on the performance of LSA under weaker conditions on the sequence {(An,bn) : n 2 N⇤} than previous works. However, in contrast, they establish polynomial concentration bounds with order depending on the stepsizes. "
SP:86c1e937755e35efafecc09dfe2606ffb1653a41,"This paper extends the options framework for temporal abstraction in reinforcement learning from discounted Markov decision processes (MDPs) to average-reward MDPs. The authors propose general convergent off-policy inter-option learning algorithms, intra-option algorithms for learning values and models, as well as sample-based planning variants of the learning algorithms. They also extend the notion of option-interrupting behavior from the discounted to the average reward formulation. They show the efficacy of the proposed algorithms with experiments on a continuing version of the Four-Room domain."
SP:7e4e1e20e7c253d02c6ae58457fb30029f130f0c,"This paper studies the performance of visual transformers (VTs) on small-data regime. The authors empirically analyse different VTs, comparing their robustness in a small training set regime, and show that, despite having a comparable accuracy when trained on ImageNet, their performance on smaller datasets can be largely different. Moreover, the authors propose an auxiliary self-supervised task which can extract additional information from images with only a negligible computational overhead. The task is used jointly with the standard (supervised) training and it does not depend on specific architectural choices, thus it can be easily plugged in the existing VTs."
SP:0132ef17585e293b23e9dc45189c0989d829b52a,"This paper proposes a new method for label-free alignment of hierarchical datasets in hyperbolic spaces. The proposed method consists of three components: translation, scaling, and rotation, which are based on the Riemannian geometry of the Lorentz model of the hyperboloid space. The authors analyze the proposed components, highlighting their useful properties for alignment. The efficacy of HPA, its theoretical properties, stability and computational efficiency are demonstrated in simulations. "
SP:3580ac64f09e3021de5d4c92411bcc0f3c5d10f3,"This paper studies the problem of differentially private (DP) query answering systems that are required to produce micro-data, i.e., the output of the DP system that produces the query data. The authors show that DP systems that do not require the micro-dataset are subject to a logarithmic error in the accuracy of the sum query (i.e. the set of queries that is equal to the sum of all the queries in the population) and the accuracy for the sub-populations (the point queries). The authors also show that the trade-off between accuracy for a population of interest (sum query) vs. accuracy for its component sub-population (point queries) depends on the amount of data that is provided to DP systems. The paper also provides lower bounds for pure, approximate, and concentrated DP systems and proposes mitigation strategies."
SP:c0e64dc8acfaed3e4d7745af12fd34003d0e5017,"This paper proposes a method to improve goal-conditioned reinforcement learning (RL) by training a planner and an RL agent together. The planner learns to decompose a long-horizon task to a tree of sub-tasks, whose layers construct coarse-to-fine sub-task sequences as plans to complete the original task. The RL agent is trained to minimize the RL agent’s cost of completing the sequence in each layer from top to bottom layers, which gradually increases the sub- tasks and thus forms an easy- to-hard curriculum for the planner. The bottom-up traversal of the tree trains the agent from easier sub-samples with denser rewards on the bottom layers to harder ones on the top layers and collects its cost to train the planner in the next episode. The authors compare the proposed method with RL, planning, and their combination (SoRB) on navigation and continuous control tasks."
SP:9911693a04a300b5a93634fb0267ef83e5489d77,This paper proposes a Bayesian framework for generating local explanations for LIME and KernelSHAP. The main idea is to use a closed form expression for the number of perturbations and hyperparameters to estimate the values of critical levels to generate explanations that satisfy the desired confidence levels. The proposed approach is evaluated on several real-world datasets and user studies.
SP:5efb4b81bd37c70640e8768e9dfb5bba14a0cfb8,"This paper proposes a method to improve the performance of Adder neural networks (ANNs). The authors claim that unordered heavy tails in ANNs could be the key component which prevents ANNs from achieving superior classification performance since fatter tails tend to overlap in feature space. The authors propose to pre-define Multivariate Skew Laplace distributions and embed the feature distributions into the loss function, which can be fully controlled and designed for various properties. Experiments conducted on several benchmarks and comparison with other distributions demonstrate the effectiveness of proposed approach for boosting the performance."
SP:cbccb65457564992d534504c0d060da44cafce8c,"This paper provides a theoretical explanation for the phenomenon of Gradient Starvation (GS) in over-parameterized neural networks. The authors identify simple properties of learning dynamics during gradient descent that lead to this imbalance, and prove that such a situation can be expected given certain statistical structure in training data. Based on the proposed formalism, the authors develop a novel but simple regularization method aimed at decoupling feature learning dynamics, improving accuracy and robustness in cases hindered by gradient starvation."
SP:8f6fe37cb0a332b66e10cc00261a44622841c8c6,"This paper studies the relationship between humans and AI agents in the cooperative card game Hanabi. The authors evaluate both rule-based and learning-based Hanabi AI in human-machine collaborative games. They also quantify subjective measures of human’s perceived performance, teamwork, interpretability, trust, and overall preference of AI teammate. They find that humans have a clear preference toward a rule based AI agent (SmartBot) over a learning based AI (Other-Play) across nearly all subjective metrics, and generally view the learning based agent negatively, despite no statistical difference in the game score."
SP:2a05e333fc1a14057515ef3addde9a40152373db,"This paper proposes a method for visual question generation (VQG) that leverages visual and answer hints to improve the quality of generated questions. The proposed method is based on the idea that the salient visual regions of interest can be viewed as a constraint to improve generation procedure for producing high-quality questions. Given the predicted salient visual region of interest, the model can focus on estimating the probability of being ground-truth questions, which in turn implicitly measures the quality. Experimental results on two benchmark datasets show that the proposed method outperforms the state-of-the-art approaches by a large margin on both automatic machine metrics and human evaluation."
SP:15756d6ef47b39ded404acea2135c93bd5ee1062,"This paper proposes a generalized data weighting (GDW) method to mitigate label noise and class imbalance by manipulating gradients at the class level. Specifically, GDW unrolls the loss gradient to class-level gradients by the chain rule and reweights the flow of each gradient separately. Extensive experiments in various settings verify the effectiveness of GDW."
SP:7a8f56a01bec51ebf70d9ff689005a62cccfe5c6,"This paper introduces a spatio-temporal language grounding task where the goal is to learn the meaning of language descriptions of behavioral traces of an embodied agent. This is achieved by training a truth function that predicts if a description matches a given history of observations. The descriptions involve time-extended predicates in past and present tense as well as spatiotemporal references to objects in the scene. To study the role of architectural biases in this task, the authors train several models including multimodal Transformer architectures; the latter implement different attention computations between words and objects across space and time. They test models on two classes of generalization: 1) generalization to randomly held-out sentences; 2) generalisation to grammar primitives. They observe that maintaining object identity in the attention computation of our Transformers is instrumental to achieving good performance on generalization overall, and that summarizing object traces in a single token has little influence on performance."
SP:3d4a9d439bc84c3b0e6600f6985a23bdf95cd67f,"This paper proposes a method for multiple object tracking and segmentation (MOTS) based on cross-attention. The proposed method first distills a space-time memory into a set of prototypes and then employs cross-Attention to retrieve rich information from the past frames. To segment each object, the proposed method adopts a prototypical appearance module to learn a sets of contrastive foreground and background prototypes, which are propagated over time. Extensive experiments demonstrate that the proposed approach outperforms the state-of-the-art methods on Youtube-VIS and BDD100K datasets."
SP:1175ad16382b349ab1a39895150172d266abe571,"This paper studies the relationship between gradient descent and gradient flow in deep neural networks. The main idea is to view gradient descent as an approximate numerical solution to the initial value problem of gradient flow, and show that the degree of approximation depends on the curvature around the gradient flow trajectory. The authors then show that over deep neural network with homogeneous activations, gradient flow trajectories enjoy favorable curvature, suggesting they are well approximated by gradient descent. This finding allows them to translate an analysis of gradient flows over deep linear neural networks into a guarantee that gradient descent efficiently converges to global minimum almost surely under random initialization."
SP:b8412e9ce82ce92125fe7cd3aff7bea8b906d16e,"This paper considers a stochastic multi-armed bandit (MAB) problem with delayed impact of actions. In this setting, actions taken in the past impact the arm rewards in the future. This delayed impact is prevalent in the real world. This paper generalizes the bandit setting to encode the dependency of this “bias” due to the action history during learning. The goal is to maximize the collected utilities over time while taking into account the dynamics created by the delayed impacts of historical actions. The authors propose an algorithm that achieves a regret of $\tilde{O}(\KT^2/3)$ and show a matching regret lower bound of $\Omega(\sqrt{T})$."
SP:9c1d678dff5f609197dc3cfb67b841827f4a439a,This paper proposes an end-to-end solution for video instance segmentation (VIS) based on transformers. The key idea is to utilize concise memory tokens as a means of conveying information as well as summarizing each frame scene. The features of each frame are enriched and correlated with other frames through exchange of information between the precisely encoded memory tokens. The proposed method achieves state-of-the-art performance (AP 42.6 on YouTube-VIS 2019 val set using the offline inference) while having a considerably fast runtime (89.4 FPS).
SP:6c922eaa358f6fb9771690b1240e4f6f08a35b69,"This paper proposes a general graph embedding method that can debias various structural biases in graphs by using random graphs. The proposed method, called residual2vec, is based on the optimization algorithm Skip-gram Negative Sampling (SGNS). The authors show that SGNS has an implicit bias arising from the skip-gram negative sampling, which happens to negate the bias due to the friendship paradox. To leverage this debiasing feature further, the authors propose a more general framework, residual1vec, that can also compensate for other systematic biases in random walks. The authors demonstrate that the proposed method performs better than conventional embedding methods in link prediction and community detection tasks."
SP:851eac96135b577a5014166edcb43db6a190cf4b,"This paper studies the problem of estimating the power sum functional of a discrete distribution p = (p1,..., pK) in the context of local differential privacy (LDP). The authors consider two settings: (i) sequentially interactive (i.e., they are allowed to use already published confidential data) or (ii) non-interactive (i), where the data is publicly available. In the interactive setting, the authors propose two plug-in type estimators that are similar to the MLE analyzed by Jiao et al. [18] in the multinomial model. However, due to the privacy constraint, the rates are slower and similar to those obtained in the Gaussian model by Collier et al [9]. In the non-Interactive setting, they introduce a two-step procedure which attains the parametric rate (nα2)-1/2 when $\gamma > 2$. The authors also give lower bounds over all α-LDP mechanisms and all estimators using the private samples."
SP:a0408b54f88a26479f33f36bb27e0a675f637ccd,"This paper studies the problem of online multiclass classification in the setting where the learner’s feedback is determined by an arbitrary directed graph. This setting generalizes the well-known online learning settings of experts (where G is the complete graph, including self-loops) and bandits. The authors introduce GAPPLETRON, a new algorithm that works with arbitrary feedback graphs. They prove surrogate regret bounds that hold, both in expectation and with high probability, for a large class of surrogate losses. They also prove a general lower bound of order max { BK, \sqrt{T} } showing that the upper bounds are not significantly improvable. Experiments on synthetic data show that for various feedback graphs our algorithm is competitive against known baselines."
SP:490262589efce6fb10b913431ec6db8d4e5b2dec,"This paper studies the problem of explainable clustering in the setting first formalized by Dasgupta, Frost, Moshkovitz, and Rashtchian (ICML 2020). A k-clustering is said to be explainable if it is given by a decision tree where each internal node splits data points with a threshold cut in a single dimension (feature), and each of the k leaves corresponds to a cluster. The authors give an algorithm that outputs an explainable cluster that loses at most a factor of $O(\log k)$ compared to an optimal (not necessarily explainable) clustering for the k-medians objective, and $\Omega(k)$ for the $k$-means objective. The algorithm is remarkably simple. In particular, given an initial non-explainable initial clustering, it is oblivious to the data points and runs in time $O(dk log k)$, independent of the number of data points n. "
SP:6a9e47be710ddaf386bffc54d003d7dc2b67fdc3,"This paper proposes a multilingual language model (PrLM) that supports both explicit universal dependency parsing and implicit language modeling. Syntax in terms of universal dependency parse serves as not only pre-training objective but also learned representation in the model, which brings unprecedented PrLM interpretability and convenience in downstream task use. The model outperforms two popular multilingual PrLM, multilingual-BERT and XLM-R, on cross-lingual natural language understanding (NLU) benchmarks and linguistic structure parsing datasets."
SP:94f4b65214a648cbc84f13beba45a825e2e9901a,"This paper proposes a new Transformer-based method for solving vehicle routing problems (VRPs). The authors argue that the existing positional encoding (PE) method is not suitable for representing VRP solutions and propose to learn embeddings for the node and positional features separately, instead of fusing them together as done in existing ones, so as to avoid potential potential noises and incompatible correlations. Moreover, the positional features are embedded through a novel cyclic positional encoding method to allow Transformer to effectively capture the circularity and symmetry of cyclic sequences. The authors train DACT using Proximal Policy Optimization and design a curriculum learning strategy for better sample efficiency. They apply DACT to solve the traveling salesman problem (TSP) and capacitated vehicle routing problem (CVRP). Results show that DACT outperforms existing Transformer based improvement models and exhibits much better generalization performance across different problem sizes on synthetic and benchmark instances."
SP:e5c8680d8da9e7548fcb9bb5c073848eb80e1dd0,"This paper proposes a method to compute the exact Bayes error of generative models learned using normalizing flows. The method relies on a fundamental result, which states that the Bayesian error is invariant under invertible transformation. The authors then use this result to compute exact Bayesian errors for the learned flow models by computing it for Gaussian base distributions, which can be done efficiently using Holmes-Diaconis-Ross integration. Moreover, the authors show that by varying the temperature of the learned flows, they can generate synthetic datasets that closely resemble standard benchmark datasets, but with almost any desired Bayes errors. They use their approach to conduct a thorough investigation of state-of-the-art classification models. "
SP:2896679f0472522bc3334178cd7574494cf12b7b,"This paper proposes an initialization method for neural networks that is architecture-agnostic and can be applied to both Adam and SGD optimizers. The proposed method, GradInit, is based on a simple heuristic; the norm of each network layer is adjusted so that a single step of SGD or Adam with prescribed hyperparameters results in the smallest possible loss value. This adjustment is done by introducing a scalar multiplier variable in front of each parameter block, and then optimizing these variables using a simple numerical scheme. GradInit accelerates the convergence and test performance of many convolutional architectures, both with or without skip connections, and even without normalization layers. It also improves the stability of the original Transformer architecture for machine translation."
SP:f69731403592fa5bdd4ca327708582d615aa131c,"This paper proposes a new approach to predict disease progression using longitudinal data. The proposed approach is based on the Riemannian manifold and learns patient-specific trajectories distributed around a central geodesic. The authors extend this approach by learning the metric from the data allowing more flexibility while keeping the interpretability. Specifically, they learn the metric as the push-forward of the Euclidean metric by a diffeomorphism. The metric update allows them to improve the forecasting of imaging and clinical biomarkers in the Alzheimer’s Disease Neuroimaging Initiative (ADNI) cohort."
SP:438e906f52c4c0538956b51a2270b3ac498b27a8,"This paper proposes a routing-by-memory mechanism for existing CNN architectures. In each stage of the network, parallel Procedural Units (PUs) are introduced, which consists of a memory head and a procedure. The memory head maintains a summary of a type of features. For an intermediate feature, we search its closest memory and forward it to the corresponding procedure in both training and testing. In this way, different procedures are tailored to different features and therefore tackle them better."
SP:d240173080cd3647dbaa5173a6422396f226775b,"This paper studies equivariant polynomial functions, i.e. functions that are universal approximations of polynomials that satisfy the symmetries of physical laws (translation, rotation, reflection, lift, lift-up, etc.). The authors show that the space of functions that satisfy equivariance can be expressed in terms of a lightweight collection of scalar products and scalar contractions of the scalar, vector, and tensor inputs. The paper also provides numerical examples that show that this scalar-based method is simple, efficient, and scalable."
SP:72c0f47566904deb27d8157da30807ec1d6b5685,"This paper proposes a new loss function for bbox regression and object detection. The main idea is to generalize the Intersection over Union (IoU) loss to a new family of power IoU losses, which have a power i.i.d. term and an additional power regularization term with a single power parameter alpha. The authors analyze properties such as order preservingness and loss/gradient reweighting. Experiments on multiple object detection benchmarks and models demonstrate the effectiveness of the proposed loss function."
SP:397125177d7007316d67194ec00d5dc57b44ac79,"This paper proposes Distributionally Robust Imitation Learning (DROIL), a method for learning a policy in a Markov Decision Process (MDP) setting where the reward function is not given, but demonstrations from experts are available. The authors show that DROIL can be seen as a framework that maximizes a generalized concept of entropy. They develop a novel approach to transform the objective function into a convex optimization problem over a polynomial number of variables for a class of loss functions that are additive over state and action spaces. Their approach lets them optimize both stationary and non-stationary policies and, unlike prevalent previous methods, it does not require repeatedly solving an inner reinforcement learning problem. They experimentally show the significant benefits of DROIL on synthetic data and a highway driving environment."
SP:58f220bbbed8d3e0633b408fca3b6838c4ad323d,"This paper proposes a general post-processing algorithm for individual fairness (IF) based on graph smoothing. The authors cast the individual fairness problem as a graph-smoothing problem corresponding to graph Laplacian regularization that preserves the desired “treat similar individuals similarly” interpretation. Theoretical results demonstrate the connection of the new objective function to a local relaxation of the original individual fairness. Empirically, the proposed algorithms correct individual biases in large-scale NLP models such as BERT, while preserving accuracy."
SP:ef791aa29decd839e7e583c9d1f71e8309ca87ef,"This paper proposes a method for cross-domain text-to-text-SQL task. The authors propose a structure-aware Dual Graph Aggregation Network (SADGA) for the question-schema linking problem. SADGA adopts the graph structure to provide a unified encoding model for both the natural language question and the database schema, and further devise a structure aware aggregation method to learn the mapping between question-graph and schema-graph. Extensive experiments are conducted to validate the effectiveness of the method."
SP:a2fa25a4539a38af61a0993f65ecc14339f26c2e,"This paper studies the problem of learning stochastic computations graphs with multiple sequential discrete components. The authors show that it is challenging to optimize the parameters of these models, mainly due to small gradients and local minima. They propose two strategies to overcome these challenges. First, increasing the scale parameter of the Gumbel noise perturbations during training improves the learning behavior. Second, they propose dropout residual connections for discrete-continuous computation graphs in Section 2.2. By randomly skipping some discrete distributions, they provide more informative gradients throughout the computation graph."
SP:bb3ec363e90269db4a2ba99d8107cb56f86e68f0,"This paper studies the robustness of Bayesian neural networks (BNNs) to covariate shift. The authors show that BNNs with high-fidelity approximate inference via full-batch Hamiltonian Monte Carlo (HMC) achieve poor generalization under covariate shifts, even underperforming classical estimation. They also show why the same issue does not affect many approximate inference procedures, or classical maximum a-posteriori (MAP) training. Finally, the authors propose novel priors that improve BNN robustness to many sources of covariances shift."
SP:f86ec7042e9b73ae071704a6d3ed17d7e3da1b75,"This paper categorizes meta-learning evaluation into two settings: in-distribution (ID), in which the train and test tasks are sampled iid from the same underlying task distribution, and out-of-dist distribution (OOD). The authors identify that most existing few-shot classification benchmarks instead reflect OOD evaluation, as they use disjoint sets of train (base) and test (novel) classes for task generation. This discrepancy is problematic because—as the authors show on numerous benchmarks—meta-learning methods that perform better on existing OOD datasets may perform significantly worse in the ID setting. In addition, in the OOD setting, even though current FSL benchmarks seem befitting, the study highlights concerns in 1) reliably performing model selection for a given meta- learning method, and 2) consistently comparing the performance of different methods. To address these concerns, the authors suggest how to construct FSL datasets to allow for ID evaluation and more reliable OOD evaluations."
SP:371f77148b4f00a929f7c118b1bb7c5a6238d264,"This paper proposes a method for open rule induction based on the pre-trained language model (LM). The authors argue that the current LM-based rule generation methods are “learning rules from rules”, which limits these methods to only produce “canned” rules whose patterns are constrained by the annotated rules, while discarding the rich expressive power of LMs for free text. Therefore, the authors propose to induce open rules utilizing the knowledge in LMs. Besides, they propose the Orion (open rule induction) system to automatically mine open rules from LMs without supervision. The authors conduct extensive experiments to verify the quality and quantity of the inducted open rules."
SP:8be2e0ea4a83fe32a4859f456007a829e5e9270a,This paper proposes Implicit Constraint Q-learning (ICQ) for offline multi-agent reinforcement learning. The main idea is to use a SARSA-like approach to evaluate Q-values and then convert the policy learning into a supervised regression problem. Experimental results show that the extrapolation error is controlled within a reasonable range and insensitive to the number of agents. ICQ achieves the state-of-the-art performance in the challenging multiagent offline tasks.
SP:1939b24b68970c33ca16ce238deed257f76d009e,"This paper proposes a method for adversarial robustness certification based on non-uniform perturbations. The method is motivated by the fact that in many real-world applications such as malware detection and credit risk prediction, features have some semantically meaningful dependencies. The authors propose a method to generate perturbation bounds that take into account these feature dependencies during adversarial training. The proposed method is evaluated on malware classification, credit-risk prediction, and spam detection."
SP:417b30930b245667d777e5d90ee80dd41546760e,"This paper considers the problem of learning with kernels, where the goal is to find an estimator $\theta$ that minimizes the expected risk $L_\theta = \sum_i \in \mathbb{R}^{\mathcal{X} \times Y$ on some unknown probability distribution $p_x,p_y$ on the set $x_i$ and $y_i=1,...,n$ that are i.i.d. samples drawn from some unknown distribution $\phi$. The authors consider generalized self concordant loss functions (GSC), which contain, e.g., the logistic loss. They show that fast and optimal rates can be achieved for GSC by using the iterated Tikhonov regularization (IT) scheme, which is intrinsically related to the proximal point method in optimization, and overcomes the limitation of the classical Tikhonskov regularization."
SP:1caeee4f00b52fe356ff4e5dd004d0203e838370,"This paper introduces a new kind of linear transform named Deformable butterfly (DeBut) that generalizes the conventional butterfly matrices and can be adapted to various input-output dimensions. It inherits the fine-to-coarse-grained learnable hierarchy of traditional butterflies and when deployed to neural networks, the prominent structures and sparsity in a DeBut layer constitutes a new way for network compression. The authors apply DeBut as a drop-in replacement of standard fully connected and convolutional layers, and demonstrate its superiority in homogenizing a neural network and rendering it favorable properties such as light weight and low inference complexity, without compromising accuracy. The natural complexity-accuracy tradeoff arising from the myriad deformations of DeBut also opens up new rooms for analytical and practical research."
SP:d345ce1d7afc367ee1a9fb68d50ff1b2219f02cb,"This paper proposes MetA Reusable Knowledge (MARK), a method to address the catastrophic forgetting problem in continual learning (CL) by encouraging weight updates that are useful for multiple tasks, and enriching the knowledge base with new knowledge as the model learns new tasks. Specifically, Mark keeps a set of shared weights among tasks and uses a metalearning approach to incrementally enrich the knowledge and to foster weight reusability among tasks. A set of trainable masks provides the key mechanism to selectively choose from the KB relevant weights to solve each task. Mark achieves state-of-the-art results in several popular benchmarks."
SP:722c52467e384058f8fdffa254d0e8db47440a64,"This paper proposes a data-driven framework to systematically improve the use of primal heuristics in the Branch and Bound (B&B) framework for exact mixed-integer programming (MIP) solvers by learning from data describing the performance of primal-heuristics. The authors propose a problem-specific schedule of MIP-specific primal heuristic that collectively find many solutions at minimal cost. They also propose an efficient algorithm for computing such a schedule. Compared to the default settings of a state-of-the-art academic MIP solver, they are able to reduce the average primal integral by up to 49% on two classes of challenging instances."
SP:5a21f0a49731dcb1d68deb06a75138e8e9d514d5,"This paper studies reinforcement learning in the setting where the learner receives binary feedback only once at the end of an episode. The authors study the case where trajectory labels are generated by an unknown parametric model, and provide a statistically and computationally efficient algorithm to solve for the optimistic policy at every episode. They show that the algorithm achieves sublinear regret. "
SP:e66bd9582058ba0f6091bb1042ce2ecfdaae1515,"This paper proposes a novel edge representation learning framework based on Dual Hypergraph Transformation (DHT), which transforms the edges of a graph into the nodes of a hypergraph, and then applies message-passing techniques for node representations to edges. The edge representations from the hypergraphs are then used to cluster or drop edges to obtain holistic graph-level edge representations. The proposed method is evaluated on graph representation and generation performance, on which it largely outperforms existing graph representation learning methods."
SP:e398873e29b05176e1d52dc6f86a59a4f405e6fd,"This paper studies the sufficiency of state representations learned by maximizing mutual information (MI) between random variables in the context of deep reinforcement learning (RL). In particular, the authors focus on representations obtained by maximizing the mutual information between states, actions, and rewards at different time-steps. The authors provide a theoretical analysis of two commonly used objectives for representation learning based on mutual information maximization, and show that one of them is insufficient for the general class of MDPs, in the most general case, and prove that another typical objective is sufficient. The theoretical results are corroborated with empirical experiments on a simulated game environment. "
SP:50181f740910195d3a50dd7d7f8cbb1c476d730b,"This paper proposes a novel method for 3D semantic analysis based on sparse steerable convolution (SS-Conv). The main idea is to use the rotation-steerable constraint of SE(3)-equivariant convolutions (SO(3) convolutions to speed up convolution operations on sparse volumetric 3D data. The authors also propose a feature-steering module to perform iterative pose refinement. Experiments are conducted on three tasks of 3D object semantic analysis, including instance-level pose estimation, category-level 6D pose and size estimation, and category level 6D tracking."
SP:d746bfb200577c980d92727bb0b1a3c23e7bfdc5,"This paper proposes a dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input. Specifically, the authors devise a lightweight prediction module to estimate the importance score of each token given the current features. The module is added to different layers to reduce redundant tokens hierarchically. To optimize the prediction module in an end-to-end manner, they propose an attention masking strategy to differentiably prune a token by blocking its interactions with other tokens. "
SP:d0b6cde42b1cba5e6e3c7c5131426fd84adbd3d7,"This paper studies the problem of distribution-free inference, i.e., the inference of the conditional mean of the underlying regression function. The authors consider the setting where the features X are continuously distributed, where the support size of the distribution of X is smaller than the square of the sample size, and the case where X takes only a small number of possible values. They show that there are several regimes in between the finite setting and the continuous setting, where vanishing-width confidence intervals are achievable if and only if the effective support size is small. "
SP:123952325765c040c3078fc7dca2b6d370e55590,"This paper proposes Representation Neutralization for Fairness (RNF) that achieves fairness by debiasing only the task-specific classification head of DNN models. The proposed method is motivated by the empirical observation that standard training can result in the classification head capturing undesirable correlations between fairness sensitive information and specific class labels. To address low-resource settings with no access to sensitive attribute annotations, the authors leverage a bias-amplified model to generate proxy annotations for sensitive attributes. Experimental results over several benchmark datasets demonstrate the effectiveness of the proposed method."
SP:210eb2c811f966bb1ac53932cacabbad9bb608fe,"This paper proposes a new convolutional neural network architecture that is rotational equivariant, i.e. is invariant to all possible rotation angles. The proposed architecture is based on the Bessel function, which is a well-known function of Bessel equation in physics. The authors propose a new way to use Bessel functions in the convolution layer of CNNs, which they call Bessel-CNNs (B-CNN). The authors show that the proposed B-CNN architecture is more robust to rotations than previous rotational-invariant CNNs."
SP:ee51ecbd476d5b65903c942a62be89ff5d91698b,"This paper proposes a new large-scale solver for kernel ridge regression. The proposed approach combines partitioning with random projections and iterative optimization to reduce space and time complexity while provably maintaining the same statistical accuracy. In particular, constructing suitable partitions directly in the feature space rather than in the input space promotes orthogonality between the local estimators, thus ensuring that key quantities such as local effective dimension and bias remain under control. Theoretical results characterize the statistical-computational tradeoff of our model, and demonstrate the effectiveness of our method by numerical experiments on large scale datasets."
SP:1f096d6fabd5b1fde43d06c552d46d87cd35cb4a,"This paper proposes a method for discrete communication between agents in the context of reinforcement learning. The authors argue that the standard practice of using one-hot vectors as discrete communication tokens prevents agents from acquiring desirable properties of communication such as compositionality and zero-shot understanding. They propose to use discrete tokens derived from a learned, continuous space. They show in a decision theoretic framework that their technique optimizes communication over a wide range of scenarios. They also validate that their trained agents learn to cluster tokens in semantically meaningful ways, allowing them to communicate in noisy environments where other techniques fail."
SP:8630ccc627534f9033bced04e2137a897ffef701,"This paper proposes a hybrid network architecture called CoAtNets, which combines the strengths from both convolutional networks and transformers. The authors show that depthwise convolution and self-attention can be unified via simple relative attention, and vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency. Experiments show that the proposed model achieves state-of-the-art performance under different resource constraints across various datasets."
SP:d3ecbeeffa5ab365743ba8653c6739f24742ee31,"This paper proposes a new second-order oracle bound for the expected risk of a weighted majority vote. The bound is based on a novel parametric form of the Chebyshev-Cantelli inequality, which is amenable to efficient minimization. The authors also derive a new concentration of measure inequality that combines PAC-Bayesian bounding with Bennett’s inequality for empirical estimation. The paper provides an empirical evaluation demonstrating that the new bounds can improve on the existing bounds."
SP:5bac542a6532d43cf100e085398b4a4783719814,"This paper proposes a weakly-supervised method for audio-visual video parsing. The proposed method exploits both the common and diverse event semantics across videos to identify audio or visual events. In addition, the method explores event co-occurrence across audio, visual, and audio- visual streams to localize segments of target events while excluding irrelevant ones. The discovered supervisory signals across different videos and modalities can greatly facilitate the training with only video-level annotations."
SP:8fd6a03c1794afa524328d45f4232eacf6f86693,"This paper proposes a personalized federated learning algorithm called QuPeD, which allows clients to learn compressed personalized models with different quantization parameters and model dimensions/structures. The authors propose an algorithm for learning quantized models through a relaxed optimization problem, where quantization values are also optimized over. The compressed personalization framework is formulated by introducing knowledge distillation loss for local client objectives collaborating through a global model. The proposed algorithm is evaluated in the centralized setting, federated setting, and local training setting. "
SP:fca8b4f1e765cf1724a37f0ae9a7dac1cb79c8b1,"This paper proposes a deep generative model for constrained clustering. The proposed model (DC-GMM) uncovers the underlying distribution of data conditioned on prior clustering preferences, expressed as pairwise constraints. These constraints guide the clustering process towards a desirable partition of the data by indicating which samples should or should not belong to the same cluster. The model assumes a Conditional Mixture-of-Gaussians prior on the latent representation, conditioned on a Gaussian Mixture Model conditioned on the user’s clustering preference, based e.g. on domain knowledge."
SP:84379c0c881b7390ecc22fb398edfaf66d1af1ff,"This paper proposes a near-input-sparsity approximation algorithm for the Neural Tangent Kernel (NTK) and convolutional NTK (CNTK). The proposed method is based on sketching the polynomial expansions of arc-cosine kernels, which can transform any image using a linear runtime in the number of pixels. The authors also prove a spectral approximation guarantee for the NTK matrix, by combining random features (based on leverage score sampling) with a sketching algorithm. The proposed methods are evaluated on various large-scale regression and classification tasks. "
SP:fa2668083ff3bb592c29a4c6822ae96ff54d0dbe,"This paper proposes a multi-person 3D motion trajectory prediction framework for long-term human motion prediction. The proposed model consists of a local-range Transformer encoder for individual motion and a global-range encoder to model social interactions, and a Transformer decoder to perform prediction for each person by taking a corresponding pose as a query which attends to both local and global range encoder features. The model is able to outperform state-of-the-art methods on long- term motion prediction, but also generates diverse social interactions."
SP:0a0e07af37c8fe8580639b1df62d27b6f63f8dee,"This paper proposes a method for automatically generating guiding programs for long-horizon reinforcement learning (LHRL). The main idea is to use a generative model to predict the unobserved portions of the world and then synthesize a program based on samples from this model in a way that is robust to its uncertainty. The proposed method is evaluated on a set of challenging benchmarks, including a 2D Minecraft-inspired environment where the agent must complete a complex sequence of subtasks to achieve its goal, and achieves a similar performance as using handcrafted programs to guide the agent."
SP:5bb42b178b0d27da271bfa60e633fdac718638c4,"This paper studies the problem of causal imitation learning in sequential settings, where the imitator must make multiple decisions per episode. The authors develop a graphical criterion that is necessary and sufficient for determining the feasibility of causal imitators. They also provide an efficient algorithm for determining imitability and corroborate their theory with simulations."
SP:85bd81f0c5b6ccbc421ebbaf6f5c72164bc70b7f,"This paper presents a slot-wise, object-based transition model that decomposes a scene into objects, aligns them with respect to an object memory, and predicts how those objects evolve over successive frames. The model is trained end-to-end without supervision using transition losses at the level of the object-structured representation rather than pixels. The proposed model is able to deal with two issues that are not handled satisfactorily by other transition models, namely object persistence and object identity. The authors show that the combination of an objectlevel loss and correct object alignment over time enables the model to outperform a state-of-the-art baseline, and allows it to deal well with object occlusion and re-appearance in partially observable environments."
SP:f32eddbb5c33a8422c075579ff08aa9833338d44,This paper considers the problem of estimating the empirical risk minimization (ERM) of a loss function in the presence of covariate shift. The authors propose an importance sampling weighted ERM algorithm for using adaptively collected data to minimize the average of the loss function over a hypothesis class and provide first-of-their-kind generalization guarantees and fast convergence rates. Their results are based on a new maximal inequality that carefully leverages the importance sampling structure to obtain rates with the good dependence on the exploration rate in the data. 
SP:f549a0c231b71bae0acbed6e3afb41890ee89cd9,"This paper proposes a novel scheme for kernel-reweighted regression by reparametrizing the sample weights using a doubly non-negative matrix. When the weighting matrix is confined in an uncertainty set using either the log-determinant divergence or the Bures-Wasserstein distance, the authors show that the adversarially reweighted estimate can be solved efficiently using first-order methods. Numerical experiments show that our reweighting strategy delivers promising results on numerous datasets."
SP:fe12e13602925b9400fd596a987755beb10aa3d1,"This paper proposes a categorical gradient estimator based on reparameterizing categorical variables as sequences of binary variables and Rao-Blackwellization. The proposed estimator is based on importance sampling and statistical couplings, which is extended to the categorical setting. Experiments show that the proposed estimators outperform the state-of-the-art estimators in the binary setting."
SP:e16fdf963ec2f9c0d79fa404e47e7862a5d6e922,"This paper proposes a new predictor-based neural architecture search method, called WeakNAS, which is based on progressively fitting a search path towards the high-performance sub-space through a set of weaker predictors. The weak predictors are learned to predict the performance of a few well-performed architectures guided by the previously learned predictor and estimate a new better weak predictor. The proposed method produces coarse-to-fine iteration to gradually refine the ranking of sampling space. Extensive experiments demonstrate that WeakNAS costs fewer samples to find top-performance architectures on NAS-bench-101 and NAS-Bench-201."
SP:8f74abb04037ba2e59dcf8320dc555b149f68ed8,"This paper proposes a method for intrinsic control, where the goal is to maximize the effective number of states the agent can reach and to which it can reliably return. The proposed method, EDDICT (Entropic Desired Dynamics for Intrinsic ConTrol), assumes fixed additive latent dynamics, which results in tractable learning and an interpretable latent space. Compared to prior methods, the proposed method is more exploratory, as demonstrated by improved state coverage and increased unsupervised performance on hard exploration games such as Montezuma's Revenge."
SP:c731a78c3e7f98ccd0253b51a0d42bf8deeb71f9,This paper proposes a fragment-based generative RL with Explorative Experience replay for Drug Design (FREED) to generate pharmacochemically acceptable molecules with large docking scores. The proposed method constrains the generated molecules to a realistic and qualified chemical space and effectively explores the space to find drugs by coupling our fragment- based generation method and a novel error-prioritized experience replay (PER). The model produces molecules of higher quality compared to existing methods while achieving state-of-the-art performance on two of the three targets in terms of the docking scores of generated molecules. The authors also show with ablation studies that the method significantly improves the model performance.
SP:b938bca513e7de1231212064caf8877a78d8b612,This paper studies the problem of learning directed acyclic graphical models from observational data in general settings without specific distributional assumptions. The approach is information-theoretic and uses a local Markov boundary search procedure in order to recursively construct ancestral sets in the underlying graphical model. The sample complexity is at most polynomial in the number of nodes. This is applied to learn the entire graph under a novel identifiability condition that generalizes existing conditions from the literature.
SP:af08109d4c45dc9401efb0e63c22167e9da28adb,"This paper considers the setting where each user has $m$ samples and the privacy protection is enforced at the level of each user’s data. The authors show that, as long as each user receives sufficiently many samples, we can learn any privately learnable class via an ("", )DP algorithm using only $O(log(1/\epsilon^2/\eps)$ users. Specifically, they consider two settings: (1) global stability and (2) user-level privacy. In both cases, they show a nearly-matching lower bound on the number of users required. "
SP:da4f21d107a7f442c4d3e3ec13bdb44b041e07cf,"This paper provides a theoretical analysis of the performance of end-to-end model-based reinforcement learning methods, i.e., methods that learn the transition and reward models of a latent Markov decision process whose value predictions fit the data. The authors show that, for a linear parametrization, gradient descent converges to global optima despite nonlinearity and non-convexity introduced by the implicit representation. They also provide convergence rates for both cases which allow them to identify conditions under which stochastic gradient descent with this implicit representation converges substantially faster than its explicit counterpart."
SP:992aa07d4f815d1c81f967374590eece933833b1,"This paper proposes a method to improve the performance of Knowledge Graph (KG) embeddings for the task of KG refinement. In particular, the authors propose a method that iteratively combines two techniques: (1) Probabilistic Soft Logic (PSL) and Markov Logic Network (MLN) and (2) Knowledge Graph Embedding (GBE). The authors claim that the proposed method is able to use both the ontological information and the KG embedding to improve both the quality of predictions and the power of the embedding."
SP:676fc4a3041af22e8f20ccba7daa2a0b1f5d6af5,"This paper proposes a novel evaluation paradigm for knowledge base completion (KBC) tasks. The proposed evaluation paradigm is based on perturbed triples, i.e., facts where one entity is replaced with a variable, and construct corresponding sets of entities that are correct answers. The authors argue that consideration of binary predictions is essential to reflect the actual KBC quality, and propose to construct the data set FB14k-QAQ to provide more transparent model selection criteria for a realistic scenario. "
SP:83fe0a496a79bcf97ccba1c6d34b7d11e7d5c330,"This paper proposes an approach to improve the performance of pre-trained language models for dialog generation. The authors propose to use two pretrained language models, one for each role, to encode and decode the speaker utterances in an alternating fashion. The approach is evaluated on two task-oriented dialog datasets: CamRes676 and MultiWOZ, and one non-task-oriented dataset, PersuasionForGood. The results show that the proposed approach outperforms or is on par with the state-of-the-art methods on both tasks."
SP:b11c06b7c4ef1aa43c59f808a679425e302d158e,"This paper proposes a confidence measure for deep neural networks based on the notion of ""implicit loss"" (i.e., the softmax values of the network are not estimates of the probabilities of class labels). The authors prove that if an uncertainty measure is an implied loss, then low uncertainty means high probability of correct (or top-k) classification on the test set. The proposed method is simple to use on existing networks: they proposed confidence measures for Top-k which can be evaluated by binning values on the tests set."
SP:ab9666e15f2a0113d96cb4b47b1cbb30fa1f7982,"This paper studies the spectrum of the NNGP and NTK kernels in the infinite-width limit of deep neural networks. The authors show that in the large depth limit, the spectrum simplifies in much the same way as that of the Neural Network Gaussian Process (NNGP) kernel. The spectrum of NTK is also shown to have the same properties as the neural network convolutional kernel. In particular, it is shown that the spectrum is weakly data-dependent, which is a necessary condition for generalization."
SP:d3470c35aae48bf92439a55fdb98ccf07100e567,"This paper proposes a graph convolutional neural network-based method to estimate the quality of protein models for quality assessment (QA) of computational protein models. The proposed method, GRAPHQA, is based on Graph Convolutional Networks (GCN), which is a graph-based representation learning method. The authors demonstrate significant improvements over the state-of-the-art for both hand-engineered and representation-learning approaches, as well as carefully evaluating the individual contributions of each component."
SP:5188280131b58a35d3deda126a0754aea8fa6e58,"This paper studies the landscape of the loss function of linear neural networks from a geometric perspective, focusing on the relationship between the functional space of the network and its parameterization. In particular, the authors define two types of critical points: pure critical points and spurious critical points, which are determined by the geometry of the function space and by the parameterization of this space by the network’s weights. They show that the absence of “bad” local minima in the loss landscape of linear networks is due to two distinct phenomena that apply in different settings: it is true for arbitrary smooth convex losses in the case of architectures that can express all linear maps (filling architectures), but it holds only for the quadratic loss when the functional spaces is a determinantal variety. "
SP:ee71597ceab23eb4db1d6608f15f80ad51f7ff6d,"This paper proposes a general framework SEED (Sampling, Encoding, and Embedding Distributions) for inductive and unsupervised representation learning on graph structured objects. Given an input graph, SEED samples a number of subgraphs whose reconstruction errors could be efficiently evaluated, encodes the subgraph samples into a collection of sub-graph vectors, and uses the embedding vector distribution as the output vector representation for the input graph. Theoretical analysis shows the close connection between SEED and graph isomorphism. The empirical study suggests the proposed SEED framework is able to achieve up to 10% improvement, compared with competitive baseline methods."
SP:d9406fdf0a180a5efc6f15ba8739524665f0f9d2,This paper proposes a new counterfactual regret minimization (CFR) algorithm for two-player zero-sum extensive games with imperfect information (TEGI). The main idea is to split the time horizon into segments and update the strategy only at the beginning of each segment and keep the strategy the same within each segment. Theoretical analysis shows that the regret of Lazy-CFR is almost the same as that of the vanilla CFR and only needs to visit a small portion of the game tree. Empirical results consistently show that the proposed algorithm is fast in practice.
SP:023aa3dca1cf7992b22993a7088e8a74c92bb47e,"This paper proposes a method for Unsupervised Domain Adaptation (UDA) by modeling the deep features from each domain as Gaussian mixture distributions. The authors propose two new domain discrepancy losses to measure the discrepancy between the two domains. The first one minimizes the distance between the corresponding Gaussian component means of the source and target data, and the second one minimises the pseudo negative log likelihood of generating the target features from the source feature distribution. The proposed method is evaluated on the Digits Image transfer task and VisDA 2017 dataset."
SP:40be996e8bb86e887077b762b87c7c34a786ac98,"This paper proposes a conditioning method for Continuous Normalizing Flows (CNFs) that partitions the latent space into a class-specific supervised code and an unsupervised code that is shared among all classes. The supervised code is used to condition the model on the given signal, while the other one is used for the inference. The authors also propose to use gating networks to learn the error tolerances of the ordinary differential equation (ODE) solvers for better speed and performance. Experiments are conducted on CIFAR-10 and Cifar-100 datasets."
SP:97764e3393216106ff2ac3f550845acf4636119f,"This paper studies the convergence of TD learning in the lazy regime, i.e., the regime where the parameters of the model vary only slightly during the learning process. The authors consider the approximation of the value function for infinite-horizon discounted Markov Reward Processes (MRP) with nonlinear functions trained with the Temporal-Difference (TD) learning algorithm. Both in the under- and over-parametrized regimes, the authors prove exponential convergence to the global and local minimizers of the above algorithm. They also give examples of models that diverge if trained with non-lazy TD learning, and in the case of neural networks."
SP:c518e4030f12b0f59ad1d7c0fc0ebd313c68ef95,"This paper proposes a reinforcement learning approach for generating and testing hypotheses about the dynamics of the environment. The main idea is to leverage the fact that most hypotheses can be formulated as a triplet of a pre-condition, an action sequence, and a post-condition that is causally related to the pretcondition and actions. The authors propose to learn an action policy that generates observations that are relevant to verification of hypotheses and a prediction function that uses the observations to predict whether the hypothesis is true or false. The proposed approach is evaluated on a set of simulated environments."
SP:6fa2f842b1bc993ed8024a3ce13dbd91529c61be,"This paper proposes a method to predict the success of rewrites of mathematical formulas in the latent space of a theorem prover (HOList). This is done by training a graph neural network to map mathematical formulas into a fixed-dimensional latent space, which is then used to predict whether a given rewrite succeeds (i.e. returns with a new formula) and also predict the latent representation of the resulting formula. Experiments on the HOList dataset show that the proposed approach is able to predict success of the rewrite after several steps of reasoning in latent space."
SP:a77ab500a5e7d4ea8430871d1e603941e92974fd,"This paper proposes a novel approach to learning depth from images and very sparse depth measurements, just a few pixels per image. To learn from such sparse supervision, the authors introduce an appropriate inductive bias by designing a specialized global-local network architecture. Experiments on several datasets show that the proposed model can learn monocular dense depth estimation when trained with very sparse ground truth. Moreover, the global parameters extracted by the network are predictive of the metric agent motion."
SP:2afba5e24478da4e9d493887c7cf00e288cc0deb,"This paper proposes a Transformer-based approach to reduce the ambiguity in Bloom filter digests. Specifically, the authors propose to use a multi-layer Transformer to map each id to multiple hash tokens in a much smaller space, similarly to a Bloom filter, and then predict at the token level, instead of the id level. The proposed approach is evaluated on the task of masked language task, i.e., predicting out the hash tokens from BERT’s masked language model. The results show that the proposed approach outperforms the baselines."
SP:745dd86d7f7bba79a02d27922003b764b620f83e,"This paper proposes a method for zero-shot shape part discovery on 3D point clouds in unseen object categories. The proposed method is based on a learning-based agglomerative clustering framework which learns a grouping policy to progressively group small part proposals into bigger ones in a bottom-up fashion. The method is evaluated on the fine-grained 3D part dataset, PartNet, and shows that it can transfer knowledge of parts learned from 3 training categories to 21 unseen testing categories."
SP:868fc6df740b04963442d5abcfe2f4845585cfc8,"This paper proposes a technique called neuron editing that learns how neurons encode an edit for a particular transformation in a latent space of an autoencoder to generate transformed data by defining an editing transformation on those neurons. By performing the transformation in the latent space, the authors encode fairly complex and non-linear transformations to the data with much simpler distribution shifts to the neuron’s activations. The technique has the advantage of being generally applicable to a wide variety of data domains, modalities, and applications. The authors first demonstrate it on image transformations and then move to two main applications in biology: removal of batch artifacts representing unwanted noise and modeling the effect of drug treatments to predict synergy between drugs."
SP:6dee6932e64fe47bb44dd42fc242fa9d89b8d89c,"This paper studies the problem of few-shot semantic segmentation. The authors propose to meta-learn the initializations for deep neural networks that must produce dense, structured predictions given an arbitrary amount of training data for a new task. Specifically, the authors propose an extension and experimental analysis of first-order model agnostic meta-learning algorithms (including FOMAML and Reptile) to image segmentation, a novel neural network architecture built for parameter efficiency and fast learning which they call EfficientLab, a formalization of the generalization error of meta- learning algorithms, which they leverage to decrease error on unseen tasks, and a small benchmark dataset, FP-k, that contains 400 training examples for 5 tasks each. They show that the proposed method outperforms random and ImageNet-trained initializations on the FSS-1000 dataset and larger datasets."
SP:ec6f390f6d45fb79c33ae5d9c8a24cadb96fbd60,"This paper proposes a method for semi-supervised few-shot learning (SS-FSL) that builds on top of prototypical networks (PN). The proposed method, Prototypical Random Walk Networks (PRWN), is based on the idea of random walk through the embeddings of unlabeled data starting from each class, and then passing through unlabelled data in the embedding space and encouraging the same class prototypes to return to the same point in the space at the end of the walk. The method is evaluated on mini-Imagenet and Omniglot datasets, and shows better performance than the baselines."
SP:d12e687bd2ee9fa60554312e644bb0a6487974f1,"This paper proposes a new self-supervised training objective, Contrastive Sensor Fusion (CSF), which exploits coterminous data from multiple sources to learn useful representations of every possible combination of those sources. This method uses information common across multiple sensors and bands by training a single model to produce a representation that remains similar when any subset of its input channels is used. CSF is trained on a dataset consisting of 47 million scenes, each consisting of four different sensors for each scene. The representations outperform fully supervised ImageNet weights on a remote sensing classification task and improve as more sensors are fused."
SP:4d8e054f07006b4f896721b5c24da805727d2c22,"This paper compares the performance of rewinding and fine-tuning techniques for neural network pruning. In particular, the authors consider the lottery ticket hypothesis, which proposes that sparse subnetworks emerge early in training that can train in isolation to the same accuracy as the original network. The authors propose a new method, learning rate rewinds, that trains the unpruned weights from their final trained values using the same learning rate schedule as weight re-winding, and then retrains them from there using the original training schedule. The proposed method is evaluated on CIFAR-10 and ImageNet."
SP:3bb1c79f9482e09828eda45fbb2e654f37219365,"This paper proposes a new notion of margin, called ""all-layer margin"", to analyze the relationship between output margin and generalization for deep models. The authors show that it has a clear and direct relationship with generalization. This enables the following concrete applications: 1) by analyzing the all-layer margins, they obtain tighter generalization bounds for neural nets which depend on Jacobian and hidden layer norms and remove the exponential dependency on depth. 2) their neural net results easily translate to the adversarially robust setting, giving the first direct analysis of robust test error for deep networks, and 3) they present a theoretically inspired training algorithm for increasing the all layer margin."
SP:3d44f27468087280e85dfb1fc7291db05179fe6d,"This paper proposes a framework for knowledge-grounded dialogue generation with limited training data. The authors propose a disentangled response decoder to isolate the parameters that depend on knowledge from the entire generation model, so that the major part of the model can be learned from a large number of ungrounded dialogues and unstructured documents, while the remaining small parameters can be well fitted using the limited training examples. The proposed framework is evaluated on two datasets, and the results show that with only 1/8 training data, the proposed framework can achieve the state-of-the-art performance and generalize well on out-ofdomain knowledge."
SP:9b555f7fe743f5effdbdc8701ed519ce3159c4b0,"This paper proposes a new neural machine translation model (NMT) architecture that integrates the source to target translation model, the target to source translation model and two language models. The authors argue that the existing NMT approaches of exploiting non-parallel data are not necessarily the best, in both training and decoding phases. The proposed MGNMT achieves state-of-the-art results on a variety of language pairs and scenarios."
SP:d7a530a0ec4112095a58cef4cda9646f8ca6449d,"This paper investigates the role of the entropy term in the performance of deep reinforcement learning (DRL) algorithms. Specifically, the authors first show that SAC, a popular RL algorithm that combines off-policy learning with maximum-entropy RL, has a similar structure to TD3, but also employs maximum entropy reinforcement learning. Based on this insight, they propose a new algorithm called Streamlined Off-Policy (SOP) that uses the standard objective without entropy term. They also propose a simple non-uniform sampling method for selecting transitions from the replay buffer during training and show that it outperforms SAC and achieves state-of-the-art performance on challenging continuous control tasks."
SP:545e8da553fcb47d84eaa044d8a4947d3cd3230e,"This paper studies the vulnerability of copyright detection systems to adversarial attacks. In particular, the authors propose to attack a well-known music identification method and implement this system in the form of a neural network, and then attack this system using simple gradient methods. Adversarial music created this way successfully fools industrial systems, including the AudioTag copyright detector and YouTube’s Content ID system. The paper is well-written and easy to follow."
SP:b511822850da3bf1079a36ed6f5ad4db80fbc424,"This paper proposes an activation decomposition framework for visual explanation of deep metric learning and explore the relationship between each activated region by point-topoint activation response between two images. The proposed method can be directly deployed to a large range of metric learning applications and provides valuable information for understanding the model. The experiments show its effectiveness on two potential applications, i.e. cross-view pattern discovery and interactive retrieval."
SP:67bf71219fe6bedec5f5525200e734638e4a6ca2,"This paper studies the problem of continual learning in a non-stationary world with limited computational resources. The authors propose a new algorithm, Adaptive Online Planning (AOP), that combines model-based planning with model-free policy learning. The main idea is to combine the model planning method of iteratively updating a planned trajectory with the model free method of updating the weights of a neural network to reduce computation for future decision making. AOP is able to call upon more extensive planning only when necessary, leading to reduced computation times."
SP:11159cb878a436a5d4fc6edb4132f2cc3c1b3f72,"This paper proposes to replace the traditional softmax attention mechanism by two alternative sparsity-promoting transformations: sparsemax and total-variation Sparse Attention (TVMAX). With sparsemax, we obtain sparse attention weights, selecting relevant features. By selecting relevant groups of features, the TVMAX transformation improves interpretability. TVMAX outperforms the other compared attention mechanisms in terms of human rating and attention relevance."
SP:fb0c3ce3db6ad674ddc615bdc6203cdcbe42c804,"This paper proposes an encoder-decoder framework for predicting the evolution of the topology of dynamic graphs. Specifically, the authors use graph neural networks (GNNs) to capture the temporal evolution patterns of the dynamic graphs, and employ a generative model to predict the graph topology at the next time step and constructs a graph instance that corresponds to that topology. The authors evaluate the proposed model on several artificial datasets following common network evolving dynamics, as well as on real-world datasets. "
SP:ff722957a1765c0568426ed88dd910a6b74054ef,"This paper proposes a method for imputing missing features and estimating the distribution of target assignments given incomplete data. The proposed method is based on a generator network that generates imputations that a discriminator network is tasked to distinguish. Then, a predictor network is trained using the imputed samples from the generator network to capture the classification uncertainties and make predictions accordingly. Experimental results show the effectiveness of the proposed method in generating imputations and providing estimates for the class uncertainties in a classification task when faced with missing values."
SP:c051b0fe779d9e4131016970b7ba469b596f3009,"This paper studies the problem of long-term reward estimation in reinforcement learning (RL). The authors propose a new estimator that computes importance ratios of stationary distributions, without knowledge of how the off-policy data are collected. They analyze its asymptotic consistency and finite-sample generalization. They empirically demonstrate the effectiveness of their method on several classic control benchmarks."
SP:065c900843011a71b70ed35357a2f71fe83872a7,"This paper proposes a generative adversarial network (GAN) framework for the Gaussian mixture model (GMM), which is a probabilistic framework that allows us to define a dataset containing K different modes. In the traditional GMM paradigm, it is straightforward to compute in closed-form, the conditional likelihood p(x|k, \theta) and the responsibility probability p(k|x,\theta), which describes the distribution index corresponds to the data. However, the Euclidean distances between the data do not allow them to form mixtures naturally, nor it’s feasible to compute responsibility distribution, making GMM unable to apply. To this end, the authors utilize the GAN framework to achieve an alternative plausible method to compute these probabilities at the data's latent space z instead of x. They devise a modified GAN to allow to define the distribution using p(z|k,. x, y) and p(y|x,. y) through an additional classification network, where z is the corresponding latent representation of x and y is the data distribution index."
SP:2da1608209058d214f8671062cc9eb0833ba4831,"This paper proposes a method to train large capacity neural networks with significantly improved accuracy and lower dynamic computational cost by gating the deep-learning architecture on a fine-grained-level. Individual convolutional maps are turned on/off conditionally on features in the network. To achieve this, the authors introduce a new residual block architecture that gates convolutionsal channels in a fine grained manner. The authors also introduce a generally applicable tool batch-shaping that matches the marginal aggregate posteriors of features in a neural network to a pre-specified prior distribution. They use this technique to force gates to be more conditional on the data."
SP:f90e9f0eb53f92601bdfa3f7bf86f71d037aad30,"This paper proposes a probabilistic importance inference approach for pruning DNNs. Specifically, it test the significance of the relevance of a connection in a DNN to the DNN’s outputs using a nonparemetric scoring test and keep only those significant ones. The proposed approach achieves better lossless compression rates than existing techniques."
SP:64cbbb6a2f6847ef71cd5a23ba3e4cc5c815a56e,"This paper proposes a method for hierarchical reinforcement learning (HRL) based on identifying behavioral ‘motifs’—repeated action sequences that can be compressed to yield a compact code of action trajectories. The proposed method iteratively compresses the action sequences to learn nested behavioral hierarchies of arbitrary depth, with actions of arbitrary lengths. The learned temporally extended actions provide new action primitives that can participate in deeper hierarchies as the agent learns. The authors demonstrate the relevance of this approach for tasks with non-trivial hierarchical structure and show that it can be used to accelerate learning in recursively more complex tasks through transfer."
SP:e1ccfb3a684aef8a0fb36194eb16af1667811e81,"This paper proposes a variational autoencoder (VAE-GAN) with a multimodal decoder. The decoder is modeled as an energy-based model (EBM) instead of an unimodal Gaussian distribution. The model is trained using variational inference, similar to a VAE, to recover latent codes conditioned on inputs. The authors also introduce a simple extension called Set-HBAE, where each data point is a collection of examples (e.g., a set of images). In this setting, the model is able to learn to reconstruct the input data points and the set of examples."
SP:1130a391afa30d1e0fddadedd2a3aaa70a4cb751,"This paper studies the effect of normalization on off-policy temporal difference (TD) methods. The authors propose a normalization scheme called cross-normalization, which is an extension of batch normalization that re-centers data for two different distributions, i.e., actions in on-policy transitions and actions proposed by the current policy. Experiments on DDPG and TD3 show the effectiveness of the proposed cross normalization."
SP:f9cafaa5131176290fa069e6d24046c079cd9eea,"This paper proposes a method to learn discriminative features that are unbiased and invariant to the confounder(s) present in the data. The authors propose a new adversarial loss function that encourages a vanished correlation between the bias and learned features. They apply their method to synthetic data, medical images, and a gender classification (Gender Shades Pilot Parliaments Benchmark) dataset. The results show that the learned features are uncorrelated with the bias or confounders variables."
SP:783049ff463edd1283c058c6106a3e1f9a033df4,"This paper proposes a lightweight Transformer-based character-level language model, called Group-Transformer, that factorizes the calculation paths by grouped embedding operators and employs inter-group linear operators to prevent performance degradation from the group strategy. The proposed model is inspired by the group convolution approaches that have effectively compressed huge image processing models for usability on mobile devices. The authors conducted extensive experiments on two benchmark datasets, enwik8 and text8, and found that the proposed model with 6M parameters outperformed all LSTM-based models with under 35M parameters."
SP:946c26d371297c88d0ac246257104099b4585edc,"This paper proposes to train deep-latent variational autoencoders (DVAEs) using Optimal Transport (OT), a non-likelihood-based framework for training generative models with Hierarchical Latent Variational Variational Hierarchies (HVHDs). Theoretical properties of OT, such as easier training convergence between distributions, are used to motivate the use of OT for training DVAEs. Experiments on MNIST, CIFAR-10 and Fashion MNIST show that the proposed method outperforms the original Wasserstein Autoencoder (WAE) with Maximum Mean Discrepancy (MMC) divergence."
SP:309b47441d227ffa33f96f9f16f2addc607e5bb0,"This paper proposes an autoregressive video generation model based on a three-dimensional self-attention mechanism. The proposed model is a generalization of the Transformer architecture of Vaswani et al. (2017) using three-dimension, block-local attention, which is implemented on Tensor Processing Units (Jouppi et al., 2017). To further reduce the memory footprint, this paper proposes to model videos as 3D volumes instead of sequences of still image frames, with direct interactions between representations of pixels across the spatial and temporal dimensions. This formulation can be implemented efficiently on TPUs."
SP:ad8fcdbc47a50dd2bf58aba2bc6cfe199e84dd4d,"This paper proposes a latent feature generation framework for generalized zero-shot ICD coding, where the goal is to improve the prediction on codes that have no labeled data without compromising the performance on seen codes. The framework generates semantically meaningful features by exploiting ICD code hierarchical structure and a novel cycle architecture that reconstructs the relevant keywords. Extensive experiments demonstrate the effectiveness of the proposed method on the public MIMIC-III dataset."
SP:3ce82ae297e5759ab957babe9927062e7a71b0ba,This paper proposes a self-supervised representation learning method to improve sample efficiency in reinforcement learning (RL). The authors propose a forward prediction objective for simultaneously learning embeddings of states and action sequences. The proposed method achieves efficient learning of high-quality policies on goal-conditioned continuous control from pixel observations in only 1-2 million environment steps. The authors demonstrate the effectiveness of the proposed method on simple 2D control tasks.
SP:11ce1616e721340eea9e80dad7460c77355ac7d1,"This paper proposes an automated relational meta-learning (ARML) framework that automatically extracts the cross-task relations and constructs the meta-knowledge graph. When a new task arrives, it can quickly find the most relevant structure and tailor the learned structure knowledge to the meta learner. Experiments on toy regression and few-shot image classification demonstrate the superiority of ARML over state-of-the-art baselines."
SP:37c209cd1c628b5c2f2b282fbeaf4bbf437c7670,"This paper proposes a method for controllable language generation that combines a pretrained LM with one or more attribute classifiers that guide text generation without any further training of the LM. The attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM, and the attribute classifier is either a bag-of-words (BoW) or single layer classifiers. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM’s hidden activations and thus guide the generation. The authors demonstrate control over a range of topics and sentiment styles. "
SP:12d0980bfea2de880905a0b87b40856969bb1c58,"This paper proposes to learn data representations with a novel type of denoising autoencoder, where the noisy input data is generated by corrupting clean data in the gradient domain. This can be naturally generalized to span multiple scales with a Laplacian pyramid representation of the input data. Experiments on several visual benchmarks demonstrate that better representations can be learned with the proposed approach, compared to its counterpart with single-scale corruption. The learned representations perform well when transferring to other vision tasks."
SP:12afc1b259e51a31cbeb72366d2b93fbee1aafaa,"This paper studies the problem of under-sensitivity in the context of natural language inference by ensuring that models do not become more confident in their predictions as arbitrary subsets of words from the input text are deleted. The authors develop a novel technique for formal verification of this specification for models based on the popular decomposable attention mechanism by employing the efficient yet effective interval bound propagation (IBP) approach. Using this method we can efficiently prove, given a model, whether a particular sample is free from the under-sensitive problem. The experiments on the SNLI and MNLI datasets show that IBP training leads to a significantly improved verified accuracy."
SP:14257af9fe83522c6e5b5d6b0d68945b944e30fb,"This paper studies the problem of off-policy deep reinforcement learning (RL) from a theoretical point of view. In particular, the authors propose a graph-based analysis of the replay memory of DQN and DDPG. They show that the structure of replay memory is related to soft divergences. Based on this analysis, they propose a QGRAPH, which is a lower bound of the Q-value for the same transition in the original continuous Q-learning problem. The authors show that this lower bound can be used to compute exact Q-values for each transition in a simplified MDP, which leads to improved sample efficiency and robustness to hyperparameters."
SP:c92c97e47d8b218dfd009bbf61f5b3547b395f91,"This paper studies the effect of the embedding complexity on generalization to the target domain in unsupervised domain adaptation. In particular, it shows that the complexity affects the upper bound on the target risk; this is reflected in experiments, too. Next, the authors specify our theoretical framework to multilayer neural networks, and develop a strategy that mitigates sensitivity to the embeddings complexity, and empirically achieves performance on par with or better than the best layer-dependent complexity tradeoff."
SP:f3f3c6fbae757836551b3f1ee54a7d1e040132b8,"This paper studies generalization error bounds for learning general non-convex objectives, which has attracted significant attention in recent years. The authors develop a new framework, termed Bayes-Stability, for proving algorithm-dependent generalisation error bounds. The new framework combines ideas from both the PAC-Bayesian theory and the notion of algorithmic stability. They obtain new data-dependent bounds for stochastic gradient Langevin dynamics (SGLD) and several other noisy gradient methods (e.g., with momentum, mini-batch and acceleration, Entropy-SGD). Their result recovers (and is typically tighter than) a recent result in Mou et al. (2018) and improves upon the results in Pensia et al (2018)."
SP:a82fcd1d3196ddf078cfe8f4bc6f445d9d2bdc11,"This paper studies the role of the hippocampus in continual learning in the context of reinforcement learning (RL). The authors analyze population-level activity of 612 hippocampal CA1 neurons of rodents learning to perform allocentric and egocentric spatial tasks. They show that hippocampal neurons encode relevant task variables such as decisions, navigational strategies and reward location. They also compare this hippocampal features with standard reinforcement learning algorithms, highlighting similarities and differences."
SP:51acf1f8108683dce543a1fb4a61fbd593f9b4cc,"This paper proposes a policy optimization method for Monte Carlo Tree Search (MCTS) in continuous action spaces. The main idea is to first train a policy using existing model-free RL methods, and then use the pre-trained policy distribution to draw actions with which to build the tree. Once the tree has been constructed, it is used to run simulations to generate experiences using an Upper Confidence Bounds for Trees (UCT) approach. Populating the tree with the action samples drawn from a pretrained policy enables to perform a computationally feasible search."
SP:1ce3bc4d31712886f7dcada5b5ae67c3c376819a,"This paper studies the lottery ticket hypothesis, which claims that neural networks contain sparse subnetworks, which, if appropriately initialized (the winning tickets), are capable of matching the accuracy of the full network when trained in isolation. However, the properties of winning tickets are not well understood, especially the importance of supervision in the generating process. In this paper, the authors aim to answer the following open questions: can we find winning tickets with few data samples or few labels? can we even obtain “good” tickets without supervision? They find that winning tickets found in these scenarios are, perhaps surprisingly, competitive with winning tickets generated on the full ImageNet dataset when evaluated on the ImageNet classification task. They also show that finding winning tickets can be accelerated by using only a subset of the data."
SP:dbcebe5b73486885d9f4478b258047c02f8481a2,This paper studies the problem of adversarial attacks on reading comprehension models. The authors formulate a noisy adversarial attack which searches among semantic variations of comprehension questions for which a model still erroneously produces the same answer as the original question – and with an even higher probability. They show that – despite comprising unanswerable questions – SQuAD2.0 and NewsQA models are vulnerable to this attack and commit a substantial fraction of errors on adversarially generated questions. This indicates that current models—even where they can correctly predict the answer—rely on spurious surface patterns and are not necessarily aware of all information provided in a given comprehension question. They further show that both data augmentation and adversarial training as defense strategies can substantially decrease a model’s vulnerability to undersensitivity attacks on held out evaluation data.
SP:5da870060778de460c1abe91562d6f3e707efef4,"This paper proposes a model-based approach to safety that allows the agent to look into the future and be aware of the future consequences of its actions. It learns the transition dynamics of the environment and generates a directed graph called the imaginative module. This graph encapsulates all possible trajectories that can be followed by the agent, and it is able to efficiently traverse through the imagined environment without ever taking any action in reality. The proposed approach can be seen as a “plug-and-play approach to ensuring safety, as it is compatible with any existing RL algorithm and any task with discrete action space."
SP:c2796f28fb067138303df8d424d646f4ada31558,This paper proposes a physics-aware difference graph networks (PA-DGN) that leverages neighboring information to learn finite differences inspired by physics equations. The proposed method leverages data-driven end-to-end learning to discover underlying dynamical relations between the spatial and temporal differences in given sequential observations. The authors demonstrate the superiority of the proposed method in the approximation of directional derivatives and the prediction of graph signals on the synthetic data and the real-world climate observations from weather stations.
SP:db8ed4f4fc3967f5dd4d208d5d029730eb99e840,"This paper considers the problem of training structured neural networks (NN) with nonsmooth regularization (e.g. l_1-norm) and constraints (i.e. interval constraints). The authors formulate training as a constrained nonconvex optimization problem and propose a convergent proximal-type stochastic gradient descent (ProxSGD) algorithm. They show that under proper learning rates, with probability 1, every limit point of the sequence generated by the proposed PropsSGD algorithm is a stationary point. The authors also provide theoretical analysis and demonstrate the flexibility of the proposed method. "
SP:2ca1f4da9faee79768764cda5d09d949cc942acc,"This paper proposes an end-to-end differentiable compression framework for lossy image compression, which is able to circumvent the quantization step by relying on a non-deterministic compression codec. The decoder maps the input image to a distribution in continuous space from which a sample can be encoded with expected code length being the relative entropy to the encoding distribution, i.e. it is bitsback efficient. The proposed method can be straight-forwardly trained using standard gradient-based optimizers. To showcase the efficiency of the method, the authors apply it to training Probabilistic Ladder Networks on the CLIC 2018 dataset."
SP:788fd2b6956dd69bf7752d39ea21883947128c8a,"This paper focuses on the problem of super-resolution (SR) from C-JPG images, i.e., compressed JPG images. The authors propose a novel SR structure with two specifically designed components, as well as a cycle loss. First, they propose a functional sub-model to generate high-quality SR images for prevalent C-jPG images; second, a functional recuperative part is proposed to recover information from the missing details; and third, they integrate cycle loss into SR solver to build a hybrid loss function for better SR generation. Experiments show that the proposed approach achieves outstanding performance among state-of-the-art methods."
SP:18dd92f2f55020be4f5a089b3b251327e47886f4,"This paper presents a deep neural network architecture to estimate the probability of passing events in soccer matches from low-level tracking data and weak labeling of each event’s success. The proposed architecture is inspired by recently developed fully convolutional networks that have been proven to be successful for image segmentation, where the objective is to provide a pixel-level classification of objects in the image. This paper presents an extreme case of weakly supervised learning where there is just a single pixel correspondence between ground-truth outcomes and the predicted probability map. The network is able to perform remarkably well from low level inputs by learning a feature hierarchy that produces predictions at different sampling levels that are merged together to preserve both coarse and fine detail."
SP:1ae31baf383fc520687b255d9cac14c3b040e253,"This paper proposes an inductive matrix completion model without using side information. The proposed method trains a graph neural network (GNN) based on 1-hop subgraphs generated from the rating matrix and maps them to their corresponding ratings. It achieves highly competitive performance with state-of-the-art transductive baselines. Moreover, IGMC is inductive – it can generalize to users/items unseen during the training (given that their interactions exist), and can even transfer to new tasks."
SP:c5cb1b50e17a69e88d5ae28848e265215162da1e,"This paper considers the unconstrained minimization of a smooth objective function in the setting where only function evaluations are possible. The authors propose and analyze stochastic zeroth-order method with heavy ball momentum. They show new complexity results for non-convex, convex and strongly convex functions. They test their method on a collection of learning to continuous control tasks on several MuJoCo Todorov et al. environments with varying difficulty."
SP:a216cfc29937eb398ea98cb1aea3481c9aed8240,"This paper proposes Action Semantics Network (ASN), a neural network architecture for multi-agent reinforcement learning (MARL) that explicitly considers the action semantics between agents. The proposed ASN can be easily combined with existing deep reinforcement learning algorithms to boost their performance. Experimental results on StarCraft II micromanagement and Neural MMO show that ASN significantly improves the performance of state-of-the-art DRL approaches compared with several network architectures."
SP:efaf3a440dc17e05177832083ffbc23760ed7c97,"This paper proposes to exploit the underlying structures of the state-action value function, i.e., Q function, for both planning and deep RL. Specifically, the authors investigate the low-rank structure, which widely exists for big data matrices, and verify empirically the existence of low-ranking Q functions in the context of control and deep reinforcement learning tasks. The authors propose a general framework that leverages Matrix Estimation (ME) techniques, which leads to a more efficient planning procedure for classical control and also a simple scheme that can be applied to value-based RL techniques to consistently achieve better performance on “low-rank” tasks. Extensive experiments on control tasks and Atari games confirm the efficacy of the approach."
SP:430336893b247b7bd45687d78b0d0511a7369e87,"This paper proposes a new algorithm, Best-Action Imitation Learning (BAIL), for batch reinforcement learning (DRL), which does not involve maximizing Q functions over the action space. Instead, BAIL first selects from the batch the actions it believes to be high-performing actions for their corresponding states; it then uses those state-action pairs to train a policy network using imitation learning. The authors demonstrate that BAIL achieves state of the art performance on the Mujoco benchmark."
SP:94078964876667e8a5d9ae7728d779d5b91a576e,"This paper proposes DeepXML, a deep extreme multi-label learning algorithm for short text documents. The main contributions of the paper are: (1) learning word embeddings on head labels and transferring them through a novel residual connection to data impoverished tail labels; (2) increasing the amount of negative training data available by extending state-of-the-art negative sub-sampling techniques; (3) re-ranking the set of predicted labels to eliminate the hardest negatives for the original classifier; and (4) extending the scalable Slice algorithm for pretrained embedding to learn the proposed deepXML architecture. The proposed method is demonstrated to be significantly more accurate and an order of magnitude faster to train than XML-CNN and AttentionXML."
SP:b1b1252d82fa1bea18309e0b0b894e0f28f48bc9,"This paper proposes an end-to-end trainable variational hashing-based collaborative filtering approach that uses the novel concept of self-masking: the user hash code acts as a mask on the items (using the Boolean AND operation), such that it learns to encode which bits are important to the user, rather than the user’s preference towards the underlying item property. This allows a binary user-level importance weighting of each item without the need to store additional weights for each user. The proposed method outperforms the state-of-the-art baselines on 4 datasets."
SP:80898d0f2b2c8dc3388fa9164e529eae36aa1b21,"This paper proposes a set of statistical tools to quantify the mode collapse of GANs. The authors propose two simple yet effective “black-box” methods to calibrate the GAN learned distribution, without accessing either model parameters or the original training data. They demonstrate the application of their tool set in analyzing the bias in unconditional face image generation: a popular benchmark task nowadays for GAN, yet remaining rather unclear how to measure its mode collapse using existing tools. "
SP:e5b5dda2f024cfda10526e744aa035e0165af58a,"This paper studies the problem of training two-layer neural networks that are beyond the NTK regime but are still governed by the Taylor expansion of the network. The authors propose to randomize the neural networks, which allows them to escape their NTK and couple with quadratic models. Theoretical results on the generalization and expressivity results on these randomized networks lead to sample complexity bounds (of learning certain simple functions) that match NTK, and can in addition be better by a dimension factor when mild distributional assumptions are present. "
SP:cef7ea513eb3e42be4edf40e4ee1701a969bcbea,"This paper proposes a method to evaluate the effectiveness of graph convolutional filters for semi-supervised node classification tasks. The authors propose a novel assessment tool, called Graph Filter Discriminant Score (GFD), to evaluate different graph filter designs. They show that there is no single filter as a “silver bullet” that performs the best on all possible graphs, and graphs with different properties are in favor of different graph filters. Based on these findings, they develop Adaptive Filter Graph Neural Network (AFGNN) that can adaptively learn data-specific filters. Experiments on both synthetic and real-world benchmark datasets show that the proposed model has the flexibility in learning an appropriate filter and consistently provides state-of-the-art performance across all the datasets."
SP:3c5ec9dbcf914c8901e4e35f3c2a7df4707422ab,"This paper studies the problem of distributionally robust optimization (DRO) for over-parameterized neural networks, where the goal is to learn models that minimize the worst-case training loss over a set of pre-defined groups. In particular, the authors consider natural language inference (NLI), facial attribute recognition (CelebA), and bird photograph recognition (CUB) tasks. The authors first show that standard ERM-based models can perfectly fit the training data, and any model with vanishing average training loss also already has vanishing worst case training loss. Then, they show that group DRO models with increased regularization—a stronger-than-typical `2 penalty or early stopping—are able to achieve substantially higher worst-group accuracies, with 10–40 percentage point improvements on a NLI task and two image tasks. "
SP:eb1ee2e0f7d8466a04b58508ecb3da7b667eecdf,"This paper proposes a local explanation method for black-box classifiers. The proposed method is based on the idea of distribution controllers and integrate it with a neural network to directly guide the distribution of relevance scores. The classification loss is introduced to optimize the proposed predictor. The benefit of this strategy is to enable discriminative scores over supporting features, and facilitate the setting of involved hyperparameters. The experimental results demonstrate that the proposed method also outperforms others in terms of faithfulness and explainability."
SP:32ea7cbc47cbdb1f703f4e07c31ce90abe083424,"This paper proposes a method for weakly-supervised multi-instance image reconstruction and classification. The proposed method is based on the idea of top-K selection, which is a non-differentiable selection/gather operation. To solve this problem, the authors propose to lift the training optimization problem to a higher dimensional one by treating the parameters defining the interest points as slack variables, and introduce a hard constraint that they must correspond to the output that the heatmap network gives. They then solve for the relaxed version of this problem where the hard constraint is turned into a soft one, and the slack variables are also optimized within the training process. The method is able to learn to detect recurring structures in the training dataset by learning to reconstruct images. It can also learn to localize structures when only knowledge on the occurrence of the object is provided."
SP:da1c5f6351d531482e90b86c3cceb52850c520de,This paper proposes a reinforcement learning-based approach for neural program synthesis. The main idea is to learn a policy network and a value network to reduce the depth and breadth of the Monte Carlo Tree Search (MCTS) and to use multi-entropy policy sampling technique to alleviate online update correlations. The proposed approach is evaluated on a subset of x86 assembly language tasks and shows higher success rates than several baselines.
SP:0d4687fc36c02e27d1b95d532a3947589f92b1da,"This paper studies the impact of model architecture on the speed of training in the context of gradient descent optimization. The authors use the ideas from prior work that shows gradient descent can be modeled as a first-order ODE and use ODE’s coefficient matrix H to characterize the convergence rate. They introduce a simple analysis technique that enumerates H in terms of all possible “paths” in the network. They show that changes in model architecture parameters reflect as changes in the number of paths and the properties of each path, which jointly control the speed."
SP:3e3bc8f617df742a395e7d315ec3810a42071294,"This paper studies the connection between strongly overparametrized neural networks (NNs) and interpolating kernel methods. The authors prove that fully-connected wide ReLU-NNs trained with squared loss are essentially a sum of two parts: the first is the minimum complexity solution of an interpolating kernels method, while the second contributes to the test error only and depends heavily on the initialization. This decomposition has two consequences: (a) the second part becomes negligible in the regime of small initialization variance, which allows the authors to transfer generalization bounds from minimum complexity interpolating Kernel methods to NNs; (b) in the opposite regime, the generalization error of wide NNs increases significantly with the initialization variance."
SP:b15ea009a36a0a76728dfc103d668d6781a8a99a,"This paper proposes to improve depth estimation and stereo-based 3D object detection in pseudo-LiDAR. The main contributions of this paper are two folds: (1) the authors propose to improve the depth estimation of far-away objects, (2) they leverage cheaper but extremely sparse LiDAR sensors, which alone provide insufficient information for 3D detection, to de-bias depth estimation, and (3) they propose a depthpropagation algorithm, guided by the initial depth estimates, to diffuse these few exact measurements across the entire depth map. They show on the KITTI object detection benchmark that their combined approach yields substantial improvements in depth estimation."
SP:983d84502264633f3385d426c1d4601a0744ea9a,"This paper proposes a principled adversarial example detection method that can withstand norm-constrained white-box attacks. The proposed method is based on one-versus-the-rest classification, where the i-th detector is trained to discriminate natural data of class i from adversarial examples perturbed from other classes. The paper further devise a generative approach to detect/classify adversarial samples by interpreting each base detector as an unnormalized density model of the classconditional data."
SP:461e9308d050bc3dc7b35233452668bb31f5d491,"This paper proposes an intrinsic reward to encourage exploration in procedurally-generated sparse reward environments. The proposed intrinsic reward encourages the agent to take actions that lead to significant changes in its learned state representation. The authors evaluate their method on multiple procedurally generated tasks in MiniGrid, as well as on tasks with high-dimensional observations used in prior work. The experiments demonstrate that this approach is more sample efficient than existing exploration methods, particularly for procedural-generated MiniGrid environments. "
SP:c002c20b5e8696588e029c0f65e88860418826c4,"This paper studies the problem of large-scale query-document retrieval: given a query, retrieve the set of relevant documents (e.g., paragraphs containing the answer) from a large document corpus. This problem is often solved in two steps: the retrieval phase first reduces the solution space, returning a subset of candidate documents, and the scoring phase re-ranks the documents. The retrieval phase remains less well studied, and this is the focus of this paper. This paper conducts a comprehensive study on the embedding-based retrieval models. It shows that the key ingredient of learning a strong embedding based Transformer model is a set of pre-training tasks. With adequately designed paragraph-level pretraining tasks, the Transformer models can remarkably improve over the widely-used BM-25 or embedding models without Transformers. "
SP:4e161e08a624f87633dfb49dfd46bd1665e15189,"This paper proposes a new graph convolution operation for graph neural networks (GNNs) that can be applied to irregularly sampled data. The proposed method, called bipartite graph convolutions (BGC), is a parametric transformation between different input and output graphs. The authors claim that the proposed method is general enough to subsume conventional GNNs and pooling as its special cases and supports multi-graph aggregation leading to a class of flexible and adaptable network architectures, termed BiGraphNet. Experiments are conducted to show the effectiveness of the method."
SP:9b9b6ee9014e5538442ba76d6059ed01f59ec8fb,"This paper addresses the problem of few-shot classification under domain shifts for metric-based methods. The core idea is to use feature-wise transformation layers for augmenting the image features using affine transforms to simulate various feature distributions under different domains in the training stage. The authors further apply a learning-to-learn approach to search for the hyper-parameters of the feature transformation layers. The experiments and ablation studies under the domain generalization setting are conducted on five benchmark datasets: mini-ImageNet, CUB, Cars, Places, and Plantae."
SP:df46627cb984a56bba36d510bfc52e00751e9107,"This paper proposes a convolutional network for learning Lagrangian fluid simulation. The authors use spatial convolutions as the main differentiable operation that relates particles to their neighbors. They show that their network architecture can simulate different materials, generalize to arbitrary collision geometries, and can be used for inverse problems. They also demonstrate that their continuous convolutions outperform prior formulations in terms of accuracy and speed."
SP:3e17f333cf07183969c02bb66afdd3ccbf25bb19,"This paper proposes a new ensemble method, called BatchEnsemble, to reduce the computational and memory cost for training and testing neural networks. The proposed method is based on the Hadamard product of a shared weight among all ensemble members and a rank-one matrix per member. The method is parallelizable across devices, where one device trains one member, and also parallelizable within a device, where multiple ensemble members are updated simultaneously for a given mini-batch. The speedup at test time is 3x and memory reduction is 3X at an ensemble of size 4."
SP:a123a425ef3eb6188833d5a42e851bc3fa59df65,"This paper proposes a neural network-based solver for solving the forward and inverse problems of PDEs. The proposed solver is grid free, mesh free and shape free, and the solution is approximated by a deep neural network. The network is trained to minimize deviations of the learned function from the strong PDE solution and satisfy the boundary conditions. The solution in turn is an explicit smooth differentiable function with a known analytical form."
SP:973d0ad0faadcf7298300f2758de9154205e7113,"This paper focuses on the analysis of binary neural networks (BNNs), a class of neural networks that allows exact representation in Boolean logic and can be analyzed formally with logic-based reasoning tools such as SAT solvers. The main bottleneck for all methods is their ability to reason about large BNNs efficiently. In this work, the authors analyze architectural design choices of BNN and discuss how they affect the performance of logic based reasoners. They propose changes to the BNN architecture and the training procedure to get a simpler network for SAT solver without sacrificing accuracy on the primary task. "
SP:ca985e758f195bd04fb9f24b290a83974d6d308b,"This paper studies the expressive power of graph neural networks falling within the message-passing framework (GNNmp). Two results are presented. First, GNNmp are shown to be Turing universal under sufficient conditions on their depth, width, node attributes, and layer expressiveness. Second, it is discovered that GNNMP can lose a significant portion of their power when their depth and width is restricted. The proposed impossibility statements stem from a new technique that enables the repurposing of seminal results from distributed computing and leads to lower bounds for an array of decision, optimization, and estimation problems involving graphs."
SP:a98ae70a91850bbe624c307ba61d3daeb2494b82,"This paper proposes a localised generative flow (LGF) method for density estimation. The basic idea is to replace the continuous bijections in normalising flows with a continuous mixture of continuous mixtures, which allows the model to focus on a local region of the target rather than the whole distribution. The authors also propose a variational scheme to approximate the log likelihood of the model. The proposed method is evaluated on a variety of density estimation tasks, and the results show that the proposed method outperforms the existing normalising flow methods."
SP:3adc341dece170f428195e4dccadfb5f5daddf2d,"This paper studies the problem of vision-and-language navigation (VLN), where an agent is given a set of instructions to navigate to a target location, and must follow these instructions to reach the target location. Previous work has shown that VLN performance drops when tested on unseen environments (i.e., environments not used in training), indicating that the neural agent models are highly biased towards training environments. This paper aims to answer three questions to this environment bias: 1. Where is the bias located? 2. Why does this bias exist? 3. How can we eliminate this bias? The paper first shows that the instruction-guided instructions and the underlying navigational graphs are not direct reasons for the performance gap, and then investigates the effect of environments on the agent’s performance. In order to conduct the analysis, the paper resplit the environment and categorize the validation data into three sets based on their visibility to the training set: path-seen data intersecting with the training paths, path-unseen data using the training environments but away from the training path, and env-seen using the unseen data. The paper claims that the low-level visual appearance conveyed by ResNet features directly affects the agent model and contributes to the environment bias in results. Based on this observation, this paper explores several kinds of semantic representations which contain less low level visual information, hence the agent learned with these features could be better generalized to unseen testing environments. The explored semantic features significantly decrease the performance gaps between seen and unseen on multiple datasets."
SP:298e0043e99f586d314fbd9d16fdc6ae885e1ebb,"This paper considers the problem of using human feedback to accelerate and optimize the training of deep reinforcement learning (DRL) agents. The authors propose to use electroencephalogram (EEG) signals from a human observer to generate error-related electric potentials (ErrPs), which are then used as an auxiliary reward function to improve the performance of the RL agent. The EEG signals are obtained by placing electrodes on the human scalp and monitoring what are known as event-related potentials. The implicit feedback is then used to augment the agent’s learning in the RL tasks. Empirical results show that the proposed approach is able to obtain and accurately decode the implicit human feedback."
SP:a8395f8b877e1eebaef9ff2e8b4e488d55a74ef4,"This paper proposes a framework to compare the performance of diverse image classifiers on the laconic classification task, where the goal is to minimize the amount of information (aka. entropy) required in individual test images to maintain correct classification. Given a classifier and a test image, we compute an approximate minimal-entropy positive image for which the classifier provides a correct classification, becoming incorrect upon any further reduction. The notion of entropy offers a unifying metric that allows to combine and compare the effects of various types of reductions (e.g., crop, colour reduction, resolution reduction) on classification performance, in turn generalizing similar methods explored in previous works. The authors propose two complementary frameworks for computing the minimal-ENTropy positive images of both human and machine classifiers, in experiments over the ILSVRC test-set, they find that machine classifier are more sensitive entropy-wise to reduced resolution (versus cropping or reduced colour for machines, as well as reduced resolution for humans), supporting recent results suggesting a texture bias in the IlsVRC-trained models used. They also find, in the evaluated setting, that humans classify the minimal entropy positive images with higher precision than machines classify those of humans."
SP:81cec8f907d8fa0653b5bc08af1f59bfefd49619,This paper provides an analysis of perturbation-based adversarial attacks on convolutional neural networks (CNNs). The authors identify a family of defenses that are based on the instability assumption. The defenses include deterministic lossy compression algorithms and randomized perturbations to the input that all lead to similar gains in robustness. The authors argue that the optimization procedure in the attacker should find the smallest distance from the original image that closes the recovery window. The paper also provides some insights into how an attacker might avoid them.
SP:a136b98e0ed478144ce9dd26e2b6d611543124e8,"This paper proposes a method for self-supervised learning of 3D feature maps for object detection and unsupervised object detection. The proposed method is based on neural 3D mapping networks, which take as input 2.5D (color and depth) video streams captured by a moving camera and lift them to stable 3d feature maps of the scene by disentangling the scene content from the motion of the camera. The model also projects its feature maps to novel viewpoints, to predict and match against target views. The authors propose contrastive prediction losses to replace the standard color regression loss, and show that this leads to better performance on complex photorealistic data."
SP:6fd61604a2eeb8a2cbbda6c40807cebef6d40f2f,"This paper studies the problem of unsupervised domain translation (UDT), which is the task of finding meaningful correspondences between two domains, without access to explicit pairings between them. The authors first define UDT in a rigorous, non-ambiguous manner, explore the implicit biases present in the approach and demonstrate the limits of the approaches. Specifically, they show that mappings produced by these methods are biased towards low energy transformations, leading them to cast UDT into an Optimal Transport (OT) framework by making this implicit bias explicit. This not only allows them to provide theoretical guarantees for existing methods, but also to solve UDT problems where previous methods fail. Finally, making the link between the dynamic formulation of OT and CycleGAN, the authors propose a simple approach to solve the UDT problem."
SP:8bb3ce11ad773685f6e41d90db3e7a5481e5ba47,"This paper proposes a new regularization method, RotationOut, for neural networks. Different from Dropout that handles each neuron/channel independently, the proposed method regards its input layer as an entire vector and introduces regularization by randomly rotating the vector. The proposed method can be used in convolutional layers and recurrent layers with small modifications. The authors further use a noise analysis method to interpret the difference between R rotationOut and Dropout in co-adaptation reduction. Extensive experiments in vision and language tasks are conducted to show the effectiveness of the proposed methods."
SP:37620ae8dc5683eb2843792e0aa4cbe6cba366f7,"This paper proposes a method to create universal adversarial perturbations (UAP) for a given CNN in a data-free manner. The proposed method is based on the dilate loss, which maximizes the Euclidean norm of the output before nonlinearity at any layer. By doing so, the perturbation constrains the ReLU activation function at every layer to act roughly linear for data points and thus eliminate the dependency on data for crafting UAPs. Extensive experiments demonstrate that the proposed method not only has theoretical support, but achieves higher fooling rate than the existing data free work."
SP:2fd7d5507a8727db743dc89379a6f021d31ed39a,"This paper proposes a transferable neural architecture search method based on meta-learning in this paper, which is termed as Transferable Neural Architecture Search (T-NAS). T-NAS learns a meta-architecture that is able to adapt to a new task quickly through a few gradient steps, which makes the transferred architecture suitable for the specific task. Extensive experiments are conducted to show the effectiveness of the proposed method."
SP:1314a79ba12474adb33ff31b3cb22bed25b94fb7,"This paper proposes a simple and effective stochastic neural network architecture for discriminative learning by directly modeling activation uncertainty and encouraging high activation variability. Compared to existing SNNs, the proposed SE-SNN is simpler to implement and faster to train, and produces state-of-the-art results on network compression by pruning, adversarial defense and learning with label noise. "
SP:bd4935d4fcf33f60f22e0f2fd9f7dc8ddfab6d17,"This paper proposes a meta-learning approach for generating curiosity mechanisms that dynamically adapt the agent’s reward signal. The outer loop will search over a space of curiosity mechanisms and the inner loop will perform standard reinforcement learning using the adapted reward signals. The proposed approach combines neural networks with other building blocks such as buffers, nearest-neighbor modules and custom loss functions. The authors demonstrate the effectiveness of the approach empirically, finding two novel curiosity algorithms that perform on par or better than human-designed curiosity algorithms in domains such as grid navigation with image inputs, acrobot, lunar lander, ant and hopper."
SP:6dff0f3a84809ae0ba9f58f36303597f1ba6dcc5,"This paper proposes a new task of Any-Code-to-Code Generation (AnyC2C) - generating code given its surrounding code without any restriction on the vocabulary or structure. The proposed approach, Structural Language Modeling (SLM), estimates the probability of the program’s abstract syntax tree (AST) by decomposing it into a product of conditional probabilities over its nodes. The authors present a neural model that computes these conditional probabilities by considering all AST paths leading to a target node. The model achieves a new state-of-the-art exact-match accuracy of 18.04% (previous SOTA: 16.93%) in generating Java and C# code."
SP:7fc60d6fd1cfcc135c34f9664d172d3fd1c0ae0a,"This paper studies the non-convex optimization problems in learning large-scale neural networks (NN). It shows that the objective functions in learning NNs are convex in the canonical model space, and the gradients between the original NN model space and the canonical space are related by a pointwise linear transformation, which is represented by the so-called disparity matrix. Furthermore, it shows that gradient descent methods surely converge to a global minimum of zero loss provided that the disparity matrices maintain full rank."
SP:78a536138570fe9b5d88350e4b16d598a7db1fe0,"This paper proposes two interactive graph-based segmentation algorithms for semantic and panoptic segmentation. The first algorithm is based on a discrete discrete Potts model, and the second one is a class-aware integer linear programming (ILP) formulation that ensures global optimum. Both algorithms can take RGB, or utilize the feature maps from any DCNN, whether trained on the target dataset or not, as input. The experiments show that incorporating the connectivity prior greatly improves the performance of the proposed algorithms."
SP:2eb90879ddbc39b6b5c05152784d6044d1940513,"This paper proposes a method to detect adversarial perturbations by using saliency maps as a defense against adversarial examples. The authors claim that the shift in the magnitude of the saliency map is due to the perturbation of the salient features of the image, and propose to use learned saliency models to capture the shifts in saliency. They also propose a defense method that distinguishes between adversarial images and natural images using salient pixels as its input. The proposed method is evaluated on MNIST, CIFAR-10, and ASSIRA datasets."
SP:fe5510d05ff091a5f133f2dbcd1b23d8d58d2c3e,"This paper studies the problem of quantifying the global adversarial robustness of neural networks. The authors propose to use concentration inequalities to provide statistical guarantees on the robustness/accuracy trade-off for a variety of neural network architectures and training methods on MNIST, Fashion-MNIST and CIFAR. They empirically observe that robustness and accuracy tend to be negatively correlated for networks trained via stochastic gradient descent and with iterative pruning techniques, while a positive trend is observed between them in Bayesian settings."
SP:8f5616a1480b68c04b496ed498d237d5a7e87794,"This paper studies the problem of robust reinforcement learning, i.e., finding the optimal policy with some extent of robustness to environmental dynamics. The authors propose to use the Wasserstein distance to measure the disturbance to the reference transition kernel, which allows them to reduce an infinite-dimensional optimization problem to a finite-dimensional risk-aware problem. They show the existence of optimal robust optimal policies, provide a sensitivity analysis for the perturbations, and then design a novel robust learning algorithm—Wasserstein Robust Advantage Actor-Critic algorithm (WRAAC). The effectiveness of the proposed algorithm is verified in the Cart-Pole environment."
SP:d85963f5f0f6b20cf08f2a7c169ae33a45db7de2,"This paper proposes a method to approximate mixed strategy Nash equilibria in multi-player continuous games. The authors propose a pushforward measure technique to represent a mixed strategy in continuous spaces, which allows them to generalize the Gradient-based Nikaido-Isoda (GNI) function to measure the distance between the players’ joint strategy profile and a Nash equilibrium. Then, they apply the gradient descent algorithm to the general GNI function, which converges to a mixed stationary Nash equilibrium under the convexity assumption on the payoff functions. In numerical experiments, the method consistently and significantly outperforms recent works on approximating Nash equilibrium for quadratic games, general blotto games, and GAMUT games."
SP:280d85cd8164a268f9d496ae5f17189c50f30dc1,"This paper proposes a neural execution tree (NExT) framework for natural language (NL) explanations to augment training data for text classification using NL explanations. NExT first converts NL explanations into executable logical forms by semantic parsing, then generalizes different types of actions specified by the logical forms for labeling data instances, which substantially increases the coverage of each NL explanation. Experiments on two NLP tasks (relation extraction and sentiment analysis) demonstrate its superiority over baseline methods."
SP:a9b5f7257dedd719cfe341fca275776734af1d98,"This paper extends the applicability of verified training by extending it to (1) recurrent neural network architectures and (2) complex specifications that go beyond simple adversarial robustness, particularly specifications that capture temporal properties like requiring that a robot periodically visits a charging station or that a language model always produces sentences of bounded length. Experiments show that while models trained using standard training often violate desired specifications, the verified training method produces models that both perform well (in terms of test error or reward) and can be shown to be provably consistent with specifications."
SP:3903680e07b676409e3cf6a1044b67291fe38630,"This paper studies the problem of visual domain generalization, i.e., generalizing to a wide range of visually different environments. The authors formalize the visual domain randomization problem and show that minimizing the policy’s Lipschitz constant with respect to the randomization parameters leads to low variance in the learned policies. They propose a regularization method where the agent is only trained on one variation of the environment and its learned state representations are regularized during training to minimize this constant. They conduct experiments that demonstrate that their technique leads to more efficient and robust learning than standard domain randomisation, while achieving equal generalization scores."
SP:c79046dc56b9ee9c926f87386046422ea134ae8d,"This paper studies the problem of deep metric learning (DML) and proposes a simple and effective framework to sample pairs in a batch of data for updating the model. The key to this framework is to define a robust loss for all pairs over a mini-batch of data, which is formulated by distributionally robust optimization. The flexibility in constructing the uncertainty decision set of the dual variable allows the authors to recover state-of-the-art complicated losses and also to induce novel variants. Empirical studies on several benchmark data sets demonstrate the effectiveness of the proposed method."
SP:38420928e40ef80c0136ad607b9275f9ab1e0769,"This paper studies the problem of finding a local minimum in non-convex finite-sum minimization with inexact gradient and Hessian estimation. The authors propose a sample-efficient stochastic trust region (STR) algorithm that finds an approximate local minimum within $\sqrt{\sqrt{n/\epsilon}$ stochastically Hessian oracle queries, which improves the state-of-the-art result by a factor of $\mathcal{O}(n^{2/3/1.5)$. The authors also develop Hessian-free STR algorithms that achieve the lowest runtime complexity. Experiments verify the efficiency of the proposed algorithms."
SP:28a35b70b5e6915af28cacebc4ea50690c9534af,"This paper proposes a method for training deep neural networks without batch normalization (BN) or weight initialization (Xavier initialization). The method is based on linear programming and uses Farkas layers, a method that ensures at least one neuron is active at a given layer. The authors empirically demonstrate the effectiveness of the proposed method on CIFAR-10 and ImageNet datasets."
SP:1d325b148e3efe407241c1f1cbe8d17400499741,"This paper proposes two methods for certifying the robustness of deep neural networks to adversarial perturbations. First, the authors show that if the eigenvalues of the Hessian of the network are bounded, they can compute a robustness certificate in the l2 norm efficiently using convex optimization. Second, they derive a computationally-efficient differentiable upper bound on the curvature of a deep network. The curvature bound is also used as a regularization term during the training of the neural network to boost its certified robustness against adversarial examples."
SP:33f6f5aa0d4655e5d75fe612e0eff05e579d45c5,"This paper proposes a method for compressed sensing recovery using untrained deep generative models. The method is based on the recently proposed Deep Image Prior (DIP), wherein the convolutional weights of the network are optimized to match the observed measurements. The proposed method does not require pre-training over large datasets. The authors also introduce a novel learned regularization technique, which incorporates prior information on the network weights. This reduces reconstruction error, especially for noisy measurements."
SP:23c0f621e6041003b59bf0532130760694cf6a4a,"This paper proposes a hierarchical reinforcement learning (HRL) framework TAIC (Temporal Abstraction with Information-theoretic Constraints), which learns the temporal abstraction from past experience or expert demonstrations without task-specific knowledge. The temporal abstraction problem is formulated as learning latent representations of action sequences and the authors propose a novel approach of regularizing the latent space by adding information theoretic constraints. Specifically, the authors maximize the mutual information between the latent variables and the state changes. The learned abstraction allows us to learn new tasks on higher level more efficiently. The proposed framework provides an efficient tool for transferring knowledge between tasks."
SP:4e54c9196ba1eb2b6a0b0eee41e4a6f3a9de72dd,"This paper proposes a layer-wise sampling strategy to accelerate the training of graph convolutional networks (GCNs). The proposed method samples the nodes layer-by-layer conditionally based on the factors of the bi-directional diffusion between layers. The authors also apply the self-attention mechanism to learn suitable weights for the sampled nodes, which allows the model to be able to incorporate both the first-order and higher-order proximities during a single layer propagation process without extra recursive propagation or skip connection. Experiments on three large benchmark graphs demonstrate the effectiveness and efficiency of the proposed model."
SP:bb0af9c011ef982c34fcadb545f6b5771818e7fa,"This paper proposes a new unsupervised state-space model for videos that explicitly reasons about objects and their positions, velocities, and interactions. It is constructed by combining an image model and a dynamics model in a compositional manner and improves on previous work by reusing the dynamics model for inference, accelerating and regularizing training. The model predicts videos with convincing physical behavior over thousands of timesteps, outperforms previous unsupervision models, and even approaches the performance of supervised baselines. The authors also demonstrate how STOVE can be employed for model-based reinforcement learning (RL)."
SP:e67b463bc0aec2345925d609fa521ea49df57fd9,This paper proposes a new variational autoencoder (VAE) model that combines the best properties of VAE and generative adversarial networks (GAN). The proposed model optimizes the $\lambda$-Jeffreys divergence between the model distribution and the true data distribution. The authors propose a novel approach to train the VAE model with an implicit likelihood by an adversarially trained discriminator. Experiments on CIFAR-10 and TinyImagent datasets demonstrate the effectiveness of the proposed model.
SP:87056d0147ddcaf5d78f6888b05161fbdbb3346c,"This paper shows that adversarial attacks can be used against the Bayes-Optimal classifier for certain class distributions, while for others the optimal classifier is robust to such attacks. The authors present analytical results showing conditions on the data distribution under which all points can be made arbitrarily close to the optimal decision boundary and show that this can happen even when the classes are easy to separate, when the ideal classifier has a smooth decision surface and when the data lies in low dimensions. The paper also introduces new datasets of realistic images of faces and digits where the Bayesian-optimality classifier can be calculated efficiently and shows that for some of these datasets the optimal classification model is robust and for others it is vulnerable to adversarial examples."
SP:a7b3a35e6a79084bdfd1e4a963dfa081279cd8bb,"This paper studies the impact of pruning on different types of examples and classes. The authors identify a subset of examples, which they call pruning identified exemplars (PIEs), and classes that are systematically more impacted by the introduction of sparsity. They show that removing PIEs from the test-set improves top-1 accuracy for both sparse and non-sparse models. They also show that pruning makes deep neural networks less robust to adversarial examples."
SP:4b17edaa7ec6201891433320d85f9a415656b763,"This paper proposes a novel approach for interactive fiction (IF) games, where the goal is to generate natural language actions that are coherent, contextually relevant, and able to effect the desired change in the world. The proposed approach builds a dynamic knowledge graph while exploring and generates actions using a template-based action space. The paper claims that the dual uses of the knowledge graph to reason about game state and to constrain natural language generation are the keys to scalable exploration of combinatorially large natural language action. Results across a wide variety of IF games show that KG-A2C outperforms current IF agents despite the exponential increase in action space size."
SP:b1784ecbb8f36eef9cae33d61ce60d80c2f9c38d,"This paper proposes a data-dependent Gaussian prior (D2GPo) to augment the current MLE training with an extra Kullback–Leibler divergence term to alleviate the negative diversity ignorance issue of MLE. Specifically, the proposed method is based on comparing two probability distributions, the first of which is the detailed model from the training prediction and the second is the ground-truth token distribution. Experiments are conducted on supervised and unsupervised machine translation, text summarization, storytelling, and image captioning tasks. The proposed method outperforms the existing methods."
SP:7c29cb5a32b14e1392408dc5daba4cd35848bea9,"This paper proposes to replace the commonly used cross-entropy loss with focal loss. The authors argue that focal loss preserves the confidence of the model’s correct predictions, which is extremely desirable for downstream tasks. They provide a thorough analysis of the factors causing miscalibration, and use the insights to theoretically justify the empirically excellent performance of focal loss, and show that it achieves state-of-the-art accuracy and calibration in almost all cases."
SP:cd6b8417ec8bcb773c78cff677bb0a76d6b3f6f3,"This paper proposes a polynomial optimization framework for computing increasingly tighter upper bounds on the Lipschitz constant of neural networks. The main idea is to use the sparse connectivity of a network, to reduce the complexity of computing the upper bounds. The authors conduct experiments on networks with random weights as well as networks trained on MNIST, showing that the proposed approach yields superior estimates."
SP:31c9dc0dd8806daddc9cb48c56ec819577fe46cd,"This paper proposes a self-supervised learning approach for video features that results in significantly improved performance on downstream tasks (such as video classification, captioning and segmentation) compared to existing methods. The method extends the BERT model for text sequences to the case of sequences of real-valued feature vectors, by replacing the softmax loss with noise contrastive estimation (NCE). The authors also show how to learn representations from sequences of visual features and sequences of words derived from automatic speech recognition (ASR)."
SP:0f24424d10f1201dd25e8c56354e10afc9b2b11c,"This paper proposes a simple yet effective framework that allows to select certain parts of the input data needed for the subsequent application of a given neural network. Both the associated selection masks and the neural network are trained simultaneously such that a good model performance is achieved while, at the same time, only a minimal amount of data is selected. During the inference phase, only the parts selected by the selection masks have to be transferred between the server and the client. The experiments indicate that it is often possible to significantly reduce the amount of the data needed without affecting the model performance much."
SP:aa4fcf5b2cae05c5c6a903c24e4992b56655dee2,"This paper proposes a new loss function for out-of-distribution (OOD) detection based on the Outlier Exposure (OE) technique. The proposed loss function consists of two regularization terms. The first term minimizes the l1 norm between the output distribution given by softmax and the uniform distribution which constitutes a distance metric between the two distributions (Deza & Deza, 2009). The second term minimises the Euclidean distance between the training accuracy of a DNN and its average confidence in its predictions on the training set (Hendrycks et al., 2019). The experiments on CIFAR-10 and ImageNet show that the proposed loss outperforms the previous work."
SP:89bc528ef801182365ac279e8963803afccb391d,"This paper proposes an end-to-end deep learning model, called E2Efold, for RNA secondary structure prediction which can effectively take into account the inherent constraints in the problem. The key idea is to directly predict the RNA base-pairing matrix, and use an unrolled algorithm for constrained programming as the template for deep architectures to enforce constraints. Extensive experiments on benchmark datasets demonstrate the superior performance of E1Efold."
SP:b68560cce8c64ebe0ca5e6534b3732c775d36452,"This paper proposes a model-based reinforcement learning (RL) setting where agents internalize their own predictive models of the environment and form a virtual simulation within which the agent plays trials of the episodes in entirety. The agents take turns to host virtual episodes within which all agents participate and interact with their own biased representations. Since agents’ biases vary, the collective policy developed while sequentially visiting the internal simulations complement one another’s shortcomings. The authors show that the collective policies consistently achieve significantly higher returns than the best individually trained policies."
SP:bd1dc08b4fd9a5cc78d26d7eb7f05dbb4a629ab1,"This paper proposes a dialog generation model that learns a semantic latent space, on which representations of semantically related sentences are close to each other. This latent space is learned by maximizing correlation between the features extracted from prompt and responses. An additional autoencoder is trained, for recovering the full sentence from the latent space. Experimental results show that the proposed model eliminates the generic response problem, while achieving comparable or better coherence compared to baselines."
SP:ef0d5fd333ed60feb3946d24002e9a90642aea66,"This paper proposes a simple yet effective salient explanation method called Gaussian light and shadow (GLAS), which estimates the spatial impact of deep models by the feature perturbation inspired by light/shadows in nature. GLAS provides a useful coarseto-fine control benefiting from scalability of Gaussian mask. The authors also devised the ability to identify multiple instances through recursive GLAS. The effectiveness of GLAS for fine-grained classification is demonstrated on the ImageNet dataset."
SP:d17ca20cc527c28ab7358cb5b14954e5fb56409f,"This paper proposes a method to remove both pixel-wise and channel-wise correlations before the data is fed into each layer of a convolutional neural network. The proposed method can be efficiently calculated at a fraction of the computational cost of the convolution layer. The authors also show that the deconvolution filters in the first layer of the network resemble the center-surround structure found in biological neurons in the visual regions of the brain. Extensive experiments on the CIFAR-10, CifAR-100, MNIST, Fashion-MNIST, Cityscapes, and ImageNet datasets show the effectiveness of the proposed method."
SP:e1b0de9a36bf8359df368b7a55a7f23e99d88db7,"This paper proposes a novel quantization method for GANs based on EM algorithms, named as QGAN. It also proposes a multi-precision algorithm to help find an appropriate quantization precision of GAN given image qualities requirements. Experiments on CIFAR-10 and CelebA show that QGAN can quantize weights in GAN models to even 1-bit or 2-bit representations with results of quality comparable to original models."
SP:58c4905f59f04a50b30d27c99521126a6455d38a,"This paper studies the last-iterate convergence of the Hamiltonian gradient descent (HGD) algorithm for convex-concave min-max problems. In particular, the authors show that HGD achieves linear convergence in a variety of more general settings, including convex concave problems that satisfy a novel “sufficiently bilinear” condition. They also prove convergence rates for stochastic HGD and for some parameter settings of the Consensus Optimization algorithm of Mescheder et al (2017)."
SP:d8556b52272321a1415ac2d85bb12e88b51ee73a,"This paper studies the stability of learning ResNet. Specifically, it considers the ResNet block hl = \phi(hl-1) + \tilde{g}(hl−1) where $\phi$ is the ReLU activation and $g$ is a scalar. The authors show that for standard initialization used in practice, $tau$ = 1/\sqrt{L}$, where L is the number of residual blocks, is a sharp value in characterizing the forward/backward process of ResNet, where stability is guaranteed for $\tau \leq 1/ \sqrt{\log(L)$. Moreover, if ResNet is properly over-parameterized, it is shown that for \tau=1/L, gradient descent is guaranteed to find the global minima 1, which significantly enlarges the range of $Tau$ that admits global convergence."
SP:cf70dc496825ece2f28fdf4f1a6f4316c69e0e48,"This paper proposes a method to train sparse neural networks with a fixed parameter count and a fixed computational cost throughout training, without sacrificing accuracy relative to existing dense-to-sparse training methods. The method updates the topology of the network during training by using parameter magnitudes and infrequent gradient calculations. The authors show that this approach requires fewer floating-point operations (FLOPs) to achieve a given level of accuracy compared to prior techniques."
SP:d2d2b892518d54d0e63e26a056f2298be3be2610,This paper proposes a method to find meaningful directions in the latent space of a generative model that can be used to control specific properties of the generated image. The proposed method is based on the idea that images are characterized by factors of variation (FoV) that are parametrized by a real-valued parameter t. The authors propose to sample directions along which we can move to control precisely specific properties such as the position or scale of the object in the image. Experiments on GANs and variational auto-encoders are conducted to show the effectiveness of the method.
SP:1c63389e972d4652fac831e9d11609cd3c3c371a,"This paper proposes a physics-as-inverse-graphics approach to perform unsupervised physical parameter estimation of systems from video, where the differential equations governing the scene dynamics are known, but labeled states or objects are not available. This framework allows us to perform long term extrapolative video prediction, as well as vision-based model-predictive control. The proposed method is evaluated on 4 datasets with different non-linear interactions and visual difficulty. The results show that the proposed method can learn physical parameters without object or state supervision."
SP:c6b8b682bf3087a65cb2379700b8a0183853c2af,"This paper considers the problem of learning a classifier from noisy labels when a few clean labeled examples are given. The structure of clean and noisy data is modeled by a graph per class and Graph Convolutional Networks (GCN) are used to predict class relevance of noisy examples. For each class, the GCN is treated as a binary classifier learning to discriminate clean from noisy examples using a weighted binary cross-entropy loss function, and then the clean probability is exploited as a relevance measure. The proposed method outperforms the transductive approach (Douze et al. 2018) that is using the same additional data without labels."
SP:dd9c9a5dccbba5dd15b03ca6b314a9e153e95548,"This paper proposes a new objective function for graph neural networks (GNNs) that maximizes the mutual information between edge features and message passing channels. The proposed objective is reformulated as a differentiable objective via a variational approach. The authors theoretically show that the newly introduced objective enables the model to preserve edge information, and empirically corroborate the enhanced performance of MI-maximized models across a broad range of learning tasks including regression on molecular graphs and relation prediction in knowledge graphs."
SP:f1cf63d728da51b4f83eb50ef69e3788b3a5ed74,"This paper proposes a new verifier, called APPROXLINE, that can certify non-trivial properties of generative networks. The proposed method performs both deterministic and probabilistic abstract interpretation and captures infinite sets of outputs of the generative network. The authors show that APPROEXLINE can verify interesting interpolations in the network’s latent space. "
SP:2b0887dcf09249e8cee30d38163aeb9ef1e92b27,"This paper studies the problem of ""suspended animation"" in graph neural networks (GNNs), i.e., when the model depth reaches a certain limit, the model will not respond to the training data any more and become not learnable. The authors propose a new method called GRESNET (Graph Residual Network) to address this problem. The proposed method is based on the graph residual network (GRESnet) framework, which creates extensively connected highways to involve nodes’ raw features or intermediate representations throughout the graph for all the model layers. Experiments on real-world benchmark datasets show the effectiveness of the proposed method."
SP:dc436ade4d04072de35a90e5e4a1bfebfddb04e9,"This paper proposes a semi-supervised method for 3D face reconstruction from a single image. The proposed method is based on linear 3D morphable models (3DMM), which is a linear combination of bases to provide statistical parametric representation of 3D faces. The main contribution of this paper is a novel adversarial loss to train the model on hybrid batches of unlabeled and labeled face images to exploit the value of large amounts of unlabelled face images from unconstrained photo collections. Comprehensive experiments demonstrate that the proposed method produces high-quality reconstruction compared to state-of-the-art methods and is robust to various expression, pose, and lighting conditions."
SP:f7bc06697b09e2d59ec06b2cbcf3c0828ece32ae,"This paper proposes a method for imitation learning in the presence of partial knowledge of the transition kernel. The transition kernel consists of two parts: (1) a synthetic kernel that simulates the transition of state components for which the known transition kernel is known, and (2) a set of demonstrations for which only the state components are known. The authors propose to use a policy gradient algorithm to solve the imitation learning problem in this setting. The proposed method is evaluated on a number of imitation learning tasks, where it is shown to outperform a simulation-free imitation learning method."
SP:82cce92821e8168ab4a6fd67573b66c1d17673b8,"This paper proposes a self-supervised reinforcement learning approach for learning to control states of interest without any external reward function. The intrinsic objective is to reward the skills that maximize the mutual information between the context states and the state of interest. The proposed method is evaluated on two simulated robotic manipulation tasks from OpenAI Gym and a navigation task in the Gazebo simulator. The results show that the proposed method can learn to manipulate the object, such as pushing and picking up, purely based on the intrinsic mutual information rewards."
SP:5db63d39cfd8132bec832ab64b8fbd403b3b8df0,"This paper proposes a more powerful trojaning attack method for large models, which outperforms existing studies in capability, capability, generality, and stealthiness. Specifically, the attack is programmable that the malicious misclassification target is not fixed and can be generated on demand even after the victim’s deployment. The attack is not limited in a small domain; one trojaned model on a large-scale dataset can affect applications of different domains that reuses its general features."
SP:35ea626ee4dd1a7a368a660eb852192924966b7f,"This paper proposes a kernel-based few-shot regression (FSR) algorithm for drug discovery. The authors propose to learn a deep network in combination with a kernel function and a differentiable kernel algorithm. The choice of kernel is critical, and the proposed algorithm learns to find the appropriate kernel for each task during inference. The proposed algorithm outperforms current state-of-the-art algorithms on both toy and novel, real-world drug discovery benchmarks."
SP:91ca4c3ee07617356250bae9f4ef9799b3b134ff,"This paper proposes a framework to characterize which reasoning tasks a network can learn well, by studying how well its computation structure aligns with the algorithmic structure of the relevant reasoning process. The authors formally define this algorithmic alignment and derive a sample complexity bound that decreases with better alignment. This framework offers an explanation for the empirical success of popular reasoning models, and suggests their limitations."
SP:a52aee8da5cf5acd2baf3c2a62cb679e13b18bd5,"This paper proposes a new metric for evaluating conditional generative adversarial networks (cGANs) based on the Fréchet Inception Distance (FJD). FJD is defined as the distance between joint distributions of images and conditioning, which allows it to implicitly capture the aforementioned properties in a single metric. The authors conduct proof-of-concept experiments on a controllable synthetic dataset, which consistently highlight the benefits of FJD when compared to current established metrics. Moreover, they use the newly introduced metric to compare existing cGAN-based models for a variety of conditioning modalities."
SP:fa822e8472efae17c7dfde8258057898383ecbbb,"This paper proposes a method to identify decision states, i.e., the parsimonious set of states where decisions affect the future states an agent can reach in an environment. The paper builds on the VIC framework (Gregor et al., 2016), which maximizes an agent’s ‘empowerment’ and formulate a sandwich bound on the empowerment objective that allows identification of decision states. Unlike previous work (Goyal et al. 2019), the decision states are discovered without extrinsic rewards – simply by interacting with the world. The results show that the proposed method is interpretable and leads to better exploration on downstream goal-driven tasks in partially observable environments."
SP:a19a51df7e28a5d3380be4fba13842efbfe3efec,"This paper proposes a framework for classifying irregularly sampled time series with unaligned measurements, focusing on high scalability and data efficiency. The method SEFT (Set Functions for Time Series) is based on recent advances in differentiable set function learning, extremely parallelizable, and scales well to very large datasets and online monitoring scenarios. The authors extensively compare their method to competitors on multiple healthcare time series datasets and show that it performs competitively whilst significantly reducing runtime."
SP:4ae89d64460b08749acc192004545c1fa8b7553b,"This paper proposes an operation called Harmonic Convolution to enable deep networks to model priors in audio signals by explicitly utilizing the harmonic structure. This is done by engineering the kernels to be supported by sets of harmonic series, instead of by local neighborhoods as convolutional kernels. The experiments show that networks using Harmonic convolution can reliably model audio priors and achieve high performance on unsupervised audio restoration and supervised musical source separation."
SP:c81a2b3fd1c56b9b18e4a358e3ff8b40aea5256a,"This paper proposes a data echoing technique to reduce the total computation used by earlier pipeline stages and speeds up training whenever computation upstream from accelerators dominates the training time. Data echoing reuses (or “echoes”) intermediate outputs from earlier pipeline stage to reclaim idle capacity. The authors investigate the behavior of different data echoing algorithms on various workloads, for various amounts of echoing, and for various batch sizes. The results show that at least one data echoing algorithm can match the baseline’s predictive performance using less upstream computation."
SP:b4cf56d3fa7d65cacde33f17cd04bd5bbc52dd71,"This paper proposes a method for unsupervised representation learning in the context of reinforcement learning. The proposed method combines the ideas of successor features (successor features) and variational intrinsic motivation (VIC) in order to improve the generalization ability of VIC and VIC. In particular, the proposed method learns controllable features that can be leveraged to provide enhanced generalization and fast task inference through the successor features framework. The method is evaluated on the Atari suite and achieves state-of-the-art performance."
SP:83500230586a9134f910ad067b7233dc563dc1ba,"This paper studies the generalization properties of deep neural networks from a functional point of view. In particular, the authors study the smoothness of the CPWL approximation and the delta-slope distribution of the network parameters. They show that for shallow networks with small widths and shallow initializations, the Delta-Slope distributions of the parameters are mean 0 with low standard deviation. In contrast, for deeper networks, the breakpoint distribution grows wider as the network depth increases, allowing deeper networks to generalize to a broader range of inputs. They also show that the depth makes it easier for GD to optimize the resulting network, allowing for a greater flexibility in the movement of breakpoints."
SP:7225825e353b711a7d023f706fafe5e17e4e2fb2,"This paper proposes a GAN-based approach for image-to-image translation. The main idea is to augment the discriminator with an attention mechanism so that it estimates the probability that its input is real, and also does it create an attention map that highlights the critical features for such prediction. This attention map then assists the generator to produce more plausible and realistic images. The proposed approach is evaluated on a number of image transfer tasks."
SP:41c089ba65393174dae1dc136f79030a0a4fc532,"This paper investigates the role of multiplicative interaction layers as a unifying framework to describe a range of classical and modern neural network architectural motifs, such as gating, attention layers, hypernetworks, and dynamic convolutions. The authors show that such layers strictly enrich the representable function classes of neural networks, and conjecture that multiplicative interactions offer a particularly powerful inductive bias when fusing multiple streams of information or when conditional computation is required. They argue that they should be considered in many situation where multiple compute or information paths need to be combined, in place of the simple and oft-used concatenation operation. Finally, they back up their claims by applying them in large-scale complex RL and sequence modelling tasks, where their use allows them to deliver state-of-the-art results."
SP:5144391584e6d3825e12684b7c053e4e282cff2b,"This paper proposes a new algorithm for batch active learning with deep neural network models. The algorithm, Batch Active learning by Diverse Gradient Embeddings (BADGE), samples groups of points that are disparate and high magnitude when represented in a hallucinated gradient space, a strategy designed to incorporate both predictive uncertainty and sample diversity into every selected batch. Crucially, BADGE trades off between uncertainty and diversity without requiring any hand-tuned hyperparameters, making it a useful option for real world active learning problems."
SP:ce6023b1e6bf45b071a6f5457b2575425ae03366,"This paper proposes a novel feature leveling network to isolate low level features from high level features within different layers of the neural network in an unsupervised manner to improve the interpretability of deep neural networks (DNNs). The authors claim that existing deep architectures are hard to interpret because each hidden layer carries a mix of low-level and high-level features. Based on this observation, the authors propose to directly pass features extracted by each layer to the final GLM layer to further improve interpretability. Experimental results show that the modified models are able to achieve competitive results on standard datasets while being more self-explainable."
SP:b70ceead1bf6c7dc684c74501716e7012b891022,"This paper proposes a new approach for extreme classification, i.e. training a classifier over a large number of classes, known as ‘extreme classification’, by drawing negative samples from an adversarial model that mimics the data distribution. The proposed approach is based on the observation that the signal-to-noise ratio in negative sampling is poor since there is no association between input features and their artificial labels. The authors prove that this adversarial sampling minimizes the gradient variance while any bias due to non-uniform sampling can be removed at test time."
SP:29b52fee83309268d9864f3b1fc3617948577d41,"This paper proposes a new approach for efficient exploration in RL that leverages a low-dimensional encoding of the environment learned with a combination of model-based and model-free objectives. The proposed approach uses intrinsic rewards that are based on a weighted distance of nearest neighbors in the low dimensional representational space to gauge novelty and leverage these intrinsic rewards for sample-efficient exploration with planning routines in representational spaces. One key element of the approach is that we perform more gradient steps in-between every environment step in order to ensure the model accuracy. Through this training scheme, the agent is also able to learn a meaningful representation of its state space in an extremely sample efficient manner."
SP:257c98dc1a9f3efcbf9544d9ee2ff524b000543d,"This paper proposes two new tasks for out-of-distribution detection in the few-shot setting and establish benchmark datasets, based on four popular few shot classification datasets. The authors also propose two new methods for this task and investigate their performance. Experimental results show that the proposed methods outperform the baselines."
SP:a3632b773143dfb3a8f104c6b658dfa1167d155b,"This paper proposes a generalized model of sequence generation that unifies decoding in directed and undirected sequence models. The proposed framework models the process of generation rather than a sequence, and under this framework, various neural sequence models such as autoregressive, semi-autoregressive and refinement-based models can be derived as special cases. This unification enables the authors to adapt decoding algorithms originally developed for directed sequence models to undirecting models. They demonstrate this by evaluating various decoding strategies for a cross-lingual masked translation model (Lample and Conneau, 2019). "
SP:eca5e2be9831dfb79c4f5e633cbfadcfd2e00eb1,"This paper proposes a two-stage method for the recognition of mathematical expressions (MEs) from images. In the first stage, an object detection algorithm is used to identify the mathematical symbols of the input image, and in the second stage, a seq2seq model is applied to translate the translated symbols with position information into LaTeX strings. The proposed method is evaluated on two types of datasets (homologous and non homologous) and compared with an end-to-end method (Deng et al. 2016b). The results show that the proposed method achieves better performance than the end-of-the-art method."
SP:923fee8623da1569a7f54a57b4b326f29440b4c0,"This paper proposes a vector quantization method that aims at preserving the quality of the reconstruction of the network outputs rather than its weights. The method only requires a set of unlabelled data at quantization time and allows for efficient inference on CPU by using byte-aligned codebooks to store the compressed weights. They validate their approach by quantizing a high performing ResNet-50 model to a memory size of 5 MB (20x compression factor) while preserving a top-1 accuracy of 76.1% on ImageNet object classification and a Mask R-CNN with a compression factor of 26x. Moreover, their approach generalizes to other tasks such as image detection."
SP:74850ad70241948f93fed95ba1f0ac11360437c1,"This paper proposes a new Transformer-based model that incorporates Tensor-Product Representation (TPR) into the transformer architecture to better support the explicit representation of relation structure. The proposed model is trained end-to-end and infers the correct answer for novel examples without any task-specific structural biases. The paper also proposes a novel attention mechanism, called TP-Attention, which explicitly encodes the relations between each Transformer cell and the other cells from which values have been retrieved by attention. The model is evaluated on the Mathematics Dataset, which contains 56 categories of free-form math wordproblems."
SP:d319df820c6630c409fab32097652a083e8f53ea,"This paper proposes a method to improve the generalization of deep neural networks (DNN) by extending the input features by concatenating encodings of them, and then train the classifier on the extended features. The idea is to use channel codes as a systematic way to increase the degree to which the training and test sets are representative of the empirical sample set. The authors derive a necessary and sufficient condition for generalization using a universal cognitive similarity metric, namely information distance, based on Kolmogorov complexity, and formulate an optimization problem to learn a more general classification function. Experiments are conducted on CIFAR-10 and ImageNet to demonstrate the effectiveness of the proposed method."
SP:b8e86f5e89330d81ba4967a7ed2dbfb56375d8a0,"This paper proposes a new graph pooling method based on compressive Haar transforms, called HaarPooling. The proposed method is based on following a chain of sequential clusterings of the input graph. The input of each pooling layer is transformed by the compressed Haar basis of the corresponding clustering. The authors show that the proposed method achieves state-of-the-art performance on diverse graph classification problems."
SP:17bea301d6718ef5f28864dd2445552b3cf65eeb,"This paper proposes three differentiable point-cloud decoders that map a shape representation to a point feature distribution, allowing an arbitrary number of sampled features to be transformed into individual output points. They develop three sample-based decoder architectures and compare their performance to each other and show their improved effectiveness over feedforward architectures. They also investigate the learned distributions to gain insight into the output transformation."
SP:51d826ead5d1d9cb89d493ce4c39728651bbc57b,"This paper conducts a large-scale study on the generalization ability of deep neural networks (DNNs) trained on real-world noisy labels. The main contribution of this paper is the introduction of a benchmark of real world noisy labels at 10 controlled noise levels. The authors create ten different noise levels from 0% to 80% by gradually replacing the original images with our annotated noisy images. They train DNNs across 10 noise levels, 7 network architectures, 6 existing robust learning methods, and 2 training settings (fine-tuning and random initialization). Their study reveals several interesting findings, including: (1) Deep Neural Networks generalize much better on real world noise. (2) DNN may not learn patterns first on realworld noisy data, (3) When networks are fine-tuned, ImageNet architectures generalize well on noisy data. (4) Real-world noise appears to be less harmful, yet it is more difficult for robust DNN methods to improve. (5) Robust learning methods that work well on synthetic noise may not work as well on real data."
SP:9873f78fb2821afdbb5551700e6ab6a0e8bcb9f0,"This paper proposes a method to combine the efficiency of rules with the quality of instance labels. The authors propose a rule-exemplar method for collecting human supervision. The supervision is coupled such that it is both natural for humans and synergistic for learning. The training algorithm jointly denoises rules via latent coverage variables, and trains the model through a soft implication loss over the coverage and label variables. The denoised rules and trained model are used jointly for inference."
SP:6f2c656dbb7629f652a4291d6971625184d8118b,This paper proposes an efficient memory layer for GNNs that can jointly learn node representations and coarsen the graph. It also introduces two new networks based on this layer: memory-based GNN (MemGNN) and graph memory network (GMN) that can learn hierarchical graph representations. The experimental results show that the proposed models achieve state-of-the-art results in eight out of nine graph classification and regression benchmarks.
SP:81bc52d734c86975d741b6482d65ca71a9d81620,"This paper studies the effect of initialization in deep linear networks. The authors prove that orthogonal initialization speeds up convergence relative to the standard Gaussian initialization with iid weights. They show that for deep networks, the width needed for efficient convergence to a global minimum is independent of the depth and scales linearly in the depth. In addition, they show that gaussian initialization leads to exponentially long convergence time if the width is too small."
SP:9f5d95fc89c2f0d59d04838aa180f3db67997dfa,"This paper studies the problem of how to optimize the bit allocation of weights and activations for deep CNNs compression. The authors first explore the additivity of output error caused by quantization and find that additivity property holds for deep neural networks which are continuously differentiable in the layers. Based on this observation, the authors formulate the optimal bit allocation problem and propose a very efficient method to solve the optimization problem via Lagrangian Formulation. The method obtains excellent results on deep neural network. It can compress deep CNN ResNet-50 down to 2 bits with 0.7% accuracy loss."
SP:7191d7b217a12b1bf9c47d790896a8227c14cc3d,"This paper proposes a novel inference WGAN (iWGAN) model, which is a principled framework to fuse auto-encoders and WGANs. The iWGAN jointly learns an encoder network and a generative network using an iterative primal dual optimization process. The authors establish the generalization error bound of iWANs. They further provide a rigorous probabilistic interpretation of our model under the framework of maximum likelihood estimation. The empirical experiments show that our model greatly mitigates the symptom of mode collapse, speeds up the convergence, and is able to provide a measurement of quality check for each individual sample."
SP:cca6ae14fd0dd12352855e594acf7f3263bb1f24,"This paper proposes a probabilistic approach for crowdsourcing anaphoric annotations. The proposed approach is based on the mention pair model (MPA), which is an extension of the previous work (Paun et al., 2018b). The authors propose to use a nonparametric partially pooled structure (based on a stick breaking process), fitting jointly with the ability of the annotators hierarchical community profiles. The individual estimates can be improved using information about the community when the data is scarce. The authors show that the proposed model performs better than its unpooled counterpart in conditions of sparsity and on par when enough observations are available."
SP:4295cae4a56a02eb21c486408c1bf37a7483cb49,"This paper proposes a hierarchical intrinsic drive (SID), which learns two separate policies to maximize the extrinsic and intrinsic rewards respectively. The intrinsic reward is based on the concept of successor feature control (SFC), which is general and not task-specific and takes into account statistics over complete trajectories and thus differs from previous methods that only use local information to evaluate intrinsic motivation. The experiments on VizDoom, DeepMind Lab and DeepMind Control Suite show that SID significantly improves exploration efficiency with SFC and the hierarchical usage of the intrinsic drives."
SP:9fa22eb03a79bce0fc1c8e84ae8640e010701eca,"This paper proposes a method for weakly-supervised video moment retrieval. The proposed method exploits a multi-level co-attention mechanism, frame-by-word interaction module, and a Word-Conditioned Visual Graph (WCVG) to learn visual-semantic representations that contain contextual information of their relative positions in the temporal sequence through iterative message-passing. Experiments on DiDeMo and Charades-STA datasets demonstrate the effectiveness of the learned representations."
SP:27ac670353f34ee7a23bb7622f80c1dfbc0985e0,"This paper proposes a neural image-guided rendering method that combines image-based rendering (IBR) and GAN-based image synthesis (GAN). The main idea is to learn to synthesize the view-dependent appearance of an object by directly training an object-specific deep neural network. The proposed method first reconstructs a proxy geometry of the object via multi-view stereo. Then, the appearance of a captured view can be warped into a new target view as in traditional IBR. Finally, a composition network can be learned to combine the reprojected images to a final output."
SP:257d124367b1da9a595dc11a9df750d6bade298e,"This paper proposes a sparse representation of model uncertainty for deep neural networks (DNNs) that relies on an inverse formulation of Multivariate Normal Distribution (MND): an information form. The authors show that the model uncertainty can be estimated in this form using a scalable Laplace Approximation scheme, which involves a diagonal correction of the Kronecker-factored eigenbasis. As this makes the inversion of the information matrix intractable an operation that is required for a full Bayesian analysis, the authors devise a novel low-rank approximation of this eigen-basis that exploits spectral sparsity of DNNs. Theoretical analysis and empirical evaluations over various benchmarks show the superiority of the approach over existing methods."
SP:2e03ceba4004b82f86f8349352a8ee4520e9c35d,"This paper proposes a load-balanced hashing method for min-wise hashing (min-hashing), which is based on the idea that the load of the bins (the number of elements in a bin) could be unbalanced, which leads to the existence of empty bins and false similarity computation. The authors propose a method to balance the load so as to reduce the empty bins in advance. The proposed method, called AHash, can generate as few empty bins as possible. The experiments on real datasets validate the claim."
SP:d73827ab98b0ff6bd92abfefea43a5f88ea40de2,This paper proposes a feature extraction method for periodic signals. The authors propose a method for extracting features from phase shift data by adding a graph structure to each data point and constructing a suitable machine learning architecture for graph data with cyclic permutation. The proposed method is evaluated on synthetic and real-world data.
SP:0df5ad333eb4ff9cca7f2d117909e2ce533a65d8,"This paper proposes a confidence-oriented decoder for data-to-text generation, which assigns a confidence score to each target position. This score is learned in training using a variational Bayes objective, and can be leveraged at inference time using a calibration technique to promote more faithful generation. Experiments on the structured data to text dataset, WikiBio, show that the proposed approach is more faithful to the source than existing state-of-the-art approaches."
SP:03307deac29173b2968fbd08f95fc77eb1f82410,"This paper proposes a simple pruning method, coined lookahead pruning, by extending the single layer optimization to a multi-layer optimization. The proposed method consistently outperforms magnitude-based pruning on various networks, including VGG and ResNet, particularly in the high-sparsity regime. The main contribution of this paper is to introduce a functional approximation perspective toward MP and motivate toward a generalization of MP and its variants."
SP:dc80fdc75bc14ae19fe4ba9b85c35ce00b12856f,"This paper proposes Moniqua, a decentralized stochastic gradient descent (SGD) algorithm that allows decentralized SGD to use quantized communication. Theoretically, the authors prove that the communication can be bounded in the number of bits per iteration, while converging at the same asymptotic rate as the original algorithm does with full-precision communication. Empirically, it is shown that the proposed algorithm converges faster with respect to wall clock time than other quantized decentralized algorithms."
SP:86c61a658d07ab86e2d84cef7e480bf7a06e4ddb,"This paper studies the problem of learning partial models in model-based reinforcement learning (MBRL). In particular, the authors identify and clarify a fundamental problem of partial models from a causal-reasoning perspective: they are confounded by the observations they don’t model, and can therefore lead to incorrect planning. To address this problem, this paper introduces a general family of partial model that are provably correct, yet remain fast because they do not need to fully model future observations. The authors also propose a family of viable solutions and empirically investigate their effects on models learned in simple MDPs."
SP:c70479b2096a52584b242de58272ca8d8565feea,"This paper proposes a variational autoencoder (VAE) model that learns a succinct common representation of two correlated data variables for conditional and joint generation tasks. The proposed model is based on two information theoretic problems --distributed simulation and channel synthesis --in which Wyner’s common information arises as the fundamental limit of the succinctness of the common representation. The model decomposes a pair of correlated data into their common representation (e.g., a shared concept) and local representations that capture the remaining randomness in respective data variables by imposing the mutual information as a regularization term. The utility of the proposed approach is demonstrated through experiments for joint and conditional generation with and without style control using synthetic data and real images."
