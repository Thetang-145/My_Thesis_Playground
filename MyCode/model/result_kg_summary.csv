,paper_id,input,output
0,SP:7f4b788b00a2a10bcd60351c3e04c8f597101e96,"local differential privacy constraints FEATURE-OF FL. communication efficiency CONJUNCTION highdimensional compatibility. highdimensional compatibility CONJUNCTION communication efficiency. communication costs EVALUATE-FOR gradient subsampling strategy. randomized rotation USED-FOR quantization error. Method are privacy - preserving FL algorithms, and gradient - based learning algorithm sqSGD. Generic is base algorithm. ","This paper studies the problem of privacy-preserving FL algorithms. In particular, the authors consider FL with local differential privacy constraints. The authors propose a gradient subsampling strategy to reduce communication costs while maintaining highdimensional compatibility. The proposed base algorithm sqSGD is a generalization of the gradient-based learning algorithm. To reduce the quantization error, randomized rotation is used."
1,SP:7f4b788b00a2a10bcd60351c3e04c8f597101e96,low communication algorithm USED-FOR multivariate mean estimation. federated learning setting FEATURE-OF multivariate mean estimation. differentially private communication USED-FOR low communication algorithm. randomized rotation USED-FOR quantization error. quantization CONJUNCTION dimension subsampling. dimension subsampling CONJUNCTION quantization. random orthogonal matrix USED-FOR quantization error. random orthogonal matrix USED-FOR randomized rotation. randomized rotation USED-FOR algorithm. quantization USED-FOR algorithm. dimension subsampling USED-FOR algorithm. algorithm USED-FOR ERM. it USED-FOR SGD. benchmark datasets EVALUATE-FOR algorithm. epsilon CONJUNCTION discretization parameter. discretization parameter CONJUNCTION epsilon. ,"This paper proposes a low communication algorithm for multivariate mean estimation in the federated learning setting using differentially private communication. The algorithm uses randomized rotation on a random orthogonal matrix to reduce the quantization error and dimension subsampling. The proposed algorithm is applied to ERM and it can be applied to SGD as well. Experiments on benchmark datasets show the effectiveness of the proposed algorithm. The main contributions of the paper are: (1) quantization, (2) epsilon and (3) discretization parameter."
2,SP:7f4b788b00a2a10bcd60351c3e04c8f597101e96,"differentially private training algorithm USED-FOR federated learning. privacy mechanism CONJUNCTION random rotation. random rotation CONJUNCTION privacy mechanism. random rotation USED-FOR quantization error. random rotation CONJUNCTION gradient coordinate selection mechanism. gradient coordinate selection mechanism CONJUNCTION random rotation. gradient coordinate selection mechanism USED-FOR communication / computation. algorithm USED-FOR SGD. components USED-FOR SGD. components PART-OF algorithm. random rotation PART-OF components. gradient coordinate selection mechanism PART-OF components. privacy mechanism PART-OF components. algorithm COMPARE baseline algorithm. baseline algorithm COMPARE algorithm. Task is communication reduction. OtherScientificTerm are differential privacy, and high \epsilon local differentially privacy guarantees. ","This paper proposes a differentially private training algorithm for federated learning. The main contribution of this paper is to propose a new algorithm for SGD with three components: a privacy mechanism, a random rotation to reduce the quantization error, and a gradient coordinate selection mechanism to reduce communication/computation. In particular, the authors focus on the problem of communication reduction in the context of differential privacy and provide high \epsilon local differentially privacy guarantees. Experiments show that the proposed algorithm outperforms the baseline algorithm."
3,SP:632666b52c7c551d67fbbe70c06ed589c3a5e187,language statistics CONJUNCTION prior knowledge. prior knowledge CONJUNCTION language statistics. language statistics USED-FOR LSTM based NMT models. prior knowledge USED-FOR LSTM based NMT models. LSTM based NMT models USED-FOR self - attention based NMT models. prior knowledge CONJUNCTION SMT model. SMT model CONJUNCTION prior knowledge. language statistics CONJUNCTION SMT model. SMT model CONJUNCTION language statistics. word frequency information CONJUNCTION prior translation lexicon information. prior translation lexicon information CONJUNCTION word frequency information. prior translation lexicon information USED-FOR bilingual data. word frequency information CONJUNCTION monolingual data. monolingual data CONJUNCTION word frequency information. alternatives USED-FOR prior knowledge. word frequency information HYPONYM-OF alternatives. prior translation lexicon information HYPONYM-OF alternatives. word frequency information HYPONYM-OF prior knowledge. prior translation lexicon information HYPONYM-OF prior knowledge. resources PART-OF hidden representations. self - attention computations USED-FOR hidden representations. method USED-FOR NMT model. WMT17 Zh->En HYPONYM-OF NMT datasets. Task is neural machine translation problem. Method is Transformer model. ,"This paper studies the neural machine translation problem and proposes to use language statistics and prior knowledge to train self-attention based NMT models that combine language statistics with prior knowledge and SMT model. The authors propose alternatives to prior knowledge such as word frequency information, prior translation lexicon information for bilingual data and monolingual data. These resources are added to the hidden representations via self attention computations. The proposed method is applied to a NMT model trained on two NMT datasets (WMT17 Zh->En and Transformer model)."
4,SP:632666b52c7c551d67fbbe70c06ed589c3a5e187,Transformer - based sentence encoders USED-FOR neural machine translation ( NMT ). method USED-FOR Transformer - based sentence encoders. method USED-FOR prior knowledge. prior knowledge PART-OF Transformer - based sentence encoders. matrix USED-FOR prior knowledge. query matrix USED-FOR NMT component. procedure USED-FOR prior knowledge representation matrix. gating mechanism USED-FOR Transformer encoder output. OtherScientificTerm is prior knowledge matrix. Method is Transformer self - attention mechanism. ,"This paper proposes a method to incorporate prior knowledge into Transformer-based sentence encoders for neural machine translation (NMT). The key idea is to use the query matrix of the NMT component as a prior knowledge matrix, and use this matrix to represent the prior knowledge of the Transformer self-attention mechanism. The paper also proposes a gating mechanism to prevent Transformer encoder output from being corrupted. Finally, the paper presents a procedure to convert the prior information representation matrix."
5,SP:632666b52c7c551d67fbbe70c06ed589c3a5e187,method USED-FOR prior knowledge. method USED-FOR Transformer models. prior knowledge PART-OF Transformer models. gating mechanism USED-FOR sefl - attention block. self - attention block USED-FOR prior knowledge. word frequency FEATURE-OF prior knowledge. Generic is approach. ,This paper proposes a method to incorporate prior knowledge into Transformer models. The key idea is to use a gating mechanism in the sefl-attention block to encourage the prior knowledge to be concentrated in the self-attentive block. The approach is based on the observation that prior knowledge with high word frequency is more likely to be clustered.
6,SP:e3e728837f26acb9da283a42c219b6c3b3e131cb,Moving Target Defense ( MTD ) systems HYPONYM-OF defender - attacker game. game - theoretic model USED-FOR Bayesian Stackelberg Markov Games ( BSMGs ). formalism USED-FOR Moving Target Defense ( MTD ) systems. game - theoretic model USED-FOR formalism. defender - attacker game USED-FOR cybersecurity. algorithm USED-FOR Stackelberg equilibrium. Stackelberg equilibrium PART-OF BSMGs. policies USED-FOR MTD settings. BSS - Q algorithm USED-FOR Strong Stackelberg Equilibrium. BSS - Q algorithm USED-FOR BSMGs. Strong Stackelberg Equilibrium PART-OF BSMGs. Method is Markov games. ,"This paper proposes a formalism for Bayesian Stackelberg Markov Games (BSMGs) based on a game-theoretic model of Markov games. The formalism is applied to Moving Target Defense (MTD) systems, a defender-attacker game in the context of cybersecurity. The authors propose an algorithm to find the Steckelberg equilibrium of BSMGs using the BSS-Q algorithm, and show that the proposed policies can be applied to MTD settings."
7,SP:e3e728837f26acb9da283a42c219b6c3b3e131cb,defense methods USED-FOR cybersecurity. model USED-FOR strategic behaviors. optimal movement policy USED-FOR BSMG. Bayesian Strong Stackelberg Q - learning USED-FOR optimal movement policy. Bayesian Strong Stackelberg Q - learning USED-FOR BSMG. Method is Bayesian Stackelberg Markov Games ( BSMG ). ,"This paper studies defense methods in the context of cybersecurity. The authors propose Bayesian Stackelberg Markov Games (BSMG), a model for learning strategic behaviors. BSMG is based on Bayesian Strong Stackekelberg Q-learning to learn an optimal movement policy for each state of the game."
8,SP:e3e728837f26acb9da283a42c219b6c3b3e131cb,Bayesian Stackelberg Markov Game ( BSMG ) model USED-FOR defensive strategies. Bayesian Strong Stackelberg Q - learning method USED-FOR defense policies. Bayesian Strong Stackelberg Equilibrium USED-FOR BSMG. solver USED-FOR Bayesian Strong Stackelberg Equilibrium. Method is game - theoretic models. ,"This paper proposes a Bayesian Stackelberg Markov Game (BSMG) model for learning defensive strategies. The main contribution of this paper is to propose a new defense policies based on the recently proposed Bayesian Strong Stackekelberg Q-learning method. The key idea is to use a solver to approximate the Bayesian version of BSMG, which is an extension of game-theoretic models. "
9,SP:97911e02bf06b34d022e7548beb5169a1d825903,technique USED-FOR disentanglement. latent spaces USED-FOR VAE models. method USED-FOR It. they USED-FOR disentangling. linear transformations CONJUNCTION cross - model ” reconstruction loss. cross - model ” reconstruction loss CONJUNCTION linear transformations. linear transformations USED-FOR VAE ensemble. Generic is models. ,"This paper proposes a technique for disentanglement of VAE models in latent spaces. It is an extension of the method proposed by [1]. The main difference is that instead of using linear transformations in the VAE ensemble, the authors propose to use linear transformations and a “cross-model” reconstruction loss. The authors claim that they are more effective at disentangling the models."
10,SP:97911e02bf06b34d022e7548beb5169a1d825903,"VAE - based approach USED-FOR unsupervised learning of disentangled representations of image data. approach USED-FOR VAEs. VAEs CONJUNCTION pair - wise linear transformations. pair - wise linear transformations CONJUNCTION VAEs. pair - wise linear transformations FEATURE-OF latent spaces. ELBO objectives USED-FOR VAE. decoding accuracy EVALUATE-FOR linearly transformed latent samples. L2 similarity objective CONJUNCTION cross - model decoding objective. cross - model decoding objective CONJUNCTION L2 similarity objective. decoding accuracy EVALUATE-FOR cross - model decoding objective. cross - model decoding objective HYPONYM-OF pressures. L2 similarity objective HYPONYM-OF pressures. pressures PART-OF objective. cross - model decoding objective PART-OF objective. ELBO objectives PART-OF objective. model COMPARE baselines. baselines COMPARE model. disentangling metric EVALUATE-FOR baselines. disentangling metric EVALUATE-FOR model. OtherScientificTerm are VAE latent space, VAE latent spaces, and linear transformations. ",This paper proposes a VAE-based approach for unsupervised learning of disentangled representations of image data. The proposed approach is motivated by the observation that VAEs and pair-wise linear transformations of the latent spaces of VAEs can be seen as a natural extension of the VAE latent space. The paper proposes an objective that combines two existing ELBO objectives for VAE to improve the performance of VAE by introducing two additional pressures: an L2 similarity objective and a cross-model decoding objective that aims to improve decoding accuracy for linearly transformed latent samples. The authors show that the proposed objective can be viewed as a combination of the two previous works that have been proposed in the literature. The main contribution of the paper is that the authors propose to learn VAEs in a way that is compatible with linear transformations. The model is evaluated on a disentangling metric and compared to several baselines.
11,SP:97911e02bf06b34d022e7548beb5169a1d825903,ensemble framework USED-FOR learning disentangled representations. Variational Autoencoders ( VAEs ) USED-FOR ensemble framework. Variational Autoencoders ( VAEs ) USED-FOR learning disentangled representations. VAEs USED-FOR entangled latent representations. linear mappings USED-FOR alignment of latent representations. linear mappings USED-FOR disentanglement. alignment of latent representations CONJUNCTION disentanglement. disentanglement CONJUNCTION alignment of latent representations. VAEs PART-OF VAE ensemble approach. linear mappings USED-FOR VAEs. approach COMPARE VAE. VAE COMPARE approach. approach COMPARE FactorVAE. FactorVAE COMPARE approach. dSprites CONJUNCTION CelebA. CelebA CONJUNCTION dSprites. beta - VAE CONJUNCTION FactorVAE. FactorVAE CONJUNCTION beta - VAE. approach COMPARE beta - VAE. beta - VAE COMPARE approach. VAE CONJUNCTION beta - VAE. beta - VAE CONJUNCTION VAE. FactorVAE disentanglement metric CONJUNCTION Distance to Orthogonality ( DtO ) metric. Distance to Orthogonality ( DtO ) metric CONJUNCTION FactorVAE disentanglement metric. FactorVAE disentanglement metric EVALUATE-FOR baseline methods. VAE COMPARE FactorVAE. FactorVAE COMPARE VAE. dSprites EVALUATE-FOR FactorVAE. Distance to Orthogonality ( DtO ) metric EVALUATE-FOR approach. FactorVAE disentanglement metric EVALUATE-FOR approach. dSprites EVALUATE-FOR approach. OtherScientificTerm is latent space structure. Method is disentangled representations. Generic is framework. ,"This paper proposes an ensemble framework for learning disentangled representations using Variational Autoencoders (VAEs). The VAE ensemble approach consists of several VAEs that learn entangled latent representations using linear mappings to encourage alignment of latent representations and disentanglement. The latent space structure of the latent representations is then used to train the disentrained representations. The proposed framework is evaluated on dSprites, CelebA, where the proposed approach outperforms VAE, beta-VAE, FactorVAE and the Distance to Orthogonality (DtO) metric of baseline methods."
12,SP:59f9de3ebe4a04d2fc8778d8e3415bf85efb7822,method USED-FOR AutoML. meta module USED-FOR method. zero - shot / real - time AutoML HYPONYM-OF model evaluations. graph structure USED-FOR meta module. Generic is pipeline. Task is supervised learning task. Method is meta - training. ,"This paper proposes a method for AutoML that uses a meta module that takes into account the graph structure of the input data. The proposed pipeline is based on the idea that model evaluations (e.g., zero-shot/real-time AutoML) should be performed in a supervised learning task, and that the meta-training should be done in an unsupervised manner."
13,SP:59f9de3ebe4a04d2fc8778d8e3415bf85efb7822,"dataset EVALUATE-FOR ML pipeline. datasets USED-FOR neural network. Generic is pipelines. Method are AutoML algorithms, and NN. OtherScientificTerm is pipeline. ","This paper proposes to evaluate an ML pipeline on a single dataset, rather than on a large number of datasets. The idea is to train pipelines on a small number of data points and then evaluate the performance on a larger dataset. The authors propose AutoML algorithms, where the goal is to find a pipeline that performs best on a given dataset. This is done by training a neural network on a set of datasets and then evaluating the performance of the NN on the new dataset."
14,SP:59f9de3ebe4a04d2fc8778d8e3415bf85efb7822,meta - features USED-FOR meta - learning. language models USED-FOR AutoML meta - learning. auto - sklearn CONJUNCTION TPOT. TPOT CONJUNCTION auto - sklearn. TPOT HYPONYM-OF AutoML systems. auto - sklearn HYPONYM-OF AutoML systems. deep learning components USED-FOR AutoML recommendation architecture. ,"This paper proposes to use meta-features for meta-learning in AutoML by leveraging language models. Two AutoML systems (i.e., auto-sklearn and TPOT) are proposed for this purpose. The authors also propose a new AutoML recommendation architecture based on deep learning components."
15,SP:0f74dff929a4908405ebfa8e60fe1860eec6364f,gradient descent USED-FOR neural networks. GD USED-FOR compositionality. ,"This paper studies gradient descent in neural networks. The authors argue that GD can be used to improve compositionality. The paper is well-written and well-motivated. However, there are a few issues that need to be addressed."
16,SP:0f74dff929a4908405ebfa8e60fe1860eec6364f,"gradient descent training USED-FOR model. gradient descent training USED-FOR compostionality. redundant information USED-FOR gradient descent trained model. Method is gradient descent. OtherScientificTerm are mapping, and model architecture. ","This paper studies the effect of gradient descent training on the model’s compostionality. The authors argue that gradient descent trains a model with redundant information, and that this redundant information can be used to improve the performance of the gradient descent trained model. To this end, the authors propose a mapping from the input to the model architecture."
17,SP:0f74dff929a4908405ebfa8e60fe1860eec6364f,gradient descent methods USED-FOR compositionality. gradient descent methods USED-FOR compositional generalization of models. compositionality CONJUNCTION compositional generalization of models. compositional generalization of models CONJUNCTION compositionality. optimization process USED-FOR models. conditional independence USED-FOR compositionality. neuro - symbolic approaches CONJUNCTION common sense reasoning. common sense reasoning CONJUNCTION neuro - symbolic approaches. common sense reasoning CONJUNCTION disentangled representation. disentangled representation CONJUNCTION common sense reasoning. vision CONJUNCTION language. language CONJUNCTION vision. language CONJUNCTION neuro - symbolic approaches. neuro - symbolic approaches CONJUNCTION language. compositionality PART-OF human intelligence. disentangled representation HYPONYM-OF AI / ML. vision HYPONYM-OF AI / ML. language HYPONYM-OF AI / ML. common sense reasoning HYPONYM-OF AI / ML. neuro - symbolic approaches HYPONYM-OF AI / ML. visual reasoning HYPONYM-OF representative problems. compositionality CONJUNCTION compositional generalization. compositional generalization CONJUNCTION compositionality. compositionality HYPONYM-OF quantitative approaches. compositional generalization HYPONYM-OF quantitative approaches. Method is optimizers. OtherScientificTerm is Raven progressive matrices. ,"This paper proposes gradient descent methods to improve compositionality and compositional generalization of models. The authors argue that the optimization process of models should be compositional, and that compositionality can be achieved through conditional independence. They propose to use Raven progressive matrices to encourage the optimizers to learn compositionality. They show that the compositionality of human intelligence can be observed in a variety of AI/ML, including vision, language, neuro-symbolic approaches, common sense reasoning, disentangled representation, etc. They also show that representative problems such as visual reasoning can be learned. They discuss the relationship between compositionality in human intelligence and other quantitative approaches such as compositional learning, compositional generative learning, and compositionally generalization."
18,SP:f99a1b2dbcb7a7b30dbfcfc60668e94b4ad53410,"approach USED-FOR KG embedding. NeoEA USED-FOR KG embedding. ontology knowledge USED-FOR KG embedding. TransE USED-FOR scoring functions. constraints COMPARE baselines. baselines COMPARE constraints. Method are embedding - based entity alignment methods, and NeoEA architecture. OtherScientificTerm are embedding features, and KG - invariant ontology knowledge. ","This paper presents an approach to learn KG embedding using ontology knowledge, which is an important problem for embedding-based entity alignment methods. The authors propose a new approach called NeoEA to learn the KG of an entity using only embedding features. The core idea of the proposed NeoEA architecture is to use TransE as the scoring functions, and then use the scores obtained by TransE to train the score of the embedding of the entity using KG-invariant ontology. The proposed constraints are compared to several baselines."
19,SP:f99a1b2dbcb7a7b30dbfcfc60668e94b4ad53410,"Entity alignment USED-FOR cross - lingual knowledge graphs. embedding - based methods USED-FOR semantic space. embedding space FEATURE-OF cross - lingual KGs. conditional neural and basic axioms USED-FOR labelled data. OtherScientificTerm are entity cross knowledge graphs, entity - level granular, and neural axioms. ","Entity alignment is an important problem for cross-lingual knowledge graphs. However, existing embedding-based methods do not capture the semantic space of the entity cross knowledge graphs and do not consider entity-level granular. This paper proposes to use neural axioms to model the embedding space of cross-languages KGs. Specifically, the authors propose conditional neural and basic axiom for modelling the labelled data."
20,SP:f99a1b2dbcb7a7b30dbfcfc60668e94b4ad53410,"embedding spaces USED-FOR KGs. margin USED-FOR less constrained / long - tail entities. models USED-FOR entity alignment. model COMPARE models. models COMPARE model. SEA CONJUNCTION RDGCN. RDGCN CONJUNCTION SEA. model USED-FOR RDGCN. model USED-FOR SEA. OtherScientificTerm are conditional ) neural axioms, and OWL2 properties. Generic is method. ","This paper proposes to learn embedding spaces for KGs using (conditional) neural axioms. Specifically, the authors propose to use margin to model less constrained/long-tail entities. The authors show that the proposed method is able to achieve state-of-the-art performance on both SEA and RDGCN using the proposed model. The proposed model is shown to outperform existing models for entity alignment in terms of (OWL2 properties)."
21,SP:0e42de72d10040289283516ec1bd324788f7d371,deep model USED-FOR visual recognition tasks. accuracy CONJUNCTION energy consumption. energy consumption CONJUNCTION accuracy. differentiable network architecture search ( DARTS ) USED-FOR deep model. phase mask PART-OF lensless camera. 2D convolutions USED-FOR phase mask. face recognition CONJUNCTION head pose estimation. head pose estimation CONJUNCTION face recognition. image classification CONJUNCTION face recognition. face recognition CONJUNCTION image classification. simulated data USED-FOR vision tasks. real world camera FEATURE-OF fabricated masks. vision tasks EVALUATE-FOR method. masks USED-FOR method. head pose estimation HYPONYM-OF vision tasks. image classification HYPONYM-OF vision tasks. face recognition HYPONYM-OF vision tasks. simulated data EVALUATE-FOR method. energy level USED-FOR recognition. OtherScientificTerm is sensor configuration. Method is architecture search. ,"This paper proposes a differentiable network architecture search (DARTS) to train a deep model for visual recognition tasks that aims to balance accuracy and energy consumption. The key idea is to add a phase mask to a lensless camera using 2D convolutions. The proposed method is evaluated on simulated data as well as on vision tasks (image classification, face recognition, and head pose estimation) using fabricated masks from a real world camera. The paper shows that the energy level of the recognition can be controlled by the sensor configuration and the architecture search."
22,SP:0e42de72d10040289283516ec1bd324788f7d371,method USED-FOR CNN - powered Phlatcam. SACoD USED-FOR CNN - powered Phlatcam. PhlatCam sensor CONJUNCTION backend CNN model. backend CNN model CONJUNCTION PhlatCam sensor. method USED-FOR PhlatCam sensor. method USED-FOR backend CNN model. coded mask PART-OF Phlatcam. convolution layer FEATURE-OF coded mask. optical layer HYPONYM-OF coded mask. energy saving CONJUNCTION model compressing. model compressing CONJUNCTION energy saving. model compressing CONJUNCTION accuracy. accuracy CONJUNCTION model compressing. model compressing EVALUATE-FOR it. energy saving EVALUATE-FOR it. accuracy EVALUATE-FOR it. OtherScientificTerm is neural network weights. ,"This paper proposes a method called SACoD to improve the performance of CNN-powered Phlatcam. The method is applied to both the PhlatCam sensor and the backend CNN model. The main idea is to add a coded mask to the convolution layer, which is similar to the optical layer used in the original neural network weights. Experiments show that it improves the energy saving, model compressing, and accuracy."
23,SP:0e42de72d10040289283516ec1bd324788f7d371,"convolutional neural network design USED-FOR task. PhlatCam HYPONYM-OF lensless imaging system. limited resource budgets FEATURE-OF IoT system. task accuracy ’s EVALUATE-FOR IoT devices. optical layer design USED-FOR features. SACoD sensor + CNN COMPARE baseline models. baseline models COMPARE SACoD sensor + CNN. SACoD COMPARE sensor / CNN joint - optimizations. sensor / CNN joint - optimizations COMPARE SACoD. accuracy / efficiency curve EVALUATE-FOR SACoD. ablation studies EVALUATE-FOR SACoD. Generic is framework. OtherScientificTerm are network architecture, phase masks, and mask fabrication process. ","This paper proposes a novel framework called SACoD (Self-Controlled Autoencoders with Gradient Co-Optimization) to improve the task accuracy’s of IoT devices with limited resource budgets. The proposed framework is based on a convolutional neural network design for a given task, where the network architecture is trained on a set of phase masks, and the optical layer design is used to encode the features. The paper presents a lensless imaging system, called PhlatCam, to demonstrate the effectiveness of the proposed mask fabrication process. The SACOD sensor + CNN is compared with several baseline models, and compared to sensor/CNN joint-optimizations, and shows a significant improvement in accuracy/efficiency curve. The authors also conduct ablation studies to show the effectiveness and efficiency of SACoders."
24,SP:493afcfa3fd64967785928ba3acecf3ffa6ce579,"method USED-FOR non - negative matrix factorization. non - negative matrix factorization USED-FOR timeseries. longitudinal honey bee interaction data EVALUATE-FOR it. basis functions USED-FOR lifetime embedding. regularizations FEATURE-OF temporal basis functions. regularizations CONJUNCTION embedding coefficients. embedding coefficients CONJUNCTION regularizations. Generic is model. OtherScientificTerm are factors, global trajectory, factor embedding, and 32 - dimensional space. ","This paper proposes a method for non-negative matrix factorization for timeseries. The model is based on the observation that the factors can be expressed as a global trajectory, and it is evaluated on longitudinal honey bee interaction data. The key idea is to learn basis functions for the lifetime embedding, which is a 32-dimensional space, and to use these basis functions as regularizations for temporal basis functions. The authors show that the factor embedding is invariant to these regularizations and embedding coefficients."
25,SP:493afcfa3fd64967785928ba3acecf3ffa6ce579,"2 - d tensor FEATURE-OF matrix. method COMPARE non - negative variant. non - negative variant COMPARE method. PARAFAC / CANDECOMP HYPONYM-OF non - negative variant. Method are matrix factorization model, modeling framework, and tensor methods. Generic is problem. Task is * tensor * factorization problem. ","This paper proposes a matrix factorization model. The problem is formulated as a matrix on a 2-d tensor, and the authors propose a modeling framework for the problem. The proposed method is compared to a non-negative variant called PARAFAC/CANDECOMP. The authors also provide a theoretical analysis of the *tensor* factorization problem. Finally, the authors conduct experiments to compare the proposed tensor methods."
26,SP:493afcfa3fd64967785928ba3acecf3ffa6ce579,A CONJUNCTION F. F CONJUNCTION A. social datasets USED-FOR F. Method is NMF formulation. OtherScientificTerm is embeddings. ,"This paper proposes an extension of the NMF formulation to the case where the embeddings of A and F are given by social datasets. The main contribution of this paper is to provide a theoretical analysis of the relationship between A, F, and A. "
27,SP:08ae056f269c731b92b5a3d59e18f9ccfc0b703c,MR physics USED-FOR data augmentation methods. pixel preserving augmentations CONJUNCTION general affine augmentations. general affine augmentations CONJUNCTION pixel preserving augmentations. image domain FEATURE-OF real and imaginary values. real and imaginary values FEATURE-OF general affine augmentations. general affine augmentations PART-OF augmentation. pixel preserving augmentations PART-OF augmentation. data generation USED-FOR accelerated MRI task. k - space data USED-FOR data generation. Task is accelerated MRI. OtherScientificTerm is k - space domain. Generic is schedules. ,This paper studies the problem of data augmentation methods based on MR physics. The proposed augmentation consists of pixel preserving augmentations and general affine augmentations with both real and imaginary values in the image domain. The data generation for accelerated MRI task is based on k-space data. The authors propose two schedules for the accelerated MRI. The first schedule is to generate the data in the k-spaces domain and the second schedules are to generate data on the real space.
28,SP:08ae056f269c731b92b5a3d59e18f9ccfc0b703c,data - augmentation pipeline USED-FOR MRI reconstruction. k - space data USED-FOR image reconstructions. noise characteristics FEATURE-OF image data. it USED-FOR small training sets. Generic is pipeline. Method is data augmentation. Material is problem domain. ,This paper proposes a new data-augmentation pipeline for MRI reconstruction. The pipeline is based on the idea that image reconstructions on k-space data can be improved by adding noise characteristics to the image data. The authors show that the proposed data augmentation is effective and that it can be applied to small training sets. The paper also provides a theoretical analysis of the problem domain.
29,SP:08ae056f269c731b92b5a3d59e18f9ccfc0b703c,data augmentation USED-FOR accelerated MRI reconstruction. data augmentation USED-FOR method. image augmentation methods USED-FOR problem. complex valued FEATURE-OF MR images. transformations USED-FOR noise distribution. Material is MRI data. ,This paper proposes a method for accelerated MRI reconstruction using data augmentation. The problem is well-motivated by the fact that MR images have a complex valued and the existing image augmentation methods fail to address this problem. The proposed method is based on the observation that the noise distribution of MRI data can be approximated by simple transformations.
30,SP:3fdaae674a2b9d437a43d32778437dc7df9c1686,decomposition way of feature PART-OF computer vision. latent vector of the encoder USED-FOR feature. comparator loss USED-FOR latent vector of the encoder. OtherScientificTerm is network structure. Method is encoder. ,This paper proposes a decomposition way of feature in computer vision. The key idea is to use the latent vector of the encoder as a feature and use a comparator loss to compare the latent vectors of different layers of the network structure. The encoder is then used to decompose the feature.
31,SP:3fdaae674a2b9d437a43d32778437dc7df9c1686,"order learning USED-FOR ordinal classification function. Generic are methods, and approach. ",This paper proposes to use order learning to learn an ordinal classification function. The main contribution of this paper is that it proposes a novel approach that is based on the observation that existing methods do not generalize well to unseen data points. This is an interesting idea and the paper is well-written and well-motivated.
32,SP:3fdaae674a2b9d437a43d32778437dc7df9c1686,"task related features CONJUNCTION non - task related features. non - task related features CONJUNCTION task related features. task related features PART-OF encoding feature space. it USED-FOR estimating rank. MAP estimation algorithm USED-FOR rank. extension USED-FOR clustering algorithm. repulsive term USED-FOR clustering algorithm. repulsive term USED-FOR extension. Generic is It. OtherScientificTerm are comparator, and max possible rank. ",This paper proposes an extension to the clustering algorithm by adding a repulsive term to the encoding feature space that encourages task related features and non-task related features to be close to each other. It is based on the idea that the comparator should not be too far away from the max possible rank. The authors show that it can be used for estimating rank using the MAP estimation algorithm.
33,SP:a5775441639529d61b7fee4b4298fd82a0c93bb5,"exploration algorithm USED-FOR procedurally generated environments. RAPID HYPONYM-OF exploration algorithm. local and global score PART-OF exploration scores. OtherScientificTerm are local score, and global score. ","This paper proposes a new exploration algorithm, RAPID, for procedurally generated environments. The main idea is to combine the local and global score in the exploration scores. The local score is used to estimate the difficulty of the environment, while the global score measures the quality of the generated environment."
34,SP:a5775441639529d61b7fee4b4298fd82a0c93bb5,exploration method USED-FOR procedurally - generated environments. RAPID HYPONYM-OF exploration method. local score CONJUNCTION global score. global score CONJUNCTION local score. exploration scores CONJUNCTION local score. local score CONJUNCTION exploration scores. global score USED-FOR long - term and historical view of exploration. local score USED-FOR per - episode view of the exploration behavior. exploration scores CONJUNCTION extrinsic reward. extrinsic reward CONJUNCTION exploration scores. extrinsic reward USED-FOR episodic exploration score. episodic exploration score USED-FOR state - action pairs. state - of - the - art algorithms USED-FOR procedurally - generated environments. OtherScientificTerm is exploration behavior. Method is agent. ,"This paper proposes RAPID, an exploration method for procedurally-generated environments, which is a generalization of the state-of-the-art exploration method (Chen et al., 2018). The main idea is to combine exploration scores, local score, and global score to provide a long-term and historical view of exploration. The local score is used to capture a per-episode view of the exploration behavior, while the global score captures the per-step view of each episode. The exploration scores and the extrinsic reward are used to generate an episodic exploration score for state-action pairs, which are then used to train the agent. The experiments show that the proposed algorithm outperforms the state of the-art algorithms on procedurally generated environments."
35,SP:a5775441639529d61b7fee4b4298fd82a0c93bb5,"deep RL USED-FOR procedurally - generated environments. exploration USED-FOR deep RL. exploration USED-FOR procedurally - generated environments. global exploration score CONJUNCTION extrinsic reward. extrinsic reward CONJUNCTION global exploration score. local exploration score CONJUNCTION global exploration score. global exploration score CONJUNCTION local exploration score. local exploration score EVALUATE-FOR agent - generated episode. behavioral cloning USED-FOR policy. policy USED-FOR agent - generated episodes. Method are exploration techniques, and RAPID. Generic is approach. OtherScientificTerm is replay buffer. ","This paper proposes to use exploration in deep RL in procedurally-generated environments to improve the performance of exploration in the context of deep RL. The proposed approach, called RAPID, is motivated by the observation that existing exploration techniques do not generalize well to new environments. To address this issue, the authors propose to use behavioral cloning to learn a policy that can generate agent-generated episodes with a local exploration score, a global exploration score and an extrinsic reward. The authors also propose a replay buffer to store the agent's experience from previous episodes."
36,SP:30024ac5aef153ae24c893a53bad93ead2526476,prototypes USED-FOR classifier. isometric propagation network USED-FOR graph. propagation USED-FOR isometric propagation network. visual and semantic space FEATURE-OF graph. attention USED-FOR graph edges. attention USED-FOR graph. episodic training method USED-FOR learning. Task is zero - shot classification. Method is embedding. OtherScientificTerm is attribute information. ,"This paper addresses the problem of zero-shot classification, where the classifier is trained on prototypes generated by a set of prototypes. The authors propose an isometric propagation network that uses propagation to generate a graph in both visual and semantic space, which is then used to train a classifier. The key idea is to use attention to map the graph edges to the embedding of the original embedding. The learning is done using an episodic training method where the attribute information is learned over time."
37,SP:30024ac5aef153ae24c893a53bad93ead2526476,"prototypes USED-FOR Zero Shot Classification. method USED-FOR prototypes. episode learning setting USED-FOR method. method COMPARE method. method COMPARE method. OtherScientificTerm are visual and semantic prototype, and visual example + prototype. ","This paper proposes a method for learning prototypes for Zero Shot Classification using an episode learning setting. The proposed method learns a visual and semantic prototype for each episode, where the visual example + prototype is a sequence of episodes, and the semantic prototype is an instance of the episode. The experimental results show that the proposed method outperforms the existing method."
38,SP:30024ac5aef153ae24c893a53bad93ead2526476,attributes CONJUNCTION relevant textual information. relevant textual information CONJUNCTION attributes. computational pipeline USED-FOR zero - shot learning. side information USED-FOR semantic embedding. relevant textual information HYPONYM-OF side information. attributes HYPONYM-OF side information. visual and semantic prototypes USED-FOR gradient descent. graph USED-FOR gradient descent. prior methods USED-FOR zero - shot learning. method COMPARE prior methods. prior methods COMPARE method. method COMPARE discriminative and generative methods. discriminative and generative methods COMPARE method. zero - shot learning EVALUATE-FOR method. discriminative and generative methods HYPONYM-OF prior methods. Material is visual instances. OtherScientificTerm is topological relationship. ,"This paper proposes a computational pipeline for zero-shot learning. The main idea is to use side information (attributes and relevant textual information) to improve the semantic embedding. The visual and semantic prototypes are used to guide the gradient descent on the graph. The key idea is that the visual instances should have a topological relationship to the semantic instances. The proposed method is compared with prior methods for zero - shot learning, including discriminative and generative methods."
39,SP:1d7c174f4f7a0eb26edceecc117f9af1528802e5,HyperGrid Transformer approach USED-FOR fine - tuning. hypernetworks USED-FOR it. tasks EVALUATE-FOR transformer model. tasks EVALUATE-FOR model. OtherScientificTerm is transformer block. Method is hyper - networks. Material is GLUE / SuperGLUE datasets. ,"This paper proposes a HyperGrid Transformer approach for fine-tuning. Specifically, it uses hypernetworks to augment the original transformer block and then fine-tune it using the learned hyper-networks. Experiments on GLUE/SuperGLUE datasets show that the proposed model can achieve state-of-the-art performance on a variety of tasks."
40,SP:1d7c174f4f7a0eb26edceecc117f9af1528802e5,grid - wise projections USED-FOR weight matrices. decomposable hypernet - work USED-FOR HyperGrid Transformers. it USED-FOR natural language understanding. model parameters USED-FOR natural language understanding. model parameters USED-FOR it. GLUE CONJUNCTION SuperGLUE. SuperGLUE CONJUNCTION GLUE. single model COMPARE multiple task - specific models. multiple task - specific models COMPARE single model. single model COMPARE baseline. baseline COMPARE single model. baseline COMPARE multiple task - specific models. multiple task - specific models COMPARE baseline. Generic is models. ,"This paper proposes a decomposable hypernet-work for HyperGrid Transformers, where grid-wise projections are used to compute the weight matrices. The proposed models, GLUE and SuperGLUE, are trained in a supervised fashion, and it can be used for natural language understanding. The experiments show that the proposed single model outperforms the baseline and multiple task-specific models."
41,SP:1d7c174f4f7a0eb26edceecc117f9af1528802e5,"HyperGrid Transformer USED-FOR model. model USED-FOR multi - tasks. multi - tasks PART-OF NLP. HyperGrid Transformer USED-FOR task - conditional dynamic weights. weights PART-OF local and global components. parameter cost EVALUATE-FOR multi - task network. OtherScientificTerm are feed - forward layers, and GLUE / SuperGLUE. ","This paper proposes a model based on HyperGrid Transformer to learn multi-tasks in NLP. The authors propose to learn task-conditional dynamic weights based on the hypergrid Transformer, which allows the model to generalize to multi-teams. The weights are split into local and global components, where the weights are learned using the feed-forward layers of GLUE/SuperGLUE. Experiments show that the proposed multi-task network can reduce the parameter cost by a factor of 2."
42,SP:d957241c02163c1c5bc03a688aa4a2eb486fb9f1,adaptive data augmentation algorithm USED-FOR random perturbations. adaptive data augmentation algorithm USED-FOR imitation learning - based self - driving network. seen and unseen perturbation types FEATURE-OF simulated data. simulated data USED-FOR Validation. Task is network. OtherScientificTerm is perturbations. Method is automated perturbed training dataset selection mechanism. ,This paper proposes an adaptive data augmentation algorithm for random perturbations to an imitation learning-based self-driving network. Validation is performed on simulated data with seen and unseen perturbation types. The authors show that the network is able to generalize to unseen and unseen-type perturbated data. They also propose an automated perturbed training dataset selection mechanism.
43,SP:d957241c02163c1c5bc03a688aa4a2eb486fb9f1,data augmentation CONJUNCTION adversarial examples. adversarial examples CONJUNCTION data augmentation. method USED-FOR ML models. ML models USED-FOR vehicle steering. data augmentation USED-FOR method. adversarial examples USED-FOR method. accuracy EVALUATE-FOR model. method USED-FOR model. accuracy EVALUATE-FOR method. blur CONJUNCTION distortion. distortion CONJUNCTION blur. distortion CONJUNCTION color representation. color representation CONJUNCTION distortion. color representation HYPONYM-OF transform. blur HYPONYM-OF transform. distortion HYPONYM-OF transform. method USED-FOR training. driving dataset EVALUATE-FOR approach. OtherScientificTerm is transformed - datasets. Metric is mean validation accuracy. Method is steering model. ,"This paper proposes a method to train ML models for vehicle steering using data augmentation and adversarial examples. The method aims to improve the accuracy of the model by training on transformed-datasets where the transform (blur, distortion, color representation, etc) are used. The approach is evaluated on a driving dataset, where the mean validation accuracy is used to evaluate the performance of the steering model."
44,SP:d957241c02163c1c5bc03a688aa4a2eb486fb9f1,algorithm USED-FOR model generalization. baseline learning algorithm USED-FOR degraded images. training algorithm USED-FOR min - max optimization problem. Generic is method. ,"This paper proposes a new algorithm for model generalization. The main idea is to use a baseline learning algorithm to generate degraded images. Then, a new training algorithm is proposed to solve the min-max optimization problem. Experiments are conducted to show the effectiveness of the proposed method."
45,SP:2df9ba21f72e041f80c7bc9ecfe89353f172b058,framework USED-FOR constraints in optimization problems. neural networks USED-FOR framework. deep neural network frameworks USED-FOR constraints. OtherScientificTerm is equality and inequality constraints. Task is large scale settings. ,"This paper proposes a framework for learning constraints in optimization problems using neural networks. The main contribution of this paper is to provide a theoretical analysis of the relationship between equality and inequality constraints. This is an important problem in large scale settings, and the authors propose deep neural network frameworks for learning these constraints."
46,SP:2df9ba21f72e041f80c7bc9ecfe89353f172b058,"deep neural networks USED-FOR constrained optimization problems. method USED-FOR neural networks. synthetic quadratic programs CONJUNCTION problems. problems CONJUNCTION synthetic quadratic programs. AC power flow application FEATURE-OF problems. Generic are methods, and network. OtherScientificTerm are arbitrary constraints, hard equality and inequality constraints, and equalities. Method is gradient steps. ",This paper proposes a method for training neural networks to solve constrained optimization problems. Previous methods have been limited to arbitrary constraints (e.g. hard equality and inequality constraints). This paper proposes to train a network that is able to solve problems with equalities. The paper presents experiments on synthetic quadratic programs and problems with AC power flow application. The experiments show that the proposed gradient steps are effective.
47,SP:2df9ba21f72e041f80c7bc9ecfe89353f172b058,"method USED-FOR hard constraints. Method is neural network. OtherScientificTerm are differentiability, predicted variables, equality constraints, gradient steps, and inequality constraints. ",This paper proposes a method for enforcing hard constraints on a neural network. The main idea is to enforce differentiability between the predicted variables and the observed variables by enforcing equality constraints on the gradient steps. The paper provides a theoretical analysis of how to enforce inequality constraints.
48,SP:61a0163b21dc8f92dd699c1e154f53d30c80b2fe,penalty factors USED-FOR regularization - based pruning methods. increasing regularization USED-FOR filters. L1 - norm USED-FOR filters. Generic is algorithm. ,This paper studies the penalty factors for regularization-based pruning methods. The authors propose an algorithm for increasing the L1-norm of the filters with increasing regularization. The algorithm is based on the idea that penalizing the weights of the weights should be proportional to the number of samples. 
49,SP:61a0163b21dc8f92dd699c1e154f53d30c80b2fe,"regularization USED-FOR network. regularization USED-FOR pruning scenario. L2 regularization USED-FOR It. L2 regularization USED-FOR neurons. neuron's importance PART-OF neuron network. coefficient \lambda FEATURE-OF regularization term. L2 regularization USED-FOR It. penalty term USED-FOR converged network. Hessian information USED-FOR algorithm. scoring criterion USED-FOR pruning process. schedule CONJUNCTION scoring criterion. scoring criterion CONJUNCTION schedule. Generic are scenario, benchmarks, and method. Method is pruning schedule. OtherScientificTerm are importance score, weight change, and Hessian. ","This paper studies the problem of regularization in a pruning scenario. It uses an L2 regularization on the neurons to reduce the importance of the network. In this scenario, the regularization term is defined as the coefficient \lambda of the difference between the neuron's importance and the importance score of the neuron network. A penalty term is added to the converged network to encourage the weight change to be small. The algorithm is trained using Hessian information. The authors provide a set of benchmarks to evaluate the proposed method. The results show that the proposed pruning schedule and scoring criterion can speed up the pruning process."
50,SP:61a0163b21dc8f92dd699c1e154f53d30c80b2fe,"L2 regularization USED-FOR deep network pruning. first HYPONYM-OF L1 - norm based filter pruning method. L1 - norms USED-FOR important / unimportant filters. rising L2 regularization FEATURE-OF filters. Hessian values HYPONYM-OF local curvatures. CIFAR10/100 CONJUNCTION ImageNet benchmarks. ImageNet benchmarks CONJUNCTION CIFAR10/100. methods COMPARE state - of - the - art methods. state - of - the - art methods COMPARE methods. ImageNet benchmarks EVALUATE-FOR methods. CIFAR10/100 EVALUATE-FOR methods. Generic are algorithms, algortihm, and method. Method are GReg-1, rising penalty scheme, Greg-2, and L1 - norm criterion. ","This paper studies the problem of deep network pruning with L2 regularization. The authors propose two algorithms. The first is an L1-norm based filter pruning method, called GReg-1. The main idea is to use L1 norms to distinguish important/unimportant filters. The second algorithm, called Greg-2, is based on the idea of the rising penalty scheme. The idea is that the local curvatures (i.e. Hessian values) of the filters are subject to the rising L2 standardization. This method is called algortihm. The proposed methods are evaluated on CIFAR10/100 and ImageNet benchmarks, where the proposed methods outperform the state-of-the-art methods. "
51,SP:7b2bf0e36c926d1ed5ab9593a11e4ebce49df6ba,planning USED-FOR model - based reinforcement learning settings. planning USED-FOR MBRL agents. planning USED-FOR MBRL. planning USED-FOR generalization. MuZero USED-FOR learning challenges. Generic is algorithm. ,"This paper studies the problem of planning in model-based reinforcement learning settings. The authors argue that planning is important for MBRL agents and that planning can help improve generalization. To this end, the authors propose an algorithm called MuZero to address the learning challenges."
52,SP:7b2bf0e36c926d1ed5ab9593a11e4ebce49df6ba,planning USED-FOR model - based reinforcement learning. ablations USED-FOR MuZero. ablations USED-FOR planning. it USED-FOR MuZero. planning USED-FOR MuZero. OtherScientificTerm is planner settings. Method is MBRL. ,"This paper studies the problem of planning in model-based reinforcement learning. The authors propose to use ablations to improve planning in MuZero, and show that it improves the performance of MuZero. The planner settings are different from the standard MBRL."
53,SP:7b2bf0e36c926d1ed5ab9593a11e4ebce49df6ba,"Hero CONJUNCTION Minipacman. Minipacman CONJUNCTION Hero. Minipacman CONJUNCTION Sokoban. Sokoban CONJUNCTION Minipacman. planning USED-FOR model - based reinforcement learning agent. discrete action spaces FEATURE-OF tasks. tasks EVALUATE-FOR MuZero. Humanoid HYPONYM-OF tasks. Acrobot HYPONYM-OF tasks. Sokoban HYPONYM-OF tasks. Hero HYPONYM-OF tasks. Minipacman HYPONYM-OF tasks. serving USED-FOR policy improvement target. Monte - Carlo rollout USED-FOR MBRL. search budget USED-FOR MBRL agent. Search USED-FOR zero - shot generalization. OtherScientificTerm are search, and policy target. Task is exploration. Method is Deep tree search. Generic is model. ","This paper proposes a model-based reinforcement learning agent that uses planning to improve the generalization of a model. The authors evaluate MuZero on a variety of tasks with discrete action spaces (e.g., Humanoid, Acrobot, Minipacman, Sokoban) and show that MuZero achieves state-of-the-art performance on most of these tasks. The main contribution of the paper is that the authors propose to use a Monte-Carlo rollout to train MBRL, where the search is performed in parallel with the exploration. Deep tree search is used to find the best policy improvement target from the serving, which is then used to train the model. Search is also used to improve zero-shot generalization. The MBRL agent is trained with a fixed search budget, and the policy target is updated as the search budget increases."
54,SP:96afc34acb196af0b37f66ca9c89ae22ee7b6521,"Value Iteration Networks USED-FOR unknown, potentially continuous state spaces. deep RL model CONJUNCTION execution model. execution model CONJUNCTION deep RL model. execution model USED-FOR VI - like operation. graph embedding model ( TransE ) CONJUNCTION deep RL model. deep RL model CONJUNCTION graph embedding model ( TransE ). graphical message passing USED-FOR VI - like operation. graph embedding model ( TransE ) CONJUNCTION execution model. execution model CONJUNCTION graph embedding model ( TransE ). execution model USED-FOR framework. deep RL model USED-FOR framework. graph embedding model ( TransE ) USED-FOR framework. graphical message passing USED-FOR execution model. continuous control environments CONJUNCTION Atari game Freeway. Atari game Freeway CONJUNCTION continuous control environments. MDP USED-FOR grid - world task. grid - world task EVALUATE-FOR baselines. ","This paper proposes Value Iteration Networks for unknown, potentially continuous state spaces. The proposed framework uses a graph embedding model (TransE), a deep RL model, and an execution model that performs a VI-like operation using graphical message passing. The authors evaluate their baselines on continuous control environments and the Atari game Freeway, as well as on a grid-world task in an MDP."
55,SP:96afc34acb196af0b37f66ca9c89ae22ee7b6521,"continuous problems CONJUNCTION problems. problems CONJUNCTION continuous problems. it USED-FOR continuous problems. it USED-FOR problems. method COMPARE value iteration networks. value iteration networks COMPARE method. XLVIN ) COMPARE value iteration networks. value iteration networks COMPARE XLVIN ). method COMPARE XLVIN ). XLVIN ) COMPARE method. Method are value - iteration - network - paradigm, and XLVINs. OtherScientificTerm is state space. ","This paper proposes a value-iteration-network-paradigm (XLVIN) that is based on the fact that it can be applied to both continuous problems and problems where the state space is finite. The authors show that the proposed method outperforms existing value iteration networks (e.g., XLVIN), and that it is able to generalize to new problems. The main contribution of this paper is the introduction of XLVINEs."
56,SP:96afc34acb196af0b37f66ca9c89ae22ee7b6521,graph representation learning CONJUNCTION neural algorithm execution. neural algorithm execution CONJUNCTION graph representation learning. self - supervised contrastive learning CONJUNCTION graph representation learning. graph representation learning CONJUNCTION self - supervised contrastive learning. Value Iteration Networks USED-FOR MDPs. self - supervised contrastive learning PART-OF policy prediction model. graph representation learning PART-OF policy prediction model. model COMPARE approaches. approaches COMPARE model. Generic is method. ,"This paper proposes a policy prediction model that combines self-supervised contrastive learning, graph representation learning, and neural algorithm execution. Value Iteration Networks are used to model MDPs. The proposed method is evaluated on a variety of datasets. The results show that the proposed model outperforms existing approaches."
57,SP:b0fa24ad48e7e60d6899bd799adcd03473cadd6e,"GD USED-FOR cumulative hinge loss. neural units PART-OF monomials of the target DNF. GD USED-FOR convex neural nets. OtherScientificTerm are monotone read - once DNF formulas, uniform distributions, and global minima. Method are convex neural networks, and monotone read - once DNF. Task is distribution - specific PAC setting. ","This paper studies the convergence of convex neural nets trained with GD on cumulative hinge loss, which is a variant of the monotone read-once DNF formulas. The main contribution of this paper is to show that GD converges to a global minima of the original monomials of the target DNF when the number of neural units in the original and target monoms is small. This is an interesting result, as it shows that GD can be used to improve the performance of existing convex deep neural networks. The authors also provide a theoretical analysis on the distribution-specific PAC setting, where uniform distributions are assumed to converge to a fixed point, and show that monotones read-of-the-target DNF converge to the global minimum. "
58,SP:b0fa24ad48e7e60d6899bd799adcd03473cadd6e,"neural networks USED-FOR DNFs. convex neural networks CONJUNCTION gradient descent. gradient descent CONJUNCTION convex neural networks. gradient descent USED-FOR read - once DNFs. OtherScientificTerm are symmetric initialization, and global minimum. Method is DNF - recovery solution. ","This paper studies the problem of DNFs with neural networks. The authors propose to use convex neural networks and gradient descent to solve read-once, read-only, and read-on-read-off (read-only) versions of read-off neural networks with symmetric initialization. The main contribution of this paper is to propose a DNF-recovery solution, which is based on minimizing a global minimum. "
59,SP:b0fa24ad48e7e60d6899bd799adcd03473cadd6e,read - once DNFs USED-FOR learning Boolean functions. neural networks USED-FOR learning Boolean functions. hidden layer with 2^D components USED-FOR Boolean functions. hidden layer with 2^D components PART-OF neural network architecture. gradient descent USED-FOR DNF expression. 2 - norm minimization USED-FOR DNF. 2^D instances USED-FOR read - once DNF. global minimum FEATURE-OF loss minimization problem. rounding heuristics USED-FOR gradient descent. network USED-FOR loss minimization problem. ,"This paper studies read-once DNFs for learning Boolean functions in neural networks. The authors propose a novel neural network architecture that consists of a hidden layer with 2^D components for Boolean functions. The main idea is to use 2-norm minimization to minimize the DNF of a read-first DNF with 2^{D instances. Then, gradient descent is used to approximate the original DNF expression using rounding heuristics. The loss minimization problem is formulated as a global minimum of the network."
60,SP:6e600bedbf995375fd41cc0b517ddefb918318af,strategy USED-FOR RL. graph structure FEATURE-OF episodic experience buffer. graph structure USED-FOR strategy. goal - oriented RL CONJUNCTION structured exploration. structured exploration CONJUNCTION goal - oriented RL. goal - oriented RL PART-OF strategy. structured exploration PART-OF strategy. benchmarks USED-FOR goal - reaching tasks. benchmarks EVALUATE-FOR technique. goal - reaching tasks EVALUATE-FOR technique. ,This paper proposes a new strategy for RL based on graph structure in the episodic experience buffer. The proposed strategy combines goal-oriented RL with structured exploration. The technique is evaluated on several benchmarks for goal-reaching tasks.
61,SP:6e600bedbf995375fd41cc0b517ddefb918318af,framework USED-FOR sparse reward challenge. GSRL USED-FOR sparse reward challenge. sub - group division CONJUNCTION attention mechanism. attention mechanism CONJUNCTION sub - group division. it USED-FOR hindsight - like goals. it USED-FOR trajectories. dynamic graph USED-FOR trajectories. sub - group division USED-FOR hindsight - like goals. attention mechanism USED-FOR hindsight - like goals. method COMPARE baselines. baselines COMPARE method. ,"This paper proposes GSRL, a framework for solving the sparse reward challenge. In particular, it learns trajectories from a dynamic graph and uses sub-group division and attention mechanism to learn hindsight-like goals. The method is evaluated on several datasets and compared to several baselines."
62,SP:6e600bedbf995375fd41cc0b517ddefb918318af,exploration CONJUNCTION exploitation. exploitation CONJUNCTION exploration. Graph Structured Reinforcement Learning ( GSRL ) framework USED-FOR exploitation. Graph Structured Reinforcement Learning ( GSRL ) framework USED-FOR RL. Graph Structured Reinforcement Learning ( GSRL ) framework USED-FOR exploration. exploitation PART-OF RL. exploration USED-FOR RL. GSRL USED-FOR dynamic graph. historical trajectories USED-FOR dynamic graph. GSRL COMPARE MAP algorithms. MAP algorithms COMPARE GSRL. GSRL COMPARE HER. HER COMPARE GSRL. robotics manipulation tasks EVALUATE-FOR GSRL. HER CONJUNCTION MAP algorithms. MAP algorithms CONJUNCTION HER. robotics manipulation tasks EVALUATE-FOR MAP algorithms. OtherScientificTerm is sparse or delayed rewards. Generic is it. Method is attention strategy. ,"This paper proposes a Graph Structured Reinforcement Learning (GSRL) framework for combining exploration and exploitation in RL. The main idea of GSRL is to learn a dynamic graph based on historical trajectories with sparse or delayed rewards, and to use it as an attention strategy. Experiments on robotics manipulation tasks show that GSRL outperforms HER and MAP algorithms."
63,SP:80c62de18a6a7433c9728fe0d731f733bb89e898,"learning potential CONJUNCTION staleness. staleness CONJUNCTION learning potential. level - ness FEATURE-OF benchmark data sets. Method are experience replay, and learning algorithm. OtherScientificTerm are implicit ) levels, replay distribution, and learning mode. Generic is tasks. ","This paper studies the problem of experience replay, where the goal is to learn a learning algorithm that can generalize to unseen (implicit) levels of experience. The paper proposes a new learning algorithm based on the observation that learning potential and staleness depend on the level-ness of the benchmark data sets. The authors propose to use the replay distribution as a proxy for the level of staleness of the task, which is then used to estimate the learning potential of the learned learning mode. Experiments are conducted on a variety of tasks, and the results show that the proposed learning algorithm is able to generalize well to unseen levels."
64,SP:80c62de18a6a7433c9728fe0d731f733bb89e898,"procedurally generated environments USED-FOR learning. training and evaluation environments USED-FOR Learning algorithms. algorithm USED-FOR level prioritization. Procgen Benchmark CONJUNCTION MiniGrid benchmarks. MiniGrid benchmarks CONJUNCTION Procgen Benchmark. approach USED-FOR implicit curriculum. sparse reward settings FEATURE-OF implicit curriculum. MiniGrid benchmarks EVALUATE-FOR approach. Procgen Benchmark EVALUATE-FOR approach. OtherScientificTerm are simulation environments, environmental factors, and training environments. ","Learning algorithms in both training and evaluation environments are based on procedurally generated environments. This paper proposes a new algorithm for level prioritization based on simulation environments. The approach is evaluated on the Procgen Benchmark and MiniGrid benchmarks, where the proposed approach is shown to learn an implicit curriculum in sparse reward settings. The paper also provides a theoretical analysis of the impact of environmental factors on the performance of training environments."
65,SP:80c62de18a6a7433c9728fe0d731f733bb89e898,"agents USED-FOR intrinsic curriculum. exploration FEATURE-OF procedurally generated episodes. average absolute magnitude FEATURE-OF generalized advantage estimate. intrinsic curriculum USED-FOR optimization / learning. Metric are heuristic measure of expected learning progress, and heuristic measures. OtherScientificTerm is environment distribution. Method is prioritization strategy. ","This paper proposes a heuristic measure of expected learning progress, which is based on a generalized advantage estimate of the environment distribution with respect to the average absolute magnitude. The authors show that agents learn an intrinsic curriculum that encourages exploration in procedurally generated episodes, and that this intrinsic curriculum can be used to guide optimization/learning. In addition, the authors propose a prioritization strategy based on the learned heuristic measures."
66,SP:fd92d766a7721a411ff8c422bec18391d028fa78,pre - training CONJUNCTION multitask learning. multitask learning CONJUNCTION pre - training. auxiliary task selection USED-FOR selecting relevant tasks. selecting relevant tasks USED-FOR pre - training. selecting relevant tasks USED-FOR multitask learning. auxiliary task selection USED-FOR deep learning. text classification CONJUNCTION image classification. image classification CONJUNCTION text classification. image classification CONJUNCTION medical imaging transfer tasks. medical imaging transfer tasks CONJUNCTION image classification. implementation USED-FOR text classification. implementation USED-FOR medical imaging transfer tasks. implementation USED-FOR image classification. ,"This paper studies the problem of selecting relevant tasks for pre-training and multitask learning using auxiliary task selection in deep learning. The proposed implementation is applied to text classification, image classification, and medical imaging transfer tasks."
67,SP:fd92d766a7721a411ff8c422bec18391d028fa78,pre - training CONJUNCTION multitask learning. multitask learning CONJUNCTION pre - training. multitask learning HYPONYM-OF data - rich related tasks. pre - training HYPONYM-OF data - rich related tasks. auxiliary task USED-FOR primary tasks. method USED-FOR auxiliary updates. Generic is primary task. ,"This paper addresses the problem of data-rich related tasks such as pre-training and multitask learning. The authors propose an auxiliary task that can be used to augment the primary tasks. The primary task is the same as the main task, but the auxiliary updates can be performed on top of the primary task. The proposed method can also be used for auxiliary updates."
68,SP:fd92d766a7721a411ff8c422bec18391d028fa78,"formulation USED-FOR multitask learning. gradients USED-FOR auxiliary task. gradients USED-FOR learning schemes. natural language datasets CONJUNCTION image datasets. image datasets CONJUNCTION natural language datasets. natural language datasets EVALUATE-FOR meta learning methods. image datasets EVALUATE-FOR meta learning methods. OtherScientificTerm are pretraining regimes, subspace, primary task, and auxiliary tass. Generic are auxiliary tasks, and subspaces. ","This paper proposes a new formulation for multitask learning. The main idea is to use the gradients of the auxiliary task to guide the learning schemes. The authors propose two pretraining regimes. The first pretraining regime is to learn a subspace of the primary task, and then the auxiliary tasks are learned in this subspaces. In the second pretraining phase, the auxiliary tass is learned in the same way as the primary tass. Experiments are conducted on both natural language datasets and image datasets to evaluate the effectiveness of the proposed meta learning methods."
69,SP:8eb8c34e56de137bfc32ea0fd8cd94e4bff5907d,3D world USED-FOR grounded language learning. word - learning CONJUNCTION stably acquired meanings. stably acquired meanings CONJUNCTION word - learning. RL USED-FOR word - learning. psychologically - inspired memory mechanism COMPARE Transformers. Transformers COMPARE psychologically - inspired memory mechanism. Transformers COMPARE LSTMs. LSTMs COMPARE Transformers. generalization FEATURE-OF action - object pairs. psychologically - inspired memory mechanism COMPARE LSTMs. LSTMs COMPARE psychologically - inspired memory mechanism. generalization EVALUATE-FOR it. Task is grounded language / multimodal representation learning. ,"This paper studies grounded language learning in a 3D world, which is an important problem in grounded language/multimodal representation learning. The authors propose to use RL to combine word-learning with stably acquired meanings. They show that their psychologically-inspired memory mechanism outperforms Transformers and LSTMs in terms of generalization to action-object pairs."
70,SP:8eb8c34e56de137bfc32ea0fd8cd94e4bff5907d,embodied environment FEATURE-OF fast - mapping. fast - mapping USED-FOR acquiring words. ,"This paper studies the problem of fast-mapping in an embodied environment. In particular, the authors propose to use fast-mapling for the task of acquiring words. The paper is well-written and easy to follow."
71,SP:8eb8c34e56de137bfc32ea0fd8cd94e4bff5907d,"DCEM HYPONYM-OF multi - modal memory - architecture. DCEM model COMPARE baselines. baselines COMPARE DCEM model. metrics EVALUATE-FOR DCEM model. metrics EVALUATE-FOR baselines. DCEM USED-FOR object names. OtherScientificTerm are grounded world, language and vision modalities, and instructions. Task is AI. Method is memory - based architecture. Material is 3D domain. Generic is discovery phase. ","This paper proposes a multi-modal memory-architecture called DCEM, which is an extension of the recently proposed DCEM. The DCEM model is a memory-based architecture that is capable of storing information from multiple language and vision modalities, which can be used in the future for training AI. Experiments on 3D domain show that DCEM can achieve better performance than the baselines on a variety of metrics. The main contribution of the paper is the discovery phase, where DCEM is able to generate object names for objects in the grounded world, which are then used to generate instructions for the agent to execute."
72,SP:9bd3d99bce743d356eb18692ef93365c78e5fcec,class imbalance USED-FOR few - shot learning problems. gradient - based meta - learning methods COMPARE metric learning approaches. metric learning approaches COMPARE gradient - based meta - learning methods. support set imbalance COMPARE base class imbalance. base class imbalance COMPARE support set imbalance. imbalance FEATURE-OF gradient - based meta - learning methods. Task is FSL. ,"This paper studies the problem of class imbalance in few-shot learning problems. The authors compare gradient-based meta-learning methods with standard metric learning approaches, and show that the imbalance of gradient-by-gradient methods is less severe than the base class imbalance. They also provide a theoretical analysis of FSL."
73,SP:9bd3d99bce743d356eb18692ef93365c78e5fcec,"backbones USED-FOR few - shot learning methods. strategies USED-FOR imbalance. strategies USED-FOR few - shot case. strategies USED-FOR supervised learning. balanced task COMPARE class - imbalance counterparts. class - imbalance counterparts COMPARE balanced task. feature - transfer CONJUNCTION metric - based methods. metric - based methods CONJUNCTION feature - transfer. imbalance FEATURE-OF dataset level. imbalance FEATURE-OF supervised learning. Task is few - shot class - imbalance. OtherScientificTerm are dataset vs. support set imbalance, imbalance distributions, and support set level. Method are rebalancing techniques, and optimization - based methods. ","This paper studies the problem of few-shot class-imbalance, where the dataset vs. support set imbalance is not well understood. The authors propose several rebalancing techniques to address this issue. The main contribution of this paper is that the authors propose two strategies to address the imbalance in supervised learning: (1) feature-transfer and (2) metric-based methods. The experiments show that the proposed strategies are effective in the first case, but not in the second case, where imbalance distributions are known at the support set level. In addition, the authors demonstrate that the performance of the balanced task is comparable to the class-inbalance counterparts, and that the imbalance at the dataset level is not observed in the optimization-based results."
74,SP:9bd3d99bce743d356eb18692ef93365c78e5fcec,"class - imbalance USED-FOR many few - shot approaches. OtherScientificTerm are few - shot class - imbalance, and imbalance distributions. Method is rebalancing techniques. ","This paper studies the problem of few-shot class-imbalance, which is an important problem for many few -shot approaches. In particular, the authors argue that class-overbalance is a critical issue for the success of many few-shots approaches, and propose rebalancing techniques to address this issue. The main contribution of this paper is a theoretical analysis of imbalance distributions, which shows that the imbalance distributions are highly correlated with the number of classes."
75,SP:2a9cbbe3661d2f02f71472d0111f22a739412226,adjacency matrix CONJUNCTION graph Laplacian. graph Laplacian CONJUNCTION adjacency matrix. linear layer USED-FOR topological distances. adjacency matrix HYPONYM-OF propagation matrix. graph Laplacian HYPONYM-OF propagation matrix. PGC COMPARE linearly stacking simple graph convolutions. linearly stacking simple graph convolutions COMPARE PGC. Method is Polynomial Graph Convolution ( PGC ). OtherScientificTerm is single layer. Task is graph classification tasks. ,"This paper proposes Polynomial Graph Convolution (PGC), which is a generalization of the single layer that is used in graph classification tasks. The main idea is to use a linear layer to compute the topological distances between two nodes in a graph. The propagation matrix is a combination of the adjacency matrix and the graph Laplacian. PGC is compared to linearly stacking simple graph convolutions."
76,SP:2a9cbbe3661d2f02f71472d0111f22a739412226,Polynomial Graph Convolution ( PGC ) USED-FOR Polynomial Graph Convolutional Networks ( PGCNs ). PGC USED-FOR k - hop information. PGC CONJUNCTION complex readout layer. complex readout layer CONJUNCTION PGC. PGC CONJUNCTION PGC. PGC CONJUNCTION PGC. k FEATURE-OF PGC. PGC PART-OF PGCNs. complex readout layer PART-OF PGCNs. PGC PART-OF PGCNs. avg USED-FOR complex readout layer. PGC COMPARE linearly stacked q PGCs. linearly stacked q PGCs COMPARE PGC. PGC USED-FOR Common graph convolution operators. PGCNs COMPARE GNNs. GNNs COMPARE PGCNs. common graph classification benchmarks EVALUATE-FOR PGCNs. ,"This paper proposes Polynomial Graph Convolution (PGC), which is a generalization of the recently proposed polynomial graph convolutional Networks (PGCNs). PGC is designed to capture k-hop information. PGC consists of two parts: PGC with k and a complex readout layer that takes the avg as input and outputs the output of PGC and PGC. The authors compare PGC to linearly stacked q PGCs and show that PGC can be used to approximate the k of the original PGC while keeping the k fixed. The paper also proposes to use PGC as an auxiliary layer in PGCNs, which can be combined with PGC or PGC without PGC, to produce a more efficient PGCN. The experiments on common graph classification benchmarks show that the proposed PGCNNs outperform the existing GNNs.    The authors also provide a theoretical analysis of the PGC for the Common graphconvolution operators. "
77,SP:2a9cbbe3661d2f02f71472d0111f22a739412226,framework USED-FOR Graph Convolutional Neural Networks ( GCNs ). PGC framework USED-FOR GNNs. PGC COMPARE GNNs. GNNs COMPARE PGC. graph classification task EVALUATE-FOR method. Method is Polynomial Graph Convolution ( PGC ). ,"This paper proposes Polynomial Graph Convolution (PGC), a framework for training Graphconvolutional Neural Networks (GCNs). The PGC framework aims to improve the performance of existing GNNs. The proposed method is evaluated on a graph classification task, and the results show that PGC is able to achieve better performance than the existing state-of-the-art GNN."
78,SP:b0a6873eb4bbf5cdc4a5dfa08782225ae91fc589,synthetic data USED-FOR scene graph generation task. framework USED-FOR scene graph generation task. synthetic data USED-FOR framework. synthetic - to - real transfer learning USED-FOR SGG. Dining - Sim CONJUNCTION Drive - Sim. Drive - Sim CONJUNCTION Dining - Sim. CLEVR CONJUNCTION Dining - Sim. Dining - Sim CONJUNCTION CLEVR. Sim2SG COMPARE baseline models. baseline models COMPARE Sim2SG. Sim2SG USED-FOR scenarios. scenarios EVALUATE-FOR baseline models. CLEVR HYPONYM-OF scenarios. Drive - Sim HYPONYM-OF scenarios. Dining - Sim HYPONYM-OF scenarios. ,"This paper proposes a framework for using synthetic data for the scene graph generation task. The main idea is to use synthetic-to-real transfer learning for SGG. Sim2SG is evaluated on three scenarios: CLEVR, Dining-Sim, and Drive-Sim and compared to baseline models."
79,SP:b0a6873eb4bbf5cdc4a5dfa08782225ae91fc589,"synthetic data CONJUNCTION unlabeled real data. unlabeled real data CONJUNCTION synthetic data. unlabeled real data USED-FOR scene graphs. synthetic data USED-FOR scene graphs. components PART-OF content gap. prediction discrepancy HYPONYM-OF components. label discrepancy HYPONYM-OF components. method USED-FOR posed domain gaps. synthetic and one real / synthetic data set EVALUATE-FOR method. Task is learning scene graphs. Material is real data. OtherScientificTerm are content and appearance gap, Scene graphs, intermediate latent space, latent space, label distributions, appearance gap, latent representation, and content distributions. Method is optimization procedure. ","This paper studies the problem of learning scene graphs from synthetic data and unlabeled real data. Scene graphs are generated from an intermediate latent space, where the labels of the real data are drawn from the latent space. The authors propose to minimize the content and appearance gap between the two latent spaces. The content gap consists of two components: (1) prediction discrepancy between the latent representation and the label distributions, and (2) appearance gap, which is the difference between the content distributions. The proposed method is evaluated on one synthetic and one real/synthetic data set, and is shown to reduce posed domain gaps. An optimization procedure is also presented."
80,SP:b0a6873eb4bbf5cdc4a5dfa08782225ae91fc589,sim2real transfer USED-FOR scene graph inference. simulated training data CONJUNCTION real test data. real test data CONJUNCTION simulated training data. real images USED-FOR models. simulated data USED-FOR models. driving scene simulator CONJUNCTION real KITTI scenes. real KITTI scenes CONJUNCTION driving scene simulator. environments EVALUATE-FOR approach. ,This paper proposes sim2real transfer for scene graph inference. The idea is that models trained on simulated images can be trained on both simulated training data and real test data. The approach is evaluated on two environments: a driving scene simulator and two real KITTI scenes.
81,SP:ccc72f26d0637476d01671c147b5cb5d30fa8c2d,sample efficiency EVALUATE-FOR model - free methods. model - free RL algorithm COMPARE model - free methods. model - free methods COMPARE model - free RL algorithm. REDQ COMPARE model - based methods. model - based methods COMPARE REDQ. REDQ COMPARE model - free methods. model - free methods COMPARE REDQ. Mujoco EVALUATE-FOR REDQ. model - free methods COMPARE model - based methods. model - based methods COMPARE model - free methods. sample efficiency EVALUATE-FOR model - free methods. SAC HYPONYM-OF model - free methods. MBPO HYPONYM-OF model - based methods. sample efficiency EVALUATE-FOR REDQ. algorithmic components USED-FOR REDQ. OtherScientificTerm is Q estimation bias. ,"This paper presents a model-free RL algorithm, REDQ, which outperforms model-based methods in terms of sample efficiency on Mujoco. The authors show that REDQ outperforms the state-of-the-art model-fractional-based (SAC, MBPO, etc) and model-without-model-free methods (MBPO, SAC, etc). The authors also provide a theoretical analysis of the Q estimation bias of REDQ and propose algorithmic components for REDQ."
82,SP:ccc72f26d0637476d01671c147b5cb5d30fa8c2d,"techniques USED-FOR soft actor - critic ( SAC ). techniques USED-FOR algorithm. update - to - data ratio USED-FOR critic update. average ensemble Q USED-FOR policy gradient. Method is REDQ. OtherScientificTerm are ensemble Qs, and Q bias. Task is ablation studies. ","This paper proposes a new algorithm for soft actor-critic (SAC) based on techniques from REDQ. The key idea is to use the update-to-data ratio of the critic update as the policy gradient instead of the average ensemble Q. The authors argue that ensemble Qs tend to be biased in the direction of the Q bias, which can lead to ablation studies."
83,SP:ccc72f26d0637476d01671c147b5cb5d30fa8c2d,modification USED-FOR double Q - learning. update - to - data ratio USED-FOR sample efficiency. Q functions USED-FOR REDQ. sample efficiency EVALUATE-FOR REDQ. update - to - data ratio USED-FOR REDQ. method COMPARE model - based algorithms. model - based algorithms COMPARE method. model - based algorithms USED-FOR tasks. continuous action space FEATURE-OF model - based algorithms. continuous action space FEATURE-OF tasks. tasks EVALUATE-FOR method. ,This paper proposes a modification to double Q-learning. The main idea is to use REDQ with different Q functions and to use the update-to-data ratio to improve the sample efficiency of REDQ. The proposed method is evaluated on a variety of tasks with continuous action space and compared to model-based algorithms.
84,SP:c424d050996a7f383d2f12418dfdcea90d94ea65,feature ordering CONJUNCTION size. size CONJUNCTION feature ordering. DIDA architecture USED-FOR distributions. DIDA architecture USED-FOR method. feature CONJUNCTION dataset size. dataset size CONJUNCTION feature. feature permutation FEATURE-OF method. architecture USED-FOR global structures. Material is continuous domain. Generic is tasks. OtherScientificTerm is meta - features. ,"This paper proposes a method that uses a modified version of the DIDA architecture to model distributions in the continuous domain. The method is motivated by the fact that the feature ordering and the size of the dataset can be affected by the feature permutation. The authors propose to model global structures using the architecture, which allows for global structures to be learned in a continuous domain, which is useful for tasks where the meta-features are not available. "
85,SP:c424d050996a7f383d2f12418dfdcea90d94ea65,"neural network layer USED-FOR distribution samples. method COMPARE DSS. DSS COMPARE method. point sets COMPARE discrete or continuous probability distributions. discrete or continuous probability distributions COMPARE point sets. learning algorithm USED-FOR model. patch identification CONJUNCTION model configuration assessment. model configuration assessment CONJUNCTION patch identification. model USED-FOR dataset. patch identification HYPONYM-OF tasks. model configuration assessment HYPONYM-OF tasks. Dataset2Vec embeddings CONJUNCTION DSS. DSS CONJUNCTION Dataset2Vec embeddings. task EVALUATE-FOR models. DSS USED-FOR models. Dataset2Vec embeddings USED-FOR models. handcrafted features CONJUNCTION DSS. DSS CONJUNCTION handcrafted features. paper COMPARE DSS. DSS COMPARE paper. paper COMPARE handcrafted features. handcrafted features COMPARE paper. task EVALUATE-FOR paper. method COMPARE DSS. DSS COMPARE method. method USED-FOR predictors. tasks EVALUATE-FOR method. DSS USED-FOR predictors. architecture CONJUNCTION robustness. robustness CONJUNCTION architecture. OtherScientificTerm are features, and Lipschitz - bounded transformations. ","This paper proposes a neural network layer that generates distribution samples from point sets instead of discrete or continuous probability distributions. The proposed method is similar to DSS, except that the features are learned using Lipschitz-bounded transformations. The authors propose a learning algorithm that learns a model to generate a dataset from a set of points and then uses the learned model to perform two tasks: patch identification and model configuration assessment. The models are trained using Dataset2Vec embeddings and DSS on the new task. The paper is compared to handcrafted features, DSS and the original paper on this new task and shows that the proposed method can produce better predictors than DSS in terms of both architecture and robustness."
86,SP:c424d050996a7f383d2f12418dfdcea90d94ea65,pairwise embedding of the set ’s elements USED-FOR set / distribution representation architecture DIDA. method USED-FOR discrete and continuous distribution representation. invariant layers CONJUNCTION local consistency. local consistency CONJUNCTION invariant layers. dataset representation tasks EVALUATE-FOR architecture. ,This paper proposes a new set/distribution representation architecture DIDA based on pairwise embedding of the set’s elements. The proposed method can be applied to both discrete and continuous distribution representation. The authors provide theoretical analysis on the properties of invariant layers and local consistency. Experiments on dataset representation tasks demonstrate the effectiveness of the proposed architecture.
87,SP:3e5d5b61dceca85c444b3d0d06577229c3146664,edges PART-OF graphs. edges USED-FOR spectral embedding / clustering. It USED-FOR embeddings. edges FEATURE-OF Laplacian eigenvalues. edges USED-FOR distortions. worst - case efficient algorithms USED-FOR Laplacian matrices. nearest neighbor graphs USED-FOR algorithm. worst - case efficient algorithms USED-FOR algorithm. accuracy EVALUATE-FOR algorithm. ,This paper proposes to use edges in graphs for spectral embedding/clustering. It aims to learn embeddings that are robust to distortions caused by edges in the Laplacian eigenvalues. The proposed algorithm is based on nearest neighbor graphs and uses worst-case efficient algorithms to approximate the parameters of the corresponding Least-squares (LSE) versions of the original Least Squares (LSR). The algorithm is evaluated on a variety of datasets and shows improved accuracy.
88,SP:3e5d5b61dceca85c444b3d0d06577229c3146664,"graph learning method USED-FOR spectral embedding. clustering CONJUNCTION dimension reduction. dimension reduction CONJUNCTION clustering. graph learning method USED-FOR problems. dimension reduction HYPONYM-OF problems. clustering HYPONYM-OF problems. it USED-FOR optimal densification. spectrally critical "" edges HYPONYM-OF spectral embedding. partial derivatives FEATURE-OF objective function. log - likelihood FEATURE-OF Gaussian graphical model. log - likelihood FEATURE-OF objective function. objective function USED-FOR spectral criticality. partial derivatives USED-FOR spectral criticality. spectral criticality CONJUNCTION distance distortion. distance distortion CONJUNCTION spectral criticality. Generic are method, and procedure. Task is sparsification of an initial graph. OtherScientificTerm are edges, and graph. ","This paper proposes a graph learning method for learning a spectral embedding (i.e., ""spectrally critical"" edges) for problems such as clustering and dimension reduction. The method is motivated by the observation that sparsification of an initial graph is not optimal, and that it is difficult to find optimal densification. The authors propose a procedure to approximate the spectral criticality of the initial graph using partial derivatives of an objective function with log-likelihood of a Gaussian graphical model. The idea is to use these partial derivatives to approximate both the ""spectral criticality"" and the ""distance distortion"" of the edges."
89,SP:3e5d5b61dceca85c444b3d0d06577229c3146664,graph Laplacian FEATURE-OF precision matrix. precision matrix FEATURE-OF graphical Lasso formulation. graphical Lasso formulation USED-FOR algorithm. Lasso CONJUNCTION spectral perturbation analysis. spectral perturbation analysis CONJUNCTION Lasso. sparse kNN graph USED-FOR algorithm. Lasso USED-FOR critical edges. spectral perturbation analysis USED-FOR critical edges. OtherScientificTerm is graph. ,"This paper proposes a new algorithm based on the graphical Lasso formulation of the precision matrix of the graph Laplacian. The algorithm is trained on a sparse kNN graph, and the critical edges are estimated using Lasso and spectral perturbation analysis. The paper is well-written and easy to follow. However, there is a lack of experimental results on the graph."
90,SP:7e6c73a642a8b3d64156c1d0ecf11f84e7222a22,"text CONJUNCTION images. images CONJUNCTION text. method USED-FOR intrinsic motivation. intrinsic motivation CONJUNCTION goal - conditioned reinforcement learning ( GCRL ). goal - conditioned reinforcement learning ( GCRL ) CONJUNCTION intrinsic motivation. method USED-FOR goal - conditioned reinforcement learning ( GCRL ). state space FEATURE-OF intrinsic motivation. perceptual space FEATURE-OF goals. text HYPONYM-OF perceptual space. images HYPONYM-OF perceptual space. policy USED-FOR mutual information. goal - conditioned policy USED-FOR latent - conditioned policy. MuJoCo manipulation and locomotion CONJUNCTION toy tasks. toy tasks CONJUNCTION MuJoCo manipulation and locomotion. method COMPARE GCRL methods. GCRL methods COMPARE method. Atari CONJUNCTION MuJoCo manipulation and locomotion. MuJoCo manipulation and locomotion CONJUNCTION Atari. tasks EVALUATE-FOR GCRL methods. tasks EVALUATE-FOR method. Atari HYPONYM-OF tasks. toy tasks EVALUATE-FOR method. Atari EVALUATE-FOR method. toy tasks HYPONYM-OF tasks. MuJoCo manipulation and locomotion HYPONYM-OF tasks. OtherScientificTerm are renderer, perceptual goals, and latent variable. Method is intrinsically motivated latent - conditioned policy. ","This paper proposes a method to combine intrinsic motivation in the state space with goal-conditioned reinforcement learning (GCRL). The intrinsic motivation is defined as a set of goals in the perceptual space (e.g., text, images). The renderer is trained to maximize the mutual information between the intrinsic goals and the latent variable. The goal is to learn an intrinsically motivated latent- conditioned policy that maximizes mutual information with the goal. The method is evaluated on Atari, MuJoCo manipulation and locomotion, and toy tasks and compared to other GCRL methods."
91,SP:7e6c73a642a8b3d64156c1d0ecf11f84e7222a22,unsupervised learning objective USED-FOR perceptual goal - conditioned policies. perceptual - goal conditioned policy USED-FOR behaviors. unsupervised discovery of high - level behaviors CONJUNCTION perceptual - goal conditioned policy. perceptual - goal conditioned policy CONJUNCTION unsupervised discovery of high - level behaviors. Generic is learning. OtherScientificTerm is policy. Method is goal - conditioned policy. ,"This paper proposes an unsupervised learning objective for perceptual goal-conditioned policies. The learning is based on the assumption that the policy is trained on a set of high-level behaviors, and that the behaviors are learned by a perceptual-goal conditioned policy. The goal is to learn a policy that maximizes the likelihood of achieving a goal, while minimizing the risk of not reaching a goal. The paper provides a theoretical analysis of the behavior of the goal-constrained policy. Experiments are conducted to demonstrate the effectiveness of the ununsupervised discovery of the behaviors, as well as the performance of the perceptual-policy conditioned policy on a variety of tasks."
92,SP:7e6c73a642a8b3d64156c1d0ecf11f84e7222a22,solution USED-FOR goal - conditioned policies. embedding space USED-FOR reward. unsupervised skill discovery USED-FOR discriminator. reward COMPARE embedding - distance based reward functions. embedding - distance based reward functions COMPARE reward. OtherScientificTerm is hand - crafted rewards. Method is goal - conditioned policy. ,"This paper proposes a solution for learning goal-conditioned policies. The key idea is to use hand-crafted rewards, where the reward is learned in an embedding space, and the discriminator is trained using unsupervised skill discovery. The authors show that this reward is more efficient than embedding-distance based reward functions, and that the goal is more interpretable. In addition, they show that the learned discriminator can be used as a discriminator, which can be applied to any goal-constrained policy."
93,SP:bdf293bf2118a927cbec6b96be03bfcad0243640,real world applications USED-FOR RL. medicine HYPONYM-OF RL. medicine HYPONYM-OF real world applications. approach USED-FOR simulation policy. approach COMPARE on - policy RL. on - policy RL COMPARE approach. medical treatment environment CONJUNCTION Atari. Atari CONJUNCTION medical treatment environment. Atari EVALUATE-FOR approach. medical treatment environment EVALUATE-FOR approach. simulation policy USED-FOR on - policy RL. OtherScientificTerm is policies. Generic is it. ,"This paper addresses the problem of RL in real world applications, such as medicine. The authors propose an approach to learn a simulation policy that can be used in conjunction with on-policy RL. The approach is evaluated on a medical treatment environment and Atari, and the results show that the proposed approach can learn policies that are similar to the original policy, and that it can generalize to unseen environments."
94,SP:bdf293bf2118a927cbec6b96be03bfcad0243640,"generic solution USED-FOR policy switches. generic solution USED-FOR RL context. Metric is switching cost. Task is deep reinforcement learning. Generic are baselines solutions, and baselines. ","This paper proposes a generic solution for policy switches in the RL context. The switching cost is an important problem in deep reinforcement learning, and the paper proposes to reduce the switching cost by a factor of $\Omega(1/\sqrt{T})$. The paper compares the proposed baselines solutions to several existing baselines."
95,SP:bdf293bf2118a927cbec6b96be03bfcad0243640,deep RL setting USED-FOR RL. low switching cost FEATURE-OF RL. heuristic USED-FOR policy. feature embeddings USED-FOR policies. method COMPARE naive algorithms. naive algorithms COMPARE method. OtherScientificTerm is behavior policy. ,"This paper studies RL in a deep RL setting, where the goal is to achieve a low switching cost. The authors propose a heuristic to train a policy that learns to switch to a different behavior policy when the current behavior policy fails. The key idea is to learn policies with different feature embeddings. The experiments show that the proposed method outperforms naive algorithms."
96,SP:d06bef9ee5e9bdda1571478b6a8a7a2d3ab42f1b,homotopy SGD ( H - SGD ) USED-FOR unconstrained problems. homotopy map CONJUNCTION homotopy parameter. homotopy parameter CONJUNCTION homotopy map. homotopy map USED-FOR homotopy SGD ( H - SGD ). homotopy parameter USED-FOR homotopy SGD ( H - SGD ). homotopy map USED-FOR unconstrained problems. algorithm USED-FOR nonconvex problems. PL condition FEATURE-OF nonconvex problems. homotopy map CONJUNCTION homotopy parameter. homotopy parameter CONJUNCTION homotopy map. OtherScientificTerm is linear convergence. ,"This paper proposes homotopy SGD (H-SGD), which uses a combination of the homotonicity map and the homhotopy parameter to solve unconstrained problems. The authors show that the proposed algorithm converges to linear convergence for nonconvex problems satisfying the PL condition. The paper also provides a theoretical analysis of the relationship between the homotropic properties of the proposed homotopic map and its homotropic parameter. "
97,SP:d06bef9ee5e9bdda1571478b6a8a7a2d3ab42f1b,homotopy strategy USED-FOR nice local structures of problems. global linear convergence FEATURE-OF H - SGD. regression and classification tasks EVALUATE-FOR H - SGD. ,This paper proposes a homotopy strategy to learn nice local structures of problems. The authors prove global linear convergence of H-SGD on regression and classification tasks.
98,SP:d06bef9ee5e9bdda1571478b6a8a7a2d3ab42f1b,Method is Homotopy - SGD. Generic is algorithm. ,"This paper proposes Homotopy-SGD, which is a generalization of the well-studied algorithm of [1]. The main contribution of this paper is the introduction of a new algorithm that is based on the idea of homotopy. The paper is well-written and easy to follow. "
99,SP:195d090d9df0bda33103edcbbaf300e43f4562be,"sparse point clouds USED-FOR way. meta learning "" approach USED-FOR way. isosurface of the SDF field USED-FOR reconstructed surface. "" task - specific "" latent vectors USED-FOR reconstruction task. decoder USED-FOR SDF value. OtherScientificTerm are SDF values, latent - vector, and R^3. Method is network. ","This paper proposes a way to reconstruct sparse point clouds using a ""meta learning"" approach. The reconstructed surface is modeled as the isosurface of the SDF field. The reconstruction task is based on ""task-specific"" latent vectors. The SDF values are sampled from the latent-vector, and the decoder is trained to predict the corresponding SDF value. The network is trained on R^3."
100,SP:195d090d9df0bda33103edcbbaf300e43f4562be,meta - learning approach USED-FOR neural implicit representation of 3D shapes. implicit function regularizations USED-FOR reconstruction. OtherScientificTerm is object information. Generic is network. ,"This paper proposes a meta-learning approach to learn a neural implicit representation of 3D shapes. The key idea is to use implicit function regularizations to encourage reconstruction of the original object information, which is then used to train the network."
101,SP:195d090d9df0bda33103edcbbaf300e43f4562be,point cloud completion USED-FOR implicit occupancy representation. sparse input point cloud USED-FOR implicit occupancy representation. approach USED-FOR shape. latent - variable conditioned occupancy function USED-FOR occupancy. latent variable USED-FOR posterior distribution. latent - variable conditioned occupancy function USED-FOR approach. latent - variable conditioned occupancy function USED-FOR shape. ,This paper proposes to learn an implicit occupancy representation from a sparse input point cloud via point cloud completion. The proposed approach learns shape using a latent-variable conditioned occupancy function that estimates occupancy for each latent variable in the posterior distribution.
102,SP:ca637a2692cf2424d1ec5c7d2051c7881a5816f4,channel suppression USED-FOR robustness. channel suppression USED-FOR adversarial examples. adversarial examples USED-FOR channels. adversarial examples COMPARE natural examples. natural examples COMPARE adversarial examples. channels COMPARE natural examples. natural examples COMPARE channels. TRADES CONJUNCTION MART. MART CONJUNCTION TRADES. adversarial training CONJUNCTION TRADES. TRADES CONJUNCTION adversarial training. CAS module USED-FOR adversarial robustness. CAS module CONJUNCTION adversarial defense methods. adversarial defense methods CONJUNCTION CAS module. MART HYPONYM-OF adversarial defense methods. adversarial training HYPONYM-OF adversarial defense methods. TRADES HYPONYM-OF adversarial defense methods. ,"This paper proposes channel suppression to improve robustness against adversarial examples. The main idea is to train a set of channels that are adversarial to the natural examples, and then train a CAS module to improve adversarial robustness. Experiments are conducted on adversarial training, TRADES, and MART."
103,SP:ca637a2692cf2424d1ec5c7d2051c7881a5816f4,"activation perspective FEATURE-OF adversarial robustness. activation magnitudes FEATURE-OF adversarial examples. channels of intermediate layers USED-FOR class prediction. OtherScientificTerm are activation channels, and redundant activations. Metric is DNN robustness. ","This paper studies adversarial robustness from an activation perspective. The authors show that the activation magnitudes of adversarial examples are highly correlated with the number of channels of intermediate layers used for class prediction. They also show that if the activation channels are large enough, the class prediction can be improved by removing redundant activations. The paper also provides a theoretical analysis of DNN robustness."
104,SP:ca637a2692cf2424d1ec5c7d2051c7881a5816f4,"channel view of activations FEATURE-OF adversarial examples. Channel - wise Activation Suppressing ( CAS ) USED-FOR frequency distribution. Channel - wise Activation Suppressing ( CAS ) USED-FOR adversarial robustness. CAS PART-OF defense methods. OtherScientificTerm are magnitude and frequency of activations, and frequency distribution issue. Method are adversarial defense ( adversarial training ), and adversarial training method. ","This paper studies adversarial defense (adversarial training) from a channel view of activations of adversarial examples. Channel-wise Activation Suppressing (CAS) is proposed to reduce the frequency distribution of the adversarial robustness. CAS is an important component of many defense methods, but its effectiveness is limited due to the magnitude and frequency of the activations. This paper proposes to address this frequency distribution issue by introducing a new adversarial training method."
105,SP:a50e9aeb17340b141f7b88d522911a5c9229f7d3,"optimization and generalization properties FEATURE-OF two - layer linear network. ( m+n-1)-th singular value FEATURE-OF imbalance "" matrix. ( m+n-1)-th singular value FEATURE-OF convergence rate. Task is over - parameterized linear regression. OtherScientificTerm are hidden width, gradient flow, global minimum, minimum L2 norm solution, and orthogonality assumption. Method is random initialization scheme. ","This paper studies the optimization and generalization properties of a two-layer linear network. The authors consider the case of over-parameterized linear regression, where the hidden width of the network is unknown and the gradient flow is unknown. They show that the convergence rate depends on the (m+n-1)-th singular value of the ""imbalance"" matrix, which is the global minimum of the minimum L2 norm solution under orthogonality assumption. They also propose a random initialization scheme."
106,SP:a50e9aeb17340b141f7b88d522911a5c9229f7d3,"convergence FEATURE-OF gradient descent. Metric is exponential convergence rate. OtherScientificTerm are smallest norm solution, generalizable solution, and SGD outputs. Generic is solution. ","This paper studies the convergence of gradient descent. The main result is an exponential convergence rate for the smallest norm solution, which is a generalizable solution that can be applied to many SGD outputs. The authors also provide a theoretical analysis of this solution."
107,SP:a50e9aeb17340b141f7b88d522911a5c9229f7d3,"gradient flow USED-FOR two - layer linear networks. convergence rate FEATURE-OF gradient flow. optimization CONJUNCTION generalization. generalization CONJUNCTION optimization. generalization CONJUNCTION overparameterization. overparameterization CONJUNCTION generalization. initialization CONJUNCTION optimization. optimization CONJUNCTION initialization. random initialization CONJUNCTION overparameterization. overparameterization CONJUNCTION random initialization. random initialization USED-FOR gradient flow trajectory. OtherScientificTerm are global minimum, low - dimensional manifold, and generalization ability. ","This paper studies the convergence rate of gradient flow for two-layer linear networks. The authors show that the gradient flow trajectory converges to a global minimum when the initialization, optimization, generalization, and overparameterization all converge to a fixed point on a low-dimensional manifold. They also provide a theoretical analysis of the generalization ability. Finally, the authors provide empirical evidence that the random initialization, the optimization, and generalization can converge to the global minimum."
108,SP:7341f8e456c0b80a59595f1cc145b776add3db3f,two - layer counterpart USED-FOR ReLU activations. kernel COMPARE two - layer counterpart. two - layer counterpart COMPARE kernel. approximation properties EVALUATE-FOR two - layer counterpart. approximation properties FEATURE-OF kernel. sphere FEATURE-OF deep fully - connected networks. deep fully - connected networks USED-FOR kernel. kernel framework USED-FOR deep networks. asymptotic eigenvalue decay FEATURE-OF dot - product kernels. differentiability properties FEATURE-OF kernel function. differentiability properties USED-FOR asymptotic eigenvalue decay. ,"This paper studies the approximation properties of the kernel compared to its two-layer counterpart for ReLU activations. The authors show that the kernel can be approximated by deep fully-connected networks on a sphere, and show that this kernel framework can be applied to deep networks. They also show the asymptotic eigenvalue decay of dot-product kernels with differentiability properties for the kernel function."
109,SP:7341f8e456c0b80a59595f1cc145b776add3db3f,reproducing kernel Hilbert space ( RKHS ) FEATURE-OF kernels. RKHS FEATURE-OF kernel. power series expansions FEATURE-OF kernel function. power series expansions USED-FOR eigenvalue decays. learning rate CONJUNCTION initialization. initialization CONJUNCTION learning rate. fully - connected ReLU networks FEATURE-OF NTK. RKHS FEATURE-OF NTK. infinite width FEATURE-OF fully - connected ReLU networks. initialization USED-FOR NTK. Laplace kernel CONJUNCTION infinitely differentiable kernels. infinitely differentiable kernels CONJUNCTION Laplace kernel. infinitely differentiable kernels HYPONYM-OF kernels. Laplace kernel HYPONYM-OF kernels. synthetic datasets CONJUNCTION MNIST / Fashion - MNIST. MNIST / Fashion - MNIST CONJUNCTION synthetic datasets. ,"This paper studies the problem of producing kernel Hilbert space (RKHS) for kernels. The authors show that the RKHS of a kernel can be expressed as a function of the power series expansions of the kernel function, which can be used to model eigenvalue decays. They show that for fully-connected ReLU networks with infinite width and infinite depth, NTK can be represented in the RkHS with a fixed learning rate and initialization. They also show that this can be done for other kernels such as the Laplace kernel and infinitely differentiable kernels. Experiments are conducted on synthetic datasets and MNIST/Fashion-MNIST."
110,SP:7341f8e456c0b80a59595f1cc145b776add3db3f,"kernels USED-FOR ReLU. deep fully - connected networks USED-FOR kernels. kernel framework USED-FOR deep networks. Method is deep learning theory papers. OtherScientificTerm are neural tangent kernel, and shallow two - layer counterpart. ","This paper proposes to use deep fully-connected networks to learn kernels for ReLU, which is an important topic in deep learning theory papers. In particular, the authors propose to learn a neural tangent kernel that is similar to the shallow two-layer counterpart of the ReLU. This kernel framework can be used to train deep networks."
111,SP:3dd495394b880cf2fa055ee3fe218477625d2605,TD3 algorithm USED-FOR deterministic policy - gradient algorithm. it USED-FOR overestimation issues. transitions USED-FOR Q - critics. convex combination USED-FOR deterministic policy gradient update. critics USED-FOR deterministic policy gradient update. critics USED-FOR convex combination. mixture parameter USED-FOR convex combination. replay mechanism PART-OF AD3 algorithm. algorithm USED-FOR this. replay mechanism USED-FOR algorithm. Method is TD3. OtherScientificTerm is on - policy distribution. ,"This paper proposes a new deterministic policy-gradient algorithm based on the TD3 algorithm. The main idea of TD3 is to use transitions to train Q-critics, and then use it to mitigate overestimation issues. The key idea is to learn a convex combination of the critics for the proposed policy gradient update, which is a mixture parameter that is parameterized by the on-policy distribution. The authors propose an algorithm for this using a replay mechanism from AD3 algorithm, which they call TD3."
112,SP:3dd495394b880cf2fa055ee3fe218477625d2605,it USED-FOR policy. two - step separation method USED-FOR mixing weights. method CONJUNCTION unbiased DRL. unbiased DRL CONJUNCTION method. AD3 ) HYPONYM-OF method. Method is Q learning. OtherScientificTerm is state - action value functions. ,"This paper proposes a two-step separation method for mixing weights in Q learning. The first step is to learn the state-action value functions, and then it is used to train a policy. The second step is the mixing of the weights. The authors compare their method (AD3) with unbiased DRL."
113,SP:3dd495394b880cf2fa055ee3fe218477625d2605,"approach USED-FOR overestimation issue. dual problem formulation USED-FOR policy parameters. two - step method USED-FOR parameters. Method is RL algorithms. OtherScientificTerm are weight parameter, and Q values. Generic is algorithm. ","This paper proposes an approach to address the overestimation issue in RL algorithms. The main idea is to use a dual problem formulation to estimate the policy parameters. The parameters are estimated using a two-step method, where the first step estimates the weight parameter and the second step updates the Q values. The algorithm is evaluated on a variety of datasets."
114,SP:a7f72a5f99f2e3e1a643e9bb83bf0416a859ec06,Monte Carlo expectation - maximization ( MCEM ) USED-FOR predictive distribution of trajectories. reward distribution parameter USED-FOR predictive distribution of trajectories. Monte Carlo expectation - maximization ( MCEM ) USED-FOR inverse reinforcement learning ( IRL ) algorithm. objectworld EVALUATE-FOR idea. OtherScientificTerm is environment dynamics. ,"This paper proposes an inverse reinforcement learning (IRL) algorithm that uses Monte Carlo expectation-maximization (MCEM) to estimate the predictive distribution of trajectories with respect to a reward distribution parameter. The idea is evaluated on a simple objectworld, where the environment dynamics are presented."
115,SP:a7f72a5f99f2e3e1a643e9bb83bf0416a859ec06,approach USED-FOR model - based inverse reinforcement learning. model - based inverse reinforcement learning USED-FOR Gaussian mixture model. reward - function parameters USED-FOR Gaussian mixture model. reward functions USED-FOR method. GMM USED-FOR reward functions. MCEM USED-FOR method. gradient - descent based maximum likelihood approach USED-FOR them. objectworld EVALUATE-FOR approach. ,This paper presents an approach to model-based inverse reinforcement learning for a Gaussian mixture model with different reward-function parameters. The proposed method is based on MCEM and uses two different reward functions derived from GMM and a gradient-descent based maximum likelihood approach to optimize them. The approach is evaluated on the objectworld.
116,SP:a7f72a5f99f2e3e1a643e9bb83bf0416a859ec06,"method USED-FOR inverse reinforcement learning. method USED-FOR inferring a ( distribution over ) reward functions. expert demonstrations USED-FOR inferring a ( distribution over ) reward functions. Bayesian methods USED-FOR probability distribution over reward functions. maximum entropy IRL HYPONYM-OF point - estimate. unknown dynamics CONJUNCTION non - linear rewards. non - linear rewards CONJUNCTION unknown dynamics. AIRL USED-FOR methods. complex environments CONJUNCTION non - linear rewards. non - linear rewards CONJUNCTION complex environments. maximum likelihood estimation USED-FOR generative model. probability distributions USED-FOR maximum likelihood estimation. It USED-FOR probability distribution. It USED-FOR non - linear rewards. prior Bayesian methods USED-FOR non - linear rewards. OtherScientificTerm is ( distribution over ) reward functions. Method are Maximum entropy IRL, and Bayesian IRL. ","This paper proposes a method for inverse reinforcement learning based on expert demonstrations for inferring a (distribution over) reward functions using Bayesian methods. The method is called maximum entropy IRL, which is a point-estimate of the probability distribution over reward functions. The methods are based on AIRL, which has been shown to work well in complex environments with unknown dynamics and non-linear rewards. The main contribution of this paper is the maximum likelihood estimation of the generative model based on probability distributions. It is shown to be able to estimate a probability distribution that is close to the true distribution of the reward function. It can also be used to approximate the true probability distribution of nonlinear rewards, which are not observed in prior Bayesian results. Maximum entropy is a generalization of Bayesian IRL."
117,SP:ee628e3ddc01de3f915b04834245c2250015e4d0,"finite sample bounds USED-FOR DNNs. generalization bounds FEATURE-OF DNNs. Method are self - training, and GAN. OtherScientificTerm are neighbor, perturbations, label distribution, and expansion assumption. ",This paper studies finite sample bounds for DNNs under the assumption of self-training. The main contribution of this paper is to provide generalization bounds on the generalization of DNN with infinite sample bounds. This is achieved by assuming that the neighbor of the data point is Gaussian and that the perturbations are independent of the label distribution. The authors show that this assumption holds for GAN and show that the expansion assumption does not hold for the case where the neighbor is not Gaussian.
118,SP:ee628e3ddc01de3f915b04834245c2250015e4d0,"expansion ” assumption USED-FOR low - probability data subset. finite - sample setting CONJUNCTION semi - supervised setting. semi - supervised setting CONJUNCTION finite - sample setting. Task is self - training, semi - supervised algorithms. OtherScientificTerm is neighborhoods. ","This paper studies the problem of self-training, semi-supervised algorithms. The authors make a “expansion” assumption on the low-probability data subset, and show that this assumption holds for both a finite-sample setting and a more general semi-unsupervised setting. In particular, the authors show that the neighborhoods of the training data can be expanded to a larger number of samples. "
119,SP:ee628e3ddc01de3f915b04834245c2250015e4d0,"self - training USED-FOR semi - supervised learning. unsupervised domain adaptation CONJUNCTION unsupervised learning. unsupervised learning CONJUNCTION unsupervised domain adaptation. semi - supervised learning CONJUNCTION unsupervised domain adaptation. unsupervised domain adaptation CONJUNCTION semi - supervised learning. self - training USED-FOR unsupervised domain adaptation. self - training USED-FOR unsupervised learning. algorithm USED-FOR self - training. Generic is assumption. OtherScientificTerm are expansion assumption, class conditional distribution, and input consistency. ","This paper studies the problem of self-training for semi-supervised learning, unsupervised domain adaptation, and the problem with the expansion assumption. The main contribution of this paper is the analysis of the assumption and how it can be improved. In particular, the authors show that under the class conditional distribution, the self-train of the algorithm converges to a point that is close to the input consistency. "
120,SP:daa229d78712808420aad4c50604fc28fd2a4aba,VAE based hierarchical model USED-FOR video prediction. recurrent model USED-FOR intermediate representations. model USED-FOR intermediate representations. pixel level information USED-FOR representations. label maps USED-FOR intermediate representations. recurrent model USED-FOR model. videos HYPONYM-OF pixel level information. domain knowledge USED-FOR representations. representations USED-FOR long term video prediction. modeling temporal evolution USED-FOR long term video prediction. representations USED-FOR temporal evolution. ,This paper proposes a VAE based hierarchical model for video prediction. The model uses a recurrent model to learn intermediate representations based on label maps. These intermediate representations are learned using pixel level information (e.g. videos) from the domain knowledge. These representations are then used for long term video prediction by modeling temporal evolution.
121,SP:daa229d78712808420aad4c50604fc28fd2a4aba,video - to - video translation model USED-FOR video prediction. Variational video prediction USED-FOR segmentation masks. Kitti CONJUNCTION Cityscapes. Cityscapes CONJUNCTION Kitti. Cityscapes CONJUNCTION dancing data. dancing data CONJUNCTION Cityscapes. model COMPARE methods. methods COMPARE model. dancing data EVALUATE-FOR methods. Kitti EVALUATE-FOR methods. Cityscapes EVALUATE-FOR model. dancing data EVALUATE-FOR model. Kitti EVALUATE-FOR model. ,"This paper proposes a video-to-video translation model for video prediction. Variational video prediction is used to generate segmentation masks. The proposed model is evaluated on Kitti, Cityscapes, and dancing data, and compared to other methods."
122,SP:daa229d78712808420aad4c50604fc28fd2a4aba,hierarchical framework USED-FOR long - term video prediction. real video sequence USED-FOR semantic map. model USED-FOR long - term video prediction. OtherScientificTerm is categorical structure space. ,"This paper proposes a hierarchical framework for long-term video prediction. The key idea is to learn a semantic map over a real video sequence, which is then mapped to a categorical structure space. The proposed model can be used for the task of long-time video prediction, and the experimental results are promising."
123,SP:e50b1931800daa7de577efd3edca523771227b3f,graph neural network architecture USED-FOR message passing. rules USED-FOR message passing. rules FEATURE-OF graph neural network architecture. Iterated Graph Neural Network System ( IGNNS ) HYPONYM-OF graph neural network architecture. it CONJUNCTION Iterated Function System ( IFS ). Iterated Function System ( IFS ) CONJUNCTION it. Iterated Function System ( IFS ) HYPONYM-OF fractal geometry. architecture COMPARE models. models COMPARE architecture. citation network datasets EVALUATE-FOR architecture. citation network datasets EVALUATE-FOR models. ,"This paper proposes a new graph neural network architecture, Iterated Graph Neural Network System (IGNNS), that uses rules for message passing. In particular, it is inspired by the fractal geometry and the Iterated Function System (IFS). Experiments on citation network datasets show that the proposed architecture outperforms existing models."
124,SP:e50b1931800daa7de577efd3edca523771227b3f,framework USED-FOR GNN. framework USED-FOR undirected and directed graphs. GNN USED-FOR undirected and directed graphs. symbol space FEATURE-OF message passing path. Iterated Function System USED-FOR symbol space. Generic is architectures. ,This paper proposes a framework to train GNN on undirected and directed graphs. The key idea is to learn a message passing path in the symbol space of the Iterated Function System. The paper provides a thorough analysis of the proposed architectures and provides a detailed comparison with existing methods.
125,SP:e50b1931800daa7de577efd3edca523771227b3f,"GNNs USED-FOR bi - directional message - passing processes. symbols space CONJUNCTION iterated function system. iterated function system CONJUNCTION symbols space. input layer CONJUNCTION IFS layer. IFS layer CONJUNCTION input layer. layer CONJUNCTION output layer. output layer CONJUNCTION layer. IFS layer CONJUNCTION layer. layer CONJUNCTION IFS layer. input layer USED-FOR FC layer. iterated function system USED-FOR IFS layer. steps PART-OF architecture. layer PART-OF steps. output layer PART-OF steps. adjacency matrix USED-FOR IFS layer. input layer HYPONYM-OF steps. IFS layer HYPONYM-OF steps. input layer PART-OF architecture. layer PART-OF architecture. IFS layer PART-OF architecture. learnable weight matrix USED-FOR output layer. output layer PART-OF architecture. Method are Bidirectional GCN, and IFS. ","This paper presents Bidirectional GCN, a generalization of GNNs for bi-directional message-passing processes. The proposed architecture consists of three steps: an input layer for the FC layer, an IFS layer that takes as input the symbols space and an iterated function system, and a layer that outputs the output of the IFS. The input layer is a learned adjacency matrix, and the output layer is an output layer with a learnable weight matrix. The authors show that the proposed architecture is able to achieve state-of-the-art performance on a variety of benchmarks. "
126,SP:89d65999a0600ec4f81daf6232fb5897676b3ce3,adversarial technics USED-FOR graph generation. Wassertain GAN USED-FOR algorithm. Wassertain GAN USED-FOR GG - GAN. similarity function USED-FOR GG - GAN. similarity function USED-FOR graph. method USED-FOR graphs. GG - GAN COMPARE approach. approach COMPARE GG - GAN. GG - GAN USED-FOR graphs. OtherScientificTerm is euclidian space. ,"This paper studies adversarial technics for graph generation. The proposed algorithm, GG-GAN, is based on Wassertain GAN. The key idea is to use a similarity function between a graph and its neighbors in euclidian space. The experiments show that the proposed method can generate graphs that are similar to the original approach."
127,SP:89d65999a0600ec4f81daf6232fb5897676b3ce3,framework USED-FOR isomorphism. OtherScientificTerm is geometric graphs. Generic is algorithm. Method is autoregressor based generative graph models. ,This paper proposes a framework for learning isomorphism between geometric graphs. The algorithm is based on the idea of autoregressor based generative graph models.
128,SP:89d65999a0600ec4f81daf6232fb5897676b3ce3,"WGAN architecture USED-FOR latent space. WGAN architecture USED-FOR generating new graphs. latent space USED-FOR generating new graphs. Generic is model. OtherScientificTerm are equivariant function, and isomorphic graphs. ",This paper proposes a WGAN architecture to learn a latent space for generating new graphs. The model learns an equivariant function that is invariant to the number of isomorphic graphs. 
129,SP:4f9388c18e44995fb1c6830256c520ff47a2e6ee,approach USED-FOR supervised learning. patttern - based rule mining USED-FOR robust rules. ImageNet CONJUNCTION Oxford Flower. Oxford Flower CONJUNCTION ImageNet. MNIST CONJUNCTION ImageNet. ImageNet CONJUNCTION MNIST. activation maps CONJUNCTION prototypes. prototypes CONJUNCTION activation maps. approach USED-FOR image processing tasks. rules COMPARE activation maps. activation maps COMPARE rules. ImageNet EVALUATE-FOR convolutional neural networks. MNIST EVALUATE-FOR convolutional neural networks. convolutional neural networks USED-FOR approach. convolutional neural networks USED-FOR image processing tasks. prototypes USED-FOR rules. Method is neural network. ,"This paper presents an approach for supervised learning that leverages patttern-based rule mining to learn robust rules. The proposed approach is based on convolutional neural networks and is evaluated on image processing tasks using MNIST, ImageNet, and Oxford Flower. The results show that the proposed rules are more robust than activation maps and prototypes, and that the neural network is able to generalize better."
130,SP:4f9388c18e44995fb1c6830256c520ff47a2e6ee,neural network USED-FOR interpretable rules. minimum description length USED-FOR approach. approach COMPARE approaches. approaches COMPARE approach. prototyping CONJUNCTION model distillation. model distillation CONJUNCTION prototyping. image data EVALUATE-FOR approach. model distillation HYPONYM-OF approaches. prototyping HYPONYM-OF approaches. OtherScientificTerm is rules. Method is downward closure lemma of apriori algorithm. ,"This paper proposes a neural network to learn interpretable rules. The proposed approach is based on minimizing the minimum description length of the rules, which is derived from the downward closure lemma of apriori algorithm. Experiments on image data show that the proposed approach outperforms existing approaches such as prototyping and model distillation."
131,SP:4f9388c18e44995fb1c6830256c520ff47a2e6ee,"method USED-FOR rules of interactions. algorithm USED-FOR rules. Minimum Description Length principle USED-FOR objective. greedy heuristic USED-FOR rule sets. MNIST CONJUNCTION GoogLeNet. GoogLeNet CONJUNCTION MNIST. GoogLeNet USED-FOR images. MNIST USED-FOR images. images EVALUATE-FOR algorithm. Method are neural network, and EXPLAINN. OtherScientificTerm are rule, and neurons. ","This paper proposes EXPLAINN, a method for learning rules of interactions between neurons in a neural network. The objective is based on the Minimum Description Length principle, where the length of a rule is defined as the distance between two neurons. The authors propose an algorithm for learning such rules. The algorithm is evaluated on images from MNIST and GoogLeNet using a greedy heuristic for learning rule sets."
132,SP:fc75d8d62ac5cc4cdde1b923ae54659a0dfba28b,"theoretical framework USED-FOR reinforcement learning algorithms. reinforcement learning algorithms USED-FOR fixed dataset policy optimization ( FDPO ) setting. learner USED-FOR model or value function. explicit or implicit uncertainty FEATURE-OF model or value function. explicitly pessimistic algorithms USED-FOR worst case error. naive approach COMPARE explicitly pessimistic approach. explicitly pessimistic approach COMPARE naive approach. proximal algorithms COMPARE naive approach. naive approach COMPARE proximal algorithms. error EVALUATE-FOR naive approach. proximal algorithms COMPARE explicitly pessimistic approach. explicitly pessimistic approach COMPARE proximal algorithms. error EVALUATE-FOR explicitly pessimistic approach. proximal algorithms PART-OF model - free batch RL. collection policy USED-FOR proximal algorithms. error EVALUATE-FOR proximal algorithms. OtherScientificTerm are policy, uncertainty, and loss. ",This paper proposes a theoretical framework for training reinforcement learning algorithms in the fixed dataset policy optimization (FDPO) setting. The key idea is to train a policy that maximizes the worst-case error of the loss over the entire dataset. This is achieved by training a learner to minimize a model or value function with explicit or implicit uncertainty. The paper shows that the naive approach can achieve better error than the explicitly pessimistic approach in terms of worst case error when the uncertainty is high. The authors also show that proximal algorithms trained with a collection policy can achieve similar error as the explicitly pessimist approach in model-free batch RL.
133,SP:fc75d8d62ac5cc4cdde1b923ae54659a0dfba28b,"naive policy evaluations USED-FOR overestimation of the value function. naive policy evaluations USED-FOR deep ) RL algorithms. theoretical guarantees FEATURE-OF policy improvements. penalty term USED-FOR data generating policy. proximal penalty term COMPARE imitation learning. imitation learning COMPARE proximal penalty term. penalty term COMPARE proximal penalty term. proximal penalty term COMPARE penalty term. data generating policy HYPONYM-OF imitation learning. visitation counts USED-FOR penalty term. OtherScientificTerm are overestimation, and state - action pairs. Generic is algorithms. ","This paper studies naive policy evaluations for (deep) RL algorithms in the context of overestimation of the value function. The authors provide theoretical guarantees for policy improvements that do not depend on the number of state-action pairs. They also propose a new penalty term for the data generating policy based on visitation counts, which is similar to the proximal penalty term in imitation learning (i.e., the policy generating policy is penalized for overestimation). The algorithms are evaluated on a variety of datasets."
134,SP:fc75d8d62ac5cc4cdde1b923ae54659a0dfba28b,"pessimism USED-FOR faulty over - estimation. finite datasets USED-FOR faulty over - estimation. naive algorithms USED-FOR optimal policy. pessimism FEATURE-OF naive algorithms. proximal family of algorithms COMPARE imitation learning. imitation learning COMPARE proximal family of algorithms. proximal "" algorithms PART-OF pessimistic algorithms. Method is uncertainty - aware algorithms. ","This paper studies pessimism in the context of faulty over-estimation on finite datasets. The authors show that naive algorithms with pessimism fail to converge to the optimal policy, and propose uncertainty-aware algorithms to address this issue. The pessimistic algorithms consist of ""proximal"" algorithms, which they call ""probabilistic"" algorithms. The proximal family of algorithms is shown to outperform imitation learning."
135,SP:363661edd15a06a800b51abc1541a3191311ee0e,"asynchronous leapfrog integrator USED-FOR numerically solving neural ODEs. constant memory guarantee FEATURE-OF method. dynamical modelling CONJUNCTION generative modelling. generative modelling CONJUNCTION dynamical modelling. classification CONJUNCTION dynamical modelling. dynamical modelling CONJUNCTION classification. tasks EVALUATE-FOR method. classification EVALUATE-FOR method. generative modelling HYPONYM-OF tasks. classification HYPONYM-OF tasks. dynamical modelling HYPONYM-OF tasks. Method are MALI, adjoint method, and adaptive checkpoint adjoint ( ACA ) method. Metric is reverse - time accuracy. OtherScientificTerm is stability region. ","This paper proposes an asynchronous leapfrog integrator for numerically solving neural ODEs. The proposed method, MALI, is based on the idea of an adjoint method, where the stability region of the forward pass of the backward-time accuracy is computed using an adaptive checkpoint adjoint (ACA) method. The method has a constant memory guarantee and is evaluated on three tasks: classification, dynamical modelling, and generative modelling."
136,SP:363661edd15a06a800b51abc1541a3191311ee0e,"methods USED-FOR gradients. memory cost EVALUATE-FOR method. Method are neural ODEs, naive method, ODE solver, and adjoint method. OtherScientificTerm are reverse trajectory errors, and numerical solution. ","This paper studies the problem of learning neural ODEs. The authors propose two methods for learning gradients. The first is a naive method, where the reverse trajectory errors are assumed to be zero. The second is an ODE solver, in which the numerical solution is given by an adjoint method. The memory cost of the proposed method is discussed."
137,SP:363661edd15a06a800b51abc1541a3191311ee0e,"algorithm USED-FOR neural ODEs. numerical solver step PART-OF neural ODE. invertible neural network USED-FOR numerical solver step. asynchronous leafprog integrator USED-FOR invertible neural network. intermediate data PART-OF numerical integration steps. algorithm COMPARE methods. methods COMPARE algorithm. memory USED-FOR algorithm. ACA HYPONYM-OF methods. OtherScientificTerm are gradient, memory savings, and inverse. Metric is theoretical stability analysis. ",This paper proposes a new algorithm for solving neural ODEs. The numerical solver step of a neural OODE is represented as an invertible neural network with an asynchronous leafprog integrator. The gradient of the inverse is optimized to minimize the number of iterations needed to solve the problem. The authors also propose to incorporate intermediate data into the numerical integration steps to achieve memory savings. The theoretical stability analysis is provided. The proposed algorithm is compared to other methods such as ACA.
138,SP:45b6d522ed9a2ecda2db0a3d45688ed3b0f32875,"Method is scene generation methods. Generic is models. OtherScientificTerm are seen conditionings, training distribution, and fine - grained conditionings. ","This paper studies the problem of scene generation methods. The authors argue that models trained on seen conditionings often fail to generalize well to unseen conditionings. They argue that this is due to the fact that the training distribution tends to be very similar to the original training distribution, which is not suitable for fine-grained conditionings (e.g. for scenes with multiple objects)."
139,SP:45b6d522ed9a2ecda2db0a3d45688ed3b0f32875,COCO - Stuff dataset USED-FOR unseen complex scene generation. unseen fine - grained conditionings CONJUNCTION unseen coarse conditionings. unseen coarse conditionings CONJUNCTION unseen fine - grained conditionings. seen conditionings CONJUNCTION unseen fine - grained conditionings. unseen fine - grained conditionings CONJUNCTION seen conditionings. image generation CONJUNCTION unseen fine - grained conditionings. unseen fine - grained conditionings CONJUNCTION image generation. seen conditionings USED-FOR image generation. FID CONJUNCTION diversity score. diversity score CONJUNCTION FID. object accuracy CONJUNCTION FID. FID CONJUNCTION object accuracy. recall CONJUNCTION conditional consistency. conditional consistency CONJUNCTION recall. F1 - score CONJUNCTION object accuracy. object accuracy CONJUNCTION F1 - score. conditional consistency CONJUNCTION F1 - score. F1 - score CONJUNCTION conditional consistency. precision CONJUNCTION recall. recall CONJUNCTION precision. object accuracy CONJUNCTION diversity score. diversity score CONJUNCTION object accuracy. diversity score FEATURE-OF object - wise and scene - wise measures. F1 - score CONJUNCTION FID. FID CONJUNCTION F1 - score. Task is scene conditional image generation. Generic is model. ,"This paper studies unseen complex scene generation on the COCO-Stuff dataset. The authors consider the problem of scene conditional image generation, where image generation is performed with seen conditionings, unseen fine-grained conditionings and unseen coarse conditionings. The proposed model is evaluated on a variety of object-wise and scene-wise measures, including precision, recall, conditional consistency, F1-score, FID, and diversity score."
140,SP:45b6d522ed9a2ecda2db0a3d45688ed3b0f32875,layout of objects CONJUNCTION label maps. label maps CONJUNCTION layout of objects. label maps USED-FOR image synthesis. layout of objects USED-FOR image synthesis. optimization CONJUNCTION training data. training data CONJUNCTION optimization. fixed backbone CONJUNCTION optimization. optimization CONJUNCTION fixed backbone. training data CONJUNCTION evaluation protocol. evaluation protocol CONJUNCTION training data. LostGAN CONJUNCTION OC - GAN. OC - GAN CONJUNCTION LostGAN. G2im CONJUNCTION LostGAN. LostGAN CONJUNCTION G2im. methodology USED-FOR approaches. layout of objects USED-FOR image. approaches USED-FOR image. layout of objects USED-FOR approaches. OC - GAN HYPONYM-OF approaches. G2im HYPONYM-OF approaches. LostGAN HYPONYM-OF approaches. ,"This paper studies the problem of image synthesis with the layout of objects and label maps. The authors propose a methodology for combining the approaches of G2im, LostGAN, and OC-GAN to generate an image with a fixed backbone, optimization, training data, and evaluation protocol. "
141,SP:77bce8c5d383f6be82ebc694bf66fb1a408ad751,Graph - Neural Networks ( GNNs ) USED-FOR graph modles. tasks EVALUATE-FOR GA - MLPs. node - classification CONJUNCTION community detection. community detection CONJUNCTION node - classification. graph - isomorphism CONJUNCTION node - classification. node - classification CONJUNCTION graph - isomorphism. GNNs USED-FOR problems. graph - isomorphism HYPONYM-OF problems. community detection HYPONYM-OF problems. node - classification HYPONYM-OF problems. ,"Graph-Neural Networks (GNNs) are widely used to learn graph modles. However, the performance of GA-MLPs is not well evaluated on a variety of tasks. This paper aims to address this issue by proposing a new set of tasks to evaluate the performance and efficiency of the GA - MLPs. In particular, the authors propose three problems that require GNNs to solve: graph-isomorphism, node-classification and community detection."
142,SP:77bce8c5d383f6be82ebc694bf66fb1a408ad751,"GA - MLPs HYPONYM-OF MLPs. nodes FEATURE-OF MLPs. nodes USED-FOR GA - MLPs. graph USED-FOR node features. graph isomorphism testing CONJUNCTION node level functions. node level functions CONJUNCTION graph isomorphism testing. Method are graph neural networks ( GNNs ), and GNNs. ","This paper studies the problem of training graph neural networks (GNNs). The authors propose GA-MLPs, which are MLPs that are trained on nodes rather than on the entire graph. The authors show that GA-mlPs can be trained on the same number of nodes as existing GNNs, and that the node features of the graph can be used to train the graph. They also provide some theoretical results on graph isomorphism testing and node level functions."
143,SP:77bce8c5d383f6be82ebc694bf66fb1a408ad751,Graph Augmented MLPs ( GA - MLPs ) HYPONYM-OF Graph Neural Networks ( GNNs ). MLP USED-FOR embeddings. augmented embedding USED-FOR GA - MLPs. linear transformations USED-FOR input representations. linear transformations USED-FOR augmented embeddings. GA - MLPs USED-FOR graph problems. GA - MLPs COMPARE GNNs. GNNs COMPARE GA - MLPs. Method is non - linear MLPs. ,"Graph Augmented MLPs (GA-MLPs) are an extension of Graph Neural Networks (GNNs), i.e., Graph Augmentation-based Gradient-based Neural Network (GANs). Unlike non-linear MLPs, which use an MLP to learn embeddings for each node, the proposed GA-MLP uses an augmented embedding. The authors propose to use linear transformations to augment the input representations of the augmented representations. The experiments show that the proposed GNNs perform better than the original GNN. In addition, the authors demonstrate the utility of using GA-mlPs for graph problems."
144,SP:5c0783e92017fc808ebd44a7d1aa7f6b92baacd8,"distillation USED-FOR distributed RL settings. CPUs USED-FOR data collection. accelerated hardware USED-FOR learning. GPU HYPONYM-OF accelerated hardware. transformer USED-FOR learner. Transformer USED-FOR Actors setup. wallclock time EVALUATE-FOR Transformer. OtherScientificTerm are policy, and LSTM actors. Metric is sample - efficiency. ","This paper studies the problem of distillation in distributed RL settings, where data collection is done on CPUs, and the policy is trained on LSTM actors. The authors propose to use accelerated hardware (e.g. GPU) to accelerate learning. The main idea is to use a transformer to train the learner in an Actors setup, and to use the transformer to reduce the wallclock time of the Transformer. They show that this improves sample-efficiency."
145,SP:5c0783e92017fc808ebd44a7d1aa7f6b92baacd8,"method USED-FOR actor - latency constrained "" settings. models USED-FOR RL. transformers HYPONYM-OF models. partial observability FEATURE-OF long - term credit assignment. transformers USED-FOR learner. LSTM agent USED-FOR actors. hardware acceleration USED-FOR transformers. Metric is sample complexity. Material is T - maze. Generic is they. ","This paper proposes a method for ""actor-latency constrained"" settings, where the learner is trained with transformers (i.e., transformers with hardware acceleration) and the actors are trained with an LSTM agent. The authors argue that models such as transformers are important for RL because they allow for partial observability of long-term credit assignment, which can reduce sample complexity. The experiments are conducted on the T-maze and show that they can achieve state-of-the-art performance."
146,SP:5c0783e92017fc808ebd44a7d1aa7f6b92baacd8,"solution USED-FOR actor - latency constrained settings. policy distillation USED-FOR learner model ”. actor - latency constrained settings PART-OF RL. policy distillation USED-FOR solution. sample efficiency EVALUATE-FOR transformer models. procedure COMPARE transformers. transformers COMPARE procedure. wall - clock run - time EVALUATE-FOR LSTM agents. sample efficiency EVALUATE-FOR procedure. wall - clock run - time EVALUATE-FOR procedure. sample efficiency EVALUATE-FOR transformers. Method are actor model ”, LSTM - based actor, and Actor - Learner Distillation ( ALD ). ","This paper proposes a solution to actor-latency constrained settings in RL using policy distillation of a “learner model”. The proposed procedure, called Actor-Learner Distillation (ALD), trains an LSTM-based actor, and then distills the learned model to a new learner model. The experimental results show that the proposed procedure improves the wall-clock run-time of standard LMT and transformer models while maintaining the same sample efficiency as transformers."
147,SP:ccd59c3acb3d0886030451bbaea68fb83ef4dfa5,"Universal Representation Transformer ( URT ) layer USED-FOR universal representation. Universal Representation Transformer ( URT ) layer USED-FOR task - adapted representations. universal representation CONJUNCTION task - adapted representations. task - adapted representations CONJUNCTION universal representation. selection procedure USED-FOR backbone. SUR USED-FOR universal representation. SUR USED-FOR method. selection procedure USED-FOR universal representation. pre - trained and domain - specific backbones USED-FOR universal representation. attention - based layer USED-FOR backbones. attention - based layer USED-FOR task. Task are multi - domain few - shot image classification, and few - shot tasks. Material is diverse data sources. OtherScientificTerm is support set. Generic are layer, it, and tasks. ","This paper addresses the problem of multi-domain few-shot image classification. The authors propose a Universal Representation Transformer (URT) layer to learn universal representation and task-adapted representations from diverse data sources. The proposed method is based on SUR and uses a selection procedure to find the best backbone for a given support set. The main idea is to use pre-trained and domain-specific backbones to learn the universal representation. The attention-based layer is used to select the best backbones for the given task, and the layer is trained so that it can be applied to new tasks. Experiments are conducted on a variety of standard and new domains for a variety on a range of few-shoot tasks."
148,SP:ccd59c3acb3d0886030451bbaea68fb83ef4dfa5,meta - dataset USED-FOR Few - shot learning. model USED-FOR domain - specific representations. features USED-FOR transformer model. domain - specific backbones USED-FOR features. model COMPARE state - of - the - art. state - of - the - art COMPARE model. Generic is model components. ,Few-shot learning with a meta-dataset is a hot topic in the recent years. This paper proposes a model that learns domain-specific representations and uses these features to train a transformer model. The model components are well-motivated and the experimental results show that the proposed model outperforms the state-of-the-art.
149,SP:ccd59c3acb3d0886030451bbaea68fb83ef4dfa5,"it USED-FOR task - adapted representation. self - attention HYPONYM-OF learnable component ( self - attention ). learnable component ( self - attention ) PART-OF work. it USED-FOR work. Generic are method, framework, and model. Method is Universal Representations. ","This paper proposes a method called Universal Representations, which is a general framework for learning representations that can be adapted to new tasks. The work consists of a learnable component (self-attention) which is similar to self-supervision, but it learns a task-adapted representation, and it can be applied to other work as well. The model is trained in a supervised fashion, and the model is evaluated on a variety of tasks."
150,SP:beaa3dfef4bdf3d8fea64d4cf86911f45edd2873,non - parametric approach USED-FOR unsupervised progressive learning ( UPL ). STAM USED-FOR unsupervised progressive learning ( UPL ). unsupervised progressive learning ( UPL ) HYPONYM-OF continual unsupervised learning. single - stream requirement FEATURE-OF continual unsupervised learning. STAM HYPONYM-OF non - parametric approach. STAM USED-FOR visual tasks. dual - memory USED-FOR prototypical features. online clustering of hierarchical visual features CONJUNCTION novelty detection. novelty detection CONJUNCTION online clustering of hierarchical visual features. novelty detection CONJUNCTION dual - memory. dual - memory CONJUNCTION novelty detection. components PART-OF It. online clustering of hierarchical visual features HYPONYM-OF components. dual - memory HYPONYM-OF components. online clustering of hierarchical visual features PART-OF It. novelty detection HYPONYM-OF components. novelty detection PART-OF It. dual - memory PART-OF It. STAM COMPARE MAS. MAS COMPARE STAM. STAM COMPARE GEM. GEM COMPARE STAM. GEM COMPARE MAS. MAS COMPARE GEM. OtherScientificTerm is hierarchical visual features. ,"This paper proposes STAM, a non-parametric approach for unsupervised progressive learning (UPL) with a single-stream requirement. It consists of three components: online clustering of hierarchical visual features, novelty detection, and dual-memory for prototypical features. STAM is applied to visual tasks and is shown to outperform MAS and GEM."
151,SP:beaa3dfef4bdf3d8fea64d4cf86911f45edd2873,approach USED-FOR unsupervised progressive learning. novelty detection CONJUNCTION dropping. dropping CONJUNCTION novelty detection. architecture + algorithms USED-FOR unsupervised progressive learning. centroids USED-FOR hierarchies. centroids CONJUNCTION novelty detection. novelty detection CONJUNCTION centroids. non - stationary environment FEATURE-OF unsupervised progressive learning. EMNIST CONJUNCTION SVHN. SVHN CONJUNCTION EMNIST. MNIST CONJUNCTION EMNIST. EMNIST CONJUNCTION MNIST. SVHN CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION SVHN. image datasets EVALUATE-FOR adapted methods. CIFAR-10 HYPONYM-OF image datasets. MNIST HYPONYM-OF image datasets. SVHN HYPONYM-OF image datasets. EMNIST HYPONYM-OF image datasets. they USED-FOR centroid to label(s ) mappings. Method is online clustering. Task is supervised setting. ,"This paper proposes an approach to unsupervised progressive learning in a non-stationary environment. The main idea is to use online clustering, where the centroids are used to represent hierarchies, and the novelty detection and dropping are performed offline. The proposed architecture + algorithms can be used to improve the performance of the unstate-of-the-art in unun supervised progressive learning. The adapted methods are evaluated on several image datasets, including MNIST, EMNIST, SVHN, and CIFAR-10, and they are able to learn centroid to label(s) mappings in a supervised setting."
152,SP:beaa3dfef4bdf3d8fea64d4cf86911f45edd2873,"clustering CONJUNCTION long - term memory ( buffered ). long - term memory ( buffered ) CONJUNCTION clustering. supervision signal ( classification ) CONJUNCTION unsupervised ( clustering ). unsupervised ( clustering ) CONJUNCTION supervision signal ( classification ). model USED-FOR UPL tasks. supervision signal ( classification USED-FOR UPL tasks. unsupervised ( clustering ) USED-FOR UPL tasks. clustering USED-FOR model. long - term memory ( buffered ) USED-FOR model. Task is Unsupervised progressive learning "" ( UPL ) problem. OtherScientificTerm is biological agents. ","This paper studies the ""Unsupervised progressive learning"" (UPL) problem. The authors propose a model that combines supervision signal (classification), unsupervised (clustering) and long-term memory (buffered) to solve UPL tasks. The model is evaluated on a variety of datasets and compared to biological agents."
153,SP:f7a8e5a580524d54f4a0cd08bd3cb0a0a074528b,consensus distance USED-FOR generalization gap. optimization CONJUNCTION generalization. generalization CONJUNCTION optimization. consensus distance USED-FOR generalization. consensus distance USED-FOR optimization. consensus distance USED-FOR generalization. centralized training USED-FOR generalization. Task is decentralized optimization problem. ,This paper studies the decentralized optimization problem and proposes to use the consensus distance to reduce the generalization gap between optimization and generalization. The authors show that the proposed consensus distance can help improve generalization in the decentralized setting. They also show that centralized training can help generalization as well.
154,SP:f7a8e5a580524d54f4a0cd08bd3cb0a0a074528b,"decentralized gradient methods USED-FOR deep networks. CIFAR CONJUNCTION tiny - ImageNet. tiny - ImageNet CONJUNCTION CIFAR. OtherScientificTerm are critical consensus distance, and disagreement. Method is synchronous symmetric averaging methods. ","This paper studies decentralized gradient methods for deep networks. The authors propose to use synchronous symmetric averaging methods, where the critical consensus distance is defined as the difference between the average and the disagreement between the two groups. Experiments are conducted on CIFAR and tiny-ImageNet."
155,SP:f7a8e5a580524d54f4a0cd08bd3cb0a0a074528b,"upper bound USED-FOR decentralized training. decentralized training COMPARE centralized one. centralized one COMPARE decentralized training. upper bound USED-FOR dissimilarity of local variables. upper bound COMPARE centralized one. centralized one COMPARE upper bound. heuristic guidelines USED-FOR consensus. OtherScientificTerm are computing units, and network. ",This paper provides an upper bound on the dissimilarity of local variables for decentralized training compared to the centralized one. The authors provide heuristic guidelines for consensus and show that the number of computing units can be as small as a fraction of the total number of nodes in the network.
156,SP:08ab81a53ae0b51b214442f2f9d6edca0df9118c,RNNs USED-FOR metric learning between sequences. dynamical system CONJUNCTION RNN. RNN CONJUNCTION dynamical system. synchronization PART-OF dynamical system. coupling USED-FOR synchronization. RNNs USED-FOR synchronization. coupling USED-FOR RNNs. Method is siamese RNNs. ,"This paper studies the problem of metric learning between sequences using RNNs. The main contribution of this paper is to propose siamese rNNs, where the synchronization between a dynamical system and an RNN is modeled as a coupling. The coupling is used to improve the synchronization of the RNN."
157,SP:08ab81a53ae0b51b214442f2f9d6edca0df9118c,"dynamic systems USED-FOR architecture. action recognition dataset EVALUATE-FOR approach. Generic are system, dataset, and systems. OtherScientificTerm is coupling. ","This paper proposes a new architecture based on dynamic systems. The idea is to train a system on a dataset, and then train a separate architecture on top of the learned systems. This is done by coupling the learned architecture with the learned system. The approach is evaluated on an action recognition dataset."
158,SP:08ab81a53ae0b51b214442f2f9d6edca0df9118c,parallel between synchronized trajectories CONJUNCTION distance between similar sequences. distance between similar sequences CONJUNCTION parallel between synchronized trajectories. dynamical systems USED-FOR parallel between synchronized trajectories. siamese style recurrent neural network USED-FOR distance between similar sequences. Gated Recurrent Unit architecture ( CGRU ) HYPONYM-OF siamese recurrent network setting. identical sub - networks CONJUNCTION dynamical systems. dynamical systems CONJUNCTION identical sub - networks. model USED-FOR synchronization of unaligned multi - variate sequences. model USED-FOR similarity metric. similarity metric CONJUNCTION synchronization of unaligned multi - variate sequences. synchronization of unaligned multi - variate sequences CONJUNCTION similarity metric. UCI activity recognition dataset ( mobile data ) EVALUATE-FOR siamese Gated Recurrent Unit ( SGRU ) architecture. Task is sequence metric learning problem. OtherScientificTerm is coupling. ,This paper studies the sequence metric learning problem and proposes a siamese style recurrent neural network to learn the parallel between synchronized trajectories and the distance between similar sequences. The authors propose the Gated Recurrent Unit architecture (CGRU) which is a variant of the existing Siamese recurrent network setting with identical sub-networks and dynamical systems. The proposed model is able to learn a similarity metric and the synchronization of unaligned multi-variate sequences. Experiments on UCI activity recognition dataset (mobile data) demonstrate the effectiveness of the proposed siamesed Gated Rebuttal Unit (SGRU). The authors also show that the proposed coupling can improve the performance.
159,SP:e32bb6044bcb26cad3b0161db19170d726c40fae,co - distillation USED-FOR distributed training. moderate batch sizes FEATURE-OF co - distillation. distillation - like techniques USED-FOR synchronous SGD training. ,This paper studies the problem of co-distillation in distributed training with moderate batch sizes. The authors propose distillation-like techniques for synchronous SGD training.
160,SP:e32bb6044bcb26cad3b0161db19170d726c40fae,"codistillation USED-FOR data parallel distributed training. loss term USED-FOR local model updates. loss term USED-FOR model. classification outcomes FEATURE-OF model. Method are minibatch SGD algorithm, and local SGD. Task is distributed training. OtherScientificTerm are local updates, and node. Generic is alternative. ","This paper proposes a codistillation for data parallel distributed training. The authors propose a minibatch SGD algorithm, where the local model updates are optimized with a loss term that penalizes the model for the classification outcomes of the local updates. The proposed alternative is called local SGD. The main contribution of this paper is to study distributed training from the perspective of the node."
161,SP:e32bb6044bcb26cad3b0161db19170d726c40fae,codistillation USED-FOR distributed training. codistillation USED-FOR regularizer. model CONJUNCTION initialization. initialization CONJUNCTION model. codistillation USED-FOR sync SGD. overfitting CONJUNCTION robustness. robustness CONJUNCTION overfitting. robustness EVALUATE-FOR hyper - parameters. OtherScientificTerm is training configurations. ,"This paper proposes to use codistillation as a regularizer for distributed training. The main idea is to train the model and initialization separately, and then use the same model to train sync SGD with different training configurations. The authors show that this can reduce overfitting and robustness to hyper-parameters."
162,SP:cd03bc0b12cf44e9d538d274de7dfe44acdb1e35,step size CONJUNCTION batch size. batch size CONJUNCTION step size. curvature CONJUNCTION step size. step size CONJUNCTION curvature. iterates USED-FOR heavy - tailed random variable. synthetic data CONJUNCTION fully connected neural networks. fully connected neural networks CONJUNCTION synthetic data. SGD USED-FOR deep learning. Task is quadratic and convex problem. ,"This paper studies the quadratic and convex problem of estimating the curvature, step size, and batch size of SGD in deep learning. The authors propose to use iterates to estimate a heavy-tailed random variable, which is then used to estimate the step size. Experiments are conducted on synthetic data and fully connected neural networks."
163,SP:cd03bc0b12cf44e9d538d274de7dfe44acdb1e35,SGD iterations USED-FOR random variables with heavy - tail random distributions. step size CONJUNCTION batch size. batch size CONJUNCTION step size. batch size CONJUNCTION problem dimension. problem dimension CONJUNCTION batch size. step size CONJUNCTION batch size. batch size CONJUNCTION step size. batch size CONJUNCTION problem dimension. problem dimension CONJUNCTION batch size. tail - index FEATURE-OF distribution. SGD USED-FOR distribution. ,"This paper studies the effect of SGD iterations on random variables with heavy-tail random distributions. The authors study the impact of step size, batch size, problem dimension, and batch size on the performance of the SGD. They show that the tail-index of the distribution learned by SGD depends on the number of iterations, the batch size and the problem dimension."
164,SP:cd03bc0b12cf44e9d538d274de7dfe44acdb1e35,step size CONJUNCTION batch size. batch size CONJUNCTION step size. curvature CONJUNCTION step size. step size CONJUNCTION curvature. SGD USED-FOR quadratic optimization problem. linear stochastic recursion USED-FOR SGD recursion. implicit renew theory USED-FOR statistical properties. step size CONJUNCTION batch size. batch size CONJUNCTION step size. Hessian structure FEATURE-OF loss function. SGD iterates USED-FOR heavy - tailed stationary distribution. Hessian structure USED-FOR heavy - tailed stationary distribution. moment bounds CONJUNCTION convergence rate. convergence rate CONJUNCTION moment bounds. ,"This paper studies SGD recursion with linear stochastic recursion in the setting of a quadratic optimization problem, where the curvature, step size, and batch size are unknown. The authors use implicit renew theory to study the statistical properties of SGD iterates and derive moment bounds and convergence rate for a heavy-tailed stationary distribution with Hessian structure of the loss function."
165,SP:89f995142f8a2fcdc8c7b9f2e2cd1a4f75df3226,GCNs USED-FOR spectral filtering. spectral viewpoint USED-FOR GCNs. low frequencies USED-FOR GCNs. MLP USED-FOR graph tasks. low frequency information ( Eigen - pairs ) FEATURE-OF MLP. smoothness CONJUNCTION high frequency ablations. high frequency ablations CONJUNCTION smoothness. OtherScientificTerm is eigen - pairs. ,This paper studies GCNs from a spectral viewpoint. The authors show that GCNs trained with low frequencies perform well in spectral filtering. They also show that MLP with low frequency information (Eigen-pairs) performs well on graph tasks. They further show that smoothness and high frequency ablations do not affect the performance of eigen-peaks.
166,SP:89f995142f8a2fcdc8c7b9f2e2cd1a4f75df3226,spectral perturbations / manipulations FEATURE-OF GCN. citeseer CONJUNCTION pubmed. pubmed CONJUNCTION citeseer. cora CONJUNCTION citeseer. citeseer CONJUNCTION cora. cora HYPONYM-OF benchmark datasets. pubmed HYPONYM-OF benchmark datasets. citeseer HYPONYM-OF benchmark datasets. MLP USED-FOR feature matrix. eigenvectors USED-FOR low - frequency domain. eigenvectors USED-FOR node feature matrix. method COMPARE vanilla GCN. vanilla GCN COMPARE method. method COMPARE baselines. baselines COMPARE method. pubmed EVALUATE-FOR baselines. pubmed EVALUATE-FOR method. ,"This paper studies the spectral perturbations/manipulations of GCN on three benchmark datasets: cora, citeseer, and pubmed. The authors propose to use an MLP to learn a feature matrix for each node, and use eigenvectors for the low-frequency domain. Experiments show that the proposed method outperforms vanilla GCN and several baselines on pubmed and cora."
167,SP:89f995142f8a2fcdc8c7b9f2e2cd1a4f75df3226,spectral manipulations USED-FOR GCN models. bandpass filtering USED-FOR GCNs. low - frequencies COMPARE high - frequencies. high - frequencies COMPARE low - frequencies. low - frequencies USED-FOR GCNs. band - pass filters USED-FOR GCN model. Task is signal processing. ,"This paper studies spectral manipulations of GCN models. The authors show that GCNs trained with bandpass filtering are sensitive to low-frequency instead of high-frequencies, which is an important issue in signal processing. The paper then proposes to modify the GCN model by adding band-pass filters."
168,SP:7fc7e37c699a1bb738c65f0c6fa983203c6fd067,graph structure CONJUNCTION node features. node features CONJUNCTION graph structure. node features USED-FOR node representations. Graph neural networks ( GNNs ) USED-FOR node representations. brain signals CONJUNCTION particle reconstruction. particle reconstruction CONJUNCTION brain signals. node features COMPARE graph structure. graph structure COMPARE node features. Method is GNNs. ,Graph neural networks (GNNs) learn node representations by combining graph structure and node features. The authors show that GNNs are able to learn from brain signals and particle reconstruction. They also show that node features are more informative than graph structure.
169,SP:7fc7e37c699a1bb738c65f0c6fa983203c6fd067,"graph structures CONJUNCTION GNN parameters. GNN parameters CONJUNCTION graph structures. parallel training CONJUNCTION self - supervised task. self - supervised task CONJUNCTION parallel training. self - supervised auxiliary task USED-FOR method. self - supervised task HYPONYM-OF self - supervised auxiliary task. parallel training HYPONYM-OF self - supervised auxiliary task. de - noised auto - encoding USED-FOR self - supervised task. fully - parameterized adjacency matrix CONJUNCTION KNN construction. KNN construction CONJUNCTION fully - parameterized adjacency matrix. KNN construction USED-FOR latent graph structure. fully - parameterized adjacency matrix USED-FOR latent graph structure. OtherScientificTerm are graph structure, and node features. Method is MLP. Generic is model. ","This paper proposes a method that uses a self-supervised auxiliary task (e.g., parallel training or self-regressive task with de-noised auto-encoding) as a method for learning graph structures and GNN parameters. The key idea is to learn a latent graph structure using a fully-parameterized adjacency matrix and a KNN construction, where the graph structure is learned as a function of the node features, and the parameters of the MLP are learned as the model is trained. "
170,SP:7fc7e37c699a1bb738c65f0c6fa983203c6fd067,labeled data CONJUNCTION missing graph structures. missing graph structures CONJUNCTION labeled data. missing graph structures USED-FOR nodes classification. labeled data USED-FOR nodes classification. solution USED-FOR unobserved graph structure. solution USED-FOR classification model. denoise autoencoder layer USED-FOR learning. supervision FEATURE-OF denoise autoencoder layer. benchmark graph data sets EVALUATE-FOR model. ,This paper addresses the problem of nodes classification with both labeled data and missing graph structures. The authors propose a solution to deal with the unobserved graph structure and propose a classification model based on the proposed solution. A denoise autoencoder layer is used for learning with supervision. The proposed model is evaluated on several benchmark graph data sets.
171,SP:8f8e1fa4cd025fc056a60c0b6ba9915e8617447f,"novelty detection module USED-FOR unsupervised class - incremental learning. SVHN CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION SVHN. MNIST CONJUNCTION SVHN. SVHN CONJUNCTION MNIST. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. SVHN EVALUATE-FOR method. MNIST EVALUATE-FOR method. CIFAR-100 EVALUATE-FOR method. CIFAR-10 EVALUATE-FOR method. Method is novelty detection. OtherScientificTerm is model update. Generic are model, and module. ","This paper proposes a novelty detection module for unsupervised class-incremental learning. The novelty detection is based on the observation that when the model update is performed on the same class as the original model, the new class is more likely to be novel than the original class. The proposed module is evaluated on MNIST, SVHN, CIFAR-10, and the recently proposed CIFR-100."
172,SP:8f8e1fa4cd025fc056a60c0b6ba9915e8617447f,batch PART-OF unsupervised learning context. method USED-FOR anomaly detection. anomaly detection CONJUNCTION incremental learning. incremental learning CONJUNCTION anomaly detection. method USED-FOR incremental learning. Method is classifier. Generic is it. ,This paper proposes a method for anomaly detection and incremental learning. The idea is to add a batch to the unsupervised learning context and train a classifier on it. 
173,SP:8f8e1fa4cd025fc056a60c0b6ba9915e8617447f,"it USED-FOR classification model. method USED-FOR novelty detection. technique USED-FOR novelty detection. technique USED-FOR confusion - based novelty detection. technique USED-FOR class - imbalance. class - imbalance USED-FOR confusion - based novelty detection. Method is unsupervised class - incremental learning. Generic are exposure, and one. Metric are classification accuracy, and accuracy. ","This paper proposes unsupervised class-incremental learning, which is an extension of the work of [1] and [2]. The idea is to train a classification model on a small amount of exposure and then use it to fine-tune the classification model so that it can generalize better to larger amounts of exposure. The method is applied to novelty detection, where the goal is to improve the classification accuracy. The main contribution of this paper is to propose a technique for confusion-based novelty detection based on class-imbalance to address the problem of class-overbalance. The paper is well-written and well-motivated, and the experiments are well-conducted. However, there are a few issues with the paper: (1) the experimental results are not very convincing, and (2) the accuracy is not very high. There are two main issues with this paper: one is the lack of comparison to [1], and the other one is a lack of analysis."
174,SP:1d242517748c52f2be8f0613316cda3a54d1d2f7,"policies USED-FOR tasks. natural language USED-FOR constraints. model USED-FOR natural language constraints. model USED-FOR intermediate representations. spatial and temporal information FEATURE-OF intermediate representations. OtherScientificTerm are observation, and safe trajectory. ",This paper proposes to learn policies for tasks where the observation is not available. The goal is to learn a safe trajectory that minimizes the distance between the observed trajectory and the target trajectory. The authors propose a model that learns natural language constraints for these constraints using natural language. The model learns intermediate representations that incorporate spatial and temporal information.
175,SP:1d242517748c52f2be8f0613316cda3a54d1d2f7,algorithm USED-FOR policy. natural language constraints USED-FOR algorithm. Hazard World HYPONYM-OF navigation task. relational constraints CONJUNCTION sequential constraints. sequential constraints CONJUNCTION relational constraints. budgetary constraints CONJUNCTION relational constraints. relational constraints CONJUNCTION budgetary constraints. sequential constraints HYPONYM-OF constraints. budgetary constraints HYPONYM-OF constraints. relational constraints HYPONYM-OF constraints. constraints PART-OF natural language. algorithm USED-FOR mapping. intermediate representation USED-FOR mapping. algorithm USED-FOR policy. intermediate representation USED-FOR policy. intermediate representation USED-FOR algorithm. Generic is two - step solution. OtherScientificTerm is natural language constraint. ,"This paper proposes an algorithm to learn a policy under natural language constraints on a navigation task (Hazard World). The authors propose a two-step solution: first, they learn a mapping between constraints in natural language (budgetary constraints, relational constraints, sequential constraints, etc.), and then they train a policy on the intermediate representation of this mapping. The authors show that the learned policy is able to generalize well to unseen environments. They also show that a natural language constraint can be used as a proxy for a policy."
176,SP:1d242517748c52f2be8f0613316cda3a54d1d2f7,"natural language constraints FEATURE-OF safe reinforcement learning agents. policy network USED-FOR RL agent. constraint interpreter USED-FOR language constraints. constraint interpreter CONJUNCTION policy network. policy network CONJUNCTION constraint interpreter. components PART-OF model. policy network HYPONYM-OF components. constraint interpreter HYPONYM-OF components. it COMPARE baselines. baselines COMPARE it. Material is Hazard World. Generic are problem, and algorithm. OtherScientificTerm is natural language. ","This paper studies the problem of learning safe reinforcement learning agents under natural language constraints. The model consists of two components: a constraint interpreter to enforce language constraints and a policy network to guide the RL agent. The authors propose a new problem, called Hazard World, and show that the proposed algorithm is able to solve the problem in natural language. They also show that it outperforms several baselines."
177,SP:bc9f37b4622868a92f9812d2ea901def79229d41,few - shot edge detection task COMPARE few - shot segmentation. few - shot segmentation COMPARE few - shot edge detection task. edge detection dataset ( BSD ) CONJUNCTION few shot segmentation dataset ( FSS ). few shot segmentation dataset ( FSS ) CONJUNCTION edge detection dataset ( BSD ). datasets USED-FOR task. Task is detecting semantic edges. Generic is method. ,"This paper proposes a new few-shot edge detection task, which is different from few -shot segmentation. The main idea is to use two datasets for the task: an edge detection dataset (BSD) and a few shot segmentation dataset (FSS). The main contribution of this paper is to propose a method for detecting semantic edges."
178,SP:bc9f37b4622868a92f9812d2ea901def79229d41,"segmentation process USED-FOR edge detector. segmentation process USED-FOR semantic information. label sparsity FEATURE-OF few - shot scenario. semantic information USED-FOR edge detector. high - dimensional embeddings USED-FOR feature matching. Multi - Split Matching Regularization ( MSMR ) HYPONYM-OF meta - learning approach. Task is few - shot semantic edge detection. OtherScientificTerm are semantic boundaries, and overfitting. ","This paper studies the problem of few-shot semantic edge detection. The authors propose a meta-learning approach called Multi-Split Matching Regularization (MSMR), which aims to improve the segmentation process of the edge detector to extract semantic information from the label sparsity in the few-shooting scenario. The key idea is to use high-dimensional embeddings for feature matching, and to enforce semantic boundaries to prevent overfitting."
179,SP:bc9f37b4622868a92f9812d2ea901def79229d41,"foreground CONJUNCTION background probability. background probability CONJUNCTION foreground. foreground USED-FOR attention. attention USED-FOR background probability. decoder USED-FOR edge - map. encoder USED-FOR feature maps. attention map USED-FOR feature maps. Task are few - shot semantic edge detection, and few - shot segmentation stage. Generic are problem, and stages. OtherScientificTerm is averaged feature vector. ","This paper considers the problem of few-shot semantic edge detection. The problem is formulated as a two-stage process. First, the edge-map is generated by a decoder and then the foreground and background probability are estimated using attention. Second, the feature maps are generated by an encoder and the attention map is used as the averaged feature vector. Both stages are repeated until convergence. Finally, the authors perform a few -shot segmentation stage."
180,SP:5e99fee48137d3d3d88017a02f7285ce35dce970,"causal effect view USED-FOR graph neural networks. edges PART-OF explanatory subgraph. method COMPARE comparing methods. comparing methods COMPARE method. Method are Causal Screening, and GNNs. OtherScientificTerm are do(\cdot)$ calculus, edge, and prediction probability. ","This paper proposes a causal effect view for graph neural networks. Causal Screening is a popular method for evaluating the performance of GNNs. In this paper, the authors propose to use the do(\cdot)$ calculus to estimate the probability that an edge will be added to an explanatory subgraph. The authors show that the prediction probability of adding an edge can be estimated by minimizing the distance between the edge and the explanatory subgroup. The proposed method is compared with several comparing methods."
181,SP:5e99fee48137d3d3d88017a02f7285ce35dce970,"procedure USED-FOR subgraph. greedy approach USED-FOR method. attribution USED-FOR edges. node clustering USED-FOR graph. Method is GNN function. OtherScientificTerm are edge, and mutual information. Metric is computational complexity. ","This paper proposes a new procedure for learning a subgraph. The method is based on a greedy approach, where the GNN function tries to maximize the mutual information between the node and the edge. This is achieved by minimizing the computational complexity of the graph using node clustering. The authors also propose to use attribution to identify the edges."
182,SP:5e99fee48137d3d3d88017a02f7285ce35dce970,"graph USED-FOR method. method USED-FOR GNN models. Method are Causal screening, and GNN. OtherScientificTerm are explanatory subgraph, and edges. Generic are it, and them. ","Causal screening is a method to train GNN models on a graph. The idea is that the explanatory subgraph can be viewed as a weighted sum of the edges of the original graph, and that it can be used to train the GNN. The method is simple and straightforward, and the experiments show that the method can be applied to a wide range of datasets."
183,SP:5b3d76b9e67bc39a813979b5d232a59f597d257d,"pruning USED-FOR network. training loss EVALUATE-FOR model. generalisation ability FEATURE-OF networks. pruning robustness CONJUNCTION generalisation ability. generalisation ability CONJUNCTION pruning robustness. Metric is generalisation measure. Generic are measure, and dataset. ","This paper proposes a new generalisation measure, which is based on the idea that pruning can improve the generalisation ability of a network by reducing the training loss of the model. The authors propose a new measure called ""pruning robustness"" and show that this measure can be used to measure the trade-off between the performance of the network and the generalised performance of a dataset. The paper also provides a theoretical analysis of the tradeoff between pruning and generalisation performance."
184,SP:5b3d76b9e67bc39a813979b5d232a59f597d257d,Method is deep networks. Metric is prunability. Task is generalization. OtherScientificTerm is Prunability. Generic is measure. ,This paper studies the problem of prunability in deep networks. Prunability is a measure that measures the ability of deep networks to generalize to unseen data points. This paper proposes a new measure called “Prunability” to measure the degree to which deep networks can generalize. 
185,SP:5b3d76b9e67bc39a813979b5d232a59f597d257d,"simplicity / complexity measure EVALUATE-FOR deep networks. it COMPARE pre - existing ( margin based ) measures. pre - existing ( margin based ) measures COMPARE it. prunability COMPARE pre - existing ( margin based ) measures. pre - existing ( margin based ) measures COMPARE prunability. generalization EVALUATE-FOR prunability. perturbation robustness CONJUNCTION flatness measures. flatness measures CONJUNCTION perturbation robustness. Generic are measure, and metric. OtherScientificTerm are low training loss, dropout, and double - descent. ","This paper proposes a new simplicity/complexity measure for evaluating deep networks. The proposed measure, called prunability, is based on the observation that a low training loss can lead to a dropout that is similar to double-descent. The authors show that this metric is robust to perturbation and flatness measures, and that it outperforms pre-existing (margin based) measures in terms of generalization."
186,SP:835d01ee91523fb29595cae8339dfe49de7d3a7c,"it USED-FOR discrete representation of images. unsupervised representation learning USED-FOR method. contrastive predictive coding ( CPC ) USED-FOR it. Gumbel Softmax USED-FOR discrete latent variable. discrete latent variable USED-FOR contrastive predictive coding ( CPC ). Gumbel Softmax USED-FOR contrastive predictive coding ( CPC ). contrastive predictive coding ( CPC ) USED-FOR discrete representation of images. Task is long horizon visual planning. Material is offline dataset of interaction. OtherScientificTerm are goal image, graphs, and graph. Method is planning procedure. ","This paper addresses the problem of long horizon visual planning. The proposed method is based on unsupervised representation learning and uses contrastive predictive coding (CPC) to learn it to learn a discrete representation of images using Gumbel Softmax as a discrete latent variable. The method is trained on an offline dataset of interaction, where the goal image is drawn from a set of graphs, and the goal is represented as a graph. The planning procedure is then repeated until convergence."
187,SP:835d01ee91523fb29595cae8339dfe49de7d3a7c,"learning discrete representations CONJUNCTION planning. planning CONJUNCTION learning discrete representations. graph search USED-FOR long horizon tasks. planning USED-FOR long horizon tasks. graph search USED-FOR planning. planning PART-OF method. learning discrete representations PART-OF method. data USED-FOR representation encoder. encodings USED-FOR discrete representation. similarity matrix FEATURE-OF contrastive learning objective. contrastive learning objective USED-FOR encoder. representations CONJUNCTION abstract planner. abstract planner CONJUNCTION representations. MPC USED-FOR action - conditional predictive model. planar planning tasks EVALUATE-FOR approach. Method are random exploration, CNN, and low - level controller. Generic are network, graph, and task. OtherScientificTerm are one - hot encodings, spatial and temporal abstractions, graph of transitions, and k - object arrangement. ","This paper proposes a method for learning discrete representations and planning in the context of graph search for long horizon tasks. The method is based on random exploration, where a representation encoder is trained on the data, and a network is trained to predict the next state in the graph. The authors propose to use one-hot encodings, which are learned to capture both spatial and temporal abstractions. The encoder uses a contrastive learning objective based on the similarity matrix between the learned representations and the abstract planner. The action-conditional predictive model is trained using MPC, where the CNN is a low-level controller, and the graph of transitions is learned to represent the k-object arrangement of the task. The proposed approach is evaluated on planar planning tasks."
188,SP:835d01ee91523fb29595cae8339dfe49de7d3a7c,enforced temporal consistency USED-FOR discrete representation. videos USED-FOR discrete representation. DORB USED-FOR long - horizon tasks. DORB USED-FOR representations. Generic is representation. OtherScientificTerm is discrete embedding. Method is MPC. ,This paper proposes enforcing temporal consistency to learn a discrete representation from videos. The representation is learned by minimizing the distance between the discrete embedding and the embedding of the video. The authors show that DORB is able to learn representations that generalize well to long-horizon tasks. They also show that MPC can be used to improve the performance.
189,SP:2e31a542a7a60b1d425d95dd26e62374ba799cb8,"binary representation USED-FOR weights. OtherScientificTerm are quantization bits ( precision ), group sparsity, and bit allocation. Task is training. ","This paper proposes to use quantization bits (precision) as a measure of group sparsity in training. Specifically, the authors propose to use a binary representation of the weights, where the weights are sampled from the binary representation. The authors argue that the bit allocation is a function of the number of groups in the group, and that this can be used to measure the group size. "
190,SP:2e31a542a7a60b1d425d95dd26e62374ba799cb8,"method USED-FOR neural networks. group lasso USED-FOR method. STE USED-FOR binary representation. bit - plane USED-FOR binary representation. accuracy CONJUNCTION compression ratio. compression ratio CONJUNCTION accuracy. accuracy EVALUATE-FOR method. compression ratio EVALUATE-FOR method. OtherScientificTerm are bit - planes, and LSBs. ","This paper proposes a method for training neural networks using group lasso. The method is based on STE, which learns a binary representation on a bit-plane, and then compresses the bit-planes into LSBs. The proposed method is evaluated on both accuracy and compression ratio."
191,SP:2e31a542a7a60b1d425d95dd26e62374ba799cb8,"Quantization of weights USED-FOR DNNs. regularization techniques USED-FOR problem. bit - level sparsity USED-FOR bit - width. Metric are computational and storage costs, and precision. Method is deep learning. OtherScientificTerm is layer - wise bit - widths. Task is optimization problem. Generic is model. ","Quantization of weights in DNNs has been a hot topic in recent years due to its computational and storage costs. This paper studies the problem of reducing the bit-width of the weights in deep learning and proposes regularization techniques to tackle this problem. Specifically, the authors propose to use bit-level sparsity as a way to reduce the bit -width, which can reduce the precision of the model. The authors also consider the optimization problem of minimizing the number of layer-wise bit-weights."
192,SP:9b8ae88357e03447c73c792ff5c173ddc3d365e7,robustness FEATURE-OF binary quantized networks. gradient vanishing issue FEATURE-OF robustness. temperature scaling approach USED-FOR attack generation. methods USED-FOR temperature scale. It USED-FOR temperature scale. methods PART-OF It. singular values of the input - output Jacobian HYPONYM-OF methods. ,"This paper studies the problem of robustness to gradient vanishing issue in binary quantized networks. It proposes a temperature scaling approach for attack generation. It consists of two methods: 1) singular values of the input-output Jacobian, and 2) a temperature scale."
193,SP:9b8ae88357e03447c73c792ff5c173ddc3d365e7,temperature rescaling USED-FOR non - linear loss functions. temperature scaling USED-FOR linear loss functions. PGD++ attack COMPARE PGD. PGD COMPARE PGD++ attack. linear DLR loss USED-FOR PGD. robustness evaluation EVALUATE-FOR FGSM. OtherScientificTerm is gradients. ,"This paper studies the problem of temperature rescaling for non-linear loss functions. The authors show that temperature scaling for linear loss functions is equivalent to temperature scaling in the case of non-Linear loss functions, and propose a PGD++ attack that is more robust than PGD with linear DLR loss. They also show that FGSM with FGSM improves the robustness evaluation in terms of gradients."
194,SP:9b8ae88357e03447c73c792ff5c173ddc3d365e7,"attack algorithm USED-FOR gradient signal. robustness EVALUATE-FOR quantized networks. scalar multiplier USED-FOR network logits. scalar multiplier USED-FOR model ’s decision boundary. attack USED-FOR it. FGSM CONJUNCTION PGD. PGD CONJUNCTION FGSM. approach USED-FOR quantized networks. approach USED-FOR floating - point networks. approach COMPARE attacks. attacks COMPARE approach. FGSM HYPONYM-OF attacks. PGD HYPONYM-OF attacks. Generic are model, approaches, and modification. OtherScientificTerm is Jacobian. ","This paper studies the problem of robustness of quantized networks. The authors propose an attack algorithm that generates a gradient signal that is independent of the model’s decision boundary using a scalar multiplier applied to the network logits. They show that the proposed attack can be applied to any floating-point network and that it is equivalent to an existing attack. They also show that their approach can be used to attack floating-points networks and compare their approach to other attacks such as FGSM and PGD. They compare their approaches to a variety of datasets and demonstrate that their modification is effective. Finally, they provide a theoretical analysis of the effect of the Jacobian."
195,SP:16dddbe1432e4ffbf4b2a9180bf3c67495bd9e81,"prototype - based model USED-FOR paragraph classification. ProtoryNet HYPONYM-OF prototype - based model. OtherScientificTerm are sentiment scores, and prototypes. ","This paper proposes a prototype-based model, ProtoryNet, for paragraph classification. The main idea is to use sentiment scores as a proxy for prototypes, and to use prototypes as a surrogate for sentiment scores. The paper is well-written and easy to follow."
196,SP:16dddbe1432e4ffbf4b2a9180bf3c67495bd9e81,RNN sequence classifying model USED-FOR prototype. prototypes USED-FOR model. accuracy EVALUATE-FOR model. model USED-FOR prototype. interpretability EVALUATE-FOR user evaluation. Generic is method. Method is LSTM. ,This paper proposes a method to evaluate the interpretability of a prototype generated by an RNN sequence classifying model. The proposed method is based on the observation that prototypes generated by a model trained on prototypes can be used to improve the accuracy of the model in order to produce a more interpretable prototype. The authors propose to use an LSTM to generate the prototypes and evaluate user evaluation based on interpretability.
197,SP:16dddbe1432e4ffbf4b2a9180bf3c67495bd9e81,"framework USED-FOR text data. prototype trajectory HYPONYM-OF sentence prototypes. sentence prototypes USED-FOR framework. layer CONJUNCTION prototype layer. prototype layer CONJUNCTION layer. layer PART-OF framework. prototype layer PART-OF framework. representation USED-FOR classification of the sentence. LSTM structure USED-FOR representation. LSTM structure USED-FOR classification of the sentence. Method are ProtoryNet, and one - hot encoding. OtherScientificTerm are prototypes, prototype trajectories, and sentence prototype. Task is entity of the text level. ","This paper proposes a framework for text data based on sentence prototypes (e.g., prototype trajectory). The proposed framework, ProtoryNet, consists of a single layer, a prototype layer, and a representation for the classification of the sentence using an LSTM structure. The prototype trajectories are encoded as an entity of the text level, and the one-hot encoding is used to represent the sentence prototype."
198,SP:ac8a9afa6e87f9c36d080c2e7085c4e096af64ff,"data likelihood function USED-FOR HMM. RNN architecture USED-FOR data likelihood function. gradient descent USED-FOR likelihood function. formulation USED-FOR Alzheimer's disease Symptom Progression. OtherScientificTerm is neurons. Generic is encoding. Method are neural network, and EM algorithm. ","This paper proposes a data likelihood function for HMM based on an RNN architecture. The likelihood function is learned by gradient descent, where the weights of the neurons are sampled from an encoding. This encoding is then fed into a neural network, and the EM algorithm is used to estimate the likelihood function. The authors apply this formulation to Alzheimer's disease Symptom Progression."
199,SP:ac8a9afa6e87f9c36d080c2e7085c4e096af64ff,recurrent neural network USED-FOR HMM. HMRNN USED-FOR model - parameters. network COMPARE HMM. HMM COMPARE network. Method is neural network learning framework. ,This paper proposes a neural network learning framework that extends the HMM with a recurrent neural network. The main idea is to use the HMRNN to learn the model-parameters. The proposed network is shown to outperform the original HMM.
200,SP:ac8a9afa6e87f9c36d080c2e7085c4e096af64ff,HMRNN USED-FOR log - likelihood. HMM USED-FOR log - likelihood. HMRNN HYPONYM-OF RNN architecture. log - likelihood FEATURE-OF HMM. HMRNN COMPARE HMM. HMM COMPARE HMRNN. Baum - Welch algorithm USED-FOR HMM. HMRNN training COMPARE HMM. HMM COMPARE HMRNN training. HMM USED-FOR Alzheimer's progression prediction. Baum - Welch algorithm USED-FOR HMM. Metric is HMRNN objective function. ,"This paper proposes a new RNN architecture, HMRNN, which is an extension of HMM, a well-known and widely-used RNN algorithm. The main idea of the paper is to use HMM to approximate the log-likelihood of the original HMM. The authors show that the proposed HMM is equivalent to HMM with the Baum-Welch algorithm in terms of the log -likelihood. The paper also shows that the HMRN objective function is similar to the original one. The experiments show that in the case of Alzheimer's progression prediction, HMM trained with the proposed Bau et al. (2018) algorithm outperforms HMM training with the standard HMM by a large margin."
201,SP:6355337707f1dd373813290e26e9c0a264b993f9,"Factorized Linear Discriminant Analysis HYPONYM-OF dimensionality reduction method. neurobiology USED-FOR method. technique USED-FOR linear projections. OtherScientificTerm are neural genes, phenotypes, and phenotypical aspect. Generic is approach. Material is Drosophila T4 / T5 cells. ",This paper proposes a dimensionality reduction method called Factorized Linear Discriminant Analysis. The method is inspired by neurobiology and is motivated by the observation that neural genes are highly correlated with phenotypes. The proposed technique is applied to linear projections and is able to remove the phenotypical aspect. Experiments on Drosophila T4/T5 cells demonstrate the effectiveness of the proposed approach.
202,SP:6355337707f1dd373813290e26e9c0a264b993f9,"factors CONJUNCTION regularizer. regularizer CONJUNCTION factors. Method is ANOVA. Generic is method. OtherScientificTerm are features, and phenotypic categories. ","This paper proposes ANOVA, a method to estimate the similarity between two groups of features (i.e., phenotypic categories). The method is based on the observation that features that are similar to each other are more likely to be similar than features that differ from each other. The authors propose two factors and a regularizer for this purpose. "
203,SP:6355337707f1dd373813290e26e9c0a264b993f9,method USED-FOR unknown genetic targets. Task is gene data analysis. Generic is it. ,"This paper proposes a method for identifying unknown genetic targets for gene data analysis. The idea is interesting and the paper is well-written. However, there are some issues with the method and it is not clear to me how it can be applied in practice."
204,SP:28e61a4f51f9f7283397d6336ea114375ae6a004,"saliency map interpretability method USED-FOR image classification. random variable USED-FOR saliency map. classifier USED-FOR image. Variational approximation USED-FOR posterior. OtherScientificTerm are posterior distribution, likelihood, and prior. ","This paper proposes a saliency map interpretability method for image classification. The saliency maps are modeled as a random variable, and the posterior distribution is learned by minimizing the likelihood of the posterior. Variational approximation is used to approximate the posterior, which is then used to train a classifier to classify an image. The posterior is trained to be similar to the prior."
205,SP:28e61a4f51f9f7283397d6336ea114375ae6a004,method USED-FOR generating saliency maps. generating saliency maps USED-FOR image classifiers. saliency map random variable USED-FOR probabilistic model. variational methods USED-FOR inference. OtherScientificTerm is saliency maps. Method is classifier. Material is pixel perturbation benchmark. ,"This paper proposes a method for generating saliency maps for image classifiers. The saliency map random variable is used to train a probabilistic model, which is then used to estimate the saliency of a classifier. The inference is done using variational methods. The authors conduct experiments on a pixel perturbation benchmark."
206,SP:28e61a4f51f9f7283397d6336ea114375ae6a004,interpretability method USED-FOR image classification networks. It USED-FOR posterior distribution. posterior distribution FEATURE-OF saliency map. random variable FEATURE-OF saliency map. saliency map USED-FOR It. likelihood function CONJUNCTION prior distribution. prior distribution CONJUNCTION likelihood function. prior distribution USED-FOR posterior distribution. posterior distribution USED-FOR classifier ’s prediction. posterior distribution USED-FOR saliency map. likelihood function USED-FOR posterior distribution. perturbation benchmark EVALUATE-FOR method. method COMPARE baselines. baselines COMPARE method. perturbation benchmark EVALUATE-FOR baselines. ,This paper proposes an interpretability method for image classification networks. It learns the posterior distribution of a saliency map over a random variable. This posterior distribution is then used to train a classifier’s prediction using a likelihood function and a prior distribution. The method is evaluated on a perturbation benchmark and compared to several baselines.
207,SP:01acd8b88768d86bcf21b8c20a930d706c5645a7,"technique USED-FOR debiasing pretrained contextual embedding models. approach USED-FOR 2 layer fully - connected neural network. 2 layer fully - connected neural network USED-FOR debiased "" representation. regularizer USED-FOR representation. representation CONJUNCTION word embedding. word embedding CONJUNCTION representation. word embedding USED-FOR biased token. regularizer USED-FOR word embedding. regularizer USED-FOR biased token. Method is pretrained model. Generic is model. ","This paper proposes a technique for debiasing pretrained contextual embedding models. The proposed approach is to train a 2 layer fully-connected neural network to learn a ""debiased"" representation of the original pretrained model. The model is trained on a set of samples from the original model, and a regularizer is applied to the representation and the word embedding of the biased token."
208,SP:01acd8b88768d86bcf21b8c20a930d706c5645a7,debiasing method USED-FOR social bias. debiasing method USED-FOR pretrained NLP models. sentence representations USED-FOR pretrained NLP model. sentence representations USED-FOR neural network. mutual information USED-FOR neural network. sentence representation CONJUNCTION sensitive word representation. sensitive word representation CONJUNCTION sentence representation. mutual information EVALUATE-FOR sentence representation. method USED-FOR bias. method COMPARE pretrained model. pretrained model COMPARE method. downstream task EVALUATE-FOR pretrained model. downstream task EVALUATE-FOR method. Method is unbiased representations. OtherScientificTerm is sensitive words. Generic is network. ,"This paper proposes a debiasing method to reduce social bias in pretrained NLP models. The idea is to train a neural network with sentence representations that maximize the mutual information between the sentence representation and the sensitive word representation. The authors argue that unbiased representations are biased towards sensitive words, and that the proposed method is able to reduce the bias without compromising the performance of the network. The method is evaluated on a downstream task and compared to a pretrained model."
209,SP:01acd8b88768d86bcf21b8c20a930d706c5645a7,mutual information FEATURE-OF embeddings. contrastive learning objective USED-FOR embeddings. WHEAT style task EVALUATE-FOR method. Method is sentence encoders. OtherScientificTerm is gender cueing words. ,This paper proposes to train sentence encoders with gender cueing words. The key idea is to use a contrastive learning objective to learn embeddings with mutual information between the gender and gender cues. The method is evaluated on a WHEAT style task.
210,SP:f2f505d3f07ca3bb2f16f6f6f5d00fee98da6531,sample - wise randomized smoothing technique USED-FOR certification of robustness. pretrain - to - finetune methodology USED-FOR networks. sample - wise randomized smoothing USED-FOR pretrain - to - finetune methodology. training methodology CONJUNCTION certification methodology. certification methodology CONJUNCTION training methodology. CIFAR CONJUNCTION MNIST. MNIST CONJUNCTION CIFAR. randomized smoothing techniques USED-FOR Smooth - Adv. OtherScientificTerm is noise level. ,"This paper proposes a pretrain-to-finetune methodology for training networks using sample-wise randomized smoothing technique for the certification of robustness. The proposed training methodology and certification methodology are evaluated on CIFAR and MNIST. The results show that Smooth-Adv can achieve state-of-the-art performance with randomized smoothed techniques, even when the noise level is high."
211,SP:f2f505d3f07ca3bb2f16f6f6f5d00fee98da6531,variance USED-FOR verified radius. variance of the Gaussian perturbations HYPONYM-OF smoothing parameters. smoothing parameters USED-FOR model. robustness EVALUATE-FOR smoothing parameter. Method is randomized smoothing. ,"This paper proposes randomized smoothing, where the smoothing parameters (variance of the Gaussian perturbations) are used to estimate the verified radius of the model. The authors show that the proposed smoothing parameter improves the robustness."
212,SP:f2f505d3f07ca3bb2f16f6f6f5d00fee98da6531,"provably defense USED-FOR adversarial perturbations. randomized smoothing USED-FOR provably defense. approach USED-FOR randomized smoothing. noise level USED-FOR small perturbations. randomized smoothing USED-FOR small perturbations. noise level FEATURE-OF randomized smoothing. Method is sample - wise randomized smoothing. OtherScientificTerm are noise levels, and noise. Generic is model. ","This paper proposes a sample-wise randomized smoothing, which is a generalization of the work of Chen et al. (2018) that extends the approach of randomized smoothed to provably defense against adversarial perturbations. The key idea is to use the same noise level for all the noise levels of the original model, and then use the new noise level as a proxy for the true noise level of the random smoothing. "
213,SP:692c7b9f6d982bbc5a22e566296a97e8a530b87c,"Tomographic auto - encoder ( TAE ) USED-FOR unsupervised recovery of corrupted data. Bayesian approach USED-FOR posterior distribution. TAE USED-FOR posterior distribution. posterior distribution FEATURE-OF clean image. TAE USED-FOR modeling uncertainty in data recovery. Bayesian approach USED-FOR TAE. hierarchical latent variable models USED-FOR prior and variational posterior. hierarchical latent variable models USED-FOR model. Method are VAE, and stochastic gradient variational inference. OtherScientificTerm is latent variable collapse. ","This paper proposes a novel Tomographic auto-encoder (TAE) for unsupervised recovery of corrupted data. TAE uses a Bayesian approach to learn the posterior distribution of a clean image. The model is based on hierarchical latent variable models that jointly learn the prior and variational posterior, which is similar to VAE, but with stochastic gradient variational inference. The authors show that TAE is effective at modeling uncertainty in data recovery, and that latent variable collapse can be avoided."
214,SP:692c7b9f6d982bbc5a22e566296a97e8a530b87c,models USED-FOR corrupted data. missing items CONJUNCTION noisy observations. noisy observations CONJUNCTION missing items. noisy observations HYPONYM-OF corruptions. missing items HYPONYM-OF corruptions. tomographic auto - encoder ( TVAE ) USED-FOR inference. data space USED-FOR inference. TVAE USED-FOR diverse samples. prior regularization USED-FOR data space. corrupted observations USED-FOR diverse samples. TVAE USED-FOR diverse samples. test ELBO EVALUATE-FOR baselines. TVAE COMPARE baselines. baselines COMPARE TVAE. test ELBO EVALUATE-FOR TVAE. ,This paper proposes to train models to detect corrupted data with two types of corruptions: missing items and noisy observations. The authors propose a tomographic auto-encoder (TVAE) to perform inference on the corrupted data. TVAE is trained to generate diverse samples from corrupted observations and uses prior regularization in the data space to improve inference performance. Experimental results show that TVAE outperforms the baselines in terms of test ELBO.
215,SP:692c7b9f6d982bbc5a22e566296a97e8a530b87c,approach USED-FOR recovery of dirty data. fully unsupervised scenarios FEATURE-OF recovery of dirty data. missing data CONJUNCTION noisy samples. noisy samples CONJUNCTION missing data. missing data PART-OF corrupted data. noisy samples PART-OF corrupted data. reduced entropy condition inference method USED-FOR VAE model. Generic is model. Material is clean examples. ,This paper presents an approach for the recovery of dirty data in fully unsupervised scenarios. The corrupted data consists of missing data and noisy samples. The authors propose a VAE model based on a reduced entropy condition inference method. The model is able to recover the corrupted data from clean examples.
216,SP:4b7d050f57507166992034e5e264cccab3cb874f,"multi - hop self - attention mechanism USED-FOR attention based graph neural networks. MAGNA HYPONYM-OF multi - hop self - attention mechanism. method USED-FOR receptive field. diffusion step FEATURE-OF attention coefficients. GAT networks USED-FOR MAGNA method. OtherScientificTerm are attention coefficient, and neighbourhood. Generic is approach. Method is GCNs. ","This paper proposes MAGNA, a multi-hop self-attention mechanism for attention based graph neural networks. The proposed method learns a receptive field by sampling from a neighbourhood of nodes in the neighbourhood, and then using the attention coefficient of each node as the input to the next node. This approach is similar to previous work on GCNs. The main difference is that the proposed MAGNA method is based on GAT networks and does not require a diffusion step to compute the attention coefficients."
217,SP:4b7d050f57507166992034e5e264cccab3cb874f,"MAGNA HYPONYM-OF attention - based GNN. multi - hop neighborhood USED-FOR receptive field. diffusion - based technique CONJUNCTION geometric distribution. geometric distribution CONJUNCTION diffusion - based technique. diffusion - based technique USED-FOR MAGNA. geometric distribution USED-FOR MAGNA. OtherScientificTerm are attention scores, and page rank. Generic is latter. ","This paper proposes MAGNA, an attention-based GNN. MAGNA is based on a diffusion-based technique and a geometric distribution, where the receptive field is modeled as a multi-hop neighborhood, and the attention scores are computed based on the page rank. The latter is used as a proxy for the number of hops in the neighborhood."
218,SP:4b7d050f57507166992034e5e264cccab3cb874f,Graph Neural Networks ( GNNs ) USED-FOR node representations. Self - Attention modules USED-FOR GNNs. means USED-FOR attention scores. OtherScientificTerm is propagation stages. Method is self - attention mechanisms. ,"Graph Neural Networks (GNNs) are trained to learn node representations. Self-Attention modules are commonly used in GNNs, but are computationally expensive. This paper proposes to reduce the computational cost of self-attention mechanisms by introducing means to compute attention scores for different propagation stages. "
219,SP:36310d761deb19e71c8a57de19b48f857707d48b,law CONJUNCTION medicine. medicine CONJUNCTION law. multiple - choice questions FEATURE-OF dataset. professional disciplines PART-OF dataset. medicine HYPONYM-OF professional disciplines. law HYPONYM-OF professional disciplines. LMs USED-FOR GPT-3 models. QA datasets USED-FOR seq2seq model. average accuracy EVALUATE-FOR models. benchmarks EVALUATE-FOR models. ,This paper presents a new dataset of multiple-choice questions for two professional disciplines: law and medicine. The authors train GPT-3 models with LMs and show that the seq2seq model can be trained on standard QA datasets. They also show that models trained on these benchmarks achieve better average accuracy.
220,SP:36310d761deb19e71c8a57de19b48f857707d48b,tasks EVALUATE-FOR large scale transformer models. GPT3 HYPONYM-OF large scale transformer models. models USED-FOR calculation - intensive tasks. Method is large - scale models. Material is massive multi - task dataset. ,"This paper studies the performance of large scale transformer models (e.g., GPT3) on a variety of tasks. The authors argue that large-scale models are prone to overfitting to new tasks, and that models trained for calculation-intensive tasks may not generalize well to unseen tasks. To address this issue, the authors propose a massive multi-task dataset."
221,SP:36310d761deb19e71c8a57de19b48f857707d48b,benchmark EVALUATE-FOR NLP models. zero - shot and few - shot settings FEATURE-OF closed - form question. human examination sets USED-FOR tasks. Generic is model. Method is GPT-3 and T5 based ) models. ,This paper presents a benchmark for evaluating NLP models on a closed-form question in both zero-shot and few-shot settings. The model is trained on a set of (GPT-3 and T5 based) models and evaluated on a variety of tasks based on human examination sets.
222,SP:7d7d34ba6e9fb36f2658cf4be44b137cdd73d34c,pretraining strategy USED-FOR table question answering. SCFG USED-FOR full supervision data. full supervision data USED-FOR Roberta model pretraining. SCFG USED-FOR Roberta model pretraining. SQL Semantic Precision ( SSP ) CONJUNCTION masked - language modeling ( MLM ). masked - language modeling ( MLM ) CONJUNCTION SQL Semantic Precision ( SSP ). natural ( training ) data USED-FOR masked - language modeling ( MLM ). Method is synchronous context - free grammar ( SCFG ). Material is synthetic data. ,This paper proposes a pretraining strategy for table question answering based on synchronous context-free grammar (SCFG). SCFG provides full supervision data for Roberta model pretraining. The authors combine SQL Semantic Precision (SSP) with masked-language modeling (MLM) on natural (training) data and on synthetic data.
223,SP:7d7d34ba6e9fb36f2658cf4be44b137cdd73d34c,"pre - training USED-FOR table semantic parsing. SPIDER CONJUNCTION WIKISQL. WIKISQL CONJUNCTION SPIDER. masked language modeling CONJUNCTION task. task CONJUNCTION masked language modeling. SSP HYPONYM-OF task. natural language sentence USED-FOR task. Spider CONJUNCTION WikiSQL. WikiSQL CONJUNCTION Spider. Spider EVALUATE-FOR model. WikiSQL EVALUATE-FOR model. OtherScientificTerm are pseudo question - SQL pairs, and SQL query. ","This paper proposes pre-training for table semantic parsing using pseudo question-SQL pairs. The idea is to combine masked language modeling with a new task (e.g., SSP) where the task is modeled as a natural language sentence and the query is a SQL query. Experiments on Spider, WIKISQL, SPIDER, and WikiSQL show that the proposed model can achieve state-of-the-art performance on Spider and Wiki."
224,SP:7d7d34ba6e9fb36f2658cf4be44b137cdd73d34c,"general - purpose pre - training approach USED-FOR table semantic parsing. table semantic parsing USED-FOR compositional semantics. linearized table headers CONJUNCTION synthetic utterances. synthetic utterances CONJUNCTION linearized table headers. masked language model ( RoBERTa ) USED-FOR understanding and grounding of compositional utterances. synchronous context free grammar USED-FOR synthetic utterances. synthetic utterances USED-FOR masked language model ( RoBERTa ). linearized table headers USED-FOR masked language model ( RoBERTa ). pre - training objective CONJUNCTION masked language modeling objective. masked language modeling objective CONJUNCTION pre - training objective. Material are natural language utterance, and compositional utterances. OtherScientificTerm are relational database tables, and tabular schema. Generic is model. ","This paper presents a general-purpose pre-training approach for table semantic parsing for compositional semantics. The authors propose a masked language model (RoBERTa) that uses linearized table headers and synthetic utterances with synchronous context free grammar to improve the understanding and grounding of compositional utterances. The main idea is to pre-train the model on relational database tables, where the input is a tabular schema and the target language is a natural language utterance. The paper also proposes a pre-training objective and masked language modeling objective."
225,SP:ebbb25902804b4f9f4985311c5debe2ef0ad7c7c,random matrix analysis USED-FOR Gaussian mixture data model. random matrix analysis USED-FOR multi - task learning methods. Gaussian mixture model USED-FOR MTL LS - SVM. MTL LS - SVM USED-FOR analysis. synthetic dataset CONJUNCTION image classification task. image classification task CONJUNCTION synthetic dataset. Generic is method. ,This paper proposes a random matrix analysis for multi-task learning methods. The analysis is based on MTL LS-SVM with a Gaussian mixture data model. The method is evaluated on a synthetic dataset and an image classification task.
226,SP:ebbb25902804b4f9f4985311c5debe2ef0ad7c7c,"Task is multi - task learning. Method are common and specific parameters modeling framework, least - squares SVM, and MTL LS - SVM. Generic is method. ",This paper addresses the problem of multi-task learning. The authors propose a common and specific parameters modeling framework. The main idea is to use least-squares SVM. The proposed method is called MTL LS-SVM.
227,SP:ebbb25902804b4f9f4985311c5debe2ef0ad7c7c,binary classification problem HYPONYM-OF SVM tasks. SVM tasks PART-OF problem. normal vector FEATURE-OF hyperplane. problem USED-FOR classification. classification USED-FOR task. optimization formulation USED-FOR problem. Task is multitask least - square SVM problem. ,"This paper studies the multitask least-square SVM problem. The problem consists of two SVM tasks: binary classification problem and multi-task least-squares problem. In the first problem, the hyperplane of the problem is a normal vector and the classification of the task is a linear combination of the two tasks. The second problem is an optimization formulation where each task is represented as a function of the other task."
228,SP:2be727b1333122fef3abfd2f7c576d2fc467893f,"symmetry PART-OF meta - learning. rotation HYPONYM-OF symmetries. symmetries FEATURE-OF function. EquivCNP HYPONYM-OF Neural Process ( NP ) model. permutation equivariance CONJUNCTION translation equivariance. translation equivariance CONJUNCTION permutation equivariance. translation equivariance USED-FOR NPs. permutation equivariance USED-FOR NPs. symmetry groups PART-OF this. Material is context data. Generic are model, and network. Method is theory - driven approach. ","This paper studies the role of symmetry in meta-learning. The authors propose a Neural Process (NP) model called EquivCNP, which is trained on context data. The model is based on a theory-driven approach where the symmetries of the function (e.g. rotation) are considered. NPs are trained with permutation equivariance and translation equivalivariance. The network is then trained on a subset of symmetry groups."
229,SP:2be727b1333122fef3abfd2f7c576d2fc467893f,convolution conditional neural processes CNPs USED-FOR Lie group equivariant CNPs. Generic is theory. ,"This paper proposes to extend convolution conditional neural processes CNPs to Lie group equivariant CNPs. The paper is well-written and well-motivated, and the theory is well motivated. However, there are some issues that need to be addressed."
230,SP:2be727b1333122fef3abfd2f7c576d2fc467893f,rotation CONJUNCTION scaling. scaling CONJUNCTION rotation. Conditional Neural Processes ( CNP ) USED-FOR EquivCNP. scaling HYPONYM-OF symmetries of the data. rotation HYPONYM-OF symmetries of the data. equivariance CONJUNCTION permutation invariance. permutation invariance CONJUNCTION equivariance. DeepSet USED-FOR equivariance. data space FEATURE-OF equivariance. DeepSet USED-FOR approach. synthetic and 2D image completion tasks FEATURE-OF 1D regression task. Material is digital clock digits dataset. ,"This paper proposes EquivCNP, which is an extension of Conditional Neural Processes (CNP) to handle symmetries of the data, such as rotation, scaling, etc. The approach is based on DeepSet, which allows for equivariance in the data space and permutation invariance. Experiments are conducted on a 1D regression task on synthetic and 2D image completion tasks, as well as on the digital clock digits dataset."
231,SP:a54b0358a0a2900f76a2da7a0a99348805c8d66a,"optimization method USED-FOR text generation. off - policy learning USED-FOR text generation. off - policy learning COMPARE on - policy learning. on - policy learning COMPARE off - policy learning. Method is MLE. Task is text generation learning. OtherScientificTerm are optimization objective, policy, and estimated rewards. ","This paper proposes a new optimization method for text generation. The main idea is to use off-policy learning to improve the performance of text generation compared to on-policy (MLE). The main difference is that the optimization objective does not depend on the current policy, but instead on the estimated rewards. The paper is well-written and well-motivated. The text generation learning is an important problem and the paper is clearly written."
232,SP:a54b0358a0a2900f76a2da7a0a99348805c8d66a,method USED-FOR generative models of text. off - policy demonstrations USED-FOR reinforcement learning. reinforcement learning USED-FOR method. off - policy demonstrations USED-FOR method. exposure bias CONJUNCTION mismatched objectives. mismatched objectives CONJUNCTION exposure bias. maximum likelihood estimation CONJUNCTION policy gradient optimization. policy gradient optimization CONJUNCTION maximum likelihood estimation. mismatched objectives FEATURE-OF learning schemes. metrics EVALUATE-FOR policy gradient optimization. BLEU HYPONYM-OF metrics. metrics EVALUATE-FOR learning schemes. policy gradient optimization HYPONYM-OF learning schemes. maximum likelihood estimation HYPONYM-OF learning schemes. policy gradient USED-FOR model. policy gradient CONJUNCTION importance weighting. importance weighting CONJUNCTION policy gradient. importance weighting USED-FOR model. off - policy demonstrations USED-FOR model. human - written text HYPONYM-OF off - policy demonstrations. summarization CONJUNCTION machine translation. machine translation CONJUNCTION summarization. machine translation EVALUATE-FOR MLE baselines. tasks EVALUATE-FOR MLE baselines. summarization EVALUATE-FOR MLE baselines. machine translation HYPONYM-OF tasks. summarization HYPONYM-OF tasks. Method is reward formulations. ,"This paper presents a method for learning generative models of text using reinforcement learning with off-policy demonstrations (e.g., human-written text). The authors evaluate three learning schemes (maximum likelihood estimation, policy gradient optimization, and importance weighting) on three metrics: BLEU, CIFAR-10, and PLEU to evaluate the impact of exposure bias, mismatched objectives, and different reward formulations. The authors compare their MLE baselines on two tasks: summarization and machine translation."
233,SP:a54b0358a0a2900f76a2da7a0a99348805c8d66a,off - policy RL objective USED-FOR conditional text generation models. MLE objective USED-FOR recall. MT CONJUNCTION summarization. summarization CONJUNCTION MT. summarization HYPONYM-OF tasks. MT HYPONYM-OF tasks. auto - reg model USED-FOR PG. importance sampling USED-FOR PG objective. importance ratio CONJUNCTION uniform sample probability. uniform sample probability CONJUNCTION importance ratio. global reward CONJUNCTION model probability. model probability CONJUNCTION global reward. gradient updates COMPARE MLE loss. MLE loss COMPARE gradient updates. loss COMPARE MLE loss. MLE loss COMPARE loss. loss USED-FOR gradient updates. model probability USED-FOR MLE loss. global reward USED-FOR MLE loss. recall EVALUATE-FOR precision. constant reward CONJUNCTION MLE based rewards. MLE based rewards CONJUNCTION constant reward. MLE based rewards HYPONYM-OF rewards. constant reward HYPONYM-OF rewards. Metric is sample diversity. ,"This paper proposes a new off-policy RL objective for conditional text generation models. The proposed MLE objective aims to improve recall on two tasks: MT and summarization. The PG objective is based on importance sampling, where the importance ratio and the uniform sample probability are computed using an auto-reg model. The authors show that the proposed loss can be used as gradient updates instead of the original MLE loss, which uses a global reward and a model probability. They also show that increasing the precision of the recall can improve the sample diversity. Finally, they compare the performance of different rewards, including constant reward and MLE based rewards."
234,SP:e77eca51db362909681965092186af2e502aaedc,"strategy USED-FOR feed - forward networks. end - to - end supervision USED-FOR strategy. local supervision USED-FOR supervisory signals. intermediate layers PART-OF network. classification HYPONYM-OF E2E objective. E2E objective HYPONYM-OF supervisory signals. classification HYPONYM-OF supervisory signals. intermediate supervision USED-FOR networks. activations PART-OF GPU memory. local training COMPARE global training. global training COMPARE local training. local supervision CONJUNCTION piecewise training. piecewise training CONJUNCTION local supervision. piecewise training CONJUNCTION global training. global training CONJUNCTION piecewise training. activations CONJUNCTION nuisance variable. nuisance variable CONJUNCTION activations. mutual information FEATURE-OF activations. activations PART-OF layer. mutual information FEATURE-OF nuisance variable. local supervision USED-FOR features. features USED-FOR intermediate layer. Method are deep networks, training strategy, direct supervision, and bounded approximation. OtherScientificTerm are loss function, nuisance, and classification prediction. ","This paper proposes a new training strategy for feed-forward networks that leverages end-to-end supervision. The main idea is to use local supervision to provide supervisory signals (e.g., classification) to the network, which are then used to train deep networks. The authors show that networks trained with intermediate supervision can achieve state-of-the-art performance when the E2E objective is the same as that of the network trained without intermediate supervision. In particular, they show that local supervision can be combined with piecewise training to achieve better performance than global training. The training strategy is based on the observation that the loss function of the intermediate layers of a network can be decomposed into two terms: (1) the mutual information between the activations of the layer and the nuisance variable, and (2) the number of activations in the GPU memory. In order to avoid direct supervision, the authors propose to use a bounded approximation to the nuisance, and then use the features from the intermediate layer as the classification prediction."
235,SP:e77eca51db362909681965092186af2e502aaedc,auxiliary loss USED-FOR locally supervised learning. information propagation USED-FOR locally supervised learning. tractable upper bound USED-FOR loss. infopro loss HYPONYM-OF loss. decoder CONJUNCTION classifier. classifier CONJUNCTION decoder. classifier USED-FOR mutual information. decoder USED-FOR mutual information. contrastive learning PART-OF framework. mutual information FEATURE-OF lower bound maximization process. lower bound maximization process USED-FOR contrastive learning. datasets EVALUATE-FOR method. ,"This paper proposes an auxiliary loss for locally supervised learning with information propagation. The proposed loss, called infopro loss, is based on a tractable upper bound. The framework combines contrastive learning with a lower bound maximization process that maximizes the mutual information between the decoder and the classifier. The method is evaluated on two datasets."
236,SP:e77eca51db362909681965092186af2e502aaedc,classification loss CONJUNCTION reconstruction loss. reconstruction loss CONJUNCTION classification loss. information propagation loss USED-FOR information collapse. reconstruction loss PART-OF information propagation loss. classification loss PART-OF information propagation loss. method USED-FOR memory footprint. Method is locally supervised training. ,This paper proposes a novel information propagation loss that combines the classification loss and the reconstruction loss to prevent information collapse. The proposed method can reduce the memory footprint while maintaining the performance of locally supervised training.
237,SP:21e44dddd20db1768de0dab869f8b0d3d5a598b7,"GNNs USED-FOR node representations. GNNs USED-FOR sampled graph. representations USED-FOR downstream tasks. representations USED-FOR node. neighborhood FEATURE-OF nodes. Method is multiple sampling. OtherScientificTerm are neighborhoods, sampled graphs, and single graph. ","This paper proposes to use GNNs to learn node representations for a sampled graph. The idea is to use multiple sampling, where nodes are sampled from a neighborhood, and then the representations for each node are used for downstream tasks. The authors show that the representations learned for the sampled graphs are similar to those learned for a single graph."
238,SP:21e44dddd20db1768de0dab869f8b0d3d5a598b7,method USED-FOR GNN. sampled graph USED-FOR shared GNN. GCN CONJUNCTION GAT. GAT CONJUNCTION GCN. method COMPARE GNN architectures. GNN architectures COMPARE method. GNN architectures USED-FOR node classification tasks. node classification tasks EVALUATE-FOR method. GAT HYPONYM-OF GNN architectures. GCN HYPONYM-OF GNN architectures. Method is diverse sampling method. ,This paper proposes a method for training GNN with a diverse sampling method. The idea is to train a shared GNN on a sampled graph. The proposed method is evaluated on node classification tasks and compared to other GNN architectures such as GCN and GAT.
239,SP:21e44dddd20db1768de0dab869f8b0d3d5a598b7,aggregation function USED-FOR GNNs. diverse sampling of the graph USED-FOR framework. GNN USED-FOR features. injective multi - set aggregation function USED-FOR representation. GNN USED-FOR graph. GCN CONJUNCTION GAT. GAT CONJUNCTION GCN. module PART-OF GAT. module USED-FOR node - based multi - class classification. module PART-OF GCN. Material is graphs. ,"This paper proposes a new aggregation function for GNNs. The proposed framework is based on diverse sampling of the graph. The authors propose an injective multi-set aggregation function that learns a representation for each node in the graph, and then uses the learned GNN to aggregate the features of each node. The paper also proposes a module for node-based multi-class classification, which is a combination of GCN and GAT. Experiments are conducted on two graphs."
240,SP:e9a8956f067a55b794508ac69f93b4b0290a664c,"video machine learning models USED-FOR bit - level corruption. adversarial training HYPONYM-OF methods. bit - level data augmentation USED-FOR corruption. bit - level data augmentation USED-FOR Bit - corruption Augmented Training ( BAT ). method COMPARE methods. methods COMPARE method. methods USED-FOR bit - level corrupted dataset. method USED-FOR bit - level corrupted dataset. Generic are they, and framework. ","This paper studies the problem of bit-level corruption in video machine learning models. In particular, the authors propose Bit-corruption Augmented Training (BAT), which is based on the idea of using bit-depth data augmentation to mitigate corruption. The authors compare their method with other methods such as adversarial training and show that they can achieve better performance. They also provide a theoretical analysis of the proposed framework. Finally, they show that the proposed method can be used to generate a more accurate and robust version of the bit-levels corrupted dataset compared to other methods."
241,SP:e9a8956f067a55b794508ac69f93b4b0290a664c,"network packet losses CONJUNCTION bit corruptions. bit corruptions CONJUNCTION network packet losses. action recognition CONJUNCTION multi - object tracking. multi - object tracking CONJUNCTION action recognition. bit - level corruption USED-FOR video models. bit corruptions HYPONYM-OF bit - level corruption. network packet losses HYPONYM-OF bit - level corruption. multi - object tracking HYPONYM-OF video models. action recognition HYPONYM-OF video models. defense method USED-FOR model. defense method USED-FOR robustness. robustness EVALUATE-FOR model. Bit - corruption Augmented Training ( BAT ) USED-FOR model. Bit - corruption Augmented Training ( BAT ) HYPONYM-OF defense method. BAT COMPARE methods. methods COMPARE BAT. model robustness EVALUATE-FOR methods. model robustness EVALUATE-FOR BAT. Adversarial Training ( AT ) HYPONYM-OF methods. OtherScientificTerm are corruption levels, and corrupted video samples. ","This paper studies the problem of bit-level corruption in video models, specifically network packet losses and bit corruptions. The authors propose a defense method called Bit-corruption Augmented Training (BAT) to improve the robustness of the model to corruption levels. BAT is shown to outperform other methods such as Adversarial Training (AT) in terms of model robustness to corrupted video samples."
242,SP:e9a8956f067a55b794508ac69f93b4b0290a664c,It USED-FOR problem. Method is video prediction models. OtherScientificTerm is signal corruption. Generic is solution. ,"This paper studies the problem of signal corruption in video prediction models and proposes a solution to this problem. It is well motivated and well motivated, and the problem is well-motivated. The paper is clearly written and well-structured. The problem is interesting and the solution is clearly presented. "
243,SP:35f77a7dcce3f6e09db0db9d22207a6da1fdbe5c,Skip Gram Negative Sampling ( SGNS ) USED-FOR time varying graphs. SGNS CONJUNCTION Matrix Factorization. Matrix Factorization CONJUNCTION SGNS. Matrix Factorization USED-FOR tensor setting. static embedding USED-FOR node. embedding USED-FOR time step. static embedding CONJUNCTION embedding. embedding CONJUNCTION static embedding. embeddings USED-FOR time - aware node embedding. method COMPARE benchmarks. benchmarks COMPARE method. Method is node embeddings of time varying graphs. ,This paper proposes Skip Gram Negative Sampling (SGNS) for time varying graphs. SGNS is based on Matrix Factorization in the tensor setting. The key idea is to use a static embedding for each node and an embedding that represents the time step. These embeddings are then used to learn a time-aware node embedding. Experiments show that the proposed method outperforms existing benchmarks.
244,SP:35f77a7dcce3f6e09db0db9d22207a6da1fdbe5c,skip - gram based graph embedding method USED-FOR time - varying graphs. method USED-FOR time - varying graphs. high - order tensors FEATURE-OF time - varying graphs. negative sampling USED-FOR method. negative sampling USED-FOR high - order tensors. time - resolved proximity networks COMPARE state - of - art baselines. state - of - art baselines COMPARE time - resolved proximity networks. Task is time - varying graph embedding problems. ,"This paper proposes a skip-gram based graph embedding method for time-varying graphs. The proposed method is motivated by the observation that high-order tensors of time-variating graphs can be represented by negative sampling. The authors provide a theoretical analysis of the time-resolved proximity networks and compare their performance with state-of-art baselines. The paper is well-written and well-motivated. However, there is still a lot of work to be done in the area of solving time-variable graph embeddings problems."
245,SP:35f77a7dcce3f6e09db0db9d22207a6da1fdbe5c,"tensor based treatment USED-FOR node. Negative sampling method COMPARE noise contrastive estimation. noise contrastive estimation COMPARE Negative sampling method. higher order tensor setting USED-FOR Negative sampling method. node classification CONJUNCTION link prediction. link prediction CONJUNCTION node classification. face - to - face proximity data USED-FOR node classification. event reconstruction USED-FOR link prediction. temporal interactions PART-OF face - to - face proximity data. method COMPARE discrete time graph representation learning model. discrete time graph representation learning model COMPARE method. method COMPARE tensor based method. tensor based method COMPARE method. discrete time graph representation learning model CONJUNCTION tensor based method. tensor based method CONJUNCTION discrete time graph representation learning model. embedding visualizations CONJUNCTION goodness of fit plots. goodness of fit plots CONJUNCTION embedding visualizations. Method are implicit tensor factorization approach, and skip gram based embedding approach. Material is dynamic networks. OtherScientificTerm are matrix to higher order tensors, temporal dimensions, and cross entropy objective. ",This paper proposes an implicit tensor factorization approach for dynamic networks. Negative sampling method in higher order tensor setting is proposed instead of noise contrastive estimation. The proposed tensor based treatment aims to learn a node that is invariant to temporal dimensions. This is achieved by a skip gram based embedding approach. Experiments on face-to-face proximity data with temporal interactions are conducted for node classification and link prediction with event reconstruction. Results show that the proposed method outperforms a discrete time graph representation learning model and a tensor-based method. Results on embedding visualizations and goodness of fit plots are also provided. The cross entropy objective is also discussed.
246,SP:bac0a2d3478dd277cb1ceafedd7fff64e107a222,self - supervised learning task USED-FOR machine learning models. self - supervised learning task USED-FOR conjecturing in higher order logic. mathematical formulas USED-FOR machine learning models. supervisory signal USED-FOR machine learning model. reasoning capabilities EVALUATE-FOR models. task USED-FOR supervisory signal. task USED-FOR training. granularities FEATURE-OF mathematical statements. self - supervised learning USED-FOR theorem proving. OtherScientificTerm is higher order logic. ,"This paper proposes a self-supervised learning task for conjecturing in higher order logic, which is an important problem for machine learning models that are trained on mathematical formulas. The main idea is to use this task as a supervisory signal for the machine learning model to evaluate the reasoning capabilities of the models during training. This is an interesting idea, as the granularities of mathematical statements are important for theorem proving, and the use of this task for training is interesting. "
247,SP:bac0a2d3478dd277cb1ceafedd7fff64e107a222,skip - tree training task USED-FOR mathematic theorem proving. self - supervised language models USED-FOR skip - tree training task. self - supervised language models USED-FOR mathematical reasoning capabilities. fine - tuning USED-FOR reasoning capabilities. skip - tree training task COMPARE skip - sequence. skip - sequence COMPARE skip - tree training task. mathematical reasoning abilities EVALUATE-FOR skip - tree training task. mathematical reasoning abilities COMPARE skip - sequence. skip - sequence COMPARE mathematical reasoning abilities. Generic is model. ,"This paper proposes a skip-tree training task for mathematic theorem proving using self-supervised language models to improve the mathematical reasoning capabilities of the existing skip-Tree training task. The main contribution of this paper is the introduction of fine-tuning to the reasoning capabilities, which is an important step towards improving the model’s performance. The authors show that the skip-trees training task can improve the mathematics reasoning abilities compared to the original skip-sequence."
248,SP:bac0a2d3478dd277cb1ceafedd7fff64e107a222,unlabeled mathematical expressions USED-FOR logical reasoning models. skip - tree method USED-FOR sub - tree. model USED-FOR masked subtree. reasoning tasks EVALUATE-FOR model. skip - sequence task EVALUATE-FOR models. task USED-FOR models. conjecturing ability EVALUATE-FOR model. Method is encoder - decoder architecture. OtherScientificTerm is S - expression. ,"This paper studies the problem of learning logical reasoning models from unlabeled mathematical expressions. The authors propose an encoder-decoder architecture, where a sub-tree is constructed using the skip-tree method. The model is trained to predict the masked subtree, and then the model is evaluated on reasoning tasks. The models are evaluated on a skip-sequence task, which is a well-known task for learning models. The conjecturing ability of the proposed model is tested on the S-expression."
249,SP:808f6d3af382876f5518e8e3a14ea73cc59c0a2b,"imbalanced gradients USED-FOR PGD - based adversarial attacks. scheme USED-FOR PGD attack. GIR CONJUNCTION attack. attack CONJUNCTION GIR. attack USED-FOR versions. metric CONJUNCTION attack. attack CONJUNCTION metric. MD CONJUNCTION MDMT. MDMT CONJUNCTION MD. GIR HYPONYM-OF metric. defenses EVALUATE-FOR attack. MDMT HYPONYM-OF versions. MD HYPONYM-OF versions. adversarial training USED-FOR defenses. OtherScientificTerm are margin losses, single - term loss, and margin loss. ","This paper studies the problem of PGD-based adversarial attacks with imbalanced gradients. The authors propose a scheme to improve the efficiency of the PGD attack by minimizing the margin losses. Specifically, the authors propose to use a single-term loss to minimize the difference between the original margin loss and the modified margin loss. The proposed metric, GIR, and the proposed attack are tested on three versions: MD, MDMT, and MDM. Experiments show the effectiveness of the proposed defenses with adversarial training."
250,SP:808f6d3af382876f5518e8e3a14ea73cc59c0a2b,"constructing adversarial examples USED-FOR classification. constructing adversarial examples USED-FOR robustness metrics. epsilon - perturbation USED-FOR attack. gradient imbalance USED-FOR perturbation targets. perturbation targets USED-FOR fool networks. Generic are model, and attacks. ","This paper studies the problem of constructing adversarial examples for classification to improve robustness metrics. The authors propose an attack based on epsilon-perturbation, where the perturbation targets are generated by the gradient imbalance between the input and the target model. They show that the proposed attacks can improve the robustness of fool networks by using the same number of perturbations targets."
251,SP:808f6d3af382876f5518e8e3a14ea73cc59c0a2b,"suboptimal local optimum USED-FOR attack optimization. OtherScientificTerm are imbalanced gradients, Imbalanced gradients, attack objective, and gradients. Task is optimization of gradient - based adversarial attacks. Metric are robustness, and attack effectiveness. Method is network. ","This paper studies the optimization of gradient-based adversarial attacks. The main contribution of this paper is to study the problem of imbalanced gradients, i.e., gradients that are highly correlated with the attack objective. The authors show that the optimal local optimum for attack optimization is a suboptimal local optimum, which leads to a significant drop in robustness. The paper also studies the effect of the network on the attack effectiveness. "
252,SP:2180e15ad0bbecc98e043b41f6525d2a8061d304,"model USED-FOR semantic equivalence. trees USED-FOR Expressions. axioms USED-FOR equivalence. model USED-FOR expression / program trees. nodes PART-OF graph. OtherScientificTerm are symbolic linear algebra expressions, path, and edges. ","This paper proposes a model for semantic equivalence between symbolic linear algebra expressions. Expressions are represented as trees, and equivalence is defined by axioms. The model is applied to express/program trees, where nodes in the graph are associated with a path, and edges correspond to programs."
253,SP:2180e15ad0bbecc98e043b41f6525d2a8061d304,"equational proofs FEATURE-OF synthetic dataset. GNN USED-FOR task. it COMPARE baselines. baselines COMPARE it. GNN USED-FOR pe - graph2axiom. Khan Academy modules USED-FOR system. Generic are dataset, and models. OtherScientificTerm are rewrite instructions, and rewrite instruction. ","This paper presents a synthetic dataset with equational proofs. The dataset consists of a sequence of sentences that are given as rewrite instructions, and the goal of the paper is to learn a GNN that can solve this task. The system is trained using Khan Academy modules, and it is compared to several baselines. The results show that the models are able to generalize to unseen sentences, and that the GNN can be used to solve the pe-graph2axiom."
254,SP:2180e15ad0bbecc98e043b41f6525d2a8061d304,vectors CONJUNCTION matrices. matrices CONJUNCTION vectors. scalars CONJUNCTION vectors. vectors CONJUNCTION scalars. algebraic expressions FEATURE-OF synthetic dataset. symbols FEATURE-OF algebraic expressions. symbols FEATURE-OF synthetic dataset. axioms HYPONYM-OF rewrite rules. matrices HYPONYM-OF symbols. scalars HYPONYM-OF symbols. vectors HYPONYM-OF symbols. checker USED-FOR prediction. ,"This paper presents a synthetic dataset of algebraic expressions with symbols such as scalars, vectors, matrices, etc. The authors propose rewrite rules such as axioms and show that the prediction can be done with a simple checker."
255,SP:19e32803278a7ad2be5343187468cd2e26335bc8,transformers USED-FOR multi - modal video understanding. BERT HYPONYM-OF transformer models. memory requirements FEATURE-OF multi - modal transformer. Generic is approach. ,"This paper studies the problem of multi-modal video understanding in transformer models (e.g., BERT). The authors propose a new approach to tackle this problem. The main contribution of this paper is to study the memory requirements of a multi-mode transformer and propose a way to reduce their memory requirements. The paper is well-written and easy to follow. "
256,SP:19e32803278a7ad2be5343187468cd2e26335bc8,method USED-FOR audiovisual ( AV ) representations. videos USED-FOR audiovisual ( AV ) representations. Transformer - based model architecture USED-FOR method. video processing CONJUNCTION Transformer - based model. Transformer - based model CONJUNCTION video processing. parameter - reducing scheme USED-FOR model. network USED-FOR self - supervised pertaining tasks. network USED-FOR AV representations. Task is audio / visual downstream tasks. ,"This paper presents a method for learning audiovisual (AV) representations from videos using a Transformer-based model architecture. The model is trained using a parameter-reducing scheme, and the method is applied to both video processing and Transformer -based model. The network is trained for self-supervised pertaining tasks, and then used to learn AV representations for audio/visual downstream tasks."
257,SP:19e32803278a7ad2be5343187468cd2e26335bc8,"ConvNets CONJUNCTION transformers. transformers CONJUNCTION ConvNets. transformers USED-FOR audio - visual representation learning. model USED-FOR audio - visual representation learning. ConvNets USED-FOR audio - visual representation learning. transformers USED-FOR model. ConvNets USED-FOR model. binary classification loss USED-FOR pre - training. ESC-50 CONJUNCTION Kinectics - Sounds. Kinectics - Sounds CONJUNCTION ESC-50. UCF101 CONJUNCTION ESC-50. ESC-50 CONJUNCTION UCF101. Kinetics-700 CONJUNCTION AudioSet. AudioSet CONJUNCTION Kinetics-700. AudioSet USED-FOR models. Kinetics-700 USED-FOR models. UCF101 EVALUATE-FOR models. OtherScientificTerm are audio - visual, and ConvNets'embeddings. Task is negative sampling. ","This paper proposes a model that uses ConvNets and transformers for audio-visual representation learning. The pre-training is based on a binary classification loss, where the binary classification is used to distinguish between audio and visual, and the negative sampling is performed on the convNets' embeddings. The proposed models are evaluated on UCF101, ESC-50, and Kinectics-Sounds using Kinetics-700 and AudioSet."
258,SP:a5c22c090413ef4448db8e7f5b39332b3db6c73f,few - shot learning(FSL ) CONJUNCTION continual learning ( CL ). continual learning ( CL ) CONJUNCTION few - shot learning(FSL ). learning paradigm USED-FOR learning environment. train - test - retrain approach USED-FOR FSL. learning environment COMPARE train - test - retrain approach. train - test - retrain approach COMPARE learning environment. few - shot learning(FSL ) PART-OF learning paradigm. continual learning ( CL ) PART-OF learning paradigm. environments CONJUNCTION dataset. dataset CONJUNCTION environments. baselines PART-OF evaluation. approach COMPARE baselines. baselines COMPARE approach. contextual memory USED-FOR ProtoNets. ProtoNets USED-FOR approach. tasks EVALUATE-FOR approach. tasks EVALUATE-FOR baselines. contextual memory USED-FOR approach. Generic is approaches. ,This paper proposes a learning paradigm that combines few-shot learning(FSL) and continual learning (CL). The learning environment is different from the standard train-test-retrain approach for FSL and CL. The evaluation consists of three environments and one dataset. The proposed approach is evaluated on three tasks and compared to two baselines using ProtoNets with contextual memory. The results show that the proposed approaches outperform the baselines.
259,SP:a5c22c090413ef4448db8e7f5b39332b3db6c73f,"online contextualized few shot learning USED-FOR human learning. setting USED-FOR human learning. online contextualized few shot learning USED-FOR setting. continual learning CONJUNCTION few shot learning. few shot learning CONJUNCTION continual learning. context switch PART-OF setting. few shot learning PART-OF setting. continual learning PART-OF setting. model USED-FOR known and new categories. datasets USED-FOR learning setting. hand - written characters HYPONYM-OF datasets. Prototypical Network USED-FOR setting. Method is learning method. Generic are method, and baselines. OtherScientificTerm is known categories. ",This paper proposes a new setting for human learning based on online contextualized few shot learning. The proposed setting combines continual learning with a context switch. The learning method is based on the Prototypical Network. The authors propose a new learning setting based on two datasets: (1) hand-written characters and (2) text with known categories. The model is trained to distinguish between known and new categories. Experiments show that the proposed method outperforms the baselines.
260,SP:a5c22c090413ef4448db8e7f5b39332b3db6c73f,"few - shot learning CONJUNCTION continual learning. continual learning CONJUNCTION few - shot learning. continual learning USED-FOR online setting. few - shot learning COMPARE model. model COMPARE few - shot learning. continual learning COMPARE model. model COMPARE continual learning. Task are learning setting, and model evaluation. Generic is paradigm. Method is Online Contextualize Few - Shot Learning. ",This paper proposes a new learning setting where the model evaluation is done online. The paper proposes to combine few-shot learning and continual learning in the online setting. The paradigm is called Online Contextualize Few-Shot Learning. The authors show that the proposed model can outperform the state-of-the-art model in a variety of settings.
261,SP:9c87f7778b8ee5d3e65fb1204b8067f12aac94e1,"sliding - window strategy USED-FOR GNNs. temporal graphs USED-FOR GNNs. historical data USED-FOR sliding - window strategy. Material is graph data. OtherScientificTerm are graph structure, and distribution shift. ","This paper proposes a sliding-window strategy for training GNNs on temporal graphs based on historical data. The idea is that the graph data should not change too much in terms of graph structure, but rather should be subject to distribution shift. "
262,SP:9c87f7778b8ee5d3e65fb1204b8067f12aac94e1,"paradigm USED-FOR GNNs. layerwise training procedure USED-FOR method. OtherScientificTerm are loss function, and paradigms. Task are training, and updating of paradigms. Method is lazy - update. Metric is training time. ","This paper proposes a new paradigm for training GNNs. The proposed method is based on a layerwise training procedure, where the loss function is updated based on the number of epochs in the training. The motivation is to reduce the training time and reduce the updating of paradigms. The authors also propose a lazy-update, which is a way to speed up the training of the model."
263,SP:9c87f7778b8ee5d3e65fb1204b8067f12aac94e1,"dynamic or time - evolving networks USED-FOR relational learning and classification. class labels FEATURE-OF node. class labels FEATURE-OF nodes. Task are online or incremental learning, time - series forecasting, and dynamic node classification problem. Metric is predictive accuracy. OtherScientificTerm are distribution shift, graph snapshots, and node features. ","This paper studies the problem of relational learning and classification with dynamic or time-evolving networks. The authors consider online or incremental learning, where the goal is to improve predictive accuracy in the face of distribution shift. In particular, the authors focus on time-series forecasting, where they focus on the dynamic node classification problem, where nodes with different class labels are sampled from the same graph snapshots, and the node features are updated over time. "
264,SP:dce0bbc266a9ac746f0db5099836fa57a3055f4a,method USED-FOR representations of images. generalization USED-FOR RL. method USED-FOR generalization. method COMPARE Rainbow. Rainbow COMPARE method. Rainbow HYPONYM-OF RL algorithm. Procgen EVALUATE-FOR Rainbow. Procgen EVALUATE-FOR method. OtherScientificTerm is embeddings of states. ,"This paper proposes a method for learning representations of images that can be used to improve generalization in RL. The method is based on the idea that the embeddings of states should be similar to each other. The proposed method is evaluated on Procgen and compared to Rainbow, an RL algorithm."
265,SP:dce0bbc266a9ac746f0db5099836fa57a3055f4a,"technique USED-FOR regularizing the representation feature space. deep RL EVALUATE-FOR approach. OpenAI ProcGen benchmark EVALUATE-FOR deep RL. OpenAI ProcGen benchmark EVALUATE-FOR approach. Material is visualized input. Method is cross - state self - constraint(CSSC ). OtherScientificTerm are representation feature space, and representation similarity. ","This paper proposes a technique for regularizing the representation feature space. The key idea is to use visualized input as a proxy for representation similarity, which is then used to enforce cross-state self-constraint(CSSC). The proposed approach is evaluated on the OpenAI ProcGen benchmark for deep RL."
266,SP:dce0bbc266a9ac746f0db5099836fa57a3055f4a,"cross - state similarity USED-FOR loss. loss USED-FOR Rainbow loss. CSSC loss USED-FOR RL algorithms. OtherScientificTerm are similarity, visual input, 1 - dim space, and logarithms of sigmoids of similarities. ","This paper proposes a new loss based on cross-state similarity. The similarity is defined as the difference between the visual input and the output of the RL algorithm. The authors show that the Rainbow loss can be viewed as a special case of the proposed loss. The main contribution of this paper is to propose a new RL algorithms based on the CSSC loss. In particular, the authors propose to use logarithms of sigmoids of similarities in the 1-dim space."
267,SP:3dda3d53fdc4bd8045db22cac740322e31e67bcf,"GNN models USED-FOR designing adversarial attacks. influence maximization problem USED-FOR attack design. threshold model USED-FOR influence maximization problem. attack method COMPARE ones. ones COMPARE attack method. OtherScientificTerm are feature, misclassified instances, activations, and threshold. ","This paper studies the problem of designing adversarial attacks with GNN models. The authors consider the influence maximization problem in the attack design as a threshold model, where the feature is defined as the sum of the misclassified instances and the activations. They show that if the threshold is large enough, then the attack method can achieve better results than existing ones."
268,SP:3dda3d53fdc4bd8045db22cac740322e31e67bcf,model parameters CONJUNCTION model predictions. model predictions CONJUNCTION model parameters. restricted black - box setup FEATURE-OF GNNs. approximation techniques USED-FOR reformulated attack problem. restricted attack problem CONJUNCTION influence maximization problem. influence maximization problem CONJUNCTION restricted attack problem. GNN models EVALUATE-FOR attack. OtherScientificTerm is features. ,"This paper studies the attack of GNNs in a restricted black-box setup where the model parameters and model predictions are not available. The authors propose a reformulated attack problem based on approximation techniques, where the features of the model are only available to the attacker. The restricted attack problem is combined with the influence maximization problem. The proposed attack is evaluated on several GNN models."
269,SP:3dda3d53fdc4bd8045db22cac740322e31e67bcf,adversarial attack CONJUNCTION influence maximization problem. influence maximization problem CONJUNCTION adversarial attack. graph neural networks USED-FOR adversarial attack. restricted black - box setup FEATURE-OF adversarial attack. restricted black - box setup FEATURE-OF graph neural networks. linear threshold model USED-FOR influence maximization problem. node feature perturbation USED-FOR adversarial attack. greedy approximation algorithms USED-FOR black - box attack strategies. objective function FEATURE-OF IM problem. greedy approximation algorithms USED-FOR problem. attacks COMPARE baselines. baselines COMPARE attacks. attacks USED-FOR GNNs. mis - classification rate EVALUATE-FOR GNNs. OtherScientificTerm is graph. ,"This paper studies the adversarial attack on graph neural networks in a restricted black-box setup. The authors propose a linear threshold model for the influence maximization problem, which is motivated by the observation that the graph can be corrupted by node feature perturbation. To solve the IM problem, the authors propose greedy approximation algorithms, which can be used as black-Box attack strategies. The proposed attacks are compared with several baselines, and the results show that the proposed attacks can improve the performance of GNNs in terms of mis-classification rate."
270,SP:fca0583b19bd08f59fdb0e46f86a4b27495dd0df,"low - rankness FEATURE-OF adjacency matrix. adjacency matrix FEATURE-OF DAG. low - rankness FEATURE-OF Bayesian network structure learning. adjacency matrix USED-FOR Bayesian network structure learning. DAG USED-FOR Bayesian network structure learning. low rank components PART-OF adjacency matrix. Generic are framework, and approach. OtherScientificTerm is rank of DAGs. ","This paper studies the problem of Bayesian network structure learning with low-rankness in the adjacency matrix of a DAG. The authors propose a framework to deal with this problem. The approach is based on the observation that the rank of DAGs is highly correlated with the number of neurons in the network. To address this issue, the authors propose to replace the low rank components of the original DAG with the proposed adjacence matrix. "
271,SP:fca0583b19bd08f59fdb0e46f86a4b27495dd0df,weighted matrices USED-FOR DAG. minimum and maximum rank FEATURE-OF weighted matrices. minimum and maximum rank FEATURE-OF rank of DAGs. head - tail vertex cover HYPONYM-OF graphical properties. skeleton CONJUNCTION moral graph. moral graph CONJUNCTION skeleton. rank FEATURE-OF DAG. norm constraints CONJUNCTION matrix factorization. matrix factorization CONJUNCTION norm constraints. matrix factorization USED-FOR SEM learning methods. low - rank assumption PART-OF learning process. Material is synthetic and real world data. OtherScientificTerm is SEM. ,"This paper studies the rank of DAGs under minimum and maximum rank of weighted matrices for a DAG. The authors consider graphical properties such as head-tail vertex cover and show that for both synthetic and real world data, the skeleton and the moral graph of the DAG have the same rank. In addition, the authors show that SEM learning methods with norm constraints and matrix factorization can benefit from the low-rank assumption in the learning process."
272,SP:fca0583b19bd08f59fdb0e46f86a4b27495dd0df,high dimensional settings FEATURE-OF low - rank DAG models. low - rank USED-FOR causal structure. approach COMPARE DAG learning algorithms. DAG learning algorithms COMPARE approach. sparse graph USED-FOR DAG learning algorithms. OtherScientificTerm is low - rank assumption. ,This paper studies the problem of training low-rank DAG models in high dimensional settings. The main contribution of this paper is the introduction of the low-ranks assumption. The authors argue that the low -rank is necessary to capture the causal structure of the data. The experimental results show that the proposed approach outperforms existing DAG learning algorithms on sparse graph.
273,SP:f2c8172adcb82ed1c0e047ffed65412f3f1c1ac7,"complex autoML pipeline USED-FOR neural networks. architecture search PART-OF full backpropagation flow. architecture search PART-OF differentiable architecture search algorithm. alternating optimization USED-FOR hyperparameters. alternating optimization USED-FOR network parameters. network parameters CONJUNCTION hyperparameters. hyperparameters CONJUNCTION network parameters. differentiable procedure CONJUNCTION alternating optimization. alternating optimization CONJUNCTION differentiable procedure. Task is autoML pipeline. Method are backpropagatable discrete sampling methods ( Gumbel softmax ), and data augmentation. ","This paper proposes a complex autoML pipeline for training neural networks. The main idea of the paper is to replace the architecture search in the full backpropagation flow with a differentiable architecture search algorithm. The authors propose to use backpropagateatable discrete sampling methods (Gumbel softmax) and show that this can improve the efficiency of the autoML framework. The paper also proposes a new differentiable procedure and alternating optimization for the network parameters and hyperparameters. Finally, the authors show that data augmentation can be used to improve the performance."
274,SP:f2c8172adcb82ed1c0e047ffed65412f3f1c1ac7,"regularization CONJUNCTION optimization hyperparameters. optimization hyperparameters CONJUNCTION regularization. learning rate HYPONYM-OF optimization hyperparameters. optimization hyperparameters HYPONYM-OF hyperparameters. regularization HYPONYM-OF hyperparameters. Bayesian Optimization HYPONYM-OF hyperparameter optimization techniques. Method are neural net based image model, and neural network. OtherScientificTerm are data augmentation, architecture, architecture parameters, and local minima. Generic is others. ","This paper proposes a neural net based image model, where the neural network is trained with data augmentation, and the architecture is trained using hyperparameter optimization techniques such as Bayesian Optimization. The hyperparameters considered in this paper are regularization and optimization hyperparamets such as the learning rate. The authors show that when the architecture parameters are fixed, the local minima of the learned neural network are small, while when the others are not, they are large. "
275,SP:f2c8172adcb82ed1c0e047ffed65412f3f1c1ac7,Neural Architecture Search CONJUNCTION Hyper Parameter Optimization. Hyper Parameter Optimization CONJUNCTION Neural Architecture Search. data augmentation CONJUNCTION Neural Architecture Search. Neural Architecture Search CONJUNCTION data augmentation. components PART-OF modeling. Hyper Parameter Optimization HYPONYM-OF components. Neural Architecture Search PART-OF modeling. data augmentation HYPONYM-OF components. Neural Architecture Search HYPONYM-OF components. data augmentation USED-FOR data argumentation transformation. data augmentation USED-FOR approach. training loss EVALUATE-FOR model. DAG USED-FOR neural architecture search. model parameter CONJUNCTION hyper parameter. hyper parameter CONJUNCTION model parameter. ImageNet EVALUATE-FOR approaches. data augmentation CONJUNCTION neural architecture search. neural architecture search CONJUNCTION data augmentation. neural architecture search CONJUNCTION hyper - parameter optimization. hyper - parameter optimization CONJUNCTION neural architecture search. hyper - parameter optimization HYPONYM-OF components. data augmentation HYPONYM-OF components. neural architecture search HYPONYM-OF components. OtherScientificTerm is hard examples. Generic is framework. ,"This paper proposes to combine three components in modeling: data augmentation for data argumentation transformation, Neural Architecture Search and Hyper Parameter Optimization. The proposed approach is based on the observation that the training loss of a model can be reduced when the model is trained on hard examples. The authors propose to use a DAG to perform neural architecture search, where the model parameter and the hyper parameter are jointly optimized. They evaluate the proposed approaches on ImageNet and show that the proposed framework can achieve state-of-the-art results."
276,SP:1ab30867e0bd8b6b65fad602cd80bada70b3f1ec,"Prior networks models USED-FOR regression problems. Prior networks models USED-FOR classification. neural networks USED-FOR modelling uncertainty in classification tasks. Prior networks HYPONYM-OF neural networks. model USED-FOR ensemble. ensemble USED-FOR modelling uncertainty in classification tasks. Prior networks models USED-FOR Dirichlet probability distribution. hierarchical approach USED-FOR uncertainty. framework USED-FOR regression tasks. approach USED-FOR regression tasks. framework USED-FOR approach. Normal distribution USED-FOR probability distribution. it USED-FOR Normal - Wishart distribution. it USED-FOR probability distribution. Normal - Wishart distribution USED-FOR probability distribution. Normal distributions USED-FOR probability distribution. OtherScientificTerm are categorical probability distributions, and Dirichlet distribution. ","Prior networks models for classification and regression problems are neural networks trained for classification. Prior networks models trained on categorical probability distributions can be used for modelling uncertainty in classification tasks as an ensemble. This paper proposes a hierarchical approach to model uncertainty by modelling the uncertainty as a Dirichlet probability distribution over a set of prior networks models. The proposed approach is applied to regression tasks using a framework based on the framework proposed in [1]. In particular, it uses a Normal distribution to represent the probability distribution and a Normal-Wishart distribution as a probability distribution based on Normal distributions. The authors also propose to use a variant of this framework for regression tasks where the uncertainty is modeled as a function of the number of samples in the ensemble. "
277,SP:1ab30867e0bd8b6b65fad602cd80bada70b3f1ec,Dirichlet prior USED-FOR categorical predictive distributions. Dirichlet prior USED-FOR Prior Networks. Normal - Wishart prior USED-FOR predictive diversity. Prior Networks USED-FOR regression setting. Normal - Wishart prior USED-FOR Prior Networks. UCI datasets CONJUNCTION monocular depth estimation. monocular depth estimation CONJUNCTION UCI datasets. synthetic data CONJUNCTION UCI datasets. UCI datasets CONJUNCTION synthetic data. model CONJUNCTION loss terms. loss terms CONJUNCTION model. analytical derivation PART-OF loss terms. monocular depth estimation EVALUATE-FOR approach. UCI datasets EVALUATE-FOR approach. synthetic data EVALUATE-FOR approach. Task is classification tasks. ,"This paper proposes to use Prior Networks with a Normal-Wishart prior to model categorical predictive distributions. Prior Networks are trained in a regression setting, where the goal is to maximize predictive diversity for classification tasks. The proposed approach is evaluated on synthetic data, UCI datasets, and monocular depth estimation. The model and loss terms include analytical derivation."
278,SP:1ab30867e0bd8b6b65fad602cd80bada70b3f1ec,interpretable uncertainty quantification USED-FOR data driven models. methods USED-FOR regression tasks. approaches USED-FOR classification. Prior Networks HYPONYM-OF methods. ,"This paper studies the problem of interpretable uncertainty quantification for data driven models. The authors propose two methods, Prior Networks and Prior Neural Networks, for regression tasks. The main contribution of the paper is that the proposed approaches can be applied to classification. "
279,SP:fe2aa4706defcac74e529d0cc3e1622d77451eca,"semi - online Bayesian approach USED-FOR meta - learning. approximations USED-FOR computation. within - task parameter USED-FOR learning. within - task parameter USED-FOR learning. Bayesian language of posterior distributions USED-FOR idea. Laplace approximation CONJUNCTION Hessian approximation. Hessian approximation CONJUNCTION Laplace approximation. Hessian approximation CONJUNCTION variational approximation. variational approximation CONJUNCTION Hessian approximation. variational approximation HYPONYM-OF approximation schemes. Laplace approximation HYPONYM-OF approximation schemes. Hessian approximation HYPONYM-OF approximation schemes. images EVALUATE-FOR application. OtherScientificTerm are batch mode, learner, catastrophic forgetting of previous tasks, and distributional shift. Method are sequential between - task Bayesian update, and MAML framework. Generic are task, and baselines. ","This paper proposes a semi-online Bayesian approach for meta-learning. The idea is based on the Bayesian language of posterior distributions, where the learner is given a set of tasks to solve, and the goal is to perform a sequential between-task Bayesian update. The learning is done by updating a within-task parameter, which is used to guide the learning in batch mode. The authors propose several approximations to speed up computation, including a Laplace approximation, a Hessian approximation, and a variational approximation. The proposed MAML framework is well-motivated. The application is evaluated on images, and compared to several baselines. The results show that the proposed learner can achieve state-of-the-art performance without catastrophic forgetting of previous tasks or distributional shift."
280,SP:fe2aa4706defcac74e529d0cc3e1622d77451eca,"Bayesian approach USED-FOR online meta - learning. model parameters CONJUNCTION meta - parameters. meta - parameters CONJUNCTION model parameters. online learning CONJUNCTION online meta - learning. online meta - learning CONJUNCTION online learning. Laplace Approximation ( LA ) HYPONYM-OF approaches. VI HYPONYM-OF approaches. sequential tasks CONJUNCTION sequential datasets. sequential datasets CONJUNCTION sequential tasks. online settings FEATURE-OF meta - learning benchmarks. Method are ( approximate ) sequential Bayesian inference, and Ritter et al ’s method. OtherScientificTerm is catastrophic forgetting. ","This paper presents a Bayesian approach for online meta-learning. The authors consider the problem of (approximate) sequential Bayesian inference, where the model parameters and the meta-parameters are learned sequentially. In contrast to previous approaches such as VI and Laplace Approximation (LA), the authors consider both online learning (i.e., Ritter et al’s method) and online meta - learning (e.g., Li et al.'s method). The authors conduct extensive experiments on a variety of meta learning benchmarks in both offline settings and on sequential tasks and sequential datasets, and demonstrate that the proposed method is able to avoid catastrophic forgetting."
281,SP:fe2aa4706defcac74e529d0cc3e1622d77451eca,Bayesian approach USED-FOR meta - learning. sequential data USED-FOR Bayesian approach. Laplace approximation USED-FOR model posterior. Laplace approximation USED-FOR first. K - FAC approximation of the Hessian USED-FOR first. variational approximation USED-FOR posterior. variational prior USED-FOR meta - learning. variational approximation USED-FOR approaches. sequential Omniglot CONJUNCTION pentathlon task. pentathlon task CONJUNCTION sequential Omniglot. pentathlon task EVALUATE-FOR method. sequential Omniglot EVALUATE-FOR method. Generic is algorithms. ,"This paper proposes a Bayesian approach to meta-learning on sequential data. The first is a K-FAC approximation of the Hessian, and the second is a Laplace approximation to the model posterior. Both approaches use variational approximation to approximate the posterior, which is a variational prior that has been shown to be useful in the context of meta-Learning. Experiments on sequential Omniglot and the pentathlon task demonstrate the effectiveness of the proposed method. However, there are some issues with the proposed algorithms that need to be addressed."
282,SP:89d2765946e70455105a608d998c3b900969cb8d,"complexity EVALUATE-FOR GNNs. information theoretic lower bound FEATURE-OF complexity. information theoretic lower bound EVALUATE-FOR GNNs. induced substructures PART-OF GNNs. Generic are procedure, and model. OtherScientificTerm are node, and representational power. Task is discriminative graph embedding. Method is RNP - GNN. ","This paper studies the complexity of GNNs under the information theoretic lower bound. The authors propose a procedure, called RNP-GNN, to reduce the number of induced substructures in GNN. The idea is to train a model that learns to distinguish between a node and a group of nodes, and then to use the discriminative graph embedding to infer the representational power of each node. "
283,SP:89d2765946e70455105a608d998c3b900969cb8d,"expressive power COMPARE models. models COMPARE expressive power. recursive neighborhood pooling strategy USED-FOR graphs. counts of subgraphs USED-FOR recursive neighborhood pooling strategy. tuple of recursion parameters USED-FOR model. Method are GNN's, neighborhood pooling strategy, and expressive representations. Metric is exponential computational complexity. OtherScientificTerm are subgraphs, and reconstruction conjecture. ","This paper studies the expressive power of GNN's and shows that expressive power is not necessarily better than other models. The authors propose a recursive neighborhood pooling strategy for graphs based on the counts of subgraphs, which is an extension of the neighborhood pooled strategy in [1]. The authors show that the exponential computational complexity of the model is bounded by a tuple of recursion parameters, and that the expressive representations of the learned model can be expressed as a function of the number of nodes in the graph. The reconstruction conjecture is also proved."
284,SP:89d2765946e70455105a608d998c3b900969cb8d,graph neural network USED-FOR subgraph. universal approximation USED-FOR subgraph. algorithm COMPARE models. models COMPARE algorithm. computational complexity EVALUATE-FOR algorithm. models USED-FOR substructures. Method is recursive neighborhood pooling graph neural network. Generic is model. ,This paper proposes a recursive neighborhood pooling graph neural network. The main idea is to use a graph network to learn a subgraph from a universal approximation. The proposed algorithm has a computational complexity of $O(\sqrt{T})$ and is shown to outperform existing models for learning substructures. The authors also provide a theoretical analysis of the proposed model.
285,SP:c43f5deb340555d78599a3496318514a826b1aae,"learning algorithms USED-FOR matrix games. Lyapunov chaos USED-FOR learning algorithms. linear programming approach USED-FOR identifying chaotic games. OtherScientificTerm are chaos, matrix domination, and chaotic games. ","This paper studies the problem of learning algorithms for matrix games with Lyapunov chaos. The main contribution of this paper is a linear programming approach for identifying chaotic games. The key insight is that when the chaos is large enough, matrix domination can be achieved. The paper also provides a theoretical analysis of the properties of chaotic games and provides some empirical results."
286,SP:c43f5deb340555d78599a3496318514a826b1aae,chaos phenomena FEATURE-OF learning. normal - form games FEATURE-OF learning. canonical decomposition USED-FOR coordination game. techniques USED-FOR game dynamics. matrix domination CONJUNCTION linear program. linear program CONJUNCTION matrix domination. linear program USED-FOR game dynamics. matrix domination HYPONYM-OF techniques. linear program HYPONYM-OF techniques. OtherScientificTerm is zero - sum and coordination games. ,"This paper studies the chaotic phenomena of learning in normal-form games. The authors consider zero-sum and coordination games, and derive a canonical decomposition of the coordination game. They then propose two techniques to study the game dynamics: matrix domination and linear program."
287,SP:c43f5deb340555d78599a3496318514a826b1aae,"optimistic MWU CONJUNCTION FTRL. FTRL CONJUNCTION optimistic MWU. general - sum n - player games FEATURE-OF payoff dynamics. L2 regularizer USED-FOR FTRL. FTRL HYPONYM-OF algorithms. optimistic MWU HYPONYM-OF algorithms. dual space FEATURE-OF Lyapunov chaos. zero - sum part CONJUNCTION coordination part. coordination part CONJUNCTION zero - sum part. chaos FEATURE-OF bi - matrix games. Lebesgue measure FEATURE-OF bi - matrix games. OtherScientificTerm are Lyapunov chaotic, game, trivial matrices, and zero - sum and coordination parts. Method is entropy regularizer. Generic is function. ","This paper studies the payoff dynamics in general-sum n-player games with two algorithms: optimistic MWU and FTRL with an L2 regularizer. In particular, the authors consider the setting where the game is Lyapunov chaotic (i.e., in the dual space). The authors show that the entropy regularizer is a function of the number of players, and that the game can be viewed as a series of trivial matrices, where the zero-sum and coordination parts are the same. The authors also show that in bi-matrix games with chaos, the Lebesgue measure is bounded."
288,SP:0cf756ba6b172f9b29e84945c093dfd89ae62803,proximal function USED-FOR marginal regret bound. proximal function USED-FOR adaptive algorithms. proximal function USED-FOR adaptive optimization algorithms. regret bound COMPARE algorithms. algorithms COMPARE regret bound. algorithms COMPARE algorithms. algorithms COMPARE algorithms. ,This paper studies the marginal regret bound of adaptive algorithms with a proximal function. The authors show that the regret bound is at least as good as the existing algorithms. 
289,SP:0cf756ba6b172f9b29e84945c093dfd89ae62803,it USED-FOR algorithms. it USED-FOR regret bound. regret bound FEATURE-OF algorithms. method COMPARE algorithms. algorithms COMPARE method. tasks EVALUATE-FOR algorithms. tasks EVALUATE-FOR method. Method is adaptive algorithms. ,"This paper studies adaptive algorithms and shows that it can improve the regret bound of existing algorithms. The proposed method is evaluated on a variety of tasks and compared to existing algorithms, and the results show that the proposed method outperforms existing algorithms in most cases."
290,SP:0cf756ba6b172f9b29e84945c093dfd89ae62803,"reduction USED-FOR online linear optimization. reduction USED-FOR online convex optimization algorithm. non - decreasing quadratic regularizers USED-FOR online mirror descent. update USED-FOR online mirror descent. online mirror descent USED-FOR algorithm. update USED-FOR algorithm. regret bound USED-FOR algorithm. strategy USED-FOR adagrad - esque regret bound. per - coordinate basis USED-FOR adagrad - esque regret bound. per - coordinate basis USED-FOR strategy. Metric are regret, mirror descent regret, and worst - case $ \sqrt{T}$ regret. Generic is analysis. OtherScientificTerm is non - decreasing condition. ","This paper studies the problem of online linear optimization with a reduction to an online convex optimization algorithm. The main contribution of this paper is the analysis of the regret of an algorithm that uses an update to online mirror descent with non-decreasing quadratic regularizers. The regret bound of this algorithm is shown to be $O(\sqrt{T})$ with respect to the original mirror descent regret. The paper also provides a strategy to obtain an adagrad-esque regret bound on a per-coordinate basis, which is in contrast to the worst-case $O(1/T)$ regret in the literature. The analysis is based on the assumption that the algorithm is invariant to the non-increasing condition."
291,SP:b6b594fc555bd12b33f156970f0665e2bf793484,"Expected Quadratic Utility Maximization ( EQUM ) framework USED-FOR policy grandient Mean - Variance control. variance minimization CONJUNCTION return targeting optimization. return targeting optimization CONJUNCTION variance minimization. constraint on expected return CONJUNCTION return targeting optimization. return targeting optimization CONJUNCTION constraint on expected return. constraint on expected return FEATURE-OF variance minimization. policy gradient algorithm USED-FOR EQUM. policy gradient algorithm CONJUNCTION Actor - Critic extension. Actor - Critic extension CONJUNCTION policy gradient algorithm. Generic are state - of - the - art methods, and approach. OtherScientificTerm is regularization. ","This paper proposes the Expected Quadratic Utility Maximization (EQUM) framework for policy grandient Mean-Variance control. The main contribution of this paper is to extend the existing state-of-the-art methods by introducing a constraint on expected return in variance minimization and return targeting optimization. The paper also proposes a policy gradient algorithm for EQUM as well as an Actor-Critic extension to improve the efficiency of the proposed approach. Finally, the paper provides a theoretical analysis of the effect of regularization."
292,SP:b6b594fc555bd12b33f156970f0665e2bf793484,policy gradient style RL algorithm USED-FOR expected quadratic utility. expected quadratic utility maximization USED-FOR policy gradient. mean and variance USED-FOR quadratic utility. policy gradient CONJUNCTION actor - critic with EQUM framework. actor - critic with EQUM framework CONJUNCTION policy gradient. actor - critic with EQUM framework HYPONYM-OF variations. policy gradient HYPONYM-OF variations. Task is risk management. OtherScientificTerm is quadratic utility function. Method is mean - variance RL methods. ,"This paper proposes a policy gradient style RL algorithm that maximizes the expected quadratic utility of the policy gradient with respect to risk management. The main contribution of this paper is to propose a new policy gradient that is based on the idea of expected quadralatic utility maximization. The authors propose two variations: policy gradient and actor-critic with EQUM framework. The key idea is to use the mean and variance to estimate the quadratial utility of a given policy. This is an interesting idea, and the authors provide a theoretical analysis of the quadrastic utility function. The experiments show that the proposed mean-variance RL methods outperform existing methods."
293,SP:b6b594fc555bd12b33f156970f0665e2bf793484,"policy gradient algorithm COMPARE SOTA methods. SOTA methods COMPARE policy gradient algorithm. mean - variance algorithm COMPARE SOTA methods. SOTA methods COMPARE mean - variance algorithm. mean - variance algorithm COMPARE policy gradient algorithm. policy gradient algorithm COMPARE mean - variance algorithm. unbiased gradient FEATURE-OF it. mean reward equality constraint FEATURE-OF variance minimization problem. variance minimization problem USED-FOR problem. mean variance constrained problem USED-FOR problem. double sampling CONJUNCTION frenchel duality. frenchel duality CONJUNCTION double sampling. unbiased policy gradient FEATURE-OF mean - variance formulation. penalized problem USED-FOR problem. frenchel duality HYPONYM-OF techniques. double sampling HYPONYM-OF techniques. methods COMPARE risk - sensitive RL methods. risk - sensitive RL methods COMPARE methods. method USED-FOR balancing risk and return. risk - sensitive RL benchmarks EVALUATE-FOR risk - sensitive RL methods. risk - sensitive RL benchmarks EVALUATE-FOR method. risk - sensitive RL benchmarks EVALUATE-FOR methods. portfolio optimization HYPONYM-OF risk - sensitive RL benchmarks. Method are quadratic utility theory, and variance formulation. ","This paper proposes a mean-variance algorithm that is more efficient than SOTA methods. The problem is formulated as a variance minimization problem with a mean reward equality constraint, which is inspired by quadratic utility theory. The variance formulation is interesting in that it has an unbiased gradient. The paper then proposes to solve the problem as a mean variance constrained problem, where the policy gradient algorithm is the same as the standard mean-vexer algorithm. The main difference between the two methods is that in the case of the unbiased policy gradient, the paper proposes to use two techniques: double sampling and frenchel duality. The proposed method is evaluated on two risk-sensitive RL benchmarks: portfolio optimization and balancing risk and return. "
294,SP:bf9d66f713b6502d274143c6273b2d071a0c045e,auxiliary tasks PART-OF coherent loss. AuxiLearn HYPONYM-OF Auxiliary Learning frame work. Method is Auxiliary Learning. ,"This paper proposes Auxiliary Learning, which is a generalization of the idea that auxiliary tasks can be incorporated into a coherent loss. AuxiLearn is a recent line of Auxiliary learning frame work, and this paper is an extension of this line of work. "
295,SP:bf9d66f713b6502d274143c6273b2d071a0c045e,"tasks USED-FOR representation. auxiliary learning HYPONYM-OF multi - task learning. linear or nonlinear function USED-FOR loss term. approach USED-FOR auxiliary task generation. Method are learning - to - learn algorithm, and implicit differentiation based optimization method. OtherScientificTerm is auxiliary losses. Generic is model. ","This paper proposes a learning-to-learn algorithm for multi-task learning, i.e., auxiliary learning, where the goal is to learn a representation for a given set of tasks. The authors propose an implicit differentiation based optimization method where the loss term is either a linear or nonlinear function, and the auxiliary losses are learned in parallel to the model. This approach allows for efficient auxiliary task generation."
296,SP:bf9d66f713b6502d274143c6273b2d071a0c045e,"grid search USED-FOR linear combination. grid search USED-FOR loss function. implicit differentiation - based approach USED-FOR ( deep ) non - linear network. ( deep ) non - linear network USED-FOR It. implicit differentiation - based approach USED-FOR It. teacher - student networks USED-FOR approach. classification CONJUNCTION segmentation. segmentation CONJUNCTION classification. approach USED-FOR model. tasks EVALUATE-FOR approach. tasks EVALUATE-FOR model. segmentation HYPONYM-OF tasks. classification HYPONYM-OF tasks. Method is AuxiLearn. Generic are framework, network, and pre - defined tasks. OtherScientificTerm is auxiliary losses. ","This paper proposes AuxiLearn, a framework for learning a model that can generalize to unseen tasks. It uses an implicit differentiation-based approach to train a (deep) non-linear network using teacher-student networks. The loss function is learned by using grid search to find a linear combination of the input and the output of the network. The proposed approach is evaluated on two tasks: classification and segmentation, and the proposed model is shown to generalize well on both tasks. The authors also show that auxiliary losses can be used to improve the performance on pre-defined tasks."
297,SP:3070fd64f8eb4d7ece6521cb975fd1fe64d6329f,technique USED-FOR Transformer - based NMT model. translation candidates USED-FOR variance - like estimate. variance - like estimate USED-FOR technique. dropout USED-FOR candidates. technique USED-FOR epistemic uncertainty. epistemic uncertainty FEATURE-OF NMT model. measure COMPARE measures. measures COMPARE measure. model COMPARE measures. measures COMPARE model. measures USED-FOR Out - of - Domain translation requests. measure USED-FOR Out - of - Domain translation requests. measure COMPARE model. model COMPARE measure. limited training data conditions EVALUATE-FOR measure. Method is decoding mechanism. ,This paper proposes a technique to improve the performance of a Transformer-based NMT model by using translation candidates as a variance-like estimate of the epistemic uncertainty of the NMT. The authors propose to use dropout to train the candidates and use a decoding mechanism to estimate the uncertainty. The proposed measure is evaluated on limited training data conditions and compared with other measures for Out-of-Domain translation requests and compared to the original model.
298,SP:3070fd64f8eb4d7ece6521cb975fd1fe64d6329f,"Baysian method USED-FOR detecting out of distribution ( OOD ). Task is machine translation. Metric are BLEU variance ( BLEUVar ), and BLEUVar. OtherScientificTerm is MC Dropout. Generic is it. ",This paper proposes a Baysian method for detecting out of distribution (OOD) in machine translation. The main idea is to use the BLEU variance (BLEUVar) as a measure of OOD. The authors propose to use MC Dropout as a proxy for OOD and show that it can be used to detect OOD with high probability. The paper also shows that MCDropout is effective in terms of reducing the variance of BLEuVar.
299,SP:3070fd64f8eb4d7ece6521cb975fd1fe64d6329f,"method USED-FOR neural machine translation ( NMT ) system. probability of translation CONJUNCTION variance in BLEU. variance in BLEU CONJUNCTION probability of translation. variance in BLEU HYPONYM-OF uncertainty metrics. probability of translation HYPONYM-OF uncertainty metrics. randomly - sampled parameters USED-FOR variance in BLEU. BLEU score EVALUATE-FOR method. OtherScientificTerm are MC Dropout, and MLE parameters. Generic is baseline method. ","This paper proposes a method for training a neural machine translation (NMT) system that is robust to uncertainty metrics such as the probability of translation and variance in BLEU with randomly-sampled parameters. The proposed method, called MC Dropout, is evaluated on a variety of datasets and is shown to improve the BBLEU score by a large margin compared to a baseline method. The main contribution of the paper is the introduction of MLE parameters."
300,SP:d5a1d9596b8329312533b3a0047c815f8e71a201,"accuracy EVALUATE-FOR approach ( 1 ). technique USED-FOR sparse mask. OtherScientificTerm are pruned network, sparse network, sparse networks, initialization, and connectivity. Method is pruned model. Material is lottery ticket hypothesis. Generic is models. ","This paper proposes a pruned network that learns a sparse mask for each layer of the pruned model. The idea is based on the lottery ticket hypothesis, i.e., the sparse network can be seen as a lottery ticket. The authors show that the accuracy of the proposed approach (1) is comparable to that of the original approach (2) in terms of accuracy. The main difference is that the authors propose a technique for learning the sparse mask, where the sparse networks are pruned at initialization and the weights of the models are updated based on connectivity. "
301,SP:d5a1d9596b8329312533b3a0047c815f8e71a201,it COMPARE pruning methods. pruning methods COMPARE it. PaI methods COMPARE ablations. ablations COMPARE PaI methods. Task is pruning. Generic is methods. ,This paper studies the problem of pruning. The authors propose a method called PaI and compare it with other pruning methods. They show that PaI methods perform better than ablations. They also provide some theoretical analysis of the proposed methods.
302,SP:d5a1d9596b8329312533b3a0047c815f8e71a201,"Method are neural network pruning, neural networks, pruning technique, and sparse initialisation methods. Generic are network, and it. ","This paper studies the problem of neural network pruning. In this paper, the authors propose a new pruning technique called sparse initialisation methods. The idea is to prune the weights of the neural networks based on the assumption that the network is sparse. The authors argue that it is a good way to reduce the number of parameters in the network, and that it can be used to improve the performance."
303,SP:1f6b266021da24bbf02b5a47f2b5eb23b4912166,privacy FEATURE-OF federated learning setup. algorithm USED-FOR dimension independent robustness guarantees. dimension independent robustness guarantees FEATURE-OF byzantine threats. Task is Byzantine threats. OtherScientificTerm is secure aggregation. ,This paper studies the problem of privacy in federated learning setup. The authors propose an algorithm that provides dimension independent robustness guarantees against byzantine threats. The main contribution of this paper is to study Byzantine threats in the context of secure aggregation.
304,SP:1f6b266021da24bbf02b5a47f2b5eb23b4912166,poisoning and backdoor attacks FEATURE-OF robustness. robust mean estimation USED-FOR robustness. robust mean estimation USED-FOR defence. Task is federated learning. Method is secure aggregation. OtherScientificTerm is malicious clients. Generic is algorithm. ,"This paper studies the problem of robustness to poisoning and backdoor attacks in federated learning. The authors propose to use robust mean estimation to improve the robustness of the defence. In particular, the authors propose a secure aggregation that is robust to malicious clients. The proposed algorithm is evaluated on several datasets."
305,SP:1f6b266021da24bbf02b5a47f2b5eb23b4912166,"Task are federated learning setting, and learning task. OtherScientificTerm are semi - honest centralized server, and estimator. Generic is they. Method are sharding technique, and estimator method. ","This paper studies the federated learning setting, where each client has access to a semi-honest centralized server, and the server is tasked with learning a learning task. The authors propose a sharding technique to make the server more robust to changes in the clients, and show that they can be used to improve the performance of the estimator. They also provide a theoretical analysis of their estimator method."
306,SP:9f89ff90b203d86a569e3d5148546942f5bf2093,model uncertainty CONJUNCTION accuracy degradation. accuracy degradation CONJUNCTION model uncertainty. optimizer USED-FOR model uncertainty. optimizer USED-FOR accuracy degradation. Task is model - based black box optimization problems. Material is limited data. OtherScientificTerm is surrogate space. Generic is algorithms. ,"This paper studies model-based black box optimization problems with limited data. The authors propose an optimizer to balance model uncertainty and accuracy degradation. The main idea is to learn a surrogate space for the data, which is then used to train the algorithms."
307,SP:9f89ff90b203d86a569e3d5148546942f5bf2093,"high - dimensional design space CONJUNCTION objective function's sensitivity. objective function's sensitivity CONJUNCTION high - dimensional design space. evaluation criterion EVALUATE-FOR benchmark. benchmark EVALUATE-FOR algorithms. Method is offline black - box optimization algorithms. Generic are methods, and evaluation procedures. Task is offline model - based optimization tasks. ","This paper studies the evaluation of offline black-box optimization algorithms. The authors propose a new benchmark based on a new evaluation criterion that considers both the high-dimensional design space and the objective function's sensitivity. The proposed methods are evaluated on a variety of offline model-based optimization tasks. The evaluation procedures are well-motivated, and the algorithms are evaluated using the new benchmark."
308,SP:9f89ff90b203d86a569e3d5148546942f5bf2093,"benchmark suite USED-FOR offline model - based optimization problems. biology CONJUNCTION material science. material science CONJUNCTION biology. material science CONJUNCTION robotics. robotics CONJUNCTION material science. material science FEATURE-OF real - world problems. biology FEATURE-OF real - world problems. real - world problems PART-OF benchmark. tasks PART-OF benchmark. material science USED-FOR tasks. biology USED-FOR tasks. real - world problems USED-FOR tasks. Generic are it, and methods. ","This paper presents a benchmark suite for offline model-based optimization problems. The benchmark consists of several tasks inspired by real-world problems in biology, material science, and robotics. The paper is well-written and well-motivated, and it is easy to follow. However, there are some issues that need to be addressed in order for the paper to be accepted by the community. For example, it is unclear how the proposed methods can be combined with existing methods."
309,SP:073958946c266bf760d1ad66bd39bc28a24c8521,PoE CONJUNCTION MoE. MoE CONJUNCTION PoE. PoE PART-OF multimodal ELBO. MoE PART-OF multimodal ELBO. MoE COMPARE mixture of experts. mixture of experts COMPARE MoE. PoE CONJUNCTION MoE. MoE CONJUNCTION PoE. MoE HYPONYM-OF MoPoE. model COMPARE PoE. PoE COMPARE model. OtherScientificTerm is modalities ’ posterior. ,"This paper proposes a multimodal ELBO that combines PoE and MoE, which is a combination of PoE with MoE (MoPoE, a variant of MoE). Compared to MoE with a mixture of experts, the authors show that MoE outperforms PoE when the modalities’ posterior is close to each other. The authors also show that the proposed model is more expressive than PoE."
310,SP:073958946c266bf760d1ad66bd39bc28a24c8521,"multimodal formulation USED-FOR VAEs. ELBO USED-FOR VAEs. MVAE CONJUNCTION MMVAE. MMVAE CONJUNCTION MVAE. multimodal formulation USED-FOR ELBO. coherence CONJUNCTION log - likelihood. log - likelihood CONJUNCTION coherence. classification accuracy CONJUNCTION coherence. coherence CONJUNCTION classification accuracy. approach COMPARE MMVAE. MMVAE COMPARE approach. MVAE CONJUNCTION MMVAE. MMVAE CONJUNCTION MVAE. approach COMPARE MVAE. MVAE COMPARE approach. multimodal datasets EVALUATE-FOR MVAE. multimodal datasets EVALUATE-FOR MMVAE. classification accuracy EVALUATE-FOR approach. multimodal datasets EVALUATE-FOR approach. method COMPARE it. it COMPARE method. performance metrics EVALUATE-FOR method. performance metrics EVALUATE-FOR it. OtherScientificTerm are approximate posterior, and full data log - likelihood. ","This paper proposes a multimodal formulation of the ELBO for VAEs, which is a generalization of MVAE and MMVAE. The key idea is to replace the approximate posterior with the full data log-likelihood. The proposed approach is evaluated on three multimmodal datasets, and compared to the original approach, MMVAe, and the state of the art in terms of classification accuracy, coherence, and log-likeness. The results show that the proposed method outperforms the state-of-the-art in all three performance metrics."
311,SP:073958946c266bf760d1ad66bd39bc28a24c8521,mixture of product of experts USED-FOR multimodal ELBO. mixture USED-FOR baselines. Method is encoder. Task is inference. ,This paper proposes a multimodal ELBO based on a mixture of product of experts. The authors show that this mixture can outperform the baselines. They also show that the encoder is more robust to inference.
312,SP:98004554447b82b3d2eb9724ec551250eec7a595,prior expert knowledge USED-FOR Bayesian optimization. prior distribution PART-OF pseudo - posterior. OtherScientificTerm is prior information. Material is hyper - parameter test cases. ,This paper studies the problem of using prior expert knowledge for Bayesian optimization. The authors propose a pseudo-posterior that incorporates the prior distribution of the pseudo-prior as the prior information. They show that this can lead to better performance in hyper-parameter test cases.
313,SP:98004554447b82b3d2eb9724ec551250eec7a595,prior distribution PART-OF Bayesian optimization ( BO ). pseudo - posterior USED-FOR EI acquisition function. prior CONJUNCTION probabilistic surrogate model of BO. probabilistic surrogate model of BO CONJUNCTION prior. Task is BO. Generic is algorithm. OtherScientificTerm is prior information. ,"This paper studies the problem of Bayesian optimization (BO) with respect to the prior distribution. In particular, the authors consider the case where the EI acquisition function is based on the pseudo-posterior. The authors propose a new algorithm that combines the prior with a probabilistic surrogate model of BO. The main contribution of this paper is that the proposed algorithm does not require prior information."
314,SP:98004554447b82b3d2eb9724ec551250eec7a595,"method USED-FOR BO. experts'knowledge PART-OF BO. experts'knowledge PART-OF method. PrBO COMPARE baselines. baselines COMPARE PrBO. it USED-FOR PrBO. Method is Prior - guided Bayesian Optimization ( PrBO ). OtherScientificTerm are user provided priors, and prior. Task is optimization. Generic is model. ","This paper proposes Prior-guided Bayesian Optimization (PrBO), a method for BO that incorporates experts' knowledge into BO. The key idea is to use user provided priors to guide the optimization. The idea is that the prior should be able to generalize well to unseen tasks. The authors show that PrBO outperforms the baselines by a large margin, and that it can be used in conjunction with PrBO. The paper also provides a theoretical analysis of the model and how the prior is learned."
315,SP:e0e9cd5f39a60b5db1c4363ffdc2c593300ef43a,"binary weights USED-FOR generative models. flow++ CONJUNCTION RVAE. RVAE CONJUNCTION flow++. RVAE HYPONYM-OF SOTA generative models. flow++ HYPONYM-OF SOTA generative models. BWN USED-FOR robust learning. OtherScientificTerm are binary activations, residual layers, and computational graph. Generic is technique. Method are weight normalization, and binary DNNs. ","This paper proposes a technique called weight normalization, which aims to normalize the binary activations of generative models with binary weights. The technique is based on the observation that the residual layers of binary DNNs can be seen as a computational graph, which can be used to improve the performance of existing SOTA (flow++ and RVAE). The authors also show that BWN can improve robust learning."
316,SP:e0e9cd5f39a60b5db1c4363ffdc2c593300ef43a,generative VAE CONJUNCTION Flow++ models. Flow++ models CONJUNCTION generative VAE. activations USED-FOR generative VAE. Weight Normalization USED-FOR models. Euclidean norm FEATURE-OF binary [ -1;1 ] vector. affine scaling USED-FOR Weight Normalization. CIFAR and ImageNet datasets EVALUATE-FOR it. Method is scaling Binary Weight Normalization. ,"This paper proposes scaling Binary Weight Normalization, which is a generalization of the generative VAE and Flow++ models with different activations. The authors propose to use the Euclidean norm of the binary [-1;1] vector instead of the standard affine scaling of Weight Normalized models. They evaluate it on CIFAR and ImageNet datasets."
317,SP:e0e9cd5f39a60b5db1c4363ffdc2c593300ef43a,variational autoencoders CONJUNCTION flow - based networks. flow - based networks CONJUNCTION variational autoencoders. method USED-FOR variational autoencoders. methods USED-FOR unsupervised problems. Method is computing systems. Metric is binary precision. ,This paper proposes a method for training variational autoencoders and flow-based networks. The method is motivated by the observation that computing systems tend to converge to binary precision when the number of parameters is small. The authors show that such methods can be applied to unsupervised problems.
318,SP:7e9a83552c0ff001d3090a5a7162013b5dc6f47f,"imperceptible L_p norm perturbations FEATURE-OF adversarial learning approaches. approach USED-FOR natural variation. unsupervised approaches USED-FOR natural variation. GANs USED-FOR natural variation. unsupervised approaches USED-FOR approach. GANs HYPONYM-OF unsupervised approaches. data augmentation USED-FOR approaches. tasks EVALUATE-FOR approach. Method is model of natural variation. OtherScientificTerm are adversarial learning objective, and L_p norm ball. ","This paper studies adversarial learning approaches with imperceptible L_p norm perturbations. The authors propose an approach to model natural variation using unsupervised approaches such as GANs. The key idea is to learn a model of natural variation, which can then be used as an adversarial training objective. The proposed approach is evaluated on a variety of tasks, where the authors show that the proposed approaches do not require data augmentation. The main contribution of the paper is the introduction of an additional L_{p norm ball."
319,SP:7e9a83552c0ff001d3090a5a7162013b5dc6f47f,"model - based framework USED-FOR image classifiers. model - based framework USED-FOR robustness. robustness EVALUATE-FOR image classifiers. adversarial training USED-FOR framework. nuisance parameter USED-FOR corruption. baselines USED-FOR model - based framework. OtherScientificTerm are average - case corruptions, and perturbation. ","This paper proposes a model-based framework for improving the robustness of image classifiers using adversarial training. The proposed framework is based on the idea that average-case corruptions should be considered as perturbations, and the authors propose to add a nuisance parameter to the corruption to encourage corruption. The authors evaluate the proposed model with two baselines and show that the proposed framework outperforms the baselines."
320,SP:7e9a83552c0ff001d3090a5a7162013b5dc6f47f,paradigm shift ” USED-FOR augmenting datasets. augmenting datasets USED-FOR CNN - based image classifiers. Gaussian noise CONJUNCTION color distortions. color distortions CONJUNCTION Gaussian noise. blur CONJUNCTION Gaussian noise. Gaussian noise CONJUNCTION blur. blur PART-OF augmentations. Gaussian noise PART-OF augmentations. color distortions PART-OF augmentations. image space FEATURE-OF norm bounds. norm bounds FEATURE-OF augmentations. adversarial training HYPONYM-OF methods. augmentations USED-FOR methods. models of natural variation USED-FOR images. models of natural variation USED-FOR method. ImageNet - C CONJUNCTION CURE - TSR dataset. CURE - TSR dataset CONJUNCTION ImageNet - C. Metric is out - of - domain accuracy. ,"This paper studies the problem of augmenting datasets for CNN-based image classifiers with “paradigm shift”. The authors propose three augmentations: blur, Gaussian noise, and color distortions, and provide norm bounds on the image space for these augmentations. The proposed method is based on models of natural variation for images, and is shown to improve out-of-domain accuracy on ImageNet-C and the CURE-TSR dataset. In addition, the authors propose two new methods: adversarial training and the use of augmentations for training."
321,SP:011dab90d225550e77235cbec1615e583ae3297e,CNNs USED-FOR non - convex optimization problem. convex problems USED-FOR these. poly time complexity FEATURE-OF convex problems. ReLU activations USED-FOR CNNs. architecture CONJUNCTION regularizer. regularizer CONJUNCTION architecture. CNN architecture USED-FOR weight regularizers. OtherScientificTerm is relevant variables. Generic is them. ,"This paper studies the problem of training CNNs for a non-convex optimization problem. In particular, these are convex problems with poly time complexity. The authors propose to train CNNs with ReLU activations, where the relevant variables are sampled from the training set. The main contribution of this paper is to propose a new architecture and a new regularizer. The proposed weight regularizers are based on the CNN architecture, and the authors show that training them is computationally efficient."
322,SP:011dab90d225550e77235cbec1615e583ae3297e,convex optimization techniques USED-FOR convolutional neural networks ( CNNs ). convex problem CONJUNCTION nonconvex training problems. nonconvex training problems CONJUNCTION convex problem. multi - layer CNNs CONJUNCTION three - layer CNNs. three - layer CNNs CONJUNCTION multi - layer CNNs. ReLU layer FEATURE-OF multi - layer CNNs. ReLU layers USED-FOR three - layer CNNs. OtherScientificTerm is dual of the nonconvex training problems. ,This paper studies convex optimization techniques for convolutional neural networks (CNNs). The authors consider the dual of the convex problem and the nonconvex training problems. The authors show that multi-layer CNNs with ReLU layer and three-layerCNNs with different ReLU layers converge to the same solution. 
323,SP:011dab90d225550e77235cbec1615e583ae3297e,convex reformulations USED-FOR non - convex problems. architecture USED-FOR implicit regularization. SGD USED-FOR problem. SGD USED-FOR global minimizer. convex reformulation USED-FOR global minimizer. Method is CNN. Generic is networks. Metric is polynomial complexity. ,"This paper studies convex reformulations for non-convex problems. The authors propose a new architecture for implicit regularization, which is similar to CNN. The main difference is that instead of using SGD to solve the problem, the authors propose to use a global minimizer with a convex reweighting of the weights of the networks, which leads to a polynomial complexity."
324,SP:98760a3b1a5058a485a5a1ed1b778c1d4fb2ff22,"disentangled representations USED-FOR learning from demonstrations ( LfD ) task. robotics FEATURE-OF learning from demonstrations ( LfD ) task. weak - supervision USED-FOR unsupervised learning frameworks. variational autoencoder USED-FOR unsupervised learning frameworks. visual data CONJUNCTION robot trajectories. robot trajectories CONJUNCTION visual data. robot trajectories PART-OF PR2 robot dabbing demonstrations. visual data PART-OF PR2 robot dabbing demonstrations. OtherScientificTerm are disentangled factors of variation, and human demonstrations. ","This paper studies the problem of learning from demonstrations (LfD) task in robotics with disentangled representations. In particular, the authors focus on unsupervised learning frameworks with weak-supervision using a variational autoencoder. The authors show that learning from human demonstrations can lead to better performance, especially when the learned representations are more robust to disentangling factors of variation. The experiments are conducted on visual data and robot trajectories from PR2 robot dabbing demonstrations."
325,SP:98760a3b1a5058a485a5a1ed1b778c1d4fb2ff22,"high dimensional multimodal inputs USED-FOR interpretable low dimensional representations. weak supervision USED-FOR interpretable low dimensional representations. lower - dimensional manifold USED-FOR tasks. weak supervision USED-FOR manifold. demonstrations USED-FOR probabilistic   generative models. variational inference USED-FOR probabilistic   generative models. conditional latent variable models USED-FOR disentangled   low dimensional represented. weak supervision USED-FOR conditional latent variable models. weak supervision USED-FOR disentangled   low dimensional represented. Task are learning   from demonstrations, leaning interpretable low dimensional representations, and learning. OtherScientificTerm are observation+action spaces, and real - world   experiments. Generic is them. ","This paper studies the problem of learning from demonstrations. In particular, the authors focus on learning interpretable low dimensional representations from high dimensional multimodal inputs. The authors propose to learn a lower-dimensional manifold for these tasks by using weak supervision on this manifold. The demonstrations are used to train probabilistic generative models with variational inference. The conditional latent variable models are trained with weak supervision to obtain a disentangled low dimensional represented. The experiments are conducted on observation+action spaces, and the authors show that learning from demonstrations is robust and easy.  The authors also conduct real-world experiments to evaluate the performance of the learned models and how learned from them."
326,SP:98760a3b1a5058a485a5a1ed1b778c1d4fb2ff22,"latent variables USED-FOR uncertainty. slow CONJUNCTION soft. soft CONJUNCTION slow. technique USED-FOR uncertainty. class labels USED-FOR task. slow HYPONYM-OF class labels. soft HYPONYM-OF class labels. latent variables USED-FOR technique. model USED-FOR task. Generic are variables, and models. OtherScientificTerm are human provided labels, and dabbing motion. ","This paper proposes a technique to model uncertainty using latent variables. These variables are generated from human provided labels. The authors propose to use class labels (slow, soft, etc) to model the task and then use the learned model to solve the task. They show that their models are able to generalize to new environments and to new objects (e.g. dabbing motion)."
327,SP:e171d8c4eadf73852734c0fb8a74a69d80969e4b,fine - tuning BERT USED-FOR low resource target tasks. fine - tuning BERT USED-FOR pretrained language models ( PLMs ). pretrained language models ( PLMs ) USED-FOR low resource target tasks. PLMs USED-FOR general - purpose knowledge. variational information bottleneck ( VIB ) USED-FOR fine - tuning framework. representation USED-FOR task prediction. latent Gaussian variable USED-FOR irrelevant and redundant features. latent Gaussian variable USED-FOR sentence representation. sever datasets EVALUATE-FOR method. Material is low resource target task. OtherScientificTerm is overfitting. ,"This paper focuses on fine-tuning BERT for low resource target tasks to improve the performance of pretrained language models (PLMs) for general-purpose knowledge. The authors propose to use a variational information bottleneck (VIB) to guide the fine -tuning framework. The key idea is to use sentence representation as a latent Gaussian variable to remove irrelevant and redundant features, and then use this representation for task prediction. The method is evaluated on sever datasets. The results show that the proposed method can improve performance on the low-resource target task without overfitting."
328,SP:e171d8c4eadf73852734c0fb8a74a69d80969e4b,information bottleneck USED-FOR pre - trained representation. features USED-FOR task. Mixout CONJUNCTION L2 - of - difference. L2 - of - difference CONJUNCTION Mixout. It USED-FOR GLUE tasks. It COMPARE baselines. baselines COMPARE It. L2 - of - difference HYPONYM-OF baselines. Mixout HYPONYM-OF baselines. generalization capacity FEATURE-OF model. out - of - domain data USED-FOR tuned model. Task is low resource settings. ,"This paper studies the problem of pre-training a pre-trained representation in low resource settings, where the information bottleneck is present. The authors propose to use features from the original task to train the pre-trained representation. It is applied to GLUE tasks and compared with two baselines: Mixout and L2-of-difference. The generalization capacity of the trained model is shown to be improved with the use of out-of -domain data."
329,SP:e171d8c4eadf73852734c0fb8a74a69d80969e4b,"large pretrained models USED-FOR downstream tasks. method USED-FOR overfitting. small scale datasets USED-FOR large pretrained models. small scale datasets USED-FOR downstream tasks. out of domain datasets EVALUATE-FOR generalization. features CONJUNCTION bottleneck features. bottleneck features CONJUNCTION features. Method is SOTA models. OtherScientificTerm are spurious correlations, and feature vectors. Metric is mutual information. ","This paper proposes a method to mitigate overfitting in large pretrained models trained on small scale datasets for downstream tasks. The authors argue that SOTA models are prone to spurious correlations between features and bottleneck features. To address this issue, the authors propose to decompose the feature vectors into two sub-spaces, where the mutual information between the features and the bottleneck features is minimized. Experiments on out of domain datasets are conducted to evaluate generalization."
330,SP:a1ab99bee74a0a1310537beced0d89dc1e5ad7be,GANs USED-FOR 3D shape. 2D image USED-FOR 3D shape. networks USED-FOR 3D parameters. GAN USED-FOR images. Method is 3D parameter network. ,This paper proposes to use GANs to learn 3D shape from a 2D image. The main idea is to use networks to learn the 3D parameters and then use a GAN to generate the images. The 3D parameter network is trained on the generated images.
331,SP:a1ab99bee74a0a1310537beced0d89dc1e5ad7be,"method USED-FOR 3D shape reconstruction. pre - trained 2D image generative adversarial networks USED-FOR method. lighting CONJUNCTION depth. depth CONJUNCTION lighting. viewpoint CONJUNCTION lighting. lighting CONJUNCTION viewpoint. depth CONJUNCTION albedo. albedo CONJUNCTION depth. differentiable renderer USED-FOR reconstruction error. albedo HYPONYM-OF graphics code. depth HYPONYM-OF graphics code. lighting HYPONYM-OF graphics code. viewpoint HYPONYM-OF graphics code. pre - trained 2D image GAN USED-FOR pseudo samples. GAN - Inversion USED-FOR pre - trained 2D image GAN. car CONJUNCTION building. building CONJUNCTION car. face CONJUNCTION car. car CONJUNCTION face. building CONJUNCTION horse. horse CONJUNCTION building. horse HYPONYM-OF categories. face HYPONYM-OF categories. car HYPONYM-OF categories. building HYPONYM-OF categories. Task is inverse - graphics problem. Material is image. OtherScientificTerm are viewpoint and lighting space, predicted depth, and data manifold. Generic is projected samples. ","This paper presents a method for 3D shape reconstruction using pre-trained 2D image generative adversarial networks. The inverse-graphics problem is formulated as the following: given an image, the goal is to reconstruct the object in both the viewpoint and lighting space. The reconstruction error is estimated using a differentiable renderer, where the reconstruction error depends on the predicted depth of the object. The paper considers three categories of graphics code: viewpoint, lighting, and depth, as well as albedo. To generate pseudo samples, the paper uses GAN-Inversion, which is trained to generate pseudo images of the target object, and then projected samples onto the data manifold. Experiments are performed on four categories: face, car, building, and horse."
332,SP:a1ab99bee74a0a1310537beced0d89dc1e5ad7be,light directions CONJUNCTION depth. depth CONJUNCTION light directions. viewpoints CONJUNCTION light directions. light directions CONJUNCTION viewpoints. depth CONJUNCTION albedo. albedo CONJUNCTION depth. iterative method USED-FOR viewpoints. nautral image manifold FEATURE-OF intermediate renderings. pre - trained GANs USED-FOR method. variants USED-FOR 3D shapes. pre - trained 2D GANs USED-FOR data generation photorealistic. 3D rotation CONJUNCTION relighting. relighting CONJUNCTION 3D rotation. relighting HYPONYM-OF 3D edits. 3D rotation HYPONYM-OF 3D edits. OtherScientificTerm is lightings. Generic is model. ,"This paper proposes an iterative method to generate viewpoints, light directions, depth, and albedo. The method is based on pre-trained GANs, which are trained to generate intermediate renderings on a nautral image manifold. The paper also proposes variants to generate 3D shapes, which can be viewed as data generation photorealistic. The authors perform experiments on 3D edits such as 3D rotation, relighting, etc. and show that the lightings produced by the proposed model are more realistic."
333,SP:eac0679dfee4dae78c1e515f8b325c9523b795dc,classification loss CONJUNCTION distribution - aware diversity loss. distribution - aware diversity loss CONJUNCTION classification loss. classification loss USED-FOR classifiers of experts. distribution - aware diversity loss USED-FOR classifiers of experts. expert assignment module USED-FOR expert decisions. OtherScientificTerm is feature extraction backbone. ,This paper proposes a feature extraction backbone that uses a combination of a classification loss and a distribution-aware diversity loss to train classifiers of experts. The expert assignment module is used to guide the expert decisions.
334,SP:eac0679dfee4dae78c1e515f8b325c9523b795dc,Routing Diverse Experts ( RIDE ) framework USED-FOR long - tailed classification problem. shared low - level feature extractor CONJUNCTION expert classifiers. expert classifiers CONJUNCTION shared low - level feature extractor. expert routing module USED-FOR joint decision. experts USED-FOR joint decision. distribution - aware diversity loss USED-FOR classification strategies. expert routing module USED-FOR experts. distribution - aware diversity loss PART-OF It. expert classifiers PART-OF It. expert routing module PART-OF It. shared low - level feature extractor PART-OF It. ,"This paper proposes the routing Diverse Experts (RIDE) framework for long-tailed classification problem. It consists of a shared low-level feature extractor, expert classifiers, and an expert routing module to make the joint decision between the experts. It also includes a distribution-aware diversity loss to encourage different classification strategies."
335,SP:eac0679dfee4dae78c1e515f8b325c9523b795dc,method USED-FOR long - tailed classifier. RoutIng Diverse Experts ( RIDE ) USED-FOR long - tailed classifier. shared architecture CONJUNCTION distribution - aware diversity loss. distribution - aware diversity loss CONJUNCTION shared architecture. distribution - aware diversity loss CONJUNCTION expert routing module. expert routing module CONJUNCTION distribution - aware diversity loss. expert routing module PART-OF components. distribution - aware diversity loss PART-OF components. shared architecture PART-OF components. components PART-OF RIDE. expert routing module PART-OF RIDE. distribution - aware diversity loss PART-OF RIDE. shared architecture PART-OF RIDE. ImageNet - LT CONJUNCTION iNaturalist. iNaturalist CONJUNCTION ImageNet - LT. CIFAR100 - LT CONJUNCTION ImageNet - LT. ImageNet - LT CONJUNCTION CIFAR100 - LT. CIFAR100 - LT HYPONYM-OF long - tailed benchmark datasets. ImageNet - LT HYPONYM-OF long - tailed benchmark datasets. iNaturalist HYPONYM-OF long - tailed benchmark datasets. classification EVALUATE-FOR long - tailed visual recognition. ,"This paper proposes a method called RoutIng Diverse Experts (RIDE) to train a long-tailed classifier. RIDE consists of three components: a shared architecture, a distribution-aware diversity loss, and an expert routing module. Experiments are conducted on three long-tailed benchmark datasets: CIFAR100-LT, ImageNet-LT and iNaturalist. Results show that RIDE can achieve state-of-the-art performance on classification for long tailed visual recognition."
336,SP:f4d0e821de6830722a3458fd40d8d6793a107827,"baseline scoring mechanisms USED-FOR filter pruning. scoring mechanisms USED-FOR pruning filters. CNNs USED-FOR pruning filters. Gaussian - like distribution FEATURE-OF filter weights. Convolution Weight Distribution Assumption "" ( CWDA ) HYPONYM-OF Gaussian - like assumption. OtherScientificTerm is filters. Generic are methods, and architectures. ","This paper proposes baseline scoring mechanisms for filter pruning based on existing methods. These scoring mechanisms are used for pruning filters in CNNs. The authors argue that the filters should have a Gaussian-like distribution over the filter weights (e.g., the ""Convolution Weight Distribution Assumption"" (CWDA). The authors also argue that these methods can be applied to different architectures."
337,SP:f4d0e821de6830722a3458fd40d8d6793a107827,"norm - based pruning criteria USED-FOR structured pruning. filters PART-OF convolutional layer. OtherScientificTerm are redundant filters, and Gaussian distribution. Method are CWDA assumption, and statistical hypothesis testing. Metric is pruning criteria. ",This paper proposes a new norm-based pruning criteria for structured pruning. The main idea is to remove redundant filters from the convolutional layer. The CWDA assumption is relaxed to a Gaussian distribution. The paper also provides a theoretical analysis of the pruned criteria and conducts statistical hypothesis testing.
338,SP:f4d0e821de6830722a3458fd40d8d6793a107827,"CDWA USED-FOR distribution. weight distribution USED-FOR well - trained network. Method are magnitude - based pruning methods, and large networks. ","This paper proposes magnitude-based pruning methods to reduce the number of parameters in large networks. The main idea is to use CDWA to estimate the distribution of the parameters of the weights of a well-trained network, and then use the weight distribution to prune the parameters. The authors provide some theoretical analysis to support their argument."
339,SP:eadb827653b2e1b608bb923d5549089cb2482d90,"data flow information USED-FOR graph representation of variables in the code. data flow graph USED-FOR structure of variables. natural language comment of the code CONJUNCTION data flow graph. data flow graph CONJUNCTION natural language comment of the code. code CONJUNCTION natural language comment of the code. natural language comment of the code CONJUNCTION code. natural language comment of the code USED-FOR pretrained model. data flow graph USED-FOR pretrained model. code USED-FOR pretrained model. Masked Language Modeling objective CONJUNCTION pretraining objectives. pretraining objectives CONJUNCTION Masked Language Modeling objective. pretraining objectives USED-FOR alignment of variables. edge of the data flow graph HYPONYM-OF pretraining objectives. data flow graph CONJUNCTION code. code CONJUNCTION data flow graph. Code Clone Detection CONJUNCTION Code Translation. Code Translation CONJUNCTION Code Clone Detection. Code Translation CONJUNCTION Code Refinement. Code Refinement CONJUNCTION Code Translation. Natural Language Code Search CONJUNCTION Code Clone Detection. Code Clone Detection CONJUNCTION Natural Language Code Search. GraphCodeBERT USED-FOR Code Clone Detection. GraphCodeBERT USED-FOR Natural Language Code Search. GraphCodeBERT USED-FOR Code Translation. GraphCodeBERT USED-FOR Code Refinement. Task is programming language. OtherScientificTerm are AST parse, and attention. Method is graph - guided masked attention. ","This paper proposes to use data flow information to learn a graph representation of variables in the code. The idea is that the structure of variables can be represented as a data flow graph, which can be used to model the underlying structure of the programming language. The pretrained model takes as input a code, a natural language comment of the code, and a corresponding natural language flow graph as input, and uses the data graph graph to represent the underlying data. The paper proposes a Masked Language Modeling objective, as well as two pretraining objectives for alignment of variables (e.g., edge of the input and the AST parse). The paper also proposes a graph-guided masked attention, where the attention is applied to a subset of the nodes in the graph. Experiments are conducted on Natural Language Code Search, Code Clone Detection, Code Translation, Code Refinement, and Code Translation with GraphCodeBERT."
340,SP:eadb827653b2e1b608bb923d5549089cb2482d90,Graph Code BERT HYPONYM-OF language model. data flow USED-FOR code representation. language model USED-FOR code representation. Graph Code BERT USED-FOR code representation. data flow USED-FOR language model. Masked Language Modeling CONJUNCTION Edge Prediction. Edge Prediction CONJUNCTION Masked Language Modeling. Edge Prediction CONJUNCTION Node Alignment. Node Alignment CONJUNCTION Edge Prediction. Node Alignment HYPONYM-OF objective functions. Masked Language Modeling HYPONYM-OF objective functions. Edge Prediction HYPONYM-OF objective functions. clone detection CONJUNCTION code translation. code translation CONJUNCTION clone detection. code translation CONJUNCTION code refinement. code refinement CONJUNCTION code translation. code search CONJUNCTION clone detection. clone detection CONJUNCTION code search. structure - aware pre - training USED-FOR code - related downstream tasks. code refinement HYPONYM-OF code - related downstream tasks. code search HYPONYM-OF code - related downstream tasks. code translation HYPONYM-OF code - related downstream tasks. clone detection HYPONYM-OF code - related downstream tasks. ,"This paper proposes Graph Code BERT, a language model that uses data flow to learn a code representation. Three objective functions are proposed: Masked Language Modeling, Edge Prediction, and Node Alignment. The paper also proposes structure-aware pre-training for code-related downstream tasks such as code search, clone detection, code translation, and code refinement."
341,SP:eadb827653b2e1b608bb923d5549089cb2482d90,code USED-FOR pretraining. masked token prediction task HYPONYM-OF structure aware pre - training tasks. pretrained model COMPARE pretrained models. pretrained models COMPARE pretrained model. pretrained model COMPARE CodeBERT baselines. CodeBERT baselines COMPARE pretrained model. CodeBERT baselines COMPARE pretrained models. pretrained models COMPARE CodeBERT baselines. tasks EVALUATE-FOR pretrained model. tasks CONJUNCTION data flow input. data flow input CONJUNCTION tasks. OtherScientificTerm is data flow. ,"This paper proposes structure aware pre-training tasks (i.e., the masked token prediction task), where the code is used for pretraining. The paper shows that the proposed pretrained model outperforms the existing CodeBERT baselines on a variety of tasks and data flow input, and is comparable to the pretrained models on some of the other tasks. "
342,SP:2c7a128e19cd2d39b0ca1b946b01604c3f7cead5,approach USED-FOR regression models. accuracy EVALUATE-FOR regression models. accuracy EVALUATE-FOR approach. skew dataset USED-FOR regression models. adversarial network USED-FOR forcing output distributions. adversarial network CONJUNCTION regularization. regularization CONJUNCTION adversarial network. parts PART-OF approach. regularization PART-OF parts. adversarial network HYPONYM-OF parts. regularization PART-OF approach. adversarial autoencoder USED-FOR regularization. accuracy EVALUATE-FOR regression model. datasets EVALUATE-FOR regression model. regression model EVALUATE-FOR approach. datasets EVALUATE-FOR approach. accuracy EVALUATE-FOR approach. ,"This paper proposes an approach to improve the accuracy of regression models trained on a skew dataset. The approach consists of two parts: an adversarial network for forcing output distributions, and a regularization based on the adversarial autoencoder. The proposed approach is evaluated on two datasets, and the proposed approach improves the accuracy on the regression model."
343,SP:2c7a128e19cd2d39b0ca1b946b01604c3f7cead5,skewed data USED-FOR regression model. encoder R_enc USED-FOR latent space. regressor network R_post USED-FOR latent representation. adversarial network USED-FOR predictive distribution. synthetic benchmark data EVALUATE-FOR approach. approach COMPARE regression model. regression model COMPARE approach. synthetic benchmark data EVALUATE-FOR regression model. skewed data EVALUATE-FOR regression model. skewed data EVALUATE-FOR approach. Generic is model. Method is latent space representations. ,This paper proposes a regression model on skewed data. The proposed model is based on the idea that the latent space representations of the encoder R_enc and the regressor network R_post can be used to approximate the latent representation of the predictive distribution of an adversarial network. Experiments on synthetic benchmark data show that the proposed approach outperforms the state-of-the-art regression model in terms of skewed data performance.
344,SP:2c7a128e19cd2d39b0ca1b946b01604c3f7cead5,semi - supervised learning approach USED-FOR regression model. output - skewed data USED-FOR regression model. AAE USED-FOR output distribution. AAE CONJUNCTION adversarial model. adversarial model CONJUNCTION AAE. adversarial model PART-OF model. AAE PART-OF model. real datasets EVALUATE-FOR model. regression accuracy EVALUATE-FOR model. ,This paper proposes a semi-supervised learning approach to train a regression model on output-skewed data. The proposed model consists of a combination of AAE to estimate the output distribution and an adversarial model to generate samples that are more likely to be skewed. Experiments on real datasets show that the proposed model can improve the regression accuracy.
345,SP:fee1e40275fa743aa6ad011ae742b3ea3fd137df,architecture USED-FOR transferability of compositionality. network PART-OF components. hidden representations USED-FOR prediction network. network PART-OF components. prediction network PART-OF components. components PART-OF architecture. network PART-OF architecture. prediction network PART-OF architecture. hidden representations USED-FOR network. DNN architectures CONJUNCTION humans. humans CONJUNCTION DNN architectures. architecture COMPARE humans. humans COMPARE architecture. datasets USED-FOR compositional generalisation. architecture COMPARE DNN architectures. DNN architectures COMPARE architecture. datasets EVALUATE-FOR DNN architectures. datasets EVALUATE-FOR humans. datasets EVALUATE-FOR architecture. ,This paper proposes a new architecture to improve the transferability of compositionality. The architecture consists of two components: a network that predicts the next state and a prediction network that uses the hidden representations of the network. The proposed architecture is evaluated on two datasets for compositional generalisation and compared to other DNN architectures and humans.
346,SP:fee1e40275fa743aa6ad011ae742b3ea3fd137df,"neural models USED-FOR compositional ” representations. manifold of individual object representations USED-FOR object representations. representations of individual object representations USED-FOR manifold. Task are transferability of compositionality ” problem, and transferability problem. Generic are approach, problem, and solution. Method is compositional representations. Material is images. OtherScientificTerm are linguistic inputs, and training and test distributions. ","This paper addresses the “transferability of compositionality” problem, which is a well-studied problem in neural models for learning “compositional” representations. The authors propose an approach to solve this problem, where the compositional representations are generated from a manifold of individual object representations, and the object representations are learned from these representations. This problem is well-motivated by the fact that images are often composed of multiple objects, and that the transferability problem can arise when the linguistic inputs are different from the training and test distributions. This paper proposes a solution to this problem."
347,SP:fee1e40275fa743aa6ad011ae742b3ea3fd137df,colored MNIST CONJUNCTION concatenated month names. concatenated month names CONJUNCTION colored MNIST. overlapped MNIST CONJUNCTION colored MNIST. colored MNIST CONJUNCTION overlapped MNIST. auxiliary reconstruction network CONJUNCTION regularized optimization. regularized optimization CONJUNCTION auxiliary reconstruction network. regularized optimization USED-FOR baselines. OtherScientificTerm is compositionality. Generic is it. Method is compositional representation. ,"This paper studies the problem of compositionality and proposes a way to learn a compositional representation that can be used in a variety of settings, including overlapped MNIST, colored MNIST and concatenated month names. In particular, it proposes an auxiliary reconstruction network and regularized optimization to improve the performance of the baselines. "
348,SP:ffab573a977c819e86601de74690c29a39c264cd,"poisoning attacks USED-FOR RL agents. RL environments EVALUATE-FOR poisoning algorithm. Vulnerability - Aware Adversarial Critic Poison HYPONYM-OF poisoning algorithm. Method are policy - based deep RL agents, and MDP model. ","This paper studies the problem of poisoning attacks against RL agents. The authors propose a poisoning algorithm called Vulnerability-Aware Adversarial Critic Poison, which is evaluated on a variety of RL environments. In particular, the authors consider policy-based deep RL agents, where the agent is trained on an MDP model."
349,SP:ffab573a977c819e86601de74690c29a39c264cd,"poisoning attacks FEATURE-OF online reinforcement learning agents. bi - level optimization USED-FOR attack. procedure USED-FOR sequential attacks. tasks EVALUATE-FOR attack. OtherScientificTerm is state - action - reward trajectories. Generic are method, and formulation. Method are RL algorithm, learning procedure, imitated policy, imitated learning procedure, and sequential optimization. ","This paper studies poisoning attacks on online reinforcement learning agents. The proposed method is based on the observation that the state-action-reward trajectories of an RL algorithm can be corrupted by an imitated learning procedure. The authors propose to use bi-level optimization to mitigate this attack. They show that this formulation can be applied to the case where the imitated policy is learned by sequential optimization. They also show that the proposed procedure can be used to mitigate sequential attacks. Finally, the authors evaluate the effectiveness of the proposed attack on several tasks."
350,SP:ffab573a977c819e86601de74690c29a39c264cd,"poisoning algorithm USED-FOR policy - based deep reinforcement learning agents. sequential bilevel optimisation problem ( Problem Q ) USED-FOR poisoning attack. stability radius HYPONYM-OF metric. OtherScientificTerm is learner. Method are VA2C - P, and adversarial critic. ","This paper proposes a poisoning algorithm for policy-based deep reinforcement learning agents. The poisoning attack is formulated as a sequential bilevel optimisation problem (Problem Q), where the learner is given a fixed number of iterations and the adversarial critic is given an arbitrary number of actions. The authors propose a new metric called stability radius, which is based on VA2C-P."
351,SP:06ebd437ff2d1b5068f7a651716d3c1a60c2a001,online algorithm USED-FOR dynamic tensor rematerialization. memory budget CONJUNCTION tensor operations. tensor operations CONJUNCTION memory budget. asymptotic order EVALUATE-FOR optimal static approach. asymptotic order FEATURE-OF memory budget. asymptotic order FEATURE-OF tensor operations. PyTorch prototype COMPARE PyTorch models. PyTorch models COMPARE PyTorch prototype. memory footprint CONJUNCTION batch size. batch size CONJUNCTION memory footprint. batch size EVALUATE-FOR PyTorch models. checkpointing USED-FOR PyTorch models. Method is optimal static checkpointing. ,"This paper proposes an online algorithm for dynamic tensor rematerialization. The authors show that the asymptotic order of the memory budget and tensor operations of the optimal static approach can be approximated by optimizing the parameters of optimal static checkpointing. The experimental results on the PyTorch prototype show improvements over the existing state-of-the-art PyTorrent models, both in terms of memory footprint and batch size. The paper also shows that checkpointing can be used to speed up the training of PyTorrents."
352,SP:06ebd437ff2d1b5068f7a651716d3c1a60c2a001,"compute overhead EVALUATE-FOR rematerialization. memory capacity CONJUNCTION recursive replay cost. recursive replay cost CONJUNCTION memory capacity. staleness CONJUNCTION memory capacity. memory capacity CONJUNCTION staleness. formalization USED-FOR heuristics. Generic are approach, it, and heuristic. Task is static analysis of the network. Metric is memory capacity saving. OtherScientificTerm is training slowdown. ","This paper proposes an approach to reduce the compute overhead of rematerialization. The approach is motivated by the observation that it is difficult to perform static analysis of the network due to staleness, memory capacity, and recursive replay cost. The authors propose a formalization of existing heuristics and show that this formalization can be used to improve the performance of the proposed heuristic. The main contribution of the paper is the memory capacity saving. The paper also shows that the training slowdown can be reduced by a factor of 1.5."
353,SP:06ebd437ff2d1b5068f7a651716d3c1a60c2a001,"heuristics USED-FOR checkpointing deep learning models. greedy algorithm CONJUNCTION heuristics. heuristics CONJUNCTION greedy algorithm. greedy algorithm USED-FOR checkpointing deep learning models. restricted GPU memory budgets USED-FOR model. method USED-FOR static and dynamic models. method COMPARE static checkpointing methods. static checkpointing methods COMPARE method. tensor operation CONJUNCTION memory budget. memory budget CONJUNCTION tensor operation. dynamical method CONJUNCTION optimal static checkpointing algorithm. optimal static checkpointing algorithm CONJUNCTION dynamical method. method COMPARE static techniques. static techniques COMPARE method. static model analysis USED-FOR method. optimal Checkmate tool HYPONYM-OF static techniques. heuristics COMPARE prior arts. prior arts COMPARE heuristics. static and dynamic models EVALUATE-FOR heuristics. OtherScientificTerm are static analysis of computation graph, equal space and time cost, and tensor operation numbers. Method is linear forward network. ","This paper proposes a combination of a greedy algorithm and heuristics for checkpointing deep learning models. The model is trained on restricted GPU memory budgets. The method is evaluated on both static and dynamic models and compared to other static checkpointing methods based on static model analysis. The main contribution of the paper is a static analysis of computation graph. The authors propose a linear forward network and show that the optimal tensor operation and memory budget can be obtained with equal space and time cost. They also propose a dynamical method and the optimal static checkpointer. Finally, the authors compare the proposed method with other static techniques such as optimal Checkmate tool and show the superiority of the proposed heuristic over prior arts."
354,SP:20efc610911443724b56f57f857060d0e0302243,"machine translation CONJUNCTION summarization. summarization CONJUNCTION machine translation. summarization HYPONYM-OF tasks. machine translation HYPONYM-OF tasks. task USED-FOR faithfulness assessment.'noisified'real data CONJUNCTION pretrained LM ( BART ). pretrained LM ( BART ) CONJUNCTION'noisified'real data. XLM - R CONJUNCTION ROBERTa. ROBERTa CONJUNCTION XLM - R. synthetic classification data USED-FOR classifier.'noisified'real data USED-FOR synthetic classification data. pretrained LM ( BART ) USED-FOR synthetic classification data. XLM - R HYPONYM-OF pre - trained LM. ROBERTa HYPONYM-OF pre - trained LM. pre - trained LM USED-FOR classifier. F1 EVALUATE-FOR MT. F1 EVALUATE-FOR summarization. MT CONJUNCTION summarization. summarization CONJUNCTION MT. Task are conditional neural generation, and classification. OtherScientificTerm is hallucinations. ","This paper studies the problem of conditional neural generation. The authors consider two tasks: machine translation and summarization. The first task is used for faithfulness assessment. The second task is for classification. The classifier is trained on synthetic classification data generated from 'noisified' real data, pretrained LM (BART), and a pre-trained LM (XLM-R, ROBERTa). Experiments are conducted on MT, summarization, and F1 on MT with and without hallucinations."
355,SP:20efc610911443724b56f57f857060d0e0302243,hallucination detection USED-FOR token level. denoising pre - trained LM USED-FOR synthetic training data. edit distance USED-FOR T. edit distance USED-FOR token - level labels. token - level labels USED-FOR classification model. OtherScientificTerm is sentence level. ,This paper proposes to use hallucination detection at the token level instead of at the sentence level. The idea is to use synthetic training data generated by denoising pre-trained LM to train a synthetic model. The authors propose to use the edit distance between T and T as token-level labels to train the classification model. 
356,SP:20efc610911443724b56f57f857060d0e0302243,neural machine translation CONJUNCTION summarization. summarization CONJUNCTION neural machine translation. method USED-FOR hallucinated tokens. labeling problem USED-FOR detecting hallucinated tokens. method USED-FOR supervision data. BART model USED-FOR method. method USED-FOR labeler. BART model USED-FOR supervision data. method USED-FOR pseudo hallucinated text T '. method USED-FOR hallucination labels. edit operations USED-FOR hallucination labels. labeler USED-FOR hallucination labels. Method is sequence generation model. ,This paper tackles the labeling problem for detecting hallucinated tokens in neural machine translation and summarization. The proposed method uses the BART model to generate supervision data and then uses the method to train a labeler to generate hallucination labels. The method first generates pseudo hallucinated text T' and then performs edit operations on T' to produce hallucination tokens. The authors also propose a sequence generation model for generating hallucination examples.
357,SP:3d0d026888cf87073df5bd74edd986f15351ff5a,multiple class - aware generator architectures USED-FOR cGAN. method USED-FOR multiple class - aware generator architectures. cGAN COMPARE class - agnostic type. class - agnostic type COMPARE cGAN. NAS USED-FOR multiple class - aware generator architectures. method USED-FOR cGAN. NAS USED-FOR method. search space USED-FOR re - training. normal and class - modulated convolutions PART-OF search space. mixed - architecture optimization USED-FOR computational burden issue. mixed - architecture optimization USED-FOR multi - net search. multi - net search USED-FOR computational burden issue. Method is cGAN models. ,This paper proposes a method based on NAS to train multiple class-aware generator architectures to improve the performance of cGAN compared to the class-agnostic type. The authors propose a new search space for re-training that combines normal and class-modulated convolutions to reduce the computational burden issue caused by multi-net search. Experiments on several cGAN models are conducted to demonstrate the effectiveness of the proposed method.
358,SP:3d0d026888cf87073df5bd74edd986f15351ff5a,mixed - architecture optimization USED-FOR training data sparsity. RL - based NAS USED-FOR NAS - caGAN. Class - Modulated Convolution USED-FOR weight - sharing. weight - sharing FEATURE-OF searched architectures. NAS - caGAN COMPARE model. model COMPARE NAS - caGAN. model COMPARE cproj. cproj COMPARE model. NAS - caGAN COMPARE cproj. cproj COMPARE NAS - caGAN. CIFAR 10 EVALUATE-FOR searched class - agnostic architecture. searched class - agnostic architecture USED-FOR model. CIFAR 100 EVALUATE-FOR cproj. CIFAR 10 EVALUATE-FOR model. Method is REINFORCE algorithm. ,"This paper proposes a mixed-architecture optimization for training data sparsity. The main contribution of this paper is to propose NAS-caGAN, an RL-based NAS that leverages the REINFORCE algorithm. Specifically, the searched architectures are trained with weight-sharing via Class-Modulated Convolution. The proposed model is evaluated on CIFAR 10 and on cproj, where the proposed model outperforms the original NAS-coraGAN and a searched class-agnostic architecture. The model is also evaluated on cifAR 100."
359,SP:3d0d026888cf87073df5bd74edd986f15351ff5a,NAS USED-FOR architecture. idea USED-FOR architecture. NAS USED-FOR idea. cGAN framework USED-FOR architecture. cGAN framework USED-FOR idea. Class - Modulated convolution ( CMconv ) HYPONYM-OF operator. sampling policy USED-FOR NAS. method USED-FOR sampling policy. Markov Decision Process ( MDP ) USED-FOR search algorithm. Markov Decision Process ( MDP ) USED-FOR method. class - aware NAS COMPARE class - agnostic NAS. class - agnostic NAS COMPARE class - aware NAS. Generic is framework. ,"This paper proposes an idea based on NAS to learn an architecture using the cGAN framework. Specifically, the authors propose a new operator called Class-Modulated convolution (CMconv), which is a generalization of the existing framework. The proposed method uses a Markov Decision Process (MDP) as the search algorithm and learns a sampling policy for NAS. Experiments show that class-aware NAS outperforms class-agnostic NAS."
360,SP:8cdf6e8af07daaec6680c2bed6c1787a53580584,"deep orthogonal networks USED-FOR unconfounded treatments ( DONUT ). unconfoundedness assumption FEATURE-OF orthogonality property. orthogonality constraint USED-FOR regularization framework. Generic are approach, and estimator. ",This paper studies the problem of unconfounded treatments (DONUT) using deep orthogonal networks. The approach is based on the observation that the orthogonality property of orthogonsality property is invariant to the unconfoundingness assumption. The authors then propose a regularization framework based on orthogonomality constraint. The main contribution of the paper is the derivation of the estimator.
361,SP:8cdf6e8af07daaec6680c2bed6c1787a53580584,regularized framework USED-FOR average treatment effect. orthogonality constraint USED-FOR it. orthogonality constraint USED-FOR regularizer. orthogonality constraint USED-FOR model parameters. asymptotically normal estimator USED-FOR average causal effect. regularization USED-FOR asymptotically normal estimator. regularization framework USED-FOR estimator. estimator USED-FOR average causal effect. feedforward neural nets USED-FOR estimator. OtherScientificTerm is unconfoudedness. ,"This paper proposes a new regularized framework for estimating the average treatment effect. The proposed regularizer is based on the orthogonality constraint on the model parameters, which encourages unconfoudedness. The authors show that the proposed regularization framework leads to an asymptotically normal estimator of the average causal effect using feedforward neural nets."
362,SP:8cdf6e8af07daaec6680c2bed6c1787a53580584,"regularization term USED-FOR loss functions. loss functions USED-FOR outcome and propensity score models. OtherScientificTerm are ATE, covariates, and no hidden confounding assumption. Method are regularizer, and loss function. ",This paper proposes a new regularization term for loss functions for outcome and propensity score models. The main idea is to use ATE as a regularizer to encourage the covariates of the loss function to be close to the true covariates under no hidden confounding assumption. Experiments are conducted to show the effectiveness of the proposed regularizer.
363,SP:77ec2512837df5c0a94000602dc2ef5c03fe41dd,"BN parameters USED-FOR deep neural networks. BN parameters USED-FOR sparse pattern. random features USED-FOR function class. Generic are network, and parameters. OtherScientificTerm is sparsity pattern. ","This paper studies the effect of BN parameters on the performance of deep neural networks. In particular, the authors show that when the network is trained with random features for a function class, the parameters tend to follow a sparsity pattern."
364,SP:77ec2512837df5c0a94000602dc2ef5c03fe41dd,"BatchNorm's affine parameters USED-FOR representational power. ResNet CONJUNCTION VGG. VGG CONJUNCTION ResNet. expressiveness EVALUATE-FOR BatchNorm coefficients. expressiveness EVALUATE-FOR neural net parameters. neural net parameters USED-FOR BatchNorm coefficients. BatchNorm coefficients COMPARE network parameters. network parameters COMPARE BatchNorm coefficients. discriminative power FEATURE-OF BatchNorm coefficients. random networks COMPARE random networks. random networks COMPARE random networks. coefficients CONJUNCTION networks. networks CONJUNCTION coefficients. BatchNorm parameters USED-FOR random networks. OtherScientificTerm are randomly - initialized parameters, affine transformations, non - useful features, and overshooting. Method is non - random networks. ","This paper studies the representational power of BatchNorm's affine parameters. The authors show that the expressiveness and discriminative power of the Batch Norm coefficients can be compared to the neural net parameters, and show that ResNet and VGG with randomly-initialized parameters have similar expressiveness, but with different affine transformations. They also show that for non-random networks, the coefficients and network parameters are similar to the random networks with the same number of parameters. They further show that random networks trained with the proposed Banchor-BatchNorm parameters are more robust to non-useful features and to overshooting."
365,SP:77ec2512837df5c0a94000602dc2ef5c03fe41dd,batchnorm parameters COMPARE parameters. parameters COMPARE batchnorm parameters. parameters CONJUNCTION activations. activations CONJUNCTION parameters. OtherScientificTerm is randomly initialized parameters. ,"This paper studies the effect of randomly initialized parameters on the performance of batchnorm parameters compared to the original parameters. The authors show that when the parameters and activations are randomly initialized, the performance can be improved. "
366,SP:6683ceea773ff6d7fb613e503c583bb2979c7e89,method USED-FOR pre - trained model. affine transformations USED-FOR layer normalization parameters. Generic is model. Task is re - collection of the domain statistics. ,This paper proposes a method to re-train a pre-trained model. The idea is to use affine transformations to the layer normalization parameters and then use the learned model for the re-collection of the domain statistics.
367,SP:6683ceea773ff6d7fb613e503c583bb2979c7e89,loss USED-FOR feature modulation layer. method USED-FOR test - time entropy. ImageNet - C benchmark CONJUNCTION unsupervised domain adaptation tasks. unsupervised domain adaptation tasks CONJUNCTION ImageNet - C benchmark. unsupervised domain adaptation tasks EVALUATE-FOR method. ImageNet - C benchmark EVALUATE-FOR method. Task is fully test - time adaptation. ,This paper studies fully test-time adaptation. The authors propose a new loss for the feature modulation layer. The proposed method is able to reduce the test time entropy by a factor of 2. The method is evaluated on the ImageNet-C benchmark and unsupervised domain adaptation tasks.
368,SP:6683ceea773ff6d7fb613e503c583bb2979c7e89,algorithm USED-FOR deep models. distributionally shifted data USED-FOR deep models. channel - wise normalization CONJUNCTION transformation. transformation CONJUNCTION channel - wise normalization. algorithm USED-FOR predictive entropy. batch - norm parameters USED-FOR predictive entropy. batch - norm parameters USED-FOR algorithm. corruption benchmarks EVALUATE-FOR image classification. DIGITS recognition - based domain adaptation shifts EVALUATE-FOR approach. image classification EVALUATE-FOR approach. corruption benchmarks EVALUATE-FOR approach. Method is Test - time Entropy ( TENT ) minimization. ,"This paper proposes a new algorithm for training deep models on distributionally shifted data, named Test-time Entropy (TENT) minimization. The proposed algorithm uses batch-norm parameters to minimize the predictive entropy, which is achieved by channel-wise normalization and transformation. The approach is evaluated on corruption benchmarks for image classification and on DIGITS recognition-based domain adaptation shifts."
369,SP:ed544ee661580592063aa17aee8924cc99919130,approach USED-FOR uncertainty modeling. recurrent neural networks USED-FOR approach. recurrent neural networks USED-FOR uncertainty modeling. discrete hidden state USED-FOR approach. Gumbel - Softmax trick USED-FOR reparameterizable approximation. reparameterizable approximation USED-FOR discrete model. out of distribution detection CONJUNCTION calibration. calibration CONJUNCTION out of distribution detection. calibration USED-FOR classification tasks. method USED-FOR problems. ,This paper presents an approach to uncertainty modeling using recurrent neural networks. The approach is based on a discrete hidden state. The discrete model is trained using a reparameterizable approximation based on the Gumbel-Softmax trick. The method is applied to two problems: out of distribution detection and calibration for classification tasks.
370,SP:ed544ee661580592063aa17aee8924cc99919130,method USED-FOR recurrent neural networks. model USED-FOR probability distribution. hidden state PART-OF RNN. discrete hidden states USED-FOR probability distribution. hidden state USED-FOR probability distribution. Gumbel softmax trick USED-FOR method. MC gradient estimation USED-FOR method. temperature parameter USED-FOR concentration of state transition distribution. sequential prediction problems EVALUATE-FOR uncertainty estimation method. reinforcement learning task HYPONYM-OF sequential prediction problems. ,"This paper proposes a method for estimating the uncertainty of recurrent neural networks. The proposed model estimates the probability distribution over discrete hidden states in an RNN. The method is based on the Gumbel softmax trick and uses MC gradient estimation. The concentration of state transition distribution is estimated using a temperature parameter. The uncertainty estimation method is evaluated on sequential prediction problems (e.g., reinforcement learning task)."
371,SP:ed544ee661580592063aa17aee8924cc99919130,method USED-FOR RNN. Bayesian RNN COMPARE method. method COMPARE Bayesian RNN. hidden state CONJUNCTION memory. memory CONJUNCTION hidden state. state transition paths FEATURE-OF probability   distribution. transition probability FEATURE-OF probability   distribution. Gumbel softmax function USED-FOR probability   distribution. hyper - parameter tau FEATURE-OF Gumbel function. OtherScientificTerm is sample variance. ,"This paper proposes a method for training an RNN that is more efficient than a Bayesian RNN. The main idea is to learn a probability  distribution over the state transition paths, which is then used to estimate the transition probability of a given state. The authors propose to use a Gumbel softmax function with a hyper-parameter tau to approximate the probability  of the transition, and to minimize the sample variance. The proposed method is shown to outperform the existing method in terms of both hidden state and memory."
372,SP:a38c523196f68a90b5db45671f9dbd87981a024c,"ODE USED-FOR it. reversibility FEATURE-OF ODE. SDE USED-FOR model. this USED-FOR DP - algorithms. addition multiplicative noise FEATURE-OF additive noise. addition multiplicative noise USED-FOR second strategy. SDE USED-FOR strategy. methods USED-FOR membership inference attack. Task is differentially private deep learning. Method are deep residual learning, and residual learning. ","This paper studies the problem of differentially private deep learning. In particular, it studies the reversibility of the ODE used to train it. The authors propose a new strategy based on the SDE of the model, which they call deep residual learning. They show that this can be used to attack DP-algorithms. The second strategy is based on adding addition multiplicative noise to the additive noise. They also show that their methods can be applied to the membership inference attack. "
373,SP:a38c523196f68a90b5db45671f9dbd87981a024c,method USED-FOR ResNets. differential privacy FEATURE-OF method. noise USED-FOR training. noise USED-FOR network. noisy gradient descent USED-FOR methods. one CONJUNCTION one. one CONJUNCTION one. differential privacy guarantees FEATURE-OF strategies. one HYPONYM-OF strategies. one HYPONYM-OF strategies. Rademacher complexity EVALUATE-FOR model. noise USED-FOR generalization. ,"This paper proposes a method for ResNets with differential privacy. The proposed methods are based on noisy gradient descent, where noise is added to the network during training. Two strategies are proposed to achieve differential privacy guarantees: one based on the Rademacher complexity of the model, and one that is based on a modified version of the one proposed in [1]. The authors show that adding noise during training improves generalization."
374,SP:a38c523196f68a90b5db45671f9dbd87981a024c,residual perturbation USED-FOR ResNet models ’ utility. privacy protection CONJUNCTION classification accuracy. classification accuracy CONJUNCTION privacy protection. benchmark datasets EVALUATE-FOR classification accuracy. benchmark datasets EVALUATE-FOR privacy protection. utility enhancement CONJUNCTION DP guarantee. DP guarantee CONJUNCTION utility enhancement. Method is SDE models. ,"This paper proposes to enhance ResNet models’ utility with residual perturbation. The main contribution of this paper is to study the effect of privacy protection and classification accuracy on benchmark datasets on the trade-off between privacy protection, classification accuracy, and utility enhancement and DP guarantee. The paper is well-written and well-motivated. The experiments are conducted on SDE models and the results are promising."
375,SP:9cbe32c1317889d6a3ec1b0798112d9b82cc7f67,"model USED-FOR inference scenarios. sandwich rule USED-FOR model. sandwich rule USED-FOR largest model. sandwich rule USED-FOR smallest model. largest model CONJUNCTION smallest model. smallest model CONJUNCTION largest model. evolutionary search USED-FOR length configuration. model USED-FOR token annotation tasks. Length - Adaptive Transformer COMPARE baseline models. baseline models COMPARE Length - Adaptive Transformer. latency level EVALUATE-FOR baseline models. latency level EVALUATE-FOR Length - Adaptive Transformer. Method are LengthDrop method, randomly sampled models, and Drop - and - Restore process. Task is inference phase. Metric is latency tradeoff. ","This paper proposes a new model for inference scenarios where the model is trained with a sandwich rule for the largest model and for the smallest model. The authors propose a LengthDrop method, which is based on the idea of using randomly sampled models. During the inference phase, the authors use evolutionary search to find the optimal length configuration for each model, and then use the Drop-and-Restore process to recover the weights of the two models. The proposed Length-Adaptive Transformer is evaluated on token annotation tasks and compared with baseline models on the latency level. The latency tradeoff is shown to be a tradeoff between the performance and the number of parameters."
376,SP:9cbe32c1317889d6a3ec1b0798112d9b82cc7f67,method USED-FOR Length - Adaptive Transformer. Length - Adaptive Transformer USED-FOR adaptive model architecture. latency constraints USED-FOR adaptive model architecture. model USED-FOR variable input lengths. method USED-FOR model. evolutionary search USED-FOR method. Method is LengthDrop. Metric is model accuracy. OtherScientificTerm is latency budget. ,"This paper proposes a method called Length-Adaptive Transformer to learn an adaptive model architecture with latency constraints. The proposed method is based on evolutionary search, where the model is trained to adapt to variable input lengths. The authors also propose a variant of LengthDrop to improve the model accuracy by reducing the latency budget."
377,SP:9cbe32c1317889d6a3ec1b0798112d9b82cc7f67,LengthDrop USED-FOR length reduction. techniques USED-FOR training. techniques USED-FOR NAS. techniques USED-FOR adaptive drop ratio search. one - shot NAS HYPONYM-OF techniques. method COMPARE BERT - base model. BERT - base model COMPARE method. BERT - base model USED-FOR inference. 1/3 - 1/2 FLOPs USED-FOR inference. 1/3 - 1/2 FLOPs FEATURE-OF BERT - base model. Task is Transformers. OtherScientificTerm is sequence length. ,"This paper proposes a new method called LengthDrop for length reduction in Transformers, which aims to reduce the sequence length. The proposed techniques are similar to one-shot NAS, which is used for training and for NAS, but for adaptive drop ratio search. The experiments show that the proposed method outperforms the BERT-base model with 1/3-1/2 FLOPs for inference."
378,SP:e5b4098ea22a5da2b9659219dc24f885c493a011,aggregators USED-FOR GNN. low - rank transformations FEATURE-OF aggregators. OtherScientificTerm is rank of hidden features. ,This paper studies the problem of learning aggregators for GNN with low-rank transformations. The authors show that the rank of hidden features can be reduced to zero when the number of transformations is small. The paper also provides a theoretical analysis of the effect of these changes.
379,SP:e5b4098ea22a5da2b9659219dc24f885c493a011,"layers USED-FOR GNNs. CombConv CONJUNCTION ExpandingConv. ExpandingConv CONJUNCTION CombConv. steps PART-OF GNNs. aggregation of the neighbourhood HYPONYM-OF steps. generation of aggregation coefficients HYPONYM-OF steps. feature extraction HYPONYM-OF steps. aggregation USED-FOR feature extraction. CombConv CONJUNCTION ExpandingConv. ExpandingConv CONJUNCTION CombConv. distinguishing strength EVALUATE-FOR approaches. expressive power EVALUATE-FOR CombConv. expressive power EVALUATE-FOR ExpandingConv. Method is GNN. OtherScientificTerm are aggregation function, and aggregation coefficients. ","This paper proposes two new layers for GNNs. CombConv and ExpandingConv are two variants of GNN with two different steps: (1) aggregation of the neighbourhood and (2) generation of aggregation coefficients, which are two steps in the original GNN. The aggregation function is defined as the sum of the aggregation coefficients of the neighbours of the original neighbourhood and the neighbourhood of the neighbouring neighbourhood. This aggregation is used for feature extraction, where the aggregation is applied to the feature extraction. The authors compare their approaches on distinguishing strength and expressive power, and show that the proposed approaches have better expressive power."
380,SP:e5b4098ea22a5da2b9659219dc24f885c493a011,graph neural networks USED-FOR representation power. aggregator functions USED-FOR aggregation coefficient matrix. aggregators PART-OF representation power. rank - preservation requirement FEATURE-OF aggregators. OtherScientificTerm is aggregation functions. ,This paper studies the representation power of graph neural networks. The authors propose to incorporate aggregators into representation power to satisfy the rank-preservation requirement. The aggregation coefficient matrix is defined as the sum of the aggregator functions that maximize the aggregation coefficients matrix. The paper also provides a theoretical analysis of the aggregation functions.
381,SP:4dd6fb8e5a356af270d3b296ce3d50ae5753513c,"disentanglement metric EVALUATE-FOR generative model. manifold M USED-FOR disentangled generative model. sub - manifolds PART-OF manifold M. model EVALUATE-FOR disentanglement. topological similarity FEATURE-OF disentanglement. Wasserstein Relative Living Times USED-FOR topological similarity. metric EVALUATE-FOR disentanglement methods. OtherScientificTerm are factor of variation, disentangled factors, manifolds, and submanifolds. Method are entangled model, and model ’s disentanglement. ","This paper proposes a disentanglement metric for evaluating a generative model on a manifold M, which is composed of sub-manifolds. The key idea is to measure the topological similarity between the disentangled factors of the manifold M and the factor of variation between the manifold and the entangled model. The authors show that the model’s performance on this metric can be used to evaluate other state-of-the-art disentangling methods.  The authors also provide a theoretical analysis of the relationship between the model and the topology of the manifolds, showing that the relation between the two manifolds can be expressed as a function of the number of layers and the dimension of the submanifold.  Finally, the authors provide an empirical study of the effect of the Wasserstein Relative Living Times on the performance of the model in terms of the topologically similarity of the generated data. "
382,SP:4dd6fb8e5a356af270d3b296ce3d50ae5753513c,unsupervised disentangling metric USED-FOR homeomorphic similarity. homeomorphic dissimilarity FEATURE-OF submanifolds. unsupervised disentangling metric USED-FOR homeomorphic dissimilarity. supervised variant USED-FOR topological similarity of submanifolds. label - spaces FEATURE-OF topological similarity of submanifolds. wasserstein distance COMPARE euclidean distance. euclidean distance COMPARE wasserstein distance. euclidean distance USED-FOR variation of RLTs. wasserstein distance USED-FOR variation of RLTs. ,This paper proposes an unsupervised disentangling metric for measuring the homeomorphic dissimilarity of submanifolds. The supervised variant is designed to measure the topological similarity of subfolders in label-spaces. The wasserstein distance is compared to the euclidean distance for a variation of RLTs.
383,SP:4dd6fb8e5a356af270d3b296ce3d50ae5753513c,metric EVALUATE-FOR evaluating disentanglement. manifold - topological perspective USED-FOR metric. metric USED-FOR disentangling. Method is disentangled representation. OtherScientificTerm is conditional sub - manifolds. ,This paper proposes a new metric for evaluating disentanglement from a manifold-topological perspective. The proposed metric is motivated by the fact that disentangling is difficult in the sense that the disentangled representation can be seen as a set of conditional sub-manifolds. 
384,SP:ef1ee7b77e1c2fb3d76db27049a3bce42760d14e,data poisoning CONJUNCTION adversarial attacks. adversarial attacks CONJUNCTION data poisoning. ones CONJUNCTION data poisoning. data poisoning CONJUNCTION ones. ones CONJUNCTION adversarial attacks. adversarial attacks CONJUNCTION ones. Projected Gradient Descent USED-FOR loss function. pretrained network USED-FOR adversarial attack benchmarks. clean data USED-FOR models. Task is protecting private data. OtherScientificTerm is adversarial samples. ,"This paper studies the problem of protecting private data from adversarial samples. The authors propose a new loss function based on Projected Gradient Descent, which can be applied to defend against data poisoning, adversarial attacks, etc. Experiments are conducted on several adversarial attack benchmarks using a pretrained network. The results show that models trained on clean data can be more robust to adversarial adversarially generated samples."
385,SP:ef1ee7b77e1c2fb3d76db27049a3bce42760d14e,"invisible noise USED-FOR personal data. min - min optimization method USED-FOR error - minimizing noise. Method is deep learning models. Generic are idea, and method. ","This paper proposes a min-min optimization method for error-minimizing noise in deep learning models. The idea is interesting, and the method is well-motivated. However, it is not clear to me how this idea can be applied to personal data with invisible noise."
386,SP:ef1ee7b77e1c2fb3d76db27049a3bce42760d14e,"real world task of face recognition EVALUATE-FOR it. Task is data protection. OtherScientificTerm are error - minimizing noise, and normal data utility. Generic is noise. ",This paper addresses the problem of data protection by adding error-minimizing noise to the training data. The authors argue that this noise is harmful to the normal data utility and propose to add it to the real world task of face recognition.
387,SP:4e8a835174f20df36d3d8d27fbcbbf2c68490032,MuZero USED-FOR nondeterministic domains ( NDMZ ). MuZero COMPARE NDMZ. NDMZ COMPARE MuZero. function CONJUNCTION distribution of chance outcomes. distribution of chance outcomes CONJUNCTION function. function USED-FOR NDMZ. MCTS search USED-FOR nondeterministic nodes. MCTS search USED-FOR tree. tree USED-FOR nondeterministic nodes. NDMZ's neural nets USED-FOR tree. ,"This paper proposes MuZero for nondeterministic domains (NDMZ). Compared to NDMZ, MuZero is able to generalize to more diverse domains than NDMD. The main difference is that MuZero does not require a function or a distribution of chance outcomes, which is not the case in NDMZ. The authors propose to learn a tree by MCTS search on the NDM Z's neural nets to find the nondeterminist nodes. "
388,SP:4e8a835174f20df36d3d8d27fbcbbf2c68490032,MuZero algorithm USED-FOR NDMZ. player identity policy CONJUNCTION chance player policy. chance player policy CONJUNCTION player identity policy. node classes USED-FOR MCTS. it USED-FOR chance. ,This paper proposes a MuZero algorithm for NDMZ. The main idea is to replace the player identity policy with a chance player policy. The idea is that MCTS can be trained with different node classes and that it can be used to estimate the chance.
389,SP:4e8a835174f20df36d3d8d27fbcbbf2c68490032,deep reinforcement learning algorithm USED-FOR model - based RL. nondeterministic MuZero HYPONYM-OF deep reinforcement learning algorithm. NDMZ HYPONYM-OF deep reinforcement learning algorithm. rules of the game USED-FOR search. NDMZ USED-FOR nondeterministic MuZero. die rolls HYPONYM-OF random events. Generic is algorithm. OtherScientificTerm is physical ) board game. ,"This paper proposes a deep reinforcement learning algorithm for model-based RL called nondeterministic MuZero, which is a variant of NDMZ. The algorithm is based on a (physical) board game where the goal is to find a sequence of random events (e.g. die rolls). The search is done using the rules of the game. The authors show that the NDMZ can be used to solve nondeterministically MuZero."
390,SP:73ae9c167dac3d92788a08891b0831f3e4997140,"HO2 HYPONYM-OF option - learning policy gradient method. method USED-FOR parameterized joint distribution. soft - continuation based approach USED-FOR method. robotic simulation tasks EVALUATE-FOR algorithm. 3D virtualized environments EVALUATE-FOR algorithm. OtherScientificTerm are option termination, and meta - parameter. Method are option - learning method, and softer loss penalization based approaches. ","This paper proposes HO2, an option-learning policy gradient method. The proposed method is based on a soft-continuation based approach, where the parameterized joint distribution of the option and the option termination is learned jointly. The method is motivated by the fact that the option-learning method has been shown to perform poorly in the past, and that softer loss penalization based approaches may not be effective. The authors propose to use a meta-parameter to penalize the loss of the policy that does not depend on the option. The algorithm is evaluated on 3D virtualized environments and on robotic simulation tasks."
391,SP:73ae9c167dac3d92788a08891b0831f3e4997140,it USED-FOR agent. Task is Hierarchical Reinforcement Learning setting. Generic is algorithm. Material is off - policy samples. OtherScientificTerm is policies. ,"This paper studies the Hierarchical Reinforcement Learning setting and proposes an algorithm that learns a set of off-policy samples, and then uses it to train an agent that is able to generalize to unseen environments. The idea is interesting and interesting. However, it is not clear to me how the proposed algorithm can be applied to the real world, and how the learned policies can be used in practice."
392,SP:73ae9c167dac3d92788a08891b0831f3e4997140,area PART-OF RL. hierarchical RL HYPONYM-OF RL. data efficiency EVALUATE-FOR hierarchical RL. TD(0 ) type objective USED-FOR option learning algorithm. action abstraction CONJUNCTION temporal abstraction. temporal abstraction CONJUNCTION action abstraction. mixture policy USED-FOR action abstraction. OtherScientificTerm is abstractions. ,"This paper proposes a new area of RL called hierarchical RL, which aims to improve data efficiency. The authors propose an option learning algorithm based on a TD(0) type objective, where the abstractions are a mixture of action abstraction and temporal abstraction. The main contribution of this paper is to propose a mixture policy for action abstraction."
393,SP:f79d9722256fb6b258bc1310bf1f6fb842303a0a,modified bellman equation USED-FOR reinforcement learning. modified bellman equation USED-FOR maximum expected single step reward. maximum expected single step reward COMPARE maximum cumulative reward. maximum cumulative reward COMPARE maximum expected single step reward. formulation USED-FOR generation of molecules. ( predicted ) chemical reactions of building blocks USED-FOR molecule generation algorithm. bellman formulation USED-FOR molecule generation algorithm. OtherScientificTerm is HIV activity targets. ,This paper proposes a modified bellman equation for reinforcement learning that maximizes the maximum expected single step reward instead of the maximum cumulative reward. The formulation is applied to the generation of molecules from (predicted) chemical reactions of building blocks. The molecule generation algorithm is based on the bellman formulation and is trained on HIV activity targets.
394,SP:f79d9722256fb6b258bc1310bf1f6fb842303a0a,objective USED-FOR reinforcement learning. expected maximum COMPARE accumulated reward. accumulated reward COMPARE expected maximum. objective USED-FOR expected maximum. monotonicity CONJUNCTION contraction. contraction CONJUNCTION monotonicity. Q - learning COMPARE Max - Q algorithm. Max - Q algorithm COMPARE Q - learning. Max - Q algorithm USED-FOR maximum rewards. TD target PART-OF PGFS algorithm. TD target USED-FOR de novo drug design task. Task is de novo drug design. Method is Bellman operator. OtherScientificTerm is simulated grid. Generic is variant. ,This paper proposes an objective for reinforcement learning that aims to maximize the expected maximum instead of the accumulated reward. The main idea is to replace the Bellman operator with a variant that encourages monotonicity and contraction. The authors show that Q-learning outperforms the Max-Q algorithm in terms of maximum rewards. They also show that the TD target of the PGFS algorithm can be used as a TD target in a de novo drug design task. The experiments are conducted on a simulated grid.
395,SP:f79d9722256fb6b258bc1310bf1f6fb842303a0a,"max reward USED-FOR reinforcement learning. cumulative reward objective USED-FOR reinforcement learning. max reward COMPARE cumulative reward objective. cumulative reward objective COMPARE max reward. objective USED-FOR applications. chemical synthesis HYPONYM-OF applications. varaint FEATURE-OF Bellman operator. max - Bellman operator HYPONYM-OF Bellman operator. gridworld CONJUNCTION simulated chemical synthesis. simulated chemical synthesis CONJUNCTION gridworld. objective modification COMPARE prior algorithms. prior algorithms COMPARE objective modification. Method are RL agent, and contraction argument. ",This paper proposes to replace the standard max reward in reinforcement learning with a cumulative reward objective. This objective can be applied to a variety of applications such as chemical synthesis. The main contribution of this paper is to propose a variant of the standard Bellman operator (max-Bellman operator) that has a varaint that depends on the state of the RL agent. The authors provide a contraction argument to show that this contraction argument holds. Experiments on gridworld and simulated chemical synthesis show that the proposed objective modification outperforms prior algorithms.
396,SP:bd4b1781448def4327214c78f07538d285119ef9,"few - shot meta - learning method USED-FOR recommender system. medical synthetic dataset CONJUNCTION e - learning dataset. e - learning dataset CONJUNCTION medical synthetic dataset. method COMPARE baselines. baselines COMPARE method. MovieLens-1 M CONJUNCTION medical synthetic dataset. medical synthetic dataset CONJUNCTION MovieLens-1 M. e - learning dataset EVALUATE-FOR method. medical synthetic dataset EVALUATE-FOR method. e - learning dataset EVALUATE-FOR baselines. medical synthetic dataset EVALUATE-FOR baselines. MovieLens-1 M EVALUATE-FOR method. MovieLens-1 M EVALUATE-FOR baselines. OtherScientificTerm are feature's meta - information, features, and network weights. Task is cold - start problem. ","This paper proposes a few-shot meta-learning method for recommender system. The key idea is to use the feature's meta-information to guide the training of the network weights. This is an important step towards solving the cold-start problem. The proposed method is evaluated on MovieLens-1M, a medical synthetic dataset and an e-learning dataset and compared with several baselines."
397,SP:bd4b1781448def4327214c78f07538d285119ef9,Contextual HyperNetworks ( CHNs ) USED-FOR auxiliary model. CHN USED-FOR P - VAE. recommender system CONJUNCTION e - learning and healthcare tasks. e - learning and healthcare tasks CONJUNCTION recommender system. CHN USED-FOR application. e - learning and healthcare tasks HYPONYM-OF application. recommender system HYPONYM-OF application. Generic is it. ,"This paper proposes Contextual HyperNetworks (CHNs) as an auxiliary model to augment P-VAE. The main idea is to use CHN as the auxiliary model and use it as a proxy for the original model. The proposed CHN can be applied to any application such as recommender system, e-learning and healthcare tasks. "
398,SP:bd4b1781448def4327214c78f07538d285119ef9,"neural network USED-FOR It. intuitive naïve strategies CONJUNCTION MAML. MAML CONJUNCTION intuitive naïve strategies. method COMPARE intuitive naïve strategies. intuitive naïve strategies COMPARE method. method COMPARE MAML. MAML COMPARE method. Task are cold start problem, recommender system, and medical application. ",This paper studies the cold start problem. It uses a neural network to predict the next state. The authors compare the proposed method with intuitive naïve strategies and MAML. The results show that the proposed recommender system is more effective. The paper also shows some interesting results in medical application.
399,SP:8e4677cc6071a33397347679308165c10dca2aae,"method USED-FOR Bayesian deep learning. method USED-FOR posterior inference. deep neural network models FEATURE-OF posterior inference. generalized Gauss - Newton Hessian approximation USED-FOR covariance. Laplace approximation CONJUNCTION generalized Gauss - Newton Hessian approximation. generalized Gauss - Newton Hessian approximation CONJUNCTION Laplace approximation. prediction accuracy CONJUNCTION uncertainty quantification. uncertainty quantification CONJUNCTION prediction accuracy. uncertainty quantification EVALUATE-FOR method. prediction accuracy EVALUATE-FOR method. OtherScientificTerm are neural network's parameters, and full - covariance approximate posterior. ","This paper proposes a method for Bayesian deep learning. The proposed method aims to improve the posterior inference in deep neural network models. The main idea is to replace the Laplace approximation with a generalized Gauss-Newton Hessian approximation to the covariance of the neural network's parameters. The authors show that the proposed method can improve the prediction accuracy and uncertainty quantification. In addition, the authors also show that a full-covariance approximate posterior can be obtained."
400,SP:8e4677cc6071a33397347679308165c10dca2aae,"Bayesian neural network USED-FOR posterior distribution. approximation USED-FOR posterior distribution. deterministic component PART-OF approximation. sub - network USED-FOR high fidelity posterior approximations. Method are sub network, and MAP point estimation. OtherScientificTerm are approximate posterior distributions, and restrictive mean field assumptions. ","This paper proposes a Bayesian neural network that approximates the posterior distribution of a posterior distribution using an approximation that includes a deterministic component. The authors show that this sub network can be used to approximate high fidelity posterior approximations. The main contribution of this paper is the derivation of MAP point estimation, which is an extension of the work of [1] and [2] that uses a sub network to approximate approximate posterior distributions under restrictive mean field assumptions. "
401,SP:8e4677cc6071a33397347679308165c10dca2aae,Bayesian NNs USED-FOR scalable approximate inference. method USED-FOR scalable BNNs. ( Wasserstein - based ) pruned subnetwork PART-OF deterministically - trained model. ( Wasserstein - based ) pruned subnetwork USED-FOR full - covariance Gaussian ) Laplace approximation. full - covariance Gaussian ) Laplace approximation USED-FOR method. full - covariance Gaussian ) Laplace approximation USED-FOR scalable BNNs. 1D regression CONJUNCTION tabular regression. tabular regression CONJUNCTION 1D regression. tabular regression CONJUNCTION larger - scale image classification. larger - scale image classification CONJUNCTION tabular regression. dataset shift setup USED-FOR larger - scale image classification. CIFAR-10 USED-FOR larger - scale image classification. method COMPARE methods. methods COMPARE method. method USED-FOR in - between uncertainty. deep ensembles HYPONYM-OF methods. Method is generalized linear model. ,"This paper proposes a method for scalable approximate inference with Bayesian NNs. The proposed method uses a (Wasserstein-based) pruned subnetwork of a deterministically-trained model to approximate a (full-covariance Gaussian) Laplace approximation for scalable BNNs. Experiments are conducted on 1D regression, tabular regression, and larger-scale image classification on CIFAR-10 under a dataset shift setup. Results show that the proposed method is able to approximate in-between uncertainty better than other methods such as deep ensembles. The authors also propose a generalized linear model that can be used to approximate the true uncertainty."
402,SP:be361952fe9de545f68b8a060f790d54c6755998,embeddings USED-FOR parameterized control policy. framework USED-FOR state and action embedding. policy gradient ( PG ) methods USED-FOR parameterized control policy. model of the environment USED-FOR framework. framework USED-FOR internal ( embedding ) policy. internal ( embedding ) policy CONJUNCTION state embedding. state embedding CONJUNCTION internal ( embedding ) policy. internal policy USED-FOR optimal overall policy. Method is Joint learning of state and action embeddings. OtherScientificTerm is overall policy. ,This paper proposes a framework for learning state and action embedding for a parameterized control policy using policy gradient (PG) methods. The proposed framework is based on a model of the environment and is able to learn both an internal (embedding) policy and a state embedding. Joint learning of state andaction embeddings can be used to learn an optimal overall policy.
403,SP:be361952fe9de545f68b8a060f790d54c6755998,latent state embedding CONJUNCTION latent action embedding. latent action embedding CONJUNCTION latent state embedding. state transition model CONJUNCTION RL policy. RL policy CONJUNCTION state transition model. latent action embedding CONJUNCTION state transition model. state transition model CONJUNCTION latent action embedding. method USED-FOR latent state embedding. latent models USED-FOR learning. learning USED-FOR discrete action domains. recommender system CONJUNCTION half - cheetah locomotion. half - cheetah locomotion CONJUNCTION recommender system. slot machine task CONJUNCTION recommender system. recommender system CONJUNCTION slot machine task. grid - world task CONJUNCTION slot machine task. slot machine task CONJUNCTION grid - world task. method COMPARE vanilla policy gradient. vanilla policy gradient COMPARE method. vanilla policy gradient USED-FOR grid - world task. grid - world task EVALUATE-FOR method. ,"This paper proposes a method for learning a latent state embedding, a latent action embedding and a state transition model, and an RL policy. The idea is to use latent models for learning in discrete action domains. Experiments are conducted on a grid-world task, a slot machine task, recommender system, and half-cheetah locomotion. The proposed method outperforms the vanilla policy gradient."
404,SP:be361952fe9de545f68b8a060f790d54c6755998,"embedded state - action space COMPARE state - action space. state - action space COMPARE embedded state - action space. embedded state - action space USED-FOR internal policy(\pi_i ). embedding model USED-FOR mapping from state to state embedding. internal policy ( \pi_i ) USED-FOR overall policy ( \pi_o ). internal state - action - value function CONJUNCTION overall state - action - value function. overall state - action - value function CONJUNCTION internal state - action - value function. OtherScientificTerm are state - action spaces, action embedding, action space, and \pi_o. Method are joint state - action embedding, and internal policy. Task is learning. Generic are mapping, and approach. ","This paper proposes to learn an internal policy(\pi_i) in an embedded state-action space instead of a state-actions space. The embedding model is trained to learn a mapping from state to state embedding. This mapping is then used to train a joint policy (i.e., \pi_o) and overall policy (\pi_y) which is trained on the joint state-active embedding and on the learned internal policy. The learning is done in two stages: first, an action embedding is learned, and then the internal policy is learned on top of the learned action space. Finally, the learning is repeated until the learned joint policy and the overall policy converge. The approach is evaluated on a variety of datasets."
405,SP:ebb6bffcc4c2129e09ef5561c19df43c42ad18c0,domain expertise USED-FOR Learning representations. self - supervision USED-FOR Learning representations. adversarial strategy USED-FOR transformations. OtherScientificTerm is data modalities. Method is self - supervision encoder. ,Learning representations with self-supervision requires domain expertise. This paper proposes an adversarial strategy to generate transformations that are robust to different data modalities. The key idea is to train a self-subservience encoder to generate the transformations.
406,SP:ebb6bffcc4c2129e09ef5561c19df43c42ad18c0,generative model USED-FOR contrastive learning. speech CONJUNCTION wearable sensor data. wearable sensor data CONJUNCTION speech. image CONJUNCTION speech. speech CONJUNCTION image. model COMPARE data augmentation methods. data augmentation methods COMPARE model. human domain knowledge USED-FOR data augmentation methods. Method is SimCLR framework. Generic is method. ,"This paper proposes a generative model for contrastive learning. The SimCLR framework is used to train the method. Experiments are conducted on image, speech, and wearable sensor data. The proposed model outperforms existing data augmentation methods based on human domain knowledge."
407,SP:ebb6bffcc4c2129e09ef5561c19df43c42ad18c0,"method USED-FOR automatic generation of data views. automatic generation of data views USED-FOR contrastive self - supervised learning of representations. adversarial perturbation model PART-OF method. $ l_p$ norm FEATURE-OF added noise. added noise USED-FOR perturbation strength. $ l_p$ norm USED-FOR perturbation strength. approach COMPARE methods. methods COMPARE approach. image, speech and wearable sensor datasets EVALUATE-FOR method. OtherScientificTerm are perturbed views, and collapse. Method are learned representations, and information - destroying perturbation model. ","This paper proposes a method for automatic generation of data views for contrastive self-supervised learning of representations. The proposed method consists of an adversarial perturbation model that generates perturbed views, which are then used to train the learned representations. In particular, the authors propose to use the added noise with a $l_p$ norm to estimate the perturbance strength. The authors also propose an information-destroying perturbator model to prevent collapse. Experiments on image, speech and wearable sensor datasets show that the proposed approach outperforms existing methods."
408,SP:ef7735be9423ad53059505c170e75201ca134573,"integrated approach USED-FOR OODs. OOD detection methods USED-FOR OODs. Method is taxonomy of OODs. Generic are taxonomy, and approach. ",This paper proposes an integrated approach to detect OODs. The main idea is to propose a taxonomy to classify the types of OOD. This taxonomy is then used to train OOD detection methods. The approach is evaluated on a variety of datasets.
409,SP:ef7735be9423ad53059505c170e75201ca134573,"taxonomy USED-FOR OOD outliers. SVNH CONJUNCTION MNIST. MNIST CONJUNCTION SVNH. MNIST CONJUNCTION STL10. STL10 CONJUNCTION MNIST. STL10 CONJUNCTION ImageNet. ImageNet CONJUNCTION STL10. approaches PART-OF one. ImageNet HYPONYM-OF data sets. SVNH HYPONYM-OF data sets. STL10 HYPONYM-OF data sets. MNIST HYPONYM-OF data sets. data sets EVALUATE-FOR it. Method are OOD detection approaches, and integrated OOD detection approach. ","This paper proposes a new taxonomy for OOD outliers. The authors propose two OOD detection approaches and combine the two approaches into one. They evaluate it on several data sets (SVNH, MNIST, STL10, and ImageNet) and show that the integrated OOD detector approach is effective."
410,SP:ef7735be9423ad53059505c170e75201ca134573,"methods USED-FOR OOD. synthetic data USED-FOR outlier. OOD detection rate EVALUATE-FOR methods. deep neural networks USED-FOR images classification datasets. combination method COMPARE baseline methods. baseline methods COMPARE combination method. detection rates EVALUATE-FOR baseline methods. detection rates EVALUATE-FOR combination method. OtherScientificTerm is outliers. Generic are one, and dataset. ","This paper proposes two methods for OOD. The first one is based on training deep neural networks on standard images classification datasets, while the second one uses synthetic data to detect an outlier. The experiments show that the proposed combination method can achieve better detection rates than baseline methods. The authors also show that outliers are more likely to be found on the second dataset."
411,SP:33920ec7ffefe3c1525cd5d4d53a851210d519da,VAEs USED-FOR SOTA bpds. architecture COMPARE U - net architecture. U - net architecture COMPARE architecture. ImageNet-32 CONJUNCTION ImageNet-64. ImageNet-64 CONJUNCTION ImageNet-32. CIFAR-10 CONJUNCTION ImageNet-32. ImageNet-32 CONJUNCTION CIFAR-10. CIFAR-10 EVALUATE-FOR SOTA bpds. stochastic layers USED-FOR VAEs. Generic is model. ,"This paper proposes VAEs to improve the SOTA bpds on CIFAR-10, ImageNet-32, and imageNet-64 using stochastic layers. The proposed architecture is shown to outperform the standard U-net architecture by a large margin. The paper also provides a theoretical analysis of the proposed model."
412,SP:33920ec7ffefe3c1525cd5d4d53a851210d519da,deep - enough VAE COMPARE autoregressive models. autoregressive models COMPARE deep - enough VAE. autoregressive architectures USED-FOR VAE model. Task is image generation. ,This paper studies the problem of image generation and proposes a deep-enough VAE that is more efficient than autoregressive models. The main contribution of this paper is to propose a VAE model that can be trained on top of existing autorgressive architectures. The paper is well-written and easy to follow.
413,SP:33920ec7ffefe3c1525cd5d4d53a851210d519da,deep hierarchical VAEs COMPARE autoregressive models. autoregressive models COMPARE deep hierarchical VAEs. images EVALUATE-FOR deep hierarchical VAEs. images EVALUATE-FOR autoregressive models. autoregressive models HYPONYM-OF hierarchical VAEs. hierarchical VAEs HYPONYM-OF universal approximators. gradient skipping CONJUNCTION prior warmup. prior warmup CONJUNCTION gradient skipping. freebits CONJUNCTION KL annealing. KL annealing CONJUNCTION freebits. freebits USED-FOR model. KL annealing USED-FOR model. Method is top - down ( LVAE ) architecture. Task is likelihood. Material is image datasets. ,"This paper proposes a new top-down (LVAE) architecture that aims to improve the likelihood of deep hierarchical VAEs compared to autoregressive models on images. In particular, the authors propose a universal approximators of the likelihood, which they call hierarchical VAE, in contrast to hierarchicalVAEs, which are usually universal approximation methods. The authors propose to use freebits and KL annealing to train the model, which is a combination of gradient skipping and prior warmup. Experiments are conducted on several image datasets."
414,SP:0a4e6c8017a1294fe2424799a0048d58eaf04cb3,"sampling strategy USED-FOR metric learning. semi - hard negative mining HYPONYM-OF sampling strategy. metric learning USED-FOR contrastive self - supervised learning. semi - hard negative mining USED-FOR contrastive self - supervised learning. sampling strategy USED-FOR contrastive self - supervised learning. IR CONJUNCTION CMC. CMC CONJUNCTION IR. sampling strategy USED-FOR contrastive learning methods. IR HYPONYM-OF contrastive learning methods. CMC HYPONYM-OF contrastive learning methods. OtherScientificTerm are negative samples, and percentile range. Metric is normalized feature distance. ","This paper proposes a new sampling strategy for metric learning in contrastive self-supervised learning, called semi-hard negative mining. The key idea is to sample negative samples from the percentile range of the normalized feature distance. This sampling strategy is applied to two popular contrastive learning methods, IR and CMC."
415,SP:0a4e6c8017a1294fe2424799a0048d58eaf04cb3,negative sample mining USED-FOR visual representation learning. dot product of representations USED-FOR noise constructive estimation ( NCE ). method USED-FOR negative samples. dot product of representations USED-FOR method. conditional distribution q USED-FOR NCE. method USED-FOR conditional distribution. ring surface USED-FOR method. Metric is mutual information. ,"This paper studies the problem of negative sample mining in visual representation learning. The authors propose a method to generate negative samples using the dot product of representations for noise constructive estimation (NCE). The NCE is based on the conditional distribution q, and the proposed method learns this conditional distribution by minimizing the mutual information between q and the ground truth. The proposed method is trained on the ring surface."
416,SP:0a4e6c8017a1294fe2424799a0048d58eaf04cb3,"hard negative mining USED-FOR deep metric learning. negative mining USED-FOR unsupervised learning. contrastive setting FEATURE-OF negative mining. Noise Contrastive Estimator HYPONYM-OF contrastive objectives. NCE CONJUNCTION mutual information. mutual information CONJUNCTION NCE. lower bound FEATURE-OF NCE. lower bound FEATURE-OF mutual information. it COMPARE NCE. NCE COMPARE it. it USED-FOR local optima. it COMPARE it. it COMPARE it. semi - hard negative mining approaches USED-FOR Ring model. OtherScientificTerm are conditional distributions, negatives, and Conditional NCE. Generic is estimator. Method is contrastive algorithms. ","This paper studies the problem of hard negative mining in deep metric learning. In particular, the authors consider negative mining under the contrastive setting, where the conditional distributions of the positive and negative samples are different. The authors propose two contrastive objectives: Noise Contrastive Estimator and Conditional NCE. The first estimator is based on the observation that the lower bound of the NCE and the mutual information between the negative and the positive samples are close to each other. The second estimator focuses on the case where the negatives are different from the positives. In contrast to other contrastive algorithms, it focuses on local optima, and it is shown that it outperforms NCE in this case. The paper also provides a theoretical analysis of semi-hard negative mining approaches for the Ring model."
417,SP:613a0e2d8cbe703f37c182553801be7537333f64,"CAFE HYPONYM-OF training algorithm. malicious server USED-FOR fake images. malicious server USED-FOR fake images. real image USED-FOR fake images. Material are federated learning setup, real images, and fake image. OtherScientificTerm are client gradients, mini - batch size, and batch index. Method are DLG, and gradient representation. Task is local training. ","This paper studies the problem of fake images generated by a malicious server in a federated learning setup. In particular, the authors propose CAFE, a training algorithm that uses client gradients to generate fake images. The key idea is that the fake images are generated from a real image, and the real images are used to train the DLG. The main idea is to minimize the mini-batch size, where the batch index is a function of the gradient representation of the fake image. The authors show that CAFE can achieve state-of-the-art results in local training."
418,SP:613a0e2d8cbe703f37c182553801be7537333f64,data leakage issue PART-OF federated learning. model parameters CONJUNCTION gradients. gradients CONJUNCTION model parameters. training batch sizes USED-FOR method. gradient matching USED-FOR It. total variation CONJUNCTION internal representation regularization. internal representation regularization CONJUNCTION total variation. data index alignment technique HYPONYM-OF regularization terms. internal representation regularization HYPONYM-OF regularization terms. total variation HYPONYM-OF regularization terms. Task is learning. ,"This paper studies the data leakage issue in federated learning. It proposes a method based on gradient matching to minimize the difference between the model parameters and the gradients during learning. The method is based on training batch sizes. The authors propose three regularization terms: total variation, internal representation regularization, and a data index alignment technique."
419,SP:613a0e2d8cbe703f37c182553801be7537333f64,"Federated Learning system USED-FOR reconstructing private data. gradients USED-FOR reconstructing private data. Method are distributed learning systems, federated learning systems, Vertical federated learning ( VFL ), and Horizontal federated learning ( HFL ). OtherScientificTerm is features. ","This paper studies the problem of reconstructing private data from a Federated Learning system using gradients. In distributed learning systems, there are many federated learning systems and each of them has its own advantages and disadvantages. This paper proposes two types of federated systems: Vertical federatedlearning (VFL) where each client is allowed to share features with other clients and Horizontal federated learnable learning (HFL), where the clients are able to share information with each other."
420,SP:ce229295081ff04b26f33829f2c3396b90897b5d,dynamic relational inference USED-FOR multi - agent trajectory prediction. method USED-FOR dynamic relational inference. static relations CONJUNCTION dynamic relations. dynamic relations CONJUNCTION static relations. neural relational inference ( NRI ) USED-FOR method. physics simulations CONJUNCTION basketball trajectories. basketball trajectories CONJUNCTION physics simulations. method COMPARE NRI. NRI COMPARE method. inferring time - varying latent variables COMPARE learning time - independent latent variables. learning time - independent latent variables COMPARE inferring time - varying latent variables. OtherScientificTerm is time - varying latent variables. ,This paper proposes a method for dynamic relational inference for multi-agent trajectory prediction based on neural relational inference (NRI). The authors compare the proposed method with NRI on physics simulations and basketball trajectories. They show that inferring time-varying latent variables is better than learning time-independent latent variables.
421,SP:ce229295081ff04b26f33829f2c3396b90897b5d,Relational Inference system USED-FOR graph structure. graph structure USED-FOR system. Method is Relational reasoning. Generic is methods. ,This paper proposes a Relational Inference system that uses graph structure to infer the graph structure of the system. Relational reasoning is a very important problem and the current methods are not very well-motivated. 
422,SP:ce229295081ff04b26f33829f2c3396b90897b5d,"history of states CONJUNCTION interaction variables. interaction variables CONJUNCTION history of states. Task is Neural Relational Inference. Method are latent variable model, and NRI. OtherScientificTerm is interaction variable. ","This paper studies the problem of Neural Relational Inference, where the goal is to infer the history of states and interaction variables. The authors propose a latent variable model, where each state is represented as a vector, and the interaction variable is a function of the current state and the past state. This is similar to NRI."
423,SP:9f4b77d39f1deca28324fb637a0a77e89976baa8,problem USED-FOR real - world recommender systems. inductive setting FEATURE-OF collaborative filtering. inductive setting FEATURE-OF problem. collaborative filtering HYPONYM-OF problem. attentive message passing framework USED-FOR user - specific representations. matrix factorization model USED-FOR relational graph. framework USED-FOR transductive and inductive settings. Task is inductive settings. Generic is representations. ,"This paper studies a problem in real-world recommender systems, i.e., collaborative filtering in the inductive setting. The authors propose an attentive message passing framework to learn user-specific representations. The key idea is to use a matrix factorization model to represent the relational graph as a set of representations. This framework can be applied to both transductive and inductive settings."
424,SP:9f4b77d39f1deca28324fb637a0a77e89976baa8,IRCF HYPONYM-OF inductive collaborative filtering method. expressiveness CONJUNCTION generalization. generalization CONJUNCTION expressiveness. expressiveness COMPARE feature - driven methods. feature - driven methods COMPARE expressiveness. generalization CONJUNCTION one - hot encoding based methods. one - hot encoding based methods CONJUNCTION generalization. matrix factorization model CONJUNCTION relation model. relation model CONJUNCTION matrix factorization model. matrix factorization model USED-FOR support users. relation model USED-FOR query users. relation model PART-OF IRCF. matrix factorization model PART-OF IRCF. support users embeddings CONJUNCTION item embeddings. item embeddings CONJUNCTION support users embeddings. transductive learning USED-FOR support users embeddings. transductive learning USED-FOR item embeddings. transductive learning USED-FOR former. relation model USED-FOR query user embeddings. relational graph USED-FOR query user embeddings. weighted sum of support user embeddings USED-FOR query user embeddings. ,"This paper proposes IRCF, an inductive collaborative filtering method, i.e., IRCCF, which is an extension of IRCF. The authors argue that expressiveness and generalization can be improved over feature-driven methods, and one-hot encoding based methods. IRCF consists of a matrix factorization model for support users and a relation model for query users. The former uses transductive learning to generate support users embeddings and item embedding, while the latter uses the relation model to generate query user embeddials from a relational graph. Finally, query users are aggregated using a weighted sum of support user embeds. "
425,SP:9f4b77d39f1deca28324fb637a0a77e89976baa8,"user - item relation graphs USED-FOR inductive recommendation framework. parametrization USED-FOR user / item representations. user - item relations USED-FOR framework. method COMPARE baselines. baselines COMPARE method. real - world datasets EVALUATE-FOR baselines. real - world datasets EVALUATE-FOR method. OtherScientificTerm are side - information, and theoretical analysis. ","This paper proposes an inductive recommendation framework based on user-item relation graphs. The framework is based on the idea of using the existing framework to model user/item relations as a parametrization for the user / item representations. The authors also propose to incorporate side-information, which is an extension of the theoretical analysis. The proposed method is evaluated on two real-world datasets and compared to several baselines."
426,SP:9f9e9b0e37e59267d8516ab914bd619c53fbc9ec,"high - quality reconstruction images CONJUNCTION disentangling. disentangling CONJUNCTION high - quality reconstruction images. DR method USED-FOR low - quality image(Y ). DR method USED-FOR method. MIG(disentanglement ) EVALUATE-FOR method. complexity EVALUATE-FOR method. method COMPARE baselines. baselines COMPARE method. FID score CONJUNCTION MIG score. MIG score CONJUNCTION FID score. MIG score EVALUATE-FOR baselines. complexity EVALUATE-FOR baselines. MIG score EVALUATE-FOR method. FID score EVALUATE-FOR method. Task are Disentangled representation ( DR ) of data, VAE - based DR, and VAE. Method are MS - VAE ), and encoded representation(Z ). Material are FILM, and high - quality reconstructed image. ","Disentangled representation (DR) of data is an important problem in the context of high-quality reconstruction images and disentangling. In this paper, the authors propose a VAE-based DR (MS-VAE), where the method uses a standard DR method to reconstruct a low-quality image(Y) and then uses an encoded representation(Z) to reconstruct the original image (Y) using FILM. The method is evaluated on FID(disentanglement) and MIG(difference between reconstructed image and original image) and compared to several baselines in terms of complexity, FID score, MIG score, etc. The results show that the proposed VAE is able to achieve state-of-the-art performance."
427,SP:9f9e9b0e37e59267d8516ab914bd619c53fbc9ec,"method USED-FOR generative model. disentangled latent representation USED-FOR generative model. method USED-FOR low - quality generation. disentanglement constraints FEATURE-OF method. method USED-FOR generative model. Method is conditional generative model. OtherScientificTerm are observed variable, latent spaces, disruptive interference, and dependent factors. Generic are models, and model. ",This paper proposes a conditional generative model where the observed variable and the latent spaces are disentangled. The method is motivated by the problem of low-quality generation due to disruptive interference. The proposed method has disentanglement constraints and can be used to train a generativemodel with a fully disentrained latent representation. The authors show that the proposed models can achieve state-of-the-art performance when the number of dependent factors in the model is small.
428,SP:9f9e9b0e37e59267d8516ab914bd619c53fbc9ec,"disentangled representation C PART-OF latent representation. β - TCVAE model USED-FOR C. first stage CONJUNCTION second stage. second stage CONJUNCTION first stage. first stage HYPONYM-OF stages. second stage HYPONYM-OF stages. β - TCVAE model USED-FOR first stage. Method are disentangled representations, and hierachical generative process. Task is data reconstruction. ","This paper proposes to learn disentangled representations that can be used for data reconstruction. The key idea is to use a hierachical generative process, where the latent representation of a latent representation is decomposed into two stages: the first stage is the disentrained representation C, and the second stage is a β-TCVAE model that learns to generate C. "
429,SP:e398873e29b05176e1d52dc6f86a59a4f405e6fd,"mutual information ( MI ) objectives USED-FOR representation learning in RL. forward objective USED-FOR optimal policy / value function. OtherScientificTerm are latent dependencies, and future states ( empowerment ). Material is game environment. ",This paper studies representation learning in RL with mutual information (MI) objectives. The forward objective aims to find the optimal policy/value function that maximizes the latent dependencies between past states (empowerment) and future states (oppression). The authors propose a game environment to demonstrate the effectiveness of the forward objective.
430,SP:e398873e29b05176e1d52dc6f86a59a4f405e6fd,mutual - information representation learning objectives USED-FOR control. state - only transition information CONJUNCTION inverse information. inverse information CONJUNCTION state - only transition information. forward information CONJUNCTION state - only transition information. state - only transition information CONJUNCTION forward information. control USED-FOR optimal policy. forward information HYPONYM-OF mutual - information representation learning objectives. inverse information HYPONYM-OF mutual - information representation learning objectives. state - only transition information HYPONYM-OF mutual - information representation learning objectives. representation USED-FOR optimal control. reward function USED-FOR representation. reward function USED-FOR optimal control. representation USED-FOR RL agent. representation USED-FOR RL agent. Task is reinforcement learning ( RL ). OtherScientificTerm is MDP cases. Material is video game. ,"This paper studies the problem of reinforcement learning (RL). The authors propose three mutual-information representation learning objectives for control: forward information, state-only transition information, and inverse information. The control is used to learn the optimal policy. The authors show that the representation learned by the RL agent can be used as a reward function for optimal control. The paper also shows that the MDP cases can be extended to video game."
431,SP:e398873e29b05176e1d52dc6f86a59a4f405e6fd,"mutual information objectives USED-FOR reinforcement learning. mutual information objectives USED-FOR state representations. state representations USED-FOR reinforcement learning. OtherScientificTerm are state - only and inverse MI objectives, and forward MI. Material is RL domain. ",This paper proposes to use mutual information objectives to learn state representations for reinforcement learning. The authors propose to use state-only and inverse MI objectives. The forward MI is a generalization of forward MI in the RL domain.
432,SP:881185782a9ec32fcbab14b42b78bf94edeba4b0,"algorithm USED-FOR global min. algorithm USED-FOR network training problem. global min FEATURE-OF network training problem. running time EVALUATE-FOR CNN. soft - thresholded SVD USED-FOR global min. Method are finite - dimensional convex copositive program, and copositive relaxation. OtherScientificTerm are data matrix, and filter size. Generic is algorithms. ","This paper proposes a new algorithm for computing the global min of a network training problem. The algorithm is based on a finite-dimensional convex copositive program, where the data matrix is a convex function of the input. The authors show that the running time of a CNN can be approximated by a soft-thresholded SVD, which is equivalent to computing a copoitive relaxation of the original matrix. They also show that their algorithms can be used to compute the global minimizer of a matrix with a fixed filter size."
433,SP:881185782a9ec32fcbab14b42b78bf94edeba4b0,"two - layered Relu network USED-FOR global optima. sign patterns FEATURE-OF ReLU unit. non - convex quadratic optimization problem USED-FOR optimization problem. copositive program USED-FOR it. convex dual USED-FOR non - convex quadratic. algorithms USED-FOR global optima. global optima FEATURE-OF two - layered network. OtherScientificTerm are linear area, ReLU, and sign pattern. ","This paper studies the problem of finding global optima of a two-layered Relu network. The optimization problem is formulated as a non-convex quadratic optimization problem, where the convex dual is a convex function of a linear area and the ReLU is a ReLU unit with sign patterns. To solve it, the authors propose a copositive program, where each sign pattern is a function of the number of layers. The authors then provide algorithms for finding the global optimas of the two-layer network."
434,SP:881185782a9ec32fcbab14b42b78bf94edeba4b0,convex formulation USED-FOR shallow neural networks. vectorial outputs USED-FOR convex formulation. Frank - Wolfe algorithm USED-FOR global optimum. Frank - Wolfe algorithm USED-FOR convex program. global optimum FEATURE-OF convex program. OtherScientificTerm is scalar outputs. ,This paper studies the convex formulation of shallow neural networks with vectorial outputs. The authors show that the Frank-Wolfe algorithm converges to the global optimum of a convex program with scalar outputs.
435,SP:8613b2fcfd076d3e28a9940bad0c490a6557c10c,unsupervised approaches USED-FOR object - centric representations. MONet CONJUNCTION Slot - Attention. Slot - Attention CONJUNCTION MONet. neuro - symbolic concept learner CONJUNCTION unsupervised approaches. unsupervised approaches CONJUNCTION neuro - symbolic concept learner. Slot - Attention HYPONYM-OF object - centric representations. MONet HYPONYM-OF object - centric representations. pre - trained object - detectors USED-FOR visual representations. pre - trained object - detectors USED-FOR NS - CL. MONet or Slot Attention USED-FOR combination. LORL USED-FOR object - centric representations. LORL USED-FOR instance segmentation. back - propagating error signals USED-FOR language - driven visual reasoning tasks. NS - CL USED-FOR language - driven visual reasoning tasks. OtherScientificTerm is MONet / Slot - Attention. ,"This paper proposes a combination of a neuro-symbolic concept learner and several unsupervised approaches to learn object-centric representations (MONet, Slot-Attention). The combination is based on MONet or Slot Attention. NS-CL uses pre-trained object-detectors to learn visual representations. LORL is used for instance segmentation. The experiments show that the combination of MONet/Slot-Attentions can improve performance on language-driven visual reasoning tasks with back-propagating error signals."
436,SP:8613b2fcfd076d3e28a9940bad0c490a6557c10c,"framework USED-FOR object - centric representation learning. unsupervised object - centric representation learning CONJUNCTION neural - symbolic concept learning. neural - symbolic concept learning CONJUNCTION unsupervised object - centric representation learning. model USED-FOR object representations. representations USED-FOR neural - symbolic program executor. reconstruction objective CONJUNCTION QA objective. QA objective CONJUNCTION reconstruction objective. stages USED-FOR model. reconstruction objective HYPONYM-OF stages. QA objective HYPONYM-OF stages. OtherScientificTerm is language supervision. Method are Object - centric Representation Learning ( LORL ), and unsupervised models. Generic is architecture. Task are object segmentations, and down - stream tasks. ","This paper proposes a framework for object-centric representation learning with language supervision, called Object-centric Representation Learning (LORL), which is a general framework for unsupervised object-centric representation learning and neural-symbolic concept learning. The proposed architecture is based on the idea that a model can learn object representations that can be used as input to a network-based neural program executor. The model is trained in three stages: a reconstruction objective, a QA objective, and a final evaluation objective. The authors show that their architecture is able to learn object segmentations that are robust to language supervision and generalize well to unseen objects. They also show that the learned representations are transferable to other ununsupervised models. Finally, the authors conduct extensive experiments on several down-stream tasks to demonstrate the effectiveness of the proposed architecture."
437,SP:8613b2fcfd076d3e28a9940bad0c490a6557c10c,segmentation CONJUNCTION referential expression interpretation. referential expression interpretation CONJUNCTION segmentation. language USED-FOR concept. idea USED-FOR downstream tasks. idea USED-FOR concept. language USED-FOR downstream tasks. language USED-FOR idea. referential expression interpretation HYPONYM-OF downstream tasks. segmentation HYPONYM-OF downstream tasks. MONet and Slot Attention HYPONYM-OF unsupervised segmentation method. object segmentation CONJUNCTION downstream tasks. downstream tasks CONJUNCTION object segmentation. objectives USED-FOR object segmentation. ,This paper proposes an unsupervised segmentation method called MONet and Slot Attention. The idea is to use language to represent a concept in downstream tasks such as segmentation and referential expression interpretation. Experiments are conducted on object segmentation with different objectives.
438,SP:5e73b99c9942dd85bf70a65ad3e3c6a45d69b66b,multi - relational reasoning USED-FOR knowledge graph ( KG ) completion. relational background knowledge USED-FOR multi - relational reasoning. work USED-FOR multi - relational reasoning. logical rules FEATURE-OF relational background knowledge. relational background knowledge USED-FOR work. superficial vector triangle linkage USED-FOR embedding models. It USED-FOR KG completion task. rules USED-FOR embeddings. rule - based reasoning USED-FOR It. rule - based reasoning USED-FOR KG completion task. FB15 K CONJUNCTION WN18. WN18 CONJUNCTION FB15 K. WN18 CONJUNCTION FB15K - R. FB15K - R CONJUNCTION WN18. Method is EM - RBR. ,"This paper presents a work that leverages relational background knowledge with logical rules for multi-relational reasoning in the context of knowledge graph (KG) completion. Specifically, the authors propose EM-RBR, which uses superficial vector triangle linkage to learn embedding models. It uses rule-based reasoning to solve the KG completion task, where the rules are used to generate embeddings. Experiments are conducted on FB15K, WN18, and a modified version of FB 15K-R."
439,SP:5e73b99c9942dd85bf70a65ad3e3c6a45d69b66b,link prediction CONJUNCTION question answering. question answering CONJUNCTION link prediction. logical reasoning CONJUNCTION statistical methods ( TransE ). statistical methods ( TransE ) CONJUNCTION logical reasoning. logical reasoning CONJUNCTION logical rule templates. logical rule templates CONJUNCTION logical reasoning. question answering HYPONYM-OF KG representation. link prediction HYPONYM-OF KG representation. KG USED-FOR Rules. AMIE USED-FOR KG. recursive backward steps USED-FOR Rules. AMIE USED-FOR Rules. OtherScientificTerm is rules. ,"This paper proposes a new KG representation for the problem of link prediction, question answering, logical reasoning, and statistical methods (TransE) that combines logical reasoning with logical rule templates. Rules are represented as a KG with recursive backward steps using AMIE. The idea is that rules can be expressed as a sequence of KG, which can then be used to generate new rules. "
440,SP:5e73b99c9942dd85bf70a65ad3e3c6a45d69b66b,framework USED-FOR Knowledge Base ( KB ) completion. EM - RBR USED-FOR triple score. triple score COMPARE EM - RBR. EM - RBR COMPARE triple score. rules PART-OF KB. embedding based method USED-FOR triple score. BFS type algorithm USED-FOR EM - RBR. EM - RBR USED-FOR translation - based embedding method. TrasnE CONJUNCTION TransH. TransH CONJUNCTION TrasnE. EM - RBR COMPARE them. them COMPARE EM - RBR. TransH HYPONYM-OF translation - based embedding method. TrasnE HYPONYM-OF translation - based embedding method. FB15k CONJUNCTION WN18. WN18 CONJUNCTION FB15k. OtherScientificTerm is score. ,"This paper proposes a framework for Knowledge Base (KB) completion. The authors propose a new embedding based method, EM-RBR, which is based on a BFS type algorithm, to compute a triple score for each word in KB. The triple score is computed as a weighted sum of all the rules in KB, and the score is then used as a proxy for the number of words in the KB. Experiments on FB15k and WN18 show that EM-REBR outperforms a translation-based embedding method, TrasnE, and TransH, by a large margin, and EM-BRR outperforms them in most cases."
441,SP:19b74093512c4e5f8c504e96c590ac1eb7e2ce9b,attention layer USED-FOR GRU cells. intuitive physics benchmarks CONJUNCTION basic reinforcement learning environment. basic reinforcement learning environment CONJUNCTION intuitive physics benchmarks. basic reinforcement learning environment EVALUATE-FOR model. intuitive physics benchmarks EVALUATE-FOR model. model COMPARE modular RNN architectures. modular RNN architectures COMPARE model. RIM HYPONYM-OF modular RNN architectures. Method is recurrent neural network architecture. ,This paper proposes a new recurrent neural network architecture. The key idea is to add an attention layer to the GRU cells. Experiments on intuitive physics benchmarks and a basic reinforcement learning environment show that the proposed model outperforms other modular RNN architectures such as RIM.
442,SP:19b74093512c4e5f8c504e96c590ac1eb7e2ce9b,"LSTM CONJUNCTION GRU. GRU CONJUNCTION LSTM. GRU PART-OF architecture. LSTM PART-OF architecture. SCOFF HYPONYM-OF architectural motif. memory PART-OF architectural motif. declarative knowledge CONJUNCTION procedural knowledge. procedural knowledge CONJUNCTION declarative knowledge. object files "" ( OF ) PART-OF architecture. OtherScientificTerm is structured, dynamic environment. ","This paper proposes a new architecture, SCOFF, which combines an LSTM and a GRU. The architectural motif consists of a memory that stores the history of past tasks, and a ""object files"" (OF) that store the current state of the architecture. The goal is to leverage both declarative knowledge and procedural knowledge in a structured, dynamic environment."
443,SP:19b74093512c4e5f8c504e96c590ac1eb7e2ce9b,RIMs USED-FOR dynamical systems. Method is procedural ( representational ) block. Generic is model. ,"This paper studies the problem of training RIMs for dynamical systems. The authors propose a procedural (representational) block, which is an extension of the RIM (representation-invariant) block proposed by [1]. The authors show that the proposed model is able to generalize to unseen states."
444,SP:42a3c0453ab136537b5944a577d63412f3c22560,NMNs USED-FOR interpretable neural models. NMNs USED-FOR video tasks. video tasks USED-FOR interpretable neural models. model USED-FOR entity references ( dialog understanding ). videos ( video understanding ) USED-FOR response generation. Method is dialogue and video understanding neural modules. ,This paper proposes to use NMNs to train interpretable neural models on video tasks. The proposed model learns entity references (dialogue understanding) and videos (video understanding) for response generation. The authors also propose dialogue and video understanding neural modules.
445,SP:42a3c0453ab136537b5944a577d63412f3c22560,method COMPARE works. works COMPARE method. works USED-FOR video QA. video QA EVALUATE-FOR method. audio - visual dialogue task EVALUATE-FOR method. Task is video - language problems. Method is Neural Module Network ( NMN ). ,This paper proposes a method for video-language problems. The proposed method is based on Neural Module Network (NMN) and is evaluated on an audio-visual dialogue task and compared to other works in video QA.
446,SP:42a3c0453ab136537b5944a577d63412f3c22560,"neural module network USED-FOR video - grounded language tasks. method USED-FOR spatio - temporal information. linguistic - based parsed program USED-FOR method. linguistic - based parsed program USED-FOR spatio - temporal information. VilNMN USED-FOR entity references. method COMPARE state - of - the - art methods. state - of - the - art methods COMPARE method. AVSD CONJUNCTION TGIF - QA. TGIF - QA CONJUNCTION AVSD. TGIF - QA EVALUATE-FOR method. AVSD EVALUATE-FOR method. OtherScientificTerm are linguistic cues, and visual cue. Generic is information. ","This paper proposes a neural module network for video-grounded language tasks. The proposed method extracts spatio-temporal information from a linguistic-based parsed program. VilNMN is trained to extract entity references from the linguistic cues, and the visual cue is used to encode the information. Experiments on AVSD and TGIF-QA show that the proposed method outperforms state-of-the-art methods."
447,SP:126ce41b7f44975e5962f8bcb43f61bf2ed315c4,"iterative application of best - response dynamics USED-FOR techniques. method USED-FOR best - response. technique USED-FOR other. OtherScientificTerm are Nash equilibrium, and NE. ","This paper proposes two techniques based on the iterative application of best-response dynamics. The first is a method to estimate the best-reaction of an agent to a given action, and the other is a technique that tries to find a Nash equilibrium between the action and the response of the agent. The main contribution of this paper is that the authors show that the NE can be approximated in a principled way."
448,SP:126ce41b7f44975e5962f8bcb43f61bf2ed315c4,"methods PART-OF Policy - Space Response Oracle framework. Mixed - Opponents USED-FOR strategy action - value estimates. Generic is approaches. Task is RL training. Method are Mixed - Oracles, and Deep RL. ","This paper proposes two new approaches for RL training. The first approach, Mixed-Oracles, is a generalization of the existing methods in the Policy-Space Response Oracle framework. The second approach is Mixed-Opponents, which uses strategy action-value estimates from Mixed-Oversamples. The authors claim that these two approaches can be combined to improve the performance of Deep RL."
449,SP:126ce41b7f44975e5962f8bcb43f61bf2ed315c4,computational and sample efficiency challenges EVALUATE-FOR PSRO style approaches. Mixed Oracles CONJUNCTION Mixed Opponents. Mixed Opponents CONJUNCTION Mixed Oracles. modifications PART-OF PSRO setup. Mixed Oracles HYPONYM-OF modifications. Mixed Opponents HYPONYM-OF modifications. approaches USED-FOR avoiding resetting learning. training FEATURE-OF stochasticity of dynamics. approaches COMPARE PSRO approach. PSRO approach COMPARE approaches. sample efficiency EVALUATE-FOR approaches. OtherScientificTerm is outer loop epoch. Method is Deep RL policies. ,"This paper studies the computational and sample efficiency challenges of PSRO style approaches. The authors propose two modifications to the standard PSRO setup: Mixed Oracles and Mixed Opponents. The proposed approaches are aimed at avoiding resetting learning by minimizing the stochasticity of dynamics during training. The experiments show that the proposed approaches can achieve better sample efficiency than the original PSRO approach, especially in the outer loop epoch. Deep RL policies are shown to be effective in this setting."
450,SP:33e0b6099b32a6a2c0f2c7a8caa57ba2935d8b00,"attention mechanism PART-OF Tacotron 2. pauses CONJUNCTION repetitions. repetitions CONJUNCTION pauses. repetitions CONJUNCTION skips. skips CONJUNCTION repetitions. OtherScientificTerm are acoustic features, token durations, and durations. Generic is methods. ","This paper proposes a new attention mechanism for Tacotron 2. The key idea is to add acoustic features to the token durations. The authors propose to add pauses, repetitions, and skips to the durations and show that the proposed methods are effective."
451,SP:33e0b6099b32a6a2c0f2c7a8caa57ba2935d8b00,Non - Attentive Tacotron HYPONYM-OF text - to - speech model. Tacotron 2 USED-FOR text - to - speech model. unaligned duration ratio ( UDR ) CONJUNCTION word deletion rate(WDR ). word deletion rate(WDR ) CONJUNCTION unaligned duration ratio ( UDR ). attention mechanism COMPARE duration predictor. duration predictor COMPARE attention mechanism. duration predictor USED-FOR robustness. metrics EVALUATE-FOR duration predictor. word deletion rate(WDR ) HYPONYM-OF metrics. unaligned duration ratio ( UDR ) HYPONYM-OF metrics. Method is semi - supervised and unsupervised duration modeling. ,"This paper proposes Non-Attentive Tacotron, a text-to-speech model that is based on a modified version of the Tacotorron 2. The authors argue that the attention mechanism is more robust than the duration predictor in terms of robustness to various metrics such as unaligned duration ratio (UDR) and word deletion rate(WDR). The authors conduct both semi-supervised and unsupervised duration modeling."
452,SP:33e0b6099b32a6a2c0f2c7a8caa57ba2935d8b00,Tacotron model USED-FOR speech synthesis. attention mechanism CONJUNCTION duration predictor. duration predictor CONJUNCTION attention mechanism. Tacotron model USED-FOR approach. robustness EVALUATE-FOR model. metrics EVALUATE-FOR model. metrics EVALUATE-FOR robustness. model COMPARE Tacotron baselines. Tacotron baselines COMPARE model. metrics EVALUATE-FOR Tacotron baselines. metrics EVALUATE-FOR model. MOS score EVALUATE-FOR Tacotron baselines. MOS score EVALUATE-FOR model. Task is semi - supervised and unsupervised training. ,"This paper proposes an approach to improve the robustness of the Tacotron model for speech synthesis. Specifically, the authors propose to combine the attention mechanism with a duration predictor. The authors conduct both semi-supervised and unsupervised training. The proposed model is evaluated on a variety of metrics to show the improved robustness compared to the Tacotorron baselines on the MOS score."
453,SP:ab9532306d294f85db84b9419ce826f046a7d95e,SBEVNet HYPONYM-OF neural network architecture. bird's - eye view ( BEV ) layout FEATURE-OF urban driving scene. SBEVNet USED-FOR bird's - eye view ( BEV ) layout. neural network architecture USED-FOR bird's - eye view ( BEV ) layout. SBEVNet USED-FOR feature volume. inverse perspective mapping ( IPM ) USED-FOR feature volume. image USED-FOR SBEVNet. SBEVNet USED-FOR BEV layout. inverse perspective mapping ( IPM ) USED-FOR SBEVNet. stereo camera USED-FOR image. supervised learning setup USED-FOR system. ,"This paper proposes SBEVNet, a neural network architecture that learns a bird's-eye view (BEV) layout of an urban driving scene using an image taken from a stereo camera. The SBIVNet learns a feature volume based on inverse perspective mapping (IPM) to estimate the feature volume of the BEV layout. The system is trained using a supervised learning setup."
454,SP:ab9532306d294f85db84b9419ce826f046a7d95e,end - to - end network USED-FOR layout estimation. stereo images USED-FOR end - to - end network. stereo images USED-FOR layout estimation. stereo matching networks USED-FOR 3D disparity volume. stereo matching networks USED-FOR approach. U - net USED-FOR semantic scene layout. stereo estimate USED-FOR image features. stereo estimate USED-FOR birds - eye - view representation. U - net USED-FOR birds - eye - view representation. KITTI and Carla generated datasets EVALUATE-FOR approach. ,This paper proposes an end-to-end network for layout estimation using stereo images. The approach uses stereo matching networks to estimate 3D disparity volume. The stereo estimate is then used to generate image features and a birds-eye-view representation using a U-net for semantic scene layout. The proposed approach is evaluated on KITTI and Carla generated datasets.
455,SP:ab9532306d294f85db84b9419ce826f046a7d95e,bird eye's view FEATURE-OF semantic layout. stereo images USED-FOR semantic layout. inverse perspective mapping CONJUNCTION projected stereo feature volume. projected stereo feature volume CONJUNCTION inverse perspective mapping. inverse perspective mapping USED-FOR framework. projected stereo feature volume USED-FOR framework. bird eye's view FEATURE-OF stereo information. stereo information USED-FOR framework. Material is KITTI and CARLA datasets. OtherScientificTerm is image information. ,This paper proposes a framework for learning semantic layout from stereo images from the bird eye's view. The framework is based on inverse perspective mapping and projected stereo feature volume. The proposed framework is able to leverage stereo information from the birds' view to learn better representations of the image information. Experiments are conducted on KITTI and CARLA datasets.
456,SP:3a151e18a5e623e9bf6e39a6065bfba1d5156fc1,gating based recurrent graph attention networks USED-FOR long - range neighbor dependencies. gating based recurrent graph attention networks USED-FOR multi - relational graphs. gated GNN models USED-FOR long - range dependencies in graphs. synthetic datasets CONJUNCTION real - world datasets. real - world datasets CONJUNCTION synthetic datasets. synthetic datasets USED-FOR node classification. real - world datasets USED-FOR node classification. ,This paper proposes gating based recurrent graph attention networks to model long-range neighbor dependencies in multi-relational graphs. The main contribution of this paper is the introduction of gated GNN models to model the long-term dependencies in graphs. Experiments are conducted on both synthetic datasets and real-world datasets for node classification.
457,SP:3a151e18a5e623e9bf6e39a6065bfba1d5156fc1,GNN model ( GR - GAT ) USED-FOR multi - relational graphs. it USED-FOR multi - relational graphs. GR - GAT USED-FOR multi - relational graphs. GAT USED-FOR GR - GAT. modifications USED-FOR long - range information. Generic is method. Method is ICLR. ,"This paper proposes a GNN model (GR-GAT) for learning multi-relational graphs. The method is based on the idea of ICLR [1] and it is able to learn multi-redundant graphs. In particular, the authors propose to use GAT to train GR-GAN to learn the multi-relationsive graphs. They also propose modifications to the GAT that allow for long-range information."
458,SP:3a151e18a5e623e9bf6e39a6065bfba1d5156fc1,"graph attention architecture USED-FOR long - range interactions. long - range interaction USED-FOR real tasks. knowledge graphs USED-FOR entity classification. Generic is architectures. Method are graph attention, and GRU - based node update function. Task is synthetic tasks. ",This paper proposes a new graph attention architecture to model long-range interactions in real tasks. The main difference between the existing architectures is that the proposed graph attention is based on a GRU-based node update function. The authors demonstrate the effectiveness of the proposed architecture on synthetic tasks as well as on entity classification on knowledge graphs.
459,SP:f9906d99f6ae5e32dda548bdccce9ae92d25b205,"image / text USED-FOR binary mask. SMT solver task USED-FOR this. Method are pos - hoc explanation method, and neural model. OtherScientificTerm is masked input. Generic is network. ","This paper proposes a pos-hoc explanation method. The idea is to train a neural model to predict the label of an image/text given a binary mask. This is done by solving an SMT solver task, where the masked input is used to train the network."
460,SP:f9906d99f6ae5e32dda548bdccce9ae92d25b205,input features USED-FOR neural network's decision. SMT problem USED-FOR problem. approach USED-FOR text classification. Task is image classification. ,"This paper studies the problem of image classification, where the input features are used to guide the neural network's decision. The problem is formulated as an SMT problem. The authors apply their approach to text classification."
461,SP:f9906d99f6ae5e32dda548bdccce9ae92d25b205,"method USED-FOR minimal input feature discovery problem. integrated gradients methods USED-FOR first - layer neurons. problems EVALUATE-FOR approach. OtherScientificTerm is features. Task are prediction, and SMT problem. Method is satisfiability modulo theory ( SMT ) solvers. ",This paper proposes a method for solving the minimal input feature discovery problem. The main idea is to use integrated gradients methods to train first-layer neurons to predict the features that are most relevant to the prediction. This is an extension of the satisfiability modulo theory (SMT) solvers. The authors evaluate their approach on three problems and show that their approach is able to solve the SMT problem.
462,SP:fc96fe4d0eeb0723bb7e4c9120c77981fc14731c,"neural mesh'representation USED-FOR pose inference. SGD USED-FOR pose inference. ObjectNet3d CONJUNCTION PASCAL3D+. PASCAL3D+ CONJUNCTION ObjectNet3d. ObjectNet3d EVALUATE-FOR approach. approach USED-FOR precise pose estimation. PASCAL3D+ EVALUATE-FOR approach. approach USED-FOR occlusion. Task is pose prediction. Method is render - and - compare approach. OtherScientificTerm are rendering pixel colors, features, mesh vertex, 3D ( learned ) features, 2D image features, rendered features, image features, and foreground occlusion. ","This paper proposes a 'neural mesh' representation for pose inference with SGD. The pose prediction is based on a render-and-compare approach, where instead of rendering pixel colors, the features are sampled from a mesh vertex and compared to 3D (learned) features. The rendered features are then compared to 2D image features. Experiments on ObjectNet3d and PASCAL3D+ demonstrate the effectiveness of the proposed approach for precise pose estimation. The proposed approach is also applied to occlusion, where the image features are compared to the foreground object."
463,SP:fc96fe4d0eeb0723bb7e4c9120c77981fc14731c,approach USED-FOR 3d pose estimation. contrastive feature learning USED-FOR approach. latent features USED-FOR 6D pose parameters. latent features COMPARE synthesized RGB colors. synthesized RGB colors COMPARE latent features. method USED-FOR latent feature vectors. template mesh FEATURE-OF latent feature vectors. backbone neural networks USED-FOR method. backbone neural networks USED-FOR latent feature vectors. appearance change CONJUNCTION partial occlusions. partial occlusions CONJUNCTION appearance change. PASCAL3D+ CONJUNCTION occluded PASCAL3D+. occluded PASCAL3D+ CONJUNCTION PASCAL3D+. approach USED-FOR appearance change. occluded PASCAL3D+ CONJUNCTION ObjectNet3D dataset. ObjectNet3D dataset CONJUNCTION occluded PASCAL3D+. render - and compare optimization COMPARE approach. approach COMPARE render - and compare optimization. render - and compare optimization USED-FOR appearance change. approach USED-FOR partial occlusions. occluded PASCAL3D+ EVALUATE-FOR formulation. PASCAL3D+ EVALUATE-FOR formulation. ObjectNet3D dataset EVALUATE-FOR formulation. OtherScientificTerm is matched regions. ,"This paper presents an approach for 3d pose estimation based on contrastive feature learning. The method learns latent feature vectors on a template mesh, and uses these latent features to approximate 6D pose parameters. The latent features are trained to be similar to synthesized RGB colors, and the method is trained with backbone neural networks. The formulation is evaluated on PASCAL3D+, occluded PASCL3D+, and ObjectNet3D dataset, and compared to render-and-compare optimization for appearance change and partial occlusions. The results show that the proposed formulation is more robust to matched regions."
464,SP:fc96fe4d0eeb0723bb7e4c9120c77981fc14731c,deep learning approaches USED-FOR 3D pose estimation. artifacts CONJUNCTION partial occlusion. partial occlusion CONJUNCTION artifacts. method USED-FOR artifacts. cuboid USED-FOR object geometry. Method is 3D neural mesh model of objects. ,"This paper proposes a 3D neural mesh model of objects that is based on deep learning approaches for 3D pose estimation. The proposed method is able to deal with artifacts and partial occlusion. The object geometry is modeled as a cuboid, and the object geometry can be represented as a cube."
465,SP:bde5b5b05d4a10634bd21a90cf0d8d22e2cda22d,"feature extractors USED-FOR features. Task is retrieval system - compatible features learning. Method are feature extractor, and old classifier. ","This paper addresses the problem of retrieval system-compatible features learning. In particular, the authors consider the case where the feature extractors are trained to generate features that are similar to the original classifier. The authors propose to train a new feature extractor on top of the old classifier, which is then used to generate new features."
466,SP:bde5b5b05d4a10634bd21a90cf0d8d22e2cda22d,model USED-FOR features. nearest class – mean classifier COMPARE linear classifier. linear classifier COMPARE nearest class – mean classifier. nearest class – mean classifier USED-FOR method. Random walk USED-FOR class means. method COMPARE baseline methods. baseline methods COMPARE method. Task is feature compatible learning. ,This paper addresses the problem of feature compatible learning. The proposed method uses a nearest class–mean classifier instead of a linear classifier. Random walk is used to generate class means. The model is trained to generate features that are close to the original class. The method is compared with several baseline methods.
467,SP:bde5b5b05d4a10634bd21a90cf0d8d22e2cda22d,constraints USED-FOR Feature Compatible Learning problem. pseudo classifiers USED-FOR model. baseline method USED-FOR problem. pseudo classifiers USED-FOR baseline method. pseudo classifiers USED-FOR problem. empirical criterion EVALUATE-FOR method. Task is problem setting. OtherScientificTerm is model ’s parameter. Method is model ’s learning. ,This paper studies the Feature Compatible Learning problem under constraints. The problem setting is that the model’s parameter should not be too different from that of the source domain. The authors propose a baseline method for this problem based on pseudo classifiers to train the model. The proposed method is evaluated on an empirical criterion and the results show that the proposed method can improve the performance of the target domain while still improving the model performance. The paper also provides a theoretical analysis of the proposed methods and its effect on the model's learning.
468,SP:a9aa11e7ee77d9f6957266e4ad822c7dc0f82354,gradient norm USED-FOR generalization. gradients USED-FOR fully connected layers ( AGN ). GN CONJUNCTION generalization error. generalization error CONJUNCTION GN. GN CONJUNCTION GN. GN CONJUNCTION GN. AGN CONJUNCTION GN. GN CONJUNCTION AGN. AGN CONJUNCTION GN. GN CONJUNCTION AGN. AGN COMPARE GN. GN COMPARE AGN. AGN CONJUNCTION generalization error. generalization error CONJUNCTION AGN. AGN USED-FOR model selection. AGN USED-FOR model selection. hyperparameter USED-FOR metric. Task is deep learning. Method is gradient norm ( GN ). Generic is models. ,"This paper studies the gradient norm (GN) for generalization in deep learning. Specifically, the authors study the gradients for fully connected layers (AGN) and compare the performance of GN, AGN, GN, and generalization error. The authors show that AGN and GN are better than GN and GN, respectively, for model selection. They also propose a new metric based on a hyperparameter and show that models trained with AGN are more general."
469,SP:a9aa11e7ee77d9f6957266e4ad822c7dc0f82354,"sum of gradient norms USED-FOR neural network. Generic is approach. OtherScientificTerm are generalization gap, and full gradient norms. ","This paper proposes an approach to reduce the generalization gap between the full gradient norms and the sum of gradient norms of a neural network. The main contribution of this paper is that the authors show that when the number of parameters of the neural network is large enough, the full gradients of the gradient norms can be reduced to a small number. The authors also show that this can be done by minimizing the generalized error of the full norm of the weights. "
470,SP:a9aa11e7ee77d9f6957266e4ad822c7dc0f82354,"gradient norm USED-FOR model selection criterion. AGN USED-FOR models. generalization error FEATURE-OF models. architectures USED-FOR models. ( approxiamte ) gradient norm USED-FOR model selection. Method are approximate gradient norm ( AGN ), and bandit - based or population - based algorithms. ","This paper proposes approximate gradient norm (AGN), a new model selection criterion based on the gradient norm. The authors show that AGN can be used to select models with low generalization error, and that models trained with different architectures can be trained with similar architectures. The main contribution of this paper is to propose a (approxiamte) gradient norm for model selection, which can be applied to bandit-based or population-based algorithms."
471,SP:13359456defb953dd2d19e1f879100ce392d6be6,method COMPARE entity retrieval methods. entity retrieval methods COMPARE method. approach COMPARE methods. methods COMPARE approach. parameter space CONJUNCTION memory. memory CONJUNCTION parameter space. parameter space USED-FOR approach. memory USED-FOR approach. entity disambiguation CONJUNCTION entity linking. entity linking CONJUNCTION entity disambiguation. entity linking CONJUNCTION entity retrieval. entity retrieval CONJUNCTION entity linking. paper USED-FOR entity disambiguation. paper USED-FOR tasks. entity disambiguation HYPONYM-OF tasks. entity retrieval HYPONYM-OF tasks. entity linking HYPONYM-OF tasks. OtherScientificTerm is entity vocabulary. Generic is model. ,"This paper proposes a method that is more efficient than existing entity retrieval methods. The proposed approach is based on parameter space and memory. The paper is applied to three tasks: entity disambiguation, entity linking, and entity retrieval. The idea is to learn an entity vocabulary and then use the model to retrieve the entities."
472,SP:13359456defb953dd2d19e1f879100ce392d6be6,sequence - to - sequence neural model USED-FOR entity linking task. approach COMPARE methods. methods COMPARE approach. memory CONJUNCTION computation costs. computation costs CONJUNCTION memory. entity representations USED-FOR methods. entity vocabularies USED-FOR approach. entity disambiguation CONJUNCTION entity linking. entity linking CONJUNCTION entity disambiguation. entity linking CONJUNCTION document retrieval. document retrieval CONJUNCTION entity linking. document retrieval USED-FOR question answering. model USED-FOR tasks. entity disambiguation HYPONYM-OF tasks. document retrieval HYPONYM-OF tasks. entity linking HYPONYM-OF tasks. ,"This paper proposes a sequence-to-sequence neural model for the entity linking task. The approach is based on entity vocabularies, which is different from existing methods that use entity representations. The authors argue that this is due to the memory and computation costs. The model is applied to three tasks: entity disambiguation, entity linking, and document retrieval for question answering."
473,SP:13359456defb953dd2d19e1f879100ce392d6be6,autoregressive approach USED-FOR entity - based problems. tasks EVALUATE-FOR model. GENRE model COMPARE models. models COMPARE GENRE model. memory usage EVALUATE-FOR models. big memory table USED-FOR models. memory usage EVALUATE-FOR GENRE model. Method is uniform framework. Generic is It. ,This paper proposes an autoregressive approach to entity-based problems. The model is trained on a set of tasks and evaluated on a variety of tasks. The GENRE model is compared to other models that use a big memory table. The paper also proposes a uniform framework for training the model. It is based on the idea that the number of tasks should be equal to the size of the dataset.
474,SP:9dfb808ce4c045c45436b35ceb03bc6fe6ed9745,"congestion function USED-FOR congestion. edge PART-OF graph. congestion functions FEATURE-OF edges. Task are routing problem, and routing decisions. Generic is model. Method is shortest path algorithm. OtherScientificTerm is random additive term. ","This paper studies the routing problem, where the goal is to minimize the congestion of a given edge in a graph. The routing problem is formulated as a function of the number of edges in the graph, and the congestion function is used to measure the congestion. The authors propose a model where the congestion functions of the edges are defined as a weighted sum of a random additive term, and then the routing decisions are made based on this model. The shortest path algorithm is proposed."
475,SP:9dfb808ce4c045c45436b35ceb03bc6fe6ed9745,"stochastic combinatorial semi - bandits USED-FOR routing. static graph USED-FOR routing. known Lipschitz constant FEATURE-OF unknown Lipschitz function. OtherScientificTerm are expected loss, regret, optimal paths, and constant function. Method is stochastic combinatorial semi - bandit. ","This paper studies the problem of routing in stochastic combinatorial semi-bandits. The routing is performed on a static graph, where the expected loss is a function of the distance to the optimal path. The regret is defined as the difference between the optimal paths and the paths that are close to the constant function. The authors consider the case where the unknown Lipschitz function is a known function, and the known loss is the product of the Lipsschitz constant of the paths."
476,SP:9dfb808ce4c045c45436b35ceb03bc6fe6ed9745,"bandit learning framework USED-FOR online learning problem. congestion function CONJUNCTION random noise. random noise CONJUNCTION congestion function. learning agent USED-FOR routing decision. UCB approach USED-FOR learning algorithm. Material are city network, and New York City network. OtherScientificTerm is reward. Generic is algorithm. ","This paper proposes a new bandit learning framework for online learning problem. The learning algorithm is based on the UCB approach, where the congestion function is replaced with random noise, and the learning agent is tasked with making a routing decision. The paper proposes to use a city network as a starting point, and to use the reward as a way to train the algorithm. Experiments are conducted on the New York City network."
477,SP:580ac3b74951bef5d5772e4471b01a805ff3dd68,"data - driven approach USED-FOR n - grams. approach COMPARE random token and random spans masking strategies. random token and random spans masking strategies COMPARE approach. pointwise mutual information USED-FOR n - grams. pointwise mutual information USED-FOR approach. Task are masking, and MLM task. ",This paper presents a data-driven approach to learn n-grams. The approach is based on pointwise mutual information between the tokens and the spans of the masking. The authors show that their approach outperforms random token and random spans masking strategies on a simple MLM task.
478,SP:580ac3b74951bef5d5772e4471b01a805ff3dd68,"masking strategy USED-FOR masked language models ( MLM ). approaches USED-FOR strategy. method USED-FOR PMI. PMI CONJUNCTION generalization. generalization CONJUNCTION PMI. local signals USED-FOR mask. local signals USED-FOR model. PMI USED-FOR Masking. transfer quality EVALUATE-FOR model. vocabulary size USED-FOR whole - word masking. local signals USED-FOR model. OtherScientificTerm are named entities, higher level semantics, and WordPiece vocabulary. Method is MLM. ","This paper proposes a masking strategy for masked language models (MLM). The proposed strategy is based on two approaches. First, the authors propose a method called PMI, which aims to improve PMI and generalization. Masking is done by adding local signals to the mask. Second, they propose to increase the vocabulary size for whole-word masking. The authors show that by increasing the size of the WordPiece vocabulary, the MLM can achieve better transfer quality."
479,SP:580ac3b74951bef5d5772e4471b01a805ff3dd68,variant USED-FOR MLM training objective. PMI USED-FOR variant. Whole Word Masking CONJUNCTION Entity - based masking. Entity - based masking CONJUNCTION Whole Word Masking. PMIs USED-FOR ngrams. collocational phrases COMPARE single words. single words COMPARE collocational phrases. longer - range dependencies CONJUNCTION higher level semantic features. higher level semantic features CONJUNCTION longer - range dependencies. training COMPARE it. it COMPARE training. models USED-FOR collocations. PMI metric EVALUATE-FOR variant. Method is PMI - based approach. Generic is method. Metric is training objective. OtherScientificTerm is PMI subphrases. ,"This paper proposes a variant of the MLM training objective based on PMI. The proposed method is motivated by the observation that PMIs are biased towards ngrams with longer-range dependencies and higher level semantic features. To address this issue, the authors propose a novel PMI-based approach, which combines Whole Word Masking with Entity-based masking. The authors show that the proposed method can improve the performance of the training objective by a significant margin. In particular, they show that their proposed method improves the PMI metric by a large margin. They also show that by adding PMI subphrases, they can achieve better performance on collocational phrases compared to single words. Finally, they demonstrate that their models are able to generate collocations that are more similar to each other than it is to the original training."
480,SP:038cdd2df643edccb16dfd72e6eb123f6a6c0839,"partial conditioning USED-FOR amortized inference. amortized inference USED-FOR variational auto - encoders. restricted family of possible distributions USED-FOR approximate posterior. amortized approximate posterior CONJUNCTION inference network. inference network CONJUNCTION amortized approximate posterior. Material are sequential data sources, and discrete observations. OtherScientificTerm are conditioning, true posterior, evidence lower bound, approximation gap, non - amortized approximate posterior, amortisation gap, inference gaps, and unconditioned information. Metric is KL divergence. ","This paper studies the problem of amortized inference in variational auto-encoders with partial conditioning. The authors consider sequential data sources, where the conditioning is applied to the true posterior and the approximate posterior is assumed to be a restricted family of possible distributions. They derive an evidence lower bound for the approximation gap between an amortised approximate posterior and an inference network, which is defined as the KL divergence between the true and non-amortized approximate posterior. They show that the amortisation gap is a function of the number of inference gaps, which depends on the amount of unconditioned information. "
481,SP:038cdd2df643edccb16dfd72e6eb123f6a6c0839,"partially conditioned variational posterior USED-FOR Bayesian inference. partially observed data USED-FOR ill - behaved variational posterior. product of experts USED-FOR partially conditioned variational posterior. product of distributions USED-FOR variational posterior. OtherScientificTerm are degenerate solution, mixture of distributions, product of densities, and product's density. ","This paper studies the problem of learning an ill-behaved variational posterior from partially observed data. The authors propose to use the product of experts as a partially conditioned variational prior for Bayesian inference. They show that when the product's density is large enough, the degenerate solution can be obtained by sampling from a mixture of distributions. They also show that if the mixture of densities is small enough, then the product can be approximated by the product. Finally, they show that the product is a function of the number of experts."
482,SP:038cdd2df643edccb16dfd72e6eb123f6a6c0839,filtering COMPARE smoothing posterior. smoothing posterior COMPARE filtering. filtering HYPONYM-OF posterior. state - space models HYPONYM-OF sequential latent variable models. VAE - style loss USED-FOR state - space models. posterior CONJUNCTION model. model CONJUNCTION posterior. missing information USED-FOR model. OtherScientificTerm is posteriors. ,"This paper proposes a VAE-style loss for state-space models (i.e., sequential latent variable models, such as state-spaces models) with a focus on removing the bias of the posterior, i.e. filtering instead of smoothing posterior. The idea is that the posterior and the model can benefit from missing information from the posteriors. "
483,SP:f2574c0d6cdec78389fa1301d6a10976d1756279,"random features CONJUNCTION communications. communications CONJUNCTION random features. generalization properties FEATURE-OF distributed kernel ridge regression ( DKRR ). communications FEATURE-OF distributed kernel ridge regression ( DKRR ). random features FEATURE-OF distributed kernel ridge regression ( DKRR ). optimal learning rates FEATURE-OF generalization bounds. random features USED-FOR DKRR. communication rounds USED-FOR DKRR - RF. Metric is optimal learning rate. OtherScientificTerm are number of partitions, partition count, and partitions. ","This paper studies the generalization properties of distributed kernel ridge regression (DKRR) with random features and communications. The authors provide generalization bounds on the optimal learning rates for DKRR with and without random features. They show that DKRR-RF with multiple communication rounds converges to an optimal learning rate that depends on the number of partitions. They also show that when the partition count is small, DKRR can converge to a solution that does not depend on the partitions."
484,SP:f2574c0d6cdec78389fa1301d6a10976d1756279,"algorithm USED-FOR distributed learning. random Fourier features USED-FOR algorithm. random Fourier features USED-FOR distributed learning. sampled M random features USED-FOR linear hypothesis. bound COMPARE one. one COMPARE bound. bound USED-FOR divide & conquer algorithm. Method is importance weighting. OtherScientificTerm are regularization parameter, and consistency bound. ","This paper proposes an algorithm for distributed learning with random Fourier features. The main idea is to use a linear hypothesis over sampled M random features, and to use importance weighting to ensure that the regularization parameter does not change too much. The authors prove a consistency bound of O(1/\sqrt{T}) where O(T) is the number of samples. This bound is better than the one for the divide & conquer algorithm."
485,SP:f2574c0d6cdec78389fa1301d6a10976d1756279,distributed kernel ridge regression CONJUNCTION random features ( DKRR - RF ). random features ( DKRR - RF ) CONJUNCTION distributed kernel ridge regression. statistical properties FEATURE-OF distributed kernel ridge regression. OtherScientificTerm is optimal generalization bounds. Generic is algorithms. ,This paper studies the statistical properties of distributed kernel ridge regression and random features (DKRR-RF). The authors provide optimal generalization bounds for both algorithms.
486,SP:129872706a12d89f0886c2ad0fd4083d0632343c,EPS USED-FOR ranking correlations. EPS USED-FOR search efficiency. ranking correlations FEATURE-OF RandomNAS. proxy search space ( PS ) PART-OF RandomNAS. NASBench-201 FEATURE-OF RandomNAS. proxy search space ( PS ) USED-FOR EPS. tournament selection CONJUNCTION aging mechanism. aging mechanism CONJUNCTION tournament selection. stages PART-OF EPS. tournament selection USED-FOR PS. aging mechanism USED-FOR PS. model - size - based regularization USED-FOR selection stage. Generic is method. ,"This paper proposes EPS to improve the ranking correlations of RandomNAS on NASBench-201. EPS consists of two stages: tournament selection and an aging mechanism. During the selection stage, a model-size-based regularization is applied. Experiments demonstrate the effectiveness of the proposed method."
487,SP:129872706a12d89f0886c2ad0fd4083d0632343c,ranking correlation EVALUATE-FOR random search - based NAS methods. proxy search space USED-FOR it. evolutionary algorithms USED-FOR it. size regularization USED-FOR NAS algorithm. approach COMPARE baseline methods. baseline methods COMPARE approach. OtherScientificTerm is small architecture traps. ,"This paper studies the ranking correlation of random search-based NAS methods. Specifically, it proposes a proxy search space and uses evolutionary algorithms to search it. It also proposes a NAS algorithm with size regularization to avoid small architecture traps. The experimental results show that the proposed approach outperforms the baseline methods."
488,SP:129872706a12d89f0886c2ad0fd4083d0632343c,"RandomNAS - based approach USED-FOR Proxy Search Space ( EPS ). GS HYPONYM-OF proxy search space ( PS ). tournament selection evolutionary algorithm CONJUNCTION aging mechanism. aging mechanism CONJUNCTION tournament selection evolutionary algorithm. tournament selection evolutionary algorithm USED-FOR PS. Metric is RandomNAS ’s search efficiency. OtherScientificTerm are top - performing architectures, and architectures. Method is EPS. ","This paper proposes a RandomNAS-based approach to Proxy Search Space (PS), specifically GS, to improve RandomNAS’s search efficiency. Specifically, the authors propose a tournament selection evolutionary algorithm and an aging mechanism for PS. The main idea is to select the top-performing architectures, and then use these architectures to optimize the EPS."
489,SP:27701f374d0b7e8b269d9133d6c3a10bca03b548,"meta - RL CONJUNCTION imitation learning ( IL ). imitation learning ( IL ) CONJUNCTION meta - RL. meta - RL CONJUNCTION IL. IL CONJUNCTION meta - RL. sample complexity EVALUATE-FOR task. meta - RL HYPONYM-OF task. demonstrations USED-FOR IL. Generic are tasks, and combination. OtherScientificTerm is Demonstrations. ","This paper proposes a combination of meta-RL and imitation learning (IL) that combines the benefits of both tasks. Demonstrations are used to compare the sample complexity of a given task (e.g., meta-LRL and IL) and show that the performance of IL on demonstrations can be improved by combining the two tasks."
490,SP:27701f374d0b7e8b269d9133d6c3a10bca03b548,Meta Imitation Learning CONJUNCTION Meta Reinforcement Learning. Meta Reinforcement Learning CONJUNCTION Meta Imitation Learning. method USED-FOR Meta Imitation Learning. method USED-FOR Meta Reinforcement Learning. context - based meta - learning USED-FOR method. context - based meta - learning USED-FOR PERIL. latent variable USED-FOR trajectories. data USED-FOR meta - learning updates. trajectories USED-FOR meta - learning updates. expert demonstrations CONJUNCTION trajectories. trajectories CONJUNCTION expert demonstrations. trajectories USED-FOR data. expert demonstrations USED-FOR data. ,"This paper proposes a method for Meta Imitation Learning and Meta Reinforcement Learning based on context-based meta-learning, which is similar to PERIL. The key idea is to use a latent variable to learn trajectories, which are then used to generate data from expert demonstrations and trajectories to perform meta-Learning updates. The paper is well-written and easy to follow."
491,SP:27701f374d0b7e8b269d9133d6c3a10bca03b548,"demonstration trajectories CONJUNCTION trajectories. trajectories CONJUNCTION demonstration trajectories. PERIL HYPONYM-OF meta RL method. policy USED-FOR trajectories. set encoder USED-FOR trajectories. set encoder USED-FOR latent vector. trajectories USED-FOR latent vector. metaIL techniques USED-FOR losses. demonstrations USED-FOR encoder. Generic is task. Method are metaRL, and imitation learning techniques. ","This paper proposes PERIL, a meta RL method, which is an extension of PERIL. The main idea is to use demonstration trajectories and trajectories generated by a policy to learn a set of trajectories, which are then used to train a set encoder to generate a latent vector from the learned trajectories. The encoder is trained using demonstrations, and the loss is learned using metaIL techniques to approximate the losses. The paper is well-written and well-motivated. The task is interesting and the idea of metaRL is interesting. However, there are some issues that need to be addressed in order for the paper to be accepted by the community. For example, it is unclear to me why the paper does not use imitation learning techniques."
492,SP:118758f563fa6e9e46d52a6f250005c06cf2f19f,"Task is generalization of convolutional neural networks. Method are convolutional neural networks, toy model, and 3 - layer neural network. ",This paper studies the generalization of convolutional neural networks. The authors propose a toy model where a 3-layer neural network is trained to predict the output of a different layer. The paper is well-written and easy to follow.
493,SP:118758f563fa6e9e46d52a6f250005c06cf2f19f,"3 - layer CNN USED-FOR image classification task. orthogonal non - overlapping patches USED-FOR image classification task. learning algorithm USED-FOR PSI. VC dimension EVALUATE-FOR network. PSI USED-FOR task. non - orthogonal patches FEATURE-OF MNIST. MNIST USED-FOR PSI. MNIST USED-FOR task. OtherScientificTerm are pattern statics inductive bias ( PSI ), and filter dimension. Metric is sample complexity. ","This paper proposes a 3-layer CNN for the image classification task with orthogonal non-overlapping patches. The main contribution of this paper is to study the pattern statics inductive bias (PSI). The authors propose a learning algorithm to learn PSI for the task on MNIST with non-orthogonal patches. They show that the VC dimension of the network is proportional to the number of patches, and that the sample complexity of the learned network is linear in the filter dimension."
494,SP:118758f563fa6e9e46d52a6f250005c06cf2f19f,"theoretical analysis USED-FOR generalization guarantees. theoretical analysis USED-FOR naïve CNN ( 3 - layers ). generalization guarantees EVALUATE-FOR naïve CNN ( 3 - layers ). binary classification task USED-FOR task. orthogonal patches PART-OF images. statistical phenomenon USED-FOR SGD. pattern detectors CONJUNCTION detected patterns. detected patterns CONJUNCTION pattern detectors. learning algorithm USED-FOR PSI. exponential sample complexity FEATURE-OF learning algorithms. OtherScientificTerm are Pattern Statistics Inductive Bias ( PSI ), dot - product, and filter. Metric is sample complexity. ","This paper presents a theoretical analysis of the generalization guarantees of a naïve CNN (3-layers) trained with Pattern Statistics Inductive Bias (PSI). The task is a binary classification task, where orthogonal patches of images are sampled from the input image. The authors show that SGD can be viewed as a statistical phenomenon, where the dot-product of the input and the output of the filter is a function of the PSI. They show that the sample complexity of PSI is exponential when the number of pattern detectors and detected patterns is large, and that the learning algorithm for PSI has exponential sample complexity."
495,SP:a051b615da3a99562d2cd2dfbec5cd78af98d9b4,contrastive learning approach USED-FOR representation learning. topic modeling assumptions USED-FOR contrastive learning approach. procedure USED-FOR representation of documents. topic posterior information FEATURE-OF representation of documents. procedure USED-FOR document classification task. Method is linear models. Task is semi - supervised setting. ,This paper proposes a contrastive learning approach for representation learning under topic modeling assumptions. The main contribution of this paper is a procedure for learning a representation of documents with topic posterior information for a document classification task. The authors show that linear models can be trained in a semi-supervised setting.
496,SP:a051b615da3a99562d2cd2dfbec5cd78af98d9b4,document level contrastive estimation USED-FOR document level representation. topic posterior information FEATURE-OF contrastive estimation. OtherScientificTerm is topic modeling assumptions. Method is linear models. ,This paper studies document level contrastive estimation for document level representation. The main contribution of this paper is to study the topic posterior information of contrastive estimations. The topic modeling assumptions are analyzed and compared to linear models.
497,SP:a051b615da3a99562d2cd2dfbec5cd78af98d9b4,contrastive learning algorithm USED-FOR document representation. learning algorithm USED-FOR cross - entropy loss function. learning algorithm USED-FOR discriminating function. function USED-FOR embedding function. topic posterior distribution CONJUNCTION topic likelihood distribution. topic likelihood distribution CONJUNCTION topic posterior distribution. topic posterior distribution USED-FOR function. topic likelihood distribution USED-FOR function. learning algorithm USED-FOR hidden topics. synthetic dataset EVALUATE-FOR learning algorithm. representation USED-FOR semi - supervised learning. visualization EVALUATE-FOR representation. ,"This paper proposes a contrastive learning algorithm for document representation. The learning algorithm first learns a cross-entropy loss function and then learns a discriminating function. The discriminator is trained by minimizing the difference between the topic posterior distribution and the topic likelihood distribution. This function is then used as an embedding function. Finally, the learning algorithm is applied to the hidden topics and is evaluated on a synthetic dataset. The authors show that the proposed representation can be used for semi-supervised learning, and also provide visualization of the learned representation."
498,SP:8d011d4a77ced1f8cd849181d5293420f161ffd3,It USED-FOR VAE based methods. learning CONJUNCTION data efficiency. data efficiency CONJUNCTION learning. It USED-FOR learning. data efficiency EVALUATE-FOR It. OtherScientificTerm is PMI. Material is multimodal data. ,This paper proposes to use PMI as an alternative to VAE based methods. It is able to improve learning and data efficiency by reducing the number of parameters required to compute PMI. It can also be used to improve the performance of VAE-based methods. The main contribution of this paper is to study the problem of multimodal data. 
499,SP:8d011d4a77ced1f8cd849181d5293420f161ffd3,multimodal ELBOs USED-FOR multimodal VAEs. semi - supervised approach USED-FOR weakly - supervised multimodal data. semi - supervised approach USED-FOR this. IWAE CONJUNCTION CUBO. CUBO CONJUNCTION IWAE. estimators USED-FOR estimation of ( 2 ). IWAE HYPONYM-OF estimators. CUBO HYPONYM-OF estimators. Method is contrastive objective. Material is unimodal samples. ,"This paper studies multimodal ELBOs for multi-modal VAEs. Specifically, the authors propose a contrastive objective, where the objective is to minimize the difference between the mean and the variance of the multimodality of the samples. This is achieved by using a semi-supervised approach to weakly-supervisioned multimodaled data, which is similar to this used for unimodal samples. Two estimators for the estimation of (2) are proposed: IWAE and CUBO."
500,SP:8d011d4a77ced1f8cd849181d5293420f161ffd3,"generative model USED-FOR multimodal learning. contrastive loss USED-FOR objective. contrastive loss FEATURE-OF max - margin optimization. max - margin optimization USED-FOR multimodal learning. IWAE estimator USED-FOR optimization. data - efficient learning CONJUNCTION label propagation. label propagation CONJUNCTION data - efficient learning. multimodal learning CONJUNCTION data - efficient learning. data - efficient learning CONJUNCTION multimodal learning. approach USED-FOR multimodal learning. approach USED-FOR data - efficient learning. approach USED-FOR label propagation. Metric are pointwise mutual information, and metrics. OtherScientificTerm is random variable relatedness. ","This paper proposes a generative model for multimodal learning with max-margin optimization with a contrastive loss as the objective. The optimization is based on the IWAE estimator, which is motivated by the observation that pointwise mutual information between the learned model and the target model can be improved by maximizing random variable relatedness. The proposed approach is applied to the problem of multi-modal learning, data-efficient learning, and label propagation. Empirical results show that the proposed metrics are effective."
501,SP:3a0d3f1d63cd57b0613c40176e694435ed3eee50,"approximate posterior CONJUNCTION gaussian prior. gaussian prior CONJUNCTION approximate posterior. KL term PART-OF ELBO. hierarchical latents CONJUNCTION EBMs. EBMs CONJUNCTION hierarchical latents. MCMC sampling USED-FOR EBMs. NCE style FEATURE-OF binary classifier. langevin dynamics CONJUNCTION re - sampling. re - sampling CONJUNCTION langevin dynamics. re - weighting term USED-FOR re - weighted prior. re - weighting term PART-OF NCE score. re - sampling USED-FOR re - weighted prior. langevin dynamics USED-FOR re - weighted prior. approach CONJUNCTION hierarchical latents. hierarchical latents CONJUNCTION approach. hierarchical latents USED-FOR generative models. Task are VAE, prior - hole problem, and sampling. OtherScientificTerm is data manifold. Generic are Prior approaches, and two stage method. Method are autoregressive models, and regular VAE. ","This paper studies the problem of training a VAE in an unsupervised manner. Prior approaches have been focused on training autoregressive models, where an approximate posterior and a gaussian prior are learned in a supervised manner, where the approximate posterior is sampled from the data manifold, and the gaussian posterior is learned by sampling from the posterior. This is called the prior-hole problem. The paper proposes a two stage method. First, a binary classifier is trained in an NCE style, where a KL term is added to the ELBO. Second, a re-weighted prior is learned using langevin dynamics and re-sampling. The authors show that the proposed approach can be combined with hierarchical latents and EBMs trained with MCMC sampling, which is an extension of the work of [1] and [2]. The authors also show that their approach is able to generalize to generative models, and that their results are comparable to regular VAE."
502,SP:3a0d3f1d63cd57b0613c40176e694435ed3eee50,"Method are variational autoencoders, decoder, and generate models. OtherScientificTerm is prior distribution. ","This paper proposes variational autoencoders, where the decoder is trained to generate samples from a prior distribution, and the models are trained on the generated samples. The authors argue that this is an efficient way to generate models that are robust to changes in the prior distribution."
503,SP:3a0d3f1d63cd57b0613c40176e694435ed3eee50,"marginal over latents PART-OF VAEs. latent space FEATURE-OF marginal distributions. neural networks USED-FOR unnormalized probability distribution. MCMC sampling HYPONYM-OF approximate inference. binary classifier USED-FOR NCP. aggregated posterior FEATURE-OF NCP. idea USED-FOR hierarchical VAEs. stochastic level FEATURE-OF binary classifier. OtherScientificTerm are non - trainable probability distribution, marginal over z's, and non - trainable distribution. Method are likelihood ratio trick, and VAE. ","This paper studies the problem of estimating marginal over latents in VAEs. In particular, the authors consider the case where the marginal distributions in the latent space are non-trainable, i.e., they are drawn from a distribution that is not normalized by neural networks. The authors propose to approximate this unnormalized probability distribution using neural networks, which is a variant of approximate inference (e.g., MCMC sampling). The authors use the likelihood ratio trick to approximate the posterior of an NCP with an aggregated posterior, where the NCP is modeled as a binary classifier at the stochastic level. The idea is applied to hierarchical VAEs, where each layer of the VAE is trained to approximate a different subset of a non-trivial probability distribution (i.e. marginal over z's), and the weights of each VAE are trained to be close to each other."
504,SP:86b813ac0f5211a7c45884451f59f3ebaeeb4b83,entropy regularization USED-FOR maximum entropy IRL. entropy regularization COMPARE convex regularizer. convex regularizer COMPARE entropy regularization. Tsallis entropy CONJUNCTION expert state - action distribution. expert state - action distribution CONJUNCTION Tsallis entropy. Tsallis entropy USED-FOR Bregman divergence. expert state - action distribution USED-FOR Bregman divergence. algorithm USED-FOR IRL. Tsallis entropy USED-FOR algorithm. Task is regularized IRL setup. ,This paper studies the problem of maximum entropy IRL with entropy regularization instead of a convex regularizer. The authors propose an algorithm for IRL that uses Tsallis entropy and expert state-action distribution to approximate the Bregman divergence. They also provide a theoretical analysis of the regularized IRL setup.
505,SP:86b813ac0f5211a7c45884451f59f3ebaeeb4b83,method USED-FOR regularized inverse RL. convex policy regularizers USED-FOR regularized MDPs. Shannon entropy HYPONYM-OF policy regularizer. algorithm USED-FOR IRL. regularized adversarial IRL ( RAIRL ) USED-FOR algorithm. regularized MDPs USED-FOR IRL. AIRL USED-FOR regularized adversarial IRL ( RAIRL ). Task is regularized IRL. ,"This paper proposes a method for regularized inverse RL. The main idea is to apply convex policy regularizers to regularized MDPs, where the policy regularizer is Shannon entropy. The proposed algorithm, AIRL, is a variant of regularized adversarial IRL (RAIRL), which is an algorithm for IRL in the case of the regularized mDPs. "
506,SP:86b813ac0f5211a7c45884451f59f3ebaeeb4b83,"regularized MDPs USED-FOR inverse reinforcement learning. Method are regularized Markov Decision Processes ( MDPs ), and policy regularization. ","This paper studies inverse reinforcement learning in regularized MDPs, which is a generalization of regularized Markov Decision Processes (MDPs). The main contribution of this paper is to study the problem of policy regularization. The paper is well-written and well-motivated. "
507,SP:6f4a520cdc9901c2c87a7e887ce2535ad0b36f69,conditional computation USED-FOR language - specific behaviour. language specific sub - layers PART-OF network. budgetary constraints FEATURE-OF language - specific parameters. architecture USED-FOR language specific parameters. Task is massively multilingual machine translation. Method is multilingual NMT architectures. OtherScientificTerm is language specific parameter sharing. ,"This paper addresses the problem of massively multilingual machine translation, where the language-specific behaviour of the network is subject to conditional computation. The authors propose to use language specific sub-layers in the network, which is an extension of multilingual NMT architectures. The architecture is designed to share language specific parameters under budgetary constraints, which allows for language specific parameter sharing."
508,SP:6f4a520cdc9901c2c87a7e887ce2535ad0b36f69,specificity CONJUNCTION generality. generality CONJUNCTION specificity. language - specific ( LS ) components CONJUNCTION components. components CONJUNCTION language - specific ( LS ) components. components PART-OF hybrid architecture. language - specific ( LS ) components PART-OF hybrid architecture. language - specific capacity EVALUATE-FOR ones. Generic is architectures. ,This paper proposes a hybrid architecture that combines language-specific (LS) components with existing components. The authors argue that such architectures are important in terms of both specificity and generality. They also argue that existing ones have limited language-special capacity.
509,SP:6f4a520cdc9901c2c87a7e887ce2535ad0b36f69,"language - specific projection layer CONJUNCTION shared projection layer. shared projection layer CONJUNCTION language - specific projection layer. CLSR layer CONJUNCTION transformer encoder and decoder layer. transformer encoder and decoder layer CONJUNCTION CLSR layer. hard gating functions PART-OF layer. Task is transformer - based multilingual NMT systems. Method is token representations. OtherScientificTerm are language - specific capacity, language - specific computations, and budget constraint. Generic is network. ","This paper studies transformer-based multilingual NMT systems, where token representations are encoded in a language-specific capacity. The proposed network consists of a CLSR layer, a transformer encoder and decoder layer, and a shared projection layer, which is a combination of the previous work of [1] and [2]. The proposed layer consists of two hard gating functions, which are designed to encourage the network to be able to handle more languages. This is achieved by making the language specific computations more efficient by imposing a budget constraint."
510,SP:1e932b21e9557b1bbc1950c4e1701f5a3ecf50df,Wasserstein distributional normalization algorithm USED-FOR classification of noisy labels. upper bound USED-FOR Wasserstein-2 distance. algorithm COMPARE SOTA approaches. SOTA approaches COMPARE algorithm. Clothing1 M EVALUATE-FOR algorithm. Generic is bound. OtherScientificTerm is network. ,"This paper proposes a Wasserstein distributional normalization algorithm for the classification of noisy labels. The authors derive an upper bound on the Wassersteine-2 distance between the true label and the noisy label, which is a bound that is tight. This bound is based on the assumption that the network is not biased towards the noisy labels and that the label is not noisy. The proposed algorithm is evaluated on Clothing1M and compared to SOTA approaches."
511,SP:1e932b21e9557b1bbc1950c4e1701f5a3ecf50df,empirical risk EVALUATE-FOR ( neural network ) classifier. Wasserstein distance USED-FOR distributional normalization. Wasserstein ball USED-FOR uncertain samples. particle based stochastic dynamics USED-FOR process. Ornstein - Ulenbeck process USED-FOR particle based stochastic dynamics. open noise CONJUNCTION real world dataset ( clothing 1 M ). real world dataset ( clothing 1 M ) CONJUNCTION open noise. symmetric noise setting CONJUNCTION open noise. open noise CONJUNCTION symmetric noise setting. real world dataset ( clothing 1 M ) EVALUATE-FOR it. classical datasets EVALUATE-FOR it. symmetric noise setting FEATURE-OF classical datasets. real world dataset ( clothing 1 M ) HYPONYM-OF classical datasets. open noise HYPONYM-OF classical datasets. Task is label noise problem. Generic is problem. Metric is small loss criteria. OtherScientificTerm is learning process. ,"This paper studies the label noise problem and proposes a way to reduce the empirical risk of a (neural network) classifier. The problem is formulated as minimizing the Wasserstein distance between the distributional normalization and the true label. The process is based on particle based stochastic dynamics inspired by the Ornstein-Ulenbeck process. The authors propose a small loss criteria that penalizes the uncertainty in the learning process. They also propose to use a modified version of the original wasserstein ball to handle uncertain samples. They evaluate it on classical datasets in the symmetric noise setting, open noise, and a real world dataset (clothing 1M)."
512,SP:1e932b21e9557b1bbc1950c4e1701f5a3ecf50df,geometric constraints FEATURE-OF logits of uncertain samples. geometric constraints USED-FOR objective function. 2 - Wasserstein ball FEATURE-OF distribution logits of uncertain samples. Wasserstein Normalization HYPONYM-OF surrogate objective. SDE grad flow USED-FOR Wasserstein normalization. moving average USED-FOR Gaussian parameters. OtherScientificTerm is ball radius. Method is batch normalization. Generic is method. ,"This paper proposes a new objective function based on geometric constraints on the logits of uncertain samples in a 2-Wasserstein ball. The proposed surrogate objective is called Wasserstein Normalization, which is based on the SDE grad flow. The ball radius is defined as a function of the number of samples, and the Gaussian parameters are parameterized by a moving average. The method is similar to batch normalization."
513,SP:e0029422e28c250dfb8c62c29a15b375030069e8,conformalized procedure USED-FOR uncertainty sets. conformalized procedure USED-FOR classification tasks. conformalized procedures USED-FOR large uncertainty sets. additive regularizer USED-FOR solution. Generic is method. OtherScientificTerm is hyper - parameters. ,This paper proposes a conformalized procedure for estimating uncertainty sets for classification tasks. The proposed method is based on the observation that conformalization of the hyper-parameters can lead to large uncertainty sets. The authors propose a solution based on an additive regularizer to address this issue.
514,SP:e0029422e28c250dfb8c62c29a15b375030069e8,regularized conformal score USED-FOR conformal prediction framework. top - p variations FEATURE-OF conformal scores. regularizer USED-FOR top - p scores. top - k scores USED-FOR regularizer. top - k scores USED-FOR top - p scores. ImageNet EVALUATE-FOR architectures. ImageNet EVALUATE-FOR large - scale evaluation. architectures USED-FOR large - scale evaluation. Metric is output conformal prediction set sizes. Method is conformal prediction algorithms. ,"This paper proposes a conformal prediction framework based on a regularized conformal score. The proposed regularizer is based on the top-k scores of the conformal scores with different top-p variations. The authors demonstrate the effectiveness of the proposed architectures on large-scale evaluation on ImageNet. They also show that the proposed regularization can improve the performance of the output conformal predictions set sizes. Finally, the authors conduct extensive experiments to compare the performance and efficiency of the different conformal classification algorithms."
515,SP:e0029422e28c250dfb8c62c29a15b375030069e8,"Prediction sets USED-FOR classification. Adaptive Prediction Sets ( APS ) USED-FOR prediction sets. Method is naive approach. OtherScientificTerm are coverage probability, probability estimations, and tail of the distribution. ","Prediction sets for classification are often used for classification. This paper proposes Adaptive Prediction Sets (APS), which is a variant of prediction sets based on the idea of adapting the coverage probability of the distribution. The naive approach is to use the probability estimations of the coverage of the target distribution as a proxy for the distribution of the source distribution, which is then used to estimate the probability of a target distribution. In this paper, the authors show that this is not optimal, and propose to adapt the coverage probabilities of the prediction sets to be more robust to changes in the tail of the training distribution."
516,SP:bf93641cbeaaa147ad0307de694e20adc23c290a,dual formulation USED-FOR Wasserstein-2 barycenter problem. ICNNs USED-FOR convex potentials. regularization terms USED-FOR congruent and conjugacy conditions. algorithm USED-FOR barycenter. Method is Wasserstein-2 barycenter computation method. OtherScientificTerm is objective function. ,"This paper proposes a dual formulation of the Wasserstein-2 barycenter problem. The main idea is to use ICNNs to approximate convex potentials, which is an extension of the well-known Wassersteinsensee-2barycenter computation method. In particular, the authors propose two regularization terms to satisfy the congruent and conjugacy conditions, respectively. The authors also propose a new algorithm for computing the true wasserstein of the original w.r.t. the objective function."
517,SP:bf93641cbeaaa147ad0307de694e20adc23c290a,"component distribution FEATURE-OF convex potential. regularization terms PART-OF objective function. congruency regularization USED-FOR variational bound. generative modeling CONJUNCTION posterior inference. posterior inference CONJUNCTION generative modeling. approach USED-FOR generative modeling. approach USED-FOR posterior inference. Task is barycenter mapping problem. OtherScientificTerm are Congruency, optimal potential functions, regularization term, and convex functions. Generic are optimization, and objective. ","This paper studies the barycenter mapping problem, where the objective function consists of two regularization terms. The first regularization term penalizes the convex potential with respect to the component distribution. Congruency is defined as the difference between the two optimal potential functions. The second congruency regularization is used to derive a variational bound on the variance of the objective. The authors show that the optimization can be viewed as an extension of the work of [1], which considers convex functions with two regularizations. The proposed approach can be applied to generative modeling and posterior inference."
518,SP:bf93641cbeaaa147ad0307de694e20adc23c290a,algorithm USED-FOR Wasserstein-2 barycenter. method COMPARE methods. methods COMPARE method. Task is Wasserstein Barycenter problems. OtherScientificTerm is marginals. ,"This paper proposes an algorithm for computing the Wasserstein-2 barycenter. This is an important problem in Wasserstein Barycenter problems, as the marginals are not known. The proposed method is compared to several existing methods."
519,SP:39aae6a094f7141bee6d4fa78be03fd20cf12b13,it USED-FOR submanifolds. deep fully - connected neural network USED-FOR model. unit sphere FEATURE-OF sub - manifolds. model generalization EVALUATE-FOR network. SGD USED-FOR network. Task is binary classification task. Material is one - dimensional case. ,"This paper proposes a binary classification task. The model is based on a deep fully-connected neural network, and it learns to represent submanifolds on a unit sphere. The network is trained with SGD, and the model generalization is evaluated on a one-dimensional case."
520,SP:39aae6a094f7141bee6d4fa78be03fd20cf12b13,"deep fully - connected network USED-FOR low - dimensional data classes. network depth USED-FOR geometrical properties of data. manifold curvature HYPONYM-OF geometrical properties of data. Task is binary classification setting. OtherScientificTerm are manifolds, network width, and network parameters. Method are classifier, and Martingale model. ","This paper proposes a deep fully-connected network for low-dimensional data classes. In particular, the authors focus on the binary classification setting, where the classifier is trained on a set of data points. The network depth is motivated by the geometrical properties of data, such as manifold curvature. The authors show that when the manifolds are large enough, the network width can be reduced to a small number of network parameters, which is similar to the Martingale model."
521,SP:39aae6a094f7141bee6d4fa78be03fd20cf12b13,gradient descent USED-FOR deep neural network. Neural Tangent Kernel approximation USED-FOR analysis. Task is machine learning community. OtherScientificTerm is finite sample regime. ,"This paper studies the problem of gradient descent in deep neural network. The paper is well-motivated and well-written. The analysis is based on the Neural Tangent Kernel approximation, which has been well-studied in the machine learning community. The main contribution of this paper is the analysis in the finite sample regime. "
522,SP:c5afd0a7485aa8dc732f6fa90d81a85a8bb51b3c,Advantage - Weighted Regression ( AWR ) HYPONYM-OF deep reinforcement learning method. imitation learning CONJUNCTION off - policy learning. off - policy learning CONJUNCTION imitation learning. AWR USED-FOR imitation learning. AWR USED-FOR off - policy learning. reinforcement learning tasks EVALUATE-FOR AWR. static datasets USED-FOR AWR. static datasets USED-FOR off - policy learning. Method is REPS. ,"This paper proposes a deep reinforcement learning method called Advantage-Weighted Regression (AWR), which is an extension of REPS. AWR is applied to both imitation learning and off-policy learning on static datasets. Experimental results on several reinforcement learning tasks demonstrate the effectiveness of AWR."
523,SP:c5afd0a7485aa8dc732f6fa90d81a85a8bb51b3c,RL learning algorithm COMPARE algorithms. algorithms COMPARE RL learning algorithm. advantage function COMPARE q function. q function COMPARE advantage function. advantage function USED-FOR RWR. it COMPARE algorithms. algorithms COMPARE it. common environments EVALUATE-FOR AWR. AWR COMPARE algorithms. algorithms COMPARE AWR. Method is reward weighted regression. OtherScientificTerm is experience replay. Generic is extensions. ,"This paper proposes an RL learning algorithm called reward weighted regression (AWR) that can be viewed as an extension of existing algorithms. In RWR, the advantage function is replaced by a q function, and the goal is to minimize the variance of the q function. The paper shows that AWR outperforms existing algorithms on common environments, and that it can achieve better performance than existing algorithms when experience replay is used. In addition, the paper proposes two extensions to the existing algorithms, which are shown to be effective."
524,SP:c5afd0a7485aa8dc732f6fa90d81a85a8bb51b3c,"advantage - weighted regression USED-FOR reinforcement learning algorithm. advantage value USED-FOR policy. policy COMPARE sampling policy. sampling policy COMPARE policy. constraint FEATURE-OF policy search. OtherScientificTerm is value function. Method are advantage weighted regression, reward - weighted regression, and Model - Free Preference - based Reinforcement Learning. Task is neural information processing systems. Generic is algorithm. ","This paper proposes a reinforcement learning algorithm based on advantage-weighted regression. The advantage weighted regression is an extension of reward-warped regression, where the value function is a weighted sum of the reward and the advantage value is used to train a policy that maximizes the advantage over the sampling policy. The authors argue that this algorithm is a natural extension of Model-Free Preference-based Reinforcement Learning, which has been shown to be effective in neural information processing systems. The main contribution of this paper is to introduce a constraint on the policy search."
525,SP:54da307c1f9aac020ae7e3c439653765dbd8b3fe,sinusoidal regularizer USED-FOR neural network quantization. it USED-FOR bit - width. CNN CONJUNCTION Transformers. Transformers CONJUNCTION CNN. Method is regularizer. OtherScientificTerm is floating - point parameters. Generic is method. ,"This paper proposes a sinusoidal regularizer for neural network quantization. The proposed regularizer is based on the observation that the floating-point parameters of a CNN and Transformers tend to be close to each other, and that it can be used to reduce bit-width. The method is evaluated on a variety of datasets."
526,SP:54da307c1f9aac020ae7e3c439653765dbd8b3fe,"regularization term USED-FOR bit - width. regularization term USED-FOR DNN weights. Sinusoidal function USED-FOR regularization. sinusoidal period USED-FOR continuous representation of the bit - width. OtherScientificTerm are quantization intervals, and penalty. ",This paper proposes a new regularization term for the bit-width of the DNN weights. Sinusoidal function is used as the regularization. The authors show that the sinusoidal period can be used to learn a continuous representation of the bit - width. The quantization intervals are also used as a penalty.
527,SP:54da307c1f9aac020ae7e3c439653765dbd8b3fe,full - precision weights USED-FOR forward propagation. full - bitwidth weights USED-FOR backprop. Method is quantized neural networks. OtherScientificTerm is quantization scheme. ,This paper studies quantized neural networks. The authors propose to use full-precision weights for forward propagation and full-bitwidth weights for backprop. They also propose a new quantization scheme.
528,SP:84ced6627d1dc3e78c9ffc726174e76db5f77795,GAN process USED-FOR neural machine translation models. noise generator USED-FOR approach. switching - aligned - words technique USED-FOR noise generator. fast - align USED-FOR alignments. switch and align approach USED-FOR noisy sentence pair generator. ,This paper proposes a novel approach to train neural machine translation models using a GAN process. The proposed approach uses a noise generator that is trained using the switching-aligned-words technique. The key idea is to use fast-align to generate alignments that are close to the original sentence. The switch and align approach is applied to a noisy sentence pair generator.
529,SP:84ced6627d1dc3e78c9ffc726174e76db5f77795,method USED-FOR data augmentation. data augmentation USED-FOR machine translation. parallel data USED-FOR word aligner. embeddings USED-FOR variant. Method is noising strategies. OtherScientificTerm is random words. ,"This paper proposes a method for data augmentation in machine translation. The main idea is to use parallel data to train a word aligner. The authors propose two different noising strategies. The first variant is based on embeddings of random words, while the second variant uses the embedding of the original word."
530,SP:84ced6627d1dc3e78c9ffc726174e76db5f77795,"attention distribution USED-FOR NMT model. attention distribution USED-FOR Alignments. fast - align HYPONYM-OF unsupervised aligner. unsupervised aligner USED-FOR Alignments. Method is data augmentation technique. OtherScientificTerm are Perturbations, aligned word, and weighted combination. Generic is method. ","This paper proposes a data augmentation technique called Perturbations. Alignments are generated using an unsupervised aligner called fast-align, which is a weighted combination of the attention distribution of the NMT model. The goal of the method is to make the aligned word more similar to the original word."
531,SP:c1890bcafac6ac8fd5a3d2ff2dd1c37b71865a5a,"local model CONJUNCTION global model. global model CONJUNCTION local model. solution COMPARE heuristic method. heuristic method COMPARE solution. Method are federated learning model, and attention - based model divergence. Generic is contribution. Task is targeting problem. ","This paper proposes a federated learning model where the local model and the global model are jointly trained. The contribution is based on the idea of attention-based model divergence, where each local model is trained to maximize the mutual information between the local and global model. The authors show that the proposed solution outperforms the heuristic method by a large margin. The main contribution of the paper is the formulation of the targeting problem."
532,SP:c1890bcafac6ac8fd5a3d2ff2dd1c37b71865a5a,"low computational complexity method USED-FOR weighting contributions of clients. weighting contributions of clients USED-FOR federated learning setting. weighting method COMPARE Shapley values. Shapley values COMPARE weighting method. OtherScientificTerm are low data volume, and Euclidean distance. Method is server model. ",This paper proposes a low computational complexity method for weighting contributions of clients in the federated learning setting. The main idea is to use a low data volume and compute the Euclidean distance between the weights of the server model and the clients. The proposed weighting method is shown to outperform Shapley values.
533,SP:c1890bcafac6ac8fd5a3d2ff2dd1c37b71865a5a,"contribution measurement approach USED-FOR federated learning. approach COMPARE Shapley Value. Shapley Value COMPARE approach. contribution measurement COMPARE Shapley Value. Shapley Value COMPARE contribution measurement. contribution measurement EVALUATE-FOR approach. OtherScientificTerm are model update, local updates, and decay rate. ",This paper proposes a contribution measurement approach for federated learning. The contribution measurement is based on the difference between the model update and the number of local updates. The authors compare the proposed approach with Shapley Value and show that the proposed contribution measurement has a lower decay rate.
534,SP:b766979b4d3b15a039db4e5eebd8353521aea4bb,nearly - linear time FEATURE-OF robustly learning fixed structure Bayesian networks. algorithm USED-FOR robust mean estimation. robust mean estimation USED-FOR problem. nearly - linear time FEATURE-OF robust mean estimation. Method is fixed structure Bayesian networks. Metric is runtime. OtherScientificTerm is sparsity. ,"This paper studies the problem of robustly learning fixed structure Bayesian networks in nearly-linear time. The authors propose an algorithm for robust mean estimation in this problem, which can be viewed as an extension of the work of [1]. The main contribution of this paper is to show that the runtime of the algorithm is linear in the number of parameters and in the sparsity. "
535,SP:b766979b4d3b15a039db4e5eebd8353521aea4bb,"fixed - structure Bayesian networks USED-FOR robust learning. eps - adversarial corruptions model USED-FOR robust learning. running time EVALUATE-FOR algorithm. O(m / eps^2 ) samples USED-FOR robust learning algorithm. OtherScientificTerm are Fixed - structure, and Bayesian network. Task is Robust learning. Method is d - node Bayes net. ",This paper studies the problem of robust learning with fixed-structure Bayesian networks. Fixed-structured networks have been shown to be effective for robust learning in the eps-adversarial corruptions model. Robust learning is an important problem in the literature. This paper proposes a new robust learning algorithm with O(m/epsilon^2) samples and O(eps^2). The running time of the proposed algorithm is O(mn/eps^3) samples. The main contribution of this paper is to show that the d-node Bayes net can be viewed as a special case of a Bayesian network. 
536,SP:b766979b4d3b15a039db4e5eebd8353521aea4bb,"adversarially corrupted data USED-FOR Bayes nets. Bayes net USED-FOR nodes. Generic is model. OtherScientificTerm are unknown $ \varepsilon$ fraction, and probability distribution. ","This paper studies adversarially corrupted data for Bayes nets. The authors propose a model where the nodes of a Bayes net are corrupted with an unknown $\varepsilon$ fraction, and the model is trained to predict the probability distribution of the corrupted data. "
537,SP:b3d507bd8fe8876f3a4f7696bc0483d0052484c8,shooting - based methods USED-FOR action sequence planning. shooting - based methods COMPARE collocation - based method. collocation - based method COMPARE shooting - based methods. learned latent - space dynamics models USED-FOR action sequence planning. shooting - based methods USED-FOR sparse - reward and long - horizon tasks. collocation method COMPARE shooting methods. shooting methods COMPARE collocation method. CEM CONJUNCTION gradient - based ). gradient - based ) CONJUNCTION CEM. scheduled Lagrange multiplier FEATURE-OF Levenberg - Marquard optimization. gradient - based ) HYPONYM-OF shooting methods. robotic tasks EVALUATE-FOR collocation method. robotic tasks EVALUATE-FOR gradient - based ). CEM HYPONYM-OF shooting methods. scheduled Lagrange multiplier USED-FOR collocation method. robotic tasks EVALUATE-FOR shooting methods. Levenberg - Marquard optimization USED-FOR collocation method. OtherScientificTerm is planning trajectories. ,"This paper proposes a new collocation-based method based on Levenberg-Marquard optimization with a scheduled Lagrange multiplier to improve the performance of shooting-based methods for action sequence planning with learned latent-space dynamics models for sparse-reward and long-horizon tasks using planning trajectories. The proposed collocation method is compared to other shooting methods (CEM, gradient-based) on a variety of robotic tasks."
538,SP:b3d507bd8fe8876f3a4f7696bc0483d0052484c8,collocation USED-FOR vision - based motion planning approach. approaches USED-FOR vision - based control. model - based control USED-FOR control tasks. planning approaches USED-FOR model - based control. shooting USED-FOR model - based control. shooting USED-FOR planning approaches. planning approaches USED-FOR approaches. difficult path constraints FEATURE-OF Collocation approaches. Method is model - based reinforcement learning. ,"This paper proposes a vision-based motion planning approach based on collocation. Collocation approaches have been shown to be effective approaches for improving the performance of model-based control for control tasks using planning approaches based on shooting, and this paper proposes to use planning approaches to improve the state-of-the-art vision-to-action (V2A) and vision-forward motion planning approaches. The paper is well-motivated and well-written, and the motivation of the work is clear and interesting. However, there are some issues that need to be addressed in order for the paper to be accepted as a contribution to the community, such as difficult path constraints and the lack of theoretical analysis of the effect of the proposed approach. In addition, there is a lack of comparison to other work on model-by-reinforcement learning. "
539,SP:b3d507bd8fe8876f3a4f7696bc0483d0052484c8,"sparse rewards USED-FOR planning. model - based RL USED-FOR trajectory optimization. model - based RL USED-FOR problem. latent models USED-FOR latent representations. latent representations USED-FOR planning problem. latent representations USED-FOR solution. latent models USED-FOR solution. Levenberg - Marquardt algorithm USED-FOR solution. Material is images. Method are zeroth - order CEM optimization, and gradient - based method. ","This paper studies the problem of planning with sparse rewards. The problem is formulated as a model-based RL for trajectory optimization. The proposed solution is based on the Levenberg-Marquardt algorithm, where the latent models are used to learn latent representations for the planning problem. The main contribution of the paper is the introduction of zeroth-order CEM optimization, which is a gradient-based method that can be applied to images."
540,SP:cfe57a61dc20207b64b7fff45f7cb33126dce558,theory USED-FOR cold posterior phenomena. VI CONJUNCTION CIFAR-10 test set. CIFAR-10 test set CONJUNCTION VI. SGLD USED-FOR CIFAR-10 test set. VI USED-FOR toy - problem. toy - problem EVALUATE-FOR theory. Material is image benchmarks. Method is generative model. Generic is dataset. ,This paper presents a theory for cold posterior phenomena. The theory is tested on a toy-problem using VI and a CIFAR-10 test set trained with SGLD. The experiments are conducted on several image benchmarks and show that the generative model is able to generalize to new datasets.
541,SP:cfe57a61dc20207b64b7fff45f7cb33126dce558,"cold posterior COMPARE Bayesian posterior. Bayesian posterior COMPARE cold posterior. Bayesian posterior USED-FOR Bayesian deep learning. toy problem CONJUNCTION image classification. image classification CONJUNCTION toy problem. image classification EVALUATE-FOR theory. toy problem EVALUATE-FOR theory. OtherScientificTerm is mis - specified likelihood function. Method are data curation process, and likelihood model. Generic is posterior. ",This paper proposes to use a cold posterior instead of a Bayesian posterior in Bayesian deep learning. The main idea is to learn a mis-specified likelihood function that is independent of the data curation process. The posterior is then used to estimate the likelihood of a given data point. The authors show that this likelihood model can be used to approximate the true posterior. The theory is evaluated on a toy problem and on image classification.
542,SP:cfe57a61dc20207b64b7fff45f7cb33126dce558,likelihood COMPARE prior. prior COMPARE likelihood. cold posteriors FEATURE-OF Bayesian neural networks. curation process USED-FOR benchmark data sets. OtherScientificTerm is cold posterior effect. ,"This paper studies the cold posteriors of Bayesian neural networks. The authors show that the likelihood of the posterior is not always better than the prior, and that the cold posterior effect can be explained by the curation process of the benchmark data sets. "
543,SP:4ebd3874ecea94ed9d0ca7b2fb13bf246b556938,"large encoder CONJUNCTION large autoregressive ( AR ) decoder. large autoregressive ( AR ) decoder CONJUNCTION large encoder. large autoregressive ( AR ) decoder USED-FOR NTM. large encoder USED-FOR NTM. translation quality EVALUATE-FOR NAR approaches. NAR USED-FOR AR decoding. it COMPARE NAR models. NAR models COMPARE it. AR models COMPARE NAR models. NAR models COMPARE AR models. AR models COMPARE it. it COMPARE AR models. design space FEATURE-OF it. Method are AR decoder, Non - Autoregressive ( NAR ) models, decoder, and CMLM and DisCo NAR models. Task is inference. OtherScientificTerm is parallelism. ","This paper proposes NTM, which uses a large encoder and a large autoregressive (AR) decoder. The AR decoder is trained in a way that is similar to Non-Autoregressive(NAR) models. The main difference between NAR approaches is the translation quality. NAR is used for AR decoding, and the inference is done in parallel. The authors compare the performance of the decoder with the CMLM and DisCo NAR models, and show that it outperforms NAR and AR models in terms of design space."
544,SP:4ebd3874ecea94ed9d0ca7b2fb13bf246b556938,autoregressive ( AR ) CONJUNCTION non - autoregressive models ( NAR ). non - autoregressive models ( NAR ) CONJUNCTION autoregressive ( AR ). suboptimal layer allocation CONJUNCTION insufficient speed measurement. insufficient speed measurement CONJUNCTION suboptimal layer allocation. OtherScientificTerm is knowledge distillation. Method is AR and NAR models. ,This paper studies the relationship between autoregressive (AR) and non-autoregressive models (NAR). The authors argue that knowledge distillation is an important factor in the performance degradation of AR and NAR models. The authors point out that suboptimal layer allocation and insufficient speed measurement are the main reasons for this phenomenon.
545,SP:4ebd3874ecea94ed9d0ca7b2fb13bf246b556938,deep encoder and shallow decoder models USED-FOR auto - regressive NMT. layer allocation CONJUNCTION speed measurement. speed measurement CONJUNCTION layer allocation. speed measurement CONJUNCTION knowledge distillation. knowledge distillation CONJUNCTION speed measurement. layer allocation HYPONYM-OF factors. knowledge distillation HYPONYM-OF factors. speed measurement HYPONYM-OF factors. 6 - 6 AR model COMPARE NAR model. NAR model COMPARE 6 - 6 AR model. speed - up EVALUATE-FOR 12E - D1 model. quality EVALUATE-FOR NAR model. they USED-FOR reordering. deep decoders USED-FOR NAR models. ,"This paper studies the problem of auto-regressive NMT with deep encoder and shallow decoder models. The authors propose three factors: (1) layer allocation, (2) speed measurement, and (3) knowledge distillation. Experiments show that the 6-6 AR model can achieve comparable speed-up to the NAR model while maintaining the same quality. In addition, the authors show that NAR models trained with deep decoders can achieve better performance when they are used for reordering."
546,SP:ce8cf444681a8e38408c6485029fe42b89a1f172,"SGD USED-FOR neural network. test error EVALUATE-FOR OV. Task is epoch wise double descent phenomena. OtherScientificTerm are SGD steps, and ` ` optimization variance ( OV ) ''. Generic is quantity. ","This paper studies epoch wise double descent phenomena, where SGD is applied to a neural network trained with SGD. The authors propose a new quantity called `optimization variance (OV)', which measures the difference between the number of SGD steps and the test error of the neural network. The OV is defined as the difference in test error between the original test error and the new test error. The quantity is defined in terms of the ratio of the OV of the original error to the new error. "
547,SP:ce8cf444681a8e38408c6485029fe42b89a1f172,"validation dataset USED-FOR rule. optimization variance COMPARE variance of gradients. variance of gradients COMPARE optimization variance. OtherScientificTerm is stopping rule. Material is validation set. Generic are datasets, it, and It. ","This paper studies the problem of learning a stopping rule from a validation set. The goal is to learn a rule on a validation dataset where the optimization variance is smaller than the variance of gradients. It is shown that if the validation set is large enough, then the stopping rule can be learned from it. The paper also shows that when the validation dataset is small enough, it is possible to learn the rule on it. "
548,SP:ce8cf444681a8e38408c6485029fe42b89a1f172,"Rethinking Bias - Variance Trade - off USED-FOR Generalization of Neural Networks. bias and variance terms PART-OF test error. OtherScientificTerm are training time, train time, variance term, bias term, and optimization variance ( OV ). Method are Epoch - Wise Double - Descent, Neural Networks, and Double Descent. Task is early stopping. ","This paper studies the Rethinking Bias-Variance Trade-off in the Generalization of Neural Networks. The authors propose Epoch-Wise Double-Descent, where the bias and variance terms are added to the test error in order to reduce the training time. The main idea is that the bias term is proportional to the number of epochs in the train time, while the variance term is a function of the optimization variance (OV). The authors show that Double Descent converges to a fixed point, and that early stopping is beneficial."
549,SP:8d8b738c676938952e62a6b2aea42e79518ece06,adversarial robustness EVALUATE-FOR model agnostic meta - learning ( MAML ). meta - update stage CONJUNCTION fine - tune stage. fine - tune stage CONJUNCTION meta - update stage. Adversarial robustness PART-OF MAML. generalization CONJUNCTION computation efficiency. computation efficiency CONJUNCTION generalization. fast attack generation method USED-FOR meta - update stage. generalization CONJUNCTION robustness. robustness CONJUNCTION generalization. contrastive representation learning USED-FOR unlabeled data. Task is fast robustness adaptation. ,"This paper studies the adversarial robustness of model agnostic meta-learning (MAML). Adversarial is a very important aspect of MAML, as it is important for improving generalization and computation efficiency. The authors propose a fast attack generation method for the meta-update stage and fine-tuning stage. They also propose to use contrastive representation learning for unlabeled data, which is also important for fast robustness adaptation."
550,SP:8d8b738c676938952e62a6b2aea42e79518ece06,adversarial robustness EVALUATE-FOR model agnostic meta learning ( MAML ). robust regularization USED-FOR adversarial robustness. robust regularization USED-FOR MAML. contrastive learning USED-FOR MAML model. Method is robust MAML methods. ,This paper studies the adversarial robustness of model agnostic meta learning (MAML). The authors propose a robust regularization for MAML. The main contribution of this paper is to propose contrastive learning for the MAMPL model. The authors also provide a theoretical analysis of the robustness to adversarial perturbations of the MAMML methods.
551,SP:8d8b738c676938952e62a6b2aea42e79518ece06,regularizing adversarial robustness USED-FOR robustness adaptation. meta - update level FEATURE-OF regularizing adversarial robustness. few - shot test tasks EVALUATE-FOR robustness adaptation. miniImageNet dataset USED-FOR few - shot image classification task. OtherScientificTerm is adversarial robustness. ,This paper studies the problem of regularizing adversarial robustness at the meta-update level for robustness adaptation on few-shot test tasks. The main contribution of this paper is to study the effect of the size of the miniImageNet dataset on the performance of a few -shot image classification task. The paper also provides a theoretical analysis of the impact of the number of epochs of training on the robustness performance.
552,SP:1fdce0afe8fd8c082f62f1a4b9823830d81860e8,"approach USED-FOR step - size adaptation. Meta - gradient descent USED-FOR step - size adaptation. it USED-FOR loss function. algorithms USED-FOR quadradic loss function. logarithm FEATURE-OF meta - objective. back - propagation USED-FOR numerical stability. generalization ability EVALUATE-FOR methods. OtherScientificTerm are step - size, weight trajectory, and vanishing / exploding gradients. ","Meta-gradient descent is an approach to step-size adaptation in the context of step-sizes. In this paper, the authors propose to use it as a loss function and use it to approximate a quadradic loss function. The authors propose two algorithms to approximate the quadratic loss function, where the meta-objective is modelled as a logarithm of the log of the step-sized and the weight trajectory. They show that back-propagation is necessary to ensure numerical stability and to avoid vanishing/exploding gradients. They also show that their methods have better generalization ability."
553,SP:1fdce0afe8fd8c082f62f1a4b9823830d81860e8,"algorithms USED-FOR learning rates. learning rates USED-FOR gradient descent. algorithms USED-FOR gradient descent. gradient descent USED-FOR gradient descent. learning rate FEATURE-OF gradient. log USED-FOR gradient. OtherScientificTerm are quadratic losses, and logarithm. ","This paper studies algorithms for learning rates for gradient descent in the setting of quadratic losses. The authors propose algorithms for computing the learning rates of the gradient of gradient descent with respect to the log of the learning rate of the original gradient. They show that the gradient can be expressed as a log of a function of the log, where the logarithm is the number of samples required to compute the gradient. "
554,SP:1fdce0afe8fd8c082f62f1a4b9823830d81860e8,vanilla GD CONJUNCTION SGD. SGD CONJUNCTION vanilla GD. step size USED-FOR vanilla GD. step size USED-FOR SGD. optimization steps CONJUNCTION back - propagating. back - propagating CONJUNCTION optimization steps. gradient explosion / vanishing CONJUNCTION over - fitting. over - fitting CONJUNCTION gradient explosion / vanishing. already - detected phenomena FEATURE-OF optimizers. gradient explosion / vanishing HYPONYM-OF already - detected phenomena. over - fitting HYPONYM-OF already - detected phenomena. OtherScientificTerm is mean - square errors. Generic is theory. ,"This paper studies the effect of the step size of vanilla GD and SGD on the mean-square errors. The authors show that the optimization steps and back-propagating of these optimizers lead to several already-detected phenomena such as gradient explosion/vanishing, over-fitting, etc. The theory is well-motivated and well-written."
555,SP:c8a9ab50888585b58369c4fb425be1170c96c14d,co - training scheme USED-FOR label sparsity problem. Method is view learners. OtherScientificTerm is consistency loss. ,This paper proposes a co-training scheme to tackle the label sparsity problem in view learners. The main idea is to add a consistency loss to the training set. The experiments are conducted to show the effectiveness of the proposed method.
556,SP:c8a9ab50888585b58369c4fb425be1170c96c14d,view - consistent framework USED-FOR expensive labels. graph neural networks CONJUNCTION graph attention networks. graph attention networks CONJUNCTION graph neural networks. graph attention networks USED-FOR latent features. graph neural networks USED-FOR latent features. classification neural networks USED-FOR node classification outcomes. it USED-FOR node classification outcomes. classification neural networks USED-FOR it. classification outcome USED-FOR view loss. classification outcome USED-FOR it. incremental strategy USED-FOR pseudo labels. incremental strategy USED-FOR it. OtherScientificTerm is termination conditions. ,"This paper proposes a view-consistent framework for learning expensive labels. In particular, it uses classification neural networks to learn node classification outcomes and graph attention networks to model the latent features. The view loss is based on the classification outcome, and it uses an incremental strategy to learn pseudo labels. The termination conditions are also provided."
557,SP:c8a9ab50888585b58369c4fb425be1170c96c14d,multi - view learning approach USED-FOR graph representation learning. graph convolution network ( GCN ) CONJUNCTION graph attention network ( GAT ). graph attention network ( GAT ) CONJUNCTION graph convolution network ( GCN ). graph attention network ( GAT ) USED-FOR graph. loss function USED-FOR It. graph attention network ( GAT ) USED-FOR It. graph convolution network ( GCN ) USED-FOR It. loss function USED-FOR consistency. OtherScientificTerm is low label rate scenario. Task is performance evaluation. ,This paper proposes a multi-view learning approach for graph representation learning. It uses a graph convolution network (GCN) and a graph attention network (GAT) to represent the graph. It also introduces a new loss function to encourage consistency and improve performance in the low label rate scenario. Experiments are conducted to demonstrate the effectiveness of the proposed performance evaluation.
558,SP:9c8619d2c0df81c1222ba28cecbacc42408d0019,"Hamiltonian dynamics USED-FOR physical systems. conserved quantities PART-OF Hamiltonian dynamics. Poisson bracket USED-FOR algebraic relations. it COMPARE canonical transformation. canonical transformation COMPARE it. change of coordinates USED-FOR network. network USED-FOR cyclic coordinates. OtherScientificTerm are canonical transformations, and transformed coordinates. Generic are approach, and term. Method is Hamiltonian neural networks. ","This paper studies Hamiltonian dynamics in physical systems, which are conserved quantities that can be used to model the dynamics of physical systems. In particular, the authors consider the case where canonical transformations are not available. The authors propose an approach to learn algebraic relations in the form of a Poisson bracket, which is a generalization of the classical approach to Hamiltonian neural networks. The key idea is to use the Poisson brackets to represent algebraic relation between two variables, and the authors show that it is equivalent to a canonical transformation. The main contribution of the paper is to show that the network can be trained to produce cyclic coordinates with a change of coordinates, and that the transformed coordinates are invariant to the change of the network's parameters. This term is an extension of the term introduced in [1] and [2]."
559,SP:9c8619d2c0df81c1222ba28cecbacc42408d0019,"neural network USED-FOR canonical transformation of the data coordinates. neural networks USED-FOR Hamiltonians. linear and angular momentum HYPONYM-OF symmetries. OtherScientificTerm are Hamiltonian, and canonical transformation. Method is HNNs. ","This paper studies the problem of learning a neural network that can perform canonical transformation of the data coordinates. The main idea is to use neural networks to learn Hamiltonians that are invariant to symmetries such as linear and angular momentum. In particular, the authors show that if the Hamiltonian is orthogonal to the canonical transformation, then HNNs can be trained to approximate it."
560,SP:9c8619d2c0df81c1222ba28cecbacc42408d0019,"energy CONJUNCTION angular momentum. angular momentum CONJUNCTION energy. NN USED-FOR symmetries in physics. 2 and 3 body problems CONJUNCTION harmonic oscillator. harmonic oscillator CONJUNCTION 2 and 3 body problems. shallow feedforward networks HYPONYM-OF training networks. cyclic "" coordinates PART-OF training networks. empirical conservation FEATURE-OF physically conserved quantities. Method is Hamiltonian dynamics. ","This paper studies the properties of NN in the context of symmetries in physics. Specifically, the authors consider 2 and 3 body problems as well as the harmonic oscillator and show that energy and angular momentum are physically conserved quantities with empirical conservation. The authors then propose two types of training networks: shallow feedforward networks (with ""cyclic"" coordinates) and ""shallow feedforward"" (with cyclic) coordinates (with Hamiltonian dynamics)."
561,SP:d1e78b1759eef8fc16e5b7ad7f0e290e9dc5dea0,gradient boosting USED-FOR GNN model. feature PART-OF graph. gradient boosting model USED-FOR feature. feature USED-FOR GNN model. gradient boosting USED-FOR GNN model. running time EVALUATE-FOR Res - GNN / BGNN. Res - GNN / BGNN COMPARE GNN methods. GNN methods COMPARE Res - GNN / BGNN. running time EVALUATE-FOR GNN methods. Method is BGNN. Material is tabular feature and graph - structured datasets. ,"This paper proposes a new GNN model based on gradient boosting that uses a feature extracted from a graph using a gradient boosting model. The proposed Res-GNN/BGNN is a generalization of BGNN and achieves better results on tabular feature and graph-structured datasets. In addition, the running time of the Res-GBN/GNN is also improved compared to other GNN methods."
562,SP:d1e78b1759eef8fc16e5b7ad7f0e290e9dc5dea0,tabular node features FEATURE-OF graphs. Existing methods USED-FOR tabular data. Existing methods USED-FOR graph - structured data. tabular data CONJUNCTION graph - structured data. graph - structured data CONJUNCTION tabular data. graph neural networks ( GNNs ) HYPONYM-OF graph - structured data. gradient boosting decision tree ( GBDT ) HYPONYM-OF tabular data. GBDT USED-FOR graph - structured data. it CONJUNCTION GNN. GNN CONJUNCTION it. ,This paper studies the problem of tabular node features in graphs. Existing methods for tabular data (e.g. gradient boosting decision tree (GBDT) and graph-structured data such as graph neural networks (GNNs) have been shown to perform poorly on both tabular and graph data. This paper aims to address this issue by combining GBDT with graph-structure data by combining it with a GNN. 
563,SP:d1e78b1759eef8fc16e5b7ad7f0e290e9dc5dea0,GBDT CONJUNCTION graph neural network. graph neural network CONJUNCTION GBDT. graph neural network USED-FOR graphs with heterogeneous tabular features. relational bias FEATURE-OF GNNs. functional gradient step PART-OF GBDT. method HYPONYM-OF ensemble tree method. graph neural network USED-FOR latent features. Generic is approaches. Material is graph - structured data. OtherScientificTerm is trees. ,"This paper proposes GBDT and a graph neural network for graphs with heterogeneous tabular features. The authors argue that GNNs have a relational bias, and propose two approaches to address this issue. First, GBDT adds a functional gradient step to the training of the graph-structured data. Second, the authors propose a method that is an ensemble tree method, where the latent features of a graph are learned by the graph network, and the trees are then used to generate new graphs."
564,SP:9f9dbff2fe7defd41b9ed1a6c9dcad07e932dea7,train - validation split USED-FOR linear centroid meta - learning problem. statistical asymptotic theory USED-FOR train - validation split. statistical bias FEATURE-OF train - train method. statistical consistency FEATURE-OF train - validation method. noise - free setting FEATURE-OF methods. statistical consistency FEATURE-OF methods. train - train method COMPARE train - validation method. train - validation method COMPARE train - train method. asymptotic MSE EVALUATE-FOR train - train method. asymptotic MSE EVALUATE-FOR train - validation method. data splitting USED-FOR train - validation method. asymptotic analysis USED-FOR optimal ratio. data splitting USED-FOR optimal ratio. Method is meta - learning. OtherScientificTerm is centroid. ,"This paper proposes a train-validation split for the linear centroid meta-learning problem based on the statistical asymptotic theory. The main contribution of this paper is to study the statistical bias of the train-train method. The authors show that the statistical consistency of the proposed methods in the noise-free setting is lower than that of the standard train-training method. They also show that in the case where the centroid is noisy, the proposed train-test method achieves a better asymptic MSE than the standard standard train -validation method. Finally, the authors propose to use the data splitting to find the optimal ratio of the training and validation data, and provide an empirical study to support their theoretical analysis. "
565,SP:9f9dbff2fe7defd41b9ed1a6c9dcad07e932dea7,"Method are meta - learning, and linear model. Task are optimization of meta - parameters, and correcting model misspecification. OtherScientificTerm is model. Metric is learning rates. Material is synthetic and real data. ","This paper studies the problem of meta-learning, where the goal is to learn a linear model that generalizes well to unseen data points. The paper focuses on the optimization of the meta-parameters, i.e., the parameters of the model that are learned during the training process. The main contribution of the paper is the analysis of the effect of correcting model misspecification on the learning rates. The results are shown on both synthetic and real data."
566,SP:9f9dbff2fe7defd41b9ed1a6c9dcad07e932dea7,train - validation split USED-FOR meta - learning theoretically. meta - learning theoretically USED-FOR meta - learning paradigms. splitting method COMPARE non - splitting method. non - splitting method COMPARE splitting method. Task is linear centroid meta - learning problem. OtherScientificTerm is structural assumptions. ,"This paper studies the linear centroid meta-learning problem and proposes a train-validation split as a way to improve the performance and efficiency of meta-learned paradigms by using the idea of training with a split. The splitting method is shown to outperform the non-splitting method in a variety of experiments. The paper is well-written and well-motivated. However, there are some structural assumptions that need to be addressed in order to make the paper more convincing."
567,SP:bb566eda95867f83a80664b2f685ad373147c87b,momentum of memorization USED-FOR hard examples. hard examples USED-FOR learning. them USED-FOR model training. image classification datasets EVALUATE-FOR accuracy. synthetic and real world label noise FEATURE-OF image classification datasets. Metric is classification accuracy. Generic is method. Material is confident data. ,This paper proposes to use momentum of memorization to generate hard examples for learning. The idea is to use these hard examples during model training and use them to improve the accuracy. The method is evaluated on image classification datasets with synthetic and real world label noise. The results show improvements in classification accuracy on confident data.
568,SP:bb566eda95867f83a80664b2f685ad373147c87b,method USED-FOR noisy labels. Me - Momentum USED-FOR noisy labels. algorithm USED-FOR hard examples. physics USED-FOR momentum. momentum USED-FOR algorithm. physics USED-FOR algorithm. robustness EVALUATE-FOR classifier. state - of - art methods COMPARE Me - Momentum. Me - Momentum COMPARE state - of - art methods. ,"This paper proposes a method, Me-Momentum, to deal with noisy labels. The algorithm uses momentum from physics to generate hard examples. The robustness of the classifier is also evaluated. Compared to state-of-art methods, the performance of Me-momentum is better."
569,SP:bb566eda95867f83a80664b2f685ad373147c87b,"approach USED-FOR hard "" confident "" samples. interactive method USED-FOR classifier. interactive method USED-FOR approach. Task is learning with label noises. Method is deep networks. ","This paper addresses the problem of learning with label noises. The authors propose an approach to generate hard ""confident"" samples by using an interactive method to train a classifier. The idea is interesting and interesting, and the experiments show that deep networks can be trained with this approach."
570,SP:ca57b693e5eff372c872f42d66b18b8aa1d07c87,"k - Nearest Neighbor ( kNN ) CONJUNCTION radius Nearest Neighbor ( rNN ). radius Nearest Neighbor ( rNN ) CONJUNCTION k - Nearest Neighbor ( kNN ). kNN CONJUNCTION rNN. rNN CONJUNCTION kNN. kNN USED-FOR poison attacks. rNN USED-FOR poison attacks. joint certificate USED-FOR rNN. certified accuracy EVALUATE-FOR rNN. certified accuracy EVALUATE-FOR joint certificate. Task is data poisoning attack. OtherScientificTerm are majority vote, poison removal budget, and vote. Generic is it. Method is certified defense methods. ",This paper studies the problem of data poisoning attack. The authors propose to use a joint certificate between a k-Nearest Neighbor (kNN) and a radius Nearest Neighbor (rNN). The idea is that kNN and rNN can be used together to defend against poison attacks. The key idea is to use the majority vote as the poison removal budget and to use it as a proxy for the quality of the vote. The proposed joint certificate improves the certified accuracy of the rNN. The experimental results show that the proposed certified defense methods are effective.
571,SP:ca57b693e5eff372c872f42d66b18b8aa1d07c87,k - NN CONJUNCTION r - NN. r - NN CONJUNCTION k - NN. r - NN USED-FOR data poisoning attacks. k - NN USED-FOR data poisoning attacks. robustness EVALUATE-FOR r - NN. robustness EVALUATE-FOR k - NN. k - NN CONJUNCTION r - NN. r - NN CONJUNCTION k - NN. k - NN CONJUNCTION r - NN. r - NN CONJUNCTION k - NN. k - NN USED-FOR data poisoning attacks. r - NN USED-FOR data poisoning attacks. OtherScientificTerm is certification guarantee. ,This paper studies the robustness of k-NN and r-NN against data poisoning attacks. The authors provide a certification guarantee and show that both k-NN and r -NN are robust to data poisoning.
572,SP:ca57b693e5eff372c872f42d66b18b8aa1d07c87,"certifiable robust model USED-FOR data poisoning attacks. nearest neighbors USED-FOR certifiable robust model. voting mechanism USED-FOR nearest neighbor models. MNIST CONJUNCTION CIFAR. CIFAR CONJUNCTION MNIST. DPA CONJUNCTION Bagging. Bagging CONJUNCTION DPA. CA COMPARE approaches. approaches COMPARE CA. approaches COMPARE Bagging. Bagging COMPARE approaches. approaches COMPARE DPA. DPA COMPARE approaches. Metric are training model's accuracy, and Certified Accuracy ( CA ). ","This paper proposes a certifiable robust model based on nearest neighbors to defend against data poisoning attacks. The nearest neighbor models are trained using a voting mechanism, and the training model's accuracy is evaluated on MNIST and CIFAR. Certified Accuracy (CA) is defined as the difference between the accuracy of the nearest neighbors and the nearest neighbor's accuracy. The authors compare CA with other approaches such as DPA and Bagging."
573,SP:6cfe70be8ac34d6f61009e7e583e537e9adeb648,training efficiency EVALUATE-FOR graph neural networks ( GNN ). accuracy EVALUATE-FOR graph neural networks ( GNN ). training efficiency CONJUNCTION accuracy. accuracy CONJUNCTION training efficiency. layer - wise and graph - wise sampling USED-FOR training. efficiency EVALUATE-FOR algorithms. computation time CONJUNCTION variance. variance CONJUNCTION computation time. computation time EVALUATE-FOR algorithms. variance EVALUATE-FOR algorithms. batch sizes USED-FOR algorithms. efficiency EVALUATE-FOR batch sizes. estimator COMPARE one. one COMPARE estimator. one USED-FOR GNN. batch size USED-FOR GNN. OtherScientificTerm is randomness of two - consecutive layers. Method is NN. ,"This paper studies the trade-off between training efficiency and accuracy in graph neural networks (GNN). The authors propose to use layer-wise and graph-wise sampling during training to improve the efficiency and variance of the algorithms in terms of computation time and variance. The authors also propose a new estimator for GNN, which is based on the randomness of two-consecutive layers, and show that this estimator is more accurate than the one used in the original NN. Finally, the authors evaluate the efficiency of different batch sizes for different GNNs."
574,SP:6cfe70be8ac34d6f61009e7e583e537e9adeb648,batch size selection USED-FOR graph neural networks. GPUs CONJUNCTION TPUs. TPUs CONJUNCTION GPUs. batch size USED-FOR parallelism. TPUs HYPONYM-OF computer architectures. GPUs HYPONYM-OF computer architectures. batch size USED-FOR graph neural networks. batch size USED-FOR graph neural networks. OtherScientificTerm is average node degree. ,"This paper studies the effect of batch size selection on the performance of graph neural networks. In particular, the authors consider two computer architectures: GPUs and TPUs. The authors show that the choice of the batch size can lead to better performance and better parallelism, and that the average node degree can be used to measure the quality of the training data. The paper also provides a theoretical analysis of the impact of the different batch size on the final performance of the graph network."
575,SP:6cfe70be8ac34d6f61009e7e583e537e9adeb648,batch size USED-FOR graph neural networks. strategy USED-FOR batch size. strategy USED-FOR graph neural networks. SGD USED-FOR graph neural networks. strategy USED-FOR node classification. average degree FEATURE-OF graph. training time CONJUNCTION accuracy. accuracy CONJUNCTION training time. average degree USED-FOR batch size. accuracy EVALUATE-FOR guidelines. training time EVALUATE-FOR guidelines. computation cost CONJUNCTION variance of the gradients. variance of the gradients CONJUNCTION computation cost. lower bound USED-FOR metric. Material is real world graphs. OtherScientificTerm is CPU /GPU memory. Task is constructing mini - batches. Metric is metric - pseudo precision rate. ,"This paper proposes a new strategy to reduce the batch size of graph neural networks trained with SGD. The main idea is to use the average degree of the graph as a proxy for batch size, which can be used for node classification. Experiments on real world graphs show that the proposed guidelines can reduce the training time and accuracy, while also reducing the computation cost and variance of the gradients. The authors also provide a lower bound for the metric, which is based on the notion of CPU/GPU memory, and show that constructing mini-batches can significantly reduce the metric-pseudo precision rate."
576,SP:30d97322709cd292a49f936c767099f11b0e2913,"confidence scores USED-FOR missclassification errors. methodology USED-FOR confidence scores. neural networks USED-FOR missclassification errors. kernels USED-FOR sparse ) GP. variance of the confidence score USED-FOR RED. OtherScientificTerm are RIO, and maximum class probability. Generic is methods. ","This paper proposes a methodology for learning confidence scores for missclassification errors in neural networks. The key idea is to train a (sparse) GP with different kernels for each class, and then use the variance of the confidence score (RED) to estimate the RIO. The authors show that the maximum class probability of a class is a function of the maximum confidence score of the class. The methods are evaluated on a variety of datasets."
577,SP:30d97322709cd292a49f936c767099f11b0e2913,"GP USED-FOR classification model. UCI datasets CONJUNCTION CIFAR-10 dataset. CIFAR-10 dataset CONJUNCTION UCI datasets. CIFAR-10 dataset EVALUATE-FOR methods. UCI datasets EVALUATE-FOR methods. Task are calibration, accuracy, and regression problems. Generic is model. Method is RED. OtherScientificTerm are predicted confidence score, and true class target confidence score. ","This paper proposes a new classification model based on GP to improve the calibration and accuracy. The proposed model, RED, is based on the idea that the predicted confidence score should be close to the true class target confidence score. The authors show that the proposed methods outperform existing methods on both UCI datasets and the CIFAR-10 dataset. They also show that RED can be applied to regression problems."
578,SP:30d97322709cd292a49f936c767099f11b0e2913,"NN USED-FOR predicting uncertainty. confidence score USED-FOR detecting misclassification errors. framework USED-FOR confidence score. framework USED-FOR detecting misclassification errors. Method are NN classifier, and Gaussian processes. ","This paper proposes a framework for estimating the confidence score for detecting misclassification errors using an NN classifier. The main idea is to use NN for the task of predicting uncertainty, and then use Gaussian processes to estimate the confidence of the class. The paper is well-written and easy to follow."
579,SP:131b3da98f56d3af273171f496b217b90754a0a7,retriever PART-OF components. components USED-FOR Information Retrieval systems. they CONJUNCTION machine learned components. machine learned components CONJUNCTION they. BM25 HYPONYM-OF metrics. metrics USED-FOR retrievers. direct supervision information USED-FOR retriever. Generic is datasets. ,"This paper proposes a new class of Information Retrieval systems that consists of two components: a retriever and a machine learned component. The retriever is trained with direct supervision information and the machine learned components are trained on a set of datasets. The retrievers are trained using two metrics, BM25 and BM25-10, and they are compared with two other state-of-the-art retrievers."
580,SP:131b3da98f56d3af273171f496b217b90754a0a7,retriever HYPONYM-OF IR system. BM25 HYPONYM-OF IR system. inverse cloze task CONJUNCTION DPR. DPR CONJUNCTION inverse cloze task. attention weights USED-FOR relevance signals. relevance signals USED-FOR retriever. attention weights USED-FOR retriever. it USED-FOR retriever. attention weights USED-FOR it. Task is open - domain QA. ,"This paper proposes BM25, an IR system (retriever trained on BM25) for open-domain QA. Specifically, it uses attention weights to generate relevance signals for the retriever, which are then used in inverse cloze task and DPR. "
581,SP:131b3da98f56d3af273171f496b217b90754a0a7,"training technique USED-FOR information retrieval models. information retrieval models USED-FOR ( open domain ) question answering. Method is reader model. OtherScientificTerm are internal information, retriever, and attention activations. Generic is model. ",This paper proposes a new training technique for information retrieval models for (open domain) question answering. The key idea is to train a reader model that takes internal information as input and outputs a retriever that predicts the answer to the question. The model is trained by minimizing the number of attention activations.
582,SP:a516fff3cabc13cea1b8ed07dbf9eb1acb7dbb0e,"formal language FEATURE-OF constraints. constraints USED-FOR constrained reinforcement learning ( RL ) formulation. deterministic finite automaton USED-FOR constraint. constrained Markov decision processes USED-FOR formulation. reward objective USED-FOR that. MuJoCo CONJUNCTION Atari environments. Atari environments CONJUNCTION MuJoCo. Safety Gym CONJUNCTION MuJoCo. MuJoCo CONJUNCTION Safety Gym. Generic is solution. OtherScientificTerm are automaton's sparse binary cost, and approximate dense cost. ","This paper proposes a constrained reinforcement learning (RL) formulation that uses constraints in a formal language. The formulation is based on constrained Markov decision processes, where the constraint is represented as a deterministic finite automaton. The solution is to minimize the automaton's sparse binary cost, and then use that as a reward objective. Experiments are conducted on Safety Gym, MuJoCo, and Atari environments. The results show that the approximate dense cost is optimal."
583,SP:a516fff3cabc13cea1b8ed07dbf9eb1acb7dbb0e,"formal language FEATURE-OF parser. parser USED-FOR cost functions. formal language USED-FOR model checking. formal language USED-FOR constraints. Method are constrained MDP framework, and deterministic finite automata ( DFA ). ","This paper proposes a constrained MDP framework, which is a generalization of deterministic finite automata (DFA). The main idea is to use a formal language of the parser to model the cost functions, which can be used for model checking. The formal language allows the constraints to be expressed in terms of constraints. The paper is well-written and easy to follow."
584,SP:a516fff3cabc13cea1b8ed07dbf9eb1acb7dbb0e,"formal languages USED-FOR constraints. constraints USED-FOR constrained MDP setting. DFAs HYPONYM-OF formal languages. identification CONJUNCTION safety verification. safety verification CONJUNCTION identification. MDP CONJUNCTION DFA. DFA CONJUNCTION MDP. solution USED-FOR cMDPs. reward shaping CONJUNCTION Lagrangian methods. Lagrangian methods CONJUNCTION reward shaping. cost CONJUNCTION solution. solution CONJUNCTION cost. reward shaping USED-FOR cMDPs. Lagrangian methods HYPONYM-OF solution. reward shaping HYPONYM-OF solution. performance CONJUNCTION constraint satisfaction. constraint satisfaction CONJUNCTION performance. recogniser state USED-FOR policy. Generic is strategy. OtherScientificTerm are constraint, translation, and hyper parameters. ","This paper studies the problem of learning formal languages for constraints in a constrained MDP setting (i.e., DFAs). The authors propose a strategy to learn a policy that satisfies a constraint in an MDP and a DFA. The key idea is to use a translation from the MDP to the DFA, where the constraint can be expressed in terms of identification and safety verification. The cost and solution to solving cMDPs (reward shaping and Lagrangian methods) are discussed. The authors show that the policy can be learnt from a recogniser state, and that the performance and constraint satisfaction can be improved by changing the hyper parameters."
585,SP:e18cfc1502c4087422d3baf655c244d4f3924a76,Method is tree ensemble. ,"This paper studies the problem of learning a tree ensemble. The main contribution of this paper is to provide a theoretical analysis of the problem. The paper is well-written and well-motivated. However, the experimental results are not convincing. "
586,SP:e18cfc1502c4087422d3baf655c244d4f3924a76,Cascading Decision Tree HYPONYM-OF decision trees. application EVALUATE-FOR method. Method is cascade of small decision trees. OtherScientificTerm is tree. ,"This paper proposes a cascade of small decision trees, i.e., Cascade Decision Tree, which is an extension of decision trees called Cascading Decision Tree. The main idea is to learn a tree that is composed of a sequence of sub-trees. The method is evaluated on a simple application where the goal is to predict the outcome of a given action."
587,SP:e18cfc1502c4087422d3baf655c244d4f3924a76,cascading decision tree HYPONYM-OF classification model. cascading decision tree HYPONYM-OF rule - based classifier. overlapping hierarchical structure FEATURE-OF rule - based classifier. induction algorithm USED-FOR them. UCI datasets CONJUNCTION propietary dataset. propietary dataset CONJUNCTION UCI datasets. Generic is models. ,"This paper proposes a new classification model called cascading decision tree, which is a rule-based classifier with an overlapping hierarchical structure. The authors show that the proposed models can be trained using an induction algorithm, and show that they can achieve state-of-the-art performance on UCI datasets and a propietary dataset."
588,SP:0508336b2ec032b9b98a1039e94ea223f3987cec,"neural network width USED-FOR network. width USED-FOR models. random static mask FEATURE-OF widened networks. OtherScientificTerm are model width, number of parameters, Gaussian Process kernel, and infinith - width limit. Method is one - hidden - layer neural network. ","This paper studies the effect of the neural network width on the performance of the network. The authors show that the width of the models can have a significant impact on the model width. They show that when the number of parameters is small, the width can be reduced to a Gaussian Process kernel. They also show that for a one-hidden-layer neural network, the infinith-width limit can be reached. Finally, they show that widened networks can be trained with a random static mask."
589,SP:0508336b2ec032b9b98a1039e94ea223f3987cec,"neural networks USED-FOR kernel - based learning. infinite - width limit FEATURE-OF kernel - based learning. Method is widening networks. Generic are networks, and algorithms. ",This paper studies the problem of kernel-based learning in neural networks in the infinite-width limit. The main contribution of this paper is to study the effect of widening networks. The authors show that the width of the networks can be increased by a factor of at least 1/\sqrt{n}. The authors then propose two algorithms to address this issue.
590,SP:0508336b2ec032b9b98a1039e94ea223f3987cec,wide and sparse approach USED-FOR neural networks. Method is neural network. ,"This paper proposes a wide and sparse approach to training neural networks. The main idea is to train a neural network on a large number of data points, and then train the weights on the remaining data points. The paper is well-written and easy to follow."
591,SP:92e5a610ed13ada6d25d433b03ac06fa5eebd963,approach USED-FOR language models. language models USED-FOR knowledge graphs. natural language texts ( English Wikipedia ) USED-FOR context representations. knowledge graphs ( Wikidata ) USED-FOR entity representations. knowledge graph question answering CONJUNCTION entity classification. entity classification CONJUNCTION knowledge graph question answering. few - shot relation classification CONJUNCTION knowledge graph question answering. knowledge graph question answering CONJUNCTION few - shot relation classification. approach COMPARE baseline methods. baseline methods COMPARE approach. natural language understanding tasks EVALUATE-FOR baseline methods. natural language understanding tasks EVALUATE-FOR approach. entity classification HYPONYM-OF natural language understanding tasks. few - shot relation classification HYPONYM-OF natural language understanding tasks. knowledge graph question answering HYPONYM-OF natural language understanding tasks. ,"This paper proposes an approach to train language models to learn knowledge graphs for entity representations from natural language texts (English Wikipedia) and context representations from knowledge graphs (Wikidata). The authors evaluate their approach on three natural language understanding tasks: few-shot relation classification, knowledge graph question answering, and entity classification. They show that their approach outperforms baseline methods on all three of these natural language learning tasks."
592,SP:92e5a610ed13ada6d25d433b03ac06fa5eebd963,language module CONJUNCTION knowledge module. knowledge module CONJUNCTION language module. parts PART-OF JAKET. parts PART-OF model. language module HYPONYM-OF parts. knowledge module HYPONYM-OF parts. language module PART-OF JAKET. language module PART-OF model. knowledge module PART-OF JAKET. knowledge module PART-OF model. relation type prediction CONJUNCTION masked token prediction. masked token prediction CONJUNCTION relation type prediction. masked token prediction CONJUNCTION masked entity prediction. masked entity prediction CONJUNCTION masked token prediction. entity category prediction CONJUNCTION relation type prediction. relation type prediction CONJUNCTION entity category prediction. tasks USED-FOR model. entity category prediction HYPONYM-OF tasks. masked entity prediction HYPONYM-OF tasks. masked token prediction HYPONYM-OF tasks. relation type prediction HYPONYM-OF tasks. framework USED-FOR fine - tuning. knowledge graphs USED-FOR fine - tuning. Generic is method. OtherScientificTerm is shared latent semantic space. ,"This paper proposes a method called JAKET, which consists of two parts: a language module and a knowledge module. The model consists of three parts: the language module, the knowledge module, and the model is trained on a set of tasks: entity category prediction, relation type prediction, masked token prediction, and masked entity prediction. The authors propose a framework for fine-tuning on knowledge graphs, which is based on a shared latent semantic space."
593,SP:92e5a610ed13ada6d25d433b03ac06fa5eebd963,knowledge graph information USED-FOR language modeling pretraining method. entity embedding PART-OF hidden layer of BERT context embedding. entity embedding CONJUNCTION graph attention embedding. graph attention embedding CONJUNCTION entity embedding. knowledge graph USED-FOR graph attention embedding. entity classification CONJUNCTION relation type prediction. relation type prediction CONJUNCTION entity classification. language related tasks CONJUNCTION knowledge graph tasks. knowledge graph tasks CONJUNCTION language related tasks. predicting masked tokens HYPONYM-OF language related tasks. predicting masked tokens PART-OF pretraining tasks. knowledge graph tasks PART-OF pretraining tasks. entity classification HYPONYM-OF knowledge graph tasks. relation type prediction HYPONYM-OF knowledge graph tasks. language related tasks PART-OF pretraining tasks. question answering CONJUNCTION entity classification. entity classification CONJUNCTION question answering. entity classification COMPARE pretraining counterparts. pretraining counterparts COMPARE entity classification. question answering COMPARE pretraining counterparts. pretraining counterparts COMPARE question answering. few - shot learning tasks EVALUATE-FOR pretraining counterparts. question answering HYPONYM-OF few - shot learning tasks. entity classification HYPONYM-OF few - shot learning tasks. ,"This paper proposes a language modeling pretraining method that leverages the knowledge graph information. The key idea is to embed the entity embedding in the hidden layer of BERT context embedding and the graph attention embedding on top of this knowledge graph. Experiments are conducted on language related tasks, knowledge graph tasks (e.g. predicting masked tokens), entity classification and relation type prediction. Results on few-shot learning tasks (question answering, entity classification, relation type) show that the proposed entity classification outperforms the pretraining counterparts."
594,SP:1db95a377f3d5ed129aa0511f840f647375e3528,generation order USED-FOR sequence generation tasks. latent variables USED-FOR sequence generation tasks. latent variables USED-FOR generation order. Variational Order Inference ( VOI ) USED-FOR ELBO. one - step Markov Decision problem USED-FOR it. policy gradient USED-FOR it. Gumbel - matching techniques USED-FOR close - form of the posterior distribution. Task is optimizing discrete latent variables. OtherScientificTerm is discrete latent variables. ,"This paper studies the problem of optimizing discrete latent variables for generating sequence generation tasks with latent variables. The authors propose Variational Order Inference (VOI) to estimate the ELBO and use it as a one-step Markov Decision problem, where it is trained using policy gradient. Gumbel-matching techniques are used to approximate the close-form of the posterior distribution."
595,SP:1db95a377f3d5ed129aa0511f840f647375e3528,"Gumbel - Sinkorn distribution USED-FOR Gumbel - Matching distribution. Gumbel - Matching distribution USED-FOR posterior distribution. policy gradient CONJUNCTION baseline. baseline CONJUNCTION policy gradient. ELBO USED-FOR encoder and decoder networks. baseline USED-FOR ELBO. policy gradient USED-FOR ELBO. VOI COMPARE Transformer - InDIGO. Transformer - InDIGO COMPARE VOI. Django and MS - COCO 2017 dataset EVALUATE-FOR VOI. Task is content and ordering of language models. Method is Variational Order Inference ( VOI ). OtherScientificTerm are latent sequence variable, and best - first generation order. ","This paper proposes Variational Order Inference (VOI), which aims to improve the content and ordering of language models by inferring the posterior distribution of the latent sequence variable from the Gumbel-Sinkorn distribution. The posterior distribution is modeled as a variant of the GUMBEL-Matching distribution, which is based on the best-first generation order. The ELBO of the encoder and decoder networks is derived from the policy gradient and baseline. The authors evaluate VOI on the Django and MS-COCO 2017 dataset and compare VOI with Transformer-InDIGO."
596,SP:1db95a377f3d5ed129aa0511f840f647375e3528,generative model USED-FOR sequence generation task. auto - regressive order USED-FOR latent variables. auto - regressive order USED-FOR generative model. policy gradient algorithm USED-FOR variational lower bound. combinatorical optimization techniques USED-FOR policy gradient algorithm. method COMPARE adaptive - order method transformer - InDIGO. adaptive - order method transformer - InDIGO COMPARE method. method COMPARE fixed - order generation. fixed - order generation COMPARE method. image caption CONJUNCTION code generation. code generation CONJUNCTION image caption. code generation EVALUATE-FOR method. image caption EVALUATE-FOR method. fixed - order generation COMPARE adaptive - order method transformer - InDIGO. adaptive - order method transformer - InDIGO COMPARE fixed - order generation. global and local level FEATURE-OF orders. COCO2017 dataset EVALUATE-FOR orders. Method is best - first strategy. ,This paper proposes a generative model for the sequence generation task using an auto-regressive order for the latent variables. The authors propose a policy gradient algorithm based on combinatorical optimization techniques to derive a variational lower bound. The proposed method is evaluated on image caption and code generation and compared to fixed-order generation and adaptive-order method transformer-InDIGO. The results show that the best-first strategy leads to better performance. The orders are evaluated on both global and local level on COCO2017 dataset.
597,SP:1c310f02acda4aa14e4d043c8d6de8c94a8ecf44,stochastic training methods USED-FOR graph neural networks. convergence FEATURE-OF stochastic training methods. compositional optimization problem USED-FOR GNN. SPIDER USED-FOR variance. SPIDER USED-FOR convergence speed. neighbor sampling USED-FOR variance. SPIDER USED-FOR variance. SPIDER USED-FOR GNNs. method COMPARE gradient descent method. gradient descent method COMPARE method. convergence rate EVALUATE-FOR gradient descent method. theoretical convergence analysis FEATURE-OF SPIDER. convergence rate EVALUATE-FOR method. Generic is algorithm. ,"This paper studies the convergence of stochastic training methods for graph neural networks. The authors propose a compositional optimization problem for training a GNN. The algorithm, SPIDER, aims to improve the convergence speed by minimizing the variance induced by neighbor sampling. The theoretical convergence analysis of SPIDER is provided. Experiments show that SPIDER can speed up the training of GNNs and achieve a better convergence rate than the standard gradient descent method."
598,SP:1c310f02acda4aa14e4d043c8d6de8c94a8ecf44,Node sampling USED-FOR GCNs. sampling methods COMPARE theorectical convergence analysis. theorectical convergence analysis COMPARE sampling methods. function approximation error CONJUNCTION layer - gradient error. layer - gradient error CONJUNCTION function approximation error. layer - gradient error FEATURE-OF convergence speed. function approximation error FEATURE-OF convergence speed. historical gradients USED-FOR doubly variance reduction. historical hidden features CONJUNCTION historical gradients. historical gradients CONJUNCTION historical hidden features. historical hidden features USED-FOR doubly variance reduction. Method is baseline sampling - based GCNs. ,"Node sampling is a popular technique for training GCNs. However, the sampling methods are not well studied in the literature, and theorectical convergence analysis has not been conducted. This paper studies the convergence speed with respect to the function approximation error and the layer-gradient error. The authors propose to use historical hidden features and historical gradients for doubly variance reduction. Experiments are conducted on baseline sampling-based GCNs and show promising results."
599,SP:1c310f02acda4aa14e4d043c8d6de8c94a8ecf44,VRGCN USED-FOR historical latent representations of nodes. full Laplacian USED-FOR historical latent representations of nodes. node embedding approximation CONJUNCTION layer - wise gradient computation. layer - wise gradient computation CONJUNCTION node embedding approximation. layer - wise gradient computation PART-OF back - propagation. node embedding approximation USED-FOR variance reduction. layer - wise gradient computation USED-FOR variance reduction. faster convergence rate EVALUATE-FOR algorithms. Method is variance reduction method. OtherScientificTerm is sampled sparse Laplacian. ,"This paper proposes a variance reduction method based on VRGCN, which uses a sampled sparse Laplacian instead of full LaplACian to learn historical latent representations of nodes in VRGCNN. The variance reduction is achieved by using node embedding approximation and layer-wise gradient computation in back-propagation. The proposed algorithms achieve faster convergence rate compared to existing algorithms."
600,SP:02e100a9ad4eedab8cba043d3726f022bc09a3af,edges CONJUNCTION segmentation masks. segmentation masks CONJUNCTION edges. image PART-OF primitive representation. edges PART-OF primitive representation. segmentation masks PART-OF primitive representation. model USED-FOR image. editing primitives USED-FOR model. Method is conditional a generative model. OtherScientificTerm is augmentation. ,"This paper proposes a conditional a generative model where the primitive representation of an image consists of edges, segmentation masks, etc. The model is trained by editing primitives of the original image, and then the model is used to generate a new image from the edited image. The augmentation is done in two stages."
601,SP:02e100a9ad4eedab8cba043d3726f022bc09a3af,method USED-FOR conditional generative models. primitive USED-FOR generation. Generic is models. Method is generative model. Material is SinGAN. ,This paper proposes a method for training conditional generative models. The idea is to train the models on a set of samples from a generative model and then use a primitive to guide generation. The method is called SinGAN.
602,SP:02e100a9ad4eedab8cba043d3726f022bc09a3af,augmentation method USED-FOR single image training. edges CONJUNCTION segmentation. segmentation CONJUNCTION edges. network USED-FOR primitive representation of the image. edges HYPONYM-OF primitive representation of the image. segmentation HYPONYM-OF primitive representation of the image. network USED-FOR it. primitive input representation USED-FOR generator. ,"This paper proposes an augmentation method for single image training. Specifically, it uses a network to learn a primitive representation of the image (e.g., edges, segmentation). The generator is trained on the primitive input representation."
603,SP:4d7c1e30fa8eb3e7c67a4ec3bccc5d3ef713a773,"induced subgraph USED-FOR maximum common subgraph detection problem. heuristics USED-FOR subgraph. graph neural network embeddings CONJUNCTION RL. RL CONJUNCTION graph neural network embeddings. Method are branch & bound algorithms, and deep Q - network. Material is synthetic and real world pairs of graphs. Generic is it. ",This paper studies the maximum common subgraph detection problem with induced subgraph. The authors propose branch & bound algorithms and show that it is possible to train a deep Q-network to detect the subgraph using heuristics. The paper also conducts experiments on both graph neural network embeddings and RL and shows promising results on both synthetic and real world pairs of graphs.
604,SP:4d7c1e30fa8eb3e7c67a4ec3bccc5d3ef713a773,maximum common subgraph USED-FOR biochemical domain. method COMPARE baselines. baselines COMPARE method. time - complexity analysis EVALUATE-FOR method. time - complexity analysis EVALUATE-FOR baselines. searching time EVALUATE-FOR method. searching time EVALUATE-FOR baselines. ,This paper proposes a method to estimate the maximum common subgraph in the biochemical domain. The proposed method is compared with several baselines in terms of searching time and time-complexity analysis.
605,SP:4d7c1e30fa8eb3e7c67a4ec3bccc5d3ef713a773,learning - based approach USED-FOR Maximum Common Subgraph ( MCS ) detection. GNNs USED-FOR reinforcement learning framework. artificial and real - world graphs EVALUATE-FOR model. Method is GLSEARCH. ,This paper proposes a learning-based approach for Maximum Common Subgraph (MCS) detection. The proposed reinforcement learning framework is based on GNNs. The model is evaluated on both artificial and real-world graphs. The experimental results show the effectiveness of GLSEARCH.
606,SP:581c6d218e75b0df808bc2c83c8731a94e94a5b3,3D point cloud USED-FOR supervised neural network. vertex location CONJUNCTION edge existence. edge existence CONJUNCTION vertex location. raw unordered 3D point cloud USED-FOR network. FCGF architecture USED-FOR it. datasets EVALUATE-FOR network. 3D models HYPONYM-OF datasets. ABC dataset HYPONYM-OF datasets. Google 3D warehouse USED-FOR 3D models. edges CONJUNCTION wireframe graph structure. wireframe graph structure CONJUNCTION edges. it COMPARE baseline methods. baseline methods COMPARE it. evaluation metrics EVALUATE-FOR accuracy. evaluation metrics EVALUATE-FOR wireframe graph structure. accuracy EVALUATE-FOR wireframe graph structure. accuracy EVALUATE-FOR it. evaluation metrics EVALUATE-FOR it. evaluation metrics EVALUATE-FOR baseline methods. Generic is method. ,This paper proposes a supervised neural network that takes as input a raw unordered 3D point cloud and uses it as input to the FCGF architecture. The proposed method is based on the observation that the difference between vertex location and edge existence can be measured by the difference in the accuracy of the network on two datasets: 1) 3D models from the Google 3D warehouse and 2) the ABC dataset. The experiments show that it outperforms baseline methods in terms of accuracy on both evaluation metrics for both edges and wireframe graph structure.
607,SP:581c6d218e75b0df808bc2c83c8731a94e94a5b3,deep architecture USED-FOR wireframe model. 3D point cloud USED-FOR wireframe model. approach CONJUNCTION evaluation. evaluation CONJUNCTION approach. ,This paper proposes a deep architecture for learning a wireframe model from a 3D point cloud. The approach is well motivated and the evaluation is thorough. The paper is well-written and easy to follow.
608,SP:581c6d218e75b0df808bc2c83c8731a94e94a5b3,PC2WF HYPONYM-OF neural network. neural network USED-FOR 3D point clouds. neural network USED-FOR wireframe model. feature vector USED-FOR PC2WF. them USED-FOR PC2WF. point features PART-OF confidence value. PC2WF USED-FOR wireframe represesntation. ,"This paper proposes PC2WF, a neural network for 3D point clouds, which is a wireframe model that uses a trained neural network to generate the final wireframe. The main idea is to use a feature vector as a confidence value and use them to train PC2WF. The point features are added to the confidence value. The experiments show that the proposed method can improve the wireframe represesntation performance."
609,SP:3e0fd62d9815d7de5e5139a1d6d2e80eea917154,noise statistics PART-OF adaptive stepsize design. noise statistics PART-OF gradient - based stochastic optimization algorithms. second - moment - dependent learning rate USED-FOR SGD. exponential moving average USED-FOR second moment. ,"This paper studies the role of noise statistics in adaptive stepsize design in gradient-based stochastic optimization algorithms. In particular, the authors propose a second-moment-dependent learning rate for SGD. The second moment is modeled as an exponential moving average."
610,SP:3e0fd62d9815d7de5e5139a1d6d2e80eea917154,"online estimation procedure USED-FOR gradient norm second moments. second moments FEATURE-OF stochastic gradient norms. Finite - time convergence rates USED-FOR algorithms. adaptive stepsize USED-FOR algorithms. Task is stochastic optimization. OtherScientificTerm are gradient noise process, stepsize sequence, ` ` idealized '' stepsize sequence, and non - stationarity. ","This paper studies the problem of stochastic optimization when the gradient noise process is non-stationary. The authors propose an online estimation procedure to estimate the gradient norm second moments. Finite-time convergence rates for algorithms with adaptive stepsize are provided. The main contribution of this paper is to provide a ``idealized'' stepsize sequence, which is a generalization of the `` idealized''. The authors also provide a theoretical analysis of the effect of non-stability."
611,SP:3e0fd62d9815d7de5e5139a1d6d2e80eea917154,"Method are adaptive learning steps, and RMSprop algorithm. OtherScientificTerm is noise level indicators. ","This paper proposes adaptive learning steps, where the RMSprop algorithm learns to predict the next state of the art. The idea is to use noise level indicators to guide the learning process. The authors also propose to use the current state-of-the-art noise level indicator as a proxy for the future state. The paper is well-written and easy to follow."
612,SP:71c4e6ab911962d730461eda0f2d72d810fc017c,word alignment USED-FOR NMT system. SMT USED-FOR word alignment. SMT COMPARE NMT. NMT COMPARE SMT. method USED-FOR word - word alignment. decoder PART-OF transformer - based NMT model. FastAlign HYPONYM-OF SMT models. SMT models USED-FOR word - word alignment. English - Romanian EVALUATE-FOR baseline. Generic is it. ,"This paper studies the problem of word alignment in the NMT system. The authors propose a method for word-word alignment using SMT instead of NMT. Specifically, they propose a transformer-based NMT model that consists of a decoder and a word-aligner. They show that SMT can be used for word alignment better than NMT, but it is computationally expensive. They also show that two SMT models (FastAlign and FastAlign+) are able to perform word - word alignment. Finally, they evaluate their baseline on English-Romanian and show that it outperforms the baseline."
613,SP:71c4e6ab911962d730461eda0f2d72d810fc017c,word alignment information USED-FOR word substitution model. representation CONJUNCTION Transformer. Transformer CONJUNCTION representation. summation USED-FOR Transformer. joint attention mechanism USED-FOR Transformer. Romanian / English and Korean / English tasks EVALUATE-FOR baseline Transformer. OtherScientificTerm is cross entropy loss. ,This paper proposes a word substitution model that takes word alignment information into account. The key idea is to combine the representation of the Transformer with the summation. The Transformer is trained with a joint attention mechanism and the cross entropy loss is applied. The experimental results on Romanian/English and Korean/English tasks show improvements over the baseline Transformer.
614,SP:71c4e6ab911962d730461eda0f2d72d810fc017c,"prior word alignments USED-FOR NMT models. Task is NMT context. OtherScientificTerm is prior alignments. Generic is method. Method are SMT methods, and SMT literature. ","This paper studies the problem of learning NMT models with prior word alignments in the NMT context. The authors propose a method that is motivated by the observation that prior alignments can be useful in the context where the number of words in a sentence can be very large. The proposed method is based on the idea that SMT methods are biased towards aligning words that are similar to each other, which is an observation that has been observed in the SMT literature. "
615,SP:c26255a8ad441f11cfbe18fd6dad14773aca4a2b,"benchmark EVALUATE-FOR reinforcement learning techniques ( MDP - playground ). It USED-FOR MDPs. MDP USED-FOR learning algorithms. stochasticity CONJUNCTION delayed reward. delayed reward CONJUNCTION stochasticity. reward sparsity CONJUNCTION stochasticity. stochasticity CONJUNCTION reward sparsity. reward sparsity HYPONYM-OF dimensions. delayed reward HYPONYM-OF dimensions. stochasticity HYPONYM-OF dimensions. domain EVALUATE-FOR classical algorithms. Generic are methods, and toolbox. ","This paper presents a new benchmark for evaluating reinforcement learning techniques (MDP-playground). It is designed for MDPs with different dimensions, including reward sparsity, stochasticity, and delayed reward. The motivation is that learning algorithms in this MDP may not generalize well to new environments. The proposed methods are evaluated on a variety of domains, and compared to classical algorithms in the same domain. The paper also presents a toolbox for evaluating the performance of the proposed methods."
616,SP:c26255a8ad441f11cfbe18fd6dad14773aca4a2b,benchmark tasks EVALUATE-FOR reinforcement learning algorithms. environments USED-FOR discrete and continuous RL agents. reward sparsity CONJUNCTION stochasticity. stochasticity CONJUNCTION reward sparsity. complexity - reward delays CONJUNCTION reward sparsity. reward sparsity CONJUNCTION complexity - reward delays. environments PART-OF MDP Playground. playground environments USED-FOR RL agents. noise CONJUNCTION reward delays. reward delays CONJUNCTION noise. reward delays CONJUNCTION action max values. action max values CONJUNCTION reward delays. noise USED-FOR agent. Generic is framework. Material is Atari and Mujoco tasks. ,"This paper presents a framework for evaluating the performance of reinforcement learning algorithms on benchmark tasks. The authors propose a set of environments for both discrete and continuous RL agents that are designed to encourage the agent to explore new environments. The environments are part of the MDP Playground, which is a collection of environments designed for RL agents to explore in order to address the challenges of complexity-reward delays, reward sparsity, and stochasticity. The agent is trained on these environments and evaluated on Atari and Mujoco tasks, where the agent is encouraged to explore environments with noise, reward delays, and action max values."
617,SP:c26255a8ad441f11cfbe18fd6dad14773aca4a2b,MDP Playground HYPONYM-OF procedurally generated MDPs. procedurally generated MDPs USED-FOR dimension of difficulty. perturbations FEATURE-OF MDP. perturbations USED-FOR learning algorithms. Method is RL algorithms. Generic is it. ,"This paper studies the dimension of difficulty in procedurally generated MDPs (e.g., MDP Playground). The main contribution of this paper is to study the effect of perturbations to the MDP on the performance of RL algorithms. The authors show that learning algorithms are sensitive to the perturbation and that it can lead to performance degradation."
618,SP:e8cbe62252aa671a6deaf12b97063063dfc6d1b0,"approach USED-FOR calibration in regression problems. nonsmooth PDFs CONJUNCTION truncation of support. truncation of support CONJUNCTION nonsmooth PDFs. calibration dataset USED-FOR linear regression problem. truncation of support HYPONYM-OF Isotonoc regression. nonsmooth PDFs HYPONYM-OF Isotonoc regression. Method are Isotonic regression, and quantile regression ( QR ). ","This paper presents an approach for calibration in regression problems. The authors consider Isotonoc regression (i.e., nonsmooth PDFs and truncation of support) and quantile regression (QR). The authors propose a calibration dataset for the linear regression problem, which is a generalization of Isotonic regression. The main contribution of the paper is the introduction of quantile linear regression (QL) and its derivation."
619,SP:e8cbe62252aa671a6deaf12b97063063dfc6d1b0,calibration mechanism USED-FOR regression models. calibration mechanism USED-FOR model prediction and uncertainty estimates. regression models USED-FOR model prediction and uncertainty estimates. model fitting USED-FOR calibration. post - hoc calibration dataset USED-FOR model fitting. post - hoc calibration dataset USED-FOR calibration. isotonic regression USED-FOR approach. post - hoc calibration dataset USED-FOR approach. approach USED-FOR implicit regression calibration. regularization USED-FOR implicit regression calibration. model training USED-FOR implicit regression calibration. regularization USED-FOR approach. Method is post - hoc processing approach. Generic is theory. ,"This paper proposes a new calibration mechanism for regression models to improve model prediction and uncertainty estimates. The proposed approach is based on isotonic regression and uses a post-hoc calibration dataset for model fitting to improve calibration. The authors also propose a new regularization to improve implicit regression calibration during model training. Finally, the authors provide a theoretical analysis of the post-hypothetical processing approach and provide empirical evidence to support the theory."
620,SP:e8cbe62252aa671a6deaf12b97063063dfc6d1b0,"regularization approach USED-FOR model. Task is learning quantile calibrated regressions model. Method are quantile calibrated regressions model, probabilistic regression model, quantile calibrated model, and Machine learning models. OtherScientificTerm is distribution. ",This paper studies the problem of learning quantile calibrated regressions model. The authors propose a regularization approach to the model that aims to encourage the model to converge to a distribution that minimizes the variance of the probabilistic regression model. Machine learning models have been shown to be sensitive to the number of samples in the training set. This paper aims to address this issue by learning a quantile calibration model.
621,SP:9c71ab8dcc433b59d9da3f0db377b74a369112bc,"Deep Variational Bayes Filter ( DBVF ) USED-FOR Deep - Learning based SLAM in 3D environments. It USED-FOR 2D environments. full 3D RGBD occupancy map CONJUNCTION 6 DoF poses ( localization ). 6 DoF poses ( localization ) CONJUNCTION full 3D RGBD occupancy map. raw stereoscopic camera data USED-FOR 6 DoF poses ( localization ). local map CONJUNCTION expected observation. expected observation CONJUNCTION local map. Differentiable ray - casting CONJUNCTION attention model. attention model CONJUNCTION Differentiable ray - casting. attention model USED-FOR global map. local map USED-FOR emission model. Variational Bayes USED-FOR ELBO equation. map CONJUNCTION control inputs. control inputs CONJUNCTION map. environment exploration CONJUNCTION path planning. path planning CONJUNCTION environment exploration. robotic control tasks PART-OF model. environment exploration HYPONYM-OF robotic control tasks. path planning HYPONYM-OF robotic control tasks. ground truth poses CONJUNCTION occupancy grid. occupancy grid CONJUNCTION ground truth poses. bird's eye view projections CONJUNCTION emitted maps. emitted maps CONJUNCTION bird's eye view projections. flying drone FEATURE-OF simulated dataset. subway and living room environments FEATURE-OF flying drone. realistic RGBD data streams USED-FOR DVBF. computational times EVALUATE-FOR method. method COMPARE SLAM techniques. SLAM techniques COMPARE method. computational times EVALUATE-FOR SLAM techniques. Method are transition model, and deep generative models. Task is inference. ",This paper proposes Deep-Learning based SLAM in 3D environments using Deep Variational Bayes Filter (DBVF). It has been previously applied to 2D environments. The model consists of two parts: 1) a full 3D RGBD occupancy map and 6 DoF poses (localization) from raw stereoscopic camera data; 2) an emission model that takes a local map and an expected observation as input. Differentiable ray-casting and an attention model are used to map the global map to the ground truth poses and the occupancy grid. The transition model is based on the ELBO equation derived from VariationalBayes. The authors propose to use deep generative models for inference. The proposed model is able to perform robotic control tasks such as environment exploration and path planning. Experiments are conducted on a simulated dataset of a flying drone in subway and living room environments with bird's eye view projections and emitted maps. DVBF is trained on realistic RGBD data streams. The experiments show that the proposed method can achieve comparable or better computational times compared to other SLAM techniques.
622,SP:9c71ab8dcc433b59d9da3f0db377b74a369112bc,geometry HYPONYM-OF state of the latent variables. deep state - space model USED-FOR problem. variational inference USED-FOR state of the latent variables. quadcopter dataset EVALUATE-FOR method. Task is dense RGB - D SLAM. Generic is approach. ,"This paper studies the problem of dense RGB-D SLAM. The authors propose a deep state-space model to solve this problem. The proposed approach is based on variational inference on the state of the latent variables (e.g., geometry). The proposed method is evaluated on a quadcopter dataset."
623,SP:9c71ab8dcc433b59d9da3f0db377b74a369112bc,"algorithm USED-FOR world map. stereo RGBD sensors USED-FOR algorithm. attention mechanism CONJUNCTION ray casting. ray casting CONJUNCTION attention mechanism. ray casting USED-FOR graphical model. attention mechanism USED-FOR graphical model. ELBO USED-FOR model. Method is learning - based visual - inertial odometry algorithm. OtherScientificTerm are occupancy grid, and agent state. Material is RGBD sensor data. ",This paper proposes a learning-based visual-inertial odometry algorithm. The algorithm is trained on stereo RGBD sensors to generate a world map. The graphical model is trained using an attention mechanism and ray casting. The model is optimized using ELBO and the occupancy grid. The agent state is estimated using RGBD sensor data.
624,SP:bacb279ab6d1997bf44b7b2af583f29679219c36,"generalization USED-FOR RL. model COMPARE partly grounded policy. partly grounded policy COMPARE model. Material are manuals / textual information, and dataset of natural language descriptions. Generic is task. OtherScientificTerm is textual description. Method is self - attention model. ","This paper studies the problem of generalization in RL, where manuals/textual information is not available. The authors propose a new task, where the goal is to generate a dataset of natural language descriptions for a given task. The task is formulated as the following: given a textual description, a self-attention model is trained to predict whether the textual description is correct or incorrect. The model is then compared to a partly grounded policy."
625,SP:bacb279ab6d1997bf44b7b2af583f29679219c36,model USED-FOR entity representations. EMMA USED-FOR entity representations. EMMA COMPARE baselines. baselines COMPARE EMMA. ,"This paper proposes a model, EMMA, to learn entity representations. Empirical results show that EMMA outperforms several baselines."
626,SP:bacb279ab6d1997bf44b7b2af583f29679219c36,"entity ” representations CONJUNCTION natural language explanations. natural language explanations CONJUNCTION entity ” representations. reinforcement learning framework USED-FOR natural language explanations. multi - modal attention network USED-FOR entity representation. entity representation CONJUNCTION text descriptions. text descriptions CONJUNCTION entity representation. parameter sharing CONJUNCTION multi - task learning. multi - task learning CONJUNCTION parameter sharing. zero - shot generalization EVALUATE-FOR RL framework. domain games EVALUATE-FOR RL framework. Task are Natural language grounding, and natural language grounding. Generic are framework, and benchmark. ",Natural language grounding is an important problem and this paper proposes a reinforcement learning framework for learning entity” representations and natural language explanations. The framework is based on a multi-modal attention network that jointly learns entity representation and text descriptions. The proposed RL framework achieves zero-shot generalization on a variety of domain games with parameter sharing and multi-task learning. The paper also presents a new benchmark for natural language grounding.
627,SP:d90da59c651ae3e97af1cf85f3ab1f12cd56d149,AVEC HYPONYM-OF critic loss. AVEC loss USED-FOR actor - critic algorithm. PPO CONJUNCTION TRPO. TRPO CONJUNCTION PPO. TRPO CONJUNCTION SAC. SAC CONJUNCTION TRPO. mean - squared - error USED-FOR loss. AVEC+SAC CONJUNCTION AVEC+TRPO. AVEC+TRPO CONJUNCTION AVEC+SAC. AVEC+PPO CONJUNCTION AVEC+SAC. AVEC+SAC CONJUNCTION AVEC+PPO. Generic is term. Method is actor - critic algorithms. ,"This paper proposes a critic loss called AVEC, which is a generalization of the original critic loss. The authors show that the AVEC loss can be used to improve the performance of an actor-critic algorithm. The main contribution of this paper is to propose a new term called mean-squared-error for the loss, which can be applied to PPO, TRPO, and SAC. Experiments are conducted on AVEC+PPO, AVEC+,AVEC+SAC, and AVEC+)TRPO. The results show the effectiveness of the proposed term and show that it can improve the performances of the actor - critic algorithms."
628,SP:d90da59c651ae3e97af1cf85f3ab1f12cd56d149,loss function USED-FOR critic. critic PART-OF Reinforcement Learning. critic predictions CONJUNCTION value estimates. value estimates CONJUNCTION critic predictions. mean squared loss USED-FOR value estimates. mean squared loss COMPARE loss function. loss function COMPARE mean squared loss. mean squared loss USED-FOR critic predictions. variance term PART-OF loss function. approach CONJUNCTION RL algorithms. RL algorithms CONJUNCTION approach. SAC CONJUNCTION PPO. PPO CONJUNCTION SAC. benchmarks EVALUATE-FOR continuous control. benchmarks EVALUATE-FOR RL algorithms. PPO HYPONYM-OF RL algorithms. SAC HYPONYM-OF RL algorithms. Method is AVEC. ,This paper proposes a new loss function for the critic in Reinforcement Learning. The proposed mean squared loss replaces the standard loss function which penalizes the difference between critic predictions and value estimates. The variance term in the proposed loss function is similar to AVEC. Experiments on standard benchmarks for continuous control show the effectiveness of the proposed approach and other RL algorithms such as SAC and PPO.
629,SP:d90da59c651ae3e97af1cf85f3ab1f12cd56d149,"idea USED-FOR value function objectives. idea USED-FOR deep RL. value function objectives PART-OF deep RL. Generic are it, and proposal. ","This paper proposes a new idea to incorporate value function objectives into deep RL. The idea is interesting, and it is well motivated. The paper is well written, and the proposal is well-motivated. "
630,SP:62d79bf04817bba3fdffb2c0c9209923a8428533,"depth dependence FEATURE-OF overparameterized networks. synthetic dataset USED-FOR fully - connected networks. random Gaussian inputs PART-OF synthetic dataset. MLPs USED-FOR global labels. MLPs USED-FOR local labels. MLPs COMPARE MLPs. MLPs COMPARE MLPs. NTK USED-FOR finite networks. OtherScientificTerm are local "" labels, and global "" labels. Method is infinite width NTK. ","This paper studies depth dependence in overparameterized networks. The authors propose a synthetic dataset for fully-connected networks with random Gaussian inputs, where ""local"" labels are given by MLPs and ""global labels"" are given via MLPs. They show that for finite networks with infinite width NTK, MLPs can learn global labels faster than MLPs for local labels. They also show that NTK can be extended to finite networks."
631,SP:62d79bf04817bba3fdffb2c0c9209923a8428533,"generalization EVALUATE-FOR network. network USED-FOR NTK. infinite width FEATURE-OF network. Method is overparametrized networks. OtherScientificTerm are local or global interactions, and features. ","This paper studies the generalization performance of overparametrized networks. The authors show that NTK can be trained with a single network with infinite width, but that the network is not robust to local or global interactions. They also show that the features of the network are not invariant to the size of the input."
632,SP:62d79bf04817bba3fdffb2c0c9209923a8428533,"neural tangent kernel HYPONYM-OF kernel function. NTK USED-FOR it. deeper networks COMPARE shallower networks. shallower networks COMPARE deeper networks. finite - width networks COMPARE NTK. NTK COMPARE finite - width networks. generalization EVALUATE-FOR NTK. generalization EVALUATE-FOR finite - width networks. Method are neural networks, and deep ReLU networks. Material is synthetic data. ","This paper studies the generalization properties of neural networks. The authors propose a new kernel function called neural tangent kernel, which is a generalization of deep ReLU networks. In particular, it is based on the NTK. Theoretical results show that NTK improves generalization over finite-width networks, and deeper networks outperform shallower networks. Experiments are conducted on synthetic data."
633,SP:9f8a9299ee67b9c707b241ce84cf41f4917ef735,sample complexity EVALUATE-FOR few - shot learning. underline representation USED-FOR tasks. linear model CONJUNCTION two - layer neural network. two - layer neural network CONJUNCTION linear model. linear model USED-FOR low dimensional and high dimensional representation. two - layer neural network PART-OF models. linear model PART-OF models. OtherScientificTerm is iid task assumption. Generic is they. ,"This paper studies the sample complexity of few-shot learning under the iid task assumption. The authors propose two models that combine a linear model for low dimensional and high dimensional representation, and a two-layer neural network for high dimensional and low dimensional representation. They show that underline representation is sufficient for both tasks, and that they can generalize to unseen tasks."
634,SP:9f8a9299ee67b9c707b241ce84cf41f4917ef735,Method is few shot learning methods. Generic is representation. OtherScientificTerm is controlled capacity. Material is i.i.d. tasks. ,"This paper studies few shot learning methods, where the goal is to learn a representation that is robust to changes in the number of shots and the controlled capacity of the training data. The authors propose to use a few shots to train the representation and show that this can lead to better performance on i.i.d. tasks."
635,SP:9f8a9299ee67b9c707b241ce84cf41f4917ef735,dimension reduction CONJUNCTION few - shot linear regression. few - shot linear regression CONJUNCTION dimension reduction. hidden layer USED-FOR feature extraction. quadratic loss USED-FOR output layer. excess risk EVALUATE-FOR task estimator. near - optimal statistical error EVALUATE-FOR meta - training. near - optimal rate of convergence FEATURE-OF excess risk. near - optimal rate of convergence EVALUATE-FOR task estimator. well - specified high - dimensional linear representation CONJUNCTION neural networks. neural networks CONJUNCTION well - specified high - dimensional linear representation. regularization conditions USED-FOR neural networks. Method is statistical analysis. ,"This paper studies the problem of dimension reduction and few-shot linear regression. The authors propose to use a hidden layer for feature extraction and a quadratic loss for the output layer. They show that the excess risk of the task estimator has a near-optimal rate of convergence, and that meta-training can achieve a near optimal statistical error. They also provide a statistical analysis of the relationship between a well-specified high-dimensional linear representation and neural networks trained under regularization conditions."
636,SP:e29ce50c1c28f9264613736b6c2d20afc4f312c1,features USED-FOR adversarial attack. features USED-FOR defense. adversarial attack CONJUNCTION defense. defense CONJUNCTION adversarial attack. PCA USED-FOR features. features USED-FOR attacks. attacks COMPARE attack. attack COMPARE attacks. AutoZoom HYPONYM-OF attack. ERAN verifier USED-FOR neighborhoods. L2 distance EVALUATE-FOR attack. Task is Defense. ,This paper proposes to use features from PCA to train an adversarial attack and a defense. The main idea is to use the same features to train the adversarial and defense simultaneously. The authors show that using the same set of features for both attacks (AutoZoom and AutoZoom) improves the performance of both attacks. Defense is also improved by using the ERAN verifier to detect neighborhoods. The L2 distance between the attack and the defense is improved by a significant margin.
637,SP:e29ce50c1c28f9264613736b6c2d20afc4f312c1,"approach USED-FOR neighborhood. Task is adversarial robustness. Method are classifier, PCA, model, and models. Generic is methods. OtherScientificTerm is model queries. ","This paper addresses the problem of adversarial robustness. The authors propose an approach to learn a neighborhood that is robust to perturbations to the classifier. The method is based on PCA, where the model is trained to predict the perturbation to the model. The methods are evaluated on a variety of datasets and models. The results show that the proposed approach is effective in reducing the number of model queries."
638,SP:e29ce50c1c28f9264613736b6c2d20afc4f312c1,feature perturbation procedure COMPARE perturbation. perturbation COMPARE feature perturbation procedure. feature mapping CONJUNCTION black - box classifier. black - box classifier CONJUNCTION feature mapping. procedure USED-FOR robust / weak features. feature mapping USED-FOR procedure. then USED-FOR tasks. robust neighborhood USED-FOR tasks. robust features USED-FOR robust neighborhood. weak features USED-FOR adversarial examples. feature - based robust neighborhood COMPARE input - based neighborhood. input - based neighborhood COMPARE feature - based robust neighborhood. feature - based robust neighborhood USED-FOR task. feature - based adversarial examples COMPARE competitive methods. competitive methods COMPARE feature - based adversarial examples. feature - based adversarial examples USED-FOR task. OtherScientificTerm is distortion. ,"This paper proposes a novel feature perturbation procedure that is more robust than the standard perturbations. The proposed procedure learns robust/weak features based on a feature mapping and a black-box classifier. The robust features are then used to train the robust neighborhood for new tasks, while the weak features are used to generate adversarial examples. The feature-based robust neighborhood is shown to outperform the input-based neighborhood on a task where the distortion is high. The experimental results show that the proposed feature-by-feature adversarial example outperforms competitive methods."
639,SP:e3fdb96a8c321a86b136e765abe796019d6f9c7a,Pendulum CONJUNCTION Bipedal Walker. Bipedal Walker CONJUNCTION Pendulum. Bipedal Walker CONJUNCTION Atari problems. Atari problems CONJUNCTION Bipedal Walker. algorithm COMPARE single - task learning baselines. single - task learning baselines COMPARE algorithm. algorithm COMPARE multi - task RL algorithm. multi - task RL algorithm COMPARE algorithm. SP CONJUNCTION PPT. PPT CONJUNCTION SP. single - task learning baselines COMPARE multi - task RL algorithm. multi - task RL algorithm COMPARE single - task learning baselines. Pendulum USED-FOR multi - task RL algorithm. Bipedal Walker EVALUATE-FOR multi - task RL algorithm. Atari problems EVALUATE-FOR multi - task RL algorithm. SP HYPONYM-OF single - task learning baselines. PPT HYPONYM-OF single - task learning baselines. Method is multi - task learning approaches. OtherScientificTerm is negative transfer. Generic is policy. ,"This paper proposes a multi-task RL algorithm based on Pendulum, Bipedal Walker, and Atari problems. The proposed algorithm outperforms single-task learning baselines such as SP, PPT, etc. The paper also provides a theoretical analysis of the performance of the proposed multi-tasks learning approaches. The main finding is that the negative transfer between the learned policy and the original policy is not significant."
640,SP:e3fdb96a8c321a86b136e765abe796019d6f9c7a,"unsupervised task clustering USED-FOR multi - task RL algorithm. tabular settings CONJUNCTION continuous control experiments. continuous control experiments CONJUNCTION tabular settings. continuous control experiments CONJUNCTION Atari. Atari CONJUNCTION continuous control experiments. EM style clustering COMPARE single - task training. single - task training COMPARE EM style clustering. OtherScientificTerm are policies, and policy. ","This paper proposes a multi-task RL algorithm based on unsupervised task clustering. The idea is to learn policies that cluster tasks based on their similarity to the current policy. Experiments are conducted in tabular settings, continuous control experiments, and Atari. Results show that EM style clustering outperforms single-task training."
641,SP:e3fdb96a8c321a86b136e765abe796019d6f9c7a,"reward functions CONJUNCTION action space. action space CONJUNCTION reward functions. task clustering USED-FOR action space. Task is multi - task RL problems. OtherScientificTerm are task clusters, policies, and relative cumulative discounted rewards. Generic is tasks. ",This paper studies multi-task RL problems where the reward functions and the action space are learned by task clustering. The authors propose to use task clusters to learn policies that maximize the relative cumulative discounted rewards across tasks. 
642,SP:b3805eb7114391ed15d5806b1c3eb383bff44250,"self - supervised encoder - discriminator based framework USED-FOR compact fixed dimensional representation. self - supervised encoder - discriminator based framework USED-FOR multivariate time series. multivariate time series USED-FOR compact fixed dimensional representation. distribution of neighboring signals COMPARE distribution of non - neighboring signals. distribution of non - neighboring signals COMPARE distribution of neighboring signals. representations USED-FOR supervised tasks. representations COMPARE competitor ( unsupervised ) approaches. competitor ( unsupervised ) approaches COMPARE representations. Generic is approach. Method are Temporal Neighborhood Coding ( TNC ), time series representations, and embedding of time series. OtherScientificTerm are stationary properties, and encoding space. ","This paper proposes a self-supervised encoder-discriminator based framework for learning a compact fixed dimensional representation of multivariate time series. The approach is inspired by Temporal Neighborhood Coding (TNC), which learns time series representations that have stationary properties. The key idea is to learn an embedding of time series such that the distribution of neighboring signals is similar to that of distribution of non-neighboring signals. This encoding space is then used to learn representations for supervised tasks, which are shown to outperform competitor (unsupervised) approaches."
643,SP:b3805eb7114391ed15d5806b1c3eb383bff44250,unsupervised encoding scheme USED-FOR time series. Positive - Unlabeled ( PU ) learning USED-FOR informative hidden representations of time series windows. statistical test USED-FOR Temporal Neighborhood Coding ( TNC ) scheme. statistical test USED-FOR non - stationarity. it USED-FOR informative hidden representations of time series windows. it CONJUNCTION Positive - Unlabeled ( PU ) learning. Positive - Unlabeled ( PU ) learning CONJUNCTION it. data sets EVALUATE-FOR classification. classification EVALUATE-FOR representations. data sets EVALUATE-FOR representations. methods CONJUNCTION $ k$NN ( for classification ) baseline. $ k$NN ( for classification ) baseline CONJUNCTION methods. OtherScientificTerm is encoding network. ,This paper proposes an unsupervised encoding scheme for time series. The authors propose a statistical test for non-stationarity of the Temporal Neighborhood Coding (TNC) scheme and combine it with Positive-Unlabeled (PU) learning to learn informative hidden representations of time series windows. The proposed representations are evaluated on two data sets for classification. The experimental results show that the proposed methods outperform the $k$NN (for classification) baseline. The main contribution of the paper is the proposed encoding network.
644,SP:b3805eb7114391ed15d5806b1c3eb383bff44250,unsupervised representation ( embedding ) learning method USED-FOR time - series. NLP CONJUNCTION vision. vision CONJUNCTION NLP. unsupervised representation learning USED-FOR fields. it USED-FOR time - series community. vision HYPONYM-OF fields. NLP HYPONYM-OF fields. CPC CONJUNCTION Triplet - Loss. Triplet - Loss CONJUNCTION CPC. ,This paper proposes an unsupervised representation (embedding) learning method for time-series. The idea is interesting and it is well-motivated by the recent developments in the time-sources community. The paper is well motivated and well-written. It is also interesting to see that the unsupervision learning is being applied to other fields such as NLP and vision. The experiments show that the proposed CPC and Triplet-Loss are effective.
645,SP:60b2ea4624997d6ccf862742fb9eb21b819d7eb1,"method USED-FOR stacked neural network blocks. linearly combining module "" template "" weights USED-FOR method. linearly combining module "" template "" weights USED-FOR stacked neural network blocks. transfer learning CONJUNCTION domain adaptation scenarios. domain adaptation scenarios CONJUNCTION transfer learning. system USED-FOR transfer learning. system USED-FOR domain adaptation scenarios. Method is isometric networks. OtherScientificTerm are stack, operational structure, templates, and task - specific components. Task is multitask learning. Generic is mixtures. ","This paper proposes a method for stacking neural network blocks by linearly combining module ""template"" weights. The idea is similar to isometric networks, where a stack is composed of modules that share the same operational structure, but with different templates. The authors argue that this is useful for multitask learning, where mixtures of different templates can be used to learn task-specific components. The system is applied to transfer learning and domain adaptation scenarios."
646,SP:60b2ea4624997d6ccf862742fb9eb21b819d7eb1,method USED-FOR network architectures. model weights USED-FOR network block. network blocks PART-OF architecture. Model weights USED-FOR task. template weights USED-FOR task. Method is network components. OtherScientificTerm is templates. ,"This paper proposes a method for learning network architectures. The proposed architecture consists of several network blocks, each of which consists of a set of model weights for each network block. Model weights for a given task are used to train the network components, while template weights are used for a different task. The templates are learned in an unsupervised manner."
647,SP:60b2ea4624997d6ccf862742fb9eb21b819d7eb1,"ImageNet EVALUATE-FOR single task isometric model. task - specific weights USED-FOR Parameter - efficient transfer learning. task - specific weights USED-FOR Parameter - efficient domain adaptation. Task is modular multi - task learning. Method are Multi - task learning, and multi - domain learning. OtherScientificTerm are parameter sharing, mixture components, specialisation, and domain - wise specificity. ","Multi-task learning is an important problem in the context of modular multi-tasks learning, where the goal is to learn a single task isometric model that can generalise to new tasks without parameter sharing. This paper proposes to address the problem of multi-domain learning by modelling the task-specific weights as mixture components. Parameter-efficient transfer learning is achieved by using task-Specific weights, which is an extension of the work on Multi-task learning. The main contribution of this paper is to show that the proposed mixture components can be used in a multi-task setting. The authors also show that this specialisation can lead to better performance on ImageNet compared to the single task performance of the proposed model. The paper also provides a theoretical analysis of the effect of domain-wise specificity on the performance of different components of the model."
648,SP:cae669c631e11fe703bf6cb511404866b19f474a,"regulariser USED-FOR latter. former HYPONYM-OF regulariser. former COMPARE latter. latter COMPARE former. OtherScientificTerm are Gaussian VAEs, posterior collapse, encoder variance, and encoder. Method is decoder Gaussian. ","This paper studies the posterior collapse of Gaussian VAEs. The former is a regulariser, and the latter is a variant of the former. The authors show that the encoder variance is proportional to the decoder Gaussian, and that posterior collapse can be explained by the difference in encoder and decoder variance. They also provide a theoretical analysis of the effect of the change of encoder."
649,SP:cae669c631e11fe703bf6cb511404866b19f474a,decoder variance USED-FOR VAE. decoder variance USED-FOR model smoothness. adaptive decoder variance USED-FOR ELBO. Method is Gaussian VAE. OtherScientificTerm is posterior collapse. Generic is model. ,"This paper proposes to use the decoder variance of a VAE to improve the model smoothness. Specifically, the authors propose to use a Gaussian VAE where the posterior collapse is assumed to be a function of the number of samples in the model. The authors show that the ELBO can be approximated by an adaptive version of the original ELBO with the help of the adaptive decoder variances. "
650,SP:cae669c631e11fe703bf6cb511404866b19f474a,"Variational Auto - Encoders HYPONYM-OF latent variable models. variance CONJUNCTION network parameters. network parameters CONJUNCTION variance. AR - ELBO HYPONYM-OF objective. objective COMPARE VAE methods. VAE methods COMPARE objective. objective USED-FOR image modelling. image modelling EVALUATE-FOR VAE methods. OtherScientificTerm are training objective, posterior collapse, and decoder distribution. ","Variational Auto-Encoders are a class of latent variable models where the training objective is a function of both the variance and the network parameters. This paper proposes a new objective called AR-ELBO, which is motivated by the observation that posterior collapse occurs when the variance of the decoder distribution converges to zero. Experiments on image modelling show that the proposed objective outperforms other VAE methods."
651,SP:cb3c10afbdd8a49cdc23e3ea71ea46ab27253b85,mixture prior CONJUNCTION global latent space. global latent space CONJUNCTION mixture prior. latent space USED-FOR interpretable features. local latent space FEATURE-OF mixture prior. mixture prior PART-OF model. model USED-FOR domain alignment. model USED-FOR global posterior representations. paper COMPARE VAE structure. VAE structure COMPARE paper. Method is deep generative model. Task is unsupervised version. OtherScientificTerm is latent variables. ,"This paper proposes a deep generative model that combines a mixture prior in the local latent space with a global latent space to learn interpretable features. The model is used for domain alignment and to learn global posterior representations. The paper is similar to the VAE structure, except for the unsupervised version where the latent variables are sampled from the latent space."
652,SP:cb3c10afbdd8a49cdc23e3ea71ea46ab27253b85,VAE - based method USED-FOR local variation factors. local variation factors CONJUNCTION global variation factors. global variation factors CONJUNCTION local variation factors. VAE - based method USED-FOR global variation factors. graphical model USED-FOR It. example - local and batch - shared variables PART-OF graphical model. ELBO USED-FOR it. MNIST CONJUNCTION CelebA. CelebA CONJUNCTION MNIST. discrete d variable PART-OF model. ,"This paper proposes a VAE-based method to estimate both local variation factors and global variation factors. It is based on a graphical model consisting of example-local and batch-shared variables, and it uses ELBO to estimate it. The model consists of a discrete d variable, and is trained on MNIST and CelebA."
653,SP:cb3c10afbdd8a49cdc23e3ea71ea46ab27253b85,noni.i.d. variational autoencoders USED-FOR global dependencies. noni.i.d. variational autoencoders USED-FOR deep generative model. model USED-FOR interpretable disentangled representations. global Gaussian latent variable USED-FOR interpretable disentangled representations. mixture model CONJUNCTION global Gaussian latent variable. global Gaussian latent variable CONJUNCTION mixture model. user - defined regularization FEATURE-OF evidence lower bound. local or data - dependent space FEATURE-OF mixture model. mixture model PART-OF model. global Gaussian latent variable PART-OF model. disentanglement CONJUNCTION domain alignment. domain alignment CONJUNCTION disentanglement. disentanglement EVALUATE-FOR model. domain alignment EVALUATE-FOR model. tasks EVALUATE-FOR model. disentanglement HYPONYM-OF tasks. domain alignment HYPONYM-OF tasks. ,"This paper proposes a deep generative model based on noni.i.d. variational autoencoders to model global dependencies. The model consists of a mixture model in a local or data-dependent space, a global Gaussian latent variable to learn interpretable disentangled representations, and a user-defined regularization to improve the evidence lower bound. The proposed model is evaluated on two tasks: disentanglement and domain alignment."
654,SP:33792375012ff9dcffab598cc8fe5ebc71c98af4,unsupervised representation learning USED-FOR downstream vision tasks. human motion and attention ( gaze ) information USED-FOR unsupervised representation learning. large spatio - temporal dataset USED-FOR task. gaze and body motion labels FEATURE-OF large spatio - temporal dataset. network USED-FOR visual focus of attention. visual instance recognition USED-FOR visual representations. NCE loss USED-FOR visual representations. body motion FEATURE-OF visual focus of attention. NCE loss USED-FOR visual instance recognition. accuracy EVALUATE-FOR visual recognition downstream tasks. approach COMPARE SOTA MOCO approach. SOTA MOCO approach COMPARE approach. accuracy EVALUATE-FOR approach. visual recognition downstream tasks EVALUATE-FOR approach. visual information USED-FOR SOTA MOCO approach. ,This paper addresses the problem of unsupervised representation learning for downstream vision tasks using human motion and attention (gaze) information. The authors propose a large spatio-temporal dataset with gaze and body motion labels for the task. The network is trained to predict the visual focus of attention in terms of body motion using visual instance recognition with an NCE loss. The proposed approach is compared with the SOTA MOCO approach that does not use visual information. Results show that the proposed approach improves the accuracy on visual recognition downstream tasks.
655,SP:33792375012ff9dcffab598cc8fe5ebc71c98af4,"visual   representation USED-FOR visual tasks. human interaction / motion USED-FOR visual   representation. depth estimation HYPONYM-OF visual tasks. interaction CONJUNCTION attention cues. attention cues CONJUNCTION interaction. attention cues PART-OF self - supervised representation. Inertial Movement Units ( IMUs ) HYPONYM-OF sensors. synchronized streams of images CONJUNCTION body part movements. body part movements CONJUNCTION synchronized streams of images. body part movements CONJUNCTION gaze information. gaze information CONJUNCTION body part movements. human interactions FEATURE-OF dataset. synchronized streams of images PART-OF dataset. gaze information PART-OF dataset. body part movements PART-OF dataset. Generic is method. OtherScientificTerm are interaction element, and visual events. ","This paper proposes a method for learning a visual  representation of human interaction/motion for visual tasks such as depth estimation. The self-supervised representation is composed of interaction and attention cues. The sensors are Inertial Movement Units (IMUs), which are designed to be able to detect the interaction element. The method is evaluated on a dataset of human interactions with synchronized streams of images, body part movements, and gaze information. The results show that the IMUs are able to distinguish between visual events and the interaction."
656,SP:33792375012ff9dcffab598cc8fe5ebc71c98af4,"physics prediction CONJUNCTION depth estimation. depth estimation CONJUNCTION physics prediction. action recognition CONJUNCTION physics prediction. physics prediction CONJUNCTION action recognition. representations COMPARE representations. representations COMPARE representations. representations USED-FOR tasks. visual, human gaze and human motion sensors USED-FOR representations. representations USED-FOR tasks. depth estimation HYPONYM-OF tasks. action recognition HYPONYM-OF tasks. physics prediction HYPONYM-OF tasks. visual input USED-FOR representations. eye gaze fixations CONJUNCTION IMU motion readings. IMU motion readings CONJUNCTION eye gaze fixations. aligned visual images CONJUNCTION eye gaze fixations. eye gaze fixations CONJUNCTION aligned visual images. system USED-FOR eye gaze. visual input USED-FOR system. image frame coordinates FEATURE-OF eye gaze. head HYPONYM-OF motion detectors. legs HYPONYM-OF motion detectors. gaze and motion prediction COMPARE visual pretext tasks. visual pretext tasks COMPARE gaze and motion prediction. Generic is Representations. Task are auxiliary visual pretext task, instance discrimination, and visual auxiliary tasks. OtherScientificTerm is latent space. ","This paper proposes a new auxiliary visual pretext task. Representations from visual, human gaze and human motion sensors are used to train representations for tasks such as action recognition, physics prediction, depth estimation, and depth estimation. These representations are trained with visual input and are compared to representations learned with no visual input. Experiments are performed on aligned visual images, eye gaze fixations and IMU motion readings. The system is trained to predict eye gaze in terms of image frame coordinates, and the system is able to reconstruct eye gaze from the latent space. In addition to instance discrimination, the paper also performs an ablation study on the effect of visual auxiliary tasks. The results show that gaze and motion prediction perform better than visual pretext tasks, especially for motion detectors such as head and legs."
657,SP:6873a5e80e6142983c9bbd22931bfded7eed2f59,pretraining USED-FOR task. pretraining COMPARE large - scale models. large - scale models COMPARE pretraining. Material is images. ,This paper studies the effect of pretraining for a new task on the performance of existing large-scale models. The authors show that pretraining on a small number of images can lead to better performance than training on larger images. 
658,SP:6873a5e80e6142983c9bbd22931bfded7eed2f59,"negative transfer phenomenon FEATURE-OF neural networks. negative pretraining effect HYPONYM-OF negative transfer phenomenon. network COMPARE network. network COMPARE network. FashionMNIST CONJUNCTION MNIST. MNIST CONJUNCTION FashionMNIST. MNIST CONJUNCTION SVHN. SVHN CONJUNCTION MNIST. CIFAR-10 CONJUNCTION FashionMNIST. FashionMNIST CONJUNCTION CIFAR-10. SVHN HYPONYM-OF visual classification datasets. CIFAR-10 HYPONYM-OF visual classification datasets. FashionMNIST HYPONYM-OF visual classification datasets. MNIST HYPONYM-OF visual classification datasets. learning rate USED-FOR negative pretraining effect. learning rate USED-FOR task. task discretization CONJUNCTION resetting model biases. resetting model biases CONJUNCTION task discretization. Generic is phenomenon. Task is sequential learning process. Method are learning process, and ResNet-18 architecture. OtherScientificTerm is model biases. ","This paper studies the negative transfer phenomenon in neural networks, i.e., the negative pretraining effect. This phenomenon is observed in the sequential learning process, where a new network is trained on top of the same network trained on the previous network. The authors study this phenomenon on several visual classification datasets (CIFAR-10, FashionMNIST, MNIST, SVHN) and show that the learning rate for a given task can be used to reduce the negative training effect. They also study the effect of task discretization and resetting model biases during the learning process. Finally, the authors propose a ResNet-18 architecture to address this issue."
659,SP:6873a5e80e6142983c9bbd22931bfded7eed2f59,"multi - task learning CONJUNCTION curriculum learning. curriculum learning CONJUNCTION multi - task learning. life long learning CONJUNCTION multi - task learning. multi - task learning CONJUNCTION life long learning. Fashion MNIST CONJUNCTION SVHN. SVHN CONJUNCTION Fashion MNIST. SVHN CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION SVHN. MNIST CONJUNCTION Fashion MNIST. Fashion MNIST CONJUNCTION MNIST. ResNet18 architecture USED-FOR MNIST. ResNet18 architecture USED-FOR Fashion MNIST. ResNet18 architecture USED-FOR SVHN. Fashion MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION Fashion MNIST. OtherScientificTerm are negative pretraining effect, and learning rates. Generic is models. ","This paper studies the relationship between life long learning, multi-task learning, and curriculum learning. The authors show that there is a negative pretraining effect on the performance of these models, and propose to use the ResNet18 architecture for MNIST, Fashion MNIST and SVHN and CIFAR-10. They also show that the learning rates can be improved by increasing the number of tasks."
660,SP:5d27e5a301ed4f224fb2baecad77006a9fbb2189,"adversarial robustness EVALUATE-FOR classifiers. networks USED-FOR robustness. adversarial examples USED-FOR networks. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. Method are adversarial training, safe - spot aware adversarial training, and out - of - distribution detection method. OtherScientificTerm are adversarial attacks, safe spots, and adversarial perturbations. Generic are network, it, and approach. ","This paper studies the problem of adversarial robustness of classifiers. The authors argue that adversarial training is ineffective because the adversarial attacks do not discriminate between different regions of the network. To address this issue, the authors propose a new approach called safe-spot aware adversarial learning. The idea is that networks trained on adversarial examples should be trained to maximize robustness to different types of attacks. To do so, they propose an out-of-distribution detection method and show that it can detect safe spots that are not affected by adversarial perturbations. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of their approach."
661,SP:5d27e5a301ed4f224fb2baecad77006a9fbb2189,bi - level optimization algorithm USED-FOR safe spots. they USED-FOR smoothed classifiers. bi - level optimization algorithm USED-FOR adversarially trained classifiers. empirical and certified robustness EVALUATE-FOR smoothed classifiers. CIFAR-10 and ImageNet datasets USED-FOR adversarially trained classifiers. empirical and certified robustness EVALUATE-FOR they. safe spots FEATURE-OF out - of - distribution detection. training scheme USED-FOR out - of - distribution detection. training scheme USED-FOR near - distribution outliers. Method is adversarial framework. OtherScientificTerm is adversarial attacks. ,"This paper proposes a bi-level optimization algorithm to find safe spots for adversarially trained classifiers on CIFAR-10 and ImageNet datasets, where they improve the empirical and certified robustness of smoothed classifiers. The authors also propose a new training scheme for out-of-distribution detection with safe spots, which is more robust to near-distortion outliers. The proposed adversarial framework is well-motivated, and the paper is clearly written and easy to follow. However, the paper suffers from a lack of experimental results on adversarial attacks."
662,SP:5d27e5a301ed4f224fb2baecad77006a9fbb2189,"them USED-FOR robust training. robust accuracy EVALUATE-FOR baseline. observation USED-FOR out of distribution data. OtherScientificTerm are data space, and adversarial attacks. Generic is technique. ","This paper proposes a technique to detect out-of-distribution (OOD) data points in the data space and use them for robust training. The idea is to use observation to identify out of distribution data and then use them to improve the robust accuracy of the baseline. The technique is well-motivated and the paper is well written. However, the paper suffers from a lack of thorough analysis and the results are not convincing. The paper also suffers from lack of comparison to adversarial attacks."
663,SP:1350ab543b6a5cf579827835fb27011751cc047f,convolutional approach USED-FOR raw spatiotemporal ( ST ) point cloud data. shared spatial convolution CONJUNCTION temporal convolution. temporal convolution CONJUNCTION shared spatial convolution. shared spatial convolution USED-FOR point spatio - temporal ( PST ) convolution. transposed PST USED-FOR point - wise predictions. transposed PST USED-FOR encoder - decoder framework ( PSTNet ). encoder - decoder framework ( PSTNet ) USED-FOR point - wise predictions. action recognition CONJUNCTION semantic segmentation. semantic segmentation CONJUNCTION action recognition. PSTNet USED-FOR action recognition. PSTNet USED-FOR semantic segmentation. point cloud sequences USED-FOR semantic segmentation. PSTNet USED-FOR convolutions. ,"This paper proposes a novel convolutional approach for raw spatiotemporal (ST) point cloud data. Specifically, the authors propose a point spatio-temporal (PST) convolution based on a shared spatial convolution and a temporal convolution. The authors also propose an encoder-decoder framework (PTSNet) that uses transposed PST to make point-wise predictions. The proposed PSTNet is applied to the task of action recognition and semantic segmentation on point cloud sequences. Experiments show that the proposed convolutions are effective."
664,SP:1350ab543b6a5cf579827835fb27011751cc047f,point spatio - temporal convolutions USED-FOR feature extraction of point cloud sequences. trainable kernel USED-FOR continuous convolution. convolution USED-FOR temporal dimension. Method is 4D convolution. Generic is network. ,"This paper proposes point spatio-temporal convolutions for feature extraction of point cloud sequences. The main idea is to use a trainable kernel for continuous convolution, which is similar to 4D convolution. The key difference is that the convolution is parameterized in temporal dimension, which allows the network to generalize to unseen points."
665,SP:1350ab543b6a5cf579827835fb27011751cc047f,convolution manner USED-FOR point cloud data. PST convolution and deconvolution operations USED-FOR tasks. classification HYPONYM-OF tasks. multiple benchmark datasets EVALUATE-FOR method. Material is point cloud. ,"This paper proposes a convolution manner for point cloud data. The main idea is to use PST convolution and deconvolution operations for two tasks: (1) classification, and (2) point cloud. The method is evaluated on multiple benchmark datasets."
666,SP:a808583e924f85ec847c6b2597bae5c3eeec0ca7,"adaptation quality CONJUNCTION similarity. similarity CONJUNCTION adaptation quality. AdaSpeech HYPONYM-OF TTS custom voice adaptation. layernorm PART-OF model. OtherScientificTerm are custom voices, and speaker embedding. Method is grammar. ","This paper proposes AdaSpeech, a TTS custom voice adaptation, which aims to improve the adaptation quality and the similarity between the original and custom voices. The model consists of a layernorm, which consists of the speaker embedding and the grammar. The grammar is learned in an end-to-end manner."
667,SP:a808583e924f85ec847c6b2597bae5c3eeec0ca7,multi - phonetic - level acoustic condition modeling CONJUNCTION conditional layer normalization. conditional layer normalization CONJUNCTION multi - phonetic - level acoustic condition modeling. two - stage modeling PART-OF method. conditional layer normalization HYPONYM-OF two - stage modeling. multi - phonetic - level acoustic condition modeling PART-OF method. multi - phonetic - level acoustic condition modeling HYPONYM-OF two - stage modeling. conditional layer normalization PART-OF method. phoneme - level acoustic condition modeling CONJUNCTION speaker and utterance - level approaches. speaker and utterance - level approaches CONJUNCTION phoneme - level acoustic condition modeling. conditional layer normalization USED-FOR adaptation. Method is text - to - speech adaptation method. ,"This paper proposes a text-to-speech adaptation method. The proposed method combines two-stage modeling: multi-phonetic-level acoustic condition modeling and conditional layer normalization. The authors show that the use of conditional layer regularization can improve the adaptation performance. The experiments are conducted on a variety of text and speech datasets and show the effectiveness of the proposed method. In addition, the authors also show the benefits of using phoneme-level as well as speaker and utterance-level approaches."
668,SP:a808583e924f85ec847c6b2597bae5c3eeec0ca7,AdaSpeech HYPONYM-OF TTS system. TTS model PART-OF FastSpeech 2. TTS model USED-FOR model. AdaSpeech COMPARE baselines. baselines COMPARE AdaSpeech. ,"This paper proposes AdaSpeech, a TTS system. The model is based on the TTS model in FastSpeech 2. The experimental results show that AdaSpeSpeech outperforms the baselines."
669,SP:66f56cc202aed1382a342e13ecfe0c5af87f6fee,effective gradient flow ( EGF ) HYPONYM-OF layer - wise normalized gradient flow. test loss CONJUNCTION test accuracy. test accuracy CONJUNCTION test loss. test accuracy HYPONYM-OF metrics. test loss HYPONYM-OF metrics. metrics EVALUATE-FOR EGF. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. CIFAR-100 HYPONYM-OF image - data - sets. CIFAR-10 HYPONYM-OF data - sets. CIFAR-100 HYPONYM-OF data - sets. ,"This paper proposes effective gradient flow (EGF), a layer-wise normalized gradient flow. EGF is evaluated on two metrics: test loss and test accuracy. Experiments are conducted on two image-data-sets: CIFAR-10 and CIFR-100."
670,SP:66f56cc202aed1382a342e13ecfe0c5af87f6fee,"initialization USED-FOR sparse training. model accuracy EVALUATE-FOR sparse training. model accuracy EVALUATE-FOR initialization. sparse training COMPARE dense training. dense training COMPARE sparse training. model accuracy EVALUATE-FOR sparse training. model accuracy EVALUATE-FOR sparse training. effective gradient flow FEATURE-OF grad norm. model weight dimensions FEATURE-OF grad norm. effective gradient flow COMPARE full gradient. full gradient COMPARE effective gradient flow. sparsified weight dimensions FEATURE-OF gradients. gradients PART-OF full gradient. model accuracy EVALUATE-FOR effective gradient flow. weight decay CONJUNCTION data augmentation. data augmentation CONJUNCTION weight decay. batch normalization USED-FOR sparse training. model accuracy EVALUATE-FOR batch normalization. OtherScientificTerm are model parameter count, and non - saturating activations. ","This paper studies the effect of initialization on the model accuracy during sparse training. The authors show that the effective gradient flow of the grad norm with respect to the model weight dimensions is smaller than that of the full gradient with sparsified weight dimensions. They also show that sparse training is more effective than dense training when the model parameter count is small. Finally, they show that batch normalization is beneficial for sparse training in terms of improving model accuracy when the number of non-saturating activations is large. They further show that weight decay and data augmentation can help improve model accuracy."
671,SP:66f56cc202aed1382a342e13ecfe0c5af87f6fee,batch norm CONJUNCTION activation functions. activation functions CONJUNCTION batch norm. batch norm USED-FOR sparse networks. hyperparameters CONJUNCTION activation functions. activation functions CONJUNCTION hyperparameters. activation functions USED-FOR sparse networks. optimizers CONJUNCTION hyperparameters. hyperparameters CONJUNCTION optimizers. optimizers USED-FOR sparse vs. dense networks. Task is optimization. OtherScientificTerm is gradient flow. ,"This paper studies the optimization of sparse networks with batch norm, hyperparameters, and activation functions. The authors show that optimizers for sparse vs. dense networks converge to the same gradient flow."
672,SP:d9f17344cd266b16a70c37d891b2c64a6d454908,"label propagation algorithm ( LPA ) CONJUNCTION graph convolution network ( GCN ). graph convolution network ( GCN ) CONJUNCTION label propagation algorithm ( LPA ). label propagation algorithm ( LPA ) PART-OF unified model. graph convolution network ( GCN ) PART-OF unified model. GCN loss CONJUNCTION LPA loss. LPA loss CONJUNCTION GCN loss. GCN loss PART-OF unified objective function. LPA loss PART-OF unified objective function. methods PART-OF unified objective function. OtherScientificTerm are edge weights, intra - class feature influence, and feature. Task is LPA ’s prediction. ","This paper proposes a unified model consisting of a label propagation algorithm (LPA) and a graph convolution network (GCN). The unified objective function consists of a GCN loss and an LPA loss. The LPA’s prediction is based on minimizing the difference between the edge weights of the two classes, while the GCN is trained to minimize the intra-class feature influence. The two methods are combined to form a unified objective. The main contribution of the paper is to show that the LPA can be used to estimate the distance between a feature and a class."
673,SP:d9f17344cd266b16a70c37d891b2c64a6d454908,"edges PART-OF graph. OtherScientificTerm are erroneous edges, inter - class edges, intra - class edges, and noisy edges. Method is GCN. ","This paper studies the problem of identifying erroneous edges in a graph. The authors argue that erroneous edges can be seen as inter-class edges, and propose a GCN to identify such edges. The main idea is to identify the intra-class and inter-distinct class edges as noisy edges, which can be removed from the graph. "
674,SP:d9f17344cd266b16a70c37d891b2c64a6d454908,label propagation CONJUNCTION graph convolutional network. graph convolutional network CONJUNCTION label propagation. graph convolutional network CONJUNCTION modeling of their latent relationships. modeling of their latent relationships CONJUNCTION graph convolutional network. label propagation CONJUNCTION modeling of their latent relationships. modeling of their latent relationships CONJUNCTION label propagation. unified model COMPARE graph diffusion network. graph diffusion network COMPARE unified model. GCN - LPA CONJUNCTION GDC. GDC CONJUNCTION GCN - LPA. GCN and LPA USED-FOR unified model. Generic is model. OtherScientificTerm is edge weights. Task is information aggregation. ,"This paper proposes a unified model that combines label propagation, graph convolutional network, modeling of their latent relationships, and information aggregation. The unified model is trained with GCN and LPA, and compared to a graph diffusion network. The model is evaluated on two datasets: GCN-LPA and GDC. The results show that the edge weights of the unified model are better than the original model."
675,SP:c5883e3a59e6575eff044251b38175a6ed024034,"perspective USED-FOR generalization error bounds. label generating function "" ( LGF ) USED-FOR perspective. co - complexity CONJUNCTION invariance co - complexity. invariance co - complexity CONJUNCTION co - complexity. dissociation co - complexity CONJUNCTION Rademacher smoothness. Rademacher smoothness CONJUNCTION dissociation co - complexity. invariance co - complexity CONJUNCTION dissociation co - complexity. dissociation co - complexity CONJUNCTION invariance co - complexity. correlated Rademacher complexity CONJUNCTION co - complexity. co - complexity CONJUNCTION correlated Rademacher complexity. invariance co - complexity CONJUNCTION Rademacher smoothness. Rademacher smoothness CONJUNCTION invariance co - complexity. correlated Rademacher complexity HYPONYM-OF complexity measures. Rademacher smoothness HYPONYM-OF complexity measures. dissociation co - complexity HYPONYM-OF complexity measures. invariance co - complexity HYPONYM-OF complexity measures. co - complexity HYPONYM-OF complexity measures. measures CONJUNCTION generalization error bound. generalization error bound CONJUNCTION measures. complexity measures USED-FOR generalization error bound. ","This paper proposes a new perspective for computing generalization error bounds based on ""label generating function"" (LGF). The authors propose four new complexity measures: correlated Rademacher complexity, co-complexity, invariance co-computation, and Rademanacher smoothness. The authors show that the proposed measures can be used to derive a generalization estimation of the true label."
676,SP:c5883e3a59e6575eff044251b38175a6ed024034,"generalization error USED-FOR machine learning model. co - complexity COMPARE entropy - ish concept. entropy - ish concept COMPARE co - complexity. classifier USED-FOR invariance transformation. invariance transformation FEATURE-OF generator. OtherScientificTerm are label generating function space, and function spaces. ",This paper studies the generalization error of a machine learning model when the label generating function space is co-complexity instead of the entropy-ish concept. The main result is that the generator is invariant to the invariance transformation induced by the classifier. The paper also provides a theoretical analysis of this phenomenon in function spaces.
677,SP:c5883e3a59e6575eff044251b38175a6ed024034,complexity measure USED-FOR classifiers'generalization gap. co - complexity HYPONYM-OF complexity measure. joint - entropy USED-FOR measure. function space FEATURE-OF LGFs. function space USED-FOR complexity measure. function space USED-FOR generator space. ad hoc constraints FEATURE-OF LGFs. generator space HYPONYM-OF function space. invariance transformations FEATURE-OF classifier's function space. Metric is generalization error. Method is classifier. ,"This paper proposes a new complexity measure, co-complexity, to reduce classifiers' generalization gap. The proposed measure is based on joint-entropy and is motivated by the observation that LGFs with ad hoc constraints can be expressed in a function space similar to the generator space. The main contribution of this paper is to show that the generalization error of a classifier can be reduced by invariance transformations in the classifier's function space."
678,SP:9bb36be61f1d4db88d806092219eba39bf1b99db,"BRECQ HYPONYM-OF Post Training Quantization ( PTQ ) method. low bit precision ( INT2 ) FEATURE-OF PTQ. OtherScientificTerm are inter and intra - layer sensitivity, and model parameters. Task is mixed precision quantization setting. ","This paper proposes BRERECQ, a Post Training Quantization (PTQ) method called BRECQ which aims to improve inter and intra-layer sensitivity. In particular, the authors consider the mixed precision quantization setting, where the model parameters are sampled from the same training set. The authors show that PTQ with low bit precision (INT2) can achieve state-of-the-art performance in this setting."
679,SP:9bb36be61f1d4db88d806092219eba39bf1b99db,"SOTA accuracy EVALUATE-FOR INT2 weight quantization. block level FEATURE-OF quantized model. image classification CONJUNCTION object detection tasks. object detection tasks CONJUNCTION image classification. object detection tasks EVALUATE-FOR approach. image classification EVALUATE-FOR approach. Method are post - training inference quantization, second - order quantization error analysis, and layer - wise reconstruction approach. ","This paper studies post-training inference quantization. The authors provide a second-order quantization error analysis, which shows that INT2 weight quantization does not improve SOTA accuracy. Based on this analysis, the authors propose a layer-wise reconstruction approach, which reconstructs the quantized model at the block level. The proposed approach is evaluated on image classification and object detection tasks."
680,SP:9bb36be61f1d4db88d806092219eba39bf1b99db,"quantized DNN COMPARE full - precision DNN. full - precision DNN COMPARE quantized DNN. Task are post - training quantization, and optimization problem. Method are DNN, and BRECQ. ","This paper studies the problem of post-training quantization. The authors propose BRECQ, an optimization problem that aims to improve the performance of a DNN by quantizing the parameters of the weights of the original DNN. They show that a quantized DNN can outperform a full-precision DNN in terms of performance."
681,SP:3035318ac36cad693a5e4ee7bed43db8df6fb492,label noise CONJUNCTION label imbalance. label imbalance CONJUNCTION label noise. label imbalance CONJUNCTION data size. data size CONJUNCTION label imbalance. data properties USED-FOR calibration error. data size HYPONYM-OF data properties. label noise HYPONYM-OF data properties. label imbalance HYPONYM-OF data properties. cifar10 CONJUNCTION cifar100. cifar100 CONJUNCTION cifar10. cifar100 CONJUNCTION eurosat. eurosat CONJUNCTION cifar100. eurosat CONJUNCTION iNaturalist. iNaturalist CONJUNCTION eurosat. calibration error CONJUNCTION Calibration error. Calibration error CONJUNCTION calibration error. cifar10 CONJUNCTION eurosat. eurosat CONJUNCTION cifar10. scale of label noise FEATURE-OF calibration error. scale of dataset size FEATURE-OF calibration error. non - uniform noise USED-FOR calibration error. data augmentations USED-FOR Calibration error. iNaturalist HYPONYM-OF computer vision datasets. cifar10 HYPONYM-OF computer vision datasets. eurosat HYPONYM-OF computer vision datasets. cifar100 HYPONYM-OF computer vision datasets. large noisy label rate CONJUNCTION large imbalance ratio. large imbalance ratio CONJUNCTION large noisy label rate. large imbalance ratio CONJUNCTION small dataset size. small dataset size CONJUNCTION large imbalance ratio. small dataset size FEATURE-OF poor calibration error. OtherScientificTerm is class - imbalance situation. Metric is noisy label rate. ,"This paper studies the calibration error under three data properties: label noise, label imbalance, and data size. Calibration error with non-uniform noise is shown to be correlated with calibration error with data augmentations, and the class-imbalance situation is studied on three computer vision datasets (cifar10, cifar100, eurosat, and iNaturalist). The results show that poor calibration error is associated with large noisy label rate, large imbalance ratio, and small dataset size. "
682,SP:3035318ac36cad693a5e4ee7bed43db8df6fb492,"convnets USED-FOR calibration problem. synthetic class - imbalance USED-FOR datasets. benchmark datasets USED-FOR synthetic class - imbalance. methods USED-FOR prediction error. methods USED-FOR imbalanced datasets. prediction error FEATURE-OF imbalanced datasets. methods USED-FOR calibration error. dataset size CONJUNCTION data augmentation. data augmentation CONJUNCTION dataset size. data augmentation USED-FOR calibration error. dataset size USED-FOR calibration error. OtherScientificTerm are minority class, and random label noise. ","This paper studies the calibration problem of convnets trained on datasets with synthetic class-imbalance (i.e., the minority class is overrepresented in the majority class). The authors propose two benchmark datasets to study the effects of synthetic class - imbalance on the performance of datasets trained on the benchmark datasets. The authors also propose two methods to reduce the prediction error of imbalanced datasets by reducing the random label noise. The calibration error can be reduced by increasing the dataset size and data augmentation."
683,SP:3035318ac36cad693a5e4ee7bed43db8df6fb492,dataset properties USED-FOR calibration. dataset curation USED-FOR calibration. AI USED-FOR real - life problem. noise CONJUNCTION labeling. labeling CONJUNCTION noise. sampling CONJUNCTION noise. noise CONJUNCTION sampling. Generic is pipeline. ,"This paper studies the problem of calibration based on dataset curation. The main idea is to use the dataset properties as a proxy for calibration. This is a real-life problem that has been studied extensively in AI. The authors propose a pipeline that combines sampling, noise, and labeling."
684,SP:17d90f9d3f5891ac56f5ed6375a21d0c1517fd62,Embodied Multi - Agent Populations FEATURE-OF emergent gesture - based communication. non - uniform distribution of intents CONJUNCTION costly communication. costly communication CONJUNCTION non - uniform distribution of intents. Task is emergent communication. Method is learned communication strategies. ,This paper studies emergent gesture-based communication in Embodied Multi-Agent Populations. The authors focus on the problem of emergent communication in the presence of non-uniform distribution of intents and costly communication. They propose two learned communication strategies to address this issue.
685,SP:17d90f9d3f5891ac56f5ed6375a21d0c1517fd62,"agents USED-FOR protocols. OtherScientificTerm is 3D environment. Generic are training approaches, and approaches. ","This paper proposes to train agents in a 3D environment, where the goal is to learn protocols that can generalize to unseen environments. The authors propose two training approaches, which are based on the idea that agents should be able to generalize well to environments that are not seen during training. The experiments show that the proposed approaches are effective."
686,SP:17d90f9d3f5891ac56f5ed6375a21d0c1517fd62,"Zipf distribution CONJUNCTION energy regulation. energy regulation CONJUNCTION Zipf distribution. Zipf distribution USED-FOR intents. energy regulation USED-FOR intents. latent energy feature USED-FOR zero - shot coordination. chance accuracy EVALUATE-FOR tasks. chance accuracy EVALUATE-FOR torque curriculum. Task are zero - shot emergent non - verbal communication, and emergent communication. OtherScientificTerm is motion of three - joint agents. Generic are agents, and universal protocol. ","This paper studies zero-shot emergent non-verbal communication. The authors propose to use the Zipf distribution and energy regulation to model intents and coordinate the motion of three-joint agents. In particular, the latent energy feature is used to model the dynamics and dynamics of the agents, which is then used to learn a universal protocol. Experiments show that the proposed torque curriculum can improve the chance accuracy on a variety of tasks. The paper is well-written and well-motivated. However, there are some issues that need to be addressed in order to improve the performance of emergent communication."
687,SP:5ba686e2eef369fa49b10ba3f41f102740836859,technique USED-FOR uncertainty estimation. regression USED-FOR uncertainty estimation. neural networks USED-FOR technique. neural networks USED-FOR uncertainty estimation. meta model USED-FOR error characteristics. meta model USED-FOR base model. error characteristics FEATURE-OF base model. validation set USED-FOR meta model. Generic is it. OtherScientificTerm is error bars. ,This paper proposes a technique for uncertainty estimation using neural networks. The idea is to train a meta model on a validation set and then use it to estimate the error of the base model. The error bars are then used to train the meta model.
688,SP:5ba686e2eef369fa49b10ba3f41f102740836859,"meta - modeling approach USED-FOR uncertainty estimates. uncertainty estimates USED-FOR sequential task. method CONJUNCTION baeslines. baeslines CONJUNCTION method. MITV CONJUNCTION SPE9PR. SPE9PR CONJUNCTION MITV. baeslines USED-FOR datasets. method USED-FOR datasets. SPE9PR HYPONYM-OF datasets. MITV HYPONYM-OF datasets. Method are joint modeling method, and meta - learner. Generic is methods. OtherScientificTerm is asymmetric uncertainty bounds. ","This paper proposes a meta-modeling approach for estimating uncertainty estimates for a sequential task. The authors propose a joint modeling method, where the meta-learner is trained to predict the uncertainty of a sequence of tasks. The method is combined with baeslines, which is applied to datasets such as MITV and SPE9PR. The experimental results show that the proposed methods outperform existing methods in terms of asymmetric uncertainty bounds."
689,SP:5ba686e2eef369fa49b10ba3f41f102740836859,method USED-FOR symmetric and asymmetric uncertainty estimates. method USED-FOR non - stationarity processes. real - world applications FEATURE-OF non - stationarity processes. deep neural networks USED-FOR sequential regression tasks. meta - modelling concept USED-FOR uncertainty quantification. approach USED-FOR uncertainty quantification. meta - modelling concept USED-FOR approach. deep neural networks USED-FOR uncertainty quantification. metrics EVALUATE-FOR approach. method USED-FOR sequential setting. meta - modelling approach USED-FOR classification task. non - sequential setting FEATURE-OF classification task. ,This paper proposes a method for estimating symmetric and asymmetric uncertainty estimates for non-stationarity processes in real-world applications. The approach is based on a meta-modeling concept for uncertainty quantification using deep neural networks for sequential regression tasks. The proposed method is applied to the sequential setting and is shown to outperform existing methods on a variety of metrics. The authors also apply the meta-mimicking approach to a classification task in a non-sequential setting.
690,SP:0a58694abd6898a925b1d917ad2a68eefd0567e9,"Gromov - Wasserstein distance USED-FOR comparing probability distributions. graphs comparisons CONJUNCTION NLP. NLP CONJUNCTION graphs comparisons. ML USED-FOR computational chemistry. It USED-FOR ML. ML USED-FOR graphs comparisons. ML USED-FOR NLP. computational chemistry CONJUNCTION graphs comparisons. graphs comparisons CONJUNCTION computational chemistry. outliers CONJUNCTION noise. noise CONJUNCTION outliers. unbalanced ’ variants USED-FOR comparison of metric spaces. noise FEATURE-OF distributions. unbalanced ’ variants USED-FOR Gromov - Wasserstein ( GW ) problem. positive measures CONJUNCTION isometries. isometries CONJUNCTION positive measures. positive measures FEATURE-OF comparison of metric spaces. rigid transformations HYPONYM-OF isometries. Task are unbalanced Gromov - Wasserstein type problem, unbalanced optimal transport, and transport problems. OtherScientificTerm are probability distributions, metric spaces, and coupling matrix. ","This paper studies the unbalanced Gromov-Wasserstein type problem. It is a well-studied problem in ML for computational chemistry, graphs comparisons, and NLP. In particular, the authors consider the problem of comparing probability distributions over metric spaces with outliers, noise, etc. The authors propose ‘unbalanced’ variants for the comparison of metric spaces under positive measures and isometries such as rigid transformations. They also consider unbalanced optimal transport and show that the coupling matrix between the two transport problems can be unbalanced."
691,SP:0a58694abd6898a925b1d917ad2a68eefd0567e9,"Gromov Wasserstein ( GW ) problem USED-FOR metric measure spaces. quadratic divergence USED-FOR marginal constraints. GPU - friendly algorithm USED-FOR UGW. unbalanced Sinkhorn algorithm USED-FOR GPU - friendly algorithm. UGW USED-FOR masses of metric measure spaces. Conic Gromov - Wasserstein ( CGW ) USED-FOR masses of metric measure spaces. Conic Gromov - Wasserstein ( CGW ) HYPONYM-OF UGW. algorithm USED-FOR CGW. OtherScientificTerm are Unbalanced GW, divergence, and UOT. Method is unbalanced optimal transport ( UOT ). ","This paper studies the Gromov Wasserstein (GW) problem for metric measure spaces. The authors propose a GPU-friendly algorithm for UGW based on the unbalanced Sinkhorn algorithm. Unbalanced GW is the case where the marginal constraints are defined as a quadratic divergence between the divergence of the optimal solution and the original solution. The paper then proposes a new UGW called Conic-Gromov-Wasserstein-CGW (CGW) to solve the masses of metrics measure spaces, which is a generalization of unbalanced optimal transport (UOT). The authors provide a theoretical analysis of CGW and provide an algorithm for CGW based on UOT."
692,SP:0a58694abd6898a925b1d917ad2a68eefd0567e9,"unbalanced optimal transport ( UOT ) USED-FOR Gromov - Wassertstein. conic formulation USED-FOR UOT. metric - measure spaces FEATURE-OF CGW. Method are Gromov - Wasserstein distance, Unbalanced Gromov - Wasserstein ( UGW ), and Conic Gromov - Wasserstein ( CGW ). Generic are version, and divergences. OtherScientificTerm are Gromov - Wasserstein, and UGW. ","This paper studies the problem of unbalanced optimal transport (UOT) for Gromov-Wassertstein. In particular, the authors propose a new version of UOT based on the conic formulation. The authors call this version, Unbalanced GromOV-Wasserstein (UGW), and show that CGW can be expressed in metric-measure spaces with divergences. They also provide a theoretical analysis of UGW."
693,SP:47dcefd5515e772f29e03219c01713e2403643ce,"accuracy EVALUATE-FOR pruned networks. sparsity ratio EVALUATE-FOR pruned networks. Generic is network. OtherScientificTerm are input connections, output connections, freed parameter budget, and sparsity. ","This paper studies the accuracy and sparsity ratio of pruned networks. In particular, the authors propose to prune a portion of the network in order to reduce the number of input connections and increase the amount of output connections. The authors also propose to use a freed parameter budget to reduce sparsity."
694,SP:47dcefd5515e772f29e03219c01713e2403643ce,"sparsity level FEATURE-OF pruned ( salient ) parameters. method USED-FOR pruning methods. network architectures EVALUATE-FOR pruning methods. method ( AAP ) HYPONYM-OF post - processing step. OtherScientificTerm are dead neurons, and connected parameters. Method are sparse network, and pruning method. ","This paper proposes a method (AAP) to improve the performance of pruning methods on various network architectures by reducing the number of pruned (salient) parameters at the sparsity level. The main idea is to prune the dead neurons in a sparse network, and to remove the connected parameters. The authors propose a post-processing step, the method (AP), which is an extension of the pruning method [1]."
695,SP:47dcefd5515e772f29e03219c01713e2403643ce,dead connections PART-OF pruned neural networks. all - alive pruning ( AAP ) HYPONYM-OF pruning method. AAP USED-FOR saliency - based pruning methods. AAP USED-FOR model architectures. saliency - based pruning methods CONJUNCTION model architectures. model architectures CONJUNCTION saliency - based pruning methods. accuracy EVALUATE-FOR methods. pruning methods USED-FOR methods. AAP USED-FOR methods. pruning methods USED-FOR AAP. accuracy EVALUATE-FOR AAP. benchmark datasets EVALUATE-FOR methods. benchmark datasets EVALUATE-FOR AAP. ,"This paper proposes a new pruning method called all-live pruning (AAP), which aims to remove dead connections in pruned neural networks. The authors show that AAP can be used to improve the accuracy of saliency-based pruning methods and other model architectures. They also show that the proposed methods can be combined with other pruned methods to improve accuracy. Finally, the authors conduct experiments on several benchmark datasets to demonstrate the effectiveness of AAP."
696,SP:9eb7b946e00085b89844c485bcd94a392146d2b7,GAN latent vector USED-FOR image attribute editing method. pre - trained GAN USED-FOR images. network T USED-FOR latent - space directions. pre - trained regressor USED-FOR image attributes. pre - trained GAN CONJUNCTION pre - trained regressor. pre - trained regressor CONJUNCTION pre - trained GAN. OtherScientificTerm is latent vector. Generic is method. ,"This paper proposes an image attribute editing method based on the GAN latent vector. The key idea is to use a pre-trained GAN to generate images, and then use the latent vector to edit the image attributes. The authors propose to use network T to map the latent-space directions. The proposed method is evaluated on a variety of datasets. The experiments show that the proposed method can achieve state-of-the-art results when combined with the pre-trainable GAN and a trained regressor to edit image attributes in a supervised manner."
697,SP:9eb7b946e00085b89844c485bcd94a392146d2b7,approach USED-FOR semantic image editing task. latent space FEATURE-OF controllable transformation. controllable transformation USED-FOR approach. attribute regression network USED-FOR transformation functions. MLP USED-FOR local transformation. regression module USED-FOR latent vector z. outputs USED-FOR cross - entropy loss. outputs USED-FOR latent vector z. regression module USED-FOR cross - entropy loss. regression module USED-FOR outputs. manipulation quality EVALUATE-FOR method. ,"This paper presents an approach for semantic image editing task based on controllable transformation in latent space. The key idea is to train an attribute regression network to learn transformation functions. The local transformation is modeled as an MLP, and the outputs are used as a cross-entropy loss. The outputs of the regression module are used to train a latent vector z. The method is evaluated on a variety of datasets and shows improvement in manipulation quality."
698,SP:9eb7b946e00085b89844c485bcd94a392146d2b7,pre - trained GAN models CONJUNCTION pre - trained regressors. pre - trained regressors CONJUNCTION pre - trained GAN models. pre - trained GAN models USED-FOR method. pre - trained regressors USED-FOR method. approach COMPARE baseline method. baseline method COMPARE approach. method USED-FOR manipulation of the latent space. method COMPARE image space editing methods. image space editing methods COMPARE method. OtherScientificTerm is image attributes. ,This paper proposes a method that uses pre-trained GAN models and pre-trained regressors. The approach is compared to a baseline method and compared to several state-of-the-art image space editing methods. The proposed method is able to perform manipulation of the latent space in a way that preserves the image attributes.
699,SP:d9d9d5ade0253be2733d8b035f755ebf82e7e18b,"sequence generative adversarial networks ( GAN ) USED-FOR GANs. Gumbel - Softmax based GAN CONJUNCTION relativistic discrimination   function. relativistic discrimination   function CONJUNCTION Gumbel - Softmax based GAN. discriminator network USED-FOR features. synthetic and real datasets EVALUATE-FOR method. method COMPARE sequence generation networks. sequence generation networks COMPARE method. synthetic and real datasets EVALUATE-FOR sequence generation networks. OtherScientificTerm are latent feature space, discriminator, and generator. Method is feature statistics alignment. ","This paper proposes a sequence generative adversarial networks (GAN) to improve the performance of GANs. Specifically, the authors propose a Gumbel-Softmax based GAN with a relativistic discrimination  function, where the latent feature space is partitioned into two parts: (1) the discriminator network is trained to distinguish between the features generated by the generator and (2) the feature statistics alignment is performed. The proposed method is evaluated on both synthetic and real datasets, and compared to other sequence generation networks."
700,SP:d9d9d5ade0253be2733d8b035f755ebf82e7e18b,"GANs USED-FOR sequence generation. relativistic discriminator USED-FOR method. Feature Statistics Alignment ( FSA ) paradigm USED-FOR method. FSA USED-FOR fine - grained "" differences. relativistic discriminator USED-FOR coarse "" differences. FSA USED-FOR It. relativistic discriminator USED-FOR It. it COMPARE baselines. baselines COMPARE it. synthetic and real datasets EVALUATE-FOR it. synthetic and real datasets EVALUATE-FOR It. acceptance CONJUNCTION grammaticality. grammaticality CONJUNCTION acceptance. It COMPARE baselines. baselines COMPARE It. grammaticality FEATURE-OF human evaluation. human evaluation EVALUATE-FOR It. human evaluation EVALUATE-FOR baselines. acceptance USED-FOR human evaluation. OtherScientificTerm is real and generated data distributions. ","This paper proposes a method based on the Feature Statistics Alignment (FSA) paradigm for sequence generation using GANs. It uses FSA to model the ""fine-grained"" differences between real and generated data distributions and uses a relativistic discriminator to represent the ""crawly"" differences. It is evaluated on both synthetic and real datasets and it outperforms baselines on both human evaluation of acceptance and grammaticality."
701,SP:d9d9d5ade0253be2733d8b035f755ebf82e7e18b,feature statistics alignment CONJUNCTION gumbel - softmax. gumbel - softmax CONJUNCTION feature statistics alignment. mode collapse CONJUNCTION unstable training. unstable training CONJUNCTION mode collapse. gumbel - softmax USED-FOR reparameterization. reparameterization USED-FOR mode collapse. gumbel - softmax PART-OF GAN - based text generation method. feature statistics alignment PART-OF GAN - based text generation method. methods USED-FOR feature statistics alignment. mean square and mean distance alignments HYPONYM-OF methods. MS COCO caption CONJUNCTION EMNLP2017 WMT news dataset. EMNLP2017 WMT news dataset CONJUNCTION MS COCO caption. synthetic dataset CONJUNCTION MS COCO caption. MS COCO caption CONJUNCTION synthetic dataset. MS COCO caption EVALUATE-FOR method. EMNLP2017 WMT news dataset EVALUATE-FOR method. synthetic dataset EVALUATE-FOR method. ablation studies EVALUATE-FOR method. Generic is them. ,"This paper proposes a GAN-based text generation method that combines feature statistics alignment with gumbel-softmax for reparameterization to prevent mode collapse and unstable training. The authors propose two methods for feature statistics alignments: mean square and mean distance alignments, and evaluate them on a synthetic dataset, MS COCO caption and EMNLP2017 WMT news dataset. The proposed method is also evaluated on ablation studies."
702,SP:3ffa34b54779998f473f4e9a52287bcd0485cec8,DQN USED-FOR value - based deep RL systems. extension USED-FOR DQN. exponentially - sized bins USED-FOR thermometer encoding. variance scaling term USED-FOR learning. algorithm USED-FOR encode returns. OtherScientificTerm is returns. ,"This paper proposes an extension to DQN for value-based deep RL systems. Specifically, the authors propose to use exponentially-sized bins for thermometer encoding. The authors also propose a variance scaling term to speed up learning. Finally, they propose a new algorithm to encode returns."
703,SP:3ffa34b54779998f473f4e9a52287bcd0485cec8,"reward progressivity FEATURE-OF reinforcement learning. reward progressivity USED-FOR Q - learning. modification FEATURE-OF Q - network output. modification PART-OF reward decomposition. method COMPARE reward re - scaling baselines. reward re - scaling baselines COMPARE method. OtherScientificTerm are training signals, and training losses. ","This paper studies the problem of reward progressivity in reinforcement learning, specifically in the context of Q-learning. The authors propose a modification to the Q-network output, which is incorporated into the reward decomposition. The main idea is to re-weight the training signals based on the training losses. Experiments show that the proposed method outperforms existing reward re-scaling baselines."
704,SP:3ffa34b54779998f473f4e9a52287bcd0485cec8,"spectral DQN HYPONYM-OF RL method. decomposition USED-FOR training loss. training loss USED-FOR tasks. decomposition USED-FOR tasks. progressive rewards FEATURE-OF those. progressive rewards HYPONYM-OF tasks. those HYPONYM-OF tasks. Atari tasks EVALUATE-FOR method. OtherScientificTerm are rewards, and extreme reward. ","This paper proposes spectral DQN, an RL method that decomposes the rewards into two parts. The first part is the extreme reward and the second part is a decomposition of the training loss into two tasks: those with progressive rewards and those with no progressive rewards. The method is evaluated on Atari tasks."
705,SP:bff215c695b302ce31311f2dd105dace06307cfc,neural networks USED-FOR generalizable representations. Metric is information theoretic measure. Generic is it. Method is neural network. OtherScientificTerm is affine + element - wise nonlinearity. ,This paper proposes an information theoretic measure of generalization ability of neural networks to learn generalizable representations. The main idea is to use it as a measure of how well a neural network is able to generalize to unseen data points. The authors show that it is possible to learn representations that are robust to affine + element-wise nonlinearity.
706,SP:bff215c695b302ce31311f2dd105dace06307cfc,"initialization CONJUNCTION implicit regularization. implicit regularization CONJUNCTION initialization. minimality CONJUNCTION sufficiency of learned representations. sufficiency of learned representations CONJUNCTION minimality. initialization USED-FOR training dynamics of neural networks. implicit regularization USED-FOR training dynamics of neural networks. initialization USED-FOR SGD. implicit regularization USED-FOR SGD. SGD USED-FOR minimal and sufficient representations. random initialization USED-FOR SGD. initialization USED-FOR SGD. Task is neural networks. OtherScientificTerm are irrelevant factors, minimal representations, and overfitting. ","This paper studies the training dynamics of neural networks with different initialization and implicit regularization. The authors show that SGD trained with a random initialization converges to minimal and sufficient representations, which is in contrast to prior work that focuses on minimality and sufficiency of learned representations. They also show that when irrelevant factors are added to the training data, the minimal representations tend to overfit."
707,SP:bff215c695b302ce31311f2dd105dace06307cfc,"neural network training USED-FOR minimal representations. SGD HYPONYM-OF neural network training. color and target information USED-FOR network. decoder network USED-FOR Information. OtherScientificTerm are irrelevant information, deeper layers, color information, direction decision, and minimal representation. Task is neuroscience - inspired task. Method is neural network. ","This paper studies the problem of neural network training (e.g., SGD) to learn minimal representations. The authors argue that the current state-of-the-art in neuroscience-inspired task is not able to handle irrelevant information. They propose to train a neural network with both color and target information, where the color information is used to guide the direction decision, while the target information is provided to the decoder network. They show that the deeper layers are able to learn the minimal representation."
708,SP:c175ea892c831c2d0c38aded9b5e86d25b86545c,"zeroth - order variance - reduced method USED-FOR min - max problem. it HYPONYM-OF zeroth - order variance - reduced method. it USED-FOR min - max problem. Method is SREDA. OtherScientificTerm are initialization computation, and objective function. Generic is algorithm. ","This paper proposes SREDA, a zeroth-order variance-reduced method for solving the min-max problem. Specifically, it is a variant of SREDE, and it is an efficient and efficient version of the zerth-order variances-reduction method. The main difference is that the initialization computation does not need to be linear in the objective function. The algorithm is computationally efficient and can be applied to a variety of problems."
709,SP:c175ea892c831c2d0c38aded9b5e86d25b86545c,"SREDA - Boost USED-FOR nonconvex - strongly - concave minimax problem. SREDA USED-FOR nonconvex - strongly - concave minimax problem. SREDA USED-FOR SREDA - Boost. it COMPARE SREDA. SREDA COMPARE it. tracking error CONJUNCTION gradient estimation error. gradient estimation error CONJUNCTION tracking error. analytical framework USED-FOR first - order optimization story. zeroth - order variance reduction algorithm USED-FOR optimization problem. complexity EVALUATE-FOR state - of - the - art. Method is SREDA - Boost algorithm. OtherScientificTerm are initialization, and step size. ","This paper proposes a new SREDA-Boost algorithm, which is a generalization of the original (SREDA) algorithm. The main contribution of the paper is to extend SREDE-Boost to solve the nonconvex-strongly-concave minimax problem, which was originally formulated as a variant of SREDa. The authors propose a new analytical framework for the first-order optimization story, where the optimization problem is solved using a zeroth-order variance reduction algorithm, and show that it is equivalent to the original version of the SREDEDA. They also show that the complexity of the new algorithm is similar to that of the state-of-the-art in terms of both tracking error and gradient estimation error, and that the initialization can be reduced to a linear combination of the step size."
710,SP:c175ea892c831c2d0c38aded9b5e86d25b86545c,"theoretical convergence rate EVALUATE-FOR modification. zero order oracle USED-FOR method. convergence rate EVALUATE-FOR method. Method are variance reduction method SEDRA, and SEDRA. OtherScientificTerm is stepsizes. Generic is it. ","This paper proposes a modification to the variance reduction method SEDRA. The proposed modification is motivated by the theoretical convergence rate of the original method, which is based on a zero order oracle. The main contribution of this paper is to show that the convergence rate for the proposed method can be improved by increasing the stepsizes. This is an interesting idea, and the paper is well-written and well-motivated. However, there are a few issues with the proposed modification, and it is not clear to me why this is a good idea. In particular, it is unclear to me how this modification can be used in practice. I would like to see the authors address these issues in more detail in the rebuttal. "
711,SP:c1617e79182c6d06c611ced9d892d7b2da5fd9eb,"COCO CONJUNCTION Objects365. Objects365 CONJUNCTION COCO. Objects365 CONJUNCTION LIVS. LIVS CONJUNCTION Objects365. PASCAL VOC EVALUATE-FOR one - shot object detection algorithms. algorithm USED-FOR foreground objects. Task are one - shot 2D object detections, and one - shot. Method is one - shot detection. OtherScientificTerm is training time. ","This paper studies the problem of one-shot 2D object detections. In particular, the authors consider COCO, Objects365, and LIVS on PASCAL VOC and compare their performance with the state-of-the-art state of the art one-shots. The authors argue that the current state of 1-shot detection is due to the large amount of training time, and propose a new algorithm that focuses on foreground objects. "
712,SP:c1617e79182c6d06c611ced9d892d7b2da5fd9eb,Task is one - shot object detection. Generic is model. Method is Siamese Faster R - CNN. ,This paper addresses the problem of one-shot object detection. The proposed model is based on the Siamese Faster R-CNN. 
713,SP:c1617e79182c6d06c611ced9d892d7b2da5fd9eb,category number USED-FOR one - shot object detection task. COCO CONJUNCTION Object365. Object365 CONJUNCTION COCO. Object365 CONJUNCTION LVIS. LVIS CONJUNCTION Object365. VOC CONJUNCTION COCO. COCO CONJUNCTION VOC. Siamese - style detector USED-FOR LVIS. Task is one - shot detection. ,"This paper studies the problem of one-shot object detection task with category number. The authors compare the performance of VOC, COCO, Object365, and LVIS with a Siamese-style detector, and show that LVIS is the best for one-shots detection."
714,SP:c1617e79182c6d06c611ced9d892d7b2da5fd9eb,COCO CONJUNCTION Objects365. Objects365 CONJUNCTION COCO. Objects365 CONJUNCTION LVIS. LVIS CONJUNCTION Objects365. PASCAL CONJUNCTION COCO. COCO CONJUNCTION PASCAL. Siamese Faster R - CNN USED-FOR datasets. PASCAL HYPONYM-OF datasets. LVIS HYPONYM-OF datasets. Objects365 HYPONYM-OF datasets. COCO HYPONYM-OF datasets. Metric is generalization gap. Generic is base classes. OtherScientificTerm is diversity of categories. ,"This paper studies the generalization gap between base classes and their variants. The authors consider three datasets (PASCAL, COCO, Objects365, LVIS) trained with Siamese Faster R-CNN. They show that the diversity of categories helps to reduce the gap."
715,SP:a472784ddb36f88e6e468f282fbd7ad74f8f7d75,method USED-FOR 3D scene reconstruction. occupancy CONJUNCTION SDF. SDF CONJUNCTION occupancy. SDF HYPONYM-OF implicit surface representations. occupancy HYPONYM-OF implicit surface representations. implicit surface representations USED-FOR method. loss functions USED-FOR dense supervision in the 3D space. loss functions USED-FOR spatial gradients. open 3D meshes HYPONYM-OF 3D labels. ShapeNet CONJUNCTION ScanNet. ScanNet CONJUNCTION ShapeNet. ShapeNet EVALUATE-FOR method. ScanNet EVALUATE-FOR method. single - image scene reconstruction tasks EVALUATE-FOR method. ,"This paper proposes a method for 3D scene reconstruction based on two implicit surface representations: occupancy and SDF. The authors propose two loss functions to provide dense supervision in the 3D space, which are then used to compute spatial gradients. The proposed method is evaluated on single-image scene reconstruction tasks, and on ShapeNet and ScanNet with 3D labels (open 3D meshes)."
716,SP:a472784ddb36f88e6e468f282fbd7ad74f8f7d75,"loss functions USED-FOR implicit 3D scene representation. gradient of the occupancy CONJUNCTION SDF. SDF CONJUNCTION gradient of the occupancy. occupancy or SDF supervision USED-FOR surfaces of objects. constraints USED-FOR SDF. constraints FEATURE-OF gradient of the occupancy. constraints USED-FOR they. they COMPARE methods. methods COMPARE they. supervisory data USED-FOR method. Material is real scan data of scenes. OtherScientificTerm are occupancy, and loss function. Method is supervised learning. Generic is benchmark datasets. ","This paper proposes two loss functions for implicit 3D scene representation. First, they are based on constraints on the gradient of the occupancy and the SDF. Second, occupancy or SDF supervision is applied to the surfaces of objects. The proposed method is evaluated on real scan data of scenes, and compared to other methods that do not use supervisory data. The results show that the proposed loss function is able to generalize to unseen objects, which is a promising direction for supervised learning. The experiments are conducted on several benchmark datasets, and the results are promising."
717,SP:a472784ddb36f88e6e468f282fbd7ad74f8f7d75,"method USED-FOR implicit 3D scene reconstructions. single image input USED-FOR method. back - propagation USED-FOR feature maps. method USED-FOR training. back - propagation USED-FOR method. Method is closed - form Differentiable Gradient Sampling. OtherScientificTerm are spatial gradient, spatial gradients, and without dense 3D supervision. ","This paper proposes a method for implicit 3D scene reconstructions based on a single image input. The proposed method, called closed-form Differentiable Gradient Sampling, uses back-propagation to learn feature maps that are independent of the spatial gradient of the input image. The authors show that the spatial gradients can be learned without dense 3D supervision."
718,SP:a472784ddb36f88e6e468f282fbd7ad74f8f7d75,method USED-FOR single view 3D reconstruction. differentiable gradient sampling method USED-FOR formulation. feature gradient USED-FOR watertight reconstruction. OtherScientificTerm is 3D scene. Material is synthetic and real datasets. ,"This paper proposes a method for single view 3D reconstruction. The formulation is based on a differentiable gradient sampling method, where each view is sampled from a 3D scene and the feature gradient is used for watertight reconstruction. Experiments are conducted on both synthetic and real datasets."
719,SP:b89ec0b50475bfb23399719ca36aa137b389fbf6,pretraining of large - scale models USED-FOR language and vision representations. memory requirements FEATURE-OF models. shared parameters USED-FOR multilayer model. method USED-FOR granular CPU offloading. GPU memory FEATURE-OF model. offloading USED-FOR GPU utilization. method USED-FOR GPU utilization. methods USED-FOR trillion parameter model. GPUs USED-FOR trillion parameter model. Method is large - scale models. OtherScientificTerm is memory load. Metric is convergence. ,"This paper addresses the problem of pretraining of large-scale models for language and vision representations. In particular, this paper studies the memory requirements of such models. The authors propose a multilayer model with shared parameters. The proposed method allows for granular CPU offloading, which can reduce the GPU utilization of the model. In addition, the authors propose two methods for training a trillion parameter model on GPUs. Theoretical results on convergence are provided. Empirical results show that the proposed method can reduce GPU utilization while maintaining high performance. The paper also provides a theoretical analysis of the memory load."
720,SP:b89ec0b50475bfb23399719ca36aa137b389fbf6,"Sharing - Delinking "" paradigm USED-FOR neural language models. method USED-FOR model. GPUs USED-FOR it. granular CPU offloading mechanism USED-FOR CPU memory. ","This paper proposes a ""Sharing-Delinking"" paradigm for training neural language models. The proposed method trains a model on top of GPUs, and then it uses GPUs to share the weights between different tasks. The authors also propose a granular CPU offloading mechanism to reduce the CPU memory."
721,SP:b89ec0b50475bfb23399719ca36aa137b389fbf6,"technique USED-FOR massive ( or Giant ) models. two phase training approach USED-FOR Giant models. Pseudo - Giant HYPONYM-OF model. Pseudo - Giant weights USED-FOR Giant model. Method are Pseudo - to - Real ( P2R ), P2R, and Granular CPU offloading. Metric is computational and time requirements. OtherScientificTerm are model parameters, CPU memory, and GPU memory consumption. ","This paper proposes a technique for training massive (or Giant) models. The proposed technique, Pseudo-to-Real (P2R), aims to reduce the computational and time requirements of training such models. P2R is a two phase training approach for training Giant models. First, the authors train a model called Pseudo - Giant, which is a variant of the original model. Then, the model parameters are used to train a Giant model with Pseudo–Giant weights. The authors also propose a new technique called ""Granular CPU offloading"" to reduce CPU memory and GPU memory consumption."
722,SP:b89ec0b50475bfb23399719ca36aa137b389fbf6,"strategy USED-FOR large scale language models. strategy USED-FOR training time. shared parameters USED-FOR models. OtherScientificTerm are tie constraints, and training time budget. ","This paper proposes a strategy to reduce the training time of large scale language models. The key idea is to impose tie constraints on the parameters of the models, and to use shared parameters between the models. This is done by reducing the training size and training time budget."
723,SP:cd1e11b270f74d5dca9efd9fe1903c0a24bcba12,"Fenchel duality formulation USED-FOR maximum likelihood loss. Fenchel duality formulation USED-FOR F1 - EBMs. maximum likelihood loss FEATURE-OF F1 - EBMs. probability measures FEATURE-OF min - max problem. min - max problem USED-FOR optimization. dual algorithm USED-FOR problem. mean - field dynamics FEATURE-OF dual algorithm. maximum - likelihood training CONJUNCTION score matching. score matching CONJUNCTION maximum - likelihood training. algorithm COMPARE MLE. MLE COMPARE algorithm. Method are dual aogrithm, and two - layer ReLU network. ","This paper proposes to use Fenchel duality formulation to approximate the maximum likelihood loss of F1-EBMs using the dual aogrithm. The optimization is formulated as a min-max problem over probability measures. The authors propose a dual algorithm to solve this problem with respect to mean-field dynamics, which is trained using a two-layer ReLU network. Experiments on maximum-likelihood training and score matching show that the proposed algorithm outperforms MLE."
724,SP:cd1e11b270f74d5dca9efd9fe1903c0a24bcba12,shallow neural networks USED-FOR energy - based models. algorithms COMPARE this. this COMPARE algorithms. Hahn decomposition USED-FOR algorithms. maximum likelihood CONJUNCTION score matching. score matching CONJUNCTION maximum likelihood. learning dynamics USED-FOR maximum likelihood. learning dynamics USED-FOR score matching. Task is Fenchel dual problems. ,This paper studies the problem of training energy-based models with shallow neural networks. The authors propose two algorithms based on the Hahn decomposition and show that their algorithms outperform this by a large margin. They also provide a theoretical analysis of the learning dynamics for maximum likelihood and score matching in Fenchel dual problems.
725,SP:cd1e11b270f74d5dca9efd9fe1903c0a24bcba12,"approaches USED-FOR Gibbs measure. MLE CONJUNCTION score matching. score matching CONJUNCTION MLE. auxiliary $ \gamma$ measure USED-FOR training approach. convergence rate EVALUATE-FOR training method. OtherScientificTerm are KL divergence, and Hyvarinen score. Task is dual problem. ","This paper proposes two approaches to improve the Gibbs measure. The first training approach uses an auxiliary $\gamma$ measure, which is derived from the KL divergence between the MLE and the score matching. The second training method uses the Hyvarinen score, which improves the convergence rate of the dual problem."
726,SP:cd1e11b270f74d5dca9efd9fe1903c0a24bcba12,"maximum likelihood maximization USED-FOR technique. Score matching HYPONYM-OF methods. statistical power EVALUATE-FOR Score matching. Maximum Likelihood CONJUNCTION Score based. Score based CONJUNCTION Maximum Likelihood. continuum FEATURE-OF training modalities. Maximum Likelihood HYPONYM-OF training modalities. Score based HYPONYM-OF training modalities. shallow neural networks HYPONYM-OF models. Radon Nikodym derivative USED-FOR energy based model. maximum likelihood USED-FOR EBM model. dynamical system USED-FOR it. \alpha PART-OF formulation. gamma CONJUNCTION neural network   parameters. neural network   parameters CONJUNCTION gamma. propagation of chaos HYPONYM-OF mean field technique. SDE USED-FOR \nu. Euler - Maruyama USED-FOR continuous time system. infinite alpha regime FEATURE-OF dynamical system ( 6 ). score matching USED-FOR dynamical system ( 6 ). synthetic dataset EVALUATE-FOR Algorithm 1. dual COMPARE primal. primal COMPARE dual. realistic dataset HYPONYM-OF network architectures. images HYPONYM-OF realistic dataset. Method are Energy Based Models ( EBM ), and Fenchel duality. Task is MCMC dynamics. OtherScientificTerm are energy landscape, Fenchel dualities, networks, Fenchel dual, static functional, and relative time - scales. Generic is algorithm. Metric is convergence rates. ","This paper proposes Energy Based Models (EBM), a technique based on maximum likelihood maximization to model the energy landscape of MCMC dynamics. The technique is inspired by Fenchel duality, which is a technique that has been shown to improve the performance of previous methods such as Score matching. Score matching improves the statistical power of Score matching by maximizing the similarity between the energy of the two training modalities (Maximum Likelihood and Score based) along a continuum. The energy based model is modeled as a Radon Nikodym derivative of the energy based derivative, and the authors propose to use the maximum likelihood of the EBM model to approximate it on a dynamical system (6) in the infinite alpha regime. The authors show that the proposed algorithm is able to converge to a state-of-the-art performance on a synthetic dataset (6). The authors argue that this is due to the fact that the energy dynamics of the system can be modeled as Fenchel Dualities, which allows the authors to train networks that are more flexible than shallow neural networks, which are models that are limited to relative time-scales.  The authors propose a formulation of \nu, where \nu is a continuous time system modeled by Euler-Maruyama, and \alpha is a mean field technique (i.e., propagation of chaos). In this formulation, gamma and neural network  parameters are modeled as the sum of the gamma of the input and the output of the neural network.  of the FenchelDuality can be viewed as a static functional, which can be expressed as a function of the \nu using the SDE.  Algorithm 1 is evaluated on the synthetic dataset, and on a realistic dataset with different network architectures (e.g. images, etc.). The authors compare the proposed dual with primal, and show that their proposed dual achieves better convergence rates. "
727,SP:4ff82f679a321ed61e02c50d5997c4e179441a0e,tight lower bounds USED-FOR differentially private ERM. tight bounds USED-FOR constrained and unconstrained settings. ,This paper provides tight lower bounds for differentially private ERM. The authors provide tight bounds for both constrained and unconstrained settings. 
728,SP:4ff82f679a321ed61e02c50d5997c4e179441a0e,"approximate differential privacy FEATURE-OF unconstrtained ERM. packing argument USED-FOR pure DP. fingerprinting codes USED-FOR approximate DP. fingerprinting codes CONJUNCTION packing argument. packing argument CONJUNCTION fingerprinting codes. OtherScientificTerm are lower bounds, $ \log(1/\delta)$ term, optimization, and pure differential privacy. ","This paper studies approximate differential privacy of unconstrtained ERM. The authors provide lower bounds on the $\log(1/\delta)$ term, which is the number of samples required for optimization. They also provide a packing argument for pure DP. Finally, the authors provide fingerprinting codes for approximate DP, and show that pure differential privacy does not hold."
729,SP:4ff82f679a321ed61e02c50d5997c4e179441a0e,"differential privacy ( DP ) USED-FOR unconstrained empirical risk minimization ( ERM ). lower bound COMPARE upper bound. upper bound COMPARE lower bound. lower bound USED-FOR problem. lower bound USED-FOR DP ERM. Generic are setting, algorithm, and constrained case. OtherScientificTerm are loss function, empirical loss, excess empirical loss, unconstrained case, and 1 - way marginal. Task are approximate - DP setting, pure - DP setting, and 1 - way marginal problem. Method is packing - style construction. ","This paper studies the problem of unconstrained empirical risk minimization (ERM) with differential privacy (DP) in the setting where the loss function does not depend on the empirical loss. In particular, the authors consider the approximate-DP setting, where the excess empirical loss is not known. The authors propose a packing-style construction of the 1-way marginal problem, and provide a lower bound for this problem, which matches the upper bound for DP ERM in the constrained case. In contrast, in the pure -DP setting the authors show that the excess marginal is known, which is not the case in the unconstraining case."
730,SP:4ff82f679a321ed61e02c50d5997c4e179441a0e,tight lower bounds USED-FOR approximate DP ERM. lower bound USED-FOR constrained case. approximate DP ERM USED-FOR general loss functions. lower bound USED-FOR unconstrained pure DP ERM. Method is differentially private empirical risk minimisation ( ERM ). ,"This paper studies differentially private empirical risk minimisation (ERM), and provides tight lower bounds for approximate DP ERM for general loss functions. The lower bound for the constrained case is tighter than the one for unconstrained pure DPERM."
731,SP:c4b4914d64e76427435bee0da345fe33b1db7d27,KL divergence CONJUNCTION generalized entropy ( non - linear diffusion ). generalized entropy ( non - linear diffusion ) CONJUNCTION KL divergence. method USED-FOR Wasserstein gradient flows. variational formulations of functional objectives USED-FOR method. JKO scheme USED-FOR method. generalized entropy ( non - linear diffusion ) HYPONYM-OF variational formulations of functional objectives. KL divergence HYPONYM-OF variational formulations of functional objectives. dual formulation USED-FOR divergences. methods USED-FOR objectives. paper COMPARE methods. methods COMPARE paper. optimization over convex functions USED-FOR JKO scheme. dual formulation USED-FOR paper. f - divergences USED-FOR objectives. method USED-FOR mini - max objective. neural networks HYPONYM-OF operators. them HYPONYM-OF operators. input - convex neural network HYPONYM-OF operators. OtherScientificTerm is convex functions. Method is density computation. Generic is PDEs. ,"This paper proposes a method for optimizing Wasserstein gradient flows using variational formulations of functional objectives, such as KL divergence and generalized entropy (non-linear diffusion). The method is inspired by the JKO scheme, which is based on optimization over convex functions. The paper uses a dual formulation of these divergences, which allows for efficient density computation. The authors compare the proposed paper to existing methods for optimizing these objectives using f-divergences and show that the proposed method converges to a mini-max objective. They also show that their method can be applied to other operators (e.g., neural networks) and extend them to other types of operators (i.e., an input-convex neural network). Finally, the authors provide a theoretical analysis of PDEs."
732,SP:c4b4914d64e76427435bee0da345fe33b1db7d27,discrete time FEATURE-OF Wasserstein Gradient Flows ( WGF ). JKO operator USED-FOR WGF. JKO operator USED-FOR methods. minimization USED-FOR JKO. pushforward USED-FOR minimization. f - divergence FEATURE-OF objective function. min max USED-FOR JKO. Adam USED-FOR problem. neural networks USED-FOR functions. Method is variational representation. ,"This paper studies the problem of Wasserstein Gradient Flows (WGF) in discrete time. Previous methods have used the JKO operator to approximate WGF. However, the minimization of JKO with pushforward is computationally expensive. This paper proposes to use the variational representation to approximate the objective function with f-divergence. The main contribution of this paper is to use Adam to solve the problem and to use neural networks to approximate these functions."
733,SP:c4b4914d64e76427435bee0da345fe33b1db7d27,"variational formulation USED-FOR JKO step. inner maximization PART-OF variational formulation. pushforward neural networks USED-FOR JKO steps. OtherScientificTerm are gradients of convex functions, density access, cubic time complexity, and log determinants of the pushforwards. Generic is algorithm. ","This paper proposes a variational formulation of the JKO step that incorporates inner maximization. The main idea is to use pushforward neural networks for JKO steps, where the gradients of convex functions are assumed to be log-invariant. This allows density access, which reduces the cubic time complexity of the algorithm. The log determinants of the pushforwards are also considered."
734,SP:c4b4914d64e76427435bee0da345fe33b1db7d27,neural networks CONJUNCTION JKO scheme. JKO scheme CONJUNCTION neural networks. method USED-FOR Wasserstein Gradient Flows ( WGFs ). JKO scheme USED-FOR method. neural networks USED-FOR method. neural networks USED-FOR Wasserstein Gradient Flows ( WGFs ). variational approximations COMPARE direct computations. direct computations COMPARE variational approximations. f - divergences FEATURE-OF WGFs of functionals. variational approximations USED-FOR WGFs of functionals. Generic is It. ,This paper proposes a method to approximate Wasserstein Gradient Flows (WGFs) using neural networks and the JKO scheme. It is motivated by the observation that variational approximations of WGFs of functionals with f-divergences are more efficient than direct computations.
735,SP:01f652a6b323db3585376a3a8e975a73ec4fed0b,approach USED-FOR meta - feature embedding. meta - feature space CONJUNCTION latent space. latent space CONJUNCTION meta - feature space. embedding USED-FOR AutoML. meta - features USED-FOR AutoML tools. hyper - parameters PART-OF MetaBu. OtherScientificTerm is hyper - parameter configurations. ,"This paper presents an approach to learn a meta-feature embedding for AutoML. The key idea is to learn the embedding in a way that is compatible with both the meta-features space and the latent space. The authors show that this embedding can be used to improve the performance of AutoML tools that use meta-samples as input. In particular, they show that the hyper-parameters of MetaBu can be learned in a similar way to MetaBu, but that the difference is that the authors propose to learn hyper-Parameter configurations that are different from MetaBu."
736,SP:01f652a6b323db3585376a3a8e975a73ec4fed0b,approach USED-FOR problem. hyperparameter configuration USED-FOR dataset. ML algorithm USED-FOR dataset. ML algorithm CONJUNCTION hyperparameter configuration. hyperparameter configuration CONJUNCTION ML algorithm. ML algorithm USED-FOR AutoML problem. meta - features USED-FOR approach. meta - features USED-FOR problem. method USED-FOR meta - features. MetaBu USED-FOR meta - features. space of distributions of hyperparameter configurations USED-FOR optimal transport. optimal transport USED-FOR meta - features. Meta - features PART-OF MetaBu. Meta - features USED-FOR topology. AutoSklearn CONJUNCTION Probabilistic Matrix Factorization. Probabilistic Matrix Factorization CONJUNCTION AutoSklearn. OpenML CC-18 benchmark EVALUATE-FOR MetaBu meta - features. MetaBu meta - features USED-FOR ML algorithm. ML algorithm CONJUNCTION pipeline. pipeline CONJUNCTION ML algorithm. topology USED-FOR intrinsic dimension. intrinsic dimension FEATURE-OF OpenML benchmark. OpenML benchmark EVALUATE-FOR ML algorithm. MetaBu meta - features USED-FOR topology. ,"This paper proposes an approach to solve the AutoML problem using meta-features extracted from MetaBu. The proposed method learns to extract meta-resets of the original dataset using a combination of an ML algorithm and a hyperparameter configuration. The key idea is to learn the optimal transport over the space of distributions of hyperparameters configurations. Meta-features are extracted from the original MetaBu and then used to learn a new topology. Experiments on the OpenML CC-18 benchmark show that the proposed MetaBu meta-samples can be used to improve the performance of the ML algorithm as well as the pipeline on AutoSklearn and Probabilistic Matrix Factorization. In addition, the proposed topology is shown to increase the intrinsic dimension of OpenML benchmark."
737,SP:01f652a6b323db3585376a3a8e975a73ec4fed0b,"Optimal Transport procedure USED-FOR hyper - parameter configurations. OpenML benchmark EVALUATE-FOR method. method USED-FOR boosting AutoML systems. Task is AutoML problem. OtherScientificTerm are manually designed meta - features, and meta - features. ",This paper studies the AutoML problem where manually designed meta-features are used to boost the performance of the system. The authors propose an Optimimal Transport procedure to find hyper-parameter configurations that maximise the mutual information between the meta features and the weights. The proposed method is evaluated on the OpenML benchmark and shown to be effective in boosting AutoML systems.
738,SP:01f652a6b323db3585376a3a8e975a73ec4fed0b,"tabular data USED-FOR AutoML problem. Wasserstein - Gromov distance FEATURE-OF Euclidean distance. Metabu COMPARE meta - learning schemes. meta - learning schemes COMPARE Metabu. seed hyperparameters USED-FOR hyperparameter optimizers. optimizer USED-FOR sampling. sampling USED-FOR hyperparameters. Metabu COMPARE meta - learning schemes. meta - learning schemes COMPARE Metabu. OpenML CC-18 suite EVALUATE-FOR Metabu. OpenML CC-18 suite EVALUATE-FOR machine learning models. Method is meta - learning based novel solution. OtherScientificTerm are optimal transport, and dataset meta - features. Generic are distance, and method. ","This paper proposes a meta-learning based novel solution to the AutoML problem on tabular data. The authors propose to use the Wasserstein-Gromov distance between the Euclidean distance and the optimal transport between the dataset meta-features. This distance is defined as a function of the number of datapoints in the dataset, and the authors propose a method called Metabu. The proposed method is based on the observation that hyperparameter optimizers with different seed hyperparameters converge to the same optimal transport when the optimizer is trained with sampling from the same dataset. Experiments on the OpenML CC-18 suite are conducted to evaluate the performance of different machine learning models and MetabU outperforms the other meta -learning schemes."
739,SP:e789c71cef2094ff2bac51b523ca912f1f04c2c9,"customisation strategy USED-FOR federated learning. heterogeneity of devices USED-FOR FL scenarios. devices'budgets CONJUNCTION dynamics. dynamics CONJUNCTION devices'budgets. robustness EVALUATE-FOR Split - max. accuracy EVALUATE-FOR Split - max. base models USED-FOR diversity. base models USED-FOR generalisable features. standard - training accuracy CONJUNCTION adversarial - training accuracy. adversarial - training accuracy CONJUNCTION standard - training accuracy. models USED-FOR adversarial - training accuracy. models USED-FOR standard - training accuracy. robustness FEATURE-OF models. adversarial accuracy EVALUATE-FOR layer - wise mixing. Split - Mix COMPARE naive approaches. naive approaches COMPARE Split - Mix. customisation USED-FOR models. OtherScientificTerm are devices'budget, and budget constraints. ","This paper proposes Split-max, a customisation strategy for federated learning that aims to address the heterogeneity of devices in FL scenarios. The key idea is to adapt the base models to encourage diversity and to learn generalisable features that can be applied to different devices' budgets and dynamics. The paper shows that the robustness of the proposed models to standard-training accuracy and adversarial-training performance is improved by adding layer-wise mixing, and that Split-Mix can achieve higher accuracy than naive approaches when the devices' budget and dynamics are constrained by budget constraints. Experiments show that the proposed customisation can improve the performance of the models."
740,SP:e789c71cef2094ff2bac51b523ca912f1f04c2c9,"Split - Mix HYPONYM-OF Federated Learning strategy. Split - Mix USED-FOR model. robustness EVALUATE-FOR model. model COMPARE base models. base models COMPARE model. OtherScientificTerm are compute / memory capabilities, and data distributions. Method is global model. Generic are Base models, network, and sub - model. ","This paper proposes Split-Mix, a Federated Learning strategy, which is a variant of Split-Mixture. The key idea is to split the global model into two sub-models, each with different compute/memory capabilities. Base models are trained on the same dataset, while the new model is trained on a different subset of the dataset. The idea is that the new network is trained to be more robust to changes in the data distributions. The new model can be trained to achieve better robustness than the base models. The sub-model is trained in parallel with the original global model."
741,SP:e789c71cef2094ff2bac51b523ca912f1f04c2c9,"Split - Mix FL HYPONYM-OF federated learning approach. sub - networks PART-OF global model. computational resource constraints FEATURE-OF base models. base models HYPONYM-OF sub - networks. Digits CONJUNCTION DomainNet. DomainNet CONJUNCTION Digits. CIFAR10 CONJUNCTION Digits. Digits CONJUNCTION CIFAR10. approach COMPARE FedAvg. FedAvg COMPARE approach. FedAvg CONJUNCTION HeteroFL. HeteroFL CONJUNCTION FedAvg. approach COMPARE HeteroFL. HeteroFL COMPARE approach. Generic is models. OtherScientificTerm are heterogeneity in data and computation resources, computational resources, and batch - norm layers. ","This paper proposes Split-Mix FL, a federated learning approach, which is a variant of the popular split-mix FL (FedAvg, HeteroFL) method. In FedAvg, the global model consists of sub-networks that are trained on different subsets of the original global model with different computational resource constraints. In this paper, the authors propose to train these models separately, in order to deal with heterogeneity in data and computation resources. The authors show that the proposed approach outperforms FedAvg as well as its variants, and also outperforms the existing approach of FedAvg and its variants (HeterOFL) on CIFAR10, Digits, and DomainNet. The main contribution of this paper is to address the issue of heterogeneity in the computational resources by introducing batch-norm layers."
742,SP:e789c71cef2094ff2bac51b523ca912f1f04c2c9,"proposal USED-FOR models. adversarial robustness levels FEATURE-OF models. Method are federated learning scheme, and Split - Mix. Generic is method. ",This paper proposes a federated learning scheme where each client can share the weights of its weights with other clients. The proposed method is called Split-Mix. The proposal aims to improve the performance of models with different adversarial robustness levels. 
743,SP:0fd50d89ffec376d136aa915c9c4e6ae281f5014,"CurvatureEG+ HYPONYM-OF methods. EG+ USED-FOR methods. non - trivial problems PART-OF nonconvex - nonconcave setting. CurvatureEG+ USED-FOR constrained and composite cases. EG+ COMPARE CurvatureEG+. CurvatureEG+ COMPARE EG+. Method are CEG+, EG, and stochastic variant. OtherScientificTerm are weak MVI condition, and limit cycles. Generic is method. Material is toyish problems. ","This paper proposes CurvatureEG+, which is a generalization of existing methods based on EG+ (Chen et al., 2019) and CEG+, EG+, a stochastic variant of EG. The main contribution of the paper is to extend EG+ to non-trivial problems in the nonconvex-nonconcave setting, where the weak MVI condition is satisfied. The authors show that CurvaturesEG+ can be applied to both constrained and composite cases, and show that the proposed method can generalize to toyish problems with limit cycles."
744,SP:0fd50d89ffec376d136aa915c9c4e6ae281f5014,"one CONJUNCTION second. second CONJUNCTION one. second CONJUNCTION iterate update. iterate update CONJUNCTION second. one CONJUNCTION extrapolation step. extrapolation step CONJUNCTION one. Alg. 1 USED-FOR weak MVI problem. step size USED-FOR iterate update. adaptive scheme USED-FOR extrapolation. variant USED-FOR weak MVI problems. non - adaptive variant PART-OF Alg. 1. CEG+ HYPONYM-OF non - adaptive variant. adaptive scheme USED-FOR non - adaptive variant. Lipschitz constant backtracking USED-FOR adaptive scheme. adaptive scheme USED-FOR CEG+. Method are ExtraGradient ( EG ), weak Minty Variational Inequality ( MVI ), EG+ method, Alg.1, and CurvatureEG+. Generic is method. OtherScientificTerm are degree of nonconvexity, weak MVI, and step - sizes. Task is stochastic setup. ","This paper proposes a variant of ExtraGradient (EG) for the weak Minty Variational Inequality (MVI) problem, called CurvatureEG+. The proposed method is motivated by the observation that the degree of nonconvexity of the weak MVI can be controlled by the step size of the iterate update. The authors propose two variants of Alg.1: one that uses the same step size for the first iteration and the second that uses a different step size in the extrapolation step. The main contribution of the paper is that the authors propose an adaptive scheme for extrapolation based on Lipschitz constant backtracking, which is similar to the EG+ method. The proposed variant is applied to weakMVI problems, where the authors show that the proposed variant, CEG+, is a non-adaptive variant of the Alg. 1, which can be used to solve the original Alg.[1] The authors also show that in a stochastic setup, the proposed method can be applied to the case where the step-sizes of the two iterations are different. The experiments are conducted on Alg 1, Alg2, and Alg1.1. The results show that CEG+. is able to achieve better performance than CEG+, which is an adaptive variant of AlG. "
745,SP:0fd50d89ffec376d136aa915c9c4e6ae281f5014,"it USED-FOR nonconvex - nonconcave minimax optimization. CEG+ CONJUNCTION CurvatureEG+. CurvatureEG+ CONJUNCTION CEG+. extragradient method USED-FOR proximal variant. extragradient method USED-FOR CEG+. Weak MVI condition FEATURE-OF nonconvex - nonconcave minimax optimization. extragradient method USED-FOR CurvatureEG+. curvature - based strategy USED-FOR lower bound requirement. adaptive stepsize strategy USED-FOR MVI parameter. adaptive stepsize strategy CONJUNCTION curvature - based strategy. curvature - based strategy CONJUNCTION adaptive stepsize strategy. adaptive stepsize strategy USED-FOR deterministic case. Material is deterministic and stochastic cases. Metric is complexities. OtherScientificTerm are lower bound, and cycling. Method is EG+. ","This paper proposes a novel extension of the extragradient method CEG+ and CurvatureEG+, which extends it to nonconvex-nonconcave minimax optimization under the Weak MVI condition. The authors show that this extension can reduce the complexities of both deterministic and stochastic cases. The main contribution of this paper is to propose an adaptive stepsize strategy for the MVI parameter, a curvature-based strategy to satisfy the lower bound requirement, and a proximal variant of CEG+. The authors also provide a lower bound for EG+. "
746,SP:0fd50d89ffec376d136aa915c9c4e6ae281f5014,unconstrainted and unregularized inclusion problems CONJUNCTION constrained and regularized counterparts. constrained and regularized counterparts CONJUNCTION unconstrainted and unregularized inclusion problems. weak Minty inequality ( MVI ) FEATURE-OF unconstrainted and unregularized inclusion problems. extragradient algorithm COMPARE algorithm. algorithm COMPARE extragradient algorithm. algorithm USED-FOR deterministic and stochastic inclusion problems. Lipschitz constant backtracking USED-FOR algorithm. OtherScientificTerm is stepsize choices. ,"This paper studies the weak Minty inequality (MVI) for unconstrainted and unregularized inclusion problems and their constrained and regularized counterparts. The authors show that the extragradient algorithm is equivalent to the algorithm that uses Lipschitz constant backtracking for both deterministic and stochastic inclusion problems, and that the stepsize choices do not matter."
747,SP:af22742091277b726f67e7155b412dd35f29e804,"regret bound USED-FOR method. shallow exploration USED-FOR neural - bandit algorithm. deep neural networks based bandit algorithms USED-FOR reward functions. approaches USED-FOR deep neural networks based bandit algorithms. large - size networks USED-FOR NTK based approaches. network parameter space FEATURE-OF exploration. approach USED-FOR deep neural network feature representation learning. deep neural contextual bandits CONJUNCTION linear contextual bandits. linear contextual bandits CONJUNCTION deep neural contextual bandits. approach USED-FOR UCB version. linear contextual bandits USED-FOR techniques. deep neural contextual bandits USED-FOR techniques. techniques USED-FOR approach. techniques USED-FOR UCB version. Generic are network, and algorithm. ","This paper proposes a neural-bandit algorithm with shallow exploration based on deep neural network feature representation learning based on shallow exploration. The proposed method has a regret bound of $O(1/\sqrt{T})$, where $T$ is the number of epochs of exploration in the network parameter space. The paper proposes two approaches to deep neural networks based bandit algorithms for learning reward functions. The first approach is based on NTK based approaches with large-size networks, where the size of the network is limited. The second approach uses techniques from deep neural contextual bandits and linear contextual bandits, which is a UCB version of the proposed algorithm."
748,SP:af22742091277b726f67e7155b412dd35f29e804,algorithm USED-FOR raw feature vector. UCB approach USED-FOR algorithm. deep ReLU neural network USED-FOR algorithm. deep ReLU neural network USED-FOR raw feature vector. neural contextual bandit algorithms COMPARE algorithm. algorithm COMPARE neural contextual bandit algorithms. computation efficiency EVALUATE-FOR algorithm. Method is neural contextual bandits. OtherScientificTerm is Regret guarantees. Generic is algorithms. ,"This paper studies the problem of neural contextual bandits. Regret guarantees are provided for the proposed algorithms. The proposed algorithm is based on the UCB approach and uses a deep ReLU neural network to approximate the raw feature vector. Compared to other neural contextual bandit algorithms, the proposed algorithm has better computation efficiency."
749,SP:af22742091277b726f67e7155b412dd35f29e804,"Neural - LinUCB HYPONYM-OF contextual bandit algorithm. OFUL HYPONYM-OF linear bandit. arm USED-FOR linear bandit. regret upper bound FEATURE-OF algorithm. regret upper bound FEATURE-OF algorithm. regret upper bound CONJUNCTION regret upper bound. regret upper bound CONJUNCTION regret upper bound. algorithm COMPARE NeuralUCB. NeuralUCB COMPARE algorithm. Neura - lLinUCB COMPARE LinUCB. LinUCB COMPARE Neura - lLinUCB. contextual bandit problems EVALUATE-FOR Neura - lLinUCB. Neura - lLinUCB COMPARE NeuralTS. NeuralTS COMPARE Neura - lLinUCB. NeuralUCB CONJUNCTION NeuralTS. NeuralTS CONJUNCTION NeuralUCB. Neura - lLinUCB COMPARE NeuralUCB. NeuralUCB COMPARE Neura - lLinUCB. LinUCB COMPARE NeuralUCB. NeuralUCB COMPARE LinUCB. contextual bandit problems EVALUATE-FOR LinUCB. Method is deep neural network. OtherScientificTerm are exploration, and tangent kernel matrix. ","This paper proposes a new contextual bandit algorithm called Neural-LinUCB. The main idea is to train a deep neural network with an additional arm that performs linear bandit (i.e., OFUL). The regret upper bound of the proposed algorithm is shown to be O(1/\sqrt{T}) where T is the tangent kernel matrix. The paper also provides a theoretical analysis of the exploration and the regret of the algorithm. Experiments on a set of contextual banditor problems show that Neura-lLinUCBF outperforms LinUCB and NeuralTS."
750,SP:af22742091277b726f67e7155b412dd35f29e804,"deep representation learning CONJUNCTION upper confidence bound algorithm. upper confidence bound algorithm CONJUNCTION deep representation learning. upper confidence bound algorithm USED-FOR contextual bandits. deep representation learning USED-FOR contextual bandits. Thompson sampling COMPARE UCB. UCB COMPARE Thompson sampling. Thompson sampling USED-FOR setting. OtherScientificTerm is regret bound. Method is UCB search. Generic are network, and baselines. Material are UCI data repo, and MNIST. ","This paper proposes a deep representation learning and an upper confidence bound algorithm for contextual bandits. The regret bound is based on the fact that the UCB search does not depend on the number of samples in the network. The authors show that in this setting, Thompson sampling is better than UCB. They also provide some experimental results on the UCI data repo and on MNIST and compare with several baselines."
751,SP:a9a2c21110e00f19882d27bef0063c422a15e576,"data - driven method USED-FOR optimal action space selection. optimal action space selection USED-FOR reinforcement learning problem. data - driven method USED-FOR reinforcement learning problem. Monte Carlo sampling method USED-FOR cut - off cardinality computation. cut - off cardinality computation USED-FOR action space. optimal step size USED-FOR action update rule. it USED-FOR ranked action set. Monte Carlo sampling based algorithm USED-FOR action search space. Generic is approach. OtherScientificTerm are cumulative reward values, and Memory size. Metric are efficiency, and CPU utilization rate. Material is cloud environment. Method is RL agent. ","This paper proposes a data-driven method for optimal action space selection in the reinforcement learning problem. The proposed approach is based on the observation that cumulative reward values tend to decrease with the size of the action space. To address this issue, the authors propose a Monte Carlo sampling based algorithm to optimize the action search space. The authors propose to use the Monte Carlo Sampling method to reduce the cut-off cardinality computation for optimizing the proposed action space, which can improve efficiency and reduce the CPU utilization rate. Memory size is also reduced by optimizing the optimal step size for the action update rule. The main contribution of this paper is that it learns a ranked action set, which is then used to train an RL agent. Experiments are conducted on a cloud environment."
752,SP:a9a2c21110e00f19882d27bef0063c422a15e576,"method USED-FOR action sets. cloud infrastructure USED-FOR resource tuning example. Task are action set selection, and training. Method is RL agent. ","This paper proposes a method for selecting action sets for an RL agent. The main idea is to use a resource tuning example with cloud infrastructure to train the agent on the task of action set selection. During training, the RL agent is encouraged to select the best action set for each task."
753,SP:a9a2c21110e00f19882d27bef0063c422a15e576,"action space exploration USED-FOR policy. action space exploration USED-FOR RL methods. method USED-FOR action space exploration. reducing high CPU utilisation EVALUATE-FOR method. cloud infrastructure workload optimisation EVALUATE-FOR method. Task is Reinforcement Learning ( RL ). OtherScientificTerm are Dispensable actions, global reward returns, and action space. ",This paper addresses the problem of Reinforcement Learning (RL) in the setting of Dispensable actions. The paper proposes a method for action space exploration in RL methods to improve the policy performance. The method is motivated by reducing high CPU utilisation and reducing the global reward returns. The proposed method is evaluated on cloud infrastructure workload optimisation. The results show that the proposed method can improve the performance of RL methods by exploring the action space.
754,SP:a9a2c21110e00f19882d27bef0063c422a15e576,training action space USED-FOR reinforcement learning. Generic is problem. ,"This paper studies the problem of learning a training action space for reinforcement learning. The problem is well-motivated and interesting. However, the paper suffers from a lack of clarity in the presentation."
755,SP:0e0adc42f6025034d341dc9c17b3f6251afebc2f,"method USED-FOR probabilistic predictor. PAC setting FEATURE-OF problem. covariate shift FEATURE-OF PAC setting. covariate shift USED-FOR problem. deep net HYPONYM-OF probabilistic predictor. score function CONJUNCTION labeled sample. labeled sample CONJUNCTION score function. unlabeled samples PART-OF target distribution. score function USED-FOR learner. Probabilistic outputs PART-OF neural nets. bounded importance weights USED-FOR algorithm. cutoff value USED-FOR score function. algorithm USED-FOR PAC guarantee. cutoff value USED-FOR algorithm. rejection sampling USED-FOR source distribution. rejection sampling USED-FOR guarantee. algorithm COMPARE baseline methods. baseline methods COMPARE algorithm. PAC guarantees EVALUATE-FOR it. OtherScientificTerm are instance space, prediction set, and importance weights. Generic is this. ","This paper proposes a method to train a probabilistic predictor (i.e., a deep net) that is robust to covariate shift in the PAC setting. The problem is formulated as a PAC setting, where the target distribution is a set of unlabeled samples drawn from the instance space, and the learner is given a score function and a labeled sample. Probabilistic outputs of neural nets are assumed to be bounded. The authors propose an algorithm that uses bounded importance weights to obtain a PAC guarantee. The guarantee is obtained by using rejection sampling on the source distribution, and using a cutoff value on the score function used by the learned learner and the labeled sample from the prediction set. They show that their algorithm outperforms the baseline methods in terms of PAC guarantees. They also show that this can be extended to the case where importance weights are not bounded."
756,SP:0e0adc42f6025034d341dc9c17b3f6251afebc2f,"labelled source dataset CONJUNCTION unlabelled target dataset. unlabelled target dataset CONJUNCTION labelled source dataset. unlabelled target dataset USED-FOR problem. covariate shift assumption USED-FOR step. probabilistic classifier USED-FOR confidence bound. step CONJUNCTION heuristic method. heuristic method CONJUNCTION step. DomainNet dataset CONJUNCTION ImageNet dataset. ImageNet dataset CONJUNCTION DomainNet dataset. PS - R COMPARE PS - M method. PS - M method COMPARE PS - R. DomainNet data EVALUATE-FOR PS - R. dataset EVALUATE-FOR PS - M method. PS - C "" method HYPONYM-OF baselines. weighted split conformal inference ( WSCI ) method HYPONYM-OF baselines. PS - R COMPARE PS - M. PS - M COMPARE PS - R. adversarially perturbed dataset EVALUATE-FOR PS - R. upper and lower bounds USED-FOR PS - W. upper and lower bounds USED-FOR method. Generic are model, procedure, algorithm, strategy, and methods. OtherScientificTerm are S_m, target coverage level, pessimistic coverage, constraint, smoothness assumptions, and adversarial perturbations. Method are Binomial tail inverse, and importance sampling procedure. Metric is error - rates. ","This paper studies the problem of learning a model that is robust to adversarial perturbations. The problem is formulated as the following: given a labelled source dataset and an unlabelled target dataset, the goal is to estimate the confidence of the model. The paper proposes a procedure called ""Binomial tail inverse"" (PS-W), where the objective is to minimize the confidence bound of the probabilistic classifier on S_m, which is the target coverage level. The method is based on two steps: the first step assumes a covariate shift assumption, and the second step is a heuristic method. The authors show that under certain assumptions, the proposed algorithm can converge to a confidence bound that is at least as good as the original confidence bound under the same assumption. This is achieved by minimizing the pessimistic coverage of the target dataset under the constraint that the importance sampling procedure does not depend on the source dataset.  The authors compare PS-R with the PS-M method on DomainNet data and the ImageNet dataset, and show that the proposed method outperforms both methods in terms of error-rates. In addition, the authors compare the proposed strategy with two baselines, namely the ""PS-C"" method and the ""weighted split conformal inference (WSCI) method. In the case of PS-C, they show that PS-W outperforms the original method with respect to upper and lower bounds on the error. In contrast, in the case, they compare the performance with the ""Pessimistic coverage"" method, which does not require smoothness assumptions on the target data. They also show that for the adversarially perturbed dataset, PS-r outperform the original PS-m method. Finally, they evaluate the proposed methods on a different dataset, where they compare their methods with the original methods. They show that their method is more robust to different adversarial types of perturbation."
757,SP:0e0adc42f6025034d341dc9c17b3f6251afebc2f,"algorithms USED-FOR PAC prediction sets. estimated / known importance weights USED-FOR method. rejection sampling based strategy USED-FOR PAC constraints. extreme case weights CONJUNCTION rejection sampling based algorithm. rejection sampling based algorithm CONJUNCTION extreme case weights. rejection sampling based algorithm USED-FOR robust variant. extreme case weights USED-FOR robust variant. OtherScientificTerm are covariate shift, importance weights, and estimation error. Generic is methodology. ","This paper proposes algorithms for PAC prediction sets that are robust to covariate shift. The method is based on estimated/known importance weights. The main contribution of the paper is a rejection sampling based strategy for adapting to PAC constraints. The authors propose a robust variant based on extreme case weights and a simple reject sampling based algorithm. In addition, the importance weights are updated based on the estimation error. The experimental results demonstrate the effectiveness of the proposed methodology."
758,SP:0e0adc42f6025034d341dc9c17b3f6251afebc2f,"method USED-FOR approximately correct ( PAC ) prediction sets. approximately correct ( PAC ) prediction sets USED-FOR uncertainty quantification. method USED-FOR uncertainty quantification. Clopper - Pearson confidence intervals USED-FOR Binomial distribution. optimization problem CONJUNCTION Clopper - Pearson confidence intervals. Clopper - Pearson confidence intervals CONJUNCTION optimization problem. rejection sampling Clopper - Pearson bound USED-FOR extension. rate shift ” CONJUNCTION support shift ”. support shift ” CONJUNCTION rate shift ”. support shift ” EVALUATE-FOR algorithm. DomainNet and ImageNet datasets EVALUATE-FOR support shift ”. DomainNet and ImageNet datasets EVALUATE-FOR algorithm. algorithm COMPARE approaches. approaches COMPARE algorithm. OtherScientificTerm are covariate shift, and PAC constraint. Generic is It. Material is PAC prediction sets. ","This paper proposes a method for approximately correct (PAC) prediction sets for uncertainty quantification. It is an extension of the rejection sampling Clopper-Pearson bound, where the covariate shift is assumed to be independent of the PAC constraint. The optimization problem is replaced by the Clopper–Pearson confidence intervals for the Binomial distribution. The proposed algorithm is evaluated on the DomainNet and ImageNet datasets under rate shift “and support shift”. The results show that the proposed algorithm outperforms existing approaches."
759,SP:0c522ffa2c90eb88296ad0c7999200a72b8755e2,approach USED-FOR semi - supervised learning. iterative pseudo - labeling USED-FOR approach. KL divergence FEATURE-OF pseudo - labeled and true data distributions. KL divergence USED-FOR generalization error bound. paper USED-FOR binary classification examples. MNIST datasets USED-FOR binary classification examples. Material is unlabelled data. OtherScientificTerm is model parameters. Method is Gaussian mixture model. Task is binary classification. ,"This paper proposes an approach to semi-supervised learning based on iterative pseudo-labeling. The idea is to train a Gaussian mixture model on unlabelled data, where the model parameters are sampled from the true data distribution. The authors derive a generalization error bound based on the KL divergence between the pseudo-labelled and true data distributions. The paper is applied to binary classification examples from MNIST datasets, where binary classification is a very important problem in binary classification."
760,SP:0c522ffa2c90eb88296ad0c7999200a72b8755e2,"pseudo labeling HYPONYM-OF semi - supervised learning algorithm. information theoretic upper bound FEATURE-OF generalization error. generalization error FEATURE-OF iterative update of pseudo labeling. data distribution CONJUNCTION pseudo labeled samples. pseudo labeled samples CONJUNCTION data distribution. generalization error EVALUATE-FOR model. Generic is problem. OtherScientificTerm are mutual information, model parameters, and KL distance. Method are binary Gaussian Mixture Model, and iterative pseudo labeling. ","This paper studies the problem of pseudo labeling, a semi-supervised learning algorithm that aims to maximize the mutual information between the data distribution and the pseudo labeled samples. The authors derive an information theoretic upper bound on the generalization error of the iterative update of the pseudo labeling. The upper bound is based on a binary Gaussian Mixture Model, where the model parameters are assumed to be Gaussian, and the KL distance between the model and the data is defined as a function of the number of pseudo labels. The main contribution of the paper is the analysis of iterative pseudo labeling and its generalization performance."
761,SP:0c522ffa2c90eb88296ad0c7999200a72b8755e2,"generalization error EVALUATE-FOR learning algorithm. information - theoretic upper bound USED-FOR generalization error. information - theoretic upper bound USED-FOR learning algorithm. Gaussian class conditionals FEATURE-OF binary classification problem. deep neural network classifiers USED-FOR datasets. Task is semi - supervised learning. Method are pseudo - labeling, and classification model. OtherScientificTerm is unlabelled data batches. Material is labelled dataset. Metric is generalization bound. ","This paper studies semi-supervised learning. The authors propose a learning algorithm based on an information-theoretic upper bound on the generalization error of the learning algorithm. The main idea is to use pseudo-labeling, where the unlabelled data batches are used to train a classification model. The binary classification problem is formulated as Gaussian class conditionals, and the labelled dataset is used as the training data. The experiments are conducted on two datasets trained with deep neural network classifiers. The generalization bound is shown to be tight."
762,SP:0c522ffa2c90eb88296ad0c7999200a72b8755e2,"information - theoretic principles USED-FOR generalization error bound. binary Gaussian mixture model ( bGMM ) HYPONYM-OF model. upper bound FEATURE-OF generalization error. Method is bGMM. OtherScientificTerm are class conditional variances, and pseudo - labelling iterations. Material is MNIST and CIFAR datasets. ","This paper proposes a generalization error bound based on information-theoretic principles. The proposed model is a binary Gaussian mixture model (bGMM). The bGMM is trained to minimize the difference between the class conditional variances. The authors provide an upper bound on the generalized error, which depends on the number of pseudo-labelling iterations. Experiments are conducted on MNIST and CIFAR datasets."
763,SP:570149eb8fb97928f94312e40bdc48dfe9885848,method USED-FOR exploration. exploration COMPARE regular single - step action noise exploration. regular single - step action noise exploration COMPARE exploration. generator USED-FOR multi - step action sequence. RNN structure USED-FOR generator. plan value function USED-FOR generator. GPM COMPARE methods. methods COMPARE GPM. methods USED-FOR continuous control tasks. GPM USED-FOR continuous control tasks. Method is Generative Planning method ( GPM ). OtherScientificTerm is state trajectory. ,This paper proposes a method for exploration that is more efficient than regular single-step action noise exploration. The Generative Planning method (GPM) is based on the idea that the state trajectory of an action can be expressed as a function of a plan value function. The generator is trained with an RNN structure and is used to generate a multi-stepaction sequence. Experiments show that GPM outperforms existing methods on continuous control tasks.
764,SP:570149eb8fb97928f94312e40bdc48dfe9885848,"generative planning method ( GPM ) USED-FOR exploration. method USED-FOR exploration. generative planning method ( GPM ) USED-FOR RL. exploration USED-FOR RL. GPM USED-FOR exploration. rule USED-FOR MPC. planner USED-FOR GPM. planner USED-FOR exploration. planner HYPONYM-OF auto - regressive model. auto - regressive Q - function USED-FOR planner. stochastic RNN HYPONYM-OF auto - regressive model. l_commit_target HYPONYM-OF hyperparameter. it COMPARE methods. methods COMPARE it. low - dimensional robot domains CONJUNCTION image - based CARLA environment. image - based CARLA environment CONJUNCTION low - dimensional robot domains. epsilon greedy policies CONJUNCTION policies. policies CONJUNCTION epsilon greedy policies. GPM COMPARE action - repeat - based exploration methods. action - repeat - based exploration methods COMPARE GPM. policies HYPONYM-OF action - repeat - based exploration methods. epsilon greedy policies HYPONYM-OF action - repeat - based exploration methods. image - based CARLA environment EVALUATE-FOR methods. image - based CARLA environment EVALUATE-FOR it. low - dimensional robot domains EVALUATE-FOR methods. low - dimensional robot domains EVALUATE-FOR it. GPM USED-FOR exploration. OtherScientificTerm are predicted Q - values, and policy. ","This paper proposes a generative planning method (GPM) for exploration in RL. The method is motivated by the observation that exploration is an important component of RL, and that GPM can be used for exploration. GPM uses a planner that is an auto-regressive model (e.g., a stochastic RNN) that takes as input a set of predicted Q-values and outputs a policy that maximizes the Q-value of the next state. This rule is then used in MPC. The planner is trained to maximize the return of the policy, and the planner is used in GPM to guide exploration. The paper also proposes a hyperparameter called l_commit_target that encourages the policy to commit to states that are close to the current state. Experiments on low-dimensional robot domains and an image-based CARLA environment show that it outperforms existing methods. In particular, GPM outperforms action-replay-based exploration methods such as epsilon greedy policies and policies."
765,SP:570149eb8fb97928f94312e40bdc48dfe9885848,"recurrent action - plan generator CONJUNCTION recurrent critic. recurrent critic CONJUNCTION recurrent action - plan generator. recurrent action - plan generator USED-FOR action plan. SAC USED-FOR method. action - sequence plan USED-FOR policy. Method are generative planning method, and model - free and model - based methods. Generic is model. OtherScientificTerm are temporally extended action plans, temporally - coordinated exploration, and action - repeat. Metric is interpretability. ",This paper proposes a generative planning method. The method is based on SAC and uses a recurrent action-plan generator and a recurrent critic to generate an action plan. The policy is then trained on the action-sequence plan generated by the model. The model is trained to generate temporally extended action plans that can be used for temporally-coordinated exploration. Experiments are conducted on both model-free and model-based methods and show that the interpretability of the proposed method is improved. The paper also shows that the proposed action-repeat is more interpretable.
766,SP:570149eb8fb97928f94312e40bdc48dfe9885848,method USED-FOR exploration. Generative Planning ( GPM ) USED-FOR exploration. exploration USED-FOR model - free RL. recurrent model USED-FOR short term plans. GPM USED-FOR short term plans. GPM USED-FOR recurrent model. GPM COMPARE prior approaches. prior approaches COMPARE GPM. GPM USED-FOR interpretable short - term plans. continuous control benchmarks EVALUATE-FOR GPM. continuous control benchmarks EVALUATE-FOR prior approaches. OtherScientificTerm is temporally extended explorations. Generic is it. ,"This paper proposes a method called Generative Planning (GPM) for exploration in model-free RL. GPM trains a recurrent model to generate short term plans for temporally extended explorations. The authors evaluate GPM on continuous control benchmarks and show that GPM can generate interpretable short-term plans and outperform prior approaches. The paper is well-written and well-motivated, and it is easy to follow."
767,SP:ce6a93847209a0926ed0be5190378a3f61db1935,"nonlinear DMF COMPARE MF. MF COMPARE nonlinear DMF. n2 COMPARE n1. n1 COMPARE n2. generalization bound EVALUATE-FOR MF. generalization bound EVALUATE-FOR nonlinear DMF. two - mode matrix factorization CONJUNCTION multi - mode   tensor factorization. multi - mode   tensor factorization CONJUNCTION two - mode matrix factorization. synthetic data CONJUNCTION real data. real data CONJUNCTION synthetic data. real data EVALUATE-FOR algorithm. algorithm COMPARE algorithms. algorithms COMPARE algorithm. synthetic data EVALUATE-FOR algorithm. algorithms USED-FOR completion tasks. real data EVALUATE-FOR algorithms. synthetic data EVALUATE-FOR algorithms. completion tasks EVALUATE-FOR algorithm. Method are LRMC and LRTC algorithms, and Multi - Mode Nonlinear deep tensor factorization. ","This paper studies the LRMC and LRTC algorithms and proposes Multi-Mode Nonlinear deep tensor factorization. The authors show that the generalization bound of the nonlinear DMF is better than the MF, and that n2 is more efficient than n1. They also propose two-mode matrix factorization and multi-mode  tensor factorsization. They evaluate the proposed algorithm on both synthetic data and real data and show that their algorithm outperforms existing algorithms on several completion tasks."
768,SP:ce6a93847209a0926ed0be5190378a3f61db1935,two mode non - linear deep matrix factorization CONJUNCTION multi - mode nonlinear deep tensor factorization. multi - mode nonlinear deep tensor factorization CONJUNCTION two mode non - linear deep matrix factorization. deep learning CONJUNCTION tensor decomposition. tensor decomposition CONJUNCTION deep learning. two mode model USED-FOR multi - mode scenario. two mode non - linear deep matrix factorization HYPONYM-OF approaches. multi - mode nonlinear deep tensor factorization HYPONYM-OF approaches. nonlinear deep matrix factorization COMPARE linear deep matrix factorization. linear deep matrix factorization COMPARE nonlinear deep matrix factorization. linear deep matrix factorization USED-FOR matrix completion. nonlinear deep matrix factorization USED-FOR matrix completion. models COMPARE models. models COMPARE models. matrix and tensor completion tasks EVALUATE-FOR models. matrix and tensor completion tasks EVALUATE-FOR models. Generic is methods. Task is matrix and tensor factorization. ,This paper proposes two approaches: two mode non-linear deep matrix factorization and multi-mode nonlinear deep tensor factorization. The two mode model is designed for the multi-modal scenario and is inspired by deep learning and tensor decomposition. The authors show that the proposed methods are able to achieve better matrix completion performance compared to the existing nonlinear and linear deep matrices. The proposed models are evaluated on both matrix and the tensor completion tasks and compared to existing models. 
769,SP:ce6a93847209a0926ed0be5190378a3f61db1935,nonlinear deep matrix factorization COMPARE matrix factorization model. matrix factorization model COMPARE nonlinear deep matrix factorization. nonlinearity USED-FOR model. method USED-FOR tensor factorization. factor matrices PART-OF Tucker decomposition. deep factorization method USED-FOR factor matrices. Task is nonlinear low - rank completion. Method is two - mode nonlinear deep matrix factorization. Material is synthetic and real - world datasets. ,"This paper studies the problem of nonlinear low-rank completion. The authors propose a two-mode nonlinear deep matrix factorization. The proposed model is motivated by the nonlinearity of the input matrix, which is in contrast to the traditional matrix factorsization model. The method is applied to tensor factorization, where the factor matrices of the Tucker decomposition are obtained by a deep factorization method. Experiments are conducted on both synthetic and real-world datasets."
770,SP:ce6a93847209a0926ed0be5190378a3f61db1935,multi - mode framework USED-FOR deep learning based tensor decomposition. multi - mode deep matrix factorization method USED-FOR matrix completion. convergence guarantee FEATURE-OF multi - mode deep matrix factorization method. convergence guarantee FEATURE-OF multi - mode nonlinear deep tensor factorization method. optimization algorithms USED-FOR models. methods COMPARE state of the arts. state of the arts COMPARE methods. synthetic and real data sets EVALUATE-FOR methods. matrix / tensor form FEATURE-OF synthetic and real data sets. Material is nonlinear high - dimensional data sets. Method is deep matrix factorization ( DMF ) method. ,"This paper proposes a multi-mode framework for deep learning based tensor decomposition. The authors propose a deep matrix factorization (DMF) method that can be applied to nonlinear high-dimensional data sets. In particular, the authors provide a convergence guarantee for the multi-modal deep tensor factorization method for matrix completion. They also provide optimization algorithms for training the models. The proposed methods are evaluated on both synthetic and real data sets in matrix/tensor form and compared to the state of the arts."
771,SP:931661154975d94fc5ba1bc89d7a7fdf643df8f2,energy based model USED-FOR correlations between structured outputs. deep neural network USED-FOR structured prediction task. energy based model USED-FOR deep neural network. datasets EVALUATE-FOR work. work COMPARE baselines. baselines COMPARE work. datasets EVALUATE-FOR baselines. LIME HYPONYM-OF baselines. Task is interpreting and explaining structured output models. Method is interpretability block. OtherScientificTerm is ground truth output. ,"This paper addresses the problem of interpreting and explaining structured output models. The authors propose an interpretability block, where a deep neural network is trained to solve a structured prediction task using an energy based model to model correlations between structured outputs. The work is evaluated on two datasets and compared with two baselines (LIME and LIME). The results show that the ground truth output is more informative than the generated output."
772,SP:931661154975d94fc5ba1bc89d7a7fdf643df8f2,"output variable PART-OF structured - output ( MAP ) inference. energy model USED-FOR structured prediction ( x, y ). neural network CONJUNCTION Gumbel - softmax activation. Gumbel - softmax activation CONJUNCTION neural network. structured hinge loss USED-FOR it. neural network USED-FOR energy model. approach COMPARE attribution techniques. attribution techniques COMPARE approach. LIME CONJUNCTION SHAP. SHAP CONJUNCTION LIME. datasets EVALUATE-FOR attribution techniques. SHAP HYPONYM-OF attribution techniques. LIME HYPONYM-OF attribution techniques. datasets EVALUATE-FOR approach. Generic is technique. ","This paper proposes to incorporate an output variable into structured-output (MAP) inference. The technique is based on the idea that the energy model used for structured prediction (x, y) can be decomposed into a neural network and a Gumbel-softmax activation, and it is trained with a structured hinge loss. The proposed approach is evaluated on two datasets and compared to other attribution techniques such as LIME and SHAP."
773,SP:931661154975d94fc5ba1bc89d7a7fdf643df8f2,energy - based training method USED-FOR model interpretability. approach USED-FOR interpretable methods. interpretable methods USED-FOR model. feature - level importance score USED-FOR model. approach USED-FOR model. synthetic and public datasets EVALUATE-FOR method. Task is instance - wise feature selection. ,This paper proposes an energy-based training method for improving model interpretability. The proposed approach aims to train interpretable methods that can be used to train a model with a feature-level importance score. The method is evaluated on both synthetic and public datasets. The main contribution of the paper is the instance-wise feature selection.
774,SP:931661154975d94fc5ba1bc89d7a7fdf643df8f2,method USED-FOR interpreting structured output model. methodology USED-FOR synthetic energy function. methodology USED-FOR structured prediction energy networks. synthetic energy function CONJUNCTION structured prediction energy networks. structured prediction energy networks CONJUNCTION synthetic energy function. Method is structured output model. OtherScientificTerm is output random variable. ,This paper proposes a method for interpreting structured output model. The methodology is applied to both synthetic energy function and structured prediction energy networks. The main idea is to use the output random variable as the input. The paper is well-written and easy to follow.
775,SP:cf9b6963c32d8689f7203dd41b17461676d08739,"policy gradient method USED-FOR arbitrary utility functions. generalization USED-FOR arbitrary utility functions. CDF COMPARE expected reward. expected reward COMPARE CDF. CDF FEATURE-OF weightings. policy gradient method USED-FOR generalization. weightings FEATURE-OF arbitrary utility functions. weighting function USED-FOR CDF of the trajectory reward. trajectory reward FEATURE-OF utility function. utility function PART-OF generalization. weighting function PART-OF generalization. conservative weightings COMPARE formulation. formulation COMPARE conservative weightings. OpenAI Safety Gym EVALUATE-FOR this. Method are policy gradient, variance reduction baselines, and C3PO. OtherScientificTerm is PPO loss. ","This paper proposes a policy gradient method for generalization to arbitrary utility functions with different weightings in terms of the CDF instead of the expected reward. The generalization consists of adding a weighting function to the original CDF of the trajectory reward and then applying policy gradient on top of it. The authors compare this with variance reduction baselines and show that conservative weightings perform better than the formulation proposed in C3PO. They also compare this on OpenAI Safety Gym, where the PPO loss is used."
776,SP:cf9b6963c32d8689f7203dd41b17461676d08739,"Risk objectives PART-OF reinforcement learning ( RL ). exponential utility CONJUNCTION value - at - risk ( VaR ). value - at - risk ( VaR ) CONJUNCTION exponential utility. exponential utility HYPONYM-OF risk measures. value - at - risk ( VaR ) HYPONYM-OF risk measures. CPT USED-FOR human decision - making. CPT USED-FOR risk measures. utility $ u$ USED-FOR risk measures. sample - based estimation USED-FOR gradient. sample - based estimation USED-FOR objective. gradient FEATURE-OF objective. risk - aware, gradient estimator PART-OF PPO - like algorithm. risk - awareness USED-FOR PPO. Method is cumulative prospect theory ( CPT ). OtherScientificTerm are risk - aware objective, and policy parameters. ",Risk objectives in reinforcement learning (RL) have been studied in the context of cumulative prospect theory (CPT). CPT has been applied to human decision-making and has been shown to generalize well to other risk measures such as exponential utility and value-at-risk (VaR). This paper proposes a new risk-aware objective that uses sample-based estimation to estimate the gradient of the objective. The paper also proposes a PPO-like algorithm that incorporates the risk-awareness into the PPO. The main idea is to use the policy parameters as a proxy for the risk.
777,SP:cf9b6963c32d8689f7203dd41b17461676d08739,approach USED-FOR distributional DRL. objective USED-FOR approach. distribution policy gradient method USED-FOR risk - sensitive RL. distributional objective CONJUNCTION policy gradient methodology. policy gradient methodology CONJUNCTION distributional objective. distributional objective USED-FOR distribution policy gradient method. policy gradient methodology USED-FOR distribution policy gradient method. approach COMPARE PPO. PPO COMPARE approach. Material is OpenAI Safety Gym environments. ,This paper presents an approach to distributional DRL based on an objective. The authors propose a distribution policy gradient method for risk-sensitive RL based on a distributional objective and a policy gradient methodology. Experiments on OpenAI Safety Gym environments show that the proposed approach outperforms PPO.
778,SP:cf9b6963c32d8689f7203dd41b17461676d08739,"policy gradient method USED-FOR CDF based criterion. CPT USED-FOR CDF based criterion. policy gradient USED-FOR objective. estimation technique USED-FOR it. algorithm USED-FOR objective. PPO USED-FOR objective. PPO USED-FOR algorithm. adverse events FEATURE-OF negative rewards. Safety Gym environments EVALUATE-FOR approach. Wang weighting function USED-FOR objectives. average reward EVALUATE-FOR aggressive ( or risk - seeking ) one. OtherScientificTerm are weighting function, risk - aversion, cautious ( or risk - averse ) objective, and risk - averse values of the parameters. Metric is average reward objective. ",This paper proposes a policy gradient method for learning a CDF based criterion based on CPT. The main idea is to use policy gradient to learn an objective that is independent of the weighting function. The algorithm is based on PPO and it uses an estimation technique to estimate the risk-averse values of the parameters. The approach is evaluated on Safety Gym environments where the negative rewards are generated by adverse events. The authors show that the average reward objective of the cautious (or risk-avoidant) objective is lower than that of the aggressive ( or risk-seeking) one. The paper also shows that the Wang weighting functions can be used to learn new objectives.
779,SP:fa405481f36da10f8ca8d9d5c066458236806a12,flexible distribution approximator USED-FOR high - dimensional and computationally intensive stochastic simulator. neural process USED-FOR active learning setting. neural process USED-FOR acquisition function. toy - ish SEIR model CONJUNCTION epidemiological model. epidemiological model CONJUNCTION toy - ish SEIR model. toy - ish SEIR model EVALUATE-FOR method. Method is spatiotemporal neural process. Task is epidemiological simulators. ,"This paper proposes a spatiotemporal neural process for learning from data in epidemiological simulators. Specifically, the authors propose a flexible distribution approximator for a high-dimensional and computationally intensive stochastic simulator. The authors also propose a neural process to approximate the acquisition function in the active learning setting. The proposed method is evaluated on a toy-ish SEIR model and an epidemiological model."
780,SP:fa405481f36da10f8ca8d9d5c066458236806a12,method USED-FOR surrogate models of stochastic simulators. complexity EVALUATE-FOR inference task. Neural processes USED-FOR method. approach USED-FOR surrogate. low dimensional SEIR problem CONJUNCTION complex spatiotemporal LEAM - US problem. complex spatiotemporal LEAM - US problem CONJUNCTION low dimensional SEIR problem. low dimensional SEIR problem HYPONYM-OF epidemiology. complex spatiotemporal LEAM - US problem HYPONYM-OF epidemiology. Method is Interactive Neural Process ( INP ). ,"This paper proposes a method for learning surrogate models of stochastic simulators using Neural processes. The main idea is to reduce the complexity of the inference task. The proposed approach, called Interactive Neural Process (INP), is able to learn a surrogate for a variety of problems including epidemiology (low dimensional SEIR problem and complex spatiotemporal LEAM-US problem)."
781,SP:fa405481f36da10f8ca8d9d5c066458236806a12,neural process USED-FOR active learning framework. neural process USED-FOR simulator dynamics. neural process USED-FOR Bayesian active learning. simulator dynamics USED-FOR Bayesian active learning. neural process USED-FOR latent. neural process USED-FOR acquisition function. latent USED-FOR acquisition function. INP USED-FOR stochastic simulation. INP COMPARE GP. GP COMPARE INP. GP USED-FOR stochastic simulation. Method is LIG. ,This paper proposes a neural process for the active learning framework. The main idea is to use the neural process to model the simulator dynamics for Bayesian active learning. The neural process is then used to learn a latent for the acquisition function. The authors compare the performance of INP to GP for stochastic simulation and show that INP outperforms LIG.
782,SP:fa405481f36da10f8ca8d9d5c066458236806a12,"statistical emulation USED-FOR mechanistic models of epidemic disease transmission. approach USED-FOR statistical emulation. Interactive Neural Processes USED-FOR Stochastic Simulation. structured neural processes USED-FOR models. structured neural processes USED-FOR temporal and spatio - temporal character. temporal and spatio - temporal character FEATURE-OF models. active learning strategy USED-FOR neural processes. methodology USED-FOR SEIR compartmental model examples. OtherScientificTerm are disease simulator outputs, homogenous population, and age and space delimited cohorts. Method is SEIR compartmental model. ","This paper presents a novel approach to statistical emulation for mechanistic models of epidemic disease transmission. The authors propose Interactive Neural Processes for Stochastic Simulation, which learns models with both temporal and spatio-temporal character using structured neural processes. The neural processes are trained using an active learning strategy, where the disease simulator outputs are sampled from a homogenous population, and the neural processes learn to predict the outcomes of age and space delimited cohorts. The proposed methodology is applied to SEIR compartmental model examples, and is shown to achieve state-of-the-art performance."
783,SP:fdabafe7d5ca2239a241eba04e1f16cb1ac2316b,DP - SGD USED-FOR NLP tasks. DP - SGD USED-FOR method. it USED-FOR applications. method USED-FOR pre - trained language models. bert CONJUNCTION gpt. gpt CONJUNCTION bert. bert HYPONYM-OF pre - trained language models. gpt HYPONYM-OF pre - trained language models. DP - SGD USED-FOR NLP models. ghost clipping USED-FOR clipping. clipping PART-OF DP - SGD. ghost clipping USED-FOR DP - SGD. OtherScientificTerm is per - example gradients. Generic is model. ,"This paper proposes a method based on DP-SGD for NLP tasks. The method is applied to pre-trained language models such as bert and gpt, and it can be applied to other applications as well. The main contribution of the paper is the introduction of per-example gradients, which allows the model to sample from a larger pool of examples. The paper also proposes to use ghost clipping, which is a form of clipping that is used in DP-SVGD to improve the performance of NLP models."
784,SP:fdabafe7d5ca2239a241eba04e1f16cb1ac2316b,DP learning algorithm USED-FOR language models. DP - SGD USED-FOR language models. DP - SGD HYPONYM-OF DP learning algorithm. It USED-FOR dataset. sentence classification CONJUNCTION table - to - text generation. table - to - text generation CONJUNCTION sentence classification. table - to - text generation CONJUNCTION dialog generation tasks. dialog generation tasks CONJUNCTION table - to - text generation. GPT CONJUNCTION Bert. Bert CONJUNCTION GPT. dialog generation tasks EVALUATE-FOR model. table - to - text generation EVALUATE-FOR model. sentence classification EVALUATE-FOR model. pre - trained language models USED-FOR dialog generation tasks. pre - trained language models USED-FOR model. Bert HYPONYM-OF pre - trained language models. GPT HYPONYM-OF pre - trained language models. Method is DP. ,"This paper proposes DP-SGD, a DP learning algorithm for language models, which is an extension of DP. It is able to generate a dataset that can be used to train language models using DP. The model is evaluated on sentence classification, table-to-text generation, and dialog generation tasks using pre-trained language models (GPT, Bert, etc). The experiments show that DP is effective."
785,SP:fdabafe7d5ca2239a241eba04e1f16cb1ac2316b,"algorithm USED-FOR approximate differentially private NLP models. private data USED-FOR NLP model. isotropic noise USED-FOR aggregated gradients. DP - SGD CONJUNCTION DP - AGAGRAD. DP - AGAGRAD CONJUNCTION DP - SGD. norm clipping USED-FOR DP - SGD. SGD CONJUNCTION ADAGRAD. ADAGRAD CONJUNCTION SGD. ADAGRAD CONJUNCTION ADAM. ADAM CONJUNCTION ADAGRAD. differential privacy definition FEATURE-OF privacy. norm FEATURE-OF gradient. per - sample gradient USED-FOR procedure. GhostClipping method USED-FOR memory. back - propagation USED-FOR aggregation. back - propagation USED-FOR clipped gradient. aggregation USED-FOR clipped gradient. back - propagation USED-FOR it. forward pass CONJUNCTION backward passes. backward passes CONJUNCTION forward pass. memory USED-FOR SGD ( or ADAM ). SGD ( or ADAM ) USED-FOR It. forward pass PART-OF memory. forward pass USED-FOR It. backward passes USED-FOR It. memory USED-FOR It. masked prediction task PART-OF fine - tuning. text classification CONJUNCTION data - to - text generation. data - to - text generation CONJUNCTION text classification. data - to - text generation CONJUNCTION dialog generation tasks. dialog generation tasks CONJUNCTION data - to - text generation. dialog generation tasks EVALUATE-FOR paper. data - to - text generation EVALUATE-FOR paper. text classification EVALUATE-FOR paper. DP methods USED-FOR it. Method are Pretrained NLP models, per - sample gradient clipping, NLP models, and multi - task finetuning. OtherScientificTerm are leakage, memory overhead, batch_size$\times$#params, partial sum of gradient element - wise square, per - sample norm, and batch size. Generic are them, and models. ","This paper proposes an algorithm for training approximate differentially private NLP models. Pretrained pre-trained on private data, an NLP model is trained on the private data without leakage. The paper proposes a new algorithm called per-sample gradient clipping, which is an efficient and effective way to fine-tune a NLP training set without memory overhead. The main idea is to use isotropic noise to generate aggregated gradients, which are then used to train the model. The authors propose two variants of norm clipping, DP-SGD and DP-AGAGRAD, which can be seen as a combination of DP-SVGD with norm clipping. The privacy is based on the differential privacy definition, which states that the norm of the gradient should not be larger than a partial sum of gradient element-wise square, and the authors propose a procedure based on per-sampling gradient to reduce the memory overhead by using the GhostClipping method. It uses memory to train SGD (or ADAM), SGD with ADAGGARAD, ADAM, and ADAM. It also uses back-propagation to perform aggregation for the clipped gradient, and it uses forward pass and backward passes for the forward pass.  The paper is evaluated on text classification, data-to-text generation, and dialog generation tasks. It is shown to outperform the existing DP methods by a large margin, and outperforms them when the batch_size$\times$#params is small. The experiments also show that fine-tuning with a masked prediction task is an effective, and that models trained with multi-task finetuning can outperform models trained on only one task.   The authors also provide a theoretical analysis of per-samples gradient clipping and show that the per-variance of the per sample norm can be used to estimate the batch size. "
786,SP:fdabafe7d5ca2239a241eba04e1f16cb1ac2316b,sentence classification CONJUNCTION language generation. language generation CONJUNCTION sentence classification. privately fine - tuning large language models USED-FOR downstream NLP tasks. language generation HYPONYM-OF downstream NLP tasks. sentence classification HYPONYM-OF downstream NLP tasks. batch size CONJUNCTION learning rate. learning rate CONJUNCTION batch size. learning rate CONJUNCTION training epochs. training epochs CONJUNCTION learning rate. training epochs CONJUNCTION clipping norm. clipping norm CONJUNCTION training epochs. fine - tuning task CONJUNCTION pretraining tasks. pretraining tasks CONJUNCTION fine - tuning task. hyper - parameters USED-FOR fine - tuning task. clipping norm HYPONYM-OF hyper - parameters. training epochs HYPONYM-OF hyper - parameters. batch size HYPONYM-OF hyper - parameters. learning rate HYPONYM-OF hyper - parameters. DP - SGD USED-FOR fine - tuning large language models. ghost clipping trick USED-FOR memory saving. Method is large language models. OtherScientificTerm is low dimensional updates. ,"This paper studies the problem of privately fine-tuning large language models for downstream NLP tasks such as sentence classification and language generation. In particular, the authors propose to fine-tune large language model using DP-SGD, where the hyper-parameters such as batch size, learning rate, training epochs, clipping norm, etc. are used to guide the fine tuning task and pretraining tasks. The authors also propose to use the ghost clipping trick for memory saving, which allows for low dimensional updates."
787,SP:33008c957718d546ecb2d7b8800ef5b03700ace4,transform - and - control policy USED-FOR robotic agents'designs. design generation and control PART-OF decision - making process. bi - level optimization USED-FOR agent design. training experience USED-FOR sample efficiency. GNN policy USED-FOR Joint - specialized MLP. Method is RL algorithm. Generic is formulation. ,"This paper proposes a transform-and-control policy for robotic agents' designs. The key idea is to incorporate both design generation and control into the decision-making process. The agent design is formulated as a bi-level optimization, and the RL algorithm is trained to maximize the sample efficiency. Joint-specialized MLP is trained with a GNN policy, and sample efficiency is improved by training experience. The paper is well-written and the formulation is interesting."
788,SP:33008c957718d546ecb2d7b8800ef5b03700ace4,"morphology design USED-FOR robots. policy USED-FOR robot's morphology. policy USED-FOR design. common behavior policy USED-FOR it. common behavior policy USED-FOR design. technique USED-FOR asymmetric morphologies. JSMPL USED-FOR asymmetric morphologies. Mujoco simulator EVALUATE-FOR evolutionary methods. sample efficiency EVALUATE-FOR evolutionary methods. Method are joint GNN policy, and GNN architecture. Generic is method. ","This paper addresses the problem of morphology design for robots. The authors propose a joint GNN policy, where it learns a common behavior policy that can be used to guide the design of a robot's morphology. The proposed method, called JSMPL, can be applied to any GNN architecture. The technique is also applied to asymmetric morphologies, which is shown to improve the sample efficiency of evolutionary methods on the Mujoco simulator."
789,SP:33008c957718d546ecb2d7b8800ef5b03700ace4,"algorithm USED-FOR simultaneous agent design. algorithm USED-FOR policy optimization. simultaneous agent design CONJUNCTION policy optimization. policy optimization CONJUNCTION simultaneous agent design. bone length CONJUNCTION size. size CONJUNCTION bone length. size CONJUNCTION motor strength. motor strength CONJUNCTION size. node attributes CONJUNCTION motor control commands. motor control commands CONJUNCTION node attributes. motor strength HYPONYM-OF node attributes. bone length HYPONYM-OF node attributes. size HYPONYM-OF node attributes. graph neural networks ( GNNs ) USED-FOR policy. PPO USED-FOR policy. method COMPARE prior approaches. prior approaches COMPARE method. evolutionary methods USED-FOR optimization. method COMPARE method. method COMPARE method. evolutionary methods USED-FOR prior approaches. policy gradient algorithms USED-FOR method. OtherScientificTerm are body structure, and skeleton structure. Method is GNNs. ","This paper proposes a new algorithm for simultaneous agent design and policy optimization. The authors propose to learn a policy using graph neural networks (GNNs). The policy is trained using PPO, where the node attributes (bone length, size, and motor strength) and the motor control commands are learned simultaneously. The proposed method is compared to prior approaches that use evolutionary methods for optimization, as well as a method based on policy gradient algorithms. The main contribution of the paper is to propose to use GNNs to learn the body structure of the agent. The skeleton structure is then used to train the policy."
790,SP:33008c957718d546ecb2d7b8800ef5b03700ace4,"simulated robot USED-FOR locomotion tasks. conditioned policy USED-FOR task. controlling of the robot USED-FOR task. morphology design of the robot CONJUNCTION design parameter adjustment. design parameter adjustment CONJUNCTION morphology design of the robot. conditioned policy USED-FOR stages. morphology design of the robot HYPONYM-OF stages. controlling of the robot HYPONYM-OF stages. design parameter adjustment HYPONYM-OF stages. graph neural networks USED-FOR morphologies. graph neural networks USED-FOR algorithm. joint - specific architecture USED-FOR network. joint - specific architecture USED-FOR algorithm. Method are reinforcement learning algorithm, and policy learning framework. OtherScientificTerm is robot. Generic is agents. ","This paper proposes a reinforcement learning algorithm that learns to control a simulated robot for locomotion tasks. The algorithm consists of two stages: morphology design of the robot and design parameter adjustment, where the conditioned policy is used to solve the task. The proposed algorithm uses graph neural networks to learn morphologies, and a joint-specific architecture to train the network. The authors also propose a policy learning framework that allows the agent to learn to control the robot in an unsupervised manner. The experiments show that the proposed agents can achieve state-of-the-art performance."
791,SP:46e8c6a9d7729e5112b3c9f8ff91d9557ea524c1,"first layer COMPARE fully connected layer. fully connected layer COMPARE first layer. image representation CONJUNCTION video representation. video representation CONJUNCTION image representation. video representation CONJUNCTION 3D shape representation. 3D shape representation CONJUNCTION video representation. Method are coordinate - based network architecture, and coordinate - based MLPs. Generic is network. ","This paper proposes a coordinate-based network architecture. The first layer is a fully connected layer and the second layer is an extension of the first layer. The network is trained on a set of datasets, including image representation, video representation, and 3D shape representation. Experiments are conducted on a variety of datasets and compare to other coordinate based MLPs."
792,SP:46e8c6a9d7729e5112b3c9f8ff91d9557ea524c1,"architecture USED-FOR implicit neural representations. fusion operation USED-FOR H x W features. outer product USED-FOR branch. MLP layers USED-FOR predicted features. MLP layers USED-FOR fused layers. splitting strategies USED-FOR branches. method CONJUNCTION splitting strategies. splitting strategies CONJUNCTION method. images CONJUNCTION videos. videos CONJUNCTION images. videos CONJUNCTION 3D shapes and NerF scenes. 3D shapes and NerF scenes CONJUNCTION videos. images HYPONYM-OF data modalities. videos HYPONYM-OF data modalities. 3D shapes and NerF scenes HYPONYM-OF data modalities. architecture USED-FOR implicit neural representations. training / inference speed EVALUATE-FOR architecture. Method are CoordX, and shared fully connected layers. OtherScientificTerm is hidden feature. Material is image. Metric is reconstruction. ","This paper proposes a new architecture for learning implicit neural representations, called CoordX. The core idea is to use shared fully connected layers, where a fusion operation is performed on the H x W features. The outer product of each branch is then used to reconstruct the original image. The fused layers are trained with MLP layers, which are used to generate the predicted features. This method is combined with other splitting strategies to split the branches into smaller ones. Experiments are conducted on various data modalities, including images, videos, 3D shapes and NerF scenes. The results show that the proposed architecture can learn implicit deep neural representations while maintaining training/inference speed. The reconstruction results are also shown."
793,SP:46e8c6a9d7729e5112b3c9f8ff91d9557ea524c1,"modification USED-FOR INR models. layers PART-OF decomposed coordinate grid. multidimensional coordinate grids USED-FOR INR models. layer USED-FOR joint space. stack of shared layers CONJUNCTION outer product. outer product CONJUNCTION stack of shared layers. outer product USED-FOR joint ( x, y ) space. videos CONJUNCTION shapes. shapes CONJUNCTION videos. fitting images CONJUNCTION videos. videos CONJUNCTION fitting images. FLOPS CONJUNCTION memory usage. memory usage CONJUNCTION FLOPS. shapes CONJUNCTION volumetric rendering. volumetric rendering CONJUNCTION shapes. compute efficiency EVALUATE-FOR implicit neural representation tasks. approach USED-FOR implicit neural representation tasks. parametric efficiency EVALUATE-FOR approach. compute efficiency EVALUATE-FOR approach. volumetric rendering HYPONYM-OF implicit neural representation tasks. fitting images HYPONYM-OF implicit neural representation tasks. shapes HYPONYM-OF implicit neural representation tasks. videos HYPONYM-OF implicit neural representation tasks. radiance fields USED-FOR volumetric rendering. OtherScientificTerm are grid, and linear layer. Generic is decomposition. ","This paper proposes a modification to INR models trained on multidimensional coordinate grids. The main idea is to decompose the grid into a joint (x,y) space using a stack of shared layers and an outer product. Each layer is then used to represent the joint space. The decomposition is done by adding a linear layer to each layer. The proposed approach is evaluated on several implicit neural representation tasks (fitting images, videos, shapes, volumetric rendering with radiance fields) and shown to improve parametric efficiency, compute efficiency, and memory usage."
794,SP:46e8c6a9d7729e5112b3c9f8ff91d9557ea524c1,network architecture USED-FOR CoortMLP. OtherScientificTerm is MAC ops. ,"This paper proposes a new network architecture for CoortMLP. The main contribution of this paper is to propose to use MAC ops. The idea is interesting and the experiments are interesting. However, there are some issues that need to be addressed."
795,SP:3ea7edab6ae65758b99615be07b7778188a6ff9f,"appearances CONJUNCTION 3D poses. 3D poses CONJUNCTION appearances. object shapes CONJUNCTION appearances. appearances CONJUNCTION object shapes. unsupervised scene decomposition model USED-FOR object shapes. unsupervised scene decomposition model USED-FOR appearances. unsupervised scene decomposition model USED-FOR 3D poses. models COMPARE structured, 3D object representations. structured, 3D object representations COMPARE models. inferred object representations USED-FOR visual reasoning task. ","This paper proposes an unsupervised scene decomposition model for object shapes, appearances, 3D poses, etc. The proposed models are shown to be more interpretable than the structured, 3D object representations. The paper also presents a visual reasoning task based on inferred object representations, which is interesting."
796,SP:3ea7edab6ae65758b99615be07b7778188a6ff9f,color CONJUNCTION pose. pose CONJUNCTION color. Slot Attention encoder CONJUNCTION GIRAFFE decoder. GIRAFFE decoder CONJUNCTION Slot Attention encoder. GIRAFFE decoder PART-OF autoencoding solution. Slot Attention encoder PART-OF autoencoding solution. encoder USED-FOR latent variables. latent variables USED-FOR Neural Radiance Field ( NeRF ). model USED-FOR object - wise scene manipulation. model COMPARE non - object - centric methods. non - object - centric methods COMPARE model. non - object - centric methods USED-FOR CATER snitch localization. transformer USED-FOR non - object - centric methods. CLEVR6 EVALUATE-FOR 2D segmentation. 2D segmentation EVALUATE-FOR model. CLEVR6 EVALUATE-FOR model. OtherScientificTerm is supervision. Method is decoder. ,"This paper proposes an autoencoding solution consisting of a Slot Attention encoder and a GIRAFFE decoder. The encoder encodes the latent variables for the Neural Radiance Field (NeRF) and the decoder predicts the color, pose, etc. without supervision. The model is applied to object-wise scene manipulation and is compared to non-object-centric methods that use transformer. The proposed model is evaluated on CLEVR6 for 2D segmentation and CATER snitch localization."
797,SP:3ea7edab6ae65758b99615be07b7778188a6ff9f,"slot - attention CONJUNCTION object NeRF functions. object NeRF functions CONJUNCTION slot - attention. slot - attention USED-FOR inference. model USED-FOR 3D scenes. slot - attention USED-FOR model. method USED-FOR slots. appearance USED-FOR colours. NeRF renderer USED-FOR slots. CATER CONJUNCTION downstream tasks. downstream tasks CONJUNCTION CATER. CLEVR data CONJUNCTION CATER. CATER CONJUNCTION CLEVR data. OtherScientificTerm are camera coordinates, density, and scene space. ","This paper proposes a model for 3D scenes that uses slot-attention and object NeRF functions for inference. The proposed method generates slots from a NeRF renderer, where the slots correspond to camera coordinates and colours correspond to appearance. The density of the slots is determined by the scene space. Experiments are conducted on CLEVR data, CATER, and downstream tasks."
798,SP:3ea7edab6ae65758b99615be07b7778188a6ff9f,"model USED-FOR structured 3D object representations. 2D scene USED-FOR structured 3D object representations. mechanism USED-FOR object slot latent code. slot latent code USED-FOR 3D object representations. inference part USED-FOR object slot latent code. Slot Attention USED-FOR object slot latent code. Slot Attention USED-FOR mechanism. Slot Attention USED-FOR inference part. mechanism USED-FOR inference part. MLP USED-FOR 3D object representations. 3D neural rendering USED-FOR rendering part. object color value CONJUNCTION occupancy value. occupancy value CONJUNCTION object color value. 3D location CONJUNCTION 2D view direction. 2D view direction CONJUNCTION 3D location. 2D view direction CONJUNCTION object latent. object latent CONJUNCTION 2D view direction. 2D view direction USED-FOR NeRF. 3D location USED-FOR NeRF. object latent USED-FOR NeRF. NeRF USED-FOR Rendering. scene rendering PART-OF Object rendering. 3D object - centric representation USED-FOR 2D visual scene. 2D object - centric scene segmentation method CONJUNCTION 3D neural rendering approach. 3D neural rendering approach CONJUNCTION 2D object - centric scene segmentation method. it USED-FOR manipulable object representation. 2D object - centric scene segmentation method COMPARE it. it COMPARE 2D object - centric scene segmentation method. 3D neural rendering approach COMPARE it. it COMPARE 3D neural rendering approach. segmentation EVALUATE-FOR it. OtherScientificTerm are visual scene, and shared NeRF function. Method is inference stage. ","This paper proposes a model for learning structured 3D object representations from a 2D scene. The inference part uses Slot Attention to generate an object slot latent code for 3D objects. The rendering part is based on 3D neural rendering, where an MLP is used to generate 3Dobject representations. Rendering is divided into scene rendering and object rendering. In the scene rendering part, NeRF takes as input the 3D location, 2D view direction, object latent, object color value, and occupancy value of the visual scene, and outputs a shared NeRF function. During the inference stage, the object latent is updated based on the object location. The authors compare their 2D object-centric scene segmentation method with a recent state-of-the-art 3D-neural rendering approach, and show that it can learn a manipulable object representation, and perform better on segmentation than it can on 2D visual scene."
799,SP:05c61145f3fc9486728aca19c4543065fe04e99c,"deconfounder D USED-FOR OOD explanations. deconfounder D USED-FOR spurious path. front - door adjustment USED-FOR causal graph. Conditional - VGAE USED-FOR graphs. graphs USED-FOR OOD case. graph generator USED-FOR front door adjustment. Method is GNN explanation methods. Generic are them, model, and evaluation method. Material is synthetic dataset. OtherScientificTerm are graph variable, and explainer variable. ","This paper studies the problem of GNN explanation methods, and proposes to use deconfounder D to generate OOD explanations. The main idea is to train a model that generates graphs that are similar to Conditional-VGAE, and then use them to generate spurious path, which is then used as a deconfounder to generate the spurious path. The authors also propose a front-door adjustment to the causal graph, where the front door adjustment is performed by a graph generator, and the graph variable is used as the explainer variable. Experiments are conducted on a synthetic dataset, and an evaluation method is presented."
800,SP:05c61145f3fc9486728aca19c4543065fe04e99c,"causal view USED-FOR OOD effect. causal view USED-FOR GNNs. causal discovery USED-FOR front - door adjustment. causal discovery USED-FOR deconfounding evaluation method. front - door adjustment USED-FOR deconfounding evaluation method. generative model USED-FOR surrogate subgraph. OtherScientificTerm is subgraphs. Task are model prediction, and evaluation. Generic is method. Method is method ( DSE ). ",This paper proposes a deconfounding evaluation method based on causal discovery as a front-door adjustment to GNNs to mitigate the OOD effect caused by the causal view. The proposed method (DSE) is based on the observation that the subgraphs of a generative model can be used as surrogate subgraph for model prediction and evaluation.
801,SP:05c61145f3fc9486728aca19c4543065fe04e99c,"conditional variational graph auto - encoder USED-FOR prediction. surrogate variable USED-FOR out - of - distribution effect. adversarial training USED-FOR model. OtherScientificTerm are out - of - distribution, and subgraph. ",This paper proposes a conditional variational graph auto-encoder for prediction. The main idea is to use a surrogate variable to mitigate the out-of-distribution effect. The model is trained with adversarial training. The paper shows that the surrogate variable can be used to reduce the out -of-dispersion effect when the subgraph is sparse.
802,SP:05c61145f3fc9486728aca19c4543065fe04e99c,"explainer - agnostic method USED-FOR biases of feature importance scores. feature attribution USED-FOR GNNs. biases of feature importance scores FEATURE-OF feature attribution. feature importance scores FEATURE-OF GNN feature attribution framework. subgraph COMPARE data graphs. data graphs COMPARE subgraph. subgraph USED-FOR subgraph important scores. method USED-FOR surrogate graphs. method USED-FOR front - door adjustment. CVGAE USED-FOR front - door adjustment. data graph distribution FEATURE-OF surrogate graphs. CVGAE USED-FOR method. CVGAE USED-FOR surrogate graphs. OtherScientificTerm are subgraph patterns, distribution of training data graphs, and distribution shift. Method is GNN explainers. Generic is framework. ","This paper proposes an explanation-agnostic method to estimate the biases of feature importance scores in feature attribution in GNNs. The main contribution of this paper is to propose a GNN feature attribution framework that considers the subgraph patterns instead of the underlying distribution of training data graphs. The subgraph important scores are computed by comparing the performance of a subgraph with respect to the data graphs of the same class. The proposed method uses CVGAE to generate surrogate graphs with different data graph distribution to perform front-door adjustment. The experimental results demonstrate the effectiveness of the proposed framework. The paper is well-written and well-motivated. However, the paper suffers from a lack of comparison to other GNN explainers, which makes it difficult to judge the impact of distribution shift."
803,SP:bb74fef9222f227343909f3936f1a8cd2322bbeb,"large, pretrained models USED-FOR active learning setup. image datasets CONJUNCTION text dataset. text dataset CONJUNCTION image datasets. text dataset EVALUATE-FOR large pre - trained models. image datasets EVALUATE-FOR large pre - trained models. AL procedure HYPONYM-OF uncertainty sampling procedure. actively labeled training dataset USED-FOR active learning procedure. Material is randomly sampled data. Generic is datasets. Metric is measuring robustness. OtherScientificTerm are distribution shifts, and data imbalance. ","This paper proposes an active learning setup with large, pretrained models. The authors evaluate large pre-trained models on two image datasets and one text dataset. The datasets are generated from randomly sampled data. The main contribution of the paper is the AL procedure, an uncertainty sampling procedure that aims at measuring robustness to distribution shifts and data imbalance. The active learning procedure is based on an actively labeled training dataset."
804,SP:bb74fef9222f227343909f3936f1a8cd2322bbeb,"uncertainty sampling USED-FOR models. pretrained embeddings USED-FOR models. uncertainty sampling USED-FOR fine - tuning image / NLP pretrained models. Waterbirds / Treeperson / iWildCam2020 - WILDS USED-FOR image classification. image classification CONJUNCTION Amazon - WILDS. Amazon - WILDS CONJUNCTION image classification. Waterbirds / Treeperson / iWildCam2020 - WILDS CONJUNCTION Amazon - WILDS. Amazon - WILDS CONJUNCTION Waterbirds / Treeperson / iWildCam2020 - WILDS. Amazon - WILDS USED-FOR review star prediction. text USED-FOR Waterbirds / Treeperson / iWildCam2020 - WILDS. least confident selection HYPONYM-OF uncertainty sampling. image datasets EVALUATE-FOR method. Method is random sampling. OtherScientificTerm are semantic meaning, and interpretable spurious associations. Generic is they. ","This paper studies the problem of fine-tuning image/NLP pretrained models using uncertainty sampling. The authors argue that models trained on pretrained embeddings are prone to misclassification, as they are trained with random sampling. To address this issue, the authors propose to use least confident selection, which is a form of uncertainty sampling that does not rely on semantic meaning but instead only on interpretable spurious associations. Experiments are conducted on image classification, Amazon-WILDS for review star prediction, and on text for Waterbirds/Treeperson/iWildCam2020 - WILDS. The proposed method is evaluated on three image datasets."
805,SP:bb74fef9222f227343909f3936f1a8cd2322bbeb,pre - training USED-FOR active learning. active learning COMPARE models. models COMPARE active learning. pre - training USED-FOR models. pre - trained models USED-FOR active learning. un - pretrained ones USED-FOR active learning. pre - trained models USED-FOR active learning. pre - trained models COMPARE un - pretrained ones. un - pretrained ones COMPARE pre - trained models. active learning COMPARE random sampling. random sampling COMPARE active learning. random sampling USED-FOR pre - trained models. active learning USED-FOR pre - trained models. Material is text and image datasets. ,"This paper studies the effect of pre-training on active learning compared to models trained without active learning. The authors show that pre-trained models trained with active learning perform better than un-pretrained ones in active learning, and that active learning is more effective than random sampling. The results are shown on text and image datasets."
806,SP:bb74fef9222f227343909f3936f1a8cd2322bbeb,pre - trained models USED-FOR vision and NLP tasks. pre - trained models USED-FOR active learning. domain shift CONJUNCTION label imbalance. label imbalance CONJUNCTION domain shift. spurious correlation CONJUNCTION domain shift. domain shift CONJUNCTION spurious correlation. domain shift FEATURE-OF datasets. label imbalance FEATURE-OF datasets. spurious correlation FEATURE-OF datasets. pre - trained models COMPARE random baseline. random baseline COMPARE pre - trained models. pre - trained models COMPARE un - pre - trained counterparts. un - pre - trained counterparts COMPARE pre - trained models. random baseline COMPARE un - pre - trained counterparts. un - pre - trained counterparts COMPARE random baseline. uncertainty acquisition function USED-FOR pre - trained models. ,"This paper studies the effect of pre-trained models for vision and NLP tasks on active learning. The authors consider datasets with spurious correlation, domain shift, label imbalance, etc. They show that the pre-pre-trained model with the uncertainty acquisition function outperforms the random baseline."
807,SP:f5e9fc0b1b6a41e43ba4dd0cfd99d5ec7008eedf,inherent ( parse-)tree structure CONJUNCTION graphical properties. graphical properties CONJUNCTION inherent ( parse-)tree structure. data - flow HYPONYM-OF graphical properties. graph - based encoder CONJUNCTION tree - edit decoder. tree - edit decoder CONJUNCTION graph - based encoder. tree - based objective EVALUATE-FOR masked language modeling. model USED-FOR near - SOTA. benchmark EVALUATE-FOR near - SOTA. real - world bug fixes USED-FOR benchmark. benchmark EVALUATE-FOR model. Task is Automated program repair. ,Automated program repair is a hot topic in the recent years due to its inherent (parse-)tree structure and graphical properties such as data-flow. This paper proposes a tree-based objective to evaluate masked language modeling. The authors propose a graph-based encoder and tree-edit decoder and show that their model can achieve near-SOTA on a new benchmark with real-world bug fixes.
808,SP:f5e9fc0b1b6a41e43ba4dd0cfd99d5ec7008eedf,model USED-FOR program repair. sequential structural tree edits USED-FOR program repair. sequential structural tree edits USED-FOR model. ASTs USED-FOR model. ASTs USED-FOR program repair. graph encode and decoder USED-FOR sequential tree edits. graph encode and decoder USED-FOR model. method USED-FOR model. model COMPARE code repair models. code repair models COMPARE model. CodeBERT CONJUNCTION CodeT5. CodeT5 CONJUNCTION CodeBERT. model COMPARE pre - trained models. pre - trained models COMPARE model. Wild Java repair dataset EVALUATE-FOR code repair models. CodeBERT HYPONYM-OF pre - trained models. CodeT5 HYPONYM-OF pre - trained models. OtherScientificTerm is subtrees. ,This paper proposes a model for program repair based on ASTs. The proposed model uses a graph encode and decoder to perform sequential tree edits to perform program repair. The method is well motivated and the proposed model is able to achieve state-of-the-art results compared to other code repair models on the Wild Java repair dataset (CodeBERT and CodeT5). The main contribution of the paper is the introduction of subtrees and the use of sequential structural tree edits.
809,SP:f5e9fc0b1b6a41e43ba4dd0cfd99d5ec7008eedf,"approach USED-FOR abstract syntax tree - based automatic program repair. it COMPARE edit - based ones. edit - based ones COMPARE it. method COMPARE edit - based and sequence - based approaches. edit - based and sequence - based approaches COMPARE method. Generic are technique, and model. Method is deleted - subtree reconstruction. OtherScientificTerm is syntax tree. ","This paper presents an approach for abstract syntax tree-based automatic program repair. The technique is based on the idea that the deleted-subtree reconstruction can be viewed as an extension of the original syntax tree. The authors show that the model is able to reconstruct the entire syntax tree, and compare it with edit-based and sequence-based approaches."
810,SP:f5e9fc0b1b6a41e43ba4dd0cfd99d5ec7008eedf,GRAPHIX HYPONYM-OF graph edit model. graph edit model USED-FOR program repair. graph edit USED-FOR program repair. accuracy CONJUNCTION complexity. complexity CONJUNCTION accuracy. multi - head graph encoder USED-FOR Hoppity. Hoppity USED-FOR GRAPHIX. multi - head graph encoder USED-FOR GRAPHIX. GRAPHIX USED-FOR edit sequence. GRAPHIX USED-FOR program repair samples. pre - training task USED-FOR model. It COMPARE baselines. baselines COMPARE It. pre - training USED-FOR baselines. pre - training USED-FOR GRAPHIX - P. model EVALUATE-FOR GRAPHIX - P. ,"This paper proposes GRAPHIX, a graph edit model for program repair using graph edit for graph edit in the context of program repair. The authors propose GRAGP, which is based on Hoppity and uses a multi-head graph encoder similar to HoppITY. The main difference between GRAPHP and previous work is that the authors propose a new edit sequence that is generated by GRAPHix instead of using the original edit sequence. The model is trained on a pre-training task and evaluated on program repair samples. It is shown to outperform baselines that do not use the pre-train. The proposed model is evaluated on a single model and compared to several baselines."
811,SP:c7b724c671def2800694fcc2625fa48d98c7cfe6,"Adversarial training USED-FOR heterogeneity of data distributions. benign distributions USED-FOR model. adversarial training CONJUNCTION federated learning settings. federated learning settings CONJUNCTION adversarial training. method COMPARE state - of - the - arts. state - of - the - arts COMPARE method. adversarial training USED-FOR method. federated learning settings USED-FOR method. Method is federated adversarial training. Metric is robustness accuracy. OtherScientificTerm are overfitted local robustness, and harsh distributions. ","Adversarial training aims to address the heterogeneity of data distributions. In federated adversarial training, the model is trained on benign distributions, and the robustness accuracy is used to train on adversarial distributions. The authors argue that this method is superior to the state-of-the-arts in terms of overfitted local robustness to harsh distributions. They demonstrate the effectiveness of their method on both adversarial and federated learning settings."
812,SP:c7b724c671def2800694fcc2625fa48d98c7cfe6,"\alpha - weighted relaxation USED-FOR Adversarial Training. Adversarial Training USED-FOR federated learning setting. \alpha - Weighted Federated Adversarial Training USED-FOR IID and Non - IID federated learning settings. Method are Federated Adversarial Training approach, and Federated Adversarial Training. ","This paper proposes a Federated Adversarial Training approach, which is based on the idea of \alpha-weighted relaxation of the standard   \alpha - weighted relaxation used in Adersarial Training in the federated learning setting. The authors propose a new variant of the Federated Federated Training approach and show the effectiveness of the proposed \alpha Weighted Federated Anti-Defamation Training (AFT) in both IID and Non-IID Federated learning settings. The paper is well-written and easy to follow."
813,SP:c7b724c671def2800694fcc2625fa48d98c7cfe6,alpha - weighted mechanism USED-FOR Federated Adversarial Training. Method is alpha Weighted Federated Adversarial Training algorithm. OtherScientificTerm is local machine. ,This paper proposes an alpha-weighted mechanism for Federated Adversarial Training. The authors propose an alpha Weighted Federated Anti-Defamation Training algorithm. The main idea is to train a local machine to minimize the variance of the weights of the local machine. 
814,SP:c7b724c671def2800694fcc2625fa48d98c7cfe6,"adversarial robustness FEATURE-OF federated learning. inner - maximization optimization of AT USED-FOR data heterogeneity. lower bound USED-FOR Federated Learning. federated learning models USED-FOR attacks. non - IID training sets USED-FOR federated learning models. Generic is algorithm. Method is inner - maximization of Adversarial Training. Material is CIFAR-10, SVHN and CIFAR-100datasets. ","This paper studies the adversarial robustness of federated learning. The authors propose an algorithm called inner-maximization of Adversarial Training (AT), which is an extension of the inner-minimization optimization of AT to account for data heterogeneity. The paper provides a lower bound for Federated Learning, which shows that the attacks can be applied to federated models trained on non-IID training sets. Experiments are conducted on CIFAR-10, SVHN and CIFR-100datasets."
815,SP:ff3c787512035e2af20778d53586752852196be9,lifelong machine learning ( LML ) USED-FOR supervised learning settings. MAKO PART-OF supervised LML model. labeled data USED-FOR data programming method. data programming method USED-FOR Labeling new data. partially labeled data USED-FOR SSL LML framework. fully labeled data USED-FOR SSL LML framework. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. image classification data sets EVALUATE-FOR MAKO. MNIST HYPONYM-OF image classification data sets. CIFAR-10 HYPONYM-OF image classification data sets. CIFAR-100 HYPONYM-OF image classification data sets. OtherScientificTerm is knowledge based overhead. Material is unlabeled data. ,"This paper studies the problem of lifelong machine learning (LML) in supervised learning settings. The authors propose MAKO, a supervised LML model where the knowledge based overhead is removed. Labeling new data is done using a data programming method with labeled data and partially labeled data. The SSL LML framework with fully labeled data is trained on unlabeled data. MAKO is evaluated on three image classification data sets: MNIST, CIFAR-10, and the recently proposed CIFR-100."
816,SP:ff3c787512035e2af20778d53586752852196be9,supervised Lifelong Machine Learning ( LML ) frameworks USED-FOR wrapper tool. data programming USED-FOR wrapper tool. semi - supervised learning / data programming USED-FOR LML. automatic label generation USED-FOR LML. semi - supervised learning / data programming USED-FOR automatic label generation. Method is LML wrapper. Generic is method. ,"This paper proposes a wrapper tool for supervised Lifelong Machine Learning (LML) frameworks based on data programming. The main idea is to use automatic label generation for LML with semi-supervised learning/data programming. In addition, the authors propose an LML wrapper that can be used in conjunction with any existing method."
817,SP:ff3c787512035e2af20778d53586752852196be9,data programming method USED-FOR semi - supervised continual learning. Mako HYPONYM-OF data programming method. weak labeling functions USED-FOR Mako. bootstrapping USED-FOR functions. OtherScientificTerm is unlabeled data. Generic is methods. ,"This paper proposes Mako, a data programming method for semi-supervised continual learning based on weak labeling functions. The main idea is to use bootstrapping to learn functions that are robust to unlabeled data. The authors show that the proposed methods outperform existing methods."
818,SP:ff3c787512035e2af20778d53586752852196be9,data programming techniques USED-FOR continual semi - supervised learning. limited labeled data USED-FOR continual semi - supervised learning. probabilistic pseudo labels PART-OF stage - wise pipeline. Snuba based Data Programming framework USED-FOR probabilistic pseudo labels. temperature scaling USED-FOR them. framework COMPARE fully supervised methods. fully supervised methods COMPARE framework. Method is Lifelong Machine Learning ( LML ) tools. ,This paper proposes data programming techniques for continual semi-supervised learning with limited labeled data. The authors propose a Snuba based Data Programming framework that incorporates probabilistic pseudo labels into a stage-wise pipeline. They also propose Lifelong Machine Learning (LML) tools and apply them with temperature scaling. The experimental results show that the proposed framework outperforms fully supervised methods.
819,SP:447df6679b2880def833d4f444bf10e61cdf0e1c,"attack USED-FOR adversarial detection methods. classification CONJUNCTION detection. detection CONJUNCTION classification. classification pipeline CONJUNCTION detection pipeline. detection pipeline CONJUNCTION classification pipeline. classification loss USED-FOR attack. gradient steps USED-FOR classification pipeline. gradients FEATURE-OF detection pipeline. attacks USED-FOR adversarial detection methods. Method is detection methods. Generic is it. OtherScientificTerm are worst - case adversaries, and classification prediction. ","This paper proposes a new attack against adversarial detection methods. The proposed attack is based on the classification loss, and the authors show that it is robust to worst-case adversaries. The authors also show that the gradients of the classification pipeline and the detection pipeline can be used as gradient steps to improve the classification prediction. Finally, the authors demonstrate that the proposed attacks can improve the performance of existing state-of-the-art adversarial Detection methods."
820,SP:447df6679b2880def833d4f444bf10e61cdf0e1c,"finding adversarial examples USED-FOR detector of adversarial examples. Generic is attacks. Method are attack techniques, and detection - based defence methods. ","This paper studies the problem of finding adversarial examples for a detector of adversarial attacks. The authors argue that existing attacks are not robust against detection-based defence methods, and that existing attack techniques are not effective."
821,SP:447df6679b2880def833d4f444bf10e61cdf0e1c,"Task is adversarial examples detection. Method are optimization algorithm, and adversarial example detection methods. ",This paper studies the problem of adversarial examples detection. The authors propose an optimization algorithm that aims to minimize the variance of the output of the adversarial example detection methods. The paper is well-written and easy to follow.
822,SP:447df6679b2880def833d4f444bf10e61cdf0e1c,techniques USED-FOR generating adversarial examples. Selective Projected Gradient Descent ( SPGD ) CONJUNCTION Orthogonal Projected Gradient Descent ( OPGD ). Orthogonal Projected Gradient Descent ( OPGD ) CONJUNCTION Selective Projected Gradient Descent ( SPGD ). Orthogonal Projected Gradient Descent ( OPGD ) HYPONYM-OF techniques. Selective Projected Gradient Descent ( SPGD ) HYPONYM-OF techniques. OPGD USED-FOR gradients. defence methods EVALUATE-FOR attacks. Generic is detector. Method is SPGD. ,"This paper proposes two techniques for generating adversarial examples using two techniques: Selective Projected Gradient Descent (SPGD) and Orthogonal Projected Generated Gradient Discent (OPGD). In SPGD, the gradients are generated using OPGD, and the detector is trained to distinguish between the generated gradients and those generated by SPGD. The authors evaluate these attacks on a variety of defence methods and show that the proposed attacks are effective."
823,SP:5eef907024017849303477eed92f317438c87a69,"cooperative game theory USED-FOR valuation problems. Shapley value CONJUNCTION Banzhaf index. Banzhaf index CONJUNCTION Shapley value. function USED-FOR importance vector. probabilistic treatment USED-FOR problem. decoupling problem USED-FOR importance vector. KL divergence USED-FOR product distribution. null player CONJUNCTION marginalism. marginalism CONJUNCTION null player. marginalism CONJUNCTION symmetric axioms. symmetric axioms CONJUNCTION marginalism. game - theoretic axioms FEATURE-OF importance vector. Shapley value HYPONYM-OF game - theoretic axioms. null player HYPONYM-OF game - theoretic axioms. marginalism HYPONYM-OF game - theoretic axioms. gradients COMPARE approximate sampling method. approximate sampling method COMPARE gradients. data valuation CONJUNCTION feature attribution. feature attribution CONJUNCTION data valuation. feature attribution USED-FOR machine learning. Shapley value CONJUNCTION Banzhaf index. Banzhaf index CONJUNCTION Shapley value. approach COMPARE Shapley value. Shapley value COMPARE approach. approach COMPARE Banzhaf index. Banzhaf index COMPARE approach. tasks EVALUATE-FOR approach. OtherScientificTerm are valuation function, probability distribution, importance value, importance score, and temperature $ T$ term. Task is probabilistic treatment of this problem. Method is coordinate ascent. ","This paper studies cooperative game theory for valuation problems. In particular, the paper considers the problem of learning a valuation function that maximizes a product of the Shapley value and the Banzhaf index. The paper considers a probabilistic treatment of this problem, where the probability distribution of the product distribution $T$ is given by a function $P(T)$, where $P$ is the importance value, and $T$. The paper shows that this function can be used to learn an importance vector that satisfies a number of game-theoretic axioms, including null player, marginalism, and symmetric axiom. The main contribution of this paper is to propose a new problem that can be solved by a simple probabilistically treatment. The importance vector can be learned by solving a decoupling problem. The KL divergence between the value of $P$, the product of $T$, and the importance score is used to estimate the product probability of the two distributions. This is done by performing coordinate ascent, where $p(T|T)$ is a function of the temperature $\tilde{T}$ and $t$ is an expectation of the KL divergence. The authors show that the gradients of their approach are better than the approximate sampling method. They also show that their approach is competitive with Shapleyvalue and the original Shapley index. Finally, they compare their approach to the original approach on two tasks: data valuation and feature attribution in machine learning."
824,SP:5eef907024017849303477eed92f317438c87a69,"Valuation criteria USED-FOR analyzing feature importance. Valuation criteria USED-FOR data subset selection. analyzing feature importance CONJUNCTION data subset selection. data subset selection CONJUNCTION analyzing feature importance. game - theory USED-FOR Valuation criteria. Shapely value HYPONYM-OF game - theory. Shapely value HYPONYM-OF Valuation criteria. ML USED-FOR subset valuation problems. maximum entropy solution USED-FOR game. one - step factored approximation USED-FOR maximum entropy solution. one - step factored approximation USED-FOR valuation criteria. multi - step factored approximation USED-FOR Variational Index. OtherScientificTerm is feature importance. Generic are criteria, it, and criterion. Method are cooperative games, and probabilistic treatment of cooperative games. ","This paper proposes a Valuation criteria for analyzing feature importance and data subset selection based on game-theoretical analysis of Shapely value, which is a well-known criterion for evaluating feature importance in cooperative games. This criteria is well-motivated as it is easy to compute and easy to apply to ML for subset valuation problems. The authors propose a one-step factored approximation to the maximum entropy solution of the game, which they call Variational Index, based on a multi-step factorored approximation. The paper also provides a probabilistic treatment of cooperative games, showing that the proposed criterion converges to the optimal solution."
825,SP:5eef907024017849303477eed92f317438c87a69,valuation problems USED-FOR cooperative games. Variational Index HYPONYM-OF valuation measure. maximum entropy criterion USED-FOR coalition probability distribution. decoupled surrogates USED-FOR distribution. decoupled surrogates USED-FOR Player valuations. gradient ascent algorithm USED-FOR decoupling. Shapley value CONJUNCTION Banzhaf index. Banzhaf index CONJUNCTION Shapley value. Banzhaf index HYPONYM-OF Classical valuation criteria. Shapley value HYPONYM-OF Classical valuation criteria. Generic is algorithms. ,"This paper studies valuation problems in cooperative games. The authors propose a new valuation measure called Variational Index, which is based on the maximum entropy criterion for the coalition probability distribution. Player valuations are obtained by using decoupled surrogates for the distribution, which can be computed using the gradient ascent algorithm. Classical valuation criteria such as Shapley value and Banzhaf index are used to evaluate the proposed algorithms."
826,SP:5eef907024017849303477eed92f317438c87a69,"energy - based perspective USED-FOR cooperative games. entropy maximizing distribution USED-FOR cooperative game. Lagrangian USED-FOR entropy maximization problem. distribution USED-FOR probability mass. gradient descent USED-FOR update rule. gradient descent USED-FOR objective. single - step update USED-FOR KL divergence minimization problem. single - step update USED-FOR Banzhaf value. single - step update USED-FOR Shapley value. sampling - based approach USED-FOR gradients. single - step update USED-FOR symmetric initialization. solution concepts PART-OF cooperative game theory. probabilistic value FEATURE-OF single - step update. Shapley / Banzhaf values HYPONYM-OF solution concepts. Shapley and Banzhaf values USED-FOR feature removal tasks. variational index USED-FOR feature removal tasks. variational index COMPARE Shapley and Banzhaf values. Shapley and Banzhaf values COMPARE variational index. OtherScientificTerm are mean coalition value, Boltzmann distribution, and high - value coalitions. Method are mean - field variational inference, VI approach, and single - step updates. Generic is it. ","This paper studies cooperative games from an energy-based perspective. The authors consider an entropy maximizing distribution for a cooperative game, where the entropy maximization problem is formulated as the Lagrangian of the mean coalition value. This distribution is then used to estimate the probability mass of the game, which is used as an update rule using gradient descent to approximate the update rule. This paper is motivated by two solution concepts in cooperative game theory, namely Shapley/Banzhaf values, and proposes a VI approach to approximate these values. First, a single-step update to the Shapley value is used to compute the Banzhaf value. Second, the authors propose a sampling-based approach to estimate gradients, where a mean-field variational inference is used, and a Boltzmann distribution is used for estimating the value of each sample. Finally, the proposed VI approach is applied to the KL divergence minimization problem, and it is shown that a single step update with respect to the probabilistic value can be used to approximate a symmetric initialization. The experiments show that the proposed variational index outperforms Shapley and Banzoaf values on feature removal tasks with high-value coalitions."
827,SP:1257373629c8584c001b69677ebd73e5f0c20d08,out - of - sample prediction error USED-FOR epistemic uncertainty. definition USED-FOR estimator. direct epistemic uncertainty prediction HYPONYM-OF estimator. error predictor USED-FOR generalization error. sequential model optimization CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION sequential model optimization. estimation USED-FOR downstream tasks. estimator USED-FOR downstream tasks. estimator USED-FOR estimation. reinforcement learning HYPONYM-OF downstream tasks. sequential model optimization HYPONYM-OF downstream tasks. Generic is task. ,"This paper proposes a new definition of epistemic uncertainty based on out-of-sample prediction error. The authors propose an estimator called direct epistemic certainty prediction, which is based on the definition of the error predictor for generalization error. This estimator can be used for estimation in downstream tasks such as sequential model optimization and reinforcement learning. The paper also provides a theoretical analysis of this task."
828,SP:1257373629c8584c001b69677ebd73e5f0c20d08,"secondary function USED-FOR generalisation risk. secondary function USED-FOR first. generalisation risk FEATURE-OF first. predictive variance FEATURE-OF primary model. generative model USED-FOR density estimates. active learning USED-FOR drug discovery. OOD rejection CONJUNCTION active learning. active learning CONJUNCTION OOD rejection. image classification CONJUNCTION active learning. active learning CONJUNCTION image classification. active learning CONJUNCTION function optimisation. function optimisation CONJUNCTION active learning. OOD rejection USED-FOR image classification. OOD rejection HYPONYM-OF experiments. function optimisation HYPONYM-OF experiments. active learning HYPONYM-OF experiments. Task are supervised task, and reinforcement learning. Method is secondary model. OtherScientificTerm are primary model error, k - fold split, and model selection bias. Generic are method, and it. ","This paper proposes a new supervised task, where the primary model is trained to maximise the predictive variance of the secondary model, and the secondary function is used to minimise the generalisation risk of the first. The main idea is to use a generative model to generate density estimates, and then use a secondary model to reduce primary model error. The method is based on the idea of k-fold split, and it is shown to be robust to model selection bias in reinforcement learning. Experiments are conducted on image classification, active learning for drug discovery, and function optimisation."
829,SP:1257373629c8584c001b69677ebd73e5f0c20d08,approach USED-FOR epistemic uncertainty. model USED-FOR epistemic uncertainty. generalization error CONJUNCTION aleatory uncertainty. aleatory uncertainty CONJUNCTION generalization error. model COMPARE model. model COMPARE model. hold - out dataset USED-FOR error predictor. hold - out dataset USED-FOR DEUP. data density estimates CONJUNCTION model variance. model variance CONJUNCTION data density estimates. RL CONJUNCTION active learning. active learning CONJUNCTION RL. DEUP USED-FOR cross - validation setting. features USED-FOR error predictor. model variance PART-OF features. data density estimates PART-OF features. RL HYPONYM-OF interactive settings. active learning HYPONYM-OF interactive settings. sequential model optimization CONJUNCTION RL. RL CONJUNCTION sequential model optimization. OOD data CONJUNCTION sequential model optimization. sequential model optimization CONJUNCTION OOD data. sequential model optimization EVALUATE-FOR DEUP. settings EVALUATE-FOR DEUP. OOD data EVALUATE-FOR DEUP. OOD data HYPONYM-OF settings. RL HYPONYM-OF settings. sequential model optimization HYPONYM-OF settings. Material is hold - out set. ,"This paper proposes a novel approach, DEUP, to model epistemic uncertainty, i.e., the difference between generalization error and aleatory uncertainty between a model trained on a hold-out dataset and a model that is trained on the full dataset. The key idea is to train DEUP on the hold-outs dataset, where the error predictor is trained using a set of features that include data density estimates, model variance, etc. The authors show that DEUP can be used in a cross-validation setting where the model is trained to be similar to the original model, but with a different label. DEUP is evaluated in three settings: OOD data, sequential model optimization, and RL and active learning. The results show that the proposed DEUP outperforms the existing methods in all three settings. The paper also provides an ablation study of the performance of DEUP in the cross validation setting, which shows that the model can generalize better to a larger number of data points than the original one. Finally, the authors also provide a theoretical analysis of the effect of training on the generalization errors of the model on the aleatory uncertainties of the original data points, which is also shown in the paper. The main contribution of the paper is the introduction of a new class of features, which combines the features of the data densities estimates and the model variance to form a new error predictor. This new feature is then used to train a new model on top of the existing data set. The experiments are conducted on a variety of settings, including OOD, sequential, RL, active learning, and continuous learning."
830,SP:1257373629c8584c001b69677ebd73e5f0c20d08,"method USED-FOR epistemic uncertainty. aleatoric error estimate PART-OF total generalization error estimate. method USED-FOR model misspecification. model misspecification FEATURE-OF process. neural network model USED-FOR total generalization error. static ( fixed data set CONJUNCTION interactive ( active learning ) settings. interactive ( active learning ) settings CONJUNCTION static ( fixed data set. technique USED-FOR interactive ( active learning ) settings. mean squared error loss function EVALUATE-FOR technique. static ( fixed data set EVALUATE-FOR technique. OtherScientificTerm are generalization error, aleatoric error, irreducible error, epistemic uncertainty estimate, and variance of the posterior distribution. Task is Estimating aleatoric error. ","This paper proposes a method to estimate epistemic uncertainty. The method is motivated by model misspecification in the training process. The aleatoric error estimate is added to the total generalization error estimate of a neural network model. The authors show empirically that the difference between the mean squared error loss function of the proposed technique in both static (fixed data set) and interactive (active learning) settings is proportional to the difference in the generalizability error between the original and the perturbed version of the data. The paper also shows that the variance of the posterior distribution can be used as a measure of the difference of the original error and the aleator error, and that the irreducible error can also be used to estimate the difference among the two versions of the same data. Estimating aleatoricity error is computationally expensive, which is why the authors propose to use the epistemic error estimate instead."
831,SP:fd3c33c9237d0f1e33d896858a46c48da2216fe3,block coordinate descent algorithm USED-FOR rotation learning. SO(n ) FEATURE-OF rotation matrix. rotation matrices PART-OF rotation matrix. greedy strategy CONJUNCTION steepest strategy. steepest strategy CONJUNCTION greedy strategy. random strategy CONJUNCTION greedy strategy. greedy strategy CONJUNCTION random strategy. greedy strategy HYPONYM-OF coordinate. random strategy HYPONYM-OF coordinate. steepest strategy HYPONYM-OF coordinate. rotations matrices USED-FOR it. Generic is algorithm. OtherScientificTerm is projection. ,"This paper proposes a block coordinate descent algorithm for rotation learning. The algorithm is based on the observation that the SO(n) of a rotation matrix can be decomposed into a set of rotation matrices. The authors propose to use a random strategy, a greedy strategy, and a steepest strategy for each coordinate. They also show that the projection of a matrix to a fixed point can be computed using rotations matrices and show that it converges to the fixed point."
832,SP:fd3c33c9237d0f1e33d896858a46c48da2216fe3,"rotation learning methods USED-FOR fixed embeddings. rotation learning methods USED-FOR quantization distortion. quantization distortion FEATURE-OF fixed embeddings. block Givens coordinate descent algorithms USED-FOR rotation matrices. convex objectives FEATURE-OF rotation matrices. Lie group theory USED-FOR geometric intuitions. geometric intuitions USED-FOR block Givens coordinate descent algorithms. algorithms COMPARE SVD method. SVD method COMPARE algorithms. runtime EVALUATE-FOR algorithms. landscape of learning rotation matrix USED-FOR approximate nearest neighbor ( ANN ) search. SVD based CONJUNCTION iterative Givens rotation - based. iterative Givens rotation - based CONJUNCTION SVD based. algorithm COMPARE rotation matrix learning algorithms. rotation matrix learning algorithms COMPARE algorithm. algorithm USED-FOR fixed embedding. algorithm USED-FOR rotation matrix. algorithm USED-FOR end - to - end training. Task are end - to - end training scenario, and end - to - end neural network training. OtherScientificTerm are embeddings, decomposition of orthogonal group, and GPUs. Method are retrieval models, and Givens coordinate block descent algorithm. ","This paper studies the end-to-end training scenario, where the embeddings are not available to the retrieval models and the goal is to reduce the quantization distortion of existing rotation learning methods for fixed embedding. The paper proposes block Givens coordinate descent algorithms for learning rotation matrices with convex objectives based on geometric intuitions from Lie group theory. The proposed algorithms are compared with the SVD method and the iterative GivENS rotation-based, and the runtime of the proposed algorithms is shown to be comparable to that of SVD based. The landscape of learning rotation matrix is also studied for approximate nearest neighbor (ANN) search. The algorithm is compared to other rotation matrix learning algorithms and compared to a decomposition of orthogonal group. The results show that the proposed algorithm is able to learn the rotation matrix efficiently and can be used to learn a more accurate and efficient solution for end-towards-end neural network training on GPUs. The authors also provide a theoretical analysis of the convergence properties of the learned rotation matrix. Finally, the authors provide an empirical study of the performance of the Gevens coordinate block descent algorithm."
833,SP:fd3c33c9237d0f1e33d896858a46c48da2216fe3,"Givens coordinate descent algorithms USED-FOR minimizing the quantization distortion. Givens coordinate descent algorithms USED-FOR rotation matrix. OtherScientificTerm are quantization distortion, special orthogonal group, and convex objectives. Generic is algorithms. ","This paper studies Givens coordinate descent algorithms for minimizing the quantization distortion of the rotation matrix. The main idea is to use a special orthogonal group, which is a special case of convex objectives. The authors provide a theoretical analysis of the algorithms and provide some empirical results."
834,SP:fd3c33c9237d0f1e33d896858a46c48da2216fe3,"rotation matrix learning USED-FOR ANN embedding search systems. rotation matrix learning USED-FOR product quantization. gradient descent of small rotation updates USED-FOR rotation matrix learning. OPQ CONJUNCTION Cayley. Cayley CONJUNCTION OPQ. Generic is approach. OtherScientificTerm are small rotation matrix, partial derivatives, product, rotation matrix, and O(n^2 ) matrix multiplications. Method is coordinate descent. ","This paper studies the problem of rotation matrix learning in ANN embedding search systems. The proposed approach is based on gradient descent of small rotation updates, where the small rotation matrix is used to estimate the product of the partial derivatives of the original matrix and the new rotation matrix. The main contribution of this paper is to show that the proposed gradient descent can be used for product quantization. Theoretical analysis is provided, and experiments are conducted on OPQ and Cayley, showing that O(n^2) matrix multiplications can be achieved with coordinate descent."
835,SP:6ec2c8456ab95f7d028d00b591dab3eadc549eb8,abstract reasoning USED-FOR learning visual analogies. vision relationship recognition CONJUNCTION concept inference. concept inference CONJUNCTION vision relationship recognition. vision relationship recognition PART-OF problem. concept inference HYPONYM-OF problem. model COMPARE models. models COMPARE model. OtherScientificTerm is systematic generalization tests. ,This paper addresses the problem of abstract reasoning for learning visual analogies. The problem is divided into two parts: vision relationship recognition and concept inference. The authors conduct systematic generalization tests and show that the proposed model outperforms existing models.
836,SP:6ec2c8456ab95f7d028d00b591dab3eadc549eb8,model USED-FOR abstract visual analogy making. Neural Structure Mapping ( NSM ) USED-FOR abstract visual analogy making. visual relationship encoder CONJUNCTION analogy inference engine. analogy inference engine CONJUNCTION visual relationship encoder. analogy inference engine PART-OF NSM model. visual relationship encoder PART-OF NSM model. visual relationship encoder USED-FOR visual domain elements. neural modular architecture USED-FOR model layout. analogy inference engine HYPONYM-OF neural modular architecture. relation HYPONYM-OF visual domain elements. object HYPONYM-OF visual domain elements. NSM COMPARE baselines. baselines COMPARE NSM. Generic is dataset. ,"This paper proposes a model called Neural Structure Mapping (NSM) for abstract visual analogy making. The proposed NSM model consists of a visual relationship encoder and an analogy inference engine. The model layout is based on a neural modular architecture. The visual domain elements are the relation between an object and a relation between a pair of objects. Experiments are conducted on a dataset of 5,000 objects and show that NSM outperforms the baselines."
837,SP:6ec2c8456ab95f7d028d00b591dab3eadc549eb8,"framework USED-FOR Raven Progressive Matrices ( RPM ) task. Raven Progressive Matrices ( RPM ) task HYPONYM-OF analogy task. Hill 2019 COMPARE NSM. NSM COMPARE Hill 2019. semantically - contrasting alternative candidates FEATURE-OF model. Task are analogical reasoning, RPM task, and systematic generalization. Method are Neural Structure Mapping ( NPM ) system, Visual Relationship Encoder, encoder, Analogy Inference Engine, and NSM system. Generic are engine, and network. ","This paper proposes a framework for solving the Raven Progressive Matrices (RPM) task, an analogy task with the goal of improving analogical reasoning. The authors propose a Neural Structure Mapping (NPM) system, which consists of a Visual Relationship Encoder and an Analogy Inference Engine. The encoder predicts the relationship between the input and the output of the NSM system, while the engine predicts the relation between the encoder and the outputs of the network. The model is trained with semantically-contrasting alternative candidates, and is evaluated on the RPM task. The results show that Hill 2019 outperforms NSM in terms of systematic generalization."
838,SP:6ec2c8456ab95f7d028d00b591dab3eadc549eb8,"architecture USED-FOR visual analogies. Gentner ’s Structure Mapping Theory USED-FOR analogies. Gentner ’s Structure Mapping Theory USED-FOR architecture. Raven ’s Progressive Matrices dataset EVALUATE-FOR it. network USED-FOR architectures. architecture USED-FOR network. test accuracy COMPARE baseline models. baseline models COMPARE test accuracy. RPM dataset EVALUATE-FOR architecture. architectural structure USED-FOR model. Method are Gentner ’s theory, and neural network model architecture. OtherScientificTerm are relational structure, relationship head, and visual analogy. ","This paper proposes an architecture based on Gentner’s Structure Mapping Theory to learn visual analogies using a neural network model architecture. The architecture is based on the notion of relational structure, where the relationship head is a representation of the visual analogy. The authors evaluate the architecture on the RPM dataset and show that it is able to achieve better test accuracy compared to baseline models. They also show that the architecture can be used to train a network that can generalize to other architectures. "
839,SP:0e8c3a3dba649d496292b41228801feb8507d3b4,semi - supervised learning methods USED-FOR general - purpose representations. unlabeled biological sequences USED-FOR general - purpose representations. unlabeled biological sequences USED-FOR semi - supervised learning methods. Self - GenomeNet HYPONYM-OF contrastive learning method. contrastive learning method USED-FOR nucleotides. reverse - complement ( RC ) context prediction USED-FOR contrastive learning method. model USED-FOR representations. representations COMPARE random nucleotide sequences. random nucleotide sequences COMPARE representations. method COMPARE self - supervised baseline models. self - supervised baseline models COMPARE method. benchmark datasets EVALUATE-FOR self - supervised baseline models. benchmark datasets EVALUATE-FOR method. OtherScientificTerm is RC. ,"This paper presents Self-GenomeNet, a contrastive learning method for nucleotides based on reverse-complement (RC) context prediction. The authors claim that this model is able to learn better representations than random nucleotide sequences. The proposed method is evaluated on three benchmark datasets and compared with two self-supervised baseline models. The results show that the proposed method outperforms the two baseline models in terms of RC performance."
840,SP:0e8c3a3dba649d496292b41228801feb8507d3b4,"self - supervised learning approach USED-FOR representation learning of genomic sequences. contrastive loss USED-FOR representation learning of genomic sequences. contrastive loss USED-FOR self - supervised learning approach. contrastive loss USED-FOR self - supervised learning. self - supervised learning USED-FOR NLP and computer vision domains. method USED-FOR genomics tasks. reverse complement information of the sequences USED-FOR it. prediction tasks CONJUNCTION transfer learning task. transfer learning task CONJUNCTION prediction tasks. method USED-FOR prediction tasks. method USED-FOR transfer learning task. CPC CONJUNCTION Contrastive - sc. Contrastive - sc CONJUNCTION CPC. generative language model CONJUNCTION self - supervised learning models. self - supervised learning models CONJUNCTION generative language model. supervised model CONJUNCTION generative language model. generative language model CONJUNCTION supervised model. self - supervised learning models CONJUNCTION Contrastive - sc. Contrastive - sc CONJUNCTION self - supervised learning models. CPC HYPONYM-OF self - supervised learning models. classification EVALUATE-FOR baseline. Task is genomics. Method are Self - supervised contrastive learning, and Self - GenomeNet. OtherScientificTerm are revers compliment, variable sequences, and semi - supervised training settings. ","This paper proposes a self-supervised learning approach based on a contrastive loss for representation learning of genomic sequences. Self-supervision contrastive learning is a well-known technique in genomics, and it leverages the reverse complement information of the sequences to improve the performance. The authors apply the proposed method to a variety of genomics tasks, including prediction tasks and a transfer learning task. The experiments are conducted on a supervised model, a generative language model, three self supervised learning models (CPC, Contrastive-sc, Self-GenomeNet) and show improvements over the baseline in terms of classification performance. However, the authors also show that the revers compliment is not transferable to variable sequences, which is a limitation of the semi-unsupervised training settings."
841,SP:0e8c3a3dba649d496292b41228801feb8507d3b4,self - genomenet HYPONYM-OF self - supervised training method. contrastive loss USED-FOR random sequences. expected reverse - complement invariance FEATURE-OF prediction function. network USED-FOR expected reverse - complement invariance. learning tasks EVALUATE-FOR it. it EVALUATE-FOR method. learning tasks EVALUATE-FOR method. Material is DNA sequences. Method is self - supervision. OtherScientificTerm is reverse - complemented ( RC ). ,"This paper proposes self-genomenet, a self-supervised training method for DNA sequences. The key idea is to use a contrastive loss for random sequences, and then to use the network to enforce the expected reverse-complement invariance of the prediction function. The method is evaluated on a variety of learning tasks, and it is shown to outperform the state-of-the-art. The main contribution of this paper is the use of self supervision, which is an important step in the development of reverse-completemented (RC)."
842,SP:0e8c3a3dba649d496292b41228801feb8507d3b4,self - supervised learning method USED-FOR nucleotide - level genomic data. reverse - complement of genomic sequences USED-FOR self - supervised learning method. architecture USED-FOR varying - length genome sequences. Self - GenomeNet USED-FOR varying - length genome sequences. Self - GenomeNet HYPONYM-OF architecture. Generic is method. ,"This paper proposes a self-supervised learning method for nucleotide-level genomic data based on the reverse-complement of genomic sequences. The proposed method, Self-GenomeNet, is a novel architecture for varying-length genome sequences."
843,SP:2af5c866ed17f156b406153d3261baaa42cf95fb,"hyperspherical output layer USED-FOR neurons. human pose dataset EVALUATE-FOR algorithm. OtherScientificTerm are geometric neurons, arbitrary rotations, frozen weights, and sanity check. Metric is accuracy. Method is steerable neuron. ","This paper proposes to train geometric neurons that are steerable to arbitrary rotations. The neurons are trained with a hyperspherical output layer, and frozen weights are used as a sanity check. The proposed algorithm is evaluated on a human pose dataset, and the accuracy is shown to be better than the state-of-the-art steerable neuron."
844,SP:2af5c866ed17f156b406153d3261baaa42cf95fb,"method USED-FOR constructing steerable spherical neurons. Geometric Neurons USED-FOR method. steerability constraint FEATURE-OF geometric neuron. constraint USED-FOR tasks. steerable spherical neutrons USED-FOR classification of 3D Tetris objects. rotations FEATURE-OF classification of 3D Tetris objects. representation USED-FOR version. 3D rotations FEATURE-OF equivariance. Method are steerable spherical neurons, steerable model, and steerable versions. Material is 3D skeleton data. OtherScientificTerm is perturbations. ","This paper proposes a method based on Geometric Neurons for constructing steerable spherical neurons. The steerability constraint of a geometric neuron is a well-known problem in many tasks, and this paper proposes to use this constraint to solve the classification of 3D Tetris objects with rotations. The authors show that steerable spheres can be trained on 3D skeleton data, and demonstrate that the equivariance to 3D rotations can be achieved by training a steerable model. They also show that the representation of their version can be improved by adding perturbations to the original steerable versions."
845,SP:2af5c866ed17f156b406153d3261baaa42cf95fb,"3D "" spherical neurons USED-FOR rotationally equivariant layers. conformal space USED-FOR operations. conformal space USED-FOR spherical and geometric neurons. steerability constraint FEATURE-OF neuron. Generic is approach. Material is rotated 3D data. ","This paper proposes to use ""3D"" spherical neurons to learn rotationally equivariant layers. The proposed approach is based on the observation that spherical and geometric neurons can be represented in a conformal space, which allows for efficient operations. The main contribution of this paper is to introduce a steerability constraint on the neuron, which is shown to be useful in the case of rotated 3D data."
846,SP:2af5c866ed17f156b406153d3261baaa42cf95fb,"spherical decision boundary USED-FOR 3D point classifiers. steerability constraint USED-FOR test - time optimization. test - time optimization USED-FOR pre - trained classifier. method USED-FOR unknown rotations. method USED-FOR rotation - invariant predictions. OtherScientificTerm are 3D rotation perturbations, and input rotation perturbations. Material is small scale datasets. ",This paper proposes a spherical decision boundary for training 3D point classifiers with 3D rotation perturbations. The steerability constraint is used for efficient test-time optimization of the pre-trained classifier. The proposed method is able to handle unknown rotations and provide rotation-invariant predictions. Experiments are conducted on small scale datasets and demonstrate the effectiveness of the proposed method.
847,SP:14330a1a1c33ec18de096ffb038ba06f04c7dccb,continual learning methods USED-FOR catastrophic forgetting. BERT CONJUNCTION RoBERTa. RoBERTa CONJUNCTION BERT. continual learning methods USED-FOR text classification tasks. pretrained language models USED-FOR continual learning methods. pretrained language models USED-FOR text classification tasks. BERT HYPONYM-OF pretrained language models. RoBERTa HYPONYM-OF pretrained language models. regularization - based CONJUNCTION dynamic architecture. dynamic architecture CONJUNCTION regularization - based. rehearsal - based CONJUNCTION regularization - based. regularization - based CONJUNCTION rehearsal - based. probing techniques USED-FOR rehearsal - based method. Method is rehearsal based methods. ,"This paper studies the problem of catastrophic forgetting in continual learning methods for text classification tasks with pretrained language models such as BERT and RoBERTa. The authors propose three rehearsal based methods: rehearsal-based, regularization-based and dynamic architecture. The rehearsal based method is based on probing techniques."
848,SP:14330a1a1c33ec18de096ffb038ba06f04c7dccb,catastrophic forgetting FEATURE-OF pretrained language models. catastrophic forgetting issue EVALUATE-FOR pre - trained models. continual learning settings EVALUATE-FOR pre - trained models. data sets EVALUATE-FOR pre - trained models. continual learning methods USED-FOR pre - trained models. ,This paper studies the catastrophic forgetting issue of pretrained language models. The authors evaluate pre-trained models under continual learning settings on three data sets and show that pre-training with continual learning methods can lead to better performance.
849,SP:14330a1a1c33ec18de096ffb038ba06f04c7dccb,pretrained language models USED-FOR continual learning setting. pretrained language models COMPARE continual learning strategies. continual learning strategies COMPARE pretrained language models. models CONJUNCTION CL approaches. CL approaches CONJUNCTION models. transformer layers USED-FOR CL approaches. Metric is everything - by - everything evaluation. Method is language models. ,"This paper studies the performance of pretrained language models in the continual learning setting. The authors compare the performance on everything-by-everything evaluation, and show that the performance is comparable to the state-of-the-art in terms of generalization ability of language models. They also compare pretrained models with CL approaches based on transformer layers."
850,SP:14330a1a1c33ec18de096ffb038ba06f04c7dccb,PLMs USED-FOR NLP classification tasks. continual learning CONJUNCTION NLP. NLP CONJUNCTION continual learning. common learning settings USED-FOR NLP. common learning settings USED-FOR continual learning. common learning settings FEATURE-OF NLP end - tasks. NLP end - tasks EVALUATE-FOR methods. layer - wise performance analysis USED-FOR layers. replay COMPARE methods. methods COMPARE replay. regularization HYPONYM-OF methods. ,"This paper studies the performance of PLMs for NLP classification tasks. The authors compare the performance on several NLP end-tasks in common learning settings for continual learning and NLP. The methods are evaluated on a variety of common learning and non-common learning settings on a range of NLP and continual learning tasks. In particular, the authors perform layer-wise performance analysis for different layers. The results show that replay performs better than other methods such as regularization."
851,SP:adb11a3bd1af2b68720f8f1b48639e31f65295fd,"federated learning settings FEATURE-OF adversarial attacks. defensive technique USED-FOR byzantine generals ” problem. byzantine generals ” problem PART-OF federated learning. technique COMPARE defensive methods. defensive methods COMPARE technique. adaptive adversaries USED-FOR evaluation. Method are ML model, and threat model. OtherScientificTerm is Adaptive adversary. Task is Feature space attacks. ",This paper studies adversarial attacks in federated learning settings. The authors propose a new defensive technique to tackle the “byzantine generals” problem in the context of “federated learning”. The main idea is to train an ML model to predict the next state of the art threat model. The evaluation is based on adaptive adversaries. Adaptive adversary is defined as an adversary that is able to adapt to the current state-of-the-art threat model in a way that the current threat model cannot adapt to it. Feature space attacks are also considered. The proposed technique is compared to other existing defensive methods.
852,SP:adb11a3bd1af2b68720f8f1b48639e31f65295fd,TESSERACT HYPONYM-OF aggregation scheme. aggregation scheme USED-FOR directed deviation attack. ,"This paper proposes TESSERACT, a new aggregation scheme for directed deviation attack. The paper is well-written and easy to follow."
853,SP:adb11a3bd1af2b68720f8f1b48639e31f65295fd,"Gradient Flip Score USED-FOR Federated Learning. Tesseract USED-FOR Federated Learning. data poisoning PART-OF federated learning. Tesseract CONJUNCTION Gradient Flip Score. Gradient Flip Score CONJUNCTION Tesseract. Gradient Flip Score USED-FOR Federated Learning. attack USED-FOR model availability. Task is Model Poisoning Attacks. Generic is defense. OtherScientificTerm are global model gradient, and gradient direction. ","This paper studies the problem of data poisoning in federated learning and proposes a defense based on the combination of Tesseract and Gradient Flip Score for Federated Learning. Model Poisoning Attacks are a well-studied problem in the literature, and this paper proposes a new attack that aims to improve model availability by changing the gradient direction of the global model gradient. "
854,SP:adb11a3bd1af2b68720f8f1b48639e31f65295fd,"TESSERACT HYPONYM-OF aggregation algorithm. aggregation algorithm USED-FOR reputation scores. TESSERACT USED-FOR attack. robustness EVALUATE-FOR TESSERACT. Method is federated learning. Task are untargeted model poisoning attacks, and poisoning attack. Generic is algorithm. ","This paper studies the problem of federated learning and proposes TESSERACT, an aggregation algorithm that aggregates the reputation scores of all the clients. The authors claim that TESERACT improves the robustness against untargeted model poisoning attacks, and propose a poisoning attack based on the proposed algorithm. Experiments are conducted to verify the effectiveness of the proposed attack."
855,SP:2047e943d2337d4fc6b0a269f43c7dfbd8ed9141,one CONJUNCTION one. one CONJUNCTION one. neural networks USED-FOR one. neural networks USED-FOR estimators. one HYPONYM-OF estimators. one HYPONYM-OF estimators. random forests USED-FOR one. estimators USED-FOR general functionals. Riesz representer USED-FOR regression function. OtherScientificTerm is unknown regression function. Method is multi - task architecture. Metric is accuracies. Task is semi - synthetic tasks. ,"This paper proposes two estimators for general functionals: one based on random forests, and one that is based on neural networks. The main idea is to use an unknown regression function as a Riesz representer for the regression function, and then use a multi-task architecture to estimate the parameters of the function. Experiments are conducted on semi-synthetic tasks, and the accuracies are reported."
856,SP:2047e943d2337d4fc6b0a269f43c7dfbd8ed9141,unknown regression function USED-FOR causal inference. unknown regression function USED-FOR moment function. Riesz representation theorem USED-FOR loss function. Riesz representer USED-FOR loss. Riesz representer CONJUNCTION regression loss. regression loss CONJUNCTION Riesz representer. regression function CONJUNCTION Riesz function. Riesz function CONJUNCTION regression function. Neural Network method ( RieszNet ) CONJUNCTION random forest method ( ForestRiesz ). random forest method ( ForestRiesz ) CONJUNCTION Neural Network method ( RieszNet ). Riesz representer PART-OF loss function. regression loss PART-OF loss function. multi - tasking Neural Network method HYPONYM-OF random forest method ( ForestRiesz ). average treatment effect CONJUNCTION average marginal effects. average marginal effects CONJUNCTION average treatment effect. RieszNet COMPARE state - of - the - art methods. state - of - the - art methods COMPARE RieszNet. Method is random forest method. ,"This paper studies the problem of causal inference with an unknown regression function for the moment function. The authors propose a new loss function based on the Riesz representation theorem. The proposed loss function consists of the original loss, the Risesz representer, the regression loss, and the regression function. Experiments are conducted on the Neural Network method (RieszNet) and the random forest method (ForestRIESz, multi-tasking Neural Network Method). Results show that the proposed RiesZNet outperforms the state-of-the-art methods in terms of average treatment effect and average marginal effects."
857,SP:2047e943d2337d4fc6b0a269f43c7dfbd8ed9141,"econometrics CONJUNCTION machine learning. machine learning CONJUNCTION econometrics. Riesz representation USED-FOR machine learning. Riesz representation USED-FOR econometrics. Riesz representation USED-FOR methods. Generic are applications, and algorithms. ",This paper studies the Riesz representation in econometrics and machine learning. The authors show that methods based on the Risesz representation can be applied to a wide range of applications. They also show that the algorithms can generalize to unseen problems.
858,SP:2047e943d2337d4fc6b0a269f43c7dfbd8ed9141,"unconfoundedness FEATURE-OF average treatment effect ( ATE ). one CONJUNCTION one. one CONJUNCTION one. one HYPONYM-OF procedures. one HYPONYM-OF procedures. random forests USED-FOR one. deep learning USED-FOR one. hand - tailored constructions USED-FOR ATE. Riesz representers USED-FOR semi- and nonparametric statistics. semi- and nonparametric statistics USED-FOR constructions. RieszNet HYPONYM-OF deep learning. RieszNet USED-FOR method. deep learning USED-FOR method. RieszNet COMPARE DragonNet. DragonNet COMPARE RieszNet. DragonNet USED-FOR propensities. RieszNet USED-FOR inverse propensities. DragonNet CONJUNCTION RieszNet. RieszNet CONJUNCTION DragonNet. RieszNet COMPARE DragonNet. DragonNet COMPARE RieszNet. RieszNet COMPARE RieszNet. RieszNet COMPARE RieszNet. DragonNet COMPARE RieszNet. RieszNet COMPARE DragonNet. DragonNet USED-FOR ATE. RieszNet USED-FOR ATE. Task is nonparametric estimation. Method are Riesz representer, and DragonNet procedure. ","This paper studies the problem of unconfoundedness of the average treatment effect (ATE) in the context of nonparametric estimation. The authors propose two procedures: one based on deep learning (RieszNet) and one that is based on random forests (DragonNet). The authors show that hand-tailored constructions of ATE can be obtained using semi- and nonparameterized statistics derived from Riesz representers. They also show that the proposed method can be used in conjunction with deep learning. Finally, the authors compare the proposed DragonNet procedure with RiesZNet and show that RiesxNet is able to estimate inverse propensities better than DragonNet."
859,SP:96e1da163020441f9724985ae15674233e0cfe0d,"finite - sample complexity EVALUATE-FOR single - agent actor - critic algorithms. Method is actor - critic algorithm. OtherScientificTerm are average reward, and rewards. ","This paper studies the finite-sample complexity of single-agent actor-critic algorithms. The main result of the paper is to show that the average reward of a given actor can be bounded by a function of the number of agents. The paper also provides a theoretical analysis of the finite sample complexity of a single actor - critic algorithm. Finally, the paper provides some numerical experiments to show the behavior of the rewards."
860,SP:96e1da163020441f9724985ae15674233e0cfe0d,mini - batch TD sharing USED-FOR actor updates. finite - sample bound USED-FOR algorithm. convergence FEATURE-OF finite - sample bound. linear value function approximation USED-FOR stationary points. Task is average reward MDPs. Method is decentralized actor - critic methods. OtherScientificTerm is joint actions. ,"This paper studies the problem of mini-batch TD sharing for actor updates in average reward MDPs, which is an important problem for decentralized actor-critic methods. The authors provide a finite-sample bound on the convergence of the proposed algorithm. They also provide a linear value function approximation to stationary points, which can be used for joint actions."
861,SP:96e1da163020441f9724985ae15674233e0cfe0d,"convergence rate CONJUNCTION sample complexity. sample complexity CONJUNCTION convergence rate. sample complexity EVALUATE-FOR algorithm. algorithm COMPARE algorithm. algorithm COMPARE algorithm. convergence rate EVALUATE-FOR algorithm. sample complexity EVALUATE-FOR algorithm. OtherScientificTerm are communication network, and stationary point. Method is consensus - based actor - critic algorithm. ","This paper proposes a consensus-based actor-critic algorithm. The main idea is to use a communication network to communicate between actors and critic. The authors show that the convergence rate and sample complexity of the proposed algorithm are better than the existing algorithm. Moreover, the authors provide a theoretical analysis of the stationary point of the algorithm."
862,SP:96e1da163020441f9724985ae15674233e0cfe0d,"model USED-FOR networked MARL problem. consensus loop USED-FOR average TD error. consensus loop USED-FOR advantage function. average TD error USED-FOR advantage function. OtherScientificTerm are global state, local action, local rewards, global action, and finite time error bound. ",This paper proposes a model for the networked MARL problem where the global state is unknown and the local action is unknown. The advantage function is defined as the average TD error of a consensus loop over all local rewards. The authors provide a finite time error bound and provide a theoretical analysis of the global action.
863,SP:8475e89f143c727e33147b652c2d0b3cdb420382,"representations USED-FOR linearly classifying downstream classes. assumptions USED-FOR contrastive learning. augmentation distribution USED-FOR connected graph. data augmentations USED-FOR contrastive learning. downstream performance of representation function CONJUNCTION contrastive loss. contrastive loss CONJUNCTION downstream performance of representation function. assumptions USED-FOR Lower and upper bounds. Average Confusion Ratio ( ACR ) "" metric EVALUATE-FOR augmentations. unlabeled data USED-FOR Average Confusion Ratio ( ACR ) "" metric. unlabeled data USED-FOR augmentations. metric USED-FOR augmentations. Method is InfoNCE inspired objective function. OtherScientificTerm is representation function. Material is CIFAR and STL datasets. Metric is ACR metric. ","This paper studies the problem of contrastive learning with data augmentations. The authors propose an InfoNCE inspired objective function, where the goal is to learn representations for linearly classifying downstream classes. Lower and upper bounds are derived under certain assumptions on the downstream performance of representation function and contrastive loss. The main idea is to use an augmentation distribution over a connected graph. The paper also proposes a ""Average Confusion Ratio (ACR)"" metric to evaluate augmentations with unlabeled data. The ACR metric is based on the fact that the representation function can be trained to be more interpretable. Experiments are conducted on CIFAR and STL datasets."
864,SP:8475e89f143c727e33147b652c2d0b3cdb420382,data augmentation USED-FOR intra - class samples. data augmentation USED-FOR contrastive learning. clustering of intra - class samples CONJUNCTION learning of class - separated representations. learning of class - separated representations CONJUNCTION clustering of intra - class samples. Metric is metric ARC. Material is synthetic and real - world datasets. ,This paper studies the effect of data augmentation in contrastive learning on the clustering of intra-class samples and the learning of class-separated representations. The authors propose a new metric ARC to measure the impact of different types of augmentation on the performance. The paper conducts extensive experiments on both synthetic and real-world datasets.
865,SP:8475e89f143c727e33147b652c2d0b3cdb420382,"data augmentation schemes USED-FOR contrastive learning techniques. augmentation USED-FOR intra - class images. downstream performance EVALUATE-FOR metric. augmentation - oriented understanding USED-FOR metric. alignment CONJUNCTION uniformity. uniformity CONJUNCTION alignment. learning of class - discriminative features USED-FOR downstream classification. instance discrimination task USED-FOR learning of class - discriminative features. Framework USED-FOR Contrastive Learning of Visual Representations. Alignment CONJUNCTION Uniformity. Uniformity CONJUNCTION Alignment. Hypersphere FEATURE-OF Uniformity. Hypersphere FEATURE-OF Alignment. Alignment USED-FOR Contrastive Representation Learning. Uniformity USED-FOR Contrastive Representation Learning. OtherScientificTerm are contrastive losses, contrastive loss, augmented views, feature space, and relaxed assumption. Method are contrastive learning, contrastive - learning, and contrastive unsupervised representation learning. Generic are perspective, and representations. Task is data augmentation. Metric are evaluation metric, and downstream classification accuracy. ","This paper studies the problem of contrastive learning techniques with data augmentation schemes. The main contribution of this paper is to propose a new metric for evaluating the downstream performance based on augmentation-oriented understanding, which is motivated by the observation that the contrastive losses often fail to generalize well to new views. The authors argue that this is due to the fact that the augmentation is typically applied to intra-class images, and that the perspective is not necessarily consistent with the training data. To address this issue, the authors propose to use contrastive-learning as an alternative evaluation metric. The contrastive loss is based on the assumption that the augmented views lie in the same feature space as the original images. This perspective allows the authors to make a relaxed assumption that representations learned by contrastive unsupervised representation learning should be similar to the original representations. The learning of class-discriminative features for downstream classification is performed using an instance discrimination task.  The authors propose a Framework for Contrastive Learning of Visual Representations based on Alignment and Uniformity on the Hypersphere. The experiments show that the proposed evaluation metric improves downstream classification accuracy."
866,SP:8475e89f143c727e33147b652c2d0b3cdb420382,theory USED-FOR contrastive representation learning. alignment CONJUNCTION augmentation. augmentation CONJUNCTION alignment. alignment USED-FOR contrastive learning. augmentations USED-FOR embeddings. augmentation USED-FOR downstream classification. Generic is metric. OtherScientificTerm is nearest ( embedding ) neighbors. ,"This paper proposes a new theory for contrastive representation learning based on alignment and augmentation. Specifically, the authors propose a metric called nearest (embedding) neighbors that measures the distance between two embeddings. The authors argue that alignment is important in contrastive learning and that augmentations to the embedding can improve the performance of downstream classification."
867,SP:b491314336c503b276e34e410cf461cb81294890,approach USED-FOR restoring degraded speech signals. VoiceFixer USED-FOR restoring degraded speech signals. clipping CONJUNCTION limited bandwidth. limited bandwidth CONJUNCTION clipping. reverberations CONJUNCTION clipping. clipping CONJUNCTION reverberations. additive noise CONJUNCTION reverberations. reverberations CONJUNCTION additive noise. limited bandwidth HYPONYM-OF speech degradations. additive noise HYPONYM-OF speech degradations. clipping HYPONYM-OF speech degradations. reverberations HYPONYM-OF speech degradations. vocoder USED-FOR speech. restored mel - spectrogram USED-FOR vocoder. restored mel - spectrogram USED-FOR speech. Generic is two stage approach. Material is VCTK dataset. OtherScientificTerm is single distortions. ,"This paper presents a two stage approach to restoring degraded speech signals from VoiceFixer. The first stage is to reconstruct the original speech from the VCTK dataset. The second stage is a two-stage approach where the speech is reconstructed from a restored mel-spectrogram and the vocoder is trained on the reconstructed speech. The speech degradations considered are additive noise, reverberations, clipping, and limited bandwidth. The results show that the proposed method can recover speech from single distortions."
868,SP:b491314336c503b276e34e410cf461cb81294890,model USED-FOR speech restoration ( GSR ) task. generative framework USED-FOR speech restoration task. VoiceFixer HYPONYM-OF generative framework. analysis and synthesis stages PART-OF VoiceFixer. analysis and synthesis stages PART-OF generative framework. ResNet USED-FOR analysis stage. TFGAN - based neural vocoder USED-FOR synthesis stage. ResNet CONJUNCTION TFGAN - based neural vocoder. TFGAN - based neural vocoder CONJUNCTION ResNet. ResNet USED-FOR VoiceFixer. VoiceFixer COMPARE single speech restoration ( SSR ) models. single speech restoration ( SSR ) models COMPARE VoiceFixer. GSR COMPARE single speech restoration ( SSR ) models. single speech restoration ( SSR ) models COMPARE GSR. GSR CONJUNCTION VoiceFixer. VoiceFixer CONJUNCTION GSR. single speech restoration ( SSR ) models COMPARE latter. latter COMPARE single speech restoration ( SSR ) models. super - resolution CONJUNCTION dereverberation. dereverberation CONJUNCTION super - resolution. dereverberation CONJUNCTION declipping. declipping CONJUNCTION dereverberation. speech denoising CONJUNCTION super - resolution. super - resolution CONJUNCTION speech denoising. speech restoration tasks PART-OF single unified task. GSR HYPONYM-OF single unified task. GSR HYPONYM-OF speech restoration tasks. speech denoising HYPONYM-OF speech restoration tasks. declipping HYPONYM-OF speech restoration tasks. dereverberation HYPONYM-OF speech restoration tasks. super - resolution HYPONYM-OF speech restoration tasks. VoiceFixer HYPONYM-OF generative speech restoration framework. ,"This paper proposes a new model for the speech restoration (GSR) task. The proposed generative framework, VoiceFixer, consists of both analysis and synthesis stages. In the analysis stage, a ResNet is used and a TFGAN-based neural vocoder is used in the synthesis stage. Experiments are conducted on a variety of speech restoration tasks, including GSR, V2G, super-resolution, dereverberation, declipping, etc. The experimental results show that the proposed GSR and the proposed voice-to-speech (V2S) models outperform the single speech restoration(SSR) models, while the latter outperforms the proposed V2S. Overall, this paper presents a new generative speech restoration framework called VoiceFixER."
869,SP:b491314336c503b276e34e410cf461cb81294890,dereverberation CONJUNCTION audio super - resolution. audio super - resolution CONJUNCTION dereverberation. denoising CONJUNCTION decliping. decliping CONJUNCTION denoising. decliping CONJUNCTION dereverberation. dereverberation CONJUNCTION decliping. audio super - resolution HYPONYM-OF speech restoration problems. denoising HYPONYM-OF speech restoration problems. dereverberation HYPONYM-OF speech restoration problems. decliping HYPONYM-OF speech restoration problems. U - Net architecture USED-FOR tasks. U - Net architecture USED-FOR speech restoration task. models COMPARE models. models COMPARE models. speech restoration task CONJUNCTION tasks. tasks CONJUNCTION speech restoration task. model CONJUNCTION analysis - synthesis procedure. analysis - synthesis procedure CONJUNCTION model. VoiceFixer combination USED-FOR speech distortions. VoiceFixer combination CONJUNCTION analysis - synthesis procedure. analysis - synthesis procedure CONJUNCTION VoiceFixer combination. VoiceFixer combination COMPARE approaches. approaches COMPARE VoiceFixer combination. VoiceFixer combination CONJUNCTION model. model CONJUNCTION VoiceFixer combination. ,"This paper studies speech restoration problems such as denoising, decliping, dereverberation, and audio super-resolution. The authors propose a U-Net architecture for these tasks, which can be combined with a speech restoration task and other tasks. The experiments show that the proposed models outperform existing models. The VoiceFixer combination, model, and analysis-synthesis procedure are shown to be effective at removing speech distortions, and the VoiceFixER combination and model are compared with other approaches."
870,SP:b491314336c503b276e34e410cf461cb81294890,system USED-FOR speech enhancement tasks. bandwidth extension ( BWE ) CONJUNCTION declipping. declipping CONJUNCTION bandwidth extension ( BWE ). dereverb CONJUNCTION bandwidth extension ( BWE ). bandwidth extension ( BWE ) CONJUNCTION dereverb. denoising CONJUNCTION dereverb. dereverb CONJUNCTION denoising. system USED-FOR denoising. denoising HYPONYM-OF speech enhancement tasks. declipping HYPONYM-OF speech enhancement tasks. bandwidth extension ( BWE ) HYPONYM-OF speech enhancement tasks. dereverb HYPONYM-OF speech enhancement tasks. analysis module CONJUNCTION synthesis module. synthesis module CONJUNCTION analysis module. analysis module USED-FOR mel - band masks. system PART-OF two - stage system. synthesis module PART-OF system. vocoder USED-FOR synthesis module. synthesis module PART-OF two - stage system. analysis module PART-OF two - stage system. architectures USED-FOR modules. systems USED-FOR tasks. systems COMPARE system. system COMPARE systems. system COMPARE systems. systems COMPARE system. it COMPARE systems. systems COMPARE it. system COMPARE it. it COMPARE system. tasks EVALUATE-FOR system. ,"This paper presents a system for speech enhancement tasks such as denoising, dereverb, bandwidth extension (BWE), and declipping. The system is a two-stage system consisting of an analysis module that generates mel-band masks, and a synthesis module that uses a vocoder to generate the final output. The two modules are trained on different architectures. The paper compares the performance of the two systems on different tasks and shows that the system outperforms the other systems."
871,SP:c80a7392ec6147395a664734601fb389a1eb4470,"tensor network USED-FOR variable space. tensor network USED-FOR variable space. tensor network USED-FOR multivariate time series forecasting. N - order residual connection approach CONJUNCTION skip - connection layer. skip - connection layer CONJUNCTION N - order residual connection approach. skip - connection layer USED-FOR long - term data. N - order residual connection approach USED-FOR long - term data. Method are series - variable encoder, and MVSRTN. ",This paper proposes to use a tensor network to represent the variable space in multivariate time series forecasting. The authors propose a series-variable encoder and a skip-connection layer to handle long-term data using an N-order residual connection approach. The experimental results show the effectiveness of MVSRTN.
872,SP:c80a7392ec6147395a664734601fb389a1eb4470,deep learning architecture USED-FOR time series forecasting. MVSRTN HYPONYM-OF deep learning architecture. blocks PART-OF model. N - Order Residual Tensor Network PART-OF blocks. Series - Variable Encoder PART-OF blocks. output layer PART-OF blocks. skip - connections PART-OF blocks. output layer PART-OF model. skip - connections PART-OF model. tensor product USED-FOR tensor network. TT - rank - constrained weight tensor USED-FOR tensor network. identity mapping USED-FOR recursive formulation. identity mapping USED-FOR hidden state. identity mapping USED-FOR residual networks. recursion across time - steps USED-FOR tensor network output. higher - order solvers USED-FOR ODEs. time series forecasting datasets EVALUATE-FOR model. Generic is it. Method is 1D CNN. OtherScientificTerm is higher - order residual connection. ,"This paper proposes MVSRTN, a new deep learning architecture for time series forecasting. The model consists of two blocks: 1) a N-Order Residual Tensor Network, where it takes as input the output of a 1D CNN, and 2) a Series-Variable Encoder, where the output layer of the model is a tensor network that takes the tensor product as input, and outputs the output as a TT-rank-constrained weight tensor. The residual networks are trained using an identity mapping to the hidden state, which is then used in a recursive formulation. The authors show that recursion across time-steps can be used to improve the tensore network output by adding a higher-order residual connection. They also show that the ODEs can be solved with higher-orders solvers. Finally, the authors evaluate the model on a set of standard time-series forecasting datasets."
873,SP:c80a7392ec6147395a664734601fb389a1eb4470,MVSRTN architecture USED-FOR multivariate time series modeling. encoder USED-FOR latent variables. encoder CONJUNCTION residual tensor network ( TN ) blocks. residual tensor network ( TN ) blocks CONJUNCTION encoder. latent variables PART-OF MVSRTN. encoder PART-OF MVSRTN. residual tensor network ( TN ) blocks PART-OF MVSRTN. TN block part PART-OF model. tensor - products USED-FOR latent variable space. N - order residual TN block USED-FOR high - order TNs. long - term time series FEATURE-OF high - order TNs. multivariate time series data USED-FOR prediction. ResNet CONJUNCTION TN. TN CONJUNCTION ResNet. OtherScientificTerm is latent space. Method is residual TN blocks. ,This paper proposes a new MVSRTN architecture for multivariate time series modeling. The proposed model consists of an encoder that maps latent variables to latent variables and residual tensor network (TN) blocks. The TN block part of the model is used to represent high-order TNs in long-term time series. The latent variable space is modeled using tensor-products. The N-order residual TN block can be used to model high-orders of the latent space. The residual TN blocks are trained in parallel with ResNet and TN. The main contribution of this paper is the use of multivariate Time series data for prediction.
874,SP:c80a7392ec6147395a664734601fb389a1eb4470,architecture USED-FOR forecasting. residual tensor network layer PART-OF layers. layers PART-OF architecture. outer products CONJUNCTION Kronecker products. Kronecker products CONJUNCTION outer products. outer products USED-FOR features. encoder layer USED-FOR features. tensor network USED-FOR tensorized features. Task is forecasting of multivariate time series. Method is tensorization. Material is benchmark datasets. ,This paper proposes a new architecture for forecasting of multivariate time series. The proposed architecture consists of two layers: a residual tensor network layer and an encoder layer. The tensorization of the features is done by combining the outer products with the Kronecker products. The features are then fed into the encoder and the tensor networks are trained to produce tensorized features. The authors conduct extensive experiments on several benchmark datasets and show promising results.
875,SP:0a92939e6a1c88bfeb4fd1dea9ee7be4fd60d967,optimization algorithm USED-FOR GNN. stochastic compositional optimization problem USED-FOR graph neural network. ,This paper proposes a new optimization algorithm for GNN. The main idea is to formulate the graph neural network as a stochastic compositional optimization problem. 
876,SP:0a92939e6a1c88bfeb4fd1dea9ee7be4fd60d967,Stochastic Compositional Optimization ( SCO ) framework USED-FOR GNNs. sparse representation USED-FOR nodes'moving averages. it USED-FOR large graphs. fixed - size buffer USED-FOR algorithm. Adam SGD USED-FOR GNN training. algorithm COMPARE Adam SGD. Adam SGD COMPARE algorithm. convergence rate EVALUATE-FOR SCO algorithm. algorithm COMPARE SCO algorithm. SCO algorithm COMPARE algorithm. algorithm USED-FOR GNN training. small memory overhead EVALUATE-FOR algorithm. convergence rate EVALUATE-FOR algorithm. OtherScientificTerm is graph size. Method is SCO algorithms. ,"This paper proposes a Stochastic Compositional Optimization (SCO) framework for training GNNs. The main idea is to learn a sparse representation for nodes' moving averages, and then use it to train large graphs. The proposed algorithm is trained on a fixed-size buffer, and the authors show that the proposed algorithm can achieve a better convergence rate than Adam SGD for GNN training with small memory overhead. The authors also provide a theoretical analysis of the effect of graph size on the performance of SCO algorithms."
877,SP:0a92939e6a1c88bfeb4fd1dea9ee7be4fd60d967,"neighbor sampling techniques USED-FOR GNNs. Stochastic Compositional Optimization ( SCO ) problem USED-FOR sampling - based GNN training. SCO algorithms USED-FOR GNNs. SpSC COMPARE naive SCO. naive SCO COMPARE SpSC. Convergence analysis USED-FOR SpSC. OtherScientificTerm are memory cost, and nodes. Method is Sparse Stochastic Compositional ( SpSC ) gradient method. ","This paper studies the Stochastic Compositional Optimization (SCO) problem for sampling-based GNN training. The authors propose a new neighborhood sampling techniques for GNNs. The main idea is to reduce the memory cost of SCO algorithms for training GNN. To this end, the authors propose the Sparse Stochmatic Compositional (SpSC) gradient method. Convergence analysis is performed to show that SpSC outperforms the naive SCO in terms of the number of nodes."
878,SP:0a92939e6a1c88bfeb4fd1dea9ee7be4fd60d967,large - scale graphs USED-FOR GNNs. sparse moving average USED-FOR sampling - based GNN training strategies. convergence rate EVALUATE-FOR approach. Material is large - scale datasets. Generic is proposal. ,"This paper studies the problem of training GNNs on large-scale graphs. In particular, the authors consider sampling-based GNN training strategies based on a sparse moving average. The authors show that their approach can achieve a better convergence rate on a wide range of datasets. The proposal is well motivated and the experimental results are promising."
879,SP:72e0cac289dce803582053614ec9ee93e783c838,Min - wise hashing ( MinHash ) HYPONYM-OF machine learning. Circulant MinHash ( C - MinHash ) USED-FOR Jaccard similarity. massive binary data USED-FOR Jaccard similarity. MinHash COMPARE C - MinHash. C - MinHash COMPARE MinHash. circulant manner USED-FOR approximation. C - MinHash USED-FOR approximation. random permutations USED-FOR approximation. circulant manner FEATURE-OF random permutations. circulant manner USED-FOR C - MinHash. random permutations USED-FOR C - MinHash. C - MinHash COMPARE MinHash. MinHash COMPARE C - MinHash. C - MinHash USED-FOR estimation variance. ,"This paper proposes Circulant MinHash (C-MinHash) to approximate Jaccard similarity on massive binary data. Min-wise hashing (MinHash), a well-known technique in machine learning, has been shown to be computationally expensive. The authors argue that MinHash is not computationally efficient and propose C-minHash, which uses random permutations in a circulant manner to approximate the approximation in a similar manner as MinHash. The experiments show that C-MinHash is able to reduce estimation variance compared to MinHash while maintaining the same performance."
880,SP:72e0cac289dce803582053614ec9ee93e783c838,MinHash data structure USED-FOR Jaccard similarity. MinHash data structure USED-FOR hash values. hash values USED-FOR binary string. independent random permutations USED-FOR hash values. hash values USED-FOR binary strings. random permutation USED-FOR binary string. random permutation USED-FOR algorithm. permutation USED-FOR algorithm. estimator COMPARE MinHash. MinHash COMPARE estimator. independent random permutations USED-FOR scheme. Generic is unbiased estimator. Method is C - MinHash. ,"This paper proposes an unbiased estimator of the Jaccard similarity between two binary strings using the MinHash data structure for computing the hash values for the binary strings. The algorithm uses a random permutation of the binary string as the permutation and uses independent random permutations to compute hash values. The proposed estimator, called C-MinHash, is shown to outperform MinHash. The authors also propose a scheme based on the idea of using independent Random permutations for computing hash values to compute binary strings and show that this estimator outperforms MinHash by a large margin."
881,SP:72e0cac289dce803582053614ec9ee93e783c838,C - MINHASH USED-FOR vanilla MINHASH. permutations USED-FOR C - MINHASH. C - MINHASH COMPARE MINHASH. MINHASH COMPARE C - MINHASH. ,"This paper proposes C-MINHASH, which is an extension of vanilla MINHASH. The main idea is to use permutations to make the training process more efficient. Experiments show that C-MinHASH can achieve better performance than the vanilla minHASH with fewer permutations. The experiments also show that the performance of C-MINEHASH is comparable to MINHAS."
882,SP:72e0cac289dce803582053614ec9ee93e783c838,approach USED-FOR MinHash. permutation USED-FOR hash values. approach USED-FOR hash values. permutation USED-FOR approach. theoretical approximation error EVALUATE-FOR approach. Material is text and image datasets. ,This paper proposes an approach to MinHash that uses a permutation to approximate the hash values. The theoretical approximation error of the proposed approach is evaluated on text and image datasets.
883,SP:d254b38331b6b6f30de398bae09380cd5c951698,method USED-FOR image classifiers. image classifiers USED-FOR $ \ell_p$ threat models. robustness EVALUATE-FOR $ \ell_p$ threat models. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. method USED-FOR fine - tuning robust models. method USED-FOR classifiers. $ \ell_p$ threat model USED-FOR fine - tuning robust models. CIFAR-10 EVALUATE-FOR method. ImageNet EVALUATE-FOR method. Method is affine classifiers. ,"This paper proposes a method for fine-tuning image classifiers to improve the robustness of $ell_p$ threat models. The method is based on the idea of affine classifiers, where the classifiers are trained to be invariant to perturbations. The proposed method is evaluated on CIFAR-10 and ImageNet, where it is shown that the proposed method can improve the performance of classifiers trained with a $\ell_{p} threat model."
884,SP:d254b38331b6b6f30de398bae09380cd5c951698,CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. CIFAR-10 EVALUATE-FOR method. APGD USED-FOR ImageNet. Task is robustness. Metric is multi - norm robustness. ,This paper proposes a method for improving the robustness of multi-norm robustness. The method is evaluated on CIFAR-10 and ImageNet with APGD.
885,SP:d254b38331b6b6f30de398bae09380cd5c951698,multiple perturbation adversarial robustness FEATURE-OF attacks. model USED-FOR multiple perturbation adversarial robustness. prior formalization USED-FOR geometry of $ \ell_p$ balls. Method is AutoAttack. ,"This paper proposes AutoAttack, a new model for multiple perturbation adversarial robustness to attacks. The authors propose a prior formalization for the geometry of $\ell_p$ balls, which is a generalization of AutoAttack. "
886,SP:d254b38331b6b6f30de398bae09380cd5c951698,"Task are defending multiple norm adversarial perturbations, and adversarial training. OtherScientificTerm are multiple norm adversarial perturbations, and E - AT fine - tune. Generic are model, and method. ",This paper addresses the problem of defending multiple norm adversarial perturbations. The authors propose a method called E-AT fine-tuning to make the model more robust to different types of adversarial training. 
887,SP:4c2928f6772664d63c02c29f913b476e1c932983,"method USED-FOR safe multi - task learning. shared encoder CONJUNCTION task - specific ( private ) encoders. task - specific ( private ) encoders CONJUNCTION shared encoder. task - specific ( private ) encoders CONJUNCTION gate. gate CONJUNCTION task - specific ( private ) encoders. gate CONJUNCTION decoder. decoder CONJUNCTION gate. It USED-FOR shared encoder. task - specific ( private ) encoders PART-OF It. OtherScientificTerm are negative transfer, and shared and task specific encoder. Method is convex combination. ","This paper proposes a method for safe multi-task learning. It consists of a shared encoder, two task-specific (private) encoders, a gate, and a decoder. The key idea is to avoid negative transfer between the shared and task specific encoder. This is achieved by learning a convex combination of the two encoder heads."
888,SP:4c2928f6772664d63c02c29f913b476e1c932983,"public encoder CONJUNCTION private encoder. private encoder CONJUNCTION public encoder. gate USED-FOR encoded features. private encoder CONJUNCTION gate. gate CONJUNCTION private encoder. public and private encoders USED-FOR encoded features. private encoder PART-OF network architecture. public encoder PART-OF network architecture. gate PART-OF network architecture. approach USED-FOR image recognition related tasks. Method are multi - task learning approach, and deep neural networks. OtherScientificTerm is negative sharing. ","This paper proposes a multi-task learning approach. The proposed network architecture consists of a public encoder, a private encoder and a gate to encode the encoded features from both public and private encoders. The key idea of the proposed approach is to avoid negative sharing between the public and the private layers of deep neural networks. The approach is applied to image recognition related tasks."
889,SP:4c2928f6772664d63c02c29f913b476e1c932983,multi - task learning FEATURE-OF negative sharing problem. variants USED-FOR safe multi - task learning. variants USED-FOR negative sharing. Multi - Task Learning ( SMTL ) model CONJUNCTION variants. variants CONJUNCTION Multi - Task Learning ( SMTL ) model. Generic is method. ,This paper studies the problem of negative sharing problem in multi-task learning. The authors propose a Multi-Task Learning (SMTL) model and variants for safe multi-tasking. The proposed method is evaluated on several datasets.
890,SP:4c2928f6772664d63c02c29f913b476e1c932983,multi - task learning model COMPARE single - task learning. single - task learning COMPARE multi - task learning model. single - task learning USED-FOR multi - task learning. Multi - Task Learning ( SMTL ) model USED-FOR negative sharing. hard - sharing model CONJUNCTION single - task learning. single - task learning CONJUNCTION hard - sharing model. model COMPARE single - task learning. single - task learning COMPARE model. hard - sharing model PART-OF model. single - task learning PART-OF model. Task is negative sharing problem. ,This paper studies the negative sharing problem and proposes a Multi-Task Learning (SMTL) model to tackle the problem of negative sharing. The proposed multi-task learning model is shown to outperform single-tasks learning. The model combines a hard-sharing model with single-tasklearning.
891,SP:c4cee0d44198559c417750ec4729d26b41061929,"computational models USED-FOR accepting languages. architectures of energy - based - models USED-FOR computational models. probability measure USED-FOR energy. poly - time FEATURE-OF energy. Method are computational model, and EBM's. OtherScientificTerm is partition function. Task is model selection. Generic is model. ","This paper studies the problem of learning computational models for accepting languages using architectures of energy-based-models. In particular, the authors consider the case where the computational model is trained on a partition function, where the partition function is a probability measure of the energy in poly-time, and the model selection is based on the probability of the partition. The authors show that EBM's are able to learn a partition of a partition into sub-problems, and that this can be used to train a model that is able to adapt to new languages."
892,SP:c4cee0d44198559c417750ec4729d26b41061929,uncomputability issues FEATURE-OF energy - based sequence models. computational model USED-FOR neural networks. EC - complete family HYPONYM-OF computational model. partition function FEATURE-OF energy - based sequence models ( EBMs ). model selection USED-FOR EBMs. rejection CONJUNCTION important sampling. important sampling CONJUNCTION rejection. important sampling HYPONYM-OF estimators. rejection HYPONYM-OF estimators. uncomputability issues FEATURE-OF restricted EBMs. Method is weighted Turing machines. ,"This paper studies the uncomputability issues of energy-based sequence models (EBMs) with partition function. The authors propose a new computational model, the EC-complete family, for training neural networks, which is inspired by weighted Turing machines. The main contribution of this paper is to study the model selection of EBMs and to propose two new estimators, rejection and important sampling, to address the uncomputability of restricted EBMs."
893,SP:c4cee0d44198559c417750ec4729d26b41061929,"poly - time Turing machine USED-FOR nonnegative weights. neural networks USED-FOR weights. RNNs CONJUNCTION Transformers. Transformers CONJUNCTION RNNs. RNNs HYPONYM-OF neural sequence model families. Transformers HYPONYM-OF neural sequence model families. computability EVALUATE-FOR primitives. primitives USED-FOR inference. EC - complete parametric families HYPONYM-OF sequence model families. model PART-OF family. rejection sampling CONJUNCTION importance sampling. importance sampling CONJUNCTION rejection sampling. importance sampling HYPONYM-OF asymptotic * estimates. rejection sampling HYPONYM-OF asymptotic * estimates. EBM COMPARE EBM. EBM COMPARE EBM. weight function USED-FOR halting problem. OtherScientificTerm are normalization, probability distribution, partition function, Godel's second incompleteness theorem, parameter vectors, and parameter identifiability. Method are ZFC, and model selection. Generic is tasks. ","This paper proposes a poly-time Turing machine for learning nonnegative weights. The weights are learned using neural networks, such as RNNs and Transformers. The authors argue that primitives that improve the computability of inference can be used as a normalization of the probability distribution. The main contribution of the paper is to study sequence model families (e.g., EC-complete parametric families). The authors provide asymptotic *approximation* estimates of the partition function, as well as *rejection sampling* and importance sampling. They also show that ZFC is equivalent to EBM in terms of the number of parameter vectors, and show that parameter identifiability can be achieved. Finally, the authors show that the weight function can be viewed as a halting problem, and that model selection can lead to better performance on these tasks."
894,SP:c4cee0d44198559c417750ec4729d26b41061929,expressiveness CONJUNCTION computability. computability CONJUNCTION expressiveness. computability FEATURE-OF partition function. expressiveness FEATURE-OF partition function. computability EVALUATE-FOR energy - based sequence models. expressiveness EVALUATE-FOR energy - based sequence models. un - computability CONJUNCTION in - approximability. in - approximability CONJUNCTION un - computability. in - approximability FEATURE-OF partition function. un - computability FEATURE-OF partition function. rejection CONJUNCTION importance sampling. importance sampling CONJUNCTION rejection. Method is unrestricted energy based models. ,"This paper studies the expressiveness and computability of the partition function of energy-based sequence models. The authors show that the partition of a sequence can have both un-computability and in-approximability. They also show that unrestricted energy based models are computationally efficient. Finally, they show that rejection and importance sampling can be used to improve the performance."
895,SP:f1eb66f24a14808d404f9ad9773ef4288efa060e,"random linear projections USED-FOR sliced Wasserstein Distance. slice - based approach USED-FOR Wasserstein distance. spatial Radon Transform USED-FOR slicing. spatial Radon Transform USED-FOR procedure. ASWD HYPONYM-OF metric. numerical algorithm USED-FOR injective mapping. NN USED-FOR injective mapping. method COMPARE approaches. approaches COMPARE method. generative modeling EVALUATE-FOR method. simulation datasets EVALUATE-FOR method. simulation datasets CONJUNCTION generative modeling. generative modeling CONJUNCTION simulation datasets. generative modeling EVALUATE-FOR approaches. OtherScientificTerm are higher dimensional space, and non linear injective mapping function. ","This paper proposes a slice-based approach to compute the Wasserstein distance in a higher dimensional space. Specifically, the sliced version of the sliced wasserstein Distance is computed using random linear projections. The proposed procedure is based on the spatial Radon Transform for slicing, which is a non linear injective mapping function. The authors also propose a new metric called ASWD, which can be used as a numerical algorithm to compute a more efficient injective map from NN to NN. The method is evaluated on several simulation datasets and generative modeling and compared to other approaches."
896,SP:f1eb66f24a14808d404f9ad9773ef4288efa060e,"neural networks USED-FOR approximation. Method are augmented sliced Wasserstein distances, and sliced Wasserstein distance. OtherScientificTerm are higher -dimensional hypersurfaces, and nonlinear maps. Generic is distance. ","This paper proposes augmented sliced Wasserstein distances, which can be used to approximate higher-dimensional hypersurfaces. The proposed approximation is based on neural networks, which are trained on nonlinear maps. The authors show that the proposed distance can be approximated with a small number of parameters, which is in contrast to the original sliced sliced wasserstein distance."
897,SP:f1eb66f24a14808d404f9ad9773ef4288efa060e,"slicing scheme USED-FOR 1D Wasserstein distances. augmented sliced Wasserstein distance ( ASWD ) HYPONYM-OF sliced Wasserstein distance ( SWD ). uniformly projection directions FEATURE-OF 1D Wasserstein distances. Radon transform CONJUNCTION polynomial generalized Random transform. polynomial generalized Random transform CONJUNCTION Radon transform. Radon transform PART-OF spatial Radon transform. polynomial generalized Random transform PART-OF spatial Radon transform. ASWD HYPONYM-OF metric. CelebA CONJUNCTION MNIST. MNIST CONJUNCTION CelebA. MNIST CONJUNCTION color transferring. color transferring CONJUNCTION MNIST. CIFAR10 CONJUNCTION CelebA. CelebA CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION MNIST. MNIST CONJUNCTION CIFAR10. CIFAR10 HYPONYM-OF generative modelling. color transferring HYPONYM-OF generative modelling. MNIST HYPONYM-OF generative modelling. CelebA HYPONYM-OF generative modelling. OtherScientificTerm are probability distributions, higher - dimensional space, and mapping. ","This paper proposes a slicing scheme for 1D Wasserstein distances with uniformly projection directions. The proposed sliced Wassersteins distance (ASWD) is a variant of the sliced-Wasserstein distance (SWD), which is based on the fact that probability distributions can be expressed in a higher-dimensional space. The spatial Radon transform consists of a Radon transformation and a polynomial generalized Random transform. The ASWD is a new metric, which is motivated by the observation that the mapping between the Radon and the generalized Random transforms can be computed efficiently. Experiments are conducted on generative modelling (CIFAR10, CelebA, MNIST, color transferring)."
898,SP:f1eb66f24a14808d404f9ad9773ef4288efa060e,augmented sliced Wasserstein distance HYPONYM-OF sliced Wasserstein distance. ASWD USED-FOR SWD. hypersurfaces USED-FOR SWD. neural networks USED-FOR ASWD. ASWD USED-FOR SWD. SWD USED-FOR high - dimensional data. flow CONJUNCTION generative modeling. generative modeling CONJUNCTION flow. generative modeling CONJUNCTION barycenters. barycenters CONJUNCTION generative modeling. AWSD COMPARE methods. methods COMPARE AWSD. tasks EVALUATE-FOR methods. tasks EVALUATE-FOR AWSD. barycenters HYPONYM-OF tasks. flow HYPONYM-OF tasks. generative modeling HYPONYM-OF tasks. ,"This paper proposes an augmented sliced Wasserstein distance, i.e., ASWD, which is an improved version of the traditional slice-and-dissolve (SWD) based on hypersurfaces. ASWD is trained using neural networks and can be used to improve the performance of SWD on high-dimensional data. Experimental results show that AWSD outperforms existing methods on a variety of tasks including flow, generative modeling, and barycenters."
899,SP:ff2433f2de48d4ed8017e27bd6cf606845cdea9e,centralized exploration reward USED-FOR multi - agent settings. on / off gate CONJUNCTION scale function. scale function CONJUNCTION on / off gate. on / off gate PART-OF exploration reward. scale function PART-OF exploration reward. mathematical derivations USED-FOR theoretical guarantees. gridworld environments USED-FOR multi - agent exploration. SMAC benchmark FEATURE-OF maps. OtherScientificTerm is switching control. ,This paper proposes a centralized exploration reward for multi-agent settings. The exploration reward consists of an on/off gate and a scale function. The authors provide mathematical derivations to provide theoretical guarantees. The paper also presents experiments on gridworld environments to demonstrate the benefits of multi-aggressively exploring and switching control. The maps are evaluated on the SMAC benchmark.
900,SP:ff2433f2de48d4ed8017e27bd6cf606845cdea9e,"Intrinsic - reward Generation Selection ( LIGS ) USED-FOR coordinated exploration. method USED-FOR coordinated exploration. method USED-FOR Intrinsic - reward Generation Selection ( LIGS ). it USED-FOR optimality. Method are LIGS, and MARL methods. OtherScientificTerm are Generator, and intrinsic reward. ","This paper proposes a method called Intrinsic-reward Generation Selection (LIGS) for coordinated exploration. LIGS is a generalization of MARL methods, where the Generator is trained to maximize the intrinsic reward. The authors argue that it can be used to improve optimality."
901,SP:ff2433f2de48d4ed8017e27bd6cf606845cdea9e,switching control system USED-FOR intrinsic control. reinforcement learning algorithm USED-FOR multi - agent system ( MARL ). generator of intrinsic reward CONJUNCTION switching control system. switching control system CONJUNCTION generator of intrinsic reward. generator of intrinsic reward USED-FOR reinforcement learning algorithm. algorithm USED-FOR exploration. algorithm USED-FOR preservation of known policies. exploration CONJUNCTION preservation of known policies. preservation of known policies CONJUNCTION exploration. convergence CONJUNCTION optimality. optimality CONJUNCTION convergence. approach COMPARE baselines. baselines COMPARE approach. mechanism USED-FOR co - ordinated RL agents. OtherScientificTerm is intrinsic reward. Task is RL. ,"This paper proposes a reinforcement learning algorithm for multi-agent system (MARL) with a generator of intrinsic reward and a switching control system for intrinsic control. The algorithm is designed for exploration, preservation of known policies, and learning to maximize the intrinsic reward. The authors provide theoretical analysis on convergence and optimality of the proposed mechanism for co-ordinated RL agents. Experiments show that the proposed approach outperforms the baselines. "
902,SP:ff2433f2de48d4ed8017e27bd6cf606845cdea9e,learning intrinsic rewards USED-FOR multi - agent reinforcement learning. learnable gating function USED-FOR agent. Generic is method. ,This paper studies the problem of learning intrinsic rewards for multi-agent reinforcement learning. The authors propose a method that learns a learnable gating function that encourages the agent to maximize the likelihood of reaching a goal. The method is well motivated and well-motivated.
903,SP:9eadc19f7f712c488cf50d091f372092f6352930,attention - based model USED-FOR conversational and multi - hop QA tasks. BERT - like pre - trained LM USED-FOR model. weighted sum of sentences ’ encodings USED-FOR paragraph embeddings. dot product attention USED-FOR weighted sum of sentences ’ encodings. dot product attention USED-FOR paragraph embeddings. paragraph embeddings PART-OF context encodings. hard attention mechanism USED-FOR models. models USED-FOR QA interaction. hard attention mechanism USED-FOR QA interaction. model COMPARE MATE model. MATE model COMPARE model. model COMPARE baselines. baselines COMPARE model. HYBRIDQA CONJUNCTION QASPER. QASPER CONJUNCTION HYBRIDQA. MATE model COMPARE baselines. baselines COMPARE MATE model. QASPER EVALUATE-FOR baselines. HYBRIDQA EVALUATE-FOR MATE model. QASPER EVALUATE-FOR model. HYBRIDQA EVALUATE-FOR model. model COMPARE baselines. baselines COMPARE model. multi - hop QA and conversational QA tasks EVALUATE-FOR model. multi - hop QA and conversational QA tasks EVALUATE-FOR baselines. expanded dataset EVALUATE-FOR baselines. expanded dataset EVALUATE-FOR model. OtherScientificTerm is sentence - level. ,This paper proposes an attention-based model for conversational and multi-hop QA tasks. The model is based on a BERT-like pre-trained LM. The key idea is to replace the weighted sum of sentences’ encodings with paragraph embeddings using dot product attention. The paragraph embedding is then combined with the context encodens to form the sentence-level. The authors also propose a hard attention mechanism to train models for QA interaction. Experiments on HYBRIDQA and QASPER show that the proposed model outperforms the original MATE model and several baselines on both the standard and the expanded dataset. The proposed model also outperforms baselines in both the multi-h hop QA and conversational QA experiments.
904,SP:9eadc19f7f712c488cf50d091f372092f6352930,"model USED-FOR complex question answering. multi - hop QA CONJUNCTION conversational QA. conversational QA CONJUNCTION multi - hop QA. DocHopper USED-FOR complex question answering. ETC FEATURE-OF hierarchical attentions. approach USED-FOR DocHopper. hierarchical attentions USED-FOR DocHopper. ETC USED-FOR DocHopper. QASPER CONJUNCTION HotpotQA. HotpotQA CONJUNCTION QASPER. ShARC CONJUNCTION QASPER. QASPER CONJUNCTION ShARC. HotpotQA CONJUNCTION HybridQA. HybridQA CONJUNCTION HotpotQA. datasets EVALUATE-FOR DocHopper. ShARC HYPONYM-OF datasets. HybridQA HYPONYM-OF datasets. QASPER HYPONYM-OF datasets. HotpotQA HYPONYM-OF datasets. datasets EVALUATE-FOR method. inference time EVALUATE-FOR method. computational cost EVALUATE-FOR method. Material is scientific documents. OtherScientificTerm are latent space, and re - encoding of queries. ","This paper proposes DocHopper, a model for complex question answering in the context of multi-hop QA and conversational QA, where the goal is to answer questions in scientific documents. The authors propose an approach called hierarchical attentions based on ETC to improve the efficiency of the hierarchical attention in the latent space, which is a key component of the approach used in DocHoop. The method is evaluated on three datasets: ShARC, QASPER, HotpotQA, and HybridQA. The proposed method achieves state-of-the-art performance in terms of inference time and computational cost without re-encoding of queries."
905,SP:9eadc19f7f712c488cf50d091f372092f6352930,"iterative approach USED-FOR multi - hop question answering. Generic are model, and datasets. OtherScientificTerm is query vector. Method is ETC encoder. ","This paper presents an iterative approach for multi-hop question answering. The model is trained on a set of datasets, where the query vector is sampled from the ETC encoder."
906,SP:9eadc19f7f712c488cf50d091f372092f6352930,MRC model ( DocHopper ) USED-FOR multi - hop QA. hierarchical attention mechanism USED-FOR DocHopper. local sentence vectors CONJUNCTION global context vector. global context vector CONJUNCTION local sentence vectors. embedding vectors PART-OF Hierarchical attention mechanism. global context vector HYPONYM-OF embedding vectors. local sentence vectors HYPONYM-OF embedding vectors. inference time EVALUATE-FOR method. question embedding USED-FOR method. computational efficiency EVALUATE-FOR model. QA CONJUNCTION multi - hop factual QA ( HotpotQA ). multi - hop factual QA ( HotpotQA ) CONJUNCTION QA. QA EVALUATE-FOR model. TableQA ( HybridQA ) CONJUNCTION QA. QA CONJUNCTION TableQA ( HybridQA ). conversational QA ( ShARC ) CONJUNCTION TableQA ( HybridQA ). TableQA ( HybridQA ) CONJUNCTION conversational QA ( ShARC ). multi - hop factual QA ( HotpotQA ) HYPONYM-OF datasets. conversational QA ( ShARC ) HYPONYM-OF datasets. QA HYPONYM-OF datasets. TableQA ( HybridQA ) HYPONYM-OF datasets. Generic is approaches. Method is ineffective modeling strategy. OtherScientificTerm is sentence / paragraph vectors. ,"This paper proposes a MRC model (DocHopper) for multi-hop QA. DocHopper uses a hierarchical attention mechanism that consists of three embedding vectors: local sentence vectors, global context vector, and sentence/paragraph vectors. The proposed method is based on question embedding and is able to reduce the inference time while maintaining computational efficiency. The model is evaluated on three datasets: conversational QA (ShARC), TableQA (HybridQA), QA and multi-hoop factual QA [1]. Compared to existing approaches, the authors show that the ineffective modeling strategy can be avoided."
907,SP:4e79b326bbda5d1509e88869dde9886764366d41,"approach USED-FOR dubbing / voice casting. method USED-FOR voice characteristics. voice characteristics USED-FOR dubbing / voice - casting. method USED-FOR dubbing / voice - casting. distance measure USED-FOR label refinement. k - means CONJUNCTION distance measure. distance measure CONJUNCTION k - means. Method are label refinement approach, and k - means clustering. Material is Skyrim. ",This paper proposes a label refinement approach that is based on the idea of k-means clustering. The proposed approach is an approach for dubbing/voice casting. The method aims to learn voice characteristics that can be used to improve dubbing / voice-casting. The authors propose to use k -means and a distance measure for label refinement. Experiments are conducted on Skyrim.
908,SP:4e79b326bbda5d1509e88869dde9886764366d41,voice similarity system task USED-FOR voice casting problem. clusters USED-FOR voice embedding network. voice character label USED-FOR voice embedding network. pseudo labeling CONJUNCTION embedding feature clustering. embedding feature clustering CONJUNCTION pseudo labeling. pseudo labeling HYPONYM-OF machine learning community. OtherScientificTerm is embedding features. Generic is method. ,"This paper tackles the voice casting problem using the voice similarity system task. The key idea is to use clusters to train a voice embedding network with a voice character label. The idea is inspired by the recent work in the machine learning community, such as pseudo labeling and embedding feature clustering. The main contribution of this paper is to propose a method to learn the embedding features."
909,SP:4e79b326bbda5d1509e88869dde9886764366d41,"finding similar - sound voices USED-FOR voice - dubbing. prior representation USED-FOR similarity between characters. p - vectors HYPONYM-OF prior representation. p - vectors USED-FOR method. k - means USED-FOR p - vectors. k - means USED-FOR method. video games Mass - Effect 3 CONJUNCTION Skyrim. Skyrim CONJUNCTION video games Mass - Effect 3. Skyrim FEATURE-OF English - French pairs of voice data. video games Mass - Effect 3 FEATURE-OF English - French pairs of voice data. method COMPARE system. system COMPARE method. accuracy EVALUATE-FOR system. accuracy EVALUATE-FOR method. representations USED-FOR gender. Material are French dialog, and Skyrim data. Task is label refining. Method is p - vector system. Metric is purity - K. OtherScientificTerm is vocal characteristics. ","This paper focuses on the problem of finding similar-sound voices for voice-dubbing. The authors propose a method that uses p-vectors, a prior representation for similarity between characters, as k-means. The method is evaluated on English-French pairs of voice data from video games Mass-Empire 3 and Skyrim. The results show that the proposed method outperforms the original system in terms of accuracy. The main contribution of the paper is the label refining. The p-vector system is trained to maximize the purity-K, which is a measure of the similarity between French dialog and English dialog. The paper also shows that the learned representations can be used to infer gender, which can be useful for modeling different vocal characteristics. The experiments are conducted on Skyrim data."
910,SP:4e79b326bbda5d1509e88869dde9886764366d41,"voice talent's voice COMPARE character's voice characteristics. character's voice characteristics COMPARE voice talent's voice. Method is "" label - refining "" technique. Task is video games. Generic are method, and these. OtherScientificTerm is data - driven clusters. ","This paper proposes a ""label-refining"" technique for video games. The method is based on the observation that the voice talent's voice is not necessarily representative of the character's voice characteristics. The authors propose to use data-driven clusters to learn these."
911,SP:9c399331a3b4a55d7e1ff9298f82a38b75b4f87d,"denoising CONJUNCTION deraining. deraining CONJUNCTION denoising. deblocking CONJUNCTION denoising. denoising CONJUNCTION deblocking. deraining CONJUNCTION deblurring. deblurring CONJUNCTION deraining. neural networks USED-FOR image processing tasks. deblurring HYPONYM-OF image processing tasks. deblocking HYPONYM-OF image processing tasks. deraining HYPONYM-OF image processing tasks. denoising HYPONYM-OF image processing tasks. heads / tails CONJUNCTION transformer backbone. transformer backbone CONJUNCTION heads / tails. method USED-FOR neural network models. Vit backbone COMPARE CNN backbones. CNN backbones COMPARE Vit backbone. Method are neural network model, and Transformer based feature backbone. Generic are model, and tasks. Task are distributed / privacy - preserving methods, and multi - task vs. single - task setting. ","This paper proposes a neural network model that can be used in a multi-task setting where the model is trained on multiple tasks simultaneously. This is an important problem for distributed/privacy-preserving methods, as neural networks are widely used in image processing tasks such as deblocking, denoising, deraining, and deblurring. The paper proposes to use a Transformer based feature backbone, where the heads/tails and the transformer backbone are shared across tasks. The proposed method can be applied to a variety of neural network models and is shown to outperform existing CNN backbones. The main contribution of this paper is to propose a method for training a model that is able to scale to multi-tasks in a single-task vs. single-tasking setting. "
912,SP:9c399331a3b4a55d7e1ff9298f82a38b75b4f87d,"task - specific head CNN CONJUNCTION tail CNN. tail CNN CONJUNCTION task - specific head CNN. task - agnostic learning USED-FOR body. image processing tasks EVALUATE-FOR task - agnostic learning. task - agnostic learning USED-FOR Transformer body. task - agnostic learning COMPARE task - specific heads and tails. task - specific heads and tails COMPARE task - agnostic learning. Method are multi - task distributed learning framework, alternating training scheme, and task - specific learning. ","This paper proposes a multi-task distributed learning framework where the task-specific head CNN and the tail CNN are trained in parallel. The authors propose an alternating training scheme where the head and tail are trained separately, and the body is trained with task-agnostic learning. Experiments on image processing tasks demonstrate the effectiveness of the proposed task - agnostic learning for the Transformer body, compared to task - specific heads and tails. The main contribution of the paper is the introduction of the alternating learning scheme, which allows for the use of task-nostic learning instead of only learning the head."
913,SP:9c399331a3b4a55d7e1ff9298f82a38b75b4f87d,"architecture USED-FOR image processing tasks. head CONJUNCTION body. body CONJUNCTION head. body CONJUNCTION tail. tail CONJUNCTION body. parts PART-OF network. head HYPONYM-OF parts. tail HYPONYM-OF parts. body HYPONYM-OF parts. Head and tail parts USED-FOR tasks. task USED-FOR loss optimization. non - distributed models COMPARE FL and SL approaches. FL and SL approaches COMPARE non - distributed models. Method are federated learning ( FedAvg ), and task - agnostic manner. OtherScientificTerm is central server. ","This paper proposes a new architecture for image processing tasks based on federated learning (FedAvg). The network consists of three parts: a head, a body, and a tail. Head and tail parts are used for different tasks, and the central server is used for training. Each task is used in the loss optimization. Experiments show that non-distributed models outperform FL and SL approaches in a task-agnostic manner."
914,SP:9c399331a3b4a55d7e1ff9298f82a38b75b4f87d,distributed learning framework USED-FOR image processing applications. vision transformer USED-FOR distributed learning framework. It USED-FOR image restoration tasks. It USED-FOR keeping privacy. keeping privacy FEATURE-OF image restoration tasks. task - agnostic vision transformer USED-FOR universal representation. training strategy USED-FOR model. ,This paper proposes a distributed learning framework based on vision transformer for image processing applications. It addresses the problem of keeping privacy in image restoration tasks. The authors propose a task-agnostic vision transformer to learn universal representation. They also propose a training strategy to train the model.
915,SP:249a72ef4e9cf02221243428174bb749068af6b2,"driving simulator CONJUNCTION covid modeling. covid modeling CONJUNCTION driving simulator. covid modeling CONJUNCTION atari game. atari game CONJUNCTION covid modeling. driving simulator HYPONYM-OF settings. atari game HYPONYM-OF settings. covid modeling HYPONYM-OF settings. Task are reward - hacking, and reward hacking. Method is intelligent agents. OtherScientificTerm is state - space. ","This paper studies the problem of reward-hacking in the context of intelligent agents. In particular, the authors focus on the setting of reward hacking, where the agent has access to the state-space of the environment and is able to manipulate the environment in a way that is beneficial to the agent. The authors consider three settings: driving simulator, covid modeling, and atari game. "
916,SP:249a72ef4e9cf02221243428174bb749068af6b2,"Reward hacking USED-FOR tasks. agent capabilities USED-FOR reward hacking. model size CONJUNCTION training steps. training steps CONJUNCTION model size. training steps CONJUNCTION action space. action space CONJUNCTION training steps. training steps HYPONYM-OF capabilities. model size HYPONYM-OF capabilities. anomaly detection HYPONYM-OF strategy. Generic is policy. OtherScientificTerm are capability, phase transitions, and reward hacking behavior. Method is monitoring strategies. ","Reward hacking is an important problem for tasks that require agent capabilities to perform well. However, the current state of the art in reward hacking is limited due to limitations such as model size, training steps, and action space. This paper proposes a new strategy called anomaly detection, which aims to detect when a policy is acting in a way that is not consistent with the capability. The paper also proposes two monitoring strategies to identify when phase transitions are occurring in the reward hacking behavior."
917,SP:249a72ef4e9cf02221243428174bb749068af6b2,"environments USED-FOR reward hacking. misspecified rewards USED-FOR reward hacking. proxy reward CONJUNCTION low real reward. low real reward CONJUNCTION proxy reward. baseline USED-FOR phase transition. anomaly detection USED-FOR phase transition. baseline USED-FOR anomaly detection. OtherScientificTerm are reward misspecification, model capacity, and agents. ","This paper studies the problem of reward hacking in environments with misspecified rewards. The authors argue that reward misspecification can lead to a phase transition between a proxy reward and a low real reward, and that this phase transition can be accelerated by using a baseline for anomaly detection. They also argue that model capacity can be improved by increasing the number of agents."
918,SP:249a72ef4e9cf02221243428174bb749068af6b2,"agent capabilities USED-FOR reward hacking. misspecified rewards FEATURE-OF RL environments. model USED-FOR policies. anomaly detection task USED-FOR reward hacking problem. trusted model USED-FOR anomaly detection task. they USED-FOR tasks. baseline anomaly detectors USED-FOR tasks. OtherScientificTerm are misspecification, phase transitions, and agent capability. Task is anomaly detector's task. ","This paper studies the problem of reward hacking in RL environments with misspecified rewards, where the agent capabilities are not robust to misspecification. The authors consider the reward hacking problem as an anomaly detection task, where a trusted model is trained to generate policies that can be used to fool an anomaly detector's task. They show that they can generalize well to other tasks, and that they are able to generalize better than baseline anomaly detectors to new tasks with different phase transitions and different agent capability."
919,SP:1c8d06fe0b2a79d5d0c0f317692c2ee869d1cc0c,"$ \chi$-deformed exponential distribution USED-FOR f - TVO objective. right - Riemann sum HYPONYM-OF integral. f - TVO COMPARE f - VI counterparts. f - VI counterparts COMPARE f - TVO. Task is VI. Method is TVO. OtherScientificTerm are $ \chi$-path, TVOs, $ \chi$-geometry, f - divergance, ELBO, EUBO, and f - divergence. ","This paper studies the f-TVO objective with a $\chi$-deformed exponential distribution, which is a variant of the standard f-VI objective. The main contribution of this paper is to show that the $TVO$ objective can be viewed as an integral of the right-Riemann sum, where the $r$-path is the $\phi$-geometry, and the $f-divergance is the integral of $r$. This is an extension of the work of [1], which shows that the TVO can be seen as an equivalent of the ELBO (which is a special case of the EUBO). The main result is that the eigenvalue of TVO is a function of the $p(x,y)$-distance between $x$ and $y$, where $p$ is the number of TVOs. The paper also shows that if $p=1$, then f-Divergence can be expressed as the product of f-difference and f-vectors, and that f-deferance can be written as the sum of the f divergence and the \frac{1}{\sqrt{n}(x)$. The paper then shows that under certain assumptions on the $pi$-approximation of the TVOs and on the $\chi$, f-Violet TVO converges to a solution with probability $1/\epsilon$. Finally, the paper shows that for any $p$, if the $xi$-space is $n$-strongly convex, then the solution can be approximated by a right-riemann-sum of the $\frac{n-1}{n-2}(y)$, which is an integral similar to the one in [1]. The paper concludes by showing that under some conditions, f-tvO(1/n)$ is equivalent to f-VDO, where $n_1$ is a constant. "
920,SP:1c8d06fe0b2a79d5d0c0f317692c2ee869d1cc0c,bound USED-FOR variational inference. bound COMPARE ELBO. ELBO COMPARE bound. distribution metrics FEATURE-OF f - divergence. thermodynamic variational object PART-OF bound. f - divergence PART-OF bound. Task is Bayesian inference tasks. ,This paper proposes a new bound for variational inference. The new bound consists of a thermodynamic variational object and a f-divergence between two distribution metrics. The authors show that the new bound is better than the standard ELBO on a variety of Bayesian inference tasks.
921,SP:1c8d06fe0b2a79d5d0c0f317692c2ee869d1cc0c,RVB CONJUNCTION CUBO. CUBO CONJUNCTION RVB. CUBO CONJUNCTION ELBO. ELBO CONJUNCTION CUBO. unified framework PART-OF $ f$-divergence TVO. CUBO PART-OF $ f$-divergence TVO. generalized $ \chi$-exponenetial family CONJUNCTION integral TVO. integral TVO CONJUNCTION generalized $ \chi$-exponenetial family. $ \chi$-path FEATURE-OF integral TVO. optimization methods USED-FOR framework. OtherScientificTerm is $ f$-divergence. Method is $ f$-TVO. ,"This paper proposes a unified framework of $f$-divergence TVO, which combines RVB, CUBO, and ELBO. The main idea is to combine the generalized $chi$-exponenetial family with the integral TVO with a $\chi$-$-path. The framework is based on the optimization methods of [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89]. "
922,SP:1c8d06fe0b2a79d5d0c0f317692c2ee869d1cc0c,f - divergence variational inference CONJUNCTION thermodynamic variational objective. thermodynamic variational objective CONJUNCTION f - divergence variational inference. f - divergence variational inference PART-OF variational inference. thermodynamic variational objective PART-OF variational inference. exponential families USED-FOR TVO. Method is reparametrization trick. ,This paper proposes to combine f-divergence variational inference with a thermodynamic variational objective. The main contribution of this paper is the reparametrization trick. The TVO can be viewed as an extension of exponential families.
923,SP:d4ce49411198fe65b8f4c2d80af222e0732a4728,deep reinforcement algorithm USED-FOR continuous control tasks. Ensemble Deep Deterministic Policy Gradients ( ED2 ) USED-FOR continuous control tasks. Ensemble Deep Deterministic Policy Gradients ( ED2 ) HYPONYM-OF deep reinforcement algorithm. algorithm USED-FOR SotA. OtherScientificTerm is MuJoCo environments. ,This paper proposes a deep reinforcement algorithm called Ensemble Deep Deterministic Policy Gradients (ED2) for continuous control tasks. The algorithm is applied to SotA and is evaluated on MuJoCo environments.
924,SP:d4ce49411198fe65b8f4c2d80af222e0732a4728,"ensemble COMPARE multi - actor learners. multi - actor learners COMPARE ensemble. replay buffer USED-FOR actors. actor initialization COMPARE critic initialization. critic initialization COMPARE actor initialization. deterministic actors USED-FOR ED2. exploration COMPARE UCB - style exploration. UCB - style exploration COMPARE exploration. Method are ensemble - based actor - critic method, learner, streamlined off - policy ( SOP ) method, soft actor - critic ( SAC ), and Ensemble Deep Deterministic ( ED2 ) method. Task are policy optimization, and RL setup. OtherScientificTerm are entropy bonus, and exploration noise. ","This paper proposes an ensemble-based actor-critic method for policy optimization. Unlike multi-actor learners, the ensemble is trained in an unsupervised manner. The learner is trained using a streamlined off-policy (SOP) method, where the actors are sampled from a replay buffer. The actor initialization is similar to the critic initialization, but with an additional entropy bonus. The authors also propose a soft actor - critic (SAC), which is a generalization of the Ensemble Deep Deterministic (ED2) method. ED2 uses deterministic actors, which is an RL setup where exploration is performed in a way similar to UCB-style exploration, but without exploration noise."
925,SP:d4ce49411198fe65b8f4c2d80af222e0732a4728,design choice USED-FOR off - policy Deep RL algorithms. off - policy Deep RL algorithms USED-FOR continuous control settings. update frequency CONJUNCTION precision. precision CONJUNCTION update frequency. initialization choices CONJUNCTION update frequency. update frequency CONJUNCTION initialization choices. additive exploration noise CONJUNCTION initialization choices. initialization choices CONJUNCTION additive exploration noise. precision USED-FOR retraining. design choices USED-FOR ED2 - an ensemble method. Material is Mujoco benchmarks. ,"This paper studies the design choice for off-policy Deep RL algorithms in continuous control settings. The authors consider additive exploration noise, initialization choices, update frequency, and precision for retraining. They propose ED2-an ensemble method that makes use of these design choices. They conduct extensive experiments on Mujoco benchmarks and show promising results."
926,SP:d4ce49411198fe65b8f4c2d80af222e0732a4728,"additive action noise USED-FOR exploration. initialization methods USED-FOR actors. initialization of critics COMPARE initialization methods. initialization methods COMPARE initialization of critics. learning EVALUATE-FOR initialization of critics. Method are ensemble deep reinforcement learning, and ensemble reinforce learning algorithms. ","This paper studies ensemble deep reinforcement learning, where the goal is to improve the performance of ensemble reinforce learning algorithms by adding additive action noise to encourage exploration. The authors compare the initialization of critics with different initialization methods for different actors, and show that the resulting learning is better than the original initialization methods."
927,SP:21819b54433fa274657d9fe418f66407eee83eeb,Equalized Loss ( EL ) fairness notion USED-FOR fair supervised learning. algorithms USED-FOR global ( sub-)optimal solution. convex ( constrained ) optimizations USED-FOR algorithms. Task is non - convex constrained optimization problem. ,This paper studies the Equalized Loss (EL) fairness notion for fair supervised learning. The main contribution of this paper is to study the non-convex constrained optimization problem. The authors propose two algorithms for finding a global (sub-)optimal solution based on convex (constrained) optimizations.
928,SP:21819b54433fa274657d9fe418f66407eee83eeb,"bounded loss USED-FOR minimization of convex losses. formulation COMPARE formulation. formulation COMPARE formulation. monotonicity property USED-FOR convex constrained optimizations. convex constrained optimizations USED-FOR "" EL "" fair predictor. approximate algorithm USED-FOR EL fair predictor. OtherScientificTerm are bounded difference of losses, and optima. ","This paper studies the minimization of convex losses with bounded loss. The main contribution of this paper is the formulation of the ""EL"" fair predictor, which is based on convex constrained optimizations that satisfy the monotonicity property. The authors provide an approximate algorithm for the EL fair predictor and show that under bounded difference of losses, the proposed formulation converges faster than the original formulation. Moreover, the authors show that the proposed optima are monotone."
929,SP:21819b54433fa274657d9fe418f66407eee83eeb,globally optimal predictor USED-FOR EL. approaches USED-FOR globally optimal predictor. Equalized Loss ( EL ) FEATURE-OF fair prediction. convex constrained optimization problems USED-FOR non - convex problem. unconstrained convex optimization USED-FOR problem. ,This paper studies the problem of Equalized Loss (EL) for fair prediction. The authors propose two approaches to find a globally optimal predictor for EL. The first approach is to solve a non-convex problem with convex constrained optimization problems. The second approach is unconstrained convex optimization.
930,SP:21819b54433fa274657d9fe418f66407eee83eeb,fairness constraints FEATURE-OF supervised learning models. fairness constraints USED-FOR ( convex ) loss minimization problem. algorithms USED-FOR problem. global optimality FEATURE-OF problem. real - world data EVALUATE-FOR algorithms. OtherScientificTerm is equalized loss fairness constraint. ,"This paper studies the fairness constraints of supervised learning models. Specifically, the authors consider the (convex) loss minimization problem with fairness constraints. The authors propose two algorithms to solve this problem with global optimality under the equalized loss fairness constraint. The proposed algorithms are evaluated on real-world data."
931,SP:336c1b8a7f293a78dfab18e7b454b0ec39822293,meaningful learning CONJUNCTION semantic linking. semantic linking CONJUNCTION meaningful learning. inductive learning perspective CONJUNCTION deductive learning perspective. deductive learning perspective CONJUNCTION inductive learning perspective. inductive learning perspective USED-FOR data augmentation methods. deductive learning perspective USED-FOR data augmentation methods. augmented data USED-FOR model variants. SCAN CONJUNCTION GEO. GEO CONJUNCTION SCAN. GEO CONJUNCTION ADV. ADV CONJUNCTION GEO. machine translation task CONJUNCTION semantic parsing task. semantic parsing task CONJUNCTION machine translation task. data augmentation methods USED-FOR semantic parsing task. data augmentation methods USED-FOR machine translation task. data augmentation methods USED-FOR inductive or the deductive category. real data EVALUATE-FOR augmentation methods. OtherScientificTerm is systematic generalization ability. Generic is models. ,"This paper studies data augmentation methods from an inductive learning perspective and a deductivelearning perspective, which aims to bridge the gap between meaningful learning and semantic linking. The main contribution of this paper is to study the effect of augmented data on model variants that are trained on augmented data. The authors show that the augmented data improves the systematic generalization ability of the models, and that the inductive or the deductive category can be further improved by using the augmentation method. Experiments are conducted on a machine translation task, a semantic parsing task, and on real data from SCAN, GEO, and ADV."
932,SP:336c1b8a7f293a78dfab18e7b454b0ec39822293,meaningful learning USED-FOR systematic generalization ability. inductive learning CONJUNCTION deductive learning. deductive learning CONJUNCTION inductive learning. augmented data USED-FOR inductive learning. augmented data USED-FOR deductive learning. augmented data USED-FOR sequence - to - sequence model. Material is real data. ,This paper studies the problem of meaningful learning to improve systematic generalization ability. The authors propose to combine inductive learning with deductive learning using augmented data. The augmented data is used to train a sequence-to-sequence model. Experiments are conducted on real data.
933,SP:336c1b8a7f293a78dfab18e7b454b0ec39822293,Task is learning novel words. Generic is approach. Method is meaningful learning. OtherScientificTerm is domain - specific rules. ,"This paper addresses the problem of learning novel words. The authors propose an approach called meaningful learning, where the goal is to learn a set of domain-specific rules that can be used to guide the learning of new words."
934,SP:336c1b8a7f293a78dfab18e7b454b0ec39822293,semantic linking USED-FOR systematic generalization. meaningful learning perspective USED-FOR inductive and deductive learning. inductive and deductive learning USED-FOR semantic linking. prior knowledge CONJUNCTION semantic linking. semantic linking CONJUNCTION prior knowledge. semantic linking USED-FOR systematic generalization. prior knowledge USED-FOR systematic generalization. SCAN CONJUNCTION real data. real data CONJUNCTION SCAN. Method is meaningful learning theory. ,"This paper studies the problem of systematic generalization with prior knowledge and semantic linking from a meaningful learning perspective in both inductive and deductive learning. The paper is well-motivated by meaningful learning theory, and the empirical results on SCAN and real data are convincing."
935,SP:5d758b9125e716c92dde5cfcc8aad67adbd30ba0,lifting scheme USED-FOR method. wavelet decomposition USED-FOR lifting scheme. method USED-FOR down - sampled approximation C. layer USED-FOR multiscale pyramid. graph convolution networks CONJUNCTION transformers. transformers CONJUNCTION graph convolution networks. lifting scheme USED-FOR point cloud processing. transformers USED-FOR lifting scheme. graph convolution networks USED-FOR lifting scheme. method COMPARE state - of - the - art baselines. state - of - the - art baselines COMPARE method. Method is 3D point cloud representation learning framework. Generic is scheme. ,"This paper proposes a new 3D point cloud representation learning framework. The proposed method uses a lifting scheme based on wavelet decomposition. The method learns a down-sampled approximation C, which is then used as a layer in a multiscale pyramid. The authors show that the proposed lifting scheme can be applied to point cloud processing using graph convolution networks and transformers. The experimental results demonstrate the effectiveness of the proposed scheme. In addition, the proposed method outperforms state-of-the-art baselines."
936,SP:5d758b9125e716c92dde5cfcc8aad67adbd30ba0,multi - scale wavelet decomposition USED-FOR 3D shape representation learning method. neural network architecture USED-FOR 3D shapes. neural network architecture USED-FOR sub - bands components. sub - bands components PART-OF 3D shapes. lifting scheme USED-FOR second - generation wavelets. model USED-FOR it. adaptive lifting scheme USED-FOR lifting scheme. adaptive lifting scheme USED-FOR model. adaptive lifting scheme USED-FOR it. transformer models USED-FOR coarse and approximate geometry. transformer models USED-FOR 3D shape. coarse and approximate geometry FEATURE-OF 3D shape. ModelNet40 CONJUNCTION ScanObjectNN dataset. ScanObjectNN dataset CONJUNCTION ModelNet40. ModelNet40 USED-FOR shape classification task. part segmentation task EVALUATE-FOR model. shape classification task EVALUATE-FOR model. ScanObjectNN dataset EVALUATE-FOR model. ShapeNet Part dataset USED-FOR part segmentation task. adaptive lifting scheme USED-FOR learning. adaptive lifting scheme USED-FOR shape representation learning. ,This paper proposes a 3D shape representation learning method based on multi-scale wavelet decomposition. The authors propose a neural network architecture to decompose 3D shapes into sub-bands components. The model is trained using an adaptive lifting scheme to modify the second-generation wavelets so that it can be learned with the same model. The paper also proposes to use transformer models to learn the coarse and approximate geometry of the 3D form. The proposed model is evaluated on a shape classification task on ModelNet40 and ScanObjectNN dataset and on a part segmentation task on ShapeNet Part dataset. The adaptive lifting schemes is shown to improve the learning performance.
937,SP:5d758b9125e716c92dde5cfcc8aad67adbd30ba0,framework USED-FOR 3D shape representation learning. multi - scale wavelet decomposition USED-FOR framework. AWT - Net HYPONYM-OF transformer - based neural network. ,"This paper proposes a framework for 3D shape representation learning based on multi-scale wavelet decomposition. The main contribution of this paper is the proposed AWT-Net, a transformer-based neural network."
938,SP:5d758b9125e716c92dde5cfcc8aad67adbd30ba0,deep neural network architecture USED-FOR 3D point cloud representation learning. wavelet decomposition USED-FOR deep neural network architecture. non - linearity PART-OF wavelet. non - linearity PART-OF data - driven adaptive lifting scheme. network USED-FOR holistic and complementary geometry of 3D shapes. wavelet transform and Transformers USED-FOR network. network USED-FOR neighboring local information. shape classification CONJUNCTION part segmentation. part segmentation CONJUNCTION shape classification. benchmarks EVALUATE-FOR it. part segmentation HYPONYM-OF benchmarks. shape classification HYPONYM-OF benchmarks. ,This paper proposes a deep neural network architecture for 3D point cloud representation learning based on wavelet decomposition. The authors propose a data-driven adaptive lifting scheme that incorporates non-linearity in the wavelet. The proposed network is inspired by wavelet transform and Transformers and is able to capture holistic and complementary geometry of 3D shapes. The network is also able to extract neighboring local information. Experiments show that it can achieve state-of-the-art performance on standard benchmarks such as shape classification and part segmentation.
939,SP:b89c04e2f8e94c7d0c3686edac835a86fab2d528,method USED-FOR natural language generation tasks. cocktail fine - tuning USED-FOR natural language generation tasks. adapter - finetuning CONJUNCTION full - finetuning. full - finetuning CONJUNCTION adapter - finetuning. In - domain data CONJUNCTION Out - of - domain data. Out - of - domain data CONJUNCTION In - domain data. cocktail fine - tuning USED-FOR Out - of - domain data. cocktail fine - tuning USED-FOR In - domain data. full - finetuning COMPARE ensembles. ensembles COMPARE full - finetuning. cocktail fine - tuning COMPARE ensembles. ensembles COMPARE cocktail fine - tuning. full - finetuning USED-FOR cocktail fine - tuning. adapter - finetuning USED-FOR cocktail fine - tuning. knowledge distillation USED-FOR full - finetuning. knowledge distillation USED-FOR adapter - finetuning. Method is multi - class logistic regression. Generic is it. ,"This paper proposes a method called cocktail fine-tuning for natural language generation tasks. The method is based on multi-class logistic regression, where each class is sampled from a pool of classes and it is used to fine-tune a subset of the classes. In-domain data and Out-of-domains data are used for cocktail fine -tuning, which is a combination of adapter-finetuning with knowledge distillation, and full- finetuning. Experiments show that full-furthering ensembles is better than full-finetuning."
940,SP:b89c04e2f8e94c7d0c3686edac835a86fab2d528,"full fine - tuning model CONJUNCTION parameter - efficient fine - tuning model. parameter - efficient fine - tuning model CONJUNCTION full fine - tuning model. ensemble model USED-FOR out - of - distribution ( OOD ). ensemble model USED-FOR full fine - tuning model. out - of - distribution ( OOD ) EVALUATE-FOR full fine - tuning model. full fine - tuning model USED-FOR ensemble model. full fine - tuning model COMPARE parameter - efficient finetuning model. parameter - efficient finetuning model COMPARE full fine - tuning model. in - distribution ( ID ) performance EVALUATE-FOR full fine - tuning model. linear interpolation CONJUNCTION distill. distill CONJUNCTION linear interpolation. ID training data USED-FOR parameter - efficient model. distill HYPONYM-OF ensembling methods. linear interpolation HYPONYM-OF ensembling methods. ensemble method USED-FOR OOD. Generic are method, and models. ",This paper proposes an ensemble model for out-of-distribution (OOD) using a full fine-tuning model and a parameter-efficient finetuning model. The method is based on the observation that the full fine tuning model can achieve better in-discriminative (ID) performance than the parameter-efficiency model trained on ID training data. The authors then propose two ensembling methods: linear interpolation and distill. The experiments show that the proposed ensemble method is effective for OOD and that the learned models can generalize to unseen datasets.
941,SP:b89c04e2f8e94c7d0c3686edac835a86fab2d528,lightweight fine - tuning CONJUNCTION full fine - tuning. full fine - tuning CONJUNCTION lightweight fine - tuning. full fine - tuning USED-FOR approaches. lightweight fine - tuning USED-FOR approaches. fine - tuning methods PART-OF model. ensemble method CONJUNCTION cocktail fine - tuning. cocktail fine - tuning CONJUNCTION ensemble method. cocktail fine - tuning HYPONYM-OF fine - tuning methods. ensemble method HYPONYM-OF approaches. cocktail fine - tuning HYPONYM-OF approaches. XSUM CONJUNCTION OpenQA. OpenQA CONJUNCTION XSUM. WebNLG CONJUNCTION XSUM. XSUM CONJUNCTION WebNLG. datasets EVALUATE-FOR tasks. WebNLG HYPONYM-OF datasets. OpenQA HYPONYM-OF datasets. XSUM HYPONYM-OF datasets. ,"This paper proposes to combine two approaches: lightweight fine-tuning and full fine-tuning. The authors propose to incorporate two different fine-fine-tune methods into the model: the ensemble method and cocktail fine -tuning. They evaluate the proposed tasks on three datasets: WebNLG, XSUM and OpenQA."
942,SP:b89c04e2f8e94c7d0c3686edac835a86fab2d528,out - of - domain data CONJUNCTION in - domain data. in - domain data CONJUNCTION out - of - domain data. cocktail fine - tuning USED-FOR full fine - tuning. lightweight model USED-FOR distillation. distillation USED-FOR full fine - tuning. toy model USED-FOR cocktail fine - tuning. Generic is model. Task is NLG tasks. Method is fine - tuning schema. ,"This paper proposes a lightweight model for distillation, which can be used for full fine-tuning on both out-of-domain data as well as in-domains data. The model is trained on NLG tasks, and the authors show that the toy model can perform well for cocktail fine-tuning. The authors also propose a fine-fine tuning schema that can be applied to other tasks."
943,SP:28fe2b3deb6a8f24f26d48240da38d280673b8f2,WARM method USED-FOR iterative and interactive weakly - supervised learning. Active learning USED-FOR labeling functions. gradient propagation USED-FOR LF parameters. gradient propagation USED-FOR DP model. LF parameters CONJUNCTION DP model. DP model CONJUNCTION LF parameters. ,This paper proposes a novel WARM method for iterative and interactive weakly-supervised learning. Active learning is used to learn labeling functions. The main idea is to use gradient propagation to update the LF parameters and the DP model. The experiments show the effectiveness of the proposed method.
944,SP:28fe2b3deb6a8f24f26d48240da38d280673b8f2,active learning approach USED-FOR weakly / programmatically supervised learning. WARM HYPONYM-OF active learning approach. Snorkel framework USED-FOR weak supervision. data programming / Snorkel paradigm USED-FOR weak supervision. labeling functions USED-FOR models. Snorkel framework USED-FOR labeling functions. data programming / Snorkel paradigm USED-FOR WARM approach. active learning approach USED-FOR sampling labeled data points. active learning approach USED-FOR LFs. medical datasets EVALUATE-FOR approach. Method is labeling functions ( LFs ). Generic is WARM setup. ,"This paper presents WARM, an active learning approach for weakly/programmatically supervised learning. The WARM approach is based on the data programming/Snorkel paradigm to provide weak supervision for labeling functions (LFs) in models. The main contribution of the paper is the proposed WARM setup. The proposed active learning approaches for sampling labeled data points can be used to train LFs. The approach is evaluated on three medical datasets."
945,SP:28fe2b3deb6a8f24f26d48240da38d280673b8f2,"algorithm USED-FOR labeling function model. method USED-FOR labeling function parameters. active labeling approaches USED-FOR weak supervision. method COMPARE active labeling approaches. active labeling approaches COMPARE method. OtherScientificTerm are labeling functions, soft "" labeling functions, and threshold. ","This paper proposes an algorithm for learning a labeling function model that is robust to changes in the labeling functions. The proposed method learns the labeling function parameters in a way that is similar to active labeling approaches for weak supervision. The main difference is that ""soft"" labeling functions are learned by minimizing a threshold."
946,SP:28fe2b3deb6a8f24f26d48240da38d280673b8f2,method USED-FOR label model. weak supervision USED-FOR label model. weighted combination of labelling functions USED-FOR weak supervision. weak supervision CONJUNCTION active learning. active learning CONJUNCTION weak supervision. approach COMPARE active learning approach. active learning approach COMPARE approach. approach COMPARE baselines. baselines COMPARE approach. approach USED-FOR model. active learning approach CONJUNCTION baselines. baselines CONJUNCTION active learning approach. real world datasets EVALUATE-FOR approach. medical domain FEATURE-OF real world datasets. weak supervision USED-FOR baselines. active learning USED-FOR baselines. active learning approach USED-FOR model. approach COMPARE fully supervised model. fully supervised model COMPARE approach. accuracy EVALUATE-FOR fully supervised model. accuracy EVALUATE-FOR approach. OtherScientificTerm is labelling functions. Method is weak supervision model. ,This paper proposes a method to train a label model with weak supervision using a weighted combination of labelling functions. The proposed approach is evaluated on real world datasets from the medical domain and compared to an active learning approach and two baselines trained with both weak supervision and active learning. The results show that the proposed approach can improve the accuracy of the model compared to a fully supervised model. 
947,SP:f663a1e64155f1d4c890a6fefae596f67ef3cb11,"ERM - based method USED-FOR classification task. group annotated training data USED-FOR ERM - based method. group annotated training data USED-FOR classification task. Group - DRO HYPONYM-OF method. Metric are regularized loss, average training loss, and convergence. ","This paper proposes an ERM-based method for classification task with group annotated training data. The proposed method, Group-DRO, is based on the idea of regularized loss, which is defined as the difference between the average training loss and the true loss. The authors provide theoretical analysis on convergence."
948,SP:f663a1e64155f1d4c890a6fefae596f67ef3cb11,"classifier USED-FOR group with worst loss. benchmarks EVALUATE-FOR setup. synthetic toy cases EVALUATE-FOR approach. Generic is algorithm. OtherScientificTerm is test distribution. Method are group - DRO, and distributionally robust optimization. ","This paper proposes a new algorithm for estimating the worst loss of a test distribution. The proposed approach, called group-DRO, is based on the idea of distributionally robust optimization, where a classifier is trained to predict the group with worst loss. The authors evaluate the proposed setup on a variety of benchmarks, as well as on synthetic toy cases."
949,SP:f663a1e64155f1d4c890a6fefae596f67ef3cb11,"method USED-FOR robust ML. it USED-FOR first - order - stationary points. algorithm USED-FOR loss function. algorithm COMPARE baselines. baselines COMPARE algorithm. WILDS Robust ML benchmark EVALUATE-FOR algorithm. synthetic datasets CONJUNCTION WILDS Robust ML benchmark. WILDS Robust ML benchmark CONJUNCTION synthetic datasets. WILDS Robust ML benchmark EVALUATE-FOR baselines. approach USED-FOR robust ML. gradient descent USED-FOR approach. synthetic and real - world datasets EVALUATE-FOR method. group robustness EVALUATE-FOR method. OtherScientificTerm are distribution shifts, worst group error, and average error. Metric is average training error. ","This paper proposes a method for robust ML that is robust to distribution shifts. Specifically, it aims to find first-order-stationary points where the worst group error is smaller than the average training error. The proposed algorithm learns a loss function that minimizes the difference between the worst and the average error. Experiments on both synthetic and real-world datasets show that the proposed algorithm outperforms the baselines on the WILDS Robust ML benchmark. In addition, the proposed method is evaluated on group robustness using gradient descent."
950,SP:f663a1e64155f1d4c890a6fefae596f67ef3cb11,"sub - population shifts CONJUNCTION domain adaptation. domain adaptation CONJUNCTION sub - population shifts. Generic are method, and benchmark dataset. ",This paper proposes a method for learning to adapt to sub-population shifts and domain adaptation. The method is well motivated and well motivated. The experiments are conducted on a benchmark dataset and show promising results.
951,SP:faad5fe1eefbcc2e24638383d0bde7ad7975ff4e,"Shapley value USED-FOR model predictions. combinatorial features USED-FOR prediction. bivariate Shapley explanation map HYPONYM-OF matrix. matrix USED-FOR feature interaction. least / most influential features CONJUNCTION directional / mutual redundancy. directional / mutual redundancy CONJUNCTION least / most influential features. directional / mutual redundancy HYPONYM-OF definition. least / most influential features HYPONYM-OF interactions. directional / mutual redundancy HYPONYM-OF interactions. definition USED-FOR interactions. method USED-FOR feature interactions. OtherScientificTerm are importance, and features. ",This paper proposes to use Shapley value as a measure of importance for model predictions. The main idea is to use combinatorial features for the prediction and use a matrix called the bivariate Shapley explanation map to measure the importance of each feature interaction. The authors propose a new definition of interactions such as least/most influential features and directional/mutual redundancy. The proposed method can be used to measure feature interactions in a variety of settings. The experiments show that the proposed method is able to distinguish between features that are important and those that are not.
952,SP:faad5fe1eefbcc2e24638383d0bde7ad7975ff4e,"bivariate feature - explanation map USED-FOR asymmetrical ( directional ) feature interactions. symmetrical feature interactions USED-FOR local - feature - interaction explainers. univariate feature - based explanation method USED-FOR approach. method USED-FOR mutual and directional redundancies. image, text, and tabular data EVALUATE-FOR method. OtherScientificTerm is mutual and directional redundancy. ","This paper proposes a bivariate feature-explanation map to handle asymmetrical (directional) feature interactions. The proposed approach is based on an univariate feature–based explanation method, which can handle symmetrical feature interactions in local-feature-interaction explainers. The method is shown to be robust to mutual and directional redundancies on image, text, and tabular data."
953,SP:faad5fe1eefbcc2e24638383d0bde7ad7975ff4e,directional feature interactions USED-FOR deep models. method HYPONYM-OF graph - based explainer. Bivariate Shapley values USED-FOR directional feature interactions. it USED-FOR directional feature interactions. it USED-FOR Bivariate Shapley values. OtherScientificTerm is graphs. ,"This paper proposes a method for graph-based explainer, i.e., a method that can be applied to graphs. Specifically, it uses Bivariate Shapley values to model directional feature interactions in deep models. "
954,SP:faad5fe1eefbcc2e24638383d0bde7ad7975ff4e,univariate Shapley method USED-FOR bivariate Shapley method. asymmetric bivariate Shapley value USED-FOR graph. graph algorithms USED-FOR graph. graph algorithms USED-FOR univariate feature importance. univariate feature importance CONJUNCTION relations. relations CONJUNCTION univariate feature importance. graph algorithms USED-FOR relations. univariate feature importance PART-OF univariate approach. datasets EVALUATE-FOR method. Method is bivariate approaches. ,This paper proposes an univariate Shapley method that can be used to replace the bivariate Shaperley method. The main idea is to use an asymmetric bivariate ShapLEY value to represent the graph. The univariate approach combines the univariate feature importance of graph algorithms to model the graph and relations. The proposed method is evaluated on two datasets and compared to other bivariate approaches.
955,SP:c8a4254e6fc2d2e7d1d41a76bb64f78f22a8639d,interpretable model USED-FOR policy. POETREE USED-FOR interpretable model. decision trees USED-FOR POETREE. decision trees USED-FOR interpretable model. POETREE USED-FOR decision tree. time series data USED-FOR POETREE. time series data USED-FOR decision tree. tree HYPONYM-OF soft - probabilistic model. interpretability FEATURE-OF tree. distribution modeling CONJUNCTION interpretability. interpretability CONJUNCTION distribution modeling. interpretability CONJUNCTION policy learning. policy learning CONJUNCTION interpretability. POETREE COMPARE baselines. baselines COMPARE POETREE. distribution modeling EVALUATE-FOR baselines. policy learning EVALUATE-FOR baselines. interpretability EVALUATE-FOR baselines. policy learning EVALUATE-FOR POETREE. distribution modeling EVALUATE-FOR POETREE. interpretability EVALUATE-FOR POETREE. Material is healthcare domain. Generic is model. Method is POMDP. ,"This paper proposes POETREE, an interpretable model for learning a policy using decision trees. The authors use time series data to train a decision tree that can be used as input to POETRE. The tree is a soft-probabilistic model, which has been shown to be interpretable in the healthcare domain. The model is trained on POMDP and is evaluated on three datasets. The experiments on distribution modeling, interpretability of the tree, and policy learning are conducted to show the effectiveness of the proposed model. The experimental results show that the performance of the trained POetREE outperforms the baselines in terms of both distribution modeling and interpretability."
956,SP:c8a4254e6fc2d2e7d1d41a76bb64f78f22a8639d,interpretability FEATURE-OF methods. ( soft ) tree - based method USED-FOR synthetic clinical datasets. interpretability FEATURE-OF ( soft ) tree - based method. partially observable Markov Decision Process ( POMDP ) USED-FOR clinical decision process. Task is clinical decision - making. OtherScientificTerm is medical diagnosis. ,This paper studies the interpretability of methods in the context of clinical decision-making. The authors propose a partially observable Markov Decision Process (POMDP) to model the clinical decision process in the setting of medical diagnosis. The proposed (soft) tree-based method is applied to synthetic clinical datasets to improve interpretability.
957,SP:c8a4254e6fc2d2e7d1d41a76bb64f78f22a8639d,approach USED-FOR learning and representing human decision - making policies. observed behavioral data USED-FOR approach. interpretability FEATURE-OF approach. canonical decision tree approaches USED-FOR probabilistic setting. decision tree model USED-FOR probabilistic setting. decision tree model USED-FOR optimization of leaf - specific parameters. canonical decision tree approaches USED-FOR decision tree model. stochastic gradient descent USED-FOR optimization of leaf - specific parameters. interpretability CONJUNCTION accuracy. accuracy CONJUNCTION interpretability. interpretability CONJUNCTION subjective measurements. subjective measurements CONJUNCTION interpretability. subjective measurements EVALUATE-FOR approach. accuracy EVALUATE-FOR approach. interpretability EVALUATE-FOR approach. synthetic and real - world datasets EVALUATE-FOR approach. Metric is modeling accuracy. ,"This paper presents an approach for learning and representing human decision-making policies from observed behavioral data. The proposed approach aims to improve the interpretability and accuracy of the learned policies. The authors propose a decision tree model that extends canonical decision tree approaches to a probabilistic setting, which allows for efficient optimization of leaf-specific parameters via stochastic gradient descent. The approach is evaluated on both synthetic and real-world datasets, and the authors demonstrate improved interpretability, accuracy, and subjective measurements. The paper also provides a theoretical analysis of the modeling accuracy."
958,SP:c8a4254e6fc2d2e7d1d41a76bb64f78f22a8639d,"method USED-FOR stationary ) interpretable policies. partially observed settings FEATURE-OF soft decision trees. soft decision trees USED-FOR method. soft decision tree structure USED-FOR policy decisions. soft decision tree structure USED-FOR recursion over time. history of collected data USED-FOR policy decisions. algorithm USED-FOR soft decision tree. algorithm USED-FOR structure / topology of the tree. pruning low probability paths PART-OF trees. global update step USED-FOR topology. global update step USED-FOR pruning low probability paths. Method are probability representation of the soft node, and local optimization. OtherScientificTerm are soft node, and clinician policy. ","This paper proposes a method for learning (stationary) interpretable policies from soft decision trees in partially observed settings. The key idea is to learn a soft decision tree structure that allows for recursion over time, and to use the history of collected data to guide policy decisions. The algorithm first learns a soft policy tree, and then learns the structure/topology of the tree using a global update step that prunes low probability paths in the trees based on the probability representation of the soft node. This local optimization is repeated until convergence to a clinician policy."
959,SP:5630707c9d0d9e21fce2efddef874e373bfed026,accuracy EVALUATE-FOR augmentation method. algorithm USED-FOR grid - wise patches. preserved semantic information USED-FOR algorithm. preserved semantic information USED-FOR grid - wise patches. unified reward function USED-FOR MARL algorithm. MARL algorithm USED-FOR algorithm. unified reward function USED-FOR algorithm. training speed EVALUATE-FOR auto - augmentation methods. image classification CONJUNCTION fine - grained image recognition tasks. fine - grained image recognition tasks CONJUNCTION image classification. image classification EVALUATE-FOR algorithm. fine - grained image recognition tasks EVALUATE-FOR algorithm. Task is image - level augmentation. OtherScientificTerm is semantic information. Generic is it. ,"This paper studies the problem of image-level augmentation, where the goal is to improve the accuracy of an augmentation method while preserving the semantic information. The authors propose an algorithm that uses preserved semantic information to generate grid-wise patches using the MARL algorithm with a unified reward function. The proposed algorithm is evaluated on image classification and fine-grained image recognition tasks, and it is shown that it can improve the training speed of auto-augmentation methods."
960,SP:5630707c9d0d9e21fce2efddef874e373bfed026,multi - agent reinforcement learning problem USED-FOR approach. image classification datasets EVALUATE-FOR approach. Method is automatic data augmentation approach. ,This paper proposes an automatic data augmentation approach. The approach is based on a multi-agent reinforcement learning problem. The proposed approach is evaluated on image classification datasets.
961,SP:5630707c9d0d9e21fce2efddef874e373bfed026,"Patch AutoAugment ( PAA ) HYPONYM-OF fine - grained automated data augmentation approach. multi - agent reinforcement learning algorithm USED-FOR optimal augmentation policies. PAA USED-FOR task. multi - agent reinforcement learning problem USED-FOR PAA. multi - agent reinforcement learning problem USED-FOR task. multi - agent reinforcement learning algorithm USED-FOR PAA. CUB-200 - 2011 CONJUNCTION Stanford Cars. Stanford Cars CONJUNCTION CUB-200 - 2011. CIFAR-100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-100. Stanford Cars CONJUNCTION FGVC - Aircraft. FGVC - Aircraft CONJUNCTION Stanford Cars. ImageNet CONJUNCTION CUB-200 - 2011. CUB-200 - 2011 CONJUNCTION ImageNet. PAA USED-FOR class - related cues. OtherScientificTerm are diversity in local regions, grid of patches, and joint optimal augmentation policies. Generic is method. ","This paper proposes a fine-grained automated data augmentation approach called Patch AutoAugment (PAA). PAA is a multi-agent reinforcement learning algorithm that learns optimal augmentation policies for a given task based on the diversity in local regions. The key idea is to learn a grid of patches, which can then be used to train joint optimal augmentation policies. Experiments on CIFAR-100, ImageNet, CUB-200-2011, Stanford Cars, and FGVC-Aircraft demonstrate the effectiveness of the proposed method. In addition, PAA can also learn class-related cues."
962,SP:5630707c9d0d9e21fce2efddef874e373bfed026,"pipeline of image data augmentation USED-FOR ML    model overfitting. rotate CONJUNCTION CutOut. CutOut CONJUNCTION rotate. shear CONJUNCTION rotate. rotate CONJUNCTION shear. transformation USED-FOR technique. CutOut HYPONYM-OF transformations. shear HYPONYM-OF transformations. rotate HYPONYM-OF transformations. shared reward mechanism USED-FOR agents. CUB-200 - 2011 CONJUNCTION Stanford Cars. Stanford Cars CONJUNCTION CUB-200 - 2011. ImageNet CONJUNCTION CUB-200 - 2011. CUB-200 - 2011 CONJUNCTION ImageNet. Stanford Cars CONJUNCTION FGVC - Aircraft. FGVC - Aircraft CONJUNCTION Stanford Cars. ImageNet CONJUNCTION Stanford Cars. Stanford Cars CONJUNCTION ImageNet. OtherScientificTerm are image level, transform, regular grid, hyperparameters, and grid size. Task is multi - agent RL ( MARL ) task. Metric is image classification accuracy. ","This paper proposes a pipeline of image data augmentation to address the problem of ML   model overfitting. The technique is based on a simple transformation, where at the image level, a transform is applied to a regular grid, and at the agent level, the transformation is applied on a subset of the grid. The authors propose a multi-agent RL (MARL) task, where agents are trained with a shared reward mechanism, and the goal is to improve the image classification accuracy. The hyperparameters are chosen based on the grid size. Experiments are conducted on ImageNet, CUB-200-2011, Stanford Cars, and FGVC-Aircraft."
963,SP:bb2a13a4d366140fc0c3e941c354cc674f6a904f,"process USED-FOR models. adversarial attacks FEATURE-OF image - based machine learning models. causal graph PART-OF adversarial data creation process. method USED-FOR models. method COMPARE baselines. baselines COMPARE method. Task is adversarial vulnerability. OtherScientificTerm are conditional distribution, and image. ",This paper studies the adversarial vulnerability of image-based machine learning models under adversarial attacks. The authors propose a new adversarial data creation process that incorporates a causal graph between the input image and the conditional distribution over the image. This process can be used to train models that are more robust to adversarial attack. The proposed method is evaluated on several datasets and compared to several baselines.
964,SP:bb2a13a4d366140fc0c3e941c354cc674f6a904f,causal perspective USED-FOR adversarially vulnerability. CIFAR10 CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION CIFAR10. robustness EVALUATE-FOR method. CIFAR100 CONJUNCTION MNIST. MNIST CONJUNCTION CIFAR100. CIFAR10 EVALUATE-FOR method. MNIST EVALUATE-FOR method. CIFAR100 EVALUATE-FOR method. attack methods EVALUATE-FOR method. OtherScientificTerm is causal graph. Method is distribution alignment method. Material is adversarial and natural data. ,"This paper studies adversarially vulnerability from a causal perspective. The authors propose a distribution alignment method that aligns the weights of adversarial and natural data based on a causal graph. The proposed method is evaluated on CIFAR10, CifAR100, and MNIST to evaluate the robustness of the proposed method. The method is shown to outperform existing attack methods."
965,SP:bb2a13a4d366140fc0c3e941c354cc674f6a904f,causal perspective USED-FOR adversarial robustness problem. method USED-FOR it. it PART-OF model. method USED-FOR model. method COMPARE baselines. baselines COMPARE method. Material is graph over content and style variable sets. OtherScientificTerm is adversarial examples. ,"This paper studies the adversarial robustness problem from a causal perspective. Specifically, the authors consider the problem of graph over content and style variable sets, and propose a method to incorporate it into the model. The proposed method is compared with several baselines, and the results show that the proposed method outperforms the baselines in terms of the number of adversarial examples."
966,SP:bb2a13a4d366140fc0c3e941c354cc674f6a904f,causal graph USED-FOR generation process of adversarial attacks. style variable CONJUNCTION class label. class label CONJUNCTION style variable. causal graph USED-FOR adversarial vulnerability. spurious correlation USED-FOR victim model. adversarial distribution CONJUNCTION natural distribution. natural distribution CONJUNCTION adversarial distribution. method USED-FOR adversarial distribution. method USED-FOR model. method USED-FOR natural distribution. ,This paper proposes a causal graph for the generation process of adversarial attacks. The causal graph is used to model the adversarial vulnerability of the victim model by measuring the spurious correlation between the style variable and the class label. The method is applied to both adversarial distribution and natural distribution to train the model.
967,SP:9f09449a47464efb5458d0732df7664865558e6f,"model USED-FOR continual learning. low - rank components PART-OF decomposition of linear filters. decomposition of linear filters USED-FOR model. atoms HYPONYM-OF low - rank components. SVD decomposition of D matrices CONJUNCTION Grassman distance. Grassman distance CONJUNCTION SVD decomposition of D matrices. atoms CONJUNCTION tasks. tasks CONJUNCTION atoms. SVD decomposition of D matrices USED-FOR tasks. Grassman distance USED-FOR tasks. Generic are components, and former. OtherScientificTerm are D, ensembles, and predictive variance. Task are optimization, task - incremental settings, class - incremental settings, and inference. Method are ensembling schemes, and task ensemble. ","This paper proposes a model for continual learning based on decomposition of linear filters with low-rank components, such as atoms and tasks. These two components can be seen as a function of the input D. The authors propose two ensembling schemes: (1) to learn a task ensemble, and (2) to train a model that learns to predict the output of the ensemble. The former can be done in the task-incremental settings, while the latter can be used in the class-implemented settings. In both cases, the authors show that the performance of the ensembles can be improved by minimizing the predictive variance between the input and the output, which can be achieved in both the optimization and inference settings. The experiments are conducted on a variety of tasks using the SVD decompose of D matrices and the Grassman distance."
968,SP:9f09449a47464efb5458d0732df7664865558e6f,continual learning algorithm USED-FOR convolutional filter. filter atoms USED-FOR low - rank filter subspace. convolutional layer USED-FOR task. filter subspace USED-FOR convolutional layer. benchmark datasets EVALUATE-FOR algorithm. OtherScientificTerm is subspace coefficients. ,"This paper proposes a new continual learning algorithm for learning a convolutional filter. The key idea is to learn a low-rank filter subspace from a set of filter atoms. The subspace coefficients are learned as a function of the number of atoms in the subspace. Then, a new convolution layer is trained to solve the new task using the new filter subspaces. The proposed algorithm is evaluated on several benchmark datasets."
969,SP:9f09449a47464efb5458d0732df7664865558e6f,low - rank filter structure USED-FOR CNN layer. low - rank filter structure USED-FOR continual learning. CNN layer PART-OF continual learning. class - incremental settings CONJUNCTION task - incremental settings. task - incremental settings CONJUNCTION class - incremental settings. intra - task ensembles CONJUNCTION inter - task ensembles. inter - task ensembles CONJUNCTION intra - task ensembles. inter - task ensembles USED-FOR task - incremental settings. inter - task ensembles USED-FOR class - incremental settings. intra - task ensembles USED-FOR task - incremental settings. intra - task ensembles USED-FOR class - incremental settings. Task is task subspace modeling literature. OtherScientificTerm is computing memory. ,"This paper proposes a low-rank filter structure for the CNN layer in continual learning, which is an interesting direction in the task subspace modeling literature. The authors propose to use intra-task ensembles, inter-tasks, and task-incremental settings for both class-implemented settings and for task-imperative settings. They also propose a way to reduce computing memory."
970,SP:9f09449a47464efb5458d0732df7664865558e6f,"low - rank filter structure USED-FOR CNN layer. low - rank filter structure USED-FOR continual learning. method USED-FOR task. filters USED-FOR task. filters USED-FOR method. filter subspace USED-FOR task. tiny size of model memory FEATURE-OF datasets. datasets EVALUATE-FOR method. Method are atom - coefficient filter decomposition, and low - rank filter scheme. ","This paper proposes a low-rank filter structure for the CNN layer for continual learning. The proposed method is based on atom-coefficient filter decomposition, which decomposes a task into a filter subspace, and then uses these filters to solve a new task. The method is evaluated on two datasets with tiny size of model memory. The results show that the proposed method outperforms the state-of-the-art on both datasets. The paper also provides a theoretical analysis of the proposed low -rank filter scheme."
971,SP:b806dd540708b39c10d3c165ea7d394a02376805,"driving force CONJUNCTION repulsive force. repulsive force CONJUNCTION driving force. driving force term USED-FOR SVGD. repulsive force HYPONYM-OF gradient term. driving force HYPONYM-OF gradient term. driving force USED-FOR MMD - descent. particle resampling USED-FOR SVGD. proportional asymptotic limit USED-FOR variance collapse phenomenon. theoretical dimensional analysis EVALUATE-FOR SVGD. Gaussian USED-FOR SVGD. Method are stein variational gradient descent ( SVGD ), and maximum mean discrepancy ( MMD ) descent. Generic is it. OtherScientificTerm is log derivative driving force. ","This paper studies stein variational gradient descent (SVGD), which is a variant of maximum mean discrepancy (MMD) descent. SVGD adds a driving force term to MMD-descent, and a repulsive force to SVGD. The authors show that SVGD with particle resampling converges to a proportional asymptotic limit to the variance collapse phenomenon. They also provide theoretical dimensional analysis of SVGD as a Gaussian, and show that it converges linearly to a log derivative driving force."
972,SP:b806dd540708b39c10d3c165ea7d394a02376805,variance collapse phenomenon FEATURE-OF SVGD. damping USED-FOR SVGD. Method is MMD - descent. ,"This paper studies the variance collapse phenomenon of SVGD with damping. The main contribution of this paper is to show that MMD-descent converges to a stationary point when the number of samples is small. The authors also show that SVGD can be improved by damping, which is an important contribution of the paper."
973,SP:b806dd540708b39c10d3c165ea7d394a02376805,"variance collapse phenomenon FEATURE-OF SVGD. algorithm USED-FOR SVGD. driving force term PART-OF SVGD. Method is MMD - descend. Generic is Theory. OtherScientificTerm are proposal limit, and overparameterized / high - dim setting. ","This paper studies the variance collapse phenomenon of SVGD. The authors propose an algorithm called MMD-descend, which is a generalization of the algorithm for SVGD with a driving force term. Theory is provided for the proposal limit and the overparameterized/high-dim setting."
974,SP:b806dd540708b39c10d3c165ea7d394a02376805,curse - of - dimensionality problem FEATURE-OF vanilla SVGD. Euclidean distance kernel USED-FOR vanilla SVGD. repulsive forces CONJUNCTION driving forces. driving forces CONJUNCTION repulsive forces. SVGD USED-FOR MMD - descent. repulsive forces FEATURE-OF they. high variance CONJUNCTION deterministic bias of the driving force. deterministic bias of the driving force CONJUNCTION high variance. MMD - descent CONJUNCTION SVGD. SVGD CONJUNCTION MMD - descent. curse - of - dimensionality problem FEATURE-OF SVGD. stationary variance FEATURE-OF SVGD. stationary variance FEATURE-OF MMD - descent. proportional limit FEATURE-OF stationary variance. isotropic Gaussian USED-FOR stationary variance. isotropic Gaussian USED-FOR SVGD. Task is variance collapse problem. ,"This paper studies the curse-of-dimensionality problem of vanilla SVGD with a Euclidean distance kernel. In particular, they suffer from high variance and deterministic bias of the driving force. The authors show that SVGD can be used as a proxy for MMD-descent and SVGD in terms of stationary variance in the proportional limit. The stationary variance of SVGD is approximated by an isotropic Gaussian and the variance collapse problem is studied."
975,SP:3721f1b12d87e95f5aa4c977a1714d5c54cb70f7,"adversarial training USED-FOR label noise. vanilla training USED-FOR classifier. vanilla CONJUNCTION adversarial training. adversarial training CONJUNCTION vanilla. entropy EVALUATE-FOR classifier. entropy EVALUATE-FOR distribution. distribution USED-FOR classifier. noisy data USED-FOR vanilla. noisy data USED-FOR adversarial training. CIFAR ( also MNIST ) EVALUATE-FOR vanilla training. label noise USED-FOR vanilla training. random label noise USED-FOR CIFAR. vanilla training USED-FOR classifier. adversarial training USED-FOR classifier. adversarial training USED-FOR noisy training data. noisy datasets EVALUATE-FOR adversarial training. correctly labeled data COMPARE incorrectly labeled training data. incorrectly labeled training data COMPARE correctly labeled data. It COMPARE PGD - based annotator baseline. PGD - based annotator baseline COMPARE It. label noise FEATURE-OF CIFAR. CIFAR USED-FOR PGD - based annotator baseline. geometry value USED-FOR confidence score. Material is 2 - dimensional synthetic binary classification dataset. Generic are clusters, quantity, and algorithm. Metric are accuracy, clean ) test accuracy, generalization, and loss(x, y ). Method is PGD steps. Task is labeling unlabeled data. OtherScientificTerm are adversarial perturbations, and label annotations. ","This paper presents a 2-dimensional synthetic binary classification dataset, where the labels are generated from clusters of unlabeled data. The authors compare vanilla training on CIFAR (also MNIST) with adversarial training on noisy data. In vanilla training, the classifier is trained on a distribution that maximizes the entropy of the labels. In adversarial testing, the noisy training data is used to train a classifier that minimizes the accuracy of the original labels.  The authors show that the proposed algorithm is robust to label noise. It outperforms a PGD-based annotator baseline that only uses label noise on the original dataset, as well as on random label noise, and outperforms (clean) test accuracy by a large margin. The main contribution of the paper is that the adversarial perturbations are only applied to the original data, and not to the label annotations. This is an important step towards improving generalization.   The paper also shows that the PGD steps can be used to improve the performance of labeling unlabeling data. It shows that on noisy datasets, adversarial performance is comparable to that of correctly labeled data, but worse than incorrectly labeled training data.  Finally, the authors propose a new confidence score based on the geometry value, which is a function of the loss(x, y). "
976,SP:3721f1b12d87e95f5aa4c977a1714d5c54cb70f7,noisy labels ( NL ) CONJUNCTION adversarial training ( AT ). adversarial training ( AT ) CONJUNCTION noisy labels ( NL ). sample selection USED-FOR noisy labels. PGD attack steps USED-FOR sample selection. adversarial training USED-FOR noisy labels. adversarial training USED-FOR model robustness problems. ,"This paper studies the relationship between noisy labels (NL) and adversarial training (AT). Specifically, the authors propose to use PGD attack steps to improve sample selection for noisy labels and AT. The authors also propose to apply adversarial to model robustness problems."
977,SP:3721f1b12d87e95f5aa4c977a1714d5c54cb70f7,label noise FEATURE-OF adversarial training. sample selection USED-FOR noisy labels. confidence score USED-FOR labeling of unlabeled instances. PGD step number USED-FOR labeling of unlabeled instances. confidence score USED-FOR PGD step number. Generic is use - cases. Method is robust annotator _ algorithm. Material is CIFAR-10 images. ,"This paper studies the problem of adversarial training with label noise. The authors propose two use-cases: (1) sample selection for noisy labels, and (2) using the confidence score as the PGD step number for the labeling of unlabeled instances. In both cases, the authors propose a _robust annotator_ algorithm, which is trained on CIFAR-10 images."
978,SP:3721f1b12d87e95f5aa4c977a1714d5c54cb70f7,"adversarial training USED-FOR label noises. adversarial training USED-FOR model. OtherScientificTerm are smooth landscape, and PGD step sizes. ",This paper proposes to use adversarial training to mitigate label noises in a model that is trained on a smooth landscape. The main idea is to use PGD step sizes that are proportional to the number of labels.
979,SP:2bcf42173d9d82fb3e517405deba4aa3d6f9d8d6,expected robustness EVALUATE-FOR neural network model. random input perturbation USED-FOR misclassification. method USED-FOR expected robustness. method USED-FOR neural network model. robustness EVALUATE-FOR models. approach USED-FOR models. Generic is model. Metric is model ’s robustness. OtherScientificTerm is input perturbation. ,This paper proposes a method to improve the expected robustness of a neural network model by adding random input perturbation to prevent misclassification. The idea is that the model’s robustness can be improved by adding the input to the training set. The authors show that their approach can improve the performance of several models in terms of robustness.
980,SP:2bcf42173d9d82fb3e517405deba4aa3d6f9d8d6,"expected robustness EVALUATE-FOR neural network model. statistical method USED-FOR expected robustness. statistical method EVALUATE-FOR neural network model. blackbox approach USED-FOR approach. transformation ( Box - Cox ) USED-FOR normal distribution. Anderson - Darling test HYPONYM-OF statistical estimation techniques. Metric is robustness. OtherScientificTerm are random input perturbation, and adversarial perturbations. ","This paper proposes a statistical method to evaluate the expected robustness of a neural network model. The proposed approach is based on a blackbox approach, where the robustness is estimated using a random input perturbation, and the normal distribution is estimated by a transformation (Box-Cox). The authors show that the proposed statistical estimation techniques (Anderson-Darling test) are robust to adversarial perturbations."
981,SP:2bcf42173d9d82fb3e517405deba4aa3d6f9d8d6,"local sampling CONJUNCTION probability computation. probability computation CONJUNCTION local sampling. RoMA HYPONYM-OF robustness evaluation framework. local sampling USED-FOR robustness evaluation framework. probability computation USED-FOR robustness evaluation framework. ( $ \epsilon$,$\delta$ ) local robustness score USED-FOR random local samples. Box - Cox transformation USED-FOR statistical estimation. OtherScientificTerm is $ \delta$-confined top-1 confidence. Generic is method. ","This paper proposes a new robustness evaluation framework, RoMA, which combines local sampling and probability computation. The key idea is to compute a (\epsilon$,$\delta$) local robustness score for random local samples, which is then used for statistical estimation using Box-Cox transformation. The authors show that the proposed method is able to achieve $1/\sqrt{T}$-confined top-1 confidence."
982,SP:2bcf42173d9d82fb3e517405deba4aa3d6f9d8d6,statistical method USED-FOR robustness. statistical method USED-FOR deep neural networks. robustness EVALUATE-FOR deep neural networks. Box - cox transformation USED-FOR distribution of confidence scores. it USED-FOR probability. Generic is method. ,"This paper proposes a statistical method to evaluate the robustness of deep neural networks. The method is based on the Box-cox transformation, which is used to estimate the distribution of confidence scores. The authors show that it can be used to compute the probability of a given input."
983,SP:6ba17dd4b31a39478abd995df894447675f2f974,"approach USED-FOR hierarchical representation. text characters CONJUNCTION quantized pixel values. quantized pixel values CONJUNCTION text characters. HCM USED-FOR tree. marginals CONJUNCTION transition frequencies. transition frequencies CONJUNCTION marginals. marginals CONJUNCTION transition frequencies. transition frequencies CONJUNCTION marginals. OtherScientificTerm are low - level inputs, integers, joint probability, independence test, and proximity. Task is interpretable grouping. Method is cognitive science. Generic is it. ","This paper presents an approach to learning a hierarchical representation for low-level inputs, where text characters and quantized pixel values are represented as integers. The idea is motivated by interpretable grouping, which is an important problem in cognitive science. The authors propose to use HCM to learn a tree, where each node is represented as a joint probability of the input and the output of the next node, and then use it to represent the input as a sequence of marginals and transition frequencies. They also propose an independence test, which measures how close a node is to another node based on its proximity to it."
984,SP:6ba17dd4b31a39478abd995df894447675f2f974,graph - learning model ( HCM ) USED-FOR hierarchical chunks. online approximation USED-FOR idealised method. online method USED-FOR interpretable chunks. online method USED-FOR positive ( and negative ) transfer. positive ( and negative ) transfer FEATURE-OF hierarchically structured environments. Material is sequential data. Method is idealised HCM method. OtherScientificTerm is learning guarantees. ,This paper proposes a graph-learning model (HCM) that learns hierarchical chunks from sequential data. The idealised method is based on an online approximation of an existing idealised HCM method. The online method is able to learn interpretable chunks in hierarchically structured environments with positive (and negative) transfer. The paper also provides learning guarantees.
985,SP:6ba17dd4b31a39478abd995df894447675f2f974,"method USED-FOR representations of non- i.i.d. data. primitive data points USED-FOR sets. OtherScientificTerm are hierarchical sets of chunks, proximity, and hierarchically - decomposable problems. Generic are set, and learning method. ","This paper proposes a method for learning representations of non-i.i.d. data. The idea is to learn hierarchical sets of chunks, where each chunk corresponds to a class of primitive data points. The set is then used to train a learning method that learns to classify the set based on its proximity to the original set. This is an important problem in hierarchically-decomposable problems."
986,SP:6ba17dd4b31a39478abd995df894447675f2f974,"non neural system USED-FOR parsing natural language text. algorithm COMPARE parsing algorithms. parsing algorithms COMPARE algorithm. chunking algorithm COMPARE RNN. RNN COMPARE chunking algorithm. low KL - divergence EVALUATE-FOR parsing algorithm. natural language data EVALUATE-FOR chunking algorithm. temporal image data CONJUNCTION video. video CONJUNCTION temporal image data. algorithm USED-FOR video. algorithm USED-FOR temporal image data. Material are natural language text, and sequence data. OtherScientificTerm are hierarchical structures, and chi^2 tests of independence. ","This paper proposes a non neural system for parsing natural language text. The authors show that the proposed algorithm can outperform existing parsing algorithms in terms of low KL-divergence (i.e., the distance between the output of the parsing algorithm and the input text) on natural language data. The algorithm is evaluated on both temporal image data and video, and is compared to a chunking algorithm and an RNN. In particular, it is shown that the algorithm can be applied to both natural language and sequence data with hierarchical structures, and that the chi^2 tests of independence can be used."
987,SP:625e3908502fd5be949bb915116ed7569ba84298,gradient flow / gradient descent USED-FOR nonconvex optimization problems. reparametrization USED-FOR optimization problem. eigenvectors USED-FOR dynamics of the flow. ,"This paper studies gradient flow/gradient descent for nonconvex optimization problems. The authors propose a reparametrization of the optimization problem, where the dynamics of the flow are modeled as eigenvectors. The paper is well-written and easy to follow."
988,SP:625e3908502fd5be949bb915116ed7569ba84298,optimization params CONJUNCTION linear operation. linear operation CONJUNCTION optimization params. reparameterization USED-FOR linear map. optimization params HYPONYM-OF linear map. graph convolution network USED-FOR linear maps. Kuramoto models CONJUNCTION persistent homology models. persistent homology models CONJUNCTION Kuramoto models. Task is non - linear non - convex optimization problems. ,"This paper studies non-linear non-convex optimization problems. The authors propose a reparameterization of the linear map (i.e., optimization params and linear operation) by a graph convolution network. Experiments are conducted on Kuramoto models and persistent homology models."
989,SP:625e3908502fd5be949bb915116ed7569ba84298,neural reparameterization USED-FOR neural reparameterization of non - convex optimization problems. NTK - based matrix USED-FOR preconditioning. approach CONJUNCTION Group Convolutional Networks. Group Convolutional Networks CONJUNCTION approach. approach COMPARE baseline gradient - based optimization. baseline gradient - based optimization COMPARE approach. OtherScientificTerm is optimization variables. Metric is convergence rate. ,"This paper studies the problem of neural reparameterization of non-convex optimization problems, where the optimization variables are not convex. The authors propose to use an NTK-based matrix for preconditioning, and show that their approach can converge faster than baseline gradient-based optimization. The convergence rate is also shown to be linear in the number of parameters. The proposed approach is combined with Group Convolutional Networks."
990,SP:625e3908502fd5be949bb915116ed7569ba84298,neural reparametrization scheme USED-FOR nonconvex nonlinear optimization problems. optimizing synchronization problems CONJUNCTION persistent homology of point - clouds. persistent homology of point - clouds CONJUNCTION optimizing synchronization problems. acceleration USED-FOR persistent homology of point - clouds. acceleration USED-FOR optimizing synchronization problems. method USED-FOR condition number. convergence speed up EVALUATE-FOR method. graph convolutional network ( GNN ) USED-FOR optimization problem. ,This paper proposes a neural reparametrization scheme for solving nonconvex nonlinear optimization problems. The main contribution of this paper is to propose an acceleration for optimizing synchronization problems and persistent homology of point-clouds. The proposed method is able to reduce the condition number and achieve a convergence speed up. The optimization problem is formulated as a graph convolutional network (GNN).
991,SP:80346eeafb0a6d1d556c304a3f8753aff037469b,"model COMPARE convolutional neural networks. convolutional neural networks COMPARE model. model USED-FOR small data regime. small data regime COMPARE convolutional neural networks. convolutional neural networks COMPARE small data regime. Method are SVM classifiers, neural networks, and SVM. OtherScientificTerm is classifiers. ",This paper studies the performance of SVM classifiers. The authors show that the proposed model outperforms convolutional neural networks in the small data regime. The main contribution of this paper is to show that neural networks trained with SVM can generalize to larger data sets than those trained without SVM. The paper also shows that the classifiers can be trained in a supervised fashion.
992,SP:80346eeafb0a6d1d556c304a3f8753aff037469b,class probabilities USED-FOR SVM classifiers. local receptive fields USED-FOR SVM classifiers. voting USED-FOR prediction. class probabilities USED-FOR SVM classifiers. voting USED-FOR SVM classifiers. SVMNet COMPARE ResNet. ResNet COMPARE SVMNet. ResNet COMPARE SVMNet. SVMNet COMPARE ResNet. ResNet COMPARE ResNet. ResNet COMPARE ResNet. Generic is datasets. ,This paper proposes to use class probabilities to train SVM classifiers with local receptive fields. The idea is to use voting for prediction. Experiments on two datasets show that SVMNet outperforms ResNet.
993,SP:80346eeafb0a6d1d556c304a3f8753aff037469b,"large annotated training sets USED-FOR deep convolutional neural networks ( DCNNs ). accuracy EVALUATE-FOR DCNNs. SVMnet COMPARE DCNNs. DCNNs COMPARE SVMnet. stacked "" SVM layers PART-OF SVMnet architecture. probability estimate USED-FOR image class. svm classifiers PART-OF SVM layer. grayscale h X w input image USED-FOR SVMnet. 5x5 kernels USED-FOR SVMnet. svm i USED-FOR probability vector. 5x5 window USED-FOR svm i. average accuracy EVALUATE-FOR svm i. k X l svms PART-OF svm layer. k X l X c probability map USED-FOR feature map. 75 dimensional feature vector FEATURE-OF 3x5x5 patch. RGB images HYPONYM-OF images. SVMnet COMPARE ResNet. ResNet COMPARE SVMnet. SVMnet COMPARE resnet. resnet COMPARE SVMnet. limited training data CONJUNCTION faster training time. faster training time CONJUNCTION limited training data. faster training time EVALUATE-FOR resnet. Generic are architecture, layer, and method. Material is small training sets. Method are svm, and svms. OtherScientificTerm are 2d array of k X l predictions, majority vote, and channels. ","This paper studies the problem of training deep convolutional neural networks (DCNNs) on large annotated training sets. The authors propose a new architecture, called SVMnet, which aims to improve the accuracy of DCNNs on large training sets while maintaining the performance on small training sets (e.g., images such as RGB images). The authors argue that the proposed architecture, which consists of ""stacked"" SVM layers, can achieve comparable or better accuracy than standard DCNN. The proposed method is based on the observation that the svm classifiers in the original SVM layer are biased towards a single layer, and that the ""majority vote"" (i.e., a 2d array of k X l predictions) is biased towards the same svm layer. To address this issue, the authors propose to use a ""supervised"" version of the sVMnet architecture, where each sVM layer consists of two ""svm i"" kernels, which are 5x5 kernels. The first svm i is used to estimate the probability vector of an image class from a grayscale h X w input image, which is then used as a probability estimate for the image class. The second svmi is trained to maximize the average accuracy of the k x l svms of the original svm layers. The k Xl svms are sampled from a 2x2x2 window of the input image. The feature map of a 3x5x5 patch is a k XL X c probability map of the feature map from the original image, and the 3x3x3 patch is the 75 dimensional feature vector of the image. In this way, the svms can be sampled from multiple channels. The paper compares the performance of the proposed method with that of ResNet, and shows that the performance gain of SVMet over ResNet is due to both limited training data and faster training time."
994,SP:80346eeafb0a6d1d556c304a3f8753aff037469b,clean - labeled dataset USED-FOR DCNNs. clean - labeled dataset USED-FOR it. SVMNET HYPONYM-OF deep learning architecture. Support Vector Machine ( SVM ) ensembles PART-OF deep learning architecture. SVMNET COMPARE deep convolutional neural networks. deep convolutional neural networks COMPARE SVMNET. ResNet-50 HYPONYM-OF deep convolutional neural networks. Metric is training time. ,"This paper proposes a new deep learning architecture, SVMNET, which combines Support Vector Machine (SVM) ensembles and uses it with a clean-labeled dataset for training DCNNs. The experiments show that SVMnet outperforms other deep convolutional neural networks such as ResNet-50 in terms of performance and training time."
995,SP:a18f4697f350a864866dac871f581b8fc67e8088,"distributed training USED-FOR graph learning tasks. gradient norm EVALUATE-FOR method. scheme COMPARE GGS. GGS COMPARE scheme. OtherScientificTerm are data privacy, full graph, and local graph partition. Method are LLCG algorithm, and server correction. ","This paper studies distributed training for graph learning tasks, where data privacy is important. The authors propose a variant of the LLCG algorithm, where instead of training on the full graph, each server is trained on a local graph partition, and the server correction is performed on the local partition. The proposed method is shown to improve the gradient norm. Empirical results show that the proposed scheme outperforms GGS."
996,SP:a18f4697f350a864866dac871f581b8fc67e8088,"communication costs CONJUNCTION memory overheads. memory overheads CONJUNCTION communication costs. global server corrections USED-FOR locally learned models. method USED-FOR residual error. Method are GNNs, distributed GNN training technique, and locally trained models. OtherScientificTerm are irreducible performance degradation, and ignoring node dependency. ","This paper proposes a distributed GNN training technique to reduce the communication costs and memory overheads of GNNs. The authors argue that the global server corrections for locally learned models can lead to irreducible performance degradation due to ignoring node dependency. To address this issue, the authors propose a method to minimize the residual error of locally trained models."
997,SP:a18f4697f350a864866dac871f581b8fc67e8088,"centralized step USED-FOR global structural information. centralized step PART-OF server. global structural information FEATURE-OF subgraph partition. averaging USED-FOR residual error. Task is distributed training of GNNs. Method are GNNs, LLCG, and averaging methods. Generic are Existing methods, and method. OtherScientificTerm are local averages, theoretical convergence guarantees, and global correction step. ","This paper studies distributed training of GNNs. Existing methods are based on averaging the local averages, which is computationally expensive. In this paper, the authors propose to add a centralized step to the server to incorporate global structural information of the subgraph partition. Theoretical convergence guarantees are provided for the proposed method, LLCG. The authors also provide a theoretical analysis of the residual error caused by averaging. Finally, a global correction step is proposed to reduce the variance of the averaging methods."
998,SP:a18f4697f350a864866dac871f581b8fc67e8088,distributed training technique USED-FOR GNN. local computations CONJUNCTION correction phase. correction phase CONJUNCTION local computations. correction phase PART-OF technique. local computations PART-OF technique. server correction phase USED-FOR irreducible error. local computation USED-FOR subgraph. technique COMPARE techniques. techniques COMPARE technique. real datasets EVALUATE-FOR technique. real datasets EVALUATE-FOR techniques. communication steps EVALUATE-FOR technique. communication steps EVALUATE-FOR techniques. ,This paper proposes a distributed training technique for GNN. The technique consists of local computations and a correction phase. The server correction phase aims to reduce the irreducible error. The local computation is used to update the subgraph. The experimental results show that the proposed technique outperforms existing techniques in terms of communication steps on two real datasets.
999,SP:6805f2245484fc91b5c13aa5f09e5478b810f97f,semantic segmentation HYPONYM-OF anytime pixel - level recognition. architecture USED-FOR anytime inference. spatial confidence adaptivity PART-OF network. non - confidence pixels USED-FOR layers. interpolation USED-FOR features. method USED-FOR human pose estimation. semantic segmentation CONJUNCTION human pose estimation. human pose estimation CONJUNCTION semantic segmentation. method USED-FOR semantic segmentation. OtherScientificTerm is reduction in FLOPs. Generic is tasks. ,"This paper addresses the problem of anytime pixel-level recognition, i.e., semantic segmentation. The authors propose a new architecture for anytime inference, which incorporates spatial confidence adaptivity into the network. The layers are trained on non-confidence pixels, and features are learned via interpolation. This leads to a reduction in FLOPs, and the proposed method is applied to both the task of semantic segmentating and human pose estimation. Experiments are conducted on both tasks."
1000,SP:6805f2245484fc91b5c13aa5f09e5478b810f97f,"anytime method USED-FOR pixel recognition tasks. semantic segmentation CONJUNCTION human pose estimation. human pose estimation CONJUNCTION semantic segmentation. human pose estimation HYPONYM-OF pixel recognition tasks. semantic segmentation HYPONYM-OF pixel recognition tasks. network USED-FOR network. prediction head USED-FOR network. confidence - based adaptive filtering mechanism USED-FOR computation budgets. score threshold FEATURE-OF spatial pixels. full - blown method COMPARE approach. approach COMPARE full - blown method. full - blown method COMPARE methods. methods COMPARE full - blown method. Cityscapes CONJUNCTION MPII. MPII CONJUNCTION Cityscapes. approach COMPARE methods. methods COMPARE approach. semantics segmentation CONJUNCTION MPII. MPII CONJUNCTION semantics segmentation. Cityscapes CONJUNCTION semantics segmentation. semantics segmentation CONJUNCTION Cityscapes. MPII USED-FOR human pose estimation. accuracy - computation tradeoff EVALUATE-FOR methods. semantics segmentation HYPONYM-OF tasks. human pose estimation HYPONYM-OF tasks. Cityscapes HYPONYM-OF tasks. tasks EVALUATE-FOR methods. MPII HYPONYM-OF tasks. accuracy - computation tradeoff EVALUATE-FOR full - blown method. methods PART-OF hardware lottery. OtherScientificTerm are budget, prediction heads, RH, intermediate feature maps, spatial regions, max prediction scores, and lacking positive signals. Metric are accuracy - computation operating points, and wallclock time metric. Generic is work. ","This paper proposes an anytime method for pixel recognition tasks (semantic segmentation, human pose estimation). The network is trained as a network with a single prediction head. The computation budgets are based on a confidence-based adaptive filtering mechanism, where the budget depends on the number of prediction heads and the score threshold of the spatial pixels. The authors propose to use RH to filter out intermediate feature maps, and to use max prediction scores to remove regions with lacking positive signals. The accuracy-computation operating points are estimated using a wallclock time metric. The proposed work is evaluated on three tasks: Cityscapes, semantics segmentation and MPII, and compared with a full-blown method and two other methods from the hardware lottery. The results show that the proposed methods achieve better accuracy-compute tradeoff on all three tasks."
1001,SP:6805f2245484fc91b5c13aa5f09e5478b810f97f,"model USED-FOR progression of predictions. anytime "" prediction HYPONYM-OF task. model USED-FOR anytime "" prediction. model USED-FOR task. end - to - end model USED-FOR problem. model USED-FOR progressive predictions. Confidence Adaptivity USED-FOR model. model USED-FOR pixel prediction. cascade of “ exits ” USED-FOR model. components USED-FOR model. Confidence Adaptivity PART-OF components. cascade of “ exits ” PART-OF components. HRNet baseline USED-FOR approach. Metric is accuracy vs computational cost tradeoff. ","This paper proposes a model for the task of ""anytime"" prediction, i.e., the progression of predictions. The authors propose an end-to-end model for this problem. The proposed model consists of two components: 1) a cascade of “exits” that encourages progressive predictions, 2) Confidence Adaptivity that encourages the model to make pixel prediction with high confidence. The approach is evaluated on the HRNet baseline and the accuracy vs computational cost tradeoff is discussed."
1002,SP:6805f2245484fc91b5c13aa5f09e5478b810f97f,"adaptive and anytime methods USED-FOR semantic segmentation. adaptive and anytime methods USED-FOR pose recognition. semantic segmentation CONJUNCTION pose recognition. pose recognition CONJUNCTION semantic segmentation. convolution layers USED-FOR sparse computation. Cityscapes semantic segmentation CONJUNCTION MPII pose estimation benchmarks. MPII pose estimation benchmarks CONJUNCTION Cityscapes semantic segmentation. Task is pixel - level "" tasks. Generic is model. Method is adaptive method. OtherScientificTerm are early exits, and interpolation. ","This paper proposes adaptive and anytime methods for semantic segmentation and pose recognition. The authors focus on ""pixel-level"" tasks, where the model is trained on a single image at a time. The main idea is to use convolution layers for sparse computation, and to use early exits for interpolation. Experiments are conducted on Cityscapes and MPII pose estimation benchmarks. The results show that the adaptive method is effective."
1003,SP:1a75aaef7ba0d2de5804514f0de39d9c769f419b,Neural Bootstrapping ( NeuBoots ) USED-FOR Bootstrapped Attentive Neural Processes ( BANP ). Bootstrapped Attentive Neural Processes ( BANP ) USED-FOR functional uncertainty. Neural Bootstrapping ( NeuBoots ) USED-FOR capturing functional uncertainty. Neural Bootstrapping ( NeuBoots ) USED-FOR method. Bayesian optimization CONJUNCTION contextual multi - armed bandits. contextual multi - armed bandits CONJUNCTION Bayesian optimization. Bayesian optimization EVALUATE-FOR NeuBANP. benchmark experiments EVALUATE-FOR NeuBANP. contextual multi - armed bandits HYPONYM-OF benchmark experiments. Bayesian optimization HYPONYM-OF benchmark experiments. Method is neural process algorithm. ,"This paper proposes a neural process algorithm, called Neural Bootstrapped Attentive Neural Processes (BANP), which is based on the idea of capturing functional uncertainty by using NeuralBootstrapping (NeuBoots). The method is a generalization of the recently proposed method, NeuBootstrap (NeurIPS). Experiments on benchmark experiments on Bayesian optimization and contextual multi-armed bandits demonstrate the effectiveness of NeuBIANP."
1004,SP:1a75aaef7ba0d2de5804514f0de39d9c769f419b,iterative prediction method PART-OF B(A)NP. B(A)NP USED-FOR NeuBANP. model USED-FOR uncertainty estimates. model USED-FOR heteroscedastic uncertainties. Bayesian optimisation CONJUNCTION contextual multi - armed bandit tasks. contextual multi - armed bandit tasks CONJUNCTION Bayesian optimisation. contextual multi - armed bandit tasks CONJUNCTION image inpainting. image inpainting CONJUNCTION contextual multi - armed bandit tasks. nonparametric regression tasks CONJUNCTION Bayesian optimisation. Bayesian optimisation CONJUNCTION nonparametric regression tasks. Method is neural bootstrapper. ,"This paper proposes NeuBANP, which extends B(A)NP, an iterative prediction method, by adding a neural bootstrapper. The proposed model is able to provide uncertainty estimates for heteroscedastic uncertainties. Experiments are conducted on nonparametric regression tasks, Bayesian optimisation, contextual multi-armed bandit tasks, and image inpainting."
1005,SP:1a75aaef7ba0d2de5804514f0de39d9c769f419b,"random sum - to - one weight USED-FOR encoder. random sum - to - one weight USED-FOR Bootstrapping Attentive Neural Processes ( NeuBANP ). overfitting problem FEATURE-OF attentive NPs. synthetic examples CONJUNCTION contextual multi - armed bandit. contextual multi - armed bandit CONJUNCTION synthetic examples. NPs CONJUNCTION GP. GP CONJUNCTION NPs. Method are Neural Bootstrapper, and NeuBANP. ","This paper proposes Bootstrapping Attentive Neural Processes (NeuBANP) with a random sum-to-one weight for the encoder. The authors argue that attentive NPs suffer from an overfitting problem, and propose a Neural Bootstrapper to address this issue. Experiments are conducted on synthetic examples and a contextual multi-armed bandit to demonstrate the effectiveness of the proposed NeuBIANP. NPs and GP are also compared."
1006,SP:1a75aaef7ba0d2de5804514f0de39d9c769f419b,bootstrapping method USED-FOR NP family. NeuBANP HYPONYM-OF bootstrapping method. NeuBANP HYPONYM-OF NP family. neural bootstrapping method USED-FOR BANP. it COMPARE ANP. ANP COMPARE it. BANP COMPARE ANP. ANP COMPARE BANP. encoder network CONJUNCTION adaptation layer. adaptation layer CONJUNCTION encoder network. adaptation layer CONJUNCTION heuristics. heuristics CONJUNCTION adaptation layer. BANP USED-FOR functional uncertainty estimation. BANP COMPARE it. it COMPARE BANP. adaptation layer USED-FOR it. encoder network USED-FOR it. heuristics USED-FOR it. computational burden EVALUATE-FOR it. NeuBANP COMPARE BANP. BANP COMPARE NeuBANP. NeuBANP USED-FOR functional uncertainty. ANP CONJUNCTION BANP. BANP CONJUNCTION ANP. NeuBANP HYPONYM-OF model. neural bootstrapping PART-OF bootstrapping procedure. multidimensional Bayesian optimization CONJUNCTION contextual multi - armed bandit. contextual multi - armed bandit CONJUNCTION multidimensional Bayesian optimization. stochastic optimization problems EVALUATE-FOR it. multidimensional Bayesian optimization HYPONYM-OF stochastic optimization problems. contextual multi - armed bandit HYPONYM-OF stochastic optimization problems. OtherScientificTerm is overfitting problem. ,"NeuBANP is a new bootstrapping method for the NP family. Compared to ANP and BANP for functional uncertainty estimation, it uses an encoder network, an adaptation layer, and some heuristics to reduce the computational burden. The authors also propose a new model called NeuBIANP, which combines neural bootstrapped in the bootstrap procedure to avoid the overfitting problem. Experiments on two stochastic optimization problems (multidimensional Bayesian optimization and contextual multi-armed bandit) show that NeuBNP is able to estimate functional uncertainty more accurately than ANP, and that it can be used in combination with ANP."
1007,SP:34e1b51ff5d524490332aed51b9c411209c89a20,transformer - based model USED-FOR DNA sequences. GeneBERT USED-FOR DNA sequences. DNA sequences CONJUNCTION regulatory elements. regulatory elements CONJUNCTION DNA sequences. GeneBERT HYPONYM-OF transformer - based model. transformers USED-FOR representations of sequencing data. representations USED-FOR region - aligned sequences. representations of sequencing data CONJUNCTION regulatory regions. regulatory regions CONJUNCTION representations of sequencing data. GeneBERT USED-FOR representations of sequencing data. transformers USED-FOR GeneBERT. open chromatin HYPONYM-OF regulatory regions. aligned sequences USED-FOR promoters. promoters CONJUNCTION CTCF binding sites. CTCF binding sites CONJUNCTION promoters. disease type CONJUNCTION RNA splicing sites. RNA splicing sites CONJUNCTION disease type. aligned sequences USED-FOR CTCF binding sites. CTCF binding sites CONJUNCTION disease type. disease type CONJUNCTION CTCF binding sites. GeneBERT USED-FOR scATAC - seq data. GeneBERT USED-FOR promoters. ,"This paper proposes GeneBERT, a transformer-based model for learning DNA sequences and regulatory elements, which is a generalization of the recently proposed GeneBERR. The authors propose to use transformers to learn representations of sequencing data and regulatory regions (e.g., open chromatin) and use these representations to generate region-aligned sequences. They show that the generated aligned sequences can be used to learn promoters, CTCF binding sites, disease type, and RNA splicing sites. They also show that using the generated promoters and CTCFs, they are able to generate scATAC-seq data and demonstrate that the learned promoters can generate more diverse sequences."
1008,SP:34e1b51ff5d524490332aed51b9c411209c89a20,"2D representation of regulatory elements USED-FOR pre - training tasks. 1D genome sequence data CONJUNCTION 2D representation of regulatory elements. 2D representation of regulatory elements CONJUNCTION 1D genome sequence data. GeneBERT USED-FOR pre - training tasks. 2D representation of regulatory elements PART-OF GeneBERT. 1D genome sequence data USED-FOR GeneBERT. large - scale single - cell ATAC dataset USED-FOR pseudo - bulk ATAC profiles. 1D representation of the data USED-FOR self - supervised training. 1D representation of the data USED-FOR genome segment prediction loss functions. self - supervised training CONJUNCTION masked genome modeling. masked genome modeling CONJUNCTION self - supervised training. masked genome modeling CONJUNCTION genome segment prediction loss functions. genome segment prediction loss functions CONJUNCTION masked genome modeling. self - supervised training CONJUNCTION genome segment prediction loss functions. genome segment prediction loss functions CONJUNCTION self - supervised training. accessibility per regulatory element USED-FOR 2D representation. infoNCE loss USED-FOR 2D representation. TFBS prediction CONJUNCTION splice site prediction. splice site prediction CONJUNCTION TFBS prediction. promoter prediction CONJUNCTION TFBS prediction. TFBS prediction CONJUNCTION promoter prediction. pre - training framework USED-FOR biological tasks. promoter prediction HYPONYM-OF biological tasks. splice site prediction HYPONYM-OF biological tasks. TFBS prediction HYPONYM-OF biological tasks. Material is genomic data. OtherScientificTerm are genomic locations, and loss function. Method is BERT framework. ","This paper proposes GeneBERT, which combines 1D genome sequence data with a 2D representation of regulatory elements for pre-training tasks. The authors propose to use a large-scale single-cell ATAC dataset to generate pseudo-bulk ATAC profiles, which can then be used to train various genome segment prediction loss functions based on the 1D representation. The main contributions of the paper are self-supervised training, masked genome modeling, and genome segmentation loss functions. In addition, the authors propose a new infoNCE loss that encourages the 2D representations to have better accessibility per regulatory element, which encourages genomic data to be closer to genomic locations. The proposed loss function is inspired by the BERT framework, and the authors show that the proposed pre-train framework can be applied to various biological tasks such as promoter prediction, TFBS prediction, and splice site prediction."
1009,SP:34e1b51ff5d524490332aed51b9c411209c89a20,pipeline USED-FOR self - supervised learning. genome sequences USED-FOR self - supervised learning. tasks USED-FOR learning procedure. NLP method USED-FOR learning procedure. OtherScientificTerm is accessible chromatin peaks. Method is transformer layers. Task is regulatory sequence classification tasks. ,"This paper proposes a pipeline for self-supervised learning on genome sequences. The learning procedure is based on a NLP method, which is trained on two tasks: (1) finding accessible chromatin peaks, and (2) finding transformer layers. The authors show promising results on regulatory sequence classification tasks."
1010,SP:34e1b51ff5d524490332aed51b9c411209c89a20,"approach USED-FOR pre - training genome data. robustness EVALUATE-FOR model. pre - training tasks USED-FOR model. robustness EVALUATE-FOR pre - training tasks. Masked Genome Modeling ( MGM ) CONJUNCTION Next Genome - Segment Prediction ( NGSP ). Next Genome - Segment Prediction ( NGSP ) CONJUNCTION Masked Genome Modeling ( MGM ). objectives USED-FOR sequence pre - training. Masked Genome Modeling ( MGM ) HYPONYM-OF sequence pre - training. Next Genome - Segment Prediction ( NGSP ) HYPONYM-OF sequence pre - training. Masked Genome Modeling ( MGM ) HYPONYM-OF objectives. Next Genome - Segment Prediction ( NGSP ) HYPONYM-OF objectives. region pre - training CONJUNCTION sequence - region matching. sequence - region matching CONJUNCTION region pre - training. sequence pre - training CONJUNCTION region pre - training. region pre - training CONJUNCTION sequence pre - training. components PART-OF GeneBERT. sequence - region matching HYPONYM-OF components. sequence pre - training HYPONYM-OF components. region pre - training PART-OF GeneBERT. sequence pre - training PART-OF GeneBERT. sequence - region matching PART-OF GeneBERT. region pre - training HYPONYM-OF components. model COMPARE DNABERT. DNABERT COMPARE model. transcription factor information USED-FOR model. transcription factor information USED-FOR genomic regions. sequence information USED-FOR DNABERT. Material are genome data, and 1d sequence of genomic data. ","This paper presents a novel approach for pre-training genome data. The authors propose GeneBERT, a method to pre-train a model on a 1d sequence of genomic data, and evaluate the robustness of the model on two pre -training tasks that are designed to improve the model’s performance. The two objectives are Masked Genome Modeling (MGM) and Next Genome-Segment Prediction (NGSP), which are two variants of the standard objectives used in the standard sequence pre-trains (e.g., DNABERT). The authors show that the three components of the proposed method, which they refer to as “GeneBERT”, are region pre-regression, sequence-region matching, and “sequencing”. They also show that their model is able to learn genomic regions with transcription factor information, which is a significant improvement over DNABerT without sequence information."
1011,SP:841b12443d0274e34b78940f220b17d36798899b,geodesic ( Fisher - Rao ) distance USED-FOR confidence scoring. method USED-FOR detecting OOD samples. geodesic ( Fisher - Rao ) distance USED-FOR method. geodesic ( Fisher - Rao ) distance USED-FOR IGEOOD. confidence scores CONJUNCTION layer - wise features. layer - wise features CONJUNCTION confidence scores. logit outputs CONJUNCTION layer - wise features. layer - wise features CONJUNCTION logit outputs. layer - wise features PART-OF deep neural network. deep neural network USED-FOR It. confidence scores USED-FOR It. logit outputs USED-FOR confidence scores. layer - wise features PART-OF It. latent features PART-OF deep network. Fisher - Rao distance USED-FOR OOD detection. Material is OOD data. ,This paper proposes a method for detecting OOD samples based on the geodesic (Fisher-Rao) distance for confidence scoring in IGEOOD. It uses a deep neural network with latent features and combines the confidence scores with the logit outputs and the layer-wise features. The authors show that the Fisher-Rsao distance can be used for OOD detection. They also provide some empirical results on OOD data.
1012,SP:841b12443d0274e34b78940f220b17d36798899b,methods USED-FOR supervised OOD detection. Fisher - Rao distance USED-FOR OOD. Fisher - Rao distance USED-FOR white - box setting. hidden layer feature space FEATURE-OF Fisher - Rao distance. Odin CONJUNCTION Mahalanobis. Mahalanobis CONJUNCTION Odin. method COMPARE baselines. baselines COMPARE method. Mahalanobis HYPONYM-OF baselines. Odin HYPONYM-OF baselines. Material is in - distribution data. ,This paper proposes methods for supervised OOD detection. The main idea is to use the Fisher-Rao distance in the hidden layer feature space to detect OOD in the white-box setting. The method is evaluated on in-distribution data and compared with two baselines: Odin and Mahalanobis.
1013,SP:841b12443d0274e34b78940f220b17d36798899b,"score USED-FOR detection of out - of - distribution samples. DNN USED-FOR detection of out - of - distribution samples. Fisher - Rao information metric USED-FOR score. SoftMax probabilities USED-FOR black - box and grey - box scenarios. features USED-FOR white - box scenario. features PART-OF DNN layers. SoftMax probability CONJUNCTION class - conditional PDFs. class - conditional PDFs CONJUNCTION SoftMax probability. class - conditional PDFs FEATURE-OF DNN layer. feature spaces FEATURE-OF class - conditional PDFs. label space FEATURE-OF SoftMax probability. SoftMax probability HYPONYM-OF posterior probabilities. class - conditional PDFs HYPONYM-OF posterior probabilities. multivariate Gaussian distributions USED-FOR latter. diagonal covariance matrices FEATURE-OF multivariate Gaussian distributions. Generic are approach, and benchmarks. ","This paper proposes a score for the detection of out-of-distribution samples by DNN based on the Fisher-Rao information metric. The proposed approach is motivated by the observation that the features of DNN layers are similar in the white-box scenario, but different in the black-box and grey-box scenarios. The authors propose to use two types of posterior probabilities: the SoftMax probability in the label space, and the class-conditional PDFs in the feature spaces of a DNN layer. The latter is based on multivariate Gaussian distributions with diagonal covariance matrices. The experiments are conducted on several benchmarks."
1014,SP:841b12443d0274e34b78940f220b17d36798899b,"runtime COMPARE MSP. MSP COMPARE runtime. runtime COMPARE energy baseline. energy baseline COMPARE runtime. MSP CONJUNCTION energy baseline. energy baseline CONJUNCTION MSP. validation OOD data USED-FOR method. Fisher - Rao distance USED-FOR pre - trained classifiers. Fisher - Rao distance USED-FOR distribution of softmax. FIsher - Rao distance COMPARE OOD metric. OOD metric COMPARE FIsher - Rao distance. FIsher - Rao distance COMPARE Mahalanobis distance. Mahalanobis distance COMPARE FIsher - Rao distance. Mahalanobis distance HYPONYM-OF OOD metric. IGEOOD HYPONYM-OF Fisher - Rao distance - based framework. out - of - distribution data CONJUNCTION in - distribution data. in - distribution data CONJUNCTION out - of - distribution data. IGEOOD COMPARE OOD metrics. OOD metrics COMPARE IGEOOD. in - distribution data EVALUATE-FOR OOD metrics. out - of - distribution data EVALUATE-FOR OOD metrics. in - distribution data EVALUATE-FOR IGEOOD. out - of - distribution data EVALUATE-FOR IGEOOD. OtherScientificTerm are White - Box, and intermediate feature layers. ","This paper proposes a Fisher-Rao distance-based framework called IGEOOD, which is an extension of the Fisher-Rsao distance to pre-trained classifiers. The method is based on validation OOD data, where the runtime is compared to MSP and an energy baseline. The authors show that the FIsher-Rs distance to the distribution of softmax is better than the standard OOD metric, Mahalanobis distance. The paper also shows that the White-Box can be used as a proxy for the number of intermediate feature layers. Experiments on out-of-distribution data and in-distributions data show that IGEOED outperforms other OOD metrics."
1015,SP:2fdca838ac3453e44cff395f1b760d839a5813bf,"theory USED-FOR deep learning of symmetric objects. pooling operations CONJUNCTION induced representations. induced representations CONJUNCTION pooling operations. Method is equivariant representations. OtherScientificTerm are transformations of a symmetry group of interest, binary classifications, and dimension of the representation. ",This paper proposes a new theory for deep learning of symmetric objects. The main idea is to learn equivariant representations that are invariant to transformations of a symmetry group of interest. This is achieved by pooling operations and induced representations. The authors show that binary classifications of the representation can be learned in this way.
1016,SP:2fdca838ac3453e44cff395f1b760d839a5813bf,group equivariance USED-FOR representation. perceptron capacity USED-FOR quantification of the expressivity. algorithm USED-FOR $ \pi$-manifolds. pooling operators USED-FOR capacity. ,This paper studies the problem of quantification of the expressivity using perceptron capacity. The authors propose to use group equivariance to learn the representation. The algorithm is applied to $\pi$-manifolds. The capacity is quantified using pooling operators.
1017,SP:2fdca838ac3453e44cff395f1b760d839a5813bf,"network capacity EVALUATE-FOR group equivariant NNs. Generic are analysis, and fraction. OtherScientificTerm are linearly separable dichotomies, trivial irreps, irreps, and group action. ",This paper studies the network capacity of group equivariant NNs. The analysis is based on the observation that linearly separable dichotomies are trivial irreps. The authors then show that the fraction of irreps that are not trivial is a function of the group action.
1018,SP:2fdca838ac3453e44cff395f1b760d839a5813bf,expressivity FEATURE-OF equivariant neural networks. generalization FEATURE-OF non - equivariant networks. features USED-FOR invariant binary classification problem. equivariant architecture USED-FOR equivariant representations. perceptron capacity USED-FOR invariant classifiers. group representation type PART-OF equivariant representation. group representation type FEATURE-OF multiplicity of the trivial representation. pooling HYPONYM-OF equivariant operations. Method is equivariant ones. Generic is it. ,"This paper studies the expressivity of equivariant neural networks. The authors show that non-equivariant networks exhibit similar generalization to equivariants ones, but that the features of the invariant binary classification problem can be expressed as a function of the group representation type of the equivariated representation. The paper then proposes an efficient and efficient way to compute the group representations of the trivial representation, and shows that it is possible to compute equivariable representations with an equivarian architecture. The main contribution of the paper is to show that the perceptron capacity of invariant classifiers can be approximated by the multiplicity of a trivial representation. In addition, the authors propose a number of new equivariante operations, such as pooling."
1019,SP:47889067620e5ac2e304681769af9d1d930f6d2b,"concepts USED-FOR image. SVM USED-FOR map. approach USED-FOR confounding factor. snow HYPONYM-OF confounding factor. method USED-FOR confounding factor. dermatology and chest x - ray datasets EVALUATE-FOR model. Method is classifier. Material are images, and dataset of animal images. Generic is concept. OtherScientificTerm are concept space, counterfactual perturbation, perturbation, and concept perturbations. ","This paper proposes an approach to remove a confounding factor, i.e., snow, from an image by perturbing the concept space. The key idea is to train a classifier to distinguish between images that are similar to the original image and images that differ in concept. The concept space is represented as a set of concepts, and the classifier is trained to predict the image based on these concepts. This is done by training an SVM to generate a map of the image, and then a counterfactual perturbation is added to this map. The authors show that the proposed method is able to remove the confounding factor by removing the concept perturbations. The proposed model is evaluated on dermatology and chest x-ray datasets, as well as a dataset of animal images."
1020,SP:47889067620e5ac2e304681769af9d1d930f6d2b,robustness EVALUATE-FOR machine learning models. trustworthiness EVALUATE-FOR machine learning models. robustness CONJUNCTION trustworthiness. trustworthiness CONJUNCTION robustness. semantically meaningful concepts USED-FOR Explaining errors. counterfactual explanation CONJUNCTION concept activation vectors. concept activation vectors CONJUNCTION counterfactual explanation. prior methods USED-FOR explainability. coneptual counterfactual explanations USED-FOR them. prior methods USED-FOR method. counterfactual explanation HYPONYM-OF prior methods. concept activation vectors HYPONYM-OF prior methods. ImageNet CONJUNCTION real diagnostic imaging applications. real diagnostic imaging applications CONJUNCTION ImageNet. Generic is training procedure. ,"Explaining errors using semantically meaningful concepts is an important problem for improving the robustness and trustworthiness of machine learning models. This paper proposes a method that leverages prior methods for explainability (counterfactual explanation, concept activation vectors) and uses them as coneptual counterfactual explanations. The training procedure is simple and effective. Experiments are conducted on ImageNet and real diagnostic imaging applications."
1021,SP:47889067620e5ac2e304681769af9d1d930f6d2b,Conceptual Counterfactual Explanation USED-FOR model. linear separator USED-FOR models representation space. OtherScientificTerm is model's mistake. Generic is method. ,This paper proposes Conceptual Counterfactual Explanation to train a model that can be used to explain the model's mistake. The method is based on a linear separator that maps the models representation space to a set of points that are close to each other.
1022,SP:47889067620e5ac2e304681769af9d1d930f6d2b,method USED-FOR concept based counterfactual explanations. Task is models prediction. Method is image classifiers. ,This paper proposes a method to generate concept based counterfactual explanations for models prediction. The main idea is to train image classifiers to predict concepts that are not present in the training data. The paper is well-written and easy to follow.
1023,SP:4aa5f00830fda36b6ca2f53d88c3a8a963058ec0,KPConv model HYPONYM-OF point cloud processing model. depthwise kernels CONJUNCTION attention over kernels. attention over kernels CONJUNCTION depthwise kernels. attention over kernels CONJUNCTION neural architecture search ( NAS ). neural architecture search ( NAS ) CONJUNCTION attention over kernels. ,"This paper proposes a point cloud processing model called KPConv model, which is an extension of the point cloud computing model. The authors propose to combine depthwise kernels, attention over kernels, and neural architecture search (NAS). "
1024,SP:4aa5f00830fda36b6ca2f53d88c3a8a963058ec0,3D point cloud neural networks USED-FOR inference. depthwise convolution CONJUNCTION inverted residual bottleneck. inverted residual bottleneck CONJUNCTION depthwise convolution. designs PART-OF 2D neural networks. inverted residual bottleneck HYPONYM-OF 2D neural networks. depthwise convolution HYPONYM-OF 2D neural networks. inverted residual bottleneck HYPONYM-OF designs. depthwise convolution HYPONYM-OF designs. predictor - based neural architecture search USED-FOR model. resource constraint FEATURE-OF model. small - scale ModelNet40 dataset CONJUNCTION large - scale SemanticKITTI dataset. large - scale SemanticKITTI dataset CONJUNCTION small - scale ModelNet40 dataset. large - scale SemanticKITTI dataset EVALUATE-FOR solution. small - scale ModelNet40 dataset EVALUATE-FOR solution. OtherScientificTerm is representation power. ,"This paper proposes 3D point cloud neural networks for inference. The authors propose two new designs of 2D neural networks: depthwise convolution and inverted residual bottleneck. The proposed model is based on predictor-based neural architecture search and is trained under a resource constraint, which allows for better representation power. Experiments on small-scale ModelNet40 dataset and large-scale SemanticKITTI dataset demonstrate the effectiveness of the proposed solution."
1025,SP:4aa5f00830fda36b6ca2f53d88c3a8a963058ec0,point - based methods USED-FOR semantic segmentation. classification CONJUNCTION semantic segmentation. semantic segmentation CONJUNCTION classification. point - based methods USED-FOR classification. semantic segmentation USED-FOR 3D applications. computational efficiency EVALUATE-FOR methods. methods USED-FOR it. methods USED-FOR applications. it USED-FOR applications. limited resources FEATURE-OF applications. mobile scenarios HYPONYM-OF applications. mobile scenarios HYPONYM-OF limited resources. neural architecture search ( NAS ) technique USED-FOR MAKPConv - based network. wide & deep neural predictor USED-FOR NAS process. computational efficiency EVALUATE-FOR deep networks. Method is kernel point convolution. Generic is benchmark datasets. ,"This paper proposes to use kernel point convolution to improve the computational efficiency of existing point-based methods for classification and semantic segmentation in 3D applications. In particular, the authors propose to use it for applications with limited resources (e.g., mobile scenarios). The authors propose a MAKPConv-based network based on the neural architecture search (NAS) technique, which uses the wide & deep neural predictor to guide the NAS process. The authors show that the proposed deep networks can achieve better computational efficiency while maintaining the same computational efficiency. The experiments are conducted on several benchmark datasets."
1026,SP:4aa5f00830fda36b6ca2f53d88c3a8a963058ec0,kernel relationship USED-FOR KPConv operation. predictor - based NAS approach USED-FOR networks. searched network COMPARE baseline. baseline COMPARE searched network. ModelNet40 CONJUNCTION SemanticKitti. SemanticKitti CONJUNCTION ModelNet40. MAKPConv USED-FOR local structure. ModelNet40 CONJUNCTION classification. classification CONJUNCTION ModelNet40. classification CONJUNCTION SemanticKitti. SemanticKitti CONJUNCTION classification. SemanticKitti USED-FOR segmentation. datasets EVALUATE-FOR searched network. datasets EVALUATE-FOR baseline. ModelNet40 HYPONYM-OF datasets. SemanticKitti HYPONYM-OF datasets. Task is point cloud analysis task. ,"This paper proposes a predictor-based NAS approach to search for networks that perform well on a point cloud analysis task. The main idea is to learn a kernel relationship between the KPConv operation and the weights of the searched network. The searched network is compared with a baseline on three datasets (ModelNet40, SemanticKitti for segmentation, ModelNet40 for classification, and SemanticCK for classification). MAKPConv is used to learn the local structure."
1027,SP:bf7d2e765c435a943ec9257cfa43d070a64c2b67,robust overfitting CONJUNCTION robustness overestimation. robustness overestimation CONJUNCTION robust overfitting. robustness overestimation CONJUNCTION robustness - accuracy trade - off. robustness - accuracy trade - off CONJUNCTION robustness overestimation. low - quality data USED-FOR adversarial training. Generic is metric. OtherScientificTerm is low - quality instances. ,"This paper studies the trade-off between robust overfitting, robustness overestimation, and robustness-accuracy trade-offs in adversarial training. The authors propose a new metric, called “low-quality instances”, to measure the impact of low-quality data on the adversarial performance. "
1028,SP:bf7d2e765c435a943ec9257cfa43d070a64c2b67,data quality USED-FOR adversarial robustness. robustness - accuracy tradeoffs CONJUNCTION robustness overestimation. robustness overestimation CONJUNCTION robustness - accuracy tradeoffs. robust overfitting CONJUNCTION robustness - accuracy tradeoffs. robustness - accuracy tradeoffs CONJUNCTION robust overfitting. data quality USED-FOR robust overfitting. metric USED-FOR data quality. OtherScientificTerm is perturbation. Metric is robustness. ,"This paper studies the effect of data quality on adversarial robustness. The authors argue that data quality plays a key role in robust overfitting, robustness-accuracy tradeoffs, and robustness overestimation. To this end, the authors propose a new metric for measuring data quality, called perturbation, which measures the impact of perturbations on the robustness of the data."
1029,SP:bf7d2e765c435a943ec9257cfa43d070a64c2b67,"robustness evaluation CONJUNCTION clean / robust accuracy trade - off. clean / robust accuracy trade - off CONJUNCTION robustness evaluation. clean / robust accuracy trade - off HYPONYM-OF angles. robustness evaluation HYPONYM-OF angles. robust overfitting HYPONYM-OF angles. AutoAttack HYPONYM-OF robust evaluations. average training robust accuracy USED-FOR quantitative definition. robust overfitting CONJUNCTION robustness evaluation. robustness evaluation CONJUNCTION robust overfitting. robustness evaluation CONJUNCTION clean / robust accuracy trade - off. clean / robust accuracy trade - off CONJUNCTION robustness evaluation. CIFAR-10/100 CONJUNCTION TinyImageNet. TinyImageNet CONJUNCTION CIFAR-10/100. TinyImageNet CONJUNCTION Adversarial Training and TRADES. Adversarial Training and TRADES CONJUNCTION TinyImageNet. OtherScientificTerm is data quality. Task is Lp - norm setting. Metric are clean and robust accuracies, and performance metrics. Generic is criterion. Method is adversarial training. ","This paper studies the trade-off between robustness evaluation and clean/robust accuracy trade-offs in the context of adversarial training. The paper proposes a quantitative definition based on the average training robust accuracy, which is a measure of the tradeoff between clean and robust accuracies. This criterion is motivated by the observation that data quality can be affected by different angles, such as robust overfitting, robust evaluation, and clean / robust accuracy trade -off. The authors propose two robust evaluations: AutoAttack and Auto-Attack, which are trained in an Lp-norm setting. The performance metrics are evaluated on CIFAR-10/100, TinyImageNet, Adversarial Training and TRADES."
1030,SP:bf7d2e765c435a943ec9257cfa43d070a64c2b67,robustness CONJUNCTION generalization. generalization CONJUNCTION robustness. generalization CONJUNCTION robustness - accuracy tradeoff. robustness - accuracy tradeoff CONJUNCTION generalization. robustness - accuracy tradeoff EVALUATE-FOR adversarially trained models. robustness EVALUATE-FOR adversarially trained models. generalization EVALUATE-FOR adversarially trained models. robustness CONJUNCTION robust overfitting. robust overfitting CONJUNCTION robustness. robustness overestimation CONJUNCTION robustness - accuracy tradeoff. robustness - accuracy tradeoff CONJUNCTION robustness overestimation. OtherScientificTerm is data quality. Generic is model. Method is training. ,"This paper studies the tradeoff between robustness and generalization in adversarially trained models. The authors argue that the trade-off is between robust overestimation and robustness-accuracy tradeoff, and that robust overfitting is the main cause of the tradeoffs. The paper also argues that the data quality of the model should be improved during training."
1031,SP:99a36b28752bfc101877bfd0da436e6fb19c69d3,neural networks USED-FOR approximating Korobov functions. Upper and lower bounds USED-FOR approximation. upper and lower bounds FEATURE-OF neural networks. shallow ( 2 - layer ) networks CONJUNCTION deep networks. deep networks CONJUNCTION shallow ( 2 - layer ) networks. ReLU(-like ) activation functions CONJUNCTION Sigmoidal activation functions. Sigmoidal activation functions CONJUNCTION ReLU(-like ) activation functions. deep networks CONJUNCTION Sigmoidal activation functions. Sigmoidal activation functions CONJUNCTION deep networks. ReLU(-like ) activation functions USED-FOR deep networks. Sigmoidal activation functions PART-OF network architectures. shallow ( 2 - layer ) networks HYPONYM-OF network architectures. deep networks HYPONYM-OF network architectures. Metric is rate. ,"This paper provides upper and lower bounds on the approximation performance of neural networks for approximating Korobov functions. The upper bounds are based on the rate of convergence and the lower bounds depend on the number of layers in the network. The authors consider various network architectures, including shallow (2-layer) networks, deep networks with ReLU(-like) activation functions, and Sigmoidal activation functions."
1032,SP:99a36b28752bfc101877bfd0da436e6fb19c69d3,"neural networks USED-FOR function class. Korobov function space HYPONYM-OF function class. local constraint FEATURE-OF local smoothness of the function space. Method is deep neural networks. OtherScientificTerm are curse of dimensionality, and matching lower bound. ","This paper studies the problem of training deep neural networks for a function class called the Korobov function space. The main contribution of this paper is to study the curse of dimensionality. The authors show that the local smoothness of the function space is subject to a local constraint, and derive a matching lower bound."
1033,SP:99a36b28752bfc101877bfd0da436e6fb19c69d3,"shallow and deep neural networks USED-FOR Korobov functions. 2 layers neural networks USED-FOR Korobov function. infinity norm FEATURE-OF Korobov function. common activation functions USED-FOR 2 layers neural networks. O(eps^{-d / r } ) parameters USED-FOR approximating Sobolev functions. 1 - hidden layer neural networks USED-FOR approximating Sobolev functions. neural networks USED-FOR Korobov functions. continuous function approximator USED-FOR Korobov space. upper bound USED-FOR NNs. Deep ReLU networks USED-FOR bandlimited functions. curse of dimensionality FEATURE-OF bandlimited functions. Deep ReLU networks USED-FOR curse of dimensionality. Metric are representation power, and L - infinity norm. OtherScientificTerm are Sobolev functions, and C^2 and non - linear activation function. Method are deep neural network, and ReLU. ","This paper studies the problem of approximating Sobolev functions with O(eps^{-d/r}) parameters using 1-hidden layer neural networks and 2 layers neural networks with common activation functions for approximating a Korobov function with infinity norm. The authors show that both shallow and deep neural networks can approximate Sobobov functions with L-infinity norm. They show that the representation power of the deep neural network is O(d^2/r) with respect to the L-incubation norm of the function. They also provide an upper bound on the L(d) and O(r) norm of NNs. The main contribution of this paper is to show that for any C^2 and non-linear activation function, a continuous function approximator can be used to approximate the Korov space of the original neural networks. Deep ReLU networks are also shown to be able to approximate bandlimited functions with curse of dimensionality, which is similar to the case of ReLU."
1034,SP:99a36b28752bfc101877bfd0da436e6fb19c69d3,neural networks USED-FOR approximating Korobov functions. multivariate functions of bounded second mixed derivatives FEATURE-OF approximating Korobov functions. NNs USED-FOR functions. continuous function approximator USED-FOR Korobov functions. # params USED-FOR continuous function approximator. shallow or deep nets USED-FOR function approximation. Method is shallow nets. OtherScientificTerm is logd. ,"This paper studies the problem of approximating Korobov functions with multivariate functions of bounded second mixed derivatives using neural networks. The authors show that NNs can approximate these functions with a continuous function approximator based on the # params. They also show that shallow or deep nets can be used for function approximation. Finally, they demonstrate that shallow nets can approximate functions with logd."
1035,SP:a0112febb28e518e87142e7cbb7e3586d06cae0b,"language structure CONJUNCTION population size. population size CONJUNCTION language structure. population heterogeneity USED-FOR emergence of structure. emergence of structure FEATURE-OF artificial agents. heterogeneity USED-FOR structure. model capacity CONJUNCTION learning speed. learning speed CONJUNCTION model capacity. model capacity USED-FOR asymmetry. learning speed USED-FOR asymmetry. ( negative ) conditional entropy CONJUNCTION topographic similarity. topographic similarity CONJUNCTION ( negative ) conditional entropy. topographic similarity CONJUNCTION generalization. generalization CONJUNCTION topographic similarity. speakers synchronization CONJUNCTION ( negative ) conditional entropy. ( negative ) conditional entropy CONJUNCTION speakers synchronization. properties FEATURE-OF emergent language. ( negative ) conditional entropy HYPONYM-OF properties. generalization HYPONYM-OF properties. speakers synchronization HYPONYM-OF properties. topographic similarity HYPONYM-OF properties. Task are sociolinguistic literature, and language emergence. OtherScientificTerm are systematic languages, capacity, network capacity, training speed, population of heterogeneous agents, and learning speeds. Method are machine learning, and Lewis game. Metric is training accuracy. Generic is metrics. ","This paper studies the emergence of structure in artificial agents under population heterogeneity, which is a well-studied phenomenon in the sociolinguistic literature. In particular, the authors study the relationship between language structure and population size, which has been studied in the context of machine learning. The authors argue that there are two main factors that influence language emergence: (1) model capacity and (2) learning speed. In the case of systematic languages, they argue that model capacity is responsible for the asymmetry between the language structure of the language and the population size. In contrast, in systematic languages the capacity is controlled by the network capacity, while the learning speed is driven by the training speed. They argue that in the Lewis game, the training accuracy of a Lewis game is a function of the population of heterogeneous agents, and that learning speeds are correlated with the number of agents in the population. They also argue that the properties of emergent language, such as speakers synchronization, (negative) conditional entropy, topographic similarity, generalization, etc., are related to the population heterogeneity. They provide empirical evidence that these metrics are predictive of language emergence."
1036,SP:a0112febb28e518e87142e7cbb7e3586d06cae0b,"population heterogeneity USED-FOR emergent communication. population size USED-FOR deep language emergence. simpler grammars CONJUNCTION idiosyncratic languages. idiosyncratic languages CONJUNCTION simpler grammars. population size USED-FOR neural emergent communication. neg - entropy CONJUNCTION generalization. generalization CONJUNCTION neg - entropy. topographic similarity USED-FOR compositional languages. Material are natural language, natural communication, and aligned languages. Task is structure of neural emergent communication. OtherScientificTerm are artificial populations, asymmetries, and heterogeneous populations. ","This paper studies the effect of population heterogeneity on emergent communication in the context of deep language emergence. The authors argue that natural language is not monotone, and that natural communication can be expressed in simpler grammars and more idiosyncratic languages. They argue that the population size is important to the success of the field in the sense that it can be used as a proxy for the structure of neural emergent. They show empirically that increasing the number of artificial populations can lead to an increase in neg-entropy and generalization. They also show that compositional languages with topographic similarity are more likely to emerge in the presence of asymmetries and that aligned languages are less likely to arise in heterogeneous populations."
1037,SP:a0112febb28e518e87142e7cbb7e3586d06cae0b,"population size CONJUNCTION systematicity measures. systematicity measures CONJUNCTION population size. NLP CONJUNCTION cognitive science. cognitive science CONJUNCTION NLP. emergent communication CONJUNCTION NLP. NLP CONJUNCTION emergent communication. OtherScientificTerm are emergent languages, languages, and second - language learners. Material are socio- and psycho - linguistics, and sociolingusitic literature. ","This paper studies the problem of learning emergent languages. The authors focus on the question of how languages can be learned by second-language learners. They consider the problem from the perspective of socio- and psycho-linguistics, where the authors consider population size and systematicity measures. They draw connections between emergent communication, NLP, cognitive science, and NLP. They also draw connections to the sociolingusitic literature."
1038,SP:a0112febb28e518e87142e7cbb7e3586d06cae0b,"neural - based models USED-FOR languages. population heterogeneity COMPARE identically distributed specifications. identically distributed specifications COMPARE population heterogeneity. population heterogeneity PART-OF neural models. Material are emerging languages, and structured languages. ","This paper studies the effect of population heterogeneity in neural models on the performance of languages learned by neural-based models for languages. The authors argue that population heterogeneity is more important than identically distributed specifications, and that this is particularly important for emerging languages, where the number of languages is increasing rapidly. They also argue that this phenomenon is not limited to structured languages, but can also be observed in more structured languages."
1039,SP:462112ea1a59ab8101ed9d908c5d838edeb844ca,polynomial graph filters USED-FOR graph neural networks. homophily CONJUNCTION heterophily. heterophily CONJUNCTION homophily. low - pass filters USED-FOR former. high - pass filters USED-FOR latter. spline polynomials USED-FOR spectral - domain filtering. spline polynomials COMPARE ` ` global '' polynomials. ` ` global '' polynomials COMPARE spline polynomials. ` ` global '' polynomials USED-FOR graph filter design. spectrum FEATURE-OF graph matrix. penalty function USED-FOR continuity. eigendecomposition USED-FOR graph matrix. complexity EVALUATE-FOR eigendecomposition. coarse partition of the spectrum USED-FOR approach. full eigendecomposition USED-FOR approach. OtherScientificTerm is graph. Generic is polynomials. Method is spline spectral filter. ,"This paper studies the problem of learning polynomial graph filters for graph neural networks. In particular, the authors consider spectral-domain filtering with spline polynomials, where the former uses low-pass filters, and the latter uses high-throughput filters. The authors argue that the former is more suitable for homophily and heterophily, while the latter is better suited for spectral-domains. They show that the spline spectral filter can be viewed as a special case of the `global'' polynmials that have been used in graph filter design, which is a generalization of `global` polynomial polynoms. The paper also shows that the complexity of eigendecomposition of the graph matrix in the spectrum of a graph matrix can be reduced to the same complexity as that of the original graph matrix. The proposed approach is based on a coarse partition of the spectrum, which can be seen as an extension of the full eigenvalue of the poleomials. A penalty function is also proposed to encourage continuity."
1040,SP:462112ea1a59ab8101ed9d908c5d838edeb844ca,node classification CONJUNCTION semi - supervised learning. semi - supervised learning CONJUNCTION node classification. heterophilic graphs USED-FOR node classification. low - pass filter - based GNNs USED-FOR heterophilic graphs. they USED-FOR neighboring node features. filters USED-FOR high - frequency content. OtherScientificTerm is low - order polynomials. ,"This paper studies the problem of node classification and semi-supervised learning on heterophilic graphs trained with low-pass filter-based GNNs. The authors propose to use low-order polynomials as filters to remove high-frequency content from neighboring node features, so that they can be used to extract more meaningful and informative information from nearby node features."
1041,SP:462112ea1a59ab8101ed9d908c5d838edeb844ca,PP - GNN HYPONYM-OF graph neural network. graph neural network USED-FOR multiple adaptive polynomial filters. algorithms USED-FOR generating top and bottom eigen components. GPR - GNN CONJUNCTION algorithms. algorithms CONJUNCTION GPR - GNN. algorithms USED-FOR eigendecomposition. piece - wise polynomial method USED-FOR latent optimal filter. OtherScientificTerm is eigenvalues. ,"This paper proposes PP-GNN, a graph neural network that learns multiple adaptive polynomial filters. GPR-GAN and two algorithms for generating top and bottom eigen components are proposed for eigendecomposition. The latent optimal filter is learned by a piece-wise polynomially method, where the eigenvalues are sampled from a fixed set."
1042,SP:462112ea1a59ab8101ed9d908c5d838edeb844ca,GNN USED-FOR prediction task. homophily HYPONYM-OF prediction task. eigenvalues FEATURE-OF sum of polynomials. sum of polynomials USED-FOR filter function. diverse node classification tasks EVALUATE-FOR GNN architecture. Method is polynomial filter. ,This paper proposes a novel GNN for the prediction task called homophily. The proposed filter function is a sum of polynomials over the eigenvalues of a polynomial filter. Experiments on diverse node classification tasks demonstrate the effectiveness of the proposed GNN architecture.
1043,SP:903545b1b340ec5c13070e0f25f550c444de4124,"distance resampling step USED-FOR optimization. distance resampling step USED-FOR method. betweenness centrality based random walk USED-FOR method. embedding method USED-FOR shortest distance relation. algorithm COMPARE algorithms. algorithms COMPARE algorithm. accuracy EVALUATE-FOR algorithms. accuracy EVALUATE-FOR algorithm. Method is graph shortest distance embedding method. OtherScientificTerm are graph, and estimated distance. ","This paper proposes a graph shortest distance embedding method. The proposed method is based on betweenness centrality based random walk. The method uses a distance resampling step during optimization to reduce the number of steps required to compute the shortest distance relation between two nodes in the graph. The authors also propose a new embeddings method to estimate the shortest distances between nodes in a graph. In the experiments, the proposed algorithm is shown to outperform existing algorithms in terms of accuracy and the estimated distance."
1044,SP:903545b1b340ec5c13070e0f25f550c444de4124,graph embeddings USED-FOR answering shortest - distance queries ( SDQs ). method USED-FOR graph embeddings. betweennes centrality distance sampling USED-FOR method. betweenness centrality USED-FOR distance - oriented embedding. walk paths USED-FOR Distance. pointwise mutual information ( PMI ) optimization USED-FOR methods. Metric is betweenness centrality measures. Task is distance calculations. ,"This paper proposes a method for learning graph embeddings for answering shortest-distance queries (SDQs). The method is based on betweennes centrality distance sampling, where the distance-oriented embedding is learned using betweenness centrality measures. Distance is estimated using walk paths, and the distance calculations are performed using pointwise mutual information (PMI) optimization."
1045,SP:903545b1b340ec5c13070e0f25f550c444de4124,framework USED-FOR node embeddings. framework USED-FOR shortest path distances. shortest path distances FEATURE-OF undirected graphs. node embeddings USED-FOR shortest path distances. shortest path distances CONJUNCTION betweenness centrality scores. betweenness centrality scores CONJUNCTION shortest path distances. distance resampling strategy USED-FOR shortest path distances. betweenness centrality scores USED-FOR distance resampling strategy. shortest path distances USED-FOR distance resampling strategy. betweenness centralities USED-FOR random walk framework. it USED-FOR shortest path distances. node embedding methods COMPARE it. it COMPARE node embedding methods. ,"This paper proposes a framework for learning node embeddings for shortest path distances in undirected graphs. The authors propose a distance resampling strategy based on shortest path distance and betweenness centrality scores, which are used in the random walk framework. The experiments show that it outperforms existing node embedding methods in terms of finding the longest path distances."
1046,SP:903545b1b340ec5c13070e0f25f550c444de4124,truncated random walks CONJUNCTION point - wise Mutual Information ( PMI ). point - wise Mutual Information ( PMI ) CONJUNCTION truncated random walks. technique COMPARE embedding - based distance prediction methods. embedding - based distance prediction methods COMPARE technique. truncated random walks USED-FOR embedding - based distance prediction methods. point - wise Mutual Information ( PMI ) USED-FOR embedding - based distance prediction methods. local optima FEATURE-OF preservation of the shortest distance relation. Distance Resampling ( DR ) USED-FOR relationship of distances. Distance Resampling ( DR ) COMPARE PMI. PMI COMPARE Distance Resampling ( DR ). betweenness centrality USED-FOR random walk. Distance Resampling ( DR ) USED-FOR BCDR. betweenness centrality USED-FOR BCDR. theoretical guarantees of performance USED-FOR exploration range. exploration distance CONJUNCTION preservation of distance relation. preservation of distance relation CONJUNCTION exploration distance. baselines CONJUNCTION exploration distance. exploration distance CONJUNCTION baselines. real datasets CONJUNCTION simulated datasets. simulated datasets CONJUNCTION real datasets. preservation of distance relation CONJUNCTION probability distance relation. probability distance relation CONJUNCTION preservation of distance relation. Method is Shortest Distance Query technique. Generic is drawbacks. OtherScientificTerm is limited distance exploration. ,"This paper proposes a Shortest Distance Query technique. The technique is similar to embedding-based distance prediction methods with truncated random walks and point-wise Mutual Information (PMI). The main difference is that BCDR uses Distance Resampling (DR) to model the relationship of distances instead of PMI, which allows for better preservation of the shortest distance relation in local optima. The authors also provide theoretical guarantees of performance for the exploration range. Experiments are conducted on real datasets and simulated datasets, comparing against baselines, exploration distance, preservation of distance relation, and probability distance relation. The main drawbacks of BCDR are the limited distance exploration and the lack of betweenness centrality for the random walk."
1047,SP:13db440061fed785f05bb41d0767225403ecf7a1,"benchmark CONJUNCTION metric. metric CONJUNCTION benchmark. metric USED-FOR retention of time - invariant world knowledge. benchmark USED-FOR retention of time - invariant world knowledge. metric USED-FOR acquisition of new knowledge. regularization CONJUNCTION rehearsal. rehearsal CONJUNCTION regularization. rehearsal CONJUNCTION parameter expansion methods. parameter expansion methods CONJUNCTION rehearsal. parameter expansion methods HYPONYM-OF training methodologies. regularization HYPONYM-OF training methodologies. rehearsal HYPONYM-OF training methodologies. Task are continual knowledge learning of language models, and continual knowledge learning problem. Material is CKL benchmark. Metric is FUAR metric. ","This paper studies the continual knowledge learning of language models. The authors propose a new benchmark and a new metric for the retention of time-invariant world knowledge. The new benchmark is based on the CKL benchmark and the new metric is designed for the acquisition of new knowledge. In addition to the new benchmark, the authors also introduce a new FUAR metric to measure the quality of the new knowledge acquired during the training process. The paper is well-written and well-motivated. However, the paper suffers from a lack of theoretical analysis and analysis. The main limitation of the paper is the lack of comparison to existing methods for the continual learning learning problem. The training methodologies considered are regularization, rehearsal, and parameter expansion methods."
1048,SP:13db440061fed785f05bb41d0767225403ecf7a1,"continual knowledge learning ( CKL ) HYPONYM-OF continual learning problem setup. slot filling - based knowledge probing tasks USED-FOR benchmark. LAMA analysis HYPONYM-OF slot filling - based knowledge probing tasks. regularization CONJUNCTION rehearsal. rehearsal CONJUNCTION regularization. rehearsal CONJUNCTION parameter expansion. parameter expansion CONJUNCTION rehearsal. parameter expansion HYPONYM-OF CL methods. regularization HYPONYM-OF CL methods. rehearsal HYPONYM-OF CL methods. LMs EVALUATE-FOR CKL methods. T5 CONJUNCTION GPT. GPT CONJUNCTION T5. Generic is benchmark resource. OtherScientificTerm are learning rate, and forgetting. ","This paper proposes a continual learning problem setup called continual knowledge learning (CKL). The benchmark is based on slot filling-based knowledge probing tasks (e.g., LAMA analysis). The main contribution of the paper is the introduction of a new benchmark resource, which is designed to evaluate the performance of CKL methods on LMs. The paper also provides a theoretical analysis of the effect of different CL methods (regularization, rehearsal, and parameter expansion) on the learning rate and the forgetting. Experiments are conducted on T5 and GPT."
1049,SP:13db440061fed785f05bb41d0767225403ecf7a1,"continuous learning USED-FOR language models. Material is LAMA tasks. Method are CL algorithms, and continuous LM learning. Generic is metric. Metric is FUAR. OtherScientificTerm is updated or newly acquired knowledge. ","This paper studies continuous learning for language models. The authors propose a new metric called FUAR, which measures the difference between the performance of CL algorithms trained with and without updated or newly acquired knowledge. The paper presents results on a variety of LAMA tasks, and shows that continuous LM learning can lead to better performance."
1050,SP:13db440061fed785f05bb41d0767225403ecf7a1,Continual Knowledge Learning ( CKL ) HYPONYM-OF continual learning ( CL ) problem. update of old knowledge CONJUNCTION acquisition of new knowledge. acquisition of new knowledge CONJUNCTION update of old knowledge. retention of time - invariant world knowledge CONJUNCTION update of old knowledge. update of old knowledge CONJUNCTION retention of time - invariant world knowledge. sub - tasks PART-OF CKL. update of old knowledge HYPONYM-OF sub - tasks. retention of time - invariant world knowledge HYPONYM-OF sub - tasks. acquisition of new knowledge HYPONYM-OF sub - tasks. metric EVALUATE-FOR models. benchmark EVALUATE-FOR models. benchmark CONJUNCTION metric. metric CONJUNCTION benchmark. metric EVALUATE-FOR sub - tasks. models USED-FOR sub - tasks. knowledge forgetting FEATURE-OF CKL. Generic is CL setups. ,"Continual Knowledge Learning (CKL) is a continual learning (CL) problem. CKL consists of three sub-tasks: (1) retention of time-invariant world knowledge, (2) update of old knowledge and (3) acquisition of new knowledge. The paper proposes a benchmark and a metric to evaluate the performance of different models on these sub-teams. The authors conduct experiments on several CL setups and show that CKL suffers from knowledge forgetting."
1051,SP:639fd88482330389019fb5be7446a909b99a8609,"approach USED-FOR decision tree induction. sub - sampling and pruning heuristics USED-FOR approach. sub - par performance EVALUATE-FOR pruning features. datasets EVALUATE-FOR method. image data USED-FOR Haar features. OtherScientificTerm are tree, feature, O(N_j ) time, subsample size, and pruning sub - par performing features. Metric is computational complexity. Generic is idea. ","This paper proposes an approach to decision tree induction based on sub-sampling and pruning heuristics. The idea is to prune features with sub-par performance and then use the tree to generate a new feature that performs better. The computational complexity of the idea is O(N_j) time, which is reduced by reducing the subsample size. The method is evaluated on two datasets and compared to Haar features trained on image data. The results show that pruning sub-performing features leads to better performance."
1052,SP:639fd88482330389019fb5be7446a909b99a8609,stochastic ( i.e. approximate ) algorithm USED-FOR node splitting. node splitting USED-FOR tree induction. method USED-FOR irrelevant features. computing times EVALUATE-FOR tree construction. tree construction EVALUATE-FOR approach. accuracy EVALUATE-FOR approach. computing times EVALUATE-FOR approach. OtherScientificTerm is features. ,This paper proposes a stochastic (i.e. approximate) algorithm for node splitting in tree induction. The proposed method is able to remove irrelevant features while preserving other features. The approach is shown to improve accuracy and reduce computing times in tree construction.
1053,SP:639fd88482330389019fb5be7446a909b99a8609,"numeric feature PART-OF axis - parallel decision trees. runtime EVALUATE-FOR iterative sub - sampling procedure. features USED-FOR split selection. split metric EVALUATE-FOR algorithm. algorithm USED-FOR consistent estimator. chi - square CONJUNCTION mRmR. mRmR CONJUNCTION chi - square. high - dimensional datasets EVALUATE-FOR method. method USED-FOR decision trees. decision trees COMPARE trees. trees COMPARE decision trees. method COMPARE trees. trees COMPARE method. mRmR USED-FOR pre - filtering features. chi - square USED-FOR pre - filtering features. pre - filtering features USED-FOR trees. MNIST CONJUNCTION F - MNIST. F - MNIST CONJUNCTION MNIST. Haar features USED-FOR trees. OtherScientificTerm are feature, objective function, and nodes. Task is binary classification scenario. Metric is training time. ","This paper proposes an iterative sub-sampling procedure to reduce the runtime of axis-parallel decision trees by adding a numeric feature to each node. The feature is used to compute the split of the objective function, which is then used for split selection of the features for the split selection in the binary classification scenario. The proposed algorithm is evaluated on the split metric and is shown to be a consistent estimator. Experiments on high-dimensional datasets show that the proposed method outperforms existing decision trees with Haar features as well as trees with pre-filtering features using chi-square and mRmR. The experiments are conducted on MNIST and F-MNIST, where the training time is also shown to decrease with the number of nodes."
1054,SP:639fd88482330389019fb5be7446a909b99a8609,computational efficiency EVALUATE-FOR construction of a decision tree. row sub - sampling CONJUNCTION adaptive column subsampling. adaptive column subsampling CONJUNCTION row sub - sampling. scheme USED-FOR computational cost. adaptive column subsampling PART-OF scheme. row sub - sampling PART-OF scheme. computational gain CONJUNCTION loss function. loss function CONJUNCTION computational gain. scheme COMPARE baselines. baselines COMPARE scheme. decision tree USED-FOR Haar features of images. tradeoffs EVALUATE-FOR baselines. Task is tree construction. Generic is application. ,"This paper studies the computational efficiency of the construction of a decision tree. The authors propose a scheme to reduce the computational cost by combining row sub-sampling and adaptive column subsampling. The computational gain and loss function are discussed and compared to baselines with different tradeoffs. The proposed scheme is applied to the problem of Haar features of images, where the decision tree is used to reconstruct the original image. The experimental results show that the proposed tree construction is effective in this application."
1055,SP:7f2640f18294519a5abb1daaa226800d2377a5e0,approach USED-FOR learning rate scheduling. scheduler USED-FOR minimax optimal rate. scheduler USED-FOR noisy quadratic problem. minimax optimal rate FEATURE-OF noisy quadratic problem. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. ImageNet EVALUATE-FOR scheduler. CIFAR-10 EVALUATE-FOR scheduler. Method is Eigencurve. OtherScientificTerm is eigenvalues of the Hessian. ,This paper proposes an approach for learning rate scheduling. The proposed scheduler aims to find the minimax optimal rate for solving a noisy quadratic problem. The main idea is to use Eigencurve to estimate the eigenvalues of the Hessian. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of the scheduler.
1056,SP:7f2640f18294519a5abb1daaa226800d2377a5e0,stepsize schemes USED-FOR linear regression. convergence rates EVALUATE-FOR SGD. SGD USED-FOR linear regression. stepsize schemes USED-FOR SGD. eigenvalue distribution of the Hessian USED-FOR stepsize scheme. deep neural network benchmarks EVALUATE-FOR stepsize schemes. Metric is minimax rate. OtherScientificTerm is Hessian. Generic is method. ,This paper studies the convergence rates of SGD with different stepsize schemes for linear regression. The authors propose a new stepsize scheme based on the eigenvalue distribution of the Hessian and show that the minimax rate can be bounded by the number of iterations of the stepsize. The method is evaluated on several deep neural network benchmarks.
1057,SP:7f2640f18294519a5abb1daaa226800d2377a5e0,convergence FEATURE-OF SGD. minimax optimal convergence rates FEATURE-OF quadratic objectives. learning rate schedules USED-FOR SGD. learning rate schedules USED-FOR convergence. learning rate schedule USED-FOR optimal last - iterate convergence rate. Eigencurve HYPONYM-OF learning rate schedule. Hessian spectrum USED-FOR learning rate schedule. Eigencurve USED-FOR image classification tasks. deep neural networks USED-FOR image classification tasks. schedule COMPARE cosine learning rate schedule. cosine learning rate schedule COMPARE schedule. Method is cosine schedule. ,"This paper studies the convergence of SGD with different learning rate schedules. The authors first show that the minimax optimal convergence rates for quadratic objectives can be obtained by optimizing a specific learning rate schedule (e.g., Eigencurve) on the Hessian spectrum. Then, the authors show that this learning rate can be used to approximate the optimal last-iterate convergence rate. Finally, they apply EigenCurve to several image classification tasks with deep neural networks and show that their proposed schedule can converge faster than the cosine schedule."
1058,SP:7f2640f18294519a5abb1daaa226800d2377a5e0,"Eigencurve HYPONYM-OF ( non)-convex optimization method. step - size schedule USED-FOR stochastic gradient descent. eigenvalue spectrum of Hessian USED-FOR step - size schedule. eigenspectrum FEATURE-OF Hessian. minimax optimal convergence rate EVALUATE-FOR stochastic gradient descent. quadratic functions FEATURE-OF stochastic gradient descent. power law USED-FOR eigenspectrum. minimax optimal convergence rate EVALUATE-FOR approach. step - decay schedule USED-FOR Eigencurve. Eigencurve USED-FOR ( non - convex ) optimization. Eigencurve USED-FOR neural network architectures. neural network architectures USED-FOR ( non - convex ) optimization. CIFAR-10 and ImageNet datasets EVALUATE-FOR neural network architectures. Metric is train loss / test accuracy. OtherScientificTerm are decay condition, and quadratics. Generic is it. Method is step - decay. ","This paper proposes Eigencurve, a (non)-convex optimization method. The main idea is to use a step-size schedule based on the eigenvalue spectrum of Hessian for stochastic gradient descent with quadratic functions. This approach achieves the minimax optimal convergence rate of stochastically gradient descent in terms of train loss/test accuracy under the decay condition. The eigenspectrum of the Hessian is defined by the power law, and it is shown that the step-decay is equivalent to a linear combination of the quadratics. The paper also shows that the proposed approach can be extended to the case of EigenCurve with the same or a different type of step-crystalization schedule. Empirically, the paper shows that Eigenurve can be used to train neural network architectures on the CIFAR-10 and ImageNet datasets."
1059,SP:8cfafcf0de6de33a8fd298593eeea82376b4697a,hyperparameter decisions USED-FOR offline model - based reinforcement learning methods. penalty weighting CONJUNCTION rollout horizon. rollout horizon CONJUNCTION penalty weighting. ensemble size CONJUNCTION penalty weighting. penalty weighting CONJUNCTION ensemble size. uncertainty penalties USED-FOR methods. rollout horizon HYPONYM-OF hyperparameters. penalty weighting HYPONYM-OF hyperparameters. ensemble size HYPONYM-OF hyperparameters. it COMPARE MOPO. MOPO COMPARE it. MOPO COMPARE hyperparameters. hyperparameters COMPARE MOPO. Bayesian optimization USED-FOR hyperparameters. Method is offline MBRL methods. ,"This paper studies the hyperparameter decisions in offline model-based reinforcement learning methods. The authors focus on three types of hyperparameters: ensemble size, penalty weighting, and rollout horizon. In particular, they focus on methods with uncertainty penalties. They compare the performance of three offline MBRL methods and show that it outperforms MOPO in all cases. They also show that Bayesian optimization can be used to find the best choice of these three types for these three different types of parameters."
1060,SP:8cfafcf0de6de33a8fd298593eeea82376b4697a,model USED-FOR uncertainty estimation. pessimistic MDP USED-FOR Model - based offline reinforcement learning algorithms. uncertainty estimation of the learned model USED-FOR pessimistic MDP. estimated uncertainty CONJUNCTION ground truth model error. ground truth model error CONJUNCTION estimated uncertainty. bayesian optimization USED-FOR hyperparameter configuration. Generic is approaches. ,Model-based offline reinforcement learning algorithms are trained in a pessimistic MDP with uncertainty estimation of the learned model. The uncertainty estimation is done by minimizing the difference between the estimated uncertainty and the ground truth model error. The hyperparameter configuration is learned using bayesian optimization. The authors compare their approaches to several existing methods.
1061,SP:8cfafcf0de6de33a8fd298593eeea82376b4697a,"uncertainty quantification heuristics USED-FOR model learning. model learning USED-FOR offline model - based reinforcement learning. uncertainty quantification heuristics USED-FOR offline model - based reinforcement learning. uncertainty based state - action penalty function USED-FOR pessimistic MDP. reward USED-FOR pessimistic MDP. reward USED-FOR uncertainty based state - action penalty function. uncertainty based state - action penalty function PART-OF MOPO. prediction error FEATURE-OF model. planning horizon CONJUNCTION penalty weights. penalty weights CONJUNCTION planning horizon. hyperparameters USED-FOR Bayesian optimization. uncertainty penalty USED-FOR Bayesian optimization. penalty weights HYPONYM-OF hyperparameters. planning horizon HYPONYM-OF hyperparameters. OtherScientificTerm are uncertainty penalties, high - percentile prediction errors, and penalty. Material is offline dataset. ","This paper proposes uncertainty quantification heuristics for model learning in offline model-based reinforcement learning. Specifically, the authors propose an uncertainty based state-action penalty function in MOPO, which is a reward for the pessimistic MDP. The uncertainty penalties are designed to penalize high-percentile prediction errors. The authors also propose to use the uncertainty penalty to improve Bayesian optimization by adjusting the hyperparameters such as the planning horizon and penalty weights. Finally, they conduct experiments on an offline dataset to demonstrate the effectiveness of the proposed penalty."
1062,SP:8cfafcf0de6de33a8fd298593eeea82376b4697a,heuristics USED-FOR model - based offline RL techniques. planning horizon HYPONYM-OF hyperparameters. OtherScientificTerm is uncertainty of the estimated MDP. Generic is ensemble. ,"This paper proposes two heuristics to improve model-based offline RL techniques. The first is to reduce the uncertainty of the estimated MDP, and the second is to change the hyperparameters (i.e., the planning horizon) of the ensemble. "
1063,SP:3833662cf92249d83e65a1200f9e2890b5b23e95,"model COMPARE Q - value. Q - value COMPARE model. reward CONJUNCTION dynamics model. dynamics model CONJUNCTION reward. discrete action space ( DQN ) CONJUNCTION continuous action space ( SAC ). continuous action space ( SAC ) CONJUNCTION discrete action space ( DQN ). model - free RL algorithms ( SAC ) CONJUNCTION model - based RL algorithms ( MBPO ). model - based RL algorithms ( MBPO ) CONJUNCTION model - free RL algorithms ( SAC ). MaPER USED-FOR continuous action space ( SAC ). MaPER USED-FOR discrete action space ( DQN ). model - based RL algorithms ( MBPO ) CONJUNCTION sparse reward tasks. sparse reward tasks CONJUNCTION model - based RL algorithms ( MBPO ). MaPER USED-FOR baseline algorithms. MaPER USED-FOR sparse reward tasks. Task is experience replay. Method are experience replay method, critic network, and Model - augmented Critic Network ( MaCN ). ","This paper studies the problem of experience replay and proposes a new experience replay method called MaPER. The main idea of MaPER is to replace the reward and dynamics model with a model that is more expressive than the original Q-value. The authors propose a critic network, called Model-augmented Critic Network (MaCN), which is trained to predict the next state of the art in a discrete action space (DQN, continuous action space, SAC) and model-free RL algorithms (SAC) as well as model-based RL algorithms(MBPO) and other sparse reward tasks using MaPER as baseline algorithms. "
1064,SP:3833662cf92249d83e65a1200f9e2890b5b23e95,method USED-FOR prioritizing experiences. prioritizing experiences USED-FOR prioritized experience replay ( PER ) method. method USED-FOR prioritized experience replay ( PER ) method. reward function CONJUNCTION transition function. transition function CONJUNCTION reward function. critic network USED-FOR reward function. critic network USED-FOR transition function. absolute TD error USED-FOR priorities. priorities PART-OF PER method. Mujoco and Atari environments EVALUATE-FOR method. Generic is approach. ,"This paper proposes a method for prioritizing experiences in the prioritized experience replay (PER) method. The proposed approach consists of two steps: (1) a critic network is trained to predict the reward function and the transition function, and (2) the absolute TD error is used to select the priorities in the PER method. Experiments on Mujoco and Atari environments demonstrate the effectiveness of the proposed method."
1065,SP:3833662cf92249d83e65a1200f9e2890b5b23e95,"critic network USED-FOR transition model. MaPER COMPARE state - of - the - art techniques. state - of - the - art techniques COMPARE MaPER. MuJoC domains EVALUATE-FOR MaPER. MuJoC domains EVALUATE-FOR state - of - the - art techniques. error values USED-FOR prioritization. network USED-FOR methods. Method is prioritized sweeping. Task are replay, and ablation study. OtherScientificTerm is TD - error. ","This paper proposes prioritized sweeping, where a critic network is used to train a transition model. The authors show that MaPER outperforms state-of-the-art techniques on MuJoC domains. They also show that the error values for prioritization are correlated with the TD-error. Finally, the authors perform an ablation study to show that methods trained with the same network can achieve better performance."
1066,SP:3833662cf92249d83e65a1200f9e2890b5b23e95,dynamics prediction error USED-FOR priority calculation. method USED-FOR model - based and model - free setting. Method is Model - augmented Prioritized Experience Replay. OtherScientificTerm is priority. ,"This paper proposes Model-augmented Prioritized Experience Replay, a method that can be applied to both model-based and model-free setting. The key idea is to use the dynamics prediction error as the priority calculation instead of the original priority. The paper also provides a theoretical analysis of the proposed method."
1067,SP:0db83e057c21ac10fe91624876498d8456797492,"agent USED-FOR expert behavior. offline RL USED-FOR agent. HACO USED-FOR agent. offline RL USED-FOR HACO. negative reward USED-FOR state transitions. negative reward USED-FOR HACO. imitation learning CONJUNCTION offline RL. offline RL CONJUNCTION imitation learning. HACO COMPARE baseline methods. baseline methods COMPARE HACO. HACO USED-FOR agent. success rates EVALUATE-FOR baseline methods. training data USED-FOR baseline methods. cumulative training cost EVALUATE-FOR baseline methods. success rates EVALUATE-FOR agent. imitation learning USED-FOR baseline methods. offline RL USED-FOR baseline methods. success rates EVALUATE-FOR HACO. OtherScientificTerm are expert interventions, simulated driving environment, human interventions, human intervention, and human in the loop. Generic is interventions. Task is simulated driving task. ","This paper proposes HACO, an agent that learns to imitate expert behavior using offline RL. The key idea is to use a negative reward for state transitions to encourage the agent to imitate the expert interventions. The agent is trained in a simulated driving environment, where the agent is given a set of human interventions and is encouraged to imitate these interventions. In the simulated driving task, a human intervention is given to the agent and a human in the loop is trained to predict the next state. The experiments show that HACOO outperforms baseline methods that use imitation learning and offline RL, and achieves higher success rates and lower cumulative training cost with less training data."
1068,SP:0db83e057c21ac10fe91624876498d8456797492,algorithm USED-FOR Human - AI Copilot Optimization. learned value function HYPONYM-OF multi - task objective. exploratory policy HYPONYM-OF multi - task objective. multi - task objective USED-FOR HACO learned policy. human interventions USED-FOR learned value function. Method is HACO. OtherScientificTerm is environment training timesteps. ,"This paper proposes an algorithm for Human-AI Copilot Optimization called HACO, which is an extension of an existing algorithm called Human-Agnostic Autoencoders (HACO). The main idea is to use a multi-task objective, i.e., an exploratory policy, to train the learned policy using a multi -task objective such as a learned value function with human interventions. The main contribution of the paper is that the learned value functions can be trained using human interventions, and that the environment training timesteps can be used to improve the performance."
1069,SP:0db83e057c21ac10fe91624876498d8456797492,method USED-FOR driving policy learning. human - AI copilot USED-FOR method. human interventions USED-FOR algorithm. algorithm USED-FOR unsafe events. OtherScientificTerm is human intervention. Method is copilot learning method. ,This paper proposes a method for driving policy learning based on a human-AI copilot. The algorithm is trained with human interventions and is able to avoid unsafe events without human intervention. The copilot learning method is evaluated on a variety of datasets.
1070,SP:0db83e057c21ac10fe91624876498d8456797492,HACO USED-FOR imitative driving policy. no - reward assumption USED-FOR HACO. CQL USED-FOR HACO. HACO USED-FOR proxy action - value function. entropy term USED-FOR exploration. entropy term USED-FOR It. cosine difference USED-FOR accumulative intervention cost. proxy action - value USED-FOR policy. closed - loop driving simulator EVALUATE-FOR HACO. HACO COMPARE RL methods. RL methods COMPARE HACO. HACO COMPARE imitation and offline RL baseline. imitation and offline RL baseline COMPARE HACO. environment rewards USED-FOR RL methods. It COMPARE RL methods. RL methods COMPARE It. OtherScientificTerm is human interventions. ,"This paper proposes HACO, an imitative driving policy that operates under a no-reward assumption. It uses CQL to learn a proxy action-value function, and uses an entropy term to encourage exploration. The accumulative intervention cost is modeled as a cosine difference between the policy’s proxy action and human interventions. Experiments are conducted on a closed-loop driving simulator, and the results show that the proposed policy is able to achieve state-of-the-art performance. It is also compared to other RL methods trained with environment rewards, and compared to an imitation and offline RL baseline."
1071,SP:7fda4f67daf3eb27cdfafe8f8a3f8d719da956c3,"high level policy network USED-FOR subskill policy. ablations EVALUATE-FOR approach. meta - world benchmark EVALUATE-FOR ablations. meta - world benchmark EVALUATE-FOR approach. Method are meta imitation learning ( DMIL ), high and low level policy networks, and DMIL. OtherScientificTerm is meta - test time. ","This paper proposes meta imitation learning (DMIL), where the high level policy network is trained to learn a subskill policy. The high and low level policy networks are trained in parallel and the meta-test time is split into two steps. The proposed approach is evaluated on ablations on a meta-world benchmark. The results show that the proposed DMIL can achieve state-of-the-art performance."
1072,SP:7fda4f67daf3eb27cdfafe8f8a3f8d719da956c3,"meta imitation learning framework USED-FOR long - horizon robot control tasks. model - agnostic meta learning framework USED-FOR hierarchical policy. approach USED-FOR model - agnostic meta learning framework. metaworld benchmark EVALUATE-FOR method. qualitative analysis EVALUATE-FOR method. metaworld benchmark EVALUATE-FOR method. OtherScientificTerm are fast adaptation capabilities, and hierarchy. ",This paper proposes a meta imitation learning framework for long-horizon robot control tasks. The proposed approach is a model-agnostic meta learning framework that learns a hierarchical policy that learns to adapt to new environments with fast adaptation capabilities. The method is evaluated on the metaworld benchmark and qualitative analysis is conducted to show the effectiveness of the proposed hierarchy.
1073,SP:7fda4f67daf3eb27cdfafe8f8a3f8d719da956c3,"Dual Meta Imitation Learning ( DMIL ) HYPONYM-OF hierarchical meta imitation learning method. MAML - based imitation learning ( IL ) USED-FOR DMIL. meta - world benchmark EVALUATE-FOR few - shot imitation learning. OtherScientificTerm is high - level network. Method are Hierarchical Imitation learning ( HIL ), and Expectation - Maximization ( EM ) algorithm. ","This paper proposes a hierarchical meta imitation learning method called Dual Meta Imitation Learning (DMIL). DMIL is based on MAML-based imitation learning (IL), where the high-level network is trained using Hierarchical Imitation learning (HIL) and the Expectation-Maximization (EM) algorithm is used. Experiments are conducted on a meta-world benchmark for few-shot imitation learning."
1074,SP:7fda4f67daf3eb27cdfafe8f8a3f8d719da956c3,"high - level policy CONJUNCTION low - level policies. low - level policies CONJUNCTION high - level policy. approach USED-FOR few - shot imitation learning. MetaWorld50 task EVALUATE-FOR approach. approach COMPARE few - shot imitation methods. few - shot imitation methods COMPARE approach. MetaWorld50 task EVALUATE-FOR few - shot imitation methods. OtherScientificTerm are high - level and low - level policies, and hierarchy. Method is meta - learning. ","This paper proposes an approach to few-shot imitation learning, where the high-level policy and the low-level policies are learned in a hierarchical fashion. The idea is to learn a hierarchy of high-levels and lower-levels, which is then used for meta-learning. Experiments on the MetaWorld50 task show that the proposed approach outperforms other recent state-of-the-art few-shots imitation methods."
1075,SP:fb0efa670729796471a7a562b231172103bb8749,GNNs CONJUNCTION graph representation. graph representation CONJUNCTION GNNs. GNNs USED-FOR embedding compression problem. graph representation USED-FOR embedding compression problem. MLP module USED-FOR embedding. two - stage method USED-FOR compressed embeddings. it USED-FOR node. embedding USED-FOR node. it USED-FOR composite code. MLP module USED-FOR node. composite code FEATURE-OF node. MLP module USED-FOR it. hashing USED-FOR composite code. pretrained graph embeddings CONJUNCTION node classification task. node classification task CONJUNCTION pretrained graph embeddings. GraphSage USED-FOR node classification task. ,"This paper tackles the embedding compression problem with GNNs and graph representation. The authors propose a two-stage method to obtain compressed embeddings. First, it uses an MLP module to learn an embedding for each node, and then it computes a composite code using hashing. Second, it performs a node classification task on pretrained graphs from GraphSage."
1076,SP:fb0efa670729796471a7a562b231172103bb8749,random projection hashing method USED-FOR code vector. code vector USED-FOR node. random projection hashing method USED-FOR hashing - based node embedding compression approach. graph adjacency matrix HYPONYM-OF auxiliary information. auxiliary information USED-FOR code vector. method USED-FOR Graph Neural Networks ( GNN ) models. method COMPARE coding schemes. coding schemes COMPARE method. embedding reconstruction task CONJUNCTION node classification task. node classification task CONJUNCTION embedding reconstruction task. coding schemes USED-FOR embedding reconstruction task. coding schemes USED-FOR node classification task. method USED-FOR embedding reconstruction task. node classification task EVALUATE-FOR method. ,"This paper proposes a hashing-based node embedding compression approach based on a random projection hashing method to compute a code vector for each node based on auxiliary information (e.g., graph adjacency matrix). The proposed method is applied to Graph Neural Networks (GNN) models and is shown to outperform existing coding schemes on both embedding reconstruction task and node classification task."
1077,SP:fb0efa670729796471a7a562b231172103bb8749,vertex embedding method USED-FOR large graphs. locally - sensitive hashing CONJUNCTION compositional coding. compositional coding CONJUNCTION locally - sensitive hashing. decoder USED-FOR real vector embeddings. compositional coding USED-FOR binary representation. binary representation USED-FOR method. compositional coding USED-FOR decoder. codebooks USED-FOR compositional coding. compositional coding USED-FOR real vector embeddings. codebooks USED-FOR real vector embeddings. hashing technique USED-FOR binary representation. compositional coding USED-FOR method. locally - sensitive hashing USED-FOR method. prior knowledge USED-FOR encoding. ,"This paper proposes a vertex embedding method for large graphs. The method uses locally-sensitive hashing and compositional coding to learn a binary representation using a hashing technique, which is then used by a decoder to generate real vector embeddings using codebooks. The encoding is based on prior knowledge."
1078,SP:fb0efa670729796471a7a562b231172103bb8749,approach USED-FOR shallow embeddings. neural decoder USED-FOR embeddings / adjacency matrix. LSH USED-FOR code vector. code vectors USED-FOR neural decoder. approach USED-FOR node classification problems. node features PART-OF OGB datasets. OtherScientificTerm is random code vector baseline. ,This paper proposes an approach to learn shallow embeddings. The key idea is to use a neural decoder that takes as input a set of code vectors and outputs a code vector using LSH. This approach is applied to node classification problems and is shown to be effective. The authors also provide a random code vector baseline and show that the proposed approach can be applied to OGB datasets with node features.
1079,SP:15c243829ed3b2505ed1e122bd499089f8a862da,"game theory CONJUNCTION domain - adversarial training. domain - adversarial training CONJUNCTION game theory. Nash equilibrium FEATURE-OF three players game. gradient descent HYPONYM-OF approaches. Runge - Kutta methods USED-FOR ODE. Runge - Kutta methods USED-FOR algorithms. convergence guarantees FEATURE-OF algorithms. Generic are latter, and method. ","This paper combines the ideas from game theory and domain-adversarial training. In particular, the authors study the Nash equilibrium of a three players game, and propose two approaches: (1) gradient descent, and (2) a variant of the latter, where the algorithm is trained in a supervised fashion. Both algorithms are based on the Runge-Kutta methods for solving the ODE, and the authors provide convergence guarantees for both algorithms. The authors also provide an ablation study of the proposed method."
1080,SP:15c243829ed3b2505ed1e122bd499089f8a862da,"Gradient Descent USED-FOR optimization problem. Task is unsupervised domain adaptation problem. Method are Domain Adversarial Learning architecture, game theory, and ODE ( ordinary differential equation ) solvers. OtherScientificTerm are source distribution, and local NE. ","This paper studies the unsupervised domain adaptation problem. The authors propose a Domain Adversarial Learning architecture, which is inspired by game theory. The main idea is to use Gradient Descent to solve the optimization problem, where the source distribution is the local NE, and the target domain is the ODE (ordinary differential equation) solvers."
1081,SP:15c243829ed3b2505ed1e122bd499089f8a862da,local Nash equilibrium USED-FOR optimal condition. game - theoretical perspective USED-FOR adversarial domain learning ( DAL ). optimization method USED-FOR DAL. asymptotic guarantees FEATURE-OF gradient - play dynamics. optimization method USED-FOR asymptotic guarantees. optimization method CONJUNCTION higher - order ordinary differential equation solvers. higher - order ordinary differential equation solvers CONJUNCTION optimization method. OtherScientificTerm is learning rates. Method is ODE method. Metric is transfer performance. ,"This paper studies adversarial domain learning (DAL) from a game-theoretical perspective. The authors propose an optimization method for DAL that provides asymptotic guarantees on the gradient-play dynamics. The optimal condition is defined as a local Nash equilibrium. The optimization method is combined with higher-order ordinary differential equation solvers. The learning rates are optimized using an ODE method, and the transfer performance is evaluated."
1082,SP:15c243829ed3b2505ed1e122bd499089f8a862da,gradient reversal method HYPONYM-OF adversarial domain adaptation training problem. game theory USED-FOR adversarial domain adaptation training problem. upper bound FEATURE-OF learning rate. asymptotic convergence guarantees FEATURE-OF local NEs. upper bound FEATURE-OF gradient - based optimizers. higher order ODE solvers USED-FOR constraints. game optimized gradient - based optimizers EVALUATE-FOR method. Runge - Kutta ODE solvers HYPONYM-OF method. MNIST / USPS digits dataset EVALUATE-FOR game optimized gradient - based optimizers. method COMPARE SOTA methods. SOTA methods COMPARE method. hyperparameter robustness EVALUATE-FOR method. image and NLP datasets EVALUATE-FOR method. ,"This paper proposes a new adversarial domain adaptation training problem based on game theory, i.e., gradient reversal method. The authors provide asymptotic convergence guarantees for local NEs and derive an upper bound on the learning rate of gradient-based optimizers. To enforce the constraints, the authors use higher order ODE solvers such as the Runge-Kutta ODEs. The proposed method is evaluated on the MNIST/USPS digits dataset and compared to other state-of-the-art game optimized gradient-by-game. The method is also evaluated on image and NLP datasets and compared with other SOTA methods, showing better hyperparameter robustness."
1083,SP:0ecbaf1770642b6ac5c9786ba2d18408310fc225,method USED-FOR overfitting. iFlood USED-FOR Flooding algorithm. iFlood USED-FOR stability. method COMPARE baselines. baselines COMPARE method. Method is Flooding scheme. ,"This paper proposes a method to mitigate overfitting in the Flooding scheme. The proposed method, iFlood, aims to improve the stability of the original Flooding algorithm. Experiments show that the proposed method outperforms the baselines."
1084,SP:0ecbaf1770642b6ac5c9786ba2d18408310fc225,regularizer USED-FOR overfitting. instance - level constraints FEATURE-OF training loss. Flood COMPARE iFlood. iFlood COMPARE Flood. instance - level constraints FEATURE-OF iFlood. mini - batch level FEATURE-OF training loss. OtherScientificTerm is noise. ,"This paper proposes a new regularizer to prevent overfitting. Compared to Flood, iFlood imposes instance-level constraints on the training loss at the mini-batch level. The authors also propose to add noise to the training data."
1085,SP:0ecbaf1770642b6ac5c9786ba2d18408310fc225,Flooding USED-FOR sample - specific level. Task is iFlooding. Generic is extension. ,"This paper proposes an extension of iFlooding. The main idea is to extend Flooding to a sample-specific level. The paper is well-written and well-motivated. However, there are a few issues with the proposed extension:"
1086,SP:0ecbaf1770642b6ac5c9786ba2d18408310fc225,flooding loss function USED-FOR training loss. positive bias FEATURE-OF training loss. training objective COMPARE regular Flooding. regular Flooding COMPARE training objective. noisy labels CONJUNCTION biased label distributions. biased label distributions CONJUNCTION noisy labels. robustness EVALUATE-FOR loss function. OtherScientificTerm is average loss. ,"This paper proposes a new flooding loss function for training loss with positive bias. The training objective is similar to regular Flooding, except that the average loss is used instead of the original loss. The authors show that the proposed loss function improves robustness to noisy labels and biased label distributions."
1087,SP:e6622975c9889cf6d3357ab439c2e268c4f4200e,Value Functions ( VFs ) USED-FOR state representation technique. VFs USED-FOR high - level space representation. high - level space representation USED-FOR hierarchical reinforcement learning ( RL ) scenario. VFS USED-FOR Practical algorithms. MiniGrid and manipulation task EVALUATE-FOR VFS. Method is learned state representation. ,This paper proposes a state representation technique based on Value Functions (VFs). VFs can be used to learn a high-level space representation for a hierarchical reinforcement learning (RL) scenario. Practical algorithms are proposed to leverage VFS on MiniGrid and manipulation task. The learned state representation is shown to be effective.
1088,SP:e6622975c9889cf6d3357ab439c2e268c4f4200e,"value functions USED-FOR state representation. state representation USED-FOR approach. model - based planning algorithm USED-FOR planning. model - free RL algorithm CONJUNCTION model - based planning algorithm. model - based planning algorithm CONJUNCTION model - free RL algorithm. state - space FEATURE-OF planning. DQN HYPONYM-OF model - free RL algorithm. value functions USED-FOR state - space. baseline algorithms USED-FOR representation learning. representation learning USED-FOR RL. RL USED-FOR robotic manipulation task. value function space COMPARE baseline algorithms. baseline algorithms COMPARE value function space. baseline algorithms USED-FOR RL. representation learning USED-FOR robotic manipulation task. OtherScientificTerm are action - space, and value function. Generic is task. ","This paper proposes an approach to learning value functions for the state representation of an action-space. The authors propose a model-free RL algorithm, DQN, which is a combination of the model-based planning algorithm for planning in the state-space, and a value function for the action space. The experiments show that the learned value function space outperforms baseline algorithms in representation learning for RL in a robotic manipulation task. The paper also provides a theoretical analysis of the proposed task."
1089,SP:e6622975c9889cf6d3357ab439c2e268c4f4200e,state abstraction USED-FOR hierarchical reinforcement learning ( HRL ). Value Function Spaces ( VFS ) HYPONYM-OF state abstraction. lower - level policies CONJUNCTION value functions. value functions CONJUNCTION lower - level policies. value estimate USED-FOR lower - level policy. prior USED-FOR lower - level policies. model - free Q - learning CONJUNCTION model - based MPC type scenario. model - based MPC type scenario CONJUNCTION model - free Q - learning. model - based MPC type scenario EVALUATE-FOR state abstraction. model - free Q - learning EVALUATE-FOR state abstraction. OtherScientificTerm is task - irrelevant information. ,"This paper proposes a new state abstraction called Value Function Spaces (VFS) for hierarchical reinforcement learning (HRL). The key idea is to learn a prior for lower-level policies and value functions, and then use this value estimate to train a lower-layer policy. The authors evaluate the state abstraction on model-free Q-learning and model-based MPC type scenario, and show that the learned prior is able to handle task-irrelevant information."
1090,SP:e6622975c9889cf6d3357ab439c2e268c4f4200e,"learned value functions USED-FOR skill learning. learned value functions USED-FOR VFSs. success probability EVALUATE-FOR competitive baselines. success probability EVALUATE-FOR tasks. representation USED-FOR long - horizon planning tasks. tasks COMPARE competitive baselines. competitive baselines COMPARE tasks. pre - trained skills USED-FOR representation. abstraction USED-FOR model - based goal - directed planning. state - transition model USED-FOR abstraction. state - transition model USED-FOR model - based goal - directed planning. VFS - space FEATURE-OF state - transition model. Method are Value Function Spaces ( VFS ), and low - dimensional representation. Task is reinforcement learning. OtherScientificTerm are learned value function, and distractor information. ","This paper proposes Value Function Spaces (VFS) as an abstraction for reinforcement learning. VFSs are learned value functions for skill learning, where the learned value function is used as a representation for long-horizon planning tasks. The authors show that the learned representation can be used to improve the success probability compared to competitive baselines on a variety of tasks with high success probability, and that this representation is learned using pre-trained skills. They also show that this abstraction is useful for model-based goal-directed planning using a state-transition model in the VFS-space, which is able to learn a low-dimensional representation without distractor information."
1091,SP:6adcd2a71ce70922c4cbe155d49f105964faee8f,Top - n HYPONYM-OF deterministic set sampling mechanism. i.i.d. sampling CONJUNCTION First - n and MLP projection. First - n and MLP projection CONJUNCTION i.i.d. sampling. set sampling mechanisms COMPARE Top - n. Top - n COMPARE set sampling mechanisms. i.i.d. sampling HYPONYM-OF set sampling mechanisms. First - n and MLP projection HYPONYM-OF set sampling mechanisms. Top - n USED-FOR one - shot sampling. Top - n USED-FOR GANs. one - shot sampling USED-FOR VAE. VAE CONJUNCTION GANs. GANs CONJUNCTION VAE. Top - n USED-FOR VAE. GANs CONJUNCTION generative models. generative models CONJUNCTION GANs. one - shot sampling USED-FOR GANs. benchmark EVALUATE-FOR prior sampling mechanism. benchmark EVALUATE-FOR set and molecular graph generation. OtherScientificTerm is collision problem. ,"This paper proposes Top-n, a deterministic set sampling mechanism. Compared to other set sampling mechanisms, such as i.i.d. sampling, First-n and MLP projection, the authors argue that Top-N is more suitable for one-shot sampling in VAE, GANs, and generative models. The authors also propose a new benchmark for set and molecular graph generation, which is designed to evaluate the prior sampling mechanism in the presence of collision problem."
1092,SP:6adcd2a71ce70922c4cbe155d49f105964faee8f,"latent vector representations USED-FOR probabilistic models. i.i.d sampling of set element representations CONJUNCTION set's vector representation. set's vector representation CONJUNCTION i.i.d sampling of set element representations. MLP - based generation USED-FOR node representations. equivariance USED-FOR learning algorithm. permutations FEATURE-OF equivariance. first - n USED-FOR top - n. differentiable sorting USED-FOR first - n. differentiable sorting USED-FOR top - n. top - n COMPARE i.i.d generation. i.i.d generation COMPARE top - n. top - n USED-FOR complex dependencies. SetMNIST CONJUNCTION synthetic molecule - like 3D structures. synthetic molecule - like 3D structures CONJUNCTION SetMNIST. synthetic molecule - like 3D structures CONJUNCTION QM9 dataset. QM9 dataset CONJUNCTION synthetic molecule - like 3D structures. synthetic molecule - like 3D structures EVALUATE-FOR method. set and graph generation tasks EVALUATE-FOR method. QM9 dataset HYPONYM-OF set and graph generation tasks. QM9 dataset EVALUATE-FOR method. SetMNIST EVALUATE-FOR method. synthetic molecule - like 3D structures HYPONYM-OF set and graph generation tasks. SetMNIST HYPONYM-OF set and graph generation tasks. Generic are approaches, and it. Method are First - n generation, latent set representation, i.i.d sampling, MLP - based generators, first - n generation, and latent vector representation. OtherScientificTerm are node permutation, stochasticity, collision problem, permutation symmetries, equivariance of functions, training dynamics, and cosine similarity. ","This paper studies the problem of learning probabilistic models from latent vector representations. First-n generation is a popular way to learn a latent set representation, but it is computationally expensive. This paper proposes to replace i.i.d sampling of set element representations with the set's vector representation, which is an MLP-based generation for node representations. The key idea is to use equivariance of the learning algorithm to the permutations of the node permutation, which avoids stochasticity and avoids the collision problem. Theoretical analysis on permutation symmetries is provided, and it is shown that first-nn generation is equivalent to first-n with differentiable sorting, and that the cosine similarity between the latent vector representation of the first and the second n is the same. Experiments on set and graph generation tasks (SetMNIST, synthetic molecule-like 3D structures, and QM9 dataset) demonstrate the effectiveness of the proposed method. The experiments also show that top-n is able to capture complex dependencies, which are not possible with i.d generation. "
1093,SP:6adcd2a71ce70922c4cbe155d49f105964faee8f,probabilistic decoder USED-FOR latent vectors. equivariance FEATURE-OF function. equivariance USED-FOR learning algorithm. VAEs USED-FOR set generation. GANs CONJUNCTION VAEs. VAEs CONJUNCTION GANs. exchangeability USED-FOR set generation. exchangeability USED-FOR GANs. Top - n HYPONYM-OF set creation mechanism. VAE CONJUNCTION GAN. GAN CONJUNCTION VAE. iid generation USED-FOR VAE. iid generation USED-FOR GAN. Top - n COMPARE iid generation. iid generation COMPARE Top - n. Top - n COMPARE generative approaches. generative approaches COMPARE Top - n. QM9 chemical dataset USED-FOR graphs. synthetic molecule dataset CONJUNCTION sets. sets CONJUNCTION synthetic molecule dataset. synthetic molecule dataset EVALUATE-FOR SetMNIST reconstruction and generative tasks. Task is one - shot ” set / graph generation. ,"This paper studies the problem of “one-shot” set/graph generation, where the latent vectors are generated by a probabilistic decoder. The authors propose a learning algorithm based on equivariance to the function. They show that GANs and VAEs with exchangeability can be used for set generation. Top-n, a set creation mechanism, is proposed and compared with iid generation for a VAE and a GAN. The experimental results on SetMNIST reconstruction and generative tasks are presented on a synthetic molecule dataset and sets from the QM9 chemical dataset, as well as on graphs from the CIFAR-10 dataset. Results show that Top-nn outperforms other generative approaches."
1094,SP:6adcd2a71ce70922c4cbe155d49f105964faee8f,exchangeability USED-FOR generative model. learning algorithms USED-FOR generative modeling. method USED-FOR generative models. Top - N USED-FOR generative models. VAE CONJUNCTION GANS. GANS CONJUNCTION VAE. GANS HYPONYM-OF generative models. VAE HYPONYM-OF generative models. OtherScientificTerm is equivariance. ,This paper studies the problem of learning algorithms for generative modeling. The authors propose a method called Top-N to train generative models with equivariance. The main idea is to use exchangeability to train the generative model. Experiments are conducted on two generative (VAE and GANS).
1095,SP:f1f1df92e3e7c6b3b9e326a78a708c0d5d990c83,truncated Fourier basis USED-FOR PDEs. Deep Ritz Method CONJUNCTION Physics - Informed Neural Networks. Physics - Informed Neural Networks CONJUNCTION Deep Ritz Method. neural networks CONJUNCTION truncated Fourier basis. truncated Fourier basis CONJUNCTION neural networks. statistical error EVALUATE-FOR Physics - Informed Neural Networks. statistical error EVALUATE-FOR Deep Ritz Method. neural networks USED-FOR Physics - Informed Neural Networks. neural networks USED-FOR statistical error. static Schrodinger equation USED-FOR prototype PDE. upper bound USED-FOR PINN. Generic is methods. ,This paper studies the statistical error of Deep Ritz Method and Physics-Informed Neural Networks with neural networks and a truncated Fourier basis for PDEs. The main contribution of the paper is to derive a static Schrodinger equation for the prototype PDE and to derive an upper bound for PINN. The authors also provide a theoretical analysis of the proposed methods.
1096,SP:f1f1df92e3e7c6b3b9e326a78a708c0d5d990c83,"sparse neural networks FEATURE-OF estimators. DRM CONJUNCTION PINNS. PINNS CONJUNCTION DRM. it USED-FOR generalization bound. upper bound COMPARE lower bound. lower bound COMPARE upper bound. minimax optimality USED-FOR DRM. OtherScientificTerm are upper bounds, truncated fourier basis, gradient squared, and power low. Method are PINNs based learning of solutions of PDEs, and modified deep ritz method. ",This paper studies the generalization of estimators with sparse neural networks. The main contribution of the paper is to derive upper bounds on the generalizability of the estimators. The upper bound is based on the truncated fourier basis and it is used to derive a generalization bound for both DRM and PINNS based learning of solutions of PDEs. The lower bound is derived based on minimax optimality for DRM. The authors also propose a modified deep ritz method where the gradient squared is used instead of the power low.
1097,SP:f1f1df92e3e7c6b3b9e326a78a708c0d5d990c83,"neural networks CONJUNCTION truncated Fourier series. truncated Fourier series CONJUNCTION neural networks. neural networks USED-FOR approximating linear Elliptic PDEs. Method is deep learning ( DL ). OtherScientificTerm are function class, and approximation errors. ","This paper studies the problem of approximating linear Elliptic PDEs with neural networks and truncated Fourier series, which is an important problem in deep learning (DL). The main contribution of this paper is to provide a theoretical analysis of the approximation errors of the function class, and to show that approximation errors do not depend on the number of parameters. "
1098,SP:f1f1df92e3e7c6b3b9e326a78a708c0d5d990c83,deep learning inspired methods USED-FOR numerical solution of a Schrodinger equation. sample complexity FEATURE-OF power law scaling. Deep Ritz ) HYPONYM-OF method. OtherScientificTerm is regularity. Generic is methods. ,This paper studies the numerical solution of a Schrodinger equation using deep learning inspired methods. The authors show that the power law scaling with respect to sample complexity can be solved by a method called (Deep Ritz) with regularity. They also provide some theoretical analysis of the proposed methods.
1099,SP:80614db60d27a48c3c1b1882844e298666b798d4,adversarial robustness CONJUNCTION cross - domain transferability. cross - domain transferability CONJUNCTION adversarial robustness. methods USED-FOR regularization. data augmentation HYPONYM-OF methods. ,"This paper studies the problem of adversarial robustness and cross-domain transferability. The authors propose two methods for regularization: (1) data augmentation, and (2) the use of a modified version of the training data. "
1100,SP:80614db60d27a48c3c1b1882844e298666b798d4,"transfer learning USED-FOR vision tasks. transferability EVALUATE-FOR transfer learning. transferability EVALUATE-FOR adversarially robust models. data augmentation CONJUNCTION regularization. regularization CONJUNCTION data augmentation. regularization CONJUNCTION robustness. robustness CONJUNCTION regularization. regularization USED-FOR generalization. robustness HYPONYM-OF regularization. OtherScientificTerm are relative domain transferability, and cross entropy loss. Metric is Def 2. Method is squared loss. ","This paper studies the transferability of adversarially robust models in the context of transfer learning in vision tasks. The authors propose to measure relative domain transferability by minimizing the squared loss between the target domain and the source domain, which they call Def 2. The cross entropy loss is defined as the sum of the squared difference between the source and target domains. They also propose to use data augmentation, regularization, and robustness as regularization to improve generalization."
1101,SP:80614db60d27a48c3c1b1882844e298666b798d4,robustness CONJUNCTION accuracy. accuracy CONJUNCTION robustness. regularisation COMPARE robustness. robustness COMPARE regularisation. Method is adversarially trained models. ,This paper studies the effect of regularisation on the trade-off between robustness and accuracy in adversarially trained models. The authors argue that regularisation is a better tradeoff than robustness alone. 
1102,SP:80614db60d27a48c3c1b1882844e298666b798d4,"adversarial robustness CONJUNCTION transferability. transferability CONJUNCTION adversarial robustness. feature extractor stage PART-OF learning algorithm. restriction USED-FOR domain generalization. restriction FEATURE-OF feature extractor stage. intrinsic and fundamental measures USED-FOR domain generalization error. intrinsic and fundamental measures USED-FOR model. Generic are problem, and measures. OtherScientificTerm are uniform convergence bounds, and sample size. ","This paper studies the problem of adversarial robustness and transferability. The authors propose a learning algorithm that consists of a feature extractor stage that is subject to a restriction on the domain generalization. The model is trained using intrinsic and fundamental measures that measure the magnitude of the domain specific and generalization error, and the authors provide uniform convergence bounds for these measures. They also provide a theoretical analysis of the effect of sample size."
1103,SP:4d49bcb069a76f108c0e2de50750827f45eb5676,memorization overfitting problem PART-OF meta learning. causality USED-FOR memorization overfitting problem. causality USED-FOR causal graphs. causality USED-FOR meta - learning. MAML Dropout CONJUNCTION MAML Bins. MAML Bins CONJUNCTION MAML Dropout. Method is'front - door'adjustment. ,"This paper studies the memorization overfitting problem in meta learning with the help of causality. The authors propose a 'front-door' adjustment and show that causality can be used to learn causal graphs, which is useful for meta-learning. Experiments are conducted on MAML Dropout and on the recently introduced MAMM Bins."
1104,SP:4d49bcb069a76f108c0e2de50750827f45eb5676,memorization overfitting problem PART-OF meta - learning. identification strategy USED-FOR interventional distribution. front door criterion USED-FOR identification strategy. interventional distribution USED-FOR meta parameters. front door criterion USED-FOR interventional distribution. ,This paper studies the memorization overfitting problem in meta-learning. The authors propose an identification strategy based on the front door criterion to identify the interventional distribution for meta parameters. The paper is well-written and easy to follow.
1105,SP:4d49bcb069a76f108c0e2de50750827f45eb5676,"undesirable memorization problem PART-OF gradient - based meta - learning. regularization - based and augmentation - based solutions USED-FOR memorization of all meta - training tasks. unified causal framework USED-FOR causal perspective of meta - learning. universal label space FEATURE-OF base model. deconfounder approaches USED-FOR memorization. dropout ( MAML - Dropout ) USED-FOR meta - knowledge. OtherScientificTerm are meta - learning knowledge, meta - training tasks, meta - training steps, and MAML - Bins. Task is task specific adaptation. Generic is methods. Method is meta - learning overfitting solutions. ","This paper studies the undesirable memorization problem in gradient-based meta-learning. The authors propose regularization-based and augmentation-based solutions to address the memorization of all meta-training tasks. In particular, the authors propose a unified causal framework for the causal perspective of meta-Learning, which is motivated by the observation that the base model has a universal label space and thus can be used for task specific adaptation. In addition, they propose dropout (MAML-Dropout) to preserve the meta-knowledge of the learned base model and propose deconfounder approaches to address memorization. The experimental results show the effectiveness of the proposed methods, as well as the efficacy of the meta -learning overfitting solutions. Finally, they provide an ablation study of the effect of different meta-train steps on the performance of MAML Bins."
1106,SP:4d49bcb069a76f108c0e2de50750827f45eb5676,causal graph USED-FOR gradient - based meta - learning. memorization overfitting PART-OF meta - learning. methods USED-FOR memorization problem. causal intervention principle USED-FOR spurious correlation. implementations EVALUATE-FOR principle. Dropout USED-FOR meta - knowledge. benchmark datasets EVALUATE-FOR algorithm. ,"This paper studies gradient-based meta-learning with a causal graph. The authors consider the problem of memorization overfitting, which is an important problem in meta-learning. Previous methods have been proposed to address the memorization problem, but the authors propose a causal intervention principle to reduce the spurious correlation. The proposed principle is evaluated on two implementations. Dropout is used to preserve the meta-knowledge and the proposed algorithm is tested on two benchmark datasets."
1107,SP:adfe205b335cf87bd4e470efd9f72bb639a4451c,"method USED-FOR ad hoc cooperation. ODITS USED-FOR ad hoc cooperation. sufficient statistic USED-FOR joint action value. encoder networks USED-FOR latent variables. methodology USED-FOR partially observable environments. OtherScientificTerm are teammate types, and agents. ",This paper proposes a method called ODITS for ad hoc cooperation between agents. The key idea is to use teammate types as latent variables and use encoder networks to model the latent variables. The joint action value is estimated using a sufficient statistic. The methodology is applied to partially observable environments and is shown to be effective.
1108,SP:adfe205b335cf87bd4e470efd9f72bb639a4451c,"agent USED-FOR ad - hoc teaming applications. observability CONJUNCTION roles. roles CONJUNCTION observability. It USED-FOR latent variable representation of teammate behaviors. policy training USED-FOR latent variable representation of teammate behaviors. information - based regularizer USED-FOR variables. information - based regularizer PART-OF framework. OtherScientificTerm is ad - hoc team. Method are ODITS, and CTDE manner. ","This paper proposes a new agent for ad-hoc teaming applications. The proposed framework is based on an information-based regularizer that encourages the variables to be similar to each other. It also learns a latent variable representation of teammate behaviors via policy training. The main contribution of this paper is to address the issues of observability and roles in the context of ad hoc teaming. In particular, the authors propose ODITS, which is a generalization of the CTDE manner."
1109,SP:adfe205b335cf87bd4e470efd9f72bb639a4451c,unknown teammate types FEATURE-OF ad hoc teamwork. teammate types CONJUNCTION full observability. full observability CONJUNCTION teammate types. framework COMPARE ODITS. ODITS COMPARE framework. coin game CONJUNCTION predator prey. predator prey CONJUNCTION coin game. approach COMPARE baselines. baselines COMPARE approach. AATEAM HYPONYM-OF domains. domains EVALUATE-FOR baselines. domains EVALUATE-FOR approach. AATEAM HYPONYM-OF baselines. coin game HYPONYM-OF domains. predator prey HYPONYM-OF domains. Method is teammate situation encoder - decoder framework. ,This paper addresses the problem of ad hoc teamwork with unknown teammate types. The authors propose a teammate situation encoder-decoder framework that is able to handle both teammate types and full observability. The proposed framework is compared to ODITS and compared to several baselines on two domains: coin game and predator prey.
1110,SP:adfe205b335cf87bd4e470efd9f72bb639a4451c,"predictor USED-FOR ad hoc agent's marginal utility. method USED-FOR predictor. local information USED-FOR predictor. latent vector USED-FOR teamwork situation. latent vector USED-FOR predictor. method USED-FOR predictor. encoder - decoder model USED-FOR method. full state - action information USED-FOR teamwork situation "" representation $ c$. integration network USED-FOR $ Q$-function. $ Q$-function USED-FOR MMDP. model USED-FOR integration network. local information USED-FOR proxy encoder. encoder - decoder models USED-FOR coherent representation. second HYPONYM-OF coherent representation. first HYPONYM-OF coherent representation. first HYPONYM-OF encoder - decoder models. second HYPONYM-OF encoder - decoder models. local information USED-FOR coherent representation. full state - action information USED-FOR second. local information USED-FOR first. estimated representation USED-FOR marginal utility predictor. encoder - decoder models USED-FOR method. architectures CONJUNCTION MARL approach ( QMIX ). MARL approach ( QMIX ) CONJUNCTION architectures. approach COMPARE architectures. architectures COMPARE approach. Task is ad hoc teamwork. OtherScientificTerm is teammate behavior. Method is teamwork situation representation. Material is predator - prey. ","This paper studies the problem of ad hoc teamwork, where a predictor is trained to predict an ad hoc agent's marginal utility. The proposed method uses an encoder-decoder model to train a predictor that takes as input a latent vector of the teamwork situation, and uses the local information to train the predictor to estimate the agent's teammate behavior. The model is used to train an integration network that outputs a $Q$-function for the MMDP, which is then fed to a proxy encoder that takes in local information and outputs a ""teamwork situation"" representation $c$ using full state-action information. The estimated representation is used as the marginal utility predictor. The authors show that the proposed method is able to learn a coherent representation $C$ using two encoder - decoder models: the first uses local information, and the second uses full state and action information. They compare the proposed approach with two existing architectures and the MARL approach (QMIX) and show that their approach outperforms both of them. They also compare their proposed method to predator-prey, where they show that they can learn a better teamwork situation representation."
1111,SP:7b04b45c4dd237d69321d280dcdcbc89fb362015,"imputation method USED-FOR high - dimensional datasets. expectation - maximization ( EM ) USED-FOR missing data imputation. normalizing flow network USED-FOR multivariate Gaussian. normalizing flow network USED-FOR EMFlow ). normalizing flow network USED-FOR method. expectation - maximization ( EM ) USED-FOR imputation. EMFlow USED-FOR regression tasks. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. EMFlow USED-FOR datasets. UCI machine learning dataset repository CONJUNCTION image classification datasets. image classification datasets CONJUNCTION UCI machine learning dataset repository. UCI machine learning dataset repository FEATURE-OF datasets. datasets USED-FOR regression tasks. CIFAR-10 HYPONYM-OF image classification datasets. MNIST HYPONYM-OF image classification datasets. missing data imputation CONJUNCTION downstream classification. downstream classification CONJUNCTION missing data imputation. imputations USED-FOR downstream classification. Method are machine learning methods, and well - constructed architecture. ","This paper proposes a new imputation method for high-dimensional datasets. The proposed method, EMFlow, uses a normalizing flow network to approximate a multivariate Gaussian. EMFlow is applied to three datasets from the UCI machine learning dataset repository and two image classification datasets (MNIST and CIFAR-10). EMFlow achieves state-of-the-art performance on regression tasks. The paper also provides a theoretical analysis of imputation using expectation-maximization (EM) to address the problem of missing data imputation and downstream classification. Empirical results show that the proposed machine learning methods can achieve better performance with fewer parameters and a well-constructed architecture."
1112,SP:7b04b45c4dd237d69321d280dcdcbc89fb362015,"online EM algorithm CONJUNCTION normalizing flow models. normalizing flow models CONJUNCTION online EM algorithm. model USED-FOR data imputation. latent space FEATURE-OF data imputation. online EM algorithm USED-FOR data imputation. normalizing flow models USED-FOR bidirectional mapping. features PART-OF data space. MNIST CONJUNCTION CIFAR-10 datasets. CIFAR-10 datasets CONJUNCTION MNIST. UCI datasets CONJUNCTION MNIST. MNIST CONJUNCTION UCI datasets. CIFAR-10 datasets EVALUATE-FOR baseline models. convergence COMPARE MCFlow. MCFlow COMPARE convergence. UCI datasets EVALUATE-FOR baseline models. Method are EMFlow, and feature - wise mapping. Task is imputation. ","This paper proposes an online EM algorithm and normalizing flow models for bidirectional mapping between features in the data space and the model for data imputation in the latent space. In particular, EMFlow proposes to use a feature-wise mapping between the input and the output of the imputation. Empirical results on UCI datasets, MNIST and CIFAR-10 datasets show better convergence compared to MCFlow."
1113,SP:7b04b45c4dd237d69321d280dcdcbc89fb362015,MCAR CONJUNCTION MAR. MAR CONJUNCTION MCAR. MCAR HYPONYM-OF imputing missing data. MAR HYPONYM-OF imputing missing data. latent variable / source variable space PART-OF normalizing flow. it USED-FOR modeling the observed data distribution. normalizing flow USED-FOR it. normalizing flow USED-FOR modeling the observed data distribution. online EM USED-FOR imputation of the latent space variables. consistency of inter - feature dependencies FEATURE-OF latent / source variable space. method COMPARE GAIN. GAIN COMPARE method. EMFlow COMPARE GAIN. GAIN COMPARE EMFlow. method COMPARE EMFlow. EMFlow COMPARE method. MisGAN CONJUNCTION MCFlow. MCFlow CONJUNCTION MisGAN. GAIN CONJUNCTION MisGAN. MisGAN CONJUNCTION GAIN. method COMPARE MisGAN. MisGAN COMPARE method. EMFlow COMPARE MisGAN. MisGAN COMPARE EMFlow. OtherScientificTerm is observed data distribution. ,"This paper addresses the problem of imputing missing data (e.g., MCAR and MAR) in the context of modeling the observed data distribution using a normalizing flow that incorporates the latent variable/source variable space into it for modeling the modeled data distribution. The imputation of the latent space variables is done via online EM, which is an extension of online EM. The method is compared with EMFlow, GAIN, MisGAN, MCFlow, and MisGAN. The authors show that the consistency of inter-feature dependencies in the latent / source variable space is important for the performance of the proposed method."
1114,SP:7b04b45c4dd237d69321d280dcdcbc89fb362015,EMFlow USED-FOR missing data imputation. accuracy EVALUATE-FOR post - imputation classification. image datasets EVALUATE-FOR post - imputation classification. Material is multivariate and image datasets. ,This paper proposes EMFlow for missing data imputation. The authors conduct experiments on multivariate and image datasets to evaluate the accuracy of post-imputation classification on image datasets.
1115,SP:a151ae8afae0a073b0df83a74fd084dfe3753a48,"dual view USED-FOR deep networks. DLGN USED-FOR computations. DLGN USED-FOR SOTA DNNs. Method are DNNs, gating network, weights network, and deep network models. OtherScientificTerm are gates, and weights. ","This paper presents a dual view of deep networks from a theoretical point of view. The authors argue that DNNs can be viewed as a gating network and a weights network, where the gates are the weights of the weights and the weights are the outputs of the gates. The main contribution of this paper is to show that DLGN can be used to perform computations that are computationally efficient. This is an interesting idea, and it is interesting to see how DLGN is applied to SOTA DNN results. The paper is well-written and well-motivated, and the paper is clearly written. However, there are a few issues with the presentation of the paper: "
1116,SP:a151ae8afae0a073b0df83a74fd084dfe3753a48,"deep gated network USED-FOR deep linearly gated network. Generic is network. Method are neural network, and neural path kernel. ","This paper proposes a deep linearly gated network that is a deep gated version of the well-studied deep network. The main idea is to train the network in a way that the output of the neural network is linearly independent of the weights of the network. In particular, the neural path kernel is trained in a manner that the weights are independent of each other. "
1117,SP:a151ae8afae0a073b0df83a74fd084dfe3753a48,rectified linear units ( ReLU ) PART-OF DNN. Deep Linearly Gated Network ( DLGN ) USED-FOR DNN. path space FEATURE-OF weighted network. ,This paper proposes to replace the rectified linear units (ReLU) in the original DNN with the Deep Linearly Gated Network (DLGN) in order to improve the performance of the DNN. The key idea is to train a weighted network on the path space of the original path space. 
1118,SP:a151ae8afae0a073b0df83a74fd084dfe3753a48,deep linearly gated networks ( DLGN ) USED-FOR DNNs. ReLU activations USED-FOR deep linearly gated networks ( DLGN ). dual view USED-FOR deep linearly gated networks ( DLGN ). gating network CONJUNCTION weight network. weight network CONJUNCTION gating network. framework USED-FOR weight network ’. framework USED-FOR gating network. DLGN COMPARE DNNs. DNNs COMPARE DLGN. benchmark datasets EVALUATE-FOR DNNs. DLGN USED-FOR classification. benchmark datasets EVALUATE-FOR DLGN. ,This paper proposes deep linearly gated networks (DLGNs) with ReLU activations with dual view to improve the performance of DNNs. The proposed framework trains a gating network and a ‘weight network’. The experiments show that DLGN improves the classification performance on several benchmark datasets compared to other state-of-the-art DNN.
1119,SP:5676944f4983676b5ad843fdb190bf029ad647bb,Layer Norm ( LN ) CONJUNCTION Instance Norm ( IN ). Instance Norm ( IN ) CONJUNCTION Layer Norm ( LN ). Instance Norm ( IN ) USED-FOR vision transformers ( ViTs ). Layer Norm ( LN ) USED-FOR token normalization method. Instance Norm ( IN ) USED-FOR token normalization method. normalization USED-FOR ViTs. capturing positional context CONJUNCTION inductive bias. inductive bias CONJUNCTION capturing positional context. normalization USED-FOR LN. LN HYPONYM-OF ViTs. LN CONJUNCTION relative positional embedding based transformation. relative positional embedding based transformation CONJUNCTION LN. LN USED-FOR Dynamic Token Normalization ( DTN ) component. relative positional embedding based transformation USED-FOR Dynamic Token Normalization ( DTN ) component. ViT CONJUNCTION Swin. Swin CONJUNCTION ViT. Swin CONJUNCTION PVT. PVT CONJUNCTION Swin. ViTs USED-FOR DTN. PVT HYPONYM-OF ViTs. ViT HYPONYM-OF ViTs. Swin HYPONYM-OF ViTs. ImageNet - C CONJUNCTION ImageNet - R. ImageNet - R CONJUNCTION ImageNet - C. ImageNet - R CONJUNCTION ListOps. ListOps CONJUNCTION ImageNet - R. ImageNet CONJUNCTION ImageNet - C. ImageNet - C CONJUNCTION ImageNet. supervised learning setting CONJUNCTION self - supervised pre - training. self - supervised pre - training CONJUNCTION supervised learning setting. ListOps USED-FOR supervised learning setting. ,"This paper proposes a new token normalization method based on Layer Norm (LN) and Instance Norm (IN) for vision transformers (ViTs). The main idea is to use normalization for ViTs such as LN and relative positional embedding based transformation. Experiments are conducted on ImageNet, ImageNet-C and ImageNet -R with ListOps in a supervised learning setting and self-supervised pre-training. Results show that DTN can be trained with ViTs including ViT, Swin, PVT, etc."
1120,SP:5676944f4983676b5ad843fdb190bf029ad647bb,vanilla layer norm PART-OF dynamic normalization. Dynamic Token Normalization ( DTN ) HYPONYM-OF dynamic normalization. global contextual information CONJUNCTION local positional context. local positional context CONJUNCTION global contextual information. Transformers USED-FOR global contextual information. Transformers USED-FOR local positional context. It USED-FOR Transformers. DTN USED-FOR Vision Transformers. DTN USED-FOR ImageNet Classification. Vision Transformers PART-OF ImageNet Classification. OtherScientificTerm is intra - token and inter - token manners. ,"This paper proposes Dynamic Token Normalization (DTN), a dynamic normalization that replaces the vanilla layer norm. It is applied to Transformers to incorporate both global contextual information and local positional context. DTN is applied on ImageNet Classification with Vision Transformers and is shown to be effective in both intra-token and inter-token manners."
1121,SP:5676944f4983676b5ad843fdb190bf029ad647bb,layer normalization CONJUNCTION instance normalization. instance normalization CONJUNCTION layer normalization. normalization operations USED-FOR vision transformers. semantics USED-FOR IN. normalization method USED-FOR vision transformers. inter- and intra- token normalization USED-FOR vision transformers. DTN HYPONYM-OF normalization method. DTN normalization USED-FOR vision transformer models. robustness CONJUNCTION IMAGENET - C. IMAGENET - C CONJUNCTION robustness. robustness EVALUATE-FOR vision transformer models. classification EVALUATE-FOR vision transformer models. robustness EVALUATE-FOR DTN normalization. ImageNet dataset EVALUATE-FOR vision transformer models. Method is LN. OtherScientificTerm is inductive bias. ,"This paper proposes a new normalization method, DTN, for vision transformers, which combines layer normalization and instance normalization. The authors argue that the normalization operations used in vision transformerers can be seen as inter- and intra-token normalization, and that the semantics of IN is similar to LN, but with an inductive bias. Experiments on ImageNet dataset show that DTN normalization improves the robustness and IMAGENET-C on classification performance of vision transformer models."
1122,SP:5676944f4983676b5ad843fdb190bf029ad647bb,long - range dependencies CONJUNCTION local positional context. local positional context CONJUNCTION long - range dependencies. DTN USED-FOR local positional context. DTN USED-FOR long - range dependencies. LN CONJUNCTION IN. IN CONJUNCTION LN. small / middle - scale Transformers USED-FOR DTN. ImageNet EVALUATE-FOR DTN. ,This paper proposes a DTN that combines long-range dependencies and local positional context by leveraging small/middle-scale Transformers. Experiments on ImageNet show that the proposed DTN is able to capture both long-term dependencies as well as local positional contexts. The experiments are conducted on LN and IN and show that DTN can achieve state-of-the-art results.
1123,SP:7a04efbf835c238bbdf70a8b8decee4ec2907a3a,"tools USED-FOR deep neural networks. label smoothing USED-FOR noise. linear interpolation technique USED-FOR smoothness of the learned function. validation data USED-FOR linear interpolation technique. model size CONJUNCTION explicit and implicit regularization. explicit and implicit regularization CONJUNCTION model size. explicit and implicit regularization HYPONYM-OF training parameters. model size HYPONYM-OF training parameters. OtherScientificTerm are high frequencies, and image space. Material is natural images. ","This paper presents a set of tools for training deep neural networks that aim to reduce the noise caused by label smoothing. In particular, the authors propose a linear interpolation technique based on validation data to improve the smoothness of the learned function. The main contribution of the paper is to study the effect of different training parameters such as model size, explicit and implicit regularization on the performance of the model. The authors show that high frequencies are more likely to be smoothed in the image space, which is in contrast to natural images."
1124,SP:7a04efbf835c238bbdf70a8b8decee4ec2907a3a,spectral bias EVALUATE-FOR neural networks. training aspects USED-FOR spectral bias. label noise USED-FOR CIFAR10 dataset. linear interpolation path FEATURE-OF loss landscape. model architecture CONJUNCTION explicit regularization. explicit regularization CONJUNCTION model architecture. explicit regularization CONJUNCTION data augmentation. data augmentation CONJUNCTION explicit regularization. spectral bias FEATURE-OF deep neural networks. Method is neural network. Generic is noise. OtherScientificTerm is design factors. ,"This paper studies the spectral bias of neural networks in terms of training aspects. The authors study the effect of label noise on the CIFAR10 dataset with label noise and show that the loss landscape has a linear interpolation path. The spectral bias in deep neural networks can be explained by the model architecture, explicit regularization, and data augmentation. In particular, the authors argue that the neural network is more sensitive to noise when the design factors are different."
1125,SP:7a04efbf835c238bbdf70a8b8decee4ec2907a3a,"methods USED-FOR smoothness of the prediction function. deep network USED-FOR smoothness of the prediction function. validation loss CONJUNCTION validation loss. validation loss CONJUNCTION validation loss. measure EVALUATE-FOR methods. explicit regularization CONJUNCTION distillation. distillation CONJUNCTION explicit regularization. measure FEATURE-OF logits. l2 norm FEATURE-OF measure. l2 norm FEATURE-OF logits. weight decay HYPONYM-OF explicit regularization. OtherScientificTerm are prediction function, and sine of desired frequency. Task are multiclass classification, and smoothness of the predicted function. ","This paper proposes methods to improve the smoothness of the prediction function of a deep network. The proposed methods are based on a measure of the l2 norm of the logits of the predicted function, which is defined as the sine of desired frequency. The authors propose to use explicit regularization (e.g., weight decay) and distillation (i.e., adding a validation loss to the validation loss), and show that these methods can improve the performance of multiclass classification. "
1126,SP:7a04efbf835c238bbdf70a8b8decee4ec2907a3a,"spectral bias FEATURE-OF neural networks. fully - connected nets CONJUNCTION NTK regime. NTK regime CONJUNCTION fully - connected nets. spectral bias USED-FOR image recognition networks. OtherScientificTerm is low - frequency information. Generic is it. Method are explicit and implicit regularization schemes, and model distillation. ","This paper studies the spectral bias of neural networks. The spectral bias is an important issue for image recognition networks, and it has been studied in both fully-connected nets and in the NTK regime. The authors argue that spectral bias can be explained by low-frequency information, and propose explicit and implicit regularization schemes to address it. They also propose model distillation to address this issue."
1127,SP:9ef61f2064db8ac3b01b16694a744b274bdbbe83,mode - switching strategy USED-FOR exploration / exploitation dilemma. mode - switching strategy COMPARE monolithic behaviour policies. monolithic behaviour policies COMPARE mode - switching strategy. monolithic behaviour policies USED-FOR diverse behaviour. granularities CONJUNCTION switching mechanisms. switching mechanisms CONJUNCTION granularities. blind vs. informed switching HYPONYM-OF switching mechanisms. Random Network Distillation ( RND ) CONJUNCTION uniform policy. uniform policy CONJUNCTION Random Network Distillation ( RND ). they USED-FOR exploration. uniform policy USED-FOR they. Random Network Distillation ( RND ) USED-FOR they. uniform policy USED-FOR exploration. Random Network Distillation ( RND ) USED-FOR exploration. Method is Atari Learning Environment ( ALE ). ,"This paper proposes a mode-switching strategy to solve the exploration/exploitation dilemma, which is different from monolithic behaviour policies that aim to learn diverse behaviour. The authors argue that this is because of the granularities and switching mechanisms (e.g., blind vs. informed switching). The authors demonstrate that they can use Random Network Distillation (RND) and a uniform policy to improve exploration performance on Atari Learning Environment (ALE)."
1128,SP:9ef61f2064db8ac3b01b16694a744b274bdbbe83,reward - free exploration phase CONJUNCTION task - dependent learning phase. task - dependent learning phase CONJUNCTION reward - free exploration phase. exploration CONJUNCTION exploitation. exploitation CONJUNCTION exploration. intra - episodic level FEATURE-OF exploration. exploration USED-FOR level. intra - episodic exploration COMPARE exploration schemes. exploration schemes COMPARE intra - episodic exploration. R2D2 base agent USED-FOR exploration schemes. informed switching component USED-FOR switching behaviors. Generic is methods. OtherScientificTerm is switching. ,"This paper proposes two methods for learning to switch between a reward-free exploration phase and a task-dependent learning phase. The first level is based on exploration at an intra-episodic level, and the second level is a combination of exploration and exploitation. The authors show that the proposed methods can achieve better performance than existing exploration schemes based on the R2D2 base agent. The switching behaviors are further improved by adding an informed switching component."
1129,SP:9ef61f2064db8ac3b01b16694a744b274bdbbe83,Atari games EVALUATE-FOR switching mechanisms. Method is reinforcement learning. ,This paper studies the switching mechanisms in Atari games. The main contribution of this paper is to study the effect of different switching mechanisms on the performance of reinforcement learning. 
1130,SP:9ef61f2064db8ac3b01b16694a744b274bdbbe83,"exploitation CONJUNCTION exploration. exploration CONJUNCTION exploitation. Method are RL learning, and intra - episodic exploration variants. ","This paper studies the problem of RL learning in the context of exploitation and exploration. The authors propose two intra-episodic exploration variants, where the agent is encouraged to explore in an unsupervised manner. The main contribution of this paper is to provide a theoretical analysis of the trade-off between exploration and exploitation."
1131,SP:c44d3749d8883fae7eb2a6378417fca28d25a4c9,"tree metric FEATURE-OF metric. additive error FEATURE-OF algorithm. differentially private setting FEATURE-OF HST's. synthetic graphs CONJUNCTION MNIST dataset. MNIST dataset CONJUNCTION synthetic graphs. weighted shortest path distance FEATURE-OF graph. metric space FEATURE-OF synthetic graphs. weighted shortest path distance USED-FOR metric space. differentially private method COMPARE methods. methods COMPARE differentially private method. initialization methods COMPARE differentially private method. differentially private method COMPARE initialization methods. initial cost FEATURE-OF initialization methods. initial cost EVALUATE-FOR differentially private method. initial cost EVALUATE-FOR methods. Task are $ k$-median clustering, and differentially private clustering. Method are local search algorithm, $ k$-means, k$-median clustering, initialization algorithm, k - median clustering, k - median++, local search method, initialization, HST, and private local search algorithm. OtherScientificTerm are hierarchically well - separated tree ( HST ), local search iterations, noise, and tree. Metric is initial costs. ","This paper studies the problem of $k$-median clustering, where the goal is to find a hierarchically well-separated tree (HST) that minimizes a local search algorithm. The authors propose a new metric based on the tree metric, which they call $k-means$. They show that the algorithm has an additive error of $O(\sqrt{T})$ and that this is due to the fact that HST's are trained in a differentially private setting. They then propose an initialization algorithm, which is called k-minimizer++, where each local search iterations is initialized with $O(T)$, where $T$ is the number of iterations needed to find the optimal tree. They evaluate their initialization on synthetic graphs and on the MNIST dataset, and show that their initialization results in a lower initial cost compared to the other methods. They also compare their initialization to the state-of-the-art initialization methods, and find that their initial costs are also lower than the state of the art initialization methods. Finally, the authors show that for k$=1, they can find a HST that is $k+1$-separate, where $k=1/k$ is k-mean and $k<k$. The authors also show that this can be achieved by training a private HST, which can be used to train a private local search method."
1132,SP:c44d3749d8883fae7eb2a6378417fca28d25a4c9,"initialization scheme USED-FOR k - median problem. metric embedding tree structure USED-FOR initialization scheme. HST USED-FOR algorithm. initialization method USED-FOR muliplicative and additive errors. initialization USED-FOR k - median++ initialization. OtherScientificTerm are graph input ( or general metric spaces, and Euclidean space. Metric is approximation factor. ","This paper proposes a new initialization scheme for the k-median problem based on the metric embedding tree structure. The algorithm is based on HST and is trained on graph input (or general metric spaces). The authors show that the proposed initialization method is robust to muliplicative and additive errors. The authors also provide a theoretical analysis of the approximation factor of the proposed algorithm. Finally, the authors provide a proof of the convergence of their initialization to the optimal solution for k-midian++ initialization in Euclidean space."
1133,SP:c44d3749d8883fae7eb2a6378417fca28d25a4c9,initialization scheme USED-FOR k - medians clustering problem. metric embeddings USED-FOR This. this USED-FOR differential privacy ( DP ) setting. approximation guarantees FEATURE-OF non - DP and DP settings. algorithms COMPARE baselines. baselines COMPARE algorithms. real world and synthetic datasets EVALUATE-FOR multiple metrics. multiple metrics EVALUATE-FOR baselines. multiple metrics EVALUATE-FOR algorithms. real world and synthetic datasets EVALUATE-FOR baselines. real world and synthetic datasets EVALUATE-FOR algorithms. OtherScientificTerm is Hierarchically well - separated trees. ,This paper proposes a new initialization scheme for the k-medians clustering problem. This is based on metric embeddings and is motivated by the idea of Hierarchically well-separated trees. The authors extend this to the differential privacy (DP) setting and provide approximation guarantees for both non-DP and DP settings. The proposed algorithms are evaluated on both real world and synthetic datasets and compared with several baselines.
1134,SP:c44d3749d8883fae7eb2a6378417fca28d25a4c9,algorithm USED-FOR metric k - median problem. Metric embedding theory USED-FOR algorithm. local search based algorithm USED-FOR k - median. initialization routine USED-FOR local search based algorithm. algorithm USED-FOR initialization routine. algorithm USED-FOR local search based algorithm. k - means++ algorithm HYPONYM-OF initialisation algorithm. datasets EVALUATE-FOR k - means++ algorithm. MNIST HYPONYM-OF datasets. Metric is k - median approximation factor. ,"This paper proposes an algorithm for solving the metric k-median problem based on Metric embedding theory. The proposed algorithm is used as an initialization routine for a local search based algorithm for k-minimising. The k-means++ algorithm is an initialisation algorithm and is evaluated on two datasets, MNIST and CIFAR-10. The experiments show that the k-mean approximation factor is small."
1135,SP:84c9eb6623e7950585d80a664dd51b3ecc356dea,"parameters USED-FOR architecture. data augmentation USED-FOR generalization. KITTI CONJUNCTION Robonet. Robonet CONJUNCTION KITTI. Robonet CONJUNCTION BAIR pushing dataset. BAIR pushing dataset CONJUNCTION Robonet. Human 3.6 M CONJUNCTION KITTI. KITTI CONJUNCTION Human 3.6 M. method COMPARE baselines. baselines COMPARE method. Human 3.6 M CONJUNCTION Robonet. Robonet CONJUNCTION Human 3.6 M. Human 3.6 M HYPONYM-OF datasets. BAIR pushing dataset HYPONYM-OF datasets. datasets EVALUATE-FOR method. datasets EVALUATE-FOR baselines. Robonet HYPONYM-OF datasets. KITTI HYPONYM-OF datasets. Task are conditional video prediction, and models underfitting. ","This paper addresses the problem of conditional video prediction and proposes a new architecture based on different parameters. The main idea is to use data augmentation to improve generalization and reduce models underfitting. The method is evaluated on three datasets: Human 3.6M, KITTI, Robonet, and BAIR pushing dataset. The proposed method outperforms the baselines."
1136,SP:84c9eb6623e7950585d80a664dd51b3ecc356dea,parameter count USED-FOR prior models. datasets EVALUATE-FOR methods. image augmentation techniques USED-FOR overfitting. prediction benchmarks EVALUATE-FOR it. image augmentation techniques USED-FOR FitVid. FitVid's architecture USED-FOR stochastic video prediction task. SE - UNet LSTM USED-FOR FitVid's architecture. Method is variational video prediction model FitVid. Material is video prediction datasets. OtherScientificTerm is underfitting. ,"This paper proposes a variational video prediction model FitVid, which is based on the SE-UNet LSTM, and it is evaluated on a variety of prediction benchmarks. The authors argue that prior models are overfitting due to the large parameter count used in prior models. To address this issue, the authors propose to use image augmentation techniques to reduce overfitting. The proposed methods are evaluated on three datasets and compared to existing methods. The results show that the proposed model, fitVid is able to achieve state-of-the-art performance on the video prediction datasets. The paper also presents a theoretical analysis of the effect of underfitting on the performance of the proposed method. Finally, the paper presents an ablation study of the fitability of the new model and the impact of different image augmentation techniques on fitability. The experimental results demonstrate that the fitfulness of the resulting model is better than the existing methods on the three datasets. Lastly, the author also presents an analysis of how the fit of the original fit of FitVID's architecture to the stochastic video prediction task with SE-unet LSPM."
1137,SP:84c9eb6623e7950585d80a664dd51b3ecc356dea,network architecture USED-FOR video prediction. FitVid USED-FOR video prediction. FitVid HYPONYM-OF network architecture. model architecture USED-FOR FitVid. FitVid COMPARE work. work COMPARE FitVid. video datasets EVALUATE-FOR FitVid. Generic is tasks. OtherScientificTerm is underfitting. ,"This paper proposes FitVid, a network architecture for video prediction. The model architecture is similar to the previous work, but the tasks are different. The authors claim that this is due to underfitting. The experiments on several video datasets show that FitVvid outperforms previous work."
1138,SP:84c9eb6623e7950585d80a664dd51b3ecc356dea,framework USED-FOR video prediction. FITVID USED-FOR video prediction. Sikp connection USED-FOR FitVid. LSTMs HYPONYM-OF modules. Sikp connection HYPONYM-OF modules. LSTMs USED-FOR FitVid. modules USED-FOR FitVid. training strategy USED-FOR FitVid. Data augmentation USED-FOR overfitting. ,This paper proposes a framework called FITVID for video prediction. FitVid is based on two modules: LSTMs and the Sikp connection. The main contribution of this paper is a new training strategy for FitVvid. Data augmentation is used to prevent overfitting.
1139,SP:6eb5ce1d85928a3af759d75016089c535941d0b0,"random features CONJUNCTION neural tangent kernel. neural tangent kernel CONJUNCTION random features. mean - squared error loss USED-FOR linear models. neural tangent kernel HYPONYM-OF linear models. random features HYPONYM-OF linear models. closed - form expression USED-FOR time - evolution of the expected test loss. one - pass SGD CONJUNCTION Gaussian features. Gaussian features CONJUNCTION one - pass SGD. data distribution CONJUNCTION sampling sequence. sampling sequence CONJUNCTION data distribution. closed - form expression USED-FOR expected test loss. closed - form expression USED-FOR one - pass SGD. expected test loss EVALUATE-FOR one - pass SGD. arbitrary covariance FEATURE-OF non - Gaussian feature maps. regularity condition FEATURE-OF fourth moments of the features. features covariance USED-FOR upper bound. expressions USED-FOR expected test and training losses. Task is dynamics of stochastic gradient descent ( SGD ). Method are stochastic gradient descent ( SGD ), and multi - pass SGD. OtherScientificTerm are arbitrary covariance structure, non - Gaussian effects, and features. ","This paper studies the dynamics of stochastic gradient descent (SGD). The authors consider linear models with mean-squared error loss, such as random features and neural tangent kernel. The authors propose a closed-form expression for the time-evolution of the expected test loss for one-pass SGD and Gaussian features. The main idea is to consider the data distribution and the sampling sequence as a function of the arbitrary covariance structure of the non-Gaussian feature maps. The paper provides an upper bound on the features covariance, which depends on the regularity condition on the fourth moments of the features. This upper bound is then used to derive expressions for both expected test and training losses, which are then applied to multi-process SGD. "
1140,SP:6eb5ce1d85928a3af759d75016089c535941d0b0,stochastic gradient descent USED-FOR linear models. learning dynamics FEATURE-OF stochastic gradient descent. structured features FEATURE-OF linear models. learning dynamics CONJUNCTION optimal batch sizes. optimal batch sizes CONJUNCTION learning dynamics. data structure USED-FOR learning dynamics. data structure USED-FOR optimal batch sizes. model USED-FOR training / test error. model USED-FOR small neural networks. training / test error FEATURE-OF small neural networks. real data USED-FOR small neural networks. ,This paper studies the learning dynamics of stochastic gradient descent for linear models with structured features. The authors propose a new data structure to model learning dynamics and optimal batch sizes. The proposed model is able to reduce the training/test error of small neural networks trained on real data.
1141,SP:6eb5ce1d85928a3af759d75016089c535941d0b0,expected test loss EVALUATE-FOR linear model. SGD training dynamics USED-FOR linear model. eigenvectors CONJUNCTION eigenvalues. eigenvalues CONJUNCTION eigenvectors. eigenvalues FEATURE-OF covariance matrix. covariance matrix FEATURE-OF features. forth - moments FEATURE-OF non - Gaussian features. formula USED-FOR Gaussian case. formalism USED-FOR online setting. Metric is time - dependent average test loss. Method is Gaussian formula. Material is real world datasets. ,"This paper studies the expected test loss of a linear model trained with SGD training dynamics. The main result is that the time-dependent average test loss can be expressed as a function of the eigenvectors and eigenvalues of the covariance matrix of the features. The authors extend the Gaussian formula to the case of non-Gaussian features with forth-moments, and extend the formula to Gaussian case as well. The formalism is extended to the online setting, and experiments are conducted on real world datasets."
1142,SP:6eb5ce1d85928a3af759d75016089c535941d0b0,"stochastic gradient descent USED-FOR linear models. iteration number CONJUNCTION batch size. batch size CONJUNCTION iteration number. structure of the data distribution CONJUNCTION iteration number. iteration number CONJUNCTION structure of the data distribution. learning curves FEATURE-OF SGD algorithm. Method are one - pass SGD, and multi - pass SGD. ","This paper studies stochastic gradient descent for linear models. The authors consider the case of one-pass SGD and show that the learning curves of the SGD algorithm can be bounded by the structure of the data distribution, the iteration number, and the batch size. In particular, the authors show that when the data is large enough, multi-process SGD can converge to a solution that is close to the optimal solution."
1143,SP:a530dd966911e387a90e3cbf9f51c8cab6152723,"SGD USED-FOR local maximizers. constant step - size USED-FOR SGD. stochastic ) gradient USED-FOR ( stochastic ) loss function. quartic loss function USED-FOR SGD. SGD USED-FOR function's sharper minimizers. AMSGrad algorithm USED-FOR undesirable maximizer. Task is minimization problem. Generic is algorithm. OtherScientificTerm are step - size, random variable, and Hessian. Method are gradient - like diffusion, and continuous - time model of ( SGD ). ","This paper studies the minimization problem of a function with constant step-size, where the goal is to find local maximizers that minimize the function's sharper minimizers. The authors propose an algorithm called AMSGrad, which is a continuous-time model of (SGD) where the (stochastic) gradient is replaced by a (stochastic) loss function, where SGD is trained with a quartic loss function. The main idea of the algorithm is to minimize a function whose Hessian can be expressed as a gradient-like diffusion, and then to use SGD to find the local minimizers of the function. In particular, the authors show that if the function is a random variable and the step-sizes are constant, then the algorithm will converge to a point where the Hessian converges to a fixed point. The paper also provides a theoretical analysis of the AMSGad algorithm for finding an undesirable maximizer."
1144,SP:a530dd966911e387a90e3cbf9f51c8cab6152723,"distributions CONJUNCTION distribution. distribution CONJUNCTION distributions. 2 - dimensional quartics FEATURE-OF distribution. local maximum FEATURE-OF non - convex distributions. local maximum FEATURE-OF AMSgrad. central limit theorem USED-FOR iterates. Method are stochastic gradient descent, and SGD. Generic is it. OtherScientificTerm are maximum, convex settings, learning rate, quadratics, convex loss, quadratic objectives, logarithm SGD's iterate, i.i.d. random variables, and small neural network experiment. ","This paper studies stochastic gradient descent from the point of view of the maximum of a distribution over 2-dimensional quartics. The authors consider the case where the distributions and the distribution of the distribution are either convex or non-convex. In the convex settings, the authors show that AMSgrad converges to a local maximum that is at least as large as it can be in the original convex setting, and that the learning rate is polynomial in the number of quadratics. They show that the same result holds for the case of SGD with quadratic objectives. They also prove that the local maximum of non-consistency is also polynomially larger than the maximum in the case that the distribution is convex. Finally, they prove a central limit theorem for such iterates, which shows that for any convex loss, the logarithm SGD's iterate will converge to a point that is close to the optimal solution of a convex function. They experimentally verify their results on i.i.d. random variables and on a small neural network experiment."
1145,SP:a530dd966911e387a90e3cbf9f51c8cab6152723,restrictive and unrealistic assumptions FEATURE-OF noise. restrictive and unrealistic assumptions USED-FOR they. AMSGrad USED-FOR local maxima. sharp minima USED-FOR SGD. Task is optimization problems. Generic is assumptions. Material is nonconvex case. ,"This paper studies optimization problems from the perspective of noise, and shows that they are subject to restrictive and unrealistic assumptions on the noise. In particular, they show that AMSGrad converges to local maxima, which is not the case in the nonconvex case. They also show that sharp minima for SGD do not converge to sharp maxima under these assumptions."
1146,SP:a530dd966911e387a90e3cbf9f51c8cab6152723,unintuitive behavior FEATURE-OF SGD. Adaptive methods USED-FOR global maximum. SGD USED-FOR local maximum. OtherScientificTerm is learning rate. Method is GD. ,"This paper studies the unintuitive behavior of SGD. Adaptive methods aim to converge to a global maximum, while SGD aims to reach a local maximum. The authors show that the learning rate of GD converges to zero as the number of iterations increases."
1147,SP:22d01913b78ef447b064c65a646fa301b861d3f7,hyperparameter optimization algorithm USED-FOR meta - learning. algorithms USED-FOR second - order hypergradients. knowledge distillation USED-FOR algorithms. knowledge distillation USED-FOR second - order hypergradients. tinyImageNet CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION tinyImageNet. Method is inner loop optimization. OtherScientificTerm is hyperparameters. ,This paper proposes a hyperparameter optimization algorithm for meta-learning. The main idea is to use knowledge distillation to improve the performance of existing algorithms for learning second-order hypergradients. The inner loop optimization is performed to find the optimal hyperparameters. Experiments are conducted on tinyImageNet and CIFAR100.
1148,SP:22d01913b78ef447b064c65a646fa301b861d3f7,hyperparameter optimization algorithm USED-FOR meta - learning. hypergradient second - order term PART-OF one - step Jacobioan - vector product. approximated algorithm USED-FOR hyperparameter optimization. hyperparameter optimization USED-FOR higher dimensional hyperparameters. benchmark datasets EVALUATE-FOR approach. OtherScientificTerm is high dimensional hyperparameters. ,This paper proposes a hyperparameter optimization algorithm for meta-learning. The hypergradient second-order term is incorporated into a one-step Jacobioan-vector product. The authors show that this approximated algorithm can be applied to hyperparametrization of higher dimensional hyperparameters. The proposed approach is evaluated on several benchmark datasets.
1149,SP:22d01913b78ef447b064c65a646fa301b861d3f7,"hyperparameter dimension CONJUNCTION memory constraints. memory constraints CONJUNCTION hyperparameter dimension. HyperDistill HYPONYM-OF gradient - based hyperparameter optimization method. Jacobian - vector product USED-FOR it. validation losses CONJUNCTION hypergradient estimates. hypergradient estimates CONJUNCTION validation losses. hypergradient estimates EVALUATE-FOR HyperDistill. validation losses EVALUATE-FOR HyperDistill. Metric is accuracy. OtherScientificTerm are hyper - gradient update terms, and hyperparameters. Method is gradient updates. Task is online setting. ","This paper proposes HyperDistill, a gradient-based hyperparameter optimization method, which is a generalization of the HyperGradill (Zhang et al., 2018) that aims to improve the accuracy while keeping the hyper-gradient update terms small. The authors argue that the current gradient updates are not scalable in the online setting due to the high number of hyperparameters and memory constraints. To address this issue, the authors propose to use a Jacobian-vector product to approximate it, which allows to reduce the number of parameters required to compute the gradient updates. Experiments are conducted on validation losses and hypergradient estimates to show the effectiveness of HyperDistrill."
1150,SP:22d01913b78ef447b064c65a646fa301b861d3f7,meta - learning USED-FOR hyperparameter optimization. Unrolled differentiation USED-FOR meta - gradient. Implicit Function Theorem CONJUNCTION Unrolled differentiation. Unrolled differentiation CONJUNCTION Implicit Function Theorem. Implicit Function Theorem USED-FOR meta - gradient. second - order approximation USED-FOR method. knowledge distillation USED-FOR method. knowledge distillation USED-FOR second - order approximation. Method is online hyperparameter optimization algorithm. Task is second - order gradient computation. ,This paper proposes an online hyperparameter optimization algorithm based on meta-learning in the context of meta-learning for the problem of second-order gradient optimization. The meta-gradient is based on the Implicit Function Theorem and Unrolled differentiation. The proposed method uses knowledge distillation to provide a second order approximation of the original method. The paper also provides a theoretical analysis of the effect of the choice of the second order gradient computation.
1151,SP:a64b26faef315c3ece590322291bab198932c604,task - aware - modulation USED-FOR global Meta - learning. rehearsed task gradient descent trajectory PART-OF task representation. network USED-FOR rehearsed task - trajectory characterization. feature representation USED-FOR network. feature representation USED-FOR rehearsed task - trajectory characterization. few - shot image classification ( meta dataset CONJUNCTION miniimagenet. miniimagenet CONJUNCTION few - shot image classification ( meta dataset. cold - start recommendation tasks EVALUATE-FOR framework. few - shot image classification ( meta dataset EVALUATE-FOR framework. OtherScientificTerm is heterogeneous distributions. Method is feature - based task characterization. Task is rehearsal task. ,"This paper proposes task-aware-modulation for global Meta-learning. The key idea is to incorporate the rehearsed task gradient descent trajectory into the task representation. The proposed framework is evaluated on few-shot image classification (meta dataset and miniimagenet) and cold-start recommendation tasks. The experiments show that the proposed feature representation can be used to train a network for the task-trajectory characterization in the presence of heterogeneous distributions. The main contribution of the paper is the feature-based task characterization, which can be applied to any rehearsal task."
1152,SP:a64b26faef315c3ece590322291bab198932c604,gradient descent USED-FOR support set. rehearsed ” learning path USED-FOR geometric information. gradient descent USED-FOR rehearsed ” learning path. task - conditioned initialization USED-FOR meta - learning approach. MAML USED-FOR meta - learning approach. loss PART-OF geometric information. GRU architecture USED-FOR path embedding. GRU architecture USED-FOR information. clustering USED-FOR feature and path embeddings. neural network USED-FOR task - specific initialization. neural network USED-FOR path and feature embeddings. rehearsing USED-FOR path embedding. tunnel ’ connection USED-FOR path embedding. feature embedding USED-FOR path embedding. meta - learned connection COMPARE rehearsing. rehearsing COMPARE meta - learned connection. few - shot classification CONJUNCTION cold - start recommender problems. cold - start recommender problems CONJUNCTION few - shot classification. approach COMPARE MAML - based approaches. MAML - based approaches COMPARE approach. cold - start recommender problems EVALUATE-FOR approach. few - shot classification EVALUATE-FOR approach. OtherScientificTerm is features. Task is task adaptation process. Generic is system. ,"This paper proposes a meta-learning approach based on MAML with task-conditioned initialization. The key idea is to learn a “rehearsed” learning path that encodes geometric information using gradient descent on the support set. This loss is then incorporated into the geometric information by a GRU architecture that learns a path embedding using a ‘tunnel’ connection. The feature and path embeddings are learned using clustering and a neural network is used for task-specific initialization. In the end, the feature embedding is used as a proxy for the path and the features are used for the task adaptation process. The authors evaluate their approach on few-shot classification and cold-start recommender problems and show that the proposed approach outperforms the other state-of-the-art, and also outperforms other MAL-based approaches. They also provide a theoretical analysis of their system."
1153,SP:a64b26faef315c3ece590322291bab198932c604,path USED-FOR sequence module. learning path USED-FOR algorithm. algorithm USED-FOR sequence module. path USED-FOR algorithm. soft cluster centers USED-FOR task feature. model parameter USED-FOR task adaptation. combination USED-FOR model parameter. combination USED-FOR task adaptation. image classification CONJUNCTION cold - start recommendation. cold - start recommendation CONJUNCTION image classification. image classification EVALUATE-FOR baseline algorithms. cold - start recommendation EVALUATE-FOR baseline algorithms. Method is clustered task - aware meta - learning algorithm. ,This paper proposes a clustered task-aware meta-learning algorithm. The algorithm uses a learning path to learn a sequence module. The soft cluster centers are used to model the task feature. A combination of the learned path and the learned model parameter is used for task adaptation. Experiments on image classification and cold-start recommendation are conducted to compare with baseline algorithms.
1154,SP:a64b26faef315c3ece590322291bab198932c604,heterogeneous tasks USED-FOR meta - learning. task cluster USED-FOR meta - learner. clustering task representations USED-FOR problem. parameter space FEATURE-OF optimization trajectory. input representation ( features ) USED-FOR tasks. ,"This paper studies the problem of meta-learning on heterogeneous tasks. The problem is formulated as clustering task representations, where each task cluster is used to train a meta-learner. The optimization trajectory is defined in parameter space, and the input representation (features) of the tasks are learned."
1155,SP:f7a7c81ed2b6e9eb958b8b751deed8166622540c,ensemble models USED-FOR transductive novelty detection method. fine - tuning USED-FOR models. framework USED-FOR OOD samples. disagreement USED-FOR framework. transductive settings USED-FOR OOD samples. Material is unlabeled test set samples. ,This paper proposes a transductive novelty detection method based on ensemble models. The authors propose a framework based on disagreement and fine-tuning to detect OOD samples in transductionive settings. The experiments are conducted on unlabeled test set samples.
1156,SP:f7a7c81ed2b6e9eb958b8b751deed8166622540c,"ensemble - based semi - supervised learning method USED-FOR novelty detection. models PART-OF ensemble. early - stopping USED-FOR implicit regularization. self - training algorithm USED-FOR training. softmax outputs PART-OF ensemble. Method is ensemble of models. Material is unlabeled pool. OtherScientificTerm are OOD samples, out- or in - distribution, average disagreement, In - distribution samples, and out - distribution samples. ","This paper proposes an ensemble-based semi-supervised learning method for novelty detection. The authors propose an ensemble of models that are trained on an unlabeled pool, where the OOD samples are sampled from the out- or in-distribution. The ensemble consists of two models: (1) models that produce softmax outputs, and (2) models which produce outputs that are close to the average disagreement between the two samples. The training is performed using a self-training algorithm, where early-stopping is used as an implicit regularization. In-distributions samples are generated by sampling from the same distribution as the out -distribution samples."
1157,SP:f7a7c81ed2b6e9eb958b8b751deed8166622540c,"semi - supervised ensemble approach USED-FOR novelty detection. early - stop criterion USED-FOR ensemble. ID data PART-OF unlabeled data. labeled in - distribution ( ID ) data USED-FOR base classifier. unlabeled data USED-FOR base classifier. labeled data USED-FOR classifier. validation accuracy CONJUNCTION training error. training error CONJUNCTION validation accuracy. OOD data USED-FOR base classifiers. Method are tuning, and regularization process. ",This paper proposes a semi-supervised ensemble approach for novelty detection. The ensemble is trained using an early-stop criterion. The base classifier is trained on unlabeled data with labeled in-distribution (ID) data. The ID data is added to the unlabeling data during tuning. The classifier can then be trained on the labeled data without any regularization process. The authors show that training on OOD data improves both validation accuracy and training error.
1158,SP:f7a7c81ed2b6e9eb958b8b751deed8166622540c,ensemble - based procedure USED-FOR semi - supervised novelty detection ( SSND ). unlabeled ID and OOD samples USED-FOR near OOD data. It USED-FOR near OOD data. unlabeled ID and OOD samples USED-FOR It. regularization technique USED-FOR diversity. diversity FEATURE-OF OOD data. regularization technique USED-FOR OOD data. regularization technique USED-FOR agreement. agreement FEATURE-OF ID data. ,This paper proposes an ensemble-based procedure for semi-supervised novelty detection (SSND). It uses unlabeled ID and OOD samples to detect near OOD data. The authors also propose a regularization technique to increase the diversity of the ID data and to improve the agreement between ID data.
1159,SP:a0ee0e08b4bb578836fd5e0781e8713f254569fb,transformer USED-FOR multi - agent motion forecasting. attention layers USED-FOR social information. motion CONJUNCTION social information. social information CONJUNCTION motion. attention layers USED-FOR motion. latent variable USED-FOR discrete motion. model COMPARE methods. methods COMPARE model. training time EVALUATE-FOR model. training time EVALUATE-FOR methods. Material is dataset. ,This paper proposes a transformer for multi-agent motion forecasting. The key idea is to use attention layers to model both motion and social information. The latent variable is used to model discrete motion. The model is evaluated on a dataset with a large number of agents and compared to other methods on a small number of tasks and training time.
1160,SP:a0ee0e08b4bb578836fd5e0781e8713f254569fb,"Transformer - based architecture USED-FOR trajectory prediction tasks. long - term temporal dependencies CONJUNCTION social interactions. social interactions CONJUNCTION long - term temporal dependencies. spatio - temporal representations USED-FOR long - term temporal dependencies. spatio - temporal representations USED-FOR social interactions. spatio - temporal representations USED-FOR multi - agent setting. discrete latent variable USED-FOR architecture. Generic is task. Method are context representation, and decoder. ","This paper proposes a Transformer-based architecture for trajectory prediction tasks. The main idea is to use spatio-temporal representations to model long-term temporal dependencies and social interactions in a multi-agent setting. The proposed architecture is based on a discrete latent variable, which represents the task as a sequence of trajectories. The context representation is learned by the decoder."
1161,SP:a0ee0e08b4bb578836fd5e0781e8713f254569fb,TrajNet CONJUNCTION predicting Omniglot strokes. predicting Omniglot strokes CONJUNCTION TrajNet. multi - agent trajectory prediction problem USED-FOR autonomous driving. method USED-FOR multi - modal distributions. method USED-FOR sequences of structured continuous variables. argoverse CONJUNCTION trajnet. trajnet CONJUNCTION argoverse. trajnet CONJUNCTION omniglot stroke prediction. omniglot stroke prediction CONJUNCTION trajnet. nuscenes CONJUNCTION argoverse. argoverse CONJUNCTION nuscenes. self - attention USED-FOR latent variable sequential set transformer. ,"This paper studies the multi-agent trajectory prediction problem in autonomous driving. The authors propose a method for learning multi-modal distributions and sequences of structured continuous variables. Experiments are conducted on nuscenes, argoverse, trajnet, and predicting Omniglot strokes. The latent variable sequential set transformer is trained with self-attention."
1162,SP:a0ee0e08b4bb578836fd5e0781e8713f254569fb,transformer - based VAE model USED-FOR motion prediction. transformer - based VAE model USED-FOR multi - model and scene - consistent predictions. transformer USED-FOR social and temporal information. Argoverse dataset EVALUATE-FOR method. nuScenes dataset EVALUATE-FOR method. ,This paper proposes a transformer-based VAE model for motion prediction for multi-model and scene-consistent predictions. The proposed method is evaluated on the Argoverse dataset and the nuScenes dataset. The main idea is to use the transformer to capture both social and temporal information.
1163,SP:e253d49bbfadb76b2f7c4e7cdd1cc33d0cebc3e7,"approach USED-FOR synthesizing the dataset. shape HYPONYM-OF predefined features. counter - factual explanation COMPARE baseline explanation. baseline explanation COMPARE counter - factual explanation. output logits USED-FOR baseline explanation. Generic are dataset, systems, and system. Method is explanation approaches. ","This paper presents an approach for synthesizing the dataset from predefined features (e.g., shape, color, etc). The dataset is generated by training two systems, where the first system generates a counter-factual explanation, and the second system produces a baseline explanation based on the output logits. The authors compare the performance of the two explanation approaches and show that the counter-factsual explanation outperforms the baseline explanation."
1164,SP:e253d49bbfadb76b2f7c4e7cdd1cc33d0cebc3e7,Material is synthetic dataset. Generic is dataset. ,"This paper presents a synthetic dataset that attempts to answer the question of whether it is possible to learn from data. The dataset is designed to be easy to use and easy to understand. The paper is well-written and well-structured, and the experiments are well-done."
1165,SP:e253d49bbfadb76b2f7c4e7cdd1cc33d0cebc3e7,"dataset USED-FOR user - study. dataset USED-FOR interpretability methods. user - study USED-FOR interpretability methods. TWO4TWO USED-FOR user - study. TWO4TWO USED-FOR interpretability methods. counterfactual and concept - based explanations HYPONYM-OF interpretability methods. model explanation USED-FOR features. explanation models COMPARE baseline method. baseline method COMPARE explanation models. user studies EVALUATE-FOR explanation techniques. OtherScientificTerm are ground - truth features, and baseline explanation. ","This paper presents a user-study of interpretability methods based on the dataset of TWO4TWO. Specifically, the authors consider two types of explainability methods: counterfactual and concept-based explanations. In the first case, the model explanation is used to extract features that are independent of the ground-truth features, while in the second case, a baseline explanation is provided. In both cases, the explanation models are compared to a baseline method, and the performance of the explanation techniques is evaluated on two user studies."
1166,SP:e253d49bbfadb76b2f7c4e7cdd1cc33d0cebc3e7,explanation methods USED-FOR image classifiers. user study USED-FOR explanation methods. animal position CONJUNCTION shape. shape CONJUNCTION animal position. background color CONJUNCTION animal position. animal position CONJUNCTION background color. toy environment USED-FOR spurrious correlations. attributes USED-FOR spurrious correlations. background color HYPONYM-OF attributes. shape HYPONYM-OF attributes. animal position HYPONYM-OF attributes. explanation methods USED-FOR features. baseline COMPARE counterfactual explanations. counterfactual explanations COMPARE baseline. baseline COMPARE concept highlighting. concept highlighting COMPARE baseline. pre - registered user studies EVALUATE-FOR baseline. counterfactual explanations CONJUNCTION concept highlighting. concept highlighting CONJUNCTION counterfactual explanations. concept highlighting COMPARE baseline. baseline COMPARE concept highlighting. baseline CONJUNCTION counterfactual method. counterfactual method CONJUNCTION baseline. interpretability method COMPARE baseline. baseline COMPARE interpretability method. Generic is model. OtherScientificTerm is generation code. ,"This paper presents a user study of explanation methods for image classifiers based on user study. Specifically, the authors use a toy environment to study spurrious correlations between attributes such as background color, animal position, shape, etc. They show that existing explanation methods fail to capture these features, and propose a new model that does so. The authors compare their baseline with counterfactual explanations, concept highlighting, and a counterfactually method on pre-registered user studies. They also show that their interpretability method outperforms the baseline, and that their generation code is more interpretable."
1167,SP:6495caf14ebb8b9c3cbf50a5f05ec1eb600864fe,"defense USED-FOR poisoning attacks. outlier detection technique USED-FOR defense. inliers CONJUNCTION outliers. outliers CONJUNCTION inliers. OtherScientificTerm is self - expanding "" sets. Method is backdoor attack. Generic is technique. ","This paper proposes a defense against poisoning attacks based on an outlier detection technique. The idea is to use ""self-expanding"" sets to detect the presence of inliers and outliers, which can be used as a backdoor attack. Experiments show that the proposed technique is effective."
1168,SP:6495caf14ebb8b9c3cbf50a5f05ec1eb600864fe,targeted attack USED-FOR neural networks. prediction accuracy EVALUATE-FOR clean data. poisoned training data USED-FOR clean data. ensemble of weaker learners USED-FOR poisoned data. Task is backdoor attacks. Generic is defense. Material is homogeneous sets. ,"This paper studies the problem of targeted attack against neural networks. In particular, the authors focus on backdoor attacks, where the poisoned training data is used to improve the prediction accuracy of the clean data. The authors propose a defense based on an ensemble of weaker learners that is trained on the poisoned data. Experiments are conducted on homogeneous sets."
1169,SP:6495caf14ebb8b9c3cbf50a5f05ec1eb600864fe,defense USED-FOR backdoor attacks. iterative training procedure USED-FOR it. boosting framework USED-FOR clean data. ensemble of weak learners USED-FOR homogeneous sub - populations. boosting framework USED-FOR poisoned data. approach COMPARE defenses. defenses COMPARE approach. CIFAR-10 CONJUNCTION dirty - label backdoor attacks. dirty - label backdoor attacks CONJUNCTION CIFAR-10. defenses USED-FOR dirty - label backdoor attacks. CIFAR-10 EVALUATE-FOR approach. CIFAR-10 EVALUATE-FOR defenses. ,"This paper proposes a new defense against backdoor attacks. Specifically, it is based on an iterative training procedure, where an ensemble of weak learners is trained to generate homogeneous sub-populations, and a boosting framework is used to generate clean data and poisoned data. Experiments on CIFAR-10 and dirty-label backdoor attacks show that the proposed approach outperforms existing defenses."
1170,SP:6495caf14ebb8b9c3cbf50a5f05ec1eb600864fe,weak learners USED-FOR backdoor defense. weak learners USED-FOR iterative data filtering / expanding approach. clean distribution CONJUNCTION backdoor distribution. backdoor distribution CONJUNCTION clean distribution. ISPL ( Inverse Self - Paced Learnin ) CONJUNCTION weak learners. weak learners CONJUNCTION ISPL ( Inverse Self - Paced Learnin ). It HYPONYM-OF iterative approach. ISPL ( Inverse Self - Paced Learnin ) USED-FOR iterative approach. weak learners PART-OF iterative approach. Theoretical formulation CONJUNCTION jusstificaiton. jusstificaiton CONJUNCTION Theoretical formulation. Theoretical formulation CONJUNCTION empirical verfifiction. empirical verfifiction CONJUNCTION Theoretical formulation. jusstificaiton CONJUNCTION empirical verfifiction. empirical verfifiction CONJUNCTION jusstificaiton. Method is classifier. ,"This paper proposes an iterative data filtering/expanding approach that uses weak learners for backdoor defense. It is a general iterative approach that combines ISPL (Inverse Self-Paced Learnin) with weak learners. Theoretical formulation, jusstificaiton, and empirical verfifiction are provided. The main idea is to train a classifier to distinguish between a clean distribution and a backdoor distribution."
1171,SP:f0ad7cbc190113bb4612b7beca98d07aeff661fd,cross attention Transformer encoder USED-FOR text sequence. MLP USED-FOR classification head. hidden states USED-FOR latent labels. Task is multi - label text classification problem. Generic is baselines. ,This paper tackles the multi-label text classification problem. The authors propose a cross attention Transformer encoder that encodes the text sequence into a sequence of hidden states. The classification head is trained using an MLP. The latent labels are learned by sampling from the hidden states of the encoder. Experiments are conducted on several datasets and compared with several baselines.
1172,SP:f0ad7cbc190113bb4612b7beca98d07aeff661fd,label correlations USED-FOR multi - label text classification. studies USED-FOR label correlations. tree - based models HYPONYM-OF studies. AAPD CONJUNCTION RCV1. RCV1 CONJUNCTION AAPD. AAPD HYPONYM-OF benchmark datasets. RCV1 HYPONYM-OF benchmark datasets. method COMPARE baselines. baselines COMPARE method. baselines USED-FOR multi - label text classification. multi - label text classification EVALUATE-FOR method. latent labels COMPARE actual labels. actual labels COMPARE latent labels. actual labels USED-FOR framework. latent labels USED-FOR framework. Method is BERT classifier. Task is classification. ,"This paper studies the label correlations for multi-label text classification using two studies: (1) tree-based models, and (2) a BERT classifier. The authors evaluate their method on two benchmark datasets: AAPD and RCV1, and show that their method outperforms the baselines in terms of performance on multi-label text classification. They also show that the latent labels of their framework are more informative than the actual labels for classification."
1173,SP:f0ad7cbc190113bb4612b7beca98d07aeff661fd,works USED-FOR label correlations. label embedding methods HYPONYM-OF works. latent labels USED-FOR label correlations. method COMPARE baselines. baselines COMPARE method. multi - label text classification benchmarks EVALUATE-FOR baselines. multi - label text classification benchmarks EVALUATE-FOR method. Task is multi - label text classification. ,This paper studies the problem of multi-label text classification and proposes two works: (1) label embedding methods and (2) label correlations based on latent labels. The proposed method is evaluated on three multi-label text classification benchmarks and compared to two baselines.
1174,SP:f0ad7cbc190113bb4612b7beca98d07aeff661fd,latent label representations USED-FOR label correlations. latent label representations USED-FOR method. randomly generated latent labels USED-FOR text tokens. randomly generated latent labels USED-FOR method. method USED-FOR BERT model. this USED-FOR method. contextual encodings USED-FOR latent labels. model COMPARE LACO algorithm. LACO algorithm COMPARE model. Hamming Loss CONJUNCTION Micro - F1. Micro - F1 CONJUNCTION Hamming Loss. Hamming Loss USED-FOR LACO. Micro - F1 USED-FOR LACO. low - frequency labels CONJUNCTION intensive - label samples. intensive - label samples CONJUNCTION low - frequency labels. method COMPARE baseline LACO algorithm. baseline LACO algorithm COMPARE method. intensive - label samples EVALUATE-FOR baseline LACO algorithm. intensive - label samples EVALUATE-FOR method. low - frequency labels EVALUATE-FOR baseline LACO algorithm. low - frequency labels EVALUATE-FOR method. Enhancing Label Correlation Feedback USED-FOR Multi - Label Text Classification. Multi - Task Learning USED-FOR Label Correlation Feedback. OtherScientificTerm is SOTA. Material is AAPD and RCV1 - V2 datasets. ,"This paper proposes a method to improve the performance of the BERT model by using latent label representations to model label correlations. The method uses randomly generated latent labels for text tokens and uses contextual encodings to represent the latent labels. The proposed method is evaluated on SOTA and on the AAPD and RCV1-V2 datasets and compared to the original model as well as the LACO algorithm with Hamming Loss and Micro-F1. The results show that the proposed method outperforms the original baseline in terms of low-frequency labels and intensive-label samples. The authors also propose a method called ""Enhancing Label Correlation Feedback for Multi-Label Text Classification with Multi-Task Learning""."
1175,SP:6e54083a06942f2c41e1796a9f911d3dd9bab0cc,two or three layers convolutional kernels COMPARE convolutional kernels. convolutional kernels COMPARE two or three layers convolutional kernels. polynomial kernels CONJUNCTION Gaussian pooling. Gaussian pooling CONJUNCTION polynomial kernels. two or three layers convolutional kernels CONJUNCTION Gaussian pooling. Gaussian pooling CONJUNCTION two or three layers convolutional kernels. polynomial kernels USED-FOR two or three layers convolutional kernels. Myrtle kernel HYPONYM-OF convolutional kernels. extra layers CONJUNCTION pooling. pooling CONJUNCTION extra layers. RKHS FEATURE-OF kernels. RKHS norm CONJUNCTION bound. bound CONJUNCTION RKHS norm. bound USED-FOR generalization error. statistical efficiency EVALUATE-FOR architecture. OtherScientificTerm is spatial dependency. ,"This paper studies the generalization performance of two or three layers convolutional kernels with polynomial kernels and Gaussian pooling, compared to the standard two-layer and three-layer convolutionals (e.g. Myrtle kernel). The authors show that the RKHS of these kernels can be improved by adding extra layers and pooling. The authors also provide a theoretical analysis of the spatial dependency between two kernels and provide an upper bound on the generalized error. Finally, the authors provide an empirical study of the statistical efficiency of the proposed architecture."
1176,SP:6e54083a06942f2c41e1796a9f911d3dd9bab0cc,"RKHS and generalization properties FEATURE-OF convolutional kernel networks. NNGP CONJUNCTION kernels. kernels CONJUNCTION NNGP. convolutional kernel networks CONJUNCTION NNGP. NNGP CONJUNCTION convolutional kernel networks. NNGP USED-FOR CNNs. image classification EVALUATE-FOR kernels. pooling USED-FOR regularization. CIFAR10 EVALUATE-FOR kernel method. convolutional kernels CONJUNCTION pooling filters. pooling filters CONJUNCTION convolutional kernels. Method are iterated convolutions, and shallower architecture. ","This paper studies the RKHS and generalization properties of convolutional kernel networks, NNGP, and kernels for CNNs. The authors show that the kernels perform well on image classification, and that the pooling can be used as a form of regularization. In addition, they show that iterated convolutions can be improved by using a shallower architecture. Finally, the authors evaluate the kernel method on CIFAR10 and compare the performance of the convional kernels and pooling filters."
1177,SP:6e54083a06942f2c41e1796a9f911d3dd9bab0cc,convolutional kernel COMPARE deep convolutional kernels. deep convolutional kernels COMPARE convolutional kernel. one layer convolutional kernel CONJUNCTION two layer convolutional kernel. two layer convolutional kernel CONJUNCTION one layer convolutional kernel. low degree polynomial activation function USED-FOR two layer convolutional kernel. generalization upper bound FEATURE-OF kernel ridge regression. sample complexity EVALUATE-FOR generalization upper bound. sample complexity EVALUATE-FOR architecture. Method is convolutional kernels. OtherScientificTerm is RKHS functions. ,"This paper studies the generalization of convolutional kernels. The authors show that the standard convolution kernel has a lower sample complexity than deep convional kernels, and that the RKHS functions can be approximated by a combination of a one layer convolutionally kernel with a low degree polynomial activation function, as well as a two layer convualal kernel with the same low degree but with a different activation function. They also provide a generalization upper bound for kernel ridge regression with respect to the sample complexity of the proposed architecture."
1178,SP:6e54083a06942f2c41e1796a9f911d3dd9bab0cc,"Method are deep convolutional models, and convolution and pooling layers. OtherScientificTerm are RKHS, generalization bounds, and convolution operations. Generic is kernels. ","This paper studies the generalization properties of deep convolutional models. The authors show that the RKHS of the convolution and pooling layers converge to a fixed point, and derive generalization bounds for the kernels of the two convolution operations."
1179,SP:7bee8d65c68765cbfe38767743fec27981879d34,"runtime CONJUNCTION memory requirements. memory requirements CONJUNCTION runtime. runtime EVALUATE-FOR finite - width NTK. memory requirements EVALUATE-FOR finite - width NTK. Jacobian - vector products USED-FOR fully - connected and convolutional neural networks. computing costs FEATURE-OF Jacobian - vector products. Metric are NTK computation cost, and memory saving. Method are structure of neural networks, and JAX library. ","This paper studies the NTK computation cost and proposes to reduce the runtime and memory requirements of finite-width NTK. The main motivation of this paper is to address the issue of memory saving. The authors propose to use Jacobian-vector products to reduce computing costs of fully-connected and convolutional neural networks, which can reduce NTK computations while maintaining the performance. The proposed structure of neural networks is similar to that of the JAX library. "
1180,SP:7bee8d65c68765cbfe38767743fec27981879d34,"structured derivative CONJUNCTION NTK - vector product. NTK - vector product CONJUNCTION structured derivative. approaches USED-FOR NN primitives. batch size CONJUNCTION output dim. output dim CONJUNCTION batch size. compute and memory requirements EVALUATE-FOR Neural Tangent Kernel ( NTK ). NTK - vector product HYPONYM-OF NN primitives. structured derivative HYPONYM-OF NN primitives. architectures CONJUNCTION hardware. hardware CONJUNCTION architectures. architectures EVALUATE-FOR approaches. hardware EVALUATE-FOR approaches. Method are naive jacobian contraction method, and JAX and Neural Tangents frameworks. ",This paper proposes a naive jacobian contraction method to reduce the compute and memory requirements of the Neural Tangent Kernel (NTK) in order to address the limitations of NN primitives such as structured derivative and NTK-vector product in terms of batch size and output dim. The proposed approaches are evaluated on a variety of architectures and hardware and compared with the JAX and Neural Tangents frameworks.
1181,SP:7bee8d65c68765cbfe38767743fec27981879d34,computation and memory requirements FEATURE-OF finite - width NTK. Task is computation problem. Method is deep learning. Generic is algorithms. ,This paper studies the computation and memory requirements of finite-width NTK. The authors consider the computation problem in the context of deep learning and propose two algorithms.
1182,SP:7bee8d65c68765cbfe38767743fec27981879d34,OtherScientificTerm is NTK. ,"This paper proposes to use NTK as a proxy for the quality of the training data. The paper is well-written and easy to follow. However, there are a few issues with the paper. "
1183,SP:1df605fc5fc828304f7b836724d8fd6c233ff80c,DICE - family method USED-FOR constrained offline reinforcement learning problems. nested constrained optimization problem CONJUNCTION unconstrained optimization problem. unconstrained optimization problem CONJUNCTION nested constrained optimization problem. neural network USED-FOR unconstrained optimization problem. CoinDICE USED-FOR confidence interval. random grid worlds CONJUNCTION continuous environments. continuous environments CONJUNCTION random grid worlds. COptiDICE COMPARE methods. methods COMPARE COptiDICE. method COMPARE baselines. baselines COMPARE method. constraint satisfaction EVALUATE-FOR methods. baselines COMPARE methods. methods COMPARE baselines. continuous environments EVALUATE-FOR baselines. continuous environments EVALUATE-FOR method. random grid worlds EVALUATE-FOR baselines. random grid worlds EVALUATE-FOR method. constraint satisfaction EVALUATE-FOR COptiDICE. Method is OptiDICE. ,"This paper proposes a DICE-family method for solving constrained offline reinforcement learning problems. Specifically, the authors consider a nested constrained optimization problem and an unconstrained optimization problem with a neural network. CoinDICE is used to estimate the confidence interval, and OptiDICE aims to minimize the variance of the learned confidence interval. The authors evaluate the proposed method on random grid worlds and continuous environments, and show that COptIDICE achieves better constraint satisfaction than the baselines."
1184,SP:1df605fc5fc828304f7b836724d8fd6c233ff80c,CMDP USED-FOR problem. COptiDICE USED-FOR stationary distribution corrections. stationary distribution corrections FEATURE-OF optimal policy. constraint satisfaction CONJUNCTION return - maximization. return - maximization CONJUNCTION constraint satisfaction. COptiDICE COMPARE baseline algorithms. baseline algorithms COMPARE COptiDICE. constraint satisfaction EVALUATE-FOR COptiDICE. constraint satisfaction EVALUATE-FOR baseline algorithms. return - maximization EVALUATE-FOR baseline algorithms. return - maximization EVALUATE-FOR COptiDICE. Task is offline constrained reinforcement learning problem. ,"This paper studies an offline constrained reinforcement learning problem. The problem is formulated as a CMDP, and the authors propose COptiDICE to learn stationary distribution corrections for the optimal policy. COptIDICE achieves better constraint satisfaction and better return-maximization than baseline algorithms."
1185,SP:1df605fc5fc828304f7b836724d8fd6c233ff80c,offline constrained MDP setting FEATURE-OF policy optimization problem. primal - dual formulation of Bellman operator USED-FOR policy visitation distribution. policy visitation distribution USED-FOR problem. approach USED-FOR upper bound of constraint violation. CoinDICE USED-FOR upper bound of constraint violation. CoinDICE USED-FOR approach. Method is policy gradient based approaches. Generic is algorithm. ,This paper studies the policy optimization problem in an offline constrained MDP setting. The problem is formulated as a policy visitation distribution over a primal-dual formulation of Bellman operator. The authors propose an approach based on CoinDICE to derive an upper bound of constraint violation. The proposed algorithm is a generalization of existing policy gradient based approaches.
1186,SP:1df605fc5fc828304f7b836724d8fd6c233ff80c,"DICE - based offline constrained RL algorithm USED-FOR constrained RL. tabular CMDPs CONJUNCTION continuous control tasks. continuous control tasks CONJUNCTION tabular CMDPs. reward maximization CONJUNCTION constraint satisfaction. constraint satisfaction CONJUNCTION reward maximization. continuous control tasks EVALUATE-FOR method. tabular CMDPs EVALUATE-FOR method. single minimization problem USED-FOR constrained offline RL. RL problem USED-FOR cost upper bound estimation. RL problem USED-FOR distribution correction. OtherScientificTerm are constraint violation, and upper bound. ","This paper proposes a DICE-based offline constrained RL algorithm for constrained RL. The proposed method is evaluated on tabular CMDPs and continuous control tasks, and is shown to achieve state-of-the-art performance in terms of reward maximization and constraint satisfaction. The main contribution of the paper is the formulation of constrained offline RL as a single minimization problem, where the constraint violation is assumed to be zero. The authors propose to use an RL problem for cost upper bound estimation, which is used for distribution correction, and show that the proposed upper bound is tight."
1187,SP:5a10c13eb78d26a25dac74601419deb68c53cb75,multigrid reduction in time ( MGRIT ) solver USED-FOR forward and back propagation of information. distributed / shared memory hybrid computing environments USED-FOR technique. Method is Gated Recurrent Unit ( GRU ). Task is classification problems. ,"This paper proposes a multigrid reduction in time (MGRIT) solver for forward and back propagation of information. The technique is based on distributed/shared memory hybrid computing environments, and is inspired by Gated Recurrent Unit (GRU). The authors demonstrate its effectiveness on classification problems."
1188,SP:5a10c13eb78d26a25dac74601419deb68c53cb75,GRU networks HYPONYM-OF recurrent neural networks. inference USED-FOR GRU networks. ` time ` dimension FEATURE-OF GRU networks. GRU architecture ( Implicit GRU ) USED-FOR stiffness. stiffness PART-OF architecture. Method is multigrid reduction in time ( MGRIT ) solver. Metric is training time. ,"This paper proposes a multigrid reduction in time (MGRIT) solver to reduce the training time of GRU networks, which are recurrent neural networks with a `time` dimension. The main contribution of this paper is to propose a GRU architecture (Implicit GRU) that incorporates stiffness into the architecture. This is an interesting idea, and the inference performance of the proposed GRU network is promising."
1189,SP:5a10c13eb78d26a25dac74601419deb68c53cb75,technique USED-FOR GRU networks. multigrid reduction in time ( MGRIT ) technique USED-FOR technique. techniques USED-FOR neural network training. mathematical framework USED-FOR MGRIT method. ordinary differential equation ( ODE ) representations USED-FOR GRU. accuracy EVALUATE-FOR sequential GRU. Method is GRU layers. Generic is method. Metric is scalability. ,"This paper proposes a technique for training GRU networks using the multigrid reduction in time (MGRIT) technique. The MGRIT method is based on a mathematical framework that allows the GRU to be trained with ordinary differential equation (ODE) representations. The authors show that the sequential GRU can achieve comparable accuracy with fewer GRU layers, and that the proposed method can also achieve better scalability. These techniques are useful for neural network training."
1190,SP:5a10c13eb78d26a25dac74601419deb68c53cb75,approaches USED-FOR Gated Recurrent Unit ( GRU ). training time CONJUNCTION model accuracy. model accuracy CONJUNCTION training time. model accuracy EVALUATE-FOR approaches. training time EVALUATE-FOR approaches. parallel training scheme USED-FOR GRU. parallel - in - time HYPONYM-OF parallel training scheme. multigrid reduction in time ( MGRIT ) solver USED-FOR parallel training scheme. hierarchical correction of the hidden state USED-FOR end - to - end communication. end - to - end communication USED-FOR forward and backward propagation. hierarchical correction of the hidden state USED-FOR speedup. OtherScientificTerm is long sequence scenario. ,"This paper proposes two approaches to reduce the training time and model accuracy of a Gated Recurrent Unit (GRU) in a long sequence scenario. The first parallel training scheme, parallel-in-time, is based on the multigrid reduction in time (MGRIT) solver. The second speedup is achieved by hierarchical correction of the hidden state for end-to-end communication for forward and backward propagation."
1191,SP:fb935a5c44d7df6958d39ab1ef877956df08994e,"model USED-FOR functional alignment of fMRI datasets. It USED-FOR model. one - in - many - out autoencoder CONJUNCTION regularization loss terms. regularization loss terms CONJUNCTION one - in - many - out autoencoder. one - in - many - out autoencoder PART-OF It. regularization loss terms PART-OF It. Material is fMRI datasets. OtherScientificTerm are GRAE, and shared latent space. Method is decoder. ","This paper proposes a model for functional alignment of fMRI datasets. It consists of a one-in-many-out autoencoder, as well as regularization loss terms. The main idea is to use GRAE, where the decoder is trained on a shared latent space."
1192,SP:fb935a5c44d7df6958d39ab1ef877956df08994e,"noisy fMRI data USED-FOR tasks. neural architecture USED-FOR tasks. neural architecture USED-FOR noisy fMRI data. MRMD - AE HYPONYM-OF neural architecture. Subject - specific decoders CONJUNCTION encoder. encoder CONJUNCTION Subject - specific decoders. PHATE HYPONYM-OF regularisation term. architecture COMPARE techniques. techniques COMPARE architecture. classification tasks EVALUATE-FOR techniques. metrics EVALUATE-FOR techniques. metrics EVALUATE-FOR classification tasks. classification tasks EVALUATE-FOR architecture. metrics EVALUATE-FOR architecture. OtherScientificTerm are low - dimensional features, and latent space. ","This paper proposes a neural architecture, MRMD-AE, for tasks with noisy fMRI data. Subject-specific decoders and an encoder are used, and a regularisation term, PHATE, is introduced to encourage low-dimensional features to be present in the latent space. The proposed architecture is evaluated on several classification tasks and compared to other techniques on different metrics."
1193,SP:fb935a5c44d7df6958d39ab1ef877956df08994e,"neural network - based modeling strategy USED-FOR common latent space. technique USED-FOR common representational patterns. common representational patterns CONJUNCTION subject - specific variations. subject - specific variations CONJUNCTION common representational patterns. subject - specific decoders USED-FOR technique. geometric regularization CONJUNCTION cross - subject embedding alignment. cross - subject embedding alignment CONJUNCTION geometric regularization. priors FEATURE-OF latent embedding. geometric regularization HYPONYM-OF priors. cross - subject embedding alignment HYPONYM-OF priors. manifold learning techniques COMPARE deep neural network modeling framework. deep neural network modeling framework COMPARE manifold learning techniques. shared space USED-FOR stimulus decoding. stimulus decoding EVALUATE-FOR framework. cross - subject translation accuracy EVALUATE-FOR framework. large fMRI datasets EVALUATE-FOR framework. Material is multi - subject fMRI data. OtherScientificTerm are PHATE embeddings, and geometric regularization loss. ",This paper proposes a neural network-based modeling strategy to learn a common latent space for multi-subject fMRI data. The technique is based on subject-specific decoders and aims to learn common representational patterns as well as subject -specific variations. The authors propose two priors on the latent embedding: geometric regularization and cross-subject embedding alignment. They show that the PHATE embeddings can be learned with geometric regularized loss. They compare the performance of manifold learning techniques with the proposed deep neural network modeling framework on large fMRI datasets and demonstrate that the proposed framework can improve the performance on stimulus decoding in a shared space. They also show that their framework improves the cross-source translation accuracy.
1194,SP:fb935a5c44d7df6958d39ab1ef877956df08994e,"model USED-FOR low - dimensional representation of fMRI data. encoder CONJUNCTION decoder. decoder CONJUNCTION encoder. auto - encoder USED-FOR model. decoder PART-OF auto - encoder. encoder PART-OF auto - encoder. Method are pre - computed manifold embedding, and learn representation. Generic is shared representation. OtherScientificTerm are manifold, and stimulus features. ",This paper presents a model for learning a low-dimensional representation of fMRI data. The model is based on an auto-encoder that consists of an encoder and a decoder. The encoder takes a pre-computed manifold embedding and outputs a shared representation of the manifold. The decoder takes the learned representation as input and outputs stimulus features.
1195,SP:95ed80753116005f1f7bae24c855d350f4af85a1,maximum unnormalized logit ( MaxLogit ) USED-FOR anomaly score. metric COMPARE maximum softmax probability ( MSP ). maximum softmax probability ( MSP ) COMPARE metric. maximum softmax probability ( MSP ) USED-FOR setup. PASCAL VOC CONJUNCTION MS - COCO. MS - COCO CONJUNCTION PASCAL VOC. MaxLogit COMPARE MSP. MSP COMPARE MaxLogit. CAOS benchmark EVALUATE-FOR metric. Method is large - scale setup. Material is ImageNet-22 K out - distribution. ,"This paper proposes a new anomaly score based on maximum unnormalized logit (MaxLogit). The proposed metric is similar to maximum softmax probability (MSP) in the original setup, but is applied to a large-scale setup. Experiments are conducted on ImageNet-22K out-distribution, PASCAL VOC, and MS-COCO, and show that MaxLogit outperforms MSP on the CAOS benchmark."
1196,SP:95ed80753116005f1f7bae24c855d350f4af85a1,large - scale benchmarks EVALUATE-FOR ODD detectors. classification CONJUNCTION segmentation. segmentation CONJUNCTION classification. classification EVALUATE-FOR ODD detectors. segmentation EVALUATE-FOR ODD detectors. baseline USED-FOR problem. ,This paper presents large-scale benchmarks to evaluate the performance of ODD detectors in terms of classification and segmentation. The paper also provides a baseline for the problem.
1197,SP:95ed80753116005f1f7bae24c855d350f4af85a1,Species HYPONYM-OF OOD test dataset. dataset COMPARE ImageNet-22k. ImageNet-22k COMPARE dataset. max - logit criterion USED-FOR outliers. max - logit USED-FOR OOD detection. multi - label environments FEATURE-OF OOD detection. datasets USED-FOR dense outlier detection. Task is outlier detection. Material is StreetHazards. Generic is it. ,"This paper proposes a new OOD test dataset, called ""Species"", which is a generalization of the original ImageNet-22k. The dataset is similar to the original dataset, but is much larger than the original one (ImageNet-20k). The authors propose to use the max-logit criterion to detect outliers in OOD detection in multi-label environments. The authors also propose a new dataset called StreetHazards, which is based on the same idea of outlier detection, and show that it is able to detect more outliers. They also show that their datasets can be used for dense outlier Detection."
1198,SP:95ed80753116005f1f7bae24c855d350f4af85a1,multi - label OOD detection CONJUNCTION anomaly segmentation. anomaly segmentation CONJUNCTION multi - label OOD detection. multi - class OOD detection CONJUNCTION multi - label OOD detection. multi - label OOD detection CONJUNCTION multi - class OOD detection. anomaly segmentation HYPONYM-OF large - scale settings. multi - class OOD detection HYPONYM-OF large - scale settings. multi - label OOD detection HYPONYM-OF large - scale settings. species dataset CONJUNCTION road anomaly dataset. road anomaly dataset CONJUNCTION species dataset. multi - class OOD detection CONJUNCTION anomaly segmentation. anomaly segmentation CONJUNCTION multi - class OOD detection. road anomaly dataset USED-FOR multi - class OOD detection. road anomaly dataset USED-FOR anomaly segmentation. species dataset USED-FOR multi - class OOD detection. setup USED-FOR multi - label OOD detection. detector USED-FOR large - scale settings. maximum logit USED-FOR detector. detector USED-FOR baseline. ,"This paper proposes a new setup for multi-label OOD detection and anomaly segmentation in large-scale settings such as multi-class OOD Detection, multi-labels OOD and multi-objective detection (OOD). The authors conduct experiments on the species dataset, the road anomaly dataset and the multi-classified OOD dataset. The authors show that the proposed detector can be used in large -scale settings with the maximum logit. They also show that their proposed detector is able to outperform a baseline."
1199,SP:abbab40e40ef09c8dccd16661af3c2a4461ebb1a,"dimensional representation of tournament CONJUNCTION structural characterization. structural characterization CONJUNCTION dimensional representation of tournament. flip classes FEATURE-OF forbidden configurations. forbidden configurations FEATURE-OF rank d tournament. minimum possible dimension FEATURE-OF representation of a tournament. OtherScientificTerm are tournaments, and lower and upper bounds. ",This paper studies the problem of learning a dimensional representation of tournament and its structural characterization. The authors consider a rank d tournament with forbidden configurations in different flip classes. They show that the minimum possible dimension of a representation of a tournament is polynomial in the number of tournaments. They also provide lower and upper bounds.
1200,SP:abbab40e40ef09c8dccd16661af3c2a4461ebb1a,"forbiden class USED-FOR rank $ d$ tournaments. this USED-FOR sign matrices. minimum dimension FEATURE-OF upper bound. Method is tournament representations. OtherScientificTerm are sign matrix, $ R$-cones, tournaments, rank 2 tournaments, and minimum feedback arc sets. Generic are representations, and bound. ","This paper studies the problem of learning tournament representations for rank $d$ tournaments in the forbiden class. The authors consider the case where the sign matrix $R$-cones are given by $R_1, R_2$ and $R_{i,j}$, where $r$ is a sign matrix and $j$ is the number of tournaments. In particular, the authors consider rank 2 tournaments, where $n$ tournaments are considered. They show an upper bound on the minimum dimension of the upper bound of this bound for sign matrices, and show that this bound is tight. They also show that the representations of rank 1 tournaments can be approximated by minimum feedback arc sets."
1201,SP:abbab40e40ef09c8dccd16661af3c2a4461ebb1a,"edges PART-OF complete graph. d - dimensional vectores USED-FOR tournament. equivalence classes FEATURE-OF forbidden structures. OtherScientificTerm are edge, tournaments, tournament T, and minimum dimension d. ","This paper studies the problem of finding edges in a complete graph that are forbidden to be added to a tournament. In particular, the authors consider the case where each edge is a d-dimensional vectores of a tournament, and the tournament T is a function of d-dimensions of d. The authors show that the forbidden structures can be represented as equivalence classes, and show that for tournaments T, the minimum dimension d can be expressed as the number of vertices."
1202,SP:abbab40e40ef09c8dccd16661af3c2a4461ebb1a,minimum dimension USED-FOR tournament. Method is tournament representations. OtherScientificTerm is tournaments. ,This paper studies the problem of learning tournament representations. The authors show that the minimum dimension of a tournament can be expressed as a function of the number of players in the tournament. They also show that tournaments can be represented in terms of a minimum dimension.
1203,SP:d39765dcc8950d4fc1d43e4c167208736578882e,context dataset USED-FOR Neural processes ( NPs ). stochastic attention USED-FOR context information. stochastic attention USED-FOR NPs. predator - prey model CONJUNCTION image completion. image completion CONJUNCTION predator - prey model. 1D regression CONJUNCTION predator - prey model. predator - prey model CONJUNCTION 1D regression. image completion CONJUNCTION MovieLens-10k dataset. MovieLens-10k dataset CONJUNCTION image completion. predator - prey model EVALUATE-FOR approach. 1D regression EVALUATE-FOR approach. image completion EVALUATE-FOR approach. MovieLens-10k dataset EVALUATE-FOR approach. ,"This paper presents a context dataset for Neural processes (NNs). NPs are trained with stochastic attention to extract context information. The proposed approach is evaluated on 1D regression, predator-prey model, image completion, and MovieLens-10k dataset."
1204,SP:d39765dcc8950d4fc1d43e4c167208736578882e,"method USED-FOR Attentive Neural Processes paradigm. cross - attention module USED-FOR context and target representations. stochasticity USED-FOR cross - attention module. Weibull distribution FEATURE-OF proposal distribution. proposal distribution USED-FOR weights. regularization term USED-FOR latent representation. KL regularization term USED-FOR proposal distribution. KL regularization term PART-OF total loss. KL regularization term COMPARE regularization term. regularization term COMPARE KL regularization term. proposal distribution COMPARE gamma distribution. gamma distribution COMPARE proposal distribution. closed - form KL divergence FEATURE-OF Weibull and Gamma distributions. reparametrization trick USED-FOR NP paper. mutual information EVALUATE-FOR representation. information theory USED-FOR loss. OtherScientificTerm are network parameters, target distribution, and latent variable. Generic is regularization. Method is NP formulation. ",This paper proposes a method for the Attentive Neural Processes paradigm. The key idea is to use stochasticity in the cross-attention module to model both context and target representations. The weights are modeled as a proposal distribution over the Weibull distribution of the network parameters. The KL regularization term is added to the total loss to encourage the latent representation to be similar to the target distribution. The authors show that the KL regularized version of the proposal distribution is equivalent to the gamma distribution with a closed-form KL divergence between the target and the proposal distributions. This regularization is motivated by the fact that the mutual information between the representation and target distribution can be estimated by the latent variable. This loss is derived from information theory and the authors use the reparametrization trick from the NP paper to make the NP formulation tractable.
1205,SP:d39765dcc8950d4fc1d43e4c167208736578882e,deterministic attention CONJUNCTION Bayesian attention module. Bayesian attention module CONJUNCTION deterministic attention. VAEs CONJUNCTION autoencoders. autoencoders CONJUNCTION VAEs. ANP USED-FOR method. variational approximation USED-FOR inference. Task is attentive neural processes ( ANP. ,This paper studies attentive neural processes (ANP) and proposes a method based on ANP that combines deterministic attention with a Bayesian attention module. Experiments are conducted on VAEs and autoencoders. The inference is performed using a variational approximation.
1206,SP:d39765dcc8950d4fc1d43e4c167208736578882e,"neural process USED-FOR context dataset. stochastic attention USED-FOR neural process. attention USED-FOR ANP. attention CONJUNCTION Bayesian attention module. Bayesian attention module CONJUNCTION attention. attention USED-FOR method. it USED-FOR context dataset. regularization of the latent space USED-FOR NP. stochastic attention USED-FOR NP. synthetic and real - world datasets EVALUATE-FOR method. Material is noisy scenarios. OtherScientificTerm are kernel, and noisy or more complicated scenarios. ",This paper proposes a method that combines attention in ANP with a Bayesian attention module to train a neural process to generate a context dataset. The main contribution of this paper is the regularization of the latent space in NP with stochastic attention. This is useful in noisy scenarios where the kernel may not be robust to noisy or more complicated scenarios. The method is evaluated on both synthetic and real-world datasets.
1207,SP:f6e7229b653a5a56a2993864cdb70809f5b6f9b4,label prediction CONJUNCTION prototype clustering. prototype clustering CONJUNCTION label prediction. framework USED-FOR prototype clustering. framework USED-FOR label prediction. similarity USED-FOR prototype. framework USED-FOR similarity. framework USED-FOR prototype. human - in - the - loop feedback FEATURE-OF prototype. framework USED-FOR human - in - the - loop feedback. Generic is model. OtherScientificTerm is post - hoc explanation. ,This paper proposes a framework for label prediction and prototype clustering. The key idea is to use the similarity between a label and a prototype as a proxy for the similarity of a prototype to human-in-the-loop feedback on the prototype. The model is trained to predict the label of a given prototype based on a post-hoc explanation.
1208,SP:f6e7229b653a5a56a2993864cdb70809f5b6f9b4,"framework USED-FOR transformer models. classification CONJUNCTION explanation generation. explanation generation CONJUNCTION classification. shared prototype embeddings USED-FOR explanation generation. shared prototype embeddings USED-FOR classification. supervision USED-FOR prototype learning. sentiment classification tasks EVALUATE-FOR ProtoTrex. Task is task prediction. Metric is interpretability. OtherScientificTerm are losses, and human in the loop. ","This paper proposes a framework for transformer models that aims to improve the interpretability of task prediction. The main idea is to use shared prototype embeddings for both classification and explanation generation. The paper also proposes to use supervision for prototype learning, which is an important step towards improving interpretability. The proposed ProtoTrex achieves state-of-the-art performance on sentiment classification tasks. The authors also propose two additional losses to improve interpretability: 1) adding a human in the loop, and 2) reducing the number of parameters."
1209,SP:f6e7229b653a5a56a2993864cdb70809f5b6f9b4,Proto - Trex model USED-FOR text classification systems. prototype layers PART-OF model. Proto - Trex COMPARE plain classification model. plain classification model COMPARE Proto - Trex. classification accuracy EVALUATE-FOR plain classification model. classification accuracy EVALUATE-FOR Proto - Trex. OtherScientificTerm is users'feedbacks. ,This paper proposes Proto-Trex model for text classification systems. The model consists of prototype layers that are trained on the input text. Proto-Trex outperforms the plain classification model in terms of classification accuracy. The authors also provide users' feedbacks to verify the effectiveness of the proposed model.
1210,SP:f6e7229b653a5a56a2993864cdb70809f5b6f9b4,"method USED-FOR transformer based text classifiers. case based reasoning USED-FOR approach. similarity scores USED-FOR labeling decision. approach COMPARE post - hoc interpretability methods. post - hoc interpretability methods COMPARE approach. feedback USED-FOR prototypes. approach COMPARE end - to - end finetuning. end - to - end finetuning COMPARE approach. Generic are network, system, and it. OtherScientificTerm is weighted similarity. Method is end - to - end classifier. Metric is comprehensiveness scores. ","This paper proposes a method for training transformer based text classifiers. The approach is based on case based reasoning, where the network is trained on a set of prototypes generated from feedback. The prototypes are then used to train an end-to-end classifier, which uses the similarity scores to make a labeling decision. The weighted similarity between the prototypes and the original input is used to guide the training of the system. The paper compares the proposed approach with post-hoc interpretability methods, and shows that the approach is able to achieve better performance than end-towards-end finetuning, and that it is more interpretable. The comprehensiveness scores are also provided."
1211,SP:60ba9cb4c42cecde6379ec0279434dece822a2b1,"continual learning settings FEATURE-OF forward knowledge transfer. them USED-FOR task. benchmarks EVALUATE-FOR method. OtherScientificTerm are trust region, frozen weights, task similarities, catastrophic forgetting, and transfer knowledge. ",This paper studies the problem of forward knowledge transfer in continual learning settings. The key idea is to learn a trust region over the frozen weights and use them to transfer knowledge to a new task. The authors show that this can help avoid catastrophic forgetting. The proposed method is evaluated on several benchmarks.
1212,SP:60ba9cb4c42cecde6379ec0279434dece822a2b1,"gradient projection ( GP ) USED-FOR incremental learning. approaches USED-FOR catastrophic forgetting. GP USED-FOR approaches. layerwise inputs USED-FOR subspaces. backward transfer FEATURE-OF network. subspaces USED-FOR tasks. tasks CONJUNCTION task. task CONJUNCTION tasks. MNIST CONJUNCTION SVHN. SVHN CONJUNCTION MNIST. SVHN CONJUNCTION not - MNIST. not - MNIST CONJUNCTION SVHN. CIFAR100 Split CONJUNCTION CIFAR100 Sup. CIFAR100 Sup CONJUNCTION CIFAR100 Split. CIFAR-10 CONJUNCTION MNIST. MNIST CONJUNCTION CIFAR-10. not - MNIST CONJUNCTION Fashion MNIST. Fashion MNIST CONJUNCTION not - MNIST. Permuted MNIST ( PMNIST ) CONJUNCTION CIFAR100 Split. CIFAR100 Split CONJUNCTION Permuted MNIST ( PMNIST ). regularization - based methods CONJUNCTION memory replay method. memory replay method CONJUNCTION regularization - based methods. GP methods CONJUNCTION regularization - based methods. regularization - based methods CONJUNCTION GP methods. GP methods COMPARE memory replay method. memory replay method COMPARE GP methods. accuracy CONJUNCTION backward transfer. backward transfer CONJUNCTION accuracy. Generic is they. Method are GP algorithms, scaled weight projection, and optimization process. OtherScientificTerm are intransigence, and ( layerwise ) Trust Regions. ","This paper proposes to use gradient projection (GP) for incremental learning to address the issue of catastrophic forgetting caused by intransigence. Previous approaches to incremental learning with GP have been shown to suffer catastrophic forgetting due to the fact that they do not scale with the number of layerwise inputs. The authors propose to use GP algorithms to address this issue by using a scaled weight projection, where the subspaces of the network are learned with layerwise input and output, and then the network is trained to minimize the backward transfer between tasks and the original task. This optimization process is called (layerwise) Trust Regions. Experiments are conducted on CIFAR-10, MNIST, SVHN, not-MNIST, Fashion MNIST and Permuted MNIST (PMNIST), CifAR100 Split, CIFR100 Sup, and a memory replay method. Results show that GP methods outperform regularization-based methods, memory replay methods, as well as other GP methods, in terms of accuracy and backward transfer."
1213,SP:60ba9cb4c42cecde6379ec0279434dece822a2b1,restrictive constrains FEATURE-OF optimization space. methods USED-FOR catastrophic forgetting. restrictive constrains USED-FOR methods. norm of gradient projection USED-FOR task correlation. norm of gradient projection USED-FOR trust region. approach USED-FOR task. scaled weight projection USED-FOR approach. scaled weight projection CONJUNCTION module. module CONJUNCTION scaled weight projection. module USED-FOR task input subspace. Trust Region CONJUNCTION scaled weight projection. scaled weight projection CONJUNCTION Trust Region. scaled weight projection USED-FOR task input subspace. continual learning approach CONJUNCTION trust region gradient projection(TRGP ). trust region gradient projection(TRGP ) CONJUNCTION continual learning approach. module USED-FOR trust region gradient projection(TRGP ). Trust Region USED-FOR trust region gradient projection(TRGP ). scaled weight projection USED-FOR trust region gradient projection(TRGP ). state - of - the - art approaches COMPARE TRGP. TRGP COMPARE state - of - the - art approaches. Task is forward knowledge transfer. ,"This paper addresses the problem of forward knowledge transfer and proposes methods to mitigate catastrophic forgetting by enforcing restrictive constrains on the optimization space. The authors propose a continual learning approach, trust region gradient projection(TRGP), and a trust region based on the norm of gradient projection to model the task correlation. The proposed approach can be applied to any task by using the Trust Region, scaled weight projection, and a module to represent the task input subspace. Experiments show that the proposed state-of-the-art approaches outperform TRGP."
1214,SP:60ba9cb4c42cecde6379ec0279434dece822a2b1,"gradient projection memory ( GPM ) USED-FOR continual learning method. OtherScientificTerm are gradient, orthogonal projection, and subspace of the correlated tasks. Method is heuristic algorithm. Generic is model. ","This paper proposes a continual learning method based on gradient projection memory (GPM). The main idea is to use the gradient as an orthogonal projection to the subspace of the correlated tasks, and then use a heuristic algorithm to estimate the gradient of the model."
1215,SP:25414fe1c6203f9b623c5317a4ffaba478085c4c,"framework USED-FOR optimization and generalization properties. Uniform - LGI condition FEATURE-OF optimization and generalization properties. PL condition USED-FOR Uniform - LGI. Rademacher complexity USED-FOR generalization error. framework USED-FOR application models. Task are Optimization, Generalization, and optimization. OtherScientificTerm are sublinear rate, and optimization path length. ","This paper proposes a framework for analyzing the optimization and generalization properties under the Uniform-LGI condition. The Uniform-LGI is an extension of the PL condition, where the goal is to minimize the generalization error with Rademacher complexity. Optimization is defined as minimizing a sublinear rate, while Generalization can be viewed as minimizing the optimization path length. The framework is applied to a variety of application models."
1216,SP:25414fe1c6203f9b623c5317a4ffaba478085c4c,convergence guarantee FEATURE-OF Uniform - LGI loss function. parametric model USED-FOR generalization guarantee. p - norm regression CONJUNCTION kernel regression. kernel regression CONJUNCTION p - norm regression. kernel regression CONJUNCTION one hidden layer neural network learning problems. one hidden layer neural network learning problems CONJUNCTION kernel regression. ,"This paper studies the convergence guarantee of the Uniform-LGI loss function. The authors propose a parametric model to improve the generalization guarantee. Experiments are conducted on p-norm regression, kernel regression, and one hidden layer neural network learning problems."
1217,SP:25414fe1c6203f9b623c5317a4ffaba478085c4c,generalization bound USED-FOR gradient flow equation. underdetermined $ \ell_p$ linear regression CONJUNCTION kernel regression. kernel regression CONJUNCTION underdetermined $ \ell_p$ linear regression. kernel regression CONJUNCTION overparameterized two - layer ReLU neural networks. overparameterized two - layer ReLU neural networks CONJUNCTION kernel regression. bound USED-FOR loss function. Łojasiewicz gradient inequality FEATURE-OF loss function. overparameterized two - layer ReLU neural networks HYPONYM-OF machine learning models. underdetermined $ \ell_p$ linear regression HYPONYM-OF machine learning models. kernel regression HYPONYM-OF machine learning models. Generic is models. OtherScientificTerm is length - based generalization bound. ,"This paper provides a generalization bound for the gradient flow equation. The bound is applied to a loss function that satisfies the Łojasiewicz gradient inequality for a variety of machine learning models, including underdetermined $\ell_p$ linear regression, kernel regression, and overparameterized two-layer ReLU neural networks. The authors show that for these models, the bound is tight. The paper also provides a length-based generalization result."
1218,SP:25414fe1c6203f9b623c5317a4ffaba478085c4c,"optimization CONJUNCTION generalization. generalization CONJUNCTION optimization. generalization USED-FOR gradient flow ( GF ). optimization USED-FOR gradient flow ( GF ). loss functions USED-FOR gradient flow ( GF ). Lojasiewicz gradient inequality FEATURE-OF loss functions. upper bound USED-FOR optimization length. linear shallow networks CONJUNCTION two - layer networks. two - layer networks CONJUNCTION linear shallow networks. upper bound USED-FOR generalization gap. two - layer networks HYPONYM-OF hypothesis class. linear shallow networks HYPONYM-OF hypothesis class. kernel regression CONJUNCTION overparametrized two - layer networks. overparametrized two - layer networks CONJUNCTION kernel regression. ReLU activation USED-FOR overparametrized two - layer networks. target function USED-FOR models. overparametrized two - layer networks HYPONYM-OF models. kernel regression HYPONYM-OF models. sample size CONJUNCTION ambient dimension. ambient dimension CONJUNCTION sample size. non - asymptotic expressions USED-FOR generalization bounds. OtherScientificTerm are global version of the Lojasiewicz gradient inequality, global minimum, loss function, optimisation path, and optimization paths. Method is GF. Generic is bound. ","This paper studies the relationship between optimization and generalization in gradient flow (GF) with respect to loss functions that satisfy the Lojasiewicz gradient inequality. The authors consider a hypothesis class, linear shallow networks, and two-layer networks with ReLU activation, and derive an upper bound on the generalization gap between the optimization length and the global minimum of the loss function. This bound is based on the observation that the generalisation gap can be bounded by a function of the number of parameters of the target function, which can be used to train models such as kernel regression and overparametrized two-Layer networks. The generalization bounds are derived using non-asymptotic expressions that take into account the sample size and the ambient dimension, as well as the choice of the optimisation path. In particular, the authors consider the case where the optimization paths do not follow the same direction as in the case of GF."
1219,SP:a9e5d81f7ba88f4052730f255cf48cb40ed80942,spatial frequencies USED-FOR adversarial robustness. spatial frequencies FEATURE-OF they. adversarial training USED-FOR accuracy vs robustness tradeoff. frequencies USED-FOR adversarial training. approach USED-FOR adversarial training. Material is adversarial examples. OtherScientificTerm is high frequency components. ,"This paper studies the problem of adversarial robustness with respect to spatial frequencies. The authors argue that adversarial examples are not robust to high frequency components, because they have spatial frequencies that are different from the original examples. To address this issue, the authors propose a new approach to adversarial training that uses frequencies to balance the accuracy vs robustness tradeoff."
1220,SP:a9e5d81f7ba88f4052730f255cf48cb40ed80942,frequency - based understanding USED-FOR deep neural networks. frequency - based understanding USED-FOR frequency - based understanding of adversarial examples. frequency - based explanation USED-FOR accuracy and robustness tradeoff. frequency constrains FEATURE-OF robust models. Material is adversarial examples. Method is high - frequency or low - frequency components. ,"This paper proposes to use frequency-based understanding of deep neural networks to improve the accuracy and robustness of adversarial examples. Specifically, the authors propose to learn high-frequency or low-frequency components of the input. The authors argue that the frequency constrains of robust models can be explained by the frequency of the original input, and propose to use the frequency-by-example explanation to balance the accuracy/robustness tradeoff."
1221,SP:a9e5d81f7ba88f4052730f255cf48cb40ed80942,Fourier Analyses USED-FOR adversarial examples. low frequency FEATURE-OF adversarial examples. Fourier Analyses USED-FOR adversarial training. Generic is framework. ,"This paper proposes a framework called Fourier Analyses for adversarial training, which aims to identify adversarial examples with low frequency. The paper is well-written and well-motivated. "
1222,SP:a9e5d81f7ba88f4052730f255cf48cb40ed80942,frequency perspective USED-FOR adversraial robustness. high - frequency phenomenon FEATURE-OF adversarial perturbations. frequency properties FEATURE-OF adversarial examples. frequency properties PART-OF adversarial training. ,This paper studies adversraial robustness from a frequency perspective. The authors show that adversarial perturbations exhibit a high-frequency phenomenon. They then propose to incorporate the frequency properties of adversarial examples into adversarial training.
1223,SP:5d94dbfd10dc2ef86415853cc41f414a24962d4f,heterophily FEATURE-OF GNNs. aggregation operations USED-FOR GNNs. backpropagation analysis USED-FOR SGC - style GNN. backpropagation analysis USED-FOR metric. similarity matrix USED-FOR metric. diversification operation USED-FOR heterophily cases. high - pass filter CONJUNCTION low - pass filter. low - pass filter CONJUNCTION high - pass filter. low - pass filter CONJUNCTION identity channel. identity channel CONJUNCTION low - pass filter. high - pass filter PART-OF Adaptive Channel Mixing GNN framework. low - pass filter PART-OF Adaptive Channel Mixing GNN framework. identity channel PART-OF Adaptive Channel Mixing GNN framework. OtherScientificTerm is graph structure. ,"This paper studies the problem of heterophily in GNNs. In particular, this paper studies aggregation operations in the context of GNN. The authors propose a new metric based on the backpropagation analysis of an SGC-style GNN and derive a similarity matrix for this metric using the similarity matrix. The paper also proposes an Adaptive Channel Mixing GNN framework which consists of a high-pass filter, a low -pass filter and an identity channel. In addition, a diversification operation is also proposed for heterophilies cases, which is motivated by the graph structure."
1224,SP:5d94dbfd10dc2ef86415853cc41f414a24962d4f,"diversification operation USED-FOR harmful heterophily cases. aggregated heterophily metric USED-FOR harmful "" heterophily. diversification operation USED-FOR aggregated heterophily metric. adaptive channel mixing ( ACM ) framework USED-FOR GNN model. Metric is heterophily metrics. Generic is model. ","This paper proposes a diversification operation to reduce harmful heterophily cases. Specifically, the authors propose to use an aggregated heterogeneity metric to reduce ""harmful"" heterophilies. The authors also propose an adaptive channel mixing (ACM) framework to train the GNN model. Experiments are conducted to show the effectiveness of the proposed model. "
1225,SP:5d94dbfd10dc2ef86415853cc41f414a24962d4f,aggregation - based homophily metric USED-FOR homophily. homophily FEATURE-OF graph. aggregation - based homophily metric USED-FOR graph. metric COMPARE metrics. metrics COMPARE metric. diversification operation USED-FOR harmful heterophily information. diversification operation USED-FOR filterbank framework. OtherScientificTerm is unharmful heterophily cases. ,"This paper proposes an aggregation-based homophily metric to measure the homophilty of a graph. The proposed metric is shown to be more accurate than existing metrics. The authors also propose a filterbank framework that uses a diversification operation to remove harmful heterophily information from the input data. The experiments show that the proposed filterbank can be used to detect the most harmful and the least harmful examples. In addition, the authors also provide some theoretical analysis to explain the existence of the unharmful heterophilies cases."
1226,SP:5d94dbfd10dc2ef86415853cc41f414a24962d4f,learning effectiveness EVALUATE-FOR Graph Neural Networks. Graph Neural Networks USED-FOR node classification tasks. heterophily FEATURE-OF Graph Neural Networks. learning effectiveness EVALUATE-FOR heterophily. diversification and identity channels USED-FOR harmful heterophily. aggregation CONJUNCTION diversification and identity channels. diversification and identity channels CONJUNCTION aggregation. architecture USED-FOR harmful heterophily. diversification and identity channels USED-FOR GNN layer. Generic is task. Method is Adaptive Channel Mixing. ,This paper studies the learning effectiveness of Graph Neural Networks with heterophily on node classification tasks. The authors propose a new architecture to mitigate harmful heterophilty by combining aggregation with diversification and identity channels in the GNN layer. The proposed task is called Adaptive Channel Mixing.
1227,SP:fd4ab1cb777b541c22a923c1c86d82ac1d8384fd,equivariant model CONJUNCTION local search heuristics. local search heuristics CONJUNCTION equivariant model. deep RL approach CONJUNCTION equivariant model. equivariant model CONJUNCTION deep RL approach. deep RL approach USED-FOR traveling salesman problems ( TSP ). equivariant model USED-FOR Euclidean symmetry. local search heuristics USED-FOR traveling salesman problems ( TSP ). equivariant model USED-FOR traveling salesman problems ( TSP ). deep RL approach CONJUNCTION local search heuristics. local search heuristics CONJUNCTION deep RL approach. graph neural network ( GNN ) CONJUNCTION multi - layer perceptron ( MLP ). multi - layer perceptron ( MLP ) CONJUNCTION graph neural network ( GNN ). multi - layer perceptron ( MLP ) CONJUNCTION attention mechanism. attention mechanism CONJUNCTION multi - layer perceptron ( MLP ). multi - layer perceptron ( MLP ) PART-OF model. graph neural network ( GNN ) PART-OF model. attention mechanism PART-OF model. smoothed policy gradient CONJUNCTION stochastic curriculum learning. stochastic curriculum learning CONJUNCTION smoothed policy gradient. stochastic curriculum learning USED-FOR policy. smoothed policy gradient PART-OF model. stochastic curriculum learning PART-OF model. large - scale randomly generated TSP CONJUNCTION realistic TSP. realistic TSP CONJUNCTION large - scale randomly generated TSP. approach COMPARE learning - based solvers. learning - based solvers COMPARE approach. learning - based solvers USED-FOR large - scale randomly generated TSP. learning - based solvers USED-FOR realistic TSP. realistic TSP EVALUATE-FOR approach. large - scale randomly generated TSP EVALUATE-FOR approach. Task is generalizability of large - scale instances. Material is large - scale instances. ,"This paper proposes a deep RL approach, equivariant model and local search heuristics for solving traveling salesman problems (TSP) with Euclidean symmetry. The model consists of a graph neural network (GNN), multi-layer perceptron (MLP), and attention mechanism. The smoothed policy gradient and stochastic curriculum learning are used to train the policy. The approach is evaluated on large-scale randomly generated TSP and realistic TSP, where the proposed approach outperforms learning-based solvers. The paper is motivated by the problem of generalizability of large -scale instances."
1228,SP:fd4ab1cb777b541c22a923c1c86d82ac1d8384fd,"model USED-FOR TSP. RL USED-FOR model. MLP CONJUNCTION attention modules. attention modules CONJUNCTION MLP. GNN CONJUNCTION MLP. MLP CONJUNCTION GNN. MLP USED-FOR encoder - decoder architecture. GNN USED-FOR encoder - decoder architecture. attention modules USED-FOR encoder - decoder architecture. policy gradient algorithms USED-FOR training. local search heuristic USED-FOR policy - generated solution. Material are TSP instance, and synthetic and realistic TSP instances. Method is curriculum learning approach. ","This paper proposes a model for TSP based on RL. The encoder-decoder architecture is based on GNN, MLP, and attention modules. During training, policy gradient algorithms are used to guide the training. A local search heuristic is used to find a policy-generated solution for each TSP instance. A curriculum learning approach is also proposed. Experiments are conducted on both synthetic and realistic TSP instances."
1229,SP:fd4ab1cb777b541c22a923c1c86d82ac1d8384fd,policy gradient CONJUNCTION local search algorithms. local search algorithms CONJUNCTION policy gradient. local search algorithms USED-FOR traveling salesman problems ( TSP ). policy gradient USED-FOR traveling salesman problems ( TSP ). local search USED-FOR tours. policy rollout USED-FOR tours. local search USED-FOR policy gradients. preprocessing steps USED-FOR problem instance features. random TSP instances CONJUNCTION those. those CONJUNCTION random TSP instances. those PART-OF TSPLIB. random TSP instances CONJUNCTION TSPLIB. TSPLIB CONJUNCTION random TSP instances. Generic is algorithm. ,"This paper proposes to combine policy gradient and local search algorithms for traveling salesman problems (TSP). The main idea is to use local search to learn the best policy gradients for different types of tours based on policy rollout. The algorithm consists of two steps: (1) preprocessing steps to extract problem instance features, and (2) training of the policy. Experiments are conducted on random TSP instances and those in TSPLIB."
1230,SP:fd4ab1cb777b541c22a923c1c86d82ac1d8384fd,equivariance properties CONJUNCTION local search heuristics. local search heuristics CONJUNCTION equivariance properties. equivariance properties USED-FOR deep learning model. local search heuristics USED-FOR deep learning model. deep learning model USED-FOR TSP instances. model USED-FOR tour. REINFORCE USED-FOR distribution ( policy ). policy gradients USED-FOR loss landscape. local search heuristics USED-FOR solution. graph neural network CONJUNCTION attention mechanisms. attention mechanisms CONJUNCTION graph neural network. rotation CONJUNCTION translation. translation CONJUNCTION rotation. equivariance FEATURE-OF TSP. preprocessing steps USED-FOR equivariance. graph neural network PART-OF model architecture. attention mechanisms PART-OF model architecture. stochastic curriculum learning USED-FOR small TSP instances. problems EVALUATE-FOR model. stochastic curriculum learning USED-FOR model. ,"This paper proposes a deep learning model that leverages equivariance properties and local search heuristics to learn TSP instances. The model first learns a tour and then learns a distribution (policy) using REINFORCE. The policy gradients are used to map the loss landscape to the training data. The solution can be viewed as a combination of the two ideas, and the authors show that the proposed model architecture consists of a graph neural network and attention mechanisms. The authors also demonstrate that the model can generalize to new problems by using stochastic curriculum learning to learn small TSP instance. Finally, the authors discuss the effect of preprocessing steps on the ability of the model to generalize and show that TSP has a high degree of equivarianance in terms of rotation and translation."
1231,SP:8aa471b92e2671d471107c087164378f45fb204f,"synthetic data USED-FOR federated training. GANs USED-FOR synthetic data. GANs USED-FOR federated training. non - IID data USED-FOR federated training. SDA - FL HYPONYM-OF federated learning algorithm. GANs USED-FOR federated learning algorithm. GAN USED-FOR synthetic data. global model CONJUNCTION synthetic data. synthetic data CONJUNCTION global model. synthetic data USED-FOR local model. local data CONJUNCTION synthetic data. synthetic data CONJUNCTION local data. local data USED-FOR local model. averaged model USED-FOR synthetic data. SDA - FL COMPARE federated learning approaches. federated learning approaches COMPARE SDA - FL. non - IID data EVALUATE-FOR SDA - FL. non - IID data EVALUATE-FOR federated learning approaches. Method are local models, and differentially private GANs. Generic is models. OtherScientificTerm is privacy budgets. ","This paper proposes a federated learning algorithm, SDA-FL, that uses GANs to generate synthetic data for federated training on non-IID data. The synthetic data is generated by training a GAN on both the global model and the synthetic data. Then, the local model is trained on both local data and synthetic data generated by the averaged model. The local models are trained in a supervised fashion, where the models are not subject to privacy budgets. The main contribution of this paper is to propose a differentially private GAN. Experiments show that SDA - FL outperforms the other federated data on both non-ID and IID datasets."
1232,SP:8aa471b92e2671d471107c087164378f45fb204f,"privacy requirement FEATURE-OF data sharing. data sharing USED-FOR non - IID problem. Image synthesis CONJUNCTION DP - GAN. DP - GAN CONJUNCTION Image synthesis. DP - GAN PART-OF components. Image synthesis PART-OF components. differentially private GAN USED-FOR synthetic data. PS USED-FOR data - sharing. data PART-OF PS. data USED-FOR data - sharing. local models USED-FOR pseudo - labels. self - training COMPARE PS. PS COMPARE self - training. local models USED-FOR PS. model training CONJUNCTION synthetic dataset updating. synthetic dataset updating CONJUNCTION model training. private data CONJUNCTION shared synthetic data. shared synthetic data CONJUNCTION private data. shared synthetic data USED-FOR non - IIDness. private data USED-FOR non - IIDness. Data sharing USED-FOR gradient descent. supervised and semi - supervised learning settings EVALUATE-FOR framework. privacy budget CONJUNCTION synthetic data. synthetic data CONJUNCTION privacy budget. Method are Federated learning, SDA - FL, FedAvg, classifier, and ServerUpdate. Task are Synthetic image labeling, and training. OtherScientificTerm is unlabeled data. Generic is labels. ","This paper addresses the non-IID problem of data sharing in the presence of a privacy requirement. Federated learning has been shown to be effective in this setting. Synthetic image labeling is an important problem. This paper proposes two components: Image synthesis and DP-GAN. Image synthesis uses a differentially private GAN to generate synthetic data. The authors compare the performance of self-training and PS with data-sharing using local models to generate pseudo-labels for unlabeled data. They show that with SDA-FL, FedAvg, and ServerUpdate, the model training and synthetic dataset updating converge to the same classifier. They also show that training with data sharing improves gradient descent performance. The proposed framework is evaluated on both supervised and semi-supervised learning settings. The results show that using both private data and shared synthetic data can improve non-IDness. The paper also shows that the choice of privacy budget and synthetic data has a significant impact on the performance."
1233,SP:8aa471b92e2671d471107c087164378f45fb204f,GAN - generated data CONJUNCTION differential privacy. differential privacy CONJUNCTION GAN - generated data. differential privacy USED-FOR non - iid problem. differential private GAN - generated data USED-FOR non - iid and local data privacy problems. label updating mechanism USED-FOR model. CIFAR-10 CONJUNCTION MNIST. MNIST CONJUNCTION CIFAR-10. MNIST CONJUNCTION fashion - MNIST. fashion - MNIST CONJUNCTION MNIST. MNIST EVALUATE-FOR algorithm. fashion - MNIST EVALUATE-FOR algorithm. CIFAR-10 EVALUATE-FOR algorithm. Task is federated learning setting. Generic is method. Method is SDA - FL. OtherScientificTerm is local data privacy. ,"This paper studies the federated learning setting and proposes a method called SDA-FL that combines GAN-generated data with differential privacy to solve the non-iid problem and local data privacy problems. The proposed algorithm is evaluated on CIFAR-10, MNIST, and fashion-MNIST. The model is trained with a label updating mechanism. The experiments show that the proposed method can achieve state-of-the-art performance while maintaining the local privacy."
1234,SP:8aa471b92e2671d471107c087164378f45fb204f,framework USED-FOR federated learning. federated learning USED-FOR non - IID issue. framework USED-FOR non - IID issue. differentially private synthetic data USED-FOR framework. local GAN USED-FOR synthetic data. pseudo labeling USED-FOR server. IID distribution FEATURE-OF local data. real and synthetic data PART-OF local data. model training CONJUNCTION synthetic data updating. synthetic data updating CONJUNCTION model training. convergence EVALUATE-FOR local models. Method is parameter server. OtherScientificTerm is global model parameters. ,"This paper proposes a framework to address the non-IID issue in federated learning using differentially private synthetic data. The synthetic data is generated by a local GAN and fed to a parameter server. The server uses pseudo labeling to distinguish between real and synthetic data, and the local data is sampled from the IID distribution. The global model parameters are then fed to the server. Experiments are conducted on model training, synthetic data updating, and convergence of the local models."
1235,SP:7656b0bd5eb7e46359d8111e5534a07744f5d7ae,empirical tricks USED-FOR loss function. training method COMPARE randomized smoothed classifiers. randomized smoothed classifiers COMPARE training method. MNIST and CIFAR-10 EVALUATE-FOR training method. MNIST and CIFAR-10 EVALUATE-FOR randomized smoothed classifiers. Generic is loss. OtherScientificTerm is radius r. ,This paper proposes a new loss function based on empirical tricks. The proposed loss is based on the fact that the radius r is a function of the number of samples. Experiments on MNIST and CIFAR-10 show that the proposed training method outperforms randomized smoothed classifiers.
1236,SP:7656b0bd5eb7e46359d8111e5534a07744f5d7ae,loss functions USED-FOR classifiers. loss functions USED-FOR certified robustness. randomized smoothing USED-FOR loss functions. it USED-FOR certified radius. Method is loss function. OtherScientificTerm is confidence level. ,This paper studies the problem of certified robustness using different loss functions for different classifiers using randomized smoothing. The main contribution of this paper is to propose a new loss function that is independent of the confidence level and to use it to estimate the certified radius.
1237,SP:7656b0bd5eb7e46359d8111e5534a07744f5d7ae,losses PART-OF CAT - RS. clean accuracy FEATURE-OF hard samples. certified radius FEATURE-OF easy samples. certified radius EVALUATE-FOR losses. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. ACR EVALUATE-FOR method. CIFAR-10 EVALUATE-FOR method. MNIST EVALUATE-FOR method. Task is randomized smoothing. Metric is average certified radius. ,"This paper studies the problem of randomized smoothing. The authors propose CAT-RS, which combines two losses to improve the certified radius of easy samples while maintaining clean accuracy for hard samples. The proposed method is evaluated on MNIST and CIFAR-10 with ACR, and the average certified radius is shown to improve."
1238,SP:7656b0bd5eb7e46359d8111e5534a07744f5d7ae,"accuracy and robustness tradeoff FEATURE-OF RS. sample - wise control of robustness USED-FOR tradeoff. robustness CONJUNCTION prediction confidence. prediction confidence CONJUNCTION robustness. prediction confidence FEATURE-OF smoothed classifiers. robustness FEATURE-OF smoothed classifiers. MNIST CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION MNIST. CIFAR10 EVALUATE-FOR method. MNIST EVALUATE-FOR method. Method are randomized smoothing ( RS ), and loss function. ","This paper proposes randomized smoothing (RS), a method for smoothed classifiers that aims to balance the accuracy and robustness tradeoff in RS. The tradeoff is modeled as a sample-wise control of robustness, and the authors propose a new loss function to optimize this tradeoff. The proposed method is evaluated on MNIST and CIFAR10, and is shown to improve robustness and prediction confidence in the presence of smoothed classes."
1239,SP:8fdfed1c38ae00a0063ab41f72fa26826f5f4570,padding tokens PART-OF Wikipedia dataset. packing methods USED-FOR BERT. Generic is methods. Method is vanilla BERT training. ,This paper proposes packing methods for BERT. The proposed methods are based on the idea of padding tokens in Wikipedia dataset. The authors show that the proposed methods can improve the performance of vanilla BERT training.
1240,SP:8fdfed1c38ae00a0063ab41f72fa26826f5f4570,"packing algorithms USED-FOR bert pretraining. Wikipedia USED-FOR bert - large training. datasets USED-FOR bert - large training. Wikipedia HYPONYM-OF datasets. packing algorithms USED-FOR Wikipedia. bin - packing algorithm comparison CONJUNCTION scaling analysis. scaling analysis CONJUNCTION bin - packing algorithm comparison. scaling analysis USED-FOR packing. core codes USED-FOR packing. Method is SPFHP. OtherScientificTerm are Packing depths, and hypermeter adjusting. ","This paper studies the packing algorithms for bert pretraining on Wikipedia, one of the most popular datasets used in bert-large training (e.g. Wikipedia). The paper provides a bin-packing algorithm comparison and scaling analysis for the packing of Wikipedia, as well as a scaling analysis of SPFHP. Packing depths are compared to the core codes for different packing algorithms, and hypermeter adjusting is discussed."
1241,SP:8fdfed1c38ae00a0063ab41f72fa26826f5f4570,packing algorithms USED-FOR fixed length sequence. modeling configuration CONJUNCTION optimizer configurations. optimizer configurations CONJUNCTION modeling configuration. Method is BERT. OtherScientificTerm is padding. ,"This paper studies packing algorithms for a fixed length sequence. The main contribution of this paper is to study the modeling configuration and optimizer configurations of BERT. In particular, the authors show that BERT does not require padding, which is a limitation of packing algorithms. "
1242,SP:8fdfed1c38ae00a0063ab41f72fa26826f5f4570,max sequence length USED-FOR BERT pretraining. positional embedding CONJUNCTION attention masks. attention masks CONJUNCTION positional embedding. attention masks CONJUNCTION optimization hyper - parameters. optimization hyper - parameters CONJUNCTION attention masks. optimization hyper - parameters USED-FOR method. attention masks USED-FOR method. positional embedding USED-FOR method. method USED-FOR optimization. Wikipedia dataset EVALUATE-FOR method. training loss EVALUATE-FOR method. ,"This paper proposes a method to reduce the max sequence length for BERT pretraining. The proposed method uses positional embedding, attention masks, and optimization hyper-parameters. The method is evaluated on Wikipedia dataset and shows that the proposed method can significantly reduce the training loss and speed up the optimization."
1243,SP:460d4cc1a5c01e34abe37d9eb1b74dd3734b1d55,"algorithm USED-FOR search objectives. regular Monte Carlo tree search algorithms COMPARE BATS. BATS COMPARE regular Monte Carlo tree search algorithms. regular Monte Carlo tree search algorithms USED-FOR value function. greedy decoding USED-FOR autoregressive models. greedy decoding USED-FOR informed playout. greedy decoding USED-FOR BATS. informed playout USED-FOR BATS. search objectives PART-OF model changes. beam search USED-FOR weaker models. translation quality EVALUATE-FOR BATS. beam search COMPARE BATS. BATS COMPARE beam search. search budget FEATURE-OF BATS. Method are beam adaptive tree search ( BATS ), search algorithm, MRT - trained autoregressive models, and MT models. OtherScientificTerm are search space, and beam size. Metric is constrained node expansion criterion. ","This paper proposes beam adaptive tree search (BATS), a new search algorithm for MRT-trained autoregressive models. Unlike regular Monte Carlo tree search algorithms that optimize the value function, BATS learns the search objectives by performing an informed playout based on greedy decoding for autoresgressive models, where the search space is restricted to a fixed number of nodes. The authors show that BATS is able to improve the translation quality by up to 1.5x while maintaining the same search budget. They also show that the beam search can be used to train weaker models, and that the proposed search objectives can be incorporated into model changes. Finally, the authors provide a constrained node expansion criterion to evaluate the performance of BATS in terms of the search budget and beam size. The experiments show that MT models can be trained to perform better than BATS."
1244,SP:460d4cc1a5c01e34abe37d9eb1b74dd3734b1d55,adaptive tree search algorithm USED-FOR text generation. autoregressive model USED-FOR value network. autoregressive model USED-FOR It. sum of token level log - probability USED-FOR metric. metric USED-FOR decoding. method COMPARE beam search. beam search COMPARE method. machine translation datasets EVALUATE-FOR method. OtherScientificTerm is max rank. ,"This paper proposes an adaptive tree search algorithm for text generation. It uses an autoregressive model to train the value network, where the max rank is computed as the sum of token level log-probability of each token. This metric is then used for decoding. The method is evaluated on machine translation datasets and compared to beam search."
1245,SP:460d4cc1a5c01e34abe37d9eb1b74dd3734b1d55,adaptive tree search algorithm USED-FOR NMT models. search objectives USED-FOR algorithm. it USED-FOR beam search bias. algorithm COMPARE baseline. baseline COMPARE algorithm. tricks USED-FOR algorithm. ,"This paper proposes an adaptive tree search algorithm for NMT models. The proposed algorithm is based on two search objectives, and it aims to reduce the beam search bias. The experimental results show that the proposed algorithm outperforms the baseline with a few tricks."
1246,SP:460d4cc1a5c01e34abe37d9eb1b74dd3734b1d55,adaptive tree search algorithm BATS USED-FOR NMT. adaptive tree search algorithm BATS HYPONYM-OF MCTS variant. beam search USED-FOR incomplete partial generations. greedy searched rollout COMPARE MCTS. MCTS COMPARE greedy searched rollout. beam search HYPONYM-OF heuristics. translation quality EVALUATE-FOR objective Max Rank. BART COMPARE beam search. beam search COMPARE BART. Metric is BLEU. Generic is algorithm. Method is autoregressive model. OtherScientificTerm is search biases. ,"This paper proposes an adaptive tree search algorithm BATS for NMT, which is an MCTS variant. The algorithm is based on an autoregressive model, where the objective Max Rank is a function of the translation quality. The authors show that the greedy searched rollout is equivalent to MMTS, and that other heuristics such as beam search can lead to incomplete partial generations. They also compare BART with beam search and show that BART outperforms beam search in terms of BLEU. Finally, the authors provide a theoretical analysis of the search biases."
1247,SP:ff7b9e6ff5303f8a4f0321d06d9d9573e4853c5f,"model USED-FOR detecting irregularities. images USED-FOR detecting irregularities. modeling approach CONJUNCTION adaptive algorithm. adaptive algorithm CONJUNCTION modeling approach. method USED-FOR anomalies. retraining USED-FOR tasks. energy based models USED-FOR data densities. learning from inpainting ” operation USED-FOR anomalies. method USED-FOR learning from inpainting ” operation. Task are manufacturing acceptance testing, and anomaly detection. ",This paper proposes a new model for detecting irregularities in images. The modeling approach and the adaptive algorithm are well-motivated. The method is able to detect anomalies using a “learning from inpainting” operation. The authors also propose to use energy based models to reduce the data densities and perform retraining for different tasks. The experiments are conducted on manufacturing acceptance testing and anomaly detection.
1248,SP:ff7b9e6ff5303f8a4f0321d06d9d9573e4853c5f,framework USED-FOR anomaly detection and localization. fast adaptation USED-FOR tasks. framework USED-FOR fast adaptation. adaptive sparse coding layer USED-FOR energy - based model ( EBM ). normal features USED-FOR adaptive sparse coding layer. meta - learning process USED-FOR common knowledge. Shrinkage functions CONJUNCTION sparse coding. sparse coding CONJUNCTION Shrinkage functions. learning by inpainting USED-FOR EBM training. Shrinkage functions USED-FOR EBM training. sparse coding CONJUNCTION learning by inpainting. learning by inpainting CONJUNCTION sparse coding. large receptive fields FEATURE-OF sparse coding. Task is few shots adaptation. ,"This paper proposes a framework for anomaly detection and localization for fast adaptation to new tasks. The main idea is to train an adaptive sparse coding layer on top of an energy-based model (EBM) using normal features. The authors propose a meta-learning process to extract common knowledge from the data. Shrinkage functions, sparse coding with large receptive fields, and learning by inpainting are used for EBM training. Experiments are conducted on few shots adaptation."
1249,SP:ff7b9e6ff5303f8a4f0321d06d9d9573e4853c5f,"deep encoder CONJUNCTION pattern matching module. pattern matching module CONJUNCTION deep encoder. encoded latent vector COMPARE pattern dictionary. pattern dictionary COMPARE encoded latent vector. deep encoder PART-OF architecture. pattern matching module PART-OF architecture. lasso regression problem USED-FOR pattern matching. few - shot learning methods CONJUNCTION synthetic sample generation approach. synthetic sample generation approach CONJUNCTION few - shot learning methods. random perturbation CONJUNCTION gradient method. gradient method CONJUNCTION random perturbation. gradient method USED-FOR synthetic sample generation approach. few - shot learning methods USED-FOR online learning approach. random perturbation USED-FOR synthetic sample generation approach. Method is image classification system. OtherScientificTerm are normal patterns, dictionary, and anomaly score. ","This paper proposes an online learning approach based on few-shot learning methods, a synthetic sample generation approach using random perturbation and a gradient method. The proposed architecture consists of a deep encoder and a pattern matching module. The pattern matching is modeled as a lasso regression problem, where the encoded latent vector is the same as the pattern dictionary, and the normal patterns are sampled from the dictionary. The anomaly score is used to train the image classification system."
1250,SP:ff7b9e6ff5303f8a4f0321d06d9d9573e4853c5f,EBM ( Energy Based Model ) USED-FOR anomaly detection algorithm. model USED-FOR pseudo - anomaly instances. ,This paper proposes an anomaly detection algorithm based on EBM (Energy Based Model). The model is trained to detect pseudo-anomalous instances.
1251,SP:801a61d01d3b159f301013b182150a80fbfe8fa2,"dataset USED-FOR task. humans CONJUNCTION neural - models. neural - models CONJUNCTION humans. ContraQA HYPONYM-OF dataset. humans USED-FOR contradicting contexts. contradicting contexts PART-OF it. neural - models USED-FOR contradicting contexts. neural - models PART-OF it. model USED-FOR generating contradicting examples. model USED-FOR fake contexts. BART - FG USED-FOR fake contexts. constituency parsing USED-FOR constituency spans. Wikipedia dump USED-FOR BART model. fake detector HYPONYM-OF transformer - based model. QA system USED-FOR fake contexts. fake detector USED-FOR trust score. misinformation - aware framework USED-FOR QA system. fake detector USED-FOR misinformation - aware framework. two USED-FOR fake contexts. BART - FG CONJUNCTION GPT-2. GPT-2 CONJUNCTION BART - FG. two USED-FOR QA model. fake contexts USED-FOR QA model. GPT-2 USED-FOR fake contexts. GPT-2 HYPONYM-OF two. BART - FG USED-FOR fake contexts. Material is SQuAD. Method are QA models, and fake detector model. ","This paper proposes a new dataset called ContraQA, which is designed for the task of generating contradicting contexts between humans and neural-models. The authors propose a transformer-based model, called fake detector, that is trained on the Wikipedia dump. The model is trained to generate fake contexts, which are generated using BART-FG and GPT-2, which use constituency parsing to generate constituency spans. The fake detector model is then used in a misinformation-aware framework to improve the trust score of a QA system. Experiments on SQuAD show that the fake contexts generated by the two QA model can improve the QA models performance."
1252,SP:801a61d01d3b159f301013b182150a80fbfe8fa2,contradicting contexts FEATURE-OF SQuAD articles. contradicting contexts FEATURE-OF closed - domain Question Answering. neural models USED-FOR contradicting contexts. neural framework USED-FOR contradicting contexts. BART - FG USED-FOR contradicting contexts. BART - FG HYPONYM-OF neural framework. SOTA systems COMPARE ContraQA. ContraQA COMPARE SOTA systems. misinformation detecting system COMPARE SOTA systems. SOTA systems COMPARE misinformation detecting system. SOTA QA systems USED-FOR task. misinformation detecting system COMPARE ContraQA. ContraQA COMPARE misinformation detecting system. Machine Reader USED-FOR misinformation detecting system. ContraQA HYPONYM-OF task. Method is SQuAD1.1. OtherScientificTerm is constituency spans. ,"This paper studies the problem of contradicting contexts in SQuAD articles. In particular, the paper focuses on the closed-domain Question Answering, which has been shown to be susceptible to contradictory contexts. The paper proposes a neural framework, BART-FG, which aims to address the problem by generating contradicting context using neural models. Experiments are conducted on a new task, ContraQA, and compared with SOTA QA systems. The results show that the proposed misinformation detecting system, based on Machine Reader, outperforms the SOTA systems, and that ContraQa is more interpretable. The main contribution of the paper is the introduction of BART - FG, which is an extension of BART-FBG, which was first proposed in the paper SQAD1.1. The authors also propose to use constituency spans to further reduce the variance of the contradicting examples."
1253,SP:801a61d01d3b159f301013b182150a80fbfe8fa2,"accuracy EVALUATE-FOR QA systems. contradictory information USED-FOR QA systems. contradictory paragraphs ( context ) PART-OF dataset. trustworthiness score CONJUNCTION quality. quality CONJUNCTION trustworthiness score. Material is contradictory data. Method are BART - FG model, RoBERTa - based model, and RoBERTa - based classifier. Task are QA, and contradictory information creation. Generic is models. ","This paper studies the problem of contradictory information in QA systems, where contradictory data is generated by the BART-FG model. The authors propose a dataset with contradictory paragraphs (context) in the dataset, which is then used to evaluate the accuracy of the QA based on the trustworthiness score and quality. The paper proposes a RoBERTa-based model that is trained on the contradictory data, and then a new RoBERTA-based classifier is proposed. The experiments show that the proposed models are able to distinguish between contradictory information creation and QA."
1254,SP:801a61d01d3b159f301013b182150a80fbfe8fa2,"human and machine generated contradictory contexts FEATURE-OF dataset. Wikipedia sentences FEATURE-OF masked constituent parses. masked constituent parses USED-FOR BART. masked constituency parse USED-FOR BART model. BART model USED-FOR contradictory contexts. BART Model USED-FOR rest. dataset USED-FOR QA. machine reading comprehension HYPONYM-OF QA. SQuAD dataset EVALUATE-FOR QA models. ROBERTA CONJUNCTION SPAN - BERT. SPAN - BERT CONJUNCTION ROBERTA. BERT CONJUNCTION ROBERTA. ROBERTA CONJUNCTION BERT. BERT CONJUNCTION SPAN - BERT. SPAN - BERT CONJUNCTION BERT. Material are SQuAD 1.1, and SQuAD. Generic are it, and models. Method are QA system, and span based passage reader. ","This paper presents a new dataset of human and machine generated contradictory contexts. The dataset is designed for QA (e.g., machine reading comprehension). The authors train BART on the masked constituent parses of Wikipedia sentences. The BART model is trained with a masked constituency parse, and the rest of the BART Model is trained on the rest. The authors evaluate several QA models on the SQuAD dataset and show that it outperforms the state-of-the-art QA system. The experiments are conducted on the original dataset, and on the new dataset, which is a modified version (SQuAD 1.1), and the authors compare the performance of the models on BERT, ROBERTA, SPAN-BERT, and a span based passage reader."
1255,SP:7effe51275b9b2e14b3e099533e410e09f5b7c5a,"optimal transport problem USED-FOR cross - domain imitation learning. Gromov - Wasserstein distance USED-FOR optimal transport problem. Gromov - Wasserstein distance USED-FOR cross - domain imitation learning. problem USED-FOR imitation learning settings. human demonstrator USED-FOR humanoid robot. state action occupancies FEATURE-OF Gromov - Wasserstein distance. isometry USED-FOR optimal policy. it USED-FOR optimal policies. OtherScientificTerm are domain mismatch, and reward function. Generic is approach. ","This paper studies the optimal transport problem of the Gromov-Wasserstein distance for cross-domain imitation learning. The problem is well-motivated in imitation learning settings where the domain mismatch between the source domain and the target domain is large. The authors propose an approach to address this issue. They train a humanoid robot with a human demonstrator, and use the state action occupancies of the two domains as the reward function. They use isometry to find the optimal policy, and show that it converges to optimal policies."
1256,SP:7effe51275b9b2e14b3e099533e410e09f5b7c5a,minimization of Gromov - Wasserstein distance USED-FOR method. distance USED-FOR pseudo - reward. optimal policy USED-FOR MDP. optimal policy USED-FOR MDP. optimal policy USED-FOR optimal policy. isometry USED-FOR MDPs. Method is RL. ,"This paper proposes a method based on the minimization of Gromov-Wasserstein distance. This distance is used as a pseudo-reward to encourage the agent to explore the MDP. The main idea is that the optimal policy for an MDP can be approximated by an optimal policy in the space of MDPs. This is an interesting idea, as isometry has been shown to be useful in MDPing. The paper is well-written and well-motivated. However, there are a few issues that need to be addressed in order for the paper to be accepted by the community in RL."
1257,SP:7effe51275b9b2e14b3e099533e410e09f5b7c5a,"Task is cross domain imitation learning problem. Material is expert demonstrations. Method are Gromov - Wasserstein distance, Gromov - Wasserstein Imitation Learning ( GWIL ), and GWIL. ","This paper studies the cross domain imitation learning problem, where the goal is to learn from expert demonstrations. The authors propose to use Gromov-Wasserstein distance between the source and target domains. The main contribution of this paper is to propose a new method called ""Gromov - Wasserstein Imitation Learning (GWIL)"" which is a generalization of GWIL."
1258,SP:7effe51275b9b2e14b3e099533e410e09f5b7c5a,"method USED-FOR cross - domain imitation learning. isometric transformations USED-FOR distance measures. collected state - action pairs USED-FOR Euclidiean distances. imitation domain CONJUNCTION expert domain. expert domain CONJUNCTION imitation domain. RL algorithm USED-FOR policy. SAC USED-FOR policy. SAC USED-FOR RL algorithm. pseudo - rewards USED-FOR SAC. OtherScientificTerm are correspondence, Gromov - Wasserstein distance, pseudo - reward, and U - maze. ","This paper proposes a method for cross-domain imitation learning. The main idea is to use Euclidiean distances between collected state-action pairs as isometric transformations of the distance measures. The correspondence is modeled as a Gromov-Wasserstein distance between the imitation domain and the expert domain. The RL algorithm is trained using SAC with pseudo-rewards, and the policy is trained to imitate the expert by maximizing the pseudo reward. Experiments are conducted on a U-maze and on a toy environment."
1259,SP:9ec000cd9c15e3c9988a41921c465b42e7d41877,"hierarchical cross contrastive self - supervised learning framework USED-FOR learning visual representation. projection levels FEATURE-OF features. image classification CONJUNCTION detection benchmarks. detection benchmarks CONJUNCTION image classification. Material is image. OtherScientificTerm are latent spaces, and contrastive loss. ",This paper proposes a hierarchical cross contrastive self-supervised learning framework for learning visual representation. The key idea is to learn features at different projection levels of an image. The latent spaces are learned by minimizing a contrastive loss. Experiments are conducted on image classification and detection benchmarks.
1260,SP:9ec000cd9c15e3c9988a41921c465b42e7d41877,hierarchical projection network USED-FOR multi - level latent representations. hierarchical projection network PART-OF method. cross contrastive loss USED-FOR invariant visual representations. segmentation CONJUNCTION object detection. object detection CONJUNCTION segmentation. classification CONJUNCTION segmentation. segmentation CONJUNCTION classification. downstream tasks EVALUATE-FOR HCCL. classification EVALUATE-FOR HCCL. object detection HYPONYM-OF downstream tasks. classification HYPONYM-OF downstream tasks. segmentation HYPONYM-OF downstream tasks. Method is Hierarchical Cross Contrastive Learning ( HCCL ) method. ,"This paper proposes a Hierarchical Cross Contrastive Learning (HCCL) method. The proposed method consists of a hierarchical projection network to learn multi-level latent representations. The cross contrastive loss is used to learn invariant visual representations. HCCL is evaluated on three downstream tasks: classification, segmentation and object detection."
1261,SP:9ec000cd9c15e3c9988a41921c465b42e7d41877,contrastive learning based representation learning approach USED-FOR extension. features USED-FOR method. features USED-FOR CL. detection CONJUNCTION segmentation. segmentation CONJUNCTION detection. segmentation CONJUNCTION few - shot learning tasks. few - shot learning tasks CONJUNCTION segmentation. classification CONJUNCTION detection. detection CONJUNCTION classification. Method is Hierarchical Cross Contrastive Learning(HCCL ). ,"This paper proposes an extension to the contrastive learning based representation learning approach, called Hierarchical Cross Contrastive Learning(HCCL). The proposed method is based on the idea that the features learned in CL can be used to improve the performance on classification, detection, segmentation, and few-shot learning tasks."
1262,SP:9ec000cd9c15e3c9988a41921c465b42e7d41877,hierarchical projectors CONJUNCTION predictors. predictors CONJUNCTION hierarchical projectors. BYOL CONJUNCTION SimSiam. SimSiam CONJUNCTION BYOL. work COMPARE HCCL. HCCL COMPARE work. SimSiam COMPARE HCCL. HCCL COMPARE SimSiam. layers of projectors FEATURE-OF contrastive loss. contrastive loss USED-FOR HCCL. hierarchical projectors USED-FOR HCCL. predictors USED-FOR HCCL. SimSiam HYPONYM-OF work. BYOL HYPONYM-OF work. BYOL CONJUNCTION SimSiam. SimSiam CONJUNCTION BYOL. iNat18 CONJUNCTION Place-205. Place-205 CONJUNCTION iNat18. SimSiam CONJUNCTION Barlow Twins. Barlow Twins CONJUNCTION SimSiam. SWAV CONJUNCTION BYOL. BYOL CONJUNCTION SWAV. self - supervised learning benchmarks EVALUATE-FOR HCCL. iNat18 HYPONYM-OF self - supervised learning benchmarks. Place-205 HYPONYM-OF self - supervised learning benchmarks. hierarchical projectors CONJUNCTION cross contrastive loss. cross contrastive loss CONJUNCTION hierarchical projectors. cross contrastive loss CONJUNCTION learning rates. learning rates CONJUNCTION cross contrastive loss. learning rates EVALUATE-FOR predictor. cross contrastive loss CONJUNCTION predictor. predictor CONJUNCTION cross contrastive loss. ,"This paper proposes HCCL, which combines hierarchical projectors, predictors, and cross contrastive loss with layers of projectors. Compared to previous work such as SWAV, BYOL, SimSiam, and BYOL as well as the HCCCL, the authors show that HOCL outperforms all of the previous work in terms of performance on several self-supervised learning benchmarks such as iNat18, Place-205, and others. The authors also compare the performance of the hierarchical projector and the predictor on SWAV (with and without layers), BYOL (with or without projectors and without hierarchical projectsors), and by combining hierarchical and predictors. In addition, they compare the results on Barlow Twins (SimSiam) and Sim-Siam (with hierarchical and without predictors) and compare their performance with the results of the original work (HCCL). They also compare their results with the original HCCLC. They show that hierarchical projects are more effective than the original one, and that the cross-contrastive loss, the predictor, and the learning rates are better."
1263,SP:943b0a3f94ba270bb7c0dc1e1f363e53bc5cf8ae,economics FEATURE-OF general equilibria. DRL CONJUNCTION structured learning curricula. structured learning curricula CONJUNCTION DRL. annealing of action space CONJUNCTION penalty coefficients. penalty coefficients CONJUNCTION annealing of action space. structured learning curricula CONJUNCTION annealing of action space. annealing of action space CONJUNCTION structured learning curricula. annealing of action space USED-FOR method. penalty coefficients USED-FOR method. DRL USED-FOR method. structured learning curricula USED-FOR method. method USED-FOR real - business - cycle model. Computing equilibria USED-FOR problems. OtherScientificTerm is government. Generic is problem. ,"This paper studies the problem of computing general equilibria in the context of economics. The proposed method uses DRL, structured learning curricula, annealing of action space, and penalty coefficients. The method is applied to a real-business-cycle model where the goal is to minimize the cost incurred by the government. The problem is formulated as computing the sum of the cost of all actions in the current cycle and the cost for the next cycle, and the paper provides a theoretical analysis of the problem and provides some empirical evidence that computing equilibres for these problems is computationally efficient."
1264,SP:943b0a3f94ba270bb7c0dc1e1f363e53bc5cf8ae,"open- and closed - economies FEATURE-OF equilriba. Method are Deep MARL, RBC ( economic market model ), and reward shaping schedule. OtherScientificTerm is low - welfare equilria. ","This paper proposes Deep MARL, an extension of RBC (economic market model) to the case of low-welfare equilria. The authors study equilriba in both open- and closed-economies, and propose a new reward shaping schedule."
1265,SP:943b0a3f94ba270bb7c0dc1e1f363e53bc5cf8ae,multi - agent reinforcement learning algorithm USED-FOR economic environment. GPUs USED-FOR structured learning curriculum. epsilon Nash equilibrium FEATURE-OF solution. Generic is algorithm. ,This paper proposes a multi-agent reinforcement learning algorithm for the economic environment. The algorithm is based on the idea that GPUs can be used to learn a structured learning curriculum. The solution is shown to converge to an epsilon Nash equilibrium.
1266,SP:943b0a3f94ba270bb7c0dc1e1f363e53bc5cf8ae,deep reinforcement learning framework USED-FOR finding dynamic general equilibrium. problem PART-OF machine learning. Markov game FEATURE-OF dynamic general equilibrium. worker - consumers FEATURE-OF real - business - cycle model. real - business - cycle model EVALUATE-FOR scheme. Task is economics. OtherScientificTerm is government. ,This paper proposes a deep reinforcement learning framework for finding dynamic general equilibrium in a Markov game. This problem is a well-studied problem in machine learning and has been studied extensively in economics. The proposed scheme is evaluated on a real-business-cycle model with worker-consumers and a government.
1267,SP:f885c992df9c685f806a653398736432ba38bd80,"defense USED-FOR model extraction attacks. approach USED-FOR information leakage. PATE HYPONYM-OF differential privacy metric. defense USED-FOR attacks. adaptive adversaries HYPONYM-OF attacks. OtherScientificTerm are computation overhead, and puzzle difficulty. ","This paper proposes a defense against model extraction attacks. The proposed approach, PATE, is a differential privacy metric that aims to minimize information leakage by minimizing computation overhead. The authors show that the proposed defense can be applied to two types of attacks: adaptive adversaries and adversarial adversaries. The main contribution of the paper is to show that PATE is robust to puzzle difficulty."
1268,SP:f885c992df9c685f806a653398736432ba38bd80,"APIs USED-FOR them. APIs USED-FOR public networks. public networks USED-FOR them. APIs USED-FOR ML model extraction attacks. slower models CONJUNCTION accuracy. accuracy CONJUNCTION slower models. victim model USED-FOR method. Generic are methods, and system. Method is machine learning practitioners. ","This paper studies ML model extraction attacks by leveraging APIs that allow them to run on public networks. The authors propose two methods: (1) using the victim model as a proxy for the attacker, and (2) training the method on a victim model. The method is based on the observation that slower models and higher accuracy are more likely to be used by the attacker than slower models. The paper also provides a theoretical analysis of the limitations of the proposed methods and shows that the proposed method can be used to fool the system, which is an important problem for many machine learning practitioners."
1269,SP:f885c992df9c685f806a653398736432ba38bd80,defense USED-FOR model stealing problem. proactive defense USED-FOR method. information leakage estimator USED-FOR proactive defense. information leakage estimator USED-FOR method. attacks COMPARE regular user querying in - distribution data. regular user querying in - distribution data COMPARE attacks. out - of - distribution data USED-FOR attacks. Method is supervised machine learning model service. Generic is model. Material is labeled data. ,"This paper proposes a novel defense to the model stealing problem. The proposed method is based on the information leakage estimator and uses proactive defense. The main idea is to train a supervised machine learning model service on out-of-distribution data, and then train the model on the labeled data. Experiments show that the proposed attacks are more effective than regular user querying in-distributive data."
1270,SP:f885c992df9c685f806a653398736432ba38bd80,defense USED-FOR model stealing. method USED-FOR model extraction attacks. OtherScientificTerm is proof - of - work puzzles. Metric is computational time. ,"This paper proposes a defense against model stealing. The method is based on the idea of solving proof-of-work puzzles, where the goal is to reduce the computational time. The proposed method is applied to model extraction attacks."
1271,SP:39845a353e75e2f854c3dc649db3817d89ad9875,approach USED-FOR high - resolution images. conditional flows USED-FOR image. image USED-FOR conditional flows. conditional flows USED-FOR approach. WaveletFlow HYPONYM-OF method. OtherScientificTerm is conditioning factor. ,"This paper presents an approach to generate high-resolution images using conditional flows on an image. The method, WaveletFlow, is based on the idea that a conditioning factor should be learned over time."
1272,SP:39845a353e75e2f854c3dc649db3817d89ad9875,multi - resolution variant of continuous normalizing flows USED-FOR images. they COMPARE regular continuous normalizing flows. regular continuous normalizing flows COMPARE they. OtherScientificTerm is image sizes. Metric is training times. ,This paper proposes a multi-resolution variant of continuous normalizing flows for images. The authors claim that they are more scalable than regular continuous normalising flows and can handle larger image sizes. They also claim that their training times are faster.
1273,SP:39845a353e75e2f854c3dc649db3817d89ad9875,multi - resolution strategy USED-FOR continuous normalizing flows. mathematical properties FEATURE-OF transformation. wavelet based decomposition / downsampling PART-OF approach. volume and range preservation HYPONYM-OF mathematical properties. Task is likelihood estimation. Generic is model. ,"This paper proposes a multi-resolution strategy for continuous normalizing flows. The proposed approach is based on wavelet based decomposition/downsampling, where the transformation is assumed to satisfy certain mathematical properties such as volume and range preservation. The likelihood estimation is also performed on the model."
1274,SP:39845a353e75e2f854c3dc649db3817d89ad9875,"architecture USED-FOR continuous normalizing flows. architecture USED-FOR images. continuous normalizing flow USED-FOR conditional distributions. model USED-FOR noise. squeeze ” layers USED-FOR normalizing flows. unit determinant FEATURE-OF invertible ) transformation. maximum likelihood USED-FOR model. image datasets EVALUATE-FOR method. architecture CONJUNCTION layer. layer CONJUNCTION architecture. layer USED-FOR multi resolution normalizing flows. architecture USED-FOR multi resolution normalizing flows. large scale datasets USED-FOR model. Imagenet128 HYPONYM-OF large scale datasets. OtherScientificTerm are unconditional distribution of coarse images, coarse image, transformation, and log likelihood. Method is generative models. ","This paper proposes a new architecture for learning continuous normalizing flows for images. The main idea is to learn conditional distributions over an unconditional distribution of coarse images, where the conditional distributions are modeled as a continuous normalising flow. The authors propose to use “squeeze” layers to train normalizing functions. The model is trained by maximizing the maximum likelihood of a (invertible) transformation with a unit determinant of the original coarse image, and then the model is fine tuned to remove noise. The proposed method is evaluated on several image datasets and compared to other generative models. The results show that the proposed architecture and layer can be used to learn multi resolution normalizingflows, and that the model can be trained on large scale datasets (e.g. Imagenet128). The authors also provide some theoretical analysis of the log likelihood."
1275,SP:d09c2fad308249261a9742505e4ccaed2b3578b3,method USED-FOR noisy labels. representations USED-FOR method. representations USED-FOR noisy labels. worst - case error bound USED-FOR ranking - based method. neighborhood information USED-FOR noisy - label detection method. Method is contrastive learning approaches. ,This paper proposes a method to detect noisy labels using representations. The authors propose a ranking-based method based on a worst-case error bound. The noisy-label detection method is based on neighborhood information. The paper is well-motivated by contrastive learning approaches.
1276,SP:d09c2fad308249261a9742505e4ccaed2b3578b3,voting CONJUNCTION ranking. ranking CONJUNCTION voting. ranking HYPONYM-OF detection methods. voting HYPONYM-OF detection methods. training - free solution USED-FOR noisy labels. Metric is worst - case error bound. ,This paper proposes a training-free solution to deal with noisy labels in detection methods such as voting and ranking. The authors provide a worst-case error bound of $\Omega(\sqrt{T})$.
1277,SP:d09c2fad308249261a9742505e4ccaed2b3578b3,"supervised or self - supervised pre - training USED-FOR models. local voting CONJUNCTION ranking methods. ranking methods CONJUNCTION local voting. CORES CONJUNCTION CL. CL CONJUNCTION CORES. CL CONJUNCTION TracIn. TracIn CONJUNCTION CL. CIFAR10/100 EVALUATE-FOR learning - based approaches. CORES HYPONYM-OF learning - based approaches. TracIn HYPONYM-OF learning - based approaches. CL HYPONYM-OF learning - based approaches. Method are training - free approach, and pre - trained models. OtherScientificTerm are noisy labels, pre - trained manifold, clean label, and corrupted labels. ","This paper proposes a training-free approach to train models without supervised or self-supervised pre-training. The motivation for this is that noisy labels can be harmful to the performance of pre-trained models. The authors propose to use local voting and ranking methods to address this issue. They evaluate several learning-based approaches on CIFAR10/100, including CORES, CL, and TracIn, and compare their performance with the state-of-the-art in terms of local voting, ranking methods, etc. The results show that the pre-trained manifold is more robust to noisy labels than the clean label, and that corrupted labels are more likely to be corrupted than clean labels."
1278,SP:d09c2fad308249261a9742505e4ccaed2b3578b3,"representation extractor USED-FOR soft labels. clusterability of representations USED-FOR soft labels. kNN USED-FOR clusterability of representations. local voting CONJUNCTION global ranking - based scoring system. global ranking - based scoring system CONJUNCTION local voting. global ranking - based scoring system USED-FOR corrupted labels. local voting USED-FOR corrupted labels. representation - based method USED-FOR deep model. corrupted data USED-FOR deep model. Method are training - free instancewise noise label detection method, deep models, supervised training, and representations. OtherScientificTerm is overfitting noisy labels. ","This paper proposes a training-free instancewise noise label detection method. The key idea is to train a representation extractor to generate soft labels based on the clusterability of representations learned by kNN. To detect corrupted labels, local voting and a global ranking-based scoring system are used. The authors also propose a representation-based method for training a deep model on corrupted data. The experiments show that deep models trained with supervised training are more sensitive to overfitting noisy labels, and the learned representations are more robust to corrupted labels."
1279,SP:ae81e2a23bf6042bf8b04ba41b957bb625268b7e,"method USED-FOR adversarial perturbation. Method is RL agent. OtherScientificTerm are perturbations, and perturbations on policies. Generic is solution. ",This paper proposes a method for adversarial perturbation against an RL agent. The main idea is to use perturbations on policies that are different from the one being perturbed. The authors provide a theoretical analysis of the proposed solution.
1280,SP:ae81e2a23bf6042bf8b04ba41b957bb625268b7e,evasion attacks PART-OF deep reinforcement learning ( RL ). approach USED-FOR evasion attacks. director and actor modules PART-OF two - component design. latter PART-OF two - component design. director and actor modules USED-FOR approach. two - component design USED-FOR approach. policy space COMPARE state space. state space COMPARE policy space. it COMPARE state - of - the - art methods. state - of - the - art methods COMPARE it. Task is optimal attack. Generic is search. ,"This paper presents an approach to evasion attacks in deep reinforcement learning (RL). The approach is based on two-component design, the latter consisting of a pair of director and actor modules. The goal is to find an optimal attack that maximizes the return of the policy space instead of the state space. The authors conduct a thorough search and compare it with state-of-the-art methods."
1281,SP:ae81e2a23bf6042bf8b04ba41b957bb625268b7e,algorithm USED-FOR optimal evasion attack. optimal evasion attack USED-FOR RL. OtherScientificTerm is agent's policy. Method is clever decomposition. ,"This paper proposes an algorithm for optimal evasion attack in RL. The key idea is to decompose the agent's policy into a set of sub-problems, which can then be solved using clever decomposition."
1282,SP:ae81e2a23bf6042bf8b04ba41b957bb625268b7e,method USED-FOR attacks. method USED-FOR state observation. state observation FEATURE-OF RL agent. attacks USED-FOR state observation. computational resources USED-FOR attacks. state space FEATURE-OF MDP problem. mapping USED-FOR attacking strategy. state observation USED-FOR attacks. state space COMPARE action space. action space COMPARE state space. state space FEATURE-OF deep RL applications. mapping COMPARE one. one COMPARE mapping. state space CONJUNCTION state space. state space CONJUNCTION state space. state space USED-FOR one. action space FEATURE-OF mapping. Generic is space. ,This paper proposes a method for performing attacks on state observation of an RL agent. The attacks are based on the observation that the state space of an MDP problem can be used as a mapping for an attacking strategy. The paper argues that this mapping is better than the one based on state space or action space in deep RL applications because the space is more expressive.
1283,SP:f7b7dfafb03090a2c940ba738234a6c80bd5ad0e,Interior Policy Differentiation USED-FOR learning diverse policies. Interior Policy Differentiation HYPONYM-OF Interior Point Method. learning diverse policies PART-OF RL. constrained optimization problem USED-FOR policy generation problem. OtherScientificTerm is empirical Wasserstein distance. Generic is it. ,"This paper proposes Interior Policy Differentiation, an Interior Point Method for learning diverse policies in RL. The key idea is to consider the policy generation problem as a constrained optimization problem and to use the empirical Wasserstein distance between the learned policy and the target policy as a proxy for it. "
1284,SP:f7b7dfafb03090a2c940ba738234a6c80bd5ad0e,gradient - free constrained optimization framework USED-FOR generating diverse policies. algorithm USED-FOR diverse policies. computationally light metric USED-FOR policy. policy COMPARE policies. policies COMPARE policy. constrained optimization USED-FOR algorithm. instant feedback USED-FOR algorithm. Wasserstein metric W2 USED-FOR computationally light metric. continuous benchmarks EVALUATE-FOR algorithm. algorithm COMPARE competitors. competitors COMPARE algorithm. IPD HYPONYM-OF algorithm. multi - objective optimization USED-FOR competitors. ,"This paper proposes a gradient-free constrained optimization framework for generating diverse policies. The proposed algorithm is based on constrained optimization with instant feedback, and uses a computationally light metric based on the Wasserstein metric W2 to compare the performance of the policy with other policies. Experiments on continuous benchmarks show that the proposed algorithm, IPD, outperforms competitors that use multi-objective optimization."
1285,SP:f7b7dfafb03090a2c940ba738234a6c80bd5ad0e,"approach USED-FOR policy iteration methods. constraint optimisation COMPARE regularised objective functions. regularised objective functions COMPARE constraint optimisation. Method are algorithm form theoretical principles, and artificial RL. ","This paper proposes an approach to improve policy iteration methods based on algorithm form theoretical principles. The main idea is to use constraint optimisation instead of regularised objective functions. The paper is well-written and well-motivated. The idea is interesting and the paper is clearly written. However, the paper suffers from a lack of comparison to other works in artificial RL."
1286,SP:f7b7dfafb03090a2c940ba738234a6c80bd5ad0e,constrained optimization literature FEATURE-OF interior point method. interior point method USED-FOR policy - seeking algorithm. mujoco tasks EVALUATE-FOR algorithm. Method is learning algorithms. Generic is metric. ,This paper proposes a policy-seeking algorithm based on the interior point method from the constrained optimization literature. The proposed algorithm is evaluated on mujoco tasks and compared to a variety of learning algorithms. The authors also propose a metric to measure the quality of the learned policy.
1287,SP:82b7860146bf3f772bdcd5b448d62136ff6d5177,"audio and video modalities USED-FOR deep neural net - based dereverberation algorithm. visually informed audio dereverberation method USED-FOR clean, anechoic speech. camera USED-FOR visual scene. reverberant speech USED-FOR clean, anechoic speech. 3D simulator USED-FOR real - world scanned environments. 3D simulator CONJUNCTION LibriSpeech data. LibriSpeech data CONJUNCTION 3D simulator. LibriSpeech data USED-FOR audio - visual dataset. 3D simulator USED-FOR audio - visual dataset. visual data CONJUNCTION RGB and depth images. RGB and depth images CONJUNCTION visual data. visual data USED-FOR deep neural networks. losses - one USED-FOR clean speech spectrogram estimation. other CONJUNCTION reverb - visual matching. reverb - visual matching CONJUNCTION other. other HYPONYM-OF losses - one. audio - visual dererberation method COMPARE baseline models. baseline models COMPARE audio - visual dererberation method. downstream tasks USED-FOR speech. downstream tasks EVALUATE-FOR audio - visual dererberation method. synthetic and real - world test data EVALUATE-FOR audio - visual dererberation method. synthetic and real - world test data EVALUATE-FOR baseline models. OtherScientificTerm is room characteristics. Material is clean speech. ","This paper proposes a deep neural net-based dereverberation algorithm for both audio and video modalities. The key idea is to use a visually informed audio dereveration method to generate clean, anechoic speech from reverberant speech. The audio-visual dataset is generated using a 3D simulator for real-world scanned environments and LibriSpeech data, where the camera is trained to capture the visual scene and the room characteristics. The authors propose two losses-one for clean speech spectrogram estimation, and the other for reverb-visual matching. The experiments show that the proposed audio-videural dererberation method outperforms baseline models on both synthetic and real -world test data, and on downstream tasks for speech. In addition, deep neural networks are trained on both visual data as well as RGB and depth images."
1288,SP:82b7860146bf3f772bdcd5b448d62136ff6d5177,"audio - visual approach USED-FOR dereverberation. panoramic HYPONYM-OF RGB and depth images. RGB and depth images USED-FOR dereverberation of speech. dataset USED-FOR task. real - world 3D scans of homes USED-FOR dataset. Librispeech data USED-FOR dataset. "" visual acoustics "" network USED-FOR direct spectrogram prediction. embeddings USED-FOR U - Net. "" visual acoustics "" network USED-FOR embeddings. U - Net USED-FOR model. PESQ CONJUNCTION WER. WER CONJUNCTION PESQ. WER CONJUNCTION EER. EER CONJUNCTION WER. EER EVALUATE-FOR speaker verification. WER EVALUATE-FOR speech recognition. speaker verification EVALUATE-FOR method. synthetic and real data USED-FOR speaker verification. synthetic and real data EVALUATE-FOR method. EER EVALUATE-FOR method. WER EVALUATE-FOR method. PESQ EVALUATE-FOR method. audio - visual method COMPARE model. model COMPARE audio - visual method. audio - only version of the model COMPARE baselines. baselines COMPARE audio - only version of the model. synthetic data EVALUATE-FOR baselines. synthetic data EVALUATE-FOR model. Material are Real data, and real data. ","This paper proposes an audio-visual approach to tackle the problem of dereverberation of speech from RGB and depth images (e.g., panoramic). The task is formulated as a dataset based on real-world 3D scans of homes, and the dataset is built on top of Librispeech data. The model is trained using a ""visual acoustics"" network for direct spectrogram prediction. The method is evaluated on speaker verification on both synthetic and real data (PESQ, WER, and EER for speech recognition). Real data is used to compare with the audio-only version of the model, which outperforms the baselines on synthetic data. On the real data, the proposed method outperforms both PESQ and WER."
1289,SP:82b7860146bf3f772bdcd5b448d62136ff6d5177,synthetic / simulated datasets CONJUNCTION real - world data. real - world data CONJUNCTION synthetic / simulated datasets. recognition CONJUNCTION enhancement. enhancement CONJUNCTION recognition. enhancement CONJUNCTION speaker verification. speaker verification CONJUNCTION enhancement. VIDA model USED-FOR speech tasks. speaker verification HYPONYM-OF speech tasks. recognition HYPONYM-OF speech tasks. enhancement HYPONYM-OF speech tasks. visual information USED-FOR dereverberation. Method is audio - visual dereverberation approach. ,"This paper proposes an audio-visual dereverberation approach. The authors conduct experiments on both synthetic/simulated datasets and real-world data. The VIDA model is applied to a variety of speech tasks including recognition, enhancement, and speaker verification. The visual information is used to guide the deerberation."
1290,SP:82b7860146bf3f772bdcd5b448d62136ff6d5177,"spectrogram U - Net USED-FOR de - reverberated spectrogram. ( panoramic, rgb+depth map ) visual input CONJUNCTION spectrogram U - Net. spectrogram U - Net CONJUNCTION ( panoramic, rgb+depth map ) visual input. spectrogram U - Net USED-FOR clean signal. speech enhancement CONJUNCTION recognition. recognition CONJUNCTION speech enhancement. recognition CONJUNCTION speaker verification problems. speaker verification problems CONJUNCTION recognition. method USED-FOR speech enhancement. recognition EVALUATE-FOR method. speaker verification problems EVALUATE-FOR method. synthetic and real data USED-FOR method. synthetic and real data USED-FOR speaker verification problems. speech recognition CONJUNCTION speaker verification. speaker verification CONJUNCTION speech recognition. method COMPARE speech enhancement. speech enhancement COMPARE method. PESQ EVALUATE-FOR speech enhancement. speaker verification EVALUATE-FOR method. speech recognition EVALUATE-FOR method. PESQ EVALUATE-FOR method. Method are audio de - reverberation method, and quantitative error analysis. ","This paper proposes an audio de-reinforcement method. The key idea is to combine the (panoramic, rgb+depth map) visual input with the spectrogram U-Net to generate a clean signal. The method is evaluated for speech enhancement, recognition, and speaker verification problems on both synthetic and real data. The quantitative error analysis is provided. The proposed method is compared with speech enhancement and speech verification on PESQ."
1291,SP:7abf578ef0d3b67d87437bdd1cff129f72c102c6,"approach USED-FOR pre - trained transformer - based language models. method USED-FOR pre - trained models. Metric is training time. OtherScientificTerm are extrapolation, and dot - product values. ",This paper proposes an approach to improve the performance of pre-trained transformer-based language models. The main idea is to reduce the training time by minimizing the extrapolation of the dot-product values. The proposed method can be applied to pre-trained models.
1292,SP:7abf578ef0d3b67d87437bdd1cff129f72c102c6,sinusoidal embedding CONJUNCTION relative positional embedding. relative positional embedding CONJUNCTION sinusoidal embedding. positional encoding methods USED-FOR language modeling tasks. relative positional embedding HYPONYM-OF positional encoding methods. sinusoidal embedding HYPONYM-OF positional encoding methods. temporal bias FEATURE-OF multi - head attention. token distances FEATURE-OF attention score. ALiBi HYPONYM-OF positional encoding method. ALiBi COMPARE positional encoding methods. positional encoding methods COMPARE ALiBi. extrapolation capability EVALUATE-FOR positional encoding methods. extrapolation capability EVALUATE-FOR ALiBi. Method is transformer - based language models. ,"This paper studies the problem of multi-head attention with temporal bias in transformer-based language models. The authors propose two positional encoding methods for language modeling tasks: sinusoidal embedding and relative positional embedding. In particular, the authors propose ALiBi, a new positional encoding method that is based on the idea that the attention score should be proportional to the token distances between the tokens. The experiments show that ALiBIE improves the extrapolation capability over the existing positional encoder and decoder methods."
1293,SP:7abf578ef0d3b67d87437bdd1cff129f72c102c6,"input length extrapolation USED-FOR Transformer language models. sinusoidal and rotary position embeddings USED-FOR Models. ALiBi HYPONYM-OF attention mechanism. Method is Transformer LMs. OtherScientificTerm are T5 bias, and extrapolation. ","This paper studies the problem of input length extrapolation in Transformer language models. Models are trained with sinusoidal and rotary position embeddings, and the authors show that Transformer LMs are prone to T5 bias. The authors propose an attention mechanism, ALiBi, to address this issue, and show that the extrapolation is effective."
1294,SP:7abf578ef0d3b67d87437bdd1cff129f72c102c6,ALiBi COMPARE transformer. transformer COMPARE ALiBi. input length extrapolation ability EVALUATE-FOR ALiBi. ALiBi USED-FOR billion scale language model. Task is extrapolation problem. Method is Attention with Linear Biases ( ALiBi ). OtherScientificTerm is attention scores. ,"This paper studies the extrapolation problem and proposes Attention with Linear Biases (ALiBi), which is an extension of attention scores. The authors show that ALiBi improves the input length extrapolation ability compared to the transformer. The main contribution of the paper is to propose ALiBI for a billion scale language model."
1295,SP:45ba2709aca444f50a133d71f33be9d2c1f887e8,"learning algorithm USED-FOR cumulative regret. Pareto regret EVALUATE-FOR learning algorithm. Pareto Suboptimality Gap ( PSG ) USED-FOR instantaneous regret. vector - valued loss function USED-FOR full - information setting. Online Mirror Descent ( OMD ) paradigm USED-FOR multi - objective OMD. benchmarks EVALUATE-FOR versions. Method are online learning model, and multi - objective online learning. Metric is cumulative losses. Generic are algorithm, and static version. OtherScientificTerm is feedback. ","This paper studies the problem of learning an online learning model that minimizes cumulative losses. The authors propose a learning algorithm that maximizes the Pareto regret of the cumulative regret. The algorithm is based on the idea that the instantaneous regret can be approximated by the so-called ""Pareto Suboptimality Gap (PSG)"". The authors also propose a vector-valued loss function for the full-information setting, which is similar to the one used in multi-objective online learning under the Online Mirror Descent (OMD) paradigm. Finally, the authors propose two versions of the algorithm. The first version is a static version, where there is no feedback, and the second version is an online version where the feedback is provided to the learner. Both versions are evaluated on three benchmarks."
1296,SP:45ba2709aca444f50a133d71f33be9d2c1f887e8,"framework USED-FOR multi - objective online convex optimization. framework COMPARE single - objective online convex optimization framework. single - objective online convex optimization framework COMPARE framework. two players repeated game USED-FOR framework. regret USED-FOR multi - objective setting. OMMD - I CONJUNCTION OMMD - II. OMMD - II CONJUNCTION OMMD - I. algorithms USED-FOR dynamic regret. multi - objective setting FEATURE-OF dynamic regret. OMMD - I HYPONYM-OF algorithms. OMMD - II HYPONYM-OF algorithms. min - norm method USED-FOR offline multi - objective optimization. min - norm method USED-FOR algorithms. convex combination of the descent directions USED-FOR composite gradient. composite gradient USED-FOR descent direction. Bregman reguralization USED-FOR descent direction. composite gradient CONJUNCTION Bregman reguralization. Bregman reguralization CONJUNCTION composite gradient. OMMD - I COMPARE OMMD - II. OMMD - II COMPARE OMMD - I. OMMD - II HYPONYM-OF algorithms. OMMD - I HYPONYM-OF algorithms. OtherScientificTerm are vector valued loss function, Pareto suboptimality gap ( PSG ), and regularization term. Generic is latter. ","This paper proposes a framework for multi-objective online convex optimization based on a two players repeated game. The proposed framework is similar to the single-agreement online conveax optimization framework, but with a vector valued loss function instead of the Pareto suboptimality gap (PSG) of the latter. The authors propose two algorithms, OMMD-I (based on the min-norm method for offline multi-optimality optimization) and OMMMD-II (which is based on the Bregman reguralization), to approximate the regret in the multi-aimer setting. The former uses a composite gradient that is a convex combination of the descent directions, while the latter uses a BREGMAN reguralized version of the composite gradient. The latter adds a regularization term that encourages the gradient to converge to a fixed point. The algorithms are shown to be able to approximate dynamic regret in a multi-agent setting. Experiments are conducted on two datasets to show the effectiveness of the proposed algorithms."
1297,SP:45ba2709aca444f50a133d71f33be9d2c1f887e8,PSG USED-FOR performance metric. dynamic regret FEATURE-OF it. sublinear multi - objective regret FEATURE-OF algorithms. Task is multi - objective online convex optimization. Generic is methods. ,"This paper studies the problem of multi-objective online convex optimization. The authors propose a new performance metric based on PSG, and show that it has a dynamic regret. They also show that existing algorithms have sublinear multi-agreement regret. Finally, they provide some theoretical analysis of the proposed methods."
1298,SP:45ba2709aca444f50a133d71f33be9d2c1f887e8,algorithm USED-FOR problem. convex combination USED-FOR mirror descent algorithm. gradient FEATURE-OF objective functions. algorithm USED-FOR convex combination. gradient USED-FOR algorithm. gradients USED-FOR convex combination. algorithm COMPARE Pareto optimal points. Pareto optimal points COMPARE algorithm. dynamic regret EVALUATE-FOR algorithm. Task is multi - objective optimization. OtherScientificTerm is loss vector. ,This paper studies the problem of multi-objective optimization. The authors propose an algorithm to solve this problem that uses the gradient of the objective functions to approximate the convex combination of the mirror descent algorithm. The gradients are then used to approximate a convex version of the loss vector. The proposed algorithm is shown to converge to Pareto optimal points with a dynamic regret of O(1/\sqrt{T})$.
1299,SP:d07fd26d0cb245e1fd1343472dd3c8300c39752a,"generative replay USED-FOR negative samples. approach COMPARE positive replay. positive replay COMPARE approach. approach COMPARE no - replay baselines. no - replay baselines COMPARE approach. positive replay CONJUNCTION negative replay. negative replay CONJUNCTION positive replay. negative replay CONJUNCTION no - replay baselines. no - replay baselines CONJUNCTION negative replay. approach COMPARE negative replay. negative replay COMPARE approach. NC and NIC scenarios FEATURE-OF complex datasets. complex datasets EVALUATE-FOR no - replay baselines. complex datasets EVALUATE-FOR approach. Task are continual learning, and learning in isolation "" problems. Method are generative model, and low resource generative model. OtherScientificTerm is positive sample. ","This paper addresses continual learning in the context of ""learning in isolation"" problems, where the goal is to learn a generative model that can generalize to unseen samples. The authors propose to use a low resource generative Model to generate the negative samples, and then use generative replay to generate negative samples. They show that their approach outperforms both positive replay and negative replay as well as no-replay baselines on complex datasets from NC and NIC scenarios. They also show that the negative sample is more informative than the positive sample."
1300,SP:d07fd26d0cb245e1fd1343472dd3c8300c39752a,"privacy issues CONJUNCTION storage overhead. storage overhead CONJUNCTION privacy issues. Task are Continual Learning ( CL ) scenarios, generation of high dimensionality data, and CWR. OtherScientificTerm is catastrophic forgetting. Method are replay methods, generative models, generating model, Generative Negative Replay, and classification head. Generic are methods, it, and solution. ","This paper studies Continual Learning (CL) scenarios where the generation of high dimensionality data is difficult due to privacy issues and storage overhead. In particular, the paper focuses on the problem of catastrophic forgetting and proposes two replay methods to address this issue. The first method, Generative Negative Replay, is based on the observation that existing generative models are prone to catastrophic forgetting. The second method is a variant of Generative Positive Replay, where the generating model is trained with a separate classification head. Both methods are shown to be effective, but it is not clear how the proposed solution can be applied to CWR."
1301,SP:d07fd26d0cb245e1fd1343472dd3c8300c39752a,learning scheme USED-FOR generative replay methods. generative replay methods USED-FOR continual learning. CORe50 CONJUNCTION ImageNet-1000. ImageNet-1000 CONJUNCTION CORe50. ImageNet-1000 EVALUATE-FOR method. CORe50 EVALUATE-FOR method. Method is generative replay method. ,This paper proposes a learning scheme to improve generative replay methods for continual learning. The proposed method is evaluated on CORe50 and ImageNet-1000. The experimental results show the effectiveness of the proposed method.
1302,SP:d07fd26d0cb245e1fd1343472dd3c8300c39752a,method USED-FOR generative replay. replayed data USED-FOR method. method USED-FOR model. ImageNet data USED-FOR model. latent space USED-FOR replay process. Metric is generated data quality. ,"This paper proposes a method for generative replay based on replayed data. The proposed method trains a model on ImageNet data, where the generated data quality is evaluated. The replay process is performed in a latent space."
1303,SP:ec18e1450dd918b1ca95e301bc9262e072d77b52,"clustering USED-FOR inductive graph partitioning. Material is time - evolving graph. OtherScientificTerm are node correspondence, and nodes. Method are incremental or evolutionary clustering algorithms, dual graph neural network ( GNN ) architecture, and simulated and real networks. Metric are clustering accuracy, and computation time. ","This paper studies the problem of clustering in inductive graph partitioning, where the goal is to partition a time-evolving graph. The authors propose incremental or evolutionary clustering algorithms, where each node is partitioned based on its node correspondence with the other nodes. The proposed dual graph neural network (GNN) architecture is evaluated on simulated and real networks, and the clustering accuracy is shown to improve with respect to the computation time."
1304,SP:ec18e1450dd918b1ca95e301bc9262e072d77b52,inductive graph partitioning framework USED-FOR NP - hard challenge. evolving graph snapshots USED-FOR inductive graph partitioning framework. dual graph neural network USED-FOR system. historical snapshots USED-FOR dual graph neural network. model USED-FOR online GP. quality CONJUNCTION efficiency. efficiency CONJUNCTION quality. OtherScientificTerm is optimization. ,"This paper proposes an inductive graph partitioning framework based on evolving graph snapshots to solve the NP-hard challenge. The system is trained using a dual graph neural network that takes historical snapshots as input and outputs the output of the system. The model is then used for online GP, where the optimization is performed in a supervised fashion. The authors show that the model can achieve state-of-the-art results in terms of quality and efficiency."
1305,SP:ec18e1450dd918b1ca95e301bc9262e072d77b52,"algorithm USED-FOR GP problem. graphs USED-FOR GP problem. efficiency CONJUNCTION quality. quality CONJUNCTION efficiency. NN architecture USED-FOR embedding. matrix multiplication USED-FOR embedding. embedding USED-FOR solution. matrix multiplication USED-FOR solution. simulated and real - world datasets EVALUATE-FOR method. Task are graph partitioning problem, and NP - hard problem. OtherScientificTerm are Graphs, unknown but fixed distribution, and i.i.d. distribution. ","This paper proposes an algorithm to solve the GP problem on graphs. The graph partitioning problem is NP-hard problem. Graphs are assumed to have an unknown but fixed distribution. The solution is based on a matrix multiplication of the embedding of the NN architecture, and the i.i.d. distribution is assumed to be fixed. The proposed method is evaluated on both simulated and real-world datasets, showing improvements in terms of efficiency and quality."
1306,SP:ec18e1450dd918b1ca95e301bc9262e072d77b52,"inductive framework USED-FOR graph partitioning. transductive   algorithms USED-FOR graph partitioning. normalized cut and modularity objectives USED-FOR framework. unsupervised communities CONJUNCTION spectral algorithms. spectral algorithms CONJUNCTION unsupervised communities. modularity optimization CONJUNCTION spectral algorithms. spectral algorithms CONJUNCTION modularity optimization. modularity optimization USED-FOR unsupervised communities. dual GNN structure USED-FOR spectral algorithms. Metric is speed. OtherScientificTerm is communities. Generic are them, and network. ","This paper proposes an inductive framework for graph partitioning using transductive  algorithms. The framework is based on normalized cut and modularity objectives. The authors show that modularity optimization and spectral algorithms based on dual GNN structure can be used for unsupervised communities. The speed of these communities is improved when the number of nodes in them is small, and the performance of the network is improved as well."
1307,SP:ad28c185efd966eea1f44a6ff474900812b4705a,"variational - autoencoder - based model USED-FOR learning and generating graph structures. multiresolution graph network ( MGN ) USED-FOR graph. nodes PART-OF coarsened graphs. hierarchical variational autoencoders USED-FOR they. generative models USED-FOR generating graphs. MG - VAEs USED-FOR generative models. multiresolution and equivariant manner FEATURE-OF generative models. multiresolution and equivariant manner FEATURE-OF generating graphs. generation of molecules CONJUNCTION graph - based image generation. graph - based image generation CONJUNCTION generation of molecules. unsupervised representation learning CONJUNCTION over link prediction. over link prediction CONJUNCTION unsupervised representation learning. over link prediction CONJUNCTION generation of molecules. generation of molecules CONJUNCTION over link prediction. citation graphs USED-FOR over link prediction. settings EVALUATE-FOR models. graph - based image generation HYPONYM-OF settings. generation of molecules HYPONYM-OF settings. unsupervised representation learning HYPONYM-OF settings. over link prediction HYPONYM-OF settings. Method are variational auto - encoder, and MGNs. ","This paper proposes a variational-autoencoder-based model for learning and generating graph structures. The main idea is to learn a multiresolution graph network (MGN) for each node of a graph, which is then fed into a variular auto-encoder. The authors argue that generative models such as MG-VAEs can be used for generating graphs in a multiregressive and equivariant manner, as they can be viewed as hierarchical variational autoencoders. Experiments are conducted on three settings: unsupervised representation learning, over link prediction on citation graphs, and generation of molecules and graph-based image generation. The results show that MGNs are able to generate graphs that are both multiresolved and equivivariant."
1308,SP:ad28c185efd966eea1f44a6ff474900812b4705a,hierarchical graph coarsening USED-FOR multi - scale graph VAE. permutation equivariant tensor operations USED-FOR group equivariance network. learnable hard partition USED-FOR clustering. Gumbel - max trick USED-FOR learnable hard partition. molecular generation CONJUNCTION molecular representation learning. molecular representation learning CONJUNCTION molecular generation. graph generation CONJUNCTION molecular generation. molecular generation CONJUNCTION graph generation. molecular representation learning CONJUNCTION link prediction. link prediction CONJUNCTION molecular representation learning. link prediction CONJUNCTION graph - based image generation. graph - based image generation CONJUNCTION link prediction. framework USED-FOR molecular generation. framework USED-FOR graph generation. framework USED-FOR graph - based image generation. framework USED-FOR molecular representation learning. framework USED-FOR graph generation community. ,"This paper proposes a multi-scale graph VAE based on hierarchical graph coarsening. The group equivariance network is trained with permutation equivariant tensor operations. A learnable hard partition for clustering is proposed based on Gumbel-max trick. The proposed framework is applied to graph generation, molecular generation and molecular representation learning, link prediction, and graph-based image generation. The experimental results show the effectiveness of the proposed framework in the graph generation community."
1309,SP:ad28c185efd966eea1f44a6ff474900812b4705a,"hierarchical structure USED-FOR encoder. hierarchical structure USED-FOR balanced partitions of the graph. differentiable graph coarsening USED-FOR hierarchical structure. molecular generation CONJUNCTION community graph generation. community graph generation CONJUNCTION molecular generation. community graph generation CONJUNCTION citation network generation. citation network generation CONJUNCTION community graph generation. citation network generation EVALUATE-FOR method. community graph generation EVALUATE-FOR method. molecular generation EVALUATE-FOR method. unsupervised and supervised molecular property predictions CONJUNCTION graph - based image generation. graph - based image generation CONJUNCTION unsupervised and supervised molecular property predictions. link prediction CONJUNCTION unsupervised and supervised molecular property predictions. unsupervised and supervised molecular property predictions CONJUNCTION link prediction. Method are multiresolution graph auto - encoder framework, and decoder. OtherScientificTerm are node permutation, graph, local adjacency matrices, and adjacency matrix. Task is multi - scale graph generation. ","This paper proposes a multiresolution graph auto-encoder framework. The key idea is to learn a hierarchical structure for the encoder based on differentiable graph coarsening to learn balanced partitions of the graph. The hierarchical structure is learned by minimizing the node permutation of each node in the graph, and the local adjacency matrices are learned to minimize the difference between the permutation matrix of the node and that of the neighboring node. The decoder is trained on the learned structure. The proposed method is evaluated on molecular generation, community graph generation, and citation network generation. Results are shown on link prediction, unsupervised and supervised molecular property predictions, and graph-based image generation. The paper is well-written and well-motivated. The idea of multi-scale graph generation is interesting."
1310,SP:ad28c185efd966eea1f44a6ff474900812b4705a,"Higher order message passing USED-FOR graph. node ordering FEATURE-OF end - to - end permutation equivariant. end - to - end permutation equivariant FEATURE-OF model. generative tasks CONJUNCTION graph link prediction. graph link prediction CONJUNCTION generative tasks. graph link prediction EVALUATE-FOR MGVAE. generative tasks EVALUATE-FOR MGVAE. OtherScientificTerm are graphs, and hierarchy of latent distributions. ",This paper proposes a model that is end-to-end permutation equivariant with respect to node ordering. Higher order message passing is used to represent the graph as a hierarchy of latent distributions. The authors show that the proposed MGVAE achieves state-of-the-art performance on several generative tasks and graph link prediction.
1311,SP:29e2e1daa6a32ef71ad225bd2fc27e33dece86c5,framework USED-FOR nonlinear ICA. volume preserving transformation USED-FOR mixing function. flow USED-FOR mixing function. nonlinear ica USED-FOR Disentanglement. incompressible - flow networks ( gin ) USED-FOR nonlinear ica. incompressible - flow networks ( gin ) USED-FOR Disentanglement. flow map USED-FOR real NVP model. Gaussian likelihood FEATURE-OF reconstruction sources. Gaussian likelihood USED-FOR mixing function. framework USED-FOR sources of generated data. framework COMPARE baseline. baseline COMPARE framework. method COMPARE iVAE. iVAE COMPARE method. framework USED-FOR MNIST. framework USED-FOR source variables. Generic is restriction. Task is identifiability. ,"This paper proposes a framework for nonlinear ICA. Disentanglement is achieved by using incompressible-flow networks (gin) to approximate the nonlinear ica. The flow is used as a mixing function with volume preserving transformation. The authors show that the flow map can be used to approximate a real NVP model. The mixing function is parameterized by a Gaussian likelihood of the reconstruction sources. The proposed framework can be applied to different sources of generated data, and the proposed method is compared to iVAE on MNIST. The paper shows that the proposed framework is able to learn source variables that satisfy a restriction on identifiability."
1312,SP:29e2e1daa6a32ef71ad225bd2fc27e33dece86c5,"volume preserving mixing functions USED-FOR nonlinear ICA. maximum likelihood optimization USED-FOR ICA model. latent distribution USED-FOR ICA model. factorized Gaussian HYPONYM-OF latent distribution. volume - preserving encoder USED-FOR ICA model. procedure USED-FOR theoretical identifiability analysis. permutation CONJUNCTION dimension - wise diffeomorphism. dimension - wise diffeomorphism CONJUNCTION permutation. triangle images CONJUNCTION MNIST. MNIST CONJUNCTION triangle images. 2D mixture of Gaussians CONJUNCTION triangle images. triangle images CONJUNCTION 2D mixture of Gaussians. 2D mixture of Gaussians HYPONYM-OF experiments. MNIST HYPONYM-OF experiments. triangle images HYPONYM-OF experiments. OtherScientificTerm are latent variables, independent distribution, observed u variables, and encoded latent variables. Method are volume - preserving mixing function, and mixer. Task are source identification, and identification. Generic is datasets. ","This paper studies the problem of nonlinear ICA with volume preserving mixing functions. The ICA model is trained by maximum likelihood optimization, where the latent distribution is a factorized Gaussian, and the volume-preserving encoder is used to encode the latent variables into an independent distribution. The authors propose a procedure for theoretical identifiability analysis, which is motivated by the fact that source identification is difficult due to the large number of observed u variables. To address this issue, the authors propose to use a volume -preserving mixing function, which can be viewed as a permutation or a dimension-wise diffeomorphism of the original mixer. Experiments are conducted on three different datasets, including a 2D mixture of Gaussians, triangle images, and MNIST. The results show that the encoded latent variables can be used for identification."
1313,SP:29e2e1daa6a32ef71ad225bd2fc27e33dece86c5,framework USED-FOR nonlinear ICA. volume - preserving transformation USED-FOR mixing function. volume - preserving Flow - based models USED-FOR framework. Material is synthetic and real data. ,This paper proposes a framework for nonlinear ICA based on volume-preserving Flow-based models. The main idea is to use a volume -preserving transformation to the mixing function. Experiments are conducted on both synthetic and real data.
1314,SP:29e2e1daa6a32ef71ad225bd2fc27e33dece86c5,"volume preserving transformation USED-FOR disentanglement problem. disentanglement problem USED-FOR latent variable models. volume preserving transformation USED-FOR latent variable models. ICA HYPONYM-OF latent variable models. factorial member of the exponential family USED-FOR ICA. volume - preserving transformation USED-FOR identifiability. volume - preserving transformation USED-FOR normalizing flows. mixed signal USED-FOR generative process. volume preserving transformation USED-FOR latent space. factorial multivariaate Gaussian USED-FOR conditional independence. Task are disentanglement, and ICA problem setup. ","This paper proposes a volume preserving transformation to solve the disentanglement problem for latent variable models such as ICA, which is a factorial member of the exponential family. The volume-preserving transformation is used to enforce identifiability and to regularize normalizing flows. The main contribution of this paper is to study the problem in the context of latent space, and to show that the mixed signal in the generative process can be viewed as a function of the latent space. The conditional independence is achieved by the factorial multivariaate Gaussian. The paper also provides a theoretical analysis of the ICA problem setup."
1315,SP:288ce587a277299765bdd4cea75a8c23e12de2b0,adaptive filter USED-FOR projection. BankGCN HYPONYM-OF convolutional operator. projection functions USED-FOR it. Linear projections USED-FOR projection functions. Graph classification task EVALUATE-FOR model. OtherScientificTerm is node input signal. ,"This paper proposes an adaptive filter for projection. The authors propose a convolutional operator called BankGCN, which takes the node input signal as input and outputs a projection. Linear projections are used for the projection functions. The model is evaluated on Graph classification task."
1316,SP:288ce587a277299765bdd4cea75a8c23e12de2b0,BankGCN HYPONYM-OF graph convolutional network. graph convolutional network USED-FOR ( chebyshev ) polynomial filters. graph USED-FOR ( chebyshev ) polynomial filters. subspace projection CONJUNCTION cosine similarity regularization. cosine similarity regularization CONJUNCTION subspace projection. subspace projection PART-OF BankGCN. cosine similarity regularization PART-OF BankGCN. BankGCN COMPARE graph networks. graph networks COMPARE BankGCN. whole graph classification tasks EVALUATE-FOR graph networks. whole graph classification tasks EVALUATE-FOR BankGCN. architectures USED-FOR MPGCNs. Generic is they. OtherScientificTerm is low frequency information. ,"This paper proposes BankGCN, a graph convolutional network for (chebyshev) polynomial filters on a graph. The main contribution of the paper is the introduction of a subspace projection and cosine similarity regularization to the BankGCNN, which is an extension of the existing graph Convolutional Neural Network (Corollary 1). The authors compare the performance of the proposed bankGCN with the state-of-the-art graph networks on whole graph classification tasks and show that they are able to capture low frequency information. The authors also show that the proposed architectures can be used to train MPGCNs."
1317,SP:288ce587a277299765bdd4cea75a8c23e12de2b0,BankGCN USED-FOR spectral graph neural networks. adaptive filter bank USED-FOR GCNs. low - pass features FEATURE-OF GCNs. adaptive filter bank USED-FOR BankGCN. spectral graph convolutional networks COMPARE method. method COMPARE spectral graph convolutional networks. free parameters FEATURE-OF spectral graph convolutional networks. graph classification task EVALUATE-FOR BankGCN. ,This paper proposes BankGCN for spectral graph neural networks. The main idea is to use an adaptive filter bank to train GCNs with low-pass features. The experimental results on graph classification task show that the proposed spectral graph convolutional networks with free parameters outperform the proposed method.
1318,SP:288ce587a277299765bdd4cea75a8c23e12de2b0,BankGCN HYPONYM-OF GCN model. graph convolutional operator USED-FOR BankGCN. graph convolutional operator USED-FOR GCN model. BankGCN COMPARE learnable message passing mechanism. learnable message passing mechanism COMPARE BankGCN. K - hop neighborhood FEATURE-OF learnable message passing mechanism. BankGCN COMPARE spectral - based GNNs. spectral - based GNNs COMPARE BankGCN. graph classification tasks EVALUATE-FOR BankGCN. graph benchmark datasets USED-FOR graph classification tasks. OtherScientificTerm is filters. ,"This paper proposes BankGCN, a GCN model that uses a graph convolutional operator. The main idea is to use different filters for each node in the graph. The authors show that the learned graph filters can be used to improve the performance of the proposed bankGCN compared to a learnable message passing mechanism in the K-hop neighborhood. The experimental results on several graph classification tasks using standard graph benchmark datasets show that BankGCNN outperforms spectral-based GNNs."
1319,SP:3abcd0700eaf11d964c280d996a1dd4f34280b1c,leave - one - out kNN supervised method USED-FOR pre - training on large - scale datasets. cosine distance CONJUNCTION softmax normalization. softmax normalization CONJUNCTION cosine distance. softmax normalization USED-FOR loss of weighted soft kNN loss. loss of weighted soft kNN loss USED-FOR method. cosine distance USED-FOR loss of weighted soft kNN loss. memory queue USED-FOR feature bank. First - in - first - out USED-FOR memory queue. memory queue USED-FOR features. kNN USED-FOR large search space and feature update problem. memory queue USED-FOR MoCo. First - in - first - out USED-FOR features. MLP USED-FOR features. MLP COMPARE linear layer. linear layer COMPARE MLP. ImageNet CONJUNCTION fine - grained datasets. fine - grained datasets CONJUNCTION ImageNet. up - stream pre - training dataset CONJUNCTION fine - grained datasets. fine - grained datasets CONJUNCTION up - stream pre - training dataset. fine - grained datasets CONJUNCTION down - stream fine - tuning datasets. down - stream fine - tuning datasets CONJUNCTION fine - grained datasets. ImageNet CONJUNCTION up - stream pre - training dataset. up - stream pre - training dataset CONJUNCTION ImageNet. ImageNet USED-FOR fine - tuning setting. fine - grained datasets EVALUATE-FOR proposed. ImageNet EVALUATE-FOR proposed. fine - tuning setting EVALUATE-FOR proposed. method COMPARE supervised and self - supervised methods. supervised and self - supervised methods COMPARE method. Generic is model. Method is SimCLR. ,"This paper proposes a leave-one-out kNN supervised method for pre-training on large-scale datasets. The proposed method is based on a loss of weighted soft kNN loss with cosine distance and softmax normalization. First-in-first-out is used to create a memory queue for the feature bank, which is then used to train the model. The main contribution of this paper is the use of kNN to address the large search space and feature update problem. The authors also propose a variant of SimCLR, which uses a separate memory queue to train MoCo. Experiments on ImageNet, fine-grained datasets, and down-stream fine-tuning datasets show that the proposed method outperforms both supervised and self-supervised methods. In addition, the authors also show that using an MLP instead of a linear layer can improve the performance."
1320,SP:3abcd0700eaf11d964c280d996a1dd4f34280b1c,supervised method USED-FOR visual pretraining. human labels USED-FOR supervised method. method USED-FOR sub - clusters. contrastive learning COMPARE approach. approach COMPARE contrastive learning. supervised baselines COMPARE approach. approach COMPARE supervised baselines. contrastive learning CONJUNCTION supervised baselines. supervised baselines CONJUNCTION contrastive learning. car / flower / pets classifications FEATURE-OF downstream tasks. downstream tasks EVALUATE-FOR approach. car / flower / pets classifications EVALUATE-FOR approach. OtherScientificTerm is intra - class instances. ,This paper proposes a supervised method for visual pretraining based on human labels. The method learns sub-clusters that are more likely to contain intra-class instances. The proposed approach is compared with contrastive learning and supervised baselines on downstream tasks on car/flower/pets classifications.
1321,SP:3abcd0700eaf11d964c280d996a1dd4f34280b1c,"self - supervised pre - training USED-FOR supervised pre - training. instance discrimination USED-FOR supervised pre - training. approach USED-FOR self - supervised pre - training. instance discrimination USED-FOR self - supervised pre - training. MoCo memory bank USED-FOR KNN lookup. transfer learning USED-FOR downstream tasks. OtherScientificTerm are weight vector, and qualitative visualizations. Method is direct, supervised learning. ","This paper proposes an approach to self-supervised pre-training for supervised pre-train using instance discrimination. The key idea is to use the MoCo memory bank for KNN lookup, and then use the weight vector from MoCo as a proxy for direct, supervised learning. Experiments are conducted on qualitative visualizations, and transfer learning is performed on downstream tasks."
1322,SP:3abcd0700eaf11d964c280d996a1dd4f34280b1c,LOOK HYPONYM-OF supervised pre - training method. supervised pre - training method USED-FOR intra - class semantic differences. KNN classifier USED-FOR pre - training task. linear classifier USED-FOR pre - training task. KNN classifier COMPARE linear classifier. linear classifier COMPARE KNN classifier. LOOK USED-FOR pre - training task. KNN classifier USED-FOR LOOK. LOOK COMPARE supervised and unsupervised pre - training methods. supervised and unsupervised pre - training methods COMPARE LOOK. ,"This paper proposes LOOK, a supervised pre-training method that aims to detect intra-class semantic differences. Look uses a KNN classifier instead of a linear classifier to solve the pre-train task. The experimental results show that LOOK outperforms both supervised and unsupervised pre - training methods."
1323,SP:2b3916ba24094c286117126e11032820f8c7c50a,"pipeline USED-FOR geometrical deformations. expressions FEATURE-OF geometrical deformations. direct expression parameters CONJUNCTION action units. action units CONJUNCTION direct expression parameters. direct expression parameters COMPARE latent code. latent code COMPARE direct expression parameters. action units COMPARE latent code. latent code COMPARE action units. expression USED-FOR deformation. 3D face USED-FOR deformation. texture map CONJUNCTION geometry. geometry CONJUNCTION texture map. single image based 3D face deformation method USED-FOR texture map. displacement map USED-FOR 3D face. single image based 3D face deformation method USED-FOR geometry. displacement map USED-FOR texture map. displacement map USED-FOR geometry. Basel Face Model USED-FOR displacement map. image USED-FOR displacement map. Basel Face Model USED-FOR 3D face. adversarial loss USED-FOR it. displacement map USED-FOR AU. Action Units(AU ) USED-FOR deformation. texture map USED-FOR Neural Texture. method USED-FOR rendering. Neural Texture USED-FOR method. Neural Texture USED-FOR rendering. components PART-OF rendering. coarse level rendering CONJUNCTION detail rendering. detail rendering CONJUNCTION coarse level rendering. method USED-FOR AU. Basel Face model USED-FOR coarse level rendering. shape and expression parameters USED-FOR coarse level rendering. DECA ( Siggraph 21 ) HYPONYM-OF FLAME 3D model based method. identity, shape and expression rendering EVALUATE-FOR method. Method are single image based 3D reconstruction, and reconstruction methods. OtherScientificTerm is appearance. ","This paper proposes a pipeline to generate geometrical deformations of a 3D face based on expressions. The main idea is to use a single image based 3D reconstruction of the face as the input for the pipeline. Instead of using direct expression parameters or action units as in the latent code, the paper proposes to use the displacement map of the original image as the texture map and geometry as the output for the 3D model. The displacement map is generated using the Basel Face Model and it is trained with an adversarial loss. The paper then proposes a method called Action Units(AU) to generate deformation from the expression to the geometry. The proposed method is based on Neural Texture, which uses a texture map as input and a displacement map as output to generate the texture and geometry for the rendering. The rendering consists of three components: coarse level rendering, detail rendering, and shape and expression rendering. Experiments are performed on DECA (Siggraph 21) which is a FLAME3D model based method, and show that the proposed method can produce better results for the task of AU. The method is also evaluated on identity, shape, expression rendering and is compared to other reconstruction methods."
1324,SP:2b3916ba24094c286117126e11032820f8c7c50a,"FaceDet3D USED-FOR facial expressions. Augmented Wrinkle Loss CONJUNCTION Detailed Shading Loss. Detailed Shading Loss CONJUNCTION Augmented Wrinkle Loss. Photometric Loss CONJUNCTION Expression Adversarial Loss. Expression Adversarial Loss CONJUNCTION Photometric Loss. FID CONJUNCTION FaceID. FaceID CONJUNCTION FID. DECA USED-FOR FaceID. large margin USED-FOR DECA. large margin USED-FOR FaceID. Auxiliary Classifier GANs USED-FOR Conditional Image Synthesis. Method are Detail Hallucination Network, and Rendering Network. OtherScientificTerm are Superresolution Losses, and wrinkles. ","This paper proposes FaceDet3D for learning facial expressions. Specifically, the paper proposes a Detail Hallucination Network, which consists of three main components: (1) Augmented Wrinkle Loss, (2) Detailed Shading Loss, and (3) Photometric Loss and (4) Expression Adversarial Loss. Superresolution Losses are used to reduce the number of pixels in the original image. The Rendering Network is used to reconstruct the original images. The Conditional Image Synthesis is performed with Auxiliary Classifier GANs. Experiments on FID, FaceID, and FaceID with DECA with large margin are conducted to show that DECA is able to reconstruct FaceID. The paper also conducts an ablation study on the effect of different types of wrinkles."
1325,SP:2b3916ba24094c286117126e11032820f8c7c50a,"OtherScientificTerm are 3D reconstructed faces, facial expression, and generative losses. Method is numerical evaluations. ",This paper studies the problem of learning 3D reconstructed faces from 3D images. The goal is to learn the facial expression of a human subject. The authors propose to use generative losses to estimate the similarity between the facial expressions of the human subject and the objects in the scene. They also propose numerical evaluations to evaluate the quality of the reconstructed faces.
1326,SP:2b3916ba24094c286117126e11032820f8c7c50a,FaceDet3D USED-FOR geometric facial details. rendering network USED-FOR detailed rendering result. Detail Hallucination Network USED-FOR geometric facial details. 3D face geometry information USED-FOR rendering network. large scale in - the - wild images CONJUNCTION small video dataset. small video dataset CONJUNCTION large scale in - the - wild images. large scale in - the - wild images USED-FOR two component models. small video dataset USED-FOR two component models. Augmented Wrinkle Loss CONJUNCTION Detailed Shading Loss. Detailed Shading Loss CONJUNCTION Augmented Wrinkle Loss. Augmented Wrinkle Loss USED-FOR rendering network. Detailed Shading Loss USED-FOR rendering network. it COMPARE DECA. DECA COMPARE it. DECA COMPARE method. method COMPARE DECA. components PART-OF method. ,"This paper proposes FaceDet3D, a new rendering network based on 3D face geometry information that can extract geometric facial details from a Detail Hallucination Network. The two component models are trained on large scale in-the-wild images and a small video dataset. The rendering network is trained with Augmented Wrinkle Loss and Detailed Shading Loss to obtain a detailed rendering result. The proposed method consists of three components: 1) a rendering network that is trained on the original 3D images, 2) a separate rendering network trained on a smaller video dataset, and 3) a final rendering network which is trained based on the combination of the AugmentedWrinkle Loss (which is the same as DECA). The experiments show that it outperforms DECA and that the proposed method is more expressive than DECA."
1327,SP:c7c50c44fdafb15b962e04d713cac309e57bc54b,latent variables USED-FOR inference model. cross - attention USED-FOR latent variables. latent variables USED-FOR ADVAE. latent variables USED-FOR model. latent variables USED-FOR syntactic roles. dependency parser USED-FOR latent variables. dependency parser USED-FOR syntactic roles. ,"This paper proposes a new inference model that uses latent variables generated by cross-attention. The proposed model, ADVAE, uses these latent variables for syntactic roles generated by a dependency parser. "
1328,SP:c7c50c44fdafb15b962e04d713cac309e57bc54b,"method USED-FOR unsupervised disentanglement of text components. neural network USED-FOR independent latent variables. Transformer encoder - decoder network PART-OF inference network. disentanglement of semantic roles EVALUATE-FOR model. syntactic roles PART-OF latent vectors. attention USED-FOR syntactic roles. attention USED-FOR latent vectors. architecture COMPARE VAEs. VAEs COMPARE architecture. architecture USED-FOR disentangling semantic roles. semantic roles PART-OF latent variables. Method are VAE framework, Transformer encoder, attention - based seq2seq architectures, and decoder. ","This paper proposes a method for unsupervised disentanglement of text components. The authors extend the VAE framework by replacing the Transformer encoder-decoder network in the inference network with an inference network that consists of a neural network that predicts independent latent variables. The model is evaluated on the task of disentangling of semantic roles, where syntactic roles are represented as latent vectors sampled from attention-based seq2seq architectures, and semantic roles are modeled as latent variables sampled from the attention of the decoder. The proposed architecture is shown to outperform other VAEs in terms of performance on disentangled semantic roles."
1329,SP:c7c50c44fdafb15b962e04d713cac309e57bc54b,framework USED-FOR disentanglement of syntactic roles. disentanglement of syntactic roles USED-FOR sentence representations. disentanglement of syntactic roles USED-FOR latent variables. model HYPONYM-OF attention - driven VAE. attention - driven VAE USED-FOR syntactic roles. encoder - decoder framework USED-FOR attention - driven VAE. syntactic role extraction CONJUNCTION latent variable influence. latent variable influence CONJUNCTION syntactic role extraction. encoder CONJUNCTION decoder. decoder CONJUNCTION encoder. latent variable influence USED-FOR decoder. disentanglement metrics PART-OF evaluation protocol. syntactic role extraction PART-OF evaluation protocol. latent variable influence PART-OF evaluation protocol. ,"This paper proposes a framework for learning disentanglement of syntactic roles for sentence representations. The proposed model is an attention-driven VAE based on the encoder-decoder framework, which learns to disentangle the syntactical roles from the latent variables. The evaluation protocol consists of three disentangling metrics: syntactic role extraction, latent variable influence, and decoder."
1330,SP:c7c50c44fdafb15b962e04d713cac309e57bc54b,Attention - Driven Variational Autoencoder ( ADVAE ) HYPONYM-OF probabilistic model. model HYPONYM-OF $ \beta$-VAE. Transformers COMPARE neural architectures. neural architectures COMPARE Transformers. RNN HYPONYM-OF neural architectures. syntax USED-FOR syntactic roles. syntactic roles USED-FOR semantics of latent variables. nouns HYPONYM-OF syntactic roles. Transformer CONJUNCTION $ \beta$-VAE framework. $ \beta$-VAE framework CONJUNCTION Transformer. latent variable USED-FOR VAE. attention matrices USED-FOR Transformer architecture. method COMPARE normal VAE. normal VAE COMPARE method. Task is quantifying syntactic disentanglement. OtherScientificTerm is latent variables. ,"The paper proposes Attention-Driven Variational Autoencoder (ADVAE), a probabilistic model for quantifying syntactic disentanglement. The model is a variant of $\beta$-VAE, where syntactic roles (e.g., nouns) are used to represent the semantics of latent variables. The authors compare Transformers with other neural architectures such as RNN and show that the attention matrices of the proposed Transformer architecture and the $beta$. The authors also compare the proposed method with a normal VAE."
1331,SP:57ace99a05a76b7d7427619cb6881fc87d74160f,"influence - based regularization CONJUNCTION curiosity - driven incentives. curiosity - driven incentives CONJUNCTION influence - based regularization. curiosity - driven incentives USED-FOR coordinated and diverse exploration. influence - based regularization USED-FOR coordinated and diverse exploration. curiosity - driven incentives USED-FOR exploration method. influence - based regularization USED-FOR exploration method. random network distillation ( RND ) USED-FOR multi - agent setting. OtherScientificTerm are one - step TD targets, and influence metric. Metric is novelty "" metric. Generic is method. ","This paper proposes an exploration method that combines influence-based regularization with curiosity-driven incentives to encourage coordinated and diverse exploration. The main idea is to use random network distillation (RND) in the multi-agent setting, where one-step TD targets are chosen based on a ""novelty"" metric. The method is evaluated on a variety of datasets, and the influence metric is used to evaluate the effectiveness of the proposed method."
1332,SP:57ace99a05a76b7d7427619cb6881fc87d74160f,"multi - agent tasks EVALUATE-FOR method. OtherScientificTerm are influencer, intrinsic reward, and diverse team behaviour. ",This paper proposes a method for learning to collaborate with an influencer. The method is evaluated on multi-agent tasks. The idea is to learn an intrinsic reward that encourages diverse team behaviour.
1333,SP:57ace99a05a76b7d7427619cb6881fc87d74160f,"techniques USED-FOR Centralized Training. Centralized Training CONJUNCTION Decentralized Execution ( CTDE ) MARL. Decentralized Execution ( CTDE ) MARL CONJUNCTION Centralized Training. intrinsic motivation CONJUNCTION intrinsic cost terms. intrinsic cost terms CONJUNCTION intrinsic motivation. intrinsic cost terms USED-FOR joint exploration of the Markov game. Multi - Agent Particle Environments CONJUNCTION OpenAI Gym continuous control environments. OpenAI Gym continuous control environments CONJUNCTION Multi - Agent Particle Environments. Starcraft Multi - Agent Challenge ( SMAC ) CONJUNCTION sparse reward settings. sparse reward settings CONJUNCTION Starcraft Multi - Agent Challenge ( SMAC ). approach USED-FOR Starcraft Multi - Agent Challenge ( SMAC ). approach USED-FOR sparse reward settings. sparse reward settings CONJUNCTION Multi - Agent Particle Environments. Multi - Agent Particle Environments CONJUNCTION sparse reward settings. OtherScientificTerm is influence "" training objective. Generic are baselines, and method. Task is SMAC ). ","This paper proposes techniques to improve Centralized Training and Decentralized Execution (CTDE) MARL. The main idea is to use intrinsic motivation and intrinsic cost terms to encourage joint exploration of the Markov game. The authors propose a ""influence"" training objective to encourage agents to explore the environment more effectively. The proposed approach is applied to Starcraft Multi-Agent Challenge (SMAC), sparse reward settings, Multi-agent Particle Environments, and OpenAI Gym continuous control environments. The experimental results show improvements over the baselines, and the proposed method is shown to be effective (especially in SMAC)."
1334,SP:57ace99a05a76b7d7427619cb6881fc87d74160f,CTDE USED-FOR cooperative MARL. intrinsic rewards USED-FOR multi - agent exploration. intrinsic rewards CONJUNCTION policy regularization term. policy regularization term CONJUNCTION intrinsic rewards. sparse - reward gridworld settings HYPONYM-OF benchmarks. multi - agent particle environment HYPONYM-OF benchmarks. StarCraft micromanagement HYPONYM-OF benchmarks. method COMPARE ablations. ablations COMPARE method. method COMPARE MARL baselines. MARL baselines COMPARE method. MARL baselines CONJUNCTION ablations. ablations CONJUNCTION MARL baselines. ,"This paper proposes a variant of cooperative MARL based on CTDE. The authors propose to use intrinsic rewards to encourage multi-agent exploration and a policy regularization term to encourage agents to explore in a cooperative manner. The experiments are conducted on two benchmarks: (1) sparse-reward gridworld settings, and (2) a multi-agents particle environment, as well as StarCraft micromanagement. Results show that the proposed method outperforms MARL baselines and ablations."
1335,SP:66784b2f0f08680057670dfea49a4ae88f7a2b38,"model USED-FOR pre - trained model. decomposed, information - rich gradient USED-FOR method. it COMPARE model editing techniques. model editing techniques COMPARE it. GPT - J CONJUNCTION T5 - XL. T5 - XL CONJUNCTION GPT - J. GPT - Neo CONJUNCTION GPT - J. GPT - J CONJUNCTION GPT - Neo. T5 - XL CONJUNCTION T5 - XXL. T5 - XXL CONJUNCTION T5 - XL. model editing techniques USED-FOR large models. BERT - base CONJUNCTION distilGPT-2. distilGPT-2 CONJUNCTION BERT - base. it USED-FOR large models. distilGPT-2 HYPONYM-OF models. BERT - base HYPONYM-OF models. GPT - Neo HYPONYM-OF large models. T5 - XL HYPONYM-OF large models. T5 - XXL HYPONYM-OF large models. GPT - J HYPONYM-OF large models. Method are MEND, fine - tuning, and meta - networks. ","This paper proposes a method called MEND, which uses decomposed, information-rich gradient to fine-tune a pre-trained model with a learned model. The authors show that it outperforms existing model editing techniques on several large models (BERT-base, distilGPT-2, GPT-Neo, Gpt-J, T5-XL, and T5 -XXL). The authors also show that fine-tuning can lead to better performance on meta-networks."
1336,SP:66784b2f0f08680057670dfea49a4ae88f7a2b38,"Model Editor Networks HYPONYM-OF small auxiliary editing networks. Gradient Decomposition ( MEND ) USED-FOR Model Editor Networks. Model Editor Networks USED-FOR post - hoc editing. gradient USED-FOR editing. model editor networks USED-FOR edits. approach USED-FOR model editor networks. fully - connected layers PART-OF neural networks. architecture USED-FOR gradient transform. MEND USED-FOR model editing. GPT CONJUNCTION BERT. BERT CONJUNCTION GPT. MEND USED-FOR edits. T5 CONJUNCTION GPT. GPT CONJUNCTION T5. approach USED-FOR model editing. BERT CONJUNCTION BART models. BART models CONJUNCTION BERT. MEND USED-FOR models. Method are pre - trained nlp models, and fine - tuning approaches. Generic are model, and pre - trained model. OtherScientificTerm are fine - tuning gradient, and gradients. ","This paper proposes Gradient Decomposition (MEND), a small auxiliary editing networks for pre-trained nlp models. Model Editor Networks are designed for post-hoc editing, where the goal is to make the model more interpretable. The authors propose an approach to train model editor networks to perform edits by fine-tuning the gradient of the model during editing. The main idea is to replace the fully-connected layers of neural networks with a single architecture for the gradient transform. This allows the authors to train a model with fewer layers, which allows for more interpretability of the gradients. Experiments on T5, GPT, BERT, and BART models show that MEND can perform well for model editing, and that the models trained with MEND are able to generalize to unseen tasks. The experiments also show that fine-tuning gradient can lead to better generalization performance than other recent methods. The paper also provides some ablation studies to show the benefits of different fine -tuning approaches."
1337,SP:66784b2f0f08680057670dfea49a4ae88f7a2b38,locality CONJUNCTION generality. generality CONJUNCTION locality. locality FEATURE-OF MEND method. generality FEATURE-OF MEND method. curated datasets EVALUATE-FOR MEND. Method is model editing method. ,"This paper proposes a model editing method, MEND, which aims to improve the locality and generality of the MEND method. MEND is evaluated on curated datasets."
1338,SP:66784b2f0f08680057670dfea49a4ae88f7a2b38,"T5 CONJUNCTION GPT - J. GPT - J CONJUNCTION T5. GPT - J HYPONYM-OF editing very large models. MEND HYPONYM-OF model editing method. it USED-FOR model. asymptotic time & memory complexity EVALUATE-FOR it. Task are model editing, and editing. OtherScientificTerm are global behavior, higher - order gradients, and model parameters. Generic are models, and pre - existing models. ","This paper studies the problem of model editing, which is an important problem for editing very large models (e.g., T5 and GPT-J). In particular, this paper focuses on model editing where the global behavior of the model is not known. The authors propose MEND, a model editing method that learns higher-order gradients for the model parameters, and then uses it to update the model in a way that reduces the asymptotic time & memory complexity of the editing. Experiments show that MEND can be applied to a wide range of models, including pre-existing models."
1339,SP:ea9a6880083555a89f5ed22dca21ba2dc109c1a2,"FINN COMPARE physics - aware models. physics - aware models COMPARE FINN. FINN COMPARE machine learning. machine learning COMPARE FINN. machine learning COMPARE physics - aware models. physics - aware models COMPARE machine learning. Task is spatiotemporal advection - diffusion processes. Generic are network, and method. ","This paper studies spatiotemporal advection-diffusion processes. The authors propose a network, called FINN, that is able to predict the future state of the system. The proposed method is based on the idea that the current state-of-the-art physics-aware models do not generalize well to unseen states, and the authors compare FINN with machine learning and with other physics -aware models."
1340,SP:ea9a6880083555a89f5ed22dca21ba2dc109c1a2,finite volume neural network USED-FOR advection - diffusion partial differential equations. advection - diffusion equation USED-FOR modules. OtherScientificTerm is flux kernel. Generic is model. ,"This paper proposes a finite volume neural network for solving advection-diffusion partial differential equations. The main idea is to learn modules that can be expressed as a variant of the standard advect-defusion equation, where the flux kernel is a function of the volume of the model. "
1341,SP:ea9a6880083555a89f5ed22dca21ba2dc109c1a2,Finite Volume neural network USED-FOR modelling fluid dynamics. finite volume method USED-FOR Finite Volume neural network. velocity CONJUNCTION spacial derivatives. spacial derivatives CONJUNCTION velocity. neural networks USED-FOR spacial derivatives. fluid simulation COMPARE physics - inspired models. physics - inspired models COMPARE fluid simulation. ,This paper proposes a Finite Volume neural network based on the finite volume method for modelling fluid dynamics. The main idea is to use neural networks to model the velocity and spacial derivatives. Experiments show that the proposed fluid simulation outperforms other physics-inspired models.
1342,SP:ea9a6880083555a89f5ed22dca21ba2dc109c1a2,initial and boundary conditions USED-FOR physics system. generalization FEATURE-OF physics system. initial and boundary conditions FEATURE-OF generalization. framework COMPARE state - of - the - art. state - of - the - art COMPARE framework. Task is advection - diffusion partial differential equations. OtherScientificTerm is PDE. ,This paper studies the problem of advection-diffusion partial differential equations. The authors study the generalization of a physics system under certain initial and boundary conditions. The main contribution of this paper is to propose a new framework that is more general than the state-of-the-art. Theoretical analysis of the PDE is provided.
1343,SP:d369e2144544908fbcaaa53aab9555d71080ced8,language network CONJUNCTION visual system. visual system CONJUNCTION language network. visual system CONJUNCTION auditory system. auditory system CONJUNCTION visual system. multi - demand network CONJUNCTION language network. language network CONJUNCTION multi - demand network. brain regions CONJUNCTION features of program code. features of program code CONJUNCTION brain regions. auditory system HYPONYM-OF brain regions. multi - demand network HYPONYM-OF brain regions. language network HYPONYM-OF brain regions. visual system HYPONYM-OF brain regions. CodeBERTa CONJUNCTION CodeTransformer. CodeTransformer CONJUNCTION CodeBERTa. seq2seq CONJUNCTION CodeBERTa. CodeBERTa CONJUNCTION seq2seq. CodeTransformer CONJUNCTION XLNet. XLNet CONJUNCTION CodeTransformer. hidden state representations of code language models CONJUNCTION tf - idf. tf - idf CONJUNCTION hidden state representations of code language models. seq2seq HYPONYM-OF hidden state representations of code language models. XLNet HYPONYM-OF hidden state representations of code language models. CodeTransformer HYPONYM-OF hidden state representations of code language models. CodeBERTa HYPONYM-OF hidden state representations of code language models. data types CONJUNCTION control flow. control flow CONJUNCTION data types. code vs. sentence contrast USED-FOR non - LM based features. BOLD signal CONJUNCTION stimulus features. stimulus features CONJUNCTION BOLD signal. linear classifiers CONJUNCTION regressors. regressors CONJUNCTION linear classifiers. BOLD activity FEATURE-OF system. BOLD activity USED-FOR linear classifiers. BOLD activity USED-FOR regressors. classification accuracy CONJUNCTION linear correlation. linear correlation CONJUNCTION classification accuracy. linear correlation EVALUATE-FOR regression. regression EVALUATE-FOR Models. linear correlation EVALUATE-FOR Models. classification accuracy EVALUATE-FOR Models. hidden - state features USED-FOR model. rank accuracy EVALUATE-FOR model. visual system USED-FOR hand - crafted code features. features USED-FOR,"This paper studies the relationship between brain regions (multi-demand network, language network, visual system, auditory system) and features of program code. The authors compare hidden state representations of code language models (seq2seq, CodeBERTa, CodeTransformer, XLNet, and tf-idf) as well as the rank accuracy of a model trained with hidden-state features. The experiments are conducted on a variety of data types and control flow, including both code vs. non-LM based features based on sentence contrast. Models are evaluated in terms of classification accuracy and linear correlation for regression. The system is trained with linear classifiers and regressors that use BOLD activity from the BOLD signal and stimulus features. Experiments show that the visual system is able to generate hand-crafted code features from the hidden state features."
1344,SP:d369e2144544908fbcaaa53aab9555d71080ced8,"self - supervised language models USED-FOR programming code. self - supervised language models USED-FOR representations of code. brain systems USED-FOR code. language system CONJUNCTION visual system. visual system CONJUNCTION language system. multiple demand system CONJUNCTION language system. language system CONJUNCTION multiple demand system. multiple demand system HYPONYM-OF brain systems. visual system HYPONYM-OF brain systems. language system HYPONYM-OF brain systems. brain systems USED-FOR program properties. Method are representations of the programming code, and machine learning models. ","This paper studies the problem of learning representations of the programming code using self-supervised language models. In particular, the authors propose to use brain systems (a multiple demand system, a language system, and a visual system) to learn representations of code that are similar to the representations learned by machine learning models. The authors show that these brain systems are able to learn program properties that can be used to predict program properties."
1345,SP:d369e2144544908fbcaaa53aab9555d71080ced8,systematical framework USED-FOR brain representations of programs. brain representations of programs CONJUNCTION code models. code models CONJUNCTION brain representations of programs. framework USED-FOR code properties. ML models USED-FOR human brain representations of computer code comperhensions. human brain USED-FOR code properties. Language CONJUNCTION Vision. Vision CONJUNCTION Language. Vision CONJUNCTION Auditory systems. Auditory systems CONJUNCTION Vision. Multiple Demand CONJUNCTION Language. Language CONJUNCTION Multiple Demand. brain systems USED-FOR code properties. ridge regressor USED-FOR brain systems. Auditory systems HYPONYM-OF brain systems. Multiple Demand HYPONYM-OF brain systems. Language HYPONYM-OF brain systems. Vision HYPONYM-OF brain systems. ridge regressor USED-FOR code properties. ridge regressor USED-FOR brain representations. ridge regressor USED-FOR learned representations. computational language models of code USED-FOR learned representations. Generic is dataset. Metric is model complexity. ,"This paper presents a systematic framework for learning brain representations of programs and code models using ML models. The framework aims to learn code properties of programs from the human brain. The authors present a dataset of 1.5 million programs and show that the model complexity of the learned representations can be reduced by using computational language models of code. The learned brain representations are trained using a ridge regressor to learn the code properties from brain systems such as Multiple Demand, Language, Vision, and Auditory systems."
1346,SP:d369e2144544908fbcaaa53aab9555d71080ced8,decoding models USED-FOR fMRI responses. machine learning models USED-FOR representations of python code. runtime information HYPONYM-OF model representations of computer code. Task is encoding of computer code. OtherScientificTerm is python code. Method is Multiple Demand system. ,"This paper studies the problem of encoding of computer code into representations of machine learning models that can be used for decoding models for fMRI responses. Specifically, the authors focus on model representations of python code (e.g., runtime information) and propose a Multiple Demand system to aggregate information from different models."
1347,SP:ab5a8934846776a7be7d0ac1973d41fd6aae89fc,"setup USED-FOR Direct Speech To Speech Translation. voice conversion setup USED-FOR this. It COMPARE Translatotron. Translatotron COMPARE It. spectrogram synthesizer CONJUNCTION phoneme / language translator. phoneme / language translator CONJUNCTION spectrogram synthesizer. encoder - decoder PART-OF architecture. speaker turns FEATURE-OF augmented dataset. Task are rendering translated voice, and cross language translation. Method are phoneme translator, and speaker embeddings. OtherScientificTerm is antispoofing. ","This paper proposes a new setup for Direct Speech To Speech Translation, which is based on the voice conversion setup. It is similar to Translatotron, except that instead of a spectrogram synthesizer, a phoneme/language translator is used for rendering translated voice. The architecture consists of an encoder-decoder, where the encoder decodes the speaker embeddings, and the phoneme translator converts the speaker to the target language. The authors also propose to use an augmented dataset with speaker turns, which they call ""antispoofing"". This is an interesting idea for cross language translation."
1348,SP:ab5a8934846776a7be7d0ac1973d41fd6aae89fc,translation quality CONJUNCTION naturalness. naturalness CONJUNCTION translation quality. naturalness CONJUNCTION robustness. robustness CONJUNCTION naturalness. speech - to - text translation CONJUNCTION TTS. TTS CONJUNCTION speech - to - text translation. robustness EVALUATE-FOR cascaded system. Translatotron 2 USED-FOR Translatotron. predicted speech COMPARE cascaded system. cascaded system COMPARE predicted speech. naturalness FEATURE-OF predicted speech. translation quality EVALUATE-FOR cascaded system. naturalness EVALUATE-FOR cascaded system. robustness FEATURE-OF predicted speech. translation quality EVALUATE-FOR predicted speech. TTS HYPONYM-OF cascaded system. speech - to - text translation HYPONYM-OF cascaded system. explicit speaker embedding CONJUNCTION speaker ID. speaker ID CONJUNCTION explicit speaker embedding. Translatotron COMPARE model. model COMPARE Translatotron. explicit speaker embedding USED-FOR Translatotron 2. speaker ID USED-FOR Translatotron 2. approaches USED-FOR Translatotron 2. Translatotron model COMPARE Translatotron 2. Translatotron 2 COMPARE Translatotron model. auxiliary target phoneme decoder USED-FOR spectrogram synthesizer. spectrogram synthesizer USED-FOR Translatotron 2. attention mechanism USED-FOR spectrogram synthesizer. auxiliary target phoneme decoder USED-FOR Translatotron 2. Method is Direct S2ST. ,"This paper proposes Translatotron 2, a variant of Translatron 2 that aims to improve the performance and robustness of a cascaded system (speech-to-text translation and TTS) in terms of both translation quality and naturalness. The authors compare the predicted speech of the original Translattoron 2 with that of the proposed Translattron 2 and show that the proposed model outperforms the original model by a large margin. They also compare the proposed approaches to Translatroron 2 using both the explicit speaker embedding and speaker ID. The main contribution of the paper is the use of an auxiliary target phoneme decoder and a spectrogram synthesizer with an attention mechanism, which is inspired by Direct S2ST."
1349,SP:ab5a8934846776a7be7d0ac1973d41fd6aae89fc,"phoneme decoder's hidden layer output PART-OF spectrogram synthesizer's input. data - centric approach USED-FOR speakers'voice. model COMPARE baseline S2ST models. baseline S2ST models COMPARE model. model COMPARE cascaded ST+TTS oracle. cascaded ST+TTS oracle COMPARE model. cascaded ST+TTS oracle COMPARE baseline S2ST models. baseline S2ST models COMPARE cascaded ST+TTS oracle. translation quality COMPARE cascaded ST+TTS oracle. cascaded ST+TTS oracle COMPARE translation quality. translation quality COMPARE baseline S2ST models. baseline S2ST models COMPARE translation quality. baseline S2ST models USED-FOR bilingual and multilingual S2ST tasks. bilingual and multilingual S2ST tasks EVALUATE-FOR model. translation quality EVALUATE-FOR model. Generic are tasks, and synthesizer. Method are parameter sharing, phoneme decoder, and zero - shot voice - transfer TTS model. OtherScientificTerm is local voice similarity. Material are TTS synthesized target speech, and generated speech. Metric is subjective ( MOS ) metrics. ","This paper proposes a data-centric approach to synthesize speakers' voice, where the phoneme decoder's hidden layer output is incorporated into the spectrogram synthesizer's input. The authors propose two tasks: (1) to improve the translation quality of the generated speech, and (2) to increase the local voice similarity between the TTS synthesized target speech and the original target speech, the authors propose to use parameter sharing between the synthesizer and the target speech. The proposed model is evaluated on both bilingual and multilingual S2ST tasks, and compared with a cascaded ST+TTS oracle, and baseline S2T models. The results show that the proposed zero-shot voice-transfer TTS model is able to achieve better translation quality compared to the baseline s2ST models, as well as higher translation quality when compared to both the original synthesizer (with and without parameter sharing) and to the cascaded Stochastic Transformer-based Transformer (ST-TTS) oracle (with parameter sharing). The authors also provide subjective (MOS) metrics to evaluate the quality of generated speech."
1350,SP:ab5a8934846776a7be7d0ac1973d41fd6aae89fc,"Translatotron HYPONYM-OF speech - to - speech translation system. Translation quality EVALUATE-FOR cascaded baseline. explicit speaker embedding USED-FOR generating spoofing audio. explicit speaker embedding USED-FOR Voice retention. Translatotron 2 COMPARE cascaded system. cascaded system COMPARE Translatotron 2. Translatotron 2 COMPARE Translatotron. Translatotron COMPARE Translatotron 2. speech naturalness CONJUNCTION speech robustness. speech robustness CONJUNCTION speech naturalness. translation quality CONJUNCTION speech naturalness. speech naturalness CONJUNCTION translation quality. Translatotron COMPARE cascaded system. cascaded system COMPARE Translatotron. speech naturalness EVALUATE-FOR Translatotron 2. speech robustness EVALUATE-FOR Translatotron 2. speech robustness EVALUATE-FOR cascaded system. speech naturalness EVALUATE-FOR cascaded system. translation quality EVALUATE-FOR Translatotron 2. translation quality EVALUATE-FOR cascaded system. Conformer Encoder CONJUNCTION Duration - based   Spectrogram synthesizer. Duration - based   Spectrogram synthesizer CONJUNCTION Conformer Encoder. SpecAugment FEATURE-OF Conformer Encoder. parallel utterances USED-FOR Translatotron2. model USED-FOR generation. cross lingual voice transfer capacity FEATURE-OF TTS model. TTS model USED-FOR Parallel utterances. voice identity FEATURE-OF Translatotron 2. Translatotron2 COMPARE Translatotron. Translatotron COMPARE Translatotron2. BLEU scores EVALUATE-FOR Translatotron2. Translatotron USED-FOR multilingual settings. Translatotron 2 EVALUATE-FOR Translatotron2. BLEU scores EVALUATE-FOR Translatotron 2. high resource languages USED-FOR Translatotron 2. multilingual settings EVALUATE-FOR Translatotron2. Material is synthesized translated speech. Method are spectrogram synthesizer, Non - Attentive Tacotron spectrogram synthesizer, and Tacotron2. Generic is architecture. OtherScientificTerm are","Translatotron is a speech-to-speech translation system, which is an extension of Translatron, a cascaded baseline that improves the quality of the synthesized translated speech. Voice retention is achieved by using an explicit speaker embedding for generating spoofing audio. Translation quality is improved compared to the original Translatoton and the cascaded baselines. Translaton 2 outperforms the original cascaded system in terms of translation quality, speech naturalness, and speech robustness. The architecture consists of a Conformer Encoder with SpecAugment, a Duration-based  Spectrogram synthesizer, and a non-Attentive Tacotron spectrogram generator. Parallel utterances are generated using the TTS model with cross lingual voice transfer capacity, and the model is trained for generation. Experiments are conducted on Translattoron 2 with different levels of voice identity, and show that the proposed Translattron 2 is able to achieve better BLEU scores compared to Translator 2 and Translatotor 1, while achieving comparable performance with respect to the Translatone 2. In addition, the authors show that Translatototron2 can achieve comparable performance to the state-of-the-art on high resource languages. The authors also show that in multilingual settings, Translatrotron2 outperforms Translatoron and Transloron on high-resource languages.  The authors propose a new spectrogram synthesized speech that is inspired by the non-attentive, Non-Attentional, non-trivial, and non-automated Translatentive (Tacotron 2) spectrogram synthesis method. The proposed architecture is called Tacotorron2. The main idea is to use the Conformer encoder and the Duration -based Spectrogram synthesizers. "
1351,SP:296102e60b842923c94f579f524fa1147328ee4b,small data constraint USED-FOR attribute detection. support set CONJUNCTION query set. query set CONJUNCTION support set. self - supervised representation learning CONJUNCTION supervised fine - tuning. supervised fine - tuning CONJUNCTION self - supervised representation learning. supervised learning phase USED-FOR episode task. pre - training phase CONJUNCTION supervised learning phase. supervised learning phase CONJUNCTION pre - training phase. self - supervised representation learning PART-OF pre - training phase. supervised fine - tuning PART-OF pre - training phase. training dataset USED-FOR pre - trained representation space. image samples PART-OF training dataset. pre - training phase COMPARE approaches. approaches COMPARE pre - training phase. few - shot attribute prediction episodes EVALUATE-FOR approaches. benchmarks EVALUATE-FOR approaches. benchmarks EVALUATE-FOR pre - training phase. learning dataset CONJUNCTION few - shot problem. few - shot problem CONJUNCTION learning dataset. logistic regression USED-FOR indicator. Method is few - shot approach. Generic is problem. ,"This paper proposes a few-shot approach to attribute detection under a small data constraint. The pre-training phase consists of self-supervised representation learning and supervised fine-tuning for the episode task. The training dataset consists of image samples from the support set and a query set from the query set. The authors evaluate their pre-train phase on several benchmarks and show that their approaches outperform existing approaches on a few -shot attribute prediction episodes. In addition, the authors propose an indicator based on logistic regression to further improve the performance of the problem. Finally, they propose to combine the learning dataset with the few-shoot problem."
1352,SP:296102e60b842923c94f579f524fa1147328ee4b,"attributes COMPARE object classes. object classes COMPARE attributes. few - shot / meta - learning paradigm USED-FOR few - shot attribute learning ( FSAL ). smiling & wearing eyeglasses HYPONYM-OF multi - label nature of attributes. Celeb - A CONJUNCTION Zappos50 K. Zappos50 K CONJUNCTION Celeb - A. Zappos50 K CONJUNCTION ImageNet - with - Attributes. ImageNet - with - Attributes CONJUNCTION Zappos50 K. benchmark datasets USED-FOR problem. Celeb - A HYPONYM-OF benchmark datasets. ImageNet - with - Attributes HYPONYM-OF benchmark datasets. Zappos50 K HYPONYM-OF benchmark datasets. benchmarks EVALUATE-FOR approaches. Generic are model, and them. ","This paper proposes a few-shot/meta-learning paradigm for the problem of few-shots attribute learning (FSAL), which is motivated by the multi-label nature of attributes (e.g., smiling & wearing eyeglasses) and the fact that attributes are often more informative than object classes. The authors propose to train a model that learns to distinguish between attributes that are informative and those that are not informative. The problem is formulated as a set of benchmark datasets (Celeb-A, Zappos50K, and ImageNet-with-Attributes) to evaluate the problem. The proposed approaches are evaluated on these benchmarks and compared to other approaches that do not use them."
1353,SP:296102e60b842923c94f579f524fa1147328ee4b,"classical representation learning algorithm USED-FOR visual representation. SimCLR HYPONYM-OF visual representation. supervised training USED-FOR representation. supervised training USED-FOR network. few - shot learning strategy USED-FOR linear classifier. Task is few - shot attribute learning. Method are classical zero - shot learning, and learning algorithm. Generic is approach. ","This paper proposes a classical representation learning algorithm for visual representation (e.g., SimCLR). The key idea is to use supervised training of the network to learn the representation. This is similar to classical zero-shot learning, but the key difference is that the learning algorithm is supervised. The paper also proposes a few-shot attribute learning, where the network is trained with a few labels. The main contribution of the paper is the introduction of a new learning algorithm, which is a linear classifier that uses a few -shot learning strategy. The experimental results show the effectiveness of the proposed approach."
1354,SP:296102e60b842923c94f579f524fa1147328ee4b,base attributes USED-FOR model. attribute annotation FEATURE-OF benchmark datasets. self - supervised learning USED-FOR backbone. base attributes USED-FOR backbone. MatchingNEt CONJUNCTION MAML. MAML CONJUNCTION MatchingNEt. MAML CONJUNCTION ProtoNet. ProtoNet CONJUNCTION MAML. few - shot learning models USED-FOR FSAL setting. ProtoNet HYPONYM-OF few - shot learning models. MatchingNEt HYPONYM-OF few - shot learning models. MAML HYPONYM-OF few - shot learning models. Generic is topic. Task is few - shot attribute learning ( FSAL ). Method is logistic regression layer. ,"This paper addresses the topic of few-shot attribute learning (FSAL), where the model is trained on a set of base attributes, and the backbone is trained using self-supervised learning. In particular, the authors focus on benchmark datasets with attribute annotation. The authors propose to use a logistic regression layer to estimate the likelihood of a given attribute, which is then used to train the backbone using the base attributes. The FSAL setting can be applied to existing state-of-the-art few-shots learning models such as MatchingNEt, MAML, and ProtoNet."
1355,SP:0f69e20b9f97439d19e7a93144c8d877cedcd714,method USED-FOR 2 - Wasserstein gradient flow. 2 - Wasserstein gradient flow USED-FOR relative entropy. neural network function approximation - based approach USED-FOR density ratios. neural network function approximation - based approach USED-FOR particle - based method. MALA COMPARE ULA. ULA COMPARE MALA. ,This paper proposes a method to approximate the relative entropy of a particle using 2-Wasserstein gradient flow. The particle-based method is based on a neural network function approximation-based approach to approximate density ratios. The experimental results show that MALA outperforms ULA.
1356,SP:0f69e20b9f97439d19e7a93144c8d877cedcd714,"unnormalized distribution USED-FOR sampling. relative entropy functional FEATURE-OF Wasserstein gradient flow. Bregman score USED-FOR logarithmic density ratio. OtherScientificTerm are unnormalized target distribution, time - varying velocity field, variable distribution, and velocity field. Generic is approximation. ","This paper studies the relative entropy functional of the Wasserstein gradient flow under the assumption of sampling from an unnormalized distribution. In particular, the authors consider the case where the target distribution is a time-varying velocity field. The authors show that the logarithmic density ratio can be approximated by the Bregman score, which is a function of the variable distribution. They also provide an approximation for the case when the velocity field is non-stationary."
1357,SP:0f69e20b9f97439d19e7a93144c8d877cedcd714,"relative entropy FEATURE-OF gradient flow. Wasserstein space of probability distributions FEATURE-OF relative entropy. variational characterization USED-FOR discretized steps. combined particle evolution USED-FOR it. combined particle evolution USED-FOR estimation. OtherScientificTerm are unnormalized distributions, and normalizing constant. Generic are flow, characterization, and algorithm. ","This paper studies the relative entropy of a gradient flow in the Wasserstein space of probability distributions. The authors propose a variational characterization of the discretized steps of the flow, and show that it can be approximated by combined particle evolution. In particular, the authors show that for unnormalized distributions, the estimation can be done by combining particle evolution with the normalizing constant, and that this characterization can be used to derive an efficient algorithm."
1358,SP:0f69e20b9f97439d19e7a93144c8d877cedcd714,unnormalized distribution USED-FOR sampling. KL divergence FEATURE-OF numerical simulation of the gradient flow. gradient flow USED-FOR sampling problem. gradient term FEATURE-OF density ratio. deep neural network USED-FOR density ratio. OtherScientificTerm is approximating distribution. Generic is method. ,This paper studies the problem of sampling from an unnormalized distribution. The sampling problem is formulated as a numerical simulation of the gradient flow with KL divergence. The authors propose to use a deep neural network to approximate the density ratio with respect to the gradient term. The approximating distribution is then used to estimate the KL divergence of the method.
1359,SP:a69b894166482ccd7a3a9db53e0f5a7e6ecff89a,trainable quantum procedures USED-FOR machine learning models. machine learning models USED-FOR classification of images. trainable quantum procedures USED-FOR classification of images. circuits USED-FOR compressed images. circuits USED-FOR quantum states. trainable unitary USED-FOR quantum states. readout register USED-FOR trainable unitary. MNIST dataset EVALUATE-FOR numerical simulation. ,"This paper proposes to use trainable quantum procedures to train machine learning models for the classification of images. Specifically, the authors train circuits to generate compressed images, and train a trainable unitary on the readout register to represent the quantum states. The numerical simulation is performed on the MNIST dataset."
1360,SP:a69b894166482ccd7a3a9db53e0f5a7e6ecff89a,quantum neural networks USED-FOR classical image classification. FRQI framework USED-FOR It. parameterized quantum circuits USED-FOR classification. parameterized quantum circuits USED-FOR transformations of quantum state. XX and ZZ gates USED-FOR parameterized quantum circuits. ,"This paper studies the problem of quantum neural networks for classical image classification. It is based on the FRQI framework. The authors propose to use parameterized quantum circuits with XX and ZZ gates for classification, which can be used to represent transformations of quantum state. "
1361,SP:a69b894166482ccd7a3a9db53e0f5a7e6ecff89a,2qubit gates USED-FOR image coding approach. Material is Downsampled MNIST digits. Method is reduced codes. ,This paper presents an image coding approach with 2qubit gates. Downsampled MNIST digits are used to generate reduced codes.
1362,SP:a69b894166482ccd7a3a9db53e0f5a7e6ecff89a,quantum computers USED-FOR image classification problem. 16 - by-16 images USED-FOR MNIST dataset. prior USED-FOR image classification. 4 - by-4 input images USED-FOR image classification. 4 - by-4 input images USED-FOR prior. ,"This paper studies the image classification problem with quantum computers. The authors propose a new prior for image classification based on 4-by-4 input images, and then apply this prior to the MNIST dataset with 16- by-16 images."
1363,SP:acf3825e96d1b7c66cdc339fc5de77330b8e8e90,framework USED-FOR face recognition network. face recognition network USED-FOR federated learning setting. differentially private local clustering mechanism USED-FOR method. consensus - aware recognition loss USED-FOR global consensuses. global consensuses FEATURE-OF face embeddings. ,This paper proposes a framework to train a face recognition network in federated learning setting. The proposed method is based on a differentially private local clustering mechanism. The authors also propose a consensus-aware recognition loss to enforce global consensuses on face embeddings.
1364,SP:acf3825e96d1b7c66cdc339fc5de77330b8e8e90,"federated learning technique USED-FOR deep face recognition models. federated learning technique USED-FOR privacy leakage issue. online clustering CONJUNCTION gaussian perturbation. gaussian perturbation CONJUNCTION online clustering. OtherScientificTerm are leakage of ID features, overlapping IDs, and local data. Method are face recognition model, and DPLC. Generic is method. ","This paper proposes a federated learning technique for deep face recognition models to address the privacy leakage issue. The leakage of ID features is caused by overlapping IDs. To address this issue, the paper proposes to train a face recognition model on top of the local data. The proposed method, DPLC, is based on online clustering and gaussian perturbation."
1365,SP:acf3825e96d1b7c66cdc339fc5de77330b8e8e90,federated learning ( FL ) framework USED-FOR face recognition. deep neural network USED-FOR feature embedding. deep neural network CONJUNCTION weight vectors. weight vectors CONJUNCTION deep neural network. weight vectors USED-FOR feature embedding. FL framework USED-FOR feature embedding network. deep neural network USED-FOR face recognition. FedAvg HYPONYM-OF FL framework. Gaussian noise USED-FOR differential privacy. Gaussian noise USED-FOR cluster centers. OtherScientificTerm is local weight vectors. ,"This paper proposes a federated learning (FL) framework for face recognition. The proposed FL framework, FedAvg, trains a feature embedding network using a deep neural network and a set of weight vectors. The local weight vectors are sampled from the training set. The cluster centers are trained with Gaussian noise for differential privacy."
1366,SP:acf3825e96d1b7c66cdc339fc5de77330b8e8e90,"FL strategy USED-FOR face recognition. FL USED-FOR face recognition. consensus - aware recognition loss USED-FOR discriminative feature learning. Method are PrivacyFace, and Differentially Private Local Clustering ( DPLC ). OtherScientificTerm are sanitized clusters, and local class centers. ","This paper proposes PrivacyFace, a FL strategy for face recognition. The main idea of PrivacyFace is to use Differentially Private Local Clustering (DPLC), where the sanitized clusters are used as local class centers and the discriminative feature learning is performed with a consensus-aware recognition loss. "
1367,SP:408d9e1299ee05b89855df9742b608626692b40d,"method USED-FOR intermediate representations. intermediate representations USED-FOR tasks. pre - trained model USED-FOR intermediate representations. features USED-FOR linear classifier. group - lasso regularization USED-FOR linear classifier. low - resource transfer scenario of image classification EVALUATE-FOR approach. method COMPARE fine - tuning. fine - tuning COMPARE method. method COMPARE ViT model. ViT model COMPARE method. ResNet model USED-FOR fine - tuning. ResNet model USED-FOR method. Method are feature selection strategy, classifier, two - step feature selection, and fine - tuning approach. OtherScientificTerm are regularization score, and regularization. ","This paper proposes a method to learn intermediate representations for tasks using a pre-trained model. The key idea is to use a feature selection strategy where the features are used to train a linear classifier with group-lasso regularization. The regularization score is computed by minimizing the difference between the output of the classifier and the original classifier. The authors propose a two-step feature selection: first, the regularization is applied to the input features, and then, the intermediate representations are learned using a fine-tuning approach. The proposed approach is evaluated in the low-resource transfer scenario of image classification. The experiments show that the proposed method outperforms the standard ViT model and the standard ResNet model in terms of performance and speed up of fine-tuning."
1368,SP:408d9e1299ee05b89855df9742b608626692b40d,intermediate layers USED-FOR linear probing. intermediate layers USED-FOR transfer learning. linear probing USED-FOR transfer learning. features USED-FOR classification head. Head2Toe COMPARE fine - tuning. fine - tuning COMPARE Head2Toe. VTAB benchmark EVALUATE-FOR Head2Toe. VTAB benchmark EVALUATE-FOR fine - tuning. VTAB benchmark EVALUATE-FOR Head2Toe. fine - tuning USED-FOR Head2Toe. Head2Toe USED-FOR out - of - distribution transfer. fine - tuning USED-FOR out - of - distribution transfer. ,This paper proposes to use intermediate layers for linear probing for transfer learning. These features are then used to train a classification head. Experiments on the VTAB benchmark show that the proposed Head2Toe outperforms fine-tuning on the fine-tuning alone. The paper also shows that using the intermediate layers improves the performance on out-of-distribution transfer.
1369,SP:408d9e1299ee05b89855df9742b608626692b40d,method USED-FOR intermediate representations of DNNs. Head2Toe USED-FOR linear probing. linear probing CONJUNCTION intermediate representations. intermediate representations CONJUNCTION linear probing. group lasso USED-FOR regularization. It USED-FOR features. group lasso USED-FOR It. Head2Toe COMPARE fine - tuning. fine - tuning COMPARE Head2Toe. Head2Toe COMPARE linear probes. linear probes COMPARE Head2Toe. VTAB benchmark EVALUATE-FOR Head2Toe. linear probes COMPARE fine - tuning. fine - tuning COMPARE linear probes. VTAB benchmark EVALUATE-FOR fine - tuning. OOD generalization EVALUATE-FOR Head2Toe. Method is transfer learning. ,"This paper proposes a method called Head2Toe for learning intermediate representations of DNNs. It uses a group lasso as a regularization to encourage the features to be similar to each other. The idea is inspired by transfer learning. The experiments on the VTAB benchmark show that the performance of the proposed method is comparable to the state-of-the-art linear probing and intermediate representations. On the OOD generalization, the proposed Head2Tee outperforms linear probes and fine-tuning."
1370,SP:408d9e1299ee05b89855df9742b608626692b40d,"intermediate layers USED-FOR downstream tasks. deep pretrained model USED-FOR downstream tasks. intermediate layers USED-FOR deep pretrained model. fine tuning USED-FOR downstream tasks. Head2toe COMPARE linear finetunning. linear finetunning COMPARE Head2toe. datasets EVALUATE-FOR ResNet-50 and ViT - B/16 models. VTAB collection FEATURE-OF datasets. intermediate representations USED-FOR Head2toe. Generic are model, approach, and network. OtherScientificTerm is classification head. ","This paper proposes a deep pretrained model with intermediate layers that can be used to perform downstream tasks with fine tuning. The proposed model, called Head2toe, is based on the idea that the intermediate representations of the network should be similar to those of the classification head. The authors evaluate the proposed ResNet-50 and ViT-B/16 models on two datasets from the VTAB collection. The results show that the proposed head2toe outperforms linear finetunning."
1371,SP:d6f11fb32851f97af287f962f83220d27a8bc76a,"planning USED-FOR text - based games. transition layers USED-FOR belief of object states. internal representation of object dynamics USED-FOR OOTD. transition layers USED-FOR OOTD. Method are model - free baselines, and OOTD components. ",This paper studies the problem of planning in text-based games. The authors propose to use OOTD with transition layers to model the belief of object states and to learn an internal representation of object dynamics. The experimental results are compared to model-free baselines and show that the proposed transition layers can improve the performance. The main contribution of the paper is the introduction of the transition layers and the analysis of the different aspects of the proposed OOTOD components.
1372,SP:d6f11fb32851f97af287f962f83220d27a8bc76a,Object - Oriented Text Dynamics USED-FOR model - based RL. TextWorld environment FEATURE-OF model - based RL. R - GCN USED-FOR memory graph. ComplEx CONJUNCTION R - GCN. R - GCN CONJUNCTION ComplEx. methods USED-FOR OOTD. R - GCN HYPONYM-OF methods. ComplEx HYPONYM-OF methods. model CONJUNCTION Dyn - Q or MCTS methods. Dyn - Q or MCTS methods CONJUNCTION model. network USED-FOR graphs. model - free RL CONJUNCTION graph models. graph models CONJUNCTION model - free RL. OOTD COMPARE baselines. baselines COMPARE OOTD. graph models HYPONYM-OF baselines. model - free RL HYPONYM-OF baselines. OtherScientificTerm is object - level information. Task is object - supervised setting. Material is self - supervised setting. ,"This paper proposes Object-Oriented Text Dynamics (OOTD), a model-based RL in the TextWorld environment that leverages object-level information. Two methods are proposed for OOTD: ComplEx and R-GCN, where ComplEx learns a memory graph that is used to model the object-levels of the text, and then uses R-GCN to reconstruct the memory graph. The model is combined with Dyn-Q or MCTS methods. The network is trained to generate graphs that are similar to those generated by the network in the self-supervised setting, but different from those generated in the object - supervised setting. Experiments show that OODD outperforms other baselines such as model-free RL and graph models."
1373,SP:d6f11fb32851f97af287f962f83220d27a8bc76a,"framework USED-FOR time - evolution of a game. transition and reward function FEATURE-OF OO POMDP game. method USED-FOR transition function. dual stream attention USED-FOR representation. one CONJUNCTION one. one CONJUNCTION one. deterministic extraction of the graph USED-FOR one. Evidence lower bound objective ( ELBo ) USED-FOR dynamics model. dynamics model USED-FOR planner. DQN CONJUNCTION DRQN. DRQN CONJUNCTION DQN. planner COMPARE baselines. baselines COMPARE planner. planner COMPARE DQN. DQN COMPARE planner. planner COMPARE DRQN. DRQN COMPARE planner. DQN HYPONYM-OF baselines. DRQN HYPONYM-OF baselines. sample efficiency EVALUATE-FOR baselines. sample efficiency EVALUATE-FOR planner. Ablations EVALUATE-FOR method. graphs EVALUATE-FOR ablations. accuracy EVALUATE-FOR ablations. Task is text - based games. Generic is domain. OtherScientificTerm is graph. Method are message passing, node representations, object representation, and object representations. ","This paper proposes a framework for time-evolution of a game. This is an important problem in text-based games, where the goal is to predict the next state of the game. In this domain, the transition and reward function of an OO POMDP game can be represented as a graph, and the authors propose a method to learn the transition function. The representation is learned using dual stream attention, where message passing is used to update the node representations and the object representation is updated using the object representations. The authors propose two variants of the dynamics model, one based on deterministic extraction of the graph and the other based on the Evidence lower bound objective (ELBo). Experiments show that the proposed planner outperforms the baselines (DQN and DRQN) in terms of sample efficiency and accuracy. Ablations on graphs are also performed to show the effectiveness of the proposed method."
1374,SP:d6f11fb32851f97af287f962f83220d27a8bc76a,approach USED-FOR task - based games. object - oriented POMDP formulation USED-FOR task. online planning algorithm USED-FOR problem. object supervision CONJUNCTION self - supervision. self - supervision CONJUNCTION object supervision. object supervision FEATURE-OF graph neural network. self - supervision FEATURE-OF graph neural network. MCTS HYPONYM-OF online planning algorithm. sample efficiency CONJUNCTION task scores. task scores CONJUNCTION sample efficiency. task scores EVALUATE-FOR approaches. sample efficiency EVALUATE-FOR approaches. ,"This paper presents an approach to solving task-based games. The task is formulated as an object-oriented POMDP formulation. An online planning algorithm, MCTS, is proposed to solve the problem. The graph neural network is trained with both object supervision and self-supervision. The proposed approaches are evaluated on both sample efficiency and task scores."
1375,SP:3e5ac4add9ab8a986fdf027b6e6a7d59698b3031,Task is cost - sensitive hierarchical classification problems. ,This paper studies cost-sensitive hierarchical classification problems. The main contribution of this paper is to provide a theoretical analysis of the problem of estimating the cost of each class. The paper is well-written and easy to follow. 
1376,SP:3e5ac4add9ab8a986fdf027b6e6a7d59698b3031,framework USED-FOR cost - sensitive hierarchical classification. deep distributionally robust learning ( DRL ) approach USED-FOR abstaining loss. deep distributionally robust learning ( DRL ) approach USED-FOR subproblems. method COMPARE DARTS. DARTS COMPARE method. decomposition USED-FOR performance profile. abstaining losses USED-FOR decomposition. abstaining losses USED-FOR performance profile. Generic is it. ,"This paper proposes a framework for cost-sensitive hierarchical classification. Specifically, the authors propose a deep distributionally robust learning (DRL) approach to decompose the abstaining loss into subproblems, and then use it to train the weights. The proposed method is compared to DARTS, and the authors show that the proposed decomposition can improve the performance profile by using abstaining losses."
1377,SP:3e5ac4add9ab8a986fdf027b6e6a7d59698b3031,approach USED-FOR hierarchical cost sensitive classification. cost sensitive problem CONJUNCTION layer wise abstaining losses. layer wise abstaining losses CONJUNCTION cost sensitive problem. it USED-FOR hierarchical setup. distributionally robust cost sensitive classification USED-FOR It. birds and cell classification datasets EVALUATE-FOR methodology. It COMPARE DARTS method. DARTS method COMPARE It. OtherScientificTerm is abstentions. ,"This paper proposes an approach for hierarchical cost sensitive classification. It is based on distributionally robust cost sensitive classifier, where the cost sensitive problem is combined with layer wise abstaining losses. The authors argue that abstentions are important because it allows for hierarchical setup. The methodology is evaluated on birds and cell classification datasets. It outperforms the DARTS method."
1378,SP:3e5ac4add9ab8a986fdf027b6e6a7d59698b3031,"label taxonomy USED-FOR cost - sensitive hierarchical classification ( CSHC ). LAM USED-FOR method. DRL framework USED-FOR learning to abstain problems. hierarchical cost - sensitive loss CONJUNCTION abstaining losses. abstaining losses CONJUNCTION hierarchical cost - sensitive loss. DRL framework USED-FOR optimization. limited benchmarks EVALUATE-FOR LAM. OtherScientificTerm are hierarchy, abstention, and bijective correspondence. Task is flat classification. ","This paper proposes cost-sensitive hierarchical classification (CSHC) based on label taxonomy. The proposed method is based on LAM, which is a generalization of LAM. The main difference is that instead of learning a hierarchy, the authors propose learning to abstain problems using the DRL framework. The authors propose to learn a hierarchical loss and abstaining losses, which can be used for optimization. The abstention is learned by minimizing the bijective correspondence between the labels and the labels. The experimental results on limited benchmarks show that LAM can achieve state-of-the-art performance on flat classification."
1379,SP:7997a1b59ef2fc0127c3fff02d191f5d655168f8,"method USED-FOR localization of several sound sources. method USED-FOR detection. detection CONJUNCTION localization of several sound sources. localization of several sound sources CONJUNCTION detection. 4 - channel signal USED-FOR method. synperiodic "" filter bank representation USED-FOR deep convolutional neural network. synperiodic "" filter bank representation USED-FOR approach. DCASE2020 dataset EVALUATE-FOR method. ","This paper proposes a method for detection and localization of several sound sources. The proposed method is based on a 4-channel signal. The approach uses a ""synperiodic"" filter bank representation to train a deep convolutional neural network. The method is evaluated on the DCASE2020 dataset."
1380,SP:7997a1b59ef2fc0127c3fff02d191f5d655168f8,"synperiodic filter "" bank representation USED-FOR deep convolutional neural network. synperiodic filter "" bank representation USED-FOR sound source localization method. filterbanks USED-FOR multi - scale perception. raw waveform USED-FOR filterbanks. time domain FEATURE-OF multi - scale perception. Generic is method. ","This paper proposes a sound source localization method based on a ""synperiodic filter"" bank representation of a deep convolutional neural network. The method is based on the observation that filterbanks trained on raw waveform can be used for multi-scale perception in the time domain."
1381,SP:7997a1b59ef2fc0127c3fff02d191f5d655168f8,multi - channel raw audio waveforms USED-FOR sound event detection and localization. backbone classifier PART-OF audio front end feature extraction scheme. Synperiodic Filterbank HYPONYM-OF audio front end feature extraction scheme. MFCC CONJUNCTION LFBEs. LFBEs CONJUNCTION MFCC. feature extraction scheme COMPARE feature extraction schemes. feature extraction schemes COMPARE feature extraction scheme. LFBEs HYPONYM-OF feature extraction schemes. MFCC HYPONYM-OF feature extraction schemes. model COMPARE baseline model. baseline model COMPARE model. Synperiodic Filterbank COMPARE baseline model. baseline model COMPARE Synperiodic Filterbank. Synperiodic Filterbank COMPARE model. model COMPARE Synperiodic Filterbank. Transformer Architecture USED-FOR model. OtherScientificTerm is time - frequency resolution. Generic is baseline. ,"This paper addresses the problem of sound event detection and localization with multi-channel raw audio waveforms. The authors propose a new audio front end feature extraction scheme, called Synperiodic Filterbank, which consists of a backbone classifier that predicts the time-frequency resolution of the input signal. The proposed model is based on the Transformer Architecture, and is compared to other feature extraction schemes such as MFCC and LFBEs. The experimental results show that the proposed model outperforms the baseline model in terms of performance. However, there are some issues with the proposed baseline."
1382,SP:7997a1b59ef2fc0127c3fff02d191f5d655168f8,synperiodic filerbank COMPARE syndistance bank. syndistance bank COMPARE synperiodic filerbank. method COMPARE synperiodic filerbank. synperiodic filerbank COMPARE method. syndistance bank USED-FOR data - dependent time - frequency resolution map. Task is sound source detection problem. ,This paper studies the sound source detection problem and proposes a method that is more efficient than the standard synperiodic filerbank and the standard syndistance bank. The main idea is to learn a data-dependent time-frequency resolution map based on the learned synistance bank and then use the learned map to predict the source signal.
1383,SP:8bc53935566be2b70403f4b46fe94686d5eae1a1,"self - training USED-FOR gradual domain adaptation. Generic is it. OtherScientificTerm are per sample intermediate distribution shift, and intermediate distribution. Method are gradual feature interpolation, and Iterative self - training. Material is synthetic and natural distributions. ","This paper studies self-training for gradual domain adaptation. Specifically, it considers the problem of per sample intermediate distribution shift. The authors propose to use gradual feature interpolation, where each sample is sampled from an intermediate distribution, and the goal is to learn a new intermediate distribution that is similar to the one sampled from the original distribution. The main contribution of this paper is to propose Iterative Self-training, which is based on the idea of sampling from the same intermediate distribution. Experiments are conducted on both synthetic and natural distributions."
1384,SP:8bc53935566be2b70403f4b46fe94686d5eae1a1,iterative self - training approach USED-FOR unsupervised domain adaptation. approach USED-FOR manifold mix - up. GIFT COMPARE iterative self - training. iterative self - training COMPARE GIFT. CIFAR-10 dataset EVALUATE-FOR synthetic domain shifts. synthetic domain shifts EVALUATE-FOR GIFT. target accuracy EVALUATE-FOR GIFT. target accuracy EVALUATE-FOR iterative self - training. CIFAR-10 dataset EVALUATE-FOR GIFT. natural domain shifts FEATURE-OF datasets. Generic is model. OtherScientificTerm is hyperparameter. Method is automatic curriculum. ,"This paper proposes an iterative self-training approach for unsupervised domain adaptation. The proposed approach, called GIFT, aims to address manifold mix-up, where the model is trained to adapt to new data points by adjusting a hyperparameter. The authors compare the performance of GIFT on synthetic domain shifts on the CIFAR-10 dataset with the state-of-the-art in terms of target accuracy compared to the state of the art in terms the target accuracy of iterative Self-Training. The paper also presents an automatic curriculum that can adapt to different datasets with natural domain shifts."
1385,SP:8bc53935566be2b70403f4b46fe94686d5eae1a1,GIFT method USED-FOR domain adaptation ( DA ). manifold mixup technique USED-FOR virtual samples. manifold mixup technique PART-OF GIFT. co - teaching strategy PART-OF GIFT. natural image datasets EVALUATE-FOR GIFT. GIFT COMPARE baseline methods. baseline methods COMPARE GIFT. natural image datasets EVALUATE-FOR baseline methods. OtherScientificTerm is mixup coefficient. ,This paper proposes a new GIF method for domain adaptation (DA). The main contribution of the paper is a manifold mixup technique to generate virtual samples. The co-teaching strategy of the proposed GIFT is also discussed. The mixup coefficient is also analyzed. Experiments on several natural image datasets show that GIFT outperforms baseline methods.
1386,SP:8bc53935566be2b70403f4b46fe94686d5eae1a1,interpolating source and target representations USED-FOR virtual samples. OtherScientificTerm is intermediate distributions. Generic is method. ,"This paper considers the problem of interpolating source and target representations to generate virtual samples. The authors propose to use the intermediate distributions of the source samples and target samples to generate the virtual samples, which are then used to train the method. "
1387,SP:862d6d76692aee384adc70fd845f0b89cfda93d3,user comments USED-FOR representations. videos CONJUNCTION titles. titles CONJUNCTION videos. representations USED-FOR retrieval. user comments CONJUNCTION videos. videos CONJUNCTION user comments. user comments CONJUNCTION titles. titles CONJUNCTION user comments. attention - based mechanism USED-FOR irrelevant user comments. comments USED-FOR contextualized representations. mechanism USED-FOR contextualized representations. mechanism USED-FOR comments. benchmarks EVALUATE-FOR contextualized representations. ,"This paper proposes to use user comments to learn representations for retrieval from user comments, videos, and titles. Specifically, the authors propose an attention-based mechanism to remove irrelevant user comments. The authors also propose a mechanism to generate contextualized representations from comments. Experiments on several benchmarks demonstrate the effectiveness of the contextualized learned representations."
1388,SP:862d6d76692aee384adc70fd845f0b89cfda93d3,visual input CONJUNCTION textual input. textual input CONJUNCTION visual input. transformer USED-FOR context adapter module. CLIP USED-FOR base architecture. Task is video - text retrieval problem. Material is titles. OtherScientificTerm is user comments. Method is attention mechanism. ,"This paper tackles the video-text retrieval problem. The authors propose a base architecture based on CLIP, which is able to handle both visual input and textual input. The key idea is to use a transformer as the context adapter module, which takes the titles and user comments as input and outputs an attention mechanism."
1389,SP:862d6d76692aee384adc70fd845f0b89cfda93d3,skip connections USED-FOR user comments. skip connections USED-FOR model. Context Adapter Module ( CAM ) HYPONYM-OF component. skip connections USED-FOR component. method COMPARE baseline models. baseline models COMPARE method. contextualized representations of videos USED-FOR method. Generic is it. OtherScientificTerm is visual and text embeddings. Material is video sharing platforms. ,"This paper proposes a new model that uses skip connections to generate user comments. Specifically, it introduces a new component called Context Adapter Module (CAM), which learns both visual and text embeddings. The method is based on contextualized representations of videos and is evaluated on several video sharing platforms. The results show that the proposed method outperforms baseline models."
1390,SP:862d6d76692aee384adc70fd845f0b89cfda93d3,"method COMPARE CLIP4Cilip. CLIP4Cilip COMPARE method. Method are text - based video retrieval method, and CLIP4clip. OtherScientificTerm is user comments. Task is retrieval. ","This paper proposes a text-based video retrieval method, CLIP4clip. The method is based on CLIP2Cilip, which is an existing method that uses user comments as input. The main difference is that the retrieval is done in an end-to-end manner."
1391,SP:a35eb46f391e1a1e347e7243245ca69f4c0f129f,"Method is unsupervised skill learning. OtherScientificTerm are low intrinsic reward, information gain auxiliary objective, ensemble of discriminators, and policy. Material are tabular grid world, and Atari Suite. ","This paper studies the problem of unsupervised skill learning in the presence of low intrinsic reward. The authors propose an information gain auxiliary objective, where an ensemble of discriminators is trained to distinguish between different states of the environment, and a policy is learned to maximize the information gain of the discriminators. Experiments are conducted on a tabular grid world, and on the Atari Suite."
1392,SP:a35eb46f391e1a1e347e7243245ca69f4c0f129f,"DIAYN CONJUNCTION VIC. VIC CONJUNCTION DIAYN. methods USED-FOR discriminator. DIAYN HYPONYM-OF methods. VIC HYPONYM-OF methods. reward bonus COMPARE base method. base method COMPARE reward bonus. bonus USED-FOR policy. DIAYN HYPONYM-OF base method. external reward CONJUNCTION lifetime state coverage. lifetime state coverage CONJUNCTION external reward. pedagogical 4 - room environment CONJUNCTION Atari suite of environments. Atari suite of environments CONJUNCTION pedagogical 4 - room environment. downstream tasks CONJUNCTION lifetime state coverage. lifetime state coverage CONJUNCTION downstream tasks. Atari suite of environments EVALUATE-FOR reward bonus. pedagogical 4 - room environment EVALUATE-FOR reward bonus. external reward HYPONYM-OF downstream tasks. Method are unsupervised skill discovery algorithm, and skill learning process. OtherScientificTerm are discrete ) skill random variable, discriminator uncertainty, disagreement, and VIC nomenclature. ","This paper proposes an unsupervised skill discovery algorithm that learns a (discrete) skill random variable that is independent of the current state of the skill learning process. The authors propose two methods to train the discriminator: DIAYN and VIC. The main idea is to add a reward bonus to the policy that is proportional to the difference between the current policy’s performance and the previous policy (DIAYN or VIC). The authors show that the proposed reward bonus is equivalent to the base method, and that the reward bonus does not depend on discriminator uncertainty, but rather on the disagreement between the policy and the prior policy. The paper evaluates the proposed bonus on a pedagogical 4-room environment, an Atari suite of environments, as well as on downstream tasks, external reward, and lifetime state coverage. The VIC nomenclature is also discussed."
1393,SP:a35eb46f391e1a1e347e7243245ca69f4c0f129f,DIAYN - style methods USED-FOR exploring new parts of the state space. pessimism FEATURE-OF DIAYN - style methods. pessimism FEATURE-OF exploring new parts of the state space. single point estimator USED-FOR discriminator. epistemic uncertainty FEATURE-OF discriminator. epistemic uncertainty PART-OF intrinsic reward. intrinsic reward USED-FOR diversity of skills reward. mixing parameter USED-FOR intrinsic reward. DIAYN - style methods CONJUNCTION count - based methods. count - based methods CONJUNCTION DIAYN - style methods. approach COMPARE methods. methods COMPARE approach. method COMPARE DIAYN - style methods. DIAYN - style methods COMPARE method. Task is exploration. Method is ensemble of discriminators. OtherScientificTerm is ensemble. ,"This paper studies the problem of exploring new parts of the state space with pessimism in DIAYN-style methods. The authors propose to address this problem by introducing an intrinsic reward that incorporates epistemic uncertainty into the discriminator, which is modeled as a single point estimator. This intrinsic reward encourages diversity of skills reward by adding a mixing parameter that encourages exploration among different skills in an ensemble of discriminators. Experiments show that the proposed approach outperforms existing methods, and is able to outperform both traditional and count-based methods."
1394,SP:a35eb46f391e1a1e347e7243245ca69f4c0f129f,"discriminator USED-FOR latent variable. policy CONJUNCTION discriminator. discriminator CONJUNCTION policy. trajectories USED-FOR discriminator. latent variable USED-FOR policy. intrinsic rewards USED-FOR skill discovery. grid world CONJUNCTION Atari. Atari CONJUNCTION grid world. skill discovery CONJUNCTION solving downstream tasks. solving downstream tasks CONJUNCTION skill discovery. Atari EVALUATE-FOR baselines. Atari EVALUATE-FOR skill discovery. Atari EVALUATE-FOR solving downstream tasks. solving downstream tasks EVALUATE-FOR baselines. grid world EVALUATE-FOR skill discovery. grid world EVALUATE-FOR solving downstream tasks. Task is unsupervised RL. OtherScientificTerm are extrinsic reward signal, and auxiliary objective. ","This paper studies unsupervised RL in the setting where there is no extrinsic reward signal. The authors propose to learn a policy and a discriminator that takes trajectories as inputs and outputs a latent variable as an auxiliary objective. The latent variable is used to train the policy and the discriminator is trained to predict the latent variable. The skill discovery is performed using intrinsic rewards and the authors evaluate the skill discovery on a grid world, Atari, and solving downstream tasks against several baselines."
1395,SP:4eafae76b923b75534cd28e6e04774ea69e3c2d1,"method USED-FOR molecular graph. spanning tree USED-FOR method. residual edges USED-FOR circular structure. transformer - based neural network USED-FOR tree construction procedure. transformer - based neural network USED-FOR generative process. method COMPARE methods. methods COMPARE method. method USED-FOR distribution of molecules. method USED-FOR molecular optimization. OtherScientificTerm are molecule, bond, and branch. ","This paper proposes a method to represent a molecular graph as a spanning tree, where each node represents a molecule and each branch represents a bond. The authors propose a generative process that uses a transformer-based neural network to guide the tree construction procedure. The residual edges are used to represent the circular structure of the molecule. Experiments show that the proposed method can represent the distribution of molecules better than existing methods. The proposed method is also applicable to molecular optimization."
1396,SP:4eafae76b923b75534cd28e6e04774ea69e3c2d1,"spanning tree based generative model ( STGG ) USED-FOR molecular graphs. STGG USED-FOR molecule's spanning tree. STGG USED-FOR residual edges. spanning tree construction COMPARE SMILES representation. SMILES representation COMPARE spanning tree construction. molecular graph COMPARE SMILES string. SMILES string COMPARE molecular graph. molecular graph USED-FOR model. attention - based predictor USED-FOR residual edge prediction. tree - based transformer USED-FOR tree generation. relative positional encoding USED-FOR tree generation. tree - based transformer CONJUNCTION attention - based predictor. attention - based predictor CONJUNCTION tree - based transformer. relative positional encoding FEATURE-OF tree - based transformer. tree - based transformer USED-FOR STGG. attention - based predictor USED-FOR STGG. method COMPARE baselines. baselines COMPARE method. ZINC250 K, QM9, and MOSES benchmarks EVALUATE-FOR baselines. ZINC250 K, QM9, and MOSES benchmarks EVALUATE-FOR method. ","This paper proposes a novel spanning tree based generative model (STGG) for molecular graphs. The proposed model is based on a molecular graph instead of the standard SMILES representation. STGG learns a molecule's spanning tree and predicts residual edges. The tree generation is performed using a tree-based transformer with relative positional encoding and an attention-based predictor for residual edge prediction. Experiments on ZINC250K, QM9, and MOSES benchmarks show that the proposed method outperforms the baselines."
1397,SP:4eafae76b923b75534cd28e6e04774ea69e3c2d1,transformer USED-FOR decision sequences. tree - based positional encoding USED-FOR transformer. bipartite graphs USED-FOR molecules. method USED-FOR intermediate graph structure. method USED-FOR molecular structures. OtherScientificTerm is molecular graph. Method is spanning tree - based grammar. Generic is algorithm. ,"This paper proposes a transformer for decision sequences based on tree-based positional encoding. The main idea is to represent molecules as bipartite graphs, where each molecule is represented as a node in a molecular graph. The proposed method learns an intermediate graph structure, which is then used to represent molecular structures. The authors also propose to use a spanning tree -based grammar, which allows the proposed algorithm to generalize to unseen molecules."
1398,SP:4eafae76b923b75534cd28e6e04774ea69e3c2d1,"decisions USED-FOR spanning tree. spanning tree USED-FOR bipartite graph. transformer network USED-FOR decisions. attach bond CONJUNCTION branch start. branch start CONJUNCTION attach bond. branch start HYPONYM-OF decisions. attach bond HYPONYM-OF decisions. Task are graph generation, and generating valid graphs. OtherScientificTerm is residual edges. Method is generation process. ","This paper studies the problem of graph generation. In particular, the authors focus on generating valid graphs. The authors propose a transformer network that learns to generate a bipartite graph from a set of decisions (e.g., attach bond, branch start, etc.). The authors show that residual edges can be removed during the generation process."
1399,SP:3a19340d6af65e3f949dda839a6d233369891c46,model USED-FOR polynomial neural network. eigenvalues FEATURE-OF induced tangent kernel. eigenvalues COMPARE two - layer network. two - layer network COMPARE eigenvalues. induced tangent kernel COMPARE two - layer network. two - layer network COMPARE induced tangent kernel. decay rate FEATURE-OF eigenvalues. polynomial neural networks USED-FOR high - frequency components. ,"This paper proposes a model for training a polynomial neural network. The authors show that the eigenvalues of the induced tangent kernel of a two-layer network have a lower decay rate than that of a standard, standard, and polynomially trained two-layered network. In particular, they show that high-frequency components can be learned efficiently by training a set of high-frequency neural networks. "
1400,SP:3a19340d6af65e3f949dda839a6d233369891c46,deep neural networks ( DNNs ) COMPARE polynomial neural network ( PNN ). polynomial neural network ( PNN ) COMPARE deep neural networks ( DNNs ). polynomial neural network ( PNN ) USED-FOR high - frequency information. PNNs USED-FOR tasks. PNNs USED-FOR face recognition. face recognition HYPONYM-OF tasks. Method is ReLU - activated two - layer neural networks. OtherScientificTerm is high - frequency components. ,"This paper proposes ReLU-activated two-layer neural networks. Compared to deep neural networks (DNNs), the proposed polynomial neural network (PNN) is able to capture more high-frequency information than the standard deep neural network. The authors show that PNNs can generalize to new tasks such as face recognition. The main contribution of this paper is to show that the proposed PNN can be used in a variety of tasks. The paper also shows that the high-frequencies of the ReLU activation of the PNN are highly correlated with the number of layers, which is an interesting observation. "
1401,SP:3a19340d6af65e3f949dda839a6d233369891c46,spectral bias FEATURE-OF polynomial neural networks. ReLU activation function USED-FOR feed - forward neural networks. polynomial networks USED-FOR low frequency and high - frequency terms. spectral analysis USED-FOR integral operator. NTK kernel FEATURE-OF integral operator. integral operator USED-FOR Theory. spectral analysis USED-FOR Theory. network USED-FOR NTK kernel. multiplicative interaction layer PART-OF network. multiplicative interaction layer USED-FOR NTK kernel. multiplicative interactions PART-OF NN. Task is learning high - frequency terms. OtherScientificTerm is high frequencies. ,"This paper studies the spectral bias of polynomial neural networks. The authors consider feed-forward neural networks with a ReLU activation function, where the low frequency and high-frequency terms are modeled by polynomials networks. Theory is based on spectral analysis of the integral operator of the NTK kernel of the network. In particular, the authors show that the multiplicative interaction layer of the NN is responsible for learning high frequencies."
1402,SP:3a19340d6af65e3f949dda839a6d233369891c46,NTK regime FEATURE-OF polynomial networks. Task is learning higher frequencies. ,This paper studies polynomial networks in the NTK regime. The main contribution of this paper is to study the problem of learning higher frequencies. 
1403,SP:ebed8b8a25cead3629832c2ba52caf0059971d3d,"hidden subnetworks PART-OF randomly initiated neural networks. transformation USED-FOR hidden subnetwork weights. transformation USED-FOR subnetworks. disguised subnetworks HYPONYM-OF subnetworks. optimisation problem USED-FOR hidden subnetworks. optimisation problem USED-FOR disguised subnetworks. heuristic algorithm USED-FOR optimisation problem. solution USED-FOR masking variable. binary neural networks USED-FOR sign flipping phase. two - phase problem USED-FOR solution process. OtherScientificTerm are hidden subnetwork, binary vector, randomly initiated weights, sparsity, identity transformation, objective function, transformations, and sign - flips. Generic are latter, and problem. Method is sparse neural networks. ","This paper studies the problem of hiding subnetworks in randomly initiated neural networks. The hidden subnetwork weights can be represented as a transformation of a binary vector, and the transformation can be applied to any subnetwork, including the so-called “confronted” ones. The authors propose an optimisation problem for the hidden subnets, i.e., the so called “discriminatorial” hidden subneighbourhoods, which they call “substituted subnets”. The idea is to use a heuristic algorithm to solve the optimisation of the “contrastive” (i.e. “hidden”) and “masking” versions of the latter. The solution process is a two-phase problem. The first phase is the sign flipping phase, which uses binary neural networks, where the randomly initiated weights are transformed into a masking variable. The second phase is a “sign-flip” phase, in which the sparsity is reduced to zero and the identity transformation is applied to the original objective function. The main contribution of this paper is to study the problem in the context of sparse neural networks and to show that the transformations can be used as sign-flips."
1404,SP:ebed8b8a25cead3629832c2ba52caf0059971d3d,"hidden subnetworks PART-OF randomly initialized NN. definition USED-FOR disguised subnetworks. hidden subnetworks PART-OF disguised subnetworks. Task is Optimizing sparse neural networks. Method are sparse neural networks, and Peek - a - Boo ( PaB ). Material is lottery ticket hypothesis. Generic are algorithm, and networks. ","Optimizing sparse neural networks is an important problem in practice. This paper proposes a new algorithm called Peek-a-Boo (PaB), which is motivated by the lottery ticket hypothesis. The authors propose a new definition of ""concealed subnetworks"" that can be seen as a subset of a randomly initialized NN. The main contribution of this paper is to propose an algorithm, which is based on the notion of ""hidden subnets"". The authors show that the networks can be trained in a way that is similar to the way that networks are trained in the real world."
1405,SP:ebed8b8a25cead3629832c2ba52caf0059971d3d,PaB USED-FOR subnetworks. transformation USED-FOR subnetwork. random weights USED-FOR mask. pruning - at - initialization techniques USED-FOR mask. OtherScientificTerm is hidden random subnetworks. Method is PaB process. ,"This paper studies the problem of learning hidden random subnetworks from data. The authors propose to use PaB to learn the underlying structure of the subnetwork using a transformation. The key idea is to use random weights as the mask, and then use pruning-at-initialization techniques to prune the mask. The main contribution of this paper is to show that the PaB process can be extended to the case where the weights are not random."
1406,SP:ebed8b8a25cead3629832c2ba52caf0059971d3d,"algorithm USED-FOR network pruning. network pruning CONJUNCTION optimization. optimization CONJUNCTION network pruning. algorithm USED-FOR optimization. Method is peek - a - boo ( PaB ). Generic are setting, and two - step algorithm. Metric is optimization complexity. ","This paper proposes a new algorithm for network pruning and optimization called peek-a-boo (PaB). In this setting, the goal is to minimize the optimization complexity. The authors propose a two-step algorithm."
1407,SP:fa9e46f1dc70dbe87ff53a6b8dd5419c14b40ef3,graph structures CONJUNCTION node features. node features CONJUNCTION graph structures. GNN model USED-FOR adversarial attacks. general unified graph neural network USED-FOR adversarial attacks. general unified graph neural network USED-FOR graph structure. graph structure CONJUNCTION node features. node features CONJUNCTION graph structure. general unified graph neural network USED-FOR node features. node features USED-FOR adversarial attacks. operation USED-FOR graph. node features USED-FOR operation. graph structure USED-FOR operation. operation USED-FOR node features. Generic is two - step approach. Material is small datasets. ,"This paper proposes a general unified graph neural network that jointly learns graph structure and node features for adversarial attacks with a GNN model. The authors propose a two-step approach. First, they learn the operation to reconstruct the original graph using the graph structure as well as the node features. Second, they use the learned node features to train the GNN. Experiments are conducted on small datasets."
1408,SP:fa9e46f1dc70dbe87ff53a6b8dd5419c14b40ef3,"Unified Graph Neural Network ( GUGNN ) framework USED-FOR graph and feature denoising. framework USED-FOR graph neural network ( R - GUGNN ). graph neural network ( R - GUGNN ) USED-FOR poison adversarial attacks. poison adversarial attacks FEATURE-OF semi - supervised node classification setting. Cora CONJUNCTION Citeseer. Citeseer CONJUNCTION Cora. GCN - SVD CONJUNCTION Pro - GNN. Pro - GNN CONJUNCTION GCN - SVD. Citeseer CONJUNCTION Cora - ML & Polblogs. Cora - ML & Polblogs CONJUNCTION Citeseer. RGCN CONJUNCTION GCN - SVD. GCN - SVD CONJUNCTION RGCN. datasets EVALUATE-FOR method. method COMPARE graph neural networks ( GCN, GAT ). graph neural networks ( GCN, GAT ) COMPARE method. method COMPARE models. models COMPARE method. graph neural networks ( GCN, GAT ) CONJUNCTION models. models CONJUNCTION graph neural networks ( GCN, GAT ). method USED-FOR perturbation. RGCN HYPONYM-OF models. Pro - GNN HYPONYM-OF models. GCN - SVD HYPONYM-OF models. Cora HYPONYM-OF datasets. Cora - ML & Polblogs HYPONYM-OF datasets. Citeseer HYPONYM-OF datasets. ","This paper proposes a unified Graph Neural Network (GUGNN) framework for graph and feature denoising. The proposed framework enables the training of a graph neural network (R-UGN) that is robust to poison adversarial attacks in the semi-supervised node classification setting. The method is evaluated on three datasets: Cora, Citeseer, Cora-ML & Polblogs. The experiments show that the proposed method is more robust to perturbation than graph neural networks (GCN, GAT) and other models such as RGCN and GCN-SVD and Pro-GNN."
1409,SP:fa9e46f1dc70dbe87ff53a6b8dd5419c14b40ef3,"framework USED-FOR graph structure. graph structure CONJUNCTION features. features CONJUNCTION graph structure. graph cleaning CONJUNCTION feature cleaning. feature cleaning CONJUNCTION graph cleaning. feature diffusion / propagation USED-FOR feature cleaning. Task are adversarial manipulations of graphs, and sparse, trace minimization problem. ","This paper proposes a framework for learning graph structure and features that can be used to defend against adversarial manipulations of graphs. Specifically, the authors consider the sparse, trace minimization problem and propose to use feature diffusion/propagation for feature cleaning and graph cleaning."
1410,SP:fa9e46f1dc70dbe87ff53a6b8dd5419c14b40ef3,robust model USED-FOR graph neural networks ( GNNs ). robust model USED-FOR adversarial attacks. perturbed structure CONJUNCTION node features. node features CONJUNCTION perturbed structure. defense mechanisms USED-FOR perturbed structure. objective function USED-FOR components. targeted attack CONJUNCTION untargeted or global attack. untargeted or global attack CONJUNCTION targeted attack. untargeted or global attack CONJUNCTION random attack. random attack CONJUNCTION untargeted or global attack. targeted attack HYPONYM-OF attack models. untargeted or global attack HYPONYM-OF attack models. random attack HYPONYM-OF attack models. ablation study CONJUNCTION hyper - parameters. hyper - parameters CONJUNCTION ablation study. Generic is baselines. ,"This paper proposes a robust model for graph neural networks (GNNs) to defend against adversarial attacks. The key idea is to design defense mechanisms that are robust to perturbed structure and node features. The proposed components are modeled as an objective function. Experiments are conducted on three different attack models: targeted attack, untargeted or global attack, and random attack. The ablation study and the hyper-parameters are also performed. Results are compared to baselines."
1411,SP:fcb14510ef8541f320ef1c3cab4c0c017e2e15b9,method USED-FOR shape and UV parameterization. SDF based implicit shape representation CONJUNCTION neural volumetric renderer IDR. neural volumetric renderer IDR CONJUNCTION SDF based implicit shape representation. regularization terms CONJUNCTION fixed scale conformal mapping. fixed scale conformal mapping CONJUNCTION regularization terms. well behaved UV space CONJUNCTION fixed scale conformal mapping. fixed scale conformal mapping CONJUNCTION well behaved UV space. well behaved UV space FEATURE-OF regularization terms. MLPs USED-FOR bijective mapping. implicit field representation USED-FOR surface. MLPs USED-FOR it. graphics techniques USED-FOR 3D->2D mapping. Method is IDR renderer. OtherScientificTerm is UV space. ,"This paper proposes a method for shape and UV parameterization based on the SDF based implicit shape representation and the neural volumetric renderer IDR. The IDR renderer is trained with several regularization terms, including a well behaved UV space, a fixed scale conformal mapping, and a bijective mapping based on MLPs. The surface is represented by an implicit field representation. The 3D->2D mapping is done using graphics techniques. The UV space is modeled using MLPs, and it is trained in a supervised manner."
1412,SP:fcb14510ef8541f320ef1c3cab4c0c017e2e15b9,"neural rendering technique USED-FOR implicit surface. implicit surface CONJUNCTION UV - parameterization. UV - parameterization CONJUNCTION implicit surface. neural rendering technique USED-FOR UV - parameterization. 2D images USED-FOR UV - parameterization. approach USED-FOR document unwarping. approach COMPARE work. work COMPARE approach. approach COMPARE DewarpNet. DewarpNet COMPARE approach. work COMPARE DewarpNet. DewarpNet COMPARE work. Local Distortion ( LD ) USED-FOR DewarpNet. Local Distortion ( LD ) EVALUATE-FOR approach. IDR USED-FOR explicit bijective texture mapping. explicit bijective texture mapping USED-FOR implicit surface. loss function formulation USED-FOR framework. implicit surfaces USED-FOR document unwarping. reduction in texture distortion FEATURE-OF implicit surfaces. MLP CONJUNCTION appearance MLP. appearance MLP CONJUNCTION MLP. appearance MLP USED-FOR color. MLP USED-FOR surface. problem formulation USED-FOR IDR. MLP PART-OF problem formulation. MLP USED-FOR IDR. appearance MLP PART-OF problem formulation. forward and backward MLP USED-FOR bijective UV mapping. function of texture coordinates FEATURE-OF appearance MLP. pixelwise rendering loss USED-FOR MLPs. Method is texture mapping. OtherScientificTerm are unwarped document, and texture space. Task is OCR. Metric is OCR metrics. Generic is It. ","This paper proposes a neural rendering technique to learn an implicit surface and UV-parameterization from 2D images. The approach is compared to previous work on document unwarping and compared to DewarpNet with Local Distortion (LD). The main contribution of the paper is to propose a loss function formulation for the proposed framework. IDR learns an explicit bijective texture mapping for the implicit surface, which is then used to learn a texture mapping to the surface of the original unwarped document. The paper shows that the learned implicit surfaces can be used to improve the performance of the document unwarping by reducing in texture distortion. The problem formulation of IDR consists of an MLP that maps the surface to a function of texture coordinates, an appearance MLP which maps the color to a texture space, and a forward and backward MLP to learn the bijection of the UV mapping. These MLPs are trained with a pixelwise rendering loss. It is shown that the proposed approach can improve OCR performance on several OCR metrics."
1413,SP:fcb14510ef8541f320ef1c3cab4c0c017e2e15b9,optimization of neural networks USED-FOR document unwrapping. mapping function CONJUNCTION inverse mapping. inverse mapping CONJUNCTION mapping function. MLPs USED-FOR implicit surface of the target. MLPs USED-FOR inverse mapping. mapping function HYPONYM-OF MLPs. differentiable rendering USED-FOR MLPs. OtherScientificTerm is 2D texture coordinates. Method is NERF. ,"This paper studies the optimization of neural networks for document unwrapping. Specifically, the authors propose two MLPs: a mapping function that maps 2D texture coordinates to the target, and an inverse mapping that maps the target to the mapping function. The MLPs are trained with differentiable rendering, which allows the MLPs to map the implicit surface of the target. NERF is also proposed."
1414,SP:fcb14510ef8541f320ef1c3cab4c0c017e2e15b9,multi - view method USED-FOR undistorting documents. method ( IDR ) USED-FOR distorted documents. bijective neural network USED-FOR distorted surfaces. 3 - D FEATURE-OF distorted documents. bijective neural network USED-FOR method. method ( IDR ) USED-FOR method. method COMPARE SOTAs. SOTAs COMPARE method. ,This paper proposes a multi-view method for undistorting documents. The proposed method (IDR) uses a bijective neural network to reconstruct distorted surfaces in 3-D. Experiments show that the proposed method outperforms SOTAs.
1415,SP:28ac9848fb69d1c59fd751fbeee9a4ac799db897,ARP COMPARE multi - stage cascade architecture. multi - stage cascade architecture COMPARE ARP. ARP USED-FOR critical regions. Task is fine - grained representation. OtherScientificTerm is fine - grained information. ,"This paper addresses the problem of learning a fine-grained representation. The authors propose a novel multi-stage cascade architecture, called ARP, which is able to learn the critical regions in a more efficient way, while preserving fine-granularity of the representation. This is an interesting direction to explore, and the authors show that ARP can achieve better performance than the multi-step cascade architecture. The paper is well-written and easy to follow. However, there are some issues that need to be addressed in order for the paper to be accepted."
1416,SP:28ac9848fb69d1c59fd751fbeee9a4ac799db897,"excessive reduction operations USED-FOR image resolution. excessive reduction operations USED-FOR discriminative features. Adaptive Region Pooling ( ARP ) HYPONYM-OF pooling algorithm. procedures PART-OF module. fine - grained classification CONJUNCTION re - identification. re - identification CONJUNCTION fine - grained classification. re - identification EVALUATE-FOR method. fine - grained classification EVALUATE-FOR method. tasks EVALUATE-FOR method. re - identification HYPONYM-OF tasks. fine - grained classification HYPONYM-OF tasks. Method are fine - grained methods, and bilinear operation. ","This paper proposes Adaptive Region Pooling (ARP), a pooling algorithm that uses excessive reduction operations to reduce image resolution while preserving discriminative features. The proposed module consists of two procedures: (1) a bilinear operation that computes the number of regions to pool and (2) an additional pooling operation that aggregates regions from different fine-grained methods. The method is evaluated on two tasks: fine-regained classification and re-identification."
1417,SP:28ac9848fb69d1c59fd751fbeee9a4ac799db897,adaptive region pooling ( ARP ) method USED-FOR fine - grained representation learning. ARP USED-FOR features. estimated scale USED-FOR ARP. estimated scale USED-FOR features. bilinear downsampling USED-FOR features. tasks EVALUATE-FOR method. ,This paper proposes an adaptive region pooling (ARP) method for fine-grained representation learning. ARP uses an estimated scale to pool features from different regions using bilinear downsampling. The proposed method is evaluated on several tasks.
1418,SP:28ac9848fb69d1c59fd751fbeee9a4ac799db897,"adaptive region pooling ( ARP ) module USED-FOR scale factors. adaptive region pooling ( ARP ) module USED-FOR discriminative regions. scale factors USED-FOR discriminative regions. ARP module USED-FOR fine - grained image recognition. ARP module USED-FOR Vehicle Re - Id tasks. fine - grained image recognition CONJUNCTION Vehicle Re - Id tasks. Vehicle Re - Id tasks CONJUNCTION fine - grained image recognition. OtherScientificTerm are discriminative region, and fine - grained information. ",This paper proposes an adaptive region pooling (ARP) module that learns scale factors for discriminative regions. The proposed ARP module is applied to fine-grained image recognition and Vehicle Re-Id tasks. The key idea is to pool the regions of the discriminatively region into smaller regions that are more likely to contain fine-granular information.
1419,SP:8648453f5a7c5e9b99a8fdbaa340f4e2b4d048d0,out - of - distribution problem USED-FOR node - level prediction on graphs. ego - graphs PART-OF graph. IID USED-FOR ego - graphs. model USED-FOR adversarial attack. OOD problem USED-FOR node - level tasks on graphs. EERM ) COMPARE baseline ERM. baseline ERM COMPARE EERM ). OtherScientificTerm is invariance perspective. Generic is approach. Method is learning approach. ,"This paper considers node-level prediction on graphs as an out-of-distribution problem, which is an important problem from an invariance perspective. The authors propose an approach called IID, which aims to learn ego-graphs in a graph that are invariant to adversarial attacks. The model is trained to defend against adversarial attack, and the authors show that their model (EERM) outperforms a baseline ERM. They also show that this OOD problem can be applied to node level tasks on graphs, and demonstrate that their learning approach is effective."
1420,SP:8648453f5a7c5e9b99a8fdbaa340f4e2b4d048d0,"invariant risk minimization approach USED-FOR distribution shift. ego - graphs PART-OF graph. auxiliary context generators USED-FOR variance loss. OtherScientificTerm are structural information, and learning objective. ","This paper proposes an invariant risk minimization approach to deal with distribution shift. The key idea is to consider ego-graphs in the graph, and to use auxiliary context generators to minimize the variance loss. The structural information is learned as a function of the learning objective."
1421,SP:8648453f5a7c5e9b99a8fdbaa340f4e2b4d048d0,distribution shifts USED-FOR out - of - distribution generalization. invariant risk minimization USED-FOR OOD problem. policy gradient USED-FOR graph editing. Method is EERM. ,"This paper studies the problem of out-of-distribution generalization under distribution shifts. The authors propose to solve the OOD problem with invariant risk minimization. The main contribution of the paper is the introduction of EERM, which is a generalization of policy gradient for graph editing."
1422,SP:8648453f5a7c5e9b99a8fdbaa340f4e2b4d048d0,"graph USED-FOR OOD generalization. Amazon - Photo CONJUNCTION Twitch - explicit. Twitch - explicit CONJUNCTION Amazon - Photo. Cora CONJUNCTION Amazon - Photo. Amazon - Photo CONJUNCTION Cora. datasets EVALUATE-FOR model. Cora HYPONYM-OF datasets. Amazon - Photo HYPONYM-OF datasets. Twitch - explicit HYPONYM-OF datasets. Distributionally robust neural networks USED-FOR group shifts. regularization USED-FOR worst - case generalization. risk extrapolation ( rex ) USED-FOR Out - of - distribution generalization. Environment inference USED-FOR invariant learning. Task are out - of - domain generalization, Invariant risk minimization, and Machine Learning. Material is graph - structured datasets. OtherScientificTerm is structural information. Generic is method. Metric is variance of risk. ","This paper addresses the problem of out-of-domain generalization (OOD) on graph-structured datasets. Invariant risk minimization is an important problem in Machine Learning, and this paper proposes a method to address OOD generalization on graph. The proposed model is evaluated on three datasets: Cora, Amazon-Image, and Twitch-explicit. Distributionally robust neural networks are used to model group shifts and regularization is used to mitigate the worst-case generalization. Out-of -distribution generalization is achieved by risk extrapolation (rex). Environment inference is used for invariant learning. The variance of risk is estimated by minimizing the difference between the mean and variance of the true risk."
1423,SP:870cd8794f7ff48fbed71c2abc9fb7dad51bd343,method USED-FOR learning augmentations. learning augmentations USED-FOR contrastive learning of time series data. Concrete / Gumbel - Softmax distributions USED-FOR learnable augmentation strategy. Method is InfoTS. Material is time - series forecasting and classification benchmarks. ,"This paper proposes InfoTS, a method for learning augmentations for contrastive learning of time series data. The learnable augmentation strategy is based on Concrete/Gumbel-Softmax distributions. Experiments are conducted on time-series forecasting and classification benchmarks."
1424,SP:870cd8794f7ff48fbed71c2abc9fb7dad51bd343,information - aware approach USED-FOR representation learning. representation learning USED-FOR time series. formulation USED-FOR data augmentations. time series datasets EVALUATE-FOR methods. forecasting CONJUNCTION classification. classification CONJUNCTION forecasting. time series datasets USED-FOR classification. time series datasets USED-FOR forecasting. OtherScientificTerm is information - theoretic viewpoints. Metric is optimization criteria. ,This paper proposes an information-aware approach to representation learning for time series from information-theoretic viewpoints. The formulation allows for data augmentations that are not constrained by optimization criteria. The proposed methods are evaluated on two time series datasets for forecasting and classification.
1425,SP:870cd8794f7ff48fbed71c2abc9fb7dad51bd343,"information theory CONJUNCTION meta learning approach. meta learning approach CONJUNCTION information theory. meta learning approach CONJUNCTION approach. approach CONJUNCTION meta learning approach. meta learning approach USED-FOR data augmentation approach. information theory USED-FOR data augmentation approach. Material are time series, and images. Method is contrastive learning. ","This paper proposes a data augmentation approach based on information theory, a meta learning approach, and an approach to augment time series with images. The idea is similar to contrastive learning."
1426,SP:870cd8794f7ff48fbed71c2abc9fb7dad51bd343,contrastive learning framework USED-FOR time series data. fidelity and variety criterion USED-FOR data augmentations. contrastive learning objective USED-FOR local and global level. criteria FEATURE-OF contrastive learning objective. time - series forecasting and classification task EVALUATE-FOR model. ,This paper proposes a contrastive learning framework for time series data. The key idea is to use fidelity and variety criterion for data augmentations. The authors propose a new contrastivelearning objective that satisfies these criteria on both local and global level. The proposed model is evaluated on a time-series forecasting and classification task.
1427,SP:6bc677d060ba4ab09f6da61458680e7a7976644b,"RG perspective USED-FOR IMP. large scale lottery ticket experiments EVALUATE-FOR theory. Task is Lottery Ticket Hypothesis. OtherScientificTerm are winning tickets, winning ticket universality, renormalization group ( RG ), and phase transitions. Method are renormalization group theory, iterative magnitude pruning ( IMP ), and RG. ","This paper studies the Lottery Ticket Hypothesis, i.e., the question of whether winning tickets can be represented by a renormalization group (RG) that is invariant to phase transitions. In particular, the authors consider the case where winning ticket universality is not guaranteed, which is an important question in renormalizing group theory. The authors propose an extension of iterative magnitude pruning (IMP) from the RG perspective to IMP. The theory is validated on large scale lottery ticket experiments."
1428,SP:6bc677d060ba4ab09f6da61458680e7a7976644b,"renormalization group ( RG ) theory USED-FOR physics. residual block PART-OF ResNet model. Task are Lottery Ticket Hypothesis ( LTH ), and IMP. Method are iterative magnitude pruning ( IMP ), pruning scheme, ResNet-50 models, IMP map, ResNet-50, and RG. OtherScientificTerm are RG flow, Hamiltonian, non - zero parameters, and scalar, dynamical quantities. Generic are quantities, and smallest model. ","This paper studies the Lottery Ticket Hypothesis (LTH) and proposes iterative magnitude pruning (IMP), a pruning scheme that aims to reduce the residual block in a ResNet model. This is motivated by the renormalization group (RG) theory in physics, where the RG flow is defined as the Hamiltonian of a set of non-zero parameters. The authors propose IMP as an extension of ResNet-50 models, which they call IMP map, and show that IMP map can be used to reduce residual block size in ResNet 50 models. They also provide a theoretical analysis of IMP, showing that IMP can be viewed as a function of scalar, dynamical quantities, and that these quantities can be expressed in terms of the smallest model. Finally, they show that the IMP map is invariant to the number of parameters in the RG."
1429,SP:6bc677d060ba4ab09f6da61458680e7a7976644b,lottery ticket USED-FOR tasks. renormalization group CONJUNCTION lottery ticket hypothesis. lottery ticket hypothesis CONJUNCTION renormalization group. renormalization group scheme USED-FOR iterative magnitude pruning. Method is ResNet families. ,This paper proposes a renormalization group scheme for iterative magnitude pruning and the lottery ticket hypothesis for tasks where a lottery ticket can be used to solve tasks. The authors propose to use ResNet families and show that the proposed renormalized version of ResNet works well.
1430,SP:6bc677d060ba4ab09f6da61458680e7a7976644b,pruning method USED-FOR lottery ticket hypothesis. pruning method CONJUNCTION concept. concept CONJUNCTION pruning method. statistical physics USED-FOR concept. renormalization group HYPONYM-OF concept. iterative magnitude pruning HYPONYM-OF pruning method. renormalization group HYPONYM-OF statistical physics. renormalization group USED-FOR Elastic Lottery Ticket Hypothesis. universality classes FEATURE-OF pruned models. Generic is task. OtherScientificTerm is renormalization group operator. ,"This paper proposes a pruning method (i.e., iterative magnitude pruning) and a new concept (the renormalization group) inspired by statistical physics to tackle the lottery ticket hypothesis. The authors propose a new task called “Elastic Lottery Ticket Hypothesis”, where the renormalized version of the task is used to prune the pruned models in universality classes. The paper also proposes a new renormization group operator."
1431,SP:59ce2e6c3674157d6fa990316812d0823c1ec586,"dual - encoder ( DE ) models USED-FOR document reranking. cross - attention ( CA ) CONJUNCTION dual - encoder ( DE ) models. dual - encoder ( DE ) models CONJUNCTION cross - attention ( CA ). cross - attention ( CA ) USED-FOR document reranking. cross - attention models COMPARE dual - encoder models. dual - encoder models COMPARE cross - attention models. infinite dimension USED-FOR dual - encoders models. generalization ability FEATURE-OF DE models. teacher ( CA ) CONJUNCTION student ( DE ) models. student ( DE ) models CONJUNCTION teacher ( CA ). margin USED-FOR probability matching. softmax cross - entropy loss USED-FOR margin. distillation approaches USED-FOR CA models. distillation approaches USED-FOR DE. generalization ability FEATURE-OF CA model. Metric is computational cost. OtherScientificTerm are Mercer's theorem, overfitting, and loss function. Method are CA and DE models, distillation algorithms, DE and CA models, and distillation approach. ","This paper studies the generalization ability of cross-attention (CA) and dual-encoder (DE) models for document reranking. In particular, the authors show that the generalizability ability of CA models can be improved by distillation approaches to DE, which can reduce the computational cost. The authors also provide a theoretical analysis of the convergence of CA and DE models to infinite dimension using Mercer's theorem. The main contribution of the paper is to propose distillation algorithms to reduce the overfitting of the DE and CA models. The proposed distillation approach consists of two steps. First, a margin is added to the probability matching using a softmax cross-entropy loss. Second, a loss function is used to minimize the variance of the loss function. Finally, the author conducts experiments on teacher (CA), student (DE), and teacher-student (CA)."
1432,SP:59ce2e6c3674157d6fa990316812d0823c1ec586,dual - encoder models CONJUNCTION cross - attention models. cross - attention models CONJUNCTION dual - encoder models. models USED-FOR information retrieval problems. cross - attention models HYPONYM-OF models. dual - encoder models HYPONYM-OF models. Cross - attention models COMPARE dual encoder models. dual encoder models COMPARE Cross - attention models. cross - attention models CONJUNCTION dual encoder models. dual encoder models CONJUNCTION cross - attention models. Task is real search systems. OtherScientificTerm is embedding dimensions. Generic is method. ,"This paper proposes two models for information retrieval problems: dual-encoder models and cross-attention models, which are two models that are commonly used in real search systems, but which are different in their embedding dimensions. The authors show that Cross-attentive models are more efficient than dual encoder models, and that the proposed method can generalize to unseen data points. "
1433,SP:59ce2e6c3674157d6fa990316812d0823c1ec586,"cross - attention BERT CONJUNCTION dual - encoder BERT. dual - encoder BERT CONJUNCTION cross - attention BERT. dual - encoder BERT USED-FOR re - ranking task. cross - attention BERT USED-FOR re - ranking task. KD method USED-FOR re - ranking tasks. Method are knowledge distillation method, cross - attention BERT model, and dual - encoder BERT model. ",This paper proposes a knowledge distillation method for re-ranking tasks. The key idea is to use cross-attention BERT and dual-encoder BERT to solve the re-rating task. The authors propose to use the KD method to re-rank tasks and train the cross-audience BERT model on top of the dual-ensecoder BERT. Experiments are conducted to show the effectiveness of the proposed KD method.
1434,SP:59ce2e6c3674157d6fa990316812d0823c1ec586,bi - encoder architectures USED-FOR re - ranking. cross - encoder USED-FOR re - ranking. re - ranking USED-FOR information retrieval. cross - encoder CONJUNCTION bi - encoder architectures. bi - encoder architectures CONJUNCTION cross - encoder. bi - encoders COMPARE cross - encoders. cross - encoders COMPARE bi - encoders. bi - encoder USED-FOR bi - encoder. distillation loss function COMPARE cross - entropy. cross - entropy COMPARE distillation loss function. Method is cross - encoder model. ,"This paper proposes a new cross-encoder model for re-ranking in the context of information retrieval. The authors compare the performance of the cross-encoders and the bi-ensecoders architectures in terms of the performance on the task of re-re-ranking with respect to the original data. They show that bi-encoders perform better than the original cross-entropy, and that the performance gap between the bi encoder and the original one can be reduced by using a distillation loss function instead of the standard cross-estentropy."
1435,SP:679e57a2027ff1855e5dc80bd3ec91f6489cc747,convergence CONJUNCTION implicit trust region. implicit trust region CONJUNCTION convergence. implicit trust region HYPONYM-OF policy improvement algorithm. convergence HYPONYM-OF policy improvement algorithm. surrogate objective USED-FOR algorithm. non - central alpha - moment FEATURE-OF finite sample objective. non - central alpha - moment FEATURE-OF surrogate objective. algorithm USED-FOR policy optimization. policy optimization USED-FOR reinforcement learning. algorithm COMPARE trust region baselines. trust region baselines COMPARE algorithm. continuous control EVALUATE-FOR algorithm. small batch sizes FEATURE-OF robustness. robustness EVALUATE-FOR trust region baselines. robustness EVALUATE-FOR algorithm. Task is Monte Carlo simulation community. ,"This paper proposes a new policy improvement algorithm, called convergence and implicit trust region, which is inspired by the Monte Carlo simulation community. The algorithm uses a surrogate objective with a non-central alpha-moment for a finite sample objective. The authors show that the proposed algorithm can be used for policy optimization in reinforcement learning. The proposed algorithm is evaluated on continuous control and compared to several trust region baselines on robustness to small batch sizes."
1436,SP:679e57a2027ff1855e5dc80bd3ec91f6489cc747,importance sampling USED-FOR variance reduction. POPE HYPONYM-OF policy optimization algorithm. algorithm USED-FOR minimal variance policy. policy USED-FOR policy improvement. policy improvement step USED-FOR minimal variance policy. TRPO CONJUNCTION POIS baselines. POIS baselines CONJUNCTION TRPO. low - dimensional control tasks EVALUATE-FOR linear policy classes. ,"This paper proposes POPE, a policy optimization algorithm that uses importance sampling for variance reduction. The algorithm first learns a minimal variance policy and then uses this policy for policy improvement. Experiments on linear policy classes are conducted on several low-dimensional control tasks and compared with TRPO and POIS baselines."
1437,SP:679e57a2027ff1855e5dc80bd3ec91f6489cc747,"IS USED-FOR RL. fixed _ policy behavior CONJUNCTION optimization policy. optimization policy CONJUNCTION fixed _ policy behavior. IS USED-FOR variance minimization technique. minimum variance behavior USED-FOR policy improvement. minimum variance USED-FOR policy. process USED-FOR local or global optimum. trust - region methods USED-FOR RL. bounded Renyi divergence FEATURE-OF policy. parameter distribution CONJUNCTION state distribution. state distribution CONJUNCTION parameter distribution. REPS CONJUNCTION TRPO. TRPO CONJUNCTION REPS. trust region FEATURE-OF operator. monotonic function $ h$ CONJUNCTION batch - size. batch - size CONJUNCTION monotonic function $ h$. monotonic function $ h$ HYPONYM-OF hyperparameters. batch - size HYPONYM-OF hyperparameters. variance reduction CONJUNCTION policy improvement. policy improvement CONJUNCTION variance reduction. IS USED-FOR variance reduction. IS USED-FOR policy improvement. improvement scheme USED-FOR policy gradient optimization. Method are Importance sampling ( IS ), and divergence minimization ( or constraint ) approaches. OtherScientificTerm are behavior policy, optimum, and return distribution. Generic are approach, divergence, approximation, unconstrained operator, algorithm, it, and approaches. Task is learning process. Material is control benchmarks. ","This paper proposes Importance sampling (IS), a variance minimization technique based on IS that aims to reduce the variance of RL using trust-region methods. The approach is motivated by the observation that in RL, the divergence between a fixed_policy behavior and an optimization policy can be bounded by a bounded Renyi divergence between the behavior policy and the optimization policy. The authors argue that this divergence can be used as an approximation to the optimum of a process that can converge to a local or global optimum. In particular, the authors show that the minimum variance behavior of the policy can lead to policy improvement when the policy is trained with minimum variance.  The authors propose two divergence minimization (or constraint) approaches. The first approach is to learn an unconstrained operator that is independent of the parameters of the parameter distribution and the state distribution. The second approach is a variant of REPS and TRPO, where the operator is learned in the trust region. The main difference between these approaches is that REPS requires the operator to be independent of hyperparameters such as a monotonic function $h$ and batch-size, whereas TRPO requires that the operator be independent from the parameters $h$. The authors show empirically that their algorithm is able to converge to an optimum that is bounded by the return distribution of the learned operator.  They also propose an improvement scheme for policy gradient optimization based on their proposed improvement scheme. They evaluate their algorithm on a variety of control benchmarks and show that their approach leads to better variance reduction and policy improvement. "
1438,SP:679e57a2027ff1855e5dc80bd3ec91f6489cc747,"policy improvement USED-FOR RL. non - negative policy improvements CONJUNCTION convergence. convergence CONJUNCTION non - negative policy improvements. OtherScientificTerm are optimal behavior policy, implicit trust regions, and policy class. Material is low - dimensional RL examples. ",This paper studies the problem of policy improvement in RL. The authors show that non-negative policy improvements and convergence to the optimal behavior policy can be achieved when the implicit trust regions of the policy class are large enough. The results are validated on low-dimensional RL examples.
1439,SP:b169c94c8fcc4f13cafdbafbe18eb26cb471ea0b,method USED-FOR large scale graph neural networks. DimeNet++ CONJUNCTION GemNet models. GemNet models CONJUNCTION DimeNet++. method USED-FOR GemNet models. method USED-FOR DimeNet++. large GNN models COMPARE baselines. baselines COMPARE large GNN models. tasks EVALUATE-FOR baselines. Open Catalyst 2020 ( OC20 ) benchmark EVALUATE-FOR baselines. Open Catalyst 2020 ( OC20 ) benchmark FEATURE-OF tasks. tasks EVALUATE-FOR large GNN models. Open Catalyst 2020 ( OC20 ) benchmark EVALUATE-FOR large GNN models. OtherScientificTerm is Graph Parallelism. ,This paper presents a method for training large scale graph neural networks. The method is applied to DimeNet++ and GemNet models. The authors show that large GNN models trained on the Open Catalyst 2020 (OC20) benchmark outperform baselines on a variety of tasks. Graph Parallelism is also discussed.
1440,SP:b169c94c8fcc4f13cafdbafbe18eb26cb471ea0b,"distributed training method USED-FOR large graph neural networks ( GNN ). distributed training method USED-FOR billion parameters. triplet update operations USED-FOR GPUs. edge update operations USED-FOR GPUs. node update CONJUNCTION global node aggregation. global node aggregation CONJUNCTION node update. global node aggregation USED-FOR it. node update USED-FOR it. GemNet CONJUNCTION DimeNet. DimeNet CONJUNCTION GemNet. method USED-FOR DimeNet. method USED-FOR GemNet. OC20 benchmark EVALUATE-FOR method. OtherScientificTerm are global synchronization, and edge vectors. ","This paper proposes a distributed training method for large graph neural networks (GNN) that can handle billion parameters at a time. The main idea is to use triplet update operations on GPUs instead of the standard edge update operations. This allows for global synchronization between nodes and edge vectors. In addition, it also uses node update and global node aggregation. The proposed method is evaluated on the OC20 benchmark for GemNet and DimeNet."
1441,SP:b169c94c8fcc4f13cafdbafbe18eb26cb471ea0b,"approach USED-FOR graph neural networks. structures USED-FOR graph neural networks. Higher - order interaction terms FEATURE-OF networks. parallel approach USED-FOR GNNs. OtherScientificTerm are GPUs, and molecular structure. Material is OpenCatalyst benchmark. ","This paper presents a novel approach to training graph neural networks with different structures. Higher-order interaction terms are introduced to encourage the networks to be more interpretable. This parallel approach is also applied to GNNs. Experiments are conducted on GPUs and on the OpenCatalyst benchmark, where the molecular structure is compared."
1442,SP:b169c94c8fcc4f13cafdbafbe18eb26cb471ea0b,"models PART-OF framework. parallel framework USED-FOR parameter count. OtherScientificTerm are extended graphs, and triplets. ",This paper proposes a framework that combines models from the literature. The main idea is to use extended graphs where the parameter count can be computed using a parallel framework. The authors propose to use triplets to represent the number of nodes in the triplets. 
1443,SP:352c177d89b9460acee0c78364e6d9c153c6a93c,"Brownian Bridge process HYPONYM-OF stochastic process. Brownian motion USED-FOR autoregressive models. GPT-2 HYPONYM-OF autoregressive models. Brownian bridge process USED-FOR generation. Brownian bridge dynamics FEATURE-OF zt. negatively sampled x't USED-FOR contrastive loss. local coherence CONJUNCTION long range order sensitivity. long range order sensitivity CONJUNCTION local coherence. long range order sensitivity CONJUNCTION generation of long sequences. generation of long sequences CONJUNCTION long range order sensitivity. approach COMPARE ablative and external baselines. ablative and external baselines COMPARE approach. local coherence EVALUATE-FOR approach. long range order sensitivity EVALUATE-FOR approach. generation of long sequences EVALUATE-FOR approach. approach USED-FOR learning of embeddings. sensitivity EVALUATE-FOR document generation. sentence order FEATURE-OF document generation. sentence order FEATURE-OF sensitivity. linear combination USED-FOR learning of embeddings. OtherScientificTerm are semantic space, and random behavior. Material is text. Method is encoder. ","This paper proposes a novel stochastic process called Brownian Bridge process, which is a variant of Brownian motion in autoregressive models (e.g., GPT-2). The generation is based on the Brownian bridge process, where the zt is modeled as a Brownian branch of the zT and the semantic space is mapped to text. The authors propose a contrastive loss based on negatively sampled x't, where x't is sampled from an encoder that is trained to imitate the Brownians bridge dynamics of zt. The proposed approach is evaluated in terms of local coherence, long range order sensitivity, and generation of long sequences. The approach is compared to ablative and external baselines, and compared to the learning of embeddings using a linear combination. The sensitivity to sentence order in document generation is evaluated by comparing the sensitivity to the sentence order. The results show that the proposed approach leads to better performance, and that the random behavior of the encoder does not lead to random behavior."
1444,SP:352c177d89b9460acee0c78364e6d9c153c6a93c,Brownian bridge process USED-FOR global coherence. contrastive loss USED-FOR Brownian bridge dynamics. encoder - decoder style setup USED-FOR model. contrastive loss USED-FOR model. model COMPARE approaches. approaches COMPARE model. generative process USED-FOR model. local and global coherence and generation tasks EVALUATE-FOR model. local and global coherence and generation tasks EVALUATE-FOR approaches. ,"This paper proposes a Brownian bridge process for global coherence. The model is based on an encoder-decoder style setup, where the model is trained with a contrastive loss on the dynamics of the Brownian bridges dynamics. The authors also propose a generative process to train the model. The proposed model is compared with other approaches on both the local and global self-supervised and unsupervised versions of the local coherence and generation tasks."
1445,SP:352c177d89b9460acee0c78364e6d9c153c6a93c,"latent space FEATURE-OF Brownian motion. Brownian bridge USED-FOR text generation. encoder USED-FOR latent plan. encoder USED-FOR decoder. bridge USED-FOR decoder. encoder USED-FOR local text dynamics. decoder USED-FOR local incoherent text. sentence order prediction task USED-FOR encoder. text - infilling task EVALUATE-FOR decoder. Generic is method. Method is Language models. OtherScientificTerm are latent space of sentence embeddings, planning latent space, and global text statistics. Material is incoherent text. Metric is overall coherence. ","This paper proposes a method called Brownian bridge for text generation. Language models are trained on the latent space of sentence embeddings, and the Brownian motion in this latent space is modeled as a latent space. The proposed method is based on the idea of Brownian bridges. The bridge is used to train a decoder that takes the bridge as input and outputs a latent plan. The encoder is trained on a sentence order prediction task and the decoder is then used to predict the latent plan of the encoder. The authors show that the encoding of the planning latent space can be used to learn local text dynamics, and that the resulting decoder can generate local incoherent text. Experiments are performed on a text-infilling task, where the authors demonstrate that the proposed method can generate incoherent texts with high overall coherence, and can also generate global text statistics."
1446,SP:352c177d89b9460acee0c78364e6d9c153c6a93c,"initial state USED-FOR language model. language model USED-FOR generation. initial state USED-FOR generation. goal state USED-FOR generation. automatic evaluation CONJUNCTION human evaluation. human evaluation CONJUNCTION automatic evaluation. generation USED-FOR text - infilling task. human evaluation USED-FOR text   structures. automatic evaluation USED-FOR text   structures. Brownian bridge USED-FOR generation. OtherScientificTerm are Brownian motion, and initial and end states. Method is Time Control. ","This paper proposes to use the initial state of a language model for generation. The goal state for generation is a Brownian bridge between the initial and end states, where the goal state is the Brownian motion of the current state. The generation is applied to a text-infilling task, where automatic evaluation and human evaluation are used to evaluate the text  structures. Time Control is also used."
1447,SP:56a74403d4471cd95641dc669f5eac89a2c93144,"supervision USED-FOR segmentations. method USED-FOR 3D location. models COMPARE method. method COMPARE models. depth map USED-FOR method. equally - spaced bins USED-FOR pose domain. depth map USED-FOR 3D location. self - supervision USED-FOR method. Task is predicting the segmentations. Material is image of a scene. Method is object - centric models. OtherScientificTerm are yaw angle, and moving camera. Generic is model. ","This paper addresses the problem of predicting the segmentations from an image of a scene. The authors propose a method that uses self-supervision to generate segmentations that are similar to object-centric models. The proposed method uses a depth map to predict the 3D location of an object in the pose domain using equally-spanned bins. The model is trained to predict a yaw angle between the moving camera and the pose of the object. Compared to existing models, the proposed method is computationally efficient."
1448,SP:56a74403d4471cd95641dc669f5eac89a2c93144,"segmentation masks CONJUNCTION 3D positions and yaws. 3D positions and yaws CONJUNCTION segmentation masks. method USED-FOR object - centric representations. image representation of the background CONJUNCTION overall depth map. overall depth map CONJUNCTION image representation of the background. method USED-FOR 3 - frame videos. image representation of the background PART-OF object - centric representations. 3D positions and yaws PART-OF object - centric representations. segmentation masks PART-OF object - centric representations. decoder USED-FOR segmentations. depth network CONJUNCTION object network. object network CONJUNCTION depth network. decoder CONJUNCTION warping / re - compositing operation. warping / re - compositing operation CONJUNCTION decoder. decoder CONJUNCTION imagination "" network. imagination "" network CONJUNCTION decoder. LSTM USED-FOR object network. depth consistency CONJUNCTION spatial term. spatial term CONJUNCTION depth consistency. image reprojection / prediction CONJUNCTION depth consistency. depth consistency CONJUNCTION image reprojection / prediction. spatial term CONJUNCTION penalty term. penalty term CONJUNCTION spatial term. penalty term USED-FOR object probabilities. consistency CONJUNCTION randomness. randomness CONJUNCTION consistency. consistency FEATURE-OF spatial term. randomness FEATURE-OF spatial term. losses USED-FOR model. penalty term PART-OF losses. spatial term PART-OF losses. image reprojection / prediction HYPONYM-OF losses. depth consistency HYPONYM-OF losses. synthetic dataset EVALUATE-FOR prior methods. prior methods COMPARE method. method COMPARE prior methods. OtherScientificTerm are constant - velocity assumption, depth maps, and camera poses. ","This paper proposes a method to learn object-centric representations for 3-frame videos, which consist of an image representation of the background, an overall depth map, and segmentation masks, 3D positions and yaws. The decoder is used to generate segmentations, and the ""imagination"" network is a LSTM that maps the depth network to the object network. The model is trained with three losses: image reprojection/prediction, depth consistency, and a spatial term that encourages consistency and randomness. The penalty term penalizes the object probabilities for a constant-velocity assumption. The paper shows that the proposed method outperforms prior methods on a synthetic dataset, where the depth maps are generated from camera poses."
1449,SP:56a74403d4471cd95641dc669f5eac89a2c93144,"unsupervised object - centric scene representation technique USED-FOR 3D locations and pose. images USED-FOR model. MONet HYPONYM-OF models. It USED-FOR 3D position and pose of objects. this CONJUNCTION known camera motion. known camera motion CONJUNCTION this. images USED-FOR 3D position and pose of objects. images USED-FOR It. predicted object location / pose / depth USED-FOR optical flow based method. depth perception network USED-FOR depth. location and pose of objects USED-FOR velocity. color / depth CONJUNCTION object masks. object masks CONJUNCTION color / depth. "" imagination "" network USED-FOR object masks. "" imagination "" network USED-FOR color / depth. object information USED-FOR "" imagination "" network. warping CONJUNCTION imagination network. imagination network CONJUNCTION warping. imagination network USED-FOR predicted color and depth images. pose CONJUNCTION depth. depth CONJUNCTION pose. object location CONJUNCTION pose. pose CONJUNCTION object location. reconstruction loss CONJUNCTION self - supervised losses. self - supervised losses CONJUNCTION reconstruction loss. self - supervised losses USED-FOR object location. reconstruction loss FEATURE-OF predicted and ground truth image. self - supervised losses USED-FOR pose. self - supervised losses USED-FOR depth. they USED-FOR model. losses USED-FOR they. self - supervised losses HYPONYM-OF losses. reconstruction loss HYPONYM-OF losses. images USED-FOR model. images USED-FOR they. Method are object extraction network, and soft - matching approach. OtherScientificTerm is depth information. ","This paper proposes an unsupervised object-centric scene representation technique for learning 3D locations and pose of objects. It uses images to predict 3D position and pose for objects using a model similar to MONet [1]. The key difference is that instead of using an object extraction network, the authors propose an optical flow based method that uses the predicted object location/pose/depth as input to a depth perception network that predicts the depth and color of an object using this and known camera motion. The ""imagination"" network is trained to predict color/depth and object masks based on object information. The authors propose a soft-matching approach where the predicted color and depth images are augmented with warping and the imagination network is used to reconstruct the original image. The model is trained using these images and they are trained with different losses such as reconstruction loss for the predicted and ground truth image, self-supervised losses for object location, pose, depth, etc."
1450,SP:56a74403d4471cd95641dc669f5eac89a2c93144,system USED-FOR neural network. neural network USED-FOR generic objects. image triplets USED-FOR system. image triplets USED-FOR neural network. network USED-FOR Object perception. object perception PART-OF system. object perception HYPONYM-OF neural network. OtherScientificTerm is viewer's motion. Material is synthetic dataset. ,This paper proposes a system to train a neural network that can generate generic objects from image triplets. Object perception is incorporated into the system and the network is trained to predict the object's position based on the viewer's motion. Experiments are conducted on a synthetic dataset.
1451,SP:244f5d31ec93b7a4bfc4b257ee6cdd5cfdb18a38,VAE model USED-FOR mobility forecasting. features PART-OF VAE schema. spatial features CONJUNCTION temporal features. temporal features CONJUNCTION spatial features. spatial features HYPONYM-OF features. temporal features HYPONYM-OF features. sequential prior USED-FOR temporal features. VAE USED-FOR features. Task is disentanglement of spatial and temporal features. OtherScientificTerm is forecasting period. Method is SOTA. ,This paper proposes a VAE model for mobility forecasting. The VAE schema consists of two types of features: spatial features and temporal features. The temporal features are learned using a sequential prior. The spatial features are learnt using the VAE. The main contribution of this paper is the disentanglement of spatial and temporal factors. The authors also propose a forecasting period based on SOTA.
1452,SP:244f5d31ec93b7a4bfc4b257ee6cdd5cfdb18a38,"VAE - based architecture USED-FOR spatial and temporal features. VAE - based architecture USED-FOR disentangled representation of spatio - temporal mobility data. regularization term PART-OF VAE loss. total correction USED-FOR regularization term. Disentanglement USED-FOR representation. total correction USED-FOR VAE loss. total correction USED-FOR Disentanglement. TaxiNYC CONJUNCTION TaxiBJ. TaxiBJ CONJUNCTION TaxiNYC. BikeNYC CONJUNCTION TaxiNYC. TaxiNYC CONJUNCTION BikeNYC. mobility datasets EVALUATE-FOR approach. TaxiBJ HYPONYM-OF mobility datasets. BikeNYC HYPONYM-OF mobility datasets. TaxiNYC HYPONYM-OF mobility datasets. Generic are them, and state - of - the - art approaches. ","This paper proposes a VAE-based architecture to disentangle spatial and temporal features in a disentangled representation of spatio-temporal mobility data. Disentanglement is used as a regularization term in the VAE loss with total correction. The proposed approach is evaluated on three mobility datasets (BikeNYC, TaxiNYC and TaxiBJ) and compared to state-of-the-art approaches. The results show that the proposed approach outperforms them."
1453,SP:244f5d31ec93b7a4bfc4b257ee6cdd5cfdb18a38,Variational Autoencoder ( VAE ) model USED-FOR disentangled spatial and temporal representations. ST raster data USED-FOR disentangled spatial and temporal representations. separation module PART-OF network. model COMPARE baselines. baselines COMPARE model. real - world datasets EVALUATE-FOR baselines. real - world datasets EVALUATE-FOR model. ,"This paper proposes a Variational Autoencoder (VAE) model for learning disentangled spatial and temporal representations from ST raster data. The separation module of the network consists of two parts: (1) a separation module that separates the input and output space, and (2) a separate separation module for the output space. The proposed model is evaluated on two real-world datasets and compared with several baselines."
1454,SP:244f5d31ec93b7a4bfc4b257ee6cdd5cfdb18a38,neural architecture USED-FOR disentangled spatio - temporal representations. disentangled spatio - temporal representations USED-FOR mobility forecasting. neural architecture USED-FOR mobility forecasting. VAE - inspired architecture USED-FOR predictive models. independence of spatial and temporal dynamics USED-FOR predictive models. approach COMPARE baselines. baselines COMPARE approach. mobility datasets EVALUATE-FOR approach. prediction EVALUATE-FOR baselines. prediction EVALUATE-FOR approach. strategy USED-FOR feature selection. short - term / daily / weekly correlation HYPONYM-OF temporal features. ,"This paper proposes a neural architecture for learning disentangled spatio-temporal representations for mobility forecasting. The authors propose a VAE-inspired architecture to train predictive models with independence of spatial and temporal dynamics. The proposed approach is evaluated on three mobility datasets and compared to several baselines in terms of prediction performance. The paper also proposes a strategy for feature selection, focusing on temporal features such as short-term/daily/weekly correlation."
1455,SP:cf781d756cf0bed5f7cdeb94be49e6d4409eeda4,model USED-FOR probabilistic interpolation of time series. HeTVAE USED-FOR probabilistic interpolation of time series. time - dependent output variance USED-FOR VAE. time - dependent output variance USED-FOR it. it USED-FOR HeTVAE. time - dependent output variance USED-FOR HeTVAE. deterministic branch PART-OF stochastic variational latent variable. deterministic branch PART-OF latter. branch PART-OF latter. HeTVAE COMPARE baselines. baselines COMPARE HeTVAE. ablation studies EVALUATE-FOR baselines. ablation studies EVALUATE-FOR HeTVAE. ,"This paper proposes a model called HeTVAE for probabilistic interpolation of time series. Specifically, it uses the time-dependent output variance of VAE and uses it to improve the performance of the proposed model. The latter is composed of a deterministic branch and a stochastic variational latent variable. The experimental results show that the proposed heTVAE outperforms the baselines on several ablation studies."
1456,SP:cf781d756cf0bed5f7cdeb94be49e6d4409eeda4,VAE - based model USED-FOR interpolation of irregularly sampled time series. intensity network USED-FOR data sparsity information. temporal input data USED-FOR latent representation. intensity network USED-FOR attention mechanism. intensity network USED-FOR latent representation. attention mechanism USED-FOR latent representation. intensity network CONJUNCTION heteroscedastic output layer. heteroscedastic output layer CONJUNCTION intensity network. HeTVAE model USED-FOR uncertainty estimates. heteroscedastic output layer USED-FOR HeTVAE model. intensity network USED-FOR HeTVAE model. model COMPARE methods. methods COMPARE model. datasets EVALUATE-FOR methods. irregularly sampled points FEATURE-OF datasets. interpolation task EVALUATE-FOR methods. interpolation task EVALUATE-FOR model. datasets EVALUATE-FOR model. ,This paper proposes a VAE-based model for interpolation of irregularly sampled time series. The latent representation is learned from temporal input data using an intensity network to capture the data sparsity information. The HeTVAE model uses the intensity network and a heteroscedastic output layer to provide uncertainty estimates. The proposed model is evaluated on three datasets with irregularly sampling points and compared to other methods on an interpolation task.
1457,SP:cf781d756cf0bed5f7cdeb94be49e6d4409eeda4,probabilistic approach USED-FOR time series interpolation. model USED-FOR Variational Autoencoder. model USED-FOR irregularly sampled time series. Variational Autoencoder USED-FOR irregularly sampled time series. probabilistic approach USED-FOR model. medical and climate domain and synthetic data FEATURE-OF real - world data sets. medical and climate domain and synthetic data EVALUATE-FOR model. real - world data sets EVALUATE-FOR model. Generic is architecture. ,"This paper proposes a probabilistic approach for time series interpolation. The proposed model, called Variational Autoencoder, aims to solve the problem of irregularly sampled time series. The architecture is well motivated and the experimental results on real-world data sets from the medical and climate domain and synthetic data demonstrate the effectiveness of the proposed model."
1458,SP:cf781d756cf0bed5f7cdeb94be49e6d4409eeda4,mTAN USED-FOR probabilistic interpolation. intensity encoding USED-FOR model. homoscedastic output distribution COMPARE heteroscedastic distribution. heteroscedastic distribution COMPARE homoscedastic output distribution. likelihood estimation CONJUNCTION mean prediction. mean prediction CONJUNCTION likelihood estimation. model COMPARE HeTVAE ). HeTVAE ) COMPARE model. mean prediction EVALUATE-FOR HeTVAE ). likelihood estimation EVALUATE-FOR HeTVAE ). likelihood estimation EVALUATE-FOR model. mean prediction EVALUATE-FOR model. ,This paper proposes to use mTAN for probabilistic interpolation. The proposed model is based on intensity encoding. The authors argue that the homoscedastic output distribution is more expressive than the heteroscedolded distribution. The experimental results show that the proposed model outperforms HeTVAE (both in likelihood estimation and mean prediction).
1459,SP:80b8488b5a7c29014b0fefbc16698afac42250a0,statistical methods USED-FOR modularity. importance CONJUNCTION coherence. coherence CONJUNCTION importance. lesions CONJUNCTION feature visualization. feature visualization CONJUNCTION lesions. proxies USED-FOR modularity. coherence HYPONYM-OF proxies. importance HYPONYM-OF proxies. feature visualization HYPONYM-OF DNN interpretability techniques. lesions HYPONYM-OF DNN interpretability techniques. Method is DNNs. OtherScientificTerm is features. ,"This paper studies the problem of modularity in DNNs and proposes statistical methods to measure modularity. The authors propose two proxies for modularity: importance and coherence. They also propose three DNN interpretability techniques: lesions, feature visualization, etc. They show that features with high importance are more modular than features with low importance."
1460,SP:80b8488b5a7c29014b0fefbc16698afac42250a0,modularity FEATURE-OF neural networks. spectral clustering USED-FOR graph. approaches USED-FOR similarity. Spearman or Pearson correlation USED-FOR approaches. network weights USED-FOR weighted connections. them USED-FOR network. image classification tasks EVALUATE-FOR clusters. them USED-FOR coherence of visual features. classification accuracy EVALUATE-FOR clusters. Method is convolutional architectures. Generic is they. Task is network's task. ,"This paper studies the role of modularity in the training of neural networks. In particular, the authors consider the case of convolutional architectures, where the graph is represented by spectral clustering. The authors propose two approaches to measure similarity, based on Spearman or Pearson correlation, and show that they can be used to improve the performance of a network's task. They also propose to use network weights as weighted connections, which can improve the network's performance. Finally, they show that clusters can improve classification accuracy on image classification tasks, and that using them can also improve the coherence of visual features."
1461,SP:80b8488b5a7c29014b0fefbc16698afac42250a0,"spectral clustering algorithm USED-FOR graph. importance * score CONJUNCTION coherence * score. coherence * score CONJUNCTION importance * score. OtherScientificTerm are modularity, undirected graph, subclusters, nodes, layer, subcluster, sub - clusters, pre - activation weights, and random. Generic are definition, modules, network, and architectures. Task is human identification. Method is trained - network. Metric is loss of accuracy. Material is image. ","This paper proposes a spectral clustering algorithm for graph. The key idea is to use modularity, i.e., to partition the undirected graph into subclusters, where each subcluster corresponds to a set of nodes, and each node is associated with a *importance* or a *coherence* score. This definition is motivated by human identification, where we want to be able to distinguish between different modules in the same network. The authors propose to use a trained-network where each layer is responsible for assigning a *pre-activation weights* to each sub-cluster, which is a weighted combination of the importance *score* and the coherence *score*. The authors show that the loss of accuracy is proportional to the number of pre-activations, and that the randomness of the weights can be reduced to zero if the weights are chosen randomly. The experiments are conducted on an image of a human, and show that different architectures are able to identify different parts of the image."
1462,SP:80b8488b5a7c29014b0fefbc16698afac42250a0,"graph - based clustering methods USED-FOR humanly comprehensible modularities. humanly comprehensible modularities PART-OF neural network. OtherScientificTerm are graph of neurons, and fisher bates p - value. Method is clustering. ","This paper studies the problem of learning humanly comprehensible modularities in a neural network using graph-based clustering methods. In particular, the authors propose to learn a graph of neurons that can be represented as a weighted sum of the fisher bates p-value of each neuron. The authors show that this clustering is computationally efficient."
1463,SP:0d7cbb544bc39203c9c18b4fee56fc94cbe78375,"lottery tickets hypothesis USED-FOR automatic speech recognition. TED - LIUM CONJUNCTION Common Voice. Common Voice CONJUNCTION TED - LIUM. LibriSpeech CONJUNCTION TED - LIUM. TED - LIUM CONJUNCTION LibriSpeech. RNN - T CONJUNCTION Transfomer. Transfomer CONJUNCTION RNN - T. CTC CONJUNCTION RNN - T. RNN - T CONJUNCTION CTC. model structures CONJUNCTION datasets. datasets CONJUNCTION model structures. LibriSpeech HYPONYM-OF datasets. Common Voice HYPONYM-OF datasets. TED - LIUM HYPONYM-OF datasets. Transfomer HYPONYM-OF model structures. CTC HYPONYM-OF model structures. RNN - T HYPONYM-OF model structures. Task is ASR task. OtherScientificTerm are noise, and structured sparsity. ","This paper proposes a lottery tickets hypothesis for automatic speech recognition. The ASR task is divided into two parts: 1) the noise is randomly generated, and 2) structured sparsity is enforced. The authors evaluate several model structures (CTC, RNN-T, Transfomer) and datasets (LibriSpeech, TED-LIUM, Common Voice)."
1464,SP:0d7cbb544bc39203c9c18b4fee56fc94cbe78375,"lottery ticket hypothesis ( LTH ) USED-FOR ASR model pruning. pruned subnetwork COMPARE full network. full network COMPARE pruned subnetwork. TED - LIUM CONJUNCTION Common Voice. Common Voice CONJUNCTION TED - LIUM. LTH USED-FOR models. Common Voice CONJUNCTION LibriSpeech. LibriSpeech CONJUNCTION Common Voice. CNN - RNN CTC CONJUNCTION RNN - T. RNN - T CONJUNCTION CNN - RNN CTC. datasets EVALUATE-FOR models. datasets EVALUATE-FOR LTH. CNN - RNN CTC HYPONYM-OF models. LibriSpeech HYPONYM-OF datasets. TED - LIUM HYPONYM-OF datasets. RNN - T HYPONYM-OF models. Common Voice HYPONYM-OF datasets. subnetwork COMPARE full model. full model COMPARE subnetwork. pruning methods COMPARE LTH. LTH COMPARE pruning methods. LTH COMPARE LTH. LTH COMPARE LTH. pretrained model COMPARE LTH. LTH COMPARE pretrained model. random initialization USED-FOR LTH. pretrained model USED-FOR LTH. block sparsity USED-FOR pruning. pruning USED-FOR chip design. block sparsity USED-FOR chip design. Generic are model, it, and method. OtherScientificTerm is sparsified subnetwork. Method is pruned network. ","This paper proposes lottery ticket hypothesis (LTH) for ASR model pruning. LTH is based on the observation that a pruned subnetwork performs better than a full network when the model is pretrained. The authors show that LTH can be applied to several models (CNN-RNN CTC, RNN-T, etc) on three datasets (TED-LIUM, Common Voice, LibriSpeech). The authors compare LTH with other pruning methods and show that it outperforms all of them. They also show that a sparsified subnetwork is better than the full model when the pruned network is trained with random initialization. Finally, they show that pruning with block sparsity can be used for chip design and show the effectiveness of the proposed method."
1465,SP:0d7cbb544bc39203c9c18b4fee56fc94cbe78375,"lottery tickets hypothesis ( LTH ) strategy USED-FOR neural network weights. neural network weights USED-FOR speech recognition. transfer learning USED-FOR noisy speech recognition tasks. method USED-FOR ASR tasks. noise CONJUNCTION reverberation. reverberation CONJUNCTION noise. reverberation CONJUNCTION interference speakers. interference speakers CONJUNCTION reverberation. acoustic interference USED-FOR recognition of speech. interference speakers HYPONYM-OF acoustic interference. distant microphones USED-FOR recognition of speech. noise HYPONYM-OF acoustic interference. reverberation HYPONYM-OF acoustic interference. signal processing CONJUNCTION deep - learning techniques. deep - learning techniques CONJUNCTION signal processing. Speech processing USED-FOR digital home assistants. deep - learning techniques PART-OF Speech processing. signal processing PART-OF Speech processing. evaluation framework USED-FOR dereverberation and recognition of reverberant speech. REVERB challenge HYPONYM-OF evaluation framework. Signal Processing USED-FOR Audio and Acoustics. transfer learning USED-FOR noisy speech recognition. data augmentation techniques COMPARE transfer learning. transfer learning COMPARE data augmentation techniques. data augmentation techniques USED-FOR noisy speech recognition. CV models COMPARE speech models. speech models COMPARE CV models. RNN backbones USED-FOR speech models. transformer CONJUNCTION conformer. conformer CONJUNCTION transformer. conformer CONJUNCTION CNN. CNN CONJUNCTION conformer. conformer USED-FOR speech models. CNN USED-FOR speech models. transformer USED-FOR speech models. downsampling USED-FOR encoder layer. KD - based methods HYPONYM-OF methods. CHiME data HYPONYM-OF real noisy speech data. Method are LTH framework, pre - trained models, and MAP / MLLR adaptation techniques. OtherScientificTerm is transfer learning scenarios. Generic are models, and they. Task are end - to - end ASR, and speaker adaptation. Material are clean speech, and noisy data. ","This paper proposes a lottery tickets hypothesis (LTH) strategy to train neural network weights for speech recognition using transfer learning to solve noisy speech recognition tasks. Speech processing is an important application of signal processing and deep-learning techniques in digital home assistants, and the LTH framework is well-motivated. However, the proposed method is not applicable to ASR tasks, where the recognition of speech from distant microphones is difficult due to acoustic interference such as noise, reverberation, and interference speakers. The paper presents an evaluation framework for dereverberation and recognition of reverberant speech, called the REVERB challenge, which is designed to evaluate the performance of pre-trained models on a variety of transfer learning scenarios.  The paper argues that data augmentation techniques are not effective transfer learning for noisy speech performance, and that there is a clear gap between the performance on clean speech and speech models trained with RNN backbones, and on noisy speech models with a transformer, a conformer, and a CNN. In particular, the paper points out that the performance gap between CV models trained on clean and noisy speech is larger than that of speech models that are trained with a CNN, and argues that these models do not generalize well to end-to-end ASR. To address this issue, the authors propose to use MAP/MLLR adaptation techniques, where they augment the encoder layer with downsampling. The proposed methods are called KD-based methods, which are based on the idea of speaker adaptation. The experiments on real noisy speech data such as CHiME data show that the proposed methods perform well. "
1466,SP:0d7cbb544bc39203c9c18b4fee56fc94cbe78375,full models USED-FOR ASR. robustness EVALUATE-FOR sparse subnetworks. Task is lottery ticket hypothesis. OtherScientificTerm is subnetworks. Generic is architectures. ,"This paper studies the lottery ticket hypothesis, which is based on the observation that full models for ASR perform poorly when the number of subnetworks is sparse. The authors argue that this is due to the fact that the robustness to perturbations of the sparse subnets is lower than that of the full models. To address this issue, the authors propose two architectures, which they call lottery ticket and lottery ticket-based ASR."
1467,SP:cb9530f5517f1092513c200b3f32e55420fdd768,"ResNet architecture CONJUNCTION initialization scheme. initialization scheme CONJUNCTION ResNet architecture. deterministic initialization USED-FOR network. deterministic initialization method COMPARE ones. ones COMPARE deterministic initialization method. deterministic initialization method COMPARE random weight initialization. random weight initialization COMPARE deterministic initialization method. method COMPARE baselines. baselines COMPARE method. method COMPARE deterministic initialization method. deterministic initialization method COMPARE method. random weight initialization USED-FOR ones. image classification datasets EVALUATE-FOR baselines. image classification datasets EVALUATE-FOR method. OtherScientificTerm are normalization layers, and forward and backward dynamics. Generic is approach. ",This paper proposes a new ResNet architecture and initialization scheme based on deterministic initialization of the network. The main idea is to replace the normalization layers with a set of weights that are independent of the forward and backward dynamics. The method is evaluated on image classification datasets and compared with several baselines and compared to a deterministic and a random weight initialization method. The proposed approach seems to work well.
1468,SP:cb9530f5517f1092513c200b3f32e55420fdd768,"skip connections CONJUNCTION Hadamard transforms. Hadamard transforms CONJUNCTION skip connections. skip connections USED-FOR ResNet architectures. Hadamard transforms USED-FOR ResNet architectures. Method is ResNet. Task is all - zero initialization. Generic is method. OtherScientificTerm are ZerO, and initialization. ","This paper proposes a method called ZerO, which is an extension of ResNet that aims to address the problem of all-zero initialization. The method is based on the idea that skip connections and Hadamard transforms can be used in ResNet architectures with skip connections. The key idea is to use ZerO as a way to make the initialization more efficient. "
1469,SP:cb9530f5517f1092513c200b3f32e55420fdd768,"ZerO USED-FOR residual networks. deterministic weight initialization scheme USED-FOR residual networks. ZerO HYPONYM-OF deterministic weight initialization scheme. skip connections CONJUNCTION Hadamard transforms. Hadamard transforms CONJUNCTION skip connections. skip connections USED-FOR ResNet architectures. Hadamard transforms USED-FOR ResNet architectures. reproducibility EVALUATE-FOR scheme. OtherScientificTerm are network weights, random weights, and batch normalization. Task is training networks. ","This paper proposes ZerO, a deterministic weight initialization scheme for residual networks, which is a variant of ZerO that aims to train residual networks with network weights that do not depend on random weights. The authors propose to use skip connections and Hadamard transforms to train ResNet architectures with skip connections, and show that the proposed scheme improves reproducibility in terms of training networks and batch normalization."
1470,SP:cb9530f5517f1092513c200b3f32e55420fdd768,"initialization scheme USED-FOR ResNet. zero - initialized weights FEATURE-OF networks. OtherScientificTerm are random, and dead neuron "" problem. ","This paper proposes a new initialization scheme for ResNet. The authors argue that networks with zero-initialized weights are prone to ""dead neuron"" problem, where the weights are random. "
1471,SP:16618b226e42a07095dcf9204ce4c0e3b2ed8bd8,models USED-FOR defense against backdoor attacks. min - max formulation USED-FOR backdoor defense. outer minimum USED-FOR adversarial loss. method USED-FOR minimax. convergence bound CONJUNCTION generalization bound. generalization bound CONJUNCTION convergence bound. OtherScientificTerm is inner maximum. ,This paper studies the defense against backdoor attacks using models that are trained on the min-max formulation of backdoor defense. The main contribution of this paper is to show that the outer minimum of the adversarial loss can be used as the inner maximum of the defense. This method can be viewed as a minimax of the method proposed in [1]. The authors provide a convergence bound and a generalization bound.
1472,SP:16618b226e42a07095dcf9204ce4c0e3b2ed8bd8,implicit hypergradient USED-FOR minimax problem. adversarial training USED-FOR backdoor poison attacks. convergence bound CONJUNCTION generalization bound. generalization bound CONJUNCTION convergence bound. generalization bound USED-FOR method. convergence bound EVALUATE-FOR method. adversarial training routine COMPARE defenses. defenses COMPARE adversarial training routine. attack settings EVALUATE-FOR adversarial training routine. Multi - trigger - multi - target attack HYPONYM-OF attack settings. One - trigger - one - target attack HYPONYM-OF attack settings. I - BAU COMPARE baseline. baseline COMPARE I - BAU. I - BAU COMPARE defenses. defenses COMPARE I - BAU. baseline USED-FOR attacks. baseline COMPARE defenses. defenses COMPARE baseline. I - BAU USED-FOR attacks. universal adversarial perturbations USED-FOR adversarial learning. Task is inner and outer optimization problems. Generic is algorithm. OtherScientificTerm is backdoor attacks. ,This paper studies the problem of adversarial training for backdoor poison attacks. The authors consider both inner and outer optimization problems and propose an algorithm called I-BAU. The main idea is to use an implicit hypergradient to solve the minimax problem. The proposed method has a convergence bound and a generalization bound. Experiments are conducted on two attack settings: One-trigger-one-target attack and Multi-triggered-multi-targets attack. The results show that the proposed adversarial learning with universal adversarial perturbations outperforms the baseline and other defenses against backdoor attacks.
1473,SP:16618b226e42a07095dcf9204ce4c0e3b2ed8bd8,"minimax formulation USED-FOR removing backdoors. removing backdoors PART-OF poisoned model. clean data USED-FOR minimax formulation. clean data USED-FOR poisoned model. implicit hypergradient USED-FOR algorithm. convergence CONJUNCTION generalizability. generalizability CONJUNCTION convergence. generalizability FEATURE-OF robustness. clean data USED-FOR minimax. minimax USED-FOR robustness. computation time FEATURE-OF backdoor attacks. OtherScientificTerm are inner and outer problems, and inner and outer optimization. Task is backdoor defense. ","This paper proposes a minimax formulation for removing backdoors from a poisoned model using only clean data. The algorithm is based on an implicit hypergradient, where the inner and outer problems are solved separately. The authors prove convergence and generalizability of the minimax for robustness to backdoor attacks with high computation time. The main contribution of this paper is to provide a theoretical analysis of backdoor defense. The paper is well-written and well-motivated, and the results are interesting. However, there are some issues that need to be addressed in the paper, such as the comparison between inner andouter optimization."
1474,SP:16618b226e42a07095dcf9204ce4c0e3b2ed8bd8,minimax formulation USED-FOR defense against adversaries. convergence bound CONJUNCTION generalization bound. generalization bound CONJUNCTION convergence bound. generalization bound USED-FOR method. convergence bound USED-FOR method. stability CONJUNCTION sensitivity. sensitivity CONJUNCTION stability. efficacy CONJUNCTION stability. stability CONJUNCTION efficacy. sensitivity CONJUNCTION efficiency. efficiency CONJUNCTION sensitivity. sensitivity EVALUATE-FOR competitive methods. stability EVALUATE-FOR competitive methods. efficiency EVALUATE-FOR competitive methods. efficacy EVALUATE-FOR competitive methods. Task is backdoor attack problem. ,"This paper studies the backdoor attack problem and proposes a minimax formulation for defense against adversaries. The method is based on a convergence bound and a generalization bound. The authors evaluate competitive methods on efficacy, stability, sensitivity, and efficiency."
1475,SP:7260bd50f600a481ec7710792b63f518218e0eaf,"permutation SGD CONJUNCTION variants. variants CONJUNCTION permutation SGD. one - dimensional functions USED-FOR permutation - based optimization. permutations USED-FOR permutation SGD. convergence EVALUATE-FOR random - reshuffling. convergence EVALUATE-FOR permutation choice. permutations COMPARE random reshuffling. random reshuffling COMPARE permutations. permutations USED-FOR SGD. smooth Hessians FEATURE-OF one - dimensional finite - sum functions. linear ( i.e. exponential ) rate EVALUATE-FOR SGD. dimension - dependent lower bound USED-FOR permutation - based SGD method. convergence rate EVALUATE-FOR random reshuffling. convergence rate EVALUATE-FOR lower bound. FlipFlop USED-FOR permutation - based SGD variants. OtherScientificTerm are quadratics, and fixed step - size. Generic are it, method, and approach. Material is synthetic data. Task is logistic regression. Method are stochastic gradient descent ( SGD ), and finite - sums of convex quadratics. ","This paper studies permutation-based optimization with one-dimensional functions, which is an important problem for permutation -based optimization in the context of finite-summing of convex quadratics. The authors consider permutation SGD with permutations and variants based on FlipFlop, and show that the convergence of the permutation choice is similar to that of random-reshuffling. The main contribution of this paper is to prove a dimension-dependent lower bound on the convergence rate of the proposed permutation based SGD method, which shows that it converges at a linear (i.e. exponential) rate compared to random reshuffling, and that it can be viewed as an extension of stochastic gradient descent (SGD). The authors also show that SGD is equivalent to using permutations with smooth Hessians, which are a special case of the problem of solving finite-solutions of quadratic functions. The paper also shows that the proposed method can be extended to the case of logistic regression, where the authors show that permutations are equivalent to SGD. Finally, the authors provide a lower bound of a fixed step-size for the proposed lower bound, and compare the proposed approach to the permutations-based SGD variants with and without FlipFlOP. The results are shown on synthetic data, as well as on finite-versus-convex and convex-solution-based problems."
1476,SP:7260bd50f600a481ec7710792b63f518218e0eaf,"without - replacement sampling ( random permutation ) COMPARE with - replacement sampling. with - replacement sampling COMPARE without - replacement sampling ( random permutation ). without - replacement sampling ( random permutation ) USED-FOR stochastic gradient descent ( SGD ). random permutations USED-FOR permutation - based SGD. optimal permutations COMPARE random permutations. random permutations COMPARE optimal permutations. optimal permutations USED-FOR 1 - dimensional functions. random permutations USED-FOR 1 - dimensional functions. constant step size FEATURE-OF optimizing convex functions. Random Reshuffle ( random ) CONJUNCTION Single Shuffle ( hybrid ). Single Shuffle ( hybrid ) CONJUNCTION Random Reshuffle ( random ). Incremental Gradient Descent ( deterministic ) CONJUNCTION Random Reshuffle ( random ). Random Reshuffle ( random ) CONJUNCTION Incremental Gradient Descent ( deterministic ). convergence FEATURE-OF quadratic functions. convergence EVALUATE-FOR permutation - based methods. Incremental Gradient Descent ( deterministic ) HYPONYM-OF permutation - based methods. Single Shuffle ( hybrid ) HYPONYM-OF permutation - based methods. Random Reshuffle ( random ) HYPONYM-OF permutation - based methods. OtherScientificTerm are strongly convex objectives, and flipflopping. ","This paper studies without-replacement sampling (random permutation) in stochastic gradient descent (SGD) instead of with-replace sampling. The authors show that the optimal permutations for 1-dimensional functions can be approximated by random permutations in permutation-based SGD, and that optimizing convex functions with constant step size can be solved with constant permutations. They also show that for strongly convex objectives, the convergence of quadratic functions converges linearly with the number of permutations, which is a result of flipflopping. Finally, they compare the convergence results of permutation -based methods such as Incremental Gradient Descent (deterministic), Random Reshuffle (random), Single Shuffle (hybrid), and Multi-Tasking (random)."
1477,SP:7260bd50f600a481ec7710792b63f518218e0eaf,random permutation USED-FOR SGD. scan ordering USED-FOR SGD. random permutation USED-FOR scan ordering. random permutations USED-FOR high dimension. high dimension CONJUNCTION 1 - dimension. 1 - dimension CONJUNCTION high dimension. Lipschitz Hessian FEATURE-OF strongly convex functions. random permutations USED-FOR strongly convex functions. random permutations USED-FOR convex quadratics. permutation USED-FOR algorithm. single shuffle CONJUNCTION incremental gradient descent. incremental gradient descent CONJUNCTION single shuffle. random reshuffling CONJUNCTION single shuffle. single shuffle CONJUNCTION random reshuffling. incremental gradient descent USED-FOR quadratic functions. FlipFlop USED-FOR random reshuffling. FlipFlop USED-FOR incremental gradient descent. FlipFlop USED-FOR quadratic functions. FlipFlop COMPARE single shuffle. single shuffle COMPARE FlipFlop. single shuffle USED-FOR quadratic functions. convergence EVALUATE-FOR FlipFlop. FlipFlop COMPARE random reshuffling. random reshuffling COMPARE FlipFlop. 1 - dimensional logistic regression EVALUATE-FOR FlipFlop. Generic is technique. ,"This paper proposes to use a random permutation for the scan ordering in SGD. The authors show that random permutations can be used for high dimension, 1-dimension, and strongly convex functions with Lipschitz Hessian. They also show that for convex quadratics, the same permutation can be applied to any algorithm. Finally, they show that FlipFlop can be combined with random reshuffling, single shuffle, and incremental gradient descent to converge to quadratic functions with high convergence. They evaluate the proposed technique on the problem of 1-dimensional logistic regression, and demonstrate the effectiveness of FlipFlFlop."
1478,SP:7260bd50f600a481ec7710792b63f518218e0eaf,"permutation - based fixed - step size SGD USED-FOR finite sum optimization. permutations USED-FOR 1 - d Hessian - smooth functions. random permutation USED-FOR algorithm. Metric is convergence rate. OtherScientificTerm are strongly - convex function, strongly - convex objective function, permutations of reverse order, and component functions. Method is FlipFlop. ","This paper studies permutation-based fixed-step size SGD for finite sum optimization. The authors consider permutations for 1-d Hessian-smooth functions and show that the convergence rate is polynomial in the number of permutations required to converge to a strongly-convex function. The algorithm is based on a random permutation of a function $f$, where $f$ is a function of $x$, and $x$ is the permutations of reverse order. The main contribution of the paper is to show that if $f \in [0,1]$ is permuted, then the algorithm converges to a point where $x \in \mathbb{R}^{n}$ is close to $f\in [1,2]$. The authors also show that when $f_1$ is strongly convex, the algorithm will converge to the point where $\mathbf{R}\in [2,3]$. In particular, the authors show that $f_{1,1}$ can be approximated by a permuted version of $f(x,y)$ with $f^*$ permutations. This is similar to the result of FlipFlop [1], which is an algorithm that approximates $f^{1,0,2}$ with permutations that are close to each other. The key difference is that the authors consider the case where the function $F$ is not permuted. In this case, instead of using the permuted function as the input to the algorithm, they use the permuting function as an input to a function, which is then used to approximate the function with the same permutation as the strongly-convolutional objective function. In addition, they show that their algorithm can be extended to the case when the component functions are permuted in a different way."
1479,SP:39062dbbe9a30a7b47fa51179c15db34a3380a0b,"approach USED-FOR searching flow architectures. approaches USED-FOR deep architecture. deep architecture PART-OF network networks. approaches COMPARE problem. problem COMPARE approaches. weighting USED-FOR candidate transformations. weighting USED-FOR problem. invertible properties FEATURE-OF model. mixing distribution approach USED-FOR invertible properties. soft weights COMPARE binarised versions. binarised versions COMPARE soft weights. approach COMPARE baseline. baseline COMPARE approach. approach COMPARE expert - based selection. expert - based selection COMPARE approach. expert - based selection HYPONYM-OF baseline. OtherScientificTerm are invertible transform, determinant of the Jacobian, and loss function. ","This paper presents an approach for searching flow architectures. Compared to existing approaches for finding a deep architecture in network networks, this problem is formulated as a weighting of candidate transformations, where the invertible transform is defined as the determinant of the Jacobian of the loss function. The authors propose a mixing distribution approach to encourage the model to have invertibly properties. The soft weights are compared to binarised versions, and the proposed approach is compared to a baseline, expert-based selection."
1480,SP:39062dbbe9a30a7b47fa51179c15db34a3380a0b,"automated search USED-FOR Normalizing Flow architectures. differentiable architecture search formulation USED-FOR automated search. approach USED-FOR normalizing flow problem. approximated upper bound of the KL divergence COMPARE optimization. optimization COMPARE approximated upper bound of the KL divergence. approximated upper bound of the KL divergence USED-FOR full network. methods USED-FOR optimization problem. grow method CONJUNCTION block method. block method CONJUNCTION grow method. block method HYPONYM-OF methods. grow method HYPONYM-OF methods. HERMASS CONJUNCTION MINIBOONE. MINIBOONE CONJUNCTION HERMASS. MINIBOONE CONJUNCTION BSDS300. BSDS300 CONJUNCTION MINIBOONE. POWER CONJUNCTION GAS. GAS CONJUNCTION POWER. GAS CONJUNCTION HERMASS. HERMASS CONJUNCTION GAS. method COMPARE manually specified architectures. manually specified architectures COMPARE method. datasets EVALUATE-FOR method. datasets EVALUATE-FOR manually specified architectures. POWER HYPONYM-OF datasets. BSDS300 HYPONYM-OF datasets. HERMASS HYPONYM-OF datasets. MINIBOONE HYPONYM-OF datasets. GAS HYPONYM-OF datasets. searched model COMPARE manual model. manual model COMPARE searched model. OtherScientificTerm are invertibility constraints, direct linear summation, and transform operations. ","This paper proposes an automated search for Normalizing Flow architectures based on a differentiable architecture search formulation. The proposed approach aims to solve the normalizing flow problem under invertibility constraints. The main idea is to use direct linear summation of the full network instead of using an approximated upper bound of the KL divergence as in standard optimization. To solve the optimization problem, the authors propose two methods: a grow method and a block method. The authors evaluate the proposed method on three datasets: POWER, GAS, HERMASS, MINIBOONE, and BSDS300. The experiments show that the searched model outperforms the manual model on all three datasets. In addition, the proposed transform operations are shown to be effective."
1481,SP:39062dbbe9a30a7b47fa51179c15db34a3380a0b,"DARTS - like method USED-FOR searching automated normalization flow models. distribution mixture USED-FOR supernet. resource constraints FEATURE-OF loss function. small - to - medium scale datasets EVALUATE-FOR method. Method are infeasible flow models, and flow model. ","This paper proposes a DARTS-like method for searching automated normalization flow models. The main idea is to search infeasible flow models in an unsupervised manner, where the supernet is trained on a distribution mixture, and the flow model is trained to maximize the likelihood of the distribution mixture. The authors also propose a new loss function with resource constraints. The proposed method is evaluated on small-to-medium scale datasets."
1482,SP:39062dbbe9a30a7b47fa51179c15db34a3380a0b,mixture distribution formulation USED-FOR optimal flow model. n layers of transformations USED-FOR optimal flow model. mixture distribution formulation USED-FOR method. block - wise optimization method USED-FOR exponentially growing optimization complexity. approximate upper bound USED-FOR optimization method. AutoNF COMPARE hand - tuned SOTA flow models. hand - tuned SOTA flow models COMPARE AutoNF. performance - cost trade - off EVALUATE-FOR hand - tuned SOTA flow models. performance - cost trade - off EVALUATE-FOR AutoNF. Method is automated normalizing flow(NF ) architecture search method. ,This paper proposes an automated normalizing flow(NF) architecture search method. The proposed method uses a mixture distribution formulation to find the optimal flow model with n layers of transformations. The authors propose a block-wise optimization method to reduce exponentially growing optimization complexity. They also provide an approximate upper bound for the optimization method. Experimental results show that AutoNF achieves better performance-cost trade-off compared to hand-tuned SOTA flow models.
1483,SP:d2656ae0259accc5207234fc4206f6f7be9598d9,measurement method EVALUATE-FOR model. test accuracy EVALUATE-FOR model. Intrinsic Dimension ( ID ) CONJUNCTION CLuster Learnability ( CL ). CLuster Learnability ( CL ) CONJUNCTION Intrinsic Dimension ( ID ). ID CONJUNCTION CL. CL CONJUNCTION ID. ID FEATURE-OF model. CL FEATURE-OF model. alignment CONJUNCTION uniformity. uniformity CONJUNCTION alignment. ID CONJUNCTION CL. CL CONJUNCTION ID. CL USED-FOR top-1 accuracy. ID USED-FOR top-1 accuracy. Pearson correlation coefficient EVALUATE-FOR predictors. Pearson correlation coefficient EVALUATE-FOR CL. uniformity HYPONYM-OF predictors. alignment HYPONYM-OF predictors. Method is DeepCluster algorithm. ,"This paper proposes a new measurement method to evaluate a model’s test accuracy by comparing its Intrinsic Dimension (ID) and CLuster Learnability (CL). The authors propose to use the DeepCluster algorithm and compare the top-1 accuracy of a model with ID, CL, and ID using the Pearson correlation coefficient between the two predictors (alignment and uniformity)."
1484,SP:d2656ae0259accc5207234fc4206f6f7be9598d9,expressiveness CONJUNCTION learnability. learnability CONJUNCTION expressiveness. metrics EVALUATE-FOR self - supervised learning representations. learnability FEATURE-OF metrics. Expressiveness COMPARE learnability. learnability COMPARE Expressiveness. Intrinsic Dimension ( ID ) CONJUNCTION Cluster Learnability ( CL ). Cluster Learnability ( CL ) CONJUNCTION Intrinsic Dimension ( ID ). expressiveness CONJUNCTION learnability. learnability CONJUNCTION expressiveness. ImageNet CONJUNCTION STL_10. STL_10 CONJUNCTION ImageNet. metrics CONJUNCTION downstream classification. downstream classification CONJUNCTION metrics. datasets EVALUATE-FOR metrics. ImageNet HYPONYM-OF datasets. STL_10 HYPONYM-OF datasets. Generic is representation. Method is self - supervised methods. ,"This paper proposes two metrics for evaluating self-supervised learning representations: expressiveness and learnability. Expressiveness is defined as the difference between the Intrinsic Dimension (ID) and Cluster Learnability (CL) of the representation. The authors evaluate these metrics and downstream classification on three datasets: ImageNet, STL_10, and CIFAR-10. The results show that the proposed metrics can be used to evaluate the performance of self-trained methods."
1485,SP:d2656ae0259accc5207234fc4206f6f7be9598d9,"Intrinsic Dimension ( ID ) CONJUNCTION Cluster Learnability ( CL ). Cluster Learnability ( CL ) CONJUNCTION Intrinsic Dimension ( ID ). learned representation quality EVALUATE-FOR pre - trained networks. expressiveness USED-FOR frameworks. prediction accuracy EVALUATE-FOR methods. ID + CL COMPARE baselines. baselines COMPARE ID + CL. methods COMPARE baselines. baselines COMPARE methods. prediction accuracy EVALUATE-FOR ID + CL. supervised and unsupervised methods USED-FOR models. CL PART-OF DeepCluster. DeepCluster HYPONYM-OF unsupervised method. CL USED-FOR prediction. OtherScientificTerm are down - stream tasks / labels, and auxiliary loss. Generic is representation. Metric is learnability. Method is KNN clustering. ","This paper proposes Intrinsic Dimension (ID) and Cluster Learnability (CL) to improve the learned representation quality of pre-trained networks. The expressiveness of the proposed frameworks is motivated by the observation that most of the down-stream tasks/labels are not well represented in the original representation. The authors propose to use an auxiliary loss to encourage the representation to be more expressive. The proposed models are trained using both supervised and unsupervised methods. The experimental results show that ID + CL improves the prediction accuracy over the baselines, while also improving the learnability. In addition, the authors propose DeepCluster, which combines CL and ID to improve prediction performance. The experiments are conducted on KNN clustering."
1486,SP:d2656ae0259accc5207234fc4206f6f7be9598d9,Cluster Learnability ( CL ) CONJUNCTION Intrinsic Dimension ( ID ). Intrinsic Dimension ( ID ) CONJUNCTION Cluster Learnability ( CL ). Cluster Learnability ( CL ) USED-FOR representations. Intrinsic Dimension ( ID ) USED-FOR representations. self - supervised learning methods USED-FOR representations. alignment CONJUNCTION uniformity. uniformity CONJUNCTION alignment. method COMPARE methods. methods COMPARE method. uniformity HYPONYM-OF methods. alignment HYPONYM-OF methods. learnability EVALUATE-FOR representations. K - means PART-OF DeepCluster. ,This paper proposes to use Cluster Learnability (CL) and Intrinsic Dimension (ID) to learn representations for self-supervised learning methods. The proposed method is evaluated on a variety of datasets and compared to other methods such as alignment and uniformity. The learnability of the learned representations is evaluated by comparing the K-means of DeepCluster.
1487,SP:4f5c00469e4425751db5efbc355085a5e8709def,"segmentation priors USED-FOR perturbation. perturbation USED-FOR salient region. Task is black - box attack. OtherScientificTerm are reduce perceptibility, and saliency regions. Metric is success rate. Method are refining procedure, and SOTA methods. Material is ImageNet. ","This paper proposes a black-box attack where the goal is to reduce perceptibility by perturbing the salient region with different segmentation priors. The success rate is evaluated on ImageNet, where the authors show that the proposed refining procedure can reduce the perceptibility of the saliency regions significantly compared to SOTA methods."
1488,SP:4f5c00469e4425751db5efbc355085a5e8709def,"query efficiency EVALUATE-FOR black - box attacks. segmentation priors USED-FOR black - box attacks. imperceptibility EVALUATE-FOR technique. Generic is techniques. OtherScientificTerm are human in the loop, and perturbations. ","This paper proposes two techniques to improve the query efficiency of black-box attacks with segmentation priors. The first technique improves the imperceptibility to perturbations by adding a human in the loop, and the second technique is based on the observation that the human can be fooled by small changes in the input. "
1489,SP:4f5c00469e4425751db5efbc355085a5e8709def,"black - box adversarial attacks USED-FOR images. segmentation techniques USED-FOR prior. Method are $ \ell_\infty$-threat model, and Saliency Attack. OtherScientificTerm is misclassification. ","This paper studies black-box adversarial attacks on images. The authors propose a $\ell_\infty$-threat model, which is based on the Saliency Attack. The prior is trained using segmentation techniques, and the authors show that misclassification can lead to the adversarial attack."
1490,SP:4f5c00469e4425751db5efbc355085a5e8709def,"method USED-FOR imperceptible attack. black box attack scenario FEATURE-OF imperceptible attack. salient object segmentation USED-FOR salient region. method USED-FOR imperceptible attacks. baselines COMPARE method. method COMPARE baselines. Imagenet examples EVALUATE-FOR baselines. metric MAD EVALUATE-FOR imperceptibility. OtherScientificTerm are salient regions, and predicted class logits. Method is tree search method. ","This paper proposes a method for imperceptible attack in the black box attack scenario. The key idea is to use salient object segmentation to identify the salient region. The salient regions are then used to train a tree search method. The proposed method is evaluated on Imagenet examples and compared to several baselines. The results show that the proposed method can be used to defend against a variety of imperceptibly attacks. The authors also show that imperceptibility can be measured using the metric MAD, which is based on the predicted class logits."
1491,SP:779821ed85084f8bf1b29d8822b312989b186ee9,GNN - based extension of transformers USED-FOR reaction prediction. GNN - based embedding of molecules PART-OF reaction embeddings. approach USED-FOR problems. OtherScientificTerm is artificial bias. Method is sequence embeddings. Task is reaction and retrosynthesis prediction. ,"This paper proposes a GNN-based extension of transformers for reaction prediction. Specifically, the authors propose to incorporate the GNN -based embedding of molecules into the reaction embeddings to avoid artificial bias. The proposed approach is applied to two problems: (1) the problem of sequence embedding, and (2) reaction and retrosynthesis prediction."
1492,SP:779821ed85084f8bf1b29d8822b312989b186ee9,retrosynthesis CONJUNCTION reaction outcome prediction. reaction outcome prediction CONJUNCTION retrosynthesis. graph - to - sequence architecture USED-FOR retrosynthesis. graph - to - sequence architecture USED-FOR reaction outcome prediction. Graph2SMILES HYPONYM-OF graph - to - sequence architecture. attention - augmented D - MPNN encoder USED-FOR local information. attention - augmented D - MPNN encoder CONJUNCTION global attention encoder. global attention encoder CONJUNCTION attention - augmented D - MPNN encoder. graph - aware positional embeddings USED-FOR global information. global attention encoder CONJUNCTION graph - aware positional embeddings. graph - aware positional embeddings CONJUNCTION global attention encoder. global attention encoder USED-FOR Graph2SMILES. attention - augmented D - MPNN encoder USED-FOR Graph2SMILES. one - step retrosynthesis CONJUNCTION reaction outcome prediction. reaction outcome prediction CONJUNCTION one - step retrosynthesis. Graph2SMILES COMPARE Transformer baselines. Transformer baselines COMPARE Graph2SMILES. Graph2SMILES COMPARE state - of - the - art methods. state - of - the - art methods COMPARE Graph2SMILES. Transformer baselines COMPARE state - of - the - art methods. state - of - the - art methods COMPARE Transformer baselines. reaction outcome prediction HYPONYM-OF tasks. one - step retrosynthesis HYPONYM-OF tasks. one - step retrosynthesis EVALUATE-FOR Graph2SMILES. one - step retrosynthesis EVALUATE-FOR state - of - the - art methods. tasks EVALUATE-FOR state - of - the - art methods. tasks EVALUATE-FOR Transformer baselines. tasks EVALUATE-FOR Graph2SMILES. ,"This paper proposes Graph2SMILES, a graph-to-sequence architecture for retrosynthesis and reaction outcome prediction, which is a generalization of the Graph-To-Sequence architecture proposed in [1]. Graph2SmILES uses an attention-augmented D-MPNN encoder to encode local information, and a global attention encoder and a learned graph-aware positional embeddings to encode global information. The authors conduct experiments on three different tasks: one-step retrostynthesis, reaction outcomes prediction, and the task of predicting the next state of the art Transformer baselines. The experiments on the three tasks show that the proposed Graph2MILES outperforms the state-of-the-art methods on all three tasks, as well as on one of the three new tasks."
1493,SP:779821ed85084f8bf1b29d8822b312989b186ee9,method USED-FOR retrosynthesis. graph - based encoder CONJUNCTION sequence based encoder. sequence based encoder CONJUNCTION graph - based encoder. graph - based encoder PART-OF model. sequence based encoder PART-OF model. global attention PART-OF encoder. positional method USED-FOR encoder. Transformer model USED-FOR decoder. relative positional encoding USED-FOR Transformer model. relative positional encoding USED-FOR decoder. retrosynthesis datasets EVALUATE-FOR method. OtherScientificTerm is mapping numbers. ,"This paper proposes a method for retrosynthesis. The model consists of a graph-based encoder and a sequence based encoder. The encoder is trained using a positional method, where global attention is applied to the mapping numbers. The decoder uses a Transformer model with relative positional encoding. The proposed method is evaluated on two retrosysnthesis datasets."
1494,SP:779821ed85084f8bf1b29d8822b312989b186ee9,graph - to - SMILES framework USED-FOR synthesis planning and reaction outcome prediction tasks. engineering techniques PART-OF graph - to - SMILES framework. graph neural networks CONJUNCTION Transformer attention model. Transformer attention model CONJUNCTION graph neural networks. Transformer decoder USED-FOR SMILES string. Transformer attention model USED-FOR graph inputs. graph neural networks USED-FOR graph inputs. Transformer decoder USED-FOR method. Transformer attention model USED-FOR method. graph neural networks USED-FOR method. approach COMPARE vanilla SMILES - to - SMILES transformer baseline. vanilla SMILES - to - SMILES transformer baseline COMPARE approach. approach COMPARE methods. methods COMPARE approach. benchmark retrosynthesis and reaction prediction tasks EVALUATE-FOR approach. benchmark retrosynthesis and reaction prediction tasks EVALUATE-FOR methods. benchmark retrosynthesis and reaction prediction tasks EVALUATE-FOR vanilla SMILES - to - SMILES transformer baseline. ,This paper proposes a graph-to-SMILES framework that combines engineering techniques for synthesis planning and reaction outcome prediction tasks. The proposed method uses graph neural networks and a Transformer attention model to model the graph inputs and the Transformer decoder to generate the SMILES string. Experiments on benchmark retrosynthesis and reaction prediction tasks show that the proposed approach outperforms the vanilla SMILE-to -SMILE transformer baseline.
1495,SP:ce3cde67564679a8d9a0539f1e12551390b91475,"hierarchical reinforcement learning ( HRL ) USED-FOR automatic disease diagnosis. hierarchical framework COMPARE baselines. baselines COMPARE hierarchical framework. accuracy CONJUNCTION symptom recall. symptom recall CONJUNCTION accuracy. hierarchical framework USED-FOR disease diagnosis. symptom recall EVALUATE-FOR baselines. accuracy EVALUATE-FOR baselines. accuracy EVALUATE-FOR hierarchical framework. symptom recall EVALUATE-FOR hierarchical framework. OtherScientificTerm is action space. Metric is training efficiency. Material are public dataset, and synthetic dataset. ",This paper proposes hierarchical reinforcement learning (HRL) for automatic disease diagnosis. The hierarchical framework is shown to outperform baselines in terms of accuracy and symptom recall. The paper also proposes a new action space to reduce the number of parameters and improve training efficiency. Experiments are conducted on a public dataset and a synthetic dataset.
1496,SP:ce3cde67564679a8d9a0539f1e12551390b91475,hierarchical policy USED-FOR dialogue policy learning. master model CONJUNCTION low models. low models CONJUNCTION master model. models PART-OF hierarchical reinforcement learning - based approach. master model HYPONYM-OF models. master model PART-OF hierarchical reinforcement learning - based approach. symptoms checkers CONJUNCTION disease classifier. disease classifier CONJUNCTION symptoms checkers. disease classifier PART-OF low models. symptoms checkers PART-OF low models. Material is real and synthetic datasets. ,This paper proposes a hierarchical policy for dialogue policy learning. The proposed hierarchical reinforcement learning-based approach consists of a master model and two low models: symptoms checkers and a disease classifier. Experiments are conducted on both real and synthetic datasets.
1497,SP:ce3cde67564679a8d9a0539f1e12551390b91475,reinforcement learning USED-FOR automatic disease diagnosis. high - level master model CONJUNCTION low - level policy. low - level policy CONJUNCTION high - level master model. hierarchical policies PART-OF dialogue policy learning. low - level policy HYPONYM-OF hierarchical policies. high - level master model HYPONYM-OF hierarchical policies. real - life and synthetic data EVALUATE-FOR approach. Material is task - oriented dialogues. ,"This paper studies reinforcement learning in the context of automatic disease diagnosis. The authors propose to learn hierarchical policies in dialogue policy learning, i.e., a high-level master model and a low-level policy. The proposed approach is evaluated on both real-life and synthetic data, where the task-oriented dialogues are used."
1498,SP:ce3cde67564679a8d9a0539f1e12551390b91475,Hierarchical Reinforcement Learning ( HRL ) USED-FOR automatic disease diagnosis. task - oriented dialogues setting USED-FOR automatic disease diagnosis. RL USED-FOR automatic disease diagnosis. symptoms HYPONYM-OF action space. high level policy CONJUNCTION low level policy. low level policy CONJUNCTION high level policy. HRL strategy COMPARE RL systems. RL systems COMPARE HRL strategy. disease diagnosis accuracy EVALUATE-FOR RL systems. disease diagnosis accuracy EVALUATE-FOR HRL strategy. Method is hierarchical dialogue policy. ,"This paper proposes Hierarchical Reinforcement Learning (HRL) for automatic disease diagnosis in a task-oriented dialogues setting. The main contribution of this paper is to propose a hierarchical dialogue policy, where a high level policy learns to communicate with a low level policy, which is trained to predict the next state in the action space (e.g., symptoms). The authors show that the proposed HRL strategy outperforms existing RL systems in terms of disease diagnosis accuracy."
1499,SP:bd9cb543b5f199ab45e1bf8609c683f12ceb7659,self - supervised learning ( SSL ) framework USED-FOR FL. SSL methods USED-FOR FL setting. SimSiam framework USED-FOR personalized federated SSL. representation regularization - based personalization method COMPARE variants. variants COMPARE representation regularization - based personalization method. ,"This paper proposes a self-supervised learning (SSL) framework for FL. The main contribution of this paper is the introduction of personalized federated SSL based on the SimSiam framework, which is a generalization of SSL methods to the FL setting. The authors also propose a representation regularization-based personalization method, which outperforms previous variants."
1500,SP:bd9cb543b5f199ab45e1bf8609c683f12ceb7659,"self - supervised learning USED-FOR limited label and data heterogeneity problems. self - supervised learning USED-FOR personalized federated learning. framework USED-FOR algorithms. consensus CONJUNCTION personalization. personalization CONJUNCTION consensus. algorithms CONJUNCTION hyper - parameters. hyper - parameters CONJUNCTION algorithms. Generic is algorithm. Method are Per- SSFL, and self - supervised FL framework. ","This paper studies the problem of personalized federated learning with self-supervised learning in the context of limited label and data heterogeneity problems. The authors propose a new algorithm called Per-SSFL, which is a generalization of the existing Self-Supervised Federated Learning (SSFL) framework. The main contribution of this paper is the introduction of a new self supervised FL framework, which aims to bridge the gap between consensus and personalization. The proposed framework can be applied to both algorithms and hyper-parameters."
1501,SP:bd9cb543b5f199ab45e1bf8609c683f12ceb7659,OtherScientificTerm is SSFL_Sumpplementary / SSFL - Source - Code. ,"This paper proposes SSFL_Sumpplementary/SSFL-source-Code, which is an extension of SSFL. The main contribution of this paper is the introduction of the idea of using a ""source-code"" instead of a ""target-source"" model. "
1502,SP:bd9cb543b5f199ab45e1bf8609c683f12ceb7659,SimSiam architecture USED-FOR feature representations. personalisation USED-FOR local client models. extensions USED-FOR SimSiam architecture. personalisation PART-OF extensions. representation COMPARE KNN classifier. KNN classifier COMPARE representation. Task is Federated Learning. Material is labelled data. ,"This paper proposes extensions to the SimSiam architecture for learning feature representations. The extensions include personalisation for local client models, which is an important problem in Federated Learning. The authors show that the learnt representation is better than a standard KNN classifier on labelled data."
1503,SP:8ff52b027a3c2a464b2c2fedb768c092b0fc6ca5,discretization of PDEs USED-FOR ResNet - like DNN model. adjust operator USED-FOR second - order convection - diffusion PDE. ResNets CONJUNCTION ResNets. ResNets CONJUNCTION ResNets. Gaussian noise injection USED-FOR ResNets. Rademacher complexity FEATURE-OF generalization guarantees. generalization guarantees USED-FOR PDE. input perturbations FEATURE-OF robustness guarantees. model hyperparameters USED-FOR ResNet. method USED-FOR ResNet. clean and adversarial datasets EVALUATE-FOR ResNet. ,"This paper proposes a ResNet-like DNN model based on the discretization of PDEs. Specifically, the authors propose an adjust operator for the second-order convection-diffusion PDE. The authors also provide generalization guarantees for the PDE with Rademacher complexity and robustness guarantees to input perturbations. Experiments on both clean and adversarial datasets demonstrate the effectiveness of the proposed method in training ResNet with different model hyperparameters. The experiments also show that ResNets trained with Gaussian noise injection can achieve comparable performance to standard ResNet."
1504,SP:8ff52b027a3c2a464b2c2fedb768c092b0fc6ca5,"discrete approximations USED-FOR PDEs. PDE USED-FOR base classifier. Method are DNNs, network, PDE theory, and neural architectures. Generic is operator. ","This paper studies the problem of training DNNs with discrete approximations to PDEs. In particular, the authors consider the case where the base classifier is approximated by a PDE, and the network is trained to predict the output of the PDE. The authors propose a new operator, which they call the ""base classifier"" and show that this operator can be used to approximate the output PDE of the network. This is an interesting and interesting idea, as it is well-motivated by PDE theory and the fact that it is a useful tool for training neural architectures. "
1505,SP:8ff52b027a3c2a464b2c2fedb768c092b0fc6ca5,transport equation CONJUNCTION diffusion equation. diffusion equation CONJUNCTION transport equation. resnet CONJUNCTION gaussian injected resnet. gaussian injected resnet CONJUNCTION resnet. architecture USED-FOR transport equation. gaussian injected resnet USED-FOR architecture. resnet USED-FOR architecture. robustness guarantees FEATURE-OF PDE framework. framework USED-FOR robustness guarantees. generalization gap EVALUATE-FOR neural network. learning algorithm USED-FOR network. resnet HYPONYM-OF NN. PDE framework USED-FOR network. NN USED-FOR PDE framework. NN USED-FOR network. it COMPARE Gaussian Noise injection. Gaussian Noise injection COMPARE it. method COMPARE Gaussian Noise injection. Gaussian Noise injection COMPARE method. ,"This paper studies the generalization gap of a neural network trained using a PDE framework with robustness guarantees. The proposed architecture is a combination of a resnet and a gaussian injected resnet. The network is trained using an NN (e.g., resnet) and the learning algorithm is based on PDE. The method is evaluated and compared to Gaussian Noise injection."
1506,SP:8ff52b027a3c2a464b2c2fedb768c092b0fc6ca5,flow map USED-FOR DNN classifiers. classifier USED-FOR flow map. convection - diffusion equation USED-FOR flow map. ResNets CONJUNCTION Gaussian noise injection. Gaussian noise injection CONJUNCTION ResNets. robustness guarantee EVALUATE-FOR classifier. PDE USED-FOR classifier. robustness EVALUATE-FOR model. model COMPARE ResNet(s ). ResNet(s ) COMPARE model. 2 - d half moon data set EVALUATE-FOR model. model USED-FOR adversarial attacks. ,"This paper studies the problem of learning a flow map for DNN classifiers. The flow map is derived from a convection-diffusion equation, and a classifier is trained to predict the flow map. The authors provide a robustness guarantee for the proposed classifier based on the PDE. The proposed model is evaluated on a 2-d half moon data set, and compared to ResNet(s) and Gaussian noise injection. The model is shown to be robust to adversarial attacks."
1507,SP:d44f0ebc2847695ecb4ed0bb3df61d6cd8cc6a40,expressivity CONJUNCTION complexity. complexity CONJUNCTION expressivity. complexity CONJUNCTION unpredictability. unpredictability CONJUNCTION complexity. unpredictability FEATURE-OF emergent languages. complexity FEATURE-OF emergent languages. expressivity FEATURE-OF emergent languages. referential games FEATURE-OF emergent languages. expressivity FEATURE-OF emergent languages. contrastive loss based training method USED-FOR referential games. contrastive loss based training method USED-FOR collapse of message types. Method is training methods. ,"This paper studies the expressivity, complexity, and unpredictability of emergent languages in referential games. The authors propose a contrastive loss based training method to address the problem of collapse of message types and show that the proposed training methods are effective."
1508,SP:d44f0ebc2847695ecb4ed0bb3df61d6cd8cc6a40,communication protocol USED-FOR emergent communication. scenario design USED-FOR communication protocol. scenario complexity CONJUNCTION scenario unpredictability. scenario unpredictability CONJUNCTION scenario complexity. expressivity EVALUATE-FOR protocol. learnability EVALUATE-FOR protocol. learnability EVALUATE-FOR expressivity. softmax loss COMPARE referential'loss. referential'loss COMPARE softmax loss. OtherScientificTerm is contrastive'loss. ,"This paper studies the problem of designing a communication protocol for emergent communication based on scenario design. The authors argue that scenario complexity and scenario unpredictability are important factors for the expressivity of the proposed protocol. To address these issues, the authors propose a 'contrastive' loss, which is a softmax loss instead of a'referential' loss."
1509,SP:d44f0ebc2847695ecb4ed0bb3df61d6cd8cc6a40,"language games FEATURE-OF emergent language. context complexity CONJUNCTION unpredicability. unpredicability CONJUNCTION context complexity. context complexity CONJUNCTION unpredicability. unpredicability CONJUNCTION context complexity. context complexity USED-FOR language expressivity. unpredicability USED-FOR language expressivity. Generic is language. Method are language game, and referential games. OtherScientificTerm are Complexity, and Unpredictability. ","This paper studies language games of emergent language in language games, where the goal is to learn a language that is more expressive than a language game. The authors argue that language expressivity is a function of context complexity, unpredicability, and complexity of the language. Complexity is defined as the difference between the language game and referential games. Unpredictability is defined in terms of how well the language is expressive."
1510,SP:d44f0ebc2847695ecb4ed0bb3df61d6cd8cc6a40,"DL - based language games FEATURE-OF emergent languages. expressivity FEATURE-OF emergent language. complexity CONJUNCTION unpredictability of context. unpredictability of context CONJUNCTION complexity. expressivity FEATURE-OF emergent language. unpredictability of context FEATURE-OF language games. measure USED-FOR expressivity. partial ordering between languages USED-FOR measure. mutual information USED-FOR expressivity of languages. contrastive loss USED-FOR collapse of message types. Method is referential games. OtherScientificTerm are discriminatory information, and unpredictability. ","This paper studies emergent languages in DL-based language games. The authors propose a measure of expressivity of emergent language that is based on partial ordering between languages, which is an extension of referential games. They show that language games with high complexity and high unpredictability of context exhibit high expressivity in terms of the mutual information between languages. They also propose a contrastive loss to prevent collapse of message types due to discriminatory information, which they call ""unpredictability""."
1511,SP:892558b9f4fb53ed5ca2a7ee440b7d728b1886d6,Sample Average Uncertainty ( SAU ) CONJUNCTION upper confidence bound ( UCB ). upper confidence bound ( UCB ) CONJUNCTION Sample Average Uncertainty ( SAU ). exploration bonus USED-FOR deep RL. Sample Average Uncertainty ( SAU ) USED-FOR exploration bonus. upper confidence bound ( UCB ) USED-FOR exploration bonus. SAU HYPONYM-OF uncertainty quantification. SAU USED-FOR UCB - type bonus. it USED-FOR optimal regret. multi - armed bandits FEATURE-OF UCB - type bonus. RL CONJUNCTION deep RL settings. deep RL settings CONJUNCTION RL. bandits settings CONJUNCTION RL. RL CONJUNCTION bandits settings. SAU - UCB - type bonus USED-FOR ( deep ) RL. RL CONJUNCTION deep RL. deep RL CONJUNCTION RL. algorithms COMPARE benchmarks. benchmarks COMPARE algorithms. Method is SAU - UCB - type exploration bonus. ,"This paper proposes a new exploration bonus for deep RL based on the combination of the Sample Average Uncertainty (SAU) and the upper confidence bound (UCB). SAU is an uncertainty quantification and it can be used to estimate the optimal regret. The UCB-type bonus is based on SAU and is shown to be optimal in multi-armed bandits. Experiments are conducted on bandits settings, RL and deep RL settings. Results show that the proposed SAU-UCB -type exploration bonus is effective for (deep) RL and outperforms existing algorithms and benchmarks."
1512,SP:892558b9f4fb53ed5ca2a7ee440b7d728b1886d6,$ \delta^2$-exploration USED-FOR reinforcement learning ( RL ). sample average uncertainty ( SAU ) PART-OF RL exploration. exploration COMPARE $ \epsilon$-greedy. $ \epsilon$-greedy COMPARE exploration. SAU PART-OF Q - learning. SAU PART-OF $ \delta^2$-exploration. $ \delta^2$-exploration COMPARE SOTA exploration algorithms. SOTA exploration algorithms COMPARE $ \delta^2$-exploration. $ \delta^2$-exploration PART-OF DQN. $ \delta^2$-explorations COMPARE bootstrapped DQN. bootstrapped DQN COMPARE $ \delta^2$-explorations. ,"This paper studies the use of $\delta^2$-exploration in reinforcement learning (RL) in the context of sample average uncertainty (SAU) in RL exploration. Compared to exploration with $\epsilon$-greedy, the authors argue that the SAU in Q-learning can be viewed as a special case of SAU. The authors propose to incorporate SAU into $\delta^{2}$, which is a generalization of DQN. The experiments show that the proposed $\tilde{O}(\frac{1}{\sqrt{delta}^2} \log(d^2) \log (d/n)$-expansion of $d/d)$ exploration outperforms the SOTA exploration algorithms. The experimental results also show that $\frac{d/2}(\log d/n \log d) \frac{n-1}{d}\log d}$-extraction of $DQN outperforms bootstrapped versions of the same algorithm."
1513,SP:892558b9f4fb53ed5ca2a7ee440b7d728b1886d6,exploration method USED-FOR RL problem. SAU in bandits HYPONYM-OF exploration method. exploration approach CONJUNCTION Q - learning algorithm. Q - learning algorithm CONJUNCTION exploration approach. approach COMPARE Q - learning algorithm. Q - learning algorithm COMPARE approach. eps - greedy exploration USED-FOR Q - learning algorithm. ,"This paper proposes a new exploration method, SAU in bandits, for RL problem. The proposed approach is shown to outperform the Q-learning algorithm based on eps-greedy exploration."
1514,SP:892558b9f4fb53ed5ca2a7ee440b7d728b1886d6,approximation of the estimation error USED-FOR exploration of action. Q - learning CONJUNCTION DQN. DQN CONJUNCTION Q - learning. uncertainty measure PART-OF tabular and continuous methods. DQN HYPONYM-OF tabular and continuous methods. Q - learning HYPONYM-OF tabular and continuous methods. Metric is SAU measure. Method is bandits. Task is RL. OtherScientificTerm is average squared temporal difference error. ,This paper studies the problem of estimation of the estimation error for exploration of action. The main contribution of this paper is to propose a new SAU measure for estimating the uncertainty of an action in bandits. This uncertainty measure can be incorporated into both tabular and continuous methods such as Q-learning and DQN. The authors also propose to use the average squared temporal difference error as a measure of uncertainty in RL.
1515,SP:2f6e266b03939c96434834579999707d3268c5d6,MLP network USED-FOR spatio - temporal dynamics of video. MLP network USED-FOR INR - based design. discriminator USED-FOR unnatural motions. UCF101 dataset EVALUATE-FOR method. Generic is design. ,This paper proposes an INR-based design that uses an MLP network to model the spatio-temporal dynamics of video. The key idea of the design is to train a discriminator to detect unnatural motions. The method is evaluated on the UCF101 dataset.
1516,SP:2f6e266b03939c96434834579999707d3268c5d6,implicit neural representations paradigm USED-FOR generative adversarial networks. generative adversarial networks USED-FOR video generation. Implicit neural representations USED-FOR continuous signals. parametrized neural networks USED-FOR Implicit neural representations. parametrized neural networks USED-FOR continuous signals. motion discriminator USED-FOR unnatural motions. INR - based video generator CONJUNCTION motion discriminator. motion discriminator CONJUNCTION INR - based video generator. identifying unnatural motions PART-OF GAN. motion discriminator PART-OF GAN. INR - based video generator PART-OF GAN. OtherScientificTerm is 3d tensors of RGB values. Generic is approach. ,"This paper proposes an implicit neural representations paradigm for training generative adversarial networks for video generation. Implicit neural representations are parametrized neural networks that are trained to generate continuous signals from 3d tensors of RGB values. The proposed GAN consists of an INR-based video generator, a motion discriminator that is trained for identifying unnatural motions. The approach is evaluated on a variety of datasets."
1517,SP:2f6e266b03939c96434834579999707d3268c5d6,"implicit neural representations ( INRs ) USED-FOR video generation approach. coordinate - based models USED-FOR continuous representations of images. coordinate - based models USED-FOR generator model. 2D discriminator USED-FOR image GAN research. 2D discriminator USED-FOR discriminator architecture. Method are "" dynamics - aware "" GAN, and discriminator. ","This paper proposes a video generation approach based on implicit neural representations (INRs). The generator model is based on coordinate-based models that can learn continuous representations of images. The discriminator architecture uses a 2D discriminator that has been widely used in image GAN research. The authors also propose a ""dynamics-aware"" GAN that learns to discriminate between different views of the same image."
1518,SP:2f6e266b03939c96434834579999707d3268c5d6,"method USED-FOR video generation. it USED-FOR temporal domain. motion codes PART-OF CNN - based works. motion codes CONJUNCTION content codes. content codes CONJUNCTION motion codes. content CONJUNCTION motion. motion CONJUNCTION content. Method are INR - GAN framework, video methods, and discriminator. Task is continuous time and space image generation. Generic is functions. ","This paper proposes a method for video generation that extends the INR-GAN framework by extending it to the temporal domain. In particular, the authors consider continuous time and space image generation, which is an important problem for video methods. The main contribution of this paper is to extend the existing CNN-based works to include motion codes and content codes. The authors propose two functions: (1) a discriminator that learns to distinguish between content and motion, and (2) a function that discriminates between content codes and motion codes. "
1519,SP:878325384328c885ced7af0ebf31bbf79287c169,"single - winner voting USED-FOR PATE type private semi - supervised learning. single - winner voting USED-FOR problem. report noisy argmax USED-FOR binary mechanism. report noisy argmax USED-FOR tau mechanism. l2 clipping USED-FOR tau mechanism. powerset mechanism USED-FOR multi - label problem. powerset mechanism USED-FOR single label problem. regular report noisy argmax USED-FOR powerset mechanism. single label problem USED-FOR multi - label problem. Method are differentially private multi - winner voting, and private mechanisms. Task is multi - label semi - supervised learning settings. OtherScientificTerm is total votes constraint. ",This paper studies the problem of differentially private multi-winner voting in multi-label semi-supervised learning settings. The problem is formulated as a variant of single-winners voting in PATE type private semi-Supervised learning. The authors propose a new powerset mechanism based on regular report noisy argmax to solve the single label problem and a new tau mechanism using l2 clipping to replace the binary mechanism. The main contribution of this paper is to study the effect of the total votes constraint on the performance of the private mechanisms.
1520,SP:878325384328c885ced7af0ebf31bbf79287c169,"multi - winner voting protocols USED-FOR differentially private single - label learning algorithms. PATE HYPONYM-OF differentially private single - label learning algorithms. binary voting HYPONYM-OF multi - label voting protocols. powerset voting HYPONYM-OF multi - label voting protocols. majority voting USED-FOR Binary voting. voting USED-FOR powerset voting. multi - label methods COMPARE benchmarks. benchmarks COMPARE multi - label methods. $ \tau$-voting COMPARE consensus. consensus COMPARE $ \tau$-voting. Method are differentially private multi - label mechanisms, and aggregation mechanism. Metric is privacy guarantee. OtherScientificTerm is $ \ell_2$-norm. ","This paper studies differentially private multi-label mechanisms. The authors propose multi-winner voting protocols, which are used in differentially public single-label learning algorithms such as PATE and PATE, to improve the privacy guarantee. Binary voting is based on majority voting, while powerset voting uses voting, i.e. the aggregation mechanism. The main contribution of this paper is to show that the $\tau$-voting is better than consensus, and that the $ \ell_2$-norm is a better measure of privacy guarantee than the $\ell_1$ norm. In addition, the authors compare the performance of the proposed multi-labels methods with other benchmarks."
1521,SP:878325384328c885ced7af0ebf31bbf79287c169,differentially private multi - winner voting USED-FOR multi - label learning. privacy constraint USED-FOR information leakage. differentially - private election USED-FOR Binary. bounded \ell_2 norm FEATURE-OF votes. \tau voting HYPONYM-OF mechanisms. Binary HYPONYM-OF mechanisms. powerset voting HYPONYM-OF mechanisms. Binary voting COMPARE powerset voting. powerset voting COMPARE Binary voting. Binary voting COMPARE naive approach. naive approach COMPARE Binary voting. multi - winner DP techniques USED-FOR single - label technique. multi - winner DP techniques USED-FOR PATE. PATE HYPONYM-OF single - label technique. OtherScientificTerm is election. Generic is approach. ,"This paper proposes a differentially private multi-winner voting for multi-label learning. The main idea is to impose a privacy constraint to prevent information leakage. Binary is trained with differentially-private election, where the votes have bounded \ell_2 norm. The paper shows that the proposed election can be applied to a variety of mechanisms such as \tau voting and powerset voting. Binary voting is shown to outperform the naive approach by a large margin. The proposed approach, PATE, is a single-label technique based on multi-winners DP techniques."
1522,SP:878325384328c885ced7af0ebf31bbf79287c169,"Private multi - winner voting USED-FOR revealing k - hot binary vectors. bounded differential privacy guarantee FEATURE-OF revealing k - hot binary vectors. composition USED-FOR Binary voting. l2 norm FEATURE-OF \tau voting. binary vector USED-FOR Powerset voting. Powerset voting COMPARE Binary voting. Binary voting COMPARE Powerset voting. mechanisms USED-FOR privacy - preserving multi - label learning. large real - world healthcare data CONJUNCTION multi - label benchmarks. multi - label benchmarks CONJUNCTION large real - world healthcare data. techniques COMPARE DPSGD. DPSGD COMPARE techniques. large real - world healthcare data EVALUATE-FOR DPSGD. multi - label benchmarks EVALUATE-FOR DPSGD. large real - world healthcare data EVALUATE-FOR techniques. multi - label benchmarks EVALUATE-FOR techniques. mechanisms USED-FOR models. techniques COMPARE others. others COMPARE techniques. mechanisms USED-FOR multi - site ( distributed ) setting. multi - site ( distributed ) setting FEATURE-OF models. centralized setting EVALUATE-FOR techniques. centralized setting EVALUATE-FOR others. OtherScientificTerm are k - hot binary vectors, and power set. ","Private multi-winner voting provides a bounded differential privacy guarantee for revealing k-hot binary vectors. Binary voting is based on composition, where the l2 norm of \tau voting is computed for each binary vector. The paper proposes to use Powerset voting instead of Binary voting, which uses the binary vector as a proxy for the power set. The proposed mechanisms for privacy-preserving multi-label learning are evaluated on large real-world healthcare data as well as several multi-labels benchmarks and compared to DPSGD. The techniques are shown to outperform others in the centralized setting and in the multi-site (distributed) setting."
1523,SP:2488d3697a4ea732526b3ef11fbbd93e27d42e81,learning rate grafting USED-FOR power and dynamics of optimizers. grafting USED-FOR tuned optimizer. tuned optimizer's group - wise magnitudes CONJUNCTION untuned optimizer's group - wise directions. untuned optimizer's group - wise directions CONJUNCTION tuned optimizer's group - wise magnitudes. tuned optimizer's group - wise magnitudes USED-FOR tuned optimizer. Method is Learning rate grafting. Generic is networks. ,"Learning rate grafting aims to improve the power and dynamics of optimizers. In particular, the authors propose to use grafting to train a tuned optimizer that maximizes the combination of the original trained optimizer's group-wise magnitudes and the untuned optimizer’s group -wise directions. The authors demonstrate the effectiveness of the proposed method on a variety of networks."
1524,SP:2488d3697a4ea732526b3ef11fbbd93e27d42e81,"optimizers USED-FOR It. experiment exploration USED-FOR optimizing mode. Generic is method. Method is optimizer grafting. OtherScientificTerm are update direction of parameters, and update stride of parameters. ",This paper proposes a method called optimizer grafting. It is based on the idea that the update direction of parameters should follow the update stride of parameters. It can be seen as an extension of existing optimizers that have been trained with different types of optimizers. The main idea is to use experiment exploration in the optimizing mode. 
1525,SP:2488d3697a4ea732526b3ef11fbbd93e27d42e81,"technique USED-FOR learning rate hyperparameter tuning. learning rate hyperparameter tuning USED-FOR deep learning. meta - algorithm USED-FOR optimizers. M#D HYPONYM-OF meta - algorithm. step magnitude PART-OF meta - algorithm. computational cost EVALUATE-FOR optimizer hyper parameter search. implicit step size schedule USED-FOR optimizer. it USED-FOR BERT model. vision models USED-FOR AdaGrad. M CONJUNCTION D. D CONJUNCTION M. computational budget FEATURE-OF optimizer hyperparameter searches. Adam USED-FOR BERT pre - training. Adam#SGD USED-FOR BERT pre - training. SGD USED-FOR BERT pre - training. SGD CONJUNCTION Adam. Adam CONJUNCTION SGD. SGD COMPARE Adam#SGD. Adam#SGD COMPARE SGD. Adam COMPARE Adam#SGD. Adam#SGD COMPARE Adam. transfer of implicit step size schedules EVALUATE-FOR optimizers. Adam#SGD COMPARE Adam. Adam COMPARE Adam#SGD. AdaGrad CONJUNCTION SGD. SGD CONJUNCTION AdaGrad. SGD CONJUNCTION SGD#AdaGrad. SGD#AdaGrad CONJUNCTION SGD. SGD CONJUNCTION AdaGrad. AdaGrad CONJUNCTION SGD. ImageNet and CIFAR-10 ) FEATURE-OF image classification. non - adaptive correction USED-FOR D. learning rate schedule USED-FOR per - layer step size correction. per - layer step size correction CONJUNCTION adaptive preconditioning. adaptive preconditioning CONJUNCTION per - layer step size correction. learning rate schedule USED-FOR per - layer variant. schedule USED-FOR transfer approach. Method are optimizer grafting, grafting meta algorithm   ( M#D ), and Partitioning. OtherScientificTerm are implicit step size schedules, and learning rate. Generic is global variant. Metric is top1 accuracy. ","This paper proposes a technique for learning rate hyperparameter tuning in deep learning. The main idea is to use a meta-algorithm, called M#D, to train optimizers that maximize the step magnitude of their implicit step size schedules. This is motivated by the computational cost of optimizer hyper parameter search in terms of computational budget. The authors propose a grafting meta algorithm  (M#D) and apply it to a BERT model. They compare the performance of SGD, Adam, and Adam#SGD for BERT pre-training with respect to the transfer of implicit step sizes schedules. They show that for AdaGrad trained on vision models, M and D with non-adaptive correction can achieve top1 accuracy on image classification (on ImageNet and CIFAR-10) while SGD and Adam can only achieve top2 accuracy. They also show that the transfer approach can be improved by using a different learning rate schedule for per-layer step size correction and adaptive preconditioning.   The main contribution of this paper is to propose optimizer grafting. Partitioning the optimizer into a global variant and then applying the learning rate to each of the global variants. They evaluate the transfer performance of the proposed optimizers on AdaGrad and SGD as well as SGD#AdaGrad. They find that SGD outperforms Adam and Adam #SGD."
1526,SP:2488d3697a4ea732526b3ef11fbbd93e27d42e81,"optimizer CONJUNCTION learning rate schedule. learning rate schedule CONJUNCTION optimizer. tuned optimizer COMPARE optimizer. optimizer COMPARE tuned optimizer. computational cost EVALUATE-FOR optimizer hyperparameter search. it USED-FOR BERT model. grafting USED-FOR non - adaptive learning rate correction. non - adaptive learning rate correction USED-FOR SGD. non - adaptive learning rate correction USED-FOR it. non - adaptive learning rate correction USED-FOR BERT model. Method is optimizer grafting. OtherScientificTerm are implicit step size schedule, and empirical performance. Task is optimizer comparisons. ","This paper proposes optimizer grafting, which is a modification of optimizer hyperparameter search that aims to reduce the computational cost incurred by optimizing the optimizer and the learning rate schedule. The main idea is to replace the original tuned optimizer with a new optimizer that is more flexible in terms of the implicit step size schedule. This is motivated by the observation that the performance of the original optimizer is highly correlated with the empirical performance. The authors propose to use grafting as a form of non-adaptive learning rate correction for SGD, and apply it to the BERT model. The experimental results show that the proposed optimizer comparisons outperform the existing state of the art."
1527,SP:83b82c145f446c1a29e863362c6ceed018e93e2b,sparse reward settings FEATURE-OF RL. sub - optimal demonstrations USED-FOR RL. RL policy USED-FOR behavior policy. offline dataset EVALUATE-FOR behavior policy. RL policy CONJUNCTION behavior policy. behavior policy CONJUNCTION RL policy. openAI gym style tasks CONJUNCTION real robot navigation task. real robot navigation task CONJUNCTION openAI gym style tasks. real robot navigation task EVALUATE-FOR method. openAI gym style tasks EVALUATE-FOR method. Method is TRPO. Generic is constraint. OtherScientificTerm is KL divergence. ,"This paper addresses the problem of RL in sparse reward settings with sub-optimal demonstrations. To this end, the authors propose TRPO, a method that jointly trains an RL policy and a behavior policy on an offline dataset. This constraint is enforced by minimizing the KL divergence between the RL policy’s output and the behavior policy. The proposed method is evaluated on openAI gym style tasks and a real robot navigation task."
1528,SP:83b82c145f446c1a29e863362c6ceed018e93e2b,TRPO algorithm USED-FOR learning guidance. TRPO algorithm USED-FOR LOGO ’. learning schedule CONJUNCTION hyper - parameter. hyper - parameter CONJUNCTION learning schedule. LOGO USED-FOR guidance. sub - optimal guidance policy data USED-FOR training. data USED-FOR guidance. sub - optimal guidance policy data USED-FOR learning contribution. data USED-FOR LOGO. trust - region methodology USED-FOR LOGO. MuJoCo continuous control tasks CONJUNCTION Gazebo TurtleBot simulation. Gazebo TurtleBot simulation CONJUNCTION MuJoCo continuous control tasks. MuJoCo continuous control tasks EVALUATE-FOR method. Gazebo TurtleBot simulation EVALUATE-FOR method. Generic is it. Task is real - world robot. ,This paper proposes a TRPO algorithm called ‘LOGO’ for learning guidance. The key idea is to use the sub-optimal guidance policy data during training to improve the learning contribution of LOGO. This is achieved by combining the learning schedule and the hyper-parameter. The authors also propose a trust-region methodology to train LOGO and show that it can be applied to real-world robot. The method is evaluated on MuJoCo continuous control tasks and the Gazebo TurtleBot simulation.
1529,SP:83b82c145f446c1a29e863362c6ceed018e93e2b,sparse reward functions USED-FOR reinforcement learning. offline demonstration USED-FOR sparse reward functions. trust region policy optimization based algorithm USED-FOR guidance. offline demonstration data USED-FOR guidance. offline demonstration data USED-FOR trust region policy optimization based algorithm. benchmark datasets EVALUATE-FOR algorithm. algorithm COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE algorithm. benchmark datasets EVALUATE-FOR state - of - the - art approaches. trajectory tracking CONJUNCTION obstacle avoidance. obstacle avoidance CONJUNCTION trajectory tracking. LOGO USED-FOR trajectory tracking. LOGO USED-FOR obstacle avoidance. mobile robot USED-FOR LOGO. Method is LOGO algorithm. ,This paper proposes a trust region policy optimization based algorithm that uses offline demonstration data to provide guidance for reinforcement learning with sparse reward functions. The proposed algorithm is evaluated on several benchmark datasets and compared to state-of-the-art approaches. The experimental results show that the proposed LOGO algorithm is able to achieve better performance on both trajectory tracking and obstacle avoidance using a mobile robot.
1530,SP:83b82c145f446c1a29e863362c6ceed018e93e2b,"heuristic CONJUNCTION human demonstrations. human demonstrations CONJUNCTION heuristic. data CONJUNCTION trust region""-based methods. trust region""-based methods CONJUNCTION data. offline behavior data USED-FOR suboptimal policy. TRPO HYPONYM-OF trust region""-based methods. heuristic HYPONYM-OF suboptimal policy. human demonstrations HYPONYM-OF suboptimal policy. physical robot ( Turtlebot ) EVALUATE-FOR approach. Material is MuJoCo environment. ","This paper proposes to train a suboptimal policy (i.e., heuristic and human demonstrations) on offline behavior data using both data and ""trust region""-based methods such as TRPO. The approach is evaluated on a physical robot (Turtlebot) in the MuJoCo environment."
1531,SP:cf857736e3dc01325948488c791cbafc24b1c0fe,"Hybrid Neural Pareto Front ( HNPF ) framework USED-FOR multi - objective optimization problems. two - stage approach USED-FOR Pareto optimal solutions. two - stage approach USED-FOR method. it USED-FOR strong Pareto optimal subset. low - dimensional problems EVALUATE-FOR method. OtherScientificTerm are decision space, and weak Pareto optimality. Method is random sampling. ","This paper proposes a Hybrid Neural Pareto Front (HNPF) framework for multi-objective optimization problems. The proposed method uses a two-stage approach to find Paret optimal solutions. The first stage is based on random sampling from the decision space, and the second stage uses it to find a strong Pareta optimal subset. The authors evaluate the proposed method on a variety of low-dimensional problems, and show that the method is able to achieve good performance while maintaining the weak pareto optimality."
1532,SP:cf857736e3dc01325948488c791cbafc24b1c0fe,analytic functions USED-FOR convex and non - convex conditions. analytic functions USED-FOR multi - objective problems. two - stage method USED-FOR Pareto front. ,This paper studies the problem of learning analytic functions for multi-objective problems under convex and non-convex conditions. The authors propose a two-stage method to solve the Pareto front.
1533,SP:cf857736e3dc01325948488c791cbafc24b1c0fe,Pareto frontier FEATURE-OF multiobjective optimization problem. algorithm USED-FOR non - convex optimization. baseline methods USED-FOR problem. algorithm USED-FOR MOO problems. algorithm USED-FOR constraints. constraints USED-FOR MOO problems. algorithm USED-FOR Pareto points. Method is operation research methods. OtherScientificTerm is non - dominated points. ,"This paper studies the multiobjective optimization problem on the Pareto frontier. The authors propose a new algorithm for non-convex optimization and compare with baseline methods for this problem. They show that the proposed algorithm can learn constraints for MOO problems, and that their algorithm can find Paret points that are close to the original objective. They also compare their algorithm with other operation research methods and show that they can find non-dominated points."
1534,SP:cf857736e3dc01325948488c791cbafc24b1c0fe,Hybrid Neural Pareto Front ( HNPF ) USED-FOR non - convex functions. MTL solvers COMPARE Hybrid Neural Pareto Front ( HNPF ). Hybrid Neural Pareto Front ( HNPF ) COMPARE MTL solvers. strategy USED-FOR weak Pareto front identification. Fritz - John conditions USED-FOR strategy. Pareto filter USED-FOR dominated points. dominated points PART-OF weak Pareto front. Pareto filter USED-FOR weak Pareto front. ,"This paper proposes Hybrid Neural Pareto Front (HNPF) for non-convex functions, which is a generalization of MTL solvers. The proposed strategy is based on the Fritz-John conditions and is applied to the problem of weak Paretto front identification. The key idea is to use a Pareoto filter to remove the dominated points from the weak part of the original weakPareto front. "
1535,SP:0085e0bb1a265a3925540fbc4873aae60b8d01ce,representation consolidation method USED-FOR transfer learning. task - specific teachers USED-FOR same - domain downstream tasks. method COMPARE multi - task training. multi - task training COMPARE method. Method is distilled representations. Material is teacher datasets. ,This paper proposes a representation consolidation method for transfer learning. The key idea is to use task-specific teachers for the same-domain downstream tasks. The distilled representations are then used to train on the teacher datasets. The method is compared to multi-task training.
1536,SP:0085e0bb1a265a3925540fbc4873aae60b8d01ce,method USED-FOR consolidated image feature representation. Task is recognition tasks. Generic is it. Material is Imagenet dataset. OtherScientificTerm is multi - head student. ,"This paper proposes a method for learning a consolidated image feature representation for recognition tasks. Specifically, it learns a multi-head student, which is trained on the Imagenet dataset. "
1537,SP:0085e0bb1a265a3925540fbc4873aae60b8d01ce,task - agnostic Knowledge Distillation method USED-FOR student forgetting. generalist model PART-OF task - agnostic Knowledge Distillation method. it USED-FOR related and unrelated downstream tasks. accuracy EVALUATE-FOR method. method COMPARE Knowledge Distillation only. Knowledge Distillation only COMPARE method. ImageNet and iFood image classification tasks EVALUATE-FOR method. Task is representation consolidation. ,"This paper proposes a task-agnostic Knowledge Distillation method that incorporates a generalist model to reduce student forgetting. The main idea of the method is representation consolidation, and it can be applied to both related and unrelated downstream tasks. The method is evaluated on ImageNet and iFood image classification tasks, and the accuracy of the proposed method is shown to be comparable to the state-of-the-art in terms of accuracy. The proposed method also outperforms the state of the art in terms performance, and is also shown to have better generalization performance than the existing method, and to be more efficient compared to the existing knowledge-distillation methods."
1538,SP:0085e0bb1a265a3925540fbc4873aae60b8d01ce,"merged models USED-FOR downstream tasks. knowledge distillation CONJUNCTION multi - teacher distillation. multi - teacher distillation CONJUNCTION knowledge distillation. method USED-FOR merged models. method USED-FOR source tasks. generalized model USED-FOR knowledge distillation. Task is multi - model consolidation. Generic are model, and merged model. Method are teacher model, and few - shot linear probe transfer learning setting. ",This paper studies the problem of multi-model consolidation. The authors propose a method for learning merged models for downstream tasks. The method is based on the observation that the source tasks can be decomposed into two parts: (1) a generalized model for knowledge distillation and (2) a multi-teacher distillation. The main idea is to learn a model for each part of the source task and then to use the merged model for the downstream task. The teacher model is trained in a few-shot linear probe transfer learning setting.
1539,SP:ab0d024d4060235df45182dab584c36db16d8e31,"conformal methods USED-FOR predictive confidence set. parametric class of learning algorithms USED-FOR predictive confidence sets. wrapper methods USED-FOR prediction uncertainty. precision FEATURE-OF predictive confidence sets. hard constraint PART-OF optimization. non - differentiable formulation USED-FOR methods. statistical efficiency EVALUATE-FOR non - differentiable formulation. Method are learning algorithm, differentiable proxy, and class of learning algorithms. OtherScientificTerm are prediction sets, and parameter space. Metric is theoretical guarantees. ","This paper proposes a parametric class of learning algorithms for learning predictive confidence sets. The main idea is to use conformal methods to learn a predictive confidence set, which can then be used as a proxy for the learning algorithm. In particular, the paper shows that wrapper methods can be used to reduce the prediction uncertainty. This is achieved by adding a hard constraint to the optimization, which is a differentiable proxy. This non-differentiable formulation is shown to improve the statistical efficiency of the proposed methods. The paper also provides theoretical guarantees on the precision of the predicted confidence sets, which are based on the fact that the prediction sets are parametric in the parameter space. Finally, the authors provide empirical results to support the theoretical results and demonstrate the effectiveness of their class of algorithms."
1540,SP:ab0d024d4060235df45182dab584c36db16d8e31,single - parameter conformal prediction USED-FOR multiple - learnable parameters. approach USED-FOR conformal predictors. valid coverage EVALUATE-FOR approach. efficiency EVALUATE-FOR approach. regression CONJUNCTION multi - output regression. multi - output regression CONJUNCTION regression. multi - output regression CONJUNCTION classification. classification CONJUNCTION multi - output regression. multi - output regression USED-FOR approach. regression USED-FOR approach. classification USED-FOR approach. Task is efficiency of conformal prediction. ,"This paper studies the efficiency of conformal prediction. In particular, the authors propose a new approach to train conformal predictors with multiple-learnable parameters, instead of relying solely on single-parameter conformal performance. The proposed approach is evaluated on valid coverage and efficiency. The approach is tested on regression, multi-output regression, and classification."
1541,SP:ab0d024d4060235df45182dab584c36db16d8e31,"conformal prediction calibration setup USED-FOR constrained empirical risk minimization problem. differentiable surrogate losses CONJUNCTION Lagrangians. Lagrangians CONJUNCTION differentiable surrogate losses. Lagrangians USED-FOR problem. differentiable surrogate losses USED-FOR problem. method COMPARE baselines. baselines COMPARE method. OtherScientificTerm are efficiency loss, coverage constraints, generalization error, and set - function classes. Generic are formulation, and approach. Method are set - based predictor, and constrained ERM. Metric is efficiency. ","This paper studies a constrained empirical risk minimization problem under a conformal prediction calibration setup. The problem is formulated as a combination of differentiable surrogate losses and Lagrangians, where the efficiency loss depends on the coverage constraints. The formulation is motivated by the observation that the generalization error can be bounded by a set-based predictor. The authors propose a novel approach, called constrained ERM, to reduce the variance of the set-function classes and improve efficiency. The experiments show that the proposed method outperforms the baselines."
1542,SP:ab0d024d4060235df45182dab584c36db16d8e31,"formulation USED-FOR conformal prediction. guaranteed coverage FEATURE-OF finite calibration set. prediction intervals COMPARE split conformal prediction. split conformal prediction COMPARE prediction intervals. multi - output regression problems CONJUNCTION multi - class classification problem. multi - class classification problem CONJUNCTION multi - output regression problems. regression datasets CONJUNCTION multi - output regression problems. multi - output regression problems CONJUNCTION regression datasets. Task is constrained optimization problem. OtherScientificTerm are coverage constraint, and coverage bounds. Metric is coverage. ","This paper proposes a new formulation for conformal prediction, where the goal is to achieve guaranteed coverage in a finite calibration set. This is a constrained optimization problem, and the authors propose to relax the coverage constraint in order to achieve better coverage. The authors provide coverage bounds for a variety of regression datasets, multi-output regression problems, and a multi-class classification problem. They also show that the prediction intervals of the proposed split conformal is better than the original split."
1543,SP:f6b88e1fa1a84d82302d960c6a596fc2ba320bf5,method USED-FOR query object localization. It USED-FOR embedding network. embedding distance USED-FOR reward function. RL agent USED-FOR reward. approach COMPARE DDT. DDT COMPARE approach. DDT USED-FOR object location. DDT COMPARE FTA. FTA COMPARE DDT. FTA USED-FOR few - shot object detection. approach COMPARE FTA. FTA COMPARE approach. approach USED-FOR object location. CUB USED-FOR object location. COCO USED-FOR few - shot object detection. OtherScientificTerm is image crop. ,This paper proposes a method for query object localization. It trains an embedding network to predict the location of objects in an image. The reward function is based on the embedding distance between an object and an image crop. An RL agent is trained to maximize the reward. The proposed approach is compared to DDT and FTA for few-shot object detection on COCO and CUB for object location.
1544,SP:f6b88e1fa1a84d82302d960c6a596fc2ba320bf5,"reinforcement learning approach USED-FOR localization in images. reward function CONJUNCTION policy. policy CONJUNCTION reward function. policy USED-FOR reward function. entropy regularisation USED-FOR policy. state features USED-FOR reward function. triplet loss USED-FOR reward function. elements PART-OF method. Material are images, and COCO. ","This paper presents a reinforcement learning approach for localization in images. The method consists of two elements: 1) a reward function that takes as input state features, and 2) a policy that uses entropy regularisation. The reward function is trained using triplet loss. Experiments are conducted on images from COCO."
1545,SP:f6b88e1fa1a84d82302d960c6a596fc2ba320bf5,metric learning method USED-FOR RL formulation of query object localization. data augmentations CONJUNCTION loss formulation. loss formulation CONJUNCTION data augmentations. IoU orderings FEATURE-OF loss formulation. loss formulation USED-FOR contrastive learning formulation of ordinal embeddings. data augmentations USED-FOR contrastive learning formulation of ordinal embeddings. contrastive learning formulation of ordinal embeddings USED-FOR metric learning method. embedding metric COMPARE baseline metric ( IoU ). baseline metric ( IoU ) COMPARE embedding metric. OtherScientificTerm is image categories. ,This paper proposes a metric learning method for RL formulation of query object localization based on a contrastive learning formulation of ordinal embeddings based on data augmentations and a loss formulation with IoU orderings. The proposed embedding metric is shown to outperform the baseline metric (IoU) on a variety of image categories.
1546,SP:f6b88e1fa1a84d82302d960c6a596fc2ba320bf5,"reward signal USED-FOR rank - preserving metric space. CUB CONJUNCTION COCO. COCO CONJUNCTION CUB. multi - MNIST data CONJUNCTION CUB. CUB CONJUNCTION multi - MNIST data. They USED-FOR generalisation setups. Task are RL - based object localization, and localization. OtherScientificTerm are hardcoded finite set of classes, IoU, distances, policy, and MNIST case. ","This paper studies the problem of RL-based object localization, where the goal is to find objects in a hardcoded finite set of classes. To this end, the authors propose to learn a rank-preserving metric space using a reward signal. The IoU is defined as the sum of distances between a set of objects and a policy that aims to find the closest object to a given class. The authors perform experiments on multi-MNIST data, CUB and COCO, and show that the proposed localization results are competitive in the MNIST case. They are also able to generalise to other generalisation setups."
1547,SP:0b23c5683b72dac05a7436cf3b49bd76263801d9,attention mechanism USED-FOR vision transformers. quadtree attention HYPONYM-OF attention mechanism. quadtree attention mechanism USED-FOR attention. token pyramids USED-FOR quadtree attention mechanism. finer tokens USED-FOR attention computation. classification CONJUNCTION object detection. object detection CONJUNCTION classification. stereo matching CONJUNCTION classification. classification CONJUNCTION stereo matching. feature matching CONJUNCTION stereo matching. stereo matching CONJUNCTION feature matching. quadtree attention USED-FOR vision tasks. object detection HYPONYM-OF vision tasks. feature matching HYPONYM-OF vision tasks. classification HYPONYM-OF vision tasks. stereo matching HYPONYM-OF vision tasks. OtherScientificTerm is coarse attention. ,"This paper proposes a novel attention mechanism for vision transformers, called quadtree attention, which is based on token pyramids. Instead of coarse attention, finer tokens are used for the attention computation. Experiments are conducted on several vision tasks with and without using the proposed Quadtree attention: feature matching, stereo matching, classification, and object detection."
1548,SP:0b23c5683b72dac05a7436cf3b49bd76263801d9,attention approach USED-FOR global or long - range attention. quadratic complexity EVALUATE-FOR attention operation. quadtree structure USED-FOR attention approach. feature matching CONJUNCTION image classification. image classification CONJUNCTION feature matching. image classification CONJUNCTION objection detection. objection detection CONJUNCTION image classification. objection detection HYPONYM-OF tasks. feature matching HYPONYM-OF tasks. image classification HYPONYM-OF tasks. ,"This paper proposes an attention approach based on the quadtree structure to perform global or long-range attention. The authors show that the attention operation has quadratic complexity. Experiments are conducted on three tasks: feature matching, image classification and objection detection."
1549,SP:0b23c5683b72dac05a7436cf3b49bd76263801d9,quadratic computational complexity EVALUATE-FOR vanilla Transformers. coarse - to - fine manner USED-FOR attention. variants USED-FOR message aggregation. QuadTree - A CONJUNCTION QuadTree - B. QuadTree - B CONJUNCTION QuadTree - A. QuadTree - A USED-FOR message aggregation. QuadTree - B USED-FOR message aggregation. attention paradigm USED-FOR quadtree structure. QuadTree - A HYPONYM-OF variants. QuadTree - B HYPONYM-OF variants. self - attention CONJUNCTION cross attention. cross attention CONJUNCTION self - attention. cross attention HYPONYM-OF computer vision tasks. self - attention HYPONYM-OF computer vision tasks. FLOPs CONJUNCTION model parameters. model parameters CONJUNCTION FLOPs. model parameters USED-FOR approach. FLOPs USED-FOR approach. Method is token pyramids. Metric is computational complexity. ,"This paper aims to reduce the quadratic computational complexity of vanilla Transformers by introducing a coarse-to-fine manner to compute attention. The authors propose two variants for message aggregation: QuadTree-A and QuadTree - B. The attention paradigm is used to learn the quadtree structure, which is then used to generate token pyramids. Experiments are conducted on several computer vision tasks, including self-attention and cross attention. Results show that the proposed approach can reduce the computational complexity by a large margin when the FLOPs and model parameters are small."
1550,SP:0b23c5683b72dac05a7436cf3b49bd76263801d9,attention algorithm USED-FOR vision transformers. quadtree USED-FOR vision transformers. quadtree USED-FOR attention algorithm. sparse key - value features PART-OF pyramid level. stereo CONJUNCTION image classification. image classification CONJUNCTION stereo. image classification CONJUNCTION object detection. object detection CONJUNCTION image classification. object detection EVALUATE-FOR method. image classification EVALUATE-FOR method. stereo EVALUATE-FOR method. OtherScientificTerm is feature pyramid. ,"This paper proposes an attention algorithm based on quadtree for vision transformers. The key idea is to replace the sparse key-value features in the pyramid level with a feature pyramid. The proposed method is evaluated on stereo, image classification, and object detection."
1551,SP:63bcbaf0c5644aaba863cf60fa10db763f382ee8,"Generic is objective. OtherScientificTerm are entropy of terminating states, and diverse terminating states. Method is option learning frameworks. ",This paper proposes a new objective that aims to minimize the entropy of terminating states. This is motivated by the observation that diverse terminating states can lead to better performance in option learning frameworks.
1552,SP:63bcbaf0c5644aaba863cf60fa10db763f382ee8,HRL algorithm USED-FOR options. VIC objective USED-FOR options. termination condition HYPONYM-OF options. VIC objective USED-FOR HRL algorithm. reward function USED-FOR options. RL agent USED-FOR downstream tasks. options USED-FOR RL agent. OtherScientificTerm is terminating states. ,"This paper proposes an HRL algorithm that uses the VIC objective to select options (e.g., termination condition) from a reward function. These options are then used to train an RL agent to solve downstream tasks. The paper shows that these options lead to better performance when the terminating states are not available."
1553,SP:63bcbaf0c5644aaba863cf60fa10db763f382ee8,"approach USED-FOR learning diverse temporally extended and reusable options. learning diverse options USED-FOR downstream tasks. Generic are It, and options. Metric is mutual information. OtherScientificTerm is state transitions. Task is downstream task. ",This paper presents an approach for learning diverse temporally extended and reusable options. It is motivated by the observation that learning diverse options for downstream tasks can improve the mutual information between the learned options and the downstream task. The authors propose a way to learn options that can be reused for different state transitions.
1554,SP:63bcbaf0c5644aaba863cf60fa10db763f382ee8,algorithm USED-FOR diversified options. diversified options PART-OF RL. algorithm USED-FOR RL. algorithm USED-FOR termination conditions of options. mutual information USED-FOR algorithm. IMTC algorithm USED-FOR tasks. IMTC algorithm USED-FOR diversified options. Method is infomax Termination Critic ( IMTC ). OtherScientificTerm is state transitions. ,This paper proposes an algorithm called infomax Termination Critic (IMTC) to learn diversified options in RL. The algorithm learns termination conditions of options based on mutual information between the options and the state transitions. The IMTC algorithm is applied to a variety of tasks and is shown to be able to learn diverse options.
1555,SP:79da8f6cacc8386e02bab32154e7eaefbe2c683c,"open world object detection HYPONYM-OF lifelong learning system. semantic topology PART-OF object feature space ( RoI features. contrastive clustering USED-FOR object features. contrastive clustering CONJUNCTION energy based out - of - distribution detection. energy based out - of - distribution detection CONJUNCTION contrastive clustering. one USED-FOR clustering operation. RoI features USED-FOR parallel streams. object category prediction loss FEATURE-OF features. RoI features USED-FOR approach. semantically meaningful object representations USED-FOR Features. Pre - trained word embeddings PART-OF language model ( CLIP ). pre - defined semantic anchors USED-FOR consistent clusters. incremental learning USED-FOR consistent clusters. sequential tasks USED-FOR problem. measures USED-FOR unknown object detection. mAP CONJUNCTION measures. measures CONJUNCTION mAP. measures EVALUATE-FOR Detection. mAP EVALUATE-FOR Detection. Method are object detector, and end - to - end trainable approach. OtherScientificTerm are Unknown objects, and object category. Material is PASCAL - VOC and MSCOCO datasets. Task is Open World Object Detection. ","This paper proposes a lifelong learning system called open world object detection, which is an extension of the object detector. The object feature space (RoI features) consists of the semantic topology of the objects. Unknown objects can be represented by a single object category. This paper proposes an end-to-end trainable approach, which uses contrastive clustering of object features, energy based out-of-distribution detection, and parallel streams based on RoI features, where one is used for the clustering operation and the other is used to train parallel streams. Features are trained with semantically meaningful object representations. Pre-trained word embeddings from a language model (CLIP) are used as pre-defined semantic anchors, and consistent clusters are learned with incremental learning. The problem is formulated as sequential tasks. Detection is evaluated on mAP and other measures for unknown object detection on PASCAL-VOC and MSCOCO datasets."
1556,SP:79da8f6cacc8386e02bab32154e7eaefbe2c683c,"features USED-FOR semantic classes. ORE detector CONJUNCTION large - scale language models. large - scale language models CONJUNCTION ORE detector. textual queries CONJUNCTION images. images CONJUNCTION textual queries. large - scale language models USED-FOR ORE detector. images USED-FOR large - scale language models. textual queries USED-FOR large - scale language models. multi - modal language models USED-FOR detection of novel classes. image data USED-FOR classes. multi - modal language models USED-FOR zero - shot setting. zero - shot setting FEATURE-OF detection of novel classes. language models USED-FOR semantic anchoring. language embedding USED-FOR image embedding alignment. language embedding USED-FOR semantic information. Method are object ORE object detector, and ORE. OtherScientificTerm are contrastive objective, feature space, Densely Sampled Embedding Space, and semantic embedding space. Generic is method. Task are knowledge transfer, and CVPR'17. Material are CVPR’21, ICLR’21, and ECCV’18. ","This paper proposes a novel object ORE object detector. The proposed method is motivated by the idea of knowledge transfer. The main idea is to use a contrastive objective to measure the similarity between objects in the feature space, and then use these features to learn semantic classes. The ORE detector and large-scale language models are trained on textual queries and images. The detection of novel classes is performed in a zero-shot setting with multi-modal language models, which are trained with ORE. The classes are learned on image data. The authors propose a Densely Sampled Embedding Space, where the semantic embedding space is sampled from the language models for semantic anchoring, and the language embedding is used for image embedding alignment. Experiments are conducted on CVPR’21, ICLR’20, and ECCV’18, and CVPR'17."
1557,SP:79da8f6cacc8386e02bab32154e7eaefbe2c683c,"semantic topology embedding USED-FOR Open - World Object Detection ( OWOD ). semantic topology USED-FOR feature space. semantic topology USED-FOR detector. feature space FEATURE-OF detector. semantic topology USED-FOR discriminative and consistent relationships. pre - deﬁned anchors USED-FOR semantic topology. pretrained language model USED-FOR pre - deﬁned anchors. pretrained language model USED-FOR semantic topology. it USED-FOR features. it USED-FOR detector. semantic topology USED-FOR open - world detectors. semantic topology USED-FOR open - world object detectors. discriminative and consistent feature space FEATURE-OF open - world detectors. Method are object detector, and incremental learning. Task is training. ","This paper proposes Open-World Object Detection (OWOD) with semantic topology embedding. The key idea is to train an object detector on a set of open-world objects. The detector is trained with a learned feature space based on the learned semantics of the objects. To train the detector, the authors propose to use pre-deﬁned anchors from a pretrained language model to learn the semantic of the object. The authors also propose to perform incremental learning to improve the performance during training. The experiments show that the learned semantically topology is able to learn discriminative and consistent relationships between objects, and that it can be used to learn new features for the detector. The paper also shows that the proposed semantic topologies can help to learn more discriminatively and consistent feature space for open-World object detectors."
1558,SP:79da8f6cacc8386e02bab32154e7eaefbe2c683c,"method USED-FOR open - world object detection. annotated data USED-FOR categories. out - of - distribution detection CONJUNCTION incremental / continuous learning. incremental / continuous learning CONJUNCTION out - of - distribution detection. incremental / continuous learning HYPONYM-OF tasks. out - of - distribution detection HYPONYM-OF tasks. language model CONJUNCTION randomly generated vectors. randomly generated vectors CONJUNCTION language model. fixed semantic anchors USED-FOR embedding vectors. language model USED-FOR embedding vectors. Generic are model, and problem. OtherScientificTerm is embeddings. Method is feature representation. ","This paper proposes a method for open-world object detection. The proposed model is based on the idea that categories can be learned from annotated data. The paper focuses on two tasks: out-of-distribution detection and incremental/continuous learning. To solve the first problem, the paper proposes to learn embedding vectors with fixed semantic anchors using a language model and randomly generated vectors. These embeddings are then used to train the feature representation."
1559,SP:97f618558f4add834e5930fd177f012a753247dc,"sub - modular score function USED-FOR batch - mode active learning strategies. methods USED-FOR diverse classes. technique USED-FOR sub - modular optimization problem. technique USED-FOR class - balance and boundary - balance constraints. class - balance and boundary - balance constraints USED-FOR sub - modular optimization problem. OtherScientificTerm are decision boundaries, label information, and class and boundary balancing. Material is image data sets. ",This paper proposes a sub-modular score function for batch-mode active learning strategies. The motivation is that existing methods fail to learn diverse classes when the decision boundaries are not well-defined. The authors propose a technique to solve the sub-module optimization problem with class-balance and boundary-balance constraints. The key idea is to use label information as a proxy for class and boundary balancing. Experiments are conducted on image data sets.
1560,SP:97f618558f4add834e5930fd177f012a753247dc,"active learning algorithm USED-FOR classification problems. uncertainty CONJUNCTION diversity. diversity CONJUNCTION uncertainty. selection criteria USED-FOR diversity. selection criteria USED-FOR uncertainty. class - balancing CONJUNCTION boundary - balancing constraints. boundary - balancing constraints CONJUNCTION class - balancing. three - pronged approach USED-FOR diversity. submodularity FEATURE-OF selection criteria. diversity metric CONJUNCTION triplet / clique "" loss "". triplet / clique "" loss "" CONJUNCTION diversity metric. constant approximation guarantee FEATURE-OF optimal selection subset. three - pronged approach USED-FOR greedy algorithm. submodularity FEATURE-OF three - pronged approach. submodularity FEATURE-OF three - pronged approach. constant approximation guarantee FEATURE-OF greedy algorithm. cosine similarity of embeddings USED-FOR diversity metric. diversity metric HYPONYM-OF selection criteria. CIFAR-100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-100. approach COMPARE state - of - the - art methods. state - of - the - art methods COMPARE approach. ImageNet CONJUNCTION CIFAR-100LT ( long - tailed ). CIFAR-100LT ( long - tailed ) CONJUNCTION ImageNet. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. CIFAR-100LT ( long - tailed ) EVALUATE-FOR approach. CIFAR-10 EVALUATE-FOR approach. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. ","This paper proposes an active learning algorithm for classification problems that aims to balance uncertainty and diversity. The authors propose a three-pronged approach to improve diversity by optimizing a greedy algorithm with respect to submodularity of two selection criteria: a diversity metric (based on cosine similarity of embeddings) and a triplet/clique ""loss"". The authors provide a constant approximation guarantee for the optimal selection subset under constant approximation guarantees. The proposed approach is evaluated on CIFAR-10 and ImageNet as well as CifAR-100, ImageNet (long-tailed) and CIFR-100LT (long tailed). The approach outperforms the state-of-the-art methods."
1561,SP:97f618558f4add834e5930fd177f012a753247dc,method USED-FOR batch active learning in deep neural networks. diversity CONJUNCTION triplet objectives. triplet objectives CONJUNCTION diversity. classes CONJUNCTION decision boundaries. decision boundaries CONJUNCTION classes. balancing constraints FEATURE-OF decision boundaries. balancing constraints FEATURE-OF classes. diversity FEATURE-OF submodular set function. triplet objectives FEATURE-OF submodular set function. balancing constraints USED-FOR submodular set function. greedy algorithm USED-FOR optimization problem. approach COMPARE baselines. baselines COMPARE approach. CIFAR10 CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION CIFAR10. CIFAR100 CONJUNCTION ImageNet and CIFAR100 - LT datasets. ImageNet and CIFAR100 - LT datasets CONJUNCTION CIFAR100. CIFAR10 EVALUATE-FOR approach. ImageNet and CIFAR100 - LT datasets EVALUATE-FOR approach. Material is CIFAR100 - LT ). ,"This paper proposes a method for batch active learning in deep neural networks. The main idea is to learn a submodular set function with diversity and triplet objectives under balancing constraints on the classes and decision boundaries. The optimization problem is formulated as a greedy algorithm, and the proposed approach is evaluated on CIFAR10,CIFAR100,ImageNet and CifAR100-LT datasets. The proposed approach outperforms the baselines."
1562,SP:97f618558f4add834e5930fd177f012a753247dc,diversity objective and balancing constraints FEATURE-OF subset selection method. submodular function USED-FOR sample selection criterion. approximation guarantees FEATURE-OF greedy algorithm. method COMPARE k - center. k - center COMPARE method. image classification datasets EVALUATE-FOR method. ,This paper proposes a subset selection method with diversity objective and balancing constraints. The sample selection criterion is based on a submodular function. The authors provide approximation guarantees for the greedy algorithm. The proposed method is evaluated on image classification datasets and compared to k-center.
1563,SP:e0432ff922708c6c6e59124d27c1386605930346,method USED-FOR generalization of segmentation. synthetic data USED-FOR generalization of segmentation. BDD CONJUNCTION IDD. IDD CONJUNCTION BDD. GTA CONJUNCTION SYNTHIA. SYNTHIA CONJUNCTION GTA. Instance - adaptive Batch Normalization ( IaBN ) USED-FOR model. synthetic source domain data USED-FOR model. SYNTHIA HYPONYM-OF synthetic source domain data. GTA HYPONYM-OF synthetic source domain data. model USED-FOR augmented images. pseudo GT mask PART-OF TT - SEG. model USED-FOR pseudo GT mask. augmented images USED-FOR pseudo GT mask. pseudo GT mask USED-FOR network. IaBN CONJUNCTION TT - SEG. TT - SEG CONJUNCTION IaBN. method COMPARE baselines. baselines COMPARE method. method COMPARE method. method COMPARE method. TT - SEG USED-FOR method. IaBN USED-FOR method. Material is real street scene data. ,"This paper proposes a method for improving the generalization of segmentation on synthetic data. The model is based on Instance-adaptive Batch Normalization (IaBN) and is trained on synthetic source domain data (GTA, SYNTHIA). The method is evaluated on real street scene data and compared to BDD, IDD, etc. The method uses IaBN and TT-SEG, where the pseudo GT mask of the network is learned on augmented images. The proposed method outperforms the baselines."
1564,SP:e0432ff922708c6c6e59124d27c1386605930346,"techniques USED-FOR generalization of semantic segmentation networks. training - time statistics FEATURE-OF batch - norm layer. test time augmentations USED-FOR pseudo - labeling. multi - dataset evaluation procedure EVALUATE-FOR techniques. evaluation protocol EVALUATE-FOR techniques. Method are semantic segmentation networks, and batch normalization. Generic is technique. ","This paper proposes two techniques for improving the generalization of semantic segmentation networks. The first technique, batch normalization, aims to improve the training-time statistics of the batch-norm layer. The second technique, pseudo-labeling, is based on test time augmentations. The proposed techniques are evaluated on a multi-dataset evaluation procedure. The evaluation protocol is well-motivated."
1565,SP:e0432ff922708c6c6e59124d27c1386605930346,"techniques USED-FOR test - time adaptation. Generic are first, and second. Method are instance - adaptive bn, and test - time training. ","This paper proposes two techniques for test-time adaptation. The first is an instance-adaptive bn, where the second is an adaptive version of the original bn. The authors show that the proposed methods can be used to improve the performance of test-times training."
1566,SP:e0432ff922708c6c6e59124d27c1386605930346,problem - domain generalization USED-FOR semantic segmentation of urban scenes. instance - adaptive batch normalization CONJUNCTION testing - time training. testing - time training CONJUNCTION instance - adaptive batch normalization. pseudo labels USED-FOR testing - time training. pseudo labels USED-FOR instance - adaptive batch normalization. t - BN CONJUNCTION p - BN. p - BN CONJUNCTION t - BN. pseudo labels USED-FOR test - time training. BDD100k CONJUNCTION IDD. IDD CONJUNCTION BDD100k. GTA5 / SYNTHIA - > Cityscapes CONJUNCTION BDD100k. BDD100k CONJUNCTION GTA5 / SYNTHIA - > Cityscapes. GTA5 / SYNTHIA - > Cityscapes EVALUATE-FOR method. Method is Instance - adaptive batch normalization. OtherScientificTerm is data distribution. ,"This paper addresses the problem of problem-domain generalization for semantic segmentation of urban scenes. Instance-adaptive batch normalization is proposed to reduce the variance of the data distribution. To this end, the authors propose to use pseudo labels during both instance-augmentation and testing-time training. In particular, t-BN and p-BN are used during instance-auxgmentation. Experiments on GTA5/SYNTHIA-> Cityscapes, BDD100k and IDD demonstrate the effectiveness of the proposed method."
1567,SP:427100edad574722a6525ca917e84f817ff60d7e,tabular data USED-FOR anomaly detection algorithm. semi - supervised setting FEATURE-OF contrastive learning. contrastive learning USED-FOR anomaly detection algorithm. masking USED-FOR contrastive learning task. contrastive loss USED-FOR model. contrastive loss USED-FOR anomaly score. task USED-FOR model. task USED-FOR contrastive loss. tabular datasets EVALUATE-FOR it. method COMPARE it. it COMPARE method. tabular datasets EVALUATE-FOR method. Material is normal data. OtherScientificTerm is features. Task is learning task. ,"This paper proposes an anomaly detection algorithm on tabular data based on contrastive learning in a semi-supervised setting. The main idea is to use masking to make the contrastive task more interpretable. In contrast to normal data, the masking is only applied to features that are relevant to the learning task. The authors propose to use contrastive loss to estimate the anomaly score for a model trained on this task. They evaluate their method on two tabular datasets and show that it outperforms the existing method."
1568,SP:427100edad574722a6525ca917e84f817ff60d7e,anomaly detection framework USED-FOR tabular data. features PART-OF tabular data. loss USED-FOR anomaly scores. Method is contrastive learning. ,This paper proposes an anomaly detection framework for tabular data. The main idea is to use contrastive learning to identify the features that are most relevant to the anomaly scores. The anomaly scores are derived by minimizing a loss that is proportional to the difference between the anomaly and the true anomaly.
1569,SP:427100edad574722a6525ca917e84f817ff60d7e,"ODDS benchmark EVALUATE-FOR rule. contrastive representation USED-FOR normal and abnormal data. normal and abnormal data PART-OF tabular data. in - window vector CONJUNCTION out - window vector. out - window vector CONJUNCTION in - window vector. it COMPARE baselines. baselines COMPARE it. default hyperparameter rules USED-FOR it. ODDS benchmark HYPONYM-OF tabular data. ODDS benchmark EVALUATE-FOR it. tabular data EVALUATE-FOR it. tabular data EVALUATE-FOR baselines. GOAD CONJUNCTION COPOD. COPOD CONJUNCTION GOAD. DROCC CONJUNCTION GOAD. GOAD CONJUNCTION DROCC. Generic is datasets. Method is contrastive learning methods. OtherScientificTerm are sliding k - sized window, hyperparameter, features, and feature orders. ","This paper proposes a new contrastive representation for normal and abnormal data in tabular data. The proposed rule is evaluated on the ODDS benchmark and compared with several baselines on three datasets. The key idea is to use a sliding k-sized window, where the in-window vector and the out- window vector are the same. The hyperparameter is chosen based on the number of samples in the sliding window. The authors show that it outperforms the baselines in the ODD benchmark, and outperforms other contrastive learning methods on DROCC, GOAD, and COPOD. The main contribution of the paper is to propose a new way to represent features, which is based on feature orders. The experiments show that the proposed feature orders are effective. "
1570,SP:427100edad574722a6525ca917e84f817ff60d7e,contrastive learning method USED-FOR unsupervised anomaly detection. general multivariate ( tabular ) data USED-FOR unsupervised anomaly detection. one CONJUNCTION one. one CONJUNCTION one. method USED-FOR encoders. one CONJUNCTION residual d − k features. residual d − k features CONJUNCTION one. one HYPONYM-OF encoders. one HYPONYM-OF encoders. contrastive loss USED-FOR embeddings. correlations CONJUNCTION statistical redundancy. statistical redundancy CONJUNCTION correlations. approach USED-FOR common dependencies. contrastive loss USED-FOR anomalies. statistical redundancy HYPONYM-OF common dependencies. correlations HYPONYM-OF common dependencies. DROCC CONJUNCTION GOAD. GOAD CONJUNCTION DROCC. method COMPARE competing methods. competing methods COMPARE method. GOAD CONJUNCTION COPOD. COPOD CONJUNCTION GOAD. datasets EVALUATE-FOR method. datasets PART-OF ODDS library. COPOD HYPONYM-OF competing methods. DROCC HYPONYM-OF competing methods. tabular data EVALUATE-FOR competing methods. GOAD HYPONYM-OF competing methods. tabular data EVALUATE-FOR method. OtherScientificTerm is features. Material is Arrhythmia. ,"This paper proposes a contrastive learning method for unsupervised anomaly detection on general multivariate (tabular) data. The proposed method learns two encoders: one that uses residual d-k features, and one that only uses the residual d −k features. The key idea of the proposed approach is to remove common dependencies such as correlations and statistical redundancy in the embeddings and to use the contrastive loss to penalize anomalies. The method is evaluated on three datasets from the ODDS library and compared to competing methods (DROCC, GOAD, and COPOD) on tabular data from Arrhythmia."
1571,SP:7782a99e3c41ff523c0c56bfbe399c855a77acf2,method USED-FOR resting - state functional connectivity. major depressive disorder CONJUNCTION schizophrenia. schizophrenia CONJUNCTION major depressive disorder. autism spectrum disorder CONJUNCTION major depressive disorder. major depressive disorder CONJUNCTION autism spectrum disorder. resting - state functional connectivity USED-FOR psychiatry disorders. method USED-FOR psychiatry disorders. schizophrenia HYPONYM-OF psychiatry disorders. major depressive disorder HYPONYM-OF psychiatry disorders. autism spectrum disorder HYPONYM-OF psychiatry disorders. diagnostic label USED-FOR conditional variational auto - encoder. conditional variational auto - encoder USED-FOR method. neuroimaging datasets EVALUATE-FOR method. ,"This paper proposes a method to estimate the resting-state functional connectivity for several psychiatry disorders, including autism spectrum disorder, major depressive disorder, and schizophrenia. The proposed method uses a conditional variational auto-encoder with a diagnostic label. The method is evaluated on several neuroimaging datasets."
1572,SP:7782a99e3c41ff523c0c56bfbe399c855a77acf2,small latent embedding spaces USED-FOR functional brain connectivity. diagnostic information USED-FOR supervision signal. conditional VAE USED-FOR small latent embedding spaces. diagnostic information FEATURE-OF functional brain connectivity. diagnostic information USED-FOR conditional VAE. continuous nosological approach USED-FOR mental illness. approach COMPARE approaches. approaches COMPARE approach. synthetic data COMPARE approaches. approaches COMPARE synthetic data. synthetic data USED-FOR approach. functional connectivity datasets EVALUATE-FOR it. psychiatric diagnoses FEATURE-OF functional connectivity datasets. cost function USED-FOR embedding space. ASD CONJUNCTION SCZ. SCZ CONJUNCTION ASD. MDD CONJUNCTION ASD. ASD CONJUNCTION MDD. Task is computational psychiatry. ,"This paper proposes a conditional VAE that learns small latent embedding spaces to model functional brain connectivity using diagnostic information as a supervision signal. The authors take a continuous nosological approach to mental illness, which is an important research direction in computational psychiatry. The proposed approach is evaluated on synthetic data and compared to other approaches. In particular, it is shown to perform well on functional connectivity datasets with psychiatric diagnoses. In addition, the authors propose a cost function for the embedding space. The experiments are conducted on MDD, ASD, and SCZ."
1573,SP:7782a99e3c41ff523c0c56bfbe399c855a77acf2,conditional variational autoencoder ( VAE ) approach USED-FOR low dimensional embedding of neuropsychiatric disorders. resting - state functional connectivity data USED-FOR low dimensional embedding of neuropsychiatric disorders. diagnostic information USED-FOR approach. contrastive learning USED-FOR clusters. method COMPARE dimension reduction approaches. dimension reduction approaches COMPARE method. synthetic dataset CONJUNCTION real datasets. real datasets CONJUNCTION synthetic dataset. synthetic dataset EVALUATE-FOR dimension reduction approaches. real datasets EVALUATE-FOR dimension reduction approaches. real datasets EVALUATE-FOR method. synthetic dataset EVALUATE-FOR method. Method is conditional VAE model. ,This paper proposes a conditional variational autoencoder (VAE) approach for learning a low dimensional embedding of neuropsychiatric disorders from resting-state functional connectivity data. The proposed approach leverages diagnostic information to learn a conditional VAE model. The key idea is to learn clusters using contrastive learning. The method is compared with other dimension reduction approaches on a synthetic dataset and two real datasets.
1574,SP:7782a99e3c41ff523c0c56bfbe399c855a77acf2,"variational autoencoder USED-FOR latent nosological relationships. functional connectivity ( FC ) features USED-FOR latent nosological relationships. functional connectivity ( FC ) features USED-FOR variational autoencoder. categorical and discrete diagnostic labels USED-FOR supervision. continuous dimensional characterization COMPARE categorical and discrete diagnostic labels. categorical and discrete diagnostic labels COMPARE continuous dimensional characterization. continuous dimensional characterization USED-FOR nosology of complex disorders. Major Depressive Disorder ( MDD ) CONJUNCTION Schizophrenia ( SCZ ). Schizophrenia ( SCZ ) CONJUNCTION Major Depressive Disorder ( MDD ). synthetic data CONJUNCTION clinical rs - fMRI datasets. clinical rs - fMRI datasets CONJUNCTION synthetic data. Autism Spectrum Disorder ( ASD ) CONJUNCTION Major Depressive Disorder ( MDD ). Major Depressive Disorder ( MDD ) CONJUNCTION Autism Spectrum Disorder ( ASD ). Schizophrenia ( SCZ ) FEATURE-OF clinical rs - fMRI datasets. Major Depressive Disorder ( MDD ) HYPONYM-OF clinical rs - fMRI datasets. clinical rs - fMRI datasets EVALUATE-FOR framework. synthetic data EVALUATE-FOR framework. latent representations USED-FOR pairwise nosological relationships. model USED-FOR dimensional characterization of multiple brain disorders. Method is autoencoder. OtherScientificTerm are high - dimensional FC space, low - dimensional embedding space, and diagnosis labels. ","This paper proposes a variational autoencoder that uses functional connectivity (FC) features to model latent nosological relationships. The authors argue that the continuous dimensional characterization of the nosology of complex disorders is more useful than categorical and discrete diagnostic labels that provide supervision. The proposed framework is evaluated on both synthetic data and clinical rs-fMRI datasets for Autism Spectrum Disorder (ASD), Major Depressive Disorder (MDD), and Schizophrenia (SCZ). The authors show that the proposed model is able to provide dimensional characterization for multiple brain disorders. They also demonstrate that the learned latent representations can be used to model pairwise nonsological relationships between the input and the output of the autencoder. Finally, the authors demonstrate that a high-dimensional FC space can be transformed into a low-dimensional embedding space, which is then used to represent the diagnosis labels."
1575,SP:b3feb15b01e519e5b2e28b1c4a144056c493e2bc,variational quantum circuit USED-FOR quantum circuit. quantum circuit USED-FOR quantum state. it USED-FOR quantum state. quantum embedding circuit USED-FOR quantum state. classical data USED-FOR quantum embedding circuit. parts PART-OF Quantum neural networks. quantum embedding circuit HYPONYM-OF parts. variational quantum circuit HYPONYM-OF parts. quantum embedding circuit PART-OF Quantum neural networks. tensor - train network COMPARE dense neural network. dense neural network COMPARE tensor - train network. parameterized angle PART-OF quantum embedding circuit. prediction accuracy EVALUATE-FOR MNIST. Metric is accuracy. ,"This paper proposes two parts of Quantum neural networks: a quantum embedding circuit and a variational quantum circuit to learn a quantum circuit that predicts the quantum state from classical data, and then uses it to predict the next quantum state. Theoretical results show that a tensor-train network with a parameterized angle is more accurate than a dense neural network, and that the accuracy is not dependent on the number of parameters. Experiments on MNIST show that the prediction accuracy is improved by a factor of 3."
1576,SP:b3feb15b01e519e5b2e28b1c4a144056c493e2bc,quantum computer USED-FOR qubits. QTT USED-FOR dimensional reduction. quantum neural network ( VQC ) USED-FOR end - to - end learning model. quantum encoding TPE CONJUNCTION quantum neural network ( VQC ). quantum neural network ( VQC ) CONJUNCTION quantum encoding TPE. QTT CONJUNCTION quantum encoding TPE. quantum encoding TPE CONJUNCTION QTT. dimensional reduction CONJUNCTION quantum encoding TPE. quantum encoding TPE CONJUNCTION dimensional reduction. Method is quantum neural networks. ,"This paper proposes a quantum computer for learning qubits. The authors use QTT for dimensional reduction, quantum encoding TPE and quantum neural network (VQC) as an end-to-end learning model. The paper is well-written and well-motivated. The main contribution of this paper is to provide a theoretical analysis of the properties of quantum neural networks. The theoretical analysis is well supported by the experimental results. The experiments are well-conducted and demonstrate the effectiveness of the proposed QTT, QTT with dimensional reduction as well as the combination of QTT (with dimensional reduction and quantum encoding) with quantum encodingTPE."
1577,SP:b3feb15b01e519e5b2e28b1c4a144056c493e2bc,"quantum states USED-FOR Variational Quantum Circuit ( VQC ). classical input data USED-FOR quantum states. quantum computers USED-FOR machine learning classifier. images HYPONYM-OF classical input data. qbits FEATURE-OF quantum computers. Tensor Train Network ( TTN ) model HYPONYM-OF compression technique. Tensor Train Network ( TTN ) model USED-FOR data compression. dense layer CONJUNCTION PCA - based dimension reduction techniques. PCA - based dimension reduction techniques CONJUNCTION dense layer. TTN compression COMPARE PCA - based dimension reduction techniques. PCA - based dimension reduction techniques COMPARE TTN compression. TTN compression COMPARE dense layer. dense layer COMPARE TTN compression. Task is dimensionally reduction of the input datasets. Material is MNIST dataset. OtherScientificTerm are quantum circuits, and noisy and noiseless scenarios. ","This paper proposes a Variational Quantum Circuit (VQC) that uses quantum states extracted from classical input data (e.g. images) to train a machine learning classifier. The authors propose a compression technique called Tensor Train Network (TTN) model, which is an extension of the recently proposed compression technique for dimensionally reduction of the input datasets. The experiments on MNIST dataset show that the proposed TTN compression outperforms the dense layer and other PCA-based dimension reduction techniques in both noisy and noiseless scenarios."
1578,SP:b3feb15b01e519e5b2e28b1c4a144056c493e2bc,end - to - end learning framework USED-FOR quantum neural networks. quantum tensor network USED-FOR quantum embeddings generation. dimension reduction CONJUNCTION quantum embeddings generation. quantum embeddings generation CONJUNCTION dimension reduction. quantum tensor network USED-FOR dimension reduction. it USED-FOR real - world applications. NISQ computers USED-FOR qubits. tensor train network ( TTN ) USED-FOR dimension reduction. dense layer USED-FOR dimension reduction. tensor train network ( TTN ) USED-FOR dense layer. quantum computer USED-FOR end - to - end training process. ,This paper proposes an end-to-end learning framework for quantum neural networks. The main idea is to use a quantum tensor network for dimension reduction and quantum embeddings generation. The authors propose to use NISQ computers to train the qubits and use a tensor train network (TTN) for the dimension reduction. The paper also proposes to use the quantum computer for the end-of-end training process and show that it can be applied to real-world applications.
1579,SP:c4b03a1b477ac94438d63beb29ef86d77acf1b1e,"embedding USED-FOR networks. networks CONJUNCTION meta - model. meta - model CONJUNCTION networks. embedding USED-FOR meta - model. neural networks USED-FOR algorithm. embeddings USED-FOR models similarity. OtherScientificTerm are network, and hidden states. Generic is models. ",This paper proposes an algorithm that uses neural networks to approximate the similarity between networks and a meta-model using an embedding. The embeddings are used to measure the models similarity between different layers of the network. The authors show that the embedding of a network can be used as a proxy for the similarity of the hidden states of the two models.
1580,SP:c4b03a1b477ac94438d63beb29ef86d77acf1b1e,"model USED-FOR neural networks. pretrained ) neural networks PART-OF model. networks USED-FOR task. encoding USED-FOR model. encoding USED-FOR meta - model. decoders USED-FOR auxiliary computational pathways. meta - model USED-FOR model - dependent latent features. networks USED-FOR meta - model. model USED-FOR interpolation or extrapolation. Generic are approach, and network. OtherScientificTerm are output and hidden states, sampled training sets, and model encoding. ","This paper proposes a model for training neural networks. The proposed model consists of (pretrained) neural networks that are trained to solve a given task. The approach consists of two steps: (1) a meta-model that learns an encoding for the model, and (2) decoders that are used as auxiliary computational pathways. The networks are trained on the same task, and the output and hidden states of the network are sampled from the same sampled training sets. The model encoding is used to learn model-dependent latent features, which are then used by the networks to perform interpolation or extrapolation."
1581,SP:c4b03a1b477ac94438d63beb29ef86d77acf1b1e,approach USED-FOR population of trained neural networks. meta - model USED-FOR model. clustering CONJUNCTION semi - supervised learning. semi - supervised learning CONJUNCTION clustering. framework USED-FOR applications. semi - supervised learning HYPONYM-OF applications. clustering HYPONYM-OF applications. ,This paper proposes an approach to learn a population of trained neural networks. The model is trained as a meta-model. The framework is applied to two applications: clustering and semi-supervised learning.
1582,SP:c4b03a1b477ac94438d63beb29ef86d77acf1b1e,neural networks USED-FOR task. global features FEATURE-OF networks. theta - space FEATURE-OF unsupervised manifold learning. theta FEATURE-OF network. unsupervised manifold learning USED-FOR networks. unsupervised manifold learning USED-FOR global features. Method is meta - model. OtherScientificTerm is network - specific ( theta ). ,This paper studies the problem of training neural networks for a new task. The authors propose to learn global features of the networks using unsupervised manifold learning in theta-space. The main idea is to train a meta-model that learns the global features (theta) of the network and the network-specific (ta). 
1583,SP:29a42fdae15b9da955513f71e3100ebd0146a28a,constraint function USED-FOR dynamics. explicit / human - defined constraints PART-OF learning - based simulation frameworks. gradient descent USED-FOR implicit constraint function. graph neural networks ( GNNs ) USED-FOR framework. implicit constraint function USED-FOR constraint. bouncing balls CONJUNCTION bouncing rigids. bouncing rigids CONJUNCTION bouncing balls. bouncing rigids CONJUNCTION BoxBath. BoxBath CONJUNCTION bouncing rigids. rope CONJUNCTION bouncing balls. bouncing balls CONJUNCTION rope. physical simulation environments EVALUATE-FOR method. rope HYPONYM-OF physical simulation environments. bouncing balls HYPONYM-OF physical simulation environments. bouncing rigids HYPONYM-OF physical simulation environments. BoxBath HYPONYM-OF physical simulation environments. C - GNS USED-FOR hand - designed constraints. accuracy EVALUATE-FOR C - GNS. solver iterations USED-FOR C - GNS. Method is constraint solver. ,"This paper proposes a new constraint function for modeling dynamics in learning-based simulation frameworks without explicit/human-defined constraints. The proposed framework is based on graph neural networks (GNNs) and uses gradient descent to approximate the implicit constraint function. The authors evaluate the proposed method on three physical simulation environments: a rope, bouncing balls, and bouncing rigids, as well as BoxBath. The experiments show that C-GNS can learn hand-designed constraints with fewer solver iterations, while maintaining high accuracy. The paper also provides a theoretical analysis of the constraint solver."
1584,SP:29a42fdae15b9da955513f71e3100ebd0146a28a,neural - network simulator USED-FOR constraints. graph neural network USED-FOR constraint solver. simulation environments EVALUATE-FOR simulator. graph neural network USED-FOR simulator. graph networks USED-FOR constraint - based simulation. Task is physics - based simulation. ,"This paper proposes a neural-network simulator that learns to solve constraints. The simulator uses a graph neural network as a constraint solver, and is evaluated on several simulation environments. The paper is well-motivated and well-written. The use of graph networks in constraint-based simulation is an interesting and important direction in physics -based simulation."
1585,SP:29a42fdae15b9da955513f71e3100ebd0146a28a,"constraint - based approach COMPARE direct prediction. direct prediction COMPARE constraint - based approach. constraint - based approach USED-FOR physics. dynamics USED-FOR GNN. constraint function USED-FOR gradient. colliding irregular shapes CONJUNCTION splashing fluids. splashing fluids CONJUNCTION colliding irregular shapes. bouncing balls CONJUNCTION colliding irregular shapes. colliding irregular shapes CONJUNCTION bouncing balls. simulated ropes CONJUNCTION bouncing balls. bouncing balls CONJUNCTION simulated ropes. colliding irregular shapes HYPONYM-OF physical domains. splashing fluids HYPONYM-OF physical domains. bouncing balls HYPONYM-OF physical domains. simulated ropes HYPONYM-OF physical domains. Generic is system. OtherScientificTerm are constraint satisfaction scalar, and solver iterations. ","This paper proposes a constraint-based approach to physics that is more efficient than direct prediction. The system is based on the idea that the dynamics of a GNN can be expressed as a function of the constraint function, which is used to compute the gradient of the solution. The constraint satisfaction scalar is then used to estimate the number of solver iterations needed to solve the problem. Experiments are conducted on several physical domains, including simulated ropes, bouncing balls, colliding irregular shapes, and splashing fluids."
1586,SP:29a42fdae15b9da955513f71e3100ebd0146a28a,numerical solutions USED-FOR Lagrangian physical simulation. constraint - based inference method USED-FOR numerical solutions. graph neural networks USED-FOR constraint - based inference method. iterative update USED-FOR inference. method USED-FOR test - time dynamical correction. iterative update PART-OF method. scalar predictor USED-FOR constraint. graph neural networks USED-FOR variable length of the physical domain. ablation studies USED-FOR hyper - parameters. ,"This paper proposes a constraint-based inference method for learning numerical solutions for Lagrangian physical simulation using graph neural networks. The proposed method consists of an iterative update for inference and a test-time dynamical correction. The constraint is modeled as a scalar predictor, which is trained to predict the variable length of the physical domain. The hyper-parameters are learned using ablation studies."
1587,SP:db07c2c0afdf27692dc504c9c54387c20211d469,"approach USED-FOR multi - objective optimisation. EDO - CS USED-FOR multi - objective optimisation. selection mechanism USED-FOR ES optimisation. clusters USED-FOR sampling policies. objective function USED-FOR ES algorithm. hyperparameter PART-OF it. fitness and behaviour - diversity terms PART-OF it. multi - arm bandit approach USED-FOR hyperparameter. MuJoCo continuous control tasks EVALUATE-FOR QD benchmarks. approach COMPARE QD benchmarks. QD benchmarks COMPARE approach. MuJoCo continuous control tasks EVALUATE-FOR approach. OtherScientificTerm are RL policies, and Pareto front. Metric is quality. ","This paper proposes an approach to multi-objective optimisation based on EDO-CS. The main idea is to use a selection mechanism for ES optimisation, where RL policies are sampled from a set of clusters, and then the clusters are used for sampling policies. The ES algorithm is trained using an objective function that is a combination of the fitness and behaviour-diversity terms, and it also includes a hyperparameter that is learned using a multi-arm bandit approach. The proposed approach is evaluated on MuJoCo continuous control tasks, and compared to standard QD benchmarks. The results show that the proposed approach achieves better performance on the Pareto front, while maintaining high quality."
1588,SP:db07c2c0afdf27692dc504c9c54387c20211d469,method USED-FOR robust policies. robust policies PART-OF reinforcement learning. diversity of behaviours USED-FOR policies. evolutionary algorithms USED-FOR policy choice. clustering algorithm USED-FOR policies. Generic is algorithm. Metric is convergence rates. ,This paper proposes a method for learning robust policies in reinforcement learning. The key idea is to use diversity of behaviours to learn policies that are robust to perturbations. The algorithm is based on evolutionary algorithms for policy choice. The authors propose a clustering algorithm to find policies that have high convergence rates.
1589,SP:db07c2c0afdf27692dc504c9c54387c20211d469,selection mechanism USED-FOR Quality - Diversity based algorithms. Clustering - Based Selection USED-FOR Reinforcement Learning. Evolutionary Diversity Optimization CONJUNCTION Clustering - Based Selection. Clustering - Based Selection CONJUNCTION Evolutionary Diversity Optimization. Evolutionary Diversity Optimization USED-FOR Reinforcement Learning. selection mechanism USED-FOR policies. K - Means algorithm USED-FOR selection mechanism. linear combination USED-FOR policy updates. reward CONJUNCTION novelty score. novelty score CONJUNCTION reward. novelty score USED-FOR policy updates. reward FEATURE-OF linear combination. novelty score FEATURE-OF linear combination. OtherScientificTerm is behaviour space. Method is bandit algorithm. ,"This paper proposes a selection mechanism for Quality-Diversity based algorithms. The authors propose a combination of Evolutionary Diversity Optimization and Clustering-Based Selection for Reinforcement Learning. The selection mechanism is based on the K-Means algorithm, where the goal is to find policies that maximize the reward and minimize the novelty of the learnt behaviour space. The paper also proposes a bandit algorithm to find the optimal combination of reward and novelty score for policy updates."
1590,SP:db07c2c0afdf27692dc504c9c54387c20211d469,evolutionary optimization approaches USED-FOR diversity / novelty - seeking agents. raw environment reward CONJUNCTION diversity. diversity CONJUNCTION raw environment reward. regularizer USED-FOR exploration. diversity USED-FOR regularizer. cluster USED-FOR policy. distance metric USED-FOR K - means. behavior function USED-FOR distance metric. diversity metric USED-FOR regularized reward. regularized reward USED-FOR ES. diversity metric USED-FOR ES. SOTA EVALUATE-FOR EDO - CS. Generic is approach. OtherScientificTerm is policies. Material is continuous control benchmarks. ,"This paper presents evolutionary optimization approaches for diversity/novelty-seeking agents. The approach is based on the observation that ES can benefit from both raw environment reward and diversity. The authors propose to use diversity as a regularizer to encourage exploration. To do so, they use a distance metric between the K-means of a policy and a cluster of policies. The distance metric is modeled as a behavior function. They show that using a diversity metric as regularized reward improves ES performance on continuous control benchmarks. They also show that EDO-CS improves SOTA."
1591,SP:e51123a76713f1a1031d252e092985bd9b298fdf,"push - sum type algorithm HYPONYM-OF decentralized algorithms. row - stochastic mixing matrices USED-FOR consensus - based linear stochastic approximation. consensus - based linear stochastic approximation HYPONYM-OF decentralized algorithms. column - stochastic mixing matrices USED-FOR push - sum type algorithm. Task is distributed linear stochastic approximation. OtherScientificTerm are time - varying directed communication networks, and ODE. Generic is algorithm. ","This paper studies distributed linear stochastic approximation in the setting of time-varying directed communication networks. The authors propose two decentralized algorithms: (1) a push-sum type algorithm with column-stochastic mixing matrices, and (2) a consensus-based linear Stochastic Markov Decision Process (VDDP) algorithm with row-Stochastic Mixing matrices. The main contribution of the paper is the theoretical analysis of the ODE of the proposed algorithm."
1592,SP:e51123a76713f1a1031d252e092985bd9b298fdf,"stochastic matrix COMPARE doubly stochastic matrix. doubly stochastic matrix COMPARE stochastic matrix. Markovian noise USED-FOR distributed stochastic approximation. stochastic matrix USED-FOR communication topologies. analysis USED-FOR consensus - type algorithm. stochastic matrices USED-FOR consensus - type algorithm. analysis USED-FOR finite - time bounds. Metric is asymptotic and finite - time error bounds. Generic is algorithm. Method are push - type distributed stochastic approximation algorithm, and distributed TD algorithms. ","This paper studies distributed stochastic approximation with Markovian noise, and provides asymptotic and finite-time error bounds. The main contribution of the paper is the analysis of a consensus-type algorithm that uses stochastics matrices instead of a doubly stochastically matrix, which is more suitable for communication topologies. The algorithm is a variant of a push-type distributed stoalastic approximation algorithm, which has been shown to be computationally efficient. The paper also provides some theoretical results on the convergence of distributed TD algorithms, as well as some finite-times bounds."
1593,SP:e51123a76713f1a1031d252e092985bd9b298fdf,linear stochastic approximation USED-FOR multi - agent setup. Non - asymptotic upper bounds FEATURE-OF error function. mean squared sense FEATURE-OF Non - asymptotic upper bounds. mean squared sense FEATURE-OF error function. Task is multi - agent distributed optimization. OtherScientificTerm is bi - directional communication. ,This paper studies the problem of multi-agent distributed optimization. The authors propose a linear stochastic approximation to the multi-agents setup. Non-asymptotic upper bounds on the error function in the mean squared sense are derived. The main contribution of this paper is to study the problem in the context of bi-directional communication.
1594,SP:e51123a76713f1a1031d252e092985bd9b298fdf,policy iteration USED-FOR reinforcement learning. consensus problem USED-FOR policy iteration. stochastic approximation FEATURE-OF consensus problem. OtherScientificTerm is interaction graph. Generic is algorithm. ,"This paper studies the problem of policy iteration in reinforcement learning. The authors propose to solve the policy iteration as a consensus problem with stochastic approximation, where the interaction graph is a weighted sum of the states of the agents. The algorithm is shown to converge to the optimal solution."
1595,SP:f7f96d545a907887396393aba310974f4d3f75ff,graph neural network approach USED-FOR learning of update rules of constrained systems. Interaction entwork CONJUNCTION EGNN. EGNN CONJUNCTION Interaction entwork. forward and inverse kinematics PART-OF algorithm. generalized coordinates FEATURE-OF updates of constrained components. SE3 - Transformers CONJUNCTION Radial Field. Radial Field CONJUNCTION SE3 - Transformers. TensorForce Networks CONJUNCTION SE3 - Transformers. SE3 - Transformers CONJUNCTION TensorForce Networks. Radial Field CONJUNCTION EGNNs. EGNNs CONJUNCTION Radial Field. problem EVALUATE-FOR method. N - body problem EVALUATE-FOR them. Method is update rules of constrained systems. OtherScientificTerm is constraints. ,"This paper proposes a graph neural network approach for the learning of update rules of constrained systems. The proposed algorithm combines forward and inverse kinematics, which is inspired by Interaction entwork and EGNN. The updates of constrained components are represented as generalized coordinates. Experiments are conducted on TensorForce Networks, SE3-Transformers, Radial Field, and various EGNNs and show that the proposed method is able to solve the problem. The authors also evaluate them on the N-body problem and show better performance under different constraints."
1596,SP:f7f96d545a907887396393aba310974f4d3f75ff,"model USED-FOR augmented interaction networks. equivariant message passing schemes USED-FOR physical realism. mechanical constraints FEATURE-OF augmented interaction networks. hinges FEATURE-OF particles. particles PART-OF toy systems. Generic are systems, and method. Method is generalised equivariant message passing. ","This paper proposes a model for augmented interaction networks with mechanical constraints. The authors argue that equivariant message passing schemes are not suitable for physical realism, and propose a method for learning such systems. The method is based on the idea of generalised equivarian message passing, and is tested on toy systems with different types of hinges and particles."
1597,SP:f7f96d545a907887396393aba310974f4d3f75ff,graph neural network model USED-FOR dynamics of n - body systems. universality properties FEATURE-OF 3D transformation equivariant layer. Generic is model. OtherScientificTerm is rigid - body constraints. ,This paper proposes a graph neural network model for modeling the dynamics of n-body systems. The model is trained to be invariant to rigid-body constraints. The universality properties of the 3D transformation equivariant layer are studied.
1598,SP:f7f96d545a907887396393aba310974f4d3f75ff,Graph Mechanics Networks ( GMN ) HYPONYM-OF E(n ) equivariant model. generalized coordinates USED-FOR exact constraint preservation. cartesian coordinates USED-FOR models. approach USED-FOR E(n ) equivariant function. E(n ) equivariant model USED-FOR exact constraint preservation. OtherScientificTerm is rigid constraints. Generic is model. ,"This paper proposes Graph Mechanics Networks (GNN), an E(n) equivariant model that is able to generalize to rigid constraints. In particular, the authors propose to use generalized coordinates for exact constraint preservation instead of using cartesian coordinates as in previous models. The authors also propose an approach to learn an E-equivariant function that can be used to approximate the exact constraints of the model."
1599,SP:ee0b94238c3fde59cb8b67a687b77984fe7d3454,"statistical heterogeneity FEATURE-OF data shards. data shards USED-FOR collaborative federated learning. memory footprint USED-FOR personalization schema. personalization schema COMPARE shared model. shared model COMPARE personalization schema. parameter mixing USED-FOR interpolation based personalization methods. output averaging USED-FOR classification setting. personalized model USED-FOR shared model. image classification CONJUNCTION next - word prediction. next - word prediction CONJUNCTION image classification. next - word prediction EVALUATE-FOR methods. image classification EVALUATE-FOR methods. non - convex settings FEATURE-OF convergence rates. OtherScientificTerm are catastrophic forgetting, shared and personalized parameters, and partial personalization. Task is regression. Method is FedAlt. ","This paper studies the problem of collaborative federated learning with data shards with statistical heterogeneity due to catastrophic forgetting. The authors propose a personalization schema that reduces the memory footprint while maintaining the same performance as the shared model. This is achieved by interpolation based personalization methods based on parameter mixing, where the shared and personalized parameters are sampled from the same pool. In the classification setting, the authors propose to use output averaging, which is similar to FedAlt, but with partial personalization. Experiments on image classification and next-word prediction are conducted to evaluate the proposed methods on non-convex settings, where regression is performed."
1600,SP:ee0b94238c3fde59cb8b67a687b77984fe7d3454,personalization USED-FOR federated setting. model parameters COMPARE full model. full model COMPARE model parameters. FedSim CONJUNCTION FedAlt. FedAlt CONJUNCTION FedSim. FedAlt USED-FOR personalized model. FedSim USED-FOR shared and personalized parameters. FedSim USED-FOR local client updates. Method is global model. Generic is model. OtherScientificTerm is shared parameter. ,"This paper studies the problem of personalization in the federated setting. In particular, the authors consider the setting where each client has access to a global model, and each client updates its own model. The authors propose FedSim and FedAlt to jointly learn a personalized model, where the model parameters are shared across clients instead of the full model. For local client updates, FedSim learns both shared and personalized parameters, while FedAlt learns a shared parameter for each client."
1601,SP:ee0b94238c3fde59cb8b67a687b77984fe7d3454,partial model personalization USED-FOR personalized FL framework. shared model CONJUNCTION personalized model. personalized model CONJUNCTION shared model. FedSim CONJUNCTION FedAlt. FedAlt CONJUNCTION FedSim. partial clients participation FEATURE-OF optimization algorithm. FedSim HYPONYM-OF optimization algorithm. FedAlt HYPONYM-OF optimization algorithm. convergence rate FEATURE-OF general smooth nonconvex function. algorithms USED-FOR general smooth nonconvex function. convergence rate EVALUATE-FOR algorithms. adapter USED-FOR personalized model. algorithm COMPARE personalization method. personalization method COMPARE algorithm. NLP or vision tasks EVALUATE-FOR algorithm. NLP or vision tasks EVALUATE-FOR personalization method. Generic is It. Method is model split methods. ,"This paper proposes a personalized FL framework based on partial model personalization. It is motivated by the observation that the optimization algorithm with partial clients participation (FedSim, FedAlt) can converge to a general smooth nonconvex function with a high convergence rate. The authors propose a shared model and a personalized model that can be learned using an adapter. The proposed algorithm is evaluated on NLP or vision tasks and compared to a personalization method. The results show that the proposed model split methods can achieve better performance."
1602,SP:ee0b94238c3fde59cb8b67a687b77984fe7d3454,partial personalization objective function USED-FOR personalized models. federated learning USED-FOR personalized models. algorithms USED-FOR nonconvex case. FedSim CONJUNCTION FedAlt. FedAlt CONJUNCTION FedSim. FedAlt USED-FOR nonconvex case. FedSim USED-FOR nonconvex case. FedAlt HYPONYM-OF algorithms. FedSim HYPONYM-OF algorithms. partial personalized models COMPARE fully personalized models. fully personalized models COMPARE partial personalized models. model structures USED-FOR fully personalized models. model structures USED-FOR partial personalized models. OtherScientificTerm is partial personalization. ,"This paper proposes a partial personalization objective function to train personalized models in federated learning. Two algorithms, FedSim and FedAlt, are proposed for the nonconvex case. Experiments show that partial personalized models trained with different model structures perform better than fully personalized models. The paper also provides a theoretical analysis of the effect of different model types on the performance of partial personalized models."
1603,SP:eb54e84275266d8909fcbfe1589da1c4396c3164,framework USED-FOR contrastive learning through time ( CLTT ). positive pairs USED-FOR contrastive learning. augmentation operations USED-FOR positive pairs. near - photorealistic training environment FEATURE-OF datasets. Material is positive data. ,This paper proposes a framework for contrastive learning through time (CLTT). The key idea is to use positive pairs as augmentation operations for the purpose of improving the quality of the contrastive learned from positive data. Experiments are conducted on two datasets with a near-photorealistic training environment.
1604,SP:eb54e84275266d8909fcbfe1589da1c4396c3164,"temporal consistency COMPARE augmentation consistency. augmentation consistency COMPARE temporal consistency. temporal consistency USED-FOR self - supervised learning ( SSL ). RELIC CONJUNCTION BYOL. BYOL CONJUNCTION RELIC. SimCLR CONJUNCTION RELIC. RELIC CONJUNCTION SimCLR. video frames USED-FOR SSL approaches. BYOL HYPONYM-OF SSL approaches. SimCLR HYPONYM-OF SSL approaches. RELIC HYPONYM-OF SSL approaches. OtherScientificTerm are augmentations, and temporal augmentation. Generic is learnt representations. ","This paper studies the problem of self-supervised learning (SSL) with temporal consistency instead of augmentation consistency. In particular, the authors focus on three SSL approaches (SimCLR, RELIC, BYOL) that are trained on video frames. The authors argue that the augmentations are not consistent with the learnt representations, and that temporal augmentation is beneficial."
1605,SP:eb54e84275266d8909fcbfe1589da1c4396c3164,successive images CONJUNCTION temporal dimension. temporal dimension CONJUNCTION successive images. image - based contrastive learning framework COMPARE data argumentation. data argumentation COMPARE image - based contrastive learning framework. successive images USED-FOR image - based contrastive learning framework. successive images USED-FOR data argumentation. method USED-FOR feature representation. method COMPARE supervised learning. supervised learning COMPARE method. supervised learning USED-FOR feature representation. Generic is dataset. ,This paper proposes an image-based contrastive learning framework that is more expressive than data argumentation with successive images and temporal dimension. The proposed method is able to learn feature representation more efficiently than supervised learning. Experiments are conducted on a large dataset.
1606,SP:eb54e84275266d8909fcbfe1589da1c4396c3164,"method USED-FOR contrastive learning. adjacent frames USED-FOR method. augmentation USED-FOR CL approaches. method COMPARE CL approaches. CL approaches COMPARE method. datasets EVALUATE-FOR method. datasets EVALUATE-FOR method. method USED-FOR latent representation similarity. Method are Contrastive Learning Through Time ( CLTT ), and representation learning. Task is biological learning. ","This paper presents Contrastive Learning Through Time (CLTT), a method for contrastive learning based on adjacent frames. The proposed method is evaluated on two datasets and compared to other CL approaches that do not use augmentation. The authors show that the proposed method can learn latent representation similarity, which is important for representation learning in biological learning."
1607,SP:2fb4af247b5022710b681037faca2420207a507a,Hindsight Experience Replay ( HER ) HYPONYM-OF algorithm. HER CONJUNCTION AlphaZero. AlphaZero CONJUNCTION HER. Method is multi - goal reinforcement learning. ,"This paper proposes Hindsight Experience Replay (HER), a new algorithm for multi-goal reinforcement learning. HER is a combination of HER and AlphaZero. "
1608,SP:2fb4af247b5022710b681037faca2420207a507a,"Hindsight Experience Replay ( HER ) USED-FOR training. Hindsight Experience Replay ( HER ) USED-FOR goal - directed planning tasks. Hindsight Experience Replay ( HER ) USED-FOR AlphaZeroHER. target policy COMPARE empirical policy. empirical policy COMPARE target policy. BitFlip CONJUNCTION navigation. navigation CONJUNCTION BitFlip. navigation CONJUNCTION quantum compiling task. quantum compiling task CONJUNCTION navigation. quantum compiling task HYPONYM-OF goal - directed planning tasks. navigation HYPONYM-OF goal - directed planning tasks. BitFlip HYPONYM-OF goal - directed planning tasks. Method are HER procedure, and AlphaZero. Generic is approach. OtherScientificTerm are reward, and HER. ","This paper proposes AlphaZeroHER, which uses Hindsight Experience Replay (HER) for training in goal-directed planning tasks. The HER procedure is based on the idea that the reward should be similar to that of AlphaZero, but the approach is different from AlphaZero in the sense that HER encourages the target policy to perform better than the empirical policy. The authors evaluate HER on three goal-directed planning tasks: BitFlip, navigation, and a quantum compiling task."
1609,SP:2fb4af247b5022710b681037faca2420207a507a,"state value estimates USED-FOR local actions. state value estimates USED-FOR MCTS. approach USED-FOR large problem spaces. RL USED-FOR control policy. Hindsight Experience Replay ( HER ) USED-FOR offline RL algorithms. Hindsight Experience Replay ( HER ) USED-FOR sample efficiency. HER USED-FOR goal directed tasks. AlphaZero USED-FOR goal directed tasks. HER USED-FOR AlphaZero. HER USED-FOR computationally intense tasks. approach COMPARE AlphaZero. AlphaZero COMPARE approach. Quantum compiling environment PART-OF AlphaZero. simulated environments EVALUATE-FOR approach. Go CONJUNCTION Chess. Chess CONJUNCTION Go. Chess HYPONYM-OF games. Go HYPONYM-OF games. computational cost EVALUATE-FOR tree re - weighting. hindsight inferred goals USED-FOR policy and value networks. AlphaZeroHER COMPARE AlphaZero. AlphaZero COMPARE AlphaZeroHER. latter COMPARE former. former COMPARE latter. AlphaZeroHER COMPARE former. former COMPARE AlphaZeroHER. simulated environments EVALUATE-FOR AlphaZeroHER. simulated environments EVALUATE-FOR AlphaZero. Method are Monte Carlo Tree Search ( MCTS ) algorithms, and MCTS algorithms. OtherScientificTerm are state space, goals state, trajectories, positive reward, rewards, success criteria, replay buffer, sub - goals, sparse rewards, policies, and sub - goal rollouts. Task is goal reaching problems. ","This paper proposes a new approach to tackle large problem spaces by leveraging Monte Carlo Tree Search (MCTS) algorithms. In MCTS, state value estimates for local actions are used to train a control policy using RL to find the optimal control policy in the state space. In contrast, Hindsight Experience Replay (HER) is used in offline RL algorithms to improve sample efficiency. HER is applied to goal directed tasks in contrast to AlphaZero, which uses HER to tackle computationally intense tasks. The authors propose to use hindsight inferred goals to train policy and value networks, where the goal state is sampled from a replay buffer, and the trajectories are sampled from the replay buffer. The goal reaching problems are formulated in terms of a positive reward, where rewards are computed based on the success criteria of the current state. In addition, the authors propose a tree re-weighting to reduce the computational cost and to encourage sparse rewards. The proposed approach, AlphaZeroHER, is evaluated on two games: Go and Chess. The Quantum compiling environment in AlphaZero is used to compare the performance of the proposed approach with AlphaZero. In both simulated environments, the results show that in both games, the proposed AlphaZeroher outperforms AlphaZero and AlphaZero with respect to the former, while the latter outperforms the former by a large margin. The main contribution of the paper is the introduction of hindsight inference of goals, which is an important step towards improving the performance and efficiency of MMTS algorithms. The paper also provides a theoretical analysis of the effect of sub-goal rollouts."
1610,SP:2fb4af247b5022710b681037faca2420207a507a,AlphaZero CONJUNCTION Hindsight Experience Replay(HER ). Hindsight Experience Replay(HER ) CONJUNCTION AlphaZero. AlphaZeroHER method USED-FOR AlphaZero agents. Hindsight Experience Replay(HER ) USED-FOR AlphaZero agents. goal - directed environments FEATURE-OF AlphaZero agents. AlphaZero PART-OF AlphaZeroHER method. 2D Navigation Task CONJUNCTION 2D Maze. 2D Maze CONJUNCTION 2D Navigation Task. AlphaZeroHER COMPARE AlphaZero. AlphaZero COMPARE AlphaZeroHER. 2D Maze CONJUNCTION Quantum Compiler environments. Quantum Compiler environments CONJUNCTION 2D Maze. BitFlip CONJUNCTION 2D Navigation Task. 2D Navigation Task CONJUNCTION BitFlip. Quantum Compiler environments EVALUATE-FOR AlphaZeroHER. 2D Navigation Task EVALUATE-FOR AlphaZeroHER. 2D Navigation Task EVALUATE-FOR AlphaZero. BitFlip EVALUATE-FOR AlphaZeroHER. BitFlip EVALUATE-FOR AlphaZero. Method is AlphaZero approach. OtherScientificTerm is goal - directed planning environments. ,"This paper proposes the AlphaZeroHER method, which combines AlphaZero and Hindsight Experience Replay(HER) to train AlphaZero agents in goal-directed environments. AlphaZero consists of two parts: (1) AlphaZero, which is an extension of the original AlphaZero approach, and (2) HER, which replaces AlphaZero with a new version of AlphaZero. The authors show that the proposed method outperforms AlphaZero on BitFlip, 2D Navigation Task and 2D Maze as well as Quantum Compiler environments. "
1611,SP:e2d33c7331db7f52b84ad1018152564d91a9f126,method USED-FOR continual learning. Recursive Gradient Optimization ( RGO ) USED-FOR continual learning. method USED-FOR direction of gradients. benchmarks EVALUATE-FOR algorithm. architectures USED-FOR algorithm. MNIST HYPONYM-OF benchmarks. Method is Feature Encoding Layer. ,"This paper proposes Recursive Gradient Optimization (RGO), a method for continual learning based on the idea of the Feature Encoding Layer. The proposed method learns the direction of gradients based on a fixed number of iterations. The algorithm is evaluated on two benchmarks (MNIST and CIFAR-10) with different architectures."
1612,SP:e2d33c7331db7f52b84ad1018152564d91a9f126,"losses USED-FOR forgetting. Taylor series expression of forgetting CONJUNCTION Recursive Least Loss ( RLL ) formulation. Recursive Least Loss ( RLL ) formulation CONJUNCTION Taylor series expression of forgetting. gradient update of normal SGD USED-FOR forgetting. modification USED-FOR gradient update of normal SGD. positive definite matrix USED-FOR RLL. optimization problem USED-FOR positive definite matrix. optimization problem USED-FOR optimization problem. positive definite matrix USED-FOR modification. RLL CONJUNCTION positive definite matrix. positive definite matrix CONJUNCTION RLL. task - specific random rearrangement USED-FOR feature maps. task - specific random rearrangement USED-FOR virtual feature encoding layer ( FEL ). single task learning USED-FOR positive transfer. benchmark datasets EVALUATE-FOR sota approaches. Method are deep neural networks, and FEL. OtherScientificTerm are continual learning, and memory footprint. Generic are network, and approach. Task is learning. Material is rotated MNIST. ","This paper proposes a modification to the standard Taylor series expression of forgetting and the Recursive Least Loss (RLL) formulation to address the problem of forgetting in deep neural networks. Specifically, the authors propose a modification of the gradient update of normal SGD to minimize forgetting using a positive definite matrix, which is formulated as an optimization problem. The authors then propose a virtual feature encoding layer (FEL) that uses a task-specific random rearrangement of the feature maps of the original network, which they call ""positive transfer"". They show that the proposed FEL is able to transfer the information from the original RLL to the positive definite matrices without continual learning. They also show that single task learning is sufficient for positive transfer and that the memory footprint of the proposed approach is small. They evaluate their sota approaches on a variety of benchmark datasets, including rotated MNIST."
1613,SP:e2d33c7331db7f52b84ad1018152564d91a9f126,recursive gradient optimization USED-FOR continual learning approach. projection matrix USED-FOR gradient modification. Generic is matrix. OtherScientificTerm is Hessian. ,This paper proposes a continual learning approach based on recursive gradient optimization. The main idea is to learn a projection matrix for gradient modification. This matrix is then used to estimate the Hessian.
1614,SP:e2d33c7331db7f52b84ad1018152564d91a9f126,"gradient - based approach USED-FOR continual learning in neural networks. Recursive Gradient Optimization ( RGO ) HYPONYM-OF gradient - based approach. gradient direction USED-FOR RGO. recursive least loss USED-FOR continual learning loss. random task - specific permutations USED-FOR current - task - first ” principle. RGO COMPARE methods. methods COMPARE RGO. continual learning image classification benchmarks EVALUATE-FOR RGO. RGO COMPARE baseline. baseline COMPARE RGO. continual learning image classification benchmarks EVALUATE-FOR methods. methods COMPARE baseline. baseline COMPARE methods. baseline USED-FOR model. OtherScientificTerm is projection matrix P. Generic are network, and method. Method is hyperparameter tuning. ","This paper proposes Recursive Gradient Optimization (RGO), a gradient-based approach for continual learning in neural networks. RGO is based on the gradient direction of the projection matrix P, and the continual learning loss is a recursive least loss. The authors propose to use random task-specific permutations to enforce the “current-task-first” principle. The proposed method is evaluated on continual learning image classification benchmarks, where RGO outperforms existing methods and a baseline. In addition, the authors show that the proposed model can generalize better than the baseline without hyperparameter tuning."
1615,SP:511226b467019dcd85e9ebf8b9b56f8f1b3ef889,parent StyleGAN model CONJUNCTION finetuned child model. finetuned child model CONJUNCTION parent StyleGAN model. semantic alignment USED-FOR parent StyleGAN model. finetuned child model USED-FOR semantic alignment. dataset EVALUATE-FOR semantic alignment. dataset EVALUATE-FOR finetuned child model. models USED-FOR semantical alignment. image morphing CONJUNCTION zero - shot image editing. zero - shot image editing CONJUNCTION image morphing. zero - shot image editing CONJUNCTION attribute classification. attribute classification CONJUNCTION zero - shot image editing. image translation CONJUNCTION image morphing. image morphing CONJUNCTION image translation. attribute classification HYPONYM-OF serval tasks. image translation HYPONYM-OF serval tasks. zero - shot image editing HYPONYM-OF serval tasks. image morphing HYPONYM-OF serval tasks. ,"This paper proposes to use semantic alignment between a parent StyleGAN model and a finetuned child model on a new dataset to improve semantic alignment performance. The authors show that the proposed models are able to achieve semantical alignment on a variety of serval tasks such as image translation, image morphing, zero-shot image editing, and attribute classification."
1616,SP:511226b467019dcd85e9ebf8b9b56f8f1b3ef889,transfer learning HYPONYM-OF GAN ’s model alignment. image translation CONJUNCTION image morphing. image morphing CONJUNCTION image translation. tasks EVALUATE-FOR it. image morphing HYPONYM-OF tasks. image translation HYPONYM-OF tasks. It USED-FOR zero - shot image recognition. shared latent space of aligned models USED-FOR It. OtherScientificTerm is custom architectures. ,"This paper studies the problem of transfer learning, which is an important aspect of GAN’s model alignment. The authors propose to use custom architectures to align the weights of aligned models, and evaluate it on two tasks: image translation and image morphing. It is shown to be effective in zero-shot image recognition, and is able to leverage a shared latent space to learn from."
1617,SP:511226b467019dcd85e9ebf8b9b56f8f1b3ef889,"transfer learning USED-FOR child "" network. pre - trained "" parent "" network USED-FOR transfer learning. model CONJUNCTION fine - tuning technology. fine - tuning technology CONJUNCTION model. shared semantic information FEATURE-OF generation network. aligned model USED-FOR tasks. image - to - image translation CONJUNCTION cross - domain image morphing. cross - domain image morphing CONJUNCTION image - to - image translation. cross - domain image morphing CONJUNCTION zero - shot classification and regression. zero - shot classification and regression CONJUNCTION cross - domain image morphing. aligned model USED-FOR image - to - image translation. image - to - image translation HYPONYM-OF tasks. zero - shot classification and regression HYPONYM-OF tasks. cross - domain image morphing HYPONYM-OF tasks. ","This paper proposes to use transfer learning from a pre-trained ""parent"" network to a ""child"" network. The model and fine-tuning technology are trained jointly. The generation network is trained with shared semantic information. The aligned model is applied to several tasks including image-to-image translation, cross-domain image morphing, zero-shot classification and regression."
1618,SP:511226b467019dcd85e9ebf8b9b56f8f1b3ef889,base model CONJUNCTION fine - tuned model. fine - tuned model CONJUNCTION base model. fine - tuned model USED-FOR secondary dataset. OtherScientificTerm is latent space modifications. Generic is models. ,"This paper proposes to train a base model and a fine-tuned model on a secondary dataset, where the base model is trained on the original dataset and the fine-tune model is used to train on the secondary dataset. The idea is that the latent space modifications of the two models are similar, and that the performance of the base and the trained model can be improved. "
1619,SP:0e13f831c211626195c118487f2fff36a6e293f6,Gromov - Wasserstein ( GW ) distance FEATURE-OF probability distributions. ( graph ) structured data FEATURE-OF probability distributions. optimization setting USED-FOR divergence. formulations USED-FOR problems. clustering of the graph CONJUNCTION graph completion. graph completion CONJUNCTION clustering of the graph. graph dictionary learning CONJUNCTION clustering of the graph. clustering of the graph CONJUNCTION graph dictionary learning. graph clustering ( partitioning ) CONJUNCTION graph dictionary learning. graph dictionary learning CONJUNCTION graph clustering ( partitioning ). divergence USED-FOR formulations. formulations USED-FOR learning with graphs. learning with graphs USED-FOR problems. clustering of the graph HYPONYM-OF problems. graph clustering ( partitioning ) HYPONYM-OF problems. graph completion HYPONYM-OF problems. graph dictionary learning HYPONYM-OF problems. applications EVALUATE-FOR divergence. graph data EVALUATE-FOR divergence. graph data USED-FOR applications. ,"This paper studies the Gromov-Wasserstein (GW) distance between two probability distributions over (graph) structured data. The authors propose a new optimization setting for computing the divergence between the two distributions. The proposed formulations can be applied to a variety of problems involving learning with graphs, including graph clustering (partitioning), graph dictionary learning, clustering of the graph, and graph completion. The experiments show that the proposed divergence can be used to improve the performance of several applications on graph data."
1620,SP:0e13f831c211626195c118487f2fff36a6e293f6,formulation USED-FOR problem. Conditional Gradient USED-FOR formulation. graph partitioning CONJUNCTION clustering. clustering CONJUNCTION graph partitioning. clustering CONJUNCTION completion. completion CONJUNCTION clustering. graphs USED-FOR applications. completion HYPONYM-OF applications. Graph Dictionary Learning USED-FOR completion. clustering HYPONYM-OF applications. graph partitioning HYPONYM-OF applications. completion HYPONYM-OF graphs. graph partitioning HYPONYM-OF graphs. clustering HYPONYM-OF graphs. Task is Gromov - Wasserstein problem. OtherScientificTerm is histograms. ,"This paper studies the Gromov-Wasserstein problem. The authors propose a new formulation of the problem based on Conditional Gradient. The main contribution of this paper is to study the applications of graphs such as graph partitioning, clustering, completion based on Graph Dictionary Learning. The experiments are conducted on histograms."
1621,SP:0e13f831c211626195c118487f2fff36a6e293f6,Gromov - Wasserstein divergence USED-FOR graphs. graph clustering CONJUNCTION graph completion. graph completion CONJUNCTION graph clustering. graph partition CONJUNCTION graph clustering. graph clustering CONJUNCTION graph partition. it USED-FOR graphs. graph clustering HYPONYM-OF graphs. graph partition HYPONYM-OF graphs. Wikipedia hyper link network CONJUNCTION Amazon product network. Amazon product network CONJUNCTION Wikipedia hyper link network. synthetic data CONJUNCTION real dataset. real dataset CONJUNCTION synthetic data. Amazon product network HYPONYM-OF real dataset. Wikipedia hyper link network HYPONYM-OF real dataset. GW CONJUNCTION spectral GW. spectral GW CONJUNCTION GW. spectral GW HYPONYM-OF methods. GW HYPONYM-OF methods. OtherScientificTerm is Gromov - Wasserstein distance. ,"This paper proposes to use Gromov-Wasserstein divergence as a metric to measure the distance between two graphs. The main idea is that it can be used for graphs such as graph partition, graph clustering, graph completion, etc. The paper presents experiments on both synthetic data and a real dataset (Wikipedia hyper link network and Amazon product network). The experiments show that the proposed methods such as GW and spectral GW are effective. "
1622,SP:0e13f831c211626195c118487f2fff36a6e293f6,"structure of the problem FEATURE-OF matchings. relaxation USED-FOR tasks. graph partitioning HYPONYM-OF tasks. Task are learning tasks on graphs, and GW graph matching. OtherScientificTerm are semi - relaxed, and marginal constraints. Material is graph datasets. Generic is baselines. ","This paper studies learning tasks on graphs. The authors propose a semi-relaxed version of GW graph matching, where the matchings are relaxed to reflect the structure of the problem. This relaxation is applied to two tasks: graph partitioning and graph matching. The marginal constraints are also relaxed. Experiments are conducted on two graph datasets and compared with several baselines."
1623,SP:d6d144be11230070ae9395db70b7c7743540bad4,"models USED-FOR human behavior. limited samples USED-FOR models. assumption COMPARE Boltzmann rationality. Boltzmann rationality COMPARE assumption. reward function USED-FOR human behavior. known reward function USED-FOR prior over human policies. known reward function USED-FOR imitation learning algorithm. approach COMPARE behaviorally cloning human behavior. behaviorally cloning human behavior COMPARE approach. OtherScientificTerm are human reward function, and Boltzmann policy distribution ( BPD ). ",This paper proposes to train models to imitate human behavior with limited samples. The authors make an assumption that the human reward function is similar to the Boltzmann policy distribution (BPD). The authors then propose an imitation learning algorithm that uses the known reward function as a prior over human policies. The proposed approach is compared to behaviorally cloning human behavior.
1624,SP:d6d144be11230070ae9395db70b7c7743540bad4,"systematic suboptimality FEATURE-OF human behavior. It COMPARE Boltzmann rational model. Boltzmann rational model COMPARE It. BPD USED-FOR systematic suboptimality. trajectory data USED-FOR human modeling and prediction. approach USED-FOR human policies. human policies COMPARE trajectories. trajectories COMPARE human policies. human reward function USED-FOR optimal human behavior. discriminator USED-FOR policy. policy CONJUNCTION policy. policy CONJUNCTION policy. discriminator USED-FOR policy. base measure USED-FOR policy. sampled human trajectories USED-FOR policy. BPD CONJUNCTION behavior cloning. behavior cloning CONJUNCTION BPD. Cross entropy CONJUNCTION BPD. BPD CONJUNCTION Cross entropy. self - play policy CONJUNCTION behavior cloning policy. behavior cloning policy CONJUNCTION self - play policy. Boltzmann rationality CONJUNCTION self - play policy. self - play policy CONJUNCTION Boltzmann rationality. policy CONJUNCTION self - play policy. self - play policy CONJUNCTION policy. Mean return CONJUNCTION policy. policy CONJUNCTION Mean return. Boltzmann rationality USED-FOR policy. Boltzmann rationality CONJUNCTION self - play policy. self - play policy CONJUNCTION Boltzmann rationality. self - play policy CONJUNCTION behavior cloning policy. behavior cloning policy CONJUNCTION self - play policy. BPD USED-FOR policy. Mean return CONJUNCTION policy. policy CONJUNCTION Mean return. self - play policy USED-FOR policy. Boltzmann rationality USED-FOR policy. BPD USED-FOR Mean return. Method are Boltzmann policy distribution ( BPD ), generative adversarial networks, generator, and Boltzmann rational models. OtherScientificTerm are Systematic suboptimality, suboptimal behavior, human action choices, human BPD policy, and human proxy "" policy. Generic are it, and networks. Material are simulated human data, and real human - with - human play data. ","This paper proposes a novel approach to learning human policies from trajectory data. It builds on top of the Boltzmann rational model (BPD) and extends it to the case of systematic suboptimality in human behavior. BPD is motivated by the observation that human modeling and prediction on trajectory data can lead to suboptimal behavior. The authors propose to learn a policy from sampled human trajectories using generative adversarial networks, where a discriminator is used to train a policy and a policy is trained using a base measure. The goal of this approach is to learn human policies that are closer to optimal human policies than trajectories generated by a human reward function.   The authors introduce a Boltuzmann policy distribution (BPG) and show how to use it to approximate the behavior of human agents. They show that the discriminator learns a policy that is close to the optimal human behavior, and that the policy is similar to the policy trained with BPD. They also show that a human action choices are similar to those of a human BPD policy. They then show that using Cross entropy, BPD, and behavior cloning, they can learn a self-play policy, a policy trained using BPD as a base metric, a self - play policy, and a behavior cloning policy. In addition, they show that their networks are able to learn ""human proxy"" policy, which is trained on simulated human data, and can be used to approximate real human-with-human play data. Finally, they demonstrate that their generator is able to generalize to unseen environments, which are not suitable for Boltzman rational models. "
1625,SP:d6d144be11230070ae9395db70b7c7743540bad4,model BPD USED-FOR suboptimality of human behavior. approximation inference method USED-FOR BPD. BPD COMPARE methods. methods COMPARE BPD. overcooked game EVALUATE-FOR methods. overcooked game EVALUATE-FOR BPD. model COMPARE data - extensive BC method. data - extensive BC method COMPARE model. model COMPARE BR. BR COMPARE model. data - extensive BC method COMPARE BR. BR COMPARE data - extensive BC method. model COMPARE human - aware RL model. human - aware RL model COMPARE model. human - AI collaboration EVALUATE-FOR model. ,This paper proposes a model BPD to address the suboptimality of human behavior. The authors propose an approximation inference method for BPD and show that BPD outperforms existing methods on an overcooked game. The model is compared to a data-exhaustive BC method and a BR. The proposed model is evaluated on human-AI collaboration and compared with a human-aware RL model.
1626,SP:d6d144be11230070ae9395db70b7c7743540bad4,predicting policies COMPARE trajectories. trajectories COMPARE predicting policies. predicting policies USED-FOR systematic suboptimality. Boltzmann policy distribution ( BPD ) COMPARE Boltzman rationality. Boltzman rationality COMPARE Boltzmann policy distribution ( BPD ). Boltzmann policy distribution ( BPD ) USED-FOR prior over human policies. policies COMPARE trajectories. trajectories COMPARE policies. deep generative models USED-FOR BPD. KL divergence EVALUATE-FOR distributions. generative network USED-FOR distribution. distribution HYPONYM-OF distributions. method USED-FOR human behavior. method USED-FOR baseline. human behavior CONJUNCTION baseline. baseline CONJUNCTION human behavior. Task is modeling human behavior. OtherScientificTerm is human policies. Method is Boltzmann rationality. ,"This paper studies the problem of modeling human behavior. The authors argue that predicting policies instead of trajectories can lead to systematic suboptimality. To address this problem, the authors propose to use the Boltzmann policy distribution (BPD) instead of Boltzman rationality as a prior over human policies. The BPD is trained using deep generative models, and the authors show that the KL divergence between the two distributions (i.e., the distribution learned by a generative network) and the one learned by human policies is lower than the one learnt by Boltzmane rationality. The proposed method is applied to model human behavior and a baseline."
1627,SP:401ef5fe2022e926b0321258efac1f369f186ace,"SQuant HYPONYM-OF data - free quantization method. approximated Hessian information USED-FOR SQuant. SQuant USED-FOR constrained absolute sum of error ( CASE ). constrained absolute sum of error ( CASE ) COMPARE MSE. MSE COMPARE constrained absolute sum of error ( CASE ). Method are post - training quantization ( PTQ ), and backpropagation. ","This paper proposes SQuant, a data-free quantization method based on approximated Hessian information. SQuant is able to achieve a constrained absolute sum of error (CASE), which is better than MSE. This is achieved by using post-training quantization (PTQ) and using backpropagation."
1628,SP:401ef5fe2022e926b0321258efac1f369f186ace,"second - order Taylor expansion of   loss USED-FOR data - free quantization method. method COMPARE data - free methods. data - free methods COMPARE method. OtherScientificTerm are Hessian matrix, quantized weights, and quantization. ",This paper proposes a data-free quantization method based on the second-order Taylor expansion of  loss. The main idea is to use the Hessian matrix as a surrogate for the quantized weights. The authors show that the proposed method is able to outperform existing data-based quantization methods.
1629,SP:401ef5fe2022e926b0321258efac1f369f186ace,"back - propagation CONJUNCTION fine - tuning. fine - tuning CONJUNCTION back - propagation. cross - layer independence USED-FOR optimization. MSE COMPARE CASE ( constrained ASE ) of weight perturbation. CASE ( constrained ASE ) of weight perturbation COMPARE MSE. DFQ method COMPARE GDFQ. GDFQ COMPARE DFQ method. GDFQ HYPONYM-OF QAT. technique USED-FOR low - bit quantization. Method are data - free quantization method, and Hessian - based optimization. OtherScientificTerm are K, and weight tensor. ","This paper proposes a data-free quantization method. The main idea is to use Hessian-based optimization, where K is a function of the weight tensor, and the optimization is based on cross-layer independence between back-propagation and fine-tuning. Compared to MSE and the CASE (constrained ASE) of weight perturbation, the proposed DFQ method outperforms GDFQ, a variant of QAT (GDFQ). The authors also propose a technique for low-bit quantization."
1630,SP:401ef5fe2022e926b0321258efac1f369f186ace,data - free quantization algorithm USED-FOR deep neural networks. SQuant HYPONYM-OF data - free quantization algorithm. SQuant USED-FOR Hessian - based optimization objective. constrained absolute sum of error ( CASE ) USED-FOR objective functions. SQuant algorithm COMPARE baseline algorithms. baseline algorithms COMPARE SQuant algorithm. accuracy EVALUATE-FOR baseline algorithms. accuracy EVALUATE-FOR SQuant algorithm. Generic is components. Method is progressive algorithm. ,"This paper proposes SQuant, a data-free quantization algorithm for deep neural networks. SQuant is a Hessian-based optimization objective, where the objective functions are constrained absolute sum of error (CASE). The authors propose two components: (1) a progressive algorithm, and (2) a linear combination of the two components. Experiments show that the proposed SQuant algorithm achieves better accuracy than baseline algorithms."
1631,SP:fa4bc3f6ad3f2a0113a930fb49d68660d63910e8,deep architecture USED-FOR segmentation of time series. CNN CONJUNCTION ResNet. ResNet CONJUNCTION CNN. LSTM USED-FOR multi - scale problem of time series. encoder - decoder module PART-OF core components. skip connections FEATURE-OF LSTM. LSTM HYPONYM-OF core components. core components PART-OF architecture. LSTM PART-OF architecture. encoder - decoder module PART-OF architecture. CNN USED-FOR encoder - decoder module. ResNet USED-FOR encoder - decoder module. convolutional layer USED-FOR stepwise classification. components USED-FOR convolutional layer. baselines EVALUATE-FOR approach. OtherScientificTerm is time series. Generic is subsequences. ,"This paper proposes a deep architecture for the segmentation of time series. The architecture consists of two core components: an LSTM with skip connections and an encoder-decoder module based on CNN and ResNet. These two components are combined to form a convolutional layer that performs stepwise classification of the time series, which is then used to generate subsequences. The proposed approach is evaluated on several baselines."
1632,SP:fa4bc3f6ad3f2a0113a930fb49d68660d63910e8,stepwise segmentation USED-FOR time series data. SegTime HYPONYM-OF stepwise segmentation. sliding window approach COMPARE SegTime. SegTime COMPARE sliding window approach. MSS - LSTM network CONJUNCTION 1D encoder - decoder network. 1D encoder - decoder network CONJUNCTION MSS - LSTM network. modules USED-FOR it. MSS - LSTM network HYPONYM-OF modules. 1D encoder - decoder network HYPONYM-OF modules. it USED-FOR fast- and slow - changing labels. step level COMPARE sliding window. sliding window COMPARE step level. atrous convolution CONJUNCTION skip - LSTMs. skip - LSTMs CONJUNCTION atrous convolution. depthwise separable convolution CONJUNCTION atrous convolution. atrous convolution CONJUNCTION depthwise separable convolution. depthwise separable convolution CONJUNCTION skip - LSTMs. skip - LSTMs CONJUNCTION depthwise separable convolution. MSS - LSTM CONJUNCTION 1D convolutional layers. 1D convolutional layers CONJUNCTION MSS - LSTM. MSS - LSTM USED-FOR Long - term dependency. 1D convolutional layers USED-FOR Long - term dependency. Generic is networks. Method is convolutional layer. OtherScientificTerm is time - step segment. ,"This paper studies the problem of stepwise segmentation for time series data, specifically SegTime, which is a popular method for learning the labels for a given time-step segment. Compared to the sliding window approach, SegTime uses two modules: a MSS-LSTM network and a 1D encoder-decoder network. The two networks are trained in parallel, where the convolutional layer is trained at the same time. The authors show that it is able to handle fast- and slow-changing labels, and that it can be trained with different modules: depthwise separable convolution, atrous convolution and skip-LSMs. Long-term dependency can be learned with MSS - LSTM and 1D convolutionAL layers. The paper also shows that the step level is more efficient than sliding window."
1633,SP:fa4bc3f6ad3f2a0113a930fb49d68660d63910e8,"approach USED-FOR problem. temporal conv net HYPONYM-OF model. architecture USED-FOR TS segmentation problems. DeepLabv3 + USED-FOR TS segmentation problems. Task are time - series ( TS ) segmentation, and TS segmentation. OtherScientificTerm are sampling rate, long - term dependences, and sliding windows. Generic are approaches, and network. ","This paper studies the problem of time-series (TS) segmentation, and proposes an approach to solve this problem. The proposed model, called temporal conv net, is a generalization of DeepLabv3 +, an architecture that has been applied to several TS segmentation problems (e.g., DeepLab v3 + for TS segmentations with a fixed sampling rate). The main difference between the proposed approaches is that the proposed network is able to handle long-term dependences, which is an important aspect of TS segmenting. The authors also show that sliding windows can be used to improve the performance."
1634,SP:fa4bc3f6ad3f2a0113a930fb49d68660d63910e8,supervised method USED-FOR time series segmentation. SegTime HYPONYM-OF supervised method. stepwise time series classification USED-FOR supervised method. window size CONJUNCTION stride. stride CONJUNCTION window size. It COMPARE approaches. approaches COMPARE It. label changing frequency FEATURE-OF It. skip connections FEATURE-OF LSTMs. MSS - LSTM HYPONYM-OF multiscale skip LSTM. 1D - DS - ResNet HYPONYM-OF deep CNN. multiscale skip LSTM HYPONYM-OF core modules. core modules USED-FOR network architecture. deep CNN HYPONYM-OF core modules. LSTMs USED-FOR multiscale skip LSTM. 1D Depthwise Separable and Atrous Convolutional layers CONJUNCTION Atrous Multiscale Pooling module ( AMSP ). Atrous Multiscale Pooling module ( AMSP ) CONJUNCTION 1D Depthwise Separable and Atrous Convolutional layers. 1D Depthwise Separable and Atrous Convolutional layers HYPONYM-OF modules. Atrous Multiscale Pooling module ( AMSP ) HYPONYM-OF modules. one CONJUNCTION one. one CONJUNCTION one. slow changing labels FEATURE-OF one. fast changing labels FEATURE-OF one. one EVALUATE-FOR method. one EVALUATE-FOR method. fast changing labels HYPONYM-OF datasets. datasets EVALUATE-FOR method. one HYPONYM-OF datasets. one HYPONYM-OF datasets. OtherScientificTerm is sliding windows. Generic is approach. ,"This paper proposes SegTime, a supervised method for time series segmentation based on stepwise time series classification. It is different from previous approaches in terms of the label changing frequency, window size, and stride. The proposed network architecture is based on three core modules: 1D-DS-ResNet, a deep CNN, and a multiscale skip LSTM (MSS-LSTM) with skip connections. The core modules are 1D Depthwise Separable and Atrous Convolutional layers, and the Atrous Multiscale Pooling module (AMSP). The proposed method is evaluated on two datasets: one with slow changing labels, and one with fast changing labels. The experimental results show that the sliding windows are beneficial for the proposed approach."
1635,SP:8ad1b170f0392a132a3816c9cd28fb7332343e65,decomposition - based explanations method USED-FOR graph neural networks. faithful explanation USED-FOR GNN predictions. synthetic and real - world datasets EVALUATE-FOR method. Method is subgraph level interpretation algorithm. OtherScientificTerm is graph nodes. ,"This paper proposes a decomposition-based explanations method for graph neural networks. The main idea is to use a subgraph level interpretation algorithm, where the graph nodes are decomposed into sub-graphs, and the GNN predictions are given as a faithful explanation. The method is evaluated on both synthetic and real-world datasets."
1636,SP:8ad1b170f0392a132a3816c9cd28fb7332343e65,"algorithm USED-FOR subgraph - level explanation. agglomeration USED-FOR algorithm. agglomeration USED-FOR subgraph - level explanation. time efficiency EVALUATE-FOR model. Method are GNN, explainable GNN framework, and GRaph nEural nEtworks. ",This paper proposes an explainable GNN framework based on GRaph nEural nEtworks. The proposed algorithm uses agglomeration to extract the subgraph-level explanation of a GNN. The authors show that the proposed model can improve the time efficiency.
1637,SP:8ad1b170f0392a132a3816c9cd28fb7332343e65,decomposition - based explanation method USED-FOR graph neural networks. approximation CONJUNCTION perturbation. perturbation CONJUNCTION approximation. perturbation USED-FOR works. approximation USED-FOR works. decomposition rules USED-FOR GCN. decomposition rules USED-FOR GAT. GCN CONJUNCTION GAT. GAT CONJUNCTION GCN. greedy approach USED-FOR maximally influential node sets. greedy approach USED-FOR subgraph groups. Material is synthetic and real - world datasets. ,This paper proposes a decomposition-based explanation method for graph neural networks. The authors propose works that use both approximation and perturbation. The decomposition rules used in GCN and GAT are also applied to subgraph groups using a greedy approach to find maximally influential node sets. Experiments are conducted on both synthetic and real-world datasets.
1638,SP:8ad1b170f0392a132a3816c9cd28fb7332343e65,"feedforward propagation mechanism PART-OF GNN. feedforward propagation mechanism USED-FOR it. DEGREE USED-FOR feedforward propagation mechanism. realistic decomposition techniques USED-FOR GNNs. approach USED-FOR subgraph - level explanation. agglomeration USED-FOR subgraph - level explanation. DEGREE COMPARE baselines. baselines COMPARE DEGREE. DEGREE USED-FOR graph data. fidelity EVALUATE-FOR baselines. fidelity EVALUATE-FOR DEGREE. OtherScientificTerm are decomposition - based explanations, and graph topology. ","This paper proposes DEGREE, a feedforward propagation mechanism in GNN, which is based on decomposition-based explanations. The main idea is to use realistic decomposition techniques to train GNNs, and to use it as a feedback propagation mechanism. This approach is able to provide a subgraph-level explanation using agglomeration, which does not require graph topology. Experiments on graph data show that the proposed DeGREE can improve the fidelity compared to baselines."
1639,SP:b28a9d1ad4c539d07d53e39376cbd76024d7745c,drop - in replacement USED-FOR downsampling layers. DiffStride HYPONYM-OF drop - in replacement. DiffStride USED-FOR downsampling layers. It USED-FOR cropping box. frequency domain FEATURE-OF cropping box. backpropagation USED-FOR It. backpropagation USED-FOR cropping box. model USED-FOR non - integer stride. model USED-FOR initial stride. Task is audio and image classification. ,"This paper proposes DiffStride, a drop-in replacement for downsampling layers. It learns a cropping box in the frequency domain using backpropagation. The model also learns a non-integer stride. Experiments on audio and image classification show that the model is able to recover the initial stride."
1640,SP:b28a9d1ad4c539d07d53e39376cbd76024d7745c,differentiable stride formulation USED-FOR stride value. Fourier domain FEATURE-OF cropping mask. large scale datasets HYPONYM-OF datasets. OtherScientificTerm is resize. ,"This paper proposes a differentiable stride formulation for estimating the stride value of a cropping mask in the Fourier domain. The authors show that the resize can be computed in a deterministic manner, which is useful for datasets such as large scale datasets."
1641,SP:b28a9d1ad4c539d07d53e39376cbd76024d7745c,"technique USED-FOR stride of downsampling operations. DiffStride USED-FOR stride of downsampling operations. stride of downsampling operations USED-FOR neural networks. gradient descent USED-FOR stride of downsampling operations. frequency domain FEATURE-OF feature map. Discrete Fourier Transform USED-FOR frequency domain. Discrete Fourier Transform USED-FOR feature map. audio and image recognition HYPONYM-OF datasets. datasets EVALUATE-FOR random stride policies. regularization objective USED-FOR downsampling. regularization objective USED-FOR small strides. computational and memory cost EVALUATE-FOR downsampling. Method are SpectralPool, and soft - relaxation of the cropping. Generic is it. OtherScientificTerm are gradient, and cropping parameters. ","This paper proposes DiffStride, a technique to reduce the stride of downsampling operations in neural networks by gradient descent. The key idea is to use a Discrete Fourier Transform on the feature map in the frequency domain, which is similar to SpectralPool. The main difference is that instead of using a soft-relaxation of the cropping, the authors propose to use it as a regularization of the gradient. The authors evaluate random stride policies on two datasets: audio and image recognition. The results show that the proposed regularization objective can reduce the number of small strides and reduce the computational and memory cost while maintaining the same cropping parameters."
1642,SP:b28a9d1ad4c539d07d53e39376cbd76024d7745c,"approach USED-FOR optimal striding parameters. optimal striding parameters FEATURE-OF convolutional networks. approach HYPONYM-OF downsampling layer. DiffStride HYPONYM-OF downsampling layer. spectral pooling USED-FOR integer output dimensions. spectral pooling USED-FOR downsampling layer. spectral pooling COMPARE DiffStride. DiffStride COMPARE spectral pooling. stop - gradient operator USED-FOR cropping. stop - gradient operator USED-FOR DiffStride. drop - in replacement USED-FOR fixed pooling layers. drop - in replacement USED-FOR approach. random initialisations USED-FOR pooling approaches. regularisation term USED-FOR time and space efficiency. OtherScientificTerm are strides, Fourier domain, and cropping mask. Metric is accuracy. ","This paper proposes an approach to find optimal striding parameters for convolutional networks. The proposed approach, DiffStride, is a downsampling layer based on spectral pooling in integer output dimensions, which is similar to DiffStrive with a stop-gradient operator for cropping. The key difference is that instead of using strides in the Fourier domain, the proposed approach uses a drop-in replacement for fixed pooling layers, which improves accuracy and time and space efficiency. The authors also propose to use random initialisations for pooling approaches and add a regularisation term to improve time andspace efficiency. Finally, the authors introduce a cropping mask to further improve performance."
1643,SP:54cdc6fe43ed138231f26daf699119f2a16473d0,anisotropic certificates USED-FOR randomized smoothing. randomized smoothing USED-FOR certifying multi - output classifiers. anisotropic Gaussian   and Bernoulli smoothing USED-FOR collective robustness. collective robustness EVALUATE-FOR it. anisotropic Gaussian   and Bernoulli smoothing USED-FOR it. semantic segmentation CONJUNCTION node classification. node classification CONJUNCTION semantic segmentation. semantic segmentation EVALUATE-FOR method. node classification EVALUATE-FOR method. ,This paper proposes to use anisotropic certificates for randomized smoothing for certifying multi-output classifiers. The authors claim that it improves the collective robustness by using aniseotropic Gaussian  and Bernoulli smoothing. The proposed method is evaluated on semantic segmentation and node classification.
1644,SP:54cdc6fe43ed138231f26daf699119f2a16473d0,"randomized smoothing USED-FOR multi - output classifiers. average softmax value COMPARE majority vote. majority vote COMPARE average softmax value. analysis method USED-FOR variance smoothing. average softmax value USED-FOR prediction rule. variance smoothing USED-FOR discrete data. majority vote USED-FOR prediction rule. discrete data USED-FOR analysis method. average softmax value USED-FOR variance smoothing. average softmax value USED-FOR analysis method. first and second - order statistics USED-FOR robustness guarantees. first and second - order statistics USED-FOR method. robustness guarantees USED-FOR method. collective certification strategy USED-FOR multi - output classifiers. common interface USED-FOR base certificates. base certificates USED-FOR collective certification strategy. common interface USED-FOR collective certification strategy. mixed - integer linear program USED-FOR multi - output certification problem. Method are smoothing distribution, multi - output classifier, and perturbation model. OtherScientificTerm is base certified regions. ","This paper studies the problem of randomized smoothing for multi-output classifiers. The authors propose an analysis method for variance smoothing on discrete data based on the average softmax value instead of the majority vote. The proposed method provides robustness guarantees based on first and second-order statistics. In particular, the authors show that the smoothing distribution is non-convex when the number of base certified regions is large enough. In addition, they propose a collective certification strategy that uses a common interface to obtain base certificates for all the base certificates of the multi-input classifier. Finally, they apply a mixed-integer linear program to solve the mult-output certification problem and show that their perturbation model is robust."
1645,SP:54cdc6fe43ed138231f26daf699119f2a16473d0,input perturbations FEATURE-OF robustness certificate. localized randomized smoothing USED-FOR collective certificate. image segmentation CONJUNCTION node classification tasks. node classification tasks CONJUNCTION image segmentation. node classification tasks EVALUATE-FOR collective certificate. image segmentation EVALUATE-FOR collective certificate. ,This paper proposes a new robustness certificate that is robust to input perturbations. The proposed collective certificate is based on localized randomized smoothing. Experiments on image segmentation and node classification tasks demonstrate the effectiveness of the collective certificate.
1646,SP:54cdc6fe43ed138231f26daf699119f2a16473d0,"localized smoothing approach USED-FOR certifying structured output models. finding worst cases adversaries USED-FOR misprediction. anisotropic smoothing USED-FOR finding adversaries. box constraints USED-FOR binary objective. linear   programs USED-FOR problem. image segmentation tasks CONJUNCTION node classification. node classification CONJUNCTION image segmentation tasks. OtherScientificTerm are bounded input perturbation, certified radii, worst cases adversaries, adversaries, and certified region. Task is certifying pixel predictions. ","This paper proposes a localized smoothing approach for certifying structured output models. The key idea is to use anisotropic smoothing, where the bounded input perturbation is replaced by a set of certified radii, and the goal is to find worst cases adversaries that can lead to misprediction. The binary objective is formulated as a function of box constraints. The problem can be solved using linear  programs. Experiments are conducted on image segmentation tasks and node classification. The results show that certifying pixel predictions is computationally efficient, especially when the certified region is large."
1647,SP:aacc31e83886c4c997412a1e51090202075eda86,"normalizing flow USED-FOR inductive biases. inductive biases PART-OF model architecture. normalizing flow USED-FOR model architecture. model COMPARE masked autoregressive flow baseline. masked autoregressive flow baseline COMPARE model. OtherScientificTerm are spherical Gaussian variable, pre - defined probabilistic program, and gated structured layer. Generic is version. ","This paper proposes a new model architecture that incorporates inductive biases using a normalizing flow. The key idea is to learn a spherical Gaussian variable, which is then fed into a pre-defined probabilistic program. The authors show that the proposed model outperforms a masked autoregressive flow baseline by a large margin. The main contribution of the paper is the introduction of a gated structured layer, which allows the authors to train a version of the model that is more interpretable."
1648,SP:aacc31e83886c4c997412a1e51090202075eda86,"invertible transformations USED-FOR it. invertible transformation USED-FOR normal distribution. invertible transformation USED-FOR layer. layer PART-OF flow model. unstructured layers PART-OF flow model. structured layers USED-FOR domain knowledge. unstructured flow layers USED-FOR domain knowledge. Method are probabilistic program, and structured layer. ","This paper proposes a probabilistic program where each layer of a flow model is represented as an invertible transformation of a normal distribution, and it is trained with different invertsible transformations. The authors argue that the structured layer can be used as a way to augment the unstructured layers of the flow model with domain knowledge from the structured layers to improve the performance of domain knowledge obtained from the original (unstructured flow layers). "
1649,SP:aacc31e83886c4c997412a1e51090202075eda86,structured layers USED-FOR domain knowledge. structured layers USED-FOR normalizing flow model. flow HYPONYM-OF probablistic programs. univariable RV HYPONYM-OF probablistic programs. gated layer USED-FOR model. user - specified model CONJUNCTION MAF. MAF CONJUNCTION user - specified model. hierarchical gaussian CONJUNCTION timeseries models. timeseries models CONJUNCTION hierarchical gaussian. toy multimodality distributions CONJUNCTION hierarchical gaussian. hierarchical gaussian CONJUNCTION toy multimodality distributions. timeseries models CONJUNCTION variational inferences. variational inferences CONJUNCTION timeseries models. method COMPARE baselines. baselines COMPARE method. variational inferences EVALUATE-FOR method. variational inferences EVALUATE-FOR baselines. OtherScientificTerm is inductive bias. ,"This paper proposes a normalizing flow model with structured layers to incorporate domain knowledge. The authors consider probablistic programs such as flow and univariable RV. The model is trained with a gated layer, and the inductive bias is removed. Experiments are performed on toy multimodality distributions, hierarchical gaussian, timeseries models, variational inferences, and a user-specified model and MAF. The proposed method outperforms the baselines in terms of performance, and is shown to be more interpretable."
1650,SP:aacc31e83886c4c997412a1e51090202075eda86,"probabilistic program USED-FOR normalising flow layer. gating "" mechanism USED-FOR layer. transformation CONJUNCTION identity transform. identity transform CONJUNCTION transformation. layer CONJUNCTION generic normalizing flow layers. generic normalizing flow layers CONJUNCTION layer. inductive bias FEATURE-OF normalising flow. probabilistic programs USED-FOR normalising flows. inductive biases FEATURE-OF normalising flows. Method is unit Gaussian. OtherScientificTerm is expressivity. ","This paper proposes a probabilistic program for a normalising flow layer. The ""gating"" mechanism is used to train the layer and the generic normalizing flow layers. The transformation and the identity transform are modeled as a unit Gaussian, and the expressivity of the normalising flows is measured by the inductive bias of the original normalising function. The authors show that normalising functions with inductive biases can be represented as probabilistically programs."
1651,SP:825a254c0725008143b260ead840ae35f9f096d1,large language models ( LMs ) USED-FOR grounded concepts of the world. LMs USED-FOR conceptual domains. LMs USED-FOR grounded world representations. textualized grid world CONJUNCTION RGB representation of colors. RGB representation of colors CONJUNCTION textualized grid world. grounded world representations HYPONYM-OF conceptual domains. RGB representation of colors HYPONYM-OF grounded world representations. textualized grid world HYPONYM-OF grounded world representations. models USED-FOR in - context learning setup. LMs USED-FOR grounded world representations. GPT-3 HYPONYM-OF LMs. text - only LMs USED-FOR grounded representations. from - scratch training USED-FOR visual language models. Generic is model. ,"This paper studies the problem of training large language models (LMs) to learn grounded concepts of the world. LMs are widely used in conceptual domains such as textualized grid world and RGB representation of colors. However, LMs trained on text-only LMs (e.g., GPT-3) are not trained on grounded world representations. This paper proposes to train models in an in-context learning setup, where the model is trained on a set of examples from the context. The paper shows that from-scratch training can improve the performance of visual language models."
1652,SP:825a254c0725008143b260ead840ae35f9f096d1,language models USED-FOR conceptual spaces. spatial directional terms CONJUNCTION cardinal directions. cardinal directions CONJUNCTION spatial directional terms. cardinal directions CONJUNCTION color terms. color terms CONJUNCTION cardinal directions. language model USED-FOR conceptual terms. supervision PART-OF few - shot learning paradigm. GPT-3 HYPONYM-OF few - shot learning paradigm. GPT-3 HYPONYM-OF model. Method is LM. ,"This paper studies the problem of learning language models to represent conceptual spaces. The authors propose a few-shot learning paradigm with supervision, where a language model is trained to model conceptual terms such as spatial directional terms, cardinal directions, and color terms. The proposed model, GPT-3, is an example of such a model. The experiments show that the proposed LM is able to generalize to unseen concepts."
1653,SP:825a254c0725008143b260ead840ae35f9f096d1,"text - only input USED-FOR grounded "" model. language USED-FOR groundedness. models USED-FOR isomophisms. models USED-FOR latent structure. latent structure PART-OF language. Material is text - only data. ","This paper proposes a ""grounded"" model for text-only input. The groundedness is defined as the ability of the language to encode isomophisms. The authors show that models that are trained on the latent structure of language are able to learn isomphisms that are invariant to the context. The paper also shows that the learned latent structure in the language can be used to generate isomorphisms that do not depend on the text only data."
1654,SP:825a254c0725008143b260ead840ae35f9f096d1,"they USED-FOR conceptual space. LMs USED-FOR task. Method is language models. Material is text. OtherScientificTerm are navigation, and colors. Task is navigation and color tasks. ","This paper studies language models in the context of text. The main idea is that language models can be used to represent concepts that are not present in the text, e.g., navigation, colors, etc. and that they can learn a conceptual space that allows them to generalize to new tasks. The authors show that LMs can generalize well to a new task, and show that they generalize better to navigation and color tasks."
1655,SP:702029739062693e3f96051cbb38f20c53f2a223,"reward shaping USED-FOR emergent communication. Shannon entropy FEATURE-OF emerged language. PPO USED-FOR outer - level optimizer. multi - step task USED-FOR task. algorithms USED-FOR multi - step task. REINFORCE HYPONYM-OF algorithms. algorithms USED-FOR task. reward shaping USED-FOR entropy. entropy FEATURE-OF emergent language. multi - step tasks CONJUNCTION PPO. PPO CONJUNCTION multi - step tasks. OtherScientificTerm are rewards, and world radius. Metric is language entropy. Generic are it, process, and network. Method are PPO learning, and Extended Chinese Restaurant Process'( ECRP ). ","This paper studies the problem of reward shaping in emergent communication. The authors propose to use PPO as an outer-level optimizer, where the learned language entropy is modeled as a function of the Shannon entropy of the emerged language. The paper proposes two algorithms, REINFORCE and ECRP, to solve this task, which is a multi-step task where the rewards are given by the agent and the agent is trained to solve the task using PPO. The main contribution of the paper is to show that reward shaping can reduce the entropy of emergent language, and that it can be used to improve the performance of PPO learning. In particular, the authors show that it is possible to learn a process where the network is able to adapt to changes in the world radius, which they call 'Extended Chinese Restaurant Process' (ECRP). The authors also show that this can be achieved by training a network in a way that is similar to PPO, but with a smaller world radius. The experiments are conducted on a variety of tasks, and the results show that the proposed algorithms can achieve state-of-the-art performance on multi-stage tasks and PPO in a range of environments."
1656,SP:702029739062693e3f96051cbb38f20c53f2a223,reward shaping USED-FOR emergent language learning. RL USED-FOR emergent language learning. Chinese restaurant process CONJUNCTION RL. RL CONJUNCTION Chinese restaurant process. experience buffer size USED-FOR RL learning algorithm. reward shaping USED-FOR agent. Metric is entropy. Task is navigation task. ,This paper studies the problem of reward shaping in emergent language learning in RL. The authors compare the Chinese restaurant process with RL and show that the agent can benefit from reward shaping. They also propose an RL learning algorithm based on the experience buffer size. They show that increasing the entropy of the buffer can improve the performance on a navigation task.
1657,SP:702029739062693e3f96051cbb38f20c53f2a223,"language CONJUNCTION agents. agents CONJUNCTION language. emergent language research USED-FOR language. shaped rewards USED-FOR learning. shaped reward USED-FOR emergent language. Task is reinforcement learning. OtherScientificTerm are rewards, semantics, and environmental variables of interest. Method is sender - receiver navigation game. Metric is entropy. ","This paper studies reinforcement learning in the context of reinforcement learning, where the rewards are shaped to encourage agents to learn language and agents to communicate with each other. The authors propose to use emergent language research in order to study how language is learned in an unsupervised way, and how shaped rewards affect learning. The paper proposes to use a sender-receiver navigation game where the sender and receiver are given a set of messages and the receiver is given a shaped reward that encourages the sender to learn the semantics of the messages. The goal is to maximize the entropy of the message, which is a measure of how well the agent can communicate with the receiver, and to minimize the entropy when the agent is interacting with environmental variables of interest."
1658,SP:702029739062693e3f96051cbb38f20c53f2a223,"auxiliary rewards USED-FOR multi - agent communication task. discrete language USED-FOR task. discrete language USED-FOR task. shaped rewards USED-FOR learned languages. shaped rewards USED-FOR emergent language. entropy FEATURE-OF languages. entropy FEATURE-OF languages. information bandwidth FEATURE-OF shaped reward setting. experience replay buffer USED-FOR PPO algorithm. differential entropies FEATURE-OF game. CRP USED-FOR experience buffer sizes. batch size FEATURE-OF batched CRP. environmental rewards USED-FOR semantics of the learned language. OtherScientificTerm are denser shaped rewards, denser rewards, language, and semantics. Task are PPO learning, and emergent communication. ","This paper studies auxiliary rewards for multi-agent communication task with discrete language. The authors argue that learned languages with shaped rewards can be used to learn emergent language with denser shaped rewards. This is motivated by the observation that languages with high entropy have high information bandwidth in the shaped reward setting, and that languages that have lower entropy are more likely to learn denser structured languages. In order to address this issue, the authors propose a PPO algorithm that uses an experience replay buffer, where the game is played with differential entropies. They show that batched CRP with a small batch size (e.g. CRP for small experience buffer sizes) is able to achieve better performance than using denser rewards. They also show that environmental rewards can capture the semantics of the learned language, which is important for PPO learning. They further show that the learned semantics can be leveraged to improve emergent communication."
1659,SP:146ef14e569e10172a7dc602acd3fadf2c3bef8b,method USED-FOR cross - lingual transfer. method USED-FOR pretrained multilingual models. cross - lingual transfer FEATURE-OF pretrained multilingual models. representation invariance CONJUNCTION distributional class shift. distributional class shift CONJUNCTION representation invariance. method USED-FOR representation invariance. method USED-FOR class shift. OtherScientificTerm is large prior shifts. ,"This paper proposes a method for improving the cross-lingual transfer of pretrained multilingual models. The method aims to improve both the representation invariance and the distributional class shift. In particular, the method is shown to improve the class shift when large prior shifts are observed."
1660,SP:146ef14e569e10172a7dc602acd3fadf2c3bef8b,"cross - lingual transfer EVALUATE-FOR multilingual neural language models. prior shift estimation CONJUNCTION correction. correction CONJUNCTION prior shift estimation. feature alignment CONJUNCTION prior shift estimation. prior shift estimation CONJUNCTION feature alignment. correction USED-FOR it. prior shift estimation USED-FOR it. feature alignment USED-FOR it. it USED-FOR unsupervised cross - lingual learning method. importance - weighted domain adaptation ( IWDA ) HYPONYM-OF unsupervised cross - lingual learning method. multilingual NER CONJUNCTION multilingual sentiment analysis tasks. multilingual sentiment analysis tasks CONJUNCTION multilingual NER. multilingual sentiment analysis tasks HYPONYM-OF NLP tasks. multilingual NER HYPONYM-OF NLP tasks. approach COMPARE semi - supervised learning approaches. semi - supervised learning approaches COMPARE approach. OtherScientificTerm are representation invariance, and distributional shift in class priors. Material is src / tgt languages. ","This paper studies the problem of cross-lingual transfer in multilingual neural language models. Specifically, it proposes an unsupervised cross-language learning method called importance-weighted domain adaptation (IWDA), which uses feature alignment, prior shift estimation, and correction to improve the representation invariance to distributional shift in class priors. Experiments are conducted on a variety of NLP tasks, including multilingual NER and multilingual sentiment analysis tasks. Results show that the proposed approach outperforms other semi-supervised learning approaches, especially in the case of src/tgt languages."
1661,SP:146ef14e569e10172a7dc602acd3fadf2c3bef8b,"importance - weighted domain adaptation ( IWDA ) HYPONYM-OF approach. NER CONJUNCTION MARC classification. MARC classification CONJUNCTION NER. tasks PART-OF UCL setup. tasks EVALUATE-FOR zero - shot transfer. IWDA USED-FOR zero - shot transfer. IWDA USED-FOR tasks. MARC classification HYPONYM-OF UCL setup. NER HYPONYM-OF UCL setup. MARC classification HYPONYM-OF tasks. NER HYPONYM-OF tasks. Method is domain adaptation. OtherScientificTerm are distributional shifts in class priors, and domain shift. Task are UCL, and UCL problem. Generic are method, setup, and model. ","This paper proposes an approach called importance-weighted domain adaptation (IWDA), which is an approach to address the problem of domain adaptation in the presence of distributional shifts in class priors. The proposed method is motivated by the observation that UCL is a challenging problem due to the large number of tasks in the UCL setup, which can lead to poor performance in the face of domain shift. To address this problem, the authors propose a new UCL problem, where each task is represented as a separate domain, and the goal is to learn a model that can adapt to the new domain. The authors show that IWDA can achieve zero-shot transfer on a variety of tasks from the original UCL setting, including NER and MARC classification. "
1662,SP:146ef14e569e10172a7dc602acd3fadf2c3bef8b,"feature invariance CONJUNCTION class - prior invariance. class - prior invariance CONJUNCTION feature invariance. feature invariance CONJUNCTION language invariant representations. language invariant representations CONJUNCTION feature invariance. language invariant representations CONJUNCTION class - prior invariance. class - prior invariance CONJUNCTION language invariant representations. feature invariance USED-FOR zero - shot cross - lingual performance. class - prior invariance USED-FOR zero - shot cross - lingual performance. MARC reviews CONJUNCTION WikiANN NER tasks. WikiANN NER tasks CONJUNCTION MARC reviews. feature invariance USED-FOR zero - shot. zero - shot performance EVALUATE-FOR feature invariance. WikiANN NER tasks EVALUATE-FOR zero - shot. adversarial loss term USED-FOR average class conditional feature representations. importance weighting term USED-FOR approach. approach COMPARE vanilla zero - shot model. vanilla zero - shot model COMPARE approach. self - training USED-FOR sentiment analysis. MARC sentiment analysis and NER tasks EVALUATE-FOR vanilla zero - shot model. MARC sentiment analysis and NER tasks EVALUATE-FOR approach. synthetic datasets EVALUATE-FOR approach. synthetic datasets USED-FOR class prior shift. NER and MARC datasets FEATURE-OF synthetic datasets. large class prior shifts FEATURE-OF approach. OtherScientificTerm are class conditional distance, class prior, class priors, and class prior shifts. Task is zero - shot cross - lingual transfer. Metric is zero - shot cross - lingual transfer performance. Generic is approaches. ","This paper studies zero-shot cross-lingual transfer, where the goal is to maximize the class conditional distance between the target language and the target class prior. The authors propose to combine feature invariance, language invariant representations, and class-prior invariance to improve zero -shot performance with respect to zero-shots by combining feature invariant, class-previous invariance and class prior invariance. They evaluate their approach on MARC reviews, MARC sentiment analysis and WikiANN NER tasks, and compare the proposed approach with a vanilla zero - shot model trained with self-training for sentiment analysis. The proposed approach is based on an importance weighting term that penalizes the average class conditional feature representations with an adversarial loss term, which encourages the class priors to be invariant to class prior shifts. Their approach is evaluated on synthetic datasets for class prior shift on both NER and MARC datasets, and the results show that their approach is able to cope with large class priors. "
1663,SP:461ed47339e08dafea90a7c015d2f20e534daeb7,meta - learning algorithm USED-FOR hyperparameter optimization. Bootstrapped Meta - Learning CONJUNCTION meta - learning algorithm. meta - learning algorithm CONJUNCTION Bootstrapped Meta - Learning. temporal difference learning techniques USED-FOR reinforcement learning. temporal difference learning techniques USED-FOR meta - learner. optimization trajectories USED-FOR differentiation. exploration hyperparameter USED-FOR behaviour policy. hyperparameter optimization USED-FOR reinforcement learning. method USED-FOR hyperparameter optimization. multi - task meta - learning HYPONYM-OF hyperparameter optimization. Method is optimization process. OtherScientificTerm is meta - learning optimization horizons. ,"This paper proposes Bootstrapped Meta-Learning and a meta-learning algorithm for hyperparameter optimization in reinforcement learning. The meta-learner is trained using temporal difference learning techniques for reinforcement learning, where the optimization trajectories are used for differentiation. During the optimization process, the behaviour policy is trained with an exploration hyperparametreter. The proposed method is applied to the problem of hyperparameters optimization (e.g., multi-task meta-learning) and is shown to achieve state-of-the-art performance on a variety of tasks. The paper also provides a theoretical analysis of the meta-Learning optimization horizons."
1664,SP:461ed47339e08dafea90a7c015d2f20e534daeb7,"bilevel optimization HYPONYM-OF meta - learning. functional form USED-FOR outer - loop updates. meta - parameters USED-FOR post - inner - loop learner parameters. outer - loop gradient - based optimization USED-FOR meta - parameters. bootstrapped meta - learning HYPONYM-OF meta - learning objectives. meta - parameters USED-FOR meta - learning objectives. learner parameters USED-FOR meta - learning objective. bootstrap computation function CONJUNCTION learner parameter matching function. learner parameter matching function CONJUNCTION bootstrap computation function. gradient - based meta - parameter optimization USED-FOR bootstrapped meta - learning. learner parameter matching function USED-FOR it. gradient - based bootstrap target functions USED-FOR bootstrapped meta - learning. optimization progress FEATURE-OF gradient - based bootstrap target functions. bootstrapped meta - learning USED-FOR few - shot image classification. exploration rate FEATURE-OF behavior policy. bootstrapped meta - learning USED-FOR meta - learning parameters. miniImageNet USED-FOR few - shot image classification. miniImageNet EVALUATE-FOR bootstrapped meta - learning. Atari-57 EVALUATE-FOR bootstrapped meta - learning. Task is reinforcement learning settings. OtherScientificTerm are differentiable inner - loop, inner - loop updates, inner - loop improvement, bootstrap target, computation graph, and task objective. Method is meta - optimization. ","This paper studies the problem of meta-learning (i.e., bilevel optimization) in reinforcement learning settings, where the goal is to learn a differentiable inner-loop. The authors propose a functional form for outer-loop updates, which they call meta-optimization. The meta-parameters learned during outer-loof gradient-based optimization are used to update the post-inner-loop learner parameters, which are then used to improve the meta-learned parameters for other meta -learning objectives, e.g., bootstrapped meta learning. The main idea is to use the learned learner parameter matching function as a bootstrap computation function and to use it as the learner's parameter to optimize a meta-Learning objective. This is done by minimizing the gradient of the optimization progress of the gradient -based bootstrap target functions, which is a function of the number of iterations of inner-looper updates and the number (or iterations of iterations) of the inner-lobule improvement. This allows the optimization of the bootstrap to be done in a way that does not require the computation graph to be too large. The paper shows that the learned meta-propagation of the learned parameters can be used to optimize the task objective. The experiments on miniImageNet for few-shot image classification show that the bootstrapping of the parameters of the behavior policy leads to a higher exploration rate. The experimental results on Atari-57 show that bootstraps meta-learner parameters can improve the performance of the performance on a few tasks."
1665,SP:461ed47339e08dafea90a7c015d2f20e534daeb7,meta - learning algorithm USED-FOR meta - optimization algorithms. curvature CONJUNCTION limited evaluation. limited evaluation CONJUNCTION curvature. geometry FEATURE-OF meta - learner's objective. K - step horizon FEATURE-OF meta - objective. algorithm USED-FOR multi - task meta - learning. Atari ALE benchmark EVALUATE-FOR model - free agents. OtherScientificTerm is future learning dynamics. Metric is distance. ,"This paper proposes a meta-learning algorithm for meta-optimization algorithms. The main idea is to model the future learning dynamics as a function of the geometry of the meta-learner's objective, which is motivated by curvature and limited evaluation. The authors show that the K-step horizon of the proposed meta-objective can be bounded by the distance between the current state and the next state. The proposed algorithm is applied to multi-task meta-Learning and is shown to outperform model-free agents on the Atari ALE benchmark."
1666,SP:461ed47339e08dafea90a7c015d2f20e534daeb7,"meta - gradients ( MG ) USED-FOR task tuning meta - parameters. Bootstrapped meta - gradients ( BMG ) USED-FOR task tuning meta - parameters. meta - gradients ( MG ) USED-FOR Bootstrapped meta - gradients ( BMG ). meta-)parameterised update rule USED-FOR learner. ( meta-)parameterised update rule USED-FOR MG. matching function USED-FOR BMG. problem CONJUNCTION BMG. BMG CONJUNCTION problem. toy RL problem CONJUNCTION Atari RL test suite. Atari RL test suite CONJUNCTION toy RL problem. Atari RL test suite CONJUNCTION multi - task few - shot adaptation. multi - task few - shot adaptation CONJUNCTION Atari RL test suite. multi - task few - shot adaptation USED-FOR image recognition task. toy RL problem EVALUATE-FOR algorithm. Atari RL test suite EVALUATE-FOR algorithm. BMG COMPARE meta - learning baselines. meta - learning baselines COMPARE BMG. features FEATURE-OF BMG. epsilon PART-OF epsilon - greedy exploration. epsilon HYPONYM-OF update rule. epsilon - greedy exploration HYPONYM-OF update rule. epsilon - greedy exploration HYPONYM-OF behavioural parameters. epsilon HYPONYM-OF behavioural parameters. OtherScientificTerm are meta - parameters, learning dynamics, K - step parameters, parameterised update rule, MG updates, parameterised learning process, learners K - step parameters, and meta - learning horizon. Generic are approach, and it. Task is Atari. ","This paper proposes Bootstrapped meta-gradients (MG) for task tuning meta-parameters. The proposed approach is based on the idea that the learning dynamics of a learner can be decomposed into K-step parameters, and MG updates can be viewed as a (meta-)parameterised update rule for the learner. The key idea is to use a matching function between the original problem and BMG. The authors demonstrate the effectiveness of the proposed algorithm on a toy RL problem, an Atari RL test suite, and a multi-task few-shot adaptation to an image recognition task. In addition, the authors propose a new update rule, epsilon-greedy exploration, which replaces the standard behavioural parameters (epsilon in Atari) with MG updates. In the experiments, BMG is shown to outperform meta-learning baselines in terms of features and performance on Atari, and it is shown that the parameterised learning process is able to adapt to the learners K-steps parameters, which is an important factor in the success of BMG, as well as to the learning horizon."
1667,SP:49435d70bf8e16d5dbf34577cf8d3a5b21b1f25a,"model - based RL COMPARE model - free RL. model - free RL COMPARE model - based RL. MuZero HYPONYM-OF model - based RL. planning CONJUNCTION representation learning. representation learning CONJUNCTION planning. representation learning CONJUNCTION data diversity. data diversity CONJUNCTION representation learning. representation learning USED-FOR generalization of agents. data diversity USED-FOR generalization of agents. Q - learning agent COMPARE MuZero. MuZero COMPARE Q - learning agent. planning HYPONYM-OF Q - learning agent. planning CONJUNCTION self - supervised representation learning. self - supervised representation learning CONJUNCTION planning. self - supervised representation learning CONJUNCTION data diversity. data diversity CONJUNCTION self - supervised representation learning. self - supervised representation learning USED-FOR generalization. reconstruction HYPONYM-OF self - supervised representation learning. self - supervision USED-FOR generalization of MBRL agents. task generalization EVALUATE-FOR it. procedural environments FEATURE-OF generalization of MBRL agents. Method are MCTS, and MBRL agents. Material is Meta World benchmarks. ","This paper studies the effect of model-based RL (i.e., MuZero) compared to model-free RL (e.g., MCTS) on the generalization of MBRL agents. In particular, the authors compare the performance of a Q-learning agent (such as planning, representation learning, and data diversity) trained with self-supervision with a MuZero and show that the performance gap between the two models is not significant. The authors also show that a Q - learning agent trained with the same amount of supervision as MuZero can outperform MuZero in terms of task generalization performance, and that the same generalization can be achieved with more data diversity. In addition, they show that self-substituted representation learning such as reconstruction can improve generalization in the presence of self supervision. Finally, they conduct experiments on the Meta World benchmarks to show that it can improve task performance as well as generalization to new tasks. They also perform experiments on several procedural environments to evaluate the performance and generalization on the task of learning MBRL agent."
1668,SP:49435d70bf8e16d5dbf34577cf8d3a5b21b1f25a,"model - based agents COMPARE model - free agents. model - free agents COMPARE model - based agents. generalization ability EVALUATE-FOR model - based agents. generalization ability EVALUATE-FOR model - free agents. MuZero COMPARE model - free agents. model - free agents COMPARE MuZero. MuZero HYPONYM-OF generalization ability. MuZero HYPONYM-OF model - based agents. self - supervised representation learning CONJUNCTION procedural data diversity. procedural data diversity CONJUNCTION self - supervised representation learning. planning CONJUNCTION self - supervised representation learning. self - supervised representation learning CONJUNCTION planning. factors USED-FOR procedural generalization. procedural data diversity HYPONYM-OF factors. planning HYPONYM-OF factors. self - supervised representation learning HYPONYM-OF factors. factors USED-FOR task generalization. rich, procedural, multi - task environments USED-FOR self - supervised model - based agents. ","This paper investigates the generalization ability of model-based agents (i.e., MuZero) compared to model-free agents. The authors propose three factors to improve procedural generalization: planning, self-supervised representation learning, and procedural data diversity. They show that these factors can improve task generalization, and demonstrate that self supervised model-trained on rich, procedural, multi-task environments can generalize better."
1669,SP:49435d70bf8e16d5dbf34577cf8d3a5b21b1f25a,"MuZero agent USED-FOR tasks. ProcGen CONJUNCTION MetaWorld. MetaWorld CONJUNCTION ProcGen. MuZero COMPARE model - free methods. model - free methods COMPARE MuZero. ProcGen EVALUATE-FOR MuZero. ProcGen EVALUATE-FOR model - free methods. tree search USED-FOR action selection. MuZero CONJUNCTION tree search. tree search CONJUNCTION MuZero. MuZero USED-FOR value functions. auxiliary SSL objectives USED-FOR generalization. Generic are components, and they. OtherScientificTerm is self - supervision. ","This paper proposes a MuZero agent that can be used to solve tasks without self-supervision. The authors evaluate MuZero on ProcGen and MetaWorld and show that MuZero outperforms model-free methods. The main contribution of the paper is to combine MuZero with tree search for action selection, which is an extension of tree search. In addition, the authors propose two additional components: (1) the use of auxiliary SSL objectives to encourage generalization, and (2) using value functions learned by MuZero to approximate the value functions. In both cases, they are shown to outperform the state-of-the-art."
1670,SP:49435d70bf8e16d5dbf34577cf8d3a5b21b1f25a,"planning and model learning USED-FOR generalization. environments USED-FOR procedural generalization. procedural generalization CONJUNCTION task generalisation. task generalisation CONJUNCTION procedural generalization. Procgen HYPONYM-OF environments. reconstruction USED-FOR MueZero agent. procedural generalization EVALUATE-FOR MueZero agent. Meta - World USED-FOR task generalization tasks. Method is MuZero agent. OtherScientificTerm are reward structure, reward function, and data diversity dimension. ","This paper studies the problem of planning and model learning for generalization. The authors propose two environments, Procgen and Meta-World, to study procedural generalization and task generalisation. The MuZero agent is trained on reconstruction of the original dataset, and the MueZero agent achieves state-of-the-art performance in terms of procedural generalisation and task performance. The main contribution of the paper is to propose a new reward structure, which encourages the agent to learn a reward function that maximises the data diversity dimension. Experiments are conducted on task generalization tasks on Procgen, as well as on a subset of tasks from Meta-world."
1671,SP:ba80e35d452d894181d51624183b60541c0f3704,"graph diffusion process USED-FOR covariance matrix of signals. unroll proximal gradient iterations USED-FOR model. polynomial model CONJUNCTION unroll proximal gradient iterations. unroll proximal gradient iterations CONJUNCTION polynomial model. polynomial model USED-FOR model. synthetic datasets CONJUNCTION neuroimaging dataset. neuroimaging dataset CONJUNCTION synthetic datasets. supervised $ ( A_O, A_L)$ pairs FEATURE-OF synthetic datasets. approach COMPARE baseline approaches. baseline approaches COMPARE approach. OtherScientificTerm are functional connectivity graph, and structural connectivity graph. ","This paper proposes a graph diffusion process to estimate the covariance matrix of signals. The proposed model is based on a polynomial model with unroll proximal gradient iterations. The functional connectivity graph is modeled as a linear combination of the structural connectivity graph. Experiments on synthetic datasets with supervised $A_O, A_L)$ pairs and a neuroimaging dataset show that the proposed approach outperforms baseline approaches."
1672,SP:ba80e35d452d894181d51624183b60541c0f3704,solution USED-FOR graph structure. unrolling algorithm USED-FOR graph structure inference problem. proximal gradient iterative solutions USED-FOR It. unrolling algorithm USED-FOR It. Graph Deconvolution Network ( GDN ) HYPONYM-OF deep neural network. deep neural network USED-FOR expressiveness. ,This paper proposes a solution to infer graph structure. It is based on proximal gradient iterative solutions and uses an unrolling algorithm to solve the graph structure inference problem. The expressiveness is achieved by a deep neural network called Graph Deconvolution Network (GDN).
1673,SP:ba80e35d452d894181d51624183b60541c0f3704,"observed symmetric adjacency matrix USED-FOR graph structures. model USED-FOR graph structures. dataset EVALUATE-FOR model. OtherScientificTerm are observed adjacency matrix, and adjacency matrix. ",This paper proposes to learn graph structures from an observed symmetric adjacency matrix. The model is trained on a dataset and is able to learn a variety of graph structures. The main contribution of this paper is to propose to learn the model from the observation of the adjacence matrix. 
1674,SP:ba80e35d452d894181d51624183b60541c0f3704,approach USED-FOR graph structure recovery. graph deconvolutional network ( GDN ) HYPONYM-OF approach. graph deconvolutional network ( GDN ) USED-FOR graph structure recovery. noisy observed graph structures USED-FOR graph structure recovery. proximal gradient computation USED-FOR structure. graph adjacency FEATURE-OF observed adjacency matrix. proximal gradient computation USED-FOR It. synthetic and brain imaging graph recovery tasks EVALUATE-FOR method. method COMPARE baselines. baselines COMPARE method. synthetic and brain imaging graph recovery tasks EVALUATE-FOR baselines. ,This paper proposes a novel approach called graph deconvolutional network (GDN) for graph structure recovery from noisy observed graph structures. It uses proximal gradient computation to reconstruct the structure from the observed adjacency matrix. The proposed method is evaluated on synthetic and brain imaging graph recovery tasks and compared with several baselines.
1675,SP:91fd4189bf04aca4ccd1288ec8459e1edb29d378,Markov game USED-FOR reward shaping method. switching scheme USED-FOR reward shaping. toy tasks CONJUNCTION complex domains. complex domains CONJUNCTION toy tasks. ,This paper proposes a reward shaping method based on a Markov game. The authors propose a switching scheme for reward shaping. Experiments are conducted on toy tasks and complex domains.
1676,SP:91fd4189bf04aca4ccd1288ec8459e1edb29d378,algorithm USED-FOR sparse reward settings. algorithm USED-FOR optimal policy. controller CONJUNCTION shaper. shaper CONJUNCTION controller. learners USED-FOR algorithm. learners USED-FOR optimal policy. shaper HYPONYM-OF learners. controller HYPONYM-OF learners. controller USED-FOR environment reward signal. shaper USED-FOR potential - based reward shaping function. cost penalty USED-FOR reward feedback. reward feedback CONJUNCTION exploration bonus. exploration bonus CONJUNCTION reward feedback. controller's objective CONJUNCTION cost penalty. cost penalty CONJUNCTION controller's objective. cost penalty CONJUNCTION exploration bonus. exploration bonus CONJUNCTION cost penalty. randomly generated state function CONJUNCTION dot product. dot product CONJUNCTION randomly generated state function. dot product USED-FOR shapers reward function. randomly generated state function USED-FOR shapers reward function. algorithm USED-FOR sparse reward problems. ,"This paper proposes an algorithm for solving sparse reward settings using two learners, a controller and a shaper. The controller is trained to maximize the environment reward signal, while the shaper is used to learn a potential-based reward shaping function. In addition, the controller's objective and the cost penalty are used to provide reward feedback and exploration bonus. The shapers reward function is trained using a randomly generated state function and a dot product. Experiments show that the proposed algorithm can solve sparse reward problems."
1677,SP:91fd4189bf04aca4ccd1288ec8459e1edb29d378,"Controller USED-FOR RL task. Shaper agent USED-FOR Controller. Controller CONJUNCTION Shaper agent. Shaper agent CONJUNCTION Controller. Shaper agent HYPONYM-OF agents. Controller HYPONYM-OF agents. switching controls USED-FOR Shaper. Markov game approach USED-FOR stable training and convergence. total return FEATURE-OF Nash Equilibrium ( NE ). curiosity CONJUNCTION bi - level optimization. bi - level optimization CONJUNCTION curiosity. method COMPARE reward shaping approaches. reward shaping approaches COMPARE method. bi - level optimization HYPONYM-OF reward shaping approaches. curiosity HYPONYM-OF reward shaping approaches. those HYPONYM-OF reward shaping approaches. bi - level optimization USED-FOR those. curiosity USED-FOR those. OtherScientificTerm are Controller's reward function, and theoretical convergence. Generic is approach. ","This paper proposes to train two agents, a Controller and a Shaper agent, to solve an RL task. The Controller is trained by switching controls, while the Shaper is trained using a Markov game approach to ensure stable training and convergence. The authors show that the Controller's reward function converges to a Nash Equilibrium (NE) in terms of total return. The proposed method is compared to other reward shaping approaches, including those based on curiosity and bi-level optimization. The paper also provides theoretical convergence results for the proposed approach."
1678,SP:91fd4189bf04aca4ccd1288ec8459e1edb29d378,"ROSA HYPONYM-OF reward shaping method. Shaper ” policy USED-FOR reward bonuses. two - player Markov game USED-FOR problem. normal reward CONJUNCTION potential - based reward. potential - based reward CONJUNCTION normal reward. potential - based reward PART-OF Controller ’s reward. normal reward USED-FOR Controller ’s reward. switching function USED-FOR shaper ’s rewards. Shaper PART-OF potential - based reward shaping term. ROSA USED-FOR reward shaping. grid - world environment EVALUATE-FOR ROSA. Solaris CONJUNCTION Super Mario. Super Mario CONJUNCTION Solaris. RND CONJUNCTION PPO. PPO CONJUNCTION RND. PPO CONJUNCTION LIRPG. LIRPG CONJUNCTION PPO. ROSA COMPARE ICM. ICM COMPARE ROSA. ICM CONJUNCTION RND. RND CONJUNCTION ICM. ROSA COMPARE RND. RND COMPARE ROSA. PPO USED-FOR Gravitar. ROSA COMPARE LIRPG. LIRPG COMPARE ROSA. RND USED-FOR Gravitar. LIRPG USED-FOR Gravitar. Super Mario EVALUATE-FOR ROSA. Solaris EVALUATE-FOR ROSA. Gravitar EVALUATE-FOR ROSA. OtherScientificTerm are environment dynamics, Shaper actions, shapers ’ reward, count - based exploration bonus, curiosity - metric, RND exploration metric, and policy invariance. Method is Shaper and Controller policies. ","This paper proposes a new reward shaping method, called ROSA. The problem is formulated as a two-player Markov game, where the “Shaper” policy learns to maximize reward bonuses based on the environment dynamics. The Controller’s reward is a combination of a normal reward and a potential-based reward. The Shaper and Controller policies are trained separately. The switching function is used to optimize the shaper’’re rewards. In addition to the Shaper actions, the authors propose to add a “count-based exploration bonus” to the shapers’ reward, which is based on a curiosity-metric. The authors also propose a potential–based reward shaping term, which adds a Shaper to the potential-by-product of the RND exploration metric, which encourages policy invariance. The experimental results on a grid-world environment show that ROSA is effective at reward shaping. On Solaris, Super Mario, and Gravitar, ROSA outperforms ICM, RND, PPO, and LIRPG."
1679,SP:20abe4d70152590c3c44fcb50c5d0293e25874ff,robustness FEATURE-OF vertical federated learning. defense framework USED-FOR vertical federated learning. robust feature subspace recovery USED-FOR low rank matrices. robustness EVALUATE-FOR scheme. Generic is framework. ,This paper proposes a defense framework for vertical federated learning to improve the robustness. The framework is based on the idea of robust feature subspace recovery to deal with low rank matrices. Experiments show that the proposed scheme can improve the overall robustness of the network.
1680,SP:20abe4d70152590c3c44fcb50c5d0293e25874ff,robust vertical federated learning framework USED-FOR backdoor attacks. robust autoencoders USED-FOR clean global model. robust autoencoders USED-FOR backdoor features. purified training USED-FOR robust autoencoders. OtherScientificTerm is uncorrupted features. ,"This paper proposes a robust vertical federated learning framework to defend against backdoor attacks. The key idea is to train robust autoencoders to extract backdoor features from uncorrupted features, and then train a clean global model using the purified training."
1681,SP:20abe4d70152590c3c44fcb50c5d0293e25874ff,"adversarial attacks FEATURE-OF federated learning. method USED-FOR backdoor attacks. backdoor attacks FEATURE-OF vertical federated learning ” setting. RVFR HYPONYM-OF method. Generic is proposal. OtherScientificTerm are adversarial scenarios, and Unclear threat model. Method is ICLR. ","This paper studies the problem of adversarial attacks in federated learning. The authors propose a proposal called RVFR, which is a method that can defend against backdoor attacks in the “vertical Federated learning” setting. The main idea of the proposal is to model the adversarial scenarios as a “Unclear threat model” (ULR). Experiments on ICLR show that the proposed RVFR is effective."
1682,SP:20abe4d70152590c3c44fcb50c5d0293e25874ff,robust vertical federated learning framework USED-FOR backdoor attacks. RVFR USED-FOR backdoor attacks. RVFR HYPONYM-OF robust vertical federated learning framework. robust auto - encoder USED-FOR features. purified features USED-FOR global model. threat models USED-FOR feature recovery. RVFR USED-FOR backdoor attacks. RVFR COMPARE baselines. baselines COMPARE RVFR. Generic is framework. Method is local feature extractors. OtherScientificTerm is embedded features. ,"This paper proposes a robust vertical federated learning framework, RVFR, to defend against backdoor attacks. The framework consists of two parts: 1) local feature extractors that extract the embedded features and 2) a robust auto-encoder that recovers the features. The purified features are then used to train a global model. The feature recovery is carried out using threat models. Experiments show that RVFR is able to defend backdoor attacks against several baselines."
1683,SP:3fbc5ebb4c598e849b3ecbb2886289e20bf1ea14,"Siamese Transformer encoders USED-FOR zero - shot dense retrieval problem. contrastive learning loss function USED-FOR encoders. ICT USED-FOR data augmentations. Task are zero - shot retrieval benchmark, and few - shot evaluation. ","This paper proposes Siamese Transformer encoders to solve the zero-shot dense retrieval problem. The authors propose a novel contrastive learning loss function to train the encoder, which can be used to improve the performance of the encodes. The paper also proposes a zero -shot retrieval benchmark, which is based on few-shot evaluation. The main contribution of the paper is the use of ICT for data augmentations."
1684,SP:3fbc5ebb4c598e849b3ecbb2886289e20bf1ea14,cropping strategy USED-FOR neural model. neural model USED-FOR IR. It COMPARE BM25. BM25 COMPARE It. It COMPARE neural models. neural models COMPARE It. BERT CONJUNCTION ones. ones CONJUNCTION BERT. BM25 COMPARE neural models. neural models COMPARE BM25. inverse cloze task EVALUATE-FOR ones. ones HYPONYM-OF neural models. BERT HYPONYM-OF neural models. cropping USED-FOR model. model USED-FOR IR tasks. task COMPARE pre - training strategies. pre - training strategies COMPARE task. task COMPARE IR. IR COMPARE task. IR COMPARE pre - training strategies. pre - training strategies COMPARE IR. zero - shot retrieval CONJUNCTION MSMARCO. MSMARCO CONJUNCTION zero - shot retrieval. zero - shot retrieval EVALUATE-FOR method. method COMPARE methods. methods COMPARE method. method COMPARE BM25. BM25 COMPARE method. MSMARCO EVALUATE-FOR method. BM25 HYPONYM-OF methods. method COMPARE BM25. BM25 COMPARE method. fine - tuning USED-FOR method. ,"This paper proposes a cropping strategy to train a neural model for IR. It outperforms BM25 and other neural models on the inverse cloze task. The authors also show that cropping can improve the model's performance on other IR tasks. Finally, the authors compare the proposed task with other pre-training strategies and show that IR is superior to BM25. The proposed method is evaluated on zero-shot retrieval and MSMARCO and compared to other methods such as BM25 with fine-tuning."
1685,SP:3fbc5ebb4c598e849b3ecbb2886289e20bf1ea14,"contrastive learning approach USED-FOR unsupervised text retrieval. MoCo algorithm USED-FOR contrastive learning. contrastive learning USED-FOR model. approach COMPARE BM25 algorithm. BM25 algorithm COMPARE approach. BM25 algorithm USED-FOR zero - shot retrieval. BEIR benchmark EVALUATE-FOR zero - shot retrieval. zero - shot retrieval EVALUATE-FOR approach. BEIR benchmark EVALUATE-FOR BM25 algorithm. BEIR benchmark EVALUATE-FOR approach. training data CONJUNCTION data augmentations. data augmentations CONJUNCTION training data. Method is Inverse Cloze Task ( ICT ) approach. Task are independent cropping, and model training. ","This paper proposes a contrastive learning approach for unsupervised text retrieval. The Inverse Cloze Task (ICT) approach is motivated by the observation that independent cropping can be problematic for model training. To address this issue, the authors propose a model that uses the MoCo algorithm for contrastive learn. The proposed approach is evaluated on zero-shot retrieval on the BEIR benchmark and compared to the BM25 algorithm. Experiments are conducted on both training data and data augmentations."
1686,SP:3fbc5ebb4c598e849b3ecbb2886289e20bf1ea14,contrastive learning - based method USED-FOR zero - shot dense IR. positive pair construction methods CONJUNCTION negative pair construction methods. negative pair construction methods CONJUNCTION positive pair construction methods. contrastive learning framework CONJUNCTION positive pair construction methods. positive pair construction methods CONJUNCTION contrastive learning framework. IR ( both sparse and dense ) baselines USED-FOR zero - shot setting. Material is BEIR benchmark. ,"This paper proposes a contrastive learning-based method for zero-shot dense IR. The authors propose a new contrastive multi-modal learning framework, as well as positive pair construction methods and negative pair reconstruction methods. The experiments are conducted on BEIR benchmark and compared with several IR (both sparse and dense) baselines for the zero-shooting setting."
1687,SP:ed4e2896dc882bd089f420f719da232d706097c5,"strategies USED-FOR fine - tuning. strategies USED-FOR in- and out - of - distribution performance. linear probing CONJUNCTION end - to - end fine - tuning. end - to - end fine - tuning CONJUNCTION linear probing. linear probing CONJUNCTION end - to - end fine - tuning. end - to - end fine - tuning CONJUNCTION linear probing. end - to - end fine - tuning CONJUNCTION two - stage approach. two - stage approach CONJUNCTION end - to - end fine - tuning. end - to - end fine - tuning USED-FOR in - distribution. it COMPARE linear probing. linear probing COMPARE it. end - to - end fine - tuning USED-FOR pre - trained features. CIFAR CONJUNCTION WILDS - FMoW. WILDS - FMoW CONJUNCTION CIFAR. CIFAR HYPONYM-OF datasets. WILDS - FMoW HYPONYM-OF datasets. OtherScientificTerm is linear layer. Generic is model. Method are two - layer networks, mitigation strategy, and two - stage fine - tuning approach. Task is out - of - distribution. ","This paper proposes strategies for fine-tuning to improve both in- and out-of-distribution performance. The main idea is to use two-layer networks, where the first linear layer is used to fine-tune the model and the second one is used as a mitigation strategy. The authors propose to combine the linear probing, end-to-end fine -tuning, and a two-stage approach. The experiments are conducted on two datasets, CIFAR and WILDS-FMoW. The results show that the two-staging fine-training approach is effective, and that it is more effective than linear probing and end-trying to learn pre-trained features. The paper also shows that in-distortion, the in-tuning is better than the out-tweaking."
1688,SP:ed4e2896dc882bd089f420f719da232d706097c5,ID CONJUNCTION OOD. OOD CONJUNCTION ID. fine - tuning CONJUNCTION linear probing. linear probing CONJUNCTION fine - tuning. LP - FT HYPONYM-OF two - step variant. linear probing HYPONYM-OF methods. fine - tuning HYPONYM-OF methods. LP - FT COMPARE FT. FT COMPARE LP - FT. FT CONJUNCTION LP. LP CONJUNCTION FT. LP - FT COMPARE LP. LP COMPARE LP - FT. LP - FT USED-FOR ID and OOD tests. Method is pre - trained model. ,"This paper proposes a two-step variant of LP-FT, which is a generalization of LP that can be used for both ID and OOD. The two methods are fine-tuning and linear probing. The authors show that the pre-trained model can generalize better than LP and FT. The experiments show that LP-TF outperforms FT and LP in ID and OD tests."
1689,SP:ed4e2896dc882bd089f420f719da232d706097c5,"fine - tuning CONJUNCTION linear probing. linear probing CONJUNCTION fine - tuning. ID / OOD EVALUATE-FOR linear probing. fine - tuning ( FT ) COMPARE linear probing ( LP ) ID. linear probing ( LP ) ID COMPARE fine - tuning ( FT ). FT COMPARE LP. LP COMPARE FT. fine - tuning USED-FOR features. features CONJUNCTION linear layer. linear layer CONJUNCTION features. it COMPARE LP. LP COMPARE it. OOD CONJUNCTION ID. ID CONJUNCTION OOD. LP CONJUNCTION FT. FT CONJUNCTION LP. it COMPARE FT. FT COMPARE it. ID EVALUATE-FOR FT. OOD EVALUATE-FOR FT. ID EVALUATE-FOR it. OOD EVALUATE-FOR it. Method are full fine - tuning, and LP - FT. Generic is method. ","This paper compares the performance of fine-tuning (FT) and linear probing (LP) ID on ID/OOD. The main finding is that FT is more effective than LP in terms of features and linear layer. The paper then proposes a method called LP-FT, which is a combination of full fine-tune and LP. The experiments show that it outperforms LP and FT on ID, OOD, etc."
1690,SP:ed4e2896dc882bd089f420f719da232d706097c5,"OOD data EVALUATE-FOR linear probing. fine - tuning USED-FOR feature representations. linear probing USED-FOR classification head. classification head USED-FOR fine - tuning. Method is model fine - tuning. OtherScientificTerm are distribution shift, inner distribution, and inner distributions. ","This paper studies the problem of model fine-tuning in the presence of distribution shift. The authors show that linear probing on OOD data can be used to fine-fine-tune the feature representations of a classification head using linear probing. In particular, they show that if the inner distribution of the classification head is shifted to the same direction as the outer distribution, then the inner distributions of the outer and inner distributions are similar."
1691,SP:96f4f90488c15167d85261a883cd70fc15e06bb9,known - class dataset USED-FOR task. K - epsilon separation HYPONYM-OF concept. sampling process CONJUNCTION Assumption ( D ). Assumption ( D ) CONJUNCTION sampling process. Task is Learning to discover novel class. OtherScientificTerm is known classes. Generic is setting. ,Learning to discover novel class is a challenging task that requires a known-class dataset. This paper proposes a new setting where the known classes are replaced by a new concept called K-epsilon separation. The sampling process and Assumption (D) are also discussed.
1692,SP:96f4f90488c15167d85261a883cd70fc15e06bb9,"meta - discovery USED-FOR meta - learning strategies. sampling strategy USED-FOR clustering procedure. Generic are problem, and competitors. OtherScientificTerm is dominant "" view. ","This paper studies the problem of meta-discovery in meta-learning strategies. The authors propose a sampling strategy for the clustering procedure, where the goal is to minimize the distance between the ""dominant"" view and the competitors. "
1693,SP:96f4f90488c15167d85261a883cd70fc15e06bb9,meta - learning - based approach USED-FOR setting. Task is novel class discovery ( NCD ). Method is NCD. Material is unlabeled data. OtherScientificTerm is semantic features. ,This paper studies novel class discovery (NCD) in the setting of unlabeled data. The authors propose a meta-learning-based approach for this setting. The main idea of NCD is to learn a set of labels for each class based on its semantic features. 
1694,SP:96f4f90488c15167d85261a883cd70fc15e06bb9,baselines CONJUNCTION ablation. ablation CONJUNCTION baselines. ablation USED-FOR methods. baselines USED-FOR methods. clustering - centric definition of the problem USED-FOR similarity learning methods. Task is L2DNC task. Generic is approaches. ,This paper studies the L2DNC task and proposes a clustering-centric definition of the problem for similarity learning methods. The proposed methods are compared with several baselines and ablation. The results show that the proposed approaches outperform the baselines.
1695,SP:262a5aaa4e675b2aac6bd14d3aa007bf411ce550,"POMDP setting FEATURE-OF causal model. causal inference problem USED-FOR model - based reinforcement learning. offline data USED-FOR regularizer. regularizer USED-FOR learning. offline data USED-FOR learning. OtherScientificTerm are learning agent, offline experiences, and privileged information. Task is toy problems. ","This paper studies model-based reinforcement learning as a causal inference problem, where the causal model is trained in a POMDP setting. The learning agent is given access to a set of offline experiences, and is encouraged to use these offline experiences as a regularizer during learning. The authors conduct extensive experiments on toy problems, and show that the offline data can be used as a good regularizer for learning, especially when privileged information is available."
1696,SP:262a5aaa4e675b2aac6bd14d3aa007bf411ce550,offline and online data USED-FOR transition model. causal perspective FEATURE-OF POMDP problem. deconfounding USED-FOR transition model. generalization guarantees EVALUATE-FOR method. synthetic toy problems EVALUATE-FOR method. ,This paper studies the POMDP problem from a causal perspective and proposes a transition model based on both offline and online data. The transition model is trained using deconfounding. The method is evaluated on synthetic toy problems and the authors provide generalization guarantees.
1697,SP:262a5aaa4e675b2aac6bd14d3aa007bf411ce550,state information FEATURE-OF offline learner. observational data HYPONYM-OF offline data. interventional data HYPONYM-OF online data. privileged POMDP USED-FOR offline data. unobserved confounder HYPONYM-OF state information. augmented learning procedure USED-FOR policy. offline data USED-FOR method. Material is offline and online data. ,"This paper considers the problem of learning from offline data (i.e., observational data) without access to the state information of the offline learner (e.g., unobserved confounder). The authors propose a privileged POMDP for offline data, where the offline data is interventional data and the online data is unsupervised. The authors then propose an augmented learning procedure to learn a policy that is robust to both offline data and online data."
1698,SP:262a5aaa4e675b2aac6bd14d3aa007bf411ce550,"interventional distributions FEATURE-OF partially observed Markov decision process ( POMDP ). system dynamics HYPONYM-OF evaluating interventional distributions. finite horizon FEATURE-OF POMDP. unbiased estimator USED-FOR evaluating system dynamics. experimental data USED-FOR unbiased estimator. OtherScientificTerm are randomized experiments, latent state, observational distribution, unobserved confounding, and unknown system dynamics. ","This paper considers the problem of evaluating interventional distributions of a partially observed Markov decision process (POMDP) in the context of evaluating system dynamics. The POMDP has a finite horizon, and randomized experiments are performed to estimate the latent state of the system. The authors propose an unbiased estimator of the latent and the observational distribution, which is based on experimental data, and is robust to unobserved confounding. The paper also provides a theoretical analysis of the effect of unknown system dynamics on the estimator."
1699,SP:bcb4e7e5c137edf04a9ea2fde014b0984c6ef89b,"free - form QA CONJUNCTION knowledge - grounded dialogue. knowledge - grounded dialogue CONJUNCTION free - form QA. retrieval component PART-OF retrieval - augmented systems. posterior signal USED-FOR retrieval component. ELBo loss USED-FOR signal. model COMPARE baselines. baselines COMPARE model. Generic are models, and it. ",This paper studies the problem of learning a retrieval component in retrieval-augmented systems in the context of free-form QA and knowledge-grounded dialogue. The authors propose to use a posterior signal as the retrieval component. The signal is modeled using ELBo loss and the authors show that the proposed model outperforms the baselines by a large margin. They also show that their models can generalize to unseen tasks.
1700,SP:bcb4e7e5c137edf04a9ea2fde014b0984c6ef89b,"retrieval USED-FOR document grounded response generator. model USED-FOR supervised ) retrieval network. posterior CONJUNCTION prior. prior CONJUNCTION posterior. posterior CONJUNCTION prior. prior CONJUNCTION posterior. expectation USED-FOR ELBOLoss. approximation USED-FOR ELBOLoss. approximation USED-FOR open - ended response generation. posterior network USED-FOR variational training. BART USED-FOR language generation. networks USED-FOR language generation. language generation CONJUNCTION CoLBERT. CoLBERT CONJUNCTION language generation. BART USED-FOR networks. Wizard of Wikipedia dataset CONJUNCTION MSMARCO NLGEN. MSMARCO NLGEN CONJUNCTION Wizard of Wikipedia dataset. success@k CONJUNCTION MRR. MRR CONJUNCTION success@k. retrieval CONJUNCTION response generation. response generation CONJUNCTION retrieval. response generation EVALUATE-FOR baseline model. retrieval EVALUATE-FOR baseline model. Generic are approach, and it. OtherScientificTerm are dialog context, top - r retrieved documents, alpha, sampling proportion, and response. Metric is MarginalizedLoss. ","This paper proposes a document grounded response generator based on retrieval. The proposed approach is motivated by the observation that the current model is not suitable as a (supervised) retrieval network. The authors propose to use the expectation of ELBOLoss as an approximation for open-ended response generation. The posterior network is used for variational training, where the posterior and the prior are sampled from the same dialog context, and the posterior is used to generate the top-r retrieved documents. The networks are trained using BART for language generation and CoLBERT, and evaluated on the Wizard of Wikipedia dataset and MSMARCO NLGEN. Results on success@k and MRR show that the proposed model outperforms the baseline model in terms of retrieval and response generation, and that it is also able to achieve better MarginalizedLoss. "
1701,SP:bcb4e7e5c137edf04a9ea2fde014b0984c6ef89b,"retrieval CONJUNCTION generator. generator CONJUNCTION retrieval. posterior - guide HYPONYM-OF model. ELBo USED-FOR models. posterior - guide COMPARE retriever. retriever COMPARE posterior - guide. It USED-FOR generator. retriever COMPARE posterior - guide. posterior - guide COMPARE retriever. Wikipedia CONJUNCTION NLGen. NLGen CONJUNCTION Wikipedia. relevance CONJUNCTION groundedness. groundedness CONJUNCTION relevance. groundedness CONJUNCTION generation quality. generation quality CONJUNCTION groundedness. model COMPARE baseline retriever. baseline retriever COMPARE model. generation quality HYPONYM-OF evaluation metrics. relevance HYPONYM-OF evaluation metrics. groundedness EVALUATE-FOR model. generation quality EVALUATE-FOR model. groundedness HYPONYM-OF evaluation metrics. relevance EVALUATE-FOR model. evaluation metrics EVALUATE-FOR model. evaluation metrics EVALUATE-FOR baseline retriever. Task are knowledge - grounded open - ended generation, generation, and short answer generation. OtherScientificTerm is supervision. Material is distant supervision data. ","This paper addresses the problem of knowledge-grounded open-ended generation, where the retrieval and generator are trained using ELBo. The authors propose a model called posterior-guide, which is similar to the retriever, but does not require supervision. It can be used in conjunction with the generator to improve the generation performance. Experiments are conducted on Wikipedia and NLGen, where distant supervision data is used for short answer generation. The proposed model is evaluated on three evaluation metrics: relevance, groundedness, and generation quality, and compared to a baseline retriever."
1702,SP:bcb4e7e5c137edf04a9ea2fde014b0984c6ef89b,"generator CONJUNCTION guide - retriever. guide - retriever CONJUNCTION generator. KL divergence PART-OF loss function. retriever CONJUNCTION guide - retriever. guide - retriever CONJUNCTION retriever. guide - retriever USED-FOR retriever. generator USED-FOR retriever. Task are knowledge grounded text generation tasks, and training. Generic is model. ","This paper addresses the problem of knowledge grounded text generation tasks. The authors propose to train a retriever with a generator and a guide-retriever, where the retriever is trained jointly with the generator and the guide-referrer. The loss function consists of a KL divergence between the generated text and the target text. The model is trained in a supervised fashion, where training is performed on a small number of examples."
1703,SP:bec15075409c71f98f3698bc35e34eeb4862d94f,neural network model - based approaches USED-FOR influence maximization ( IM ). GLIE USED-FOR influence. GNN USED-FOR optimization algorithms. GNN USED-FOR GLIE. CELF HYPONYM-OF optimization algorithms. GNN USED-FOR influence. GRIM USED-FOR influence. two - layer MLP USED-FOR marginal gain. PUN USED-FOR influence. GNN hidden states USED-FOR features. features USED-FOR influence. GNN hidden states USED-FOR influence. method USED-FOR graphs. solution quality EVALUATE-FOR PUN. ,"This paper proposes neural network model-based approaches for influence maximization (IM) in the context of graphs. In particular, the authors propose GLIE, which uses a GNN to estimate influence for optimization algorithms such as CELF and GRIM. The marginal gain is estimated using a two-layer MLP. The authors also propose PUN, which estimates influence using GNN hidden states as features. The proposed method can be applied to graphs with multiple nodes and is shown to improve the solution quality."
1704,SP:bec15075409c71f98f3698bc35e34eeb4862d94f,"learning - based method USED-FOR Influence Maximization. learning - based method USED-FOR Influence Estimation. Influence Estimation CONJUNCTION Influence Maximization. Influence Maximization CONJUNCTION Influence Estimation. RL DQN based method CONJUNCTION influence function. influence function CONJUNCTION RL DQN based method. Method are GNN - based, and CELF optimization. Material is synthetic and real - world datasets. ","This paper proposes a learning-based method for Influence Estimation and Influence Maximization. The main idea is to combine an RL DQN based method with an influence function, which is GNN-based. The authors also propose a CELF optimization. Experiments are conducted on both synthetic and real-world datasets."
1705,SP:bec15075409c71f98f3698bc35e34eeb4862d94f,learning methods USED-FOR influence maximization problem. Q - learning CONJUNCTION greedy algorithm. greedy algorithm CONJUNCTION Q - learning. graph neural networks USED-FOR seed nodes. greedy algorithm USED-FOR seed nodes. learned representation USED-FOR greedy algorithm. Q - learning USED-FOR seed nodes. datasets EVALUATE-FOR influence estimation. datasets EVALUATE-FOR influence maximization. influence estimation CONJUNCTION influence maximization. influence maximization CONJUNCTION influence estimation. ,This paper studies the influence maximization problem using learning methods. The authors propose to use graph neural networks to learn seed nodes using Q-learning and a greedy algorithm based on the learned representation. Experiments are conducted on two datasets for the influence estimation and influence minimization.
1706,SP:bec15075409c71f98f3698bc35e34eeb4862d94f,Influence Estimation method USED-FOR Influence Maximization. Influence Estimation method USED-FOR methods. method COMPARE baselines. baselines COMPARE method. Method is neural network approach ( GLIE ). OtherScientificTerm is graph. ,This paper proposes a neural network approach (GLIE) to estimate the influence of each node in a graph. The proposed methods are based on the Influence Estimation method for Influence Maximization. The experimental results show that the proposed method outperforms the baselines.
1707,SP:0c55b1f5e544e1e9510a12981107ae6c9f1eeb2e,"k - medoid solution USED-FOR active learning. k - medoid solution USED-FOR domain adaptation. active learning USED-FOR domain adaptation. labeling function USED-FOR source domain. Rademacher average CONJUNCTION localized discrepancy. localized discrepancy CONJUNCTION Rademacher average. localized discrepancy USED-FOR general loss functions. Rademacher average USED-FOR Generalization bounds. localized discrepancy USED-FOR Generalization bounds. OtherScientificTerm are hypothesis space, and labeling functions. Generic is bounds. Task is k - medoid problem. ",This paper proposes a k-medoid solution to the problem of active learning for domain adaptation. The main idea is to learn a labeling function for the source domain and use that labeling function to learn the target domain. Generalization bounds are derived based on the Rademacher average and localized discrepancy for general loss functions. These bounds are then extended to the case where the hypothesis space is k-dimensional and the labeling functions are k-modal. The paper also provides a theoretical analysis of the k-milder problem.
1708,SP:0c55b1f5e544e1e9510a12981107ae6c9f1eeb2e,"discrepancy term PART-OF upper bound. accelerated algorithm USED-FOR problem. K - medoids problem USED-FOR clustering. K - medoids problem USED-FOR problem. method USED-FOR domain shift. benchmark data EVALUATE-FOR method. Method are active learning method, and active learning algorithm. Task are domain adaptation problems, and domain adaptation. OtherScientificTerm are localized discrepancy, and hypothesithes. Metric is expected risk. Generic are discrepancy, and it. ","This paper proposes an active learning method for domain adaptation problems. The problem is formulated as an accelerated algorithm to solve the problem using a K-medoids problem for clustering. The authors propose an upper bound on the discrepancy term in the upper bound, which is a measure of localized discrepancy. The discrepancy is defined as the difference between the expected risk and the true risk, and the authors show that it is a function of the number of hypothesithes. The proposed active learning algorithm is evaluated on benchmark data and the proposed method is shown to be effective for domain shift."
1709,SP:0c55b1f5e544e1e9510a12981107ae6c9f1eeb2e,active learning USED-FOR domain adaptation. localized discrepancy COMPARE discrepancy measures. discrepancy measures COMPARE localized discrepancy. localized discrepancy USED-FOR generalization bound. Task is selection of a query target sample set. Generic is algorithm. ,"This paper studies the problem of active learning for domain adaptation. Specifically, the authors consider the selection of a query target sample set. The authors propose an algorithm that uses the localized discrepancy instead of discrepancy measures to derive a generalization bound."
1710,SP:0c55b1f5e544e1e9510a12981107ae6c9f1eeb2e,active learning USED-FOR domain adaption. active learning USED-FOR Lipschitz functions. accelerated K - medoid algorithm USED-FOR distance. Lipschitzness USED-FOR localized discrepancy. X domain FEATURE-OF distance measure. distance measure USED-FOR localized discrepancy. theoretical guarantees COMPARE ones. ones COMPARE theoretical guarantees. algorithm COMPARE ones. ones COMPARE algorithm. ,"This paper studies the problem of domain adaption using active learning for Lipschitz functions. The authors propose an accelerated K-medoid algorithm to estimate the distance between two points in an X domain. They show that the distance measure in the X domain can be estimated using Lipsschitzness, which can be used to estimate localized discrepancy. They also provide theoretical guarantees that are better than the existing ones. Finally, they compare the proposed algorithm with existing ones on several datasets."
1711,SP:f63c10ba7d6f5ef1c167faa8a221b3ab5cc06006,generalised gamma mean - field distribution USED-FOR variational inference. normalising flow USED-FOR desingularization map. normalising flow FEATURE-OF generalised gamma mean - field distribution. approximate posterior CONJUNCTION desingularization map. desingularization map CONJUNCTION approximate posterior. log normalised evidence USED-FOR variational inference. tighter bound USED-FOR variational inference. tighter bound USED-FOR log normalised evidence. approximate posterior USED-FOR tighter bound. Method is singular learning theory. OtherScientificTerm is Bayesian Neural Network literature. Metric is confidence score. Generic is theory. ,"This paper proposes to use a generalised gamma mean-field distribution with a normalising flow as a desingularization map for variational inference. The authors derive a tighter bound for the log normalised evidence of variational inferences based on the approximate posterior and the corresponding Descent of the Bayesian Neural Network literature. The paper is motivated by the singular learning theory, where the confidence score is a function of the number of samples. The theory is well-motivated and well-written."
1712,SP:f63c10ba7d6f5ef1c167faa8a221b3ab5cc06006,log - probability CONJUNCTION log - probability. log - probability CONJUNCTION log - probability. log - probability CONJUNCTION model. model CONJUNCTION log - probability. log - probability CONJUNCTION model. model CONJUNCTION log - probability. log - probability HYPONYM-OF approximation of the log - ratio. Laplacian approximation USED-FOR normalized evidence. method USED-FOR Laplacian approximation. mean - field distribution CONJUNCTION resolution map. resolution map CONJUNCTION mean - field distribution. normalizing flow USED-FOR transformation. OtherScientificTerm is log - ratio. Generic is approximation. ,"This paper proposes an approximation of the log-ratio, i.e., a log-probability, log-predictive model, and a model that takes as input both the mean-field distribution and the resolution map. The authors propose a method to approximate the Laplacian approximation of normalized evidence, which is an approximation to the true log-rater. The main idea is to use a normalizing flow to approximate this transformation. The approximation is shown to be optimal."
1713,SP:f63c10ba7d6f5ef1c167faa8a221b3ab5cc06006,mean - field variational approximation USED-FOR ELBO. approximation family USED-FOR mean - field variational approximation. Gamma distribution USED-FOR approximation family. Gamma distribution USED-FOR source distribution. Gamma distribution USED-FOR normalizing flow. source distribution FEATURE-OF normalizing flow. ELBO EVALUATE-FOR source distribution. normal distribution USED-FOR source distribution. OtherScientificTerm is dimensionality d of the parameter space. ,"This paper proposes a new approximation family for the mean-field variational approximation to the ELBO, based on the Gamma distribution. In particular, the authors show that a normalizing flow with a Gamma distribution is equivalent to a source distribution with a normal distribution. The authors also show that the source distribution can be approximated with the same ELBO as the normal distribution, but with a different dimensionality d of the parameter space."
1714,SP:f63c10ba7d6f5ef1c167faa8a221b3ab5cc06006,"variational algorithm USED-FOR posterior distribution. asymptotic expression USED-FOR ELBO. Method are Bayesian neural networks, affine coupling network, and Gaussian and generalized gamma approximating families. OtherScientificTerm are model singularities, normalizing flow, and desingularized parameter space. ","This paper studies Bayesian neural networks. The authors propose a variational algorithm for approximating the posterior distribution of the posterior of an affine coupling network. The main contribution of this paper is to provide an asymptotic expression for the ELBO of the model singularities, which is derived from the affine bonding network. This is achieved by using Gaussian and generalized gamma approximating families, and by using a normalizing flow to approximate the parameters of the normalized flow. The paper also provides a theoretical analysis of the desingularized parameter space."
1715,SP:9ba33d09bd68d8598e2aff428ecca5060922a4dc,average - case formulation USED-FOR DG. complexity CONJUNCTION out - of - distribution ( OOD ). out - of - distribution ( OOD ) CONJUNCTION complexity. learning - theoretic bound USED-FOR average - case formulation. trade - off FEATURE-OF shallow neural networks. trade - offs FEATURE-OF linear models. Task is domain generalization ( DG ). Method is predictors. ,This paper studies the problem of domain generalization (DG). The authors propose an average-case formulation for DG based on a learning-theoretic bound on the complexity and out-of-distribution (OOD) of the predictors. The authors show that the trade-off of shallow neural networks is similar to that of linear models.
1716,SP:9ba33d09bd68d8598e2aff428ecca5060922a4dc,ERM COMPARE DG methods. DG methods COMPARE ERM. out - of - domain accuracy EVALUATE-FOR DG methods. ERM USED-FOR domain generalization. out - of - domain accuracy EVALUATE-FOR ERM. model complexity USED-FOR ood generalization. DG methods USED-FOR generalizing models. cross - domain validation USED-FOR generalizing models. cross - domain validation USED-FOR DG methods. ,This paper studies the effect of ERM on domain generalization and shows that ERM improves the out-of-domain accuracy compared to DG methods. The authors argue that model complexity plays a key role in ood generalization. They also show that cross-domain validation improves the performance of DG methods for generalizing models with cross-domains.
1717,SP:9ba33d09bd68d8598e2aff428ecca5060922a4dc,theory USED-FOR domain generalization. training loss CONJUNCTION model complexity. model complexity CONJUNCTION training loss. statistical learning theory ( Rademacher complexity ) USED-FOR theory. model selection USED-FOR complexity control. domain - wise cross - validation USED-FOR model selection strategy. hyper - parameter search COMPARE domain - wise cross - validation. domain - wise cross - validation COMPARE hyper - parameter search. DomainBed benchmark EVALUATE-FOR method. Metric is Rademacher complexity. ,This paper proposes a new theory for domain generalization based on statistical learning theory (Rademacher complexity). The main idea is to use the Rademacher complex between the training loss and the model complexity as a measure of model selection for complexity control. The proposed method is evaluated on the DomainBed benchmark and compared with hyper-parameter search and domain-wise cross-validation as a model selection strategy.
1718,SP:9ba33d09bd68d8598e2aff428ecca5060922a4dc,complexity control strategy USED-FOR bias - variance trade - off. domain - wise validation USED-FOR model complexity. optimization USED-FOR within - domain performance. Regularisation COMPARE optimization. optimization COMPARE Regularisation. Regularisation USED-FOR within - domain performance. OtherScientificTerm is DG algorithm performance variability. ,This paper proposes a complexity control strategy to balance the bias-variance trade-off. The main idea is to use domain-wise validation to reduce the model complexity. Regularisation is shown to improve the within-domain performance compared to standard optimization. The DG algorithm performance variability is also studied.
1719,SP:b1f622cbc827e880f98de9e99eca498584efe011,"look - ahead tie breaking FEATURE-OF maiximization of marginal gains. maiximization of marginal gains USED-FOR greedy approach. problem USED-FOR vaccine design. ILP approach USED-FOR covid 19 vaccine. greedy approach CONJUNCTION ILP approach. ILP approach CONJUNCTION greedy approach. greedy approach USED-FOR covid 19 vaccine. toy data sets EVALUATE-FOR problem. Task is multi set multi cover problem. OtherScientificTerm are overlays, objective function, n - coverage, and marginal gain maximization objective. Method is ILP formulation. ","This paper studies the multi set multi cover problem, where the goal is to minimize the number of overlays. This problem has been studied in the context of vaccine design. The authors propose a greedy approach based on the look-ahead tie breaking and an ILP approach for the covid 19 vaccine. The main contribution of the paper is the formulation of the objective function. The ILP formulation is motivated by the observation that the n-coverage of the target is a function of the marginal gain maximization objective. Experiments on toy data sets demonstrate the effectiveness of the proposed problem."
1720,SP:b1f622cbc827e880f98de9e99eca498584efe011,set covering problem USED-FOR peptide - based vaccine design. optimization problem USED-FOR peptide - based vaccine design. greedy algorithm USED-FOR optimization. approach COMPARE formulations. formulations COMPARE approach. Generic is problem. Task is set covering problems. ,This paper studies the set covering problem in peptide-based vaccine design as an optimization problem. The problem is motivated by the observation that set covering problems are difficult to solve. The authors propose a greedy algorithm to solve the optimization. The experimental results show that the proposed approach outperforms existing formulations.
1721,SP:b1f622cbc827e880f98de9e99eca498584efe011,"application USED-FOR design of COVID vaccines. it USED-FOR design of COVID vaccines. it USED-FOR application. ML pipeline PART-OF application. it COMPARE greedy method. greedy method COMPARE it. artificial data EVALUATE-FOR algorithm. Task are n - times coverage problem, set - cover, combinatorial optimization problem, and vaccine design. Generic is problem. Method is greedy algorithm. ","This paper proposes an application to the design of COVID vaccines, where the ML pipeline is incorporated into the application and it is used to solve the n-times coverage problem. The problem is formulated as a set-cover, which is a combinatorial optimization problem, and the authors propose a greedy algorithm to solve it. The proposed algorithm is evaluated on artificial data, where it is shown to outperform the existing greedy method. The paper is well-written and well-motivated. It is a good contribution to the field of vaccine design."
1722,SP:b1f622cbc827e880f98de9e99eca498584efe011,"generalization of set cover USED-FOR combinatorial optimization problem. combinatorial optimization problem USED-FOR task. marginally greedy algorithm CONJUNCTION mixed integer linear programming approach. mixed integer linear programming approach CONJUNCTION marginally greedy algorithm. mixed integer linear programming approach HYPONYM-OF algorithms. marginally greedy algorithm HYPONYM-OF algorithms. beam search USED-FOR marginally greedy algorithm. Task are vaccine design, and combinatorial problem. OtherScientificTerm are overlay, and overlays. Method is machine learning based tool. ","This paper studies the problem of vaccine design. The task is formulated as a combinatorial optimization problem based on the generalization of set cover. The authors propose two algorithms: a marginally greedy algorithm based on beam search, and a mixed integer linear programming approach. The main contribution of the paper is to propose a machine learning based tool that learns to generate a set of overlays. The overlays are then used to solve the original problem. "
1723,SP:11ad277db038a77d5935e7504cc640e74bfc4efe,"weight initialization USED-FOR deep spiking neural networks ( SNNs ). training speed EVALUATE-FOR weight initialization. accuracy EVALUATE-FOR weight initialization. initialization methods USED-FOR ANNs. initialization methods USED-FOR RNNs. ANNs CONJUNCTION RNNs. RNNs CONJUNCTION ANNs. theoretical first - order approximation USED-FOR response curve. MNIST CONJUNCTION CIFAR. CIFAR CONJUNCTION MNIST. non - leaking CONJUNCTION fast leaking. fast leaking CONJUNCTION non - leaking. fast convergence EVALUATE-FOR initialization scheme. CIFAR10 EVALUATE-FOR initialization schemes. Method are SNNs, weight initialization scheme, and encoding schemes. Generic are they, and network. OtherScientificTerm is gradients. ","This paper studies the effect of weight initialization on the training speed of deep spiking neural networks (SNNs). In particular, the authors consider the case of SNNs where the weights of the weights are not fixed, but rather, they are randomly chosen. The authors show that weight initialization can lead to a significant drop in accuracy, and that this is due to the fact that the gradients of the weight initialization scheme can be arbitrarily large. They then propose two initialization methods for ANNs and RNNs, and show that the proposed encoding schemes can achieve fast convergence. Finally, they propose a theoretical first-order approximation of the response curve, which can be used to train the network. They evaluate the proposed initialization schemes on MNIST, CIFAR10, and CIFR20, showing non-leaking and fast leaking."
1724,SP:11ad277db038a77d5935e7504cc640e74bfc4efe,parameter initialization problem USED-FOR SNNs. theoretical response FEATURE-OF spiking neurons. initialization method USED-FOR gradient vanishing problem. slant asymptote USED-FOR initialization method. training speed CONJUNCTION accuracy. accuracy CONJUNCTION training speed. training speed EVALUATE-FOR method. accuracy EVALUATE-FOR method. ,This paper studies the parameter initialization problem for SNNs. The authors propose an initialization method based on the slant asymptote to solve the gradient vanishing problem. The theoretical response of spiking neurons is also studied. The experiments show that the proposed method can improve the training speed and accuracy.
1725,SP:11ad277db038a77d5935e7504cc640e74bfc4efe,"initialization USED-FOR spiking neural net training. error backpropagation efficiency USED-FOR SNNs. optimization algorithms CONJUNCTION response functions. response functions CONJUNCTION optimization algorithms. MNIST CONJUNCTION N - MNIST. N - MNIST CONJUNCTION MNIST. optimization algorithms USED-FOR approach. N - MNIST EVALUATE-FOR approach. MNIST EVALUATE-FOR approach. Method are surrogate backprop methods, piecewise - linear   iterative expression, and SNN training. OtherScientificTerm are random weight initialization, and random weights. ","This paper studies the problem of initialization in spiking neural net training. In particular, the authors focus on improving the error backpropagation efficiency of SNNs by using surrogate backprop methods. The authors propose to use a piecewise-linear  iterative expression, where the random weight initialization is a function of the number of samples. The proposed approach is evaluated on MNIST and N-MNIST with different optimization algorithms and different response functions. The results show that the proposed approach can improve SNN training performance, especially when the random weights are used."
1726,SP:11ad277db038a77d5935e7504cc640e74bfc4efe,"initialization method USED-FOR spiking neurons. method USED-FOR spiking neuron response. SNN training methods USED-FOR ANN - to - SNN conversions. LIF neurons USED-FOR ANN - to - SNN conversions. Method are BPTT training of SNNs, deep learning training, and iterative systems. Material is MNIST, and neuromorphic datasets. OtherScientificTerm is response curve. ","This paper proposes a new initialization method for spiking neurons in deep learning training. The method is based on the observation that ANN-to-SNN conversions with LIF neurons have been shown to improve the performance of BPTT training of SNNs. This observation is supported by experiments on MNIST, as well as on neuromorphic datasets, which show that the proposed method is able to predict the spiking neuron response. The paper also provides a theoretical analysis of the response curve. Finally, the paper provides some experimental results on iterative systems."
1727,SP:f7e8602b40b37f26277e3f44f60a11f879978986,daytime modes CONJUNCTION nighttime modes. nighttime modes CONJUNCTION daytime modes. federated learning USED-FOR distribution shift. mixture of distributions USED-FOR federated learning. mixture of distributions USED-FOR distribution shift. two - group clustered federated learning method USED-FOR method. model parameters of prediction layers USED-FOR client clustering. ,This paper studies the problem of distribution shift in federated learning with a mixture of distributions in both the daytime modes and nighttime modes. The proposed method is based on the two-group clustered federated learnable learning method. The client clustering is done by changing the model parameters of prediction layers.
1728,SP:f7e8602b40b37f26277e3f44f60a11f879978986,"federated learning method USED-FOR non - IID challenge. backbone network USED-FOR distributions. branch USED-FOR distributions. branch PART-OF backbone network. multi - branch model COMPARE multi - task learning. multi - task learning COMPARE multi - branch model. backbone network USED-FOR data representations. Gaussian component PART-OF mixture. M - step USED-FOR representation. representation USED-FOR Gaussian component. E - step CONJUNCTION M - step. M - step CONJUNCTION E - step. EM algorithm USED-FOR clusters. backbone network USED-FOR data representation. mixture of isotropic Gaussian distributions USED-FOR data representation. label smoothing CONJUNCTION temporal prior. temporal prior CONJUNCTION label smoothing. temporal prior CONJUNCTION linear / cosine / soft schedule. linear / cosine / soft schedule CONJUNCTION temporal prior. EMNIST CONJUNCTION CIFAR. CIFAR CONJUNCTION EMNIST. CIFAR CONJUNCTION Stackflow datasets. Stackflow datasets CONJUNCTION CIFAR. FL FEATURE-OF Stackflow datasets. EMNIST EVALUATE-FOR day - night distribution shift. label smoothing CONJUNCTION period lengths. period lengths CONJUNCTION label smoothing. OtherScientificTerm are data distribution, and branches. Generic is method. ","This paper proposes a federated learning method for solving the non-IID challenge. The proposed multi-branch model is similar to multi-task learning, where a backbone network is used to learn data representations, and a branch is trained to predict distributions from the data distribution. The main difference of the proposed method is that instead of using a mixture of isotropic Gaussian distributions as the data representation, the authors propose to use a mixture where the Gaussian component is learned using an E-step followed by an M-step to learn the representation of the other two branches. The authors also propose to learn clusters based on the EM algorithm. Experiments are conducted on EMNIST, CIFAR, and Stackflow datasets with FL. Results show that the day-night distribution shift can be alleviated by label smoothing, temporal prior, and linear/cosine/soft schedule."
1729,SP:f7e8602b40b37f26277e3f44f60a11f879978986,mixture of distributions USED-FOR distributional shift. multi - branch network USED-FOR shifting distribution. next word prediction EVALUATE-FOR algorithm. Stack Overflow dataset USED-FOR next word prediction. EMNIST and CIFAR datasets USED-FOR image classification. Task is federated learning production setting. Method is Federated Expectation - Maximization algorithm. ,This paper studies the problem of distributional shift in a mixture of distributions in the federated learning production setting. The authors propose a Federated Expectation-Maximization algorithm to mitigate this issue. The proposed algorithm is evaluated on the next word prediction on Stack Overflow dataset and on the EMNIST and CIFAR datasets for image classification. The main contribution of the paper is to propose a multi-branch network to handle the shifting distribution.
1730,SP:f7e8602b40b37f26277e3f44f60a11f879978986,"periodic client distribution shift PART-OF cross - device FL. temporal prior USED-FOR GMM model. federated EM scheme USED-FOR GMM. method COMPARE baseline approaches. baseline approaches COMPARE method. Method are semi - cyclic SGD ), NN model, and EMNIST. Material is StackOverflow datasets. ","This paper addresses the problem of periodic client distribution shift in cross-device FL. The authors propose a GMM model with a temporal prior, which is based on a federated EM scheme. The GMM is trained using a semi-cyclic SGD (similar to the NN model used in EMNIST) and the method is evaluated on StackOverflow datasets and compared to several baseline approaches."
1731,SP:e38efcfcf63f0488b6e20a74a86b78aad1ead363,"methodology USED-FOR deep network pruning. max - affine spline USED-FOR methodology. max - affine spline formulation CONJUNCTION empirical pruning techniques. empirical pruning techniques CONJUNCTION max - affine spline formulation. OtherScientificTerm is redundant subdivision splines. Generic are benchmarked models, and method. ",This paper proposes a methodology for deep network pruning based on max-affine spline. The key idea is to remove redundant subdivision splines. The authors propose to combine the max-infinite spline formulation with empirical pruning techniques. Experiments on benchmarked models demonstrate the effectiveness of the proposed method.
1732,SP:e38efcfcf63f0488b6e20a74a86b78aad1ead363,"spline operators USED-FOR activation functions. node / filter / channel pruning CONJUNCTION partition of input space. partition of input space CONJUNCTION node / filter / channel pruning. neural nets USED-FOR partition of input space. spline operators USED-FOR neural nets. network USED-FOR splines. splines USED-FOR decision boundary. fully - connected networks CONJUNCTION convolutional networks. convolutional networks CONJUNCTION fully - connected networks. accuracy EVALUATE-FOR pruning. prunning method COMPARE pruning methods. pruning methods COMPARE prunning method. CIFAR10/100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR10/100. pruning ratio EVALUATE-FOR prunning method. pruning ratio EVALUATE-FOR pruning methods. accuracy EVALUATE-FOR pruning methods. accuracy EVALUATE-FOR prunning method. OtherScientificTerm are early - bird tickets, and binary activation patterns. Method are prunning, and pruning strategy. ","This paper proposes to prune neural nets with spline operators for activation functions, which is a combination of node/filter/channel pruning and partition of input space in neural nets. The idea is that the splines of the network can be used as early-bird tickets for prunning, and that the decision boundary can be represented as a set of splines. The authors compare fully-connected networks with convolutional networks, and show that the pruning strategy is effective. The prunning method is compared with other pruning methods on CIFAR10/100 and ImageNet, and the prunning ratio is compared to the pruned methods on accuracy. The results show that pruning is effective in reducing the number of binary activation patterns."
1733,SP:e38efcfcf63f0488b6e20a74a86b78aad1ead363,Spline theory USED-FOR deep neural network pruning. Spline theory CONJUNCTION pruning methods. pruning methods CONJUNCTION Spline theory. space partition perspective FEATURE-OF pruning methods. Method is pruning algorithm approach. Generic is algorithm. ,This paper studies deep neural network pruning using Spline theory and pruning methods from a space partition perspective. The authors propose a new pruning algorithm approach based on the idea that pruning the weights of the weights in the training set is a function of the space partition of the training data. The algorithm is evaluated on a variety of datasets.
1734,SP:e38efcfcf63f0488b6e20a74a86b78aad1ead363,max - affine spline function USED-FOR network pruning. network pruning PART-OF deep learning architecture. examples USED-FOR splines visualization. fully - connected NN CONJUNCTION CNN. CNN CONJUNCTION fully - connected NN. pruning rates FEATURE-OF splines visualization. slope CONJUNCTION biases. biases CONJUNCTION slope. cosine similarity USED-FOR policy. structured pruning CONJUNCTION unstructured pruning. unstructured pruning CONJUNCTION structured pruning. energy efficiency EVALUATE-FOR metric. accuracy EVALUATE-FOR metric. ,This paper proposes a max-affine spline function for network pruning in deep learning architecture. The authors use examples from splines visualization with different pruning rates to compare the performance of a fully-connected NN and a CNN. They show that the slope and biases of the policy are correlated with the cosine similarity of the splines. They also compare structured pruning with unstructured pruning and show that their metric improves the accuracy and energy efficiency.
1735,SP:64ce86f8bd8572f699809c808aea8364fbbe4ef3,bi - level optimization problem USED-FOR fair representation. fixed representation USED-FOR fair predictors. error gap EVALUATE-FOR implicit algorithm. Method is data representation. ,"This paper studies the problem of fair representation as a bi-level optimization problem. In particular, the authors consider the case where the data representation is fixed. The authors propose an implicit algorithm that minimizes the error gap between the fixed representation and the fair predictors."
1736,SP:64ce86f8bd8572f699809c808aea8364fbbe4ef3,"methodology USED-FOR fair representation learning. accuracy - fairness tradeoff EVALUATE-FOR approaches. Generic is proposal. OtherScientificTerm are sufficient rule, and sufficient gap. ","This paper proposes a methodology for fair representation learning. The proposal is based on the observation that there is a sufficient rule for each class to be fair, and that the sufficient gap is a function of the number of classes in the class. Theoretical analysis of the accuracy-fairness tradeoff of the proposed approaches is provided."
1737,SP:64ce86f8bd8572f699809c808aea8364fbbe4ef3,"bi - level implicit path alignment algorithm USED-FOR it. sufficient rule HYPONYM-OF fairness notion. empirical algorithm USED-FOR DNN. empirical algorithm USED-FOR sufficient rule. Task are fair representation learning problem, and regression. Generic is algorithm. ","This paper studies the fair representation learning problem and proposes a bi-level implicit path alignment algorithm to solve it. The main contribution of the paper is the introduction of a fairness notion, i.e., the sufficient rule, which is an empirical algorithm to train a DNN. The algorithm is shown to converge to the optimal solution in regression."
1738,SP:64ce86f8bd8572f699809c808aea8364fbbe4ef3,( group ) sufficiency rule USED-FOR fairness. fairness CONJUNCTION optimization path alignment. optimization path alignment CONJUNCTION fairness. optimization path alignment USED-FOR fairness requirement. ( group ) sufficiency rule USED-FOR fairness requirement. ( group ) sufficiency rule CONJUNCTION optimization path alignment. optimization path alignment CONJUNCTION ( group ) sufficiency rule. bi - level optimization problem USED-FOR representation function. implicit function theorem USED-FOR gradients. algorithm COMPARE baseline approaches. baseline approaches COMPARE algorithm. Task is learning of fair representations ( and classifiers ). OtherScientificTerm is representation fair. Method is optimal subgroup classifiers. ,"This paper considers the problem of learning of fair representations (and classifiers). The authors propose to use the (group) sufficiency rule to enforce fairness and optimization path alignment to satisfy the fairness requirement. The representation function is formulated as a bi-level optimization problem, where the goal is to make the representation fair. The authors prove an implicit function theorem for the gradients of the optimal subgroup classifiers. The proposed algorithm is compared with several baseline approaches."
1739,SP:9bd0a519881297066ee60ccf62ee27e4c109047d,Reinforcement Learning HYPONYM-OF approaches. Supervised Learning ( RvS ) USED-FOR Reinforcement Learning. policy architecture CONJUNCTION regularization. regularization CONJUNCTION policy architecture. RvS USED-FOR policies. capacity CONJUNCTION regularization. regularization CONJUNCTION capacity. capacity FEATURE-OF neural networks. regularization FEATURE-OF neural networks. compositional behavior FEATURE-OF policies. goal / reward based conditioning HYPONYM-OF conditioning variables. neural networks USED-FOR RvS approaches. Task is offline RL approaches. Method is reduction to weighted / conditional behavior cloning. ,"Reinforcement Learning with Supervised Learning (RvS) is one of the most popular approaches in offline RL approaches. RvS has been shown to be able to learn policies with compositional behavior, but there are many challenges in RvL: capacity, regularization, etc. This paper proposes to address these challenges by introducing a reduction to weighted/conditional behavior cloning, where conditioning variables (e.g., goal/reward based conditioning) are learned using neural networks."
1740,SP:9bd0a519881297066ee60ccf62ee27e4c109047d,methods USED-FOR policies. policies USED-FOR offline RL. methods COMPARE average behavior. average behavior COMPARE methods. supervised learning USED-FOR policies. supervised learning USED-FOR offline RL. benchmark problems EVALUATE-FOR methods. Task is behavior cloning. ,This paper presents methods for learning policies for offline RL with supervised learning. The methods are evaluated on several benchmark problems and compared to average behavior. The authors also provide a theoretical analysis of behavior cloning.
1741,SP:9bd0a519881297066ee60ccf62ee27e4c109047d,model size CONJUNCTION regularization. regularization CONJUNCTION model size. behavior cloning based strategies USED-FOR offline RL algorithms. conditioning CONJUNCTION validation based evaluation. validation based evaluation CONJUNCTION conditioning. OtherScientificTerm is regularization characteristics. ,This paper studies the behavior cloning based strategies for offline RL algorithms. The authors study the relationship between model size and regularization. They show that conditioning and validation based evaluation can be used to identify the regularization characteristics.
1742,SP:9bd0a519881297066ee60ccf62ee27e4c109047d,design decisions USED-FOR supervised learning type reinforcement learning algorithms. large sequence models CONJUNCTION value - based weighting schemes. value - based weighting schemes CONJUNCTION large sequence models. RvS methods COMPARE prior methods. prior methods COMPARE RvS methods. optimal data FEATURE-OF datasets. datasets HYPONYM-OF offline RL benchmarks. offline RL benchmarks EVALUATE-FOR RvS methods. offline RL benchmarks EVALUATE-FOR prior methods. ,"This paper studies the design decisions for supervised learning type reinforcement learning algorithms. The authors focus on large sequence models and value-based weighting schemes. They show that RvS methods outperform prior methods on several offline RL benchmarks, including datasets with optimal data."
1743,SP:af89e1cdd2b39df9982ca5cd9446ec66a4d317f2,"program induction USED-FOR maps of unseen spaces. Method are map induction, and probabilistic models. Task are robotics applications, navigation of autonomous systems, exploring unknown environments, and map induction task. ","This paper studies the problem of map induction, which is an important problem in robotics applications where the goal is to learn maps of unseen spaces using program induction. This is particularly important in the context of navigation of autonomous systems and exploring unknown environments. The authors propose to use probabilistic models to learn the map induction task. "
1744,SP:af89e1cdd2b39df9982ca5cd9446ec66a4d317f2,"shared patterns USED-FOR spatial regions. discrete 2D computational model USED-FOR map. probabilistic program induction USED-FOR discrete 2D computational model. flips CONJUNCTION rotations. rotations CONJUNCTION flips. rotations CONJUNCTION concatenations. concatenations CONJUNCTION rotations. transformations FEATURE-OF small extracted regions. flips HYPONYM-OF transformations. concatenations HYPONYM-OF transformations. rotations HYPONYM-OF transformations. model - based planning USED-FOR exploration & reward - collection behavior. map - induction model USED-FOR model - based planning. MAP estimate CONJUNCTION uninformed model. uninformed model CONJUNCTION MAP estimate. computational model USED-FOR real - life exploration behavior. Method is hierarchical spatial representation. OtherScientificTerm are hierarchy, empty map regions, structural assumptions, rewards, and map completions. Generic are model, and task. Task is control task. ","This paper proposes a hierarchical spatial representation, where spatial regions are learned from shared patterns. The model is trained using probabilistic program induction, where a discrete 2D computational model is used to learn a map of the environment. The authors show that small extracted regions exhibit a variety of transformations, such as flips, rotations, concatenations, etc. They also show that the model can learn a hierarchy, where empty map regions are replaced with new regions. They show that model-based planning with a map-imitation model can lead to better exploration & reward-collection behavior, under some structural assumptions (e.g., no rewards for map completions). Finally, the authors demonstrate that the computational model can be used to model real-life exploration behavior, by comparing the MAP estimate with an uninformed model, and showing that the learned model can generalize to a control task."
1745,SP:af89e1cdd2b39df9982ca5cd9446ec66a4d317f2,"hierarchical bayesian generative framework USED-FOR exploration behavior. Task is Map Induction Task "" ( MIT ). Generic is distribution. ","This paper proposes a hierarchical bayesian generative framework to model the exploration behavior. The authors call this ""Map Induction Task"" (MIT). The main idea is to learn a distribution over a set of points and then to sample from this distribution."
1746,SP:af89e1cdd2b39df9982ca5cd9446ec66a4d317f2,program induction USED-FOR prediction of possible spatial map. models USED-FOR map prediction. OtherScientificTerm is distribution of possible maps. ,This paper studies the problem of program induction for the prediction of possible spatial map. The main contribution of this paper is to study the distribution of possible maps. The authors show that models for map prediction can be trained in a supervised manner.
1747,SP:cccdcc95c4177b5531bad23b662060fdd0d88849,"neural network weights USED-FOR factors. particle belief    propagation USED-FOR factor parameters. neural network weights HYPONYM-OF factor parameters. stochastic gradient descent USED-FOR factor parameters. algorithm USED-FOR continuous - domain state estimation tasks. articulated objects FEATURE-OF continuous - domain state estimation tasks. images USED-FOR hand pose estimation. hand pose estimation HYPONYM-OF continuous - domain state estimation tasks. NNs USED-FOR unary and pairwise potentials. algorithm USED-FOR NNs. algorithm USED-FOR uncertainty estimates. synthetic and real datasets USED-FOR articulated object state estimation. synthetic and real datasets EVALUATE-FOR algorithm. approach COMPARE baselines. baselines COMPARE approach. it COMPARE state of the art. state of the art COMPARE it. parametric human hand tracking EVALUATE-FOR approach. it USED-FOR parametric human hand tracking. LSTM HYPONYM-OF baselines. Method are nonparametric belief propagation methods, end - to - end    learning, and inference procedure. Generic is learning. OtherScientificTerm are particle    resampling stage, and GT values. ","This paper proposes an algorithm for continuous-domain state estimation tasks on articulated objects (i.e. hand pose estimation from images) using nonparametric belief propagation methods. The key idea is to use particle belief   propagation to learn factor parameters, which are parameterized by neural network weights. The learning is done end-to-end, with a particle   resampling stage. The factor parameters are learned using stochastic gradient descent. The algorithm is applied to NNs for unary and pairwise potentials, and the uncertainty estimates are derived using the learned algorithm. The inference procedure is similar to that of LSTM, except that the GT values are learned in parallel. The proposed algorithm is evaluated on both synthetic and real datasets for articulated object state estimation, and it is compared to several baselines, and compared to the state of the art on parametric human hand tracking."
1748,SP:cccdcc95c4177b5531bad23b662060fdd0d88849,"pairwise and unary marginals USED-FOR model estimate. likelihood maximisation USED-FOR model estimate. trees USED-FOR MRFs. finite hidden state spaces FEATURE-OF MRFs. finite hidden state spaces FEATURE-OF trees. approximated belief propagation ( BP ) approach USED-FOR networks. infinite state spaces FEATURE-OF MRFs. neural nets USED-FOR pairwise and unary potentials. method USED-FOR hand pose estimation. first - person perspective FEATURE-OF RGB - D image sequences. RGB - D image sequences USED-FOR hand pose estimation. Method are MLE, and BP. Generic is task. ","This paper proposes a method for hand pose estimation from RGB-D image sequences from a first-person perspective. The main idea is to use pairwise and unary marginals as the model estimate using likelihood maximisation, which is based on MLE. The authors propose to use the approximated belief propagation (BP) approach to train networks to estimate the pairwise potentials of pairwise (and unary) potentials using neural nets, which are trained on finite hidden state spaces instead of infinite state spaces as in MRFs with trees. This is an interesting idea, as BP has been shown to be computationally efficient. The paper also provides a theoretical analysis of the proposed method for the task."
1749,SP:cccdcc95c4177b5531bad23b662060fdd0d88849,"graphical model USED-FOR nonparametric belief propagation ( NBP ) methods. domain - specific hand crafted factors COMPARE learned factors. learned factors COMPARE domain - specific hand crafted factors. neural network USED-FOR factor. vanilla neural net based solutions COMPARE DNBP. DNBP COMPARE vanilla neural net based solutions. DNBP USED-FOR uncertainty. FPHAB dataset EVALUATE-FOR hand pose estimation. articulated pose tracking EVALUATE-FOR method. hand pose estimation EVALUATE-FOR method. FPHAB dataset EVALUATE-FOR method. method COMPARE neural network based baselines. neural network based baselines COMPARE method. Method are neural networks, and Differentiable nonparametric belief propagation "" ( DNBP ). ","This paper proposes a new graphical model for nonparametric belief propagation (NBP) methods. The key idea is to use domain-specific hand crafted factors instead of learned factors. The authors propose to use neural networks to learn the factor and then use the neural network to predict the next factor. The proposed method, called ""Differentiable nonparameterized belief propagation"" (DNBP), is evaluated on the FPHAB dataset for hand pose estimation and articulated pose tracking. Compared to vanilla neural net based solutions, DNBP is able to model uncertainty. The experiments show that the proposed method outperforms other neural network based baselines."
1750,SP:cccdcc95c4177b5531bad23b662060fdd0d88849,pairwise potential functions CONJUNCTION particle diffusion function. particle diffusion function CONJUNCTION pairwise potential functions. unary potential functions CONJUNCTION pairwise potential functions. pairwise potential functions CONJUNCTION unary potential functions. non - parametric belief propagation method USED-FOR method. labeled data USED-FOR networks. simulated articulated spider CONJUNCTION first - person hand action. first - person hand action CONJUNCTION simulated articulated spider. simulated double pendulum CONJUNCTION simulated articulated spider. simulated articulated spider CONJUNCTION simulated double pendulum. tasks EVALUATE-FOR DNBP. first - person hand action HYPONYM-OF tasks. simulated double pendulum HYPONYM-OF tasks. simulated articulated spider HYPONYM-OF tasks. application EVALUATE-FOR DNBP. DNBP COMPARE baseline. baseline COMPARE DNBP. Method is feed - forward neural networks. Generic is it. ,"This paper proposes DNBP, a method for training feed-forward neural networks. The proposed method is based on a non-parametric belief propagation method, where unary potential functions, pairwise potential functions and a particle diffusion function are used. The networks are trained on labeled data. The authors evaluate DNBP on three tasks: a simulated double pendulum, a simulated articulated spider and a first-person hand action. In the first application, the authors show that DNBP outperforms a baseline, and in the second application, it outperforms the baseline."
1751,SP:76e858a6ef79a3bd861803395e25d7f65fd29a00,method USED-FOR iterative small molecule generation. autoencoder framework CONJUNCTION graph neural networks. graph neural networks CONJUNCTION autoencoder framework. autoencoder framework USED-FOR method. method USED-FOR molecular scaffolds. structural motifs USED-FOR molecular scaffolds. unconstrained molecular optimization tasks CONJUNCTION tasks. tasks CONJUNCTION unconstrained molecular optimization tasks. scaffold USED-FOR tasks. ,This paper proposes a method for iterative small molecule generation based on the autoencoder framework and graph neural networks. The method learns molecular scaffolds based on structural motifs. The authors conduct experiments on unconstrained molecular optimization tasks and tasks with a different scaffold.
1752,SP:76e858a6ef79a3bd861803395e25d7f65fd29a00,"MLP decoder layer USED-FOR fragment. encoder of molecular graph PART-OF model MoLeR. bonding atoms PART-OF model MoLeR. Graph convolutional neural network USED-FOR encoder of molecular graph. MLP decoder layer PART-OF model MoLeR. self - reconstruction loss CONJUNCTION property prediction MSE loss. property prediction MSE loss CONJUNCTION self - reconstruction loss. KL term CONJUNCTION self - reconstruction loss. self - reconstruction loss CONJUNCTION KL term. losses USED-FOR multi - task learning. it USED-FOR multi - task learning. variational posterior CONJUNCTION prior of latent representation. prior of latent representation CONJUNCTION variational posterior. it USED-FOR model. losses PART-OF it. KL term HYPONYM-OF losses. property prediction MSE loss HYPONYM-OF losses. self - reconstruction loss HYPONYM-OF losses. It COMPARE LSTM. LSTM COMPARE It. JTVAE CONJUNCTION CDDD - MCTS. CDDD - MCTS CONJUNCTION JTVAE. LSTM CONJUNCTION JTVAE. JTVAE CONJUNCTION LSTM. It COMPARE CDDD - MCTS. CDDD - MCTS COMPARE It. It COMPARE JTVAE. JTVAE COMPARE It. Task is fragment - based molecule generation. Method is GNN. OtherScientificTerm are partial molecules, and training molecule distribution. Material is GulcaMol. ","This paper studies fragment-based molecule generation. The model MoLeR consists of two bonding atoms and a Graph convolutional neural network (GNN). The MLP decoder layer is used to generate a fragment from the partial molecules. The GNN is trained in a way that it can be used for multi-task learning. The authors propose three losses: a KL term, a self-reconstruction loss, and a property prediction MSE loss. The variational posterior and the prior of latent representation are learned separately. The training molecule distribution is sampled from GulcaMol. It is compared to LSTM, JTVAE, and CDDD-MCTS."
1753,SP:76e858a6ef79a3bd861803395e25d7f65fd29a00,"generative model USED-FOR molecules. motifs USED-FOR approach. node and edge embeddings PART-OF vectorial latent space. GCN encoder USED-FOR node and edge embeddings. autoregressive models USED-FOR generative approach. latent space USED-FOR generative approach. single - step transformations USED-FOR it. OtherScientificTerm are acyclic bonds, and nodes. Generic are model, and method. ","This paper proposes a generative model for molecules that can be expressed as a set of motifs. The generative approach is based on autoregressive models, where the node and edge embeddings of the vectorial latent space are encoded by a GCN encoder. The key idea is to learn acyclic bonds, which are then used to represent the nodes and edges of the latent space. The model is trained using single-step transformations, and it is shown that the method is able to generalize to unseen molecules."
1754,SP:76e858a6ef79a3bd861803395e25d7f65fd29a00,"graph - based generative model USED-FOR molecule generation. scaffolds USED-FOR MoLeR. method COMPARE methods. methods COMPARE method. method COMPARE methods. methods COMPARE method. methods COMPARE methods. methods COMPARE methods. scaffold - based tasks EVALUATE-FOR methods. unconstrained molecular optimization tasks EVALUATE-FOR method. scaffold - based tasks EVALUATE-FOR method. unconstrained molecular optimization tasks EVALUATE-FOR methods. OtherScientificTerm are scaffold, and generation history. ","This paper proposes a graph-based generative model for molecule generation. The proposed method, MoLeR, is based on the idea that scaffolds can be used to generate new molecules. The scaffold is learned by taking the generation history as input. Experiments on unconstrained molecular optimization tasks show that the proposed method outperforms existing methods."
1755,SP:318b3c294a475960c13a4914b035fd3a2ea84661,"algorithm USED-FOR imitation learning. expert ’s occupancy distribution CONJUNCTION expected reward. expected reward CONJUNCTION expert ’s occupancy distribution. algorithm USED-FOR expert ’s occupancy distribution. sufficient expert data USED-FOR algorithm. expert rollouts FEATURE-OF limiting distribution of the expert ’s policy. expected agreement FEATURE-OF expert - data. probabilistic bound USED-FOR expected agreement. probabilistic bound FEATURE-OF expert - data. mixing time USED-FOR probabilistic bound. bound USED-FOR expected agreement. Material are demonstration - set, and control benchmarks. Metric is expected extrinsic reward. ","This paper proposes an algorithm for imitation learning with sufficient expert data. The algorithm learns to estimate the expert’s occupancy distribution and the expected reward for a given demonstration-set. The expected extrinsic reward is a function of the number of expert rollouts required to approximate the limiting distribution of a given policy. The paper provides a probabilistic bound on the expected agreement of the expert-data with respect to the mixing time, which is then used to derive a bound on expected agreement between the expert and the policy. Experiments are conducted on several control benchmarks."
1756,SP:318b3c294a475960c13a4914b035fd3a2ea84661,imitation learning problem USED-FOR policy. policy USED-FOR expert behavior. stationary reward USED-FOR reinforcement learning. expert datasets USED-FOR stationary reward. expected per - step intrinsic reward CONJUNCTION extrinsic reward. extrinsic reward CONJUNCTION expected per - step intrinsic reward. extrinsic reward EVALUATE-FOR imitation policy. expected per - step intrinsic reward EVALUATE-FOR imitation policy. expert policy CONJUNCTION imitation policy. imitation policy CONJUNCTION expert policy. method COMPARE algorithms. algorithms COMPARE method. method COMPARE algorithm. algorithm COMPARE method. algorithms COMPARE algorithm. algorithm COMPARE algorithms. Task is continuous control tasks. ,This paper proposes an imitation learning problem to learn a policy that mimics expert behavior. The stationary reward for reinforcement learning is learned from expert datasets. The authors evaluate the imitation policy on the expected per-step intrinsic reward and the extrinsic reward. The experiments are conducted on continuous control tasks and show that the proposed method outperforms existing algorithms. The main contribution of the paper is the comparison between the expert policy and the proposed imitation policy.
1757,SP:318b3c294a475960c13a4914b035fd3a2ea84661,"deterministic experts USED-FOR imitation learning. stationary reward FEATURE-OF reinforcement learning. reduction USED-FOR continuous control tasks. Task is recovery of expert reward. OtherScientificTerm is total variation distance. Method are imitation learner, and adversarial imitation learning. ","This paper studies the problem of imitation learning with deterministic experts. In particular, the authors study the recovery of expert reward when the total variation distance between the expert and the imitation learner is large. The main contribution of this paper is a reduction to reinforcement learning with a stationary reward. This reduction is applied to continuous control tasks and is shown to be effective in adversarial imitation learning."
1758,SP:318b3c294a475960c13a4914b035fd3a2ea84661,deterministic experts USED-FOR imitation learning ( IL ). stationary reward USED-FOR total variation distance. setting USED-FOR IL. RL USED-FOR IL. stationary reward USED-FOR RL. stationary reward USED-FOR IL. ,This paper studies imitation learning (IL) with deterministic experts. The authors propose a new setting for IL where the total variation distance between the expert and the learner is determined by a stationary reward. This stationary reward can be used to train RL for IL.
1759,SP:fd1a9b4c5ee36159286f4a35fa93ed0c23120906,worst - group accuracy EVALUATE-FOR over - parametrized setting. algorithm USED-FOR interpolators. reweighting USED-FOR solution. linear models CONJUNCTION linearized networks. linearized networks CONJUNCTION linear models. it USED-FOR training error. OtherScientificTerm is regularization. ,"This paper studies the worst-group accuracy in the over-parametrized setting. The authors propose an algorithm to train interpolators that are more robust to regularization. The solution is based on reweighting, and it is shown to reduce the training error. Experiments are conducted on linear models and linearized networks."
1760,SP:fd1a9b4c5ee36159286f4a35fa93ed0c23120906,importance reweighing USED-FOR overparameterized deep networks. squared loss CONJUNCTION independent data points. independent data points CONJUNCTION squared loss. linear models CONJUNCTION linearized neural networks. linearized neural networks CONJUNCTION linear models. model USED-FOR linear models. model USED-FOR linearized neural networks. celeba CONJUNCTION waterbirds. waterbirds CONJUNCTION celeba. OtherScientificTerm is regularization. ,"This paper studies importance reweighing in overparameterized deep networks. The authors propose a model for linear models and linearized neural networks that combines squared loss with independent data points. They show that the proposed regularization improves performance on a variety of datasets, including celeba, waterbirds, etc."
1761,SP:fd1a9b4c5ee36159286f4a35fa93ed0c23120906,"linear models CONJUNCTION linearized neural networks. linearized neural networks CONJUNCTION linear models. $ L_2 $ regularization USED-FOR training. regularization CONJUNCTION early stopping. early stopping CONJUNCTION regularization. test accuracy EVALUATE-FOR approaches. reweighting USED-FOR approaches. regularization USED-FOR approaches. early stopping USED-FOR approaches. OtherScientificTerm are worst - case subgroup performance, pre - specified subgroup, overparameterization regime, and overparamerized regime. Generic is methods. ","This paper studies the problem of training linear models and linearized neural networks with $L_2$ regularization during training, in order to reduce the worst-case subgroup performance. The authors argue that existing approaches that use reweighting or regularization or early stopping can lead to lower test accuracy, which is not necessarily beneficial for the pre-specified subgroup. In particular, the authors point out that in the overparameterization regime, the performance of these methods can be significantly worse than the overparamerized regime."
1762,SP:fd1a9b4c5ee36159286f4a35fa93ed0c23120906,Importance Weighting CONJUNCTION Group DRO. Group DRO CONJUNCTION Importance Weighting. reweighting algorithms USED-FOR worst - group performance. Group DRO HYPONYM-OF reweighting algorithms. Importance Weighting HYPONYM-OF reweighting algorithms. overfitting problems PART-OF reweighting algorithms. algorithms CONJUNCTION wide fully - connected neural networks. wide fully - connected neural networks CONJUNCTION algorithms. wide fully - connected neural networks CONJUNCTION squared loss. squared loss CONJUNCTION wide fully - connected neural networks. reweighting algorithm COMPARE ERM. ERM COMPARE reweighting algorithm. worst - group test performance EVALUATE-FOR reweighting algorithm. ,"This paper proposes two reweighting algorithms, Importance Weighting and Group DRO, to improve the worst-group performance by addressing overfitting problems in existing reweighted algorithms. The authors compare the performance of these two algorithms with wide fully-connected neural networks, squared loss, and other algorithms. They show that the proposed reweighter outperforms ERM in terms of best-group test performance."
1763,SP:318ace9202e42d1d278eb79fe1853138e1d00a06,"Rational Inattention Reinforcement Learning ( RIRL ) HYPONYM-OF multi - agent reinforcement learning framework. economics literature USED-FOR classical models. principal - agent problems EVALUATE-FOR RIRL. Method are behavioral model of rational inattention, and RIRL - actor architecture. OtherScientificTerm are rational inattention, mutual information, and heterogeneous cognitive costs. Generic are framework, and simulation. ","This paper proposes a multi-agent reinforcement learning framework called Rational Inattention Reinforcement Learning (RIRL) which is an extension of the behavioral model of rational inattention. This framework is motivated by the observation that classical models from the economics literature fail to capture the dynamics of the relationship between the agent and the environment. The authors propose to model the relationship of the agent to the environment as a function of its own behavior, and propose an RIRL-actor architecture that learns to maximize the mutual information between the environment and the agent, while minimizing heterogeneous cognitive costs. Experiments on principal-agent problems demonstrate the effectiveness of the proposed framework, and the authors also conduct a simulation to evaluate the performance."
1764,SP:318ace9202e42d1d278eb79fe1853138e1d00a06,"this PART-OF RL. observation component CONJUNCTION action. action CONJUNCTION observation component. observation component PART-OF policy. information penalty USED-FOR action. RIRL model USED-FOR game theory games. Method are Bayesian - type decision making models, and Rational inattention models. OtherScientificTerm are conditional distribution, and capacity constraint. ","This paper proposes to replace Bayesian-type decision making models with Rational inattention models where the policy consists of an observation component and an action that is penalized by an information penalty. The authors argue that this is a natural extension of RL and that this can be seen as an extension of the conditional distribution of the policy. The RIRL model is applied to several game theory games, where the capacity constraint is introduced."
1765,SP:318ace9202e42d1d278eb79fe1853138e1d00a06,RIRL USED-FOR optimal RL model. RIRL USED-FOR rational inattention suboptimality. RIRL behavior COMPARE rational model. rational model COMPARE RIRL behavior. emergent behavior FEATURE-OF RIRL. ,This paper studies the problem of rational inattention suboptimality in RIRL. The authors show that RIRR can converge to an optimal RL model when the number of agents is small. They also show that the emergent behavior of RIRRL is similar to that of a rational model.
1766,SP:318ace9202e42d1d278eb79fe1853138e1d00a06,multi - agent reinforcement learning USED-FOR human bounded rationality. multi - timestep dynamics CONJUNCTION information channels. information channels CONJUNCTION multi - timestep dynamics. information channels FEATURE-OF RIRL framework. multi - timestep dynamics FEATURE-OF RIRL framework. heterogeneous processing costs FEATURE-OF information channels. Principal - Agent problems EVALUATE-FOR RIRL framework. Method is rational inattention model. ,This paper studies the problem of multi-agent reinforcement learning for human bounded rationality. The authors propose a novel RIRL framework with multi-timestep dynamics and information channels with heterogeneous processing costs. They also propose a rational inattention model. They evaluate the proposed RISTL framework on Principal-Agent problems.
1767,SP:100c91da177504d89f1819f4fdce72ebcf848902,idea USED-FOR imperceptible adversarial attacks. imperceptible adversarial attacks USED-FOR ASR. phase perturbations USED-FOR spectrogram. method COMPARE counterpart methods. counterpart methods COMPARE method. method USED-FOR adversarial examples. method USED-FOR academic Transformer - based ASR system. adversarial examples USED-FOR academic Transformer - based ASR system. OtherScientificTerm is phase information. ,"This paper proposes a new idea for imperceptible adversarial attacks in ASR. The key idea is to use phase perturbations to the spectrogram, which is then used to generate adversarial examples. The authors show that the proposed method outperforms the counterpart methods by a large margin, and demonstrate that the phase information can be used to improve the performance of an academic Transformer-based ASR system."
1768,SP:100c91da177504d89f1819f4fdce72ebcf848902,constructing adversarial examples USED-FOR automatic speech recognition. phase of the short - time Fourier transformation USED-FOR constructing adversarial examples. Material is adversarial examples. OtherScientificTerm is phase - oriented audio adversarial samples. ,This paper studies the problem of constructing adversarial examples for automatic speech recognition using the phase of the short-time Fourier transformation. The main contribution of this paper is to propose a new class of adversarial samples that are phase-oriented audio adversarial sample. The paper is well-written and easy to follow.
1769,SP:100c91da177504d89f1819f4fdce72ebcf848902,phase - oriented algorithm PhaseFool USED-FOR imperceptible audio adversarial attacks. energy dissipation FEATURE-OF imperceptible audio adversarial attacks. spectrogram consistency USED-FOR STFT. attack effectiveness EVALUATE-FOR attack. Task is ASR systems. ,"This paper proposes a phase-oriented algorithm PhaseFool for imperceptible audio adversarial attacks with energy dissipation. The main contribution of the paper is the introduction of spectrogram consistency to STFT, which is an important problem in ASR systems. Experiments show that the proposed attack improves the attack effectiveness."
1770,SP:100c91da177504d89f1819f4fdce72ebcf848902,"adversarial attack method USED-FOR ASR network. phase information COMPARE magnitude information. magnitude information COMPARE phase information. imperceptibility EVALUATE-FOR it. it USED-FOR imperceptible perturbation. it COMPARE state - of - the - art method. state - of - the - art method COMPARE it. Generic are attack, and method. OtherScientificTerm is energy dissipation. Material is harmonic parts of speech. ","This paper proposes a new adversarial attack method against ASR network. The proposed attack is based on energy dissipation, where phase information is used instead of magnitude information. The method is simple and effective, and it achieves imperceptible perturbation. Experiments on harmonic parts of speech show that it outperforms the state-of-the-art method."
1771,SP:713c57555a88d922516f42e7ff0ddd5bfbd90a24,method USED-FOR self - supervised learning ( SSL ). theoretical model USED-FOR linear neural networks. weight decay coefficient USED-FOR features. algorithm COMPARE DirectPred. DirectPred COMPARE algorithm. DirectCopy HYPONYM-OF DirectSet($\alpha$ ). Method is eigen - decomposition. ,"This paper proposes a method for self-supervised learning (SSL) based on eigen-decomposition. The authors propose a theoretical model for linear neural networks and propose a new algorithm called DirectCopy (DirectSet($\alpha$), which is a generalization of DirectPred. The key idea is to use a weight decay coefficient to decompose the features."
1772,SP:713c57555a88d922516f42e7ff0ddd5bfbd90a24,"ncSSL COMPARE contrastive learning. contrastive learning COMPARE ncSSL. it USED-FOR threshold. threshold USED-FOR noisy features. Method is prior analysis. OtherScientificTerm are data augmentation, stable features, and Contrastive Pairs. Task is Self - Supervised Learning Dynamics. ","This paper presents a prior analysis of the effect of data augmentation on the performance of ncSSL compared to contrastive learning. In particular, it proposes a new threshold for noisy features and shows that it can be used as a threshold to distinguish noisy features from stable features. Contrastive Pairs are also proposed. The paper is well-written and well-motivated. The main contribution of this paper is the introduction of the concept of Self-Supervised Learning Dynamics."
1773,SP:713c57555a88d922516f42e7ff0ddd5bfbd90a24,"data distribution CONJUNCTION augmentation process. augmentation process CONJUNCTION data distribution. they USED-FOR representations. BYOL CONJUNCTION Sim - Siam. Sim - Siam CONJUNCTION BYOL. data distribution USED-FOR representations. augmentation process USED-FOR representations. Sim - Siam HYPONYM-OF non - contrastive SSL methods. BYOL HYPONYM-OF non - contrastive SSL methods. Metric is sample - complexity. Method are DirectPred, and predictor network. Generic is method. OtherScientificTerm is features. ","This paper proposes DirectPred, a method to improve the sample-complexity of non-contrastive SSL methods (e.g., BYOL, Sim-Siam). The main idea is to train a predictor network that predicts the next layer of the data distribution and the augmentation process, and then use they to learn representations that are more robust to changes in the underlying data distribution or to the different types of augmentation processes. The proposed method is called DirectPred. The main contribution of the paper is to show that the features learned by the predictor network can be used to augment the representations learned by BYOL and Sim-siam."
1774,SP:713c57555a88d922516f42e7ff0ddd5bfbd90a24,"algorithms USED-FOR non - conctrastive self - supervised learning. DirectSet(\alpha ) USED-FOR non - conctrastive self - supervised learning. non - contrastive self - supervision USED-FOR demystifying representation learning. positive pairs USED-FOR non - conctrastive self - supervised learning. other HYPONYM-OF linear subspaces. algorithm USED-FOR projection matrix. invariant sub - space FEATURE-OF projection matrix. method COMPARE DirectPred. DirectPred COMPARE method. OtherScientificTerm are linear layers, data distribution assumption, and data augmentations. Generic are this, and representation. ","This paper proposes two algorithms for non-conctrastive self-supervised learning with DirectSet(\alpha) and DirectPred(\alpha). The first algorithm is motivated by the problem of demystifying representation learning with non-contrastive non-supervision, i.e., using positive pairs. The second algorithm aims to learn a projection matrix in an invariant sub-space (one of the linear subspaces and the other of the other). The main difference between these two algorithms is that the linear layers are assumed to be invariant to the data distribution assumption, which is not the case for DirectPred. The authors argue that this is due to the fact that the representation is invariant under different data augmentations. The experiments show that the proposed method outperforms DirectPred on a variety of datasets."
1775,SP:8433900e40c5c5df1f003dd1d4fb08c7aafd51f8,"circuit USED-FOR discretized step. discretized step FEATURE-OF multi - scale ODEs. dynamical systems CONJUNCTION multi - scale dynamical systems. multi - scale dynamical systems CONJUNCTION dynamical systems. class USED-FOR LSTMs. dynamical systems USED-FOR dynamical system. hidden state values CONJUNCTION gradients. gradients CONJUNCTION hidden state values. l_infinity FEATURE-OF upper bounds. upper bounds FEATURE-OF gradients. architectural component USED-FOR sequence problems. Method is neural networks. Generic is system. Metric is representation capability. Task are exploding gradient problem, and vanishing gradient problem. OtherScientificTerm is partial gradient. ","This paper studies the problem of learning neural networks that can represent a sequence of points in a system. The authors consider multi-scale ODEs with a discretized step in a circuit, and show that this class of LSTMs is equivalent to learning a dynamical system with dynamical systems, and also to learning the representation capability of the system. They also provide upper bounds on the l_infinity of the hidden state values and gradients, which they call the exploding gradient problem. Finally, they show that the partial gradient can be used as an architectural component to solve sequence problems, and that the vanishing gradient problem can be solved."
1776,SP:8433900e40c5c5df1f003dd1d4fb08c7aafd51f8,time constants FEATURE-OF system of ODEs. long expressive memory ( LEM ) HYPONYM-OF RNN architecture. it USED-FOR vanishing gradient problem. it USED-FOR dynamical systems. prediction EVALUATE-FOR LEM - based sequence models. tasks EVALUATE-FOR LEM - based sequence models. ,This paper proposes a novel RNN architecture called long expressive memory (LEM) that can store time constants of a system of ODEs. The authors claim that it can be used to solve the vanishing gradient problem in dynamical systems. Experiments on several tasks demonstrate the effectiveness of LEM-based sequence models in terms of prediction performance.
1777,SP:8433900e40c5c5df1f003dd1d4fb08c7aafd51f8,architecture USED-FOR recurrent networks. it USED-FOR dynamical systems. dynamical systems CONJUNCTION multiscale dynamical systems. multiscale dynamical systems CONJUNCTION dynamical systems. it USED-FOR multiscale dynamical systems. sequential MNIST / CIFAR-10 classification CONJUNCTION EigenWorms classification. EigenWorms classification CONJUNCTION sequential MNIST / CIFAR-10 classification. Speech Commands dataset CONJUNCTION character - level modeling. character - level modeling CONJUNCTION Speech Commands dataset. synthetic two - scale dynamical system CONJUNCTION sequential MNIST / CIFAR-10 classification. sequential MNIST / CIFAR-10 classification CONJUNCTION synthetic two - scale dynamical system. heart rate prediction CONJUNCTION Speech Commands dataset. Speech Commands dataset CONJUNCTION heart rate prediction. EigenWorms classification CONJUNCTION heart rate prediction. heart rate prediction CONJUNCTION EigenWorms classification. problem HYPONYM-OF synthetic two - scale dynamical system. EigenWorms classification HYPONYM-OF synthetic two - scale dynamical system. Penn Treebank USED-FOR character - level modeling. architecture COMPARE methods. methods COMPARE architecture. tasks EVALUATE-FOR architecture. OtherScientificTerm is fine discretization. Method is gradient propagation. ,"This paper proposes a new architecture for recurrent networks, and applies it to dynamical systems and multiscale dynamical Systems. The main idea is to use fine discretization, and to use gradient propagation. Experiments are conducted on a synthetic two-scale dynamical system (the problem is called “EigenWorms classification”), sequential MNIST/CIFAR-10 classification, EigenWorns classification, heart rate prediction, Speech Commands dataset, and character-level modeling on Penn Treebank. Results show that the proposed architecture outperforms existing methods on all tasks."
1778,SP:8433900e40c5c5df1f003dd1d4fb08c7aafd51f8,vanishing gradient CONJUNCTION exploding gradient. exploding gradient CONJUNCTION vanishing gradient. recurrent architecture USED-FOR recurrent models. architecture USED-FOR numerical discretization of ODEs. implicit - explicit time - stepping scheme USED-FOR numerical discretization of ODEs. model architecture USED-FOR multiscale data. Hodgkin - Huxley equations CONJUNCTION heterogeneous multiscale methods. heterogeneous multiscale methods CONJUNCTION Hodgkin - Huxley equations. vanilla LSTM CONJUNCTION Hodgkin - Huxley equations. Hodgkin - Huxley equations CONJUNCTION vanilla LSTM. heterogeneous multiscale methods USED-FOR ODEs. method CONJUNCTION vanilla LSTM. vanilla LSTM CONJUNCTION method. methods USED-FOR gradient exploding and vanishing issues. methods USED-FOR informative representations. informative representations USED-FOR long / short sequence data. methods COMPARE baselines. baselines COMPARE methods. Generic is tasks. ,"This paper proposes a new recurrent architecture for recurrent models that can handle multiscale data. The proposed architecture allows for numerical discretization of ODEs using an implicit-explicit time-stepping scheme. The authors compare the proposed method with vanilla LSTM, Hodgkin-Huxley equations, and heterogeneous multiiscale methods for ODE and show that the proposed methods are able to handle gradient exploding and vanishing issues, and can also learn informative representations for long/short sequence data. Experiments are conducted on several tasks and the authors show improved performance compared to the baselines."
1779,SP:dbf896dd31627b27f0a902c716aff940e5ab7ac2,"method USED-FOR learning functions. small point clouds USED-FOR learning functions. attention framework USED-FOR Permutation equivariance. Crystal structure identification CONJUNCTION Molecular force regression. Molecular force regression CONJUNCTION Crystal structure identification. Molecular force regression CONJUNCTION backmapping of coarse - grained operators. backmapping of coarse - grained operators CONJUNCTION Molecular force regression. backmapping of coarse - grained operators USED-FOR molecule simulations. Crystal structure identification EVALUATE-FOR model. scientific tasks EVALUATE-FOR model. backmapping of coarse - grained operators HYPONYM-OF scientific tasks. Crystal structure identification HYPONYM-OF scientific tasks. Molecular force regression HYPONYM-OF scientific tasks. Generic are they, and it. ","This paper proposes a method for learning functions on small point clouds. Permutation equivariance is achieved using an attention framework. The proposed model is evaluated on three scientific tasks: Crystal structure identification, Molecular force regression, and backmapping of coarse-grained operators to molecule simulations. The experiments show that the proposed method is able to generalize to unseen points in the point clouds, and that they can generalize well to unseen point clouds as well. The authors also show that it can generalise to unseen locations."
1780,SP:dbf896dd31627b27f0a902c716aff940e5ab7ac2,geometric algebra attention network USED-FOR small point clouds. geometric algebra ’s multivector USED-FOR attention. crystal structure identification CONJUNCTION molecular force regression. molecular force regression CONJUNCTION crystal structure identification. molecular force regression CONJUNCTION back mapping of coarse - graining operators. back mapping of coarse - graining operators CONJUNCTION molecular force regression. geometric algebra attention USED-FOR domain - specific applications. back mapping of coarse - graining operators HYPONYM-OF domain - specific applications. crystal structure identification HYPONYM-OF domain - specific applications. molecular force regression HYPONYM-OF domain - specific applications. OtherScientificTerm is equivariance. ,"This paper proposes a geometric algebra attention network for small point clouds. The attention is based on geometric algebra’s multivector, which allows for equivariance. The authors demonstrate the effectiveness of geometric algebra algebra attention on several domain-specific applications such as crystal structure identification, molecular force regression, and back mapping of coarse-graining operators."
1781,SP:dbf896dd31627b27f0a902c716aff940e5ab7ac2,rotation and permutation equivariant geometric deep learning model USED-FOR problems. geometric algebra formulations USED-FOR equivariance properties. geometric products of multivectors CONJUNCTION permutation - equivariance. permutation - equivariance CONJUNCTION geometric products of multivectors. invariant terms FEATURE-OF attention mechanism. attention mechanism USED-FOR geometric products of multivectors. attention mechanism USED-FOR permutation - equivariance. geometric products of multivectors USED-FOR rotation - equivariance. model COMPARE approaches. approaches COMPARE model. applications EVALUATE-FOR approaches. applications EVALUATE-FOR model. features FEATURE-OF models. attention maps HYPONYM-OF features. OtherScientificTerm is small points clouds. Generic is products. ,"This paper proposes a rotation and permutation equivariant geometric deep learning model for solving problems with small points clouds. The authors use geometric algebra formulations to derive equivariance properties and propose geometric products of multivectors, permutation-equivariance based on an attention mechanism with invariant terms. The proposed model is evaluated on a variety of applications and compared to existing approaches. The paper shows that the proposed models are able to generalize to new features such as attention maps, and that these products are invariant."
1782,SP:dbf896dd31627b27f0a902c716aff940e5ab7ac2,"rotation - invariance CONJUNCTION permutation equivariance. permutation equivariance CONJUNCTION rotation - invariance. geometric product of geometric algebra USED-FOR rotation - invariant attributes. geometric product of geometric algebra USED-FOR One. geometric products USED-FOR permutation - equivariant reduction. attention mechanism USED-FOR permutation - equivariant reduction. crystal structure identification CONJUNCTION molecular force regression. molecular force regression CONJUNCTION crystal structure identification. applications USED-FOR physical science. molecular force regression CONJUNCTION backmapping of coarse - graining operators. backmapping of coarse - graining operators CONJUNCTION molecular force regression. crystal structure identification HYPONYM-OF applications. molecular force regression HYPONYM-OF applications. backmapping of coarse - graining operators HYPONYM-OF physical science. crystal structure identification HYPONYM-OF physical science. molecular force regression HYPONYM-OF physical science. Method is deep neural network. Generic are method, and other. ","This paper proposes a deep neural network that learns rotation-invariant and permutation-equivariant representations of the input space. The method consists of two parts: One is based on a geometric product of geometric algebra that aims to learn rotation- invariant attributes (i.e., the rotation of a point in a space), and the other uses geometric products to learn permutation - equivariant reduction using an attention mechanism. The authors demonstrate the effectiveness of the proposed method on several applications in physical science, including crystal structure identification, molecular force regression, and backmapping of coarse-graining operators."
1783,SP:5a6099feb5da2c35f99d4d76c7e0ff3cd3e9c196,"GNNs USED-FOR imitation learning. imitation learning USED-FOR order fulfillment problem. SCIP package USED-FOR expert policy. tripartite graph of orders USED-FOR Graph Attention Transformer. Graph Attention Transformer USED-FOR model. heuristic COMPARE method. method COMPARE heuristic. Pointer Network CONJUNCTION GAT. GAT CONJUNCTION Pointer Network. heuristic CONJUNCTION Pointer Network. Pointer Network CONJUNCTION heuristic. inference latency EVALUATE-FOR method. Task is fulfillment problem. OtherScientificTerm are hierarchy in order, warehouses, and edge features. Method is behavior cloning. ","This paper studies imitation learning in GNNs for imitation learning for order fulfillment problem. In order to solve the fulfillment problem, the authors propose to learn an expert policy using the SCIP package. The model is based on Graph Attention Transformer, which takes as input a tripartite graph of orders and learns a hierarchy in order. The authors compare the performance of the heuristic and the method with a Pointer Network and a GAT, and show that the inference latency of the method is reduced by a factor of 1.5 when the number of warehouses is small. They also show that behavior cloning can be used to improve the quality of the learned edge features."
1784,SP:5a6099feb5da2c35f99d4d76c7e0ff3cd3e9c196,"Graph Neural Network ( GNN ) model USED-FOR supervised learning of optimal solutions. supervised learning of optimal solutions USED-FOR order fulfillment problem. order fulfillment problem PART-OF supply chain management. GNN USED-FOR problem. nodes PART-OF GNN model. Nodes CONJUNCTION edges. edges CONJUNCTION Nodes. feature vectors USED-FOR edges. features USED-FOR GNN. optimal solutions USED-FOR binary classification formulation. optimal solutions USED-FOR model. non - ML heuristic CONJUNCTION exact solver. exact solver CONJUNCTION non - ML heuristic. randomly generated instances EVALUATE-FOR GNN. GNN COMPARE GNN solutions. GNN solutions COMPARE GNN. GNN solutions CONJUNCTION non - ML heuristic. non - ML heuristic CONJUNCTION GNN solutions. GNN USED-FOR near - optimal solutions. near - optimal solutions COMPARE GNN solutions. GNN solutions COMPARE near - optimal solutions. GNN solutions COMPARE exact solver. exact solver COMPARE GNN solutions. problem EVALUATE-FOR GNN. OtherScientificTerm are orders, warehouses, and Edges. ","This paper proposes a Graph Neural Network (GNN) model for supervised learning of optimal solutions for order fulfillment problem in supply chain management. The problem is formulated as a GNN with nodes, edges, and feature vectors. The model is trained using optimal solutions in a binary classification formulation. Edges are chosen based on the number of orders in the warehouse. The GNN is evaluated on randomly generated instances and compared to GNN solutions with a non-ML heuristic and an exact solver. The results show that the GNN can learn near-optimal solutions, which is better than the original GNN."
1785,SP:5a6099feb5da2c35f99d4d76c7e0ff3cd3e9c196,graph based deep learning model USED-FOR order fulfillment problem. SCIP HYPONYM-OF MIP solver. graph neural network USED-FOR optimal solution. edge features CONJUNCTION nodes. nodes CONJUNCTION edge features. supervised learning USED-FOR dataset. edge features PART-OF graph neural network. supervised learning USED-FOR graph neural network. test problem sizes EVALUATE-FOR model. Generic is problem. ,"This paper proposes a graph based deep learning model for the order fulfillment problem. The authors propose SCIP, an MIP solver for this problem, which uses a graph neural network with supervised learning on the dataset to find the optimal solution. The model is evaluated on several test problem sizes."
1786,SP:5a6099feb5da2c35f99d4d76c7e0ff3cd3e9c196,heuristic graph based machine learning model USED-FOR order fulfillment problems. tripartite graph USED-FOR order fulfillment problems. feed forward ( FF ) layer CONJUNCTION assignment layer. assignment layer CONJUNCTION feed forward ( FF ) layer. graph attention network ( GAT ) USED-FOR features. graph attention network ( GAT ) CONJUNCTION feed forward ( FF ) layer. feed forward ( FF ) layer CONJUNCTION graph attention network ( GAT ). parts PART-OF model. graph attention network ( GAT ) PART-OF parts. feed forward ( FF ) layer PART-OF parts. assignment layer PART-OF parts. graph attention network ( GAT ) PART-OF model. assignment layer PART-OF model. feed forward ( FF ) layer PART-OF model. Task is mixed integer programming problem. ,This paper proposes a heuristic graph based machine learning model for order fulfillment problems on a tripartite graph. The model consists of three parts: a graph attention network (GAT) that learns the features and a feed forward (FF) layer and an assignment layer. Experiments are conducted on a mixed integer programming problem.
1787,SP:7f7f8245914ecc5b00570916bbcdb6c9b49d26de,"VisDial dataset USED-FOR task. MSCOCO captioning dataset USED-FOR VisDial dataset. dialogue USED-FOR reference caption. WordNet - based F1 metric EVALUATE-FOR model. WordNet - based F1 metric EVALUATE-FOR Summarization models. Task is dialogue summarization task. Method are CODC, and CoDC. Material are image, image's caption, and images. OtherScientificTerm is dialogue summary. ",This paper proposes a dialogue summarization task. The task is based on the VisDial dataset from the MSCOCO captioning dataset. The key idea is to use the dialogue as a reference caption and then use CODC to convert the image to the image's caption. Summarization models are evaluated on a WordNet-based F1 metric and the proposed model is compared with CoDC. The results show that the proposed CoDC is able to convert images to dialogue summary.
1788,SP:7f7f8245914ecc5b00570916bbcdb6c9b49d26de,ML model USED-FOR concepts. rules PART-OF wordnet. noun phrases USED-FOR concepts. metrics CONJUNCTION model. model CONJUNCTION metrics. model EVALUATE-FOR systems. systems USED-FOR CODC problems. metrics EVALUATE-FOR systems. Generic is problem. Task is inferencing Concepts. ,"This paper proposes an ML model for inferencing concepts from noun phrases. The problem is formulated as a wordnet, where the rules of the wordnet are used to represent concepts, and the ML model is trained to generate concepts from these rules. The authors show that their systems are able to solve CODC problems using a variety of metrics and model, and show that the systems can generalize to other types of problems. The paper is well-written and well-motivated. However, there are a few issues with the paper: (1) it is unclear how the problem is framed, (2) the authors do not provide a clear definition of the problem, (3) there is no discussion of how the concepts are generated, and (4) the paper does not provide any discussion of the importance of the concepts. Concepts are not clearly defined in the paper. "
1789,SP:7f7f8245914ecc5b00570916bbcdb6c9b49d26de,them USED-FOR text generator. ConceptNet USED-FOR model. CODC evaluation CONJUNCTION manual human evaluation. manual human evaluation CONJUNCTION CODC evaluation. automatic evaluation CONJUNCTION CODC evaluation. CODC evaluation CONJUNCTION automatic evaluation. CODC evaluation EVALUATE-FOR baselines. manual human evaluation EVALUATE-FOR baselines. automatic evaluation EVALUATE-FOR baselines. Task is dialogue summarization. Generic is concepts. OtherScientificTerm is CODC. Metric is evaluation metrics. ,"This paper addresses the problem of dialogue summarization. The authors propose a model based on ConceptNet, which learns concepts and uses them as input to a text generator. The evaluation metrics are based on CODC, and the authors compare the performance of the proposed baselines on automatic evaluation (with a small number of examples), and COCO and manual human evaluation."
1790,SP:e1591b266d6c329c6c07f4e5234253249ab1db8c,"score functions USED-FOR vector embeddings. embeddings spaces USED-FOR modeling monotonic and non - monotonic reasoning rules. vectors HYPONYM-OF embeddings spaces. Relu based model USED-FOR monotonic and non - monotonic reasoning. Method are monotonic reasoning, and vector - based embedding and label functions. OtherScientificTerm is monotonic reasoning part. ","This paper studies the problem of modeling monotonic and non-monotonic reasoning rules in embeddings spaces (i.e. vectors). In particular, the authors consider the case where the score functions for the vector embedding are non-convex. The authors propose a vector-based embedding and label functions, which can be used to model both monotonically and monodependently. They also propose a Relu based model that can model both the monotonal and non -monotonal reasoning. The monotony reasoning part can be viewed as a generalization of the previous work."
1791,SP:e1591b266d6c329c6c07f4e5234253249ab1db8c,them USED-FOR semantic dependencies. embedding approaches USED-FOR logical / structured attribute representations. known logical or structured dependencies FEATURE-OF logical / structured attribute representations. embedding models USED-FOR logical rules. OtherScientificTerm is entity embeddings. ,"This paper proposes a set of embedding approaches to learn logical/structured attribute representations with known logical or structured dependencies, and uses them to model semantic dependencies between entity embeddings. In particular, the authors propose embedding models that learn logical rules that can be used to represent logical rules. "
1792,SP:e1591b266d6c329c6c07f4e5234253249ab1db8c,"they USED-FOR logical dependencies. representational capacity EVALUATE-FOR KB embedding models. logical dependencies FEATURE-OF knowledge base. model USED-FOR dependencies. entity embeddings USED-FOR attribute representations. OtherScientificTerm is monotonic and non - monotonic dependencies. Generic is them. Material are real data, and real - world data settings. ","This paper studies the representational capacity of KB embedding models and how they capture logical dependencies in the knowledge base. The authors show that the model is able to capture these dependencies in both monotonic and non-monotonic dependencies. They also show that entity embeddings can be used to learn attribute representations, and show that using them leads to better performance on real data. The paper is well-written and well-motivated, and the experiments are well-designed. The experiments are conducted on real-world data settings."
1793,SP:794cca5205d667900ceb9a1332b6272320752ef4,"BERT USED-FOR tasks. BERT USED-FOR logical reasoning. logical reasoning USED-FOR tasks. model USED-FOR reasoning. natural language USED-FOR reasoning. facts CONJUNCTION rules. rules CONJUNCTION facts. BERT - based model USED-FOR reasoning tasks. text understanding CONJUNCTION mathematical reasoning. mathematical reasoning CONJUNCTION text understanding. text understanding HYPONYM-OF models. mathematical reasoning HYPONYM-OF models. Parity HYPONYM-OF tasks. OtherScientificTerm are negations, word - order, and background knowledge. Method are BERT - based models, deductive reasoning, transformers, and transformer models. Generic is them. ","This paper presents a study of BERT-based models for reasoning in natural language. BERT has been widely used for tasks that require logical reasoning, but BERT is not well suited for tasks involving negations. This paper proposes a model for reasoning with natural language that can handle reasoning with facts, rules, and sentences. The authors show that the proposed model is able to generalize to reasoning tasks that do not require knowledge of negations (e.g., word-order, etc.). The authors also show that their proposed model can generalize well to other reasoning tasks, such as text understanding, mathematical reasoning, and text understanding with proofs. The paper also shows that their BERT - based model generalizes well to a variety of reasoning tasks such as Parity. The main contribution of the paper is that they show that deductive reasoning can be done with transformers, and that their transformer models can be used to generalise to new tasks without requiring any additional background knowledge. They also show how to train them and show that they generalize better."
1794,SP:794cca5205d667900ceb9a1332b6272320752ef4,"reasoning FEATURE-OF transformer - based models. mispriming CONJUNCTION negations. negations CONJUNCTION mispriming. models USED-FOR heuristics. word order FEATURE-OF under - sensitivity. Horn clauses FEATURE-OF patterns. natural language inference CONJUNCTION mathematical reasoning. mathematical reasoning CONJUNCTION natural language inference. they USED-FOR natural language understanding tasks. mathematical reasoning HYPONYM-OF natural language understanding tasks. natural language inference HYPONYM-OF natural language understanding tasks. OtherScientificTerm are commonsense reasoning, and temporal knowledge. ","This paper studies the problem of reasoning in transformer-based models. The authors point out that models trained for heuristics such as mispriming and negations often fail to generalize to commonsense reasoning. They also point out under-sensitivity to word order and to patterns such as Horn clauses. Finally, they show that models that do not require temporal knowledge perform poorly on natural language understanding tasks such as natural language inference and mathematical reasoning."
1795,SP:794cca5205d667900ceb9a1332b6272320752ef4,"transformer based models USED-FOR reasoning. transformers USED-FOR reasoning. transformers USED-FOR tasks. OtherScientificTerm are lexical overlap, and word order. ",This paper studies the problem of reasoning in transformer based models. The authors show that transformers can be used to perform reasoning in a variety of tasks. They show that the lexical overlap between the input and output of transformers is due to the word order. They also show that there is a correlation between the number of tokens and the length of tokens.
1796,SP:3a16ffa27e7ef0684e6d0f3ee744787aef108a07,"typed input - output data USED-FOR function. English CONJUNCTION Spanish. Spanish CONJUNCTION English. English CONJUNCTION English. English CONJUNCTION English. subfunction USED-FOR representation. RL agent USED-FOR subfunction. subfunctions CONJUNCTION RL agent. RL agent CONJUNCTION subfunctions. neural networks USED-FOR RL agent. input -output data USED-FOR neural networks. subfunctions USED-FOR RL agent. Generic are it, solution, and algorithm. OtherScientificTerm are input - output type, and analogy. Material is French. Method is compositional recursive learner ( CRL ). ","This paper proposes a compositional recursive learner (CRL) that learns a function from typed input-output data, and then uses it to learn a representation of a given input and output type. The authors propose a solution to this problem, which is based on the idea of learning a representation from a subfunction that takes as input the representation of the original input and outputs the subfunction of the corresponding representation. The proposed algorithm is called Compositional Recursive Recursive Learning (CRL), and the authors show that it can be used to learn representations of English, English, Spanish, and French. They also show that neural networks trained on the input-input data can learn the subfunctions of the RL agent, and show that the learned representation of French is similar to the representation learned in English. They show that this analogy can be extended to other languages as well."
1797,SP:3a16ffa27e7ef0684e6d0f3ee744787aef108a07,"model USED-FOR problems. compositional and recursive structure FEATURE-OF problems. transformations USED-FOR composite problem. it USED-FOR problem. differential functions CONJUNCTION controller ( policy ). controller ( policy ) CONJUNCTION differential functions. parts PART-OF CRL model. controller ( policy ) PART-OF parts. differential functions HYPONYM-OF parts. controller ( policy ) PART-OF CRL model. differential functions PART-OF CRL model. controller USED-FOR function. back - propagation USED-FOR compositional function. policy gradient USED-FOR controller. unknown random sequence of affine transformations USED-FOR MNIST digits. CRL COMPARE RNN. RNN COMPARE CRL. arithmetic task EVALUATE-FOR RNN. arithmetic task EVALUATE-FOR CRL. MNIST EVALUATE-FOR CRL. Method is Compositional Recursive Learner ( CRL ). Task are recursive problem, translational problem, and real - world problems. Material are English, French, Spanish, and synthetic datasets. OtherScientificTerm is arithmetic expression. ","This paper proposes a model called Compositional Recursive Learner (CRL) for solving problems with compositional and recursive structure. The CRL model consists of two parts: differential functions and a controller (policy) that takes the differential functions as input and outputs a recursive problem. The authors define a composite problem as a set of transformations, and then use it to solve the problem. In particular, the authors propose to use back-propagation to learn a compositional function, which is a function that takes as input an arithmetic expression and outputs an output. The controller is trained using the policy gradient, and the authors show that the controller is able to solve a translational problem. CRL is evaluated on MNIST digits generated from an unknown random sequence of affine transformations and is compared to an RNN on an arithmetic task. The experiments are conducted on English, French, Spanish, and synthetic datasets. The results show that CRL outperforms the RNN in most of the experiments, and is also able to generalize to real-world problems."
1798,SP:7f91f3805bd643e3b796e885b00f88a77aa49d15,"integral model compression method USED-FOR weight and activation pruning. network weight CONJUNCTION activation sparsity. activation sparsity CONJUNCTION network weight. activation sparsity USED-FOR network computation. 2 - stage process USED-FOR solution. OtherScientificTerm are network weights, and activation. ","This paper proposes an integral model compression method for weight and activation pruning. The main idea is to reduce the network weight and the activation sparsity during the network computation. The proposed solution is a 2-stage process: first, the network weights are pruned and then the activation is pruned."
1799,SP:7f91f3805bd643e3b796e885b00f88a77aa49d15,"activation pruning CONJUNCTION weight pruning. weight pruning CONJUNCTION activation pruning. Integral Pruning ( IP ) USED-FOR Deep neural networks ( DNN ). computation cost EVALUATE-FOR Deep neural networks ( DNN ). approach USED-FOR Deep neural networks ( DNN ). activation sparsity USED-FOR DNN accelerator designs. technique USED-FOR IP. weight pruning USED-FOR IP. computation cost EVALUATE-FOR IP. dynamic activation masks USED-FOR activation sparsity. static masks USED-FOR weight pruning techniques. activation masks USED-FOR non - zero activation entries. threshold USED-FOR activation masks. validation data EVALUATE-FOR activation pruning. technique COMPARE weight pruning techniques. weight pruning techniques COMPARE technique. technique COMPARE intrinsic sparse ReLU activations. intrinsic sparse ReLU activations COMPARE technique. intrinsic sparse ReLU activations CONJUNCTION weight pruning techniques. weight pruning techniques CONJUNCTION intrinsic sparse ReLU activations. Method are exclusive weight pruning, ReLU, DNN, and deep networks. Generic is network. Metric are accuracy, and winner rate'measure. OtherScientificTerm is winner rates. ","This paper proposes Integral Pruning (IP), an approach to reduce the computation cost of training Deep neural networks (DNN) by combining activation pruning and weight pruning. The main idea is to use dynamic activation masks to encourage activation sparsity in DNN accelerator designs. The authors argue that this technique can be used to improve the performance of IP by reducing the computational cost compared to using only static masks. To do so, the authors propose exclusive weight Pruning, where the weights of the network are pruned only when the accuracy is less than a certain threshold. This threshold is defined as the number of activation masks required to produce non-zero activation entries.  The authors compare the proposed technique with intrinsic sparse ReLU activations and other weight pruned techniques, and show that the ReLU is more effective than the DNN. They also provide a 'winner rate' measure, which measures the difference between the winner rates of the two deep networks. Finally, they conduct experiments on validation data to show the effectiveness of activation pruned."
1800,SP:d34277109f713f78abd3b911c7a38baf18c8c8c1,knockoff method USED-FOR FDR control. FDR control USED-FOR feature selection problems. knockoff method USED-FOR feature selection problems. method USED-FOR distribution of features. knockoff framework USED-FOR input features. knockoff framework USED-FOR supervised feature selection setting. knockoffs CONJUNCTION real features. real features CONJUNCTION knockoffs. features CONJUNCTION FDR estimates. FDR estimates CONJUNCTION features. feature selction criteria USED-FOR features. feature selction criteria USED-FOR FDR estimates. OtherScientificTerm is FDR. Method is knockoff. ,"This paper proposes a novel knockoff method for FDR control in feature selection problems. The proposed method aims to estimate the distribution of features in a supervised setting where FDR is not available. Specifically, the proposed knockoff framework is applied to the input features in the supervised feature selection setting. The knockoffs and real features are compared using feature selction criteria to compare features and FDR estimates."
1801,SP:d34277109f713f78abd3b911c7a38baf18c8c8c1,GAN USED-FOR distributions. GAN USED-FOR feature selection method. W - GAN USED-FOR generative models. knockoff USED-FOR feature selection. W - GAN HYPONYM-OF works. knockoff HYPONYM-OF works. work USED-FOR knockoffs. multivariate Gaussian distribution USED-FOR feature distribution. knockoff COMPARE work. work COMPARE knockoff. distribution FEATURE-OF knockoffs. multivariate Gaussian distribution USED-FOR knockoff. OtherScientificTerm is prior knowledge. ,"This paper proposes a feature selection method based on GAN to learn distributions that are independent of prior knowledge. The main contribution of the paper is to propose a knockoff for feature selection, which is a variant of the W-GAN used in generative models. The paper compares the proposed knockoff with other works, such as the original knockoff and the work based on the multivariate Gaussian distribution of the feature distribution, and shows that the proposed work can learn knockoffs with a different distribution."
1802,SP:7bf79b020c2cafaced61f2595ad17e8238c3dc5d,spatial structured pruning CONJUNCTION Winograd direct pruning. Winograd direct pruning CONJUNCTION spatial structured pruning. spatial structured pruning PART-OF spatial - Winograd pruning framework. Winograd direct pruning PART-OF spatial - Winograd pruning framework. Winograd direct pruning USED-FOR sparsity. Winograd direct pruning USED-FOR Winograd domain. Winograd domain FEATURE-OF sparsity. framework COMPARE approaches. approaches COMPARE framework. MobileNet CONJUNCTION ShuffleNet. ShuffleNet CONJUNCTION MobileNet. OtherScientificTerm is spatial domain. Generic is state - of - the - art approaches. ,This paper proposes a spatial-Winograd pruning framework that combines spatial structured pruning with Winograd direct pruning to reduce the sparsity in the Winovar domain. The proposed framework is evaluated on MobileNet and ShuffleNet and compared to other state-of-the-art approaches.
1803,SP:7bf79b020c2cafaced61f2595ad17e8238c3dc5d,technique USED-FOR convolutional layers. winograd algorithm USED-FOR convolutions. image CONJUNCTION filter. filter CONJUNCTION image. multiplication USED-FOR Winograd convolutions. sparsity FEATURE-OF regular domain. sparsity FEATURE-OF winograd domain. OtherScientificTerm is image space. ,"This paper proposes a technique to reduce the number of convolutional layers in the training process. The main idea is to use the winograd algorithm for convolutions. Winograd convolutions can be viewed as multiplication of an image and a filter, where the image and the filter correspond to different points in the image space. The authors argue that the sparsity of the regular domain is not the same as that of the new domain, and that the same sparsity can be achieved in the new form of the original."
1804,SP:35e050c84f55f30b5a958128fa5bdaa1cb3f7e90,Generative Adversarial Networks ( GAN ) USED-FOR unsupervised and semi - supervised clustering. Neural network based generators USED-FOR sampling. mixture model USED-FOR Neural network based generators. discriminator USED-FOR generated distributions. MNIST and SVHN datasets EVALUATE-FOR models. Generic is generators. ,"This paper proposes Generative Adversarial Networks (GAN) for unsupervised and semi-supervised clustering. Neural network based generators for sampling are trained with a mixture model, where the generators are trained to maximize the similarity between the generated distributions generated by the discriminator. The proposed models are evaluated on MNIST and SVHN datasets."
1805,SP:35e050c84f55f30b5a958128fa5bdaa1cb3f7e90,generative model USED-FOR unsupervised and semi - supervised data clustering. method COMPARE latent   variable. latent   variable COMPARE method. continuous and discrete variables PART-OF latent   variable. ALI HYPONYM-OF methods. neural network USED-FOR methods. neural network USED-FOR conditional probability. clustering error rate EVALUATE-FOR methods. clustering error rate EVALUATE-FOR MNIST data. ,This paper proposes a generative model for unsupervised and semi-supervised data clustering. The proposed method is different from previous methods such as ALI in that the latent  variable consists of both continuous and discrete variables. The authors propose to use a neural network to estimate the conditional probability of each variable. Experiments on MNIST data show that the proposed method can reduce the clustering error rate compared to previous methods.
1806,SP:c65ea3a1cc796e65465e8b4dc05ae103316e2cb3,variance - reduction technique USED-FOR expected loss gradient. independent binary random variables USED-FOR expectation. discrete latent space USED-FOR VAEs. ,This paper proposes a variance-reduction technique to reduce the expected loss gradient. The expectation is modeled as independent binary random variables. The authors show that VAEs can be trained on a discrete latent space.
1807,SP:c65ea3a1cc796e65465e8b4dc05ae103316e2cb3,antithetic sampling USED-FOR variance - reduction mechanism. reparameterization USED-FOR antithetic sampling. augmented space FEATURE-OF antithetic sampling. ARM estimator USED-FOR prediction. computational complexity EVALUATE-FOR ARM estimator. OtherScientificTerm is binary layers. Method is binary neural networks. ,"This paper proposes a variance-reduction mechanism based on antithetic sampling in augmented space using reparameterization. The authors propose an ARM estimator for prediction with computational complexity of $O(\sqrt{T})$ for binary layers, which can be applied to binary neural networks."
1808,SP:c54ee7a7d321a487257d2554c7e689967cf0ceaa,"random variables CONJUNCTION probabilistic distribution. probabilistic distribution CONJUNCTION random variables. probabilistic distribution PART-OF modules. random variables USED-FOR modules. inference methods PART-OF modules. inference methods USED-FOR distribution. modules USED-FOR probabilistic distributions. probabilistic distributions CONJUNCTION inference methods. inference methods CONJUNCTION probabilistic distributions. Method are MXFusion language, probabilistic modules, and generic inference systems. ","This paper proposes a modification of the MXFusion language to allow for the use of probabilistic modules. The proposed modules consist of random variables, a probabilistically distribution, and inference methods to approximate the distribution. The authors show that the proposed modules can be used to model probabilism distributions and inference systems. The paper also provides a theoretical analysis of the properties of generic inference systems and provides some empirical results."
1809,SP:c54ee7a7d321a487257d2554c7e689967cf0ceaa,"TensorFlow CONJUNCTION PyTorch. PyTorch CONJUNCTION TensorFlow. probabilistic modules USED-FOR complex probabilistic models. languages USED-FOR deep learning. languages COMPARE language. language COMPARE languages. probabilistic modules PART-OF language. TensorFlow HYPONYM-OF deep learning. PyTorch HYPONYM-OF languages. TensorFlow HYPONYM-OF languages. inference USED-FOR probabilistic models. modularity USED-FOR probabilistic programming. inference methods PART-OF probabilistic modules. approximate inference USED-FOR module. inference technique USED-FOR approximate inference. inference technique USED-FOR module. deep kernel learning CONJUNCTION Bayesian Gaussian process latent variable model. Bayesian Gaussian process latent variable model CONJUNCTION deep kernel learning. Bayesian linear regression CONJUNCTION deep kernel learning. deep kernel learning CONJUNCTION Bayesian linear regression. MXFusion USED-FOR probabilistic models. Bayesian Gaussian process latent variable model HYPONYM-OF probabilistic models. Bayesian linear regression HYPONYM-OF probabilistic models. deep kernel learning HYPONYM-OF probabilistic models. Method are Probabilistic Programming Language ( PPL ) MXFusion, and approximate inference methods. ","This paper proposes a probabilistic Programming Language (PPL) MXFusion, which is a language that combines several existing languages for deep learning (TensorFlow, PyTorch, etc) with a few additional probabilistically modules to learn complex probiable models. The authors argue that this modularity is important for probabilistics programming and that languages with probabilism modules can be more efficient than language without probabilisms. The main contribution of this paper is to propose a new inference technique for approximate inference for each module in the proposed module. The inference methods in the probabilic modules of the proposed language can be seen as an extension of existing approximate inference methods. The paper shows that the proposed probabilists models trained with the proposed model of the probablistic programming language MXFusions are able to generalize well to other types of models trained using the same inference methods, such as Bayesian linear regression, deep kernel learning, and Bayesian Gaussian process latent variable model."
1810,SP:b65eb92fcbea57626721a156be6e6cbbad3c071c,SNIP USED-FOR neural network weights. SNIP USED-FOR prunable weights. normalised gradient USED-FOR prunable weights. method COMPARE methods. methods COMPARE method. Generic is criterion. OtherScientificTerm is gradient. ,"This paper proposes SNIP, a method for pruning neural network weights. SNIP aims to find prunable weights with normalised gradient. The authors propose a criterion for this criterion, which is based on the difference between the gradient of the weights of the original and pruned weights. The method is compared with several existing methods."
1811,SP:b65eb92fcbea57626721a156be6e6cbbad3c071c,"Method are pruning neural networks, nn, and retraining pruned nn. OtherScientificTerm are nodes, and connections. Generic are network, and it. ","This paper studies the problem of pruning neural networks. The main idea is to prune the nodes of the network so that the nn is more robust to changes in the input. This is done by retraining pruned nn on the original input. The idea is that if the network is pruned, the connections between the original nn and the retrained nn will be more similar, and it will be easier to retrain it. "
1812,SP:986b9781534ffec84619872cd269ad48d235f869,"beam search decoding algorithm USED-FOR recurrent models. recurrent models USED-FOR inference. BLEU HYPONYM-OF evaluation metrics. heuristic fix USED-FOR constrained decoding mechanism. OtherScientificTerm are beam widths, log - probabilities, and search discrepancies. ","This paper proposes a beam search decoding algorithm for recurrent models for inference of recurrent models. The main contribution of this paper is a heuristic fix to the constrained decoding mechanism, where beam widths are constrained to be small. The authors also propose two evaluation metrics, namely BLEU and BERU, which are based on the log-probabilities of the search discrepancies."
1813,SP:986b9781534ffec84619872cd269ad48d235f869,large beam widths USED-FOR NMT models. decoding USED-FOR training set predictions. beam widths USED-FOR decoding. summarization HYPONYM-OF tasks. NMT CONJUNCTION summarization / captioning tasks. summarization / captioning tasks CONJUNCTION NMT. ,"This paper studies the effect of large beam widths on the performance of NMT models. The authors show that the beam width affects the decoding for training set predictions, and that this affects the performance on other tasks such as summarization. The paper provides some empirical evidence that NMT and summarization/captioning tasks suffer from this issue."
1814,SP:b2a8f5c3a417390582f26981fe0c81c16d2bb07d,"method USED-FOR sparse reward RL methods. backward curriculum USED-FOR expert demonstrations. backward curriculum USED-FOR method. backward curriculum USED-FOR sparse reward RL methods. expert demonstration CONJUNCTION resettable simulator. resettable simulator CONJUNCTION expert demonstration. OtherScientificTerm are rewarding state, and curriculum. ",This paper proposes a method to train sparse reward RL methods using a backward curriculum for expert demonstrations and a resettable simulator. The goal is to learn a rewarding state that is similar to the one learned in the expert demonstrations. The curriculum is based on the assumption that the expert demonstration can be resettled to a new state.
1815,SP:b2a8f5c3a417390582f26981fe0c81c16d2bb07d,"strategy USED-FOR sparse reward tasks. RL USED-FOR sparse reward tasks. sample complexity EVALUATE-FOR method. sample complexity EVALUATE-FOR MDP. MDP EVALUATE-FOR method. maze navigation task CONJUNCTION game Pommerman. game Pommerman CONJUNCTION maze navigation task. game Pommerman EVALUATE-FOR method. maze navigation task EVALUATE-FOR method. Method is Backplay. OtherScientificTerm are demonstration trajectory, and initial state distribution. ","This paper proposes Backplay, a strategy for solving sparse reward tasks in RL. The method is evaluated on a maze navigation task and a game Pommerman, where the sample complexity of the MDP is evaluated. The main idea is to use the demonstration trajectory as the initial state distribution."
1816,SP:426c98718b2dbad640380ec4ccb2b656958389bc,"multi - layer pruning method USED-FOR neural networks. MLPrune USED-FOR neural networks. MLPrune HYPONYM-OF multi - layer pruning method. It USED-FOR network. K - FAC USED-FOR Fisher matrix. it USED-FOR Fisher matrix. K - FAC USED-FOR it. network USED-FOR model. OtherScientificTerm are compression ratios, Hessian matrix, and connections. ","This paper proposes a multi-layer pruning method, MLPrune, for neural networks. It prunes the network using K-FAC, which computes the Fisher matrix for each layer based on the compression ratios. The Hessian matrix is then used to train the model by pruning the connections."
1817,SP:426c98718b2dbad640380ec4ccb2b656958389bc,approach USED-FOR neural network. Optimal Brain Surgeon method USED-FOR idea. Fisher information USED-FOR Hessian matrix. OtherScientificTerm is network parameters. Generic is algorithm. ,This paper proposes an approach to learn a neural network that is robust to changes in the network parameters. The idea is inspired by the Optimal Brain Surgeon method. The authors propose to use Fisher information to estimate the Hessian matrix of the parameters of the algorithm.
1818,SP:b97549a4c1f4b2407f97576fed46c25cbf669009,analytic framework USED-FOR GANs. analytic framework USED-FOR semantic segmentation model. semantic segmentation model USED-FOR GANs. unit ( feature map ) level FEATURE-OF GANs. semantic segmentation model USED-FOR parsing. Method is GAN representations. Task is synthesis of semantic objects. Generic is framework. ,"This paper proposes an analytic framework to train a semantic segmentation model for GANs at the unit (feature map) level. The authors argue that GAN representations are not expressive enough for the synthesis of semantic objects, and propose a framework to learn representations that are expressive enough. They also propose to use the semantic segmented model for parsing."
1819,SP:b97549a4c1f4b2407f97576fed46c25cbf669009,visualization framework USED-FOR generative neural network. generative neural network USED-FOR GAN models. object concepts PART-OF images. OtherScientificTerm is interpretable units. Generic is units. ,This paper proposes a visualization framework for training a generative neural network for GAN models. The idea is to represent object concepts in images as interpretable units. These units are then used to train a GAN model.
1820,SP:252c20661ef36f8c32f7412db315747925d3a3d0,L2 space FEATURE-OF distances. distances COMPARE parameter l2 distances. parameter l2 distances COMPARE distances. parameter l2 distances FEATURE-OF networks. constraint USED-FOR catastrophic forgetting. constraint USED-FOR permuted MNIST task. CNN ( CIFAR10 ) CONJUNCTION LSTM ( permuted MNIST ). LSTM ( permuted MNIST ) CONJUNCTION CNN ( CIFAR10 ). Hillber - constrained gradient descent ( HCGD ) HYPONYM-OF gradient descent algorithm. LSTM ( permuted MNIST ) EVALUATE-FOR it. CNN ( CIFAR10 ) EVALUATE-FOR it. Method is neural networks. Metric is l2 parameter distance. OtherScientificTerm is function space. ,This paper studies the problem of training neural networks with distances in the L2 space. The authors show that the distances of the learned networks are much smaller than the parameter l2 distances of these networks. They propose a constraint on the l2 parameter distance to prevent catastrophic forgetting and apply this constraint to the permuted MNIST task. They also propose a new gradient descent algorithm called Hillber-constrained gradient descent (HCGD) and evaluate it on CNN (CIFAR10) and LSTM (permutedMNIST) and show that it achieves state-of-the-art performance. The main contribution of this paper is to study the function space and to provide a theoretical analysis of the effect of the constraint.
1821,SP:252c20661ef36f8c32f7412db315747925d3a3d0,"functional regularization USED-FOR neural nets. method USED-FOR functional regularization. method USED-FOR neural nets. Hilbert norm HYPONYM-OF L2 norm. OtherScientificTerm are function space, catastrophic forgetting, and natural gradient. Method are synthetic multi - task variant of MNIST, gradient updates, and stochastic gradient style learning. ","This paper proposes a method for functional regularization for neural nets. The main idea is to use the L2 norm (i.e. Hilbert norm) of the function space to prevent catastrophic forgetting. The method is based on a synthetic multi-task variant of MNIST, where the gradient updates are sampled from the natural gradient. The experiments are conducted on stochastic gradient style learning."
1822,SP:f6cb7efaef82aff9849c8e157bfe5db5092a6271,biological systems FEATURE-OF temporal measurements. graph CONJUNCTION network. network CONJUNCTION graph. noise CONJUNCTION sparsity. sparsity CONJUNCTION noise. DyMoN USED-FOR biological high - throughput data sets. sparsity CONJUNCTION temporal resolution. temporal resolution CONJUNCTION sparsity. approach USED-FOR biological high - throughput data sets. method USED-FOR complex biological systems. Calcium imaging of visual cortex neurons CONJUNCTION T -- cell development. T -- cell development CONJUNCTION Calcium imaging of visual cortex neurons. T -- cell development CONJUNCTION Human embryonic stem cell differentiation. Human embryonic stem cell differentiation CONJUNCTION T -- cell development. thymus FEATURE-OF T -- cell development. simulated data sets CONJUNCTION face - recognition data set. face - recognition data set CONJUNCTION simulated data sets. simulated data sets EVALUATE-FOR method. face - recognition data set EVALUATE-FOR method. features FEATURE-OF NN. running time EVALUATE-FOR approaches. Generic is those. OtherScientificTerm is dependency structure. ,"This paper addresses the problem of temporal measurements in biological systems. The authors propose DyMoN, an approach to tackle biological high-throughput data sets with noise, sparsity, and temporal resolution. The idea is to combine a graph with a network that takes into account the dependency structure between the graph and the network. The method is applied to complex biological systems such as Calcium imaging of visual cortex neurons, T -- cell development in thymus, and Human embryonic stem cell differentiation. The proposed method is evaluated on simulated data sets and a face-recognition data set. Compared to existing approaches, the running time of the proposed approaches is significantly faster."
1823,SP:f6cb7efaef82aff9849c8e157bfe5db5092a6271,"hidden layer CONJUNCTION output layer. output layer CONJUNCTION hidden layer. Markovian functions FEATURE-OF output layer. Markovian functions FEATURE-OF hidden layer. OtherScientificTerm are Markovian setup, and conditional probabilities. Method is FFNN architecture. ",This paper proposes to use a Markovian setup where the hidden layer and the output layer are Markovians functions. The main idea is to use conditional probabilities to estimate the likelihood of the next state. The authors show that the FFNN architecture can be viewed as a special case of this.
1824,SP:4828e4160b70ea11e364b48db24cb68cdf86edfc,"connector network USED-FOR latent space. MNIST CONJUNCTION CIFAR. CIFAR CONJUNCTION MNIST. Task is unsupervised classification. Method are CycleGAN framework, and generator. OtherScientificTerm is image space. ","This paper addresses the problem of unsupervised classification. The authors propose a CycleGAN framework, where the latent space is modeled as a connector network, and the generator is trained to predict the output of the generator. Experiments are conducted on MNIST and CIFAR. Results show that the proposed CycleGAN can generalize to unseen images in the image space."
1825,SP:4828e4160b70ea11e364b48db24cb68cdf86edfc,CycleGAN USED-FOR unsupervised classification algorithm. discriminator network CONJUNCTION generator network. generator network CONJUNCTION discriminator network. it USED-FOR piece - wise linear mapping. Connector Network HYPONYM-OF piece - wise linear mapping. cycle - consistency loss USED-FOR learning objective. unsupervised image classification HYPONYM-OF problem. ,"This paper proposes CycleGAN, an unsupervised classification algorithm based on CycleGAN. Specifically, it learns a piece-wise linear mapping between a discriminator network and a generator network, which is called Connector Network. The learning objective is based on a cycle-consistency loss. The paper is well-motivated and well-written. The problem considered in this paper is a very important problem, i.e., unsuperunsupervised image classification."
1826,SP:d5f5f6a83f0290415ea94b3740a95360a8fa16e3,"cost FEATURE-OF permutation. them USED-FOR doubly stochastic matrices. Sinkhorn operator USED-FOR doubly stochastic matrices. gradient descent USED-FOR permutation. RNN CONJUNCTION CNN. CNN CONJUNCTION RNN. CNN HYPONYM-OF module. RNN HYPONYM-OF module. Generic are method, and algorithm. OtherScientificTerm are pairwise costs, Permutations, permuted sequence, and lattices. Method is optimization procedure. ","This paper proposes a method for computing the cost of a permutation of a sequence of matrices. Permutations can be viewed as pairwise costs, and the authors extend them to doubly stochastic matrices with the Sinkhorn operator. The authors propose an optimization procedure where the permuted sequence is represented as a set of lattices, and gradient descent is used to compute the cost for each permutation. The algorithm is shown to converge to a fixed point, which is then used to train a module such as an RNN or a CNN."
1827,SP:d5f5f6a83f0290415ea94b3740a95360a8fa16e3,algorithm USED-FOR permutation - invariant representation. permutation USED-FOR algorithm. reparameterization USED-FOR doubly - stochastic matrices. Sinkhorn operator USED-FOR reparameterization. Sinkhorn operator USED-FOR doubly - stochastic matrices. optimization USED-FOR cost function. pairwise ordering cost USED-FOR cost function. Method is permutation optimizations. Task is combinatorial optimization problem. OtherScientificTerm is optimization constraints. ,"This paper proposes an algorithm for learning permutation-invariant representation from a permutation. The main idea is to use the Sinkhorn operator as a reparameterization for doubly-stochastic matrices. The authors show that the permutation optimizations can be viewed as a combinatorial optimization problem, where the optimization constraints can be expressed as a pairwise ordering cost, which can be used as a cost function during optimization."
1828,SP:cf74c553bae2b1194beaba4df1545d35e66aa5b3,Projective Subspace Network ( PSN ) USED-FOR few - shot learning. subspace USED-FOR PSN. SVD USED-FOR subspace. projection error USED-FOR subspace. projection error USED-FOR method. subspace representation USED-FOR outliers. prototype USED-FOR subspace representation. Matching Networks CONJUNCTION Prototypical Networks. Prototypical Networks CONJUNCTION Matching Networks. Matching Networks USED-FOR it. Prototypical Networks USED-FOR it. ,"This paper proposes a Projective Subspace Network (PSN) for few-shot learning. PSN learns a subspace based on SVD. The proposed method uses projection error to learn the subspace. The subspace representation is learned from a prototype, which can be used to detect outliers. The method is trained using Matching Networks and Prototypical Networks."
1829,SP:cf74c553bae2b1194beaba4df1545d35e66aa5b3,"Matching Networks CONJUNCTION Prototypical Networks. Prototypical Networks CONJUNCTION Matching Networks. distance USED-FOR subspace. pure embedding space FEATURE-OF distance. top n left singular vectors USED-FOR class's subspace. normalized embeddings USED-FOR truncated Singular Value Decomposition. truncated Singular Value Decomposition USED-FOR low - dimensional subspace. top n left singular vectors USED-FOR truncated Singular Value Decomposition. masked - mean computation CONJUNCTION zero - mean cluster. zero - mean cluster CONJUNCTION masked - mean computation. model USED-FOR semi - supervised few - shot learning setting. zero - mean cluster USED-FOR distractor items. Mini - Imagenet CONJUNCTION Tiered - ImageNet. Tiered - ImageNet CONJUNCTION Mini - Imagenet. Mini - Imagenet EVALUATE-FOR few - shot learning setting. Tiered - ImageNet USED-FOR semi - supervised few - shot learning setting. Mini - Imagenet CONJUNCTION Mini - Imagenet. Mini - Imagenet CONJUNCTION Mini - Imagenet. Task is few - shot learning. Method are embedding - based approach, and prototypical networks. ","This paper studies few-shot learning and proposes an embedding-based approach, which is inspired by Matching Networks and Prototypical Networks. The key idea is to learn a low-dimensional subspace using truncated Singular Value Decomposition with normalized embeddings, which uses top n left singular vectors to represent the class's subspace, and a distance between the subspace and the pure embedding space. The model is trained in a semi-supervised few -shot learning setting on Mini-Imagenet and Tiered-ImageNet, where the masked-mean computation and the zero-mean cluster are used to detect distractor items. Experiments show that the proposed model outperforms prototypical networks."
1830,SP:d7544bc4a0ae3237daa207e789a522363fb5170d,"method USED-FOR meta - learning method. MAML USED-FOR meta - learning method. MAML HYPONYM-OF method. back - propagation of inner - loop USED-FOR context parameters. OtherScientificTerm are context and shared parameters, Shared - parameters, and outer - loop. Task is embedding. ","This paper proposes a method called MAML as a meta-learning method. The key idea is to learn both context and shared parameters. Shared-parameters are learned in the inner-loop, while the context parameters are learned via back-propagation of inner -loop. The outer-loop is used to learn the embedding."
1831,SP:d7544bc4a0ae3237daa207e789a522363fb5170d,"CAML HYPONYM-OF gradient - based meta - learning method. MAML HYPONYM-OF gradient - based meta - learning method. approach COMPARE MAML. MAML COMPARE approach. adaptation learning rate EVALUATE-FOR approach. Generic are It, and model. OtherScientificTerm are model parameters, task - independent parameters, and embedding. ","This paper proposes CAML, a gradient-based meta-learning method that is similar to MAML. It is based on the idea that the model parameters should be independent of task-independent parameters. The key difference is that the embedding of the model should not depend on the task. The proposed approach is shown to have a better adaptation learning rate than MAMPL."
1832,SP:8a5e86b6770a3c08f861fbf682296dc3a6c02204,"privatizer USED-FOR user data. loss function USED-FOR privatizer. information theoretic bound FEATURE-OF privacy loss. algorithm USED-FOR privatizer. Method is privacy framework. Material are unsanitized data, and sanitized data. OtherScientificTerm are utility, and user privacy. ",This paper proposes a new privacy framework for the case where the user has access to unsanitized data. The main idea is to learn a privatizer for user data using a loss function that maximizes the utility of the user privacy. The authors provide an information theoretic bound on the privacy loss and provide an algorithm for learning the privatizer. They also provide some empirical results on sanitized data to support their theoretical results.
1833,SP:8a5e86b6770a3c08f861fbf682296dc3a6c02204,"they USED-FOR learning models. algorithmic approach USED-FOR representation. KL divergence based privacy notion CONJUNCTION algorithmic approach. algorithmic approach CONJUNCTION KL divergence based privacy notion. Task is representing data records. OtherScientificTerm are hidden variable, sensitive hidden variable, and utility privacy trade - off. ",This paper studies the problem of representing data records and how they can be used for learning models. The authors propose a KL divergence based privacy notion and an algorithmic approach to learn the representation. The main idea is to use a hidden variable that is independent of the sensitive hidden variable. The utility privacy trade-off is discussed.
1834,SP:6b0e9a8f0c046a767dce8790489b3e90e12e2c46,trick USED-FOR GANs. trick USED-FOR stability. stability FEATURE-OF GANs. gradient FEATURE-OF generator. vanishing gradient problem USED-FOR GANs. OtherScientificTerm is discriminator. ,"This paper proposes a new trick to improve the stability of GANs. The main idea is to minimize the gradient of the generator, which is then used to train the discriminator. This is similar to the vanishing gradient problem that has been studied in GAN."
1835,SP:6b0e9a8f0c046a767dce8790489b3e90e12e2c46,stability EVALUATE-FOR GANs. decision boundary USED-FOR discriminator. FID scores EVALUATE-FOR approach. Generic is task. OtherScientificTerm is bitstring. ,"This paper aims to improve the stability of GANs. The authors propose a new task where the discriminator is trained on a decision boundary, where the bitstring is sampled from a fixed set of samples. The approach is evaluated on a variety of FID scores."
1836,SP:c210982ccdd134d4b293dbe144990398eefe1a86,"rotation - equivariant filters PART-OF CNN model. weight sharing HYPONYM-OF rotation - equivariant filters. rotation - equivariant CNN USED-FOR V1 cells. OtherScientificTerm are V1, RFs, and V1 receptive fields. Generic is model. Method is Gabor filters. ","This paper proposes to incorporate rotation-equivariant filters (e.g., weight sharing) into the CNN model. The key idea is to use V1 cells as input to the rotation of the CNN, and then use RFs to rotate the V1 receptive fields. The proposed model is based on the Gabor filters. The experiments show that the proposed rotations of the RFs can improve the performance of the rotation - equivariant CNN for V1 cell."
1837,SP:c210982ccdd134d4b293dbe144990398eefe1a86,"rotational equivariant neural network architecture CONJUNCTION sparse coding read - out layer. sparse coding read - out layer CONJUNCTION rotational equivariant neural network architecture. neuron responses CONJUNCTION network. network CONJUNCTION neuron responses. rotational equivariant architecture COMPARE CNN. CNN COMPARE rotational equivariant architecture. feature maps USED-FOR CNN. Material are mouse brain, and Visual stimuli. ",This paper proposes a rotational equivariant neural network architecture with a sparse coding read-out layer. The authors compare the performance of the learned neuron responses and network with a CNN trained on feature maps. Visual stimuli are generated from the mouse brain and the results show that the proposed rotation-equivariant architecture outperforms the CNN.
1838,SP:f17090812ace9c83d418b17bf165649232c223e3,distributed implementation USED-FOR signSGD. majority vote USED-FOR distributed implementation. majority vote USED-FOR signSGD. convergence guarantee FEATURE-OF signSGD. OtherScientificTerm is adversarial nodes. ,This paper proposes a distributed implementation of signSGD based on majority vote. The authors prove a convergence guarantee for sign SGD with respect to the number of adversarial nodes.
1839,SP:f17090812ace9c83d418b17bf165649232c223e3,"stochastic gradient USED-FOR updating. 1 / sqrt(T ) rate EVALUATE-FOR signSGD. Method is signSGD algorithm. OtherScientificTerm are minibatch size, gradient noise, and stochastic gradients. Metric is convergence rate. ","This paper studies the signSGD algorithm. The main idea is to use a stochastic gradient for the updating, and to minimize the minibatch size by minimizing the gradient noise. The authors show that the 1/sqrt(T) rate of the sign SGD can be improved to 1/T with stoched gradients. The convergence rate is also improved to 0.5."
1840,SP:0ceece0754a1fe9c46a978bb2854932905685fa4,Generative Adversarial Network ( GAN ) methodology USED-FOR distribution of limit orders. equity market FEATURE-OF distribution of limit orders. approach USED-FOR discrete ) structure. convolutional layers USED-FOR conditioning information. recurrent neural networks USED-FOR conditional Wasserstein - GAN. conditional Wasserstein - GAN USED-FOR modeling. OtherScientificTerm is time dependency. Material is synthetic and real data. ,"This paper proposes a Generative Adversarial Network (GAN) methodology for modeling the distribution of limit orders in the equity market. The proposed approach is motivated by the (discrete) structure of the market and the time dependency. The modeling is based on conditional Wasserstein-GAN with recurrent neural networks, where convolutional layers are used to encode conditioning information. Experiments are conducted on both synthetic and real data."
1841,SP:0ceece0754a1fe9c46a978bb2854932905685fa4,"GAN USED-FOR stock data. OtherScientificTerm are order stream, Markov chain, stationary distribution, and orders. Method is machine learning. Material is financial data. ","This paper studies the problem of using GAN on stock data, where the order stream is a Markov chain and the stationary distribution is a function of the number of orders. This is an important problem in machine learning, and the paper is well-motivated and well-written. The paper is clearly written and easy to follow. The experiments are conducted on financial data."
1842,SP:ba66503753b3c57781b435c55c47fc9f69450e65,perturbed rewards USED-FOR deep RL setting. noise FEATURE-OF reward function. ,This paper studies the problem of perturbed rewards in the deep RL setting. The main contribution of this paper is to study the effect of noise on the reward function. 
1843,SP:ba66503753b3c57781b435c55c47fc9f69450e65,"surrogate reward signal USED-FOR noise in reward signals. surrogate reward COMPARE reward signal. reward signal COMPARE surrogate reward. OtherScientificTerm are Bellman equation, finite and continuous rewards, and noise. Metric is convergence time. Generic is approach. ","This paper proposes to use a surrogate reward signal to mitigate the noise in reward signals. The main idea is to use the Bellman equation to show that if the surrogate reward is close to the reward signal, then the convergence time is O(1/\sqrt{T}) for both finite and continuous rewards. The paper also provides a theoretical analysis of the proposed approach."
1844,SP:0e62f75b81b696bf794932d0ceee60e9f665f1da,"RNNs USED-FOR NLP and speech recognition. gradient norms HYPONYM-OF step sizes. convergence speed EVALUATE-FOR RNNs. LNNs USED-FOR convergence of optimization. LNNs CONJUNCTION nonlinear neural networks. nonlinear neural networks CONJUNCTION LNNs. Method are deep neural networks, and linear neural networks ( LNN ). OtherScientificTerm are minimum validation loss, initialization, and hidden layers widths. ","This paper studies the convergence speed of RNNs in NLP and speech recognition. In particular, the authors consider deep neural networks (LNNs) and show that the convergence of optimization of LNNs converges to a minimum validation loss when the step sizes (i.e., gradient norms) are small (up to a factor of 1/\sqrt{T}). The authors also show that for linear neural networks and nonlinear neural networks, the convergence speeds of linear network convergence can be improved when the initialization is small and when the hidden layers widths are small."
1845,SP:0e62f75b81b696bf794932d0ceee60e9f665f1da,over - parametrization USED-FOR neural network training. Generic is algorithm. OtherScientificTerm is gradients. ,This paper studies the problem of over-parametrization in neural network training. The authors propose an algorithm that is able to estimate the gradients of the weights of a trained neural network. The main contribution of this paper is that the authors provide a theoretical analysis of this problem.
1846,SP:40e210d36298e2eafd06d9dc45312ea4fd586ade,"framework USED-FOR online combinatorial problems. reinforcement learning USED-FOR framework. global parameters CONJUNCTION succinct data structure. succinct data structure CONJUNCTION global parameters. succinct data structure USED-FOR MDP states. deep RL methods USED-FOR problem. mixture of input distributions USED-FOR models. hard distributions USED-FOR lower bounds. Generic are others, and algorithms. ","This paper proposes a framework for solving online combinatorial problems using reinforcement learning. The key idea is to use global parameters and a succinct data structure to represent MDP states. The authors show that deep RL methods can be applied to this problem, and provide lower bounds on the number of parameters required to solve the problem. They also show that models trained on a mixture of input distributions can achieve better performance than models trained with hard distributions. Finally, the authors provide some experiments to show the effectiveness of their algorithms."
1847,SP:40e210d36298e2eafd06d9dc45312ea4fd586ade,reinforcement learning USED-FOR online combinatorial optimization ( OCO ) problems. near - optimal strategies USED-FOR OCO tasks. reinforcement learners COMPARE near - optimal strategies. near - optimal strategies COMPARE reinforcement learners. policy gradient and DQN methods USED-FOR MDP framework. online budget allocation CONJUNCTION online knapsack. online knapsack CONJUNCTION online budget allocation. online knapsack CONJUNCTION secretary problem. secretary problem CONJUNCTION online knapsack. framework USED-FOR OCO tasks. online budget allocation HYPONYM-OF OCO tasks. secretary problem HYPONYM-OF OCO tasks. online knapsack HYPONYM-OF OCO tasks. model COMPARE near - optimal “ handcrafted ’’ algorithms. near - optimal “ handcrafted ’’ algorithms COMPARE model. ,"This paper studies the problem of using reinforcement learning to solve online combinatorial optimization (OCO) problems. The authors show that reinforcement learners can outperform the near-optimal strategies for OCO tasks. To this end, the authors propose an MDP framework based on policy gradient and DQN methods. The proposed framework is applied to three OCO scenarios: online budget allocation, online knapsack, and secretary problem. The experimental results show that the proposed model outperforms the state-of-the-art and near-optimistic “handcrafted’’ algorithms."
1848,SP:b99732087f5a929ab248acdcd7a943bce8671510,domain knowledge CONJUNCTION hyper - parameter tuning. hyper - parameter tuning CONJUNCTION domain knowledge. hyper - parameter tuning USED-FOR biases. domain knowledge USED-FOR biases. generality CONJUNCTION performance. performance CONJUNCTION generality. Method is deep reinforcement learning methods. OtherScientificTerm is inductive biases. Metric is generalization. ,This paper studies the effect of biases learned from domain knowledge and hyper-parameter tuning on the generalization performance of deep reinforcement learning methods. The authors argue that biases learned through domain knowledge or hyper-tuning can lead to better generalization. They also argue that inductive biases are beneficial for generality and performance.
1849,SP:b99732087f5a929ab248acdcd7a943bce8671510,heuristics USED-FOR reinforcement learning. reward clipping CONJUNCTION discounting. discounting CONJUNCTION reward clipping. discounting CONJUNCTION repeating actions. repeating actions CONJUNCTION discounting. repeating actions CONJUNCTION network structures. network structures CONJUNCTION repeating actions. discounting PART-OF heuristics. network structures PART-OF heuristics. reward clipping PART-OF heuristics. repeating actions PART-OF heuristics. training algorithms USED-FOR RL agents. ,"This paper proposes a set of heuristics for reinforcement learning, which include reward clipping, discounting, repeating actions, and network structures. The idea is interesting, and the paper is well-written and well-motivated. However, the paper suffers from a lack of comparison to existing training algorithms for RL agents."
1850,SP:47b0c8a984480eb353b36fd877d9775213fb1a5f,progress monitor CONJUNCTION visual - textual co - grounding module. visual - textual co - grounding module CONJUNCTION progress monitor. method USED-FOR vision+language navigation. visual - textual co - grounding module USED-FOR method. progress monitor USED-FOR method. benchmark EVALUATE-FOR method. Generic is model. ,This paper proposes a method for vision+language navigation based on a progress monitor and a visual-textual co-grounding module. The proposed method is evaluated on a benchmark and compared to several state-of-the-art methods. The results show that the proposed model is able to achieve better performance.
1851,SP:47b0c8a984480eb353b36fd877d9775213fb1a5f,"visual input USED-FOR visual - textual module. OtherScientificTerm is natural language route instructions. Material is images. Generic are ( "" self - aware "" ) approach, and architecture. Method is progress monitor. ","This paper proposes a (self-aware ""self-supervised"") approach to learning to follow natural language route instructions. The main idea is to use visual input to train a visual-textual module, which takes as input a sequence of images and outputs a progress monitor. The architecture is simple and easy to implement."
1852,SP:7e70c97e9b7b182e974b071c93baafef8b11cf90,partial / incomplete program execution USED-FOR guiding program synthesis. it COMPARE state - of - the - art. state - of - the - art COMPARE it. Karel dataset program synthesis task EVALUATE-FOR it. Karel dataset program synthesis task EVALUATE-FOR state - of - the - art. Method is ensembling synthesizers. ,"This paper addresses the problem of guiding program synthesis with partial/incomplete program execution. The authors propose ensembling synthesizers and evaluate it on a Karel dataset program synthesis task, where it outperforms the state-of-the-art."
1853,SP:7e70c97e9b7b182e974b071c93baafef8b11cf90,program semantics USED-FOR neural program synthesis approaches. ideas USED-FOR neural program synthesis approaches. program semantics USED-FOR ideas. execution based semantic information USED-FOR decoding. decoding USED-FOR program. ensembling approach USED-FOR synthesizers. majority vote CONJUNCTION shortest length criterion. shortest length criterion CONJUNCTION majority vote. majority vote USED-FOR program. shortest length criterion USED-FOR program. Karel synthesis domain EVALUATE-FOR ideas. ,"This paper proposes to use ideas from program semantics to improve neural program synthesis approaches. The main idea is to use execution based semantic information to guide decoding of the program. The authors propose an ensembling approach to train synthesizers, where the program is encoded using a majority vote and a shortest length criterion. The proposed ideas are evaluated on the Karel synthesis domain."
1854,SP:dc7dfc1eec473800580dba309446871122be6040,batch normalization USED-FOR ordinary least square ( OLS ) objective. convergence properties FEATURE-OF batch normalization. convergence properties FEATURE-OF ordinary least square ( OLS ) objective. OLS objective CONJUNCTION small scale neural networks. small scale neural networks CONJUNCTION OLS objective. Task is machine learning community. ,"This paper studies the convergence properties of batch normalization for the ordinary least square (OLS) objective. The paper is well-written and well-motivated, and the paper is of interest to the machine learning community. The main contribution of this paper is to provide a theoretical analysis of the OLS objective and small scale neural networks."
1855,SP:dc7dfc1eec473800580dba309446871122be6040,"theoretical analysis USED-FOR batch normalization. theoretical analysis USED-FOR scenario. ordinary least squares problem HYPONYM-OF scenario. gradient descent ( GDBN ) USED-FOR batch normalization. GDBN USED-FOR stationary point. OtherScientificTerm are learning rate, and condition number. ","This paper presents a theoretical analysis of batch normalization with gradient descent (GDBN) in a new scenario, the ordinary least squares problem. The main result is that GDBN converges to a stationary point when the learning rate is bounded by a factor of the condition number."
1856,SP:9984d73a1fcfce932cfcafb4d200f70b07723bf3,Bayesian RNNs USED-FOR data noising technique. linear interpolation CONJUNCTION Kneser - Ney smoothing. Kneser - Ney smoothing CONJUNCTION linear interpolation. linear interpolation USED-FOR those. Kneser - Ney smoothing USED-FOR those. Kneser - Ney smoothing HYPONYM-OF variants. linear interpolation HYPONYM-OF variants. those HYPONYM-OF variants. methods COMPARE baselines. baselines COMPARE methods. language modeling EVALUATE-FOR methods. technique USED-FOR sequence tasks. Task is word embedding noising. Method is trational data smoothing techniques. ,"This paper proposes a data noising technique based on Bayesian RNNs for word embedding noising. The authors propose two variants: linear interpolation and Kneser-Ney smoothing, which are variants of those proposed in [1] and [2]. The proposed methods are evaluated on language modeling and compared to several baselines. The technique is applied to sequence tasks and is shown to perform better than other trational data smoothing techniques."
1857,SP:9984d73a1fcfce932cfcafb4d200f70b07723bf3,"smoothing / regularization methods COMPARE approaches. approaches COMPARE smoothing / regularization methods. Generic are problem, task, and methods. Method is regularization approaches. Metric is perplexity values. Material is Penn Treebank. ","This paper studies the problem of smoothing/regularizing a problem where the task is difficult to solve. The authors propose several regularization approaches and show that their smoothing / regularization methods outperform existing approaches. They also show that the perplexity values of the smoothed/regularized versions of the problem can be used as a measure of the difficulty of the task. Finally, they compare their methods with existing methods on a variety of datasets, including the Penn Treebank."
1858,SP:f4a914d3df1a5a21a7365ba78279420f39210884,"classifier - agnostic method USED-FOR saliency map extraction. classifiers USED-FOR saliency mapping. classifier USED-FOR saliency map extraction. classifier structure HYPONYM-OF classifiers. mask m USED-FOR saliency mapping. min - max game USED-FOR framework. function f PART-OF min - max game. classifiers USED-FOR function f. mask - out classification error EVALUATE-FOR f. OtherScientificTerm are relevant features, and m. Metric is masked - out classification error. ","This paper proposes a classifier-agnostic method for saliency map extraction from a single classifier. The proposed framework is based on a min-max game, where the classifiers (i.e., the classifier structure) are used to learn a saliency mapping from a mask m to a set of relevant features. The function f is then trained using the learned classifiers, and the masked-out classification error of f is evaluated."
1859,SP:f4a914d3df1a5a21a7365ba78279420f39210884,model USED-FOR saliency map. posterior probabilities USED-FOR classifiers. scheme USED-FOR solution. networks USED-FOR scheme. networks USED-FOR solution. network USED-FOR classifier. formulation USED-FOR salience map extraction. Method is adversarial training procedure. Metric is accuracy. ,This paper proposes a new adversarial training procedure where the model is trained to extract the saliency map. The proposed scheme is based on networks that are trained to predict the posterior probabilities of the classifiers. The network is then used to train the classifier. The authors show that the proposed formulation can be used for salience map extraction without compromising accuracy.
1860,SP:df038354c6a7638116a98d150aa4a8f5f2b0a2da,heuristics USED-FOR NN. NNs USED-FOR tasks. heuristics USED-FOR NNs. OtherScientificTerm is semi - formal flavour. Task is model selection. ,"This paper proposes to use heuristics to improve the performance of an NN on a variety of tasks. In particular, the authors propose to use a semi-formal flavour, where the model selection is done in an unsupervised manner. The idea is interesting and interesting. However, the paper suffers from a lack of clarity and clarity."
1861,SP:df038354c6a7638116a98d150aa4a8f5f2b0a2da,method USED-FOR multiple teacher networks. networks USED-FOR feature representations. linear remappings CONJUNCTION weighted combinations. weighted combinations CONJUNCTION linear remappings. student network USED-FOR task. task COMPARE teacher task. teacher task COMPARE task. linear remappings USED-FOR hidden layers. weighted combinations USED-FOR hidden layers. teachers USED-FOR task. system USED-FOR teachers. combination USED-FOR system. Generic is Networks. Task is distillation. ,"This paper proposes a method for training multiple teacher networks. Networks are trained in an unsupervised manner. The networks are trained to learn feature representations that are similar to each other, but different from each other. Each student network is trained on a different task, where the task is different from the teacher task. The hidden layers are learned using linear remappings and weighted combinations of the hidden layers. The system is trained to select teachers for each task using a combination of the learned teachers. The distillation is done by minimizing the number of parameters."
1862,SP:a72072879f7c61270d952f06d9ce995e8150632c,"model USED-FOR complex dynamic system. information bottleneck method USED-FOR method. OtherScientificTerm are dynamic system, and B_k. Method is information bottleneck hierarchy "" method. ","This paper proposes a model for a complex dynamic system. The proposed method is based on the information bottleneck method. The authors propose a ""information bottleneck hierarchy"" method, where B_k is the number of nodes in the dynamic system, and the goal is to maximize the information between nodes."
1863,SP:a72072879f7c61270d952f06d9ce995e8150632c,Information Bottleneck Hierarchy ( IBH ) HYPONYM-OF Information Bottleneck Principle. Markov Chain USED-FOR IBH. algorithm USED-FOR IBH. IBH USED-FOR practical problems. OtherScientificTerm is Gaussian linear dynamic. ,"This paper studies the Information Bottleneck Hierarchy (IBH), which is a variant of the original information Bottleneck Principle. The IBH is based on the Markov Chain, where the Gaussian linear dynamic is assumed to be Gaussian. The authors propose an algorithm for optimizing the IBH for practical problems."
1864,SP:2b03b7ea1264c2671d29e8fa5f3a828412ea7996,deep generative models USED-FOR missing data imputation. observed features USED-FOR latent variable deep generative model. prior distribution USED-FOR masking variable. mask USED-FOR features. Inference USED-FOR latent variable model. inference network USED-FOR Inference. OtherScientificTerm is lower bound. Method is generative model. ,"This paper studies the problem of missing data imputation in deep generative models. The authors propose to use observed features to train a latent variable deep generalization model. Inference of the latent variable model is performed using an inference network, where the masking variable is sampled from a prior distribution. The paper provides a lower bound on the number of samples needed to train the generative model."
1865,SP:2b03b7ea1264c2671d29e8fa5f3a828412ea7996,"model USED-FOR conditional distribution. reconstruction loss USED-FOR reconstruction of masked ( unobserved ) variables. method COMPARE approaches. approaches COMPARE method. GANS USED-FOR task. approaches USED-FOR missing data imputation. universal marginalizer USED-FOR conditional densities. image inpainting COMPARE GANS. GANS COMPARE image inpainting. UCI benchmarks EVALUATE-FOR missing data imputation. feedforward / autoregressive architecture USED-FOR universal marginalizer. feedforward / autoregressive architecture USED-FOR conditional densities. UCI benchmarks EVALUATE-FOR approaches. Material is observed and masked parts. Method is conditional VAE framework. OtherScientificTerm are posterior, and observed variables. ","This paper proposes a conditional VAE framework, where the model learns a conditional distribution over observed and masked parts, and uses a reconstruction loss to encourage the reconstruction of masked (observed) variables. The method is compared to existing approaches for missing data imputation on UCI benchmarks, and compared to GANS for the same task on image inpainting. The conditional densities are learned using a universal marginalizer that uses a feedforward/autoregressive architecture. The posterior is trained to minimize the gap between observed variables and masked variables."
1866,SP:f46f0cb43274fb20cba91ef7318305f668bc6928,"8/4 - bit approximation of activations USED-FOR memory cost. memory cost USED-FOR gradient computation. fp16 CONJUNCTION fp32. fp32 CONJUNCTION fp16. Generic are technique, method, and model. ","This paper proposes a technique to reduce the memory cost of gradient computation by using an 8/4-bit approximation of activations. The method is simple and effective, and the experiments on fp16 and fp32 demonstrate the effectiveness of the proposed model."
1867,SP:f46f0cb43274fb20cba91ef7318305f668bc6928,"procedure USED-FOR memory footprint. procedure USED-FOR deep networks. memory footprint FEATURE-OF deep networks. it USED-FOR convolutional architectures. accuracy EVALUATE-FOR it. batch normalization USED-FOR quantization. Cifar100 CONJUNCTION ImageNet. ImageNet CONJUNCTION Cifar100. Cifar10 CONJUNCTION Cifar100. Cifar100 CONJUNCTION Cifar10. performance accuracy EVALUATE-FOR ResNets. ImageNet EVALUATE-FOR ResNets. Cifar10 EVALUATE-FOR ResNets. Cifar100 HYPONYM-OF ResNets. vector quantization USED-FOR activation statistics. Method are back propagation, and activation quantization. Generic are scheme, and forward pass. Metric is computational overhead. OtherScientificTerm are backward pass, and memory compression factors. ","This paper proposes a procedure to reduce the memory footprint of deep networks. The proposed scheme is called back propagation and it is applied to convolutional architectures to improve the accuracy and reduce the computational overhead. The key idea is to use batch normalization as the quantization. The forward pass is used to compute the activation statistics, while the backward pass computes the memory compression factors. Experiments on ResNets (Cifar10, Cifar100, and ImageNet) are conducted to evaluate the performance accuracy and the effect of activation quantization on the performance."
1868,SP:6ad33c6fbdee78c13d9190601637e07d20fe024f,GANs USED-FOR clean faces. CelebA - HQ EVALUATE-FOR method. framework USED-FOR face - related tasks. unconstrained face recognition HYPONYM-OF face - related tasks. method USED-FOR face - related applications. ,"This paper proposes a framework for face-related tasks such as unconstrained face recognition, where GANs are trained to generate clean faces. The proposed method is evaluated on CelebA-HQ, and the results show that the proposed method can be applied to a variety of face-based applications."
1869,SP:6ad33c6fbdee78c13d9190601637e07d20fe024f,"complex generative framework USED-FOR image completion. complex generative framework USED-FOR human face completion. human face completion HYPONYM-OF image completion. progressively attentive GAN USED-FOR face image. controllable attributes USED-FOR progressively attentive GAN. OtherScientificTerm are low and high resolution, and synthetic content. Method are complex post - processing, post - processing, and frequency - oriented attentive module ( FAM ). ","This paper presents a complex generative framework for image completion, specifically human face completion. The authors propose a progressively attentive GAN with controllable attributes to generate a face image at both low and high resolution. They also propose a complex post-processing, where the pre-processing is used to generate synthetic content, and the post-process is performed using a frequency-oriented attentive module (FAM)."
1870,SP:a300122021e93d695af85e158f2b402d21525bc8,"limited precision training and inference USED-FOR deep learning hardware. mantissa bits USED-FOR partial summations. analytical method USED-FOR mantissa bits. mantissa PART-OF accumulator. variance USED-FOR lost information. OtherScientificTerm are accumulators, inner products, unlimited precision, quantization, and noise. Method are convolutional and fully connected layers, and information theoretic approach. Task is variance reduction. ","This paper studies the problem of limited precision training and inference in deep learning hardware. The authors propose an analytical method to estimate the mantissa bits of partial summations, which are then used in accumulators to estimate inner products. The main contribution of this paper is to propose an information theoretic approach to estimate mantissa in an accumulator, which allows for unlimited precision without quantization. The paper also proposes to use convolutional and fully connected layers, which can be used for variance reduction. The variance is used to estimate lost information due to noise."
1871,SP:a300122021e93d695af85e158f2b402d21525bc8,"numeric precision FEATURE-OF accumulation operations. accumulation operations PART-OF neural network training. Variance Retention Ratio ( VRR ) USED-FOR analysis. OtherScientificTerm is floating point accumulator. Method are vision models, and theoretical analysis. ","This paper studies the accumulation operations in neural network training with numeric precision. The analysis is based on the Variance Retention Ratio (VRR), which is a function of the number of iterations of the floating point accumulator. The authors provide theoretical analysis and empirical results on vision models."
1872,SP:3a1655a2efdf0246f459b6f82a2948aafc7438a9,"gradient descent CONJUNCTION gradient flow. gradient flow CONJUNCTION gradient descent. deep linear networks USED-FOR gradient flow. linearly separable data USED-FOR deep linear networks. induced linear predictor USED-FOR max margin solution. induced linear predictor USED-FOR logistic and exponential loss. OtherScientificTerm are weight matrix, and weight matrices. ","This paper studies the connection between gradient descent and gradient flow in deep linear networks with linearly separable data. The authors show that the max margin solution of the induced linear predictor for the logistic and exponential loss can be expressed as a function of the weight matrix, and show that this can be extended to the case where the weight matrices are not separable."
1873,SP:3a1655a2efdf0246f459b6f82a2948aafc7438a9,gradient flow USED-FOR deep linear networks. gradient descent USED-FOR deep linear networks. gradient flow CONJUNCTION gradient descent. gradient descent CONJUNCTION gradient flow. linearly separable data USED-FOR deep linear networks. normalized weight matrix USED-FOR rank-1 matrix. logistic loss HYPONYM-OF strictly decreasing loss. maximum margin solution USED-FOR linear function. linear function USED-FOR logistic loss. Metric is loss. OtherScientificTerm is rank-1 matrices. ,"This paper studies the relationship between gradient flow and gradient descent in deep linear networks trained on linearly separable data. The authors propose a strictly decreasing loss, called logistic loss, which is a linear function with a maximum margin solution. This loss is defined as a function of the number of rank-1 matrices, where the rank-2 matrix is the normalized weight matrix and the rank of the rank 1 matrix is a function on the number 1 matrices. "
1874,SP:868dd531fe7886b0260295d25b75cc6d6d28f12d,attribute embeddings USED-FOR persona - based responses. adversarial framework USED-FOR multi - turn dialogue model. extension USED-FOR hredGAN. hredGAN HYPONYM-OF adversarial framework. history utterances CONJUNCTION speakers ’ persona. speakers ’ persona CONJUNCTION history utterances. utterance encoding CONJUNCTION attribute embeddings. attribute embeddings CONJUNCTION utterance encoding. phredGAN_a USED-FOR discriminator. dual discriminator USED-FOR phredGAN_d. Method is phredGAN. ,"This paper proposes an extension to hredGAN, an adversarial framework for multi-turn dialogue model, which is called phredGAN. The key idea is to use utterance encoding and attribute embeddings to generate persona-based responses. The authors propose to use both history utterances and the speakers’ persona. The discriminator is trained using phredAN_a, which uses a dual discriminator. "
1875,SP:868dd531fe7886b0260295d25b75cc6d6d28f12d,"it USED-FOR Multi - turn Dialogue Response Generation. Adversarial Learning Framework USED-FOR Multi - turn Dialogue Response Generation. Hierarchical Recurrent Attention Network USED-FOR Response Generation. persona based model COMPARE work. work COMPARE persona based model. work COMPARE model. model COMPARE work. persona based model COMPARE model. model COMPARE persona based model. Perplexity CONJUNCTION Bleu. Bleu CONJUNCTION Perplexity. attribute information PART-OF dialogue model. Method are Persona - Based Neural Conversation Model, and adversarial training. Task are response generation, SEQUENCE - TO - SEQUENCE CHATBOT RESPONSE, and personalization of responses. Metric is metrics. Generic is methods. ","This paper proposes a Persona-Based Neural Conversation Model and applies it to Multi-turn Dialogue Response Generation in the Adversarial Learning Framework. Specifically, the authors propose a Hierarchical Recurrent Attention Network for Response Generation. The authors compare the performance of the proposed model with the existing work and show that the proposed work outperforms the previous work in terms of response generation performance. In addition to the performance improvement, the proposed method SEQUENCE-TO-SEQUENCE CHATBOT RESPONSE is also evaluated on two metrics. Perplexity and Bleu are used to evaluate the ability of the dialogue model to incorporate attribute information, and the proposed methods are evaluated on a variety of tasks, including adversarial training and personalization of responses."
1876,SP:017b66d6262427cca551ef50006784498ffc741d,"CoDraw HYPONYM-OF task. cartoon style FEATURE-OF fixed background scene. fixed background scene PART-OF drawing environment. learning CONJUNCTION evaluation. evaluation CONJUNCTION learning. similarity USED-FOR learning. similarity USED-FOR evaluation. metric USED-FOR similarity. Method is clip art component. Material is describing language. OtherScientificTerm are components, and cross talk. ","This paper presents a new task called CoDraw, where the drawing environment consists of a fixed background scene in a cartoon style, and a clip art component. The goal of CoDraw is to learn a language that can be used to describe objects in the scene without describing language. The key idea is to use the similarity between the components as a metric for both learning and evaluation. The main contribution of the paper is that the cross talk between the clip art and the language component can be done in an unsupervised way."
1877,SP:017b66d6262427cca551ef50006784498ffc741d,grounded and goal - driven dialogue environment USED-FOR collaborative drawing. CoDraw HYPONYM-OF grounded and goal - driven dialogue environment. interactive and grounded evaluation environment USED-FOR NLG / NLU agents. well - specified nonlinguistic task EVALUATE-FOR system. models USED-FOR metric. success metric EVALUATE-FOR collaborative drawing task. grounded and goal - driven communication paradigm FEATURE-OF dataset. ,"This paper presents CoDraw, a grounded and goal-driven dialogue environment for collaborative drawing. The authors propose an interactive and grounded evaluation environment for NLG/NLU agents, where the system is evaluated on a well-specified nonlinguistic task. They also propose a new success metric for the collaborative drawing task, which is based on models that are grounded to the task. The dataset is designed to be grounded to both the task and the goal, and the authors conduct extensive experiments to demonstrate the effectiveness of the proposed dataset."
1878,SP:d5126851b9e75b49522d953ee2b253e3e6c836ba,inclusive - divergence minimization USED-FOR generative model. stochastic gradient Langevin dynamics ( SGLD ) USED-FOR sampling. Method is inclusive neural random field model. Generic is model. Material is synthetic and real - world datasets. ,This paper proposes an inclusive neural random field model. The generative model is based on inclusive-divergence minimization. The sampling is done using stochastic gradient Langevin dynamics (SGLD). The model is evaluated on both synthetic and real-world datasets.
1879,SP:d5126851b9e75b49522d953ee2b253e3e6c836ba,"inclusive auxiliary generator USED-FOR neural networks. inclusive auxiliary generator USED-FOR learning the random field. neural networks USED-FOR learning the random field. KL divergence FEATURE-OF auxiliary generator. KL divergence HYPONYM-OF inclusive - divergence. SGLD / SGHMC USED-FOR auxiliary generator. OtherScientificTerm are neural random fields, and intractable entropy term. Method is sampling methods. ","This paper proposes an inclusive auxiliary generator for training neural networks for learning the random field. The main idea is to use an inclusive-divergence, i.e., the KL divergence between the output of the auxiliary generator and the input of the neural random fields. The auxiliary generator is trained using SGLD/SGHMC, where the intractable entropy term is assumed to be zero. Experiments show that the proposed sampling methods outperform existing methods."
1880,SP:0841febf2e95da495b41e12ded491ba5e9633538,poisoned graph parameters USED-FOR graph neural network. meta - learning USED-FOR second - order derivatives. second - order derivatives USED-FOR meta - gradients. meta - learning USED-FOR meta - gradients. approximate methods USED-FOR graph. graph datasets EVALUATE-FOR model. misclassification rate EVALUATE-FOR unlabeled nodes. misclassification rate EVALUATE-FOR model. ,This paper proposes a graph neural network with poisoned graph parameters. The authors propose to use meta-learning to learn the second-order derivatives of the meta-gradients. The proposed model is evaluated on two graph datasets and the authors show that the misclassification rate of unlabeled nodes is lower than that of the original graph using approximate methods.
1881,SP:0841febf2e95da495b41e12ded491ba5e9633538,"data poisoning attacking USED-FOR graph neural networks. meta - learning USED-FOR adversarial attacks. adversarial attacks USED-FOR graph neural networks. OtherScientificTerm are graph structures, and hyperparameters. Generic is approach. ","This paper studies the problem of data poisoning attacking against graph neural networks using meta-learning. In particular, the authors focus on adversarial attacks against the state-of-the-art adversarial networks that are trained on graph structures. The proposed approach is based on the observation that the hyperparameters of the adversarial attack can be learned in an unsupervised manner."
1882,SP:beb54248806f7a68beb60167c3dbbd45b34dad83,"Cramer - Wold AutoEncoders ( CWAE ) HYPONYM-OF regularized auto - encoder architecture. encoded training distribution COMPARE normal. normal COMPARE encoded training distribution. encoded training distribution USED-FOR regularizer. CWAE method COMPARE WAE - MMD model. WAE - MMD model COMPARE CWAE method. OtherScientificTerm are reconstruction term, Cramer - Wold distance, and D - dimensional points. Method is generative model. Metric is FID scores. ",This paper proposes a regularized auto-encoder architecture called Cramer-Wold AutoEncoders (CWAE). The main idea is to replace the reconstruction term with a regularizer that is based on the encoded training distribution instead of the normal. The authors also propose a generative model that learns to discriminate between D-dimensional points by minimizing the distance between two points. Experiments show that the proposed CWAE method outperforms the WAE-MMD model in terms of FID scores.
1883,SP:beb54248806f7a68beb60167c3dbbd45b34dad83,Cramer - Wold Theorem USED-FOR Cramer - Wold distance. average L2 distances USED-FOR kernel density estimates. projections USED-FOR kernel density estimates. random slices FEATURE-OF one dimensional projections of the distributions. random slices USED-FOR kernel density estimates. random slices USED-FOR projections. Cramer - Wold distance USED-FOR latent distribution. Cramer - Wold distance USED-FOR generative autoencoder. Method is Cramer - Wold autoencoder. OtherScientificTerm is prior distribution. ,"This paper proposes a generative autoencoder based on the Cramer-Wold distance, which is derived from Cramer's Wold Theorem. The main idea is to use the projections from random slices of the distributions as the one dimensional projections of the prior distribution and use the average L2 distances as the kernel density estimates for the random slices. The authors also propose to use Cramer - Wold distance to estimate the latent distribution of the generative Autoenca."
1884,SP:57538c4cac6a4510a0c79e6da3deffae4d6c3b91,"methods USED-FOR memory - based KNN classifier. inductive bias PART-OF classifier. methods USED-FOR inductive bias. class hierarchy FEATURE-OF coarse - to - fine prediction. coarse - to - fine prediction USED-FOR classifier. coarse - to - fine prediction USED-FOR inductive bias. imagenet CONJUNCTION omniglot. omniglot CONJUNCTION imagenet. omniglot USED-FOR benchmark datasets. imagenet USED-FOR benchmark datasets. benchmarks EVALUATE-FOR methods. Task are learning, and many class / few shot classification scenario. Generic is scenario. ","This paper proposes methods to improve the performance of a memory-based KNN classifier by incorporating inductive bias into the classifier via coarse-to-fine prediction with respect to the class hierarchy. The motivation for this is that the learning is difficult in the many class/few shot classification scenario, and the authors propose methods to address this issue. The proposed methods are evaluated on two benchmark datasets from imagenet and omniglot. The results show that the proposed methods outperform the existing methods on both benchmarks. The authors also provide some ablation studies on the proposed scenario."
1885,SP:57538c4cac6a4510a0c79e6da3deffae4d6c3b91,class hierarchy USED-FOR many - class few - short learning problem. supervised learning CONJUNCTION meta - learning. meta - learning CONJUNCTION supervised learning. supervised learning USED-FOR many - class few - short learning problem. meta - learning USED-FOR many - class few - short learning problem. coarse - class label USED-FOR fine - class prediction. model USED-FOR DNN. supervision information USED-FOR DNN. coarse - class and fine - class label USED-FOR model. coarse - class label USED-FOR DNN. at KNN classifier CONJUNCTION Memory Update mechanism. Memory Update mechanism CONJUNCTION at KNN classifier. memory - augmented attention model PART-OF DNN. at KNN classifier PART-OF memory - augmented attention model. Memory Update mechanism PART-OF memory - augmented attention model. memory utility rate CONJUNCTION cache and clustering component. cache and clustering component CONJUNCTION memory utility rate. prototypes USED-FOR data sub - distribution. cache and clustering component PART-OF Memory Update mechanism. re - writable memory slots USED-FOR KNN classifier. re - writable memory slots USED-FOR prototypes. matching networks CONJUNCTION prototypical networks. prototypical networks CONJUNCTION matching networks. ,"This paper studies the many-class few-short learning problem with respect to the class hierarchy using supervised learning and meta-learning. The proposed model uses both coarse-class and fine-class label to train a DNN with supervision information. The memory-augmented attention model consists of an at KNN classifier, a Memory Update mechanism that updates the memory utility rate, and a cache and clustering component that learns prototypes for each data sub-distribution using re-writable memory slots. Experiments are conducted on matching networks and prototypical networks."
1886,SP:ae9b6f7f2bd29ad1d24c4acbe1ecd345fcd6a081,"Structural - Jump - LSTM model USED-FOR machine reading. Skim - LSTM CONJUNCTION LSTM - Shuffle. LSTM - Shuffle CONJUNCTION Skim - LSTM. LSTM - Jump CONJUNCTION Skim - LSTM. Skim - LSTM CONJUNCTION LSTM - Jump. LSTM - Shuffle HYPONYM-OF speed reading models. Skim - LSTM HYPONYM-OF speed reading models. LSTM - Jump HYPONYM-OF speed reading models. sentence - wise jumping USED-FOR jumping. sentence - wise jumping COMPARE models. models COMPARE sentence - wise jumping. LSTM - Jump HYPONYM-OF models. reinforcement learning algorithm COMPARE LSTM - Jump. LSTM - Jump COMPARE reinforcement learning algorithm. Generic is model. Method are LSTM ( skip ), and actor - critic approach. OtherScientificTerm are punctuation ( jump ), and word - wise skipping operation. ","This paper proposes a Structural-Jump-LSTM model for machine reading. The model is based on the idea of LSTM (skip), where punctuation (jump) is replaced by a word-wise skipping operation. The authors propose to use an actor-critic approach to evaluate the performance of the model. The experiments show that sentence-wise jumping improves the jumping performance compared to other speed reading models such as Skim-LSTM, LstM-Jump, and LStM-Shuffle. In addition, the authors show that a reinforcement learning algorithm is able to outperform the proposed model, and that the performance improvement is comparable to that of the LSTMs."
1887,SP:ae9b6f7f2bd29ad1d24c4acbe1ecd345fcd6a081,model USED-FOR neural speed reading. reinforcement learning problem USED-FOR problem. benchmark datasets EVALUATE-FOR techniques. ,This paper proposes a model for neural speed reading. The problem is formulated as a reinforcement learning problem. The proposed techniques are evaluated on several benchmark datasets.
1888,SP:9be782b532e64c6aad140531a17fbba1dd3342cd,defense USED-FOR adversarial examples. radial basis features USED-FOR defense. linearity FEATURE-OF convolutional networks. radial basis features PART-OF convnet architectures. Method is radial basis functions. ,"This paper proposes a defense against adversarial examples based on radial basis features of convolutional networks. The radial basis functions of convnet architectures have been shown to exhibit linearity, which is an interesting observation. The paper is well-written and easy to follow."
1889,SP:9be782b532e64c6aad140531a17fbba1dd3342cd,"Generic are method, model, and algorithm. OtherScientificTerm is math expressions. Task is optimization problem. ",This paper proposes a method for learning a model that can be used to solve problems where math expressions are not available. The method is based on the observation that the number of steps in the optimization problem can be arbitrarily large. The authors propose an algorithm that is able to solve the problem in a way that is computationally efficient. 
1890,SP:b08dc82d5098474ddd68ab13003013ee6e7ba989,"distribution of plausible subnetworks USED-FOR temporally consistent exploration. dropout USED-FOR deep networks. policy - space FEATURE-OF KL regularizers. Method are Neural   AdaptiveDropout Policy Exploration ( NADPEx ), deep reinforcement learning agents, and dropout transformation. Task is continuous control learning tasks. ","This paper proposes Neural  AdaptiveDropout Policy Exploration (NADPEx), which aims to improve the performance of deep reinforcement learning agents by learning a distribution of plausible subnetworks for temporally consistent exploration. The main contribution of this paper is to propose a dropout transformation to deep networks. The dropout is motivated by the observation that KL regularizers in the policy-space tend to converge to the same point in continuous control learning tasks."
1891,SP:b08dc82d5098474ddd68ab13003013ee6e7ba989,dropout USED-FOR neural network. neural network USED-FOR exploration. dropout USED-FOR temporally consistent exploration. Gaussian multiplicative dropout USED-FOR algorithm. benchmark environments EVALUATE-FOR algorithm. it COMPARE vanilla PPO. vanilla PPO COMPARE it. OtherScientificTerm is sparse rewards. ,This paper proposes to use dropout to train a neural network for exploration with sparse rewards. The authors argue that dropout can lead to temporally consistent exploration. The proposed algorithm is based on Gaussian multiplicative dropout and is evaluated on several benchmark environments. The results show that it outperforms vanilla PPO.
1892,SP:304930c105cf036ab48e9653926a5f61879dfea6,"whitened data distribution CONJUNCTION orthogonal matrix. orthogonal matrix CONJUNCTION whitened data distribution. OtherScientificTerm are nonlinearity coefficient, and data distribution. Material is whitened input data. Task is linear case. Metric is NLC. Method is linear model. ","This paper considers the problem of whitened input data. In particular, the authors consider the linear case where the nonlinearity coefficient is a function of the whitened data distribution and the orthogonal matrix. The authors show that if the NLC of the data distribution is large enough, then the linear model will converge."
1893,SP:304930c105cf036ab48e9653926a5f61879dfea6,nonlinearity coefficient HYPONYM-OF quantity. network architectures CONJUNCTION activation functions. activation functions CONJUNCTION network architectures. method USED-FOR local nonlinearity of activation functions. Method is neural networks. ,"This paper studies the nonlinearity coefficient, a quantity that measures the difference between the output of two neural networks. The authors propose a method to measure the local nonlinearities of activation functions. The method is motivated by the observation that the network architectures and the activation functions are different. "
1894,SP:17d8dc884e15131636a8c2490085ce42c05433c1,weak features USED-FOR bias. feature selection methods USED-FOR weak features. bias CONJUNCTION accuracy. accuracy CONJUNCTION bias. Task is bias amplification. OtherScientificTerm is features. Method is classifier. Material is real - world dataset. ,"This paper studies the problem of bias amplification, i. The authors propose to use feature selection methods to select weak features to reduce bias and improve accuracy. The main idea is that the weak features can be used to reduce the bias and increase accuracy. In particular, the authors show that the features that are selected by the classifier are more likely to be biased. The experiments are conducted on a real-world dataset."
1895,SP:17d8dc884e15131636a8c2490085ce42c05433c1,methods USED-FOR bias. methods USED-FOR logistic regression models. logistic regression models CONJUNCTION neural network models. neural network models CONJUNCTION logistic regression models. logistic output layers USED-FOR neural network models. benchmark datasets EVALUATE-FOR methods. OtherScientificTerm is marginal class probability bias. Material is synthetic and real datasets. ,This paper proposes methods to reduce bias in both logistic regression models and neural network models with logistic output layers. The authors argue that the marginal class probability bias is due to the fact that the weights of the neural network are not uniformly distributed. The proposed methods are evaluated on benchmark datasets and compared with existing methods on both synthetic and real datasets.
1896,SP:2b84207c0015dba126d4ef4a89ef9cc29656f2f8,"implicit bias of minimizers FEATURE-OF regularized cross entropy loss. two - layer network FEATURE-OF regularized cross entropy loss. ReLU activations PART-OF two - layer network. lifted feature space FEATURE-OF infinite - size network. l1 svm margin FEATURE-OF infinite - size network. maximum normalized margin CONJUNCTION l1 svm margin. l1 svm margin CONJUNCTION maximum normalized margin. lifted feature space FEATURE-OF l1 svm margin. perturbed Wasserstein gradient flow USED-FOR global minimum. polynomial time FEATURE-OF global minimum. polynomial time FEATURE-OF perturbed Wasserstein gradient flow. OtherScientificTerm are generalization upper bound, and scaling factor. Method is infinite - sized networks. ","This paper studies the implicit bias of minimizers of a regularized cross entropy loss on a two-layer network with ReLU activations. The authors prove a generalization upper bound of $O(1/\sqrt{T})$ for infinite-sized networks, where $T$ is the maximum normalized margin and l1 svm margin of an infinite-size network in the lifted feature space. They also prove a scaling factor of $\Omega(T)$. Finally, they show that a perturbed Wasserstein gradient flow converges to a global minimum in polynomial time."
1897,SP:2b84207c0015dba126d4ef4a89ef9cc29656f2f8,max normalized margin FEATURE-OF positive homogenous functions. normalized margin COMPARE max normalized margin. max normalized margin COMPARE normalized margin. vanishing regularization FEATURE-OF logistic loss. l_1 constraint FEATURE-OF sign measure. sign measure USED-FOR one hidden layer NN. l_2 norm constraint USED-FOR max margin. convergence rate FEATURE-OF mean - field view. mean - field view FEATURE-OF hidden layer NN. Wasserstein gradient flow HYPONYM-OF mean - field view. OtherScientificTerm is regularization. ,This paper studies the max normalized margin of positive homogenous functions. The main result is that the normalized margin is a function of the logistic loss with vanishing regularization. The authors show that the max margin can be bounded by the l_2 norm constraint on the sign measure of one hidden layer NN with l_1 constraint. The convergence rate of the mean-field view (Wasserstein gradient flow) of the hidden layer is shown. 
1898,SP:91459c66bb597751ffce8410e283ce3f094bdd5f,"method USED-FOR generation of multiple objects. supervision USED-FOR method. bounding boxes HYPONYM-OF supervision. Multi - MNIST CONJUNCTION CLEVR. CLEVR CONJUNCTION Multi - MNIST. object pathway modifications USED-FOR StackGAN and AttGAN models. StackGAN and AttGAN models USED-FOR MSCOCO ( IS and FID ). image captions USED-FOR models. OtherScientificTerm are object pathway, and supervision of bounding box. Method is GAN framework. Generic is model. ","This paper presents a method for the generation of multiple objects under supervision (i.e., bounding boxes). The authors propose a GAN framework where the object pathway is modelled as the supervision of bounding box, and the model is trained on Multi-MNIST and CLEVR. The object pathway modifications are applied to StackGAN and AttGAN models trained on MSCOCO (IS and FID) with image captions."
1899,SP:91459c66bb597751ffce8410e283ce3f094bdd5f,"method USED-FOR image generation. generative adversarial networks USED-FOR method. MNIST CONJUNCTION CLEVR. CLEVR CONJUNCTION MNIST. Generic is model. Material is COCO. OtherScientificTerm are bounding boxes, and varied bounding box locations. ","This paper presents a method for image generation based on generative adversarial networks. The model is trained on MNIST, CLEVR, and COCO. The main contribution of the paper is that the bounding boxes are randomly generated, which allows for varied bounding box locations."
1900,SP:fbfe2c90a70a6adf39fa4d4a3c28f6b5adbc6c06,locally - linear dynamical systems CONJUNCTION variational auto - encoders ( VAE ). variational auto - encoders ( VAE ) CONJUNCTION locally - linear dynamical systems. end to end approach USED-FOR model based reinforcement learning. images USED-FOR model based reinforcement learning. images HYPONYM-OF features. low dimensional latent representation USED-FOR features. parametric random functions USED-FOR low dimensional latent representation. neural networks USED-FOR latter. recognition model USED-FOR reinforcement learning task. convolutional neural networks USED-FOR recognition model. VAE CONJUNCTION linear dynamics. linear dynamics CONJUNCTION VAE. variational framework USED-FOR VAE. variational framework USED-FOR linear dynamics. latent state USED-FOR linear dynamics. cost function CONJUNCTION optimal policy. optimal policy CONJUNCTION cost function. linear quadratic system ( LQS ) USED-FOR cost function. linear quadratic system ( LQS ) USED-FOR optimal policy. OtherScientificTerm is latent space. Generic is model. ,"This paper proposes an end to end approach for model based reinforcement learning on images. The authors combine locally-linear dynamical systems with variational auto-encoders (VAE). The latter is trained using neural networks. The main idea is to learn a low dimensional latent representation using parametric random functions. The latent space is then used to train a recognition model for a reinforcement learning task using convolutional neural networks, and the latent state is used to model the VAE and the linear dynamics. The cost function and the optimal policy are learned using a linear quadratic system (LQS)."
1901,SP:fbfe2c90a70a6adf39fa4d4a3c28f6b5adbc6c06,"model based RL algorithm USED-FOR low dimensional embedding. linear dynamics PART-OF latent space. Bayesian regression USED-FOR linear dynamics. cost function USED-FOR policy. dynamics CONJUNCTION cost function. cost function CONJUNCTION dynamics. dynamics USED-FOR policy. planning CONJUNCTION imaginary roll - outs. imaginary roll - outs CONJUNCTION planning. dynamics USED-FOR imaginary roll - outs. dynamics USED-FOR planning. model - based RL algorithms COMPARE dynamics. dynamics COMPARE model - based RL algorithms. OtherScientificTerm are quadratic cost function, and KL - bound. ",This paper proposes a model based RL algorithm for learning a low dimensional embedding. The key idea is to learn linear dynamics in the latent space using Bayesian regression. The dynamics and the cost function are used to train a policy. The quadratic cost function is defined as the sum of a KL-bound on the difference between the learned dynamics of the policy and the original dynamics for planning and imaginary roll-outs. Experiments show that the learned model-based RL algorithms outperform the dynamics.
1902,SP:9a4c7d9df6685347e75e0ae72928225b7622a73c,"counterfactual model USED-FOR policy evaluation and search method. Guided Policy Search COMPARE vanilla GPS. vanilla GPS COMPARE Guided Policy Search. Guided Policy Search COMPARE counterfactual model ( CF - GPS ). counterfactual model ( CF - GPS ) COMPARE Guided Policy Search. counterfactual model ( CF - GPS ) COMPARE vanilla GPS. vanilla GPS COMPARE counterfactual model ( CF - GPS ). vanilla GPS COMPARE model based RL algorithm. model based RL algorithm COMPARE vanilla GPS. counterfactual model ( CF - GPS ) COMPARE model based RL algorithm. model based RL algorithm COMPARE counterfactual model ( CF - GPS ). ( empirical ) sample complexity EVALUATE-FOR model based RL algorithm. Method are vanilla ( non - causal ) models, and policy evaluation estimator. ",This paper proposes a new policy evaluation and search method based on a counterfactual model. The authors compare the performance of Guided Policy Search with the counterfactually model (CF-GPS) compared to vanilla GPS and a model based RL algorithm with (empirical) sample complexity. They show that vanilla (non-causal) models do not converge to the optimal policy evaluation estimator.
1903,SP:9a4c7d9df6685347e75e0ae72928225b7622a73c,"counterfactual inference USED-FOR approximate simulator. approximate simulator USED-FOR policy evaluation. sampled trajectories USED-FOR counterfactual inference. structural causal models USED-FOR Counterfactual inference. partially - observed Sokoban problems EVALUATE-FOR method. model USED-FOR observation histories. model USED-FOR conditional distribution. GPS - like "" algorithm USED-FOR domains. CF - GPS COMPARE GPS - like "" algorithm. GPS - like "" algorithm COMPARE CF - GPS. CF - GPS COMPARE model - based policy search. model - based policy search COMPARE CF - GPS. domains EVALUATE-FOR CF - GPS. stochastic value gradient CONJUNCTION CF - GPS. CF - GPS CONJUNCTION stochastic value gradient. GPS USED-FOR MDPs. GPS HYPONYM-OF CF - GPS. Method is dynamics model. ","This paper proposes to use counterfactual inference from sampled trajectories as an approximate simulator for policy evaluation using structural causal models. The method is evaluated on partially-observed Sokoban problems, where the model is trained to model observation histories and to approximate the conditional distribution of the observed trajectories using a dynamics model. Experimental results show that CF-GPS outperforms a ""GPS-like"" algorithm on these domains, and is competitive with a model-based policy search. The main contribution of this paper is to combine stochastic value gradient with CF - GPS (GPS for MDPs)."
1904,SP:9371d08e2b3a821e40cc9d4757c22f6cdb731b6a,"adversarial robustness CONJUNCTION generalization. generalization CONJUNCTION adversarial robustness. analyzing loss surfaces USED-FOR adversarial robustness. analyzing loss surfaces USED-FOR generalization. analyzing loss surfaces COMPARE measuring input loss surfaces. measuring input loss surfaces COMPARE analyzing loss surfaces. parameter space FEATURE-OF analyzing loss surfaces. loss surfaces CONJUNCTION decision surfaces. decision surfaces CONJUNCTION loss surfaces. input Jacobian CONJUNCTION Hessian. Hessian CONJUNCTION input Jacobian. input Jacobian USED-FOR regularization method. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. Method are adversarial attack methods, and adversarially sensitive and robust models. OtherScientificTerm is decision surface. ","This paper studies the problem of analyzing loss surfaces to improve adversarial robustness and generalization. In particular, the authors consider adversarial attack methods, where the adversary is trying to fool the adversarially sensitive and robust models. The authors argue that analyzing the loss surfaces in parameter space is more effective than measuring input loss surfaces. They propose a regularization method based on the input Jacobian and the Hessian of the decision surface. Experiments are conducted on MNIST and CIFAR-10."
1905,SP:9371d08e2b3a821e40cc9d4757c22f6cdb731b6a,robustness FEATURE-OF adversarial examples. parameter space FEATURE-OF loss surface. generalization and adversarial robustness EVALUATE-FOR neural network. parameter space CONJUNCTION input space. input space CONJUNCTION parameter space. input space USED-FOR neural network. input space FEATURE-OF geometric properties of the loss surface. parameter space FEATURE-OF geometric properties of the loss surface. loss surface USED-FOR decision surface. decision surfaces FEATURE-OF adversarial attack trajectory. input space FEATURE-OF decision surfaces. robust training method USED-FOR decision surface. indicator USED-FOR robust training method. Metric is adversarial robustness indicator. ,"This paper studies the problem of robustness to adversarial examples. The authors propose an adversarial robustness indicator, which measures the difference between the generalization performance of a neural network trained on the same parameter space and on a different input space. They show that the geometric properties of the loss surface in the parameter space as well as in the input space can be used to estimate the decision surface of the adversarial attack trajectory. Based on this indicator, they propose a robust training method that learns a decision surface based on the indicator."
1906,SP:6f94f59bc936a11d95ded7309dc2458fee6d2595,"method USED-FOR neural network training. method USED-FOR energy consumption. Cache CONJUNCTION register file. register file CONJUNCTION Cache. DRAM CONJUNCTION Cache. Cache CONJUNCTION DRAM. systolic array hardware architecture USED-FOR energy consumption. DRAM HYPONYM-OF memory. register file HYPONYM-OF memory. Cache HYPONYM-OF memory. greedy algorithm USED-FOR projection of the weight tensors. energy constraint FEATURE-OF network loss. greedy algorithm PART-OF training framework. OtherScientificTerm are hard energy constraint, and weight and activation tensors. Task is inference. ","This paper proposes a method to reduce the energy consumption during neural network training. The authors propose a systolic array hardware architecture to reduce energy consumption while maintaining a hard energy constraint on the memory (DRAM, Cache, and the register file). The proposed training framework consists of a greedy algorithm for the projection of the weight tensors under the energy constraint of the network loss, which is then used for inference. "
1907,SP:6f94f59bc936a11d95ded7309dc2458fee6d2595,optimization algorithm USED-FOR it. loss USED-FOR NN. energy constraints FEATURE-OF NN. energy budget USED-FOR method. energy consumption EVALUATE-FOR method. accuracy EVALUATE-FOR method. Method is deep neural networks. Task is compression. OtherScientificTerm is memory footprint. Generic is approaches. ,This paper studies the energy consumption of deep neural networks and proposes an optimization algorithm to reduce it. The main idea is to add a loss to the NN to reduce the energy constraints of NN. The proposed method is based on an energy budget of $O(\sqrt{T})$ and is shown to reduce energy consumption while maintaining accuracy. The compression is done in a way that minimizes the memory footprint. Experiments are conducted to compare the proposed approaches.
1908,SP:7f07f3fa8a10b48bb380a7c84bc012ce3541122b,policy gradient algorithm USED-FOR Bayes - Adaptive MDP ( BAMDP ). algorithm USED-FOR MDPs. trajectory USED-FOR sampled MDP. algorithm USED-FOR trajectory. prior distribution USED-FOR MDPs. Bayes filter USED-FOR posterior belief distribution. algorithm USED-FOR posterior belief distribution. Bayes filter USED-FOR algorithm. algorithm USED-FOR policy. sampled trajectories USED-FOR algorithm. TRPO algorithm USED-FOR algorithm. TRPO algorithm USED-FOR policy. ,"This paper proposes a policy gradient algorithm for the Bayes-Adaptive MDP (BAMDP). The proposed algorithm aims to learn MDPs with a prior distribution. The algorithm learns a trajectory for each sampled MDP using a Bayes filter to approximate the posterior belief distribution. Then, the algorithm uses the sampled trajectories to train a policy using the TRPO algorithm."
1909,SP:7f07f3fa8a10b48bb380a7c84bc012ce3541122b,policy optimization framework USED-FOR Bayesian RL ( BPO ). Bayesian model - based RL formulation USED-FOR BPO. exploration USED-FOR RL. POMDP planning tasks CONJUNCTION RL. RL CONJUNCTION POMDP planning tasks. POMDP planning tasks HYPONYM-OF multiple domains. RL HYPONYM-OF multiple domains. Method is Bayesian approach. OtherScientificTerm is model uncertainty. ,"This paper presents a policy optimization framework for Bayesian RL (BPO). The authors propose a Bayesian model-based RL formulation for BPO. The Bayesian approach is motivated by the observation that exploration in RL can lead to better performance in multiple domains (e.g., POMDP planning tasks and RL). The main contribution of this paper is to address the issue of model uncertainty."
1910,SP:3823faee83bc07a989934af5495dafd003c27921,"histogram over context word vectors USED-FOR word representation. Glove HYPONYM-OF approaches. context vectors USED-FOR clustering. semantic textual similarity CONJUNCTION hypernym detection tasks. hypernym detection tasks CONJUNCTION semantic textual similarity. OtherScientificTerm are distances between words, histograms, and polysemous words. Task is representations of polysemous words. Generic are approach, and baselines. ","This paper proposes to learn word representation from histogram over context word vectors, where distances between words are represented as histograms. This approach is motivated by the fact that representations of polysemous words are difficult to learn, and that existing approaches (e.g., Glove) do not generalize well to such words. The authors propose to use the context vectors for clustering, and show that the proposed approach is able to generalize better than baselines in terms of semantic textual similarity and hypernym detection tasks. "
1911,SP:3823faee83bc07a989934af5495dafd003c27921,"bins PART-OF histogram. histogram weights USED-FOR contextual association. optimal transport USED-FOR Context Mover Distance. Generic are method, model, and representations. OtherScientificTerm are vector space, context objects, and discrepancy between distributions. ","This paper proposes a method for learning representations of objects in vector space. The proposed model is based on the idea that context objects can be represented as bins in a histogram, and that the histogram weights can be used to model the contextual association between bins. The Context Mover Distance between bins is modeled as optimal transport between bins, and the discrepancy between distributions between bins and bins is estimated using the representations."
1912,SP:9ce5b80147ea2c7d0711ec98e31f4bbb5eac534e,"multi - step prediction model USED-FOR model - based RL. model - predictive control loop CONJUNCTION planning. planning CONJUNCTION model - predictive control loop. cross - entropy method USED-FOR planning. Generic are model, and tasks. Method is single - step models. ","This paper proposes a multi-step prediction model for model-based RL. The model consists of a model-predictive control loop and planning based on a cross-entropy method. The authors show that the proposed model can achieve state-of-the-art performance on a variety of tasks, compared to single-step models."
1913,SP:9ce5b80147ea2c7d0711ec98e31f4bbb5eac534e,model USED-FOR model predictive control. method COMPARE model - based and model - free algorithms. model - based and model - free algorithms COMPARE method. long horizon times EVALUATE-FOR method. model USED-FOR long horizon time scales. Method is Plan - Conditional Predictor ( PCP ). ,"This paper proposes a new model for model predictive control. The proposed method, Plan-Conditional Predictor (PCP), is evaluated on long horizon times and compared to model-based and model-free algorithms. The authors show that the proposed model is able to generalize to long horizon time scales."
1914,SP:da14205470819495a3aad69d64de4033749d4d3e,end - to - end precision highway USED-FOR accumulated quantization error. end - to - end precision highway USED-FOR deep neural networks. end - to - end precision highway USED-FOR ultra - low precision. ultra - low precision FEATURE-OF deep neural networks. 3- and 2 - bit quantizations CONJUNCTION 2 - bit quantization. 2 - bit quantization CONJUNCTION 3- and 2 - bit quantizations. 2 - bit quantization USED-FOR LSTM model. ResNet-18/50 CONJUNCTION 2 - bit quantization. 2 - bit quantization CONJUNCTION ResNet-18/50. ResNet-18/50 HYPONYM-OF 3- and 2 - bit quantizations. Task is neural network quantization. Generic is method. ,"This paper proposes an end-to-end precision highway to reduce accumulated quantization error in deep neural networks with ultra-low precision. The main contribution of this paper is to study the problem of neural network quantization from a theoretical perspective. The authors propose a method that can be applied to 3- and 2-bit quantizations (ResNet-18/50, 2- bit quantization in an LSTM model) as well as 3-and 2-bits quantizations in a more general way."
1915,SP:da14205470819495a3aad69d64de4033749d4d3e,methods USED-FOR quantized neural networks. quantization method CONJUNCTION fine tuning scheme. fine tuning scheme CONJUNCTION quantization method. teacher network USED-FOR distillation. fine tuning scheme HYPONYM-OF methods. Laplace distribution USED-FOR quantization method. quantization method HYPONYM-OF methods. distillation HYPONYM-OF methods. OtherScientificTerm is full - precision residual connections. Method is fully - quantized convolutions. ,"This paper proposes two methods for training quantized neural networks: a quantization method based on the Laplace distribution, and a fine tuning scheme based on distillation with a teacher network. The main contribution of the paper is the introduction of full-precision residual connections, which is an improvement over fully-quantized convolutions."
1916,SP:0355b54430b39b52df94014d78289dd6e1e81795,method USED-FOR image restoration. GAN USED-FOR method. MAP framework USED-FOR latent variable. Method is G(z ). ,"This paper proposes a method for image restoration based on GAN. The main idea is to use the MAP framework to learn a latent variable called G(z), which is then used to reconstruct the original image."
1917,SP:0355b54430b39b52df94014d78289dd6e1e81795,method USED-FOR image restoration. restored image USED-FOR MAP estimate. pretrained GAN USED-FOR prior distribution. noise - free images USED-FOR prior distribution. degradation function USED-FOR likelihood. degradation function USED-FOR constraint. optimization algorithm USED-FOR constrained optimization problem. Method is GAN. OtherScientificTerm is degraded image. ,"This paper proposes a method for image restoration. The key idea is to use a pretrained GAN to estimate a prior distribution over noise-free images, and then use the restored image to compute a MAP estimate. The constraint is enforced by a degradation function that minimizes the likelihood of the reconstructed image. The authors propose an optimization algorithm to solve the constrained optimization problem, where the GAN is trained to minimize the MAP estimate of the degraded image."
1918,SP:2feef921a0563d52fde1c074da754f73e6cabef8,"framework USED-FOR convolution neural networks. 1x1 convolutions USED-FOR fitting objective. 1x1 point - wise convolution per layer USED-FOR regression problem. CIFAR-100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-100. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. pruning techniques USED-FOR ImageNet. FSKD COMPARE FitNet. FitNet COMPARE FSKD. FitNet CONJUNCTION fine - tuning. fine - tuning CONJUNCTION FitNet. FSKD COMPARE fine - tuning. fine - tuning COMPARE FSKD. non - trivial margins USED-FOR FSKD. non - trivial margins USED-FOR fine - tuning. Method is student network. Generic is approach. Task are knowledge distillation, and alignment procedure. Metric is sample efficiency. ","This paper proposes a framework for training convolution neural networks. The proposed approach is based on the idea of knowledge distillation, where a student network is trained to maximize the sample efficiency of the teacher network. The fitting objective is formulated as a combination of 1x1 convolutions per layer, and a regression problem is solved as a 1-1 point-wise convolution per layer. The alignment procedure is performed on CIFAR-10, CifAR-100, and ImageNet with various pruning techniques. FSKD is compared to FitNet and fine-tuning with non-trivial margins."
1919,SP:2feef921a0563d52fde1c074da754f73e6cabef8,"re - training algorithm USED-FOR neural networks. Method are Hinton's distillation, and 1x1 convolutions. OtherScientificTerm is intermediate layers. Generic are them, and it. ","This paper proposes a re-training algorithm for neural networks. The main idea is to use Hinton's distillation, i.e. 1x1 convolutions, to replace the intermediate layers with a set of intermediate layers, and to train them in parallel. The authors show that it is computationally efficient."
1920,SP:ca491b166bd8bf1a7c71657471a2f58b7fd36609,"error exponent CONJUNCTION DTM matrix. DTM matrix CONJUNCTION error exponent. information vectors CONJUNCTION error exponent. error exponent CONJUNCTION information vectors. error exponent FEATURE-OF H(f ). quantities USED-FOR H(f ). H(f ) CONJUNCTION H(f_opt ). H(f_opt ) CONJUNCTION H(f ). OtherScientificTerm are H score H(f ), and goodness of feature f(x ). Metric is transferability. ","This paper studies the H score H(f), which is a function of the information vectors, the error exponent, and the DTM matrix. These quantities are used to compute H(h,f) and H(g_opt) respectively. The authors show that H(H(f) can be expressed as the sum of H(x) + H(y) where y is the goodness of feature f(x). The authors also show that the transferability can be measured by the difference between H(F(x,y)) with respect to f(y)."
1921,SP:ca491b166bd8bf1a7c71657471a2f58b7fd36609,transfer learning USED-FOR source domain selection problem. normalized correlation USED-FOR H - score. H - score USED-FOR transferability. image data EVALUATE-FOR H - score. OtherScientificTerm is feature representation function. ,This paper studies transfer learning in the source domain selection problem. The authors propose to use normalized correlation as the H-score to measure the transferability between source and target domains. The paper also proposes a new feature representation function. Experiments on image data demonstrate the effectiveness of the proposed H-scores.
1922,SP:c6884b04001bd0d43aa47e2d72ebbe2bbc89ab3d,"neural machine translation model USED-FOR generating diverse translations. beam - search USED-FOR models. discrete latent codes USED-FOR generation. Discrete Autoencoders USED-FOR Sequence Models. approach USED-FOR generating diverse translations. approach USED-FOR Discrete Autoencoders. unsupervised approach USED-FOR it. POS tags HYPONYM-OF supervised data. small data - sets EVALUATE-FOR weak translation baseline. Generic are method, and baselines. ","This paper proposes a neural machine translation model for generating diverse translations. The proposed method is based on the idea that models trained with beam-search can be trained with discrete latent codes for generation. The authors apply their approach to Discrete Autoencoders for Sequence Models and show that it can be applied to an unsupervised approach to supervised data (e.g. POS tags). The authors also show that their method is able to generate diverse translations compared to baselines on small data-sets. Finally, the authors compare their method to a weak translation baseline."
1923,SP:c6884b04001bd0d43aa47e2d72ebbe2bbc89ab3d,latent encoding of the overall structure USED-FOR generation. simplified part - of - speech tags USED-FOR latent encoding of the overall structure. conditional autoencoder USED-FOR latent code. latent code USED-FOR tag sequence. pairwise BLEU scores FEATURE-OF diversity metric. latent codes USED-FOR structural diversity. latent codes COMPARE beam search. beam search COMPARE latent codes. beam search USED-FOR translation. Task is modeling structural diversity of translations. ,This paper addresses the problem of modeling structural diversity of translations by learning a latent encoding of the overall structure for generation from simplified part-of-speech tags. The key idea is to use a conditional autoencoder to generate a latent code for each part of the tag sequence. The authors propose a diversity metric based on pairwise BLEU scores. The proposed latent codes are shown to capture structural diversity better than beam search for translation.
1924,SP:51810c5f8d40d9ec40469349f1612bf2eefe9aad,generator CONJUNCTION discriminator. discriminator CONJUNCTION generator. IPM based GANs USED-FOR GAN model. GAN CONJUNCTION tweak. tweak CONJUNCTION GAN. FID metric EVALUATE-FOR GAN. FID metric EVALUATE-FOR tweak. Material is CIFAR10 and CAT datasets. ,"This paper proposes a GAN model based on IPM based GANs, where the generator and discriminator are jointly trained. The GAN and the tweak are evaluated on CIFAR10 and CAT datasets and the FID metric of the GAN is compared to the original GAN."
1925,SP:51810c5f8d40d9ec40469349f1612bf2eefe9aad,"GANs COMPARE non - relativistic counterparts. non - relativistic counterparts COMPARE GANs. Method are generator, and relativistic discriminator. Generic is methods. ","This paper proposes to train GANs that are more robust than their non-relativistic counterparts. The main idea is to train a generator that takes as input a set of samples from the target distribution, and then train a relativistic discriminator to discriminate between the generated samples and the source distribution. The proposed methods are evaluated on a variety of datasets."
1926,SP:8df1599919dcb3329553e75ffb19059f192542ea,method USED-FOR continual learning. parameter generator USED-FOR classifier. components PART-OF model. data generator PART-OF components. parameter generator PART-OF components. datasets EVALUATE-FOR method. ,"This paper proposes a method for continual learning. The proposed model consists of three components: a data generator, a parameter generator, and a classifier. The method is evaluated on two datasets."
1927,SP:8df1599919dcb3329553e75ffb19059f192542ea,Dynamic Parameter Generator ( DPG ) USED-FOR classification model. Data Generator ( DG ) USED-FOR catastrophic forgetting. DG USED-FOR internal representations of data. ,This paper proposes a Dynamic Parameter Generator (DPG) for training a classification model. The proposed Data Generator (DG) is designed to prevent catastrophic forgetting. The DG learns internal representations of data.
1928,SP:1342b6e11d1ccf04ee95b63d8b7a88b184dee43e,"graph neural networks USED-FOR relational reasoning. relational reasoning USED-FOR multi - agent systems. graph neural networks USED-FOR multi - agent systems. training speeds EVALUATE-FOR non relational reasoning baseline methods. Method are Relational Forward Modeling, RFM, and RFM - aumented RL agent. OtherScientificTerm is multi - agent environments. ","This paper proposes to use graph neural networks for relational reasoning in multi-agent systems using relational reasoning. Relational Forward Modeling (RFM) is an extension of RFM. The RFM-aumented RL agent is trained in an end-to-end manner, and the authors show that the RFM can achieve better performance on multi-agents environments and faster training speeds compared to non relational reasoning baseline methods."
1929,SP:1342b6e11d1ccf04ee95b63d8b7a88b184dee43e,"neural network architecture USED-FOR predicting multi - agent behavior. recurrent component PART-OF graph network. RFMs COMPARE ablations. ablations COMPARE RFMs. RFMs COMPARE baselines. baselines COMPARE RFMs. baselines CONJUNCTION ablations. ablations CONJUNCTION baselines. Generic are architecture, and tasks. Method are relational forward model ( RFM ), and RFM. OtherScientificTerm is edge activation magnitudes. ",This paper proposes a neural network architecture for predicting multi-agent behavior. The proposed architecture is based on the relational forward model (RFM). The RFM consists of a recurrent component that takes as input a graph network and outputs the edge activation magnitudes of each agent. Experiments on three tasks show that RFMs outperform baselines and ablations.
1930,SP:f2f01c7c4fb68c25d6e5ac56cbf79615ed1ee9ee,lacking sufficient demonstrations USED-FOR inverse reinforcement learning ( IRL ) problems. reward function USED-FOR task. meta gradient expression USED-FOR parameter update. MAML CONJUNCTION maximal entropy ( MaxEnt ) IRL framework. maximal entropy ( MaxEnt ) IRL framework CONJUNCTION MAML. gradient - based meta learning algorithm CONJUNCTION MAML. MAML CONJUNCTION gradient - based meta learning algorithm. SpriteWorld HYPONYM-OF synthetic grid - world problem. SpriteWorld EVALUATE-FOR algorithm. synthetic grid - world problem EVALUATE-FOR algorithm. algorithm USED-FOR optimal policy. reward function USED-FOR algorithm. reward function USED-FOR optimal policy. Method is meta learning approach. OtherScientificTerm is task distribution. ,"This paper addresses the problem of lacking sufficient demonstrations for inverse reinforcement learning (IRL) problems. The authors propose a meta learning approach where the parameter update is modeled as a meta gradient expression, and the reward function for the task is learned by minimizing the difference between the gradient-based meta learning algorithm (MAML) and the maximal entropy (MaxEnt) IRL framework. The proposed algorithm is evaluated on a synthetic grid-world problem, SpriteWorld, and is shown to converge to an optimal policy with respect to a reward function that is close to the task distribution."
1931,SP:f2f01c7c4fb68c25d6e5ac56cbf79615ed1ee9ee,data - set coverage issue FEATURE-OF Inverse reinforcement learning based approaches. these USED-FOR parametrized reward function. parametrized reward function USED-FOR IRL. coverage EVALUATE-FOR IRL. Method is meta - learning framework. Material is SpriteWorld synthetic data - set. ,This paper studies the data-set coverage issue of Inverse reinforcement learning based approaches. The authors propose a meta-learning framework and propose to use these to learn a parametrized reward function for IRL to improve the coverage. Experiments are conducted on SpriteWorld synthetic data-Set.
1932,SP:4c2f45c7fd0cac662a33be602985cf360b45fe4d,it USED-FOR few - shot learning. meta - learning framework CONJUNCTION approximate probabilistic inference. approximate probabilistic inference CONJUNCTION meta - learning framework. it CONJUNCTION predictive distribution. predictive distribution CONJUNCTION it. approximate predictive distribution CONJUNCTION predictive distribution. predictive distribution CONJUNCTION approximate predictive distribution. KL - divergence FEATURE-OF approximate predictive distribution. it USED-FOR approximate predictive distribution. framework USED-FOR Versatile Amortized Inference. VERSA HYPONYM-OF Versatile Amortized Inference. optimization USED-FOR test time. optimization CONJUNCTION posterior inference. posterior inference CONJUNCTION optimization. VERSA COMPARE amortized and non - amortized variational inference. amortized and non - amortized variational inference COMPARE VERSA. amortized and non - amortized variational inference COMPARE it. it COMPARE amortized and non - amortized variational inference. Method is meta - learner. ,"This paper proposes a new meta-learning framework and approximate probabilistic inference, and applies it to the problem of few-shot learning. The framework is called Versatile Amortized Inference, or VERSA, and it learns an approximate predictive distribution over the KL-divergence between the predictive distribution and the true predictive distribution. The meta-learner is trained in a supervised manner, where the optimization is done at test time, and the posterior inference is performed at inference time. Experiments show that VERSA outperforms both amortized and non-amortized variational inference."
1933,SP:4c2f45c7fd0cac662a33be602985cf360b45fe4d,"task - specific inference USED-FOR single - layer head models. setup USED-FOR task - specific inference. predictive distributions USED-FOR task. task - specific inference USED-FOR it. predictive distributions USED-FOR objective. objective USED-FOR it. setup USED-FOR it. network USED-FOR Inference. optimization CONJUNCTION gradients. gradients CONJUNCTION optimization. network USED-FOR inference. optimization USED-FOR non - amortized approaches. gradients USED-FOR non - amortized approaches. Task is few - shot ( or meta ) learning. OtherScientificTerm are log marginal likelihood, and task training split. ","This paper studies few-shot (or meta) learning and proposes a new setup for doing task-specific inference for single-layer head models. Specifically, it uses a new objective based on predictive distributions for each task. Inference is performed by a network that is trained to maximize the log marginal likelihood of each task, which is a function of the task training split. In addition, non-amortized approaches are proposed that combine optimization and gradients."
1934,SP:44e0f63ffee15796ba6135463134084bb370627b,"images PART-OF dataset. CNN features PART-OF linear - chain CRF. deep structured model USED-FOR task. CNN features PART-OF deep structured model. lower - rank matrices USED-FOR class embedding ”. lower - rank matrices USED-FOR pairwise potentials. Training efficiency EVALUATE-FOR objective. piecewise pseudolikelihood USED-FOR training - time inference. linear complexity EVALUATE-FOR training - time inference. piecewise pseudolikelihood USED-FOR objective. batch normalization USED-FOR features. batch normalization USED-FOR CRF model. model / training procedure COMPARE models / training procedures. models / training procedures COMPARE model / training procedure. OtherScientificTerm are object boundaries, ultrafine - grained ” class labels, and spatial layout. ","This paper proposes a deep structured model for the task, which consists of CNN features of a linear-chain CRF. The dataset consists of images with different object boundaries and different “ultrafine-grained” class labels. The authors propose to learn pairwise potentials using lower-rank matrices for the “class embedding”. Training efficiency of the objective is improved by using piecewise pseudolikelihood, which reduces the linear complexity of training-time inference. The CRF model is trained with batch normalization to encourage the features to have similar spatial layout. Experiments show that the proposed model/training procedure outperforms other models/training procedures."
1935,SP:44e0f63ffee15796ba6135463134084bb370627b,spatial conference of object labels USED-FOR instance - wise prediction. sequential inference problem USED-FOR task. CRF USED-FOR sequential inference problem. factorized pairwise - potential CONJUNCTION approximation of CRF objective. approximation of CRF objective CONJUNCTION factorized pairwise - potential. Method is approximated CRF. Task is training. ,This paper studies instance-wise prediction from a spatial conference of object labels. The task is formulated as a sequential inference problem with CRF. The authors propose a factorized pairwise-potential and an approximation of CRF objective. The approximation of the approximated CRF is shown to improve the performance during training.
1936,SP:18be2cb182761b64fa232c1b7d1899882e5bcf15,log magnitudes CONJUNCTION instantaneous frequencies. instantaneous frequencies CONJUNCTION log magnitudes. GAN framework USED-FOR audio. spectral domain FEATURE-OF sufficient frequency resolution. sufficient frequency resolution USED-FOR instantaneous frequencies. GAN framework USED-FOR approach. it COMPARE WaveNet. WaveNet COMPARE it. NSynth dataset EVALUATE-FOR it. NSynth dataset EVALUATE-FOR WaveNet. Parallel WaveNet CONJUNCTION Tacotran. Tacotran CONJUNCTION Parallel WaveNet. Tacotran USED-FOR speech synthesis. WaveNET CONJUNCTION Parallel WaveNet. Parallel WaveNet CONJUNCTION WaveNET. method COMPARE WaveNet. WaveNet COMPARE method. deep generative models USED-FOR speech synthesis. Parallel WaveNet USED-FOR speech synthesis. method USED-FOR speech synthesis. WaveNet CONJUNCTION Parallel WaveNet. Parallel WaveNet CONJUNCTION WaveNet. Parallel WaveNet CONJUNCTION Tacotran. Tacotran CONJUNCTION Parallel WaveNet. method COMPARE Parallel WaveNet. Parallel WaveNet COMPARE method. WaveNET HYPONYM-OF deep generative models. Tacotran HYPONYM-OF deep generative models. Parallel WaveNet HYPONYM-OF deep generative models. ,"This paper proposes an approach based on the GAN framework for audio with sufficient frequency resolution in the spectral domain for both log magnitudes and instantaneous frequencies. Experiments on the NSynth dataset show that it outperforms WaveNet, Parallel WaveNet and Tacotran for speech synthesis. The proposed method is compared to WaveNet (which is the state-of-the-art in speech synthesis) and ParallelWaveNet, which is the most recent deep generative models for speech synthesization (WaveNet and Parallel Wavenet)."
1937,SP:18be2cb182761b64fa232c1b7d1899882e5bcf15,GANs USED-FOR strategy. treatment COMPARE image generation. image generation COMPARE treatment. autoregressive Wavenet based model USED-FOR Neural Audio Synthesis of Musical Notes. WaveNet AutoEncoders USED-FOR Neural Audio Synthesis of Musical Notes. GANs USED-FOR image generation. WaveGAN CONJUNCTION Adversarial Audio Synthesis. Adversarial Audio Synthesis CONJUNCTION WaveGAN. NSynth dataset USED-FOR it. OtherScientificTerm is interpretable latent code. Method is Wavenet model. ,"This paper proposes an autoregressive Wavenet based model for Neural Audio Synthesis of Musical Notes with WaveNet AutoEncoders. The proposed strategy is based on using GANs to generate an interpretable latent code. The authors compare the proposed treatment with image generation with the existing state-of-the-art GAN and Adversarial audio Synthesis, and show that it can be trained on the NSynth dataset. The paper also provides an ablation study of the proposed method. Finally, the authors provide a theoretical analysis of the WavenET model."
1938,SP:0c0f078c208600f541a76ecaae49cf9a98588736,mixed integer linear programming approaches USED-FOR verifying robustness. mixed integer linear programming approaches USED-FOR neural networks. verifying robustness FEATURE-OF neural networks. adversarial perturbations FEATURE-OF neural networks. restricted domain CONJUNCTION progressive bound tightening. progressive bound tightening CONJUNCTION restricted domain. Asymmetric bounds CONJUNCTION restricted domain. restricted domain CONJUNCTION Asymmetric bounds. progressive bound tightening USED-FOR scalable verification algorithms. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. MILP solvers COMPARE complete / incomplete verifiers. complete / incomplete verifiers COMPARE MILP solvers. verifying robustness CONJUNCTION generating adversarial attacks. generating adversarial attacks CONJUNCTION verifying robustness. MILP solvers USED-FOR generating adversarial attacks. verifying robustness EVALUATE-FOR complete / incomplete verifiers. generating adversarial attacks COMPARE PGD attacks. PGD attacks COMPARE generating adversarial attacks. architectures EVALUATE-FOR approach. CIFAR-10 EVALUATE-FOR architectures. MNIST EVALUATE-FOR architectures. verifying robustness EVALUATE-FOR MILP solvers. Method is MILP formulations of neural network verification. Task is neural network verification. ,"This paper proposes mixed integer linear programming approaches for verifying robustness of neural networks to adversarial perturbations. Asymmetric bounds, restricted domain, and progressive bound tightening have been proposed for scalable verification algorithms in the past, but the MILP formulations of neural network verification are not scalable. This paper proposes a new approach that is scalable and efficient. The proposed approach is evaluated on a variety of architectures on MNIST and CIFAR-10, where MILP solvers are compared to complete/incomplete verifiers in terms of both the task of verifying the robustness and generating adversarial attacks compared to PGD attacks."
1939,SP:0c0f078c208600f541a76ecaae49cf9a98588736,Mixed Integer Linear Programming ( MILP ) approach USED-FOR robustness. Mixed Integer Linear Programming ( MILP ) approach USED-FOR neural networks. robustness FEATURE-OF neural networks. ReLU activations USED-FOR neural networks. progressive bound tightening approach USED-FOR MILP solving. robustness EVALUATE-FOR networks. Material is CIFAR-10. ,This paper proposes a Mixed Integer Linear Programming (MILP) approach to improve the robustness of neural networks with ReLU activations. The authors propose a progressive bound tightening approach to MILP solving and provide empirical results on CIFAR-10 to show that the proposed networks achieve better robustness.
1940,SP:dc48dbfb8f4f25d3ceb7be607e8f2e0bc8f99f14,"modes PART-OF decision making. OtherScientificTerm are cognitive science literature, information theoretic objective, and default "" behaviour. ","This paper is an interesting contribution to the cognitive science literature. In particular, the authors propose a new information theoretic objective, ""default"" behaviour, which is motivated by the observation that there are two different modes of decision making, which can be seen as ""defaults"" and ""non-defaults"". This is a very interesting idea, and it is well-motivated by the fact that it is a well-studied topic in the cognitive sciences literature. "
1941,SP:dc48dbfb8f4f25d3ceb7be607e8f2e0bc8f99f14,KL - regularization USED-FOR reinforcement learning ( RL ). information asymmetry FEATURE-OF KL - regularization. default policy COMPARE fixed default policy. fixed default policy COMPARE default policy. default policy USED-FOR it. agent policy CONJUNCTION default policy. default policy CONJUNCTION agent policy. default policy USED-FOR transfer learning. Generic is algorithm. ,"This paper studies the problem of KL-regularization in reinforcement learning (RL) in the context of information asymmetry. The authors propose a new algorithm, called Transfer Learning with KL-Regularization (TL-KL), and show that it is equivalent to using a default policy instead of a fixed default policy. The main idea is that the agent policy and the default policy can be used for transfer learning."
1942,SP:08a6a48b05e2c00d77a73413cbba52cda08e184c,"FlowQA USED-FOR conversation reading comprehension. context integration CONJUNCTION question flow. question flow CONJUNCTION context integration. neural network USED-FOR process. datasets EVALUATE-FOR FlowQA. CoQA CONJUNCTION QuAC. QuAC CONJUNCTION CoQA. it COMPARE models. models COMPARE it. CoQA EVALUATE-FOR it. QA datasets EVALUATE-FOR it. SCONE EVALUATE-FOR models. QuAC HYPONYM-OF QA datasets. CoQA HYPONYM-OF QA datasets. Task is single - turn reading comprehension. OtherScientificTerm are parallelism, trainable parameters, and concept Flow. Generic is model. ","This paper proposes FlowQA for conversation reading comprehension, which aims to address the problem of single-turn reading comprehension. The main idea is to combine context integration with question flow. The authors propose to use a neural network to guide the process, which allows for parallelism and allows for trainable parameters. The proposed model is based on the concept Flow, which is a generalization of concept Flow. The experiments on two standard QA datasets (CoQA and QuAC) show that FlowQQA outperforms existing models on SCONE and it outperforms other models on CoQA."
1943,SP:08a6a48b05e2c00d77a73413cbba52cda08e184c,FLOWQA USED-FOR conversational question answering ( CoQA ). machine reading comprehension ( MRC ) COMPARE CoQA. CoQA COMPARE machine reading comprehension ( MRC ). conversation history PART-OF CoQA. encoder USED-FOR FLOWQA. CoQA USED-FOR FLOWQA. classifier PART-OF It. ,"This paper proposes FLOWQA for conversational question answering (CoQA), which is a generalization of the recently proposed machine reading comprehension (MRC) instead of CoQA, which incorporates the conversation history into the training process. The encoder is trained to predict the answer to a given question. It also includes a classifier that predicts whether the answer is correct or incorrect. The paper also provides a theoretical analysis of the proposed method. "
1944,SP:fbb7bb8b4f75715f139c702750b28e7e87aa0e1f,baselines USED-FOR predicting edits. history of changes USED-FOR predicting edits. accuracy EVALUATE-FOR edit model. Material is source code. Task is code text writing process. ,This paper proposes to use the history of changes in the source code to improve the accuracy of predicting edits. The authors compare with several existing baselines for predicting edits and show that the edit model can achieve better accuracy with fewer parameters. The paper also provides a theoretical analysis of the code text writing process.
1945,SP:fbb7bb8b4f75715f139c702750b28e7e87aa0e1f,one CONJUNCTION one. one CONJUNCTION one. explicit ( heavy ) model COMPARE implicit model. implicit model COMPARE explicit ( heavy ) model. representations USED-FOR models. OtherScientificTerm is edits. ,"This paper proposes to replace the explicit (heavy) model with an implicit model. The authors propose two models: one that learns representations that are similar to each other, and one that is different from the other one. This is done by making edits to the weights of the two models. "
1946,SP:dbb06f953788696f65013765f0a4e6967444fa0f,"multi - class classifiers PART-OF binary classifiers. binary classifiers PART-OF multi - class classifiers. multi - class classifiers USED-FOR method. OtherScientificTerm are multi - class labels, hidden variables, and binary similarity labels. Generic is technique. ","This paper proposes a method that combines multi-class classifiers with binary classifiers. The proposed method is based on the idea that the multi-classes of the binary classifier can be decomposed into a set of hidden variables, which can be used as multi-labels. The idea is that the hidden variables can be represented as binary similarity labels. The technique is evaluated on a variety of datasets."
1947,SP:dbb06f953788696f65013765f0a4e6967444fa0f,conditional independence assumptions USED-FOR joint distribution of nodes. work HYPONYM-OF density estimation problems. Statistics FEATURE-OF density estimation problems. Statistics CONJUNCTION machine learning. machine learning CONJUNCTION Statistics. ideas USED-FOR machine learning. ideas USED-FOR Statistics. anchor words assumptions USED-FOR Topic modeling. Topic modeling HYPONYM-OF ideas. work USED-FOR multi - class classifications. OtherScientificTerm is hard classification rule. Method is soft classification. ,This paper studies density estimation problems in Statistics and machine learning under conditional independence assumptions on the joint distribution of nodes. The authors propose two ideas for combining Statistics with machine learning: Topic modeling under anchor words assumptions and a hard classification rule. The work is applied to multi-class classifications and is shown to outperform soft classification.
1948,SP:c5c84ea1945b79b70521e0b73f762ad643175020,"cardinality - based strategy CONJUNCTION pairing - based strategy. pairing - based strategy CONJUNCTION cardinality - based strategy. pairing - based strategy HYPONYM-OF algorithmic approaches. cardinality - based strategy HYPONYM-OF algorithmic approaches. controlled numerosity and spatial layouts FEATURE-OF abstract visual scenes. pyscholinguistics USED-FOR methodologies. model USED-FOR approximate number system ( ANS ). Method are FiLM visual question answering ( VQA ) model, and VQA models. OtherScientificTerm are quantifier, first order logic, and high - order logic. ","This paper presents a FiLM visual question answering (VQA) model that is based on the idea that a quantifier should be able to distinguish between a cardinality-based strategy (i.e., the cardinality of the question and the answer) and a pair of other algorithmic approaches (e.g., a pairing -based strategy and a pairing-by-pairing strategy). The methodologies are based on pyscholinguistics, and are trained on abstract visual scenes with controlled numerosity and spatial layouts. The model is trained as an approximate number system (ANS), where the first order logic is used to predict the answer to the question, and the high-order logic is applied to the answer. Experiments show that the proposed VQA models are able to generalize to unseen objects."
1949,SP:c5c84ea1945b79b70521e0b73f762ad643175020,Method is visual question answering model ( FiLM ). Generic is model. ,This paper proposes a visual question answering model (FiLM) that is based on the idea of asking a question to an image. The model is trained on a large dataset of images. The paper is well-written and well-motivated.
1950,SP:0fb732fe65ef1081b046a6aa6e1972e40cfdc247,parameter uncertainty estimation CONJUNCTION hyperparameter optimisation. hyperparameter optimisation CONJUNCTION parameter uncertainty estimation. it USED-FOR parameter uncertainty estimation. it USED-FOR hyperparameter optimisation. posterior parameter distributions FEATURE-OF entity and relation embeddings. Variational EM USED-FOR variational parameters \gamma. MAP estimate USED-FOR posterior approximating Gaussians. variational EM USED-FOR hyperparameters. MAP training phase PART-OF training process. per - entity / per - relation hyperparameters USED-FOR MAP training phase. MRR CONJUNCTION HITS@10. HITS@10 CONJUNCTION MRR. Task is knowledge graph embeddings. Method is density - based DistMult and ComplEx variants. Material is FB and WN datasets. ,"This paper studies the problem of learning knowledge graph embeddings. The authors propose density-based DistMult and ComplEx variants and apply it to parameter uncertainty estimation and hyperparameter optimisation. Variational EM is used to estimate the variational parameters \gamma of the posterior parameter distributions of the entity and relation embedding, and it can be used for both the purpose of parameter uncertainty estimation, as well as for the task of hyperparametrically optimising the hyperparameters. The MAP estimate is used for posterior approximating Gaussians. The paper also proposes a MAP training phase, which is a part of the training process, where per-entity/per-relation hyperparametry is used. Experiments are conducted on MRR and HITS@10, and on FB and WN datasets."
1951,SP:0fb732fe65ef1081b046a6aa6e1972e40cfdc247,"entity embeddings CONJUNCTION relation embeddings. relation embeddings CONJUNCTION entity embeddings. prior Multivariate Normal distribution USED-FOR entity embeddings. prior Multivariate Normal distribution USED-FOR relation embeddings. maximum likelihood USED-FOR ( hyper-)parameters. Variational Inference ( VI ) USED-FOR posterior distribution. Stochastic VI USED-FOR Evidence Lower BOund ( ELBO ). posterior distribution FEATURE-OF embeddings. MAP estimation USED-FOR models. MAP estimation USED-FOR embedding matrices. intractable marginal likelihood USED-FOR ELBO. Method are Neural Link Prediction models, generative process, and ELBO ( Eq. 6 ). Material is large datasets. ","This paper presents a theoretical analysis of Neural Link Prediction models. The authors propose to use a prior Multivariate Normal distribution to model entity embeddings and relation embedding, which is a generalization of the prior to embedding matrices from a generative process. The main idea is to use Variational Inference (VI) to estimate the posterior distribution of the embeds, and then use maximum likelihood to approximate the (hyper-)parameters. The Evidence Lower BOund (ELBO) is derived from Stochastic VI and is based on the ELBO (Eq. 6) with intractable marginal likelihood. The paper also proposes to use MAP estimation to train the models and show that MAP estimation can be used to estimate embedding matrixrices. Experiments are conducted on large datasets."
1952,SP:5ff0668b433a190d87d5833d8b2a8ca04daa299c,online learning algorithm USED-FOR supervised dimension reduction. PCA problem USED-FOR SIR problem. inverse of covariance matrix USED-FOR PCA problem. incremental PCA USED-FOR top eigenvector. ISIR USED-FOR overlapping case. Method is incremental sliced inverse regression ( ISIR ). Generic is transformation. Task is SIR. ,"This paper proposes incremental sliced inverse regression (ISIR), an online learning algorithm for supervised dimension reduction. The main idea is to reformulate the SIR problem as a PCA problem over the inverse of covariance matrix. This transformation is called incremental PCA, where the top eigenvector is sampled from the original SIR. The paper also proposes ISIR for the overlapping case."
1953,SP:5ff0668b433a190d87d5833d8b2a8ca04daa299c,"Sliced Inverse Regression USED-FOR EDR space. Task are supervised dimension reduction problems, and updating few principal components of covariance matrices. OtherScientificTerm are eigenvalues of covariance matrices, and principal components of covariance matrices. ","This paper studies supervised dimension reduction problems, where the goal is to solve the problem of updating few principal components of covariance matrices. The main contribution of this paper is to extend the Sliced Inverse Regression to the EDR space, which is a well-studied problem in the literature. In particular, the authors consider the case where the eigenvalues of the covariance matrix are not known, and the goal of this work is to estimate the principal components. "
1954,SP:4d5b993c6be6e55bdf98eca9a3b23a1bab5d2499,shared multimodal discriminative factors CONJUNCTION modality specific generative factors. modality specific generative factors CONJUNCTION shared multimodal discriminative factors. Wassertein Auto - Encoders USED-FOR factorized joint distributions. multimodal space FEATURE-OF factorized joint distributions. wasserstein autoencoder based method USED-FOR method. Method is Multimodal Factorization model. Material is multimodal case. ,"This paper proposes a Multimodal Factorization model that combines shared multimodal discriminative factors and modality specific generative factors. The proposed method is based on the wasserstein autoencoder based method, which is well-motivated by the fact that the Wassertein Auto-Encoders can be used to learn factorized joint distributions in the multimmodal space. The authors also provide a theoretical analysis of the proposed method. The main contribution of this paper is the theoretical analysis that shows that the proposed Multimmodal factorization model can be applied to the multimodality case."
1955,SP:4d5b993c6be6e55bdf98eca9a3b23a1bab5d2499,Multimodality learning HYPONYM-OF multimedia and human computer interaction. multimodal discriminative factors CONJUNCTION modality - specific generating factors. modality - specific generating factors CONJUNCTION multimodal discriminative factors. Bayesian latent variable model USED-FOR multimodality representation. Approximate inference USED-FOR model. generalised mean - field assumption USED-FOR Approximate inference. generalised mean - field assumption USED-FOR model. ,"Multimodality learning is an important problem in multimedia and human computer interaction. This paper proposes a Bayesian latent variable model to learn a multimodality representation, which combines multimodal discriminative factors and modality-specific generating factors. Approximate inference of the model is performed under a generalised mean-field assumption."
1956,SP:cae76d3c3da91e50fe29cc3b6e204bb3e0793d7e,"adaptation technique USED-FOR TTS. small data USED-FOR adaptation. wavenet USED-FOR adaptation technique. Material are speech data, and speaker data. Method are transfer learning, and large networks. Generic are network, and model. ","This paper proposes an adaptation technique for TTS based on wavenet. The adaptation is based on small data, i.e., speech data, and large networks. The main idea is to train a network on the small data and then train a model on the large data. The idea is similar to transfer learning, but with speaker data."
1957,SP:cae76d3c3da91e50fe29cc3b6e204bb3e0793d7e,approaches COMPARE voice cloning. voice cloning COMPARE approaches. approaches USED-FOR linguistic feature conditioned WaveNet. Task is speaker adaption. ,"This paper proposes two approaches for linguistic feature conditioned WaveNet, which are different from voice cloning. The main contribution of this paper is to address the problem of speaker adaption. The paper is well-written and well-motivated. The experimental results show that the proposed approaches are superior to voice cloning in terms of performance."
1958,SP:e80d6118fc3b9ff3195fea2f6adac88e59d350c2,Huber ’s \epsilon - contamination model USED-FOR robust estimation problem. theoretical statistics USED-FOR problem. min - max problem USED-FOR robust estimation problem. formulation USED-FOR optimal statistical rate. TCS community USED-FOR approaches. OtherScientificTerm is theoretical statistics community. Method is depth functions. ,"This paper studies the robust estimation problem under Huber’s \epsilon-contamination model. The problem is formulated as a min-max problem, and theoretical statistics are used to solve the problem. The authors propose a new formulation of the optimal statistical rate, which is well-motivated by the theoretical statistics community. The proposed approaches are inspired by the TCS community, and are based on depth functions."
1959,SP:e80d6118fc3b9ff3195fea2f6adac88e59d350c2,"Huber ’s contamination model USED-FOR robust high dimensional estimation. Tukey depth CONJUNCTION matrix depth. matrix depth CONJUNCTION Tukey depth. matrix depth USED-FOR problems. Tukey depth USED-FOR problems. estimators USED-FOR problem. Generic are algorithm, it, and rates. OtherScientificTerm are eps, unconstrained noise distribution, P, noise, and identity covariance. Metric is optimal minimax rates. ","This paper studies robust high dimensional estimation using Huber’s contamination model. The authors propose a new algorithm, called Tukey depth and matrix depth, and prove that it converges to the optimal minimax rates when the eps are drawn from an unconstrained noise distribution. They also show that the rates converge when the noise is non-identical to the identity covariance. Finally, they show that their estimators can be applied to any problem."
1960,SP:861c5336fda684e5bdd8a05f0af10dd442bf5339,"object detection and segmentations model CONJUNCTION mask r - cnn. mask r - cnn CONJUNCTION object detection and segmentations model. scene programs USED-FOR image editing. scene programs USED-FOR visual analogy. image editing CONJUNCTION visual analogy. visual analogy CONJUNCTION image editing. Material are visual scenes, images, and synthetics datasets. Generic are programs, and model. ","This paper presents a set of scene programs for image editing and visual analogy. The program consists of an object detection and segmentations model, and a mask r-cnn that predicts the location of objects in visual scenes. The programs are trained on a variety of datasets, including images, videos, and synthetics datasets. The model is evaluated on several datasets and compared to state-of-the-art."
1961,SP:861c5336fda684e5bdd8a05f0af10dd442bf5339,system USED-FOR 3D scenes. sequence - to - sequence network USED-FOR neural machine translation. sequence - to - sequence network USED-FOR DSL program. OtherScientificTerm is simple primitives. Method is perceptual module. Generic is group. ,"This paper presents a system for 3D scenes. The DSL program is a sequence-to-sequence network for neural machine translation, where simple primitives are represented by a perceptual module. The perceptual module takes as input a group of objects and predicts the next object in the group."
1962,SP:a8df2aa6870a05f8580117f433e07e70a5342930,LSTM cells USED-FOR Gaussian gate. gate USED-FOR network. longer memory persistence CONJUNCTION gradient flow. gradient flow CONJUNCTION longer memory persistence. budget term USED-FOR time - gate. Gaussian - gated LSTMs COMPARE regular LSTMs. regular LSTMs COMPARE Gaussian - gated LSTMs. long temporal dependencies FEATURE-OF regular LSTMs. long temporal dependencies FEATURE-OF tasks. tasks EVALUATE-FOR Gaussian - gated LSTMs. tasks EVALUATE-FOR regular LSTMs. curriculum training schedule USED-FOR LSTMS. Method is phased LSTM. OtherScientificTerm is gaussian gates. ,This paper proposes a new phase-based Gaussian gate based on LSTM cells. The key idea of the proposed gate is to allow the network to learn longer memory persistence and better gradient flow. The proposed time-gate is a budget term that is added to each time-step of a phase of the phase of a Gaussian gates. Experiments show that the proposed Gaussian-gated LSTMs outperform the regular LSTm on a variety of tasks with long temporal dependencies. The authors also propose a curriculum training schedule for LSTMS. 
1963,SP:a8df2aa6870a05f8580117f433e07e70a5342930,budget term PART-OF loss function. Task is reduction of training time. OtherScientificTerm is time gate. Generic is it. ,"This paper studies the problem of reduction of training time. In particular, the authors propose to add a budget term to the loss function. This budget term is defined as the difference between the time gate and the output of the training process. The authors show that this budget term can be used to reduce the training time and show that it can be done efficiently."
1964,SP:e39bcc2ee6db054f0f1d8e8d04291a78488886ae,"algorithm USED-FOR out - of - distribution samples. statistics of neural activations of deep networks USED-FOR out - of - distribution samples. statistics of neural activations of deep networks USED-FOR algorithm. linear classifier USED-FOR feature representations. OtherScientificTerm is BatchNorm layers. Generic are state - of - art, and method. ","This paper proposes an algorithm to generate out-of-distribution samples based on the statistics of neural activations of deep networks. The proposed method, called BatchNorm layers, is based on a linear classifier that learns the feature representations. The experimental results show that the proposed method is able to generate samples that are more diverse than the state of the art."
1965,SP:e39bcc2ee6db054f0f1d8e8d04291a78488886ae,statistics CONJUNCTION information summary measures. information summary measures CONJUNCTION statistics. information summary measures USED-FOR deep nets. statistics USED-FOR deep nets. mean and variance of Z - scores USED-FOR ID and OOD samples. mean and variance of Z - scores USED-FOR features. ,This paper proposes to combine statistics and information summary measures to improve the performance of deep nets. The main idea is to use the mean and variance of Z-scores for ID and OOD samples to estimate the features. 
1966,SP:827f95cdefae78e38a9c4b5718fcf294606a1989,"sigmoid activation function USED-FOR hidden layer neural network. gradient descent USED-FOR teacher network ’s parameter. statistical accuracy EVALUATE-FOR gradient descent. local convergence FEATURE-OF gradient descent. negative log likelihood loss FEATURE-OF classification problem. OtherScientificTerm are Gaussian input, and initialization. ",This paper studies the problem of training a hidden layer neural network with a sigmoid activation function with Gaussian input. The authors show that gradient descent with respect to the teacher network’s parameter converges with high statistical accuracy. The local convergence of gradient descent is proved for the classification problem with a negative log likelihood loss. The paper also provides a theoretical analysis of the initialization.
1967,SP:827f95cdefae78e38a9c4b5718fcf294606a1989,cross - entropy loss FEATURE-OF neural network. neural network USED-FOR global minimizer. gradient descent USED-FOR neural network. spectral learning USED-FOR method. OtherScientificTerm is objective function. ,"This paper studies the problem of learning a global minimizer of a neural network with cross-entropy loss using gradient descent. The proposed method is based on spectral learning, where the objective function is a function of the number of parameters."
1968,SP:2b4a39b997934ccf0e6b5fcb4d1e62253592b05f,feature boosting and suppression method USED-FOR dynamic channel pruning. affine function USED-FOR method. ,This paper proposes a feature boosting and suppression method for dynamic channel pruning. The proposed method is based on an affine function.
1969,SP:2b4a39b997934ccf0e6b5fcb4d1e62253592b05f,"computational burden CONJUNCTION memory consumption. memory consumption CONJUNCTION computational burden. method USED-FOR channels. Method are CNN network, pruning strategy, and CNN. Task is real - world application. ",This paper proposes a pruning strategy to reduce the computational burden and memory consumption of the CNN network. The proposed method prunes the channels that are not useful for the CNN. The experiments show that the proposed method is effective in real-world application.
1970,SP:2b1813a3cc39d6e1eba546b456bf8d1f9cc8657c,mixed equilibrium objective function USED-FOR GANS. mirror descent / mirror prox USED-FOR continuous games. infinite dimensional spaces FEATURE-OF algorithms. OtherScientificTerm is Continuous Games. ,This paper studies the problem of optimizing a mixed equilibrium objective function for GANS. Continuous Games are a special case of this problem. The authors propose two algorithms in infinite dimensional spaces based on mirror descent/mirror prox for continuous games.
1971,SP:2b1813a3cc39d6e1eba546b456bf8d1f9cc8657c,they USED-FOR mixed Nash equilibrium. mirror - descent and mirror - prox algorithms USED-FOR infinite dimensional Banach spaces. mixed Nash equilibrium FEATURE-OF generative adversarial networks. sample - based practical algorithm USED-FOR infinite dimensional algorithms. visual appeal USED-FOR evaluation criterion. OtherScientificTerm is finite dimensional spaces. ,"This paper studies the mirror-descent and mirror-prox algorithms for infinite dimensional Banach spaces, and shows that they converge to a mixed Nash equilibrium for generative adversarial networks. The authors then propose a sample-based practical algorithm for infinite-dimensional algorithms, which is similar to previous work on finite dimensional spaces. The evaluation criterion is based on visual appeal."
1972,SP:79ece684e3c4aca516b4ec41aa8fcb7d86449784,"Method are neural network models, and network model. OtherScientificTerm is batch normalisation related parameters. Task is classification. ","This paper studies the problem of batch normalisation of neural network models. The authors argue that batch normalising the weights of the network model is a good way to reduce the number of parameters needed to train the network. The main contribution of this paper is to propose a way to train a network model that does not need to learn batch normalised related parameters. This is an interesting idea, and the authors show that this can lead to significant improvements in classification performance."
1973,SP:79ece684e3c4aca516b4ec41aa8fcb7d86449784,model patches USED-FOR network. multi - task learning CONJUNCTION domain adaptation. domain adaptation CONJUNCTION multi - task learning. transfer learning CONJUNCTION multi - task learning. multi - task learning CONJUNCTION transfer learning. settings EVALUATE-FOR fine - tuning. domain adaptation HYPONYM-OF settings. accuracy EVALUATE-FOR fine - tuning. transfer learning HYPONYM-OF settings. multi - task learning HYPONYM-OF settings. Method is fine - tuning neural networks. Generic is model. ,"This paper studies the problem of fine-tuning neural networks. The authors propose to fine-tune the model patches of a network by adding model patches to the original network. The idea is that the model can be trained to be more robust to changes in the input data. The experiments are conducted in three settings: transfer learning, multi-task learning, and domain adaptation. The results show that fine -tuning can improve accuracy in all three settings."
1974,SP:82b8270b33110e50b5914246f3ca75d3bdbffb6e,"Mode Normalization ( MN ) HYPONYM-OF normalization technique. BN USED-FOR normalization technique. gating mechanism CONJUNCTION attention mechanism. attention mechanism CONJUNCTION gating mechanism. gating mechanism USED-FOR It. Method are Batch Normalization ( BN ), and normalization. OtherScientificTerm are batch size, mean, multi - modal features, and mini - batch. ","This paper proposes a new normalization technique called Mode Normalization (MN), which is a variant of Batch Normalization(BN) that is based on BN. It uses a gating mechanism and an attention mechanism to encourage the batch size to be smaller than the mean. The main idea is to use multi-modal features, i.e., the mini-batch of the original batch should be larger than the full batch. The authors show that this kind of normalization can lead to better performance."
1975,SP:82b8270b33110e50b5914246f3ca75d3bdbffb6e,spatial dimensions FEATURE-OF unit activations. mixture of modes USED-FOR unit activation statistics. affine parameters CONJUNCTION softmax. softmax CONJUNCTION affine parameters. gating functions CONJUNCTION softmax. softmax CONJUNCTION gating functions. affine parameters USED-FOR gating functions. variant USED-FOR Group Normalisation. Task is Batch Normalisation ( BN ). Method is convolutional networks. OtherScientificTerm is mode specific means and variances. ,"This paper studies the problem of Batch Normalisation (BN) in the context of convolutional networks. In particular, the authors consider the case where the unit activations are expressed in spatial dimensions and the mode specific means and variances are not known. The authors propose to use a mixture of modes to approximate the unit activation statistics. The main idea is to use gating functions based on affine parameters and softmax. This variant is called Group Normalisation."
1976,SP:034c3bc2b2fe4991f56f168ea7b4b552c500b9ad,"pruning USED-FOR sparse architectures. generalization accuracy EVALUATE-FOR subnetworks. OtherScientificTerm is sparse subnetworks. Method are pruned networks, and overparameterized networks. Material is lottery ticket hypothesis. Generic is algorithm. ","This paper studies the problem of pruning for sparse architectures. The authors argue that pruned networks are more robust to overparameterized networks, and that sparse subnetworks can lead to better generalization accuracy. To support this argument, the authors propose a lottery ticket hypothesis, and propose an algorithm that prunes the weights of a subset of the weights."
1977,SP:034c3bc2b2fe4991f56f168ea7b4b552c500b9ad,"smaller networks COMPARE network. network COMPARE smaller networks. large neural networks USED-FOR smaller networks. small sub - networks PART-OF large networks. pruning methods USED-FOR compressing networks. method USED-FOR winning tickets. architectures CONJUNCTION tasks. tasks CONJUNCTION architectures. architectures EVALUATE-FOR hypothesis. tasks EVALUATE-FOR hypothesis. pruning methods USED-FOR method. Method are small networks, large network, and randomly initialized large networks. OtherScientificTerm are Lottery Ticket Hypothesis, connectivity, and subnetworks. ","This paper studies the Lottery Ticket Hypothesis, which states that smaller networks trained on large neural networks are more likely to win than a network trained on small networks. The authors argue that this is due to the fact that small sub-networks in large networks tend to be more connected to each other than to the original large network. To prove this hypothesis, the authors propose a method for compressing winning tickets using pruning methods and show that randomly initialized large networks have higher connectivity to their subnetworks. The hypothesis is tested on different architectures and tasks."
1978,SP:08c662296c7cf346f027e462d29184275fd6a102,"state representation USED-FOR exploration in RL. state representation USED-FOR sparse reward task. clustering scheme USED-FOR state ’s cluster index. convolutional network USED-FOR pixel representation. convolutional network USED-FOR feature map. attention mechanism USED-FOR inverse dynamics models. mechanism USED-FOR agent ’s coordinates. Atari games EVALUATE-FOR mechanism. RAM state USED-FOR agent ’s coordinates. Montezuma ’s Revenge HYPONYM-OF Atari games. coordinates USED-FOR count - based exploration. coordinates COMPARE vanilla A2C. vanilla A2C COMPARE coordinates. count - based exploration COMPARE vanilla A2C. vanilla A2C COMPARE count - based exploration. OtherScientificTerm are controllable ( learned ) features, and x, y coordinates. Task are Atari game, and exploration in sparse reward settings. Generic are position, and model. Method are count - based exploration mechanisms, inverse dynamics model, and attention model. ","This paper studies the problem of learning a state representation for exploration in RL that can be used in a sparse reward task. The authors propose a clustering scheme to learn the state’s cluster index, which is used to estimate the position of the agent in an Atari game. The key idea is to use controllable (learned) features to represent the state. The feature map is learned using a convolutional network that maps the pixel representation to the x, y coordinates of the state, which are then used to train inverse dynamics models with an attention mechanism. The proposed mechanism is evaluated on two Atari games (Montezuma‘s Revenge, and a variant of the Atari game called “Battleship”). The authors show that the learned coordinates are useful for count-based exploration mechanisms, and that the proposed inverse dynamics model outperforms the vanilla A2C in terms of the number of actions required to learn these coordinates. "
1979,SP:08c662296c7cf346f027e462d29184275fd6a102,attentive dynamics model ( ADM ) USED-FOR contingency - aware exploration. self supervised manner USED-FOR ADM. approach COMPARE count based techniques. count based techniques COMPARE approach. agent's curiosity USED-FOR exploration. technique USED-FOR tasks. intrinsic motivation CONJUNCTION self - supervised dynamics model. self - supervised dynamics model CONJUNCTION intrinsic motivation. ADM COMPARE methods. methods COMPARE ADM. methods USED-FOR contingency - awareness. self - supervised dynamics model HYPONYM-OF methods. ADM USED-FOR contingency - awareness. intrinsic motivation HYPONYM-OF methods. OtherScientificTerm is agents policy. Generic is it. Method is modelling techniques. ,"This paper proposes an attentive dynamics model (ADM) for contingency-aware exploration in a self supervised manner. The approach is similar to count based techniques, where the agent's curiosity is used to guide exploration. The technique is applied to a variety of tasks where the agents policy is not fully supervised. The authors show that ADM outperforms other methods on the problem of contingency-awareness, such as intrinsic motivation and self-supervised dynamics model, by a large margin. They also show that it is compatible with other modelling techniques."
1980,SP:614f742a75039b1509343d53e0fb4a6d4088ab3e,deep neural networks USED-FOR task. approach USED-FOR task. hyper networks CONJUNCTION adversarial auto - encoders. adversarial auto - encoders CONJUNCTION hyper networks. hyper networks USED-FOR architecture. adversarial auto - encoders USED-FOR architecture. low dimensional latent space USED-FOR networks. ensemble of networks USED-FOR uncertainty estimation. approach USED-FOR ensemble of networks. ,This paper proposes an approach to solving a new task using deep neural networks. The architecture is based on hyper networks and adversarial auto-encoders. The networks are trained on a low dimensional latent space. The approach is applied to an ensemble of networks for uncertainty estimation.
1981,SP:614f742a75039b1509343d53e0fb4a6d4088ab3e,technique USED-FOR distribution over parameters. neural network USED-FOR distribution over parameters. adversarial loss USED-FOR distribution of parameters. approach USED-FOR sampled parameters. Gaussian distributed FEATURE-OF distribution of parameters. adversarial loss USED-FOR approach. approach USED-FOR uncertainty estimates. ensembling USED-FOR uncertainty estimates. out - of - distribution examples USED-FOR uncertainty estimates. ensembling USED-FOR approach. problems EVALUATE-FOR approach. OtherScientificTerm is distribution. ,This paper proposes a technique to learn a distribution over parameters using a neural network. The approach is based on using an adversarial loss on the distribution of parameters that is Gaussian distributed. The authors show that this approach can be used to estimate the uncertainty of sampled parameters. The proposed approach uses ensembling to obtain uncertainty estimates from out-of-distribution examples. The paper also shows that the proposed approach can generalize to new problems.
1982,SP:230b3e008e687e03a8b914084b93fc81609051c0,"gradient FEATURE-OF reconstruction loss. gradient FEATURE-OF ELBO. encoder parameters FEATURE-OF reconstruction loss. reparameterization USED-FOR low - variance estimate. reparameterization USED-FOR VAEs. reparameterization CONJUNCTION likelihood - ratio estimators. likelihood - ratio estimators CONJUNCTION reparameterization. trick USED-FOR VAEs. discrete latent variables USED-FOR VAEs. Task are gradient of the ELBO, and importance - weighted estimate of the gradient. Method are gradient estimators, and importance - sampling estimate. OtherScientificTerm is importance sampling distribution. Generic is distribution. ","This paper studies the gradient of the ELBO, which is a function of the reconstruction loss with respect to the encoder parameters. The main contribution of this paper is to provide an importance-weighted estimate of the gradient, which can be used to derive a low-variance estimate for VAEs using reparameterization and likelihood-ratio estimators. This trick can be applied to VAEs with discrete latent variables, where the importance sampling distribution is assumed to be the same as that of the original distribution. The authors show that the gradient estimators can be trained to converge to the importance-sampling estimate."
1983,SP:230b3e008e687e03a8b914084b93fc81609051c0,"expected log likelihood ( ELL ) term PART-OF ELBO. importance sampling USED-FOR VAEs. discrete latent variables USED-FOR VAEs. variational distribution USED-FOR importance sampling distribution. MNIST CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION MNIST. Bernoulli and categorical latent variables USED-FOR Fashion - MNIST. Generic is it. OtherScientificTerm are reparametrization gradients, and ELL gradient. ","This paper studies the problem of importance sampling in VAEs with discrete latent variables. Specifically, the authors propose to replace the expected log likelihood (ELBO) term in the ELBO with a variational distribution, and show that it can be used to estimate the reparametrization gradients. The authors also show that the ELL gradient converges to zero when the number of samples is small. Experiments on MNIST and Fashion-MNIST with Bernoulli and categorical latent variables are provided."
1984,SP:153fe1172e689b345729c0c848cfb38bdae0e5f7,"RBM feature extractor CONJUNCTION CNN classifiers. CNN classifiers CONJUNCTION RBM feature extractor. MNIST USED-FOR small mean field boltzmann machine. RBM 8x8 feature representation USED-FOR fixed convolutional layer. it USED-FOR CNN. RBM layer USED-FOR denoiser. Metric is robustness. OtherScientificTerm is adversarial attacks. Method are 8x8 feature extractor, and RBMs. ","This paper proposes to improve the robustness of the RBM feature extractor and CNN classifiers to adversarial attacks. The main idea is to use a small mean field boltzmann machine trained on MNIST with an RBM 8x8 feature representation as a fixed convolutional layer. The RBM layer is used as a denoiser, and it is then used to train a CNN. Experiments are conducted to show that RBMs are more robust."
1985,SP:153fe1172e689b345729c0c848cfb38bdae0e5f7,"Bayes rule inversion USED-FOR classification. optimization based inference USED-FOR loglikelihoods. OtherScientificTerm is adversarial attacks. Method are variational autoencoders, and inference network. ","This paper studies adversarial attacks against variational autoencoders. The authors propose to use Bayes rule inversion for classification. The main idea is to use optimization based inference to estimate loglikelihoods, which can then be used to train the inference network."
1986,SP:40ade446aa4a700cb1519b9115e8d6cdf33db4a4,visible regions PART-OF minimal recognizable images. Method is DNNs. Metric is DNN ability. ,This paper studies the performance of DNNs in the presence of visible regions in minimal recognizable images. The authors show that the DNN ability can be improved when the visible regions are present in minimalizable images. 
1987,SP:40ade446aa4a700cb1519b9115e8d6cdf33db4a4,minimally recognizable patches COMPARE deep neural network. deep neural network COMPARE minimally recognizable patches. training methodology CONJUNCTION pooling architectures. pooling architectures CONJUNCTION training methodology. architectures USED-FOR fragility. deep CNNs COMPARE human vision. human vision COMPARE deep CNNs. fragile behavior FEATURE-OF deep CNNs. Generic is it. ,This paper proposes a new training methodology and pooling architectures to address the fragility of deep CNNs. The main idea is to use minimally recognizable patches instead of a deep neural network. The authors show that it is possible to learn a deep CNN with fragile behavior and compare it to human vision.
1988,SP:8ab0bb3eb38958d607fe6b6ebbd921b8abdf149d,generating contracts USED-FOR multiagent tasks. deep reinforcement learning approach USED-FOR contracts. agent modeling USED-FOR agent skills and preferences. Generic is approach. ,This paper focuses on the problem of generating contracts for multiagent tasks using a deep reinforcement learning approach. The key idea of the approach is to use agent modeling to learn agent skills and preferences. 
1989,SP:8ab0bb3eb38958d607fe6b6ebbd921b8abdf149d,"resource collection CONJUNCTION crafting. crafting CONJUNCTION resource collection. domains EVALUATE-FOR approach. crafting HYPONYM-OF domains. resource collection HYPONYM-OF domains. Method are manager agent, and manager. Generic is agents. ",This paper proposes a manager agent that learns to select a set of agents that can be used to improve the performance of other agents. The proposed approach is evaluated on two domains: resource collection and crafting. The manager agent is trained to select agents that perform well in both domains. The authors also show that the manager agent can learn to generalize to unseen tasks.
1990,SP:50a5e5227932ff1196706f53fb82f1785da45e2a,time features CONJUNCTION global static and changing features. global static and changing features CONJUNCTION time features. sparse features CONJUNCTION time features. time features CONJUNCTION sparse features. features USED-FOR recurrent neural network. cell state CONJUNCTION output state update rules. output state update rules CONJUNCTION cell state. model COMPARE time - variant LSTM ( TLSTM ). time - variant LSTM ( TLSTM ) COMPARE model. modified UCI dataset CONJUNCTION proprietary dataset. proprietary dataset CONJUNCTION modified UCI dataset. proprietary dataset EVALUATE-FOR model. modified UCI dataset EVALUATE-FOR model. OtherScientificTerm is dense features. ,"This paper proposes a recurrent neural network with sparse features, time features, global static and changing features. These features are learned by training a recurrent network with cell state and output state update rules. The proposed model is compared to a time-variant LSTM (TLSTM) on a modified UCI dataset and a proprietary dataset, where dense features are used."
1991,SP:50a5e5227932ff1196706f53fb82f1785da45e2a,"LTSM method USED-FOR modeling time series. power consumption data set CONJUNCTION churn prediction application. churn prediction application CONJUNCTION power consumption data set. Generic are applications, and they. Task is IoT applications. ",This paper proposes an LTSM method for modeling time series. The paper focuses on two applications: a power consumption data set and a churn prediction application. Both of these applications are well-motivated as they are important for IoT applications. 
1992,SP:f2c3dd2b485d6307847c759a5609b7ebe24b7058,"Method are neural model, feedforward neural network, and recurrent neural networks. OtherScientificTerm are logical formula, and tautology. Generic is network. ","This paper studies the problem of learning a neural model that predicts the output of a given neural model. In particular, the authors propose a feedforward neural network, which is a generalization of recurrent neural networks. The authors provide a logical formula for how to train such a network, and provide a tautology for how the network should be trained. "
1993,SP:f2c3dd2b485d6307847c759a5609b7ebe24b7058,"neural net USED-FOR model. neural net USED-FOR parse tree. model COMPARE models. models COMPARE model. Evans et al.'s data set USED-FOR logical entailment queries. Evans et al.'s data set EVALUATE-FOR model. Method are neural - net model of logical formulae, and RNN - based neural net. Generic is it. Task is logical entailment. OtherScientificTerm is parse trees. ","This paper proposes a neural-net model of logical formulae. The model uses a neural net to generate a parse tree, and then uses it to query for logical entailment. The authors show that the proposed model outperforms existing models on Evans et al.'s data set for solving logical entailMENT queries. The main contribution of the paper is the introduction of an RNN-based neural net, which can be used to generate parse trees."
1994,SP:845ae21e5758a8aabfa610c291fdcc5f61af7748,curriculum learning ( CL ) USED-FOR dnn. scoring function CONJUNCTION pacing function. pacing function CONJUNCTION scoring function. DNN USED-FOR CL. Generic is parts. ,This paper proposes curriculum learning (CL) for dnn. CL is a generalization of DNN with two parts: a scoring function and a pacing function. 
1995,SP:845ae21e5758a8aabfa610c291fdcc5f61af7748,"Curriculum Learning ( CL ) USED-FOR deep learning. non - random order USED-FOR CL. scoring function CONJUNCTION pacing function. pacing function CONJUNCTION scoring function. former CONJUNCTION latter. latter CONJUNCTION former. former HYPONYM-OF functions. latter HYPONYM-OF functions. pacing function HYPONYM-OF functions. scoring function HYPONYM-OF functions. Task are learning, and generalization. ","Curriculum Learning (CL) is an important problem in deep learning, and the authors propose to use non-random order to train CL. The authors propose three functions: a scoring function, a pacing function, and a latter. The main contribution of the paper is to study the effect of the three functions on the learning performance and generalization. "
1996,SP:b33a6a1fe4bbae422ba001cbe656f31d07a62025,"generalization bounds FEATURE-OF ( deep ) neural networks. Gaussian noise USED-FOR network weight parameters. layer Jacobian norm CONJUNCTION layer output norm. layer output norm CONJUNCTION layer Jacobian norm. layer output norm CONJUNCTION pre - activation value. pre - activation value CONJUNCTION layer output norm. empirical margin CONJUNCTION complexity term. complexity term CONJUNCTION empirical margin. empirical margin USED-FOR ( ReLU ) neural network classifier. Method are deterministic predictor, and neural networks stochastic surrogates. OtherScientificTerm are PAC - Bayesian bounds, and noise - resilience "" properties. ","This paper studies generalization bounds for (deep) neural networks. The authors propose to use Gaussian noise as the network weight parameters and derive PAC-Bayesian bounds for a deterministic predictor. The main contributions of the paper are: (1) Theorems on the correlation between the layer Jacobian norm, the layer output norm, and the pre-activation value of a (ReLU) neural network classifier with empirical margin and complexity term, and (2) Theorem 3.1 and (3.2) for neural networks stochastic surrogates. Theoretical results on the ""noise-resilience"" properties are provided."
1997,SP:b33a6a1fe4bbae422ba001cbe656f31d07a62025,PAC - Bayesian framework USED-FOR model. generalization error EVALUATE-FOR model. PAC - Bayesian framework USED-FOR generalization error. perturbed weights USED-FOR networks. OtherScientificTerm is PAC - Bayesian bounds. Generic is network. ,"This paper proposes a PAC-Bayesian framework to evaluate the generalization error of a model. The authors show that networks trained with perturbed weights converge to a fixed point, which is in contrast to the existing state-of-the-art results. They also provide a set of PAC - Bayesian bounds on the number of perturbed points in a network."
1998,SP:d0533cb69d938d4128d17b1a6d8aeb8d1ca6e3fd,training procedure USED-FOR VQ - VAE. training algorithm CONJUNCTION EM algorithm. EM algorithm CONJUNCTION training algorithm. soft EM COMPARE learning procedure. learning procedure COMPARE soft EM. image and text datasets EVALUATE-FOR soft EM. image and text datasets EVALUATE-FOR learning procedure. Method is soft EM algorithm. ,"This paper proposes a new training procedure for VQ-VAE. The authors propose a soft EM algorithm, which is a combination of a training algorithm and an EM algorithm. Experiments on image and text datasets show that the proposed soft EM outperforms the original learning procedure."
1999,SP:d0533cb69d938d4128d17b1a6d8aeb8d1ca6e3fd,"VQ - VAE USED-FOR learning discrete latent variables. non - autoregressive decoder USED-FOR latency. VQ - VAE USED-FOR NMT. non - autoregressive decoder USED-FOR NMT. EMA technique USED-FOR discrete latent states. EMA technique USED-FOR hard EM. Monte - Carlo EM algorithm USED-FOR learning technique. discrete latent states CONJUNCTION hard EM. hard EM CONJUNCTION discrete latent states. latent Transformer USED-FOR EN - DE NMT. OtherScientificTerm are discrete latent variables, and latent variables. ",This paper proposes a VQ-VAE for learning discrete latent variables for NMT with a non-autoregressive decoder to reduce latency. The learning technique is based on the Monte-Carlo EM algorithm. The authors also propose an EMA technique to learn both discrete latent states and hard EM. Experiments on EN-DE NMT using latent Transformer show the effectiveness of the proposed latent variables.
2000,SP:60628f7db9cfcac3f0dbe6ce0b2a161310525ba0,multiview framework USED-FOR sentence representation in NLP tasks. other HYPONYM-OF architectures. one HYPONYM-OF architectures. discriminative objective USED-FOR architectures. generative objective USED-FOR architectures. discriminative objective USED-FOR one. generative objective USED-FOR one. discriminative objective USED-FOR other. recurrent based encoding function CONJUNCTION linear model. linear model CONJUNCTION recurrent based encoding function. frameworks COMPARE baselines. baselines COMPARE frameworks. NLP tasks EVALUATE-FOR frameworks. ,"This paper proposes a multiview framework for sentence representation in NLP tasks. Two architectures are proposed, one using a generative objective and the other using a discriminative objective. A recurrent based encoding function and a linear model are also proposed. The proposed frameworks are evaluated on a variety of standard NLP task and compared with several baselines."
2001,SP:60628f7db9cfcac3f0dbe6ce0b2a161310525ba0,multi - view framework USED-FOR sentence representations. generative one CONJUNCTION discriminative one. discriminative one CONJUNCTION generative one. them HYPONYM-OF encoders. other HYPONYM-OF encoders. RNN USED-FOR them. linear projection of averaged word embeddings USED-FOR other. discriminative one HYPONYM-OF objective functions. encoders USED-FOR objective functions. generative one HYPONYM-OF objective functions. multi - view framework FEATURE-OF objective functions. multi - view framework COMPARE independent encoders. independent encoders COMPARE multi - view framework. ,"This paper proposes a multi-view framework for learning sentence representations. The authors propose two objective functions, a generative one and a discriminative one, based on two encoders, one of which is a linear projection of averaged word embeddings and the other is an RNN trained on the output of the first encoder. The results show that the proposed multi-views framework is able to learn better sentence representations than independent encoder."
2002,SP:f5da908b5f6c19a059d2447b9cda15af5e12dc55,"straggling computing nodes USED-FOR distributed optimization. Method are synchronous distributed optimization approach, Anytime MiniBatch ( AMB ) approach, consensus mechanism, and Fixed MiniBatch ( FMB ) approach. OtherScientificTerm are stragglers, synchronization operation, and computing node. Task is optimization process. ","This paper proposes a synchronous distributed optimization approach. The main idea is to use straggling computing nodes for distributed optimization. The authors propose an Anytime MiniBatch (AMB) approach, where the stragglers are selected by a consensus mechanism, and the synchronization operation is performed on each computing node. The optimization process is then repeated until convergence. In contrast to the Fixed MiniBandatch (FMB) approach where the computing node is randomly selected, the authors propose to use a fixed number of computing nodes."
2003,SP:f5da908b5f6c19a059d2447b9cda15af5e12dc55,"fully distributed topology FEATURE-OF online stochastic convex optimization. slow nodes USED-FOR slow progress. Anytime Minibatch ( AMB ) HYPONYM-OF online distributed optimization method. Task is synchronous setting. OtherScientificTerm are stragglers, AMB, minibatch size, computation time, network, and nodes. ","This paper studies the problem of online stochastic convex optimization with fully distributed topology. In particular, the authors consider the synchronous setting, where slow nodes are responsible for slow progress. The authors propose Anytime Minibatch (AMB), an online distributed optimization method that can handle stragglers. AMB is based on minimizing the minibatch size and the computation time of the network, which is achieved by minimizing the number of nodes."
2004,SP:f167ad4bb1e140f692ec71c8baf0a59bff7bbc6f,human emotional reaction USED-FOR autonomous driving. reinforcement learning framework USED-FOR autonomous driving. human emotional reaction USED-FOR reinforcement learning framework. extrinsic ( goal oriented ) reward CONJUNCTION intrinsic reward. intrinsic reward CONJUNCTION extrinsic ( goal oriented ) reward. intrinsic reward PART-OF reward function. extrinsic ( goal oriented ) reward PART-OF reward function. virtual environment FEATURE-OF task. blood volume pulse wave ( BVP ) USED-FOR emotional response. intrinsic reward USED-FOR deep Q networks. extrinsic reward USED-FOR deep Q networks. OtherScientificTerm is reward. ,This paper proposes a reinforcement learning framework based on human emotional reaction for autonomous driving. The reward function consists of an extrinsic (goal oriented) reward and an intrinsic reward. The intrinsic reward is used to train deep Q networks. The emotional response is modeled using blood volume pulse wave (BVP). The task is performed in a virtual environment.
2005,SP:f167ad4bb1e140f692ec71c8baf0a59bff7bbc6f,signals USED-FOR RL framework. autonomic visceral responses USED-FOR decision making. model USED-FOR RL reward function. human nervous system responses USED-FOR model. Generic is these. ,This paper proposes an RL framework that uses signals from autonomic visceral responses to guide decision making. The model is based on human nervous system responses and uses these to learn an RL reward function.
2006,SP:2db0ece25ebfb4d5e3aa8eb145964ce4be19409f,"probablistic method USED-FOR modeling distributions of functions. Neural process ( NP ) USED-FOR modeling distributions of functions. Neural process ( NP ) HYPONYM-OF probablistic method. mean - aggregation step PART-OF encoder. attention mechanism USED-FOR deterministic path. tasks EVALUATE-FOR method. OtherScientificTerm are distributions of functions, under - fitting, and hypoethesize. Method are NP, and decoder. ",This paper proposes a probablistic method called Neural process (NP) for modeling distributions of functions. The main idea of NP is to decompose the encoder into a mean-aggregation step and a decoder. The attention mechanism is then used to generate a deterministic path. The method is evaluated on several tasks and is shown to be able to generalize to unseen functions without under-fitting or hypoethesize.
2007,SP:2db0ece25ebfb4d5e3aa8eb145964ce4be19409f,cross - attention USED-FOR query - specific representation. self - attention CONJUNCTION cross - attention. cross - attention CONJUNCTION self - attention. attention processes PART-OF neural processes. cross - attention HYPONYM-OF attention processes. self - attention HYPONYM-OF attention processes. MLPs CONJUNCTION mean pooling. mean pooling CONJUNCTION MLPs. mean pooling HYPONYM-OF attention processes. MLPs HYPONYM-OF attention processes. underfitting problem FEATURE-OF NPs. ANPs COMPARE NPs. NPs COMPARE ANPs. ,"This paper proposes to incorporate attention processes in neural processes, such as self-attention and cross-attentive, to learn a query-specific representation. The proposed attention processes include MLPs, mean pooling, etc. The paper also studies the underfitting problem of NPs. The results show that ANPs can outperform NPs in some cases."
2008,SP:26535b26a3178050d8aae56b7c9669c9d2408ac8,"MAML CONJUNCTION E - MAML. E - MAML CONJUNCTION MAML. gradient calculation USED-FOR MAML. gradient calculation USED-FOR E - MAML. casual dependence USED-FOR MAML. auto - differentiation USED-FOR gradient. MAML CONJUNCTION E - MAML. E - MAML CONJUNCTION MAML. methods COMPARE MAML. MAML COMPARE methods. meta - RL tasks EVALUATE-FOR methods. OtherScientificTerm are gradients, and low - variance but biased gradient. Generic is algorithms. Method are DiCE formulation, and DiCE objective formulation. ","This paper proposes two algorithms, MAML and E-MAML, which combine the gradient calculation of MAMM with E-MAAML. The main idea is to use casual dependence between the gradients of the two algorithms. In particular, the authors propose to use auto-differentiation to compute the gradient of the gradient in E-MMAML instead of the standard DiCE formulation. The authors also propose a DiCE objective formulation, which allows to compute a low-variance but biased gradient. Experiments on several meta-RL tasks show that the proposed methods outperform the state-of-the-art in terms of performance on a variety of tasks, and that the performance of both methods is comparable to that of the state of the art on MAMAL."
2009,SP:26535b26a3178050d8aae56b7c9669c9d2408ac8,surrogate loss USED-FOR Hessian. surrogate loss USED-FOR Meta - reinforcement learning. method COMPARE meta - learning algorithms. meta - learning algorithms COMPARE method. Mujoco benchmarks EVALUATE-FOR meta - learning algorithms. Mujoco benchmarks EVALUATE-FOR method. gradient variance CONJUNCTION average return. average return CONJUNCTION gradient variance. gradient variance EVALUATE-FOR DiCE. average return EVALUATE-FOR DiCE. OtherScientificTerm is small bias. ,Meta-reinforcement learning uses a surrogate loss to approximate the Hessian. The paper shows that the proposed method DiCE outperforms other meta-learning algorithms on Mujoco benchmarks. DiCE achieves better gradient variance and average return with small bias.
2010,SP:be5f2c827605914206f5645087b94a50f59f9214,"deep, message passing neural net USED-FOR satisfiability of CNF instances. NeuroSAT architecture USED-FOR satisfiability of CNF instances. deep, message passing neural net USED-FOR NeuroSAT architecture. architecture USED-FOR satisfiable assignment. SAT case FEATURE-OF satisfiable assignment. permutation invariance CONJUNCTION negation invariance. negation invariance CONJUNCTION permutation invariance. message passing USED-FOR symmetries. symmetries FEATURE-OF SAT instances. negation invariance HYPONYM-OF symmetries. permutation invariance HYPONYM-OF symmetries. vertex covers CONJUNCTION dominating sets. dominating sets CONJUNCTION vertex covers. graph colorings CONJUNCTION vertex covers. vertex covers CONJUNCTION graph colorings. unstructured ( RS ) problems CONJUNCTION structured ones. structured ones CONJUNCTION unstructured ( RS ) problems. graph colorings CONJUNCTION dominating sets. dominating sets CONJUNCTION graph colorings. unstructured ( RS ) problems FEATURE-OF random SAT instances. structured ones FEATURE-OF random SAT instances. random SAT instances EVALUATE-FOR architecture. dominating sets HYPONYM-OF structured ones. graph colorings HYPONYM-OF structured ones. vertex covers HYPONYM-OF structured ones. Material is UNSAT case. ","This paper proposes a deep, message passing neural net for improving the satisfiability of CNF instances using the NeuroSAT architecture. The proposed architecture aims to learn a satisfiable assignment in the SAT case, which is a special case of the UNSAT case. The main contribution of this paper is to study the symmetries of SAT instances via message passing. The authors evaluate the performance of the proposed architecture on random SAT instances from unstructured (RS) problems, structured ones (graph colorings, vertex covers, dominating sets, etc)."
2011,SP:be5f2c827605914206f5645087b94a50f59f9214,"neural network USED-FOR satisfiability problems. classifier USED-FOR satisfiability. it USED-FOR classifier. it USED-FOR satisfiability. supervision USED-FOR classifier. NeuroUNSAT USED-FOR contradictions. UNSAT cores FEATURE-OF contradictions. NeuroUNSAT USED-FOR unsatisfiable problems. Method are message passing neural network, and NeuroSAT. Generic is network. ","This paper proposes a message passing neural network, called NeuroSAT, to solve satisfiability problems. The proposed network is based on the idea that it can be used as a classifier for satisfiability without supervision. NeuroUNSAT is trained to identify contradictions in the UNSAT cores and to solve unsatisfiable problems."
2012,SP:a99fddee87b684b2783ef3a21f8c15c19631953b,framework USED-FOR self - driving policy. loss terms PART-OF imitation loss. pose trajectories USED-FOR down - stream controller. parsed representation of the scene COMPARE raw images. raw images COMPARE parsed representation of the scene. pose trajectories USED-FOR policy. parsed representation of the scene USED-FOR policy. perturbations FEATURE-OF generalizability. perturbations PART-OF simulated data. simulated data USED-FOR method. ablations EVALUATE-FOR loss terms. ablations EVALUATE-FOR framework. ,"This paper proposes a framework for learning a self-driving policy. The policy is trained using a parsed representation of the scene instead of raw images, and the pose trajectories are used to train a down-stream controller. The proposed method is trained on simulated data with perturbations to improve generalizability. The authors evaluate the proposed framework on ablations to show the effectiveness of the proposed loss terms in the imitation loss."
2013,SP:a99fddee87b684b2783ef3a21f8c15c19631953b,"route CONJUNCTION roadmap. roadmap CONJUNCTION route. raw image COMPARE handcrafted features. handcrafted features COMPARE raw image. roadmap HYPONYM-OF handcrafted features. route HYPONYM-OF handcrafted features. regularizing loss terms USED-FOR model. Method are vehicle ’s trajectory planner, and recurrent neural network. Generic is method. ","This paper proposes to train a vehicle’s trajectory planner using a combination of the raw image and handcrafted features (e.g., route and roadmap). The proposed method is based on the idea that the model can be trained by adding regularizing loss terms to the model. The model is trained using a recurrent neural network."
2014,SP:f5be102f16ed9ac70a2e9e2580111226fb0d8b71,"proxy model USED-FOR method. Metric are training time, and test accuracy. ",This paper proposes a method that uses a proxy model to estimate the difference between the training time and the test accuracy. The method is based on the idea that the proxy model can be used as a proxy for the true test accuracy of the training data. The paper is well-written and easy to follow.
2015,SP:f5be102f16ed9ac70a2e9e2580111226fb0d8b71,"Method are deep neural networks, and light weighted proxy models. Generic are models, and model. Material is CIFAR10 / SVHN / Amazon Review Polarity. ","This paper studies the problem of training deep neural networks in the presence of light weighted proxy models. The authors argue that these models are not robust to changes in the number of layers in the model. The paper proposes to use light weightedproxy models, where the weights of the model are weighted according to the similarity of the input image to the target image. The experiments are conducted on CIFAR10/SVHN/Amazon Review Polarity."
2016,SP:4332dfe46b715595e9f1dd3f6a79b82a646b4c23,sequential Monte Carlo Planning algorithm USED-FOR planning. inference problem USED-FOR planning. SMC USED-FOR inference problem. Bayesian filtering CONJUNCTION smoothing. smoothing CONJUNCTION Bayesian filtering. problem CONJUNCTION Bayesian filtering. Bayesian filtering CONJUNCTION problem. algorithm USED-FOR complex continuous tasks. ,"This paper proposes a sequential Monte Carlo Planning algorithm for planning. The main idea is to use SMC as the inference problem for planning, and then combine the problem with Bayesian filtering and smoothing. The proposed algorithm is able to solve complex continuous tasks."
2017,SP:4332dfe46b715595e9f1dd3f6a79b82a646b4c23,SMC USED-FOR planning problems. trend USED-FOR inference problem. inference problem USED-FOR planning problem. Method is Sequential Monte Carlo ( SMC ). Generic is approach. ,"This paper proposes Sequential Monte Carlo (SMC), which is a generalization of the well-known approach to planning problems. The main idea of SMC is to use a trend as an inference problem to solve the planning problem. The authors show that SMC can be used to solve planning problems in a more efficient way."
2018,SP:d3e4e2c267fd9ae536ab1816d5c1ba8e8fec19be,accuracy EVALUATE-FOR models. robustness EVALUATE-FOR model. clean accuracy EVALUATE-FOR models. clean accuracy CONJUNCTION robust accuracy. robust accuracy CONJUNCTION clean accuracy. robust accuracy EVALUATE-FOR models. non - perturbed examples USED-FOR models. adversarially perturbed examples USED-FOR models. robustness EVALUATE-FOR model. distribution FEATURE-OF CIFAR ( saturation ) data. OtherScientificTerm is marginal distribution. Task is adversarial attack. ,"This paper studies the relationship between the accuracy and robustness of models trained on non-perturbed examples and on adversarially perturbed examples. The authors compare the clean accuracy and the robust accuracy of different models on both non-turburbed and perturbed versions of CIFAR (saturation) data, and show that the marginal distribution of the data is highly correlated with the robustness to adversarial attack. They also provide a theoretical analysis of the distribution of CifAR (surplus) data."
2019,SP:d3e4e2c267fd9ae536ab1816d5c1ba8e8fec19be,"adversarial input presentation FEATURE-OF robustness. images USED-FOR multiclass classification. l - inf bounded perturbations USED-FOR Adversarial inputs. adversarial robustness EVALUATE-FOR shape of the input distribution. OtherScientificTerm are semantic - lossless'shifts, perturbable volume, and inter - class distance. ","This paper studies the problem of robustness to adversarial input presentation in the context of multiclass classification on images. Adversarial inputs with l-inf bounded perturbations are considered, and the authors show that'semantic-lossless' shifts in the perturbable volume of the input distribution can lead to a significant drop in adversarial robustness. The authors also show that the inter-class distance between the input and the output of the classifier can be used to measure the shape of an input distribution."
2020,SP:a49fd0479a977c8fb45199210f9ff7dd2c0dabaf,Batch - norm CONJUNCTION Group - norm. Group - norm CONJUNCTION Batch - norm. Equi - Norm HYPONYM-OF normalization technique. approach COMPARE Batch - norm. Batch - norm COMPARE approach. approach COMPARE group norm. group norm COMPARE approach. Batch - norm CONJUNCTION group norm. group norm CONJUNCTION Batch - norm. datasets EVALUATE-FOR Batch - norm. datasets EVALUATE-FOR group norm. Imagenet HYPONYM-OF datasets. datasets EVALUATE-FOR approach. dataset EVALUATE-FOR method. Method is EquiNorm method. OtherScientificTerm is positive and negative kernel weights. ,"This paper proposes a new normalization technique called Equi-Norm, which is a combination of Batch-norm and Group-norm. The key idea of the EquiNorm method is to replace the positive and negative kernel weights. The proposed approach is evaluated on two datasets: Imagenet and CIFAR-10, where the proposed approach shows better performance than Batch - norm and group norm. The method is also evaluated on a new dataset."
2021,SP:a49fd0479a977c8fb45199210f9ff7dd2c0dabaf,"layer - wise transform USED-FOR batch normalization. EquiNorm HYPONYM-OF layer - wise transform. batch normalization COMPARE procedure. procedure COMPARE batch normalization. scaling factor CONJUNCTION shift. shift CONJUNCTION scaling factor. mini batch USED-FOR shift. method COMPARE BatchNorm. BatchNorm COMPARE method. method COMPARE GroupNorm. GroupNorm COMPARE method. BatchNorm CONJUNCTION GroupNorm. GroupNorm CONJUNCTION BatchNorm. computer vision datasets EVALUATE-FOR BatchNorm. computer vision datasets EVALUATE-FOR GroupNorm. computer vision datasets EVALUATE-FOR method. test accuracy EVALUATE-FOR BatchNorm. method COMPARE BatchNorm. BatchNorm COMPARE method. test accuracy EVALUATE-FOR method. Method is linear transform. OtherScientificTerm are layers, layer weights, positive and negative weights, spread, and overfit. Task are optimization, and data augmentation. ","This paper proposes a new layer-wise transform called EquiNorm, which is a variant of batch normalization that is more efficient than the standard procedure. Instead of using a linear transform, the authors propose to use a scaling factor and a shift based on a mini batch. The idea is that the layers should be close to each other in terms of the number of layer weights, and the shift should be proportional to the difference between the positive and negative weights. The authors show that the proposed method outperforms BatchNorm and GroupNorm on a variety of computer vision datasets and achieves better test accuracy compared to Batchnorm. The paper also provides some theoretical analysis of the optimization and shows that the spread between the training and test batches can lead to overfit, which can be alleviated by data augmentation."
2022,SP:8188f15c8521099305aa8664e05f102ee6cea402,label noise CONJUNCTION feature noise. feature noise CONJUNCTION label noise. method USED-FOR instance - dependent label noise. robust CONJUNCTION feature noise. feature noise CONJUNCTION robust. robust CONJUNCTION label noise. label noise CONJUNCTION robust. Task is training procedure. ,"This paper proposes a method to deal with instance-dependent label noise and feature noise. The main contribution of this paper is to propose a training procedure that is robust to label noise, robust to both label noise as well as feature noise, which is an important problem in practice. The paper is well-written and easy to follow."
2023,SP:8188f15c8521099305aa8664e05f102ee6cea402,"method USED-FOR incorrectly labeled / noisy examples. correct labeling CONJUNCTION incorrect labeling. incorrect labeling CONJUNCTION correct labeling. threshold USED-FOR incorrectly labeled loss distribution. Method are ODD, and neural networks. Material is mislabeled data. Task is generalization. Generic is model. OtherScientificTerm are learning rate, loss distributions, and loss distribution. ","This paper proposes a method to detect incorrectly labeled/noisy examples. ODD is an important problem for neural networks, as mislabeled data can lead to poor generalization. This paper proposes to train a model to distinguish between correct labeling and incorrect labeling. The key idea is to set a threshold for the incorrectly labeled loss distribution, which is then used to set the learning rate. The authors show that this threshold can be used to determine which loss distributions are more likely to be incorrectly labeled."
2024,SP:fbf023a772013e6eca62f92982aecf857c16a428,"pretrained representations USED-FOR prediction. pretrained representations USED-FOR downstream task. prediction USED-FOR downstream task. downstream predictor USED-FOR mutual information. probe USED-FOR pretrained representations. linear classifier HYPONYM-OF probe. pretrained representations HYPONYM-OF representations. word embeddings HYPONYM-OF representations. conditional distribution COMPARE conditional distribution. conditional distribution COMPARE conditional distribution. h_1 CONJUNCTION h_0. h_0 CONJUNCTION h_1. posterior distributions USED-FOR downstream task. h_0 FEATURE-OF posterior distributions. h_1 FEATURE-OF posterior distributions. posterior distribution FEATURE-OF latent variables. argmax attention USED-FOR predictor. Method are multitask learning, pretrained cloze language model, cloze language model, and latent - variable model. Generic is they. Material are pretraining data, and infinite training data. Metric is log - loss. ","This paper studies multitask learning, where pretrained representations are used to perform prediction for a downstream task. In particular, the paper considers a pretrained cloze language model, where a probe (i.e., a linear classifier) is trained to predict the output of two different representations: (i) word embeddings, and (ii) a conditional distribution over the input word embedding. The paper shows that the downstream predictor can capture the mutual information between the two two representations, and that they can be trained jointly. The main contribution of the paper is to show that the posterior distributions of the latent variables (h_1 and h_0) for the downstream task are similar to those of the pretraining data. The predictor is trained with argmax attention, and the log-loss is shown to be linear with respect to the number of latent variables. The authors also show that for infinite training data, the latent-variable model is able to generalize to unseen data."
2025,SP:fbf023a772013e6eca62f92982aecf857c16a428,"pre - trained language models USED-FOR downstream tasks. pre - trained language models CONJUNCTION head and prompt tuning. head and prompt tuning CONJUNCTION pre - trained language models. head and prompt tuning USED-FOR downstream tasks. HMM USED-FOR generative model of language. head or prompt tuning USED-FOR LM. LM USED-FOR downstream label. HMM CONJUNCTION memory augmented HMM. memory augmented HMM CONJUNCTION HMM. memory augmented HMM USED-FOR data distribution. HMM USED-FOR data distribution. vocabulary size FEATURE-OF data generating distribution. memory augmented HMMs USED-FOR downstream tasks. independence assumption USED-FOR prompt tuning. Prompt tuning COMPARE head tuning. head tuning COMPARE Prompt tuning. hidden state size COMPARE token vocabulary size. token vocabulary size COMPARE hidden state size. hidden state sizes EVALUATE-FOR head tuning. memory USED-FOR downstream task information. OtherScientificTerm are distribution of language, downstream labels, token emission probability matrix, HMM hidden states, hidden state dimensionality, non - degeneracy condition, and memory cells. Task is downstream task. Method is memory - augmented HMMs. ","This paper studies the effect of pre-trained language models, head and prompt tuning on the performance of downstream tasks. Specifically, the authors consider a generative model of language trained with HMM and a memory augmented HMM, where the distribution of language is modelled as a function of the number of tokens in the token emission probability matrix, and the downstream labels are learned using either head or prompt tuning. The authors show that HMM can be used to approximate the data distribution of the data generating distribution with respect to the vocabulary size, and that memory-augmented HMMs can generalize well to downstream tasks, as long as the HMM hidden states are independent of the hidden state dimensionality. Prompt tuning can be viewed as an extension of independence assumption in the context of prompt tuning, where a LM is trained to predict the downstream label given the current state of the downstream task, and a non-degeneracy condition is imposed on the size of the memory cells to ensure that the memory is not used to store downstream task information. The hidden state sizes of head tuning are compared to the token vocabulary size."
2026,SP:fbf023a772013e6eca62f92982aecf857c16a428,head and prompt tuning USED-FOR pre - trained LM. Hidden Markov Model ( HMM ) CONJUNCTION memory augmented HMM. memory augmented HMM CONJUNCTION Hidden Markov Model ( HMM ). Hidden Markov Model ( HMM ) USED-FOR analysis framework. memory augmented HMM USED-FOR analysis framework. HMM USED-FOR marginal P(X_i ). head tuning CONJUNCTION prompt tuning. prompt tuning CONJUNCTION head tuning. framework USED-FOR binary classification task. posterior distribution USED-FOR HMM. posterior distribution USED-FOR prompt tuning. prompt tuning USED-FOR binary classification task. head tuning USED-FOR binary classification task. posterior distribution USED-FOR binary classification task. HMM CONJUNCTION memory augmented HMM. memory augmented HMM CONJUNCTION HMM. Task is Analyzing complicated neural networks. Method is masked LM ( BERT ). ,This paper proposes an analysis framework based on Hidden Markov Model (HMM) and memory augmented HMM. HMM is used to estimate marginal P(X_i) for a binary classification task with head tuning and prompt tuning for pre-trained LM. Analyzing complicated neural networks is an important problem and this paper proposes a masked LM (BERT) to address this problem. The proposed HMM and the memory augmented version of HMM are evaluated on several datasets. The posterior distribution of the HMM can be used for head tuning or prompt tuning.
2027,SP:fbf023a772013e6eca62f92982aecf857c16a428,analysis framework USED-FOR pre - training and downstream tasks. latent variables PART-OF generative model. latent variables FEATURE-OF posterior distribution. generative model USED-FOR posterior distribution. HMM USED-FOR generative model. non - degeneracy assumptions FEATURE-OF classification head. classification head USED-FOR downstream recovery. prompt tuning USED-FOR non - degeneracy conditions. memory - augmented HMM USED-FOR generative model. Generic is framework. ,This paper proposes an analysis framework for pre-training and downstream tasks. The framework is based on the observation that the posterior distribution of a generative model over latent variables can be represented as a HMM. The authors propose to use a memory-augmented HMM in order to train the generative for downstream recovery with non-degeneracy assumptions on the classification head. They also propose prompt tuning to satisfy the non-degeneracy conditions.
2028,SP:217c4205a99f9b37283137826c4be6ab9bfb4e8e,total variation CONJUNCTION Wasserstein distance. Wasserstein distance CONJUNCTION total variation. transferability FEATURE-OF features. transferability COMPARE common distribution discrepancy measures. common distribution discrepancy measures COMPARE transferability. Wasserstein distance HYPONYM-OF common distribution discrepancy measures. total variation HYPONYM-OF common distribution discrepancy measures. transferability FEATURE-OF measure. transferability FEATURE-OF features. domain generalization algorithms USED-FOR features. transferability USED-FOR algorithm. domain generalization datasets EVALUATE-FOR it. satellite dataset HYPONYM-OF domain generalization datasets. Metric is transferability measure. ,"This paper proposes a new transferability measure, which measures the transferability of the features learned by domain generalization algorithms. The proposed measure is based on the observation that transferability is more important than other common distribution discrepancy measures such as total variation and Wasserstein distance. The authors evaluate the proposed algorithm on two different domains and show that it performs well on both standard and novel domains and on a satellite dataset."
2029,SP:217c4205a99f9b37283137826c4be6ab9bfb4e8e,"transferability CONJUNCTION discrepancy measure. discrepancy measure CONJUNCTION transferability. transferability measure USED-FOR joint distributions. Generic are quantity, and proposal. ","This paper proposes a new quantity called ""transferability"" to measure the discrepancy between two distributions. The proposal is motivated by the fact that the transferability measure can be used to compare two joint distributions. "
2030,SP:217c4205a99f9b37283137826c4be6ab9bfb4e8e,transferability FEATURE-OF learning algorithms. adversarial optimisation problem USED-FOR algorithm. OtherScientificTerm is upper bound. ,"This paper studies the transferability of learning algorithms. The authors propose an algorithm based on an adversarial optimisation problem, and provide an upper bound on the number of iterations needed to reach convergence."
2031,SP:217c4205a99f9b37283137826c4be6ab9bfb4e8e,"formulation USED-FOR transferability. Method are near - optimal source classifier, and near - optimal classifiers. OtherScientificTerm is genralization bounds. Generic is algorithm. ","This paper proposes a new formulation for transferability between source and target classes. The main idea is to train a near-optimal source classifier, and then train the target classifier on top of that. The authors provide genralization bounds for the proposed algorithm, and show that the performance of the target classes can be improved when the source classifiers are trained on the target. Finally, the authors provide a theoretical analysis of the transferability of the proposed method."
2032,SP:46f5874c8cbdb0832e92adcea85ca8a1b9ddc28a,Markovian reward functions USED-FOR tasks. partial ordering over policies CONJUNCTION partial ordering over trajectories. partial ordering over trajectories CONJUNCTION partial ordering over policies. policies CONJUNCTION partial ordering over policies. partial ordering over policies CONJUNCTION policies. OtherScientificTerm is Markovian reward. Method is linear programs. Generic is reward. ,"This paper studies Markovian reward functions for tasks that are linear programs. The authors show that Markovians reward functions can be used to solve tasks that require partial ordering over policies (i.e., partial ordering of policies, partial ordering on trajectories, and so on). The authors also show that the reward can be expressed as a function of the number of states and actions. "
2033,SP:46f5874c8cbdb0832e92adcea85ca8a1b9ddc28a,reward function CONJUNCTION discount rate. discount rate CONJUNCTION reward function. approach USED-FOR learning. OtherScientificTerm is reward functions. Generic is algorithm. ,This paper proposes an approach for learning when the reward function and the discount rate are different. The main idea is to learn reward functions that are independent of each other. The authors provide a theoretical analysis of the algorithm and provide some empirical results.
2034,SP:46f5874c8cbdb0832e92adcea85ca8a1b9ddc28a,definitions USED-FOR notion of task. partial ordering over policies ( PO ) CONJUNCTION partial ordering over trajectories ( TO ). partial ordering over trajectories ( TO ) CONJUNCTION partial ordering over policies ( PO ). partial ordering over policies ( PO ) HYPONYM-OF definitions. task CONJUNCTION controlled Markov process. controlled Markov process CONJUNCTION task. polynomial algorithm USED-FOR reward function. linear programming USED-FOR polynomial algorithm. random problem instances EVALUATE-FOR theory. Task is reinforcement learning context. ,"This paper proposes two definitions for the notion of task: partial ordering over policies (PO) and partial ordering of trajectories (TO). These definitions are motivated by the reinforcement learning context, where a task and a controlled Markov process are considered. The authors propose a polynomial algorithm for the reward function based on linear programming. The theory is evaluated on random problem instances."
2035,SP:46f5874c8cbdb0832e92adcea85ca8a1b9ddc28a,"Markov reward USED-FOR task. Markov reward USED-FOR SOAP task specification. Method is Controlled Markov Process ( CMP ). OtherScientificTerm are task intent, partial ordering over policies ( PO ), and reward. Generic is tasks. Task is Markov reward optimization problem. ","This paper proposes a Controlled Markov Process (CMP) that learns a Markov reward for each task based on the task intent. The main idea is to use partial ordering over policies (PO), where each policy is trained to maximize the reward for a subset of tasks. The authors show that this Markov policy optimization problem is NP-hard, and propose to solve the SOAP task specification with Markov rewards."
2036,SP:9d8b57d60a0e59f9d9a90605094e8ef895f1c7de,Bayesian RL USED-FOR generalization. RL USED-FOR generalization. ensemble - based algorithm USED-FOR problem. LEEP HYPONYM-OF ensemble - based algorithm. epistemic POMDP USED-FOR generalization problem. Procgen benchmark EVALUATE-FOR LEEP. LEEP COMPARE PPO. PPO COMPARE LEEP. Heist CONJUNCTION BigFish. BigFish CONJUNCTION Heist. Maze CONJUNCTION Heist. Heist CONJUNCTION Maze. BigFish CONJUNCTION Dodgeball. Dodgeball CONJUNCTION BigFish. PPO USED-FOR Maze. LEEP USED-FOR Maze. PPO USED-FOR Heist. generalization EVALUATE-FOR LEEP. ensembles USED-FOR LEEP. generalization EVALUATE-FOR LEEP. ablation study EVALUATE-FOR LEEP. ` max_i π_i ` link function USED-FOR inductive bias. ,"This paper studies the problem of generalization in Bayesian RL. The authors formulate the generalization problem as an epistemic POMDP, and propose LEEP, an ensemble-based algorithm for solving this problem. LEEP is evaluated on the Procgen benchmark, and compared to PPO on Maze, Heist, BigFish, and Dodgeball. The generalization of LEEP with ensembles is shown to be better than PPO, and an ablation study is conducted to evaluate the inductive bias induced by the `max_i π_i` link function."
2037,SP:9d8b57d60a0e59f9d9a90605094e8ef895f1c7de,state / action space FEATURE-OF dynamics. dynamics USED-FOR reinforcement learning. POMDP USED-FOR problem. policies USED-FOR sampled environments. OtherScientificTerm is hidden information. Method is LEEP. Generic is policy. ,"This paper studies the problem of learning dynamics in the state/action space for reinforcement learning. The problem is formulated as a POMDP, where the hidden information of the state and action space is assumed to be the same. The authors propose LEEP, which learns a policy that maximizes the probability that the state is the same as the action. The proposed policies are then applied to sampled environments."
2038,SP:9d8b57d60a0e59f9d9a90605094e8ef895f1c7de,"contextual MDPs FEATURE-OF generalization. RL agent USED-FOR context test MDP. learning CONJUNCTION generalizing. generalizing CONJUNCTION learning. methods USED-FOR epistemic uncertainty. sampling - based method USED-FOR POMDP. POMDP USED-FOR generalization. methods COMPARE baselines. baselines COMPARE methods. OtherScientificTerm are MDP, and MDPs. ","This paper studies the problem of generalization in contextual MDPs. Specifically, the authors propose to train an RL agent to solve a context test MDP, where the MDP is constructed from a set of examples. The authors propose two methods to address epistemic uncertainty in the context of learning and generalizing. The first method, POMDP, is a sampling-based method, which aims to improve generalization performance in the setting where the examples come from different contexts. The second method, MDP-POMD, is an extension of the work of Chen et al. (2018). The authors compare their methods with several baselines and show that their methods outperform the baselines."
2039,SP:9d8b57d60a0e59f9d9a90605094e8ef895f1c7de,"generalization PART-OF reinforcement learning. epistemic uncertainty FEATURE-OF MDP. epistemic uncertainty USED-FOR partial observability. optimal policy PART-OF epistemic POMDP. optimal policy USED-FOR epistemic POMDP. generalization EVALUATE-FOR policy. memoryless policies USED-FOR empirical epistemic POMDP. regularization term PART-OF algorithm. max CONJUNCTION PPO. PPO CONJUNCTION max. it COMPARE PPO. PPO COMPARE it. it COMPARE Distral. Distral COMPARE it. PPO CONJUNCTION Distral. Distral CONJUNCTION PPO. procgen benchmark EVALUATE-FOR it. algorithm COMPARE it. it COMPARE algorithm. procgen benchmark EVALUATE-FOR algorithm. algorithm USED-FOR overfitting. agreement penalty term CONJUNCTION max link function. max link function CONJUNCTION agreement penalty term. agreement penalty term USED-FOR LEEP. max link function USED-FOR LEEP. OtherScientificTerm are posterior distribution of MDPs, MDP posterior, and disagreement penalty. Method are POMDP, and bootstrap sampling. Generic is latter. ","This paper studies the problem of generalization in reinforcement learning. The authors consider the case where the posterior distribution of MDPs is not known. The epistemic uncertainty of the MDP can lead to partial observability, and the authors propose an algorithm, LEEP, which adds a regularization term that penalizes the deviation of the policy from the optimal policy in the epistemic POMDP. This is motivated by the fact that the empirical epistemic MOMDP can be represented by memoryless policies, and that the generalization of such a policy can be measured by comparing the performance of LEEP with that of the optimal MDP posterior. The proposed algorithm is evaluated on the procgen benchmark, where it is compared to PPO and Distral, and it is shown to outperform PPO by a large margin. The algorithm is also shown to prevent overfitting by adding an agreement penalty term and a max link function. The latter is based on bootstrap sampling, where the disagreement penalty is added to the max."
2040,SP:10de45510320b7ddb7ffb18b33e67f7cad609418,framework USED-FOR high - order derivatives of value functions. off - policy evaluation USED-FOR framework. off - policy evaluation USED-FOR high - order derivatives of value functions. off - policy evaluation methods USED-FOR value estimator. derivative USED-FOR estimate. technique USED-FOR meta reinforcement learning. ,This paper proposes a framework to estimate high-order derivatives of value functions using off-policy evaluation. The main idea is to use off-party evaluation methods to train a value estimator that can be used to estimate the derivative of the estimate. The technique is applied to meta reinforcement learning.
2041,SP:10de45510320b7ddb7ffb18b33e67f7cad609418,"gradient - based adaptations USED-FOR meta - RL. estimation of the Hessians of value functions USED-FOR gradient - based adaptations. estimation of the Hessians of value functions USED-FOR meta - RL. approaches USED-FOR unbiased / biased estimations of Hessian. off - policy evaluation USED-FOR estimating higher - order derivatives of value functions. second - order estimate PART-OF meta - RL algorithms. OtherScientificTerm are Hessians of value functions, and higher - order derivatives of value functions. Generic is estimate. ","This paper studies the estimation of the Hessians of value functions for gradient-based adaptations in meta-RL. The authors propose two approaches to obtain unbiased/biased estimations of Hessian. The first approach is based on off-policy evaluation, which is used for estimating higher-order derivatives of values. The second approach is to incorporate a second-order estimate of the value of a function, which can be used as a proxy for estimating the Hessian of the function. This estimate is then used to train the second policy. The experimental results show that the proposed methods are able to improve the performance of existing meta RL algorithms."
2042,SP:10de45510320b7ddb7ffb18b33e67f7cad609418,gradient - based meta learning USED-FOR reinforcement learning. off - policy evaluation USED-FOR gradient - based meta learning. policy gradient methods USED-FOR reinforcement learning. Taylor policy optimization USED-FOR variance reduction method. variance reduction method USED-FOR doubly robust policy gradient framework. Method is likelihood ratio derivative estimator. OtherScientificTerm is higher order derivatives. ,This paper studies the problem of gradient-based meta learning with off-policy evaluation in reinforcement learning with policy gradient methods. The authors propose a doubly robust policy gradient framework that uses a variance reduction method based on Taylor policy optimization. They also propose a likelihood ratio derivative estimator that can be used to estimate higher order derivatives.
2043,SP:10de45510320b7ddb7ffb18b33e67f7cad609418,Taylor expansions USED-FOR value functions. they USED-FOR Off - Policy Evaluation. OPE CONJUNCTION meta - learning. meta - learning CONJUNCTION OPE. ideas USED-FOR meta - learning. Generic is baselines. ,This paper proposes to use Taylor expansions to learn value functions and apply they to Off-Policy Evaluation. The authors also discuss how these ideas can be applied to meta-learning. The experimental results show improvements over baselines.
2044,SP:54a60315416c6e304f59741490c335fb1e2ce95d,"MCM ~ algorithm USED-FOR bidirectional compression. OtherScientificTerm are convergence guarantees, vanilla setting, and global model. ","This paper proposes a new MCM ~ algorithm for bidirectional compression. The authors provide convergence guarantees for the vanilla setting, as well as for the global model."
2045,SP:54a60315416c6e304f59741490c335fb1e2ce95d,MCM USED-FOR bidirectional compression. method USED-FOR bidirectional compression. method USED-FOR MCM. federated learning USED-FOR bidirectional compression. method USED-FOR linear convergence. compression rate EVALUATE-FOR method. ,This paper proposes a method to improve MCM for bidirectional compression in federated learning. The proposed method achieves linear convergence and achieves a better compression rate.
2046,SP:54a60315416c6e304f59741490c335fb1e2ce95d,"bidirectional compression algorithm USED-FOR parameter - server - based distributed training. variants COMPARE uplink - compression - only compression algorithm. uplink - compression - only compression algorithm COMPARE variants. MCM CONJUNCTION Rand - MCM. Rand - MCM CONJUNCTION MCM. Rand - MCM COMPARE uplink - compression - only compression algorithm. uplink - compression - only compression algorithm COMPARE Rand - MCM. MCM COMPARE uplink - compression - only compression algorithm. uplink - compression - only compression algorithm COMPARE MCM. MCM HYPONYM-OF variants. Rand - MCM HYPONYM-OF variants. Rand - MCM USED-FOR ( simulated ) random smoothing. model compression USED-FOR ( simulated ) random smoothing. OtherScientificTerm are downlink - compression - only, local models, and global model. ","This paper proposes a directional compression algorithm for parameter-server-based distributed training. The proposed variants, MCM and Rand-MCM, are shown to outperform the standard uplink-compression-only compression algorithm. The main contribution of the paper is the introduction of the idea of downlink-compress-only, where local models are used to compress the global model. In addition, the authors demonstrate the effectiveness of the proposed variants on (simulated) random smoothing with model compression."
2047,SP:54a60315416c6e304f59741490c335fb1e2ce95d,algorithm USED-FOR bidirectional compression. convergence rate EVALUATE-FOR algorithm. Method is MCM. ,"This paper proposes an algorithm for bidirectional compression. The main contribution of the paper is the convergence rate of the proposed algorithm. The algorithm is based on MCM, which is a generalization of MCM. The paper is well-written and easy to follow."
2048,SP:6b19f16c429ffa7f613b57d082bde3794a8e29e0,conditions USED-FOR counterfactual invariant predictions. causal graphs USED-FOR these. natural language datasets EVALUATE-FOR conditions. worst - domain error EVALUATE-FOR conditions. Task is worst - case domain generalization. ,"This paper proposes two conditions for counterfactual invariant predictions, which are based on causal graphs. The authors evaluate these conditions on two natural language datasets, and show that these conditions can reduce the worst-domain error by a factor of at least 1.5. This is an interesting contribution to the field of worst-case domain generalization."
2049,SP:6b19f16c429ffa7f613b57d082bde3794a8e29e0,"counterfactual invariance FEATURE-OF machine learning models. causal ” features COMPARE spurious correlations. spurious correlations COMPARE causal ” features. causal ” features USED-FOR counterfactually invariant predictors. one CONJUNCTION another. another CONJUNCTION one. regularization schemes USED-FOR counterfactually invariant predictors. causal model USED-FOR data generation mechanism. one HYPONYM-OF regularization schemes. another HYPONYM-OF regularization schemes. regularization scheme USED-FOR causal model. causal mechanisms USED-FOR strategies. review helpfulness classification CONJUNCTION natural language inference. natural language inference CONJUNCTION review helpfulness classification. natural language inference HYPONYM-OF text classification tasks. review helpfulness classification HYPONYM-OF text classification tasks. Generic are models, and it. Method is anticausal model. Material is counterfactual examples. ","This paper studies the problem of counterfactual invariance in machine learning models. The authors propose two regularization schemes, one that encourages counterfactually invariant predictors to have “causal” features instead of spurious correlations, and another that encourages the causal model to be invariant to the data generation mechanism. Both strategies are based on causal mechanisms, and the authors show empirically that the proposed models outperform the anticausal model in a variety of text classification tasks, including review helpfulness classification and natural language inference. In addition, the authors also show that it is possible to train a causal model that is invariant even when there are no counterfactul examples."
2050,SP:6b19f16c429ffa7f613b57d082bde3794a8e29e0,"Metric is robustness. OtherScientificTerm are spurious correlations, stress testing, counterfactual invariance, causal structures, and counterfactual - invariance. Generic are model, and schemes. Task is Stress testing. Method is regularization schemes. ","This paper studies the problem of robustness to spurious correlations in the context of stress testing. In particular, the authors consider the case where the model is trained on a set of data points, and the causal structures of the data points are not known. The authors propose two regularization schemes to improve the robustness of the model. The main idea is that the counterfactual invariance of the learned model should not be affected by the underlying causal structures, and that the proposed schemes can be used to ensure that the model does not change too much in the presence of spurious correlations. Experiments are conducted to demonstrate the effectiveness of the proposed regularization scheme. The paper also provides a theoretical analysis of the effect of these schemes."
2051,SP:6b19f16c429ffa7f613b57d082bde3794a8e29e0,"models USED-FOR spurious correlations. counterfactual invariance HYPONYM-OF invariance. Material is language datasets. Method is DAGs. OtherScientificTerm are invariant feature, predictor, counterfactuals, counterfactual invariant predictor, and min - max optimality. ","This paper studies the problem of learning models that are robust to spurious correlations in language datasets. In particular, the authors focus on the case where the invariance (i.e., counterfactual invariance) is not known. The authors propose to use DAGs to model this invariance and show that when the invariant feature is known, the predictor can be trained to be robust to such spurious correlations. The main contribution of the paper is that the authors propose a way to train a counterfactually invariant predictor that is robust to the existence of spurious correlations between the original predictor and the predicted one. This is achieved by minimizing the number of training examples and training the predictor to be invariant to the counterfactuls. This leads to min-max optimality."
2052,SP:6ff26839a14991597555ead4c82eb6ddb61e4dbc,"limited data USED-FOR GANs. method COMPARE state - of - the - art methods. state - of - the - art methods COMPARE method. limited data USED-FOR state - of - the - art methods. datasets with limited data EVALUATE-FOR method. StyleGAN-2 backbone USED-FOR method. OtherScientificTerm are GAN discriminators, discriminator, GAN optima, and overfitting of the discriminator. Method is overfitting heuristic. ",This paper studies the problem of training GANs on limited data. The authors propose to train GAN discriminators on datasets with limited data and show that the proposed method outperforms the state-of-the-art methods on these datasets. The method is based on the StyleGAN-2 backbone and the authors propose an overfitting heuristic to train the discriminator. The main idea is to learn GAN optima that are robust to the overfitting of the discrimator.
2053,SP:6ff26839a14991597555ead4c82eb6ddb61e4dbc,generator USED-FOR real data distribution. fake images USED-FOR real data distribution. generator USED-FOR APA. overfittingness FEATURE-OF discriminator. overfittingness FEATURE-OF Adaptiveness. APA COMPARE GAN. GAN COMPARE APA. limited - data regime FEATURE-OF systnesis quality. limited - data regime EVALUATE-FOR APA. systnesis quality EVALUATE-FOR APA. AFHQ - Cat CONJUNCTION CUB. CUB CONJUNCTION AFHQ - Cat. FFHQ CONJUNCTION AFHQ - Cat. AFHQ - Cat CONJUNCTION FFHQ. APA COMPARE GAN. GAN COMPARE APA. Task is discriminator overfitting problem. Method is adaptive pseudo augmentation ( APA ). ,"This paper studies the discriminator overfitting problem and proposes adaptive pseudo augmentation (APA). APA uses a generator to generate a real data distribution from fake images. Adaptiveness to overfittingness of discriminator is studied. The systnesis quality of APA is evaluated in the limited-data regime and compared with GAN. Results are shown on FFHQ, AFHQ-Cat, and CUB."
2054,SP:6ff26839a14991597555ead4c82eb6ddb61e4dbc,"data augmentation strategy USED-FOR GANs. adaptive replacement probability USED-FOR discriminator training step. data augmentation scheme USED-FOR low data regime. method CONJUNCTION data augmentation scheme. data augmentation scheme CONJUNCTION method. ADA HYPONYM-OF data augmentation scheme. Method are generator, and APA. OtherScientificTerm is discriminator overfitting. ","This paper proposes a new data augmentation strategy for GANs. The main idea is to add an adaptive replacement probability to the discriminator training step. The generator is trained to maximize the replacement probability. The authors propose a method, called APA, and a new, more efficient, and more effective data augmentation scheme, ADA, for the low data regime, to avoid discriminator overfitting."
2055,SP:6ff26839a14991597555ead4c82eb6ddb61e4dbc,"discriminator USED-FOR APA. Generic is method. Material is limited data. Method are adaptive pseudo augmentation ( APA ), and GAN discriminator. OtherScientificTerm are generated samples, and overfitting. ","This paper proposes adaptive pseudo augmentation (APA), a method for augmenting the training data with limited data. APA is based on training a GAN discriminator on the generated samples, and then using the discriminator to augment the training samples. The idea is to prevent overfitting."
2056,SP:0f3fcffb6dfbf344bd5ef73c3f6d3d84d2f13887,"method USED-FOR causal analysis of long streams of event data. multivariate point processes ( MPPs ) USED-FOR causal analysis of long streams of event data. expected difference of average outcome rates USED-FOR ATE. inverse probability of treatment weighting ( IPTW ) USED-FOR unbiased estimator. proximal graphical event models CONJUNCTION Hawkes processes. Hawkes processes CONJUNCTION proximal graphical event models. IPTW estimation method USED-FOR ATE. IPTW estimation method COMPARE baseline conditional intensity scores. baseline conditional intensity scores COMPARE IPTW estimation method. ATE COMPARE baseline conditional intensity scores. baseline conditional intensity scores COMPARE ATE. IPTW COMPARE conditional intensity scores. conditional intensity scores COMPARE IPTW. diabetes dataset EVALUATE-FOR IPTW. recall COMPARE conditional intensity scores. conditional intensity scores COMPARE recall. OtherScientificTerm are average treatment effect ( ATE ), covariates, assumptions, propensity score, and conditional intensity rate. Method is MPPs. ","This paper proposes a method for causal analysis of long streams of event data generated by multivariate point processes (MPPs). The main idea is to estimate the average treatment effect (ATE) using the expected difference of average outcome rates, which is an unbiased estimator based on inverse probability of treatment weighting (IPTW). The authors propose to use proximal graphical event models and Hawkes processes as the covariates. The authors show that the proposed IPTW estimation method can estimate ATE better than baseline conditional intensity scores. Under certain assumptions, the authors also show that IPTW can estimate the propensity score, which can be used to compute the conditional intensity rate. Experiments on the diabetes dataset demonstrate the effectiveness of IPTW compared to conditional intensity scales. In particular, the recall is shown to be better than conditional intensity values. "
2057,SP:0f3fcffb6dfbf344bd5ef73c3f6d3d84d2f13887,model USED-FOR average causal effect. rate parameter FEATURE-OF point process. point processes FEATURE-OF average causal effect. propensity score USED-FOR balancing weight. weights USED-FOR outcome model. point process model USED-FOR outcome model. Generic is formulation. Method is stabilization of inverse propensity scores. ,"This paper proposes a new formulation of the stabilization of inverse propensity scores. The model aims to estimate the average causal effect in point processes. The authors propose to use the rate parameter of the point process as a balancing weight, and use the propensity score as the balancing weight. These weights are used to train an outcome model that takes as input a point process model."
2058,SP:0f3fcffb6dfbf344bd5ef73c3f6d3d84d2f13887,"causal inference framework USED-FOR multivariate point process model. propensity scores USED-FOR unbiased estimation of ATE. synthetic experiments CONJUNCTION real - world datasets. real - world datasets CONJUNCTION synthetic experiments. method COMPARE baseline approaches. baseline approaches COMPARE method. real - world datasets EVALUATE-FOR method. synthetic experiments EVALUATE-FOR method. OtherScientificTerm are binary processes, and average treatment event ( ATE ). ",This paper proposes a causal inference framework for multivariate point process model. The main idea is to model binary processes as a function of the average treatment event (ATE) and use propensity scores for unbiased estimation of ATE. The proposed method is evaluated on both synthetic experiments and real-world datasets and compared to baseline approaches.
2059,SP:0f3fcffb6dfbf344bd5ef73c3f6d3d84d2f13887,"tools USED-FOR average treatment effect ( ATE ). quantity USED-FOR multivariate point processes. method USED-FOR ATE. synthetic and real world datasets EVALUATE-FOR method. OtherScientificTerm are Neyman – Rubin causality, and propensity score. ","This paper proposes two tools to estimate the average treatment effect (ATE) of a given treatment. The first method is based on the Neyman–Rubin causality, where the propensity score of the treatment is used as a measure of ATE. The second quantity is used to model multivariate point processes. The method is evaluated on both synthetic and real world datasets."
2060,SP:5db39fbba518e24a22b99c8256491295048ec417,adaptive residual connection CONJUNCTION feature aggregation. feature aggregation CONJUNCTION adaptive residual connection. message passing CONJUNCTION feature aggregation. feature aggregation CONJUNCTION message passing. feature aggregation USED-FOR abnormal node features. message passing USED-FOR abnormal node features. adaptive residual connection PART-OF Adaptive Message Passing ( AMP ). feature aggregation PART-OF Adaptive Message Passing ( AMP ). adaptive residual connection PART-OF message passing module. feature aggregation PART-OF message passing module. Adaptive Message Passing ( AMP ) HYPONYM-OF message passing module. AirGNN COMPARE baselines. baselines COMPARE AirGNN. abnormal node CONJUNCTION normal node classification. normal node classification CONJUNCTION abnormal node. AirGNN USED-FOR abnormal node. AMP USED-FOR message - passing layer. AirGNN USED-FOR normal node classification. message - passing layer PART-OF GNN. message - passing layer USED-FOR AirGNN. AMP USED-FOR AirGNN. Generic is module. OtherScientificTerm is local feature. ,"This paper proposes Adaptive Message Passing (AMP), a message passing module that combines adaptive residual connection and feature aggregation to extract abnormal node features from the message passing. The proposed module is called AirGNN. The main idea is to use AMP as a message-passing layer in the GNN, where the local feature is aggregated and the abnormal node is extracted from the original message. Experimental results show that the proposed airGNN outperforms the baselines in terms of both abnormal node and normal node classification performance."
2061,SP:5db39fbba518e24a22b99c8256491295048ec417,GNNs USED-FOR abnormal node features. resilience FEATURE-OF abnormal node features. resilience FEATURE-OF GNNs. features FEATURE-OF randomly selected nodes. random Gaussian noise USED-FOR features. residual connections USED-FOR models. OtherScientificTerm is abnormal features. ,This paper studies the resilience of GNNs to abnormal node features. The authors show that the features of randomly selected nodes are highly sensitive to random Gaussian noise. They also show that models with residual connections are more sensitive to abnormal features.
2062,SP:5db39fbba518e24a22b99c8256491295048ec417,"feature aggregation USED-FOR abnormal features. residual link USED-FOR feature smoothness. over - smoothing USED-FOR normal features. feature smoothness USED-FOR normal features. AMP method CONJUNCTION AirGNN model. AirGNN model CONJUNCTION AMP method. OtherScientificTerm are normal and abnormal features, and abnormal feature scenarios. Method is GNNs. ","This paper studies the problem of feature aggregation for abnormal features. The authors propose to use residual link as a measure of feature smoothness for normal features to avoid over-smoothing. The main contribution of this paper is to study the relationship between normal and abnormal features, and to show that the AMP method and the AirGNN model can be used to model both normal feature scenarios and abnormal feature scenarios. The paper is well-written and well-motivated. However, there are some issues with the presentation of GNNs."
2063,SP:5db39fbba518e24a22b99c8256491295048ec417,"GNNs USED-FOR AirGNN. aggregated embeddings USED-FOR AirGNN. OtherScientificTerm are node features, and random / adversarial noise. Task is classification. ","This paper proposes AirGNN, which is a generalization of GNNs that uses aggregated embeddings. The key idea is to use node features as input to the model, and then to use the aggregated features as output to train the model. The main contribution of the paper is that the classification is done in an unsupervised manner, which means that the model does not have to deal with random/adversarial noise."
2064,SP:66c5acd36a5fb74478d3f5ecaffc8479868dbe81,"expected regret FEATURE-OF sequential decision - making problems. evidence lower bound PART-OF variational inference. random game CONJUNCTION constrained bandit problem. constrained bandit problem CONJUNCTION random game. constrained bandit problem EVALUATE-FOR variational Thompson sampling algorithm. random game EVALUATE-FOR variational Thompson sampling algorithm. Method is Thompson sampling. OtherScientificTerm are upper bound, rate function, heavy tails, and exploration. ","This paper studies the expected regret in sequential decision-making problems. The authors propose to incorporate an evidence lower bound into variational inference, which is similar to Thompson sampling. The main difference is that the upper bound does not depend on the rate function, but instead depends on the number of steps required to reach the goal. The paper then presents a variational Thompson sampling algorithm that is evaluated on a random game and a constrained bandit problem. The experiments show that heavy tails can lead to poor exploration performance."
2065,SP:66c5acd36a5fb74478d3f5ecaffc8479868dbe81,"vanilla TS CONJUNCTION method. method CONJUNCTION vanilla TS. algorithm USED-FOR stochastically optimistic "" policy. subgaussian noise FEATURE-OF stochastic multi - armed bandit setting. VTS algorithm USED-FOR bilinear saddle point problems. sublinear regret FEATURE-OF bilinear saddle point problems. sublinear regret EVALUATE-FOR VTS algorithm. Method are Variational Thompson Sampling, and TS. OtherScientificTerm is computationally tractable upper bound. ","This paper studies Variational Thompson Sampling (VTS), a variant of TS with a computationally tractable upper bound. The authors propose an algorithm for learning a ""stochastically optimistic"" policy in the stochastic multi-armed bandit setting with subgaussian noise, and compare the performance of vanilla TS and the proposed method. They show that the proposed VTS algorithm achieves sublinear regret for bilinear saddle point problems."
2066,SP:66c5acd36a5fb74478d3f5ecaffc8479868dbe81,policies USED-FOR online learning. variational Thompson sampling ( VTS ) HYPONYM-OF policies. TS PART-OF stochastically optimistic policies. stochastically optimistic policies USED-FOR VTS. zero - sum two player games CONJUNCTION constrained bandits. constrained bandits CONJUNCTION zero - sum two player games. UCB CONJUNCTION K - learning. K - learning CONJUNCTION UCB. VTS COMPARE policies. policies COMPARE VTS. TS USED-FOR problems. VTS COMPARE TS. TS COMPARE VTS. K - learning HYPONYM-OF policies. zero - sum two player games HYPONYM-OF problems. UCB HYPONYM-OF policies. constrained bandits HYPONYM-OF problems. ,"This paper proposes variational Thompson sampling (VTS), a family of policies for online learning based on stochastically optimistic policies. VTS is a generalization of stochastic optimistic policies that include TS. The authors show that VTS outperforms existing policies (UCB, K-learning, etc) on a variety of problems, including zero-sum two player games, constrained bandits, etc. "
2067,SP:66c5acd36a5fb74478d3f5ecaffc8479868dbe81,"regret bounds USED-FOR heuristic. Thompson sampling USED-FOR heuristic. pure exploitation "" term CONJUNCTION exploration "" term. exploration "" term CONJUNCTION pure exploitation "" term. belief distribution USED-FOR policy. pure exploitation "" term PART-OF objective. exploration "" term PART-OF objective. OtherScientificTerm is belief state. Metric is probability of optimality. ","This paper provides regret bounds for a heuristic based on Thompson sampling. The objective consists of a ""pure exploitation"" term that encourages the agent to exploit the belief state, and an ""exploration"" term which encourages the policy to explore the belief distribution. The authors show that the probability of optimality is bounded by a factor of 2."
2068,SP:c1b7b550b9f90bd5e9bf5218e22d1977ed1686a5,"sampling rules USED-FOR stochastic variance reduced method. convergence rates FEATURE-OF random reshuffling. Prox - DFinito HYPONYM-OF proximal method. Method are cyclic sampling, adaptive variant, and iid sampling. OtherScientificTerm is data heterogeneity. Metric is convergence rate. ","This paper proposes a stochastic variance reduced method based on sampling rules. The main idea is to replace cyclic sampling with an adaptive variant where the data heterogeneity is minimized. The authors prove convergence rates for random reshuffling and show that iid sampling can achieve a convergence rate of $O(1/\sqrt{T})$. The authors also propose a proximal method called Prox-DFinito, which is an adaptive version of the original method."
2069,SP:c1b7b550b9f90bd5e9bf5218e22d1977ed1686a5,"Prox - DFinito HYPONYM-OF method. proximal Finito with without - replacement sampling USED-FOR Prox - DFinito. proximal Finito with without - replacement sampling USED-FOR method. complexity bounds USED-FOR method. sample - size independent bound USED-FOR convex case. OtherScientificTerm are squared distance, and objective function. Metric is rate of Gradient Descent. ","This paper proposes Prox-DFinito, a method based on proximal Finito with without-replacement sampling. The method is motivated by the complexity bounds on the squared distance between the input and the objective function. The authors provide a sample-size independent bound for the convex case and a bound on the rate of Gradient Descent."
2070,SP:c1b7b550b9f90bd5e9bf5218e22d1977ed1686a5,cyclic sampling CONJUNCTION random reshuffling. random reshuffling CONJUNCTION cyclic sampling. algorithm COMPARE proximal GD. proximal GD COMPARE algorithm. convergence rate EVALUATE-FOR proximal GD. random reshuffling CONJUNCTION algorithm. algorithm CONJUNCTION random reshuffling. algorithm USED-FOR cyclic sampling. convergence rate EVALUATE-FOR algorithm. this HYPONYM-OF shuffling based variance reduction algorithm. convergence rate EVALUATE-FOR shuffling based variance reduction algorithm. that USED-FOR importance based reshuffling. norm USED-FOR optimality of sampling orders. importance based reshuffling USED-FOR heuristic. that USED-FOR heuristic. algorithm COMPARE variance reduction algorithms. variance reduction algorithms COMPARE algorithm. convergence rates FEATURE-OF convex functions. Method is Finito algorithm. Generic is algorithms. ,"This paper studies the convergence rate of an algorithm for cyclic sampling and random reshuffling. The authors show that the algorithm converges faster than the proximal GD. The main contribution of this paper is a shuffling based variance reduction algorithm, this is similar to the Finito algorithm. However, the authors propose a heuristic based on importance based reshuffing, that uses a norm to encourage optimality of sampling orders. The algorithm is compared to other variance reduction algorithms, and the authors show the convergence rates for convex functions. The paper also provides some theoretical analysis of the algorithms."
2071,SP:c1b7b550b9f90bd5e9bf5218e22d1977ed1686a5,"first - order algorithm USED-FOR composite convex minimization problems. without - replacement strategy USED-FOR first - order algorithm. without - replacement strategy CONJUNCTION damping step. damping step CONJUNCTION without - replacement strategy. damping step USED-FOR scheme. without - replacement strategy USED-FOR scheme. optimality residual EVALUATE-FOR algorithm. O(1 / k ) rate EVALUATE-FOR algorithm. shuffling strategy USED-FOR weight norm. method COMPARE schemes. schemes COMPARE method. coordinate descent method CONJUNCTION cyclic rule. cyclic rule CONJUNCTION coordinate descent method. cyclic rule HYPONYM-OF schemes. GD HYPONYM-OF schemes. coordinate descent method HYPONYM-OF schemes. optimal cyclic rule USED-FOR sampling. logistic regression problem EVALUATE-FOR methods. Method are Finito / MISO method, proximal gradient methods, and adaptive variant. Metric is rate. ","This paper proposes a new first-order algorithm for composite convex minimization problems that uses an without-replacement strategy and a damping step. The proposed algorithm achieves O(1/k) rate with optimality residual, which is faster than the Finito/MISO method. The authors also propose a shuffling strategy to reduce the weight norm. The method is compared to other schemes such as GD and is shown to outperform other proximal gradient methods. Finally, the authors propose an adaptive variant where the rate is reduced to O(k/k). The authors evaluate the proposed methods on a logistic regression problem and show that the optimal cyclic rule can be used for sampling."
2072,SP:35c14ef59d9ed68f3f6e3a8cac95fdf3216a9d8f,"optimization errors FEATURE-OF policy. REPS USED-FOR policy optimization problem. unconstrained convex optimization problem FEATURE-OF regularized linear optimization problem. regularized linear optimization problem USED-FOR policy optimization problem. dual function USED-FOR calculating policy updates. gradient norm CONJUNCTION problem - dependent constants. problem - dependent constants CONJUNCTION gradient norm. optimization issues USED-FOR problems. gradient norm USED-FOR approximate solution. gradient norm USED-FOR policy suboptimality. gradient norm FEATURE-OF approximate solution. generative model USED-FOR estimates of the gradients. stochastic gradient descent method USED-FOR estimates of the gradients. generative model USED-FOR stochastic gradient descent method. Method are Relative Entropy Policy Search ( REPS ) algorithm, accelerated gradient descent method, and unbiased gradient estimates. Task is tabular reinforcement learning. OtherScientificTerm are dual REPS objective, additive optimization errors, gradients, and accuracy level. Generic is approach. ","This paper proposes a Relative Entropy Policy Search (REPS) algorithm for tabular reinforcement learning. REPS considers a policy optimization problem that is a regularized linear optimization problem with an unconstrained convex optimization problem, where the optimization errors of the policy can be expressed as a function of the gradient of the dual REPS objective. The dual function is used for calculating policy updates. The authors propose an accelerated gradient descent method, which is based on the observation that the gradient norm of the approximate solution to the policy suboptimality can be decomposed into a set of problem-dependent constants, which are then used to estimate the additive optimization errors. In addition, the authors propose a stochastic grad descent method that uses a generative model to compute estimates of the gradients, which can be used to compute unbiased gradient estimates. The proposed approach is evaluated on a variety of problems with different optimization issues, and it is shown that the proposed approach can converge to a solution that is close to the optimal accuracy level."
2073,SP:35c14ef59d9ed68f3f6e3a8cac95fdf3216a9d8f,relative entropy policy search ( REPS ) method USED-FOR LP - formulated MDP problems. convergence FEATURE-OF relative entropy policy search ( REPS ) method. regularized primal objective function CONJUNCTION policy. policy CONJUNCTION regularized primal objective function. sublinear convergence FEATURE-OF REPS. sublinear convergence FEATURE-OF regularized primal objective function. regularized primal objective function FEATURE-OF REPS. policy FEATURE-OF REPS. sample - complexity EVALUATE-FOR sample - based REPS. REPS USED-FOR exact and stochastic settings. OtherScientificTerm is gradients. ,"This paper studies the convergence of the relative entropy policy search (REPS) method for LP-constrained MDP problems. Specifically, the authors show that REPS with a regularized primal objective function and a policy converges with sublinear convergence. The authors also show that the sample-complexity of sample-based REPS is polynomial in the number of gradients. Finally, they extend REPS to exact and stochastic settings."
2074,SP:35c14ef59d9ed68f3f6e3a8cac95fdf3216a9d8f,sample complexity EVALUATE-FOR relative entropy policy search ( REPS ). algorithm USED-FOR optimal policy. optimal policy USED-FOR MDP. linear programming formulation USED-FOR algorithm. linear programming formulation USED-FOR optimal policy. near stationary policies USED-FOR near optimal policy. biased stochastic gradients USED-FOR convergence. Task is planning setting. ,This paper studies the sample complexity of relative entropy policy search (REPS) in the planning setting. The authors propose an algorithm that uses a linear programming formulation to search for an optimal policy in an MDP. The main idea is to use near stationary policies to find a near optimal policy. The convergence is achieved by using biased stochastic gradients.
2075,SP:35c14ef59d9ed68f3f6e3a8cac95fdf3216a9d8f,convergence guarantees FEATURE-OF REPS. Relative Entropy Policy Search ( REPS ) HYPONYM-OF algorithm. duality USED-FOR RL. performance convergence analysis FEATURE-OF REPS algorithm. policy search CONJUNCTION policy gradient algorithms. policy gradient algorithms CONJUNCTION policy search. accelerated gradient ascent USED-FOR dual formulation. convergence guarantees USED-FOR dual objective of REPS. off - policy samples CONJUNCTION estimating distribution ratios. estimating distribution ratios CONJUNCTION off - policy samples. Task is dual formulation of RL objectives. OtherScientificTerm is on - policy samples. ,"This paper studies the dual formulation of RL objectives. The authors propose a new algorithm called Relative Entropy Policy Search (REPS), which is motivated by the duality in RL. The dual formulation is based on accelerated gradient ascent, and the authors provide convergence guarantees for REPS. The paper also provides a performance convergence analysis of the REPS algorithm. Finally, the authors conduct experiments on policy search and policy gradient algorithms, and show that the on-policy samples are more informative than the off-policy and estimating distribution ratios."
2076,SP:3945d1fb07900d63b7706ca0bce5e451ebfe476b,translation CONJUNCTION scale. scale CONJUNCTION translation. scale CONJUNCTION local 3D structure. local 3D structure CONJUNCTION scale. rotation CONJUNCTION translation. translation CONJUNCTION rotation. PointNet++ CONJUNCTION DGCNN. DGCNN CONJUNCTION PointNet++. PointNet CONJUNCTION PointNet++. PointNet++ CONJUNCTION PointNet. rotation FEATURE-OF point cloud classification networks. DGCNN HYPONYM-OF point cloud classification networks. PointNet HYPONYM-OF point cloud classification networks. PointNet++ HYPONYM-OF point cloud classification networks. 3D smoothness FEATURE-OF network. DNNs USED-FOR rotation. OtherScientificTerm is adversarial perturbations. ,"This paper studies the effect of rotation, translation, scale, and local 3D structure on the performance of point cloud classification networks (PointNet, PointNet++, DGCNN) trained with rotation and translation. The authors show that the 3D smoothness of the network is affected by adversarial perturbations. They also show that DNNs are not robust to rotation."
2077,SP:3945d1fb07900d63b7706ca0bce5e451ebfe476b,scale CONJUNCTION translation. translation CONJUNCTION scale. rotation CONJUNCTION scale. scale CONJUNCTION rotation. translation CONJUNCTION local structure. local structure CONJUNCTION translation. rotation FEATURE-OF point cloud networks. metrics EVALUATE-FOR representations. metrics USED-FOR encoded local 3d structures. network USED-FOR complexity of representations. ModelNet40 classification task CONJUNCTION shapenet part segmentation task. shapenet part segmentation task CONJUNCTION ModelNet40 classification task. shapenet part segmentation task EVALUATE-FOR point - based neural networks. ModelNet40 classification task EVALUATE-FOR point - based neural networks. Metric is sensitivity. Method is 3D point cloud networks. ,"This paper studies the sensitivity of point cloud networks to rotation, scale, translation, and local structure. The authors propose two metrics to evaluate the representations of encoded local 3d structures. The first metric is the sensitivity to rotation and the second one is the complexity of representations learned by the network. The experiments on ModelNet40 classification task and shapenet part segmentation task show that point-based neural networks perform well on the ModelNet 40 classification task. The paper is well-written and well-motivated, and the paper is clearly written. However, there are some issues with the presentation of the experiments and the comparison of 3D point cloud network."
2078,SP:3945d1fb07900d63b7706ca0bce5e451ebfe476b,"representation quality EVALUATE-FOR deep - net. metrics USED-FOR representation quality. metrics EVALUATE-FOR deep - net. metrics USED-FOR regional sensitivities. DNN USED-FOR rotations. architectures USED-FOR spatial smoothness. representation complexity EVALUATE-FOR DNN.'multi - order interaction'metric USED-FOR PC data.'multi - order interaction'metric USED-FOR DNN.'multi - order interaction'metric USED-FOR representation complexity. these USED-FOR PC - object classification. PC - object classification CONJUNCTION part - based object - segmentation. part - based object - segmentation CONJUNCTION PC - object classification. PointNet++ CONJUNCTION DGCNN. DGCNN CONJUNCTION PointNet++. these USED-FOR part - based object - segmentation. metrics USED-FOR PC - DNN architectures. PointNet++ HYPONYM-OF PC - DNN architectures. DGCNN HYPONYM-OF PC - DNN architectures. PointNet USED-FOR local 3D structures. metrics EVALUATE-FOR architectures. metrics EVALUATE-FOR tasks. rotation / translation attacks FEATURE-OF PC - DNN. OtherScientificTerm are global PC structures, and local structures. Generic is training. ","This paper proposes new metrics to evaluate the representation quality of a deep-net. The proposed metrics are designed to measure the regional sensitivities of different regions of the input space, and to quantify the representation complexity of a DNN trained on global PC structures. The authors propose a'multi-order interaction' metric for PC data, which measures the distance between local structures and the global structures during training. These metrics are used to evaluate different PC-DNN architectures (PointNet++, DGCNN) on PC-object classification and part-based object-segmentation, and show that these architectures improve spatial smoothness and reduce the number of rotations required by the DNN to perform well on rotations. Finally, the authors show that PointNet can be used to learn local 3D structures, and that these metrics can be applied to other tasks as well. The experiments show that the proposed metrics can help to improve the performance of a PC - DNN under rotation/translation attacks."
2079,SP:3945d1fb07900d63b7706ca0bce5e451ebfe476b,point cloud DNNs USED-FOR knowledge representations. metrics EVALUATE-FOR model. Shapley value USED-FOR metrics. Method is point DNNs. ,This paper studies the problem of learning knowledge representations from point cloud DNNs. The authors propose two metrics based on the Shapley value to evaluate the model's performance. The main contribution of this paper is to provide a theoretical analysis of the performance of the point DNN. 
2080,SP:81db7f494ca61d3586adb505bf5d2e6e9e2c2bd0,fairness score FEATURE-OF allocation. PreferenceNet HYPONYM-OF neural network. regret CONJUNCTION preference loss. preference loss CONJUNCTION regret. regret CONJUNCTION fairness. fairness CONJUNCTION regret. one USED-FOR fair allocations. fairness EVALUATE-FOR RegretNet. RegretNet COMPARE one. one COMPARE RegretNet. regret EVALUATE-FOR RegretNet. small instances EVALUATE-FOR this. it USED-FOR real world preferences. real world studies USED-FOR user preference samples. mathematical models of preference USED-FOR preference. Task is revenue optimizing auctions. Method is deep neural network based PreferenceNet. ,"This paper studies the problem of revenue optimizing auctions and proposes a deep neural network based PreferenceNet, which is a neural network that aims to maximize the fairness score of the allocation. RegretNet is a combination of regret and preference loss, where regret is the sum of the preference loss and the fairness is the difference between the regret and the preference. The authors compare this with a simple one that only considers fair allocations, and show that this improves performance on small instances. They also show that it can be applied to real world preferences, and provide user preference samples from real world studies. The paper also provides some mathematical models of preference that can be used to model preference."
2081,SP:81db7f494ca61d3586adb505bf5d2e6e9e2c2bd0,"learning constraints PART-OF auction setting. learning constraints PART-OF problem. that USED-FOR auctions. RegretNet architecture USED-FOR auctions. that USED-FOR RegretNet architecture. function USED-FOR RegretNet. learning then constraint USED-FOR implementation. PreferenceNet COMPARE RegretNet. RegretNet COMPARE PreferenceNet. constraints USED-FOR RegretNet. OtherScientificTerm are auction, allocations, feasibility constraint, and binary qualitative fairness of allocations. Metric is Fairness in allocations. Generic are approach, and constraint. Task are revenue maximizing auctions, and binary classification of fairness. ","This paper considers the problem of learning constraints in an auction setting. Fairness in allocations is a fundamental problem in auctions. The paper proposes a RegretNet architecture that can be applied to auctions with learning constraints. The core idea of the proposed approach is to learn a function that maximizes the probability that an auction is fair. This function is then used to train a RegRegretNet that is trained to maximize the probability of an auction being fair. The main difference between the proposed PreferenceNet and the original regretNet is that the proposed implementation uses the learning then constraint instead of the original feasibility constraint. This constraint is motivated by the fact that the binary classification of fairness is difficult in the context of revenue maximizing auctions, and the binary qualitative fairness of allocations is not well understood. The proposed constraints are used to improve the performance and efficiency of the resulting RebuttalNet. "
2082,SP:81db7f494ca61d3586adb505bf5d2e6e9e2c2bd0,"PreferenceNet HYPONYM-OF deep learning approach. deep learning approach USED-FOR revenue - maximizing auctions. RegretNet USED-FOR PreferenceNet. OtherScientificTerm are human preferences, and loss functions. ","This paper presents PreferenceNet, a deep learning approach for revenue-maximizing auctions based on RegretNet. The main idea is to use human preferences to guide the choice of loss functions. The paper is well-written and easy to follow."
2083,SP:81db7f494ca61d3586adb505bf5d2e6e9e2c2bd0,socially desirable constraints PART-OF auction mechanisms. regretNet framework USED-FOR auction mechanisms. metric USED-FOR mechanisms. training procedure USED-FOR constraints. neural network CONJUNCTION training procedure. training procedure CONJUNCTION neural network. neural network USED-FOR constraints. preferenceNet HYPONYM-OF training procedure. synthetic preferences CONJUNCTION human preferences. human preferences CONJUNCTION synthetic preferences. it COMPARE approaches. approaches COMPARE it. human preferences EVALUATE-FOR approaches. synthetic preferences EVALUATE-FOR it. synthetic preferences EVALUATE-FOR approaches. Generic is approach. ,"This paper proposes to incorporate socially desirable constraints into auction mechanisms based on the regretNet framework. The proposed approach is motivated by the observation that existing mechanisms do not use a metric that measures the quality of the constraints. To address this issue, the authors propose a neural network and a training procedure (preferenceNet) to model constraints. Experiments on synthetic preferences and human preferences show that it outperforms existing approaches."
2084,SP:ee24606a968ab17b7827e7f3982af11636f6a2ee,"embedding USED-FOR linear regression. DP guarantees CONJUNCTION utility bounds. utility bounds CONJUNCTION DP guarantees. DP guarantees USED-FOR algorithm. utility bounds USED-FOR algorithm. bounds COMPARE upper bound. upper bound COMPARE bounds. bounds USED-FOR non private case. exponential mechanism USED-FOR upper bound. bounds FEATURE-OF excess risk. OtherScientificTerm are information theoretic bound, and privacy budget. ","This paper studies the problem of embedding for linear regression. The authors propose an algorithm that combines DP guarantees and utility bounds. The bounds for the non private case are tighter than the upper bound derived by an exponential mechanism, which is an information theoretic bound. For the private case, the bounds on the excess risk are tighter, and the privacy budget is tighter."
2085,SP:ee24606a968ab17b7827e7f3982af11636f6a2ee,"approach USED-FOR problem. approach USED-FOR user - level differential privacy guarantees. method USED-FOR privacy guaranteees. exponential mechanism USED-FOR method. Generic are setting, representation, algorithm, and that. Method is shared, low - dimensional representation. ","This paper presents an approach to the problem of computing user-level differential privacy guarantees. The setting is that the user has access to a shared, low-dimensional representation, and the goal is to compute the privacy guarantee of that representation. The method is based on an exponential mechanism, which allows the method to compute privacy guaranteees in a way that is computationally tractable. The algorithm is well-motivated and easy to follow."
2086,SP:ee24606a968ab17b7827e7f3982af11636f6a2ee,"user - level joint differential privacy USED-FOR personalized model learning. differentially private algorithms USED-FOR model personalization task. personalized model's accuracy EVALUATE-FOR algorithms. Generic are problem, and model. Method is user - specified model. ","This paper studies the problem of personalized model learning with user-level joint differential privacy. The authors propose differentially private algorithms for the model personalization task, where the goal is to improve the personalized model's accuracy. The problem is formulated as the following: given a user-specified model, the goal of the algorithm is to minimize the difference between the accuracy of the model and that of the user."
2087,SP:ee24606a968ab17b7827e7f3982af11636f6a2ee,"user - level differential privacy FEATURE-OF model personalization. 2 - layer neural network USED-FOR problem. algorithms HYPONYM-OF algorithms. inefficient algorithms HYPONYM-OF algorithms. exponential mechanism USED-FOR inefficient algorithms. alternating minimization USED-FOR algorithms. alternating minimization framework USED-FOR near - optimal embedding. squared error loss FEATURE-OF linear regression. OtherScientificTerm are shared structure, and information - theoretic upper bounds. Generic is second layer. Method is DP minimizer. ","This paper studies the problem of model personalization with user-level differential privacy. The problem is formulated as a 2-layer neural network, where the first layer learns a shared structure, and the second layer learns an embedding of the shared structure. The authors provide information-theoretic upper bounds on the squared error loss of linear regression, and propose algorithms based on alternating minimization, which are inefficient algorithms with an exponential mechanism. They also propose an alternative DP minimizer, which is based on the alternating minimizability framework, to obtain a near-optimal embedding."
2088,SP:3925fc528de17b8b2e93808f5440ea0503895b75,validation dataset EVALUATE-FOR VQA models. crowdsourcing USED-FOR validation dataset. dataset EVALUATE-FOR VQA research. Method is VQA model. Generic is models. ,This paper proposes a new validation dataset for VQA models based on crowdsourcing. The dataset is designed to evaluate the quality of the proposed models. The goal of the paper is to show that the proposed dataset can be used as a benchmark for evaluating the quality and efficiency of VQAA research. 
2089,SP:3925fc528de17b8b2e93808f5440ea0503895b75,"adversarial examples USED-FOR models. adversarial benchmark USED-FOR VQA. Adversarial VQA "" ( AdVQA ) HYPONYM-OF adversarial benchmark. human annotators CONJUNCTION state - of - the - art models. state - of - the - art models CONJUNCTION human annotators. adversarial examples PART-OF AdVQA dataset. Task is visual question answering. Material are evaluation dataset, and adversarial questions. Method is AdVQA. ","This paper presents an adversarial benchmark for VQA, called ""Adversarial VQAA"" (AdVQA), which aims to improve the performance of models trained on adversarial examples. The paper focuses on visual question answering, where the adversarial questions are generated from a large evaluation dataset. The authors conduct extensive experiments on both human annotators and state-of-the-art models, and show that AdVQAA outperforms existing methods. The main contribution of the paper is the introduction of adversarial versions of the standard AdvQA dataset, which is a nice addition to the literature."
2090,SP:3925fc528de17b8b2e93808f5440ea0503895b75,human - and - model in the loop USED-FOR adversarial VQA dataset. VQA systems USED-FOR VQA problem. adversarial VQA dataset EVALUATE-FOR models. models USED-FOR VQA v2. models COMPARE AdVQA. AdVQA COMPARE models. AdVQA EVALUATE-FOR VQA models. VQA v2 COMPARE human parity. human parity COMPARE VQA v2. Material is VQA challenge. ,"This paper presents an adversarial VQA dataset with both human-and-model in the loop. The authors propose to use VQAs systems to solve the VQAB problem. The proposed models are evaluated on the adversarial version of AdVQA, and compared to the state-of-the-art models on VQAA v2. The results show that the proposed models can achieve better performance than the state of the art on the original VQE challenge, while maintaining human parity."
2091,SP:3925fc528de17b8b2e93808f5440ea0503895b75,"AdVQA USED-FOR Visual Question Answering. VQA CONJUNCTION TextVQA. TextVQA CONJUNCTION VQA. VQA models USED-FOR AdVQA. Method is SOTA model. Generic are model, and dataset. ","This paper proposes AdVQA for Visual Question Answering, which is an extension of the SOTA model. The authors propose to combine VQA and TextVQAs to improve the performance of the proposed model, and show that the proposed dataset is more interpretable than the original dataset. They also propose to use the existing state-of-the-art VQQA models to train the new model."
2092,SP:04f90c10f4ceca0dace727ad875265ce405fff9f,RNN models USED-FOR MEC cells. place cells HYPONYM-OF MEC cells. network USED-FOR place cell - mediated path integration. network USED-FOR MEC - cell dynamics. model COMPARE stereotypical ones. stereotypical ones COMPARE model. heterogeneous neurons COMPARE stereotypical ones. stereotypical ones COMPARE heterogeneous neurons. heterogeneous neurons PART-OF model. model USED-FOR network. heterogeneous neurons USED-FOR network. Method is reward - based foraging model. ,"This paper proposes a reward-based foraging model for place cells, i.e., MEC cells (place cells), which are a subclass of RNN models that are trained to imitate the dynamics of place cells. The authors propose a network for place cell-mediated path integration, which is a network that models the MEC-cell dynamics. The proposed model consists of heterogeneous neurons instead of the stereotypical ones, which allows the network to learn a network with more diverse neurons."
2093,SP:04f90c10f4ceca0dace727ad875265ce405fff9f,"non - grid cells PART-OF MEC. they USED-FOR models of MEC functionality. cross validation USED-FOR spatial bins. rate maps of cells CONJUNCTION cross validation. cross validation CONJUNCTION rate maps of cells. models COMPARE model. model COMPARE models. functional relevance FEATURE-OF grid and non - grid cells. ablation studies EVALUATE-FOR functional relevance. Generalization EVALUATE-FOR models. reward - biased setting USED-FOR models. reward - biased setting USED-FOR Generalization. OtherScientificTerm are noise, and path integration ability. Generic is them. Method is linear mapping. Task is Generalization of the models. ","This paper studies the functional relevance of grid and non-grid cells in MEC, and how they affect the performance of models of MEC functionality. The authors propose to use rate maps of cells and cross validation for spatial bins, and use them to train models that are more robust to noise. Generalization of the models is evaluated in a reward-biased setting, where the model is trained to maximize the path integration ability. The functional relevance is also evaluated in ablation studies, where a linear mapping is used."
2094,SP:04f90c10f4ceca0dace727ad875265ce405fff9f,linear regression USED-FOR robustness. robustness EVALUATE-FOR non - grid - like responsive cells. linear regression USED-FOR non - grid - like responsive cells. regression USED-FOR similarity of responses. goal driven training of RNNs USED-FOR MEC responses. goal driven training of RNNs USED-FOR similarity of responses. non - grid - like responses PART-OF RNNS. low - rank decomposition of place fields USED-FOR neural activity. non - grid - like responses USED-FOR path integration. goal - driven training CONJUNCTION low rank decomposition. low rank decomposition CONJUNCTION goal - driven training. similarity matching EVALUATE-FOR goal - driven training. low rank decomposition USED-FOR similarity matching. Material is MEC cells. ,"This paper proposes to improve the robustness of non-grid-like responsive cells by using linear regression to measure the similarity of responses using regression. The goal driven training of RNNs to model MEC responses is used to improve similarity of the MEC cells. In addition, the authors propose to incorporate the non-rigid-like responses into RNNS by adding a low-rank decomposition of place fields to model the neural activity. The authors show that goal-driven training and low rank decomposition can improve the similarity matching between MEC response and path integration."
2095,SP:04f90c10f4ceca0dace727ad875265ce405fff9f,predictability EVALUATE-FOR computational models of MEC neurons. similarity transform'methods USED-FOR computational models of MEC neurons. model USED-FOR firing patterns. firing patterns FEATURE-OF grid cells. model USED-FOR grid cells. experimental approaches USED-FOR heterogeneous cells. heterogeneous cells USED-FOR path integration. heterogeneous cells COMPARE grid cells. grid cells COMPARE heterogeneous cells. ,"This paper aims to improve the predictability of computational models of MEC neurons using'similarity transform' methods. Specifically, the authors propose a model that predicts the firing patterns of grid cells. The experimental approaches have shown that heterogeneous cells are more effective at path integration than grid cells, and the authors show that the proposed model is able to predict the grid cells as well."
2096,SP:57f9812fa5e7d0c66d412beb035301684d760746,"KL - regularized reinforcement learning USED-FOR locomotion tasks. expert demonstrations USED-FOR conditional neural density models. neural networks HYPONYM-OF conditional neural density models. KL penalty USED-FOR Gaussian policies. expert demonstrations USED-FOR models. Gaussian Process regression models HYPONYM-OF models. expert demonstrations USED-FOR KL penalty. expert policy variance CONJUNCTION KL penalty. KL penalty CONJUNCTION expert policy variance. KL penalty CONJUNCTION policy gradients. policy gradients CONJUNCTION KL penalty. variance collapse USED-FOR RL optimization process. KL regularization CONJUNCTION expert policy. expert policy CONJUNCTION KL regularization. expert policy USED-FOR methods. KL regularization USED-FOR methods. OtherScientificTerm are Gaussian distribution, policies, variance, and kernel. Material is expert data. Method are KL - regularized policies, and Gaussian Process models. Generic is collapse. ","This paper studies KL-regularized reinforcement learning for locomotion tasks. The authors consider conditional neural density models (i.e., neural networks) trained with expert demonstrations, where the KL penalty is applied to Gaussian policies trained on expert demonstrations. The models considered here are Gaussian Process regression models, which are trained on a Gaussian distribution. The KL penalty penalizes the deviation between the expert policy variance and the standard KL penalty, and the policy gradients. The main contribution of this paper is to study the effect of variance collapse in the RL optimization process, and to show that the collapse is not caused by the expert data, but by the fact that the policies trained with the KL regularization are trained to maximize the variance of the kernel. The paper then proposes two methods that combine KL regularized policies with an expert policy, and shows that the proposed methods outperform methods that do not use the KL penalization and the original expert policy."
2097,SP:57f9812fa5e7d0c66d412beb035301684d760746,"expert demonstrations USED-FOR KL - based reinforcement learning. out - of - sample variance predictions EVALUATE-FOR NN expert policies. GP approach USED-FOR non - parametric expert policies. OtherScientificTerm are RL exploration, expert data distribution, and numerical instability. ","This paper studies the problem of KL-based reinforcement learning with expert demonstrations. The authors show that NN expert policies have out-of-sample variance predictions, which is a major obstacle to RL exploration. To address this issue, the authors propose a GP approach to learn non-parametric expert policies, where the expert data distribution is sampled from and the numerical instability is minimized."
2098,SP:57f9812fa5e7d0c66d412beb035301684d760746,"expert demonstrations USED-FOR KL - regularized RL. non - parametric behavioral policies USED-FOR control tasks. Method is parametric behavioral policies. OtherScientificTerm are collapse of predictive variance, and collapse of variance. Generic is algorithm. ","This paper studies the problem of KL-regularized RL with expert demonstrations, where the goal is to learn parametric behavioral policies that are robust to the collapse of predictive variance. The authors propose an algorithm that does not suffer from this collapse of variance, and show that this algorithm is able to learn non-parametric behavioral behaviors for control tasks."
2099,SP:57f9812fa5e7d0c66d412beb035301684d760746,"OOD regions FEATURE-OF uncertainty collapse of parametric models. Maximum Likelihood estimation USED-FOR parametric models. non - parametric models USED-FOR behavior policy. continuous control benchmarks EVALUATE-FOR non - parametric models. Method are KL - regularized reinforcement learning, and gradient - based learning algorithms. OtherScientificTerm are small predictive variance, exploding gradients, and uncertainty collapse. ","This paper studies the uncertainty collapse of parametric models in OOD regions. Maximum Likelihood estimation has been shown to be a useful tool for training parametric, but its application in KL-regularized reinforcement learning is limited due to the small predictive variance. This paper proposes to use non-parametric models to train the behavior policy on continuous control benchmarks, and show that gradient-based learning algorithms suffer from exploding gradients, leading to uncertainty collapse."
2100,SP:cb38b58054581db865d8c2a4065f062724ca0a5e,"one - hidden layer locally - connected NNs CONJUNCTION convolutional NNs. convolutional NNs CONJUNCTION one - hidden layer locally - connected NNs. generalization error EVALUATE-FOR kernel regression. kernel regression USED-FOR teacher - student scenario. NTK USED-FOR one - hidden layer locally - connected NNs. NTK USED-FOR convolutional NNs. one - hidden layer locally - connected NNs USED-FOR kernels. NTK USED-FOR kernels. Gaussian random field USED-FOR teacher. statistical physics predictions USED-FOR exponent. translation USED-FOR invariance. Gaussian universality assumption CONJUNCTION framework. framework CONJUNCTION Gaussian universality assumption. framework USED-FOR lower bound. lower bound FEATURE-OF learning curve exponent. OtherScientificTerm are student kernel, learning curve exponents, orthonormal basis functions, ambient dimension d, patch size, locality, curse of dimensionality, and prefactor. Material is synthetic data. ","This paper studies the generalization error of kernel regression in the teacher-student scenario, where the teacher is a Gaussian random field and the student kernel is an NTK trained on one-hidden layer locally-connected NNs and convolutional NNs. The authors show that the learning curve exponents of the teacher and student are orthonormal basis functions, and that the kernels of the NTK can be expressed as a function of the ambient dimension d. The exponent is derived from statistical physics predictions, and the authors provide a lower bound on the exponent under the Gaussian universality assumption and the framework of translation to ensure invariance. They also provide a theoretical analysis of the dependence of the exponent on the patch size and show that this dependence is due to the curse of dimensionality. Finally, the authors conduct experiments on synthetic data to show the effect of the prefactor."
2101,SP:cb38b58054581db865d8c2a4065f062724ca0a5e,"convolutional neural networks COMPARE hypothesis classes. hypothesis classes COMPARE convolutional neural networks. convolutional structure FEATURE-OF kernel. OtherScientificTerm are lazy training, expected generalization error, and Gaussian random field. Method is neural networks. Material is teacher - student setting. ","This paper studies the problem of lazy training in neural networks. The authors show that convolutional neural networks are more likely to be lazy than hypothesis classes. They show that the expected generalization error is bounded by a Gaussian random field, and that the kernel of the neural networks can be decomposed into two parts: (1) the convolutionality structure of the kernel, and (2) the number of parameters. They also show that in the teacher-student setting, the teacher can be lazy."
2102,SP:cb38b58054581db865d8c2a4065f062724ca0a5e,local patch CONJUNCTION translation - invariant. translation - invariant CONJUNCTION local patch. local patch PART-OF CNNS. translation - invariant PART-OF CNNS. teacher - student framework USED-FOR kernel regression. curse of dimensionality FEATURE-OF locality of CNNs. OtherScientificTerm is learning curve. ,This paper proposes a teacher-student framework for kernel regression that combines a local patch and translation-invariant in CNNS. The main contribution of this paper is to address the curse of dimensionality in the locality of CNNs and to reduce the learning curve.
2103,SP:cb38b58054581db865d8c2a4065f062724ca0a5e,generalization error EVALUATE-FOR CNN. local kernel size FEATURE-OF student network. teacher - student framework USED-FOR problem. learning curve exponent FEATURE-OF ridgeless case. kernel ridge regression USED-FOR learning curve exponent. ridge coefficient USED-FOR kernel ridge regression. OtherScientificTerm is data dimension. Metric is test error. ,"This paper studies the generalization error of a CNN when the data dimension is large. The problem is formulated in a teacher-student framework, where the teacher network is trained to maximize the local kernel size of the student network. The authors show that the learning curve exponent of the ridgeless case can be approximated by kernel ridge regression with the ridge coefficient, which can be used to estimate the test error."
2104,SP:7e35e4e610e75c922f2b5219ce625e417f010eeb,GMM priors USED-FOR generative model. GMM priors USED-FOR autoencoders. approach COMPARE VAEs. VAEs COMPARE approach. generation quality EVALUATE-FOR approach. generation quality EVALUATE-FOR VAEs. FID metric EVALUATE-FOR generation quality. FID metric EVALUATE-FOR VAEs. FID metric EVALUATE-FOR approach. loss function USED-FOR GMM. GMM likelihood USED-FOR GMM. loss function CONJUNCTION GMM likelihood. GMM likelihood CONJUNCTION loss function. ,This paper proposes to use GMM priors to train autoencoders to improve the generative model. The proposed approach is evaluated on the FID metric to evaluate the generation quality of VAEs and shows that the proposed approach outperforms VAEs in terms of generation quality. The authors propose a new loss function and GMM likelihood to train the GMM.
2105,SP:7e35e4e610e75c922f2b5219ce625e417f010eeb,"deterministic autoencoder USED-FOR complex multimodal latent space. GMM USED-FOR complex multimodal latent space. GMM USED-FOR latent space. GMM USED-FOR deterministic autoencoder. KS distance CONJUNCTION constant covariance term. constant covariance term CONJUNCTION KS distance. KS distance HYPONYM-OF regularizers. constant covariance term HYPONYM-OF regularizers. regularizers USED-FOR model. deep clustering CONJUNCTION latent model. latent model CONJUNCTION deep clustering. latent model USED-FOR discrete and complex structures. reconstruction and sampling quality CONJUNCTION deep clustering. deep clustering CONJUNCTION reconstruction and sampling quality. deep clustering USED-FOR discrete and complex structures. chemical molecules HYPONYM-OF discrete and complex structures. OtherScientificTerm are marginal CDF, and GMM prior. ","This paper proposes a deterministic autoencoder that uses GMM to learn a complex multimodal latent space. The model is trained with two regularizers: a KS distance and a constant covariance term. The marginal CDF of the latent space is learned by minimizing the GMM prior. The authors show that the combination of reconstruction and sampling quality, deep clustering, and latent model can learn both discrete and complex structures (e.g., chemical molecules)."
2106,SP:7e35e4e610e75c922f2b5219ce625e417f010eeb,deterministic auto - encoder loss USED-FOR multi - model latent representation. GMM USED-FOR multi - model latent representation. it USED-FOR multiple dimensions. KS test metric USED-FOR loss. OtherScientificTerm is ancestrally resample latent codes. ,This paper proposes a deterministic auto-encoder loss for learning multi-model latent representation with GMM. The loss is based on the KS test metric and it can be applied to multiple dimensions. The authors also propose to ancestrally resample latent codes.
2107,SP:7e35e4e610e75c922f2b5219ce625e417f010eeb,Gaussian prior VAEs CONJUNCTION deterministic AEs. deterministic AEs CONJUNCTION Gaussian prior VAEs. noise inputation USED-FOR deterministic AEs. term USED-FOR covariance matching. regularization CONJUNCTION term. term CONJUNCTION regularization. univariate Kolmogorov - Smirnof distance CONJUNCTION term. term CONJUNCTION univariate Kolmogorov - Smirnof distance. regularization FEATURE-OF deterministic AE. GMM USED-FOR latent distribution ex post. univariate Kolmogorov - Smirnof distance USED-FOR regularization. heuristic USED-FOR heyper - parameter setting. heuristic USED-FOR regularizers. heyper - parameter setting USED-FOR regularizers. unsupervised clustering CONJUNCTION generating discrete structures. generating discrete structures CONJUNCTION unsupervised clustering. image datasets CONJUNCTION unsupervised clustering. unsupervised clustering CONJUNCTION image datasets. Method is regularized deterministic AEs. OtherScientificTerm is GMM prior. Generic is method. ,"This paper proposes regularized deterministic AEs. The main idea is to combine Gaussian prior VAEs and deterministic AE with noise inputation. The authors propose two regularizers: 1) an univariate Kolmogorov-Smirnof distance to regularize the latent distribution ex post using GMM, and 2) a term for covariance matching. The regularizers are trained using a heuristic in the heyper-parameter setting. Experiments on image datasets, unsupervised clustering, and generating discrete structures are conducted to demonstrate the effectiveness of the proposed method."
2108,SP:6232d8738592c9728feddec4462e61903a17d131,correct and incorrect class / semantic features USED-FOR autoencoders. SOTA methods USED-FOR adversarial detection. method COMPARE SOTA methods. SOTA methods COMPARE method. method USED-FOR adversarial detection. method USED-FOR adaptive adversarial attacks. Generic is representation. ,This paper proposes to learn both correct and incorrect class/semantic features for autoencoders. The proposed method is compared to SOTA methods for adversarial detection. The authors claim that the proposed method can be used to defend against adaptive adversarial attacks by learning a representation that is more robust to changes in the input.
2109,SP:6232d8738592c9728feddec4462e61903a17d131,defense mechanism USED-FOR adversarial attacks. autoencoder architecture USED-FOR defense mechanism. self - supervised learning USED-FOR semantic and class features. autoencoder USED-FOR clean and adversarial examples. self - supervised learning USED-FOR architecture. victim model CONJUNCTION autoencoder. autoencoder CONJUNCTION victim model. approach COMPARE autoencoder approach. autoencoder approach COMPARE approach. victim model CONJUNCTION victim model. victim model CONJUNCTION victim model. ,This paper proposes an autoencoder architecture that can be used as a defense mechanism against adversarial attacks. The proposed architecture is based on self-supervised learning for both semantic and class features. The authors demonstrate that the proposed approach can outperform the autoencoders on both clean and adversarial examples. Experiments are conducted on both a victim model and an autenocoder.
2110,SP:6232d8738592c9728feddec4462e61903a17d131,"disentangled latent representations USED-FOR adversarial example detection strategy. class features CONJUNCTION semantic features. semantic features CONJUNCTION class features. class features PART-OF latents. semantic features PART-OF latents. semantics FEATURE-OF adversarial samples. losses USED-FOR counterexamples. losses USED-FOR Training. Task is Detection. Method are victim model, and adaptive attack. Generic are attacks, and methods. ",This paper proposes an adversarial example detection strategy based on disentangled latent representations. Detection is based on the observation that adversarial samples with different semantics have different class features and different semantic features in different latents. Training is performed using different losses to generate counterexamples and to train the victim model. An adaptive attack is also proposed. The authors conduct extensive experiments to evaluate the effectiveness of the proposed attacks and show that the proposed methods outperform existing methods.
2111,SP:6232d8738592c9728feddec4462e61903a17d131,"class feature CONJUNCTION semantic feature. semantic feature CONJUNCTION class feature. model USED-FOR semantic feature. model USED-FOR class feature. Auto - Encoder USED-FOR features. Fashion - MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION Fashion - MNIST. MNIST CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION MNIST. AE USED-FOR adversarial detection. Fashion - MNIST EVALUATE-FOR adversarial detection. MNIST EVALUATE-FOR adversarial detection. CIFAR-10 EVALUATE-FOR adversarial detection. Method are disentangled auto - encoder, and disentangled AE. OtherScientificTerm are Class features, and semantic features. Metric is generalization ability. ","This paper proposes a disentangled auto-encoder. The key idea is to disentangle the features of the Auto-Encoder into two parts: a class feature and a semantic feature. Class features are learned separately and the semantic features are learnt together. The authors claim that disentangling AE improves the generalization ability. Experiments on adversarial detection with AE on MNIST, Fashion-MNIST and CIFAR-10 are conducted."
2112,SP:e9d9ad4fb9dc3cb25f7282c0979a8ccb252f692a,"fine - grained information USED-FOR predictions of neural activity. syntactic representations USED-FOR predictions of neural activity. fine - grained information FEATURE-OF syntactic representations. subgraph embedding representations USED-FOR syntactic representation. subgraph embeddings USED-FOR brain activity. Task is brain encoding study. OtherScientificTerm are syntactic complexity, and language system. Metric is complexity metrics. ","This paper presents a brain encoding study that aims to understand how syntactic representations encode fine-grained information for predictions of neural activity. The authors propose to use subgraph embedding representations to learn a syntactic representation of the brain activity, which can then be used for predicting the syntactic complexity of the language system. The main contribution of this paper is to provide a set of complexity metrics to measure the syntactical complexity of a language system, which is an interesting and important topic. "
2113,SP:e9d9ad4fb9dc3cb25f7282c0979a8ccb252f692a,"complexity metrics USED-FOR embeddings. Method are syntactic tree embeddings, and syntactic representations. Generic is them. Material is fMRI data. ",This paper studies syntactic tree embeddings. The authors propose to use complexity metrics to evaluate the quality of the embedding. They show that syntactic representations can be learned in a way that makes them more interpretable and interpretable. They conduct experiments on fMRI data to demonstrate the effectiveness of their results.
2114,SP:e9d9ad4fb9dc3cb25f7282c0979a8ccb252f692a,"incomplete sentence CONJUNCTION partial parses. partial parses CONJUNCTION incomplete sentence. partial parses USED-FOR word embedding scheme. incomplete sentence USED-FOR word embedding scheme. ridge regression USED-FOR fMRI signals. OtherScientificTerm are syntactic structure, sentence structure, language syntaxes, and syntax structures. Generic is schemes. ",This paper proposes a word embedding scheme based on an incomplete sentence and partial parses. The idea is that the syntactic structure of the sentence should be similar to that of the language syntaxes. This is achieved by using ridge regression on fMRI signals. The authors show that the proposed schemes are able to embed sentences with similar syntax structures.
2115,SP:e9d9ad4fb9dc3cb25f7282c0979a8ccb252f692a,"representational spaces USED-FOR syntactical information. complexity - based metrics USED-FOR language neuroscience. syntactical information FEATURE-OF natural language. complexity - based metrics USED-FOR syntactical information. constituency tree USED-FOR representations. subgraph embedding algorithm USED-FOR 15 - D representation. subgraph embedding algorithm USED-FOR subtree. probabilistic model USED-FOR complete parses. subspaces COMPARE complexity / load - based metrics. complexity / load - based metrics COMPARE subspaces. BERT USED-FOR semantic feature space. encoding models USED-FOR feature spaces. variance partitioning USED-FOR them. graph - based spaces COMPARE complexity - based features. complexity - based features COMPARE graph - based spaces. isolated region USED-FOR syntactic processing. Method is parser. OtherScientificTerm are syntactic information, incomplete subtrees, PSG production rules, cortex, semantic features, and syntax. Generic is network. Material is fMRI data. ","This paper studies the problem of learning representational spaces that encode syntactical information in natural language using complexity-based metrics in language neuroscience. In particular, the authors propose to learn representations from a constituency tree, where each subtree is represented by a subgraph embedding algorithm that encodes a 15-D representation of the input to the parser. The authors show that these subspaces are more informative than the complexity/load-based metric used in the context of language neuroscience, and that complete parses can be learned using a probabilistic model, while incomplete subtrees are learned using PSG production rules. The semantic feature space is learned using BERT, and the feature spaces are learned by encoding models that are similar to those used in graph-based spaces, but with variance partitioning applied to them. The network is trained on fMRI data, and it is shown that in the cortex, the semantic features are learned in an unsupervised manner, and syntactic processing is performed in an isolated region."
2116,SP:b4ad4632cd55a85b5403e936c4bd828e484473f7,"conditional samples PART-OF unconditional StyleGAN. conditioning information USED-FOR classifier. conditional distribution FEATURE-OF StyleGAN latent codes. Bayes Theorem USED-FOR conditional distribution. unconditional distribution FEATURE-OF latent codes. unconditional distribution CONJUNCTION distribution of conditioning information. distribution of conditioning information CONJUNCTION unconditional distribution. normal distribution FEATURE-OF unconditional distribution. unconditional distribution FEATURE-OF conditional distribution. normal distribution FEATURE-OF latent codes. Gibbs distribution USED-FOR latter. gradient descent USED-FOR classifier - derived energy. Langevin sampling HYPONYM-OF approaches. approach COMPARE StyleFlow. StyleFlow COMPARE approach. Method is pre - trained generative model. Generic is it. OtherScientificTerm are unnormalized distribution, and conditionings. ","This paper proposes a pre-trained generative model where the conditional samples in unconditional StyleGAN are sampled from an unnormalized distribution. The conditional distribution of the StyleGAN latent codes is derived from the Bayes Theorem, and it is assumed that the latent codes are drawn from an unconditional distribution with a normal distribution and a distribution of conditioning information that is used to train the classifier. The latter is modeled as a Gibbs distribution, and the authors propose two approaches: (1) Langevin sampling and (2) using gradient descent to estimate classifier-derived energy. The authors compare their approach with StyleFlow and show that it outperforms it. They also show that the two conditionings can be combined with each other."
2117,SP:b4ad4632cd55a85b5403e936c4bd828e484473f7,EBM USED-FOR pre - trained top - down generator. latent space FEATURE-OF pre - trained top - down generator. latent space FEATURE-OF EBM. EBM USED-FOR conditional learning. StyleGANs HYPONYM-OF pre - trained top - down generator. EBM HYPONYM-OF joint distribution. conditional sampling CONJUNCTION sequential editing. sequential editing CONJUNCTION conditional sampling. method COMPARE state - of - the - art. state - of - the - art COMPARE method. state - of - the - art USED-FOR conditional sampling. state - of - the - art USED-FOR sequential editing. sequential editing EVALUATE-FOR method. conditional sampling EVALUATE-FOR method. latent space energy - based learning CONJUNCTION score - based generative model. score - based generative model CONJUNCTION latent space energy - based learning. StyleGAN CONJUNCTION latent space energy - based learning. latent space energy - based learning CONJUNCTION StyleGAN. technologies USED-FOR conditional learning. stochastic differential equations USED-FOR score - based generative model. score - based generative model HYPONYM-OF technologies. StyleGAN HYPONYM-OF technologies. latent space energy - based learning HYPONYM-OF technologies. OtherScientificTerm is ordinary differential equation. ,"This paper proposes to use EBM as a joint distribution over the latent space of a pre-trained top-down generator (e.g., StyleGANs) to perform conditional learning. The method is evaluated on conditional sampling and sequential editing and compared to the state-of-the-art for conditional sampling. In particular, the authors propose three new technologies for conditional learning: latent space energy-based learning, a score-based generative model based on stochastic differential equations, and StyleGAN. Theoretical analysis of the joint distribution of the EBM is provided, which is similar to the ordinary differential equation."
2118,SP:b4ad4632cd55a85b5403e936c4bd828e484473f7,"labelled real data USED-FOR EBM. pretrained GAN USED-FOR latent space. latent space USED-FOR EBM. pretrained GAN USED-FOR EBM. ODE solver USED-FOR EBM. EBM COMPARE Langevin Dynamics. Langevin Dynamics COMPARE EBM. ODE solver COMPARE Langevin Dynamics. Langevin Dynamics COMPARE ODE solver. EBM USED-FOR conditional generation compositional editing. zero - shot scenarios FEATURE-OF conditional generation compositional editing. OtherScientificTerm are pixel space, and hyperparameter settings. ","This paper proposes a new EBM based on labelled real data. The proposed EBM uses a pretrained GAN to learn a latent space, which is then used to train an ODE solver to solve the EBM in pixel space. The authors compare the performance of EBM with Langevin Dynamics under different hyperparameter settings and show that EBM can be used for conditional generation compositional editing in zero-shot scenarios."
2119,SP:b4ad4632cd55a85b5403e936c4bd828e484473f7,"approach USED-FOR conditional image generation. GANs USED-FOR approach. EBM USED-FOR latent space. ODE solver USED-FOR sampling. ODE solver COMPARE LD solvers. LD solvers COMPARE ODE solver. compositional generation CONJUNCTION zero - shot generation. zero - shot generation CONJUNCTION compositional generation. sequential generation CONJUNCTION compositional generation. compositional generation CONJUNCTION sequential generation. Method is image generators. OtherScientificTerm are generator, and hyperparameters. Material is CIFAR-10 and FFHQ datasets. ","This paper presents an approach to conditional image generation based on GANs. The key idea is to use an EBM to map the latent space of the image generators to the latent of the generator. An ODE solver is then used for sampling, which is similar to LD solvers. Experiments are conducted on CIFAR-10 and FFHQ datasets and show improvements in sequential generation, compositional generation, and zero-shot generation with different hyperparameters."
2120,SP:f5bf6d43bcc90a3bc5f2157fcc041f18224f95e0,"geomereic structure FEATURE-OF linear rewars. Fed - PE HYPONYM-OF collaborative algorithm. synthetic and real datasets EVALUATE-FOR algorithms. Task is federated setup. Method are stochastic contextual bandit, and bandits. OtherScientificTerm are nodes, local feature vectors, and paramers. ","This paper studies the problem of federated setup in which is motivated by the observation that linear rewars have a geomereic structure. The authors propose a stochastic contextual bandit, which is an extension of the work of Wang et al. (2019) which is a collaborative algorithm called Fed-PE. The key idea is to train a set of nodes that share the same local feature vectors, and to train the nodes in a federated fashion. This is similar to bandits, where the paramers are shared across all nodes. The proposed algorithms are evaluated on both synthetic and real datasets."
2121,SP:f5bf6d43bcc90a3bc5f2157fcc041f18224f95e0,"federated setting FEATURE-OF linear contextual bandits problem. global parameters USED-FOR rewards. matching lower bound USED-FOR regret. G - optimal design USED-FOR balanced exploration. OtherScientificTerm are exploration, and time - horizon. Generic is algorithm. Method is Fed - PE. ",This paper studies a linear contextual bandits problem in a federated setting. The rewards are determined by global parameters and the exploration is restricted to a fixed time-horizon. The authors derive a matching lower bound on the regret and propose an algorithm called Fed-PE. They also propose a G-optimal design for balanced exploration.
2122,SP:f5bf6d43bcc90a3bc5f2157fcc041f18224f95e0,"linear reward functions USED-FOR federated multi - armed bandit problem. agent - dependent contexts FEATURE-OF linear structure. linear structure USED-FOR formulation. multi - agent G - optimal design USED-FOR algorithm. reasonable collinearity assumption FEATURE-OF minimax rates. FedPE CONJUNCTION FedPE. FedPE CONJUNCTION FedPE. simulated and real - world benchmarks EVALUATE-FOR FedPE. OtherScientificTerm are agent reward functions, and heterogeneity. ","This paper studies the federated multi-armed bandit problem with linear reward functions. The formulation is motivated by the linear structure in agent-dependent contexts. The algorithm is based on the multi-agent G-optimal design. The authors provide a reasonable collinearity assumption on the minimax rates, and show that the agent reward functions converge to a fixed point. They also show that FedPE and FedPE with heterogeneity converge to the same fixed point on both simulated and real-world benchmarks."
2123,SP:f5bf6d43bcc90a3bc5f2157fcc041f18224f95e0,lower bound USED-FOR policies. communication cost EVALUATE-FOR algorithm. Task is federated linear contextual bandits problem. OtherScientificTerm is regret guarantees. ,This paper studies the federated linear contextual bandits problem. The authors provide a lower bound on the regret of policies that are able to communicate with each other in a federated setting. The regret guarantees are obtained by minimizing the communication cost of the algorithm.
2124,SP:d3ff3012c614638c8d86322cfe461a9383f082ab,approach USED-FOR model - based offline RL. synthetic data USED-FOR Q - function. CQL CONJUNCTION MOPO. MOPO CONJUNCTION CQL. CQL USED-FOR method. MOPO PART-OF method. synthetic data USED-FOR extrapolation. uncertainty estimation USED-FOR extrapolation. benchmarks EVALUATE-FOR modification. D4RL benchmarks CONJUNCTION vision based benchmarks. vision based benchmarks CONJUNCTION D4RL benchmarks. Generic is model. OtherScientificTerm is roll - outs. ,"This paper presents an approach for model-based offline RL. The method combines CQL and MOPO, and uses synthetic data to train the Q-function. The extrapolation is based on uncertainty estimation, and the model is trained with roll-outs. The proposed modification is evaluated on several benchmarks, including D4RL benchmarks and vision based benchmarks."
2125,SP:d3ff3012c614638c8d86322cfe461a9383f082ab,COMBO HYPONYM-OF model - based offline RL algorithm. model - based learning USED-FOR random rollouts. model - based learning USED-FOR algorithm. rollouts USED-FOR Conservative Q - learning. Task is value function training. OtherScientificTerm is constraint. Method is orignal CQL algorithm. ,"This paper proposes COMBO, a model-based offline RL algorithm. The proposed algorithm uses a combination of (1) random rollouts with (2) model-free learning to improve the performance of the value function training. Conservative Q-learning with rollouts can be viewed as a variant of the orignal CQL algorithm, where the constraint on the number of rollouts is relaxed."
2126,SP:d3ff3012c614638c8d86322cfe461a9383f082ab,lower bound optimization USED-FOR policy. lower bound optimization USED-FOR pessimistic model. actor - critic method USED-FOR value function. COMBO USED-FOR value function. offline and synthetic datasets USED-FOR COMBO. actor - critic method USED-FOR COMBO. state - action pairs USED-FOR Q - values. OtherScientificTerm is uncertainty quantification. Material is offline dataset. ,"This paper proposes COMBO, an actor-critic method for estimating the value function of a policy using lower bound optimization for a pessimistic model. COMBO is trained on both offline and synthetic datasets. The offline dataset is used for uncertainty quantification, and the synthetic dataset for estimating Q-values from state-action pairs."
2127,SP:d3ff3012c614638c8d86322cfe461a9383f082ab,offline dataset USED-FOR model - based reinforcement learning. uncertainty estimation USED-FOR out - of - distribution states and actions. uncertainty estimation USED-FOR mbrl. offline datasets USED-FOR mbrl. dynamics USED-FOR rollouts. Conservative Q - Learning(CQL ) algorithm USED-FOR uncertainty estimation. policy USED-FOR rollouts. they COMPARE Model - based offline Reinforcement Learning. Model - based offline Reinforcement Learning COMPARE they. uncertainty estimation USED-FOR Model - based offline Reinforcement Learning. Generic is dataset. ,"This paper studies the problem of model-based reinforcement learning on an offline dataset. The authors propose to use uncertainty estimation for out-of-distribution states and actions to improve mbrl performance on offline datasets. The uncertainty estimation is based on the Conservative Q-Learning(CQL) algorithm, and the authors show that the dynamics of rollouts from the policy can be used to estimate the uncertainty of the dataset. They also show that they outperform Model-based offline Reinforcement Learning with uncertainty estimation."
2128,SP:ca6f11ed297290e487890660d9a9a088aa106801,"system of linear SDEs USED-FOR features. isotropic feature learning model CONJUNCTION logics - as - features model. logics - as - features model CONJUNCTION isotropic feature learning model. isotropic feature learning model HYPONYM-OF features. logics - as - features model HYPONYM-OF features. model USED-FOR dynamics of features. GeoMNIST and CIFAR10 dataset EVALUATE-FOR deep convolutional networks. ODEs USED-FOR class means. SDEs CONJUNCTION ODEs. ODEs CONJUNCTION SDEs. training process USED-FOR real feature dynamics. Method are stochastic differential equation model, neural networks, and system of ODEs. OtherScientificTerm are local elasticity phenomenon, local elasticity, local elasticity effect, and real trajectories. Generic is models. ","This paper proposes a system of linear SDEs to model the dynamics of features in a stochastic differential equation model. The features are modeled using two features: an isotropic feature learning model and a logics-as-features model. In particular, the authors show that the local elasticity phenomenon can be observed in the case of deep convolutional networks trained on GeoMNIST and CIFAR10 dataset. The authors argue that this phenomenon is due to the fact that the neural networks are trained in an unsupervised manner, and that the model is not able to capture the underlying dynamics of the features. To address this issue, they propose to use a combination of SDE and ODEs for modeling the class means. They show that this combination of models can be used to model both the dynamics and the class mean of the feature. They also show that by modeling the local Elasticity effect, the training process can capture the real feature dynamics. Finally, they conduct experiments on real trajectories to demonstrate the effectiveness of the system of ODE."
2129,SP:ca6f11ed297290e487890660d9a9a088aa106801,"proxy tractable dynamics USED-FOR latent features. latent features FEATURE-OF neural networks. inter - class separability FEATURE-OF linear ODE. linear ODE PART-OF dynamics. H matrix USED-FOR dynamics. higher - order terms PART-OF noise term. OtherScientificTerm are locally elastic ” condition, time - dependence, linearization, linear relationship, linear term, and law - of - large - numbers average. Generic are theory, and proxy. Method is proxy dynamics. ","This paper studies the problem of learning proxy tractable dynamics for latent features of neural networks. The dynamics consists of a linear ODE with inter-class separability under the “locally elastic” condition. The theory is motivated by the fact that the time-dependence of the latent features can be explained by the linearization of the data. The authors show that the dynamics can be represented as a H matrix, where the linear term is a law-of-large-numbers average, and the higher-order terms of the noise term are a function of the dimension of the proxy dynamics."
2130,SP:ca6f11ed297290e487890660d9a9a088aa106801,feature space FEATURE-OF gradient update. local elasticity HYPONYM-OF deep learning models. SDE's modeling SGD USED-FOR back propagation. SDEs USED-FOR dynamics. neural collapse HYPONYM-OF phenomenon. OtherScientificTerm is features. Material is CIFAR-10. ,"This paper studies the phenomenon of local elasticity in deep learning models, where the gradient update in the feature space can be seen as a function of the distance between the features. The authors propose to use an SDE's modeling SGD for back propagation to model this phenomenon, which they call neural collapse. The dynamics are modeled using SDEs. Experiments are conducted on CIFAR-10."
2131,SP:ca6f11ed297290e487890660d9a9a088aa106801,"phenomenological model of neural networks USED-FOR K - class classification. SDEs USED-FOR features. intra - class drift coefficients COMPARE inter - class coefficients. inter - class coefficients COMPARE intra - class drift coefficients. isotropic features CONJUNCTION logit features. logit features CONJUNCTION isotropic features. isotropic features HYPONYM-OF features. logit features HYPONYM-OF features. latter USED-FOR feature evolution. latter COMPARE former. former COMPARE latter. former USED-FOR feature evolution. CIFAR CONJUNCTION dataset. dataset CONJUNCTION CIFAR. feature evolution USED-FOR NN. latter USED-FOR NN. dataset EVALUATE-FOR NN. CIFAR EVALUATE-FOR NN. OtherScientificTerm are feature modeling NN features, Separation, and hyperplanes. ","This paper presents a phenomenological model of neural networks for K-class classification. The main idea is to use SDEs to model the features of the NN. Separation between the intra-class drift coefficients and the inter-class coefficients is used to estimate the distance between the features (i.e., isotropic features and logit features). The feature modeling NN features is done by comparing the former to the former for feature evolution in NN on CIFAR and a new dataset. The hyperplanes of the two datasets are also compared."
2132,SP:a1cb0ca55bc919125f4dad5bcc6e0ad6c2527c1e,"method USED-FOR programmatic policies. weak reward signals USED-FOR programmatic policies. Cross Entropy Method USED-FOR latent space. it USED-FOR programmatic policies. LEAPS USED-FOR tasks. Karel domain FEATURE-OF tasks. Method are reinforcement learning, and zero - shot learning. OtherScientificTerm are smooth latent manifold, and state spaces. ","This paper proposes a method for learning programmatic policies with weak reward signals. The motivation is that reinforcement learning has been shown to converge to a smooth latent manifold, which is not possible in zero-shot learning. The authors propose to use the Cross Entropy Method to approximate the latent space, and then use it to learn a set of state spaces, which can then be used to train programmatically policies. Experiments are conducted on two tasks from the Karel domain using LEAPS."
2133,SP:a1cb0ca55bc919125f4dad5bcc6e0ad6c2527c1e,representing policies USED-FOR MDPs. programs USED-FOR representing policies. trial - and - error experience USED-FOR programmatic policies. hierarchical RL CONJUNCTION deep RL. deep RL CONJUNCTION hierarchical RL. deep RL CONJUNCTION program synthesizer. program synthesizer CONJUNCTION deep RL. method COMPARE baselines. baselines COMPARE method. program synthesizer USED-FOR KAREL domain. hierarchical RL USED-FOR approaches. deep RL USED-FOR approaches. approaches PART-OF baselines. program synthesizer USED-FOR approaches. KAREL tasks EVALUATE-FOR programmatic policy representations. OtherScientificTerm is embedding. Generic is they. ,"This paper studies the problem of representing policies in MDPs using programs. The authors propose to use trial-and-error experience to learn programmatic policies. The proposed method is compared to several baselines that combine hierarchical RL with deep RL and a program synthesizer for the KAREL domain. The results show that the embedding of the learned policies is not necessary, and that they can be learned in an unsupervised manner. Finally, the authors evaluate the performance of programmatic policy representations on a variety of standard and novel MDP, and KEREL tasks."
2134,SP:a1cb0ca55bc919125f4dad5bcc6e0ad6c2527c1e,"two - stage method USED-FOR reinforcement learning. is COMPARE baselines. baselines COMPARE is. baselines USED-FOR gridworld tasks. is USED-FOR gridworld tasks. out - of - distribution generalisation FEATURE-OF gridworld tasks. program synthesis USED-FOR reinforcement learning. latent space USED-FOR programs. latent space USED-FOR programs. process USED-FOR latent space. Generic are second stage, and task. ","This paper proposes a two-stage method for reinforcement learning. The first stage is program synthesis, where the learned programs are used to generate new programs in a latent space. The second stage is to use the generated programs to generate a new task. The authors show that the proposed is outperforms baselines on gridworld tasks with out-of-distribution generalisation."
2135,SP:a1cb0ca55bc919125f4dad5bcc6e0ad6c2527c1e,"two - stage method USED-FOR program synthesis. embedding space USED-FOR semantics of the program    space. variational autoencoder USED-FOR programs. encoder USED-FOR latent space. losses PART-OF Loss function. policy USED-FOR loss. latent program    embedding USED-FOR policy. policy CONJUNCTION encoder. encoder CONJUNCTION policy. cross    entropy method USED-FOR program. cross    entropy method USED-FOR variational autoencoder. latent space USED-FOR cross    entropy method. Viper HYPONYM-OF decision tree policy. method COMPARE methods. methods COMPARE method. deep reinforcement learning    ( DRL ) based CONJUNCTION Viper. Viper CONJUNCTION deep reinforcement learning    ( DRL ) based. deep reinforcement learning    ( DRL ) based HYPONYM-OF methods. Viper HYPONYM-OF methods. Method are decoder, loss functions, loss    function components, latent    space search method, and DRL agent. OtherScientificTerm are syntactically similar programs, latent    space, syntactically close programs, semantically    similar programs, syntactically different programs, execution traces, latent space distribution of programs, and latent program. Metric are Classical beta VAE loss, Program behavior reconstruction loss, and Latent    behavior reconstruction loss. ","This paper proposes a two-stage method for program synthesis. First, a variational autoencoder is trained to generate programs using a cross   entropy method on the latent space of the decoder. This embedding space is used to model the semantics of the program   space. Second, a policy is trained on the learned latent program   embedding. Loss function consists of two losses:   Classical beta VAE loss, where syntactically similar programs are generated from the learned   latent   space, and   semantically   similar programs   are generated   from the original   program. Program behavior reconstruction loss, which is defined as the difference between the execution traces and the latent   distribution of programs generated by the DRL agent.  The proposed method is compared to two existing methods: deep reinforcement learning   (DRL) based and Viper, a decision tree policy."
2136,SP:4be92f235a78f030c4f09c920dc41eab0ba69aa8,"diffusion CONJUNCTION convection. convection CONJUNCTION diffusion. convection HYPONYM-OF PDE systems. diffusion HYPONYM-OF PDE systems. PINNs USED-FOR representations. neural network USED-FOR functions. PDEs PART-OF learning pipeline. PDEs PART-OF PINNs. spatial and temporal domain USED-FOR neural network. sequential prediction task USED-FOR PINN problem. OtherScientificTerm are failure modes, and non - trivial functions. ","This paper studies the problem of learning representations from PINNs. PINNs consist of two types of PDE systems: diffusion and convection. The authors propose a learning pipeline that combines the two PDEs in PINNs by training a neural network in both spatial and temporal domain. The PINN problem is formulated as a sequential prediction task, where failure modes are defined as non-trivial functions."
2137,SP:4be92f235a78f030c4f09c920dc41eab0ba69aa8,"PDE learning cases PART-OF PINN framework. physics informed penalty coefficient USED-FOR loss landscape. condition number FEATURE-OF PINN penalty. warm - up CONJUNCTION sequence to sequence approach. sequence to sequence approach CONJUNCTION warm - up. sequence to sequence approach HYPONYM-OF solutions. warm - up HYPONYM-OF solutions. Method is PINN networks. OtherScientificTerm are PDE coefficient values, and diffusion / conduction coefficient. ","This paper studies the PDE learning cases in the PINN framework. The authors propose a physics informed penalty coefficient for the loss landscape, which is based on the fact that the original PINN penalty has a condition number that depends on the condition number of the input sequence. They propose two solutions: a warm-up and a sequence to sequence approach. They show that the proposed PINN networks converge to PDE coefficient values that are close to the diffusion/conduction coefficient. "
2138,SP:4be92f235a78f030c4f09c920dc41eab0ba69aa8,"Physics Informed NNs ( PINNs ) USED-FOR PDEs of physical relevance. convection coefficient CONJUNCTION viscosity coefficient. viscosity coefficient CONJUNCTION convection coefficient. dominant hessian eigenvectors USED-FOR loss landscape. regularization term PART-OF optimization objective. sequence to sequence learning USED-FOR ill conditioned problems. Method is PINNs. OtherScientificTerm are training regime, closed - form solution, and pde constraint. Task is optimization problem. ","This paper proposes Physics Informed NNs (PINNs) for solving PDEs of physical relevance. PINNs are trained in a two-step process: first, the training regime is conditioned on a closed-form solution to the pde constraint, and then, the optimization problem is solved by adding a regularization term to the optimization objective. The authors propose to use dominant hessian eigenvectors to model the loss landscape, which is defined as the sum of the convection coefficient and the viscosity coefficient. They also propose to apply sequence to sequence learning for ill conditioned problems."
2139,SP:4be92f235a78f030c4f09c920dc41eab0ba69aa8,"physical ( often differential ) equations PART-OF loss function. loss function PART-OF neural network. physical ( often differential ) equations PART-OF models. convection and diffusion equations HYPONYM-OF scientific literature. convection / diffusion coefficients CONJUNCTION forcing terms. forcing terms CONJUNCTION convection / diffusion coefficients. sequence - to - sequence learning task USED-FOR problem. warm start or preconditioning USED-FOR initialization. OtherScientificTerm are numerical difficulties, and dynamical system. Method are physics informed neural networks "" ( PINNs ), and PINNs. Generic is system. ","This paper studies the problem of learning models that incorporate physical (often differential) equations into the loss function of a neural network. This is a well-studied problem in the scientific literature, such as convection and diffusion equations. The problem is formulated as a sequence-to-sequence learning task, and the numerical difficulties are well-motivated. The authors propose ""physics informed neural networks"" (PINNs), which are based on the idea that a dynamical system can be represented as a set of convection/diffusion coefficients and forcing terms, and that the system can then be learned using PINNs. The initialization is based on a warm start or preconditioning."
2140,SP:cfd501bca783590a78305f0592f537e8f20bce27,cycle self - training step USED-FOR backbone representations. self - training USED-FOR classifier. cycle self - training ( CST ) HYPONYM-OF unsupervised domain adaptation ( UDA ) algorithm. backbone representations USED-FOR classifier. Tsallis entropy USED-FOR self - training objective. Material is UDA benchmarks. ,"This paper proposes an unsupervised domain adaptation (UDA) algorithm called cycle self-training (CST), which is an extension of the cycle self training step to learn backbone representations for the classifier. The authors propose to use Tsallis entropy as the self-train objective, and show that the proposed method can achieve state-of-the-art performance on several UDA benchmarks."
2141,SP:cfd501bca783590a78305f0592f537e8f20bce27,cycle self - training algorithm USED-FOR domain adaptation. de - noising methods USED-FOR tasks. ad - hoc hyper - parameters USED-FOR de - noising methods. cyclic training pipeline USED-FOR pseudo labels. Tsallis Entropy USED-FOR label quality. visual classification CONJUNCTION linguistic sentiment classification. linguistic sentiment classification CONJUNCTION visual classification. linguistic sentiment classification EVALUATE-FOR method. visual classification EVALUATE-FOR method. Generic is network. OtherScientificTerm is source - domain knowledge. ,"This paper proposes a cycle self-training algorithm for domain adaptation. The main idea is to use de-noising methods with ad-hoc hyper-parameters to adapt to new tasks. The authors propose a cyclic training pipeline to generate pseudo labels, which are then used to train the network. The label quality is estimated using Tsallis Entropy. The proposed method is evaluated on visual classification and linguistic sentiment classification. The results show that the proposed method can adapt to source-domain knowledge."
2142,SP:cfd501bca783590a78305f0592f537e8f20bce27,approach USED-FOR unsupervised domain adaptation ( UDA ). cycle self - training USED-FOR approach. self - training COMPARE classifiers. classifiers COMPARE self - training. source loss USED-FOR source classifier. source classifier USED-FOR pseudo labels. pseudo labels USED-FOR self - training loss. self - training loss USED-FOR classifier. Tsallis entropy USED-FOR regularization term. ,"This paper proposes an approach for unsupervised domain adaptation (UDA) based on cycle self-training. The authors argue that self-train with pseudo labels is more effective than using classifiers. To this end, the authors propose to use a source loss to encourage the source classifier to produce pseudo labels that can be used to train the classifier. A regularization term based on Tsallis entropy is also proposed."
2143,SP:cfd501bca783590a78305f0592f537e8f20bce27,cyclical self - training USED-FOR unsupervised domain adaptation method. cyclical self - training algorithm USED-FOR bilevel optimization problem. outer loop USED-FOR classifier. components USED-FOR method. cyclical self - training algorithm PART-OF components. Gibbs entropy USED-FOR self - training methods. Tsallis entropy USED-FOR uncertainty measure. computer vision and NLP datasets EVALUATE-FOR method. method COMPARE baseline methods. baseline methods COMPARE method. computer vision and NLP datasets EVALUATE-FOR baseline methods. decent margin USED-FOR baseline methods. decent margin USED-FOR method. cyclical self - training CONJUNCTION Tsallis entropy. Tsallis entropy CONJUNCTION cyclical self - training. components PART-OF algorithm. Tsallis entropy PART-OF algorithm. cyclical self - training PART-OF algorithm. cyclical self - training HYPONYM-OF components. Tsallis entropy HYPONYM-OF components. target pseudo - label quality EVALUATE-FOR approach. OtherScientificTerm is shared representations. ,"This paper proposes an unsupervised domain adaptation method based on cyclical self-training. The proposed method consists of two components: (1) a cyclical version of the standard Gibbs entropy that is used in most self-trainable, but not always effective, methods, and (2) an outer loop that trains a classifier on the shared representations of the two components. The algorithm is composed of two main components: the cyclical part of the Gibbs entropy and the Tsallis entropy used as an uncertainty measure. The method is evaluated on computer vision and NLP datasets and compared to baseline methods with decent margin. The results show that the proposed approach improves the target pseudo-label quality."
2144,SP:af51c83f2f16cbbd4eb087adb978d7dc1c2d7d76,"monotonically increasing gate function USED-FOR neurons / channels. trainable parameter USED-FOR monotonically increasing gate function. monotonically increasing gate function USED-FOR method. Method is single stage pruning method ( DAM ). OtherScientificTerm are gate function, and fine - tuning. ",This paper proposes a single stage pruning method (DAM). The method uses a monotonically increasing gate function with a trainable parameter to prune neurons/channels. The gate function is trained in a way that allows for fine-tuning.
2145,SP:af51c83f2f16cbbd4eb087adb978d7dc1c2d7d76,structured pruning method USED-FOR neuron - level sparsity. DiscriminAtive Masking ( DAM ) USED-FOR neuron - level sparsity. DiscriminAtive Masking ( DAM ) HYPONYM-OF structured pruning method. relu - tanh gating function USED-FOR neuron activations. single stage method USED-FOR method. OtherScientificTerm is L0 norm. Method is after - prune finetuning. ,"This paper proposes a structured pruning method called DiscriminAtive Masking (DAM) to improve neuron-level sparsity. The method is based on a single stage method, where the L0 norm of the neuron activations is modelled as a relu-tanh gating function, and the after-prune finetuning is performed on top of this."
2146,SP:af51c83f2f16cbbd4eb087adb978d7dc1c2d7d76,DAM HYPONYM-OF gradual structured pruning method. Generic is methods. ,"This paper proposes a gradual structured pruning method called DAM, which is an extension of DAR. The paper is well-written and well-motivated. However, there are some issues that need to be addressed in order for the paper to be accepted. In particular, the proposed methods are not well-organized and the experiments are limited."
2147,SP:af51c83f2f16cbbd4eb087adb978d7dc1c2d7d76,in - training model pruning algorithm USED-FOR sparse feature representation. optimization USED-FOR L0 sparsity. single learnable parameter USED-FOR L0 sparsity. Metric is accuracy. Generic is applications. ,"This paper proposes an in-training model pruning algorithm for sparse feature representation. The main idea is to use optimization to reduce L0 sparsity with a single learnable parameter. The authors show that this can improve the accuracy and reduce the number of parameters, which can be useful for various applications."
2148,SP:f831d25830efa88434b43e900241a5ad81119360,"modules USED-FOR model. self - attention USED-FOR model. function FEATURE-OF modules. core module PART-OF ModAttn. dynamic weights FEATURE-OF self - attention module. self - attention module USED-FOR core module. Generic is It. Method are Transformer, and n_f learnable codes. ","This paper proposes ModAttn, a model that uses self-attention. It is inspired by the Transformer. The core module of Modattn consists of a core module that takes as input a sequence of n_f learnable codes and outputs a function for each of these modules. This core module is trained with dynamic weights and is then used to train a new model. "
2149,SP:f831d25830efa88434b43e900241a5ad81119360,drop - in replacement USED-FOR Transformer layers. Neural Interpreter ( NI ) HYPONYM-OF drop - in replacement. self - attention USED-FOR computation path. computation units PART-OF deep learning. modular functions USED-FOR self - attention. self - attention USED-FOR NI. convolution layer HYPONYM-OF deep learning. convolution layer HYPONYM-OF computation units. modular functions USED-FOR NI. NI USED-FOR transfer - learning and systematic generalization tasks. Material is set - valued inputs. Generic is architecture. ,"This paper proposes a drop-in replacement for Transformer layers called Neural Interpreter (NI) which is a generalization of the drop-ins replacement of the Transformer layer. The main idea of NI is to use self-attention to guide the computation path of the computation units in deep learning (e.g., the convolution layer). NI uses modular functions to learn self -attention, which can be applied to set-valued inputs. The authors demonstrate that NI can be used for transfer-learning and systematic generalization tasks and demonstrate the effectiveness of the proposed architecture."
2150,SP:f831d25830efa88434b43e900241a5ad81119360,programming functions CONJUNCTION compilation. compilation CONJUNCTION programming functions. method USED-FOR modularizing neural networks. programming functions USED-FOR method. programming functions USED-FOR modularizing neural networks. compilation USED-FOR modularizing neural networks. stacked MLP CONJUNCTION attention layers. attention layers CONJUNCTION stacked MLP. neural interpreter ( NI ) USED-FOR architecture. stacked MLP PART-OF architecture. attention layers PART-OF architecture. layered functions USED-FOR architecture. NIs USED-FOR specialized functions. embeddings USED-FOR NIs. it COMPARE vision transformers. vision transformers COMPARE it. image datasets CONJUNCTION Raven matrices. Raven matrices CONJUNCTION image datasets. NI USED-FOR Raven matrices. image datasets EVALUATE-FOR NI. Method is attention and transformer architectures. OtherScientificTerm is Functions. ,"This paper proposes a method for modularizing neural networks using programming functions and compilation. The proposed architecture is based on a neural interpreter (NI). The architecture consists of a stacked MLP, attention layers, and layers of layered functions. Functions are learned using NIs with embeddings, which are similar to attention and transformer architectures. The authors evaluate the NI on image datasets and Raven matrices, and show that it outperforms vision transformers."
2151,SP:f831d25830efa88434b43e900241a5ad81119360,Neural Interpreter USED-FOR self - attention networks. image classiﬁcation CONJUNCTION visual abstract reasoning. visual abstract reasoning CONJUNCTION image classiﬁcation. experiments EVALUATE-FOR method. visual abstract reasoning HYPONYM-OF experiments. image classiﬁcation HYPONYM-OF experiments. Method is deep models. Metric is interpretability. Generic is it. ,"This paper proposes Neural Interpreter, a method to train self-attention networks that can be used in conjunction with deep models to improve interpretability. The method is evaluated on two experiments: image classiﬁcation and visual abstract reasoning. The results show that it is able to generalize well to unseen objects."
2152,SP:b78c78fd0b10a94466c049e97c59a56ea5455df6,"pretrained models USED-FOR deep RL context. approach USED-FOR pretrained models. multi - step option USED-FOR exploration. reward - free setting USED-FOR Pretraining. transfer approach USED-FOR agent. agent USED-FOR task. pretrained model USED-FOR task. pretrained policy USED-FOR policy. NGU USED-FOR unsupervised pretraining objective. method USED-FOR pretrained policy. unsupervised pretraining objective USED-FOR RL. Method are neural network, supervised learning, and epsilon - greedy exploration strategy. Material are Atari games, and Atari tasks. ","This paper proposes an approach to train pretrained models in a deep RL context. Pretraining is done in a reward-free setting, where a neural network is trained on a set of Atari games. The agent is trained using a transfer approach, where the agent is given a new task and a new pretrained model to solve that task. The exploration is done using a multi-step option, which is similar to supervised learning, but with an epsilon-greedy exploration strategy. The method is applied to train a pretrained policy that can then be used to learn a new policy using an unsupervised pretraining objective inspired by NGU. Experiments are conducted on a variety of Atari tasks."
2153,SP:b78c78fd0b10a94466c049e97c59a56ea5455df6,"method USED-FOR fine - tuning “ behaviors ”. it USED-FOR temporally extended exploration procedure. Atari games EVALUATE-FOR this. Method are exploration policy, and NeurIPS. OtherScientificTerm is downstream policy. ","This paper proposes a method for fine-tuning “behaviors”, i.e., how well an exploration policy is able to adapt to new environments. The method is called NeurIPS, and it is based on the idea of temporally extended exploration procedure, where the downstream policy is updated as the environment changes. Experiments on Atari games show that this is effective."
2154,SP:b78c78fd0b10a94466c049e97c59a56ea5455df6,"Atari-57 benchmark suite EVALUATE-FOR transfer of unsupervised RL agents. method USED-FOR transfer. behavior transfer ( BT ) USED-FOR transfer. BT USED-FOR downstream off - policy learner. pre - trained policy USED-FOR downstream learner. pre - trained policy USED-FOR downstream learning. BT USED-FOR downstream learning. never give up ( NGU ) objective USED-FOR pre - trained policy. BT USED-FOR recurrent replay distributed DQN ( R2D2 ). pre - trained policy USED-FOR BT. recurrent replay distributed DQN ( R2D2 ) USED-FOR downstream learning. BT COMPARE R2D2 baselines. R2D2 baselines COMPARE BT. transfer COMPARE R2D2 baselines. R2D2 baselines COMPARE transfer. Atari-57 EVALUATE-FOR transfer. Atari-57 EVALUATE-FOR R2D2 baselines. BT USED-FOR transfer. pre - trained policy USED-FOR BT methods. zero - shot pre - trained policy USED-FOR BT. BT COMPARE R2D2 baselines. R2D2 baselines COMPARE BT. tasks EVALUATE-FOR BT. Ms Pacman and Hero games FEATURE-OF tasks. pre - trained policy USED-FOR fine - tuning. fine - tuning USED-FOR BT. pre - trained policy USED-FOR BT. fine - tuning USED-FOR R2D2+BT. R2D2 CONJUNCTION R2D2+BT. R2D2+BT CONJUNCTION R2D2. fine - tuning USED-FOR R2D2. Atari-57 EVALUATE-FOR fine - tuning. finetuning COMPARE variant. variant COMPARE finetuning. BT USED-FOR finetuning. BT USED-FOR variant. OtherScientificTerm are pseudo - action, action space, and unsupervised pre - training. Task is exploitation. Method is pre - training. Material is hard exploration games. ","This paper presents a method for transfer of unsupervised RL agents on the Atari-57 benchmark suite. Specifically, the authors propose behavior transfer (BT) which aims to improve the transfer from one agent to another. BT trains a downstream off-policy learner using a pre-trained policy with a never give up (NGU) objective, and then uses the pre-trained policy to perform downstream learning using a recurrent replay distributed DQN (R2D2). The authors evaluate BT on three tasks from the Ms Pacman and Hero games, and show that BT improves the transfer on Atari-56 compared to R2D-2 baselines. The authors also compare the performance of BT with other BT methods that do not use pre-training. They show that using a zero-shot pre-train policy, BT does not improve transfer. They also show that fine-tuning with a pre -trained policy for BT improves transfer performance. Finally, they show that finetuning with BT is equivalent to using a variant of BT that does not pre-tune. They compare the results of fine -tuning for R2DP2 and R2DD2+BT with the performance gain of pre-pre-training, and conclude that exploitation can be achieved by fine-tuning with a pseudo-action that is not in the action space. The paper also shows that the benefits of using pre -training can be seen in hard exploration games, which is an important aspect of ununsupervised pre-raining."
2155,SP:b78c78fd0b10a94466c049e97c59a56ea5455df6,pre - trained behavior USED-FOR exploration. exploration PART-OF reinforcement learning. transferring pre - trained behavior USED-FOR reinforcement learning. pre - trained policy USED-FOR temporally - extended exploration. value estimates USED-FOR pre - trained policy. pre - trained policy USED-FOR approached. pre - trained policy USED-FOR BT ). intrinsic motivation objectives USED-FOR complex behaviors. large - scale pre - training CONJUNCTION intrinsic motivation objectives. intrinsic motivation objectives CONJUNCTION large - scale pre - training. OtherScientificTerm is rewards. ,"This paper studies the problem of transferring pre-trained behavior to facilitate exploration in reinforcement learning. The authors propose an approach to do this by using a pre-trained policy to perform temporally-extended exploration based on value estimates. The approach is motivated by the fact that the rewards in the environment can change over time, and the authors propose to use a large-scale pre-training with intrinsic motivation objectives to encourage complex behaviors (BT)."
2156,SP:8f0eb77f64b185627b7a82005e0b9e368197c8cd,surrogates USED-FOR ranking metrics optimization. PiRank USED-FOR ranking metrics optimization. PiRank HYPONYM-OF surrogates. temperature - controlled relaxation USED-FOR continuous and differentiable sorting operator. it USED-FOR continuous and differentiable sorting operator. temperature - controlled relaxation USED-FOR permutation matrix. temperature - controlled relaxation USED-FOR it. divide - and - conquer method USED-FOR metric computation. complexity EVALUATE-FOR metric computation. complexity EVALUATE-FOR divide - and - conquer method. OtherScientificTerm is ranking lists. Generic is methods. ,"This paper proposes two surrogates for ranking metrics optimization, namely PiRank and PiRank-1, which is a generalization of PiRank. In particular, it uses a temperature-controlled relaxation of the permutation matrix and uses it as a continuous and differentiable sorting operator. The authors also propose a divide-and-conquer method to reduce the complexity of metric computation by a factor of 2. The main contribution of this paper is that it proposes to divide the ranking lists into two sub-lists, which are then used to compute the ranking metrics of each sub-list. The proposed methods are evaluated on a variety of datasets."
2157,SP:8f0eb77f64b185627b7a82005e0b9e368197c8cd,differentiable sorting operators USED-FOR PiRank. MSLR - WEB30 K CONJUNCTION Yahoo! C14. Yahoo! C14 CONJUNCTION MSLR - WEB30 K. MSLR - WEB30 K HYPONYM-OF LTR data sets. Yahoo! C14 HYPONYM-OF LTR data sets. ,"This paper proposes to use differentiable sorting operators for PiRank. Experiments are conducted on two LTR data sets, MSLR-WEB30K and Yahoo! C14."
2158,SP:8f0eb77f64b185627b7a82005e0b9e368197c8cd,"metrics USED-FOR predicted ranking. permutation matrix PART-OF metrics. relaxed, unimodal matrix USED-FOR permutation matrix. relaxed, unimodal matrix USED-FOR metrics. temperature USED-FOR relaxed, unimodal matrix. relaxation PART-OF Neuralsort. divide and conquer strategy USED-FOR Neuralsort. complexity EVALUATE-FOR approach. benchmark learning to rank datasets EVALUATE-FOR method. It COMPARE sota methods. sota methods COMPARE It. Task is Learning To Rank. Generic are model, and relaxed metrics. ","This paper addresses the problem of Learning To Rank. In this paper, the authors propose two metrics for predicting ranking based on a relaxed, unimodal matrix that is a permutation matrix of a temperature. The authors propose a model called Neuralsort that combines this relaxation with a divide and conquer strategy, where the model is divided into two parts: (1) the relaxation of the model and (2) the permutation of the relaxed metrics. The proposed approach is shown to reduce the complexity of the approach. The method is evaluated on benchmark learning to rank datasets and compared to other sota methods."
2159,SP:8f0eb77f64b185627b7a82005e0b9e368197c8cd,scalable listwise loss USED-FOR ranking. PiRank HYPONYM-OF scalable listwise loss. PiRank HYPONYM-OF ranking. NeuralSort algorithm USED-FOR temperature - controlled. temperature - controlled USED-FOR differentiable relaxation of NDCG metric. differentiable relaxation of NDCG metric USED-FOR loss. divide - and - conquer strategy USED-FOR Scalability. Metric is NDCG. OtherScientificTerm is temperature. Method is sorting relaxation. ,"This paper proposes a scalable listwise loss for ranking, PiRank, a ranking based on the NDCG. The loss is based on a differentiable relaxation of the original loss, which is temperature-controlled by the NeuralSort algorithm. Scalability is achieved by a divide-and-conquer strategy, where the temperature is controlled by a sorting relaxation."
2160,SP:c4d1c99a2d53e90336c7e110738bc1eb8a38f3b4,"deep reinforcement learning USED-FOR Ansatz optimization problems. framework USED-FOR Ansatz optimization problems. quantum chemistry FEATURE-OF Ansatz optimization problems. deep reinforcement learning PART-OF framework. quantum reinforcement learning CONJUNCTION variational circuit learning. variational circuit learning CONJUNCTION quantum reinforcement learning. Rotosolve COMPARE DDQN - based method. DDQN - based method COMPARE Rotosolve. DDQN USED-FOR RL. heuristic method USED-FOR quantum circuit architectures search ( QCAS ). DDQN - based method USED-FOR QCAS. DDQN HYPONYM-OF DRL algorithm. DQN HYPONYM-OF DRL algorithm. Material is lithium hydride. Metric are time complexity, and complexity. Generic are version, and algorithm. Method is ML framework. Task are neural architecture search ( NAS ) community, and Ansatz optimization. OtherScientificTerm is DQNs. ","This paper proposes a framework for solving Ansatz optimization problems in quantum chemistry that combines deep reinforcement learning with variational circuit learning. The ML framework is inspired by the neural architecture search (NAS) community. The authors compare Rotosolve, a DDQN-based method for QCAS, with a heuristic method for quantum circuit architectures search (QCAS), and show that the time complexity of the proposed version is comparable to that of the original DQN (DDQN for RL). The authors also compare the proposed algorithm with a DRL algorithm (DQN, for lithium hydride) and find that DQNs have a similar complexity."
2161,SP:c4d1c99a2d53e90336c7e110738bc1eb8a38f3b4,Noisy Intermediate Scale Quantum ( NISQ ) computer USED-FOR ground state of quantum systems. method USED-FOR ground state of quantum systems. Noisy Intermediate Scale Quantum ( NISQ ) computer USED-FOR method. Ansatz USED-FOR ground - state wave - function. classical algorithm EVALUATE-FOR Ansatz. classical algorithm USED-FOR circuit. single - qubit and two - qubit gates HYPONYM-OF quantum circuit. NISQ USED-FOR classical algorithm. quantum circuit USED-FOR Ansatz. reinforcement learning ( RL ) based approach USED-FOR small circuits. double deep - Q learning CONJUNCTION curriculum learning. curriculum learning CONJUNCTION double deep - Q learning. double deep - Q learning USED-FOR RL method. curriculum learning USED-FOR RL method. method USED-FOR ground state energy. ground state energy FEATURE-OF LiH. method USED-FOR LiH. method USED-FOR bond distances. chemical accuracy EVALUATE-FOR method. Method is variational quantum eigensolver ( VGE ). OtherScientificTerm is electronic Hamiltonian. ,"This paper proposes a method based on the Noisy Intermediate Scale Quantum (NISQ) computer to learn the ground state of quantum systems. The main idea is to use a variational quantum eigensolver (VGE) to approximate the ground-state wave-function of the system. The circuit is trained using a classical algorithm on NISQ, and then the circuit is optimized using a quantum circuit (single-qubit and two-qbit gates). The authors also propose a reinforcement learning (RL) based approach to learn small circuits. The RL method is based on double deep-Q learning and curriculum learning. The authors show that the proposed method is able to learn ground state energy of LiH, and can also learn bond distances. The method is evaluated on chemical accuracy."
2162,SP:c4d1c99a2d53e90336c7e110738bc1eb8a38f3b4,"ansatz USED-FOR Variational Quantum Eigensolver ( VQE ) quantum circuits. deep reinforcement learning framework USED-FOR ansatz. ansatz USED-FOR VQE circuit. deep reinforcement learning framework USED-FOR VQE circuit. curriculum learning USED-FOR energy threshold. LiH molecule EVALUATE-FOR method. OtherScientificTerm are low estimated energy, and learning failure. ","This paper proposes to use ansatz to train Variational Quantum Eigensolver (VQE) quantum circuits. The authors use a deep reinforcement learning framework to learn an ansatz for a VQE circuit. The key idea is to use curriculum learning to estimate the energy threshold, and to use low estimated energy to avoid learning failure. The method is evaluated on the LiH molecule."
2163,SP:c4d1c99a2d53e90336c7e110738bc1eb8a38f3b4,"reinforcement learning USED-FOR variational quantum eigensolver ( VQE ) algorithm. intrinsic motivation FEATURE-OF reinforcement learning. algorithm USED-FOR ground state energy of molecules. algorithm USED-FOR quantum chemistry. quantum chemistry USED-FOR ground state energy of molecules. LiH HYPONYM-OF ground state energy of molecules. DDQN USED-FOR VQE - relevant reward function. COBYLA CONJUNCTION rotosolve. rotosolve CONJUNCTION COBYLA. rotation angles FEATURE-OF ansatz's gates. Adam USED-FOR DDQN. RL USED-FOR circuit depth improvement. OtherScientificTerm are simulated noiseless quantum environment, and ansatzes. ","This paper proposes a variational quantum eigensolver (VQE) algorithm based on reinforcement learning with intrinsic motivation. The proposed algorithm is motivated by quantum chemistry, which aims to learn the ground state energy of molecules (LiH) in a simulated noiseless quantum environment. The main contribution of the paper is to propose a variant of DDQN based on Adam, which learns a VQE-relevant reward function. Experiments are conducted on COBYLA and rotosolve, where the ansatz's gates are trained with different rotation angles. The experiments show that RL can lead to circuit depth improvement, especially when the ansatzes are noisy."
2164,SP:4fc9a0b34192e1b3587c8e2128851c6aebddd26b,transductive few - shot learning methods USED-FOR class - imbalanced FSL tasks. Dirichlet distribution USED-FOR class distribution. method USED-FOR imbalance problem. $ \alpha$-divergences USED-FOR imbalance problem. $ \alpha$-divergences USED-FOR method. method COMPARE baselines. baselines COMPARE method. $ \alpha$-TIM COMPARE baselines. baselines COMPARE $ \alpha$-TIM. ,This paper proposes to use transductive few-shot learning methods for class-imbalanced FSL tasks. The proposed method uses $\alpha$-divergences to address the imbalance problem and uses a Dirichlet distribution as the class distribution. Experiments show that the proposed method outperforms the baselines.
2165,SP:4fc9a0b34192e1b3587c8e2128851c6aebddd26b,arbitrary class distributions FEATURE-OF transductive few - shot learning. Dirichlet distribution USED-FOR random samples. simplex FEATURE-OF random samples. Dirichlet distribution USED-FOR setting. PT - MAP CONJUNCTION TIM. TIM CONJUNCTION PT - MAP. mutual - information loss CONJUNCTION \alpha divergences. \alpha divergences CONJUNCTION mutual - information loss. methods USED-FOR class - balance prior. PT - MAP HYPONYM-OF transductive methods. TIM HYPONYM-OF transductive methods. mutual - information loss USED-FOR TIM. mini - ImageNet CONJUNCTION tiered - ImageNet. tiered - ImageNet CONJUNCTION mini - ImageNet. tiered - ImageNet CONJUNCTION CUB. CUB CONJUNCTION tiered - ImageNet. method COMPARE inductive and transductive few - shot learning methods. inductive and transductive few - shot learning methods COMPARE method. benchmark datasets EVALUATE-FOR method. CUB HYPONYM-OF benchmark datasets. mini - ImageNet HYPONYM-OF benchmark datasets. benchmark datasets EVALUATE-FOR \alpha - divergence approach. tiered - ImageNet HYPONYM-OF benchmark datasets. OtherScientificTerm is random variables. ,"This paper studies the problem of transductive few-shot learning with arbitrary class distributions. In this setting, random samples are sampled from a simplex using a Dirichlet distribution. The authors propose two methods to learn a class-balance prior: PT-MAP and TIM, which use mutual-information loss and \alpha divergences. The proposed \alpha-divergence approach is evaluated on several benchmark datasets, including mini-ImageNet, tiered-imageNet, and CUB. Results show that the proposed method outperforms the existing inductive and transduced few-Shot learning methods."
2166,SP:4fc9a0b34192e1b3587c8e2128851c6aebddd26b,few - shot learning USED-FOR transductive evaluation. transductive fashion USED-FOR Models. metrics EVALUATE-FOR Models. model USED-FOR transductive evaluation. Dirichlet distribution USED-FOR class counts. OtherScientificTerm is class predictions. Task is transductive evaluation scenario. Method is transductive evaluation - based methods. ,"This paper studies the problem of transductive evaluation with few-shot learning. Models are trained in a standard, but not in a fully-transductive fashion, and evaluated on a variety of metrics. The main contribution of this paper is to propose a model that can be used to perform transductionive evaluation, where the class predictions are sampled from a Dirichlet distribution, and the class counts are used to train the model. This is an interesting idea, but the paper is not well-motivated in the sense that it is focused on the standard, transductively evaluation-based methods. It is not clear to me why this is a good idea, and why this paper should be considered in the transduced evaluation scenario."
2167,SP:4fc9a0b34192e1b3587c8e2128851c6aebddd26b,"Dirichlet - distributed random variables COMPARE known and fixed uniform distribution. known and fixed uniform distribution COMPARE Dirichlet - distributed random variables. generalization USED-FOR mutual - information loss. Shannon mutual information USED-FOR generalization. alpha - divergences USED-FOR mutual - information loss. alpha - divergences USED-FOR generalization. Method are transductive inference, transductive methods, and inductive methods. OtherScientificTerm are arbitrary class distributions, class - balance artefact, and class - distribution variations. Task is class - balanced tasks. Generic is model. ","This paper studies the problem of transductive inference in the setting of arbitrary class distributions. In particular, the authors consider Dirichlet-distributed random variables instead of the known and fixed uniform distribution. The authors show that the generalization of the mutual-information loss can be achieved by using alpha-divergences, which is a variant of Shannon mutual information, and show that this generalization leads to better generalization than using inductive methods. They also show that transductivity can be improved by introducing a class-balance artefact, which they call the ""class-balance"". The authors provide empirical results on class-balanced tasks, where they show that their model can generalize to arbitrary class-distribution variations."
2168,SP:eb760d20f3820827c41358ff191d22f4fb78847e,CNN USED-FOR inference. patch - by - patch scheduler USED-FOR peak memory. layer - by - layer execution COMPARE patch - by - patch scheduler. patch - by - patch scheduler COMPARE layer - by - layer execution. peak memory FEATURE-OF CNN. CNN USED-FOR MCU. MCU FEATURE-OF inference. network redistribution method USED-FOR computation overhead. methods USED-FOR inference peak memory. computation overhead EVALUATE-FOR methods. accuracy EVALUATE-FOR methods. OtherScientificTerm is patch - by - patch execution. ,This paper proposes to use a layer-by-layer execution instead of a patch-by -patch scheduler to reduce the peak memory of a CNN used for inference in a MCU. The authors also propose a network redistribution method to reduce computation overhead. The experimental results show that the proposed methods can reduce the inference peak memory while maintaining accuracy.
2169,SP:eb760d20f3820827c41358ff191d22f4fb78847e,Working memory utilization EVALUATE-FOR CNN based models. tiny devices USED-FOR visual tasks. peak memory utilization EVALUATE-FOR patch based inference method. OtherScientificTerm is Tiny devices. Method is redistribution process. ,This paper studies the problem of working memory utilization of CNN based models. Tiny devices are very important for visual tasks. The authors propose a patch based inference method to reduce the peak memory utilization. A redistribution process is also proposed.
2170,SP:eb760d20f3820827c41358ff191d22f4fb78847e,"approach USED-FOR memory footprint of activations. compute overhead EVALUATE-FOR per - patch processing. NAS - based mechanism USED-FOR receptive filed redistribution strategies. NAS - based mechanism USED-FOR architectures. architectures CONJUNCTION receptive filed redistribution strategies. receptive filed redistribution strategies CONJUNCTION architectures. OtherScientificTerm are large activations, MCUs, compute, receptive filed, filter WxH, and memory peak of activations. Generic are networks, and network. Metric are MACs, and latency. ","This paper proposes a novel approach to reduce the memory footprint of activations in networks. The main motivation is that large activations can lead to significant compute overhead in per-patch processing. The authors propose two architectures and two receptive filed redistribution strategies based on a NAS-based mechanism. The first is to use MCUs as the compute and the receptive filed as the filter WxH. The second is to compute the entire network as a single network. The experiments show that the MACs can be significantly reduced and the latency can be reduced. In addition, the authors also show that there is a trade-off between the memory peak of the activations and the size of the network."
2171,SP:eb760d20f3820827c41358ff191d22f4fb78847e,peak memory footprint FEATURE-OF neural networks. tiling CONJUNCTION layer fusion - like ideas. layer fusion - like ideas CONJUNCTION tiling. Method is layer - by - layer execution. Task is Neural Architecture Search. ,This paper studies the peak memory footprint of neural networks. The authors propose to use layer-by-layer execution to reduce the memory footprint. They also propose tiling and layer fusion-like ideas. They conduct extensive experiments on Neural Architecture Search.
2172,SP:b147639f58dd3197beb928c609d636e853c6bdd6,"dynamic mechanism design USED-FOR MDP - like setting. planning ” type problem USED-FOR optimal mechanism. linear program USED-FOR optimal mechanism. polynomial time FEATURE-OF linear program. polynomial time FEATURE-OF optimal mechanism. Task is principal - agent problems. OtherScientificTerm are transitions dynamics, policy, and time horizon. Method are mechanism design, and MAXSAT. ","This paper studies dynamic mechanism design in an MDP-like setting, where the transitions dynamics are unknown. The authors consider principal-agent problems where the optimal mechanism is a “planning” type problem. The optimal mechanism can be found in polynomial time by solving a linear program. The main contribution of this paper is to study the problem of mechanism design from the perspective of the policy, and to show that MAXSAT converges to an optimal mechanism within a time horizon."
2173,SP:b147639f58dd3197beb928c609d636e853c6bdd6,linear program USED-FOR unstructured dynamic mechanism design. payments CONJUNCTION individual - rationality constraints. individual - rationality constraints CONJUNCTION payments. LP USED-FOR payments. LP USED-FOR individual - rationality constraints. LP USED-FOR optimal mechanism. optimal mechanism USED-FOR unstructured dynamic environment. finite time horizon FEATURE-OF unstructured dynamic environment. polynomial runtime guarantee USED-FOR optimal mechanism. Method is LP formulation. ,"This paper studies the problem of unstructured dynamic mechanism design in the form of a linear program. In particular, the authors consider the LP formulation and show that the LP can be used to solve both the problems of making payments and individual-rationality constraints. The authors also provide a polynomial runtime guarantee for the optimal mechanism in the case of an unstructuring dynamic environment with a finite time horizon."
2174,SP:b147639f58dd3197beb928c609d636e853c6bdd6,dynamic unstructured environment USED-FOR computing optimal mechanisms. time horizon FEATURE-OF dynamic environment. LP USED-FOR mechanism. Material is static environment. Generic is problem. ,"This paper studies computing optimal mechanisms in a dynamic unstructured environment. The dynamic environment has a longer time horizon than the static environment, and the authors propose a mechanism based on LP to solve this problem."
2175,SP:b147639f58dd3197beb928c609d636e853c6bdd6,"automated mechanism design USED-FOR dynamic unstructured dynamic environments. mechanism's payment function PART-OF randomized dynamic mechanisms. OtherScientificTerm are finite time horizon, mechanism, constant time horizon, and principal's utility. Method is LP. Metric is computational complexity. ","This paper studies automated mechanism design in dynamic unstructured dynamic environments. The authors consider randomized dynamic mechanisms, where the mechanism's payment function is learned over a finite time horizon, and the principal's utility is estimated over a constant time horizon. The main contribution of this paper is to show that LP can be used to reduce the computational complexity."
2176,SP:1c9c01a77aee3bf00e33bffd6be9ec49d2e5ba29,differentiable graph structure learning CONJUNCTION neural architecture search. neural architecture search CONJUNCTION differentiable graph structure learning. framework USED-FOR GNN design. GNN CONJUNCTION MLP. MLP CONJUNCTION GNN. NAS USED-FOR GNN. NAS USED-FOR MLP. OtherScientificTerm is graph. ,This paper proposes a framework for GNN design that combines differentiable graph structure learning and neural architecture search. The main idea is to use NAS to jointly train a GNN and an MLP on the same graph. 
2177,SP:1c9c01a77aee3bf00e33bffd6be9ec49d2e5ba29,gradient - based NAS method USED-FOR GNN architectures. DARTs method USED-FOR operators. graph structure USED-FOR operators. graph - structure learning term USED-FOR graph structure. graph - structure learning term USED-FOR optimization process. OtherScientificTerm is noised in graph. Generic is baselines. ,This paper proposes a gradient-based NAS method for training GNN architectures. The main idea is to use the DARTs method to learn operators that are noised in graph. The optimization process uses a graph-structure learning term to learn the graph structure for the operators. The experimental results are compared to several baselines.
2178,SP:1c9c01a77aee3bf00e33bffd6be9ec49d2e5ba29,"graph structure learning USED-FOR search procedure. GASSO HYPONYM-OF NAS algorithm. graph structure learning USED-FOR denoising process. denoising process USED-FOR search procedure. graph structure learning USED-FOR gradient based NAS methods. Method are DARTS, and gradient based NAS. OtherScientificTerm is graph. Task is searching suboptimal GNN architectures. ","This paper proposes a new NAS algorithm called GASSO (GASSO), which is an extension of DARTS. The main idea is to use graph structure learning as a denoising process to improve the search procedure of the original search procedure. This is an interesting and important direction for gradient based NAS methods, as the graph is very important for searching suboptimal GNN architectures. The paper is well-written and well-motivated. However, there are some issues that need to be addressed in order to make this work useful to the community."
2179,SP:1c9c01a77aee3bf00e33bffd6be9ec49d2e5ba29,graph neural architecture search method USED-FOR graph structures. operations PART-OF GNNs. synthetic datasets EVALUATE-FOR NAS method. feature smoothness constraints USED-FOR graph architecture. ,This paper proposes a graph neural architecture search method for learning graph structures. The main idea is to replace the operations in GNNs with feature smoothness constraints. The NAS method is evaluated on two synthetic datasets.
2180,SP:1ff7a4f6f2ef647b7a9d224f8250b46b7935359a,"group egalitarian CONJUNCTION group leximen. group leximen CONJUNCTION group egalitarian. group utilitarian CONJUNCTION group egalitarian. group egalitarian CONJUNCTION group utilitarian. k - median CONJUNCTION k - means. k - means CONJUNCTION k - median. approximation ratio EVALUATE-FOR clustering algorithm. additive approximation USED-FOR fairness objective. complexity theory lower bounds USED-FOR problem. algorithmic upper bound CONJUNCTION complexity theory lower bounds. complexity theory lower bounds CONJUNCTION algorithmic upper bound. Method are fair clustering, and bi - criteria approximation. OtherScientificTerm is clustering cost. Generic is algorithm. ","This paper studies the problem of fair clustering in the setting of group utilitarian, group egalitarian, and group leximen. The authors propose a bi-criteria approximation, where the k-median and k-means of the clustering algorithm are computed as the approximation ratio between the original clustering cost and the new algorithm. The fairness objective is formulated as an additive approximation. The algorithmic upper bound and complexity theory lower bounds for the problem are provided."
2181,SP:1ff7a4f6f2ef647b7a9d224f8250b46b7935359a,"group utilitarian "" objective CONJUNCTION group egalitarian "". group egalitarian "" CONJUNCTION group utilitarian "" objective. group leximin "" objective HYPONYM-OF generalization. group egalitarian objective USED-FOR maximum fairness violation. group utilitarian objective COMPARE group egalitarian objective. group egalitarian objective COMPARE group utilitarian objective. leximin ( LM ) objective USED-FOR maximum fairness violation. clustering HYPONYM-OF objectives. approximation ratio EVALUATE-FOR color - blind clustering algorithm. algorithms USED-FOR objectives. fair assignment CONJUNCTION flow computation. flow computation CONJUNCTION fair assignment. LP USED-FOR fair assignment. color blind clustering algorithm CONJUNCTION LP. LP CONJUNCTION color blind clustering algorithm. LP USED-FOR flow computation. OtherScientificTerm are fairness desiderata, clustering cost, cost constraints, fairness violations, and fairness guarantees. Metric is fairness. Method is approximation algorithm. Material is adult and the census data sets. ","This paper considers the problem of generalization of the ""group leximin"" objective, which is a generalization that aims to balance the fairness desiderata between the group utilitarian ""objective"" objective and the group egalitarian ""group egalitarian"". The group utilitarian objective aims to minimize the maximum fairness violation, while a group egalitarian objective aims at minimizing the maximum unfair violation. The paper proposes algorithms for both objectives (clustering and clustering). The paper shows that the approximation ratio of the color-blind clustering algorithm to the LP for fair assignment and flow computation is O(1/\sqrt{T}) where T is the clustering cost, and O(T) is the number of samples required to compute the approximation algorithm. In addition, the paper also shows that if the cost constraints are satisfied, then the LP can be used to perform fair assignment or flow computation. Finally, the authors show that for both adult and the census data sets, the fairness guarantees are guaranteed."
2182,SP:1ff7a4f6f2ef647b7a9d224f8250b46b7935359a,"balanced clusters requirement FEATURE-OF fair clustering. clustering quality USED-FOR fairness. GROUP - EGALITARIAN CONJUNCTION GROUP - LEXIMIN. GROUP - LEXIMIN CONJUNCTION GROUP - EGALITARIAN. GROUP - UTILITARIAN CONJUNCTION GROUP - EGALITARIAN. GROUP - EGALITARIAN CONJUNCTION GROUP - UTILITARIAN. GROUP - UTILITARIAN HYPONYM-OF notions. GROUP - LEXIMIN HYPONYM-OF notions. GROUP - EGALITARIAN HYPONYM-OF notions. GROUP - LEXIMIN HYPONYM-OF lexicographical minimizing. Task is clustering. OtherScientificTerm are PoF, unfairness, upper bound, fair clustering requirements, and min value. ","This paper studies the problem of clustering in the setting of PoF, where the goal is to achieve fair clustering with balanced clusters requirement. The main contribution of this paper is to provide an upper bound on the degree of unfairness. The authors show that fairness can be achieved by considering the clustering quality as a measure of fairness. The upper bound is obtained by considering three notions: GROUP-UTILITARIAN, GROUP-EGALITARIAN, and GROUP-LEXIMIN, which are variants of lexicographical minimizing (GOLD, LEXIMI). The authors also show that the upper bound does not depend on the number of samples, but rather on the min value. "
2183,SP:1ff7a4f6f2ef647b7a9d224f8250b46b7935359a,theoretical bound USED-FOR fair clustering. porotype based algorithms USED-FOR theoretical bound. porotype based algorithms USED-FOR fair clustering. upper bound USED-FOR unfairness. fairness CONJUNCTION fairness clustering setting. fairness clustering setting CONJUNCTION fairness. upper bound USED-FOR unfairness. Generic is algorithm. Method is theoretical analysis. ,This paper provides a theoretical bound for fair clustering using porotype based algorithms. The authors provide an upper bound for unfairness under both fairness and fairness clustering setting. They also provide a theoretical analysis of the algorithm.
2184,SP:581faa3e1fd39ddefc2740985fa8f94cacdf2b64,edge independent models HYPONYM-OF graph generative models. models USED-FOR graphs. subgraph properties FEATURE-OF real - world graphs. subgraph properties FEATURE-OF graphs. ,"This paper proposes edge independent models, which are graph generative models that do not depend on the number of nodes in the graph. These models are able to generate graphs with subgraph properties similar to real-world graphs. "
2185,SP:581faa3e1fd39ddefc2740985fa8f94cacdf2b64,structural properties FEATURE-OF edge - independent graph model. model USED-FOR graphs. Method is edge - independent baseline models. OtherScientificTerm is sampled graphs. Material is graph dataset. ,"This paper studies the structural properties of an edge-independent graph model. The authors show that the proposed model is able to learn graphs that are more interpretable than existing state-of-the-art edge-dependent baseline models. They also show that sampled graphs are interpretable. Finally, the authors conduct extensive experiments on a graph dataset to verify their claims."
2186,SP:581faa3e1fd39ddefc2740985fa8f94cacdf2b64,"methods USED-FOR graphs. models COMPARE real graphs. real graphs COMPARE models. models USED-FOR triangles. triangles CONJUNCTION k - cycles. k - cycles CONJUNCTION triangles. expected density FEATURE-OF k - cycles. minimal overlap FEATURE-OF Erdös - Rényi graphs. one HYPONYM-OF edge - independent models. Method is edge - independent generative graph models. OtherScientificTerm are matrix of connection probabilities, and edges. Generic is model. ","This paper proposes edge-independent generative graph models, which are methods for learning graphs with minimal overlap between edges. The authors show that the proposed models are able to learn triangles and k-cycles with high expected density, which is in contrast to real graphs. They also show that Erdös-Rényi graphs have minimal overlap, and show that one of the edge-independence models (the one with a matrix of connection probabilities) is able to generalize to larger graphs than the other model."
2187,SP:581faa3e1fd39ddefc2740985fa8f94cacdf2b64,edge independent graph models USED-FOR graph models. edge independent models USED-FOR graphs. triangle CONJUNCTION subgraph densities. subgraph densities CONJUNCTION triangle. subgraph densities FEATURE-OF graphs. triangle FEATURE-OF graphs. way USED-FOR graph. Material is real world networks. OtherScientificTerm is graph statistics. ,"This paper proposes to use edge independent graph models to train graph models. The idea is to train edge independent models for graphs that have a triangle, subgraph densities, etc. The authors show that this way can be used to learn a graph that is close to the real world networks in terms of graph statistics."
2188,SP:0d77c22df0830cb675b11ad883d014e3a1933c8e,network width CONJUNCTION depth. depth CONJUNCTION network width. batch size CONJUNCTION BatchNorm. BatchNorm CONJUNCTION batch size. training set size CONJUNCTION network width. network width CONJUNCTION training set size. depth CONJUNCTION batch size. batch size CONJUNCTION depth. BatchNorm CONJUNCTION Dropout. Dropout CONJUNCTION BatchNorm. Generic is architectures. Method is DNN frameworks. Task is generalization. ,"This paper studies the effect of training set size, network width, depth, batch size, BatchNorm, and Dropout on the generalization performance of different architectures. The paper provides a thorough analysis of the impact of different DNN frameworks on generalization. The main findings are that: (1) the number of training samples and training set sizes, (2) the network width and depth, and (3) the batch size have a significant impact on the performance of the architectures."
2189,SP:0d77c22df0830cb675b11ad883d014e3a1933c8e,ReLU'[0 ] USED-FOR neural networks. fp16 CONJUNCTION fp32. fp32 CONJUNCTION fp16. fp32 COMPARE fp64. fp64 COMPARE fp32. neural network sizes CONJUNCTION topologies. topologies CONJUNCTION neural network sizes. networks CONJUNCTION batch normalization. batch normalization CONJUNCTION networks. networks HYPONYM-OF topologies. OtherScientificTerm is learned parametric values. Metric is precision. ,"This paper proposes to use 'ReLU'[0] to train neural networks with learned parametric values. The authors compare the performance of fp16, fp32, and fp64 on fp128 and compare the accuracy of the learned parameters with the precision of the original parameters. They compare the neural network sizes, topologies, networks, and batch normalization."
2190,SP:0d77c22df0830cb675b11ad883d014e3a1933c8e,"gradient based methods USED-FOR they. OtherScientificTerm are ReLU function, and derivative. Method is ReLU neural networks. Task is optimization. ","This paper studies the optimization of ReLU neural networks. In particular, they focus on gradient based methods, where the ReLU function is parameterized by a derivative. "
2191,SP:0d77c22df0830cb675b11ad883d014e3a1933c8e,non - smooth activation function USED-FOR neural network. gradient FEATURE-OF non - smooth activation function. ReLU derivative USED-FOR gradient descent. low - precision training CONJUNCTION SGD optimizer. SGD optimizer CONJUNCTION low - precision training. SGD optimizer CONJUNCTION batch norm training. batch norm training CONJUNCTION SGD optimizer. SGD optimizer USED-FOR ReLU'(0 ). ,"This paper studies the gradient of a non-smooth activation function of a neural network. The authors show that the ReLU derivative of the gradient descent converges to ReLU'(0) after low-precision training, SGD optimizer, and batch norm training."
2192,SP:73e6281bf556a6ae92bdcf8d68e6e8973bc8b56b,"approach USED-FOR learning parameterized policies. policy objective CONJUNCTION probabilistic dynamic model objective. probabilistic dynamic model objective CONJUNCTION policy objective. probabilistic dynamic model objective CONJUNCTION compression objective. compression objective CONJUNCTION probabilistic dynamic model objective. it USED-FOR policy objective. it USED-FOR compression objective. it USED-FOR probabilistic dynamic model objective. model predictions CONJUNCTION compression. compression CONJUNCTION model predictions. that USED-FOR RL. it COMPARE variational information bottleneck ( VIB ). variational information bottleneck ( VIB ) COMPARE it. OtherScientificTerm are parameterized policies, bottleneck encoding distribution, and encoding distribution. Generic is optimization. ","This paper proposes an approach for learning parameterized policies. The main idea is to learn a bottleneck encoding distribution, where it is used as a policy objective, a probabilistic dynamic model objective, and a compression objective. The optimization is based on the assumption that the encoding distribution is convex. The authors show that that can be used in RL and compare it to the variational information bottleneck (VIB). The authors also provide some theoretical analysis on the connection between model predictions and compression."
2193,SP:73e6281bf556a6ae92bdcf8d68e6e8973bc8b56b,"reinforcement learning method USED-FOR policy. robust predictable control HYPONYM-OF reinforcement learning method. limited representational resources USED-FOR policy. fully observable MDP USED-FOR problem. variational information bottleneck constraint USED-FOR compressed representation of the state space. compressed state space FEATURE-OF prior dynamics model. OtherScientificTerm are bottleneck, variational prior, representational resources, prior dynamics, and predictable and robust behaviors. Method is dynamics model. Material is RL benchmarks. Task is compression. ","This paper proposes a reinforcement learning method called robust predictable control, which learns a policy with limited representational resources. The problem is formulated as a fully observable MDP, where the bottleneck is a variational prior. The authors propose to use the variational information bottleneck constraint to learn a compressed representation of the state space. The compressed state space of the prior dynamics model is then used to train the dynamics model. Experiments on standard RL benchmarks demonstrate that the compression leads to more predictable and robust behaviors."
2194,SP:73e6281bf556a6ae92bdcf8d68e6e8973bc8b56b,"per - observation basis CONJUNCTION per - sequence basis. per - sequence basis CONJUNCTION per - observation basis. latent model USED-FOR per - sequence basis. latent model USED-FOR it. Generic is method. Task is robust predictable control ( RPC ). OtherScientificTerm are information bottleneck, encoding cost, loss, and reward. ","This paper proposes a method for robust predictable control (RPCR), where the goal is to avoid the information bottleneck by minimizing the encoding cost. The method is based on the idea that it uses a latent model to model both the per-observation basis and per-sequence basis. The idea is that if the loss is small enough, then the reward will be small."
2195,SP:73e6281bf556a6ae92bdcf8d68e6e8973bc8b56b,"method USED-FOR policies. latent representation USED-FOR decision - making. latent space FEATURE-OF action - conditioned dynamics model. action - conditioned dynamics model PART-OF method. encoder CONJUNCTION latent dynamics model. latent dynamics model CONJUNCTION encoder. latent dynamics model CONJUNCTION policy. policy CONJUNCTION latent dynamics model. latent dynamics model USED-FOR training objective. encoder USED-FOR training objective. encoder and dynamics model USED-FOR encoded environment observations. latent dynamics model formulation USED-FOR theoretical guarantees. nominal case CONJUNCTION distorted environments. distorted environments CONJUNCTION nominal case. information - constrained agent USED-FOR nominal case. information - constrained agent USED-FOR distorted environments. Task are Robust Predictable Control, and information - constrained closed - loop control. OtherScientificTerm are information bottleneck, bottleneck, behaviour, and information constraint. Metric is expected task return. ","This paper proposes a method for learning policies for Robust Predictable Control. The method consists of an action-conditioned dynamics model in the latent space, where the latent representation is used for decision-making. The training objective is a combination of an encoder, a latent dynamics model, and a policy. The encoder and dynamics model are trained jointly to extract encoded environment observations, and the policy is trained to maximize the expected task return. The authors provide theoretical guarantees on the latent dynamics models formulation, and provide empirical results on information-constrained closed-loop control, where an information bottleneck is introduced and the bottleneck forces the agent to learn a behaviour that is consistent with the information constraint. Experiments are performed on both the nominal case and distorted environments with an information-confined agent, and show that the information-consistency of the learnt behaviour is improved in both the information constrained case and the distorted environments."
2196,SP:dff08f0b290f3d138fd0299933052f3dc363b2d3,"positional encoding USED-FOR text transformers. learned positional encoding ( LPE ) USED-FOR graph transformers. eigenfunctions USED-FOR node positions. eigenvalues USED-FOR node positions. eigenvalues CONJUNCTION eigenfunctions. eigenfunctions CONJUNCTION eigenvalues. node positions PART-OF graph. them PART-OF transformer model. Spectral Attention Network ( SAN ) HYPONYM-OF model. LPE USED-FOR Spectral Attention Network ( SAN ). LPE USED-FOR model. OtherScientificTerm are physics, and electric potential. Generic is method. ","This paper proposes a learned positional encoding (LPE) for graph transformers, which is inspired by the positional encoding used in text transformers. The key idea is to learn the eigenvalues and eigenfunctions of the node positions in the graph, and incorporate them into the transformer model. The proposed model, called Spectral Attention Network (SAN), is based on the LPE and is able to generalize to unseen regions of the graph. The physics of the proposed method is well-motivated, and the electric potential is demonstrated."
2197,SP:dff08f0b290f3d138fd0299933052f3dc363b2d3,"learning positional encoding USED-FOR graph transformer. spectral attention network ( SAN ) USED-FOR learning positional encoding. spectral attention network ( SAN ) USED-FOR graph transformer. full Laplacian spectrum USED-FOR spectral attention network ( SAN ). over - squashing CONJUNCTION physical interactions. physical interactions CONJUNCTION over - squashing. Method are spectral attention, and SAN. ",This paper proposes a spectral attention network (SAN) for learning positional encoding in graph transformer based on the full Laplacian spectrum. The main contribution of this paper is the introduction of spectral attention and its application to graph transformer. The paper also provides a theoretical analysis of SAN and its properties. The theoretical analysis shows that SAN is robust to over-squashing and physical interactions.
2198,SP:dff08f0b290f3d138fd0299933052f3dc363b2d3,"graph Laplacian USED-FOR eigenvalues and eigenvectors information. eigenvalues and eigenvectors information USED-FOR position encoding mechanism. graph Laplacian USED-FOR position encoding mechanism. position encoding CONJUNCTION embedded node feature. embedded node feature CONJUNCTION position encoding. edge feature USED-FOR graph structural information. it USED-FOR common over - squashing problem. common over - squashing problem PART-OF GNNs. real edges CONJUNCTION added edges. added edges CONJUNCTION real edges. real graph datasets EVALUATE-FOR model. OtherScientificTerm are graph, and attention. ",This paper proposes a position encoding mechanism based on graph Laplacian to encode the eigenvalues and eigenvectors information of the graph. The position encoding is combined with the embedded node feature. The edge feature is used to encode graph structural information. The proposed model is evaluated on two real graph datasets and it is shown to be able to solve the common over-squashing problem in GNNs. The experiments show that the proposed attention is able to distinguish between real edges and added edges.
2199,SP:dff08f0b290f3d138fd0299933052f3dc363b2d3,"transformer USED-FOR graph structured data. GNNs COMPARE transformer. transformer COMPARE GNNs. structure encoding of the nodes USED-FOR transformer. spectrum of the graphs Laplacian USED-FOR PE scheme. fully connected transformer COMPARE GNNs. GNNs COMPARE fully connected transformer. OtherScientificTerm are structure of the graph, and graphs. Method is positional encoding. ","This paper proposes a transformer for graph structured data based on the structure encoding of the nodes. The PE scheme is based on a spectrum of the graphs Laplacian, where the structure of the graph is encoded using a positional encoding. Experiments show that a fully connected transformer can achieve comparable performance to GNNs, while being much faster than a transformer without the graphs."
2200,SP:f2bee0c4a6c558970538b422e5e36750447cd9bc,"social choice CONJUNCTION information aggregation. information aggregation CONJUNCTION social choice. information aggregation PART-OF setting. social choice PART-OF setting. Task are social choice problem, and information aggregation setting. OtherScientificTerm is majority wish. ","This paper studies the social choice problem. The setting considered in the paper is a combination of social choice and information aggregation. In particular, the authors focus on the information aggregation setting, where the majority wish is aggregated and the majority of agents are aggregated. "
2201,SP:f2bee0c4a6c558970538b422e5e36750447cd9bc,"truthful reporting HYPONYM-OF Bayes Nash equilibrium. Material is elections. OtherScientificTerm are Candidate - friendly and candidate - unfriendly agents, and unobservable state variable. Generic is mechanism. ","This paper studies the problem of elections in the setting where there are both candidate-friendly and candidate-unfriendly agents. The authors propose a mechanism called truthful reporting, which is a Bayes Nash equilibrium (truthful reporting is a variant of the original notion of truthful reporting) where the unobservable state variable is a function of the agent’s actions. The mechanism is motivated by the fact that the agent may not be able to observe the other agents’ actions. "
2202,SP:f2bee0c4a6c558970538b422e5e36750447cd9bc,"mechanism USED-FOR truthful reporting. OtherScientificTerm are unobservable state, publicly known distribution, and truthful information. ",This paper proposes a mechanism for truthful reporting in the presence of an unobservable state. The key idea is to use the publicly known distribution of the unobserved state as a proxy for the truthful information. Theoretical analysis is provided to support the effectiveness of the proposed mechanism.
2203,SP:f2bee0c4a6c558970538b422e5e36750447cd9bc,imperfectly informed voters USED-FOR two - alternative voting problem. mechanism USED-FOR majority voting winner. information aggregation literature USED-FOR mechanism. approach USED-FOR two - alternative social choice setting. ,This paper studies the two-alternative voting problem with imperfectly informed voters. The authors propose a mechanism based on the information aggregation literature to find the majority voting winner. The proposed approach is applied to the two -alternative social choice setting.
2204,SP:0823bd0dbb8045648e81a4c93e9782069cf2c605,"Hessian FEATURE-OF linear neural network. identity activations HYPONYM-OF Hessian. identity activations HYPONYM-OF linear neural network. outer - product Hessian CONJUNCTION functional Hessian. functional Hessian CONJUNCTION outer - product Hessian. nonlinear activations FEATURE-OF neural networks. OtherScientificTerm are Hessian rank, and rank. Method is 1 - hidden layer nonlinear networks. ","This paper studies the Hessian of a linear neural network (i.e., identity activations). The authors show that the outer-product Hessian and the functional Hessian converge to the same Hessian rank. The authors also show that neural networks with nonlinear activations converge to this rank. Finally, the authors conduct experiments on 1-hidden layer nonlinear networks."
2205,SP:0823bd0dbb8045648e81a4c93e9782069cf2c605,"rank of Hessian USED-FOR deep linear networks. rank upper bound USED-FOR outer - product. numerical rank FEATURE-OF nonlinear networks. linear rank bound USED-FOR numerical rank. Hessian degeneracy FEATURE-OF 1 - hidden - layer networks. OtherScientificTerm are squared loss, weight matrices, Hessian rank, and bias. ",This paper studies the rank of Hessian in deep linear networks. The authors derive a rank upper bound for the outer-product of the squared loss of the weight matrices. The numerical rank of nonlinear networks is given by a linear rank bound. The Hessian degeneracy of 1-hidden-layer networks is studied and the authors show that the Hessian rank does not depend on the bias.
2206,SP:0823bd0dbb8045648e81a4c93e9782069cf2c605,Hessian matrix FEATURE-OF fully connected neural networks. population loss FEATURE-OF Hessian. square loss FEATURE-OF linear fully - connected neural network. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. losses CONJUNCTION initializations. initializations CONJUNCTION losses. non - linear activations CONJUNCTION losses. losses CONJUNCTION non - linear activations. CIFAR-10 EVALUATE-FOR small neural networks. MNIST EVALUATE-FOR small neural networks. Generic is upper bound. ,"This paper studies the Hessian matrix of fully connected neural networks. The authors derive an upper bound on the population loss of the Hessians. The upper bound is based on the square loss of a linear fully-connected neural network. They evaluate small neural networks on MNIST and CIFAR-10 with non-linear activations, losses, and initializations."
2207,SP:0823bd0dbb8045648e81a4c93e9782069cf2c605,Hessian rank FEATURE-OF deep linear networks. tight upper bounds FEATURE-OF deep linear networks. tight upper bounds FEATURE-OF Hessian rank. rank formulae USED-FOR numerical rank. rank formulae USED-FOR linear case. rank formulae USED-FOR non - linear regime. non - linear regime FEATURE-OF numerical rank. OtherScientificTerm is rank bounds. ,"This paper provides tight upper bounds on the Hessian rank of deep linear networks. The authors provide rank formulae for the linear case and for the non-linear regime for the numerical rank. The paper is well-written and well-motivated. However, the paper suffers from a lack of clarity in the analysis of the rank bounds. "
2208,SP:24cdcb12fca34680d8b34bc61c51b9003368228a,framework USED-FOR linear symmetry based disentanglement ( LSBD ). toy datasets USED-FOR VAE. symmetry transformations FEATURE-OF toy datasets. OtherScientificTerm is LSBD. Generic is metric. Metric is disentanglement. ,This paper proposes a framework for linear symmetry based disentanglement (LSBD) based on a framework called VAE. The LSBD is defined as a metric that measures the degree to which a VAE is disentangled from the ground truth. The authors propose to use toy datasets with different symmetry transformations to train VAE and show that the resulting metric can be used to measure the degree of disentranglement.
2209,SP:24cdcb12fca34680d8b34bc61c51b9003368228a,"loss function USED-FOR disentangled representations. synthetic datasets EVALUATE-FOR ideas. LSBD - based metric CONJUNCTION method. method CONJUNCTION LSBD - based metric. disentangling metrics CONJUNCTION methods. methods CONJUNCTION disentangling metrics. method COMPARE methods. methods COMPARE method. euclidean plane FEATURE-OF rotations. disentangling metrics EVALUATE-FOR method. euclidean plane HYPONYM-OF multi - dimensional disentangled factors. rotations HYPONYM-OF multi - dimensional disentangled factors. formalism USED-FOR disentangling metrics. supervision USED-FOR D_LSBD metric. Method are symmetry - based disentanglement idea, and LSBD method. ",This paper proposes a new symmetry-based disentanglement idea. The idea is to use a loss function to learn disentangled representations. The LSBD-based metric and the proposed method are evaluated on two synthetic datasets. The proposed method is compared with other disentangling metrics and methods based on a formalism. The D_LSBD metric is also evaluated with supervision and compared with the LSBD method. The results show that the proposed methods are more efficient. The paper also shows that the multi-dimensional disenangled factors such as rotations on the euclidean plane can be learned.
2210,SP:24cdcb12fca34680d8b34bc61c51b9003368228a,"metric evaluation metric CONJUNCTION semi - supervised VAE - based model. semi - supervised VAE - based model CONJUNCTION metric evaluation metric. equivariance FEATURE-OF encoding map. dispersion FEATURE-OF inverse group representation. PCA USED-FOR inverse group representation. dispersion USED-FOR approximation. $ \Delta$-VAE USED-FOR latent space. LSBD - VAE HYPONYM-OF $ \Delta$-VAE. evaluation metric EVALUATE-FOR LSBD - VAE. method COMPARE unsupervised disentanglement approaches. unsupervised disentanglement approaches COMPARE method. Method are linearly disentangled group representation, and LSBD. Generic is metric. ","This paper proposes a new metric evaluation metric and a semi-supervised VAE-based model, LSBD-VAE, which is based on a linearly disentangled group representation. The metric is motivated by the observation that the equivariance of the encoding map depends on the dispersion of the inverse group representation learned by PCA. The authors propose an approximation based on this dispersion, which they call LSBD. LSBD is a variant of $\Delta$-VaE that maps the latent space to a latent space, and the authors propose a new evaluation metric for LSBD and show that the proposed method outperforms other unsupervised disentanglement approaches."
2211,SP:24cdcb12fca34680d8b34bc61c51b9003368228a,metric USED-FOR Linear Symmetry Based Disentanglement. it USED-FOR Linear Disentanglement. loss term USED-FOR $ \Delta$-VAE model. metric USED-FOR $ \Delta$-VAE model. metric USED-FOR loss term. OtherScientificTerm is Disentanglement. Generic is approach. ,"This paper proposes a new metric for Linear Symmetry Based Disentanglement, which is a generalization of the existing metric, and applies it to the case of Linear Disentranglement. The authors propose a new loss term for the $\Delta$-VAE model based on this metric, which can be used as a loss term to improve the performance of the original $\Delta$. The authors show that the proposed approach can achieve better performance than existing methods. "
2212,SP:374bfeb067fcea966c97e1721d65cd9d03d26ed3,constrained optimization framework USED-FOR VAE type models. constrained optimization framework USED-FOR deep state space model case. VAE type models USED-FOR deep state space model case. hierarchical prior approach USED-FOR initial distribution. linear transition matrices USED-FOR model. neural network weighting learned basis matrices USED-FOR linear transition matrices. Auxiliary observation variables USED-FOR linear observation model. linear observation model CONJUNCTION encoder / decoder. encoder / decoder CONJUNCTION linear observation model. OtherScientificTerm is smoothing distributions. ,"This paper proposes a constrained optimization framework for VAE type models in the deep state space model case. The authors propose a hierarchical prior approach to estimate the initial distribution of the model using linear transition matrices learned from neural network weighting learned basis matrices. Auxiliary observation variables are used to train a linear observation model and an encoder/decoder. In addition to smoothing distributions, the authors also propose to train the model in a supervised fashion."
2213,SP:374bfeb067fcea966c97e1721d65cd9d03d26ed3,"approach USED-FOR unobserved velocity. RNN encoding USED-FOR nonlinearity. accuracy EVALUATE-FOR models. RNN encoding CONJUNCTION locally linear model. locally linear model CONJUNCTION RNN encoding. accuracy EVALUATE-FOR latent dynamics. Method are REWO, and motion models. Metric is predictive accuracy. Task is closed form inference. ","This paper proposes REWO, an approach to infer unobserved velocity from motion models. The main idea is to use an RNN encoding to model nonlinearity and a locally linear model to model the latent dynamics. The accuracy of these models is evaluated by comparing the predictive accuracy of the model to the accuracy of closed form inference."
2214,SP:374bfeb067fcea966c97e1721d65cd9d03d26ed3,"framework USED-FOR neural network - based state - space models. raw data USED-FOR framework. ELBO USED-FOR constrained optimization ( CO ) framework. reconstruction term PART-OF CO. Kalman - VAE framework COMPARE RNN - based transition model. RNN - based transition model COMPARE Kalman - VAE framework. pendulum data CONJUNCTION reacher environment. reacher environment CONJUNCTION pendulum data. predictive modeling CONJUNCTION ELBO values. ELBO values CONJUNCTION predictive modeling. framework COMPARE baselines. baselines COMPARE framework. pendulum data EVALUATE-FOR framework. ELBO values EVALUATE-FOR framework. ELBO values EVALUATE-FOR baselines. predictive modeling EVALUATE-FOR framework. predictive modeling EVALUATE-FOR baselines. Recurrent Latent Variable Model USED-FOR Sequential Data. Hierarchical Priors USED-FOR VAEs. Disentangled Recognition and Nonlinear Dynamics Model USED-FOR Unsupervised Learning. Method is RNN - based approach. OtherScientificTerm are hierarchical prior, and Gaussian. ","This paper proposes a framework for training neural network-based state-space models on raw data. The main idea is to use the ELBO in the constrained optimization (CO) framework as a reconstruction term in CO. The proposed RNN-based approach is similar to the Kalman-VAE framework, but with a hierarchical prior. The authors compare their proposed framework with several baselines on pendulum data and the reacher environment, and show that the proposed framework outperforms the baselines in terms of predictive modeling and ELBO values. The paper also proposes a Recurrent Latent Variable Model for Sequential Data and a Hierarchical Priors for VAEs for Unsupervised Learning with Disentangled Recognition and Nonlinear Dynamics Model. "
2215,SP:374bfeb067fcea966c97e1721d65cd9d03d26ed3,deep sequential latent variable models COMPARE deep state space models. deep state space models COMPARE deep sequential latent variable models. locally linear mappings CONJUNCTION KVAE variable separation. KVAE variable separation CONJUNCTION locally linear mappings. method USED-FOR empirical priors. method CONJUNCTION model. model CONJUNCTION method. KVAE variable separation PART-OF model. locally linear mappings PART-OF model. images CONJUNCTION joint angles. joint angles CONJUNCTION images. system identification CONJUNCTION prediction accuracy. prediction accuracy CONJUNCTION system identification. MBRL USED-FOR state representations. joint angles FEATURE-OF pendulum and reacher datasets. images FEATURE-OF pendulum and reacher datasets. Method is constrained optimization method. OtherScientificTerm is VHP. ,"This paper proposes a constrained optimization method for learning deep sequential latent variable models, which is in contrast to deep state space models. The proposed method is motivated by empirical priors, and the proposed model combines locally linear mappings and KVAE variable separation. Experiments are conducted on pendulum and reacher datasets with images, joint angles, and system identification and prediction accuracy. MBRL is used to learn the state representations, and VHP is applied to train the model."
2216,SP:15ed638782cc0398df38ec49eed5c5ca9962d3b9,approaches USED-FOR counterfactual images. approaches USED-FOR adversarial - looking approaches. method USED-FOR quality of counterfactuals. Celebi CONJUNCTION ISIC. ISIC CONJUNCTION Celebi. Method is Deep Inversion. ,"This paper proposes approaches to generate counterfactual images that are more robust to adversarial-looking approaches. Specifically, the authors propose Deep Inversion, a method that aims to improve the quality of counterfactuallys. Experiments are conducted on Celebi and ISIC."
2217,SP:15ed638782cc0398df38ec49eed5c5ca9962d3b9,"approach USED-FOR deep model inversion. deep model inversion USED-FOR pre - trained deep classifier. classifier USED-FOR pre - trained deep classifier. deep model inversion USED-FOR counterfactual explanation. perturbation of query image USED-FOR classification decision. deep classifiers USED-FOR image synthesis. image prior USED-FOR Counterfactual image. un - trained UNET - based model CONJUNCTION Fourier mapping. Fourier mapping CONJUNCTION un - trained UNET - based model. Fourier mapping USED-FOR implicit neural representation ( INR ). deep image prior ( DIP ) USED-FOR un - trained UNET - based model. SIREN activation USED-FOR implicit neural representation ( INR ). SIREN activation USED-FOR Fourier mapping. ISO CONJUNCTION LSO. LSO CONJUNCTION ISO. manifold consistency ( MC ) loss USED-FOR model. auxiliary loss predictor function CONJUNCTION radial basis kernel similarity. radial basis kernel similarity CONJUNCTION auxiliary loss predictor function. radial basis kernel similarity USED-FOR DUQ. DEP CONJUNCTION radial basis kernel similarity. radial basis kernel similarity CONJUNCTION DEP. auxiliary loss predictor function USED-FOR DEP. radial basis kernel similarity USED-FOR MC - loss. auxiliary loss predictor function USED-FOR MC - loss. methods USED-FOR epistemic uncertainty. functional consistency loss USED-FOR counterfactual image. functional consistency loss USED-FOR model. progressive optimization USED-FOR model. Method are generative models, and deep classifier. Task are classification outcome, and adversarial attack. Material are unrealistic images, and counterfactual and query images. OtherScientificTerm are pixel - level manipulations, and data manifold. ","This paper presents an approach for deep model inversion to provide a counterfactual explanation for a pre-trained deep classifier trained on the original classifier. This is an important problem for generative models, as deep classifiers are often used for image synthesis, where the classification decision is made based on the perturbation of query image. Counterfactual image is generated using the image prior. The authors propose to use an un-trained UNET-based model with deep image prior (DIP) and a Fourier mapping with SIREN activation to learn an implicit neural representation (INR). The model is trained with manifold consistency (MC) loss, which penalizes pixel-level manipulations. The MC-loss is combined with an auxiliary loss predictor function for DEP and radial basis kernel similarity for DUQ. The proposed model is also trained with progressive optimization, which allows the model to adapt to new data manifold. The experimental results show that the proposed methods are able to reduce epistemic uncertainty, i.e., the classification outcome is not affected by the adversarial attack. The experiments are conducted on unrealistic images, and the results show significant improvements over standard ISO and LSO. The paper also provides a comparison between counterfactually and query images."
2218,SP:15ed638782cc0398df38ec49eed5c5ca9962d3b9,pre - trained classifier USED-FOR counterfactual explanations. deep inversion approach USED-FOR counterfactual explanations. pre - trained classifier USED-FOR deep inversion approach. image priors CONJUNCTION manifold consistency objective. manifold consistency objective CONJUNCTION image priors. progressive optimization strategy USED-FOR counterfactual explanations. manifold consistency objective CONJUNCTION progressive optimization strategy. progressive optimization strategy CONJUNCTION manifold consistency objective. inductive biases USED-FOR counterfactual explanations. inductive biases USED-FOR method. progressive optimization strategy USED-FOR method. progressive optimization strategy HYPONYM-OF inductive biases. image priors HYPONYM-OF inductive biases. manifold consistency objective HYPONYM-OF inductive biases. Material is image. ,"This paper proposes a deep inversion approach to generate counterfactual explanations from a pre-trained classifier. The method uses three inductive biases: image priors, manifold consistency objective, and a progressive optimization strategy to produce counterfactually explanations. The main contribution of this paper is to propose a method that can be applied to any image."
2219,SP:15ed638782cc0398df38ec49eed5c5ca9962d3b9,deep classifier USED-FOR counterfactuals. GAN HYPONYM-OF generator. ,This paper proposes to generate counterfactuals using a deep classifier. The generator is a GAN. The paper is well-written and easy to follow.
2220,SP:e536acfe82bb5e41fa61929d44dad0b8f7c5ab19,"iterative algorithm USED-FOR regions of heterogeneity. iterative algorithm USED-FOR criteria. semi - synthetic dataset CONJUNCTION real clinical data. real clinical data CONJUNCTION semi - synthetic dataset. real clinical data USED-FOR doctor recommendations. real clinical data EVALUATE-FOR algorithm. semi - synthetic dataset EVALUATE-FOR algorithm. iterative algorithm USED-FOR criteria. iterative algorithm USED-FOR criteria. observational data USED-FOR criteria. OtherScientificTerm are causal criteria, and feature space. ","This paper proposes an iterative algorithm to identify regions of heterogeneity in the causal criteria. The proposed algorithm is evaluated on a semi-synthetic dataset and real clinical data for doctor recommendations. The results show that the proposed criteria can be derived from observational data, and that the selected criteria are robust to changes in the feature space."
2221,SP:e536acfe82bb5e41fa61929d44dad0b8f7c5ab19,"iterative algorithm USED-FOR objective function. algorithm COMPARE direct models. direct models COMPARE algorithm. agent identifiers CONJUNCTION observable features. observable features CONJUNCTION agent identifiers. observable features CONJUNCTION TARNet. TARNet CONJUNCTION observable features. direct models COMPARE direct models. direct models COMPARE direct models. direct models CONJUNCTION TARNet. TARNet CONJUNCTION direct models. logistic regressions USED-FOR direct models. algorithm USED-FOR region AUC. observational dataset USED-FOR medical treatment decisions. OtherScientificTerm are binary decision, features, agent disagreement, assignment mechanism of agents, agent assignment, feature space, tuning parameter, generalization bound, recidivism risk predictions, decision rules, and region of disagreement. Task are pretrial release setting, and Estimating Regions of Heterogeneity. Metric are conditional relative agent bias, and measure of disagreement. Method are assignment mechanism of decision makers, and assignment mechanism. Generic are measure, and estimand. ","This paper considers the pretrial release setting where the agent is given a binary decision and the goal is to maximize the conditional relative agent bias. The authors propose an iterative algorithm to estimate the objective function. The algorithm is compared to direct models trained with logistic regressions, direct models with agent identifiers, observable features, and TARNet. The main difference between the two is that the authors consider the assignment mechanism of decision makers, where each agent is assigned to a region of disagreement (i.e., a set of features that are independent of the other agents). The authors show that the region AUC of the proposed algorithm is a function of the agent disagreement, which is defined as the difference between agent assignment and the actual performance of each agent in the feature space.  The authors also provide a generalization bound on the variance of the estimand, which shows that the variance in the measure of disagreement depends on the tuning parameter of the algorithm.  In addition, the authors provide an empirical study on medical treatment decisions on an observational dataset, where they show that their estimand converges linearly with the number of agents.   "
2222,SP:e536acfe82bb5e41fa61929d44dad0b8f7c5ab19,"OtherScientificTerm are covariate space, expected ( potential ) outcome, and legal or medical domain. Generic is method. Material is causal literature. Method is potential outcomes model. ","This paper proposes a method for estimating the expected (potential) outcome of a set of events in a covariate space. The method is motivated by the causal literature, where the potential outcomes model has been developed in the past. The authors argue that this is an important problem in the legal or medical domain. "
2223,SP:e536acfe82bb5e41fa61929d44dad0b8f7c5ab19,"method USED-FOR regions of disagreement. optimization algorithm USED-FOR subgroup. optimization algorithm USED-FOR generalization bounds. OtherScientificTerm is human decision - makers. Generic are model, and task. Material is semi - synthetic and real - world datasets. ","This paper proposes a method to identify regions of disagreement between human decision-makers. The model is trained on a set of data points, where each data point corresponds to a different task. The authors then propose an optimization algorithm to select a subgroup of the data points and derive generalization bounds for this subgroup. Experiments are conducted on semi-synthetic and real-world datasets."
2224,SP:8fa76926a21fb41c5c9fd357246c06a42ae26b9f,TokenGAN USED-FOR unconditional image synthesis. content tokens CONJUNCTION style tokens. style tokens CONJUNCTION content tokens. style tokens USED-FOR token - based generator. latent space USED-FOR style tokens. content tokens USED-FOR token - based generator. Cross - attention USED-FOR styles. Cross - attention USED-FOR content tokens. styles FEATURE-OF content tokens. token - based structures USED-FOR style - based generator. style - based generator USED-FOR model. StyleGAN USED-FOR style - based generator. OtherScientificTerm is style - modulated content tokens. Material is FFHQ and LSUN CHURCH datasets. Method is StyleGAN2. ,"This paper proposes TokenGAN for unconditional image synthesis. The proposed token-based generator combines both content tokens and style tokens from the latent space. Cross-attention is applied to the styles of the content tokens, while style-modulated content tokens are used for the style tokens. The model is based on StyleGAN2, which uses the style-based generators from StyleGAN to generate tokens for the model. The authors demonstrate the effectiveness of the proposed model on FFHQ and LSUN CHURCH datasets."
2225,SP:8fa76926a21fb41c5c9fd357246c06a42ae26b9f,"cross - attention layers COMPARE AdaIN. AdaIN COMPARE cross - attention layers. generator CONJUNCTION convolution - free architecture. convolution - free architecture CONJUNCTION generator. cross - attention layers USED-FOR style - content modulation. granularities FEATURE-OF decoupled control of style and content. approach COMPARE StyleGAN2. StyleGAN2 COMPARE approach. FFHQ CONJUNCTION LSUN - Churches. LSUN - Churches CONJUNCTION FFHQ. StyleGAN2 USED-FOR unconditional image synthesis. FID scores EVALUATE-FOR StyleGAN2. approach USED-FOR unconditional image synthesis. FFHQ USED-FOR unconditional image synthesis. LSUN - Churches USED-FOR unconditional image synthesis. FID scores EVALUATE-FOR approach. Method are ubiquitous StyleGAN model, and StyleGAN. ","This paper proposes a generalization of the ubiquitous StyleGAN model. The key idea is to use cross-attention layers instead of AdaIN for style-content modulation, which allows for decoupled control of style and content with different granularities. This is achieved by combining the generator with a convolution-free architecture. Experiments show that the proposed approach outperforms StyleGAN2 in terms of FID scores for unconditional image synthesis with FFHQ and LSUN-Churches."
2226,SP:8fa76926a21fb41c5c9fd357246c06a42ae26b9f,transformer based model USED-FOR image generation. StyleGAN architecture USED-FOR transformer based model. model USED-FOR image patches. attention mechanism USED-FOR style tokens. style tokens USED-FOR image patches. attention mechanism USED-FOR image patches. precision CONJUNCTION recall. recall CONJUNCTION precision. model COMPARE StyleGAN2. StyleGAN2 COMPARE model. FID CONJUNCTION precision. precision CONJUNCTION FID. precision EVALUATE-FOR StyleGAN2. FID EVALUATE-FOR StyleGAN2. recall EVALUATE-FOR StyleGAN2. precision EVALUATE-FOR model. FID EVALUATE-FOR model. recall EVALUATE-FOR model. FFHQ and LSUN Church datasets EVALUATE-FOR model. ,"This paper proposes a transformer based model based on the StyleGAN architecture for image generation. The proposed model generates image patches using the attention mechanism to generate style tokens. The model is evaluated on FFHQ and LSUN Church datasets and compared to StyleGAN2 in terms of FID, precision, and recall."
2227,SP:8fa76926a21fb41c5c9fd357246c06a42ae26b9f,"transformer USED-FOR image generation. StyleGAN HYPONYM-OF style - based generator. convolutional layers PART-OF generator. Method are convolutional layer, attention, and transformer - liked architecture. ","This paper presents StyleGAN, a style-based generator based on a transformer for image generation. The generator consists of two convolutional layers: the first one is a regularized version of the original transformer, and the second one is an attention-based version of a transformer-like architecture. The main idea is to add a convolution layer to each layer of the generator to encourage the output to be similar to the input. "
2228,SP:fa34d40d07c0f154a69841b241a2743fe721f95c,regularization USED-FOR robust population risk. regularization USED-FOR overparameterization regime. linear regression CONJUNCTION logistic regression. logistic regression CONJUNCTION linear regression. sufficient statistical settings USED-FOR logistic regression. sufficient statistical settings USED-FOR linear regression. regularization COMPARE population test risk. population test risk COMPARE regularization. regularization USED-FOR population robust risk. OtherScientificTerm is noiseless regime. ,"This paper studies the effect of regularization on robust population risk in the overparameterization regime. The authors show that under sufficient statistical settings for linear regression and logistic regression, regularization can reduce the population robust risk by a factor of at least 1.5 in the noiseless regime compared to the population test risk."
2229,SP:fa34d40d07c0f154a69841b241a2743fe721f95c,generalization EVALUATE-FOR regularized adversarial linear regression and classification models. regularized adversarial linear regression and classification models USED-FOR Gaussian data. overparameterized setting FEATURE-OF Gaussian data. l^2 weight decay CONJUNCTION l^\infty perturbation. l^\infty perturbation CONJUNCTION l^2 weight decay. generalization EVALUATE-FOR non - zero regularization. OtherScientificTerm is generalization error. ,"This paper studies the generalization of regularized adversarial linear regression and classification models for Gaussian data in the overparameterized setting. The authors show that non-zero regularization does not improve generalization performance, and that l^2 weight decay and l^\infty perturbation do. The paper also provides a theoretical analysis of generalization error."
2230,SP:fa34d40d07c0f154a69841b241a2743fe721f95c,"non - vanishing regularization USED-FOR generalization. non - vanishing regularization USED-FOR overparameterized classification and regression problems. generalization USED-FOR overparameterized classification and regression problems. generalization EVALUATE-FOR models. explicit regularization USED-FOR generalization. OtherScientificTerm are implicit bias effect, and adversarial robust risk. ","This paper studies the effect of non-vanishing regularization on generalization in overparameterized classification and regression problems. The authors show that explicit regularization does not improve generalization, but that the implicit bias effect does. They also show that adversarial robust risk does not help with generalization."
2231,SP:fa34d40d07c0f154a69841b241a2743fe721f95c,"interpolation CONJUNCTION regularization. regularization CONJUNCTION interpolation. regularization USED-FOR robust generalization. interpolation USED-FOR robust generalization. regression CONJUNCTION classification. classification CONJUNCTION regression. squared loss CONJUNCTION classification. classification CONJUNCTION squared loss. log loss USED-FOR classification. squared loss USED-FOR regression. Method are linear model, and regularized estimators. Metric is robust generalization error. OtherScientificTerm is consistent adversarial perturbations. ","This paper studies the relationship between interpolation and regularization in the context of robust generalization. In particular, the authors consider a linear model where the linear model is assumed to be robust to consistent adversarial perturbations. The authors show that for regression, squared loss, classification with log loss, and regression with squared loss and classification with standard regularized estimators, interpolation, regularization, and other forms of regularization can be used to improve the performance of the model. They also show that when interpolation is used together with regularization the robust generalized error decreases linearly with the number of samples. "
2232,SP:09f080f47db81b513af26add851822c5c32bb94e,"global latent vector CONJUNCTION spherical canonical embedding. spherical canonical embedding CONJUNCTION global latent vector. autoencoder architecture USED-FOR point cloud ’s representation. category - specific point clouds USED-FOR It. synthetic ShapeNet data EVALUATE-FOR method. OtherScientificTerm is canonical map. Generic are formulation, and it. ","This paper proposes an autoencoder architecture that learns a point cloud’s representation by combining a global latent vector and a spherical canonical embedding. It is based on category-specific point clouds, where the canonical map is defined as a function of the category. The formulation is well motivated and the method is evaluated on synthetic ShapeNet data, where it achieves state-of-the-art results."
2233,SP:09f080f47db81b513af26add851822c5c32bb94e,solution USED-FOR dense correspondence between un - aligned 3D shapes. self - supervised manner USED-FOR dense correspondence between un - aligned 3D shapes. auto - encoder USED-FOR mapping. canonical space USED-FOR 3D shapes. 3D semantic keypoint transfer CONJUNCTION part segmentation transfer tasks. part segmentation transfer tasks CONJUNCTION 3D semantic keypoint transfer. approach USED-FOR 3D semantic keypoint transfer. approach USED-FOR part segmentation transfer tasks. OtherScientificTerm is point cloud. ,"This paper proposes a solution to learn dense correspondence between un-aligned 3D shapes in a self-supervised manner. The key idea is to learn a mapping from a point cloud to a canonical space, and then use an auto-encoder to generate the mapping. The proposed approach is applied to 3D semantic keypoint transfer and part segmentation transfer tasks."
2234,SP:09f080f47db81b513af26add851822c5c32bb94e,end - to - end learnable method USED-FOR dense 3D point correspondences of point clouds. canonical spherical UV space USED-FOR point clouds. decoder USED-FOR point cloud. instance - based feature vector USED-FOR point cloud. instance - based feature vector USED-FOR decoder. instance - based feature vector CONJUNCTION UV coordinates. UV coordinates CONJUNCTION instance - based feature vector. UV coordinates USED-FOR correspondence information. ,This paper proposes an end-to-end learnable method for learning dense 3D point correspondences of point clouds in canonical spherical UV space. The main idea is to use an instance-based feature vector for each point cloud and UV coordinates for the decoder to reconstruct the point cloud. The correspondence information is extracted from UV coordinates.
2235,SP:09f080f47db81b513af26add851822c5c32bb94e,"3D sphere USED-FOR canonical space. method COMPARE ones. ones COMPARE method. Method is canonical point autoencoder. OtherScientificTerm are 3D shapes, template, and non genus 0 shapes. ","This paper proposes a canonical point autoencoder. The canonical space is represented as a 3D sphere, and the goal is to generate 3D shapes that are orthogonal to the template. The method is compared to existing ones, and is shown to be able to generate non genus 0 shapes."
2236,SP:8f28988012f8dca74c90316f7feeda15d49af2c5,Stochastic Weighting Averaging ( SWA ) USED-FOR Domain Generalization ( DG ). flat minima USED-FOR domain generalization. DomainBed benchmark EVALUATE-FOR algorithm. Method is SWA. ,"This paper proposes Stochastic Weighting Averaging (SWA) for Domain Generalization (DG) based on flat minima. The main contribution of this paper is to propose SWA, which is based on the observation that the domain generalization can be achieved by finding the flat minimum of a set of points. The proposed algorithm is evaluated on the DomainBed benchmark."
2237,SP:8f28988012f8dca74c90316f7feeda15d49af2c5,model USED-FOR domain generalization problems. weight averaging ( WA ) USED-FOR flat minima. generalization bounds USED-FOR method. benchmark datasets EVALUATE-FOR method. OtherScientificTerm is loss landscape. Task is model generalization. ,"This paper studies the problem of learning a model for domain generalization problems. In particular, the authors study the loss landscape and propose to use weight averaging (WA) to find flat minima. The authors provide generalization bounds for the proposed method and evaluate the method on several benchmark datasets. The paper is well-written and well-motivated, and the paper is clearly written. However, the paper suffers from a lack of clarity on the underlying problem of model generalization."
2238,SP:8f28988012f8dca74c90316f7feeda15d49af2c5,modification USED-FOR stochastic weight averaging ( SWA ) method. seeking flat minima USED-FOR generalization. SWAD COMPARE prior methods. prior methods COMPARE SWAD. DG testbeds EVALUATE-FOR prior methods. SWAD CONJUNCTION prior methods. prior methods CONJUNCTION SWAD. CORAL HYPONYM-OF SWAD. CORAL HYPONYM-OF prior methods. Task is domain generalization ( DG ) problem setting. Generic is method. Method is SWA. OtherScientificTerm is overfit parameters. ,"This paper proposes a modification to the standard stochastic weight averaging (SWA) method in the domain generalization (DG) problem setting. The proposed method, called SWAD, is motivated by the observation that seeking flat minima can lead to better generalization. The authors show empirically that SWAD outperforms prior methods such as CORAL in terms of performance on DG testbeds when the overfit parameters are small."
2239,SP:8f28988012f8dca74c90316f7feeda15d49af2c5,training methodology USED-FOR domain generalization. SWAD USED-FOR domain generalization. SWAD HYPONYM-OF training methodology. domain generalization EVALUATE-FOR flat solutions. robust risk minimization FEATURE-OF domain generalization. approach USED-FOR flat solutions. Task is distribution generalization. Generic is method. ,"This paper proposes a new training methodology, SWAD, for domain generalization with robust risk minimization. The proposed approach is motivated by the observation that flat solutions have poor performance in terms of generalization to new domains, which is an important problem in distribution generalization. To address this problem, the authors propose a method called SWAD."
2240,SP:5068e491ee0ae7282cd98ef966b471389e2ab069,"model based predictors CONJUNCTION learning curve based predictor. learning curve based predictor CONJUNCTION model based predictors. learning curve based predictor CONJUNCTION zero nas predictors. zero nas predictors CONJUNCTION learning curve based predictor. model based predictors HYPONYM-OF performance predictors. learning curve based predictor HYPONYM-OF performance predictors. DARTS CONJUNCTION NASBench201. NASBench201 CONJUNCTION DARTS. NAS - Bench101 CONJUNCTION DARTS. DARTS CONJUNCTION NAS - Bench101. NASBench201 CONJUNCTION NLP. NLP CONJUNCTION NASBench201. benchmarks EVALUATE-FOR predictors. NAS - Bench101 HYPONYM-OF benchmarks. NASBench201 HYPONYM-OF benchmarks. NLP HYPONYM-OF benchmarks. DARTS HYPONYM-OF benchmarks. NGBOOST CONJUNCTION SemiNAS predictors. SemiNAS predictors CONJUNCTION NGBOOST. SOTL - E and jacobian variance USED-FOR features. SOTL - E and jacobian variance USED-FOR NGBOOST. features PART-OF NGBOOST. Generic are it, and them. ","This paper proposes a new class of performance predictors called NGBOOST, which combines model based predictors, learning curve based predictor, and zero nas predictors. The authors evaluate these predictors on several benchmarks (NAS-Bench101, DARTS, NASBench201 and NLP) and show that it outperforms all of them. The main contribution of the paper is that the authors propose to combine the features of NGBOST with SemiNAS predictors by using the SOTL-E and jacobian variance. "
2241,SP:5068e491ee0ae7282cd98ef966b471389e2ab069,Neural Architecture Search ( NAS ) USED-FOR network architectures. human - defined search space FEATURE-OF network architectures. heuristics USED-FOR candidate architectures. search space FEATURE-OF candidate architectures. learning curve extrapolation CONJUNCTION supervised learning. supervised learning CONJUNCTION learning curve extrapolation. supervised learning CONJUNCTION one - shot models. one - shot models CONJUNCTION supervised learning. one - shot models CONJUNCTION zero - shot metrics. zero - shot metrics CONJUNCTION one - shot models. early stopping CONJUNCTION learning curve extrapolation. learning curve extrapolation CONJUNCTION early stopping. zero - shot metrics HYPONYM-OF techniques. one - shot models HYPONYM-OF techniques. supervised learning HYPONYM-OF techniques. early stopping HYPONYM-OF techniques. learning curve extrapolation HYPONYM-OF techniques. techniques USED-FOR compute budgets. supervised learning CONJUNCTION zero - shot metrics. zero - shot metrics CONJUNCTION supervised learning. zero - shot metrics HYPONYM-OF techniques. supervised learning HYPONYM-OF techniques. NASBench-201 CONJUNCTION NASBench-301 / DARTS search space. NASBench-301 / DARTS search space CONJUNCTION NASBench-201. NASBench-101 CONJUNCTION NASBench-201. NASBench-201 CONJUNCTION NASBench-101. NASBench-301 / DARTS search space CONJUNCTION NASBench - NLP. NASBench - NLP CONJUNCTION NASBench-301 / DARTS search space. Metric is size / accuracy tradeoffs. ,"Neural Architecture Search (NAS) aims to find network architectures in a human-defined search space. The paper proposes a set of heuristics to find candidate architectures in the search space that maximize the efficiency of the search while minimizing the size/accuracy tradeoffs. The proposed techniques include early stopping, learning curve extrapolation, supervised learning, one-shot models, zero-shot metrics, etc. These techniques can be used to reduce the compute budgets while still achieving state-of-the-art performance. Experiments are conducted on NASBench-101, NASBench201 and NASBench301/DARTS search space as well as NASBench - NLP."
2242,SP:5068e491ee0ae7282cd98ef966b471389e2ab069,predictive power EVALUATE-FOR predictors. Material is NAS. ,"This paper studies the predictive power of different predictors in the context of NAS. In particular, the authors show that the predictors with the highest predictive power are the ones with the lowest variance. The authors also provide some theoretical analysis to support their findings."
2243,SP:5068e491ee0ae7282cd98ef966b471389e2ab069,"techniques USED-FOR predicting neural network architecture. techniques USED-FOR task. task EVALUATE-FOR predicting neural network architecture. initialization time CONJUNCTION query time. query time CONJUNCTION initialization time. OMNI USED-FOR NAS. method USED-FOR NAS. OMNI HYPONYM-OF method. Generic are technique, approaches, framework, and model. OtherScientificTerm is weight - sharing. ","This paper proposes techniques for predicting neural network architecture on a new task. The technique, called weight-sharing, is motivated by the observation that existing approaches do not take into account the differences between initialization time and query time. The authors propose a framework to address this issue and show that the proposed model, OMNI, can be used in NAS."
2244,SP:c883fe9c7f4a5f950340ac79b6d7194278b1a1eb,Dirichlet sampling CONJUNCTION private normalized histogram publishing. private normalized histogram publishing CONJUNCTION Dirichlet sampling. differential privacy USED-FOR tasks. private normalized histogram publishing HYPONYM-OF tasks. Dirichlet sampling HYPONYM-OF tasks. OtherScientificTerm is Dirichlet posterior. Metric is privacy and utility guarantees. Method is Gaussian mechanism. ,"This paper studies the problem of differential privacy for two tasks: Dirichlet sampling and private normalized histogram publishing. The main contribution of this paper is to provide privacy and utility guarantees for both cases. In particular, the authors show that for the case where the data is generated by a Gaussian mechanism, the privacy guarantees can be improved to the case when the data comes from the Dirichlett posterior."
2245,SP:c883fe9c7f4a5f950340ac79b6d7194278b1a1eb,method USED-FOR releasing differentially private samples. Dirichlet posterior distribution USED-FOR releasing differentially private samples. privacy analysis USED-FOR method. Gaussian mechanism USED-FOR method. analytical utility guarantees USED-FOR method. OtherScientificTerm is differentially private samples. ,This paper proposes a method for releasing differentially private samples from a Dirichlet posterior distribution based on privacy analysis. The proposed method is based on a Gaussian mechanism and provides analytical utility guarantees.
2246,SP:c883fe9c7f4a5f950340ac79b6d7194278b1a1eb,posterior sampling mechanism USED-FOR Dirichlet distribution. posterior sampling mechanism USED-FOR approximate DP privacy. It USED-FOR histogram release. OtherScientificTerm is alpha parameters. ,"This paper proposes a posterior sampling mechanism for the Dirichlet distribution for approximate DP privacy. It can be used for histogram release, where alpha parameters are not available."
2247,SP:c883fe9c7f4a5f950340ac79b6d7194278b1a1eb,"truncated concentrated Differential Privacy(tCDP ) FEATURE-OF Dirichlet posterior sampling. tCDP CONJUNCTION approximate DP. approximate DP CONJUNCTION tCDP. approximate differential privacy FEATURE-OF Dirichlet Posterior sampling. privacy preserving properties FEATURE-OF posterior sampling. Dirichlet posterior sampling COMPARE Gaussian mechanism. Gaussian mechanism COMPARE Dirichlet posterior sampling. Gaussian mechanism USED-FOR small data regime. Dirichlet posterior sampling USED-FOR small data regime. sample complexity EVALUATE-FOR Gaussian mechanism. OtherScientificTerm are Dirichlet posterior, prior, sample complexity bounds, and normalized histogram. ","This paper studies the problem of Dirichlet posterior sampling with truncated concentrated Differential Privacy(tCDP) and approximate differential privacy (approximate DP). In particular, the authors study the privacy preserving properties of posterior sampling in the setting where the data is sampled from the Dirichlett posterior and the prior is Gaussian. The authors provide sample complexity bounds for tCDP and approximate DP and show that for small data regime, the sample complexity of the proposed Dirichle posterior sampling is asymptotically smaller than the Gaussian mechanism with sample complexity $O(\sqrt{T})$. The authors also provide a theoretical analysis of the normalized histogram of the posterior sampling."
2248,SP:aaea75b9c614f77e8025922780f9a8dd9c9d4aab,"random walks USED-FOR Massively Parallel Computation ( MPC ) model. logarithmic number of rounds USED-FOR random walks. O(log \ell ) rounds USED-FOR MPC algorithm. O(log \ell ) rounds USED-FOR stitching procedure. O(mB ) total memory USED-FOR algorithm. total memory USED-FOR MPC algorithm. OtherScientificTerm are undirected graph, and walks. Method is stitching. ","This paper proposes to use random walks in the Massively Parallel Computation (MPC) model with a logarithmic number of rounds. The random walks are constructed by stitching an undirected graph, and the MPC algorithm uses O(log \ell) rounds to perform the stitching procedure. The algorithm is trained with O(mB) total memory, which is much smaller than the total memory of the original MPC. The paper also provides a theoretical analysis of the effect of the number of walks."
2249,SP:aaea75b9c614f77e8025922780f9a8dd9c9d4aab,algorithm USED-FOR independent random walks. algorithm USED-FOR MPC model. algorithm USED-FOR random walks. Method is Massively Parallel Computation. OtherScientificTerm is stationary distribution. Generic is system. ,This paper studies the problem of Massively Parallel Computation. The authors propose an algorithm for learning independent random walks in an MPC model. The algorithm is based on the idea that the stationary distribution of the system can be viewed as a function of the number of samples. The paper then proposes an algorithm to learn random walks that are independent of each other.
2250,SP:aaea75b9c614f77e8025922780f9a8dd9c9d4aab,application USED-FOR local graph clustering. memory requirements FEATURE-OF SOTA methods. algorithm COMPARE SOTA methods. SOTA methods COMPARE algorithm. memory requirements EVALUATE-FOR algorithm. Method is MPC model. ,This paper proposes a new application for local graph clustering. The proposed algorithm is based on the MPC model and achieves better performance than SOTA methods with higher memory requirements.
2251,SP:aaea75b9c614f77e8025922780f9a8dd9c9d4aab,parallel rounds FEATURE-OF algorithm. personalized PageRank estimation CONJUNCTION local clustering. local clustering CONJUNCTION personalized PageRank estimation. local clustering HYPONYM-OF applications. personalized PageRank estimation HYPONYM-OF applications. Task is single - source and subset random walk computations. OtherScientificTerm is random walk. ,"This paper studies the problem of single-source and subset random walk computations. The authors propose an algorithm with parallel rounds, which can be used in two applications: personalized PageRank estimation and local clustering. The main contribution of this paper is to provide a theoretical analysis of the random walk."
2252,SP:5739081ab7aaf71d389705c28f14a316fbb0a728,"objective function PART-OF M - estimator family. L1 - LinR HYPONYM-OF objective function. L1 - LogR ( pseudolikelihood ) CONJUNCTION Interaction Screening. Interaction Screening CONJUNCTION L1 - LogR ( pseudolikelihood ). quadratic data - dependent loss CONJUNCTION L1 regularization term. L1 regularization term CONJUNCTION quadratic data - dependent loss. mismatched objective COMPARE L1 - LogR ( pseudolikelihood ). L1 - LogR ( pseudolikelihood ) COMPARE mismatched objective. mismatched objective FEATURE-OF it. linear regression USED-FOR quadratic data - dependent loss. quadratic data - dependent loss PART-OF It. L1 regularization term PART-OF It. Task is Ising model selection problem. OtherScientificTerm are interaction network, and Ising configurations. ","This paper studies the Ising model selection problem. The authors propose a new objective function in the M-estimator family called L1-LinR. It consists of a quadratic data-dependent loss based on linear regression, and an L1 regularization term that encourages the interaction network to be similar to the original Ising configurations. Theoretically, the authors show that the mismatched objective of the proposed L1 - LinR is better than the original L1 LogR (pseudolikelihood) and Interaction Screening."
2253,SP:5739081ab7aaf71d389705c28f14a316fbb0a728,graph structure FEATURE-OF Ising model. random regular graphs FEATURE-OF Ising models. paramagnetic phase FEATURE-OF random regular graphs. statistical physics USED-FOR estimator. OtherScientificTerm is model selection consistency. ,This paper studies the graph structure of the Ising model. The authors show that Ising models with random regular graphs in the paramagnetic phase exhibit the same paramagnetism as random regular graph with different graph structure. They then propose a new estimator based on statistical physics and show that this estimator can be used to improve model selection consistency.
2254,SP:5739081ab7aaf71d389705c28f14a316fbb0a728,"precision CONJUNCTION recall rates. recall rates CONJUNCTION precision. recall rates EVALUATE-FOR theory. precision EVALUATE-FOR theory. theory USED-FOR graphs. Task is Ising model selection problem. Method are replica method, and $ \ell_1$-regularized linear regression. Metric is sample complexity. Generic is estimators. ","This paper studies the Ising model selection problem, where the goal is to minimize the sample complexity. The authors propose a replica method, which is a variant of the $\ell_1$-regularized linear regression. Theoretical results on the precision and recall rates of the proposed theory are provided for graphs, as well as for graphs with different sizes. The results show that the proposed estimators converge to $\ell_{1/\epsilon}$."
2255,SP:5739081ab7aaf71d389705c28f14a316fbb0a728,"Markov random field FEATURE-OF edge couplings. ell - one regularized least - squares estimation USED-FOR edge couplings. degree CONJUNCTION coupling strength. coupling strength CONJUNCTION degree. replica method USED-FOR couple equations of state ( EOS ). precision CONJUNCTION recall. recall CONJUNCTION precision. degree CONJUNCTION coupling strength. coupling strength CONJUNCTION degree. observed data USED-FOR EOS. Task are estimating the edge couplings, reconstruction, and exact recovery. Generic are model, equations, and expressions. OtherScientificTerm is MRF. Method is numerical simulations. ","This paper considers estimating the edge couplings in a Markov random field using ell-one regularized least-squares estimation. The authors propose a replica method for solving couple equations of state (EOS) where the model is parameterized by the degree, the coupling strength, and the MRF. These expressions are then used to estimate the precision and recall of the EOS from observed data. The reconstruction is done by minimizing the MRFs of the two equations, and exact recovery is performed using numerical simulations."
2256,SP:9d6202ab0010166f383d6d064aebe02ae97a1dfc,"cluster membership oracle USED-FOR fuzzy k - means problem. uniform random sample USED-FOR algorithm. it USED-FOR clusters. structural assumption USED-FOR it. method USED-FOR clustering. method USED-FOR algorithm. sequential algorithm USED-FOR algorithm. OtherScientificTerm are cluster membership vector, membership queries, cluster memberships, cluster centers, and cluster. Method is polynomial time algorithms. Generic are algorithms, and It. ","This paper studies the cluster membership oracle for the fuzzy k-means problem, where the goal is to find a cluster membership vector that minimizes the variance of the membership queries. The authors consider polynomial time algorithms, where each algorithm takes a uniform random sample and uses it to find clusters based on a structural assumption. The algorithm can be viewed as a sequential algorithm where the algorithm is trained using a method for clustering. It is assumed that the cluster memberships are independent of the cluster centers, and that each cluster is independent of each other. "
2257,SP:9d6202ab0010166f383d6d064aebe02ae97a1dfc,similarity oracle USED-FOR hidden fuzzy k - mean clustering. algorithm USED-FOR hidden clustering. Method is fuzzy k - means clustering. ,"This paper proposes a similarity oracle for the hidden fuzzy k-mean clustering. The main idea is to use a similarity oracle to estimate the likelihood of a given sample. The algorithm is applied to the problem of hidden clustering, where the goal is to find the closest sample to the true label. The paper is well-written and well-motivated. "
2258,SP:9d6202ab0010166f383d6d064aebe02ae97a1dfc,algorithm USED-FOR soft k - means problem. similarity queries USED-FOR algorithm. k - means problem USED-FOR soft k - means problem. centres CONJUNCTION membership weights. membership weights CONJUNCTION centres. Method is clustering algorithm. Task is k - means / median problems. ,This paper proposes a clustering algorithm. The proposed algorithm uses similarity queries to solve the soft k-means problem of the k-midsimple problem. The main idea is to use centres and membership weights as the weights of the centres. This is similar to previous work on k-mean/median problems.
2259,SP:9d6202ab0010166f383d6d064aebe02ae97a1dfc,"Same - Cluster Queries USED-FOR Clustering. similarity queries CONJUNCTION membership queries. membership queries CONJUNCTION similarity queries. performance metric EVALUATE-FOR fuzzy clustering. benchmark EVALUATE-FOR fuzzy clustering solution. performance metric EVALUATE-FOR proximity. fuzzy clustering COMPARE clustering. clustering COMPARE fuzzy clustering. similarity queries USED-FOR fuzzy clustering algorithm. OtherScientificTerm are same - cluster queries, and membership vectors. ",This paper proposes a new fuzzy clustering algorithm based on similarity queries and membership queries. Clustering with Same-Cluster Queries is based on the idea of using same-cluster queries. The authors propose a new benchmark to evaluate the performance of the proposed fuzzy clusterering solution and show that the performance metric for proximity is a better performance metric compared to the traditional clustering. They also propose to use membership vectors to compare the similarity of the similarity queries of different clusters.
2260,SP:a8057c4708dceb4f934e449080043037a70fabf7,value functions CONJUNCTION approximate   ( learned ) models. approximate   ( learned ) models CONJUNCTION value functions. approximate model USED-FOR synthetic experience. synthetic experience USED-FOR model - based learning algorithm. Method is self - consistent reinforcement learning. ,This paper proposes a model-based learning algorithm that uses synthetic experience from an approximate model to improve the performance of self-consistent reinforcement learning. The idea is to combine value functions and approximate  (learned) models. The paper is well-written and easy to follow.
2261,SP:a8057c4708dceb4f934e449080043037a70fabf7,loss USED-FOR model. model CONJUNCTION value function. value function CONJUNCTION model. model COMPARE value function. value function COMPARE model. sample efficiency EVALUATE-FOR self - consistency loss. Task is model - based reinforcement learning. ,This paper proposes a self-consistency loss to improve sample efficiency in model-based reinforcement learning. The authors argue that this loss can be used to improve the model and the value function.
2262,SP:a8057c4708dceb4f934e449080043037a70fabf7,"Bellman operator CONJUNCTION value function. value function CONJUNCTION Bellman operator. value function PART-OF model. L_SC HYPONYM-OF squared TD error. rollouts USED-FOR model. stop - gradient FEATURE-OF squared TD error. rollouts USED-FOR L_SC. model CONJUNCTION value functions. value functions CONJUNCTION model. model and value functions PART-OF MuZero / Muesli. sample efficiency EVALUATE-FOR model - free baseline. sample efficiency EVALUATE-FOR Dyna. L_SC term COMPARE model - free baseline. model - free baseline COMPARE L_SC term. Atari games EVALUATE-FOR model - free baseline. Dyna COMPARE model - free baseline. model - free baseline COMPARE Dyna. L_SC term COMPARE Dyna. Dyna COMPARE L_SC term. L_SC term USED-FOR model and value functions. sample efficiency EVALUATE-FOR L_SC term. Method are model - based RL methods, and Bellman eqn. Task are MBRL, and search control problem. OtherScientificTerm are self - consistency, and self - consistency loss ” ( L_SC ). Generic is loss. ","This paper studies model-based RL methods and proposes a new loss called “self-consistency loss” (L_SC) that aims to improve the model-free performance of MBRL. The proposed loss is motivated by the observation that the squared TD error (i.e., the stop-gradient of squared TD with rollouts) of the original model with L_SC is bounded by the sum of the Bellman operator and the value function of the model. The authors propose to use Bellman eqn as an alternative to the standard loss, and propose to add a term to the loss that encourages the model and value functions of MuZero/Muesli to be close to each other. They show that the proposed “Self-Consistency Loss” term improves the sample efficiency of Dyna on Atari games compared to a model -free baseline. They also propose a new search control problem where the search is performed by minimizing the variance of the loss."
2263,SP:a8057c4708dceb4f934e449080043037a70fabf7,"approach USED-FOR model - based reinforcement learning. approximate model PART-OF ( Dyna ) model - based RL paradigm. ground truth transitions USED-FOR approximate model. update USED-FOR approximate model. update USED-FOR it. imagined ( virtual ) experience USED-FOR it. bellman residual FEATURE-OF approximate model. sample efficiency EVALUATE-FOR method. Generic is model. Method are self - consistency update, and value equivalent updates. OtherScientificTerm are policy, bellman equation, residual, and tabular and function approximate settings. Metric is self - consistency objective. ","This paper presents an approach to model-based reinforcement learning that aims to improve the sample efficiency of model training. The authors propose an approximate model that is part of the (Dyna-Dyna) model - based RL paradigm, where the approximate model is trained using ground truth transitions, and it is updated with an update based on an imagined (virtual) experience. The model is learned using a self-consistency update, where a policy is trained to maximize the residual of the bellman equation, and the residual is updated based on the current state of the model. This is done in tabular and function approximate settings, where value equivalent updates are used. The proposed method is shown to improve sample efficiency by a factor of at least 1.5, and also to be able to achieve better performance in terms of the self-constraint objective."
2264,SP:cd01ab8f03cfb1bb067478ca82944d3a42826ca4,episode difficulty CONJUNCTION sampling schemes. sampling schemes CONJUNCTION episode difficulty. sampling schemes USED-FOR few - shot classification algorithms. episode difficulty USED-FOR few - shot classification algorithms. importance sampling USED-FOR sampling strategies. negative log predictive likelihood FEATURE-OF few - shot learner. negative log predictive likelihood USED-FOR difficulty. architectures CONJUNCTION learning algorithms. learning algorithms CONJUNCTION architectures. uniform sampling scheme USED-FOR difficulty. online scheme USED-FOR importance weights. OtherScientificTerm is episodic difficulty. ,"This paper studies the effect of episode difficulty and sampling schemes on the performance of few-shot classification algorithms with respect to episode difficulty. The authors propose two sampling strategies based on importance sampling. First, the difficulty is defined as the negative log predictive likelihood of the few - shot learner. Second, the authors propose a uniform sampling scheme for the difficulty, where the episodic difficulty is determined by the number of episodes. The importance weights are learned using an online scheme. Experiments are conducted on different architectures and learning algorithms."
2265,SP:cd01ab8f03cfb1bb067478ca82944d3a42826ca4,episode / task difficulty FEATURE-OF few - shot learning. negative log - likelihood USED-FOR task difficulty. normal distribution FEATURE-OF episode difficulty. importance sampling USED-FOR method. method COMPARE sampling procedures. sampling procedures COMPARE method. OtherScientificTerm is model parameters. Method is UNIFORM. ,"This paper studies the problem of episode/task difficulty in few-shot learning. The authors propose to use negative log-likelihood to measure the task difficulty, and show that the episode difficulty follows a normal distribution. The proposed method, UNIFORM, is based on importance sampling, where the model parameters are sampled based on the importance of each episode. The method is compared to other sampling procedures."
2266,SP:cd01ab8f03cfb1bb067478ca82944d3a42826ca4,"episode - sampling strategies USED-FOR meta - training. episode - sampling strategies USED-FOR importance sampling. sampling USED-FOR target distribution. episodic sampling strategies COMPARE random episodic sampling. random episodic sampling COMPARE episodic sampling strategies. random episodic sampling HYPONYM-OF dominant strategy. Method are episodic sampling, and meta - learning models. OtherScientificTerm are episodic difficulty, model parameters, and episode loss. ","This paper studies episodic sampling in meta-training, where the goal is to reduce the episodic difficulty of meta-learning models. In particular, the authors study episode-sampling strategies for importance sampling, which is a popular technique in the context of meta -training. The main idea is to use sampling to approximate the target distribution of an episode, and then use the learned model parameters to estimate the episode loss. The authors show empirically that the proposed episodes of importance sampling are more effective than the random episodic (i.e., the dominant strategy) sampling, and that episodic learning models are more robust to episodic variation."
2267,SP:cd01ab8f03cfb1bb067478ca82944d3a42826ca4,"meta - learning framework USED-FOR task sampling probabilities. OtherScientificTerm are negative log - likelihood, normal distribution, target sampling distribution, and uniform target distribution. Generic is difficulties. Method is learning framework. ","This paper proposes a meta-learning framework for learning task sampling probabilities. The main idea is to learn a negative log-likelihood of the target sampling distribution, which is a normal distribution. The authors show that this can lead to difficulties, and propose a learning framework that learns a uniform target distribution."
2268,SP:b31b1ee7067d4da70916986ba13e80bb14e2fdfe,"multinomial logistic ( MNL ) regression bandits HYPONYM-OF binary logistic bandits. It COMPARE MNL bandits. MNL bandits COMPARE It. MNL bandits USED-FOR combinatorial action selection. UCB - based algorithm USED-FOR problem. sub - linear regret bound USED-FOR UCB - based algorithm. algorithm CONJUNCTION theoretical guarantees. theoretical guarantees CONJUNCTION algorithm. OtherScientificTerm are decision - making process, and constant \kappa. Method is logistic bandits. ","This paper studies binary logistic bandits, specifically multinomial logistic (MNL) regression bandits. It is a generalization of MNL bandits for combinatorial action selection, where the decision-making process is linear in the constant \kappa. The authors propose a UCB-based algorithm for this problem with a sub-linear regret bound. The proposed algorithm and its theoretical guarantees are well-motivated and interesting. However, there is a lot of room for improvement in this paper, especially in terms of the comparison to other logistic bands."
2269,SP:b31b1ee7067d4da70916986ba13e80bb14e2fdfe,"MNL - UCB Algorithm USED-FOR contextual MNL bandits problem. degree of ( non)-smoothness FEATURE-OF MNL model. regret EVALUATE-FOR MNL - UCB. regret bound EVALUATE-FOR algorithm. OtherScientificTerm are confidence region, problem - dependent constant \kappa, and second order term. Generic is It. ","This paper proposes a new MNL-UCB Algorithm for contextual MNL bandits problem. It is motivated by the observation that the degree of (non)-smoothness of the MNL model depends on the confidence region. The authors propose a new problem-dependent constant \kappa, which is a second order term. The regret bound of the proposed algorithm is proved."
2270,SP:b31b1ee7067d4da70916986ba13e80bb14e2fdfe,"discrete distribution USED-FOR rewards. multionimal logit model USED-FOR discrete distribution. UCB algorithm USED-FOR problem. that USED-FOR regret analysis. regret EVALUATE-FOR algorithm. smoothness FEATURE-OF logit function. lower order terms FEATURE-OF algorithm. exponential order dependence FEATURE-OF algorithms. algorithms USED-FOR binary ) logistic bandit setting. multinomial bandit COMPARE combinatorial problem. combinatorial problem COMPARE multinomial bandit. dynamic assortment selection literature FEATURE-OF combinatorial problem. Method is stochastic contextual bandits. Generic are setting, and approach. Task is online advertising. OtherScientificTerm are confidence sets, and ICML. ","This paper studies stochastic contextual bandits, where the rewards are drawn from a discrete distribution over a multionimal logit model. In this setting, the authors propose a UCB algorithm to solve the problem, and provide a regret analysis of the regret of the algorithm in terms of the smoothness of the logit function. In particular, they show that the algorithm with lower order terms has exponential order dependence on the confidence sets, which is in contrast to previous algorithms in the (binary) logistic bandit setting. The authors also show that their approach can be applied to online advertising, where they compare their multinomial bandit to a combinatorial problem from the dynamic assortment selection literature, which they call ICML."
2271,SP:b31b1ee7067d4da70916986ba13e80bb14e2fdfe,"UCB based algorithm USED-FOR cumulative regret. real - life motivated scenarios FEATURE-OF problem setup. degree of non - smoothness FEATURE-OF multi - nomial problem. \kappa CONJUNCTION degree of non - smoothness. degree of non - smoothness CONJUNCTION \kappa. Task are multinomial logistic regression bandit problem, and generalized linear bandit problem. OtherScientificTerm is action space. Method is simulation based study. ","This paper studies the multinomial logistic regression bandit problem. The authors propose an UCB based algorithm to minimize cumulative regret. The problem setup is based on real-life motivated scenarios, where the action space is multi-nomial. The main contribution of this paper is to study the relationship between \kappa and the degree of non-smoothness of the multi-nominial problem. This is a generalization of the work of [1], which studies the generalized linear bandit problems. The paper is well-written and easy to follow. The experimental results are promising, and the simulation based study is interesting."
2272,SP:0eaf058ed224464f6682cbbd80f716c89759f467,regularization term PART-OF reward function. policy entropy USED-FOR exploration. regularization term USED-FOR policy entropy. regularization term USED-FOR frameworks. objective USED-FOR RL agents. method COMPARE baselines. baselines COMPARE method. RL agents USED-FOR rare states. low entropy FEATURE-OF rare states. Method is max - entropy RL frameworks. OtherScientificTerm is Q - function. ,This paper studies max-entropy RL frameworks and proposes a new regularization term in the reward function. The authors argue that the existing frameworks use a regularized version of the Q-function and propose to add this regularization to the policy entropy in order to encourage exploration. This objective can be used to train RL agents to explore rare states with low entropy. Experiments show that the proposed method outperforms the baselines.
2273,SP:0eaf058ed224464f6682cbbd80f716c89759f467,maximum - entropy principle USED-FOR exploration of this low entropy state. function approximation error CONJUNCTION off - policy learning. off - policy learning CONJUNCTION function approximation error. algorithm USED-FOR reward. negative entropy FEATURE-OF reward. soft - actor - critic algorithm USED-FOR policy improvement. algorithm COMPARE baselines. baselines COMPARE algorithm. algorithm COMPARE SAC. SAC COMPARE algorithm. SAC HYPONYM-OF baselines. OtherScientificTerm is low entropy. Material is 4 - room maze. Metric is empirical entropy. Method is Q function. ,"This paper studies the problem of exploration of this low entropy state based on the maximum-entropy principle. The authors consider a 4-room maze where the reward is a function of the negative entropy of the environment, and the goal is to maximize the empirical entropy. The main contribution of this paper is to study the relationship between the function approximation error and off-policy learning, and to propose a soft-actor-critic algorithm for policy improvement. The proposed algorithm is compared with two baselines: SAC and SAC with a different Q function."
2274,SP:0eaf058ed224464f6682cbbd80f716c89759f467,"positive feedback loop USED-FOR exploration. policy update USED-FOR entropy. Max - Min Entropy Rl USED-FOR unwanted feedback loop. common Maze tasks CONJUNCTION Mujoco tasks. Mujoco tasks CONJUNCTION common Maze tasks. Mujoco tasks EVALUATE-FOR It. common Maze tasks EVALUATE-FOR It. Method is maximum entropy reinforcement learning. OtherScientificTerm are replay buffer, and policy. ","This paper proposes maximum entropy reinforcement learning, where the goal is to learn a positive feedback loop for exploration. The key idea is to use a policy update to maximize the entropy of the learned policy, which is then fed into a replay buffer. The authors propose Max-Min Entropy Rl to avoid unwanted feedback loop. It is evaluated on common Maze tasks and Mujoco tasks."
2275,SP:0eaf058ed224464f6682cbbd80f716c89759f467,"soft actor - critic ( SAC ) algorithm USED-FOR maximum entropy reinforcement learning ( RL ). undesired behavior FEATURE-OF SAC. SAC USED-FOR finite MDP setups. SAC USED-FOR optimal convergence. no - reward environment USED-FOR function approximation. no - reward environment USED-FOR SAC. function approximation USED-FOR SAC. entropy EVALUATE-FOR policy. SAC USED-FOR policy. policy update USED-FOR entropy. function approximation USED-FOR SAC. soft Q - function USED-FOR exploration. entropy term PART-OF Q - function. soft Q - function USED-FOR entropy. sparse mujoco tasks CONJUNCTION delayed mujoco tasks. delayed mujoco tasks CONJUNCTION sparse mujoco tasks. dense rewards FEATURE-OF mujoco environments. delayed mujoco tasks EVALUATE-FOR approach. sparse mujoco tasks EVALUATE-FOR approach. algorithm COMPARE SOTA approaches. SOTA approaches COMPARE algorithm. OtherScientificTerm are Q value, and sparse - reward shapes. Generic is solution. Method is Max - Min Entropy RL ( MME ). ","This paper proposes a soft actor-critic (SAC) algorithm for maximum entropy reinforcement learning (RL). SAC has been shown to exhibit undesired behavior in finite MDP setups, and the authors propose a solution called Max-Min Entropy RL (MME) to address this issue. The key idea of SAC is to use a function approximation in a no-reward environment to achieve optimal convergence. The entropy of the policy learned by SAC can be estimated using a policy update. The authors propose to add an entropy term to the Q-function to encourage exploration. The proposed approach is evaluated on sparse mujoco tasks and delayed mujococo tasks with dense rewards. The experiments show that the proposed algorithm outperforms the SOTA approaches in most cases, especially when the Q value is small and when sparse-rewards shapes are used."
2276,SP:19107a648d3d23403a8693b065ee842833a0b893,"continuous - time markov chains USED-FOR time evolution of discrete sets. approximate formulation USED-FOR likelihood maximization. state sequence recovery EVALUATE-FOR theoretical framework. TCGA FEATURE-OF real world cancer dataset. real world cancer dataset EVALUATE-FOR framework. Generic is problems. Material is cross - sectional data. OtherScientificTerm are underspecification, time order, and copy number aberration events. ","This paper studies the time evolution of discrete sets using continuous-time markov chains. The authors propose an approximate formulation for likelihood maximization, which can be applied to a variety of problems. The theoretical framework is evaluated on state sequence recovery and on TCGA on a real world cancer dataset, where the authors show that the proposed framework is able to recover state sequence from cross-sectional data without underspecification. The paper also shows that the time order can be improved by adding copy number aberration events."
2277,SP:19107a648d3d23403a8693b065ee842833a0b893,"genetic mutations HYPONYM-OF time evolution of discrete states. continuous - time Markov chains USED-FOR time evolution of discrete states. cross - sectional data USED-FOR learning task. approximate likelihood maximization method USED-FOR continuous - time Markov chains. high - dimensional data EVALUATE-FOR methods. synthetic and real cancer data EVALUATE-FOR methods. OtherScientificTerm are time order, and underspecification. ","This paper proposes to learn continuous-time Markov chains for time evolution of discrete states (e.g., genetic mutations) using cross-sectional data. The authors propose an approximate likelihood maximization method to learn the parameters of such continuous-temporally-exponential Markov Chains. The proposed methods are evaluated on both synthetic and real cancer data, and on high-dimensional data. In particular, the authors show that the learned time order is consistent with the underlying learning task, and that there is no underspecification."
2278,SP:19107a648d3d23403a8693b065ee842833a0b893,continuous - time Markov Chain method USED-FOR time evolution of events. Material is single - cell sequencing data sets. ,This paper proposes a continuous-time Markov Chain method to model the time evolution of events. The authors conduct experiments on single-cell sequencing data sets.
2279,SP:19107a648d3d23403a8693b065ee842833a0b893,"pairwise interaction model USED-FOR rates. stochastic approximation USED-FOR likelihood ’s gradient. independent elements USED-FOR posterior distribution of observation time. OtherScientificTerm are time stamps, and binary components. Generic is class. ","This paper proposes a pairwise interaction model to estimate rates for rates that are independent of time stamps. The authors propose a stochastic approximation to the likelihood’s gradient, which consists of two binary components. The posterior distribution of observation time is modeled as a set of independent elements, and the class is defined as a weighted sum of these elements."
2280,SP:68dfc737a8ea591da2c7fe048a5b8995c89e1fec,self - supervised pre - training framework USED-FOR document understanding. UniDoc HYPONYM-OF self - supervised pre - training framework. gated cross attention USED-FOR cross - modal correlation. image feature HYPONYM-OF multimodal data. multimodal data USED-FOR UniDoc. gated cross attention USED-FOR UniDoc. downstream tasks EVALUATE-FOR UniDoc. ,"This paper proposes UniDoc, a self-supervised pre-training framework for document understanding. UniDoc uses multimodal data (e.g., image feature) and uses gated cross attention to encourage cross-modal correlation between documents. Experiments on several downstream tasks demonstrate the effectiveness of UniDoc."
2281,SP:68dfc737a8ea591da2c7fe048a5b8995c89e1fec,unified framework USED-FOR pre - training image / text encoders. unified framework USED-FOR visual and textual features. pre - training image / text encoders USED-FOR visual and textual features. Masked Sentence Modeling CONJUNCTION Visual Contrastive Learning. Visual Contrastive Learning CONJUNCTION Masked Sentence Modeling. Visual Contrastive Learning CONJUNCTION Vision - Language Alignment. Vision - Language Alignment CONJUNCTION Visual Contrastive Learning. pre - training tasks USED-FOR Transformer model. Masked Sentence Modeling HYPONYM-OF pre - training tasks. Vision - Language Alignment HYPONYM-OF pre - training tasks. Visual Contrastive Learning HYPONYM-OF pre - training tasks. multimodal embeddings USED-FOR Transformer model. UniDoc model COMPARE baselines. baselines COMPARE UniDoc model. Method is image / text encoders. OtherScientificTerm is visual & textual features. Generic is pre - trained model. ,"This paper proposes a unified framework for pre-training image/text encoders to learn both visual and textual features simultaneously. The authors propose to pre-train a Transformer model with multimodal embeddings on three pre-trained tasks: Masked Sentence Modeling, Visual Contrastive Learning, and Vision-Language Alignment. They show that the pre-trained model is able to generalize to unseen images and text. They also compare the performance of the UniDoc model with several baselines."
2282,SP:68dfc737a8ea591da2c7fe048a5b8995c89e1fec,multi - modal pretraining method USED-FOR document understanding. hierarchical model architecture CONJUNCTION pretraining tasks. pretraining tasks CONJUNCTION hierarchical model architecture. hierarchical model architecture PART-OF It. pretraining tasks PART-OF It. It COMPARE SOTA methods. SOTA methods COMPARE It. them EVALUATE-FOR It. tasks EVALUATE-FOR It. tasks EVALUATE-FOR SOTA methods. models USED-FOR architecture. Task is multi - modal pretraining tasks. ,This paper proposes a multi-modal pretraining method for document understanding. It consists of a hierarchical model architecture and a set of pretraining tasks. It is evaluated on a variety of tasks and compared to SOTA methods on all of them. The results show that the proposed architecture is able to generalize well to new models. The paper also provides a theoretical analysis of the multi-modesality of the architecture.
2283,SP:68dfc737a8ea591da2c7fe048a5b8995c89e1fec,multimodal pre - trained model USED-FOR document understanding. UniDoc HYPONYM-OF multimodal pre - trained model. sentence embeddings CONJUNCTION region embeddings. region embeddings CONJUNCTION sentence embeddings. fusion layer CONJUNCTION task layer. task layer CONJUNCTION fusion layer. pre - training objectives USED-FOR model parameters. feature extraction layer USED-FOR sentence embeddings. fusion layer CONJUNCTION textual and visual information. textual and visual information CONJUNCTION fusion layer. feature extraction layer CONJUNCTION region embeddings. region embeddings CONJUNCTION feature extraction layer. feature extraction layer CONJUNCTION fusion layer. fusion layer CONJUNCTION feature extraction layer. pre - training objectives USED-FOR task layer. feature extraction layer PART-OF layers. task layer PART-OF layers. fusion layer PART-OF layers. layers PART-OF UniDoc. task layer PART-OF UniDoc. feature extraction layer PART-OF UniDoc. gated cross - attention mechanism USED-FOR fusion layer. fusion layer PART-OF UniDoc. Receipt Understanding CONJUNCTION Document Classification. Document Classification CONJUNCTION Receipt Understanding. Document Classification CONJUNCTION Document Object Detection. Document Object Detection CONJUNCTION Document Classification. Form Understanding CONJUNCTION Receipt Understanding. Receipt Understanding CONJUNCTION Form Understanding. Document Object Detection HYPONYM-OF downstream tasks. Form Understanding HYPONYM-OF downstream tasks. Document Classification HYPONYM-OF downstream tasks. Receipt Understanding HYPONYM-OF downstream tasks. Form Understanding CONJUNCTION Receipt Understanding. Receipt Understanding CONJUNCTION Form Understanding. SOTA baselines COMPARE UniDoc. UniDoc COMPARE SOTA baselines. UniDoc USED-FOR Receipt Understanding. UniDoc USED-FOR Form Understanding. Receipt Understanding CONJUNCTION Document Classification. Document Classification CONJUNCTION Receipt Understanding. Document Classification EVALUATE-FOR UniDoc. pre - training tasks CONJUNCTION visual backbones. visual backbones CONJUNCTION pre - training tasks. V / L / V+L CONJUNCTION pre - training tasks. pre - training tasks CONJUNCTION V /,"This paper presents UniDoc, a multimodal pre-trained model for document understanding. UniDoc consists of three layers: a fusion layer, a task layer, and two pre-training objectives for the model parameters. The fusion layer is trained with a gated cross-attention mechanism, and the feature extraction layer is used to extract sentence embeddings, and region embedding. The authors evaluate UniDoc on four downstream tasks: Form Understanding, Receipt Understanding, Document Classification, and Document Object Detection. Compared to SOTA baselines, UniDoc outperforms both V/L/V+L and V/R/R+L in terms of performance on the pre-train tasks and visual backbones."
2284,SP:de6c4c1a418d1ebadc294d77dda18612c163d9c0,"Generic is model. OtherScientificTerm are open center, clustering objective functions, k - median and k - means objectives, and k - median objective. Method are k - means, and LP - rounding techniques. ","This paper proposes a new model for clustering objective functions. The model is based on the idea that the open center of the data points should be close to the center of an open center. The authors propose to use clustering objectives functions that are a combination of the k-median and k-means objectives. The k-minimizer is a generalization of the classical k-mean objective, and the authors use LP-rounding techniques to approximate the k -means. "
2285,SP:de6c4c1a418d1ebadc294d77dda18612c163d9c0,l_p norm USED-FOR p values. LP rounding USED-FOR algorithm. It USED-FOR bi - criteria approximation guarantee. bi - criteria approximation guarantee FEATURE-OF clustering objective. runtime EVALUATE-FOR algorithm. sparsification USED-FOR runtime. sparsification USED-FOR algorithm. ,"This paper proposes an algorithm based on LP rounding, where the l_p norm is used to estimate the p values. It provides a bi-criteria approximation guarantee for the clustering objective. The runtime of the proposed algorithm is improved by sparsification."
2286,SP:de6c4c1a418d1ebadc294d77dda18612c163d9c0,\ell_p norm CONJUNCTION center - based clustering. center - based clustering CONJUNCTION \ell_p norm. fairness consistent USED-FOR center - based clustering. Linear Programming based ( approximation ) algorithm USED-FOR problem. OtherScientificTerm is fairness constraint. ,This paper studies the problem of fairness consistent in the context of \ell_p norm and center-based clustering. The authors propose a Linear Programming based (approximation) algorithm to solve this problem. The main contribution of this paper is the introduction of the fairness constraint.
2287,SP:de6c4c1a418d1ebadc294d77dda18612c163d9c0,"approximation algorithm USED-FOR individual fair k - clustering. metric space FEATURE-OF approximation algorithm. metric space FEATURE-OF individual fair k - clustering. It USED-FOR p - norm objective. It USED-FOR bi - criterial. optimum solution USED-FOR linear programming relaxation of the problem. solution USED-FOR filter function. Plesnik / Rita USED-FOR filter function. sparsification technique USED-FOR LP - relaxation. running time EVALUATE-FOR algorithm. OtherScientificTerm are individual fairness, and open facility. Generic are function, and approach. Method are filter, and LP - rounding algorithm. ","This paper proposes an approximation algorithm in the metric space of individual fair k-clustering in the context of individual fairness. It is motivated by the fact that the p-norm objective can be viewed as a bi-criterial, and that the optimum solution of the linear programming relaxation of the problem can be expressed as a function of the number of samples. The authors propose to use this solution to approximate a filter function based on Plesnik/Rita, which is a generalization of the LP-rounding algorithm. The paper also proposes a sparsification technique for this LP-relaxation. The proposed approach is based on the idea that the filter function can be decomposed into two parts: (1) an open facility, where the parameters of the filter can be sampled from the open facility and (2) a closed facility. The running time of the proposed algorithm is shown to be O(1/\sqrt{T}) where T is the dimension of the function."
2288,SP:feb4664dfd5066cff582f6b4f9b17c6169049ceb,"sampling CONJUNCTION sparsification. sparsification CONJUNCTION sampling. methods USED-FOR max k - way cuts. sparsification HYPONYM-OF methods. sampling HYPONYM-OF methods. methods USED-FOR methods. clustering instances EVALUATE-FOR method. Method are semi - definite program, and spectral graph sparsifiers. Metric are memory usage, and relative error. ","This paper proposes a semi-definite program to reduce the memory usage of spectral graph sparsifiers. The methods for max k-way cuts are based on methods such as sampling, sparsification, etc. The method is evaluated on clustering instances and the relative error is shown."
2289,SP:feb4664dfd5066cff582f6b4f9b17c6169049ceb,"algorithm USED-FOR MAX-$k$-CUT. sparsification techniques USED-FOR algorithm. O(n\log n)$ space USED-FOR algorithm. relaxation USED-FOR Frank - Wolfe with Gaussian sampling. Frank - Wolfe with Gaussian sampling USED-FOR algorithm. OtherScientificTerm are SDP bound, SDP guarantee, and regularization. Method is SDP formulation. Task is correlation clustering. Generic is bounds. ",This paper proposes an algorithm for MAX-$k$-CUT based on sparsification techniques. The algorithm is based on Frank-Wolfe with Gaussian sampling with a relaxation to the O(n\log n)$ space. The authors prove an SDP bound of $O(\sqrt{n})$ where $n$ is the number of samples in the SDP formulation. The SDP guarantee depends on $n \log n$. The authors also provide bounds for correlation clustering and show that the regularization is not necessary.
2290,SP:feb4664dfd5066cff582f6b4f9b17c6169049ceb,max - K - cut CONJUNCTION max - agree variant   of correlation clustering. max - agree variant   of correlation clustering CONJUNCTION max - K - cut. n^2 $ variables CONJUNCTION n^2 $ constraints. n^2 $ constraints CONJUNCTION n^2 $ variables. methods USED-FOR max - K - cut. n^2 $ variables FEATURE-OF semidefinte programs ( SDPs ). memory USED-FOR semidefinte programs ( SDPs ). it CONJUNCTION graph sparsification. graph sparsification CONJUNCTION it. approach USED-FOR dense graphs cases. Metric is approximation guarantees. Method is polynomial - time Gaussian sampling - based algorithms. ,"This paper proposes methods for optimizing max-K-cut and the max-agree variant  of correlation clustering, i.e., max-k-cut with approximation guarantees. The main idea is to use polynomial-time Gaussian sampling-based algorithms, where the memory for semidefinte programs (SDPs) with n^2$ variables and n^{2$ constraints can be stored in memory. The authors show that their approach can be applied to dense graphs cases, where it can be combined with graph sparsification."
2291,SP:feb4664dfd5066cff582f6b4f9b17c6169049ceb,"polytime sampling - based algorithms USED-FOR problems. spectral sparsification USED-FOR streaming model of computation. OtherScientificTerm are limited memory setting, edge set, dense graphs, and quadratic memory requirements. Metric is approximation guarantees. ","This paper studies the problem of streaming model of computation with spectral sparsification in a limited memory setting. In particular, the authors consider problems with quadratic memory requirements and propose polytime sampling-based algorithms for solving these problems. The main contribution of this paper is to provide approximation guarantees for the edge set. The authors also provide a theoretical analysis of the effect of dense graphs on the performance."
2292,SP:cfd6cf88a823729c281059e179788248238a6ed7,method USED-FOR video prediction. Motion - Aware Unit ( MAU ) USED-FOR method. MAU USED-FOR temporal receptive field. attention mechanism USED-FOR features. attention mechanism USED-FOR temporal receptive field. aggregate features CONJUNCTION content ” features. content ” features CONJUNCTION aggregate features. encoder CONJUNCTION decoder. decoder CONJUNCTION encoder. baselines pixel based metrics CONJUNCTION perceptual metric. perceptual metric CONJUNCTION baselines pixel based metrics. Method is Information recalling scheme. ,This paper proposes a method for video prediction based on Motion-Aware Unit (MAU). The MAU learns a temporal receptive field using an attention mechanism to aggregate features and “content” features. The encoder and decoder are jointly trained. Information recalling scheme is used. The experimental results are compared with baselines pixel based metrics and a perceptual metric.
2293,SP:cfd6cf88a823729c281059e179788248238a6ed7,"Motion - Aware Unit ( MAU ) USED-FOR video prediction. module USED-FOR temporal information. temporal receptive field USED-FOR temporal information. module USED-FOR autoencoder. next frames prediction CONJUNCTION early action recognition. early action recognition CONJUNCTION next frames prediction. benchmarks EVALUATE-FOR early action recognition. early action recognition EVALUATE-FOR method. next frames prediction EVALUATE-FOR method. next frames prediction HYPONYM-OF benchmarks. benchmarks EVALUATE-FOR method. Method are attention mechanism, and MAU layers. OtherScientificTerm is spatial and temporal information. ",This paper proposes a Motion-Aware Unit (MAU) for video prediction. The proposed module encodes temporal information into a temporal receptive field and feeds it to an autoencoder. The attention mechanism is based on the idea that spatial and temporal information should be encoded separately in the MAU layers. The method is evaluated on two benchmarks: next frames prediction and early action recognition.
2294,SP:cfd6cf88a823729c281059e179788248238a6ed7,Motion - Aware Unit ( MAU ) USED-FOR video prediction. attention module CONJUNCTION fusion module. fusion module CONJUNCTION attention module. attention module USED-FOR it. fusion module USED-FOR it. it USED-FOR attention. attentive temporal state CONJUNCTION current spatial state. current spatial state CONJUNCTION attentive temporal state. spatial and temporal update gates USED-FOR attentive temporal state. spatial and temporal update gates USED-FOR current spatial state. ,"This paper proposes a Motion-Aware Unit (MAU) for video prediction. Specifically, it combines an attention module with a fusion module. The attentive temporal state and the current spatial state are updated using spatial and temporal update gates. The fusion module is also trained to combine attention and spatial information."
2295,SP:cfd6cf88a823729c281059e179788248238a6ed7,Motion - Aware Unit ( MAU ) USED-FOR video prediction tasks. attention module CONJUNCTION fusion module. fusion module CONJUNCTION attention module. modules PART-OF MAU. fusion module HYPONYM-OF modules. attention module HYPONYM-OF modules. attention module PART-OF MAU. fusion module PART-OF MAU. attention map USED-FOR motion information ( AMI ). fusion module USED-FOR augmented motion information ( AMI ). Generic is state - of - the - arts. ,"This paper proposes a Motion-Aware Unit (MAU) for video prediction tasks. The MAU consists of two modules: an attention module and a fusion module. The attention map is used to extract motion information (AMI) from the video, while the fusion module is used for augmenting the augmented motion information. The proposed MAU is shown to outperform the state-of-the-arts."
2296,SP:07fa7cd4344d77a6e0f180d4f251cc8356b5202f,two - layer neural network function approximation CONJUNCTION generative model. generative model CONJUNCTION two - layer neural network function approximation. two - layer neural network function approximation USED-FOR sample efficient RL. policy completeness assumption CONJUNCTION Bellman completeness assumption. Bellman completeness assumption CONJUNCTION policy completeness assumption. near - optimal policy USED-FOR stochastic MDPs. poly samples USED-FOR optimal policy. ( non - optimistic ) value regression USED-FOR algorithms. OtherScientificTerm is deterministic MDP. Metric is sample complexity. ,"This paper studies sample efficient RL with a two-layer neural network function approximation and a generative model. The main contributions of the paper are: (1) a policy completeness assumption and the Bellman completeness assumptions, which allow to learn a near-optimal policy for stochastic MDPs, and (2) a deterministic MDP, which allows to learn an optimal policy with poly samples. The algorithms are based on (non-optimistic) value regression, and the sample complexity is shown to be O(1/\sqrt{T})$."
2297,SP:07fa7cd4344d77a6e0f180d4f251cc8356b5202f,1 - hidden layer neural network CONJUNCTION low - rank polynomial function approximation schemes. low - rank polynomial function approximation schemes CONJUNCTION 1 - hidden layer neural network. 1 - hidden layer neural network USED-FOR reinforcement learning. low - rank polynomial function approximation schemes USED-FOR reinforcement learning. realizability CONJUNCTION completness. completness CONJUNCTION realizability. planning PART-OF episodic reinforcement learning. sample complexity EVALUATE-FOR algorithms. sample complexity EVALUATE-FOR baselines. baselines USED-FOR function approximation schemes. They COMPARE baselines. baselines COMPARE They. sample complexity EVALUATE-FOR They. Method is generative model. ,This paper proposes a generative model that combines a 1-hidden layer neural network with low-rank polynomial function approximation schemes for reinforcement learning with the goal of improving realizability and completness. The main contribution of this paper is to study the problem of planning in episodic reinforcement learning. They show that their algorithms have a better sample complexity than baselines for function approximation.
2298,SP:07fa7cd4344d77a6e0f180d4f251cc8356b5202f,neural function approximation USED-FOR RL. two - layer NNs USED-FOR generative model. realizability USED-FOR algorithm. two - layer NNs USED-FOR algorithm. two - layer NNs USED-FOR algorithm. linear methods CONJUNCTION methods. methods CONJUNCTION linear methods. bounded eluder dimension FEATURE-OF methods. Task is linear feature approximation. ,This paper studies the problem of neural function approximation in RL. The authors propose an algorithm based on realizability that uses two-layer NNs to learn a generative model. The main contribution of the paper is the analysis of linear feature approximation and the comparison with linear methods and methods with bounded eluder dimension.
2299,SP:07fa7cd4344d77a6e0f180d4f251cc8356b5202f,sampling efficiency EVALUATE-FOR nonlinear reinforcement learning. sampling efficiency EVALUATE-FOR neural network approximations. neural network approximations USED-FOR nonlinear reinforcement learning. hidden layer PART-OF two - layer fully connected neural network. sampling complexity EVALUATE-FOR It. ,This paper aims to improve the sampling efficiency of neural network approximations for nonlinear reinforcement learning. The authors propose a two-layer fully connected neural network with one hidden layer. It is shown to reduce the sampling complexity by an order of magnitude.
2300,SP:cac881243abde92a28c110f5bd84d115ed189bda,"training and testing data splits USED-FOR deep metric learning ( DML ). ResNet-50 USED-FOR FID. benchmarks CONJUNCTION Recall@k metric. Recall@k metric CONJUNCTION benchmarks. train / test split USED-FOR generalization. OtherScientificTerm are train / test splits, train and test distributions, and train / test FID distances. Task is DML generalization. ","This paper studies the problem of deep metric learning (DML) with training and testing data splits. The authors propose to use ResNet-50 to compute FID between the train/test splits, and show that the train and test distributions are similar. They also show that DML generalization can be improved when the train / test split is used to improve the generalization. Finally, the authors provide some empirical results on several benchmarks and the Recall@k metric, showing that the training/test FID distances are correlated."
2301,SP:cac881243abde92a28c110f5bd84d115ed189bda,"benchmark USED-FOR deep metric learning. method USED-FOR train / test split. OtherScientificTerm are distribution shift, and FID. Method is metric learning methods. ","This paper proposes a new benchmark for deep metric learning. The proposed method is based on the idea of train/test split, which is motivated by the observation that distribution shift can lead to poor performance of metric learning methods. To address this issue, the authors propose to use FID."
2302,SP:cac881243abde92a28c110f5bd84d115ed189bda,benchmark EVALUATE-FOR deep metric learning algorithms. deep metric learning algorithms USED-FOR zero - shot tasks. benchmark EVALUATE-FOR generalization. out - of - distribution shifts FEATURE-OF generalization. FID score USED-FOR distribution shift. methods CONJUNCTION architectures. architectures CONJUNCTION methods. query - support framework USED-FOR few - shot learning. generalization EVALUATE-FOR query - support framework. Metric is FID measure. Method is ROC curve. OtherScientificTerm is task difficulty. ,"This paper presents a benchmark for evaluating the generalization of deep metric learning algorithms on zero-shot tasks under out-of-distribution shifts. The FID measure is based on the ROC curve, and the authors propose to use the FID score as a measure of distribution shift. The authors also propose a query-support framework for few-shot learning, and compare the proposed methods and architectures to several existing methods. The paper also provides a theoretical analysis of the task difficulty."
2303,SP:cac881243abde92a28c110f5bd84d115ed189bda,way USED-FOR metric - learning algorithms. proxy - metric USED-FOR ranking dataset generalization difficulty. FID score USED-FOR proxy - metric. score USED-FOR train / test splits. AUC type score EVALUATE-FOR methods. self - supervised pre - trained models CONJUNCTION fine - tuning. fine - tuning CONJUNCTION self - supervised pre - trained models. self - supervised pre - trained models USED-FOR network capacity. few - shot learning USED-FOR fine - tuning. Metric is proxy - metric of OOD generalization difficulty. Method is metric learning methods. ,"This paper proposes a new way to evaluate the performance of metric-learning algorithms. The authors propose a proxy-metric of OOD generalization difficulty based on the FID score, which can be used for ranking dataset generalization difficulties. The proposed score is used to compute train/test splits, which are then used to evaluate different methods on the AUC type score. Experiments show that self-supervised pre-trained models and fine-tuning with few-shot learning can improve the network capacity."
2304,SP:bacff3685476855a32549d03095375649fd89df2,model selection USED-FOR unsupervised outlier detection. data - driven approach USED-FOR model selection. data - driven approach USED-FOR unsupervised outlier detection. matrix factorization method USED-FOR model performance predictor. METAOD USED-FOR method. matrix factorization method USED-FOR method. matrix factorization method USED-FOR METAOD. 300 + models USED-FOR implementations. implementations EVALUATE-FOR METAOD. parameter configurations FEATURE-OF outlier detection methods. Method is outlier detection models. Task is test time model evaluation. ,"This paper proposes a data-driven approach to model selection for unsupervised outlier detection. The proposed method, METAOD, uses a matrix factorization method to improve the model performance predictor. The authors evaluate the effectiveness of the proposed method on three implementations with 300 + models. The results show that the proposed parameter configurations of outlier detector methods can be used to evaluate the performance of different outlier prediction methods. In addition, the authors provide a theoretical analysis of the effect of different parameter configurations on the performance and efficiency of the outlier generation models. Finally, the paper provides some empirical results on test time model evaluation."
2305,SP:bacff3685476855a32549d03095375649fd89df2,model selection approach USED-FOR outlier detection. METAOD USED-FOR outlier detection. METAOD HYPONYM-OF model selection approach. METAOD USED-FOR outlier detection models. historical information USED-FOR detection model. Collaborative Filtering USED-FOR METAOD. METAOD COMPARE model - selection baselines. model - selection baselines COMPARE METAOD. METAOD USED-FOR outlier detection model selection. Method is meta - learning. OtherScientificTerm is meta - features. ,"This paper proposes METAOD, a model selection approach for outlier detection, which is based on Collaborative Filtering. The main idea is to use meta-learning, where the meta-features of the detection model are extracted from the historical information of the original detection model. The authors show that the proposed METAED can improve the performance of outlier Detection models by improving the performance on several datasets. The experimental results show that MTAOD outperforms other model-selection baselines. The paper also provides a theoretical analysis of the effect of meta-learned features on the final performance. Finally, the authors provide an empirical evaluation of the effectiveness of the proposed model selection model selection based on the performance and the proposed meta-filtering."
2306,SP:bacff3685476855a32549d03095375649fd89df2,hyper - parameter values PART-OF models. method USED-FOR outlier detection model. outlier detection model CONJUNCTION hyper - parameter values. hyper - parameter values CONJUNCTION outlier detection model. model architecture CONJUNCTION model configuration. model configuration CONJUNCTION model architecture. model configuration CONJUNCTION hyper - parameter values. hyper - parameter values CONJUNCTION model configuration. model configuration PART-OF model. model architecture PART-OF model. Method is MetaOD. Task is meta - training stage. Generic is it. ,"This paper proposes a method called MetaOD to jointly train an outlier detection model and hyper-parameter values for different models. The idea of MetaOD is that the model architecture, the model configuration, and the model values should be learned together during the meta-training stage. The authors show that it is possible to train a model with a combination of the proposed model architecture and model configuration as well as the learned model configuration."
2307,SP:bacff3685476855a32549d03095375649fd89df2,METAOD USED-FOR unsupervised outlier model selection problem. data - driven method USED-FOR unsupervised outlier model selection problem. METAOD HYPONYM-OF data - driven method. meta - learning USED-FOR METAOD. model USED-FOR dataset. model USED-FOR METAOD. historical outlier detection benchmark datasets USED-FOR detection models. detection models USED-FOR METAOD. meta - features USED-FOR outlying characteristics. METAOD CONJUNCTION meta - learning database. meta - learning database CONJUNCTION METAOD. ,"This paper proposes METAOD, a data-driven method to solve the unsupervised outlier model selection problem using meta-learning. The model is trained on the original dataset and then used to train new detection models based on historical outlier detection benchmark datasets. The meta-features are used to detect outlying characteristics. The authors conduct extensive experiments on both the original datasets and the new dataset to demonstrate the effectiveness of METAOOD and the meta -learning database."
2308,SP:bf1c45ef27953acab2195d54c8197d360c1e8190,"method USED-FOR prediction + programming "" problem. soft constraints PART-OF objective. hard constraints PART-OF optimization problem. unconstrained optimization problem USED-FOR optimization problem. surrogate function USED-FOR soft constraints. method USED-FOR approximate optimization problem. two - stage methods CONJUNCTION SPO+. SPO+ CONJUNCTION two - stage methods. synthetic settings EVALUATE-FOR method. Task is programming problem. OtherScientificTerm are constraint violations, gradients, and end - to - end. Method is neural network. ","This paper proposes a method for solving the ""prediction + programming"" problem. The programming problem is formulated as an unconstrained optimization problem, where the objective consists of soft constraints and hard constraints. The soft constraints are modeled as a surrogate function, and the hard constraints are represented as gradients of a neural network. The proposed method is able to solve an approximate optimization problem. Experiments on two-stage methods and SPO+ demonstrate the effectiveness of the proposed method in both synthetic settings and end-to-end."
2309,SP:bf1c45ef27953acab2195d54c8197d360c1e8190,"Generic are approach, and It. Method are derivative - based learning methods, and objective penalization. OtherScientificTerm are hard constraints, soft constraints, objective change, and infeasibility. ","This paper proposes an approach to penalize the performance of derivative-based learning methods. It is motivated by the observation that objective penalization does not work well under hard constraints, but works well under soft constraints. The authors argue that this is due to the infeasibility of the objective change."
2310,SP:bf1c45ef27953acab2195d54c8197d360c1e8190,"surrogate function USED-FOR objective functions. closed - form solution USED-FOR objective functions. closed - form solution FEATURE-OF surrogate function. soft constraints FEATURE-OF objective functions. convex hull of feasible solutions USED-FOR corners of infeasible points. geometric perspective USED-FOR bounds. SPO+ loss function CONJUNCTION DF metric. DF metric CONJUNCTION SPO+ loss function. approach COMPARE SPO+ loss function. SPO+ loss function COMPARE approach. approach COMPARE DF metric. DF metric COMPARE approach. DF metric USED-FOR portfolio optimization. Generic are those, functions, and surrogate. OtherScientificTerm is hard constraints. ","This paper proposes a surrogate function for objective functions with soft constraints, which is a closed-form solution to the objective functions that satisfy those constraints. The authors derive bounds from a geometric perspective, showing that the corners of infeasible points can be represented as a convex hull of feasible solutions, and that these functions can be approximated by a surrogate that satisfies the hard constraints. They compare their approach to the SPO+ loss function and the DF metric for portfolio optimization."
2311,SP:bf1c45ef27953acab2195d54c8197d360c1e8190,soft constraints PART-OF predict+programming paradigm. prediction component PART-OF optimization / solving. mathematical programming models CONJUNCTION prediction component. prediction component CONJUNCTION mathematical programming models. soft constraints PART-OF method. surrogate USED-FOR problem. piecewise linear constraints FEATURE-OF problem. ,This paper proposes a method to incorporate soft constraints into the predict+programming paradigm. The authors combine mathematical programming models with a prediction component in the optimization/solving. The main idea is to use a surrogate to solve the problem with piecewise linear constraints.
2312,SP:41a9806ee6c0c84e046b7de79eb54dfe00de6995,"nodes PART-OF graph. synthetic and real - world datasets EVALUATE-FOR DropGNNs. Method are Dropout Graph Neural Networks ( DropGNNs ), dropout, GNNs, DropGNN, and GNN. Generic is approach. OtherScientificTerm are embeddings, and node features. ","This paper proposes Dropout Graph Neural Networks (DropGNNs), an approach to reduce the size of dropout in GNNs. The key idea of DropGNN is to replace the embeddings of the original GNN with the original node features of a graph. The authors demonstrate the effectiveness of the proposed DropGANs on both synthetic and real-world datasets."
2313,SP:41a9806ee6c0c84e046b7de79eb54dfe00de6995,technique USED-FOR graph neural networks. technique USED-FOR expressive power. DropGNN HYPONYM-OF technique. expressive power FEATURE-OF graph neural networks. DropGNN COMPARE vanilla GNN. vanilla GNN COMPARE DropGNN. expressive power EVALUATE-FOR vanilla GNN. expressive power EVALUATE-FOR DropGNN. Generic is It. OtherScientificTerm is nodes. Method is dropout. ,"This paper proposes DropGNN, a technique to improve the expressive power of graph neural networks. It is based on the idea that nodes should not be too close to each other, which is similar to dropout. Experiments show that DropGAN can achieve better expressive power than vanilla GNN."
2314,SP:41a9806ee6c0c84e046b7de79eb54dfe00de6995,"DropGNN HYPONYM-OF GNN exension. DropGNNs USED-FOR neighborhoods. GIN HYPONYM-OF GNNs. synthetic and real - world datasets EVALUATE-FOR method. Generic is model. OtherScientificTerm are graph, and dropout combinations. ","This paper proposes DropGNN, a GNN exension, which is a variant of GNNs (e.g., GIN). The key idea of the proposed model is to learn a neighborhood of nodes in a graph, which can then be used to generate new dropout combinations. This is an interesting idea, and the authors demonstrate the effectiveness of their method on both synthetic and real-world datasets. The authors also provide a theoretical analysis of the properties of DropGnns for neighborhoods, and show that the proposed method is able to generalize to unseen neighborhoods."
2315,SP:41a9806ee6c0c84e046b7de79eb54dfe00de6995,method USED-FOR GNN toolbox. random dropout USED-FOR graph nodes. OtherScientificTerm is graph. Generic is evaluations. Task is classification / regression. ,This paper proposes a method to improve the GNN toolbox. The key idea is to use random dropout on the graph nodes. The evaluations are performed on classification/regression tasks.
2316,SP:090dc0471d54e237f423034b1e1c46a510202807,Dual - stream Network USED-FOR image classification. self - attention CONJUNCTION convolution. convolution CONJUNCTION self - attention. It USED-FOR representation of local and global pattern features. convolution USED-FOR It. self - attention USED-FOR It. convolution USED-FOR representation of local and global pattern features. self - attention USED-FOR representation of local and global pattern features. Intrascale Propagation module USED-FOR resolutions. block USED-FOR FPN. ,This paper proposes a Dual-stream Network for image classification. It uses self-attention and convolution to learn the representation of local and global pattern features. The Intrascale Propagation module is used to generate different resolutions. The block is used in FPN.
2317,SP:090dc0471d54e237f423034b1e1c46a510202807,high - resolution convolutions CONJUNCTION low - resolution vision transformers. low - resolution vision transformers CONJUNCTION high - resolution convolutions. low - resolution vision transformers USED-FOR global patterns. high - resolution convolutions USED-FOR local features. local features CONJUNCTION low - resolution vision transformers. low - resolution vision transformers CONJUNCTION local features. high - resolution convolutions PART-OF streams. low - resolution vision transformers HYPONYM-OF streams. intra - scale propagation module CONJUNCTION inter - scale alignment module. inter - scale alignment module CONJUNCTION intra - scale propagation module. dual - stream blocks PART-OF feature pyramid. image classification CONJUNCTION object detection. object detection CONJUNCTION image classification. object detection CONJUNCTION instance segmentation. instance segmentation CONJUNCTION object detection. DS - Net USED-FOR object detection. DS - Net USED-FOR image classification. DS - Net USED-FOR instance segmentation. Task is visual recognition. ,"This paper addresses the problem of visual recognition. The authors propose two streams: high-resolution convolutions for local features and low-resolution vision transformers for global patterns. The feature pyramid is composed of two dual-stream blocks, where the first block is a feature pyramid and the second block is an intra-scale propagation module and an inter-scale alignment module. Experiments are conducted on image classification, object detection, and instance segmentation using DS-Net for image classification and object detection."
2318,SP:090dc0471d54e237f423034b1e1c46a510202807,hybrid convolution+transformer model USED-FOR 2d images. stages PART-OF it. one CONJUNCTION one. one CONJUNCTION one. one HYPONYM-OF depthwise convolution. parallel paths PART-OF stage. coarse ( capturing global effects ) resolution FEATURE-OF multi - headed attention. multi - headed attention USED-FOR one. one HYPONYM-OF parallel paths. one HYPONYM-OF parallel paths. attention module USED-FOR local. attention module USED-FOR paths. lateral paths PART-OF FPN. dual stream ” modules PART-OF FPN. dual stream ” modules PART-OF lateral paths. image classification CONJUNCTION detection / segmentation. detection / segmentation CONJUNCTION image classification. detection / segmentation USED-FOR COCO. image classification CONJUNCTION ImageNet. ImageNet CONJUNCTION image classification. Deit HYPONYM-OF Transformer - only models. Generic is model. Material is Resnet. Method is DS - FPN. ,"This paper proposes a hybrid convolution+transformer model for 2d images. The model is called DS-FPN and it consists of two stages: one for depthwise convolution and one for parallel paths in the second stage. The parallel paths are modeled as “dual stream” modules in the FPN, where the first one is a multi-headed attention with coarse (capturing global effects) resolution, and the second one is an attention module for local. Experiments are conducted on image classification, detection/segmentation for COCO, and image classification and ImageNet with Transformer-only models such as Deit and Resnet."
2319,SP:090dc0471d54e237f423034b1e1c46a510202807,"architecture USED-FOR visual recognition. convolutional architectures COMPARE representation. representation COMPARE convolutional architectures. multi - scale hierarchy FEATURE-OF representation. deeper layers PART-OF multi - scale hierarchy. coarse, global representation PART-OF hierarchy. depth - wise convolutions CONJUNCTION global ones. global ones CONJUNCTION depth - wise convolutions. Local features CONJUNCTION global ones. global ones CONJUNCTION Local features. depth - wise convolutions USED-FOR Local features. multi - head self - attention USED-FOR global ones. cross - attending USED-FOR local and global features. cross - attending USED-FOR they. global self - attention CONJUNCTION local / global cross - attention. local / global cross - attention CONJUNCTION global self - attention. local depth - wise convolutions CONJUNCTION global self - attention. global self - attention CONJUNCTION local depth - wise convolutions. dense convolutions USED-FOR intermediate representation. FPN's USED-FOR intermediate representation. coarser one USED-FOR intermediate representation. local depth - wise convolutions PART-OF Dual - Stream "" block. global self - attention PART-OF Dual - Stream "" block. local / global cross - attention PART-OF Dual - Stream "" block. dense convolutions USED-FOR FPN's. convolutional architectures COMPARE transformer - based ones. transformer - based ones COMPARE convolutional architectures. ImageNet and detection CONJUNCTION instance segmentation. instance segmentation CONJUNCTION ImageNet and detection. recognition backbone USED-FOR classification. COCO USED-FOR instance segmentation. ResNet HYPONYM-OF convolutional architectures. Method is Feature Pyramid Networks. ","This paper proposes a new architecture for visual recognition, called Feature Pyramid Networks, which is an extension of the recently proposed architecture of the ""Feature Pyramid Networks"" [1]. Unlike previous convolutional architectures, the proposed representation consists of a multi-scale hierarchy with deeper layers and a coarse, global representation. Local features are represented by depth-wise convolutions, global ones by multi-head self-attention, and they are concatenated using cross-attending between local and global features. The ""Dual-Stream"" block consists of: 1) local depth-wider convolutions for local features and global ones, 2) global self-tongue, 3) local/global cross, and 4) global/global/local/global coarser one. FPN's are trained with dense convolutions as the intermediate representation, which are then used to train FPN's. Experiments on ImageNet and detection and instance segmentation with COCO show that the proposed recognition backbone is able to achieve state-of-the-art performance for classification. The authors also compare the performance of the proposed Convolutional and transformer-based versions of ResNet and compare their performance with other recent convolutionally architectures."
2320,SP:9dd460c3506a9a508b92baa63dff6b487e0eeca0,VRDP HYPONYM-OF method. neuro - symbolic concept learner CONJUNCTION differentiable physics model. differentiable physics model CONJUNCTION neuro - symbolic concept learner. object detector CONJUNCTION neuro - symbolic concept learner. neuro - symbolic concept learner CONJUNCTION object detector. modules PART-OF VRDP. differentiable physics model HYPONYM-OF modules. neuro - symbolic concept learner HYPONYM-OF modules. object detector HYPONYM-OF modules. object detector PART-OF VRDP. neuro - symbolic concept learner PART-OF VRDP. differentiable physics model PART-OF VRDP. VRDP COMPARE SOTA. SOTA COMPARE VRDP. synthetic dataset CLEVERER CONJUNCTION dataset of real world videos. dataset of real world videos CONJUNCTION synthetic dataset CLEVERER. datasets EVALUATE-FOR SOTA. datasets EVALUATE-FOR VRDP. dataset of real world videos HYPONYM-OF datasets. synthetic dataset CLEVERER HYPONYM-OF datasets. OtherScientificTerm is evolution of physical scenes. ,"This paper proposes a method called VRDP, which is a method for predicting the evolution of physical scenes. VRDP consists of three modules: an object detector, a neuro-symbolic concept learner, and a differentiable physics model. Experiments on two datasets, a synthetic dataset CLEVERER and a dataset of real world videos, show that VRDP outperforms SOTA."
2321,SP:9dd460c3506a9a508b92baa63dff6b487e0eeca0,"video USED-FOR differentiable physics model. differentiable physics model USED-FOR visual question answering. programs CONJUNCTION grounded concepts. grounded concepts CONJUNCTION programs. programs USED-FOR concept learner. grounded concepts USED-FOR concept learner. modules USED-FOR objects ’ physical parameters. modules USED-FOR differentiable physical engine. Generic is pipeline. Method are visual perception module, and symbolic execution part. OtherScientificTerm is objects ’ features. ","This paper proposes a differentiable physics model for visual question answering from video. The proposed pipeline consists of two parts: (1) a visual perception module that predicts the objects’ features, and (2) a concept learner that learns programs and grounded concepts. The symbolic execution part consists of a sequence of modules that are used to predict the objects ’ physical parameters, which are then used to train the differentiable physical engine."
2322,SP:9dd460c3506a9a508b92baa63dff6b487e0eeca0,concept learner CONJUNCTION physics simulator. physics simulator CONJUNCTION concept learner. visual perception CONJUNCTION concept learner. concept learner CONJUNCTION visual perception. components PART-OF modular unit. visual perception HYPONYM-OF components. visual perception PART-OF modular unit. modular unit USED-FOR dynamic visual reasoning. physics simulator HYPONYM-OF components. physics simulator PART-OF modular unit. concept learner PART-OF modular unit. concept learner HYPONYM-OF components. system USED-FOR system identification. physics model USED-FOR system. visual and language modalities USED-FOR system identification. physics model USED-FOR system identification. it USED-FOR imagined future trajectories. physics model USED-FOR it. counterfactual tasks EVALUATE-FOR competitive methods. ,"This paper proposes a modular unit for dynamic visual reasoning consisting of three components: visual perception, concept learner, and physics simulator. The system uses the physics model to perform system identification in both visual and language modalities, and it is able to generate imagined future trajectories. The competitive methods are evaluated on several counterfactual tasks."
2323,SP:9dd460c3506a9a508b92baa63dff6b487e0eeca0,"composite model USED-FOR visual question answering. differentiable physics model USED-FOR evolution of rigid body trajectories. concept learner USED-FOR natural language QA pair. visual perception module USED-FOR object - centric trajectories. differentiable physics model CONJUNCTION neurosymbolic program executor. neurosymbolic program executor CONJUNCTION differentiable physics model. concept learner USED-FOR neurosymbolic concept vectors. intermediate concept and trajectory features USED-FOR neurosymbolic program executor. visual perception module PART-OF model pipeline. differentiable physics module USED-FOR trajectory prediction. video input sample USED-FOR physics parameters. physics parameters USED-FOR differentiable physics module. mass CONJUNCTION friction. friction CONJUNCTION mass. friction CONJUNCTION restitution. restitution CONJUNCTION friction. restitution HYPONYM-OF physical properties. mass HYPONYM-OF physical properties. friction HYPONYM-OF physical properties. synthetic CLEVRER and realistic Real - billiard benchmarks EVALUATE-FOR approach. CLEVRER EVALUATE-FOR data efficiency. few - shot setup EVALUATE-FOR prior art. data efficiency EVALUATE-FOR It. CLEVRER EVALUATE-FOR It. counterfactual questions EVALUATE-FOR It. Method is dynamic visual reasoning. Task are neurosymbolic visual reasoning, and human interpretation. Generic is model. OtherScientificTerm is physical attributes. ","This paper proposes a composite model for visual question answering. The proposed model is inspired by dynamic visual reasoning, where a differentiable physics model is trained to predict the evolution of rigid body trajectories. The model pipeline consists of a visual perception module that predicts object-centric trajectories, and a neurosymbolic program executor that uses the intermediate concept and trajectory features from the concept learner to generate a natural language QA pair. The trajectory prediction is performed using the learned physics parameters from the video input sample, which are then used to predict physical attributes such as mass, friction, and restitution. The approach is evaluated on both synthetic CLEVRER and realistic Real-billiard benchmarks. It is shown to achieve state-of-the-art performance on CLEVRer, while maintaining data efficiency compared to prior art in a few-shot setup. It also performs well on counterfactual questions, where the goal is to infer human interpretation."
2324,SP:c511066c38f9793bacb4986c564eafa36e032f39,"Active learning USED-FOR minimizing labeling costs. accuracy EVALUATE-FOR model. out - of - distribution data CONJUNCTION redundancy. redundancy CONJUNCTION out - of - distribution data. submodular information measures USED-FOR acquisition functions. SIMILAR HYPONYM-OF active learning framework. submodular information measures USED-FOR SIMILAR. submodular information measures USED-FOR active learning framework. SIMILAR COMPARE state of the art methods. state of the art methods COMPARE SIMILAR. OtherScientificTerm are human in the loop, and imbalance or rare classes. Method is active learning methods. Generic is they. ","Active learning has been shown to be effective in minimizing labeling costs while maintaining accuracy of the model. However, active learning methods are expensive to train, as they require a human in the loop, which can lead to imbalance or rare classes, out-of-distribution data, and redundancy. This paper proposes SIMILAR, an active learning framework that uses submodular information measures to improve the acquisition functions. Experiments show that SIMILEAR outperforms state of the art methods."
2325,SP:c511066c38f9793bacb4986c564eafa36e032f39,methods USED-FOR batch active learning. graph cut function CONJUNCTION log - determinant function. log - determinant function CONJUNCTION graph cut function. facility location function CONJUNCTION graph cut function. graph cut function CONJUNCTION facility location function. submodular functions USED-FOR criterion. facility location function HYPONYM-OF submodular functions. log - determinant function HYPONYM-OF submodular functions. graph cut function HYPONYM-OF submodular functions. similarity matrix USED-FOR submodular functions. inner product of the gradients of loss functions USED-FOR similarity matrix. rare classes CONJUNCTION redundancy. redundancy CONJUNCTION rare classes. redundancy CONJUNCTION out - of - distribution data points. out - of - distribution data points CONJUNCTION redundancy. methods USED-FOR batch active learning. image datasets CONJUNCTION deep neural network models. deep neural network models CONJUNCTION image datasets. methods COMPARE methods. methods COMPARE methods. methods USED-FOR batch active learning. methods USED-FOR batch active learning. ,"This paper proposes methods for batch active learning. The criterion is based on three submodular functions: a facility location function, a graph cut function, and a log-determinant function. The similarity matrix between the two submodul functions is computed using the inner product of the gradients of loss functions. The authors conduct extensive experiments on image datasets and deep neural network models to show that the proposed methods can outperform existing methods in the context of batch activelearning in terms of rare classes, redundancy, and out-of-distribution data points."
2326,SP:c511066c38f9793bacb4986c564eafa36e032f39,"submodular information measure PART-OF framework. AL CONJUNCTION AL. AL CONJUNCTION AL. algorithm COMPARE active learning algorithms. active learning algorithms COMPARE algorithm. AL CONJUNCTION AL. AL CONJUNCTION AL. AL CONJUNCTION AL. AL CONJUNCTION AL. active learning algorithms USED-FOR tasks. AL CONJUNCTION class imbalance. class imbalance CONJUNCTION AL. redundant data USED-FOR AL. out of distribution data USED-FOR AL. tasks EVALUATE-FOR algorithm. AL HYPONYM-OF tasks. AL HYPONYM-OF tasks. AL HYPONYM-OF tasks. AL HYPONYM-OF tasks. Method is unified active learning framework. OtherScientificTerm are submodular score function, dissimilarity, and rare classes. Task is class imbalance task. ","This paper proposes a unified active learning framework. The framework is based on a submodular information measure, where each class is represented as a function of a sub modular score function. The authors show that the proposed algorithm outperforms existing active learning algorithms on a variety of tasks, including AL, AL with redundant data and AL with out of distribution data. The paper also shows that the dissimilarity between classes can be improved by adding rare classes. The main contribution of the paper is the introduction of a class imbalance task."
2327,SP:c511066c38f9793bacb4986c564eafa36e032f39,"information functions USED-FOR submodular optimization ( SO ) problems. information functions USED-FOR diversity based active learning algorithm. Submodular Conditional Gain ( SCG ) CONJUNCTION Submodular Conditional Mutual Information ( SCMI ). Submodular Conditional Mutual Information ( SCMI ) CONJUNCTION Submodular Conditional Gain ( SCG ). redundant data CONJUNCTION out - of - distribution(OOD ) data. out - of - distribution(OOD ) data CONJUNCTION redundant data. Submodular Mutual Information ( SMI ) CONJUNCTION Submodular Conditional Gain ( SCG ). Submodular Conditional Gain ( SCG ) CONJUNCTION Submodular Mutual Information ( SMI ). SCG CONJUNCTION SMI. SMI CONJUNCTION SCG. it USED-FOR unified ” acquisition function. submodular information measures ( SIM ) USED-FOR SO. rare class CONJUNCTION redundant data. redundant data CONJUNCTION rare class. Submodular Conditional Mutual Information ( SCMI ) HYPONYM-OF submodular information measures ( SIM ). Submodular Mutual Information ( SMI ) HYPONYM-OF submodular information measures ( SIM ). Submodular Conditional Gain ( SCG ) HYPONYM-OF submodular information measures ( SIM ). Graph cut ( GC ) CONJUNCTION Log Determinant ( LOGDET ). Log Determinant ( LOGDET ) CONJUNCTION Graph cut ( GC ). Facility location ( FL ) CONJUNCTION Graph cut ( GC ). Graph cut ( GC ) CONJUNCTION Facility location ( FL ). utility function USED-FOR graph - based SO problems. graph - based SO problems USED-FOR diversity coverage. utility function USED-FOR SIM acquisition function. Log Determinant ( LOGDET ) HYPONYM-OF graph - based SO problems. Facility location ( FL ) HYPONYM-OF graph - based SO problems. Graph cut ( GC ) HYPONYM-OF graph - based SO problems. MNIST / CIFAR-10 / ImageNet FEATURE-OF synthetic datasets. BADGE HYPONYM-OF baselines. Metric is SCMI. OtherScientificTerm are class imbalance, mutual information, similarity kernels, and model - predicted label.","This paper proposes a diversity based active learning algorithm that uses three information functions for solving submodular optimization (SO) problems: Submodular Conditional Gain (SCG) (for rare class, redundant data, and out-of-distribution(OOD) data) and (Submodular Mutual Information (SMI) for rare class and redundant data). SCG and SMI are derived from the notion of “mutual information” (i.e., the mutual information between the model-predicted label and the true label) and it is used as a “unified” acquisition function for SO. The utility function is applied to several graph-based SO problems (Facility location (FL), Graph cut (GC), and Log Determinant (LOGDET) to improve the efficiency of the SIM acquisition function. Experiments are conducted on synthetic datasets (MNIST/CIFAR-10/ImageNet) and compared with two baselines (BADGE) and show improvements in SCMI."
2328,SP:c141dc29b487ebfaa20ee50786886b0383d938bc,spread parameter FEATURE-OF Mallows model. asymptotic one USED-FOR non - asymptotic test. average rank aggregation COMPARE central ranking. central ranking COMPARE average rank aggregation. Method is asymptotic and non - asymptotic test. OtherScientificTerm is unknown central ranking. Generic is method. ,"This paper proposes an asymptotic and non-asymptotic test for estimating the spread parameter of the Mallows model. The asymPTotic one is used to estimate the number of samples in the dataset, while the non-asymptotically test is used for estimating an unknown central ranking. The authors show that the average rank aggregation is better than the central ranking, and that the proposed method can be applied to unseen datasets."
2329,SP:c141dc29b487ebfaa20ee50786886b0383d938bc,UMPU test USED-FOR Mallows model's spread parameter. UMPU test CONJUNCTION math tools. math tools CONJUNCTION UMPU test. ,This paper proposes a new UMPU test to evaluate the Mallows model's spread parameter. The main contribution of this paper is to combine the UMPU Test with math tools. The paper is well-written and easy to follow. 
2330,SP:c141dc29b487ebfaa20ee50786886b0383d938bc,identity testing USED-FOR Mallows model. Mallows model HYPONYM-OF parametrized distribution over rankings. identity testers USED-FOR non - asymptotic and asymptotic setting. Method is identity tester. OtherScientificTerm is central ranking. ,"This paper studies the problem of identity testing for the Mallows model, a parametrized distribution over rankings. The authors consider the non-asymptotic and asymptotic setting of identity testers, where the identity tester does not have access to the central ranking."
2331,SP:c141dc29b487ebfaa20ee50786886b0383d938bc,central ranking CONJUNCTION spread parameter. spread parameter CONJUNCTION central ranking. hypothesis testing USED-FOR Mallows ranking models. one CONJUNCTION other. other CONJUNCTION one. optimal learning approach USED-FOR methods. Type I error USED-FOR methods. other HYPONYM-OF methods. one HYPONYM-OF methods. Type I error USED-FOR one. optimal learning approach USED-FOR other. theory USED-FOR testing Mallows rankings. OtherScientificTerm is Mallows rankings. ,This paper studies hypothesis testing for Mallows ranking models. The authors propose two methods: one based on the Type I error and the other based on an optimal learning approach. The central ranking and the spread parameter are derived from the theory. The theory is applied to the problem of testing Mallows rankings.
2332,SP:4c00bcc561832b581f479905b5e3310aeb3bdce2,"problem COMPARE Neural Body. Neural Body COMPARE problem. SMPL fits USED-FOR videos. keyframes USED-FOR feed - forward model. attention USED-FOR time - wise and camera - wise aggregation. quality COMPARE per - scene optimised Neural Bodies. per - scene optimised Neural Bodies COMPARE quality. Method are neural network, and neural net. Generic are it, and extension. ","This paper proposes a neural network for videos with SMPL fits. The problem is similar to the Neural Body, but it is different from it in that the authors propose an extension of the feed-forward model that takes keyframes as input and aggregates them into a single neural net. The main difference is that instead of using attention for time-wise and camera-wise aggregation, the authors use attention for both. The authors show that the proposed extension is able to achieve better quality than per-scene optimised Neural Bodies. "
2333,SP:4c00bcc561832b581f479905b5e3310aeb3bdce2,sparse multiview images USED-FOR method. pixel - aligned features USED-FOR NeRF - like rendering fields. pixel - aligned features HYPONYM-OF time - augmented skeletal features. transformer networks USED-FOR fusing multiview image features. features USED-FOR implicit function. ZJU - MoCap and AIST datasets EVALUATE-FOR method. OtherScientificTerm is image features. Method is human mesh model. ,"This paper proposes a method for fusing sparse multiview images. The key idea is to use time-augmented skeletal features, i.e., pixel-aligned features, to approximate NeRF-like rendering fields. This is achieved by using transformer networks to perform the task of fusing multiiview image features. These features are then used to train an implicit function that maps image features to a human mesh model. The proposed method is evaluated on the ZJU-MoCap and AIST datasets."
2334,SP:4c00bcc561832b581f479905b5e3310aeb3bdce2,free - viewpoint videos USED-FOR dynamic humans. method USED-FOR free - viewpoint videos. human identities CONJUNCTION unseen poses. unseen poses CONJUNCTION human identities. transformer modules USED-FOR spatio - temporal information. spatio - temporal information PART-OF sparse multi - view data. transformer modules USED-FOR neural radiance fields. neural radiance fields USED-FOR unseen poses. public multi - camera datasets EVALUATE-FOR method. Method is sparse multi - camera system. ,"This paper proposes a method for learning free-viewpoint videos of dynamic humans. The method is based on a sparse multi-camera system, where each camera is trained separately to capture both human identities and unseen poses. To incorporate spatio-temporal information into the sparse multi multi-view data, the authors propose to use transformer modules to generate neural radiance fields to capture unseen poses and identities. The proposed method is evaluated on two public multi-cameras datasets."
2335,SP:4c00bcc561832b581f479905b5e3310aeb3bdce2,generalizable NeRF network USED-FOR view synthesis. view synthesis USED-FOR dynamic human scenes. feed - forward manner USED-FOR generalizable NeRF network. NeRF representation USED-FOR appearance generalization. it USED-FOR appearance generalization. visual features USED-FOR It. tracked parametric body model USED-FOR visual features. tracked parametric body model USED-FOR It. pixel - aligned image features USED-FOR NeRF representation. temporal Transformer CONJUNCTION multiview Transformer. multiview Transformer CONJUNCTION temporal Transformer. generalization capability EVALUATE-FOR It. OtherScientificTerm is sparse camera views. Method is PixelNeRF. ,This paper proposes a generalizable NeRF network for view synthesis in dynamic human scenes in a feed-forward manner. It uses a tracked parametric body model to extract visual features and uses pixel-aligned image features to train the NeRF representation to improve appearance generalization. It is evaluated on temporal Transformer and multiview Transformer with sparse camera views and shows improved generalization capability compared to PixelNeRF.
2336,SP:5495d9168a8770eb2493e2d2bb6b68423e82b9e6,S3 HYPONYM-OF neural architecture search method. S3 USED-FOR vision transformers. S3 USED-FOR search space. image classification CONJUNCTION object detection. object detection CONJUNCTION image classification. object detection CONJUNCTION semantic segmentation. semantic segmentation CONJUNCTION object detection. semantic segmentation EVALUATE-FOR S3 discovered architectures. computer vision tasks EVALUATE-FOR S3 discovered architectures. image classification EVALUATE-FOR S3 discovered architectures. OtherScientificTerm is architectures. ,"This paper proposes S3, a neural architecture search method. S3 is an extension of vision transformers and aims to improve the search space of existing architectures. The authors evaluate the S3 discovered architectures on several computer vision tasks on image classification, object detection, and semantic segmentation."
2337,SP:5495d9168a8770eb2493e2d2bb6b68423e82b9e6,NAS USED-FOR vision transformer architecture. it USED-FOR search space. error distribution PART-OF E - T error. E - T error USED-FOR search space. linear function USED-FOR E - T error. supernet USED-FOR E - T errors. evolutionary search USED-FOR algorithm. search space FEATURE-OF algorithm. architecture COMPARE state - of - the - art models. state - of - the - art models COMPARE architecture. architecture USED-FOR vision tasks. Generic is architectures. OtherScientificTerm is search dimension. ,"This paper proposes a new vision transformer architecture based on NAS. The main contribution of this paper is that it extends the search space of the original search space using the E-T error, which is a linear function of the error distribution. The authors propose two architectures: (1) a supernet that is trained to minimize the E -T errors of the supernet, and (2) an algorithm that uses evolutionary search to find the optimal search space for the search dimension. The proposed architecture is evaluated on several vision tasks and compared to state-of-the-art models."
2338,SP:5495d9168a8770eb2493e2d2bb6b68423e82b9e6,method USED-FOR searching the search space of vision. E - T error USED-FOR optimal search space. them USED-FOR optimal search space. E - T error USED-FOR them. search dimensions USED-FOR search space. OtherScientificTerm is subspace. ,This paper proposes a method for searching the search space of vision. The key idea is to use the E-T error as a proxy for the optimal search space and to use them to estimate the E -T error to find a suitable search space based on the search dimensions of the subspace. The paper is well-written and easy to follow. 
2339,SP:5495d9168a8770eb2493e2d2bb6b68423e82b9e6,once - for - all NAS methods USED-FOR architectures. search space USED-FOR transformers. E - T error metric EVALUATE-FOR search spaces. OtherScientificTerm is vision transformers. Method is search method. ,"This paper studies the problem of training vision transformers. The authors propose a new search space for transformers based on once-for-all NAS methods, which can be used to search for architectures that are similar to the original architectures. The main contribution of the paper is to propose a search method that can be applied to any search space. The search spaces are evaluated using the E-T error metric."
2340,SP:c77b83c667a9b63fe15582336a77a34e96fd667b,"NP - hardness reduction USED-FOR Label Covering problem. Method are label proportions ( LLP ), and OR - LTF. OtherScientificTerm are LTF, and bags. Generic is algorithm. ","This paper studies the NP-hardness reduction for the Label Covering problem. The authors propose a new algorithm called label proportions (LLP), which is based on OR-LTF. The main idea is to use the LTF to reduce the number of bags. The algorithm is evaluated on a variety of datasets."
2341,SP:c77b83c667a9b63fe15582336a77a34e96fd667b,"label proportions USED-FOR linear threshold function ( LTF ). polynomial time algorithm USED-FOR LTF. Task is PAC learning setting. OtherScientificTerm are labeled examples, lower bound, and mixed pairs. Generic is algorithm. ","This paper studies the PAC learning setting where the labeled examples are given by a set of labels. The authors propose a linear threshold function (LTF) based on the label proportions. The LTF is approximated by a polynomial time algorithm, and the authors prove a lower bound of $O(1/\sqrt{T})$ where $T$ is the number of labeled examples. The algorithm is shown to converge to the optimal solution of the LTF. The lower bound is proved for the case of mixed pairs."
2342,SP:c77b83c667a9b63fe15582336a77a34e96fd667b,"learning of linear threshold functions ( LLFs ) USED-FOR learning from label proportions model. polynomial time algorithm USED-FOR LLF. Method is linear threshold functions ( LLFs ). OtherScientificTerm are monotone OR, and LLFs. ",This paper studies the learning of linear threshold functions (LLFs) for learning from label proportions model. The authors propose a polynomial time algorithm for learning an LLF. The main idea is to learn a monotone OR for the weights of the LLFs.
2343,SP:c77b83c667a9b63fe15582336a77a34e96fd667b,"Method are learning from label proportions framework, linear threshold functions, polynomial time algorithm, and linear threshold function. Generic is framework. OtherScientificTerm is feature vectors. ",This paper proposes a learning from label proportions framework. The framework is based on the observation that linear threshold functions are not optimal. The authors propose a polynomial time algorithm that iteratively updates the linear threshold function until the feature vectors converge to a fixed point.
2344,SP:2eb193c76355aac08003c9b377895202fd3bd297,benchmark dataset USED-FOR NAS. benchmark dataset USED-FOR NAS - Bench - x11. NAS - Bench-301 CONJUNCTION NAS - Bench - NLP. NAS - Bench - NLP CONJUNCTION NAS - Bench-301. surrogate model USED-FOR learning curve. NAS - Bench-101 CONJUNCTION NAS - Bench-301. NAS - Bench-301 CONJUNCTION NAS - Bench-101. benchmarks COMPARE NAS - Bench - x11. NAS - Bench - x11 COMPARE benchmarks. NAS - Bench - x11 HYPONYM-OF surrogate benchmark. learning curve FEATURE-OF candidate architectures. surrogate benchmark USED-FOR surrogate model. NAS - Bench-101 HYPONYM-OF benchmarks. NAS - Bench - NLP HYPONYM-OF benchmarks. NAS - Bench-301 HYPONYM-OF benchmarks. singular value decomposition CONJUNCTION noise modeling. noise modeling CONJUNCTION singular value decomposition. singular value decomposition USED-FOR learning curve. noise modeling USED-FOR learning curve. OtherScientificTerm is learning curves. Generic is it. ,"This paper presents a new benchmark dataset for NAS called NAS-Bench-x11. The authors compare the learning curve of candidate architectures with a surrogate model trained on the new benchmark benchmark, and show that the learning curves of the surrogate model can be used to compare the performance of different benchmarks, such as NAS - Bench-101, NAS- Bench-301, and NAS-bench-NLP. The learning curve can be improved by using singular value decomposition and noise modeling. The paper is well-written and easy to follow, and it is easy to understand."
2345,SP:2eb193c76355aac08003c9b377895202fd3bd297,NAS benchmark datasets EVALUATE-FOR multi - fidelity algorithms. learning curve extrapolation HYPONYM-OF multi - fidelity algorithms. NAS - Bench-101 HYPONYM-OF NAS benchmark datasets. NAS - Bench-311 CONJUNCTION NAS - Bench - NLP11. NAS - Bench - NLP11 CONJUNCTION NAS - Bench-311. NAS - Bench-111 CONJUNCTION NAS - Bench-311. NAS - Bench-311 CONJUNCTION NAS - Bench-111. singular value decomposition and noise modeling methods USED-FOR surrogate benchmark datasets. NAS - Bench-111 HYPONYM-OF surrogate benchmark datasets. NAS - Bench-311 HYPONYM-OF surrogate benchmark datasets. NAS - Bench - NLP11 HYPONYM-OF surrogate benchmark datasets. datasets EVALUATE-FOR method. full training information USED-FOR datasets. Method is architectures. ,"This paper proposes to evaluate multi-fidelity algorithms (e.g., learning curve extrapolation) on three NAS benchmark datasets (NAS-Bench-101, NAS-Bench101, and NAS-bench-NLP11) to evaluate the performance of multi-faithful algorithms. The authors propose to use singular value decomposition and noise modeling methods to train surrogate benchmark datasets, including NAS-Briefcase-100, NAS -Bench-111, NAS – Bench-311, NAS–NLP-11. The proposed method is evaluated on three datasets with full training information. The experiments show that the proposed architectures perform well on the three datasets."
2346,SP:2eb193c76355aac08003c9b377895202fd3bd297,learning curves USED-FOR surrogate benchmarks. NAS - Bench versions USED-FOR surrogate benchmarks. architecture representation USED-FOR embeddings. learning curve extrapolation USED-FOR speeding - up NAS. Method is learning curve embeddings. OtherScientificTerm is training noise. ,"This paper proposes to extrapolate learning curves to surrogate benchmarks based on NAS-Bench versions. The learning curve embeddings are learned using an architecture representation, which is then used to compute the embeds. The authors argue that this learning curve extrapolation is a useful tool for speeding-up NAS in the presence of training noise."
2347,SP:2eb193c76355aac08003c9b377895202fd3bd297,per - epoch information USED-FOR multi - fidelity NAS algorithms. surrogate model USED-FOR learning curve. architecture encoding USED-FOR surrogate model. NASBench-301 CONJUNCTION NASBench - NLP. NASBench - NLP CONJUNCTION NASBench-301. NASBench-101 CONJUNCTION NASBench-301. NASBench-301 CONJUNCTION NASBench-101. multi - fidelity algorithms USED-FOR NAS. procedure USED-FOR single - fidelity algorithms. single - fidelity algorithms CONJUNCTION multi - fidelity ones. multi - fidelity ones CONJUNCTION single - fidelity algorithms. learning curve extrapolation USED-FOR procedure. method USED-FOR multi - fidelity NAS algorithms. Material is NAS benchmarks. Generic is predictor. OtherScientificTerm is per - epoch accuracies. Method is learning curve predictor. ,"This paper proposes to use per-epoch information to train multi-fidelity NAS algorithms. The main idea is to train a surrogate model that predicts the learning curve based on architecture encoding. The surrogate model is trained on standard NAS benchmarks (NASBench-101, NASBench-301, and NASBench - NLP), and the predictor is trained to predict the per-empirical accuracies. The proposed procedure is based on learning curve extrapolation, and is applied to both single-faithfulness algorithms and multi-faithful ones in NAS. The method is shown to improve the performance of multi-idelityNAS algorithms."
2348,SP:6ed1637ac697821931f685db0d476b9f7b56971a,"Method is black - box neural networks. OtherScientificTerm are top - level hidden layer, non - linearities, softmax, hidden layers, and integrated gradient. ","This paper studies the problem of training black-box neural networks with non-linearities. Specifically, the authors consider the case where the top-level hidden layer is non-convex. The authors show that the softmax of the hidden layers can be decomposed into two parts: (1) the integrated gradient, and (2) the number of hidden layers."
2349,SP:6ed1637ac697821931f685db0d476b9f7b56971a,method USED-FOR hidden representation. SimplEx HYPONYM-OF method. SimplEx USED-FOR hidden representation. linear combination of hidden representations of examples USED-FOR SimplEx. SimplEx COMPARE kNN baselines. kNN baselines COMPARE SimplEx. Task is predicting cancer mortality. Material is MNIST and time series data. ,"This paper proposes SimplEx, a method to learn a hidden representation of examples using a linear combination of hidden representations of examples. SimplEx is motivated by the problem of predicting cancer mortality. The authors conduct experiments on MNIST and time series data and show that SimplEx outperforms kNN baselines."
2350,SP:6ed1637ac697821931f685db0d476b9f7b56971a,method USED-FOR model prediction. SimplEx HYPONYM-OF post - hoc explanation method. feature saliency CONJUNCTION example - based explanations. example - based explanations CONJUNCTION feature saliency. example - based explanations USED-FOR SimplEx. feature saliency USED-FOR SimplEx. SimplEx COMPARE baselines. baselines COMPARE SimplEx. Task is clinical risk prediction. OtherScientificTerm is features. Method is latent - space generalization of Integrated Gradients. ,"This paper proposes SimplEx, a post-hoc explanation method for clinical risk prediction. SimplEx is a method for model prediction that uses both feature saliency and example-based explanations. The features are learned by a latent-space generalization of Integrated Gradients. Experiments show that SimplEx outperforms several baselines."
2351,SP:6ed1637ac697821931f685db0d476b9f7b56971a,"latent space FEATURE-OF weighted sum of nearest neighbors. user - specified corpus USED-FOR weighted sum of nearest neighbors. method COMPARE explanation - by - example - comparison methods. explanation - by - example - comparison methods COMPARE method. KNN - like explanations HYPONYM-OF explanation - by - example - comparison methods. Concept Activation Vectors CONJUNCTION Deep KNN. Deep KNN CONJUNCTION Concept Activation Vectors. hidden layer USED-FOR model. linear - mapping constraint FEATURE-OF layer. integrated ( and projected ) jacobians USED-FOR it. technique USED-FOR feature. model COMPARE dKNN inspired methods. dKNN inspired methods COMPARE model. MNIST EVALUATE-FOR model. instance - level decompositions COMPARE explainability methods. explainability methods COMPARE instance - level decompositions. latent and output space FEATURE-OF instance - level decompositions. Method are SimplEx, and softmax. Task is model predictions. OtherScientificTerm are latent variable space, and latent representation. Generic are constraint, and approximation. Material are clinical risk dataset, and comparison corpus. ","This paper proposes SimplEx, a method for comparing model predictions to explanations in the latent variable space. The method is similar to existing explanation-by-example-contrast methods (e.g., KNN-like explanations) but uses a weighted sum of nearest neighbors in latent space extracted from a user-specified corpus. The model consists of a hidden layer that is constrained to satisfy a linear-mapping constraint, and it is trained with integrated (and projected) jacobians. This constraint is enforced by a softmax, and the authors show that the proposed method is able to outperform other explanation-based-explanation methods (such as Concept Activation Vectors and Deep KNN). The model is evaluated on MNIST and on a clinical risk dataset, and compared to other dKNN inspired methods. The comparison corpus is a subset of a larger comparison corpus, and this technique is applied to each feature separately. The authors compare instance-level decompositions of the latent and output space with other explainability methods, and show that their model outperforms the other existing state-of-the-art methods. "
2352,SP:c8f82ec90f891d7394933483b7f926155ac363ef,Swin Transformer USED-FOR vision - language pretraining. visual domain FEATURE-OF Swin Transformer. visual relationship learning CONJUNCTION inter - modal alignment. inter - modal alignment CONJUNCTION visual relationship learning. VL pretraining models USED-FOR cross - modal transformer. inter - modal alignment PART-OF transformer network. visual relationship learning PART-OF transformer network. image tokens USED-FOR cross - modal transformer. visual relationship USED-FOR inter - modal alignment learning. independent processing of visual relationship PART-OF framework. visual - only transformer USED-FOR visual relationship. visual - only transformer USED-FOR cross - modal transformer. metric USED-FOR cross - modal interactions. inter - modality flow ( IMF ) USED-FOR cross - modal interactions. inter - modality flow ( IMF ) HYPONYM-OF metric. masked feature regression USED-FOR pretraining task. COCO CONJUNCTION Visual Genome. Visual Genome CONJUNCTION COCO. visual reasoning CONJUNCTION visual entailment. visual entailment CONJUNCTION visual reasoning. VQA CONJUNCTION visual reasoning. visual reasoning CONJUNCTION VQA. VQA EVALUATE-FOR Evaluation. OtherScientificTerm is image and text tokens. Task is Pretraining. ,"This paper proposes a new variant of Swin Transformer for vision-language pretraining in the visual domain. The authors propose a cross-modal transformer based on VL pretraining models, which takes image tokens as input and outputs image and text tokens as output. The proposed transformer network combines visual relationship learning with inter-modual alignment in the transformer network. The framework is based on independent processing of visual relationship between the input and output tokens, where a visual-only transformer is used to model the visual relationship and inter -modal alignment learning is performed on the input image tokens. The paper also introduces a new metric called inter-Modality flow (IMF) to measure the cross modal interactions. The pretraining task is performed using masked feature regression. Evaluation is conducted on VQA, visual reasoning, and visual entailment on COCO and Visual Genome. Pretraining is performed in a supervised fashion."
2353,SP:c8f82ec90f891d7394933483b7f926155ac363ef,"vision transformer USED-FOR vision - language pre - training. vision transformer USED-FOR visual backbone. visual backbone USED-FOR vision - language pre - training. metric USED-FOR information flow. visual entailment CONJUNCTION visual reasoning. visual reasoning CONJUNCTION visual entailment. VQA CONJUNCTION visual entailment. visual entailment CONJUNCTION VQA. VQA EVALUATE-FOR method. OtherScientificTerm are vision and language modalities, and attention weight. Metric is Inter - modality flow. ","This paper proposes a vision transformer to learn a visual backbone for vision-language pre-training. The proposed method is evaluated on VQA, visual entailment, and visual reasoning. The authors propose a metric to measure information flow between vision and language modalities. Inter-modality flow is defined as the ratio of the attention weight between the input and the output."
2354,SP:c8f82ec90f891d7394933483b7f926155ac363ef,"masked feature regression ( MFR ) USED-FOR transformer - based visual encoders. inter - modality flow ( IMF ) HYPONYM-OF quantity. Method are MRM, attention flow method, and vision - and - language pretraining models. ","This paper proposes to use masked feature regression (MFR) to train transformer-based visual encoders. The MRM is based on the attention flow method, and the authors propose a quantity called inter-modality flow (IMF), which is a generalization of vision-and-language pretraining models."
2355,SP:c8f82ec90f891d7394933483b7f926155ac363ef,"transformer based visual backbone USED-FOR vision and language understanding. vision transformer backbone ( Swin Transformer ) USED-FOR visual representation. It USED-FOR visual representation. mask generation module USED-FOR visual representation. vision transformer backbone ( Swin Transformer ) USED-FOR It. text tokens embeddings USED-FOR multimodal transformer model. SNLI - VE CONJUNCTION NLVR^2. NLVR^2 CONJUNCTION SNLI - VE. VQA CONJUNCTION SNLI - VE. SNLI - VE CONJUNCTION VQA. NLVR^2 CONJUNCTION Image - Text Retrieval. Image - Text Retrieval CONJUNCTION NLVR^2. SNLI - VE CONJUNCTION Image - Text Retrieval. Image - Text Retrieval CONJUNCTION SNLI - VE. SNLI - VE EVALUATE-FOR architecture. Image - Text Retrieval EVALUATE-FOR architecture. NLVR^2 EVALUATE-FOR architecture. VQA EVALUATE-FOR architecture. OtherScientificTerm are masked tokens, pretraining objectives, and visual and textual modalities. Method is Inter Modality Flow metric. ","This paper proposes a transformer based visual backbone for vision and language understanding. It uses a vision transformer backbone (Swin Transformer) to generate a visual representation using a mask generation module. The text tokens embeddings are used to train a multimodal transformer model, where the masked tokens are used as pretraining objectives. The architecture is evaluated on VQA, SNLI-VE, NLVR^2, and Image-Text Retrieval. The Inter Modality Flow metric is used to compare the performance of visual and textual modalities."
2356,SP:ecc173185ec28d0ef75c60df260ac4faba059f61,"dynamics of privacy loss FEATURE-OF noisy gradient descent algorithm. Langevin diffusion process HYPONYM-OF continuous - time analogue. Renyi divergence FEATURE-OF probability distributions. optimal utility USED-FOR differential policy algorithms. gradient complexity FEATURE-OF optimal utility. OtherScientificTerm are smooth and strongly convex loss functions, privacy loss, and Lipschitz, smooth and strongly convex loss functions. Method is composition - based analysis of the same algorithms. ","This paper studies the dynamics of privacy loss of a noisy gradient descent algorithm. The authors consider a continuous-time analogue of the Langevin diffusion process, where the privacy loss is defined as a function of the Renyi divergence between the probability distributions. In particular, the authors consider Lipschitz, smooth and strongly convex loss functions, and show that the optimal utility of differential policy algorithms can be expressed in terms of the gradient complexity. They also provide a composition-based analysis of the same algorithms."
2357,SP:ecc173185ec28d0ef75c60df260ac4faba059f61,"Renyi divergence USED-FOR differential privacy metric. discrete algorithmic method USED-FOR Noisy Gradient Descent. stochastic differential equation of Focker - Plank Equation HYPONYM-OF stochastic continuous analogue. discrete / continuous parameters USED-FOR optimization. semi - continuous interpolation USED-FOR information disclosure. Gaussian perturbation USED-FOR smooth and strongly convex loss functions. Gaussian perturbation USED-FOR Noisy Gradient Descent. Generic are theoretical framework, and dataset. OtherScientificTerm is optimization function. Method is tracing diffusion process. ","This paper proposes a discrete algorithmic method for Noisy Gradient Descent, which is motivated by the theoretical framework of Renyi divergence as a differential privacy metric. The main contribution of this paper is to propose a stochastic continuous analogue of the stoched differential equation of Focker-Plank Equation, where the optimization is performed with discrete/continuous parameters. The authors also propose a semi-continuous interpolation for information disclosure, which can be used as a proxy for the optimization function. Finally, the authors propose a tracing diffusion process, and provide an empirical study on a dataset. They show that using Gaussian perturbation for both smooth and strongly convex loss functions, NoisyGradient Desence converges to the optimal solution."
2358,SP:ecc173185ec28d0ef75c60df260ac4faba059f61,"Gaussian noise FEATURE-OF gradients. privacy loss EVALUATE-FOR DPGD. Langevin Dynamics USED-FOR it. Renyi divergences FEATURE-OF DPGD. Method are Differentially Private Gradient Descent ( DPGD ), gradient descent, and approximate DP. OtherScientificTerm are f, distribution p, loss functions, divergence bound, divergence, and epsilon. Task is approximate differential privacy. ","This paper studies Differentially Private Gradient Descent (DPGD), which is a variant of gradient descent where the gradients are subject to Gaussian noise. DPGD has been shown to have a privacy loss of $\epsilon$ and it is based on Langevin Dynamics. The main contribution of this paper is to prove a divergence bound for the loss functions of DDPD with Renyi divergences. The authors show that the divergence of the loss function f can be expressed as a function of the distribution p, where p is the number of samples and epsilon is the dimension of the data. This is similar to approximate DP, where the divergence can be written as the sum of p and the dimension p, and the authors prove that approximate differential privacy can be achieved."
2359,SP:ecc173185ec28d0ef75c60df260ac4faba059f61,"noisy gradient descent ( GD ) algorithms USED-FOR smooth and strongly convex loss functions. information leakage FEATURE-OF smooth and strongly convex loss functions. information leakage FEATURE-OF noisy gradient descent ( GD ) algorithms. Task is dynamics of ( Renyi ) privacy loss. Metric is rate of privacy loss. OtherScientificTerm are Renyi divergence, and model parameters. Method is composition - based approaches. ","This paper studies the dynamics of (Renyi) privacy loss. The authors show that noisy gradient descent (GD) algorithms suffer from information leakage in both smooth and strongly convex loss functions. The rate of privacy loss depends on the Renyi divergence between the model parameters and the data points. To address this issue, the authors propose composition-based approaches."
2360,SP:acb1e0dc8d6ef5607e7d3ec9893b5364b9a6e831,method USED-FOR QP solving. reinforcement learning USED-FOR method. regularization parameter USED-FOR OSQP solver. RL policy USED-FOR regularization parameter. approach USED-FOR OSQP. ,This paper proposes a method for QP solving based on reinforcement learning. The main idea is to use an RL policy as a regularization parameter for the OSQP solver. The approach is shown to be effective for solving OSQPs.
2361,SP:acb1e0dc8d6ef5607e7d3ec9893b5364b9a6e831,"RL formulation USED-FOR quadratic programming. training pipeline USED-FOR quadratic programming. RL formulation CONJUNCTION training pipeline. training pipeline CONJUNCTION RL formulation. multi - dimensional multiplier USED-FOR ADMM. multi - agent Markov game USED-FOR problem. single agent policy CONJUNCTION specialized architecture. specialized architecture CONJUNCTION single agent policy. single agent policy USED-FOR algorithm. specialized architecture USED-FOR algorithm. specialized architecture USED-FOR single agent problem. specialized architecture USED-FOR it. RL USED-FOR optimization problem. RL USED-FOR paper. Generic is architecture. OtherScientificTerm are R^6, and R^1. Task is Markov games. ","This paper proposes an RL formulation and a training pipeline for quadratic programming. The proposed architecture is based on ADMM with a multi-dimensional multiplier, where R^6 is the number of agents and R^1 is the dimension of the problem. The problem is formulated as a single-agent Markov game, and the proposed algorithm uses a single agent policy and a specialized architecture to solve it. The paper uses RL to solve the optimization problem, which is an important problem in Markov games."
2362,SP:acb1e0dc8d6ef5607e7d3ec9893b5364b9a6e831,RL USED-FOR step size policy. OSQP HYPONYM-OF ADMM - based solver. RL USED-FOR OSQP. ADMM - based solver USED-FOR quadratic programming problems. OSQP USED-FOR step size policy. policies USED-FOR scalar and a diagonal step size. QPLIB CONJUNCTION Netlib. Netlib CONJUNCTION QPLIB. Netlib CONJUNCTION Maros - Meszaros. Maros - Meszaros CONJUNCTION Netlib. benchmark instances EVALUATE-FOR OSQP. ,"This paper proposes OSQP, an ADMM-based solver for quadratic programming problems that uses RL to learn a step size policy. The authors propose two policies for scalar and a diagonal step size, and show that the two policies converge to the same solution. Experiments on QPLIB, Netlib, and Maros-Meszaros are conducted to show the effectiveness and efficiency of the proposed policies. Finally, experiments on benchmark instances demonstrate the effectiveness of the OSQLP."
2363,SP:acb1e0dc8d6ef5607e7d3ec9893b5364b9a6e831,"method USED-FOR QP   ( quadratic programming ) solving process. reinforcement learning USED-FOR QP   ( quadratic programming ) solving process. reinforcement learning USED-FOR method. ADMM step size vector rho PART-OF action space. benchmark suite EVALUATE-FOR RLQP. Generic is formulation. OtherScientificTerm are internal variables, OSQP's internal variables, internal states, and step vector rho. Method are QP solver, QP, and multi - agent single - policy formulation. ","This paper proposes a method to accelerate the QP  (quadratic programming) solving process using reinforcement learning. The formulation is based on the observation that the state of a QP solver can be expressed as a function of its internal variables, which is similar to OSQP's internal variables. The authors propose to replace the ADMM step size vector rho in the action space with a vector of internal states, which can be represented as a step vector. This allows the authors to extend QP to a multi-agent single-policy formulation, where each agent has its own internal states. Experiments on a benchmark suite demonstrate the effectiveness of RLQP."
2364,SP:eb68e98d9baf9118381d25d4b2da030a6f78577f,"principal component bias FEATURE-OF deep linear neural networks. gradient descent USED-FOR deep linear neural networks. gradient descent USED-FOR principal component bias. learning rate USED-FOR principal component bias. deep linear networks COMPARE single layer linear networks. single layer linear networks COMPARE deep linear networks. OtherScientificTerm are network, and PC bias. ",This paper studies the principal component bias of deep linear neural networks trained with gradient descent. The authors show that the learning rate of the network can be used as a proxy for the principal part of the PC bias. They also show that deep linear networks are more PC biased than single layer linear networks.
2365,SP:eb68e98d9baf9118381d25d4b2da030a6f78577f,training dynamics FEATURE-OF over - parameterized deep linear networks. convergence behavior FEATURE-OF deep linear networks. PC - bias CONJUNCTION LOC - effect. LOC - effect CONJUNCTION PC - bias. early stopping CONJUNCTION convergence behavior. convergence behavior CONJUNCTION early stopping. label noise FEATURE-OF convergence behavior. ,"This paper studies the training dynamics of over-parameterized deep linear networks. The authors provide a theoretical analysis of the convergence behavior of deeplinear networks in the presence of PC-bias and LOC-effect. In particular, the authors show that early stopping and convergence behavior under label noise are important factors in the convergence of deep linear network performance."
2366,SP:eb68e98d9baf9118381d25d4b2da030a6f78577f,"gradient descent USED-FOR deep over - parametrized linear network. Metric is convergence rate. OtherScientificTerm are principal components, and singular values. ",This paper studies the convergence rate of gradient descent in deep over-parametrized linear network. The main result is that the principal components of the network converge to singular values. The convergence rate is shown to be O(1/\sqrt{T}) where T is the number of parameters.
2367,SP:eb68e98d9baf9118381d25d4b2da030a6f78577f,"principal component bias FEATURE-OF learning of deep wide nets. PCs USED-FOR learning. linear nets USED-FOR learning. Method are linear networks, and linear and nonlinear models. ",This paper studies the principal component bias in the learning of deep wide nets. The authors show that linear networks tend to be biased towards nonlinear models. They also show that learning with PCs is similar to learning with linear nets.
2368,SP:1598bad835a657e56af3261501c671897b7e9ffd,"deep neural networks USED-FOR backdoored data. deep neural networks COMPARE benign samples. benign samples COMPARE deep neural networks. Anti - Backdoor Learning ( ABL ) USED-FOR benign model. poisoned datasets USED-FOR benign model. ABL USED-FOR backdoored data. training loss gap EVALUATE-FOR ABL. ABL USED-FOR backdoored model. detected data USED-FOR ABL. detected data USED-FOR backdoored model. Material are clean samples, and backdoored examples. Metric is loss values. ","This paper proposes Anti-Backdoor Learning (ABL), which aims to train deep neural networks on backdoored data instead of benign samples. The main idea is to train a benign model on poisoned datasets and then use ABL on the poisoned datasets to train the benign model. The training loss gap between the clean samples and the poisoned samples is shown to be smaller than that of the original clean samples. ABL can also be used to train backdodged data on detected data. The paper also shows that ABL is able to train an efficient backdoorized model on the detected data, and that the loss values of ABL are smaller than the original loss values. "
2369,SP:1598bad835a657e56af3261501c671897b7e9ffd,backdoor attack examples COMPARE clean examples. clean examples COMPARE backdoor attack examples. gradient descent USED-FOR clean examples. gradient ascent method USED-FOR backdoor. Clean Accuracy ( CA ) EVALUATE-FOR method. Attack Success Rate ( ASR ) EVALUATE-FOR method. Material is backdoor examples. ,This paper studies the problem of generating backdoor attack examples that are more accurate than clean examples generated by gradient descent. The authors propose a gradient ascent method to generate a backdoor that is more robust to backdoor examples. The proposed method is evaluated on the Clean Accuracy (CA) and Attack Success Rate (ASR) benchmarks.
2370,SP:1598bad835a657e56af3261501c671897b7e9ffd,"anti - backdoor learning USED-FOR clean models. backdoor - poisoned data USED-FOR anti - backdoor learning. Anti - Backdoor Learning ( ABL ) USED-FOR backdoor attack. poisoned data USED-FOR models. clean data USED-FOR models. models USED-FOR backdoor. two - step method USED-FOR clean model. poisoned data USED-FOR clean model. second step USED-FOR backdoors. poisoned samples USED-FOR clean model. Task is backdoor task. OtherScientificTerm are backdoor target class, and isolation step. Generic are first step, second stage, and method. Metric is precision. ","This paper proposes Anti-Backdoor Learning (ABL) to defend against backdoor attack using backdoor-poisoned data. The main idea is to use anti-backdoor learning to train clean models on the poisoned data and then use the clean data to train the models to attack the backdoor. The authors propose a two-step method to train a clean model on poisoned data. In the first step, the clean model is trained on poisoned samples and then the backdoor target class is selected. The second step is to train backdoors on the clean samples and the second stage is an isolation step. The method is evaluated on a variety of datasets and the precision is shown to be high."
2371,SP:1598bad835a657e56af3261501c671897b7e9ffd,backdoored data COMPARE clean data. clean data COMPARE backdoored data. models COMPARE clean data. clean data COMPARE models. backdoored data USED-FOR models. training loss FEATURE-OF backdoored data. backdoored data COMPARE clean data. clean data COMPARE backdoored data. poisoned data USED-FOR clean models. anti - backdoor learning ( ABL ) HYPONYM-OF training procedure. threshold USED-FOR ABL. gradient ascent CONJUNCTION gradient descent. gradient descent CONJUNCTION gradient ascent. gradient ascent USED-FOR backdoored set. gradient descent USED-FOR clean set. backdoored set USED-FOR ABL. gradient descent USED-FOR ABL. gradient ascent USED-FOR ABL. ABL COMPARE backdoor removal baselines. backdoor removal baselines COMPARE ABL. benchmarks EVALUATE-FOR ABL. reducing attack success rate CONJUNCTION retaining normal accuracy. retaining normal accuracy CONJUNCTION reducing attack success rate. reducing attack success rate EVALUATE-FOR ABL. retaining normal accuracy EVALUATE-FOR ABL. OtherScientificTerm is backdoored portion. Generic is It. ,"This paper proposes a training procedure called anti-backdoor learning (ABL), which aims to reduce the training loss of backdoored data compared to clean data. The main idea is to train models on poisoned data, where the poisoned data is used to train clean models. ABL uses a threshold to decide whether to use the poisoned or clean data, and then uses gradient ascent on the poisoned set and gradient descent on the clean set. It is shown that ABL outperforms backdoor removal baselines on several benchmarks in terms of reducing attack success rate and retaining normal accuracy."
2372,SP:6c0b7cb37e285cb9342f049d7b61af4565fe01fd,density distribution USED-FOR generative radiance field models. generating relitable reflectance fields USED-FOR density distribution. shading regularization USED-FOR naturally looking shapes. radiance fields USED-FOR naturally looking shapes. OtherScientificTerm is lightings. Method is discriminator. ,This paper proposes to learn a density distribution for generative radiance field models by generating relitable reflectance fields. The lightings are generated by a discriminator. The paper also proposes shading regularization to encourage naturally looking shapes in radiance fields.
2373,SP:6c0b7cb37e285cb9342f049d7b61af4565fe01fd,"GAN USED-FOR relightable radiance field. unlabeled dataset of face images USED-FOR GAN. volume density field CONJUNCTION albedo "" field. albedo "" field CONJUNCTION volume density field. 3D location CONJUNCTION latent code. latent code CONJUNCTION 3D location. relightable radiance field generator USED-FOR volume density field. latent code USED-FOR relightable radiance field generator. 3D location USED-FOR relightable radiance field generator. GAN style USED-FOR training. generator USED-FOR 3D volume. model USED-FOR 3D shapes. auxiliary network USED-FOR surface location. latent code CONJUNCTION viewing direction. viewing direction CONJUNCTION latent code. viewing direction USED-FOR auxiliary network. latent code USED-FOR auxiliary network. auxiliary network USED-FOR rendering. OtherScientificTerm are light location, discriminator loss, random camera view, training data distribution, color - shape ambiguities, and sampling of unoccupied space. Material are photorealistic face image, and 2D images. Method is pi - GAN. Generic is network. ","This paper proposes a GAN that generates a relightable radiance field from an unlabeled dataset of face images. The paper uses GAN to generate a photorealistic face image, where the light location of the face is given by a 3D location, a latent code, and a ""volume density field"" and an ""albedo"" field. The training is done in GAN style, where a discriminator loss is applied to the input image and the output is a random camera view. The generator is trained to generate 3D volume of the image, and the model is used to render 3D shapes, which are then used to train pi-GAN. An auxiliary network is used for surface location and viewing direction, and an auxiliary network for rendering is used. The network is trained on 2D images and the training data distribution is randomly sampled from the original 2D image. This is done to avoid color-shape ambiguities caused by sampling of unoccupied space."
2374,SP:6c0b7cb37e285cb9342f049d7b61af4565fe01fd,method USED-FOR generative radiance fields. generative radiance fields USED-FOR 3D shape reconstruction. method USED-FOR 3D shape reconstruction. it USED-FOR image. it USED-FOR albedo. lighting conditions FEATURE-OF image. regularization USED-FOR shape reconstruction. synthesized albedo CONJUNCTION normal. normal CONJUNCTION synthesized albedo. normal USED-FOR realistic images. synthesized albedo USED-FOR realistic images. lighting conditions FEATURE-OF realistic images. normal USED-FOR method. synthesized albedo USED-FOR method. 2D CNN USED-FOR depth map. latent code CONJUNCTION camera poses. camera poses CONJUNCTION latent code. rendering speed EVALUATE-FOR it. volume ray tracing USED-FOR it. camera poses USED-FOR depth map. 2D CNN USED-FOR it. latent code USED-FOR depth map. method COMPARE state - of - the - arts. state - of - the - arts COMPARE method. image synthesis quality EVALUATE-FOR state - of - the - arts. method USED-FOR shape reconstruction. image synthesis quality EVALUATE-FOR shape reconstruction. image synthesis quality EVALUATE-FOR method. OtherScientificTerm is color - shape ambiguity. ,"This paper proposes a method to learn generative radiance fields for 3D shape reconstruction. The method uses a synthesized albedo and a normal to generate realistic images under different lighting conditions, and then uses it to estimate the albed of the original image. This regularization improves shape reconstruction by removing color-shape ambiguity, and it also improves rendering speed by using volume ray tracing. Finally, it uses a 2D CNN to generate a depth map from the latent code and camera poses. Experiments show that the proposed method can improve shape reconstruction performance and image synthesis quality compared to the state-of-the-arts."
2375,SP:6c0b7cb37e285cb9342f049d7b61af4565fe01fd,"generative model USED-FOR neural radiance field. random lighting configurations USED-FOR shading effects. system USED-FOR implicit radiance field representation. OtherScientificTerm are Shading, and geometry. ",This paper proposes a generative model that learns a neural radiance field. Shading effects are modeled as random lighting configurations. The system is able to learn an implicit radiancefield representation that is invariant to geometry.
2376,SP:4b3dad77d79507c512877867dfea6db87a78682d,approach USED-FOR causal effect. it USED-FOR causal effect. uncertain quantification USED-FOR instrumental variable ( IV ) regression. it USED-FOR approach. confounded observational data USED-FOR causal effect. kernelized IV models USED-FOR procedure. Gaussian process prior USED-FOR quasi - posterior. randomized prior trick USED-FOR kernel conditional expectation estimator. Method is quasi - Bayesian procedure. ,"This paper proposes an approach to estimate the causal effect from confounded observational data using uncertain quantification in instrumental variable (IV) regression. The proposed procedure is based on kernelized IV models, where the quasi-posterior is modeled as a Gaussian process prior. The kernel conditional expectation estimator is trained using a randomized prior trick, which is a quasi-Bayesian procedure."
2377,SP:4b3dad77d79507c512877867dfea6db87a78682d,"quasi - Bayesian approach USED-FOR instrumental variable ( IV ) regression. empirical estimate of the ‘ likelihood ’ USED-FOR Gibbs posterior. empirical estimate of the ‘ likelihood ’ USED-FOR quasi - Bayesian approach. likelihood USED-FOR moment conditions. simulated datasets EVALUATE-FOR method. randomized prior trick USED-FOR computation approach. Generic are approach, and posterior. Method is Gaussian process approach. OtherScientificTerm is moment constraint. ","This paper proposes a quasi-Bayesian approach for instrumental variable (IV) regression based on an empirical estimate of the ‘likelihood’ of the Gibbs posterior. The approach is motivated by the observation that the likelihood for moment conditions can be computed efficiently. The computation approach is based on a randomized prior trick, where the posterior is sampled from a Gaussian process approach and the moment constraint is relaxed. The method is evaluated on two simulated datasets."
2378,SP:4b3dad77d79507c512877867dfea6db87a78682d,scalable quasi - Bayesian inference USED-FOR instrumental variable regression. quadratic kernel IV loss function USED-FOR quasi - posterior. convergence analysis USED-FOR randomized prior trick. stochastic gradient descent - ascent USED-FOR randomized prior trick. Generic is approach. ,This paper studies scalable quasi-Bayesian inference for instrumental variable regression. The authors propose a randomized prior trick based on stochastic gradient descent-ascent with convergence analysis. The quasi-posterior is modeled as a quadratic kernel IV loss function. The proposed approach is evaluated on a variety of datasets.
2379,SP:4b3dad77d79507c512877867dfea6db87a78682d,"scalable quasi - Bayesian method USED-FOR nonparametric instrumental variable ( IV ) regression. scalable quasi - Bayesian method USED-FOR uncertainty quantification. uncertainty quantification USED-FOR IV models. Radon - Nikodym derivative USED-FOR quasi - posterior. kernelized IV models USED-FOR approach. randomized prior trick USED-FOR Gaussian process regression. scalable inference USED-FOR quasi - posterior. randomized prior trick USED-FOR scalable inference. it COMPARE approaches. approaches COMPARE it. approaches USED-FOR uncertainty quantification ( Bayesian, bootstrap ). OtherScientificTerm is Gaussian process prior. Metric are posterior contraction criteria, and conservative posterior contraction rate. Method is inference algorithm. Generic is method. ","This paper proposes a scalable quasi-Bayesian method for nonparametric instrumental variable (IV) regression. The proposed approach is based on kernelized IV models, where the quasi-posterior is modeled as a Radon-Nikodym derivative of the Gaussian process prior. The authors propose a scalable inference of the quasi posterior using a randomized prior trick that is similar to that of Gaussian processes. In addition, the authors propose posterior contraction criteria to ensure that the inference algorithm obtains a conservative posterior contraction rate. Empirical results demonstrate the effectiveness of the proposed method and show that it outperforms existing approaches for uncertainty quantification (Bayesian, bootstrap)."
2380,SP:fd0d72d0689f170f8157dc7f79deb01348e414b3,"solution USED-FOR cross language retrieval based question answering. Material are language - specific annotated training data, and cross - lingual setting. Method is DPR algorithm. ","This paper proposes a solution for cross language retrieval based question answering. The main idea is to use language-specific annotated training data, and then use the DPR algorithm to retrieve the correct answer in the cross-lingual setting."
2381,SP:fd0d72d0689f170f8157dc7f79deb01348e414b3,cross - lingual passage retrieval algorithm CONJUNCTION multilingual autoregressive generation model. multilingual autoregressive generation model CONJUNCTION cross - lingual passage retrieval algorithm. iterative training approach USED-FOR retrieval. languages with limited resources USED-FOR retrieval. iterative training approach USED-FOR annotated data. multilingual open question answering benchmarks EVALUATE-FOR method. Task is multilingual open QA task. Generic is approach. ,This paper proposes a cross-lingual passage retrieval algorithm and a multilingual autoregressive generation model to solve the multilingual open QA task. The authors propose an iterative training approach to generate annotated data for efficient retrieval in languages with limited resources. The proposed method is evaluated on multilingual answer answering benchmarks and the results show the effectiveness of the proposed approach.
2382,SP:fd0d72d0689f170f8157dc7f79deb01348e414b3,"data augmentation setup USED-FOR multilingual training data. retrieve - and - read ODQA model USED-FOR task. wikipedias USED-FOR CORA. wikipedias USED-FOR solution. MBERT USED-FOR DPR. mT5 USED-FOR generative reader. limited multilingual raining data USED-FOR MDPR and MGEN model components. TyDi QA USED-FOR model. multilingual training data CONJUNCTION language coverage. language coverage CONJUNCTION multilingual training data. state - of - the - art EVALUATE-FOR CORA. evaluation - only dataset EVALUATE-FOR CORA. MKQA HYPONYM-OF evaluation - only dataset. TyDI QA EVALUATE-FOR CORA. Task is multilingual Open - Domain Question Answering. Generic are system, Models, and iterative approach. Method are multi - lingual pretrained models, data augmentation strategy, and expansion mechanism. Material are low - resource language, high resource languages, and wikipedia “ language links ” resource. ","This paper addresses the problem of multilingual Open-Domain Question Answering, i.e., answering questions in a language other than the original language. The authors propose a system called CORA, which aims to address this problem by augmenting the retrieve-and-read ODQA model for this task with a data augmentation setup to generate multilingual training data. The proposed solution is based on wikipedias, where the goal is to augment the wikipedia “language links” resource with questions from a low-resource language to high resource languages. Models are trained in a similar fashion to multi-lingual pretrained models. The MDPR and MGEN model components are trained on limited multilingual raining data, where MBERT is used for DPR and mT5 is used as a generative reader. The model is evaluated on TyDi QA and is compared to the state-of-the-art on the evaluation-only dataset, MKQA, and on the TyDI QA where the model is trained on the full dataset. The results show that CORA is able to achieve better performance on both the evaluation and training datasets, both in terms of terms of both the number of training samples and language coverage. The main contribution of the paper is an iterative approach where the authors propose an expansion mechanism to generate new questions for each language. "
2383,SP:fd0d72d0689f170f8157dc7f79deb01348e414b3,mDPR HYPONYM-OF multilingual dense passage retrieval model. labeled multilingual Q - P pairs USED-FOR mDPR part. passage pairs USED-FOR answer generation model. mDPR USED-FOR answer generation model. multilingual corpus USED-FOR passage pairs. mGEN HYPONYM-OF answer generation model. mDPR USED-FOR passage pairs. question - passage pairs USED-FOR mDPR. TyDi QA CONJUNCTION MKQA. MKQA CONJUNCTION TyDi QA. open - domain QA setting EVALUATE-FOR TyDi QA. open - domain QA setting EVALUATE-FOR MKQA. DPR USED-FOR multilingual / cross - lingual setting. Material is Wikipedia corpus. Generic is baselines. ,"This paper proposes a multilingual dense passage retrieval model called mDPR, which is an extension of mGEN. Specifically, the main idea is to use the labeled multilingual Q-P pairs to train the mMPR part, and then use the passage pairs from the multilingual corpus to train an answer generation model (e.g., mGEN). To generate passage pairs, the mDRP is trained on the question-passage pairs generated by mGEN, and is trained using the Wikipedia corpus. Experiments are conducted on both TyDi QA and MKQA in the open-domain QA setting, where the authors compare with two baselines. DPR is also applied to multilingual/cross-lingual setting."
2384,SP:aa4d44b283ef4fea4335847c89fc7b5874169850,ERM models PART-OF domainBed pipeline. Metric is target - entropy. ,This paper proposes a domainBed pipeline that combines ERM models from the domainBed framework. The main idea is to use target-entropy as a metric to measure the quality of the training data. The paper is well-written and easy to follow.
2385,SP:aa4d44b283ef4fea4335847c89fc7b5874169850,models EVALUATE-FOR theory - based measures. DomainBed Benchmarks USED-FOR models. theory based measure USED-FOR OOD generalization behavior. entropy CONJUNCTION Fisher. Fisher CONJUNCTION entropy. sharpness CONJUNCTION entropy. entropy CONJUNCTION sharpness. empirical measures USED-FOR OOD generalization behavior. sharpness CONJUNCTION Fisher. Fisher CONJUNCTION sharpness. Fisher HYPONYM-OF empirical measures. entropy HYPONYM-OF empirical measures. sharpness HYPONYM-OF empirical measures. Task is out - of - distribution generalization measures. Generic is predictive measures. ,"This paper studies out-of-distribution generalization measures. The authors evaluate models trained on DomainBed Benchmarks against theory-based measures. They show that empirical measures such as sharpness, entropy, and Fisher are correlated with OOD generalization behavior, while the theory based measure is not predictive of OOD performance. They also show that predictive measures are not correlated with performance."
2386,SP:aa4d44b283ef4fea4335847c89fc7b5874169850,measures USED-FOR out - of - domain generalization. out - of - domain generalization FEATURE-OF deep neural networks. measures USED-FOR deep neural networks. Empirical Risk Minimization ( ERM ) USED-FOR deep neural networks. Jacobian Norm based measures CONJUNCTION Mixup based Measures. Mixup based Measures CONJUNCTION Jacobian Norm based measures. Mixup based Measures CONJUNCTION entropy. entropy CONJUNCTION Mixup based Measures. Fisher based measures CONJUNCTION Jacobian Norm based measures. Jacobian Norm based measures CONJUNCTION Fisher based measures. entropy HYPONYM-OF measures. Fisher based measures HYPONYM-OF measures. Jacobian Norm based measures HYPONYM-OF measures. Mixup based Measures HYPONYM-OF measures. Method is domain adaptation theory. Generic is it. OtherScientificTerm is single factor. Metric is Spearman's ρ. ,"This paper presents Empirical Risk Minimization (ERM) for deep neural networks trained with Empirically-adapted measures for out-of-domain generalization in the context of domain adaptation theory. In particular, the authors propose three measures for evaluating the performance and generalization performance of deep neural nets: Fisher based measures, Jacobian Norm based measures and Mixup based Measures, as well as entropy. The authors show empirically that it is possible to reduce the variance of ERM to a single factor, which they call Spearman's ρ. "
2387,SP:aa4d44b283ef4fea4335847c89fc7b5874169850,empirical risk minimization USED-FOR domain generalization tasks. theory USED-FOR domain adaptation. theory USED-FOR target domain test error. theory - based measures USED-FOR target domain test error. measures COMPARE theory. theory COMPARE measures. measures COMPARE in - domain test error. in - domain test error COMPARE measures. OtherScientificTerm is target domain labels. Metric is empirical measures. ,"This paper studies the problem of empirical risk minimization in domain generalization tasks. The authors propose theory-based measures to estimate the target domain test error, which can be used as a proxy for domain adaptation. The proposed measures are empirically shown to be better than the in-domain test error when target domain labels are not available, and to be even better than empirical measures when target labels are available."
2388,SP:340c5353a63884b49cfdc46ddb6153b28b2e894f,"excess model capacity CONJUNCTION perturbation robustness. perturbation robustness CONJUNCTION excess model capacity. perturbation robustness CONJUNCTION backdoor poisoning. backdoor poisoning CONJUNCTION perturbation robustness. adversarial training USED-FOR backdoored data. adversarial training USED-FOR classifier. backdoors USED-FOR classifier. Method are theoretical framework, and backdoor - robust classifier. Task are machine learning, and pruning corrupted train data. Material is MNIST dataset. ","This paper proposes a theoretical framework to study the relationship between excess model capacity, perturbation robustness, and backdoor poisoning in machine learning. Specifically, the authors propose to train a backdoor-robust classifier by pruning corrupted train data, and then use adversarial training to train the classifier on the backdoored data. The authors conduct extensive experiments on the MNIST dataset to demonstrate the effectiveness of the proposed method."
2389,SP:340c5353a63884b49cfdc46ddb6153b28b2e894f,robustness EVALUATE-FOR binary classifiers. backdoors FEATURE-OF binary classifiers. measure USED-FOR classifiers'memorization capacity. adversarial training USED-FOR vulnerability. adversarial training USED-FOR backdoors. Generic is formulation. ,"This paper studies the robustness of binary classifiers against backdoors. The authors propose a new measure to measure the classifiers' memorization capacity. The formulation is based on the observation that adversarial training can reduce vulnerability to backdoors, and the authors provide empirical evidence to support their formulation."
2390,SP:340c5353a63884b49cfdc46ddb6153b28b2e894f,"theoretical framework USED-FOR backdoor poisoning attacks. nonzero memorization capacity USED-FOR backdoor attack. limited poisoned data USED-FOR attack. adversarial training USED-FOR backdoor attacks. OtherScientificTerm are data distribution, off - distribution data, and finite VC - dimension. Method are ERM learner, learner, and backdoor - resistant model. Metric is memorization capacity. Task is filtering poisoned data. ","This paper presents a theoretical framework for backdoor poisoning attacks. The main idea is to train an ERM learner with nonzero memorization capacity for a backdoor attack with limited poisoned data. Theoretically, the authors show that adversarial training for backdoor attacks is equivalent to training a backdoor-resistant model with a finite VC-dimension, where the data distribution of the poisoned data is the same as that of the off-distribution data. In practice, this is not always the case, and the authors propose to use the learned model to improve the memorization capacities of the learner. The authors also propose a new way of filtering poisoned data, which is based on the observation that the number of poisoned samples is increasing with the size of the network."
2391,SP:340c5353a63884b49cfdc46ddb6153b28b2e894f,"theoretical framework USED-FOR backdoor attacks. memorization capacity USED-FOR vulnerability of the learning problem. backdoor attacks FEATURE-OF vulnerability of the learning problem. backdoors USED-FOR problems. backdoor filtering CONJUNCTION robust generalization. robust generalization CONJUNCTION backdoor filtering. assumptions USED-FOR backdoor filtering. OtherScientificTerm are learning problem, and explicit backdoors. Generic is framework. Task is learning problems. Method is backdoor filtering algorithms. ",This paper proposes a theoretical framework for backdoor attacks. The framework is motivated by the observation that the memorization capacity of the learning problem can be used as a measure of the vulnerability to backdoor attacks in learning problems. The authors argue that backdoors for these problems can be viewed as explicit backdoors. The main contribution of this paper is to provide a theoretical analysis of backdoor filtering and robust generalization under certain assumptions. The paper also provides empirical results on several backdoor filtering algorithms.
2392,SP:4c925cde6e5b9813946452fdd6b47816e2490f49,deep neural networks USED-FOR GP. infinite width limit FEATURE-OF deep GPs. single - layer GP USED-FOR deep GPs. infinite width limit FEATURE-OF GP. OtherScientificTerm is finite width. Method is neural network. ,"This paper studies the infinite width limit of deep neural networks for GP. In particular, the authors show that deep GPs can be viewed as a single-layer GP with finite width. The authors also show that the neural network can be trained to be infinite-width."
2393,SP:4c925cde6e5b9813946452fdd6b47816e2490f49,"deep GP prior USED-FOR GP limit. data adaptivity FEATURE-OF hidden layers. data adaptivity FEATURE-OF deep GP posterior. deep GP prior COMPARE GP. GP COMPARE deep GP prior. heavy - tailed behaviors FEATURE-OF deep GP prior. Method are deep Gaussian Processes, and deep GP. OtherScientificTerm is depth. ","This paper studies deep Gaussian Processes and proposes a new deep GP prior that extends the GP limit. The authors show that the data adaptivity of the deep GP posterior is a function of the depth, and that the depth of the hidden layers can be used to measure the depth. They also show that deep GP has heavy-tailed behaviors, which is not observed in the GP."
2394,SP:4c925cde6e5b9813946452fdd6b47816e2490f49,capacity CONJUNCTION representative power. representative power CONJUNCTION capacity. neural networks HYPONYM-OF composite finite basis function models. Method is composite Gaussian process models. ,"This paper studies the relationship between capacity and representative power in composite finite basis function models (i.e., neural networks). In particular, the authors focus on composite Gaussian process models. "
2395,SP:4c925cde6e5b9813946452fdd6b47816e2490f49,Deep Gaussian Process ( DGP ) COMPARE ordinary Gaussian Process ( GP ). ordinary Gaussian Process ( GP ) COMPARE Deep Gaussian Process ( DGP ). covariance function FEATURE-OF hidden GP layer. marginal prior covariance FEATURE-OF DGP. DGP USED-FOR GP. deep neural networks ( DNN ) USED-FOR GP. DNN CONJUNCTION DGP. DGP CONJUNCTION DNN. Method is DGP models. OtherScientificTerm is hidden covariance functions. ,This paper proposes a novel Deep Gaussian Process (DGP) that is more efficient than ordinary Gaussian Process (GP). The main difference between the two DGP models is that the covariance function of the hidden GP layer is different from the marginal prior covariance of the original DGP. The main contribution of this paper is to show that the GP can be approximated by deep neural networks (DNN) and that the DGP can be used to approximate the GP with DNN and DGP without the need to learn the hidden covariance functions. 
2396,SP:3f74dc3dc2cb444b3097aae1288dad5355e9a4d4,framework USED-FOR Federated optimization. FedLin HYPONYM-OF Federated optimization. client - specific local steps CONJUNCTION biased compression. biased compression CONJUNCTION client - specific local steps. It USED-FOR linear convergence. biased compression PART-OF It. client - specific local steps PART-OF It. lower bounds USED-FOR infrequent communication. Task is least squares and logistic regression problems. ,This paper proposes a framework for Federated optimization called FedLin. It combines client-specific local steps and biased compression to achieve linear convergence. The authors provide lower bounds for infrequent communication and provide empirical results on least squares and logistic regression problems.
2397,SP:3f74dc3dc2cb444b3097aae1288dad5355e9a4d4,"algorithms USED-FOR problem. FedLin algorithm USED-FOR minimizer. communication complexity EVALUATE-FOR parallel GD. FedLin COMPARE parallel GD. parallel GD COMPARE FedLin. communication complexity EVALUATE-FOR FedLin. matching lower bound USED-FOR FedLin. server - side sparsification COMPARE client - side sparsification. client - side sparsification COMPARE server - side sparsification. gradient sparsification schemes COMPARE client - side sparsification. client - side sparsification COMPARE gradient sparsification schemes. gradient sparsification schemes COMPARE server - side sparsification. server - side sparsification COMPARE gradient sparsification schemes. Task is distributed optimization setup. Method are global model, and gradient sparsification. Material is deterministic setting. OtherScientificTerm are full local gradients, global minimizer, diminishing step - sizes, and local steps. ","This paper studies a distributed optimization setup where each client has access to a global model and the goal is to minimize the global minimizer. The authors propose two algorithms to solve this problem. In the deterministic setting, the authors propose the FedLin algorithm to find a minimizer that minimizes all local gradients. The communication complexity of FedLin is compared to parallel GD and the authors provide a matching lower bound for FedLin. The paper also provides a theoretical analysis of diminishing step-sizes and shows that gradient sparsification is equivalent to client-side sparsifying, and that the full global gradients are equivalent to the local steps. "
2398,SP:3f74dc3dc2cb444b3097aae1288dad5355e9a4d4,"correction term USED-FOR objective heterogeneity. client - specific learning rate USED-FOR local steps. full gradient oracle USED-FOR algorithm. sparse gradients CONJUNCTION local steps. local steps CONJUNCTION sparse gradients. FedLin USED-FOR objective and system heterogeneity. objective and system heterogeneity CONJUNCTION sparse gradients. sparse gradients CONJUNCTION objective and system heterogeneity. convex - smooth setting CONJUNCTION non - convex setting. non - convex setting CONJUNCTION convex - smooth setting. iterate sub - optimality recursion USED-FOR strongly convex - smooth setting. upper bounds USED-FOR client and server compression. compression CONJUNCTION non - convex setting. non - convex setting CONJUNCTION compression. theoretical guarantees USED-FOR FedLin. upper bounds CONJUNCTION compression. compression CONJUNCTION upper bounds. matching upper and lower bounds USED-FOR strongly convex - smooth setting. compression FEATURE-OF convex - smooth setting. upper bounds CONJUNCTION non - convex setting. non - convex setting CONJUNCTION upper bounds. strongly convex - smooth setting FEATURE-OF client and server compression. stochastic gradient oracle USED-FOR iterate sub - optimality recursion. matching upper and lower bounds HYPONYM-OF theoretical guarantees. OtherScientificTerm is local gradients. Method are federated learning, and gradient oracle. ","This paper proposes FedLin, an algorithm that uses a full gradient oracle to approximate the objective of a server with local gradients. The main idea is to add a correction term to account for objective heterogeneity, which is an important problem in federated learning. The authors propose to use a client-specific learning rate for the local steps, which they call the ""correction term"". The authors provide theoretical guarantees for FedLin in terms of matching upper and lower bounds for client and server compression in the strongly convex-smooth setting with iterate sub-optimality recursion with a stochastic gradientoracle, as well as in the non-convex setting. They show that FedLin is robust to both objective and system heterogeneity (i.e., sparse gradients and local steps) in both convex and non-consistency settings. They also provide a theoretical analysis of the convergence of FedLin with respect to the full oracle."
2399,SP:3f74dc3dc2cb444b3097aae1288dad5355e9a4d4,"optimization method USED-FOR federated learning. optimization method USED-FOR FedLin. FedLin USED-FOR global optimum. FedAvg CONJUNCTION FedProx. FedProx CONJUNCTION FedAvg. FedProx CONJUNCTION FedNova. FedNova CONJUNCTION FedProx. surrogate loss USED-FOR methods. FedNova HYPONYM-OF methods. FedAvg HYPONYM-OF methods. FedProx HYPONYM-OF methods. quadratics FEATURE-OF FedLin. gradient sparsification USED-FOR convergence. FedLin COMPARE methods. methods COMPARE FedLin. synthetic least squares and logistic regression problems EVALUATE-FOR FedLin. synthetic least squares and logistic regression problems EVALUATE-FOR methods. OtherScientificTerm are fast convergence, non - convex functions, client learning rate, client loss, objective mismatch, and lower bounds. Material is heterogeneous data. ","This paper proposes FedLin, an optimization method for federated learning that aims to achieve fast convergence in the case of non-convex functions. Specifically, FedLin aims to find a global optimum that maximizes the client learning rate while minimizing the client loss. In contrast to other methods that use a surrogate loss, such as FedAvg, FedProx, FedNova, etc., FedLin uses quadratics. The authors show that FedLin outperforms other methods on both synthetic least squares and logistic regression problems. They also show that gradient sparsification can improve the convergence in cases of heterogeneous data, and provide lower bounds on the objective mismatch."
2400,SP:29f44f2f7d0e9748eed6732ed19ca3335acb04e3,1D Wasserstein distance USED-FOR Gaussian projection directions. 1D Wasserstein distance USED-FOR technique. uniformly sampling random projections USED-FOR Monte Carlo approximation. Monte Carlo approximation USED-FOR SW distance. Wasserstein distance FEATURE-OF Gaussian distributions. closed - form solution USED-FOR Wasserstein distance. Method is approximate SW. ,This paper proposes a technique based on the 1D Wasserstein distance for Gaussian projection directions. The main idea is to approximate the SW distance using a Monte Carlo approximation by uniformly sampling random projections. The authors show that this approximate SW converges to a closed-form solution for the Wassersteine distance of Gaussian distributions.
2401,SP:29f44f2f7d0e9748eed6732ed19ca3335acb04e3,"deterministic approximation USED-FOR Sliced - Wasserstein Distance ( SWD ). random linear projection USED-FOR high - dimensional vector. approximately Gaussian distribution FEATURE-OF random linear projection. approximation COMPARE Monte Carlo estimate of SWD. Monte Carlo estimate of SWD COMPARE approximation. synthetic data EVALUATE-FOR approximation. application USED-FOR image generation. OtherScientificTerm are tuning parameters, and $ d$ dimensions. Material is high - dimensional data. ","This paper proposes a deterministic approximation to the Sliced-Wasserstein Distance (SWD). The main idea is to use a random linear projection of a high-dimensional vector from an approximately Gaussian distribution. The tuning parameters are assumed to be linear in $d$ dimensions. Experiments on synthetic data show that the proposed approximation outperforms the Monte Carlo estimate of SWD. The paper also provides an application to image generation, where the high dimensional data is not available."
2402,SP:29f44f2f7d0e9748eed6732ed19ca3335acb04e3,"method USED-FOR Sliced - Wasserstein distance. Gaussian approximation of the projected data USED-FOR method. method COMPARE Monte Carlo SW distance. Monte Carlo SW distance COMPARE method. computational speed EVALUATE-FOR Monte Carlo SW distance. computational speed EVALUATE-FOR method. toy data set CONJUNCTION real data sets. real data sets CONJUNCTION toy data set. MNIST CONJUNCTION Celeb. Celeb CONJUNCTION MNIST. real data sets EVALUATE-FOR method. Celeb HYPONYM-OF real data sets. toy data set EVALUATE-FOR method. MNIST HYPONYM-OF real data sets. OtherScientificTerm are SW distance, and Gaussians. ",This paper proposes a method for computing the Sliced-Wasserstein distance between two points. The proposed method is based on a Gaussian approximation of the projected data. The authors show that the proposed method achieves better computational speed than the standard Monte Carlo SW distance. The method is evaluated on a toy data set and two real data sets (MNIST and Celeb) and compared to the state-of-the-art. The SW distance is also compared to Gaussians.
2403,SP:29f44f2f7d0e9748eed6732ed19ca3335acb04e3,approach USED-FOR sliced Wasserstein distance. central limit theorems USED-FOR random projections. centering assumption CONJUNCTION weak dependence assumption. weak dependence assumption CONJUNCTION centering assumption. central limit theorems USED-FOR deterministic approximation of sliced Wasserstein. closed form of Wasserstein distance USED-FOR deterministic approximation of sliced Wasserstein. approach COMPARE Monte Carlo approximation. Monte Carlo approximation COMPARE approach. Monte Carlo approximation USED-FOR sliced Wasserstein distance. synthetic data EVALUATE-FOR theory. Method is deterministic approximation of sliced Wasserstein (. ,This paper proposes an approach to approximate the sliced Wasserstein distance using central limit theorems for random projections under a centering assumption and a weak dependence assumption. The main contribution of the paper is to derive a deterministic approximation of sliced WASSERSTEIN using a closed form of Wasserstein distance. The proposed approach is shown to outperform the Monte Carlo approximation for estimating the squared Wasserstenstein distance. Empirical results on synthetic data demonstrate the effectiveness of the theory.
2404,SP:7d5ec55a01247b65e4a8f1973d448214585d6baa,"representations COMPARE fMRI data. fMRI data COMPARE representations. approach USED-FOR problems. brain activations FEATURE-OF representations. Method are NLP representations, and language representations. OtherScientificTerm is semantic depth of analysis. ","This paper studies the problem of learning NLP representations from brain activations. In particular, the authors focus on the semantic depth of analysis and show that the representations learned by comparing the representations with fMRI data are more similar to language representations than to the original representations. The authors then propose a new approach to solve these problems, which is based on the observation that language representations tend to be more similar than representations learned in the brain. "
2405,SP:7d5ec55a01247b65e4a8f1973d448214585d6baa,"human fMRI data USED-FOR organization of linguistic representation. regularized linear encoding models USED-FOR organization of linguistic representation. encoder - decoder framework USED-FOR task'( linguistic representation ) affinity matrix. word embeddings USED-FOR hierarchy of representation. Generic are analysis, and matrix. Task is neural taskonomy. OtherScientificTerm is transfer learning affinity. Method are natural language representations, and MDS. ","This paper studies the organization of linguistic representation in human fMRI data using regularized linear encoding models. The authors propose an encoder-decoder framework to learn a 'task' (linguistic representation) affinity matrix. The analysis is based on the fact that the matrix is a function of the neural taskonomy, and that the transfer learning affinity between natural language representations can be expressed in terms of MDS. The hierarchy of representation is defined using word embeddings."
2406,SP:7d5ec55a01247b65e4a8f1973d448214585d6baa,language representations USED-FOR language models. similarity measure EVALUATE-FOR language representations. MDS USED-FOR matrix of similarities. linear encoding models of fMRI recordings USED-FOR MDS dimension. MDS dimension PART-OF cortical surface. matrix of similarities USED-FOR linear encoding models. Method is low - dimensional space of language representations. Material is fMRI recordings. ,"This paper proposes a new similarity measure for evaluating language representations in language models. The main contribution of this paper is to study the low-dimensional space of language representations. The authors propose to measure the MDS dimension of the matrix of similarities between language representations using linear encoding models of fMRI recordings, which is based on the matrix MDS. The paper also proposes to embed this matrix into the cortical surface of the brain, and show that this embedding of MDS can be used to estimate the similarity of the language representations of different language models, which can then be used as a proxy for the similarity measure. Experiments are conducted on a variety of datasets, including a large number of standard and new datasets, as well as a small number of new datasets."
2407,SP:7d5ec55a01247b65e4a8f1973d448214585d6baa,"low - dimensional space USED-FOR language representations. representation embeddings USED-FOR low - dimensional space. task EVALUATE-FOR models. low - dimensional representation embedding space USED-FOR model. OtherScientificTerm are representation embedding space, low - dimensional behavior, and spatial representation of language. Task are linguistic processing, and brain encoding task. ","This paper proposes to learn language representations in a low-dimensional space based on representation embeddings. The authors argue that this representation embedding space is important because of its ability to capture the dynamics of linguistic processing. They show that models trained on this task are able to generalize well to new tasks. They also show that a model trained on a new task can generalize better to a new model that is trained in the low-dimensionality of the original language representation. They further show that this is due to the fact that the learned representations exhibit low dimensional behavior, which is consistent with the spatial representation of language. Finally, the authors conduct experiments on a brain encoding task and show that their model generalizes well to unseen tasks."
2408,SP:698d6c344fe94ea4ec3ce54601f5976d82d00b85,VAE CONJUNCTION latent space diffusion. latent space diffusion CONJUNCTION VAE. VAE PART-OF joint model. latent space diffusion PART-OF joint model. contrastive loss USED-FOR training process. approach USED-FOR few - shot generation of images. classifier scores FEATURE-OF latent codes. latent space USED-FOR classifier. latent space FEATURE-OF Langevin - like approach. Langevin - like approach USED-FOR manipulation. Task is semantic editing. ,"This paper proposes a joint model that combines VAE and latent space diffusion. The training process is based on a contrastive loss. The proposed approach is applied to few-shot generation of images, where the classifier scores of latent codes are used to train a classifier. The manipulation is performed using a Langevin-like approach in the latent space. The paper is well-written and well-motivated, and the results are promising for semantic editing."
2409,SP:698d6c344fe94ea4ec3ce54601f5976d82d00b85,setup USED-FOR few - shot conditional generation. self - supervised learning USED-FOR encoded representation. diffusion model USED-FOR unconditional generation. representation quality EVALUATE-FOR self - supervised learning. face datasets COMPARE CIFAR. CIFAR COMPARE face datasets. FID scores EVALUATE-FOR face datasets. FID scores EVALUATE-FOR conditional generation task. sample quality EVALUATE-FOR models. StyleGAN2 COMPARE models. models COMPARE StyleGAN2. StyleGAN2 COMPARE GAN - based model. GAN - based model COMPARE StyleGAN2. qualitative depiction EVALUATE-FOR image manipulation tasks. Generic is methods. OtherScientificTerm is posterior. Material is ImageNet. ,"This paper proposes a new setup for few-shot conditional generation. The proposed methods are based on the idea that self-supervised learning can improve the encoded representation quality while maintaining the representation quality. The main idea is to use a diffusion model for unconditional generation, where the posterior is sampled from the ground truth. The conditional generation task is evaluated using FID scores on two face datasets, compared to CIFAR. The sample quality of the models is compared with StyleGAN2 and a GAN-based model on ImageNet. The qualitative depiction of the image manipulation tasks is also compared."
2410,SP:698d6c344fe94ea4ec3ce54601f5976d82d00b85,Diffusion Decoding Models CONJUNCTION Contrastive Representations ( D2C ). Contrastive Representations ( D2C ) CONJUNCTION Diffusion Decoding Models. latent space FEATURE-OF diffusion modeling. diffusion modeling USED-FOR VAE model. contrastive learning USED-FOR representations. D2C USED-FOR VAEs. representations USED-FOR Few - Shot conditional generation. D2C COMPARE NVAE. NVAE COMPARE D2C. NVAE CONJUNCTION DDIM. DDIM CONJUNCTION NVAE. D2C USED-FOR few - shot conditional generation. DDIM USED-FOR few - shot conditional generation. DDIM USED-FOR unconditional generation. D2C COMPARE DDIM. DDIM COMPARE D2C. D2C COMPARE DDIM. DDIM COMPARE D2C. NVAE USED-FOR few - shot conditional generation. unconditional generation EVALUATE-FOR D2C. Method is contrastive learning based training of the inference network. OtherScientificTerm is diffusion modeling of the latent space. ,"This paper proposes Diffusion Decoding Models and Contrastive Representations (D2C) for Few-Shot conditional generation based on contrastive learning based training of the inference network. D2C is a VAE model based on diffusion modeling of the latent space, which is an extension of diffusion modeling in latent space. The authors show that the learned representations of D2Cs for VAEs can be used for few-shot conditional generation, and demonstrate empirically that the proposed representations are better than NVAE and DDIM for unconditional generation. "
2411,SP:698d6c344fe94ea4ec3ce54601f5976d82d00b85,"diffusion models HYPONYM-OF likelihood - based generative models. D2C USED-FOR diffusion models. parameterized Markov chains USED-FOR likelihood - based generative models. diffusion USED-FOR data space. prior USED-FOR latent space. prior USED-FOR image VAE ( NVAE ). latent space FEATURE-OF image VAE ( NVAE ). latent representations USED-FOR classification. D2C USED-FOR latent representations. SimCLR - style contrastive loss USED-FOR image latents. SimCLR - style contrastive loss USED-FOR D2C. rejection sampling CONJUNCTION Langevin dynamics. Langevin dynamics CONJUNCTION rejection sampling. binary attribute classifiers USED-FOR prior sampling. latents USED-FOR binary attribute classifiers. rejection sampling USED-FOR prior sampling. Langevin dynamics USED-FOR prior sampling. Task are denoising score matching lens, and conditional image generation and manipulation. ","This paper proposes to use D2C to train diffusion models, i.e., likelihood-based generative models with parameterized Markov chains, as a prior to an image VAE (NVAE) with latent space. The main idea is to use diffusion to map the data space to the latent space of the latent representations used for classification, and to use a SimCLR-style contrastive loss to learn image latents. The prior sampling is done using rejection sampling and Langevin dynamics, and the latents are used to train binary attribute classifiers for prior sampling. Experiments are performed using a denoising score matching lens, as well as conditional image generation and manipulation."
2412,SP:36d11071cbf989e1f02232d39f52a42e781a5b2b,"population augmentation graph USED-FOR contrastive learning. contrastive learning objective USED-FOR spectral decomposition. linear probe accuracy EVALUATE-FOR objective. labeled data USED-FOR linear probe. objective COMPARE SSL methods. SSL methods COMPARE objective. setup EVALUATE-FOR SSL methods. setup EVALUATE-FOR spectral contrastive objective. OtherScientificTerm are graph, and finite - sample regime. ","This paper studies the problem of contrastive learning on a population augmentation graph. Specifically, the authors propose a spectral decomposition of the graph using a contrastive learned objective. The authors show that the proposed objective improves linear probe accuracy on labeled data in a finite-sample regime. The proposed spectral contrastive objective is evaluated on a simple setup and compared to other SSL methods."
2413,SP:36d11071cbf989e1f02232d39f52a42e781a5b2b,"loss USED-FOR downstream classification guarantees. normalized adjacency matrix FEATURE-OF augmentations graph. matrix factorization FEATURE-OF normalized adjacency matrix. population guarantees CONJUNCTION finite sample guarantees. finite sample guarantees CONJUNCTION population guarantees. Method are contrastive learning, and Rademacher analysis. OtherScientificTerm is augmentations. ",This paper studies the problem of contrastive learning and proposes a new loss to improve downstream classification guarantees. The main idea is to use the normalized adjacency matrix of the augmentations graph with matrix factorization. The Rademacher analysis is used to derive population guarantees and finite sample guarantees for augmentations.
2414,SP:36d11071cbf989e1f02232d39f52a42e781a5b2b,"generalization bounds USED-FOR learning. contrastive objective USED-FOR generalization bounds. augmented views of population distribution PART-OF nodes. statistical learning theory USED-FOR contrastive learning. unrealistic assumption FEATURE-OF positive pair generation. Task is positive pair generation process. Generic is approach. OtherScientificTerm are augmentation graph, graph, disconnected subgraphs, and single - instance - level. Method is contrastive unsupervised representation learning. ","This paper studies the positive pair generation process and proposes a novel approach, called contrastive unsupervised representation learning. The key idea is to learn an augmentation graph, where nodes are augmented views of population distribution, and the goal is to obtain generalization bounds for learning using a contrastive objective. The main contribution of this paper is to apply statistical learning theory to contrastive learning, and to show that the unrealistic assumption in positive pairgeneration can be alleviated by learning disconnected subgraphs, which can be trained at a single-instance-level."
2415,SP:36d11071cbf989e1f02232d39f52a42e781a5b2b,graph of augmented examples HYPONYM-OF augmentation graph. augmentation graph USED-FOR contrastive learning. contrastive loss function USED-FOR loss. augmentation graph USED-FOR spectral clustering. spectral clustering USED-FOR loss. linear readout USED-FOR supervised objective. edges PART-OF augmentation graph. representation CONJUNCTION linear probe. linear probe CONJUNCTION representation. sampled data USED-FOR linear probe. contrastive loss COMPARE baseline contrastive loss functions. baseline contrastive loss functions COMPARE contrastive loss. Method is contrastive self - supervised learning. Metric is sample complexity. ,"This paper studies contrastive self-supervised learning. In contrastive learning, an augmentation graph (i.e., a graph of augmented examples) is used. The authors propose a new loss based on spectral clustering on the augmentation graphs using a contrastive loss function. The supervised objective is based on a linear readout. The edges of the augmented graph are sampled from the original graph. The representation and the linear probe are trained on the sampled data. The proposed contrastive weight is shown to outperform baseline contrastive losses in terms of sample complexity."
2416,SP:ae4bc7f2a00feb13e458ab17804c06709374ceee,parameterized complexity EVALUATE-FOR Bayesian Network Structure Learning. parametrization USED-FOR fixed - parameter tractability. non - zero representation CONJUNCTION additive representation. additive representation CONJUNCTION non - zero representation. complexity EVALUATE-FOR structural learning of bayesian networks. Task is Polytree Learning. ,"This paper studies the parameterized complexity of Bayesian Network Structure Learning. The authors propose a parametrization for fixed-parameter tractability, which allows for non-zero representation and additive representation. The complexity of structural learning of bayesian networks is studied in the context of Polytree Learning."
2417,SP:ae4bc7f2a00feb13e458ab17804c06709374ceee,"Method is BNSL. OtherScientificTerm are local feedback edge number, FPT, and local scores. ","This paper proposes a variant of BNSL, where the local feedback edge number is computed as a function of the FPT. The authors show that the local scores can be expressed as a weighted sum of local scores. "
2418,SP:ae4bc7f2a00feb13e458ab17804c06709374ceee,"parametrized complexity EVALUATE-FOR BN structural learning. decision problem USED-FOR learning task. dynamic programming approach USED-FOR additive case. Generic is tasks. OtherScientificTerm are local scores, treewidth, and superstructure. Metric is tractability. Task is learning polytree - shaped models. Method is polytree - shaped models. ","This paper studies the parametrized complexity of BN structural learning. The authors consider two tasks: (1) learning polytree-shaped models, and (2) learning a decision problem to solve the learning task. For the additive case, the authors propose a dynamic programming approach, where the local scores are updated based on the treewidth of the tree, and the superstructure is learned as the number of nodes increases. The tractability of the proposed approach is demonstrated on a variety of datasets."
2419,SP:ae4bc7f2a00feb13e458ab17804c06709374ceee,fixed - parameter tractability FEATURE-OF Bayesian Network Structure Learning ( BNSL ) problem. parameters FEATURE-OF network superstructure. parameters USED-FOR fixed - parameter tractability. vertex - related parameters FEATURE-OF BNSL problem. feedback edge number ( fen ) CONJUNCTION local ” version ( lfen ). local ” version ( lfen ) CONJUNCTION feedback edge number ( fen ). local ” version ( lfen ) COMPARE tree - cut width ( tcw ). tree - cut width ( tcw ) COMPARE local ” version ( lfen ). local ” version ( lfen ) HYPONYM-OF parameters. feedback edge number ( fen ) HYPONYM-OF parameters. additive representations CONJUNCTION bounded additive representations. bounded additive representations CONJUNCTION additive representations. non - zero representations CONJUNCTION additive representations. additive representations CONJUNCTION non - zero representations. non - zero representations HYPONYM-OF scoring ( function ) representations. bounded additive representations HYPONYM-OF scoring ( function ) representations. additive representations HYPONYM-OF scoring ( function ) representations. fixed - parameter tractability FEATURE-OF polytrees. OtherScientificTerm is edge - related parameters. ,"This paper studies the problem of fixed-parameter tractability in the Bayesian Network Structure Learning (BNSL) problem. Specifically, the authors consider the BNSL problem with vertex-related parameters and show that the parameters of the network superstructure can be reduced to a fixed number of parameters (e.g., feedback edge number (fen), local” version (lfen), and tree-cut width (tcw). In particular, they show that non-zero representations, additive representations, and bounded additive representations can be used as scoring (function) representations. The authors also show that polytrees can achieve similar or better performance in terms of fixed parameter tractability."
2420,SP:3dc67f04c04466b0fe5aebb01c7578cd24caee0c,"stream - based ( online ) active learning USED-FOR classifiers. classifiers USED-FOR binary classification setting. strategy USED-FOR classifiers. surrogate loss USED-FOR classifiers. label complexity CONJUNCTION error. error CONJUNCTION label complexity. multi - layer perceptrons USED-FOR classification models. labeling cost EVALUATE-FOR ALPS. multi - layer perceptrons USED-FOR datasets. Method are ALPS *, and learner. OtherScientificTerm are Pseudo - labels, and Surrogate losses. ","This paper proposes a stream-based (online) active learning to train classifiers in a binary classification setting. The authors propose a strategy called *ALPS* that trains classifiers with a surrogate loss, which aims to reduce the label complexity and the error of classifiers trained in the binary classifying setting. Pseudo-labels are learned by the learner, which are then used to train the classifiers. Surrogate losses are trained in a supervised fashion. The labeling cost of ALPS is evaluated on two datasets with multi-layer perceptrons for training classification models."
2421,SP:3dc67f04c04466b0fe5aebb01c7578cd24caee0c,online active learning USED-FOR classification. surrogate loss function USED-FOR classification algorithm. Generic is method. ,This paper studies the problem of online active learning for classification. The authors propose a novel method that learns a surrogate loss function for the classification algorithm. The proposed method is evaluated on a variety of datasets.
2422,SP:3dc67f04c04466b0fe5aebb01c7578cd24caee0c,active learning algorithm USED-FOR binary classification tasks. model USED-FOR surrogate loss. algorithm USED-FOR model. weak labels USED-FOR algorithm. OtherScientificTerm is labeled and weak - labeled points. ,This paper proposes an active learning algorithm for binary classification tasks. The proposed algorithm uses weak labels to train a model that learns a surrogate loss between labeled and weak-labeled points.
2423,SP:3dc67f04c04466b0fe5aebb01c7578cd24caee0c,"active learning algorithm USED-FOR binary classification. Surrogate Losses ( ALPS ) HYPONYM-OF active learning algorithm. streaming setting FEATURE-OF surrogate losses. requestor functions USED-FOR noise. pseudo - labeling USED-FOR noise. generalization error CONJUNCTION label complexity. label complexity CONJUNCTION generalization error. generalization error EVALUATE-FOR ALPS. label complexity EVALUATE-FOR ALPS. IWAL HYPONYM-OF ALPS. OtherScientificTerm are Pseudo - Labels, pseudo - labels, ground - truth labels, pseudo - label, and requestor function class. Generic is assumption. ","This paper proposes Surrogate Losses (ALPSL), an active learning algorithm for binary classification with Pseudo-Labels, i.e., surrogate losses in the streaming setting. The main idea is to use requestor functions as pseudo-labels to reduce the noise caused by pseudo-labeling. The assumption is that the ground-truth labels are not corrupted by the pseudo label. The authors prove that under this assumption, the generalization error and label complexity of ALPS (i.e. IWAL) can be bounded by the requestor function class."
2424,SP:0e5812d8ed33d5b6d9d59dbb2312c7b1c9363f3d,function complexity FEATURE-OF measure. complexity measure USED-FOR generalization upper bounds. It USED-FOR generalization upper bounds. generalization upper bounds USED-FOR classifiers. complexity measure USED-FOR It. empirical method USED-FOR measure. test accuracy EVALUATE-FOR baselines. small benchmark datasets EVALUATE-FOR baselines. empirical method USED-FOR it. small benchmark datasets EVALUATE-FOR empirical method. label noise FEATURE-OF resilience. resilience EVALUATE-FOR method. ,"This paper proposes a new measure of function complexity. It uses the complexity measure to derive generalization upper bounds for classifiers, and it uses an empirical method to evaluate the proposed measure. The experimental results show that the proposed method improves test accuracy over baselines on small benchmark datasets. The proposed method also improves resilience to label noise."
2425,SP:0e5812d8ed33d5b6d9d59dbb2312c7b1c9363f3d,"regularization techniques USED-FOR deep neural networks. generalization gap EVALUATE-FOR techniques. Generic are technique, network, and model. OtherScientificTerm is label noise. ",This paper studies regularization techniques for deep neural networks. The technique is based on the observation that the generalization gap between the trained network and the test set can be reduced when label noise is added to the model. 
2426,SP:0e5812d8ed33d5b6d9d59dbb2312c7b1c9363f3d,"generalization ability FEATURE-OF deep neural networks. Kolmogorov Growth "" ( KG ) HYPONYM-OF complexity measure. complexity measure USED-FOR approach. neural network training approach USED-FOR low KG condition. noisy - label case investigation EVALUATE-FOR KG - based N2N training of neural networks. OtherScientificTerm is generalization gap. ","This paper studies the generalization ability of deep neural networks. The authors propose an approach based on a complexity measure called ""Kolmogorov Growth"" (KG). The authors show that the proposed neural network training approach converges to a low KG condition, which leads to a generalization gap of $O(1/\sqrt{T})$. The authors conduct a noisy-label case investigation on KG-based N2N training of neural networks and show that their approach is effective."
2427,SP:0e5812d8ed33d5b6d9d59dbb2312c7b1c9363f3d,complexity EVALUATE-FOR neural network. Kolmogorov Growth ( KG ) EVALUATE-FOR neural network. network - to - network regularization HYPONYM-OF regularizing neural networks. KG USED-FOR generalization bounds. image benchmarks EVALUATE-FOR regularization scheme. Generic is bound. Method is N2N ). ,This paper studies the complexity of a neural network trained with Kolmogorov Growth (KG). The authors propose two ways of regularizing neural networks: (1) network-to-network regularization (N2N) and (2) using KG to derive generalization bounds. The authors show that the proposed regularization scheme is effective on image benchmarks and provide a bound on the KG of N2N.
2428,SP:d5608d3317c2b246375eb14006b9e6a6026e0ab6,"batch normalization CONJUNCTION feature - wise normalization. feature - wise normalization CONJUNCTION batch normalization. feature - wise normalization CONJUNCTION output quantization. output quantization CONJUNCTION feature - wise normalization. output quantization CONJUNCTION stop gradient. stop gradient CONJUNCTION output quantization. stop gradient CONJUNCTION memory banks. memory banks CONJUNCTION stop gradient. weight sharing CONJUNCTION batch normalization. batch normalization CONJUNCTION weight sharing. weight sharing HYPONYM-OF techniques. batch normalization HYPONYM-OF techniques. video HYPONYM-OF multi - modal signals. Method are self - supervised learning method, and self - supervised learning. Generic is it. OtherScientificTerm is branches. ","This paper proposes a self-supervised learning method. The main idea is to use multi-modal signals (e.g., video, audio, text, video, etc) as input and train it separately. This is an interesting idea, as it can be seen as an extension of existing techniques such as weight sharing, batch normalization and feature-wise normalization, output quantization, stop gradient, memory banks etc. The paper is well-written and well-motivated. The idea is interesting and interesting. However, the paper is not very well-organized and there are a lot of typos and typos in the paper, which makes it hard to follow. Also, it is not clear to me why the authors do not mention the importance of branches. The authors need to do a better job of explaining why this is a good idea and why this should be considered as an important part of self supervised learning. "
2429,SP:d5608d3317c2b246375eb14006b9e6a6026e0ab6,"covariance FEATURE-OF embedding variables. mining of contrastive pairs USED-FOR it. Task is self - supervised learning. Metric is total loss function. OtherScientificTerm are hinge loss, representation collapse, and embedding branches. Generic are model, and regularization. ",This paper studies the problem of self-supervised learning when the total loss function is large. The authors propose a hinge loss that penalizes the representation collapse when the embedding variables have high covariance. The model is trained using the mining of contrastive pairs and it is shown that this regularization leads to better embedding branches.
2430,SP:d5608d3317c2b246375eb14006b9e6a6026e0ab6,"variance term USED-FOR variance. covariance term USED-FOR network. Task is self - supervised learning. OtherScientificTerm are regularizations terms, and embedded space. ","This paper studies the problem of self-supervised learning and proposes two regularizations terms. First, the authors propose a covariance term to reduce the variance of the network. Second, they propose a new embedding of the embedded space."
2431,SP:d5608d3317c2b246375eb14006b9e6a6026e0ab6,"normalization CONJUNCTION weight sharing. weight sharing CONJUNCTION normalization. weight sharing CONJUNCTION quantization. quantization CONJUNCTION weight sharing. batch norm CONJUNCTION normalization. normalization CONJUNCTION batch norm. encoders CONJUNCTION loss function. loss function CONJUNCTION encoders. variance and covariance regularization terms USED-FOR loss function. loss function USED-FOR model. encoders USED-FOR model. model USED-FOR multimodal inputs and tasks. regularization terms FEATURE-OF encoder branch. ImageNet classification CONJUNCTION object detection tasks. object detection tasks CONJUNCTION ImageNet classification. regularization terms USED-FOR baseline methods. Method is self - supervision technique. OtherScientificTerm are variance terms, and collapsing problem. ","This paper proposes a self-supervision technique where the model is trained with encoders, a loss function with variance and covariance regularization terms, and a batch norm, normalization, weight sharing, and quantization. The model is then used to model multimodal inputs and tasks. The authors show that the encoder branch with the additional regularization term is more robust to the variance terms, which is a key factor in the collapsing problem. Experiments are conducted on ImageNet classification and object detection tasks, and the authors show the effectiveness of the proposed regularization methods compared to baseline methods."
2432,SP:bc8a9fcf7de41f1a1b0c6d0fc3fdcac5c5f87613,active reward learning method USED-FOR optimal policy. IDRL HYPONYM-OF active reward learning method. plausibly optimal policies USED-FOR optimal policy. IDRL USED-FOR policies. IDRL COMPARE baselines. baselines COMPARE IDRL. Method is reward model. OtherScientificTerm is reward. ,"This paper proposes an active reward learning method, IDRL, to learn the optimal policy from plausibly optimal policies. The key idea is to train a reward model where the reward is a function of the state of the environment. The authors show that IDRL can learn policies that are more likely to maximize the reward than baselines."
2433,SP:bc8a9fcf7de41f1a1b0c6d0fc3fdcac5c5f87613,Bayesian method USED-FOR reward function. Bayesian method USED-FOR RL setting. RL setting FEATURE-OF reward function. Generic is algorithm. Method is reward model. ,This paper proposes a Bayesian method to estimate the reward function in an RL setting. The algorithm is based on the observation that the reward model can be seen as a function of the number of states and actions taken by the agent. 
2434,SP:bc8a9fcf7de41f1a1b0c6d0fc3fdcac5c5f87613,"query selection strategy USED-FOR reward learning. reward learning USED-FOR reinforcement learning. environment dynamics USED-FOR strategy. approach USED-FOR Deep RL techniques. OtherScientificTerm is pairwise trajectory queries. Generic are approaches, and method. ","This paper proposes a query selection strategy for reward learning in reinforcement learning. The proposed strategy is based on environment dynamics, where pairwise trajectory queries are generated. Deep RL techniques have been shown to benefit from this approach. However, there has been a lot of work in recent years on improving the performance of these approaches. This paper presents a method to improve the performance."
2435,SP:bc8a9fcf7de41f1a1b0c6d0fc3fdcac5c5f87613,"approximations CONJUNCTION heuristics. heuristics CONJUNCTION approximations. approximations USED-FOR it. heuristics USED-FOR it. reward learning queries USED-FOR IDRL. preference comparisons HYPONYM-OF reward learning queries. sample efficiency EVALUATE-FOR IDRL. IDRL COMPARE baselines. baselines COMPARE IDRL. sample efficiency EVALUATE-FOR baselines. Method is Information Directed Reward Learning ( IDRL ). OtherScientificTerm are reward function, state space, optimal policy, and feedback modality. ","This paper proposes Information Directed Reward Learning (IDRL), a method for learning a reward function that is independent of the state space. In particular, it uses approximations and heuristics to approximate the optimal policy. IDRL is based on reward learning queries (preference comparisons), where the goal is to find an optimal policy that maximizes the utility of the feedback modality. Experiments show that IDRL improves sample efficiency over baselines in terms of sample efficiency."
2436,SP:d88f2bb3ed48deb04fae1b8f008ca69d8566819f,Graph hypernetworks USED-FOR method. Graph hypernetworks USED-FOR diverse architectures. in - distribution and out of distribution architectures FEATURE-OF DEEPNETS-1 M benchmark. long - range interactions PART-OF GHN. CPU CONJUNCTION GPU. GPU CONJUNCTION CPU. CIFAR-10 CONJUNCTION Imagenet. Imagenet CONJUNCTION CIFAR-10. training distribution FEATURE-OF large Resnet-50s. CIFAR10 EVALUATE-FOR they. accuracy EVALUATE-FOR they. OtherScientificTerm is normalization of predicted parameters. Method is meta - batching of architectures. ,"Graph hypernetworks are used in the proposed method, which aims to learn diverse architectures. The main contribution of the paper is the meta-batching of architectures, which allows for both in-distribution and out of distribution architectures on the DEEPNETS-1M benchmark. The authors also propose to incorporate long-range interactions in GHN, which is a natural extension of normalization of predicted parameters. The experiments on CIFAR-10 and Imagenet show that they can improve accuracy on large Resnet-50s with the same training distribution (CPU, GPU)."
2437,SP:d88f2bb3ed48deb04fae1b8f008ca69d8566819f,parameter prediction USED-FOR deep neural networks. predicting parameters USED-FOR network structures. hypernetwork USED-FOR predicting parameters. hypernetwork USED-FOR network structures. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. large neural networks USED-FOR method. ImageNet EVALUATE-FOR method. CIFAR-10 EVALUATE-FOR method. method USED-FOR architecture representation learning. ,"This paper studies the problem of parameter prediction in deep neural networks. The authors propose to use a hypernetwork for predicting parameters for different network structures. The method is evaluated on CIFAR-10 and ImageNet with large neural networks, and the results show that the proposed method can be used for architecture representation learning."
2438,SP:d88f2bb3ed48deb04fae1b8f008ca69d8566819f,"DeepNets-1 M dataset USED-FOR parameter prediction. Graph HyperNetwork USED-FOR unseen and diverse networks. Method are iterative optimization, and neural architecture representation. ","This paper proposes to use the DeepNets-1M dataset for parameter prediction. The authors propose to use iterative optimization to learn the parameters of unseen and diverse networks using Graph HyperNetwork, which is trained to predict unseen parameters. The paper also proposes a neural architecture representation for unseen networks."
2439,SP:d88f2bb3ed48deb04fae1b8f008ca69d8566819f,way USED-FOR image based deep network. SGD USED-FOR it. Method is optimization algorithms. ,This paper proposes a new way to train an image based deep network. The main idea is to train it using SGD. The experiments show that the proposed optimization algorithms outperform existing methods.
2440,SP:8b233a2a5049ccda84e8840b97b800ffc5862e16,perception - distortion USED-FOR ( image ) enhancement tradeoff. distribution FEATURE-OF reconstructions. Task is image super resolution. Generic is tradeoff. Metric is point - wise reconstruction quality ( distortion ). Material is high - resolution data. OtherScientificTerm is Gaussian distributions. ,"This paper studies perception-distortion in the (image) enhancement tradeoff. The paper focuses on image super resolution, where the tradeoff is between the point-wise reconstruction quality (distortion) and the quality of the original image. The authors show that high-resolution data with Gaussian distributions are more likely to have distortion, and that reconstructions with a different distribution will have more distortion."
2441,SP:8b233a2a5049ccda84e8840b97b800ffc5862e16,SNR FEATURE-OF image denoising. natural images USED-FOR criterion. SNR HYPONYM-OF criterion. Wasserstein bound USED-FOR conditional distribution. Wasserstein bound USED-FOR criterion. optimizer USED-FOR Distortion - perception function. visual aspect EVALUATE-FOR models. Task is Distortion - perception. Metric is Mean Squared Error ( MSE ). ,"This paper proposes a new criterion for image denoising, called SNR, based on natural images. The criterion is based on the Wasserstein bound on the conditional distribution of the target image. Distortion-perception function is trained with an optimizer that maximizes the Mean Squared Error (MSE). The authors evaluate their models on the visual aspect."
2442,SP:8b233a2a5049ccda84e8840b97b800ffc5862e16,MSE distortion CONJUNCTION Wasserstein-2 perception index. Wasserstein-2 perception index CONJUNCTION MSE distortion. perception constraint P FEATURE-OF Distortion - perception function. estimators USED-FOR Distortion - perception curve. one CONJUNCTION one. one CONJUNCTION one. MSE EVALUATE-FOR one. perfect perceptual quality constraint USED-FOR one. MSE EVALUATE-FOR one. one HYPONYM-OF estimators. one HYPONYM-OF estimators. estimators USED-FOR estimators. closed form expression USED-FOR Distortion - perception curve. Task is image compression literature. OtherScientificTerm is second - order stats. Method is super - resolution. ,"This paper studies the Distortion-perception function under the perception constraint P, which is a combination of the MSE distortion and the Wasserstein-2 perception index. In the image compression literature, the authors propose two estimators, one based on the perfect perceptual quality constraint, and one that is based on second-order stats. The authors show that these estimators can be used to derive a Distortion - perception curve with a closed form expression. They also show that the super-resolution of the original image can be reduced to a single image."
2443,SP:8b233a2a5049ccda84e8840b97b800ffc5862e16,"Wasserstein-2 distance FEATURE-OF estimators. mean - square error EVALUATE-FOR estimator. linear interpolation USED-FOR estimator. perceptual quality EVALUATE-FOR estimator. MSE EVALUATE-FOR estimator. interpolated estimator COMPARE Wasserstein-2 distance. Wasserstein-2 distance COMPARE interpolated estimator. MSE EVALUATE-FOR interpolated estimator. perceptual quality EVALUATE-FOR interpolated estimator. Metric are Wasserstein-2 distance ( perception ), and minimal mean - square error. Method is distortion - perception function. OtherScientificTerm is Wasserstein-2 ball. Task is deep image super - resolution. Generic is it. ","This paper studies the problem of estimating the Wasserstein-2 distance (perception) between two estimators with different mean-square error. The authors propose a distortion-perception function, where the estimator is a linear interpolation of the original estimator. The main contribution of this paper is to prove that the interpolated estimator has minimal mean-squares error, and that it is more accurate than the original interpolator in terms of the MSE. The paper also provides a theoretical analysis of the effect of the interpolation on the perceptual quality of the proposed estimator, and compares it to the original and interpolated versions of the WASSERSTEIN-2-distance. Finally, the authors conduct experiments on deep image super-resolution, where they show that the proposed interpolated estimation can be used to approximate the original Wasserstein-2 ball."
2444,SP:60ce257ca7c1dbbc88e4f36bad40f7eeb133368a,"means USED-FOR text feature. text feature USED-FOR nodes in textual graphs. layerwise aggregation module PART-OF Transformer encoder block. multi - head self - attention USED-FOR layerwise aggregation module. contextual representations USED-FOR node. normal data USED-FOR model. OtherScientificTerm is hidden representations of ( sampled ) neighbor nodes. Method are aggregation module, and two - stage training schedule. ","This paper proposes a means to aggregate text feature for nodes in textual graphs. The layerwise aggregation module of the Transformer encoder block is trained with multi-head self-attention, where the hidden representations of (sampled) neighbor nodes are used as input to the aggregation module. The contextual representations of each node are then used to aggregate the information of the node. The model is trained on normal data, with a two-stage training schedule."
2445,SP:60ce257ca7c1dbbc88e4f36bad40f7eeb133368a,language models CONJUNCTION GNN. GNN CONJUNCTION language models. Cascaded Transformers - GNN architectures USED-FOR representation learning on textual graphs. text encoding CONJUNCTION graph structures. graph structures CONJUNCTION text encoding. information fusion CONJUNCTION graph structures. graph structures CONJUNCTION information fusion. information fusion USED-FOR text encoding. information fusion FEATURE-OF approach. GNN components PART-OF transformer layers of language models. GraphFormers HYPONYM-OF GNN - nested Transformers. unidirectional graph attention USED-FOR graph network modules. progressive learning strategy CONJUNCTION unidirectional graph attention. unidirectional graph attention CONJUNCTION progressive learning strategy. progressive learning strategy USED-FOR graph network modules. Wiki CONJUNCTION Product Ads datasets. Product Ads datasets CONJUNCTION Wiki. DBLP CONJUNCTION Wiki. Wiki CONJUNCTION DBLP. textual graph datasets EVALUATE-FOR GraphFormers. Product Ads datasets HYPONYM-OF textual graph datasets. DBLP HYPONYM-OF textual graph datasets. Wiki HYPONYM-OF textual graph datasets. Material is textual graphs. ,"This paper proposes Cascaded Transformers-GNN architectures for representation learning on textual graphs. The proposed approach combines information fusion for text encoding and graph structures. The GNN components of the transformer layers of language models and GNN are combined to form a GNN-nested Transformers, called GraphFormers. The progressive learning strategy and unidirectional graph attention are used to train the graph network modules. The experimental results on textual graph datasets (DBLP, Wiki, and Product Ads datasets) show the effectiveness of the proposed graph formers."
2446,SP:60ce257ca7c1dbbc88e4f36bad40f7eeb133368a,GAT CONJUNCTION Transformers. Transformers CONJUNCTION GAT. Transformers USED-FOR textual graphs. architecture USED-FOR textual graphs. GraphFormer USED-FOR textual graphs. GAT USED-FOR textual graphs. GraphFormer HYPONYM-OF architecture. GAT USED-FOR GraphFormer. Transformers USED-FOR GraphFormer. Transformers USED-FOR architecture. GAT USED-FOR architecture. techniques USED-FOR textual graphs. language model CONJUNCTION GNN. GNN CONJUNCTION language model. cascade process USED-FOR techniques. language model USED-FOR nodes. ML models CONJUNCTION electrical devices. electrical devices CONJUNCTION ML models. graph aggregation CONJUNCTION encoding. encoding CONJUNCTION graph aggregation. GraphFormers USED-FOR graph aggregation. electrical devices USED-FOR transformers. ML models USED-FOR transformers. link prediction USED-FOR injected noise. GNN component PART-OF GraphFormer model. injected noise FEATURE-OF graph. Method is 2 - step training procedure. ,"This paper proposes GraphFormer, an architecture that combines GAT and Transformers to generate textual graphs. The techniques are based on the cascade process, where a language model is used to generate nodes, and a GNN is trained to predict the output of the language model. The transformers are trained using ML models and electrical devices. The authors propose a 2-step training procedure, where graph aggregation and encoding are performed using GraphFormers. The GNN component of the GraphFormer model is trained by injecting injected noise into the graph via link prediction."
2447,SP:60ce257ca7c1dbbc88e4f36bad40f7eeb133368a,GraphFormers USED-FOR text graph tasks. GNNs PART-OF transformer - based pretrained language model. graph structure USED-FOR semantic representations. Two - stage progressive training CONJUNCTION unidirectional graph aggregation. unidirectional graph aggregation CONJUNCTION Two - stage progressive training. unidirectional graph aggregation USED-FOR representation quality. Two - stage progressive training USED-FOR representation quality. Wikidata CONJUNCTION Product Graph. Product Graph CONJUNCTION Wikidata. DBMLP CONJUNCTION Wikidata. Wikidata CONJUNCTION DBMLP. OtherScientificTerm is unnecessary computation. Method is cascaded transformers - GNN models. ,"This paper proposes GraphFormers for text graph tasks. The main idea is to incorporate GNNs into a transformer-based pretrained language model to reduce unnecessary computation. Two-stage progressive training and unidirectional graph aggregation are used to improve the representation quality. The authors also propose to use the graph structure to learn semantic representations. Experiments on DBMLP, Wikidata, and Product Graph demonstrate the effectiveness of cascaded transformers-GNN models."
2448,SP:199a281592df47d71c57fdcbd24b40a0b0de9d76,"user - level differential privacy FEATURE-OF learning problems. private mean estimation algorithm USED-FOR 1 - dimensional case. empirical mean CONJUNCTION Laplace noise term. Laplace noise term CONJUNCTION empirical mean. ell-2 norm CONJUNCTION ell - infty norm. ell - infty norm CONJUNCTION ell-2 norm. random rotation USED-FOR d - dimensional setting. algorithms USED-FOR user - level DP algorithms. user - level DP algorithms USED-FOR empirical risk minimization. user - level DP algorithms USED-FOR stochastic convex optimization ( SCO ). tools USED-FOR user - level DP algorithms. subgaussian gradients USED-FOR stochastic convex optimization ( SCO ). OtherScientificTerm are privacy, privacy parameters, euclidean ball, and lower bounds. Generic are algorithm, and application. Method are ( epsilon, delta)-DP, group - privacy, 1 - dimensional solution, user - level DP, and SCO. Metric are squared ell-2 error, and error bounds. ","This paper studies the problem of user-level differential privacy in learning problems. The authors propose a private mean estimation algorithm for the 1-dimensional case, where privacy is assumed to be linear in the number of data points. The algorithm is based on (epsilon, delta)-DP, where the privacy parameters are assumed to lie in an euclidean ball, and the empirical mean and the Laplace noise term are defined as the product of the ell-2 norm and an ell-infty norm. For the d-dimensional setting, the authors consider random rotation, and derive lower bounds on the squared ellipt-2 error. These algorithms are then used to train user-levels DP algorithms for empirical risk minimization, which are then applied to stochastic convex optimization (SCO) with subgaussian gradients. In this application, group-privacy is assumed and the authors show that for any 1-dimension solution, the error bounds converge to the same as for user-Level DP, which is then used for SCO."
2449,SP:199a281592df47d71c57fdcbd24b40a0b0de9d76,"estimation error FEATURE-OF private mean estimation mechanism. mechanism USED-FOR stochastic optimization algorithms. query concentration radius FEATURE-OF error scaling. error scaling FEATURE-OF private mean estimation mechanism. algorithms USED-FOR user - level private ERM. algorithms USED-FOR user - level private SCO. user - level private ERM CONJUNCTION user - level private SCO. user - level private SCO CONJUNCTION user - level private ERM. error scaling USED-FOR algorithms. OtherScientificTerm are concentration radius of queries, concentrated queries, and concentration radius of gradients. ","This paper studies the estimation error of the private mean estimation mechanism with error scaling in the query concentration radius. The authors propose a mechanism to improve the performance of stochastic optimization algorithms by reducing the concentration radius of queries. The main contribution of this paper is to propose algorithms for both user-level private ERM and user -level private SCO that use error scaling to reduce the number of concentrated queries. In addition, the authors propose algorithms that do not rely on the concentration of gradients."
2450,SP:199a281592df47d71c57fdcbd24b40a0b0de9d76,"algorithms USED-FOR learning tasks. empirical risk minimisation CONJUNCTION stochastic convex optimisation. stochastic convex optimisation CONJUNCTION empirical risk minimisation. solutions USED-FOR problems. mean estimation CONJUNCTION empirical risk minimisation. empirical risk minimisation CONJUNCTION mean estimation. stochastic convex optimisation CONJUNCTION learning hypothesis classes. learning hypothesis classes CONJUNCTION stochastic convex optimisation. solutions USED-FOR mean estimation. finite metric entropy FEATURE-OF learning hypothesis classes. learning hypothesis classes HYPONYM-OF problems. mean estimation HYPONYM-OF problems. stochastic convex optimisation HYPONYM-OF problems. empirical risk minimisation HYPONYM-OF problems. mean estimation CONJUNCTION stochastic convex optimisation. stochastic convex optimisation CONJUNCTION mean estimation. algorithms USED-FOR mean estimation. algorithms USED-FOR stochastic convex optimisation. lower bounds USED-FOR algorithms. 1D distributions USED-FOR mean estimation. empirical risk minimisation CONJUNCTION stochastic convex optimisation. stochastic convex optimisation CONJUNCTION empirical risk minimisation. OtherScientificTerm are user - level differential privacy, uniformly concentrated queries, concentration parameter, and TV distance. Metric is privacy cost. ","This paper studies algorithms for learning tasks with user-level differential privacy. The authors consider three problems: mean estimation with 1D distributions, empirical risk minimisation and stochastic convex optimisation with learning hypothesis classes with finite metric entropy. They provide lower bounds on the privacy cost of algorithms for mean estimation in 1D, for empirical risk maximisation and for stocholderexploration with uniformly concentrated queries. They also provide an upper bound on the concentration parameter that depends on the TV distance between the query and the query."
2451,SP:199a281592df47d71c57fdcbd24b40a0b0de9d76,user - level USED-FOR DP. SGD algorithm CONJUNCTION iterative aggregator. iterative aggregator CONJUNCTION SGD algorithm. Generic is approach. ,This paper studies the problem of DP at the user-level. The authors propose a new approach to address this problem. The main idea is to combine the SGD algorithm with an iterative aggregator. Theoretical analysis of the proposed approach is provided.
2452,SP:ad5b98e656cac6eb931f80d852c397d117cf1609,"infinite time mean predictor USED-FOR model. weight decay USED-FOR infinite time mean predictor. noisy full batch gradient descent USED-FOR weight decay. formalism USED-FOR model weights. MSE loss CONJUNCTION quadratic teacher student fully connected DNNs. quadratic teacher student fully connected DNNs CONJUNCTION MSE loss. weight decay CONJUNCTION gradient noise parameters. gradient noise parameters CONJUNCTION weight decay. OtherScientificTerm are model output, and lazy and feature learning regimes. Generic is case. ",This paper studies the problem of learning an infinite time mean predictor for a model using weight decay with noisy full batch gradient descent. The authors propose a formalism for learning model weights that can be expressed as the sum of the MSE loss and the quadratic teacher student fully connected DNNs. The main contribution of the paper is to provide a theoretical analysis of the relationship between weight decay and the gradient noise parameters. The paper also provides a theoretical justification for the choice of model output and shows that this case can be extended to both lazy and feature learning regimes.
2453,SP:ad5b98e656cac6eb931f80d852c397d117cf1609,finite - width DNNs COMPARE infinite - width GP limit. infinite - width GP limit COMPARE finite - width DNNs. GP limit USED-FOR DNNs. noisy gradients CONJUNCTION weight decay. weight decay CONJUNCTION noisy gradients. mean predictor PART-OF finite DNN. weight decay USED-FOR finite DNN. noisy gradients USED-FOR finite DNN. cumulants of the prior USED-FOR approximation. framework USED-FOR toy models. large - training - set approximation USED-FOR cumulants. feature - learning regime FEATURE-OF phase transition. large - training - set approximation USED-FOR two - layer linear CNN. Task is feature learning. Method is GP regression. ,"This paper studies the problem of feature learning in finite-width DNNs instead of the infinite-width GP limit commonly used in DNN. In particular, the authors consider a finite DNN with noisy gradients and weight decay as the mean predictor. The authors propose an approximation based on the cumulants of the prior, which can be trained using a large-training-set approximation for a two-layer linear CNN. The proposed framework is applied to toy models, where the authors show that the phase transition in the feature-learning regime is similar to GP regression."
2454,SP:ad5b98e656cac6eb931f80d852c397d117cf1609,"finite size effects FEATURE-OF Deep neural networks ( DNNs ). GPs CONJUNCTION DNNs. DNNs CONJUNCTION GPs. regime FEATURE-OF DNNs. feature learning HYPONYM-OF regime. it USED-FOR feature learning regime. partition function FEATURE-OF DNN posterior distribution. MSE loss FEATURE-OF noisy ( full batch ) gradient flow. hyper - parameter USED-FOR over - parametrization. GP kernel USED-FOR GP - DNN equivalence. GP - like expression USED-FOR infinite-$C$ limit. GP - like expression USED-FOR predictive mean. second order expansion of the action USED-FOR posterior covariance. teacher - student set up FEATURE-OF linear two - layer CNN. cumulants USED-FOR architecture. feature learning transition USED-FOR model. rank - one perturbation FEATURE-OF Wishart matrix. rank - one perturbation FEATURE-OF empirical weight covariance matrix. Wishart matrix USED-FOR empirical weight covariance matrix. average pooling and quadratic activations FEATURE-OF two - layer FCN. rank-1 teacher FEATURE-OF two - layer FCN. OtherScientificTerm are infinite width / channel limit, DNNs ’ parameters, prior distribution, Equivalent Kernel limit, analytical expression, proportionality factor, spiked Marchenko - Pastur eigenvalue spectrum, and cumulant generating function. Method are saddle point approximation, DNN, and DNN architectures. Generic is two. ","Deep neural networks (DNNs) with finite size effects have been studied in the infinite width/channel limit. In this paper, the authors study the infinite size effects of DNNs in the regime of feature learning, i.e., when the DNN’s parameters are drawn from the prior distribution. In particular, they consider the case of a linear two-layer CNN with a teacher-student set up with a rank-1 teacher and a teacher and student set up, where the teacher is the class of GPs and the student is a DNN. In the infinite-$C$ limit, they show that in the Equivalent Kernel limit, GPs can be viewed as DNN-GP equivalence with a GP kernel, which is a generalization of the GP-DNN equivalence in the $C$ regime. The authors show that under the saddle point approximation, this can be extended to an infinite width / channel limit, and extend it to the feature learning regime, where it is shown that the posterior covariance can be expressed as a second order expansion of the action. They also show that the MSE loss of a noisy (full batch) gradient flow can be approximated as a partition function of the original DNN posterior distribution, and show that this is equivalent to an over-parametrization of the hyper-parameter, which they call the proportionality factor. Finally, they provide an analytical expression for the effect of the proportional factor on the predictive mean, which uses a GP-like expression to derive the $p(x,y)$ limit of the posterior mean of a given DNN, which can be interpreted as a function of x,y,y. They show that for any DNN with average pooling and quadratic activations, the empirical weight covariance matrix can be represented as a Wishart matrix with rank-one perturbation to the Wishart matrices. They then show that if the model is trained with a feature learning transition, then the corresponding cumulant generating function can be seen as a linear combination of cumulants of the architecture. They provide a theoretical analysis of two DNN architectures, one of which is based on the two. The first one is a variant of the spiked Marchenko-Pastur eigenvalue spectrum."
2455,SP:ad5b98e656cac6eb931f80d852c397d117cf1609,"wide neural networks USED-FOR feature learning. shifted targets FEATURE-OF GP regression. GP regression USED-FOR mean predictions. noisy gradients USED-FOR DNNs. feature learning COMPARE infinite - width GPs. infinite - width GPs COMPARE feature learning. sample complextiy FEATURE-OF student - teacher tasks. wide enough but finite NNs COMPARE infinite - width GPs. infinite - width GPs COMPARE wide enough but finite NNs. wide enough but finite NNs USED-FOR feature learning. sample complextiy EVALUATE-FOR feature learning. student - teacher tasks EVALUATE-FOR infinite - width GPs. sample complextiy EVALUATE-FOR infinite - width GPs. feature learning regime CONJUNCTION non - feature learning regime. non - feature learning regime CONJUNCTION feature learning regime. phase transition FEATURE-OF distribution of eigenvalues. OtherScientificTerm are MSE loss, outlier eigenvalue, and teacher's features. Method are toy CNN models, and CNN instantiation. ","This paper studies the problem of feature learning in wide neural networks. In particular, the authors consider the case of DNNs with noisy gradients, where the mean predictions are obtained using GP regression with shifted targets. The authors show that feature learning with wide enough but finite NNs can outperform infinite-width GPs in terms of sample complextiy on student-teacher tasks. The main contribution of the paper is the analysis of the MSE loss, which shows that the outlier eigenvalue is a function of the teacher's features. The paper also presents toy CNN models to demonstrate the effect of the CNN instantiation on the feature learning regime and the non-feature learning regime. Finally, the paper provides a theoretical analysis of phase transition in the distribution of eigenvalues."
2456,SP:231655b9fad6c76eb0ff1ba305ed421f5c293623,inductive biases USED-FOR compositional communication. noisy channel USED-FOR compositional language emergence. loss function FEATURE-OF noisy channel. Task is learning compositional emergent communication protocols. Method is compositional emergent communication protocols. OtherScientificTerm is speaker - listener game. ,"This paper studies the problem of learning compositional emergent communication protocols. The authors propose to use inductive biases to learn compositional communication. Specifically, the authors consider a speaker-listener game, where the speaker is the listener and the listener is the speaker. In this setting, a noisy channel with a different loss function is used for compositional language emergence."
2457,SP:231655b9fad6c76eb0ff1ba305ed421f5c293623,"noise FEATURE-OF emergent communication. OtherScientificTerm are noisy channel, languages, and compositionality. Task are emergent communication protocol, and emergent communication literature. Metric is accuracy. ","This paper studies the problem of emergent communication with noise in the presence of a noisy channel. In particular, the authors consider the setting where the noisy channel is generated from a set of languages, and the goal is to learn an emergent protocol that is robust to the noise. The authors provide a theoretical analysis of the effect of the noise on the accuracy of the learned language, and provide some empirical results that are consistent with the results in the emergent language literature. They also provide some theoretical analysis on compositionality, which is an important topic in the literature."
2458,SP:231655b9fad6c76eb0ff1ba305ed421f5c293623,"utterance noise USED-FOR compositional utterances. utterance noise PART-OF sender - receiver emergent communications architecture. local minimum FEATURE-OF loss function. non - symbolic inputs COMPARE bag of attributes input. bag of attributes input COMPARE non - symbolic inputs. conflict count CONJUNCTION context independence. context independence CONJUNCTION conflict count. topographic similarity CONJUNCTION conflict count. conflict count CONJUNCTION topographic similarity. context independence CONJUNCTION positional disentanglement. positional disentanglement CONJUNCTION context independence. positional disentanglement HYPONYM-OF compositional metrics. topographic similarity HYPONYM-OF compositional metrics. context independence HYPONYM-OF compositional metrics. conflict count HYPONYM-OF compositional metrics. OtherScientificTerm are global minimum, bag of attributes, vocab size, noise, inductive biases, and message length. Method are sender network, Gumbel, and discrete sampling. ","This paper proposes a sender-receiver emergent communications architecture that incorporates utterance noise to generate compositional utterances. The loss function is defined as the local minimum of a global minimum, where the global minimum is the difference between the non-symbolic inputs and the bag of attributes input. The sender network is trained using Gumbel, and the receiver network uses discrete sampling. The compositional metrics considered are topographic similarity, conflict count, context independence, and positional disentanglement. The authors show that the vocab size of the input is correlated with the noise, and that the inductive biases are correlated with message length."
2459,SP:231655b9fad6c76eb0ff1ba305ed421f5c293623,"noisy channel USED-FOR emergence of compositionality. encoder decoder setup FEATURE-OF noisy channel. definition USED-FOR compositionality. definition USED-FOR non - hierarchical case. compositionality FEATURE-OF non - hierarchical case. noisy channel USED-FOR inductive bias. J_2 USED-FOR encoding. Task are compositional generalisation, and compositional emergence. OtherScientificTerm are prior information, and J_1. Method are training process, and compositional encoding. ","This paper studies the emergence of compositionality in the presence of a noisy channel in an encoder decoder setup. The authors propose a new definition of “compositionality” in the non-hierarchical case, which is motivated by the problem of compositional generalisation. They show that the inductive bias induced by the noisy channel can lead to compositional emergence. They then propose a training process where the prior information, J_1, is used to learn the compositional encoding, and J_2 is used for the final encoding."
2460,SP:9d326254d77a188baf5bde39229c09b3966b5418,"self - attention primitive PART-OF Transformers. linear projections USED-FOR it. full receptive field FEATURE-OF DWise Conv. DWise Conv USED-FOR spatial mixing. top-1 accuracy EVALUATE-FOR ConvNet baselines. ImageNet benchmark EVALUATE-FOR ConvNet baselines. throughput EVALUATE-FOR ConvNet baselines. throughput EVALUATE-FOR top-1 accuracy. short sequence benchmarks PART-OF NLP tasks. Translation HYPONYM-OF short sequence benchmarks. Method are MLP - based architecture, and MLP - Mixer. OtherScientificTerm is params. ","This paper proposes an MLP-based architecture for spatial mixing. In particular, the authors propose to replace the self-attention primitive in Transformers with a DWise Conv that has a full receptive field and it uses linear projections instead of the linear projections used in Transformers. The authors claim that this allows to achieve better top-1 accuracy compared to the ConvNet baselines on the ImageNet benchmark, while maintaining the same throughput as the other ConvNet baseline.  The authors also propose a variant of MLP - Mixer, which is able to mix the params of the original MLP.  Experiments are conducted on short sequence benchmarks in NLP tasks such as Translation. "
2461,SP:9d326254d77a188baf5bde39229c09b3966b5418,"linear layer USED-FOR interaction between the patches. MLP HYPONYM-OF residual operations. image patches USED-FOR ResMLP. linear layer USED-FOR ResMLP. linear layer HYPONYM-OF residual operations. single hidden layer USED-FOR MLP. residual operations USED-FOR ResMLP. self - attention layer CONJUNCTION linear layer cross patches. linear layer cross patches CONJUNCTION self - attention layer. ViT COMPARE ResMLP. ResMLP COMPARE ViT. pooling layers CONJUNCTION normalization layers. normalization layers CONJUNCTION pooling layers. Residual Multi - Layer Perceptrons COMPARE ImageNet-1k training only. ImageNet-1k training only COMPARE Residual Multi - Layer Perceptrons. accuracy / complexity trade - offs EVALUATE-FOR ImageNet-1k training only. accuracy / complexity trade - offs EVALUATE-FOR Residual Multi - Layer Perceptrons. accuracy / complexity trade - offs COMPARE Vision Transformers. Vision Transformers COMPARE accuracy / complexity trade - offs. self - supervised method DINO USED-FOR ResMLP. distillation methods USED-FOR ResMLP. linear layer USED-FOR spatial interaction. ResMLP USED-FOR machine translation. Task is image classification. Generic are them, and network. Method is linear classifier. ","This paper proposes ResMLP, which uses residual operations (i.e., a linear layer to model the interaction between the patches and a single hidden layer for the MLP) for image classification. In particular, the authors propose to use residual operations such as a self-attention layer, linear layer cross patches, and linear layer for each layer of the original MLP. The authors argue that these residual operations can be used to improve the performance of image classification by reducing the number of parameters and making them more interpretable. They compare ViT with ResMLM and show that ViT outperforms ResMLMLP in terms of accuracy/complexity trade-offs compared to ImageNet-1k training only, while Residual Multi-Layer Perceptrons outperform Vision Transformers in accuracy / complexity trade-offs. They also show that the self-supervised method DINO can be applied to ResMLL with distillation methods, and that the network can be trained with pooling layers and normalization layers. Finally, they show that using a linear classifier can improve the spatial interaction with a single linear layer, which is useful for machine translation."
2462,SP:9d326254d77a188baf5bde39229c09b3966b5418,"vision Transformers USED-FOR tasks. linear / MLP layers USED-FOR image patches. setups EVALUATE-FOR ResMLP. supervised and self - supervised training HYPONYM-OF setups. Method are Transformer architecture, and neural architectures. Generic is It. ","This paper proposes a new Transformer architecture called ResMLP. It is based on the idea that vision Transformers can be used for different tasks. The key idea is to use linear/MLP layers to represent image patches, which is similar to previous neural architectures. Experiments on two setups (supervised and self-supervised training) demonstrate the effectiveness of ResMLM."
2463,SP:9d326254d77a188baf5bde39229c09b3966b5418,$ T \times T$ linear layer USED-FOR long - range communication. self - attention layer PART-OF vision transformer. $ T \times T$ linear layer USED-FOR self - attention layer. model COMPARE vision transformer. vision transformer COMPARE model. self - supervised and transfer learning CONJUNCTION knowledge distillation. knowledge distillation CONJUNCTION self - supervised and transfer learning. vision transformer USED-FOR self - supervised and transfer learning. vision transformer USED-FOR supervised. supervised CONJUNCTION self - supervised and transfer learning. self - supervised and transfer learning CONJUNCTION supervised. vision transformer USED-FOR knowledge distillation. self - supervised and transfer learning EVALUATE-FOR model. Method is ResMLP architecture. ,"This paper proposes a new ResMLP architecture. The key idea is to replace the self-attention layer in the vision transformer with a $T \times T$ linear layer for long-range communication. The proposed model is evaluated on supervised, self-supervised and transfer learning, and knowledge distillation."
2464,SP:6dabaca9a77620b7c4019bf5f9c2a88628fc691c,"learning algorithms USED-FOR settings. learning setting COMPARE multiclass learning setting. multiclass learning setting COMPARE learning setting. Task is online learning setting. Method are distance function, and learner. Generic is algorithms. ","This paper studies the online learning setting, where the distance function between the learner and the teacher is known. The authors propose learning algorithms for both settings, and compare their performance in the learning setting with the multiclass learning setting. They also provide theoretical analysis of their algorithms."
2465,SP:6dabaca9a77620b7c4019bf5f9c2a88628fc691c,"Method are online multiclass classifier, and classifier. OtherScientificTerm are loss function, and distance based loss. Generic is algorithm. ",This paper studies the problem of online multiclass classifier. The main idea is to learn a loss function that maximizes the distance between the classifier and the target class. The authors propose to use a distance based loss. The algorithm is evaluated on a variety of datasets.
2466,SP:6dabaca9a77620b7c4019bf5f9c2a88628fc691c,"inner product CONJUNCTION p - norm. p - norm CONJUNCTION inner product. inner product HYPONYM-OF distance ”. p - norm HYPONYM-OF distance ”. Task is online prediction. OtherScientificTerm are k - centred nearest - neighbour partition, distance ” function, loss bounds, and margin ” term. Method is online learning. ","This paper studies the problem of online prediction, where the goal is to predict a k-centred nearest-neighbour partition. The authors propose a “distance” function, which is a combination of “inner product” and “p-norm”. They provide loss bounds for the “margin” term, and show that online learning can be done efficiently."
2467,SP:6dabaca9a77620b7c4019bf5f9c2a88628fc691c,"lower bounds FEATURE-OF convex classes. lower bounds FEATURE-OF it. Task is online learning problem. OtherScientificTerm are d - dimensional unit ball, nearest - neighbor partition of the ball, and upper bounds. ","This paper studies the online learning problem in the setting of a d-dimensional unit ball, where the learner has access to the nearest-neighbor partition of the ball. In particular, it provides lower bounds on convex classes, and upper bounds on the number of samples required to solve the problem."
2468,SP:5c0114535065d5125349f00bafdbccc911461ede,loss terms USED-FOR Transformer - based models. Transformer - based models USED-FOR explicit reasoning. oracle - trained VQA models COMPARE ones. ones COMPARE oracle - trained VQA models. loss terms USED-FOR explicit reasoning. predicted programs USED-FOR executable programs. loss USED-FOR auxiliary regularization. loss USED-FOR training. loss USED-FOR modular network. predicted programs USED-FOR inference. out - of - distribution evaluation PART-OF GQA - OOD. out - of - distribution evaluation FEATURE-OF GQA dataset. GQA dataset EVALUATE-FOR advantage. training efficiency EVALUATE-FOR system. OtherScientificTerm is executable program / functions. Method is regular Transformer - based VQA models. ,"This paper proposes two new loss terms for Transformer-based models to improve explicit reasoning. The first loss is an auxiliary regularization that encourages the oracle-trained VQA models to be more interpretable than the ones trained without the loss. The second loss is a modular network that learns to predict executable program/functions. The advantage is evaluated on the GQA dataset with out-of-distribution evaluation (GQA-OOD), where the predicted programs are used for inference. The results show that the proposed system improves training efficiency compared to regular Transformer -based VQAs models."
2469,SP:5c0114535065d5125349f00bafdbccc911461ede,"noisy / imperfect visual input USED-FOR VQA models. program supervision USED-FOR transferring reasoning patterns. clean / oracle visual inputs USED-FOR transferring reasoning patterns. sample complexity EVALUATE-FOR predicting programs. Method is object detectors. OtherScientificTerm are reasoning patterns, imperfect inputs, and reasoning skills. Material is GQA / GQA - OOD benchmarks. ","This paper studies the problem of transferring reasoning patterns from clean/oracle visual inputs to VQA models with noisy/imperfect visual input. The authors show that object detectors can transfer reasoning patterns to unseen objects without program supervision. They also show that predicting programs with high sample complexity can transfer well to environments with imperfect inputs. Finally, the authors conduct extensive experiments on GQA/GQA-OOD benchmarks to demonstrate the effectiveness of reasoning skills."
2470,SP:5c0114535065d5125349f00bafdbccc911461ede,visual input USED-FOR transfer learning. program supervision USED-FOR loss terms. loss terms USED-FOR transformer based models. noise FEATURE-OF model. robustness EVALUATE-FOR model. Task is visual question answering. ,"This paper studies the problem of transfer learning with visual input. Specifically, the authors focus on the task of visual question answering. The authors propose to use program supervision to design loss terms for transformer based models. The proposed loss terms improve the robustness of the model to noise."
2471,SP:5c0114535065d5125349f00bafdbccc911461ede,approach USED-FOR VQA. model USED-FOR it. oracle visual data USED-FOR model. it USED-FOR distributional shifts. model USED-FOR reasoning program. reasoning program CONJUNCTION VQA answer. VQA answer CONJUNCTION reasoning program. models USED-FOR reasoning skills. oracle visuals USED-FOR models. pre - trained vision models USED-FOR noisier visuals. ,"This paper presents an approach to VQA where a model is trained on oracle visual data, and then it is used to generate a new reasoning program and a new version of the original reasoning program. The authors argue that models trained on the oracle visuals can learn better reasoning skills and that it can better handle distributional shifts. They also argue that pre-trained vision models are more sensitive to noisier visuals."
2472,SP:40fd96105e77063de4a07d4b36fe19385434c533,"unbounded - precision neurons USED-FOR RNN. bounded - precision neurons CONJUNCTION stacks. stacks CONJUNCTION bounded - precision neurons. stacks USED-FOR RNN. bounded - precision neurons USED-FOR RNN. RNN USED-FOR Turing machine. bounded - precision neurons CONJUNCTION stacks. stacks CONJUNCTION bounded - precision neurons. bounded - precision neurons USED-FOR RNN. bounded tape USED-FOR Turing machine. OtherScientificTerm are maximum tape length, and RNN neurons. ",This paper proposes to use unbounded-precision neurons to train an RNN. The RNN is trained with bounded-probability neurons and stacks. The authors show that the RNN can be used to train a Turing machine on bounded tape. The maximum tape length is defined as the number of RNN neurons.
2473,SP:40fd96105e77063de4a07d4b36fe19385434c533,"Turing completeness FEATURE-OF recurrent neural networks. memory - augmented RNN architecture COMPARE stack RNNs. stack RNNs COMPARE memory - augmented RNN architecture. construction USED-FOR Turing machine. unbounded - precision RNN USED-FOR construction. unbounded - precision RNN USED-FOR Turing machine. construction COMPARE that. that COMPARE construction. simulation time USED-FOR construction. neurons USED-FOR simulation. precision FEATURE-OF neurons. dynamic memory module USED-FOR RNN. it USED-FOR Turing machine. OtherScientificTerm are Turing machine's tape, simulated Turing machine, simulated Turing machine's computation length, infinite precision, Turing machine computations, and tape. Generic is architecture. ","This paper studies the problem of Turing completeness in recurrent neural networks. The authors propose a memory-augmented RNN architecture that is more efficient than stack RNNs. The construction of a Turing machine with an unbounded-precision RNN requires less simulation time than that of the original Turing machine. The main contribution of the paper is that the authors show that the Turing machine's tape can be used to construct a simulated Turing machine that has infinite precision. The paper also shows that this architecture is computationally efficient since it can be applied to any Turing machine computations. In addition, the authors propose to use a dynamic memory module to update the parameters of the RNN during the simulation. The precision of the neurons in the simulation is controlled by the length of the tape."
2474,SP:40fd96105e77063de4a07d4b36fe19385434c533,dynamic - growing memory module USED-FOR RNNs. dynamic - growing memory module USED-FOR Turing machines of bounded precision. RNNs USED-FOR Turing machines of bounded precision. Turing machine USED-FOR unbounded precision RNN. fractal encoding USED-FOR symbols. fractal encoding USED-FOR encoding. unbounded precision RNN USED-FOR Turing machine. neurons PART-OF RNN. stack of neurons PART-OF memory module. memory module USED-FOR unbounded - precision definitions. bounded - precision RNN USED-FOR Turing machine. stack USED-FOR bounded - precision RNN. Turing - completeness FEATURE-OF precision - bounded RNN. bounded - precision RNN USED-FOR Turing machine. Generic is it. Method is growing memory module. ,"This paper proposes a dynamic-growing memory module for training RNNs for Turing machines of bounded precision. The encoding is based on fractal encoding for symbols. The memory module consists of a stack of neurons, each of which is used to train an unbounded precision RNN for a Turing machine. This memory module can be used to derive unbounded-precision definitions, and it can also be used as a growing memory module. Theoretical results are provided to show that a precision-bounded RNN with Turing-completeness can be trained with this stack."
2475,SP:40fd96105e77063de4a07d4b36fe19385434c533,un - bounded precision RNNs USED-FOR turing machines. growing / shrinking memory module USED-FOR un - bounded precision RNNs. Task is simulating turing machines. ,This paper studies the problem of simulating turing machines with un-bounded precision RNNs with a growing/shrinking memory module. 
2476,SP:3f33489b98ba6145fd4e334669493f15a63455f4,"quantile regression USED-FOR quantile. quantile regression algorithm USED-FOR predictor. Method are linear quantile regression, pinball - loss based ERM quantile regression solution, and linear quantile model. OtherScientificTerm are predictive interval, one - sided interval, dimensionality, dimension, and weight estimation error. Metric is coverage. ","This paper studies the problem of linear quantile regression, where the quantile of the predictor is estimated using the standard quantile regressions. The authors propose a pinball-loss based ERM quantile regressive solution where the predictive interval is a one-sided interval and the dimensionality is a function of the dimension of the data points. The main contribution of this paper is to propose a quantile model that is linear in the sense that the weight estimation error is proportional to the dimension and not to the number of points in the interval. The paper also proposes a quanticle regression algorithm that learns a predictor that is invariant to dimensionality and thus can be used to estimate the coverage. "
2477,SP:3f33489b98ba6145fd4e334669493f15a63455f4,"under - coverage bias FEATURE-OF linear quantile regression. estimation error FEATURE-OF linear coefficients. estimation error FEATURE-OF bias term. under- or over - coverage effect FEATURE-OF estimation error. Task is coverage of quantile regression algorithms. Method is quantile regression algorithms. OtherScientificTerm are marginal coverage, and noise distribution. ","This paper studies the under-coverage bias in linear quantile regression. The main contribution of this paper is to study the bias term of the estimation error of the linear coefficients, which can have an under- or over-covering effect. The authors show that the marginal coverage of the coefficients can be bounded by a function of the noise distribution. This is an interesting result, as it can be used to improve the coverage of quantile regressions."
2478,SP:3f33489b98ba6145fd4e334669493f15a63455f4,Quantifying predictive uncertainty USED-FOR high - stakes prediction problems. approach USED-FOR quantifying predictive uncertainty. quantifying predictive uncertainty USED-FOR regression problems. quantile regression HYPONYM-OF approach. under - coverage FEATURE-OF quantile regressors. Method is realizable linear setting. OtherScientificTerm is functional class. Task is under - parameterized setting. ,"Quantifying predictive uncertainty for high-stakes prediction problems is an important problem. This paper proposes an approach called quantile regression, which is a general approach to quantifying the predictive uncertainty of regression problems in a realizable linear setting. The main contribution of this paper is to show that quantile regressors suffer from under-coverage due to the under-parameterization of the functional class. The paper also provides a theoretical analysis of the underparameterized setting."
2479,SP:3f33489b98ba6145fd4e334669493f15a63455f4,"under - coverage phenomenon FEATURE-OF quantile regression. under - parameterized high - dimensional proportional limit FEATURE-OF Gaussian linear model. Gaussian linear model USED-FOR under - coverage. OtherScientificTerm are continuous distribution, noise distribution, and high - dimensional linear coefficient. Material is synthetic and real data. ","This paper studies the under-coverage phenomenon in quantile regression, where the continuous distribution of the data is different from the noise distribution. The authors show that under-covering occurs in a Gaussian linear model with an under-parameterized high-dimensional proportional limit, and show that this is due to the fact that the high-dimensionality of the noise is not bounded by a constant, but rather by a fixed function of the dimension of the input data. The paper also provides a theoretical analysis of this phenomenon on both synthetic and real data."
2480,SP:ecb9c7c11dfb450d8e76504d42309b1888023d26,RL - based memory management policy USED-FOR CIL problem. RL framework USED-FOR policy. policy gradient method USED-FOR policy. RL framework USED-FOR policy gradient method. LUCIR CONJUNCTION PODNet. PODNet CONJUNCTION LUCIR. RMM COMPARE state - of - the - art methods. state - of - the - art methods COMPARE RMM. PODNet HYPONYM-OF state - of - the - art methods. LUCIR HYPONYM-OF state - of - the - art methods. OtherScientificTerm is memory. ,"This paper proposes an RL-based memory management policy for solving the CIL problem. The policy gradient method is based on the RL framework to learn a policy that maximizes the performance of the policy. The main contribution of this paper is the introduction of the idea of RMM, which is more efficient than previous state-of-the-art methods such as LUCIR and PODNet in terms of memory."
2481,SP:ecb9c7c11dfb450d8e76504d42309b1888023d26,"random / herding USED-FOR memory sampling strategy. sampler USED-FOR pseudo - CL problem. CL method USED-FOR model. sampling configuration USED-FOR CL method. policy components USED-FOR CL. policy USED-FOR memory. memory USED-FOR CL problem. continual learning approaches USED-FOR CL problem. memory selection COMPARE memories. memories COMPARE memory selection. CIFAR100 EVALUATE-FOR memories. Imagenet1000 datasets EVALUATE-FOR memories. Generic is strategy. Method are RL policy, and RL algorithm. OtherScientificTerm is CL setup. Material is pseudo - CL test set. ","This paper proposes a new memory sampling strategy based on random/herding. The proposed strategy is based on the observation that the RL policy tends to sample from a small subset of the training set. The authors propose to use a pseudo-CL problem with a sampler to sample the samples from the RL algorithm and then use a CL method with the same sampling configuration to train the model. The main idea is that the CL problem can be solved with a single memory that can be sampled from the policy components of the CL. The paper shows that continual learning approaches can be used to solve this CL problem. Experiments on CIFAR100 and Imagenet1000 datasets show that the proposed memory selection performs better than the original memories in the original CL setup. Finally, the authors provide a set of experiments on a pseudo -CL test set to demonstrate the effectiveness of the proposed strategy."
2482,SP:ecb9c7c11dfb450d8e76504d42309b1888023d26,"memory management USED-FOR Class Incremental Learning ( CLI ). high - entropy samples CONJUNCTION low - entropy samples. low - entropy samples CONJUNCTION high - entropy samples. method COMPARE POD - AANets. POD - AANets COMPARE method. CIFAR CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR. method COMPARE LUCIR. LUCIR COMPARE method. POD - AANets CONJUNCTION LUCIR. LUCIR CONJUNCTION POD - AANets. ImageNet EVALUATE-FOR POD - AANets. CIFAR EVALUATE-FOR POD - AANets. ImageNet EVALUATE-FOR LUCIR. CIFAR EVALUATE-FOR LUCIR. ImageNet EVALUATE-FOR method. CIFAR EVALUATE-FOR method. memory management system COMPARE systems. systems COMPARE memory management system. Generic is models. Method are hierarchical reinforcement learning approach, and hierarchical RL. OtherScientificTerm is old and new memories. ",This paper studies the problem of memory management in Class Incremental Learning (CLI). The authors propose a hierarchical reinforcement learning approach where the models are trained jointly on both old and new memories. The hierarchical RL is based on the idea that the high-entropy samples should be used to train the models and the low-Entropy samples are used to learn the models. The proposed method is evaluated on CIFAR and ImageNet and compared to POD-AANets and LUCIR. The results show that the proposed memory management system outperforms the other systems.
2483,SP:ecb9c7c11dfb450d8e76504d42309b1888023d26,approach USED-FOR class - incremental learning problem. Reinforcement learning USED-FOR dynamic memory management. Method is class - incremental learning. OtherScientificTerm is forgetting. ,"This paper proposes an approach to tackle the class-incremental learning problem. Reinforcement learning is an important problem in dynamic memory management, and this paper proposes a way to address the problem of learning to adapt to new tasks without forgetting. The paper is well-written and well-motivated. "
2484,SP:1137ed24393a24f24e9a36e1586e6924a55d627e,"communication frequency FEATURE-OF local SGD. homogeneous data assumption USED-FOR local SGD. convergence rates COMPARE convergence rates. convergence rates COMPARE convergence rates. convergence rates USED-FOR FedAC ( Yuan and Ma ). linear convergence FEATURE-OF it. Method are One - shot averaging, and Federated Accelerated Stochastic Gradient Descent. OtherScientificTerm is FedAC's guarantee. Task is Neural Information Processing Systems. ","This paper studies the problem of local SGD with communication frequency under the homogeneous data assumption. One-shot averaging is used to approximate the convergence of FedAC (Yuan and Ma). The authors provide convergence rates that are better than the standard convergence rates for FedAC. The main contribution of this paper is to prove that Federated Accelerated Stochastic Gradient Descent (FedAC) has linear convergence, which is in contrast to FedAC's guarantee. Experiments are conducted on Neural Information Processing Systems."
2485,SP:1137ed24393a24f24e9a36e1586e6924a55d627e,"convergence FEATURE-OF Local SGD. logarithmic terms PART-OF Local SGD. convergence FEATURE-OF strongly convex functions. uniform - with - strong - growth noise FEATURE-OF strongly convex functions. sub - Gaussian noise USED-FOR one - shot averaging. one - shot averaging FEATURE-OF PL functions. sub - Gaussian noise FEATURE-OF PL functions. linear speed - up EVALUATE-FOR it. neural network training USED-FOR FL. Generic is theory. OtherScientificTerm are iteration counter, and communication rounds. ","This paper studies the convergence of Local SGD with logarithmic terms in the case of strongly convex functions with uniform-with-strong-growth noise. The main contribution of the paper is to prove that PL functions with sub-Gaussian noise are invariant to one-shot averaging. The theory is based on the observation that if the iteration counter is small enough, then it can achieve linear speed-up in the number of communication rounds. The paper then applies this theory to FL with neural network training and shows that FL can be speeded up by a factor of 2."
2486,SP:1137ed24393a24f24e9a36e1586e6924a55d627e,local SGD CONJUNCTION one - shot averaging. one - shot averaging CONJUNCTION local SGD. Method is synchronization scheme. ,This paper proposes a synchronization scheme that is based on the idea of local SGD and one-shot averaging. The main contribution of this paper is the introduction of the synchronization scheme. The paper is well-written and easy to follow. 
2487,SP:1137ed24393a24f24e9a36e1586e6924a55d627e,communication efficacy CONJUNCTION linear speed - up. linear speed - up CONJUNCTION communication efficacy. local SGD CONJUNCTION One - Shot averaging. One - Shot averaging CONJUNCTION local SGD. linear speed - up EVALUATE-FOR local SGD. communication efficacy EVALUATE-FOR local SGD. linear speed - up EVALUATE-FOR One - Shot averaging. communication efficacy EVALUATE-FOR One - Shot averaging. local SGD approaches CONJUNCTION Sync SGD. Sync SGD CONJUNCTION local SGD approaches. communication rounds USED-FOR Sync SGD. OtherScientificTerm is strong convexity. Generic is it. Method is adaptive local step. ,"This paper studies the trade-off between communication efficacy and linear speed-up between local SGD and One-Shot averaging. The authors propose to use strong convexity as a tradeoff, and show that it can be achieved by adding an adaptive local step. They compare the performance of different localSGD approaches and Sync SGD with different communication rounds."
2488,SP:8a78fee6173dc6639dfd9e33a10d0c8432a08512,"convex domains FEATURE-OF linear optimization. lazy subgradient method USED-FOR linear optimization. OtherScientificTerm are expected regret bound, adversarial and stochastic adversaries, prior bounds, and loss contributions. Metric is adversarial regret. Generic are algorithm, and method. Method is lazy subgradient. Task is online linear optimization settings. ","This paper proposes a new lazy subgradient method for linear optimization in convex domains. The main contribution of the paper is to derive an expected regret bound for the adversarial and stochastic adversaries. The adversarial regret is defined in terms of the difference between the loss of the algorithm and that of the prior bounds. The paper also provides a theoretical analysis of the proposed method, showing that the loss contributions of the method are linear in the number of iterations. The authors also provide some empirical results on online linear optimization settings."
2489,SP:8a78fee6173dc6639dfd9e33a10d0c8432a08512,"strongly convex constraints sets USED-FOR online linear optimization. Method is online subgradient descent. Metric is O(\sqrt{N})$ regret. OtherScientificTerm are adversarial case, i.i.d. case, and strongly convex case. ","This paper studies the problem of online linear optimization with strongly convex constraints sets. In particular, the authors consider the case of online subgradient descent, where the goal is to minimize the O(\sqrt{N})$ regret. The authors consider both the adversarial case and the i.i.d. case, and show that in both cases, the regret is O(n^2/\sqrt n)$ for the strongly concave case."
2490,SP:8a78fee6173dc6639dfd9e33a10d0c8432a08512,strongly convex domain FEATURE-OF full information feedback setting. strongly convex domain FEATURE-OF online linear optimization. full information feedback setting FEATURE-OF online linear optimization. hyper - parameters USED-FOR lazy sub - gradient method. O(log{N } ) regret FEATURE-OF stochastic setting. adversarial setting EVALUATE-FOR lazy sub - gradient method. O(log{N } ) regret EVALUATE-FOR lazy sub - gradient method. O(\sqrt{N } ) regret EVALUATE-FOR lazy sub - gradient method. differential geometry methods USED-FOR algorithm. Generic is setting. Method is lazy subgradient method. Task is adversarial and stochastic settings. ,"This paper studies the problem of online linear optimization in a strongly convex domain in the full information feedback setting. In this setting, the authors propose a lazy subgradient method that uses hyper-parameters to minimize the O(log{N}) regret of the stochastic setting. The authors show that in the adversarial setting, their lazy sub-gradient method can achieve O(\sqrt{N}) regret. The proposed algorithm is based on differential geometry methods, and the authors provide theoretical analysis on the performance of the proposed algorithm in both adversarial and stoanchastic settings."
2491,SP:8a78fee6173dc6639dfd9e33a10d0c8432a08512,"strongly convex domains FEATURE-OF Lazy Online Subgradient Algorithm. O(\sqrt T)$ regret EVALUATE-FOR it. Task is online linear optimization. OtherScientificTerm is optimal rates. Method is differential geometry. Generic are algorithm, and technique. Metric is regret. ","This paper studies the problem of online linear optimization. The authors propose a Lazy Online Subgradient Algorithm on strongly convex domains, and show that it achieves O(\sqrt T)$ regret. The algorithm is motivated by the fact that optimal rates can be obtained from differential geometry, and the authors show that this technique can be used to reduce the regret."
2492,SP:59e7ff1cdee42c9623615f6105c0e0f44e7b75a5,Riemannian optimization USED-FOR symmetric positive definite ( SPD ) matrix manifold. Bures - Wasserstein ( BW ) geometry USED-FOR Riemannian optimization. approach COMPARE Affine - Invariant ( AI ) geometry. Affine - Invariant ( AI ) geometry COMPARE approach. approach COMPARE counterpart. counterpart COMPARE approach. approach COMPARE AI geometry. AI geometry COMPARE approach. AI geometry FEATURE-OF counterpart. BW geometry USED-FOR approach. OtherScientificTerm is SPD matrices. ,"This paper studies Riemannian optimization on a symmetric positive definite (SPD) matrix manifold using Bures-Wasserstein (BW) geometry. The proposed approach is compared to Affine-Invariant (AI) geometry and its counterpart, which is based on the BW geometry. In particular, the authors show that the SPD matrices can be expressed as a function of the dimension of the manifold."
2493,SP:59e7ff1cdee42c9623615f6105c0e0f44e7b75a5,affine - invariant ( AI ) geometry USED-FOR Riemannian optimization. it COMPARE affine - invariant ( AI ) geometry. affine - invariant ( AI ) geometry COMPARE it. Bures - Wasserstein ( BW ) geometry FEATURE-OF symmetric positive - definite ( SPD ) matrices. BW metric COMPARE AI metric. AI metric COMPARE BW metric. condition number FEATURE-OF Riemannian Hessian. Riemannian Hessian COMPARE AI metric. AI metric COMPARE Riemannian Hessian. curvature constant EVALUATE-FOR BW metric. curvature constant COMPARE AI metric. AI metric COMPARE curvature constant. Riemannian Hessian USED-FOR BW metric. BW metric COMPARE AI metric. AI metric COMPARE BW metric. condition number COMPARE AI metric. AI metric COMPARE condition number. SPD matrix USED-FOR BW metric. Riemannian steepest descent CONJUNCTION Riemannian trust - region. Riemannian trust - region CONJUNCTION Riemannian steepest descent. convergence FEATURE-OF Riemannian trust - region. BW metric USED-FOR Riemannian trust - region. metric learning CONJUNCTION log - det maximization. log - det maximization CONJUNCTION metric learning. Lyapunov equations CONJUNCTION trace regression. trace regression CONJUNCTION Lyapunov equations. log - det maximization CONJUNCTION Gaussian mixture model. Gaussian mixture model CONJUNCTION log - det maximization. weighted least squares CONJUNCTION Lyapunov equations. Lyapunov equations CONJUNCTION weighted least squares. trace regression CONJUNCTION metric learning. metric learning CONJUNCTION trace regression. Gaussian mixture model HYPONYM-OF problems. weighted least squares HYPONYM-OF problems. metric learning HYPONYM-OF problems. log - det maximization HYPONYM-OF problems. Lyapunov equations HYPONYM-OF problems. trace regression HYPONYM-OF problems. Method is BW geometry. ,"This paper studies the Bures-Wasserstein (BW) geometry of symmetric positive-definite (SPD) matrices, and shows that it is equivalent to the affine-invariant (AI) geometry used in Riemannian optimization. The authors show that the curvature constant of the BW metric is the same as that of the AI metric under the same condition number as in the case of the standard RL-based BW metric. The BW geometry is then extended to the case where the SPD matrix is non-convex, and the authors prove the convergence of the RL-style BW metric to the Rigmannian trust-region, which is in contrast to the state-of-the-art results for the RL version. The paper is well-written and easy to follow. The problems considered are weighted least squares, Lyapunov equations, trace regression, metric learning, log-det maximization, and Gaussian mixture model."
2494,SP:59e7ff1cdee42c9623615f6105c0e0f44e7b75a5,"Bures - Wasserstein   ( BW ) geometry FEATURE-OF positive definite matrices. BW metric USED-FOR algorithms. convergence theoretical convergence EVALUATE-FOR algorithms. optimum FEATURE-OF Riemannian Hessian. Riemannian gradient descent CONJUNCTION Riemannian trust region methods. Riemannian trust region methods CONJUNCTION Riemannian gradient descent. Riemannian gradient descent USED-FOR BW geometry. Riemannian trust region methods USED-FOR BW geometry. synthetic problems EVALUATE-FOR Riemannian trust region methods. Method are Affine Invariant ( AI ) metric, AI, and AI geometry. Task is optimization. ","This paper studies the Affine Invariant (AI) metric, which is a generalization of the Bures-Wasserstein  (BW) geometry for positive definite matrices. The main contribution of this paper is to study the convergence theoretical convergence of algorithms based on the BW metric. In particular, the authors show that the optimum of the Riemannian Hessian of the BW geometry can be approximated by Rigmannian gradient descent and Riemagnian trust region methods on synthetic problems. This is an interesting result, as the optimization of the AI can be viewed as an extension of the work on AI geometry. "
2495,SP:59e7ff1cdee42c9623615f6105c0e0f44e7b75a5,Bures - Wasserstein ( BW ) metric COMPARE Affine Invariant ( AI ) one. Affine Invariant ( AI ) one COMPARE Bures - Wasserstein ( BW ) metric. quadratic dependence FEATURE-OF AI. BW COMPARE AI. AI COMPARE BW. BW USED-FOR ill - conditioned problems. Generic is metrics. Metric is BW metric. OtherScientificTerm is log - det function. ,"This paper proposes two metrics to compare the Bures-Wasserstein (BW) metric and the Affine Invariant (AI) one. The BW metric is based on the fact that the log-det function can be expressed as a function of the number of samples, while the AI has a quadratic dependence. The authors show that BW can be used to study ill-conditioned problems, and that the BW can outperform the AI."
2496,SP:1f835a54c74d396ae2e8620b01bed0ec53646f3a,Dynaboard HYPONYM-OF evaluation framework. fairness CONJUNCTION robustness. robustness CONJUNCTION fairness. throughput CONJUNCTION memory usage. memory usage CONJUNCTION throughput. memory usage CONJUNCTION fairness. fairness CONJUNCTION memory usage. robustness HYPONYM-OF metrics. throughput HYPONYM-OF metrics. memory usage HYPONYM-OF metrics. fairness HYPONYM-OF metrics. Dynascore HYPONYM-OF measure. Generic is models. ,"This paper proposes Dynaboard, an evaluation framework based on Dynascore, which aims to evaluate the performance of models on a variety of metrics, including throughput, memory usage, fairness, robustness, etc. The authors also propose a new measure, DynascORE, which is a measure of the quality of the training data. "
2497,SP:1f835a54c74d396ae2e8620b01bed0ec53646f3a,dynaboard HYPONYM-OF evaluation service framework. evaluation service framework PART-OF dynabench platform. reproducibility EVALUATE-FOR NLP models. Dyanboard USED-FOR models. dynascore USED-FOR models. Metric is memory usage. Method is evaluation platform. ,"This paper proposes dynaboard, an evaluation service framework within the dynabench platform that aims to improve the reproducibility of NLP models. Dyanboard evaluates models on a dynascore, which is designed to reduce the memory usage of the evaluation platform."
2498,SP:1f835a54c74d396ae2e8620b01bed0ec53646f3a,"platform USED-FOR NLP models. Dynaboard USED-FOR NLP models. cloud server USED-FOR NLP models. memory use CONJUNCTION throughput. throughput CONJUNCTION memory use. robustness CONJUNCTION fairness metrics. fairness metrics CONJUNCTION robustness. performance CONJUNCTION memory use. memory use CONJUNCTION performance. fairness metrics EVALUATE-FOR models. throughput CONJUNCTION robustness. robustness CONJUNCTION throughput. robustness EVALUATE-FOR models. web platform USED-FOR models. fairness metrics HYPONYM-OF metrics. performance HYPONYM-OF metrics. throughput HYPONYM-OF metrics. robustness HYPONYM-OF metrics. memory use HYPONYM-OF metrics. metrics EVALUATE-FOR models. performance EVALUATE-FOR models. accessibility CONJUNCTION backwards / forwards compatibility. backwards / forwards compatibility CONJUNCTION accessibility. reproducibility CONJUNCTION accessibility. accessibility CONJUNCTION reproducibility. aggregated metric USED-FOR models. Dynascore HYPONYM-OF aggregated metric. economic theory USED-FOR aggregated score. indifference curves USED-FOR Dynascore measure. metrics PART-OF Dynaboard. performance CONJUNCTION throughput. throughput CONJUNCTION performance. throughput CONJUNCTION metrics. metrics CONJUNCTION throughput. NLP tasks EVALUATE-FOR models. metrics EVALUATE-FOR models. throughput EVALUATE-FOR models. performance EVALUATE-FOR models. Dynascore COMPARE average z - score. average z - score COMPARE Dynascore. rankings COMPARE average z - score. average z - score COMPARE rankings. rankings FEATURE-OF SuperGLUE leaderboard. they USED-FOR rankings. Dynascore USED-FOR rankings. Material is cloud. OtherScientificTerm are aggregation criteria, and average marginal rate of substitution. Generic are metric, and curve. ","This paper presents Dynaboard, a platform for training NLP models on a cloud server. The models trained on the web platform are evaluated on a variety of metrics, including performance, memory use, throughput, robustness, fairness metrics, etc. The authors propose to use an aggregated metric called Dynascore, which is based on economic theory, to evaluate the performance of models trained in the cloud. The aggregation criteria are based on the average marginal rate of substitution between the model and the server, and the authors use indifference curves to derive the Dynascor measure. The metric is then used to compare the performance and throughput of different models on different NLP tasks, and to evaluate reproducibility, accessibility, and backwards/forward compatibility. The paper also presents rankings of the SuperGLUE leaderboard based on these metrics, and they show that the proposed rankings are better than the average z-score."
2499,SP:1f835a54c74d396ae2e8620b01bed0ec53646f3a,fairness CONJUNCTION robustness. robustness CONJUNCTION fairness. throughput CONJUNCTION memory. memory CONJUNCTION throughput. memory CONJUNCTION fairness. fairness CONJUNCTION memory. performance CONJUNCTION throughput. throughput CONJUNCTION performance. evaluation platform EVALUATE-FOR models. models USED-FOR NLP tasks. evaluation platform USED-FOR NLP tasks. DynaBoard EVALUATE-FOR models. evaluation platform EVALUATE-FOR metrics. robustness HYPONYM-OF metrics. performance HYPONYM-OF metrics. metrics EVALUATE-FOR models. throughput HYPONYM-OF metrics. memory HYPONYM-OF metrics. fairness HYPONYM-OF metrics. ,"This paper presents a new evaluation platform for evaluating models for NLP tasks on DynaBoard. The authors evaluate models on a variety of metrics including performance, throughput, memory, fairness, and robustness."
2500,SP:44dc6a69f5d65ca0b271177ac67d1beb12a154a0,"video USED-FOR text to speech generation. transformer USED-FOR mel - spectrograms. text and visual lip motion representations PART-OF transformer architecture. transformer architecture USED-FOR method. human evaluation scores CONJUNCTION audio - visual synchronisation scores. audio - visual synchronisation scores CONJUNCTION human evaluation scores. method COMPARE TTS systems. TTS systems COMPARE method. LRS2 CONJUNCTION Lip2Wav. Lip2Wav CONJUNCTION LRS2. STOI CONJUNCTION Word error rates. Word error rates CONJUNCTION STOI. PESQ CONJUNCTION STOI. STOI CONJUNCTION PESQ. PESQ CONJUNCTION Word error rates. Word error rates CONJUNCTION PESQ. metrics USED-FOR TTS systems. Lip2Wav CONJUNCTION Vid2Speech. Vid2Speech CONJUNCTION Lip2Wav. methods USED-FOR speech. single speaker Lip2Wav task CONJUNCTION multi - speaker Lip2Wav. multi - speaker Lip2Wav CONJUNCTION single speaker Lip2Wav task. LRW dataset USED-FOR multi - speaker Lip2Wav. Lip2Wav USED-FOR LRW dataset. lip motion USED-FOR methods. Lip2Wav USED-FOR multi - speaker Lip2Wav. Vid2Speech USED-FOR multi - speaker Lip2Wav. lip motion USED-FOR speech. Task are video - text guided speech generation task, and silent video dubbing. ","This paper proposes a video-text guided speech generation task. The method is based on a transformer architecture that combines text and visual lip motion representations. The transformer is trained to generate mel-spectrograms, which are then used for text to speech generation in video. The proposed method is evaluated on a single speaker Lip2Wav task and a multi-speaker Lip2Wave and Vid2Speech using the LRW dataset. The results show that the proposed method outperforms existing TTS systems in terms of human evaluation scores, audio-visual synchronisation scores, PESQ, STOI, and Word error rates. The authors also show that lip motion can be used to improve the performance of the proposed methods for speech. Finally, the authors conduct a series of experiments on silent video dubbing, comparing LRS2, Lip2V2, Vid2V3, and showing that their proposed method can outperform existing methods."
2501,SP:44dc6a69f5d65ca0b271177ac67d1beb12a154a0,method USED-FOR TTS. timbre USED-FOR speech. timbre USED-FOR face. speaking face USED-FOR TTS. Task is automatic dubbing of videos. OtherScientificTerm is face image. ,"This paper proposes a method for improving TTS by using timbre for speech and face for automatic dubbing of videos. The main idea is to use the timbre of the speaking face for TTS, and then use the face image for the dubbing. Experiments are conducted to show the effectiveness of the method."
2502,SP:44dc6a69f5d65ca0b271177ac67d1beb12a154a0,"generating realistic audio USED-FOR silent video. generating realistic audio PART-OF task. stages PART-OF pipeline. speaker embeddings USED-FOR decoder. embeddings CONJUNCTION attention network. attention network CONJUNCTION embeddings. images USED-FOR decoder. Task are silent video dubbing, generating phoneme and video embeddings, and generating speaker embeddings. Generic is method. Method are vid2speech, and single - speaker and multi - speaker SVD. OtherScientificTerm is text guidance. ","This paper addresses the problem of silent video dubbing by generating realistic audio for silent video. The proposed method, vid2speech, consists of two stages: 1) generating phoneme and video embeddings for each speaker, and 2) generating audio for the decoder. The first two stages of the pipeline are based on the idea of single-speaker and multi-speech SVD. The decoder takes as input a set of images as input, and generates speaker embedding for each of the speakers, and the attention network is trained to distinguish between the two. The second stage is based on text guidance."
2503,SP:44dc6a69f5d65ca0b271177ac67d1beb12a154a0,method USED-FOR dubbing silent videos. face image FEATURE-OF speech characteristics. lip - synced TTS USED-FOR It. added component ( ISE ) PART-OF lip - synced TTS. aligned speech COMPARE methods. methods COMPARE aligned speech. TTS method COMPARE SOTA TTS. SOTA TTS COMPARE TTS method. Method is Neural Dubber. OtherScientificTerm is prosody. ,"This paper proposes a method for dubbing silent videos. It is based on lip-synced TTS with an added component (ISE), which is inspired by Neural Dubber. The key idea is to align speech characteristics on a face image with the speech characteristics in the video. The prosody of the speech is also aligned. The aligned speech is compared to other methods and compared to the original TTS method and SOTA TTS."
2504,SP:24ea12428bd675459f0509aa7cee821fa236382e,vision transformer's ( ViT ) USED-FOR CXR tasks. ViT USED-FOR split learning. network USED-FOR client - server mechanism. ViT PART-OF framework. decomposable design USED-FOR federated and split learning. robust representations USED-FOR model. FESTA framework USED-FOR model. ,This paper proposes to use vision transformer's (ViT) for CXR tasks. The proposed framework combines ViT for split learning with a decomposable design for federated and split learning. The network is used as the client-server mechanism. The model is based on the FESTA framework and is trained with robust representations.
2505,SP:24ea12428bd675459f0509aa7cee821fa236382e,"multi - task learning CONJUNCTION vision transformers. vision transformers CONJUNCTION multi - task learning. federated learning CONJUNCTION multi - task learning. multi - task learning CONJUNCTION federated learning. split learning CONJUNCTION federated learning. federated learning CONJUNCTION split learning. federated learning CONJUNCTION vision transformers. vision transformers CONJUNCTION federated learning. patient level healthcare data ( CXRs ) USED-FOR models. framework USED-FOR large deep learning models. private datasets USED-FOR framework. private datasets USED-FOR large deep learning models. head networks USED-FOR local data. local data USED-FOR forward passes. embedded representations USED-FOR forward pass. segmentation CONJUNCTION detection. detection CONJUNCTION segmentation. image classification CONJUNCTION segmentation. segmentation CONJUNCTION image classification. image classification HYPONYM-OF tasks. detection HYPONYM-OF tasks. segmentation HYPONYM-OF tasks. modality ( CXRs ) USED-FOR tasks. Method are vision transformer model, and vision transformer. Generic are representations, and task. OtherScientificTerm are tail networks, gradients, and backprop. ","This paper presents a framework for training large deep learning models on patient level healthcare data (CXRs) using private datasets in order to tackle the challenges of multi-task learning, federated learning, and vision transformers. The main idea is to train a vision transformer model on top of CXRs, where the head networks are used to generate local data for forward passes, and the tail networks are trained to generate representations for the forward pass, which are then used to train the vision transformer on the new task. This is done in a manner similar to split learning, but with gradients instead of backprop. Experiments are performed on a variety of tasks, including image classification, segmentation, and detection. The tasks are trained on different modality (CTRs)."
2506,SP:24ea12428bd675459f0509aa7cee821fa236382e,natural scalibility of visual transformer USED-FOR multi - task FL process. federated learning ( FL ) framework USED-FOR COVID-19 xray image analysis. federated learning ( FL ) framework USED-FOR multi - task FL process. natural scalibility of visual transformer USED-FOR federated learning ( FL ) framework. detection CONJUNCTION segmentation. segmentation CONJUNCTION detection. classification CONJUNCTION detection. detection CONJUNCTION classification. method USED-FOR x - ray - based tasks. split learning manner USED-FOR segmentation. segmentation HYPONYM-OF x - ray - based tasks. classification HYPONYM-OF x - ray - based tasks. detection HYPONYM-OF x - ray - based tasks. multi - task federated learning framework COMPARE centralized training model. centralized training model COMPARE multi - task federated learning framework. multi - task federated learning framework USED-FOR COVID-19 diagnosis task. transformer FEATURE-OF multi - task federated learning framework. OtherScientificTerm is shared transformer body. ,"This paper proposes a federated learning (FL) framework for COVID-19 xray image analysis that leverages the natural scalibility of visual transformer in the multi-task FL process. The proposed method can be applied to x-ray-based tasks such as classification, detection, and segmentation in a split learning manner, where the shared transformer body can be shared across multiple tasks. The paper demonstrates the effectiveness of the proposed multi-tasking of the transformer in a COVID19 diagnosis task by comparing the performance of the new multi-teacher and the centralized training model."
2507,SP:24ea12428bd675459f0509aa7cee821fa236382e,application - wise decoder ( tail ) components USED-FOR task. task PART-OF distributed / federated learning setting. application - wise decoder ( tail ) components PART-OF distributed / federated learning setting. shared feature encoder ( body ) USED-FOR multi - task learning framework. application - wise decoder ( tail ) components PART-OF multi - task learning framework. multi - head self - attention modules USED-FOR body part. multi - head self - attention modules PART-OF transformer model. head / tail parts CONJUNCTION body parts. body parts CONJUNCTION head / tail parts. model parameters FEATURE-OF head / tail parts. normal CONJUNCTION pneumonia. pneumonia CONJUNCTION normal. pneumothorax segmentation CONJUNCTION pneumonia detection. pneumonia detection CONJUNCTION pneumothorax segmentation. datasets USED-FOR COVID-19 classification. datasets CONJUNCTION pneumothorax segmentation. pneumothorax segmentation CONJUNCTION datasets. datasets USED-FOR pneumonia detection. COVID-19 classification CONJUNCTION pneumothorax segmentation. pneumothorax segmentation CONJUNCTION COVID-19 classification. pneumonia HYPONYM-OF datasets. datasets PART-OF chest x - ray datasets. pneumothorax segmentation HYPONYM-OF chest x - ray datasets. normal HYPONYM-OF COVID-19 classification. pneumonia HYPONYM-OF COVID-19 classification. method COMPARE learning paradigms. learning paradigms COMPARE method. ,"This paper proposes a multi-task learning framework with a shared feature encoder (body) and application-wise decoder (tail) components for each task in a distributed/federated learning setting. The transformer model consists of multi-head self-attention modules for each body part and head/tail parts for each model parameters. Experiments are conducted on several chest x-ray datasets, including standard datasets for COVID-19 classification (normal, pneumonia), pneumothorax segmentation and pneumonia detection. The proposed method outperforms existing learning paradigms."
2508,SP:2065a8cb8b53140569b64fca1f00f7230f1ae2cc,"differentiable points ( + normals ) USED-FOR mesh layer. differentiable formulation of Poisson surface reconstruction USED-FOR differentiable points ( + normals ). surface meshes USED-FOR objective function. point cloud optimisation tasks EVALUATE-FOR method. method USED-FOR 3D shapes. oriented point cloud FEATURE-OF 3D shapes. learning setting USED-FOR 3D shapes. shape parameterization COMPARE neural implicit fields. neural implicit fields COMPARE shape parameterization. Shape As Points ( SAP ) HYPONYM-OF shape parameterization. OtherScientificTerm are watertight shapes, and arbitrary topology. ","This paper proposes a differentiable formulation of Poisson surface reconstruction to learn differentiable points (+ normals) for each mesh layer. The objective function is based on surface meshes. The method is evaluated on several point cloud optimisation tasks and is shown to be able to learn 3D shapes on an oriented point cloud under a learning setting. The paper also shows that the shape parameterization, Shape As Points (SAP), is more effective than neural implicit fields and can learn watertight shapes with arbitrary topology."
2509,SP:2065a8cb8b53140569b64fca1f00f7230f1ae2cc,Poisson Surface Reconstruction algorithm USED-FOR oriented point sets. It USED-FOR Poisson equation. 3D grid FEATURE-OF indicator function values. spectral methods USED-FOR It. 3D grid USED-FOR Poisson equation. marching cubes algorithm USED-FOR mesh. marching cubes algorithm USED-FOR grid. method USED-FOR optimization - based applications. differentiability FEATURE-OF approach. models COMPARE learning - based methods. learning - based methods COMPARE models. datasets EVALUATE-FOR learning - based methods. datasets EVALUATE-FOR models. ,This paper proposes a Poisson Surface Reconstruction algorithm for oriented point sets. It uses spectral methods to approximate the Poisson equation on a 3D grid of indicator function values. The grid is constructed using a marching cubes algorithm to form a mesh. The proposed method can be applied to optimization-based applications. The paper also discusses the differentiability of the proposed approach. Experiments on two datasets show that the proposed models outperform other learning-based methods.
2510,SP:2065a8cb8b53140569b64fca1f00f7230f1ae2cc,point clouds USED-FOR shape representation. differentiable poisson solver layer USED-FOR oriented point clouds. spectral technique USED-FOR poisson surface reconstruction problem. OtherScientificTerm is full volume representing the shape. Method is neural workflows. Task is surface reconstruction. Generic is representation. ,"This paper proposes to learn shape representation from point clouds. The main idea is to use a differentiable poisson solver layer to generate oriented point clouds, which are then used to generate a full volume representing the shape. The paper also proposes a spectral technique to solve the poisson surface reconstruction problem. Experiments show that the proposed neural workflows are able to achieve state-of-the-art performance on surface reconstruction, and the learned representation is able to be used in a variety of applications."
2511,SP:2065a8cb8b53140569b64fca1f00f7230f1ae2cc,"differentiable Poisson Surface Reconstruction layer USED-FOR oriented point cloud. indicator function USED-FOR oriented point cloud. spectral domain USED-FOR Poisson solver. GPU USED-FOR spectral domain. GPU USED-FOR Poisson solver. it USED-FOR indicator function. It COMPARE neural implicit representations. neural implicit representations COMPARE It. voxel grid USED-FOR network. layer PART-OF tasks. optimization - based 3D shape reconstruction CONJUNCTION learning - based 3D reconstruction task. learning - based 3D reconstruction task CONJUNCTION optimization - based 3D shape reconstruction. learning - based 3D reconstruction task HYPONYM-OF tasks. optimization - based 3D shape reconstruction HYPONYM-OF tasks. non - differentiable step of Marching Cubes PART-OF pipelines. learning - based task EVALUATE-FOR feature extraction network. OtherScientificTerm are spherical point cloud initialization, noisy point cloud, ground truth mesh, and inverse surface normal. ","This paper proposes a differentiable Poisson Surface Reconstruction layer that generates an oriented point cloud from a spherical point cloud initialization. It is similar to neural implicit representations, except that it learns an indicator function that predicts the orientation of the oriented pointcloud. The Poisson solver is trained on a GPU in the spectral domain. The network is trained using a voxel grid, and the output of the layer is used in two tasks: optimization-based 3D shape reconstruction and a learning-based threeD reconstruction task. The feature extraction network is evaluated on the learning-by-learning-based task, where a noisy point cloud is sampled from the ground truth mesh, and an inverse surface normal is computed. The pipelines are trained with a non-differentiable step of Marching Cubes."
2512,SP:76b64e6b104818ed26e9331d134df0125d84291c,"CLIP HYPONYM-OF models. gaussian noise CONJUNCTION blurring. blurring CONJUNCTION gaussian noise. missing pixels CONJUNCTION gaussian noise. gaussian noise CONJUNCTION missing pixels. noise extrapolation CONJUNCTION dataset tranferability. dataset tranferability CONJUNCTION noise extrapolation. dataset tranferability EVALUATE-FOR baseline. Generic are method, and task. Method are student contrastively, student's representation of the distorted image, matching functions, and imagenet supervised. Metric is label efficiency. Material are distorted images, and distorted imagenet-100. ","This paper proposes a method for learning to distinguish between images that are distorted and those that are not. The proposed method, CLIP, is based on the observation that models trained on CLIP (e.g., CLIP-100) do not generalize well to images with missing pixels, gaussian noise, and blurring. To address this issue, the authors propose to train a student contrastively, where the student's representation of the distorted image is used as the input to the matching functions. The authors claim that this improves label efficiency and improves dataset tranferability compared to the baseline. Experiments on distorted imagenet-100 show that the proposed method is able to generalize to a new task. The experiments are conducted on a small set of images, and the results show that CLIP can generalize better than the baseline, especially when the images are not distorted. The results are also shown on a larger set of distorted images, where CLIP is trained on the original image and the new image is not distorted, and on a smaller set of manipulated images, which are not manipulated, and are not trained on an original image, but on an image that is distorted and not blurred. The paper also conducts experiments on a large dataset of images from the original dataset, and shows improved performance on the new dataset. The main contribution of the paper is the introduction of the concept of ""imagenet supervised""."
2513,SP:76b64e6b104818ed26e9331d134df0125d84291c,"pre - trained representation USED-FOR regularization. inversion method USED-FOR pre - trained representation. method USED-FOR robust representation. distortions CONJUNCTION train / test time discrepancy. train / test time discrepancy CONJUNCTION distortions. distortions FEATURE-OF robust representation. train / test time discrepancy FEATURE-OF robust representation. Material are corrupted images, and clean images. OtherScientificTerm are pixel space, pre - trained representation space, and contrastive objective function. Generic is representation. ","This paper proposes an inversion method to regularize a pre-trained representation for regularization. The method aims to learn a robust representation that is robust to distortions and train/test time discrepancy in corrupted images, while being robust to clean images. The main idea is to use a contrastive objective function, where the pixel space of the corrupted image is mapped to the pre-trained representation space, and the representation is trained to minimize the difference between the corrupted and clean image."
2514,SP:76b64e6b104818ed26e9331d134df0125d84291c,CLIP USED-FOR clean input image. corrupted version of the image USED-FOR clean input image. clean input image USED-FOR feature representation. CLIP USED-FOR feature representation. features USED-FOR downstream tasks. features COMPARE clean image. clean image COMPARE features. classification HYPONYM-OF downstream tasks. this USED-FOR transfer of knowledge. labeled data USED-FOR downstream task. CLIP USED-FOR transfer of knowledge. L2 - Loss CONJUNCTION contrastive loss. contrastive loss CONJUNCTION L2 - Loss. student model USED-FOR features. contrastive loss USED-FOR student model. L2 - Loss USED-FOR student model. Gaussian noise CONJUNCTION Gaussian blur. Gaussian blur CONJUNCTION Gaussian noise. random masking CONJUNCTION Gaussian noise. Gaussian noise CONJUNCTION random masking. Gaussian noise HYPONYM-OF corruptions. random masking HYPONYM-OF corruptions. baseline method COMPARE approach. approach COMPARE baseline method. end - to - end with corrupted images USED-FOR baseline method. labeled data USED-FOR approach. OtherScientificTerm is clean feature. Material is ImageNet. ,"This paper proposes to use CLIP to generate a clean input image and a corrupted version of the image to learn a feature representation. The idea is that this can be used for transfer of knowledge between downstream tasks (e.g. classification) using labeled data. The student model is trained using L2-Loss and contrastive loss to generate features that are more similar to the clean image than the clean feature. The experiments are conducted on ImageNet and compare with a baseline method trained end-to-end with corrupted images and an approach that uses labeled data for each downstream task. The results show that the proposed approach is effective. The authors also show that different types of corruptions (random masking, Gaussian noise, and Gaussian blur) are effective."
2515,SP:76b64e6b104818ed26e9331d134df0125d84291c,CLIP network USED-FOR feature map or representation. representation USED-FOR feature representation. image USED-FOR representation. distortion process USED-FOR distorted image. image USED-FOR distortion process. representations CONJUNCTION distorted images. distorted images CONJUNCTION representations. paper USED-FOR student $ S$. ImageNet-100 HYPONYM-OF supervised learning task. contrastive loss USED-FOR paper. contrastive loss USED-FOR student $ S$. supervised learning task EVALUATE-FOR representation. student $ S$ USED-FOR representation. method COMPARE pretrained ResNet. pretrained ResNet COMPARE method. method COMPARE robust encoder. robust encoder COMPARE method. pretrained ResNet COMPARE robust encoder. robust encoder COMPARE pretrained ResNet. Generic is problem. Task is classification task. Material is labeled distorted images. OtherScientificTerm is ablations studies. ,This paper studies the problem of learning a feature map or representation from a CLIP network. The problem is formulated as learning a representation for a feature representation from an image that is distorted by a distortion process. The paper evaluates the representation of a student $S$ on a supervised learning task (ImageNet-100) using a contrastive loss. Experiments show that the proposed method outperforms a pretrained ResNet and a robust encoder on a classification task with labeled distorted images and ablations studies.
2516,SP:ca846ae9653fa843a6a64ce7361d44a0c31c5990,"coagent networks USED-FOR neural network training. finite - horizon reinforcement - learning problem USED-FOR neural network training. non - Markovian nature FEATURE-OF terminal reward signal. finite horizon POMDP USED-FOR neural network training. policy - gradient techniques USED-FOR local, asynchoronous optimization of the network. coagent network training COMPARE backprop. backprop COMPARE coagent network training. local, adaptive policies USED-FOR neural network training. backprop USED-FOR local, adaptive policies. backprop USED-FOR neural network training. supervised learning setting EVALUATE-FOR backprop. Method are agent, backpropagation, and REINFORCE estimator. OtherScientificTerm are coagent actions, and partial observability. Generic are network, and formulation. Material is continual learning setup. ","This paper studies neural network training with coagent networks as a finite-horizon reinforcement-learning problem. The main contribution of this paper is to study the non-Markovian nature of the terminal reward signal in the finite horizon POMDP, which is used to motivate the use of the agent to learn local, asynchoronous optimization of the network using policy-gradient techniques. The authors show that coagent network training outperforms backpropagation in a supervised learning setting, where the coagent actions are not available to the network, and that the REINFORCE estimator can be used as a proxy for partial observability. The paper also provides a theoretical analysis of the formulation, and provides empirical results on a continual learning setup."
2517,SP:ca846ae9653fa843a6a64ce7361d44a0c31c5990,RL methods USED-FOR credit assignment problem. neural network USED-FOR credit assignment problem. finite horizon RL problem USED-FOR credit assignment problem. REINFORCE USED-FOR coagents. off - policy algorithm COMPARE on - policy algorithm. on - policy algorithm COMPARE off - policy algorithm. off - policy algorithm COMPARE backpropagation. backpropagation COMPARE off - policy algorithm. continual learning task EVALUATE-FOR backpropagation. continual learning task EVALUATE-FOR off - policy algorithm. OtherScientificTerm is coagents ’ policies. Task is learning. Method is Q - learning like off - policy algorithm. ,This paper studies the credit assignment problem in RL methods and proposes a neural network to solve the credit allocation problem in a finite horizon RL problem. The main idea is to use REINFORCE to train the coagents’ policies and then use an off-policy algorithm to learn the coagent’s policies during learning. The authors propose a Q-learning like off-policing algorithm and show that the proposed off policy algorithm outperforms backpropagation on a continual learning task.
2518,SP:ca846ae9653fa843a6a64ce7361d44a0c31c5990,reinforcement learning USED-FOR neural networks. individual layers PART-OF finite horizon partially observable MDP. REINFORCE USED-FOR hidden layer networks. REINFORCE USED-FOR cooperative agent framework. baseline approaches CONJUNCTION activations. activations CONJUNCTION baseline approaches. baseline approaches USED-FOR stability. cooperative agent framework USED-FOR hidden layer networks. partitioning schemes CONJUNCTION baseline approaches. baseline approaches CONJUNCTION partitioning schemes. partitioning schemes USED-FOR network. MNIST / Boston Housing USED-FOR hidden layer networks. partitioning schemes HYPONYM-OF parameters. baseline approaches HYPONYM-OF parameters. activations HYPONYM-OF parameters. policy variance HYPONYM-OF parameters. CoAN+REINFORCE approach COMPARE backprop. backprop COMPARE CoAN+REINFORCE approach. off - policy Q learning approach USED-FOR cooperative agents. bootstrapping USED-FOR off - policy Q learning approach. REINFORCE COMPARE backprop. backprop COMPARE REINFORCE. it COMPARE REINFORCE. REINFORCE COMPARE it. CoAN+REINFORCE approach COMPARE backprop. backprop COMPARE CoAN+REINFORCE approach. continual learning setting EVALUATE-FOR CoAN+REINFORCE approach. continual learning setting EVALUATE-FOR backprop. OtherScientificTerm is global agent. Generic is baselines. Method is on - policy SARSA. ,"This paper studies reinforcement learning for neural networks. The authors propose to use REINFORCE in a cooperative agent framework to train hidden layer networks on MNIST/Boston Housing, where individual layers are embedded in a finite horizon partially observable MDP. The network is trained with different partitioning schemes, baseline approaches to stability, and activations. The proposed CoAN+REINFCE approach is shown to outperform backprop in a continual learning setting, where the global agent is trained using bootstrapping. The experimental results are compared to baselines, where on-policy SARSA is used, and show that it outperforms REINforCE and backprop. Finally, the authors propose an off-policy Q learning approach for cooperative agents, which is also shown to be effective."
2519,SP:ca846ae9653fa843a6a64ce7361d44a0c31c5990,neural networks USED-FOR credit assignment problem. RL algorithms USED-FOR credit assignment problem. finite - horizon MDP USED-FOR problem. nondeterminism FEATURE-OF problem. critic USED-FOR off - policy RL. Method is RL. ,"This paper studies the credit assignment problem in neural networks. The problem is formulated as a finite-horizon MDP, and the RL algorithms are trained to solve the problem with nondeterminism. The main contribution of this paper is the introduction of a critic for off-policy RL, which is a novel idea in RL."
2520,SP:7aa09356b2c85d54933c0d0d89a3f8fe2e37b27b,"model USED-FOR visual system. functional specializations FEATURE-OF pathways. representationally and functionally distinct regions FEATURE-OF visual system. pathways PART-OF primate visual system. learned representations COMPARE neural representations. neural representations COMPARE learned representations. ANN COMPARE neural representations. neural representations COMPARE ANN. ANN USED-FOR learned representations. mouse visual cortex FEATURE-OF calcium imaging data. two - pathway 3D ResNet model COMPARE baseline models. baseline models COMPARE two - pathway 3D ResNet model. contrastive predictive coding ( CPC ) task USED-FOR two - pathway 3D ResNet model. natural videos USED-FOR two - pathway 3D ResNet model. dataset USED-FOR architecture. supervised task USED-FOR architecture. architecture HYPONYM-OF baseline models. object recognition CONJUNCTION motion discrimination. motion discrimination CONJUNCTION object recognition. pathways PART-OF self - supervised model. pathways USED-FOR tasks. motion discrimination HYPONYM-OF tasks. object recognition HYPONYM-OF tasks. OtherScientificTerm are ventral and dorsal areas, and functional specialization. ","This paper presents a model for the visual system with representationally and functionally distinct regions. The authors propose a two-pathway 3D ResNet model that is trained on natural videos using a contrastive predictive coding (CPC) task. The learned representations are compared to the neural representations learned using ANN on calcium imaging data from mouse visual cortex. The architecture is trained using a supervised task, where the ventral and dorsal areas are trained separately. The pathways in the self-supervised model are then used to perform tasks such as object recognition and motion discrimination. The functional specialization of the pathways is demonstrated through experiments."
2521,SP:7aa09356b2c85d54933c0d0d89a3f8fe2e37b27b,two - stream 3d ResNet architecture USED-FOR representation. ventral / dorsal stream split in visual cortex FEATURE-OF representation. contrastive predictive coding ( CPC ) objective USED-FOR two - stream 3d ResNet architecture. ,This paper proposes a two-stream 3d ResNet architecture with contrastive predictive coding (CPC) objective to learn a representation with ventral/dorsal stream split in visual cortex. The paper is well-written and easy to follow.
2522,SP:7aa09356b2c85d54933c0d0d89a3f8fe2e37b27b,loss function USED-FOR specialized neural responses. loss function USED-FOR deep neural network. deep neural network USED-FOR neural responses. neural network COMPARE architecture. architecture COMPARE neural network. deep neural network COMPARE neural network. neural network COMPARE deep neural network. convolutional hierarchy USED-FOR ventral stream. ventral and dorsal streams FEATURE-OF neural responses. self - supervised predictive objective USED-FOR deep neural network. convolutional hierarchy PART-OF neural network. parallel convolutional hierarchies PART-OF deep neural network. supervised objective USED-FOR architecture. object categorization CONJUNCTION motion detection. motion detection CONJUNCTION object categorization. object categorization EVALUATE-FOR two - pathway model. Generic is model. ,"This paper proposes a new loss function to learn specialized neural responses by training a deep neural network with a self-supervised predictive objective. Compared to a standard neural network trained with a supervised objective, the proposed architecture consists of two parallel convolutional hierarchies, one for the ventral stream and one for dorsal stream. The proposed model is evaluated on the task of object categorization and motion detection and compared to a two-pathway model."
2523,SP:7aa09356b2c85d54933c0d0d89a3f8fe2e37b27b,"contrastive predictive coding USED-FOR deep networks. UCF101 videos USED-FOR deep networks. representations COMPARE mouse calcium - imaging data. mouse calcium - imaging data COMPARE representations. representational similarity USED-FOR representations. Allen Brain Observatory FEATURE-OF mouse calcium - imaging data. two - stream network COMPARE mouse visual cortex data. mouse visual cortex data COMPARE two - stream network. dorsal stream USED-FOR discrimination of random - dot motion direction. ventral stream PART-OF model. OtherScientificTerm are dorsal mouse areas, ventral mouse areas, and mouse visual hierarchy. Generic is other. Material is CIFAR-10. ","This paper proposes to use contrastive predictive coding to train deep networks on UCF101 videos. The authors show that their representations are comparable to mouse calcium-imaging data from the Allen Brain Observatory. The two-stream network is compared to mouse visual cortex data, where the dorsal mouse areas are trained to be similar to the ventral mouse areas. The dorsal stream is trained for the discrimination of random-dot motion direction, while the other is trained to predict the direction of the mouse visual hierarchy. Experiments are conducted on CIFAR-10."
2524,SP:fa11c4da16c01c6a3449f15b25a6e4e228ebbf4a,"generative model USED-FOR hierarchical topics. Gaussian embeddings USED-FOR SawETM. SawETM HYPONYM-OF neural topic model. TopicNet USED-FOR hierarchical topics. knowledge - graph USED-FOR TopicNet. Gaussian SawETM USED-FOR TopicNet. TopicNet USED-FOR hypernym - induced topics. rich interpretability FEATURE-OF TopicNet. OtherScientificTerm are structured prior knowledge graph, and hierarchies. ","This paper proposes a generative model for learning hierarchical topics using a structured prior knowledge graph. Specifically, the authors propose a neural topic model called SawETM, which is an extension of the recently proposed SawetM with Gaussian embeddings. The authors propose TopicNet, which uses the knowledge-graph to learn hierarchical topics. The main contribution of the paper is that TopicNet is trained using Gaussian sawETM and is able to learn hypernym-induced topics with rich interpretability. The paper also shows that hierarchical topics can be represented as hierarchies."
2525,SP:fa11c4da16c01c6a3449f15b25a6e4e228ebbf4a,prior ( tree - structured ) knowledge USED-FOR hierarchical topic discovery framework. entailment relationship USED-FOR prior semantic knowledge. perplexity CONJUNCTION topic quality. topic quality CONJUNCTION perplexity. topic quality CONJUNCTION clustering / classification protocols. clustering / classification protocols CONJUNCTION topic quality. benchmarks EVALUATE-FOR clustering / classification protocols. topic quality FEATURE-OF benchmarks. perplexity FEATURE-OF benchmarks. clustering / classification protocols EVALUATE-FOR model. topic quality EVALUATE-FOR model. benchmarks EVALUATE-FOR model. perplexity EVALUATE-FOR model. Generic is framework. OtherScientificTerm is semantic hierarchy. ,"This paper proposes a hierarchical topic discovery framework that leverages prior (tree-structured) knowledge. The framework is based on the notion of entailment relationship between the prior semantic knowledge and the semantic hierarchy. The proposed model is evaluated on three benchmarks that measure perplexity, topic quality, and clustering/classification protocols."
2526,SP:fa11c4da16c01c6a3449f15b25a6e4e228ebbf4a,"it CONJUNCTION knowledge graph. knowledge graph CONJUNCTION it. model USED-FOR document representations. Method is SawETM hierarchical topic model. OtherScientificTerm are Gaussian distribution, and hierarchical ordering constraint. ","This paper proposes a SawETM hierarchical topic model. The model learns document representations by combining it with a knowledge graph. The key idea is to learn a Gaussian distribution over documents, where each document is represented by a hierarchical ordering constraint. "
2527,SP:fa11c4da16c01c6a3449f15b25a6e4e228ebbf4a,"Gaussian embeddings USED-FOR topic model. Gaussian embeddings USED-FOR latent space. public datasets EVALUATE-FOR model. tasks EVALUATE-FOR model. It COMPARE baselines. baselines COMPARE It. classification tasks EVALUATE-FOR It. clustering tasks EVALUATE-FOR baselines. clustering tasks EVALUATE-FOR It. OtherScientificTerm are prior knowledge, semantic topic relations, and topic hierarchy. ","This paper proposes a topic model that uses Gaussian embeddings to represent the latent space of the topic model without prior knowledge of semantic topic relations. The proposed model is evaluated on two public datasets. It is compared to several baselines on classification tasks and clustering tasks. The results show that the proposed model outperforms the baselines in most of the tasks. In addition, it is shown that the learned topic hierarchy can be used to better represent the topic hierarchy."
2528,SP:b7ad495901eb2f73a8a26aa5c9325908451cfe09,initialized pretrained network USED-FOR object detection task. pretraining USED-FOR object detection. object - level features learning USED-FOR pretraining. object level contrastive learning framework USED-FOR model. COCO CONJUNCTION PASCAL. PASCAL CONJUNCTION COCO. COCO EVALUATE-FOR method. PASCAL EVALUATE-FOR method. Material is ImageNet. ,This paper proposes to use a initialized pretrained network to solve an object detection task. The pretraining is based on object-level features learning. The model is trained using an object level contrastive learning framework. The method is evaluated on COCO and PASCAL. Results on ImageNet are promising.
2529,SP:b7ad495901eb2f73a8a26aa5c9325908451cfe09,self - supervised pre - training method USED-FOR downstream object detection task. selective search USED-FOR object proposals. method USED-FOR object - level features. Mask R - CNN USED-FOR object - level features. selective search USED-FOR method. COCO object detection CONJUNCTION instance segmentation. instance segmentation CONJUNCTION COCO object detection. Mask R - CNN USED-FOR COCO object detection. Mask R - CNN USED-FOR instance segmentation. Method is self - supervised object - level representation learning. ,"This paper proposes a self-supervised pre-training method for the downstream object detection task. The proposed method uses selective search to generate object proposals, and then uses Mask R-CNN to extract object-level features. Experiments on COCO object detection and instance segmentation show the effectiveness of the proposed method. Overall, this paper is well-written and well-motivated. However, there is a lot of room for improvement, especially in the area of self-substituted representation learning."
2530,SP:b7ad495901eb2f73a8a26aa5c9325908451cfe09,"self supervised method USED-FOR object detection. BYOL USED-FOR self supervised method. BYOL - style loss USED-FOR ROI - Aligned features. FPN CONJUNCTION RCNN head. RCNN head CONJUNCTION FPN. BYOL CONJUNCTION SWAV. SWAV CONJUNCTION BYOL. It COMPARE SSL methods. SSL methods COMPARE It. FPN PART-OF detector. RCNN head PART-OF detector. BYOL HYPONYM-OF SSL methods. SWAV HYPONYM-OF SSL methods. COCO CONJUNCTION pascal. pascal CONJUNCTION COCO. pascal EVALUATE-FOR It. COCO EVALUATE-FOR It. Method are RCNN paradigm, and non - parametric method ( selective search ). OtherScientificTerm is backbone. Generic are methods, and code. ","This paper proposes a self supervised method based on BYOL for object detection. The main idea is to use a BYOL-style loss to encourage ROI-Aligned features, which is similar to the RCNN paradigm. The detector consists of a FPN, an RCNN head, and a non-parametric method (selective search). It is evaluated on COCO and pascal, and compared to SSL methods such as BYOL and SWAV. The results show that the proposed methods outperform the existing methods. The code is well-organized and easy to follow."
2531,SP:b7ad495901eb2f73a8a26aa5c9325908451cfe09,"method USED-FOR self - supervised representation learning. self - supervised representation learning USED-FOR downstream object detection and instance segmentation tasks. downstream tasks CONJUNCTION self - supervised pre - training. self - supervised pre - training CONJUNCTION downstream tasks. FPN CONJUNCTION R - CNN head. R - CNN head CONJUNCTION FPN. representation learning scheme USED-FOR object level scale and translation invariance. components PART-OF downstream detector. R - CNN head HYPONYM-OF components. FPN HYPONYM-OF downstream detector. representation learning scheme USED-FOR object detection. FPN HYPONYM-OF components. coarser FPN pyramid levels USED-FOR representations. finer FPN pyramid levels USED-FOR representations. Selective search HYPONYM-OF hand - crafted region proposal technique. random resized crop CONJUNCTION resize operations. resize operations CONJUNCTION random resized crop. scale aware assignment USED-FOR RoiAlign. RoiAlign USED-FOR FPN features. scale aware assignment USED-FOR FPN features. EMA teacher network CONJUNCTION projection head. projection head CONJUNCTION EMA teacher network. MS COCO and Pascal VOC object detection CONJUNCTION MS COCO instance segmentation. MS COCO instance segmentation CONJUNCTION MS COCO and Pascal VOC object detection. Mask - RCNN models USED-FOR MS COCO and Pascal VOC object detection. Mask - RCNN models USED-FOR MS COCO instance segmentation. pre - training CONJUNCTION fine - tuning. fine - tuning CONJUNCTION pre - training. architecture EVALUATE-FOR fine - tuning. view size CONJUNCTION momentum. momentum CONJUNCTION view size. momentum USED-FOR EMA. momentum CONJUNCTION batch - size. batch - size CONJUNCTION momentum. batch - size HYPONYM-OF hyper - parameters. momentum HYPONYM-OF hyper - parameters. view size HYPONYM-OF hyper - parameters. Method are SoCo, ResNet backbone, BYOL, R50 - C4, and ImageNet linear eval. OtherScientificTerm are alignment, finer pyramid levels, coarser pyramid levels, BYOL","This paper proposes a method for self-supervised representation learning for downstream object detection and instance segmentation tasks. The proposed method, called SoCo, aims to address the problem of self-sampling and fine-tuning a downstream detector that consists of three components: FPN, R-CNN head, and the EMA teacher network and the projection head. The authors propose a representation learning scheme for object detection that aims to improve object level scale and translation invariance, which is a key challenge for both downstream tasks and self-studied pre-training. Selective search, a hand-crafted region proposal technique, is proposed to improve the performance of the ResNet backbone. RoiAlign, a scale aware assignment of the FPN features using the learned alignment between the finer pyramid levels and the coarser FPN pyramid levels, is used to learn representations that are more robust to changes in scale. A random resized crop and resize operations are used to replace the resize operations. Experiments are conducted on MS COCO and Pascal VOC object detection with Mask-RCNN models and MS CocaO and MSCOCO instance segmentations with Mask - RCNN models on MSCocO and PASCAL-C4 and ImageNet linear eval. Results show that SoCo is able to achieve state-of-the-art performance on both pre-train and fine -tuning on the same architecture. Results are also shown for EMA using momentum, momentum, view size, and batch-size. Results on BYOL show that BYOL can achieve comparable or better performance on R50-C2 and R50C4. Results also show that EMA with momentum and momentum can outperform BYOL."
2532,SP:b4dcb19fd97a906ed37e6af407260f0dedbbd402,local search technique USED-FOR vehicle routing problems ( VRP ). exact VRP solver HYPONYM-OF subsolver. transformer model USED-FOR problem. Generic is method. ,"This paper proposes a local search technique for solving vehicle routing problems (VRP). The proposed method is based on the idea that the exact VRP solver is a subsolver, and that the problem can be solved using a transformer model."
2533,SP:b4dcb19fd97a906ed37e6af407260f0dedbbd402,iterative framework USED-FOR large - scale Vehicle Routing Problems. component USED-FOR VRP subproblem. VRP solver ( LKH-3 ) USED-FOR subproblem. subproblem selector USED-FOR subsolution cost. Transformer encoder USED-FOR subproblem selector. VRP solver LKH EVALUATE-FOR method. computation times EVALUATE-FOR method. OtherScientificTerm is subsolution. Generic is it. ,"This paper proposes an iterative framework for solving large-scale Vehicle Routing Problems. The proposed method is based on the VRP solver LKH, which consists of a component that solves a VRP subproblem, and a subproblem selector that estimates the subsolution cost using a Transformer encoder. The authors show that the proposed method can reduce the computation times by up to 1.5x, and that it can be applied to larger problems."
2534,SP:b4dcb19fd97a906ed37e6af407260f0dedbbd402,decomposition steps PART-OF heuristic search algorithm. heuristic search algorithm USED-FOR capacitated vehicle routing problem. learning algorithm USED-FOR decomposition. learning algorithm COMPARE baseline decomposition algorithms. baseline decomposition algorithms COMPARE learning algorithm. random HYPONYM-OF baseline decomposition algorithms. client distribution USED-FOR approach. ,This paper proposes a heuristic search algorithm for the capacitated vehicle routing problem that consists of decomposition steps. The proposed learning algorithm is able to perform better decomposition than baseline decomposition algorithms such as random. The approach is based on the client distribution.
2535,SP:b4dcb19fd97a906ed37e6af407260f0dedbbd402,supervised learning USED-FOR decomposition method. regression USED-FOR methodology. exact approaches USED-FOR VRP. heuristics USED-FOR VRP. heuristics COMPARE exact approaches. exact approaches COMPARE heuristics. LKH-3 HYPONYM-OF heuristics. Method is solver. ,"This paper proposes a decomposition method based on supervised learning. The methodology is based on regression, where the solver is trained on a set of data points. The authors show that the proposed heuristics (LKH-3) outperform exact approaches to VRP."
2536,SP:f5c80f76cb1e651fd808e7da4bfe6fdfd75b7155,approach USED-FOR continual learning. Bayesian continual learning USED-FOR active forgetting. synaptic expansion - convergence USED-FOR active forgetting. synaptic expansion - convergence FEATURE-OF Bayesian continual learning. Bayesian continual learning USED-FOR approach. visual classification CONJUNCTION Atari reinforcement tasks. Atari reinforcement tasks CONJUNCTION visual classification. CIFAR10 regression CONJUNCTION visual classification. visual classification CONJUNCTION CIFAR10 regression. continual learning benchmarks EVALUATE-FOR method. Atari reinforcement tasks HYPONYM-OF continual learning benchmarks. CIFAR10 regression HYPONYM-OF continual learning benchmarks. visual classification HYPONYM-OF continual learning benchmarks. Generic is tasks. ,"This paper presents an approach to continual learning based on Bayesian continual learning with synaptic expansion-convergence for active forgetting. The method is evaluated on several continual learning benchmarks, including CIFAR10 regression, visual classification, and Atari reinforcement tasks. The results show that the proposed method can achieve state-of-the-art performance on these tasks."
2537,SP:f5c80f76cb1e651fd808e7da4bfe6fdfd75b7155,"active forgetting mechanisms USED-FOR negative transfer. biological neural networks USED-FOR active forgetting mechanisms. CL benchmarks EVALUATE-FOR method. Task are Bayesian continual learning setting, and CL. ",This paper studies the problem of negative transfer in the Bayesian continual learning setting. The authors propose to use biological neural networks to learn active forgetting mechanisms to prevent negative transfer. The method is evaluated on CL benchmarks. The results show that the proposed method is effective in CL.
2538,SP:f5c80f76cb1e651fd808e7da4bfe6fdfd75b7155,"continual learning method USED-FOR forward transfer. laplace approximation USED-FOR posterior. laplace approximation USED-FOR learning objective. weighted product distribution USED-FOR posterior. model USED-FOR task. model COMPARE one. one COMPARE model. model PART-OF regularization penalty. negative knowledge FEATURE-OF task. method COMPARE regularization- and memory - based methods. regularization- and memory - based methods COMPARE method. OtherScientificTerm are forgetting, and active forgetting. Method is Elastic Weight Consolidation ( EWC ). Task is regression, classification and RL tasks. ","This paper proposes a new continual learning method for forward transfer. The learning objective is based on a laplace approximation of the posterior as a weighted product distribution. The authors propose Elastic Weight Consolidation (EWC) and show that the proposed model is more robust to the negative knowledge of the task compared to the one imposed by the regularization penalty. They also show that their method outperforms other regularization- and memory-based methods in the presence of forgetting. They conduct extensive experiments on regression, classification and RL tasks to demonstrate the effectiveness of active forgetting."
2539,SP:f5c80f76cb1e651fd808e7da4bfe6fdfd75b7155,"method USED-FOR continual learning. method USED-FOR catastrophic forgetting. Bayesian continual learning framework USED-FOR method. expanded network parameters USED-FOR AFEC. network parameters USED-FOR task. network parameters USED-FOR task. EWC term HYPONYM-OF term. term USED-FOR Catastrophic forgetting. regression, classification and reinforcement learning tasks EVALUATE-FOR datasets. forward transfer setting EVALUATE-FOR method. OtherScientificTerm is loss function. Task is active forgetting. ","This paper proposes a method for continual learning that aims to mitigate catastrophic forgetting using a Bayesian continual learning framework. Catastrophic forgetting is defined as a term called the EWC term, which penalizes the loss function when the network parameters for a given task are changed. The authors propose AFEC with expanded network parameters to mitigate this issue. The proposed method is evaluated on a forward transfer setting, and on regression, classification and reinforcement learning tasks on three datasets. The results show that the proposed method can mitigate active forgetting."
2540,SP:a5945ec13e2f362df03b42511d44827ef081f4c3,"OtherScientificTerm is convergence analysis. Method are accelerated random search ( ARS ) algorithm, prior - guided zeroth - order optimization algorithms, and ARS algorithm. ","This paper provides a convergence analysis of the accelerated random search (ARS) algorithm. The main contribution of the paper is the convergence analysis. The authors show that the prior-guided zeroth-order optimization algorithms are not optimal, and that the proposed ARS algorithm is optimal. The paper also provides a theoretical analysis on the convergence of the proposed algorithm."
2541,SP:a5945ec13e2f362df03b42511d44827ef081f4c3,"deterministic setting FEATURE-OF zeroth - order optimization. estimator USED-FOR gradient method. gradient USED-FOR problems. convergence guarantee USED-FOR prior guided zeroth order algorithms. convergence guarantees CONJUNCTION robustness. robustness CONJUNCTION convergence guarantees. learning rate FEATURE-OF robustness. OtherScientificTerm are objective function, function evaluations, small enough perturbation term, and g_{I}$s. Method is gradient estimator. ","This paper studies zeroth-order optimization in a deterministic setting under the assumption that the objective function is deterministic. The authors propose a new estimator for the gradient method that can be used as a generalization of the standard gradient method. The main contribution of this paper is to provide a convergence guarantee that is similar to prior generalization results of prior guided Zeroth order algorithms. This convergence guarantee is based on the fact that the gradient of the problems can be expressed as a function of a small enough perturbation term, and the authors show that this gradient estimator can be applied to any $g_{I}$s. The paper also provides convergence guarantees and robustness to small changes in the learning rate of the function evaluations."
2542,SP:a5945ec13e2f362df03b42511d44827ef081f4c3,"convergence rates EVALUATE-FOR prior guided zero order optimization methods. statistical methods USED-FOR zero order methods. gradient estimation accuracy CONJUNCTION function evaluations. function evaluations CONJUNCTION gradient estimation accuracy. benchmark functions CONJUNCTION black box adversarial attacks. black box adversarial attacks CONJUNCTION benchmark functions. black box adversarial attacks EVALUATE-FOR algorithms. benchmark functions EVALUATE-FOR algorithms. OtherScientificTerm is function gradient. Method are learning rate selection, and accelerated random search. ",This paper studies the convergence rates of prior guided zero order optimization methods. The authors propose to use statistical methods to evaluate the performance of zero order methods and show that the gradient estimation accuracy and function evaluations are correlated with the function gradient. They also propose a learning rate selection based on accelerated random search. The proposed algorithms are evaluated on several benchmark functions and black box adversarial attacks.
2543,SP:a5945ec13e2f362df03b42511d44827ef081f4c3,"zeroth order optimization algorithms USED-FOR gradient estimation. prior information USED-FOR gradient estimation. convergence analysis EVALUATE-FOR prior guided algorithms. framework USED-FOR historical priors. prior information USED-FOR accelerated random search algorithm. OtherScientificTerm are gradient estimates, and sub - optimal learning rate. Generic is prior. ",This paper studies the convergence analysis of prior guided algorithms that use prior information for gradient estimation in zeroth order optimization algorithms. The authors propose a framework for learning historical priors that can be used to guide the gradient estimates. They also propose an accelerated random search algorithm that uses the prior information to reduce the sub-optimal learning rate. 
2544,SP:ef18f4188426bc01be309633b486884b0e7a81a4,student network USED-FOR teacher network. linear convergence speed FEATURE-OF one - hidden - layer neural networks. one - hidden - layer neural networks USED-FOR recovery. OtherScientificTerm is sparsity of the teacher network. ,This paper studies the recovery of one-hidden-layer neural networks with linear convergence speed with respect to the sparsity of the teacher network. The main contribution of this paper is to show that the student network can be used as a substitute for the original teacher network in order to improve the recovery performance.
2545,SP:ef18f4188426bc01be309633b486884b0e7a81a4,"finite number of training samples FEATURE-OF teacher - student setup. Method are pruned neural networks, and pruned networks. Metric is faster training convergence. OtherScientificTerm is enlarged convex region. ","This paper studies the problem of pruned neural networks. In particular, the authors study the finite number of training samples in a teacher-student setup. The authors show that pruned networks converge to an enlarged convex region, which leads to faster training convergence."
2546,SP:ef18f4188426bc01be309633b486884b0e7a81a4,"geometric structure of the objective function USED-FOR pruned network. generalization error EVALUATE-FOR pruned network. geometric structure of the objective function USED-FOR generalization error. teacher - student fashion USED-FOR pruned network. pruned network USED-FOR zero generalization error. winning tickets COMPARE dense networks. dense networks COMPARE winning tickets. Task is LTH. Generic is model. OtherScientificTerm are desirable ( convex ) region, and un - pruned weights. Metric is sample complexity. Method is pruned models. ","This paper studies the problem of LTH, where the model is pruned to a desirable (convex) region. The authors study the generalization error of the pruned network under the geometric structure of the objective function, and show that a pruned model trained in a teacher-student fashion converges to zero generalization. They also show that the sample complexity of pruned models converges linearly with the number of un-pruned weights. Finally, they show that winning tickets can be learned faster than dense networks."
2547,SP:ef18f4188426bc01be309633b486884b0e7a81a4,"accelerated gradient descent method USED-FOR pruned   network. convergence and sample complexity analysis EVALUATE-FOR algorithm. convex region FEATURE-OF pruned network. empirical risk function EVALUATE-FOR pruned network. convex region FEATURE-OF empirical risk function. model COMPARE teacher network. teacher network COMPARE model. AGD algorithm USED-FOR pruned network. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. Metric is generalization error. Material is teacher - student setting. Method are unknown teacher network, and student network. ","This paper proposes an accelerated gradient descent method for pruned  network. The authors provide convergence and sample complexity analysis for the proposed algorithm. The generalization error is shown to be bounded in the teacher-student setting, where the unknown teacher network is pruned and the student network is not pruned. The empirical risk function of the pruned network is in the convex region. The AGD algorithm is also shown to converge to a pruned version of the original network. Experiments on MNIST and CIFAR-10 show that the proposed model outperforms the teacher network."
2548,SP:3ef8660de61a1a73858934fcf8edfec104133ae7,"loss function CONJUNCTION distributional family. distributional family CONJUNCTION loss function. differentially private synthetic data generation algorithms PART-OF unifying framework. Adaptive Measurements HYPONYM-OF unifying framework. loss function USED-FOR algorithms. distributional family USED-FOR algorithms. Private Entropy Projection ( PEP ) CONJUNCTION Generative networks. Generative networks CONJUNCTION Private Entropy Projection ( PEP ). Generative networks HYPONYM-OF synthetic data generation algorithms. Private Entropy Projection ( PEP ) HYPONYM-OF synthetic data generation algorithms. PEP USED-FOR data distribution. public data USED-FOR GEM. generative neural networks USED-FOR GEM. Generic are framework, and it. ","This paper proposes Adaptive Measurements, a unifying framework that combines differentially private synthetic data generation algorithms. The proposed framework is based on the observation that the loss function and the distributional family of the algorithms can be viewed as a function of the data distribution. The authors propose to use two types of synthetic data generating algorithms: Private Entropy Projection (PEP) and Generative networks. They show that PEP can be used to estimate the underlying data distribution and that Generative Neural Networks (GEM) can be applied to GEM with public data. They also show that GEM can be trained with generative neural networks and show that it can generalize to unseen datasets."
2549,SP:3ef8660de61a1a73858934fcf8edfec104133ae7,algorithms USED-FOR DP synthetic data generation. algorithms USED-FOR query release. DP synthetic data generation USED-FOR query release. framework USED-FOR algorithms. private entropy projection ( PEP ) CONJUNCTION generative networks. generative networks CONJUNCTION private entropy projection ( PEP ). exponential mechanism ( GEM ) USED-FOR generative networks. private entropy projection ( PEP ) HYPONYM-OF algorithms. generative networks HYPONYM-OF algorithms. algorithms COMPARE ones. ones COMPARE algorithms. accuracy EVALUATE-FOR GEM. public data USED-FOR GEM. ,This paper proposes a framework for developing algorithms for DP synthetic data generation for query release. The proposed algorithms include private entropy projection (PEP) and generative networks based on exponential mechanism (GEM). The authors show that the proposed algorithms outperform existing ones in terms of accuracy. The authors also show that GEM can be trained on public data.
2550,SP:3ef8660de61a1a73858934fcf8edfec104133ae7,"synthetic data generation USED-FOR query release. PEP CONJUNCTION GEM. GEM CONJUNCTION PEP. GEM HYPONYM-OF algorithms. PEP HYPONYM-OF algorithms. maximum entropy distribution USED-FOR PEP. generator network USED-FOR high dimensional space. GEM USED-FOR high dimensional space. PEP HYPONYM-OF approaches. generator network USED-FOR GEM. it USED-FOR pretraining. pretraining USED-FOR GEM. public data USED-FOR GEM. it USED-FOR GEM. GEM COMPARE prior work. prior work COMPARE GEM. ACS CONJUNCTION adult datasets. adult datasets CONJUNCTION ACS. GEM COMPARE PMW. PMW COMPARE GEM. GEM COMPARE PEP. PEP COMPARE GEM. domain shift FEATURE-OF public data. PMW CONJUNCTION PEP. PEP CONJUNCTION PMW. accuracy EVALUATE-FOR GEM. ACS EVALUATE-FOR GEM. accuracy EVALUATE-FOR prior work. adult datasets EVALUATE-FOR GEM. Generic are task, formulation, and they. Task are min max problem, and constrained optimization problem. Method is iterative algorithm. OtherScientificTerm is exponential runtime. ","This paper studies the problem of synthetic data generation for query release. The authors propose two algorithms: PEP and GEM. PEP is based on the maximum entropy distribution of the data, which is an iterative algorithm. GEM uses a generator network to map the data to a high dimensional space, and it is used for pretraining. In this paper, the authors consider the min max problem, where the goal is to minimize the exponential runtime of PEP. In order to solve this task, they propose a formulation where they consider a constrained optimization problem. They compare GEM with prior work on ACS and adult datasets, and show that GEM outperforms PMW and PEP on public data with domain shift."
2551,SP:3ef8660de61a1a73858934fcf8edfec104133ae7,"differentially private ( DP ) data USED-FOR query release. algorithms USED-FOR problem. MWEM HYPONYM-OF algorithms. algorithms PART-OF framework. algorithm USED-FOR framework. PEP USED-FOR exponentially weighted distributions. PEP COMPARE MWEM. MWEM COMPARE PEP. ADULT and ACS datasets USED-FOR 3 - way marginal queries. MWEM CONJUNCTION PEP. PEP CONJUNCTION MWEM. public data USED-FOR synthetic dataset. PMW$^{Pub}$ HYPONYM-OF state - of - the - art. public data USED-FOR generator network. GEM$^{Pub}$ HYPONYM-OF GEM. GEM$^{Pub}$ COMPARE PMW$^{Pub}$. PMW$^{Pub}$ COMPARE GEM$^{Pub}$. GEM$^{Pub}$ CONJUNCTION PMW$^{Pub}$. PMW$^{Pub}$ CONJUNCTION GEM$^{Pub}$. large & similarly - distributed public data EVALUATE-FOR PMW$^{Pub}$. public data USED-FOR GEM$^{Pub}$. Metric are maximum error, max error, and computational cost. Method are FEM, Adaptive Measurements, exponential mechanism, Private Entropy Projection ( PEP ) algorithm, gradient descent, and GEM algorithm. Generic are it, datasets, network, and approach. OtherScientificTerm are maximum entropy, noisy measurements, distribution, generic queries, gradient, and features. Task is k - way marginal queries. Material are sensitive data, and public dataset. ","This paper studies query release from differentially private (DP) data. The authors propose a framework that combines two existing algorithms (MWEM and PEP) to solve this problem. The first algorithm, FEM, is a variant of Adaptive Measurements, where the maximum entropy is estimated from noisy measurements. The second algorithm, Private Entropy Projection (PEP) algorithm, is an extension of gradient descent, where instead of estimating the maximum error from the noisy measurements, the authors estimate the distribution from the generic queries. PEP can be applied to exponentially weighted distributions, and the authors show that PEP outperforms MWEM on 3-way marginal queries on ADULT and ACS datasets. The paper also presents a synthetic dataset that uses public data to train the generator network, and compares it to the state-of-the-art, PMW$^{Pub}$ and GEM$, which is a GEM algorithm that uses only the public data. In particular, the paper shows that the gradient of PEP converges to zero as the number of queries increases, which is an exponential mechanism that can be used to reduce the computational cost of training the network. In addition to this approach, they also propose a way to train a generator network on sensitive data without using the public dataset."
2552,SP:d789e92c1e4f6a44de373210cd732198a6f809be,mask classification USED-FOR semantic segmentation. per - pixel classification CONJUNCTION mask classification. mask classification CONJUNCTION per - pixel classification. per - pixel classification USED-FOR semantic segmentation. mask classification USED-FOR semantic and instance segmentation. benchmarks USED-FOR semantic and panoptic segmentation. ,This paper proposes to use per-pixel classification and mask classification for semantic segmentation. The authors conduct extensive experiments on semantic and instance segmentation using both standard benchmarks for semantic and panoptic segmentation and show promising results.
2553,SP:d789e92c1e4f6a44de373210cd732198a6f809be,semantic segmentation methods USED-FOR task. per - pixel classification USED-FOR task. FCN kind of structure USED-FOR semantic segmentation methods. binary mask prediction CONJUNCTION extra - label classification. extra - label classification CONJUNCTION binary mask prediction. binary mask prediction HYPONYM-OF task. extra - label classification HYPONYM-OF task. method USED-FOR panoptic segmentation tasks. method USED-FOR semantic segmentation. semantic segmentation CONJUNCTION panoptic segmentation tasks. panoptic segmentation tasks CONJUNCTION semantic segmentation. MaskFormer HYPONYM-OF method. ,"This paper proposes to use semantic segmentation methods with FCN kind of structure to solve the task of per-pixel classification. The proposed method, MaskFormer, is able to solve two tasks: binary mask prediction and extra-label classification. Experiments show that the proposed method can achieve state-of-the-art results in semantic segmentations and panoptic segmentation tasks."
2554,SP:d789e92c1e4f6a44de373210cd732198a6f809be,"technique USED-FOR semantic and panoptic segmentation. MLP USED-FOR mask embedding. backbone encoder USED-FOR image embedding. backbon decoder USED-FOR pixel level embedding. out of the transformer decode USED-FOR mask embedding. MLP USED-FOR out of the transformer decode. backbone encoder PART-OF method. mask embeddings CONJUNCTION pixel embeddings. pixel embeddings CONJUNCTION mask embeddings. pixel embeddings USED-FOR mask prdication. mask embeddings USED-FOR mask prdication. MLP output USED-FOR class predictions. semantic segmentation datsets EVALUATE-FOR method. OtherScientificTerm are per pixel labels, and region masks. Method is transform decoder. ",This paper proposes a technique for semantic and panoptic segmentation. The method consists of a backbone encoder that generates the image embedding and a backbon decoder that produces the pixel level embedding. The out of the transformer decode uses an MLP to compute the mask embedding for each pixel. The mask embeddings are used to prune the per pixel labels and the pixel embeds are used for mask prdication. The transform decoder is used to generate the region masks. The MLP output is used for class predictions. The proposed method is evaluated on semantic segmentation datsets.
2555,SP:d789e92c1e4f6a44de373210cd732198a6f809be,transformer - based model USED-FOR semantic segmentation task. MaskFormer HYPONYM-OF transformer - based model. MaskFormer USED-FOR semantic segmentation. MaskFormer USED-FOR binary masks. mask classification problem USED-FOR MaskFormer. mask classification problem USED-FOR semantic segmentation. approach USED-FOR panoptic segmentaion task. semantic segmentation CONJUNCTION panoptic segmentation tasks. panoptic segmentation tasks CONJUNCTION semantic segmentation. panoptic segmentation tasks EVALUATE-FOR method. semantic segmentation EVALUATE-FOR method. ,"This paper proposes MaskFormer, a transformer-based model for semantic segmentation task. MaskFormer solves the mask classification problem in order to generate binary masks. The proposed approach is applied to the panoptic segmentaion task. The method is evaluated on both semantic classation and on the standard panoptimation segmentation tasks."
2556,SP:3aee15083ee1c0a75fedd67a50f9d729bf5ee411,"adversarial examples USED-FOR random depth-2 ReLU networks. O(1 ) upper bound CONJUNCTION \Omega(1 ) lower bound. \Omega(1 ) lower bound CONJUNCTION O(1 ) upper bound. ReLU activation COMPARE smooth activations. smooth activations COMPARE ReLU activation. Method are depth-2 networks, gradient step, smooth and ReLU activations, deeper networks, and ReLU networks. OtherScientificTerm are normal distribution, uniform distribution, polylogarithmic expression, layers, and \eta. Generic is network. ","This paper studies adversarial examples for random depth-2 ReLU networks trained on a set of randomly generated samples from a normal distribution. The authors first show that the O(1) upper bound and the \Omega(1 ) lower bound of the O(\sqrt{T}) upper bound for depth-1 and O(T) lower bounds for deep-2 networks can be derived from a polylogarithmic expression of the gradient step, where T is the number of samples from the normal distribution and T is a uniform distribution. Then, the authors show that for deeper networks, the O((1/\sqrt(T)) upper bound can be obtained as a function of the dimension of the network, which can be expressed as a linear combination of smooth and ReLU activations. In particular, they show that a ReLU activation can be as large as smooth activations, and that for a given number of layers, if the number (\eta) of layers is O(n^{-1/2} \log n^{-2}/n, then for any layer of depth 2, the \eta can be approximated by a function that is O((n^2/n^3) \log \sqrt (N^3/n), where n is the dimension, and n is a number of neurons in the network. "
2557,SP:3aee15083ee1c0a75fedd67a50f9d729bf5ee411,"adversarial examples USED-FOR random two - layer networks. ReLU or smooth activations USED-FOR random two - layer networks. gradient descent USED-FOR adversarial examples. landscape FEATURE-OF network function. gradient step USED-FOR adversarial example. OtherScientificTerm are gradient, and multiple layers. ","This paper studies adversarial examples for random two-layer networks with ReLU or smooth activations using gradient descent. The authors show that the landscape of the network function changes as the gradient increases, and that the gradient step for an adversarial example is independent of the number of layers. They also show that multiple layers are more likely to be adversarial."
2558,SP:3aee15083ee1c0a75fedd67a50f9d729bf5ee411,adversarial examples USED-FOR neural networks. gradient descent USED-FOR neural networks. gradient update USED-FOR adversarial examples. random initialization USED-FOR gradient update. gradient update USED-FOR two layer networks. ,This paper studies the effect of gradient descent on the performance of neural networks trained on adversarial examples. The authors show that the gradient update of two layer networks trained with random initialization converges to a gradient update that is robust to adversarial example.
2559,SP:3aee15083ee1c0a75fedd67a50f9d729bf5ee411,"randomly initialized parameters USED-FOR neural network. Method is two - layer neural networks. OtherScientificTerm are random initialization, and activation functions. ","This paper studies the problem of training two-layer neural networks with random initialization. The authors propose to train a neural network with random initialized parameters. The idea is to train the neural network on a set of randomly initialized parameters, and then train the activation functions on top of the random initialized weights. "
2560,SP:220db9ed147bbe67de5d82778720a1549656e48d,latent space FEATURE-OF variational autoencoder. variational autoencoder USED-FOR score - based generative models. latent space FEATURE-OF score - based generative models. normal prior USED-FOR VAE. encoder distribution CONJUNCTION SGM prior. SGM prior CONJUNCTION encoder distribution. cross entropy USED-FOR training objective. Method is score - based generative model. Task is variance reduction of the loss function. ,"This paper studies the latent space of a variational autoencoder, which is a popular choice for training score-based generative models in latent space. The authors propose a VAE with a normal prior. The training objective is based on cross entropy between the encoder distribution and the SGM prior. They also propose a variance reduction of the loss function."
2561,SP:220db9ed147bbe67de5d82778720a1549656e48d,"score - based prior PART-OF Variational Autoencoder ( VAE ) framework. score - based models COMPARE LSGM. LSGM COMPARE score - based models. data space FEATURE-OF score - based models. latent space FEATURE-OF score - based model. score - based model USED-FOR LSGM. latent space FEATURE-OF score - based prior. decoder USED-FOR data space. ELBO USED-FOR model. Multiple variance reduction techniques CONJUNCTION training tricks. training tricks CONJUNCTION Multiple variance reduction techniques. training tricks USED-FOR objective function. Multiple variance reduction techniques USED-FOR objective function. model USED-FOR high fidelity images. OtherScientificTerm are base distribution ( standard Gaussian ), and time - dependent marginal score function. ","This paper proposes a Variational Autoencoder (VAE) framework that adds a score-based prior to the base distribution (standard Gaussian) and a time-dependent marginal score function. The authors show that the proposed LSGM can be trained with a score -based model in the latent space, which is different from the standard LSGM in terms of the data space used by the decoder. The proposed model is trained using ELBO. Multiple variance reduction techniques and training tricks are used to optimize the objective function. Experiments show the proposed model can generate high fidelity images."
2562,SP:220db9ed147bbe67de5d82778720a1549656e48d,prior / inference scheme USED-FOR generative model. denoising score - matching USED-FOR prior / inference scheme. VAE framework USED-FOR generative model. observed space FEATURE-OF score - based generation. sample quality CONJUNCTION likelihood of data. likelihood of data CONJUNCTION sample quality. OtherScientificTerm is latent space. Metric is sampling time. Method is generative modelling techniques. ,"This paper proposes a new prior/inference scheme based on denoising score-matching to train a generative model in a VAE framework. The authors argue that score-based generation in the observed space is inefficient because the latent space is too large and the sampling time is too slow. To address this issue, the authors propose two generative modelling techniques, which aim to balance sample quality and likelihood of data."
2563,SP:220db9ed147bbe67de5d82778720a1549656e48d,score - based generative model USED-FOR flexible VAE prior. score - based model COMPARE score - based model. score - based model COMPARE score - based model. latent space USED-FOR score - based model. high - dim pixel space USED-FOR score - based model. generative FID score CONJUNCTION negative log - likelihood. negative log - likelihood CONJUNCTION generative FID score. negative log - likelihood CONJUNCTION ablation studies. ablation studies CONJUNCTION negative log - likelihood. generative FID score USED-FOR model. negative log - likelihood USED-FOR model. ablation studies EVALUATE-FOR model. ,"This paper proposes a score-based generative model for flexible VAE prior. Compared to the existing state-of-the-art, the proposed model uses a latent space instead of the standard score -based model which uses a high-dim pixel space. The proposed model is evaluated using a generative FID score, a negative log-likelihood, and ablation studies."
2564,SP:d681e4e28c03f610acf6817a9e57db0c41c196b4,1 hidden - layer convolutional neural network COMPARE finite width CNTK. finite width CNTK COMPARE 1 hidden - layer convolutional neural network. training dynamics EVALUATE-FOR 1 hidden - layer convolutional neural network. data distribution EVALUATE-FOR finite width CNTK. neural networks COMPARE kernel methods. kernel methods COMPARE neural networks. neural networks CONJUNCTION kernel methods. kernel methods CONJUNCTION neural networks. neural networks USED-FOR localized influential features. localized influential features CONJUNCTION noisy background. noisy background CONJUNCTION localized influential features. kernel methods USED-FOR localized influential features. CNN CONJUNCTION finite width CNTK. finite width CNTK CONJUNCTION CNN. finite width CNTK USED-FOR noise extended variants. CNN USED-FOR noise extended variants. CIFAR-10 dataset FEATURE-OF noise extended variants. CIFAR-10 dataset EVALUATE-FOR finite width CNTK. it COMPARE neural networks. neural networks COMPARE it. OtherScientificTerm is ` ` '' Local Signal Adaptivity ''. Method is kernels. ,This paper studies the training dynamics of a 1 hidden-layer convolutional neural network with finite width CNTK on a data distribution. The authors compare the performance of neural networks and kernel methods on localized influential features and noisy background. They show that the noise extended variants of CNN and finite widthCNTK outperform neural networks on the CIFAR-10 dataset. They also show that ``''Local Signal Adaptivity''' is an important property of kernels and that it can be achieved by neural networks.
2565,SP:d681e4e28c03f610acf6817a9e57db0c41c196b4,"NNs COMPARE kernel methods. kernel methods COMPARE NNs. NTK HYPONYM-OF kernel methods. non - linear NNs USED-FOR sparse signal. sparse signal FEATURE-OF data distribution setting. GD USED-FOR single filter CNN. model USED-FOR underparameterised linear NN. non - linear NNs USED-FOR noise. noise FEATURE-OF non - linear and linear NNs. OtherScientificTerm are local signal adaptivity ( LSA ), and large noise. Method is LSA. ","This paper studies the problem of local signal adaptivity (LSA) and shows that non-linear NNs are able to adapt to sparse signal in a data distribution setting, which is in contrast to kernel methods such as NTK. The main contribution of this paper is to show that GD can be used as a single filter CNN and that NNs with LSA are more robust to large noise than kernel methods. In particular, the authors show that the model can adapt to an underparameterised linear NN. The authors also show that for large noise, the noise can be reduced to a small amount of noise for both nonlinear and linear NNs."
2566,SP:d681e4e28c03f610acf6817a9e57db0c41c196b4,"CNN architecture USED-FOR templates. templates PART-OF gaussian noise. CNN architecture USED-FOR toy binary classification problem. neural tangent kernel approach USED-FOR problem. naive matched filtering USED-FOR template. weight sharing CONJUNCTION output averaging. output averaging CONJUNCTION weight sharing. CNN USED-FOR task. finite samples USED-FOR data generating distribution. custom denoising nonlinearity CONJUNCTION soft thresholding function. soft thresholding function CONJUNCTION custom denoising nonlinearity. mini - batch SGD USED-FOR logistic loss. output averaging FEATURE-OF two $ d$-strided convolutions. mini - batch SGD USED-FOR finite samples. weight sharing FEATURE-OF two $ d$-strided convolutions. mini - batch SGD USED-FOR data generating distribution. gaussian random initialization USED-FOR mini - batch SGD. finite samples USED-FOR CNN. logistic loss USED-FOR data generating distribution. soft thresholding function PART-OF CNN. custom denoising nonlinearity PART-OF CNN. network USED-FOR whp. convolutional NTK USED-FOR whp. procedure USED-FOR classifier. polynomial samples CONJUNCTION step sizes. step sizes CONJUNCTION polynomial samples. two filter network HYPONYM-OF neural network. convolutional NTK PART-OF network. polynomial samples USED-FOR classifier. step sizes USED-FOR classifier. neural network COMPARE NTK. NTK COMPARE neural network. ImageNet CONJUNCTION gaussian noise. gaussian noise CONJUNCTION ImageNet. random backgrounds FEATURE-OF CIFAR-10 images. classification tasks EVALUATE-FOR NTK. classification tasks EVALUATE-FOR neural network. Task is natural image classification tasks. Method are CNNs, and data model. OtherScientificTerm are nuisance background, architectural separation, i.i.d. gaussian noise, image patches, independent noise, noise variance, and soft thresholding function's parameter. ","This paper proposes a novel CNN architecture for learning templates in gaussian noise for a toy binary classification problem. The problem is formulated as a neural tangent kernel approach, where each template is trained using naive matched filtering. The authors argue that CNNs should be able to generalize to natural image classification tasks where the nuisance background is not present. To solve this task, the authors propose to use a mini-batch SGD with gaussian random initialization to generate finite samples for the data generating distribution, which are then used to train a CNN with custom denoising nonlinearity, a soft thresholding function, and output averaging for two $d$-strided convolutions with weight sharing. The proposed procedure allows the classifier to be trained with polynomial samples and step sizes, which is an architectural separation from the NTK (a neural network with a two filter network). The authors show that the proposed neural network can generalize whp with a convolutional NTK, and that the resulting data model is robust to i.i.d. gaussian and independent noise. Experiments are conducted on CIFAR-10 images with random backgrounds, where the authors compare the performance of the neural network to NTK on a variety of classification tasks. The results show that their neural network generalizes better than NTK when the noise variance is small."
2567,SP:d681e4e28c03f610acf6817a9e57db0c41c196b4,"neural networks COMPARE neural tangent kernel. neural tangent kernel COMPARE neural networks. neural networks USED-FOR problem setting. two layer neural network USED-FOR problem class. relu activations CONJUNCTION logistic loss. logistic loss CONJUNCTION relu activations. two layer neural network COMPARE neural tangent kernel counterpart. neural tangent kernel counterpart COMPARE two layer neural network. relu activations PART-OF two layer neural network. logistic loss PART-OF two layer neural network. sample and time complexity EVALUATE-FOR two layer neural network. two layer neural network USED-FOR problem. image classification settings EVALUATE-FOR problem. Task is binary classification problem. OtherScientificTerm are background of noise, and toy - like distribution. ","This paper studies a binary classification problem where the input is a sequence of images with a background of noise and the output is a toy-like distribution. In this problem setting, neural networks are used instead of the standard neural tangent kernel. The authors propose a two layer neural network for the problem class, which consists of relu activations and a logistic loss. The sample and time complexity of the proposed problem is compared to that of the original two-layer neural network as well as the standard, but not necessarily the standard (or even the standard two layer, but the standard) Neural Tangent Kernel. The problem is evaluated on several image classification settings."
2568,SP:c347796244fcf9b5de19c68bcc5c811b7448217d,convergence analysis USED-FOR algorithm. stochastic gradient tracking algorithms HYPONYM-OF algorithm. Task is decentralized nonconvex optimization problems. OtherScientificTerm is quadratic case. ,"This paper studies decentralized nonconvex optimization problems. The authors propose an algorithm called stochastic gradient tracking algorithms, which is based on convergence analysis. In particular, the authors prove convergence results for the quadratic case."
2569,SP:c347796244fcf9b5de19c68bcc5c811b7448217d,gradient tracking method USED-FOR decentralized optimization. it COMPARE methods. methods COMPARE it. convergence rate COMPARE methods. methods COMPARE convergence rate. convergence rate EVALUATE-FOR it. strongly convex problems CONJUNCTION nonconvex problems. nonconvex problems CONJUNCTION strongly convex problems. nonstrongly convex problems CONJUNCTION strongly convex problems. strongly convex problems CONJUNCTION nonstrongly convex problems. tighter analysis USED-FOR nonstrongly convex problems. tighter analysis USED-FOR strongly convex problems. tighter analysis USED-FOR nonconvex problems. Metric is Faster convergence rates. ,"This paper proposes a gradient tracking method for decentralized optimization. Compared to existing methods, it achieves a faster convergence rate than existing methods. Faster convergence rates are achieved by using a tighter analysis for both strongly convex problems and nonconvex problems."
2570,SP:c347796244fcf9b5de19c68bcc5c811b7448217d,"gradient tracking ( GT ) algorithm USED-FOR stochastic distributed optimization problems. convergence rate EVALUATE-FOR network parameters. Material is undirected, static graphs. ","This paper studies the gradient tracking (GT) algorithm for stochastic distributed optimization problems. The authors show that for undirected, static graphs, the convergence rate of the network parameters converges linearly with the number of iterations."
2571,SP:c347796244fcf9b5de19c68bcc5c811b7448217d,"Gradient Tracking HYPONYM-OF decentralized optimization algorithm. Gradient Tracking USED-FOR stochastic setting. leading order term COMPARE centralized mini - batch SGD. centralized mini - batch SGD COMPARE leading order term. GT variables FEATURE-OF consensus difference. consensus difference USED-FOR recursion. rates EVALUATE-FOR Gradient Tracking. graph - dependent constants PART-OF higher order terms. Method is gradient tracking algorithms. Generic are it, settings, methods, and framework. OtherScientificTerm is X_t. ","This paper proposes Gradient Tracking, a decentralized optimization algorithm, which is a generalization of gradient tracking algorithms. The main contribution of the paper is to extend Gradient tracking to the stochastic setting, where it is shown that it can achieve better rates than centralized mini-batch SGD. This is achieved by adding a leading order term that is independent of X_t, instead of the standard standard linear combination of graph-dependent constants in higher order terms. The authors also show that the recursion can be reduced to a consensus difference over GT variables. The paper also provides a theoretical analysis of the convergence of the proposed methods and provides some theoretical guarantees for the proposed framework."
2572,SP:24d637e8c3489bfe50b17bf684097776ad6ee485,"UCB CONJUNCTION Thompson sampling algorithms. Thompson sampling algorithms CONJUNCTION UCB. arm sampling behavior FEATURE-OF UCB. Thompson sampling algorithms USED-FOR arm sampling behavior. asymptotic behavior FEATURE-OF arm sampling. minimax regret EVALUATE-FOR UCB. incomplete learning phenomenon FEATURE-OF Thomson sampling. Material is two - arm case. OtherScientificTerm are suboptimality gap, time horizon, and sample - split. ","This paper studies the asymptotic behavior of UCB and Thompson sampling algorithms in the two-arm case. The authors show that UCB achieves a minimax regret of $O(1/\sqrt{T})$ with a suboptimality gap of $\Omega(T)$. The authors also show that Thompson sampling suffers from an incomplete learning phenomenon, where the time horizon of the sample-split can be arbitrarily long."
2573,SP:24d637e8c3489bfe50b17bf684097776ad6ee485,UCB CONJUNCTION Thompson Sampling. Thompson Sampling CONJUNCTION UCB. Thompson Sampling HYPONYM-OF bandit algorithms. UCB HYPONYM-OF bandit algorithms. UCB CONJUNCTION Thompson Sampling. Thompson Sampling CONJUNCTION UCB. UCB USED-FOR arm sampling distribution. Thompson Sampling USED-FOR arm sampling distribution. Generic is distributions. ,"This paper proposes two new bandit algorithms, UCB and Thompson Sampling. UCB aims to learn an arm sampling distribution that is similar to the one used in the original UCB. The authors show that these two distributions can be trained in parallel. "
2574,SP:24d637e8c3489bfe50b17bf684097776ad6ee485,UCB CONJUNCTION Thompson sampling. Thompson sampling CONJUNCTION UCB. asymptotic behavior FEATURE-OF arm - sampling distributions. UCB USED-FOR arm - sampling distributions. Thompson sampling USED-FOR arm - sampling distributions. algorithm - specific worst case bound CONJUNCTION diffusion - limit performance. diffusion - limit performance CONJUNCTION algorithm - specific worst case bound. characterization USED-FOR UCB algorithm. Method is asymptotic characterization of the distributions. Generic is distributions. OtherScientificTerm is arm sampling rates. ,This paper studies the asymptotic behavior of arm-sampling distributions learned by UCB and Thompson sampling. The authors provide an algorithm-specific worst case bound and a diffusion-limit performance for the two distributions. They also provide a new characterization of the UCB algorithm and provide a theoretical analysis of the arm sampling rates.
2575,SP:24d637e8c3489bfe50b17bf684097776ad6ee485,upper confidence bound ( UCB ) algorithm USED-FOR multi - armed bandits ( MAB ). asymptotic regret lower bound USED-FOR UCB. Brownian motion USED-FOR UCB. asymptotical empirical sum USED-FOR UCB. Brownian motion USED-FOR asymptotical empirical sum. two - armed bandits USED-FOR UCB. asymptotical arm - sampling rate USED-FOR Bernoulli Thompson sampling. two - armed * deterministic * bandits FEATURE-OF Bernoulli Thompson sampling. OtherScientificTerm is asymptotical behaviors. Material is MAB. ,"This paper studies the upper confidence bound (UCB) algorithm for multi-armed bandits (MAB) and provides an asymptotic regret lower bound for UCB. UCB uses Brownian motion to estimate the asymntotical empirical sum of the UCB with respect to a fixed set of arms, which is then used to compute the upper bound of UCB in two-armed bands. The paper also provides an upper bound on the variance of the upper and lower bound of the variance for MAB, which depends on the arm-sampling rate. The authors also show that the variance in the variance is proportional to the variance (i.e., the variance) of the number of arms used in the algorithm, as well as to the variability of the asmptotical behaviors of the arms. Finally, the authors provide an analysis of Bernoulli Thompson sampling with an appropriate (and possibly lower variance) asymmetric arm-samples rate for two-arm *deterministic* bandits, and show that MAB converges to a stationary point with high variance."
2576,SP:43c8bcc93aa034965fa6d959d44b529ffc110cc7,framework USED-FOR cross - domain item cold - start recommendation. Method is Stein Variational Gradient Descent. OtherScientificTerm is embedding distribution of warm items. Metric is computational time costs. ,"This paper proposes a framework for cross-domain item cold-start recommendation based on Stein Variational Gradient Descent. The key idea is to learn an embedding distribution of warm items, which can reduce computational time costs."
2577,SP:43c8bcc93aa034965fa6d959d44b529ffc110cc7,Cross - Domain Recommendation ( CDR ) setting FEATURE-OF cold - start issue. rating prediction module CONJUNCTION embedding distribution alignment module. embedding distribution alignment module CONJUNCTION rating prediction module. modules PART-OF DisAlign framework. embedding distribution alignment module HYPONYM-OF modules. rating prediction module HYPONYM-OF modules. Stein path alignment CONJUNCTION proxy Stein path alignment. proxy Stein path alignment CONJUNCTION Stein path alignment. benchmark datasets EVALUATE-FOR model. baselines COMPARE model. model COMPARE baselines. benchmark datasets COMPARE baselines. baselines COMPARE benchmark datasets. Generic is it. ,"This paper addresses the cold-start issue in the Cross-Domain Recommendation (CDR) setting. The authors propose the DisAlign framework, which consists of three modules: a rating prediction module, an embedding distribution alignment module, and a Stein path alignment. The proposed model is evaluated on three benchmark datasets and compared with two baselines. The experiments show that the proposed model outperforms the baselines and that it is able to generalize to unseen domains."
2578,SP:43c8bcc93aa034965fa6d959d44b529ffc110cc7,auxiliary information USED-FOR cold - start item recommendation. Stein path USED-FOR domain distributions. Stein path USED-FOR heterogeneity/ domain discrepancy. source item auxiliary embedding CONJUNCTION source item collaborative embedding. source item collaborative embedding CONJUNCTION source item auxiliary embedding. cubic time complexity EVALUATE-FOR proxy Stein path. real - world datasets EVALUATE-FOR SOTA methods. ,"This paper proposes to use auxiliary information for cold-start item recommendation. The authors propose to use a proxy Stein path with cubic time complexity to approximate the domain distributions, and use the Stein path to reduce heterogeneity/ domain discrepancy between source item auxiliary embedding and source item collaborative embedding. Experiments on real-world datasets show improvements over SOTA methods."
2579,SP:43c8bcc93aa034965fa6d959d44b529ffc110cc7,user - item ratings CONJUNCTION item descriptions. item descriptions CONJUNCTION user - item ratings. rating prediction module CONJUNCTION embedding distribution alignment module. embedding distribution alignment module CONJUNCTION rating prediction module. modules PART-OF DisAlign. modules PART-OF recommendation framework. embedding distribution alignment module HYPONYM-OF modules. DisAlign HYPONYM-OF recommendation framework. rating prediction module HYPONYM-OF modules. embedding distribution alignment module PART-OF recommendation framework. Generic is problem. OtherScientificTerm is user - item interactions. ,"This paper proposes a recommendation framework, DisAlign, which consists of three modules: a rating prediction module, an embedding distribution alignment module, and a combination of user-item ratings and item descriptions. The main idea of the paper is to tackle the problem of predicting the distribution of items in an unsupervised manner. This is done by predicting the likelihood of each item in a given set of items, which is then used to predict the distribution over the items in the set. The paper also proposes to model user-items interactions and show that the predicted distribution over items can be used as a proxy for the actual distribution."
2580,SP:729c3e22a6f0170bdc8e1f511812dc9e9a4fd4a8,"attention - based architecture USED-FOR computer vision ( classification ). self - attention layer PART-OF ViT. self - attention layer PART-OF computational blocks. Fourier transform PART-OF computational blocks. self - attention layer PART-OF architecture. FFT CONJUNCTION IFFT. IFFT CONJUNCTION FFT. FFT USED-FOR self - attention. quadratic complexity EVALUATE-FOR self - attention. log - linear computational cost FEATURE-OF quadratic complexity. Method are pre - trained vision transformer, and transfer learning. Material is ImageNet. ",This paper proposes an attention-based architecture for computer vision (classification). The proposed architecture consists of a self-attention layer that is trained on top of ViT and a pre-trained vision transformer. The computational blocks are based on Fourier transform. The authors show that using FFT and IFFT leads to a quadratic complexity with a log-linear computational cost. They also show that transfer learning can be performed on ImageNet.
2581,SP:729c3e22a6f0170bdc8e1f511812dc9e9a4fd4a8,einsum CONJUNCTION 2D inverse FFT. 2D inverse FFT CONJUNCTION einsum. 2D FFT CONJUNCTION einsum. einsum CONJUNCTION 2D FFT. self - attention layer CONJUNCTION global filter layer. global filter layer CONJUNCTION self - attention layer. 2D FFT PART-OF global filter layer. einsum PART-OF global filter layer. 2D inverse FFT PART-OF global filter layer. GFNet COMPARE Transformer - like models. Transformer - like models COMPARE GFNet. accuracy - FLOPs basis EVALUATE-FOR Transformer - like models. ImageNet EVALUATE-FOR GFNet. ImageNet EVALUATE-FOR Transformer - like models. Generic is approach. Task is attention computation. Metric is time - complexity. ,"This paper proposes a novel approach to reduce the number of parameters in attention computation. The proposed approach, called GFNet, consists of a self-attention layer, a global filter layer that combines einsum, 2D FFT, and 2D inverse FFT. The authors show that GFNet outperforms Transformer-like models on the accuracy-FLOPs basis on ImageNet. The time-complexity of the proposed approach is also discussed."
2582,SP:729c3e22a6f0170bdc8e1f511812dc9e9a4fd4a8,2D fast Fourier transform ( FFT ) CONJUNCTION point - wise multiplication. point - wise multiplication CONJUNCTION 2D fast Fourier transform ( FFT ). point - wise multiplication CONJUNCTION 2D inverse FFT. 2D inverse FFT CONJUNCTION point - wise multiplication. self - attention USED-FOR vision transformer. self - attention PART-OF Global Filter Layer. 2D inverse FFT PART-OF Global Filter Layer. point - wise multiplication PART-OF Global Filter Layer. 2D fast Fourier transform ( FFT ) PART-OF Global Filter Layer. layer COMPARE self - attention. self - attention COMPARE layer. compute cost CONJUNCTION memory footprint. memory footprint CONJUNCTION compute cost. it USED-FOR quadratic dependency. quadratic dependency COMPARE log - linear rate. log - linear rate COMPARE quadratic dependency. compute cost EVALUATE-FOR quadratic dependency. fine - tuning USED-FOR downstream task. model USED-FOR supervised classification. model USED-FOR fine - tuning. model USED-FOR downstream task. ImageNet USED-FOR supervised classification. Method is MLP - mixer. ,"This paper proposes a Global Filter Layer that combines 2D fast Fourier transform (FFT) with point-wise multiplication and 2D inverse FFT. The proposed layer is similar to self-attention in vision transformer. However, it introduces a quadratic dependency instead of the log-linear rate, which reduces the compute cost and memory footprint. The paper also proposes a MLP-mixer to combine the two layers. The model is applied to a downstream task on ImageNet."
2583,SP:729c3e22a6f0170bdc8e1f511812dc9e9a4fd4a8,discrete Fourier transform ( DFT ) CONJUNCTION global filter layer. global filter layer CONJUNCTION discrete Fourier transform ( DFT ). global filter layer USED-FOR Global Filter Networks ( GFNet ). discrete Fourier transform ( DFT ) USED-FOR Global Filter Networks ( GFNet ). depthwise global circular convolution HYPONYM-OF operations. convolutional network USED-FOR work. global convolutional layers PART-OF convolutional network. self - attention CONJUNCTION spatial MLP. spatial MLP CONJUNCTION self - attention. FFT USED-FOR method. method COMPARE methods. methods COMPARE method. spatial MLP HYPONYM-OF methods. self - attention HYPONYM-OF methods. ImageNet EVALUATE-FOR methods. ImageNet EVALUATE-FOR method. Method is convolution theorem. ,This paper proposes Global Filter Networks (GFNet) that combine a discrete Fourier transform (DFT) with a global filter layer. The work is based on a convolutional network that consists of two operations: (1) depthwise global circular convolution and (2) global convolution. The convolution theorem is proved and the method is shown to be equivalent to FFT. Experiments on ImageNet show that the proposed method outperforms existing methods such as self-attention and spatial MLP.
2584,SP:e278079529d6da9e2ea26b47730dbc1256ffe2db,"methods USED-FOR classifier. loss functions USED-FOR binary classification task. uncertainty estimates CONJUNCTION focal loss. focal loss CONJUNCTION uncertainty estimates. focal loss USED-FOR binary classification task. focal loss HYPONYM-OF loss functions. uncertainty estimates HYPONYM-OF loss functions. steep slope loss USED-FOR features. Imagenet CONJUNCTION stylized / adversarial test sets. stylized / adversarial test sets CONJUNCTION Imagenet. stylized / adversarial test sets EVALUATE-FOR oracle / target classifier combinations. Imagenet EVALUATE-FOR oracle / target classifier combinations. Material is large scale datasets. Metric are FPR, and TNR. Generic is loss. ",This paper proposes methods to train a classifier that can generalize to large scale datasets. The authors propose two loss functions for binary classification task: uncertainty estimates and focal loss. The main contribution of the paper is to propose a steep slope loss that penalizes the features that are most likely to be highly correlated with the FPR. This loss is motivated by the observation that the features are more correlated with FPR than with TNR. Experiments on Imagenet and stylized/adversarial test sets show that the proposed oracle/target classifier combinations outperform the state-of-the-art oracle / target classifier combination.
2585,SP:e278079529d6da9e2ea26b47730dbc1256ffe2db,steep slope loss USED-FOR trustworthiness. supervised training setup USED-FOR steep slope loss. pre - trained neural network USED-FOR trustworthiness. steep slope loss USED-FOR pre - trained neural network. binary classification problem USED-FOR trustworthiness. ResNet CONJUNCTION ViT model. ViT model CONJUNCTION ResNet. ImageNet USED-FOR ResNet. ImageNet CONJUNCTION ViT model. ViT model CONJUNCTION ImageNet. ViT model EVALUATE-FOR approach. ImageNet EVALUATE-FOR approach. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. ,This paper proposes to use steep slope loss in a supervised training setup to improve the trustworthiness of a pre-trained neural network. The trustworthiness is modeled as a binary classification problem. The proposed approach is evaluated on ResNet trained on ImageNet and ViT model on MNIST and CIFAR-10.
2586,SP:e278079529d6da9e2ea26b47730dbc1256ffe2db,"Steep Slope loss HYPONYM-OF loss function. loss function USED-FOR binary classification task of prediction trustworthiness. cross - entropy loss CONJUNCTION focal loss. focal loss CONJUNCTION cross - entropy loss. focal loss CONJUNCTION true classification probability loss ( TCP ). true classification probability loss ( TCP ) CONJUNCTION focal loss. loss functions USED-FOR domain. datasets EVALUATE-FOR classifiers. accuracy EVALUATE-FOR classifiers. cross - entropy loss HYPONYM-OF loss functions. focal loss HYPONYM-OF loss functions. true classification probability loss ( TCP ) HYPONYM-OF loss functions. hyperparameters USED-FOR derivative and gradient updates. slope parameters USED-FOR negative and positive classification thresholds. hyperparameters USED-FOR slope parameters. AUPRC CONJUNCTION AUROC. AUROC CONJUNCTION AUPRC. AUROC CONJUNCTION FPR95. FPR95 CONJUNCTION AUROC. SS loss USED-FOR detecting incorrectly classified points. FPR95 HYPONYM-OF binary classification metrics. AUROC HYPONYM-OF binary classification metrics. AUPRC HYPONYM-OF binary classification metrics. binary classification metrics EVALUATE-FOR loss. TNR EVALUATE-FOR SS loss. MNIST, CIFAR, and Imagenet datasets EVALUATE-FOR loss. OtherScientificTerm are class imbalance, signed distance, and hyperplane of classification. Method is classifier. ","This paper proposes a new loss function called Steep Slope loss for the binary classification task of prediction trustworthiness, which is a loss function that aims to reduce the class imbalance between positive and negative samples. The authors propose three loss functions for this domain: a cross-entropy loss, a focal loss, and a true classification probability loss (TCP). The authors evaluate the classifiers on MNIST, CIFAR, and Imagenet datasets and show that the proposed SS loss improves the accuracy of the classifier on all three datasets. The paper also shows that the SS loss can be used for detecting incorrectly classified points that are close in signed distance to the hyperplane of classification. The proposed slope parameters for both negative and positive classification thresholds are learned using different hyperparameters for the derivative and gradient updates. Experiments on binary classification metrics such as AUPRC, AUROC, and FPR95 show the effectiveness of the proposed loss on TNR."
2587,SP:e278079529d6da9e2ea26b47730dbc1256ffe2db,"past trustworthiness predicting methods USED-FOR overfitting. training loss USED-FOR generalizability of trustworthiness predictors. steep slope loss HYPONYM-OF training loss. focal loss CONJUNCTION TCP loss. TCP loss CONJUNCTION focal loss. cross - entropy loss CONJUNCTION focal loss. focal loss CONJUNCTION cross - entropy loss. methods CONJUNCTION focal loss. focal loss CONJUNCTION methods. cross - entropy loss FEATURE-OF methods. OtherScientificTerm are label imbalance, trustworthiness predictors, and negative class. Material is ImageNet. Generic is method. ","This paper studies the problem of overfitting in past trustworthiness predicting methods. The authors propose a new training loss, called steep slope loss, to improve the generalizability of trustworthiness predictors. In particular, the authors argue that label imbalance can lead to overfitting to the negative class. The proposed method is evaluated on ImageNet and compared with methods with cross-entropy loss, focal loss, and TCP loss."
2588,SP:c5704a709f318c6e9a5c716e5e7f250acccf46a8,linear components PART-OF weight matrix W. non - linear functions PART-OF networks. linearity USED-FOR adversarial models. weight and feature dimensions FEATURE-OF clustering effects. clustering effects FEATURE-OF adversarially trained models. adversarial attack CONJUNCTION transfer learning. transfer learning CONJUNCTION adversarial attack. supervision USED-FOR tasks. adversarial attack HYPONYM-OF tasks. transfer learning HYPONYM-OF tasks. Method is linear components of adversarial robust models. OtherScientificTerm is W. ,"This paper studies the linear components of the weight matrix W, which are non-linear functions in networks. The linearity of adversarial models has been studied in the literature, and the authors argue that the linearity is necessary for adversarially trained models to be robust to clustering effects in both the weight and feature dimensions. The authors propose to use linear components in adversarial robust models as a way to reduce the variance of the W. They also propose two tasks that require supervision: adversarial attack and transfer learning."
2589,SP:c5704a709f318c6e9a5c716e5e7f250acccf46a8,adversarial data USED-FOR Adversarial robustness. adversarial attack evaluations USED-FOR robust classifier. linear components USED-FOR adversarial robustness. statistical properties USED-FOR comprehensively robust classifiers. hierarchical clustering FEATURE-OF robust classifiers. robustness boosting CONJUNCTION domain adaption. domain adaption CONJUNCTION robustness boosting. this USED-FOR tasks. domain adaption HYPONYM-OF tasks. robustness boosting HYPONYM-OF tasks. Method is linear sub - networks. ,Adversarial robustness to adversarial data is an important topic in the literature. This paper studies the problem of adversarial attack evaluations for a robust classifier. The authors propose to use linear components of the robust classifiers as linear sub-networks. The main contribution of this paper is to study the statistical properties of the comprehensively robustclassifiers. They show that the hierarchical clustering of the learned classifiers leads to better robustness. They apply this to two tasks: robustness boosting and domain adaption.
2590,SP:c5704a709f318c6e9a5c716e5e7f250acccf46a8,"non - linearities ( ReLUs ) PART-OF network. hierarchical clustering FEATURE-OF representations. adversarial robustness FEATURE-OF hierarchical clustering. regularization term USED-FOR clustering behavior. adversarial robustness CONJUNCTION domain adaptation. domain adaptation CONJUNCTION adversarial robustness. domain adaptation EVALUATE-FOR models. adversarial robustness EVALUATE-FOR models. Method are adversarially robust models, class - wise feature representations, and CNNs. Generic is it. ","This paper studies adversarially robust models. The authors propose to incorporate non-linearities (ReLUs) into the network to improve the robustness of representations with hierarchical clustering. Specifically, the authors propose a regularization term to encourage clustering behavior that encourages the class-wise feature representations to be close to each other, and show that it can improve the adversarial robustness and domain adaptation performance of models trained with CNNs."
2591,SP:c5704a709f318c6e9a5c716e5e7f250acccf46a8,linear sub - networks USED-FOR adversarial robustness. implicit weight expression USED-FOR approximated class - wise responses. back propagation USED-FOR implicit weight expression. linear sub - networks PART-OF pre - trained network. back propagation USED-FOR linear sub - networks. back propagation USED-FOR pre - trained network. matrix USED-FOR hierarchy of classes. class hierarchy FEATURE-OF feature distance. class hierarchy USED-FOR feature distance regularization. ,"This paper proposes to use linear sub-networks of a pre-trained network with back propagation to learn an implicit weight expression for approximated class-wise responses for adversarial robustness. The main idea is to use a matrix to represent the hierarchy of classes, which is then used to regularize the feature distance along the class hierarchy for feature distance regularization."
2592,SP:c5704a709f318c6e9a5c716e5e7f250acccf46a8,method USED-FOR linear model exploration. implicit linear matrix expression USED-FOR method. clustering effect FEATURE-OF adversarial robustness. linearity of weights USED-FOR adversarial robustness. OtherScientificTerm is weight clustering effect. ,This paper proposes a method for linear model exploration based on an implicit linear matrix expression. The main contribution of this paper is the analysis of the clustering effect of adversarial robustness to the linearity of weights. The paper provides a theoretical analysis of this effect and empirically shows that the weight clustering effects can be controlled.
2593,SP:590b67b1278267e966cf0b31456d981441e61bb1,learning based approach USED-FOR end - to - end reconstruction of operators. end - to - end reconstruction of operators USED-FOR ill - posed problems. variational approach CONJUNCTION optimal transport based regularizer. optimal transport based regularizer CONJUNCTION variational approach. Task is image reconstruction. ,This paper proposes a learning based approach for end-to-end reconstruction of operators for ill-posed problems. The main idea is to combine a variational approach with an optimal transport based regularizer. Experiments are conducted on image reconstruction.
2594,SP:590b67b1278267e966cf0b31456d981441e61bb1,method USED-FOR image reconstruction. method USED-FOR adversarial regularization. adversarial regularization USED-FOR image reconstruction. system CONJUNCTION reconstruction. reconstruction CONJUNCTION system. method COMPARE regularization methods. regularization methods COMPARE method. regularization methods USED-FOR task. method USED-FOR task. Metric is convergence. ,This paper proposes a method for adversarial regularization for image reconstruction. The method is shown to outperform existing regularization methods for this task. The authors also provide a theoretical analysis of the convergence of the system and reconstruction.
2595,SP:590b67b1278267e966cf0b31456d981441e61bb1,"deep learning based reconstruction USED-FOR ill - posed inverse problems. end - to - end supervised training CONJUNCTION learned regularization. learned regularization CONJUNCTION end - to - end supervised training. learned regularization USED-FOR variational setting. end - to - end supervised training USED-FOR deep learning based reconstruction. deep neural network USED-FOR regularization. ground - truth images CONJUNCTION images. images CONJUNCTION ground - truth images. regularizer USED-FOR images. regularizer USED-FOR ground - truth images. unrolled network USED-FOR images. learned primal - dual algorithm USED-FOR unrolled network. end - to - end network USED-FOR reconstruction. learned regularizer USED-FOR variational setting. initialization USED-FOR reconstruction. end - to - end network USED-FOR initialization. learned regularizer USED-FOR reconstruction. end - to - end model USED-FOR fast reconstruction. variational model USED-FOR refinement. approach COMPARE state - of - the - art methods. state - of - the - art methods COMPARE approach. Generic are method, and this. ","This paper proposes a deep learning based reconstruction for ill-posed inverse problems using end-to-end supervised training and learned regularization in a variational setting. The method is based on the idea that the regularization can be learned by a deep neural network, and that the learned regularizer can be applied to both ground-truth images and images generated by an unrolled network trained with a learned primal-dual algorithm. The reconstruction is performed using an end-towards-end network, which is trained to perform initialization and reconstruction with the help of the new learned regularized parameters. The authors claim that this allows for fast reconstruction, and also that the use of the learned variational model can be used for refinement. Experiments show that the proposed approach outperforms the state-of-the-art methods."
2596,SP:590b67b1278267e966cf0b31456d981441e61bb1,framework USED-FOR adversarial network. regularization functional USED-FOR image reconstruction. discriminator HYPONYM-OF regularization functional. framework USED-FOR image reconstruction. unfolding network USED-FOR it. unfolding network USED-FOR latter. variational inference USED-FOR former. forward model PART-OF loss function. forward model PART-OF UAR. Generic is method. OtherScientificTerm is supervision. Metric is optimality. Method is end - to - end learning & regularization. ,"This paper proposes a framework for training an adversarial network using a framework to train an image reconstruction using a regularization functional (i.e., a discriminator). The proposed method, UAR, consists of two parts: (1) the forward model of the loss function and (2) the latter is trained using an unfolding network. The former uses variational inference, while the latter uses the learned parameters of the unfolding network as supervision. Theoretical analysis of the optimality of the proposed method is provided. Empirical results demonstrate the effectiveness of end-to-end learning & regularization."
2597,SP:590b67b1278267e966cf0b31456d981441e61bb1,"iterative deep unfolding neural network ( DUNN ) CONJUNCTION adversarial discriminator. adversarial discriminator CONJUNCTION iterative deep unfolding neural network ( DUNN ). framework USED-FOR iterative deep unfolding neural network ( DUNN ). framework USED-FOR adversarial discriminator. adversarial discriminator USED-FOR image inverse problems. framework USED-FOR image inverse problems. Wasserstein distance USED-FOR adversarial discriminator. AR method COMPARE method. method COMPARE AR method. method USED-FOR reconstruction. DUNN USED-FOR reconstruction. pseudo - inverse reconstruction USED-FOR AR method. unrolling networks USED-FOR adversarial training. LPD CONJUNCTION AR. AR CONJUNCTION LPD. U - Net CONJUNCTION LPD. LPD CONJUNCTION U - Net. method USED-FOR CT inverse problem. method COMPARE supervised and unsupervised deep learning methods. supervised and unsupervised deep learning methods COMPARE method. AR HYPONYM-OF supervised and unsupervised deep learning methods. U - Net HYPONYM-OF supervised and unsupervised deep learning methods. LPD HYPONYM-OF supervised and unsupervised deep learning methods. Method are UAR, adversarial regularization ( AR ) method, and iterative primal - dual algorithm. ","This paper proposes a framework to train an iterative deep unfolding neural network (DUNN) and an adversarial discriminator for image inverse problems using the Wasserstein distance. The proposed method is similar to the adversarial regularization (AR) method, except that the AR method uses pseudo-inverse reconstruction, whereas the proposed method uses DUNN for reconstruction. The main difference between UAR and AR is the use of unrolling networks for adversarial training. The method is applied to the CT inverse problem and compared to supervised and unsupervised deep learning methods such as U-Net, LPD, AR, etc. The authors also propose an iterate primal-dual algorithm."
2598,SP:115d679338ab35829dbc594472d13cc02be5ed4c,"VQA CONJUNCTION national language. national language CONJUNCTION VQA. image - text retrieval CONJUNCTION VQA. VQA CONJUNCTION image - text retrieval. method USED-FOR text - image matching. it USED-FOR language - vision downstream tasks. national language USED-FOR visual reasoning. visual reasoning HYPONYM-OF language - vision downstream tasks. national language HYPONYM-OF language - vision downstream tasks. image - text retrieval HYPONYM-OF language - vision downstream tasks. VQA HYPONYM-OF language - vision downstream tasks. cross - attention layers PART-OF multimodal encoder. representations USED-FOR downstream tasks. them USED-FOR multimodal   decoder. image - text contrastive loss FEATURE-OF image / text embeddings. multi - modal encoder USED-FOR them. momentum model USED-FOR pseudo - target labels. moving average USED-FOR supervision. moving average USED-FOR momentum model. Material is image. OtherScientificTerm are embeddings, mutual information, and image - text pair. Generic are losses, approach, tasks, and downstream task. ","This paper proposes a method for text-image matching, and applies it to language-vision downstream tasks such as image-text retrieval, VQA, and the recently introduced national language for visual reasoning. The key idea is to use two losses: (1) an image-image contrastive loss that penalizes the similarity between the image and text embeddings, and (2) a multi-modal encoder with cross-attention layers, where the mutual information between the input image and the target image is penalized. These two losses are combined to form a multimodal  decoder, which uses them as pseudo-target labels, and uses a momentum model to generate pseudo -target labels based on the moving average of the supervision. The downstream tasks are trained on these representations, and this approach is shown to improve performance on all three tasks."
2599,SP:115d679338ab35829dbc594472d13cc02be5ed4c,image - text contrastive ( ITC ) CONJUNCTION masked language modeling ( MLM ). masked language modeling ( MLM ) CONJUNCTION image - text contrastive ( ITC ). masked language modeling ( MLM ) CONJUNCTION image - text matching ( ITM ) losses. image - text matching ( ITM ) losses CONJUNCTION masked language modeling ( MLM ). momentum distillation USED-FOR SOTA representations. image - text matching ( ITM ) losses USED-FOR SOTA representations. image - text matching ( ITM ) losses CONJUNCTION momentum distillation. momentum distillation CONJUNCTION image - text matching ( ITM ) losses. ALBEF HYPONYM-OF vision - language representation learning framework. masked language modeling ( MLM ) USED-FOR ALBEF. image - text contrastive ( ITC ) USED-FOR ALBEF. image - text matching ( ITM ) losses USED-FOR ALBEF. KL divergence FEATURE-OF predicted probability distribution. predicted probability distribution CONJUNCTION soft pseudo targets. soft pseudo targets CONJUNCTION predicted probability distribution. KL divergence FEATURE-OF soft pseudo targets. predicted probability distribution FEATURE-OF image or text or masked tokens. momentum model USED-FOR soft pseudo targets. visual entailment CONJUNCTION NLVR. NLVR CONJUNCTION visual entailment. NLVR CONJUNCTION VQA. VQA CONJUNCTION NLVR. VQA CONJUNCTION grounding. grounding CONJUNCTION VQA. image - text retrieval CONJUNCTION visual entailment. visual entailment CONJUNCTION image - text retrieval. VQA EVALUATE-FOR pipeline. NLVR EVALUATE-FOR pipeline. grounding EVALUATE-FOR pipeline. visual entailment EVALUATE-FOR pipeline. tasks EVALUATE-FOR pipeline. image - text retrieval EVALUATE-FOR pipeline. Qualitative visualizations EVALUATE-FOR representations. heat maps EVALUATE-FOR representations. Method is ITC and MLM losses. Generic is prior methods. ,"This paper proposes ALBEF, a vision-language representation learning framework that combines image-text contrastive (ITC), masked language modeling (MLM), and momentum distillation to learn SOTA representations. The ITC and MLM losses are designed to minimize the KL divergence between the predicted probability distribution of the image or text or masked tokens and the soft pseudo targets generated by the momentum model. Qualitative visualizations of the learned representations are performed on heat maps and compared to prior methods. The proposed pipeline is evaluated on a variety of tasks, including image-Text retrieval, visual entailment, NLVR, VQA, and grounding."
2600,SP:115d679338ab35829dbc594472d13cc02be5ed4c,momentum distillation USED-FOR vision - and - language pretraining framework. pseudo - targets USED-FOR self - training approach. momentum model USED-FOR self - training approach. momentum model USED-FOR pseudo - targets. contrastive learning USED-FOR representations. cross - modal attention USED-FOR concatenated sequences of image regions. cross - modal attention USED-FOR approaches. cross - modal attention USED-FOR representations. Material is noisy web data. ,This paper proposes a vision-and-language pretraining framework based on momentum distillation. The key idea is to use pseudo-targets generated by the momentum model as a self-training approach. The proposed approaches use cross-modal attention to concatenated sequences of image regions and use contrastive learning to learn representations. Experiments are conducted on noisy web data.
2601,SP:115d679338ab35829dbc594472d13cc02be5ed4c,unimodal encoders USED-FOR image and text representations. Momentum Distillation approach USED-FOR pseudo - targets. supervision USED-FOR pseudo - targets. cross modal attention USED-FOR multimodal encoder. multimodal encoder USED-FOR representations. approach COMPARE SOTA methods. SOTA methods COMPARE approach. downstream V+L tasks EVALUATE-FOR approach. contrastive loss function USED-FOR multi - modal representations. it CONJUNCTION contrastive loss function. contrastive loss function CONJUNCTION it. pre - trained object detector CONJUNCTION high resolution images. high resolution images CONJUNCTION pre - trained object detector. it USED-FOR multi - modal representations. ITC loss CONJUNCTION masked language modeling ( MLM ) loss. masked language modeling ( MLM ) loss CONJUNCTION ITC loss. momentum encoder model USED-FOR weak correlations. pseudo - targets USED-FOR masked language modeling ( MLM ) loss. pseudo - targets USED-FOR ITC loss. weak correlations FEATURE-OF noisy image - text web data. momentum encoder model USED-FOR pseudo - targets. momentum encoder model USED-FOR masked language modeling ( MLM ) loss. reasoning and retrieval tasks EVALUATE-FOR methods. Method is joint vision and language representations. OtherScientificTerm is bounding box annotations. Material is large scale web based image - text datasets. ,"This paper proposes a Momentum Distillation approach to learn pseudo-targets with supervision. The key idea is to use unimodal encoders to jointly learn image and text representations. To learn the representations, the authors propose a multimodal encoding based on cross modal attention. The proposed approach is evaluated on downstream V+L tasks and compared to SOTA methods. The authors also propose to combine the joint vision and language representations by combining it with a contrastive loss function to learn multi-modal representations. Finally, the proposed momentum encoder model is used to generate pseudo-trajectories for ITC loss and masked language modeling (MLM) loss. Experiments are conducted on large scale web based image-text datasets with pre-trained object detector and high resolution images with bounding box annotations. The methods are evaluated on reasoning and retrieval tasks."
2602,SP:115d679338ab35829dbc594472d13cc02be5ed4c,multimodal transformer USED-FOR fusion. vision transformer CONJUNCTION text transformer. text transformer CONJUNCTION vision transformer. moving average “ teacher ” USED-FOR pseudo targets. ITC loss CONJUNCTION moving average “ teacher ”. moving average “ teacher ” CONJUNCTION ITC loss. method COMPARE E2E VLP methods. E2E VLP methods COMPARE method. method COMPARE VLP methods. VLP methods COMPARE method. ALBEF ) COMPARE E2E VLP methods. E2E VLP methods COMPARE ALBEF ). method COMPARE ALBEF ). ALBEF ) COMPARE method. ALBEF ) COMPARE VLP methods. VLP methods COMPARE ALBEF ). object region features USED-FOR VLP methods. ,This paper proposes a multimodal transformer for fusion between vision transformer and text transformer. The ITC loss and the moving average “teacher” are used as pseudo targets. Experiments show that the proposed method outperforms ALBEF and E2E VLP methods. The experiments also show that object region features can be used to improve the performance of the proposed methods.
2603,SP:e5323a171f40c109722a7ea0aebdcd53c151b72d,"error form FEATURE-OF estimator. Method are linear DM method, misspecification of Q - functions, and misspecification of W - functions. OtherScientificTerm is residual functions. ","This paper studies the error of the linear DM method. The authors show that the error form of the estimator depends on the misspecification of Q-functions, which is similar to the case of the residual functions. They also show that this error form can be reduced to zero by a simple fix of the estimate. Finally, the authors provide a theoretical analysis of the effect of the missing Q-function and of the same effect on the estimate of the missculpification of W-function."
2604,SP:e5323a171f40c109722a7ea0aebdcd53c151b72d,"linear function approximation USED-FOR unrealizability setting. linear function approximation USED-FOR off - policy evaluation. nonparametric tile - coding estimators USED-FOR unrealizability though. OtherScientificTerm are linear direct method(DM ), approximation residual, Bellman residual, and residual of marginal density ratio. ","This paper studies the problem of off-policy evaluation with linear function approximation in the unrealizability setting. In particular, the authors propose a linear direct method(DM) to estimate the approximation residual, which is a variant of the Bellman residual. The authors also propose nonparametric tile-coding estimators to address the issue of unrealizabilities though, which are based on the residual of marginal density ratio."
2605,SP:e5323a171f40c109722a7ea0aebdcd53c151b72d,"linear function approximation USED-FOR RL off - policy evaluation. direct method COMPARE FQE. FQE COMPARE direct method. FQE USED-FOR value function. direct method USED-FOR dynamics. unrealizability FEATURE-OF linear approximators. tile - coding USED-FOR methods. Method are linear function approximations, and non - parametric estimation. ","This paper studies the problem of RL off-policy evaluation with linear function approximation. The authors argue that linear function approximations suffer from unrealizability due to non-parametric estimation. To address this issue, the authors propose a direct method to estimate the dynamics instead of FQE for estimating the value function. The proposed methods are based on tile-coding."
2606,SP:e5323a171f40c109722a7ea0aebdcd53c151b72d,linear function approximation USED-FOR discounted reward setting. linear function approximation USED-FOR offline policy evaluation ( OPE ) error. limit of Fitted Q - Evaluation algorithm HYPONYM-OF Linear Direct OPE algorithm. function class CONJUNCTION density ratio. density ratio CONJUNCTION function class. OPE error FEATURE-OF tile - coding estimator. Metric is asymptotic error. ,"This paper studies the offline policy evaluation (OPE) error with linear function approximation in the discounted reward setting. The authors propose a Linear Direct OPE algorithm, which is a variant of the limit of Fitted Q-Evaluation algorithm, and provide an asymptotic error of $O(1/\sqrt{T})$. The authors also propose a tile-coding estimator with OPE error that depends on the function class and density ratio."
2607,SP:e5323a171f40c109722a7ea0aebdcd53c151b72d,"off - policy evaluation error EVALUATE-FOR direct estimator. linear function approximation USED-FOR direct estimator. closed form USED-FOR asymptotic bias term. OtherScientificTerm are unrealizability, feature mappings, and convergence analysis. Generic is method. ","This paper studies the off-policy evaluation error of a direct estimator based on linear function approximation. The main contribution of the paper is to provide a closed form for the asymptotic bias term, which avoids unrealizability. The paper also provides a convergence analysis of the proposed method. Finally, the paper provides a theoretical analysis of feature mappings."
2608,SP:b45f6966fcc07f3a33f70a57e72507b16fc7bb24,"heavy - tailed noise FEATURE-OF problems. Markov's inequality USED-FOR expectation. logarithmic dependence FEATURE-OF tighter bounds. Method is clipped stochastic gradient methods. OtherScientificTerm are sub - Gaussian assumption, Lipschitz functions, Lipschitz gradients, stochastic gradients, bounded variance, and Lipschitz gradient. Task is stochastic optimization. ","This paper studies clipped stochastic gradient methods, where the expectation is given by Markov's inequality. The main contribution of this paper is to provide tighter bounds with logarithmic dependence under the sub-Gaussian assumption. This is an important result, as the Lipschitz functions have been shown to be non-convex, which is important for problems with heavy-tailed noise. In particular, the authors show that for stoched gradients with bounded variance, the expectation can be approximated by Lipsichitz gradients, which can be used to derive tighter bounds. The authors also provide a theoretical analysis of the Lipchitz gradient, and provide an empirical study of the behavior of Lipschy gradients in the context of the problem of learning a function for which the expectation has bounded variance. The paper is well-written and well-motivated, and the results are useful for the community in the area of learning Lipschiq functions for stoachastic optimization."
2609,SP:b45f6966fcc07f3a33f70a57e72507b16fc7bb24,clipped - SSTM USED-FOR stochastic nonsmooth optimization. heavy tailed noise FEATURE-OF stochastic nonsmooth optimization. clipped - SSTM COMPARE SGD methods. SGD methods COMPARE clipped - SSTM. real models ( Bert ) EVALUATE-FOR clipped - SSTM. real models ( Bert ) EVALUATE-FOR SGD methods. Generic is algorithm. ,This paper proposes clipped-SSTM for stochastic nonsmooth optimization with heavy tailed noise. The proposed algorithm is based on the idea of minimizing the number of iterations needed to converge to the optimal solution. Experiments on real models (Bert) show that clipped-SSTM outperforms existing SGD methods.
2610,SP:b45f6966fcc07f3a33f70a57e72507b16fc7bb24,"high - probability convergence bounds USED-FOR machine learning optimisation problems. high - probability bound USED-FOR algorithms. heavy tailed noise FEATURE-OF convex, non - smooth losses. it COMPARE non - heavy tailed noise bound. non - heavy tailed noise bound COMPARE it. Generic is bounds. OtherScientificTerm is confidence bound. ","This paper studies the problem of computing high-probability convergence bounds for machine learning optimisation problems. In particular, the authors consider convex, non-smooth losses with heavy tailed noise and propose two bounds. The first bounds is a confidence bound on the number of iterations needed to converge to the optimal solution. The second bounds is an upper bound of the probability of convergence of algorithms. The authors show that it is better than the non-heavy tailed noisy bound."
2611,SP:b45f6966fcc07f3a33f70a57e72507b16fc7bb24,"Holder continuity condition FEATURE-OF gradient. gradient clippings technique USED-FOR optimizers. condition USED-FOR smooth and non - smooth settings. optimizers USED-FOR smooth problems. optimizers USED-FOR non - smooth setting. convergence rate EVALUATE-FOR optimizers. step - size schemes USED-FOR problem formulation. Task is convex stochastic optimization problem. OtherScientificTerm are stochastic gradient, and smoothness assumption. ",This paper studies a convex stochastic optimization problem where the gradient is assumed to satisfy a Holder continuity condition. This condition can be applied to both smooth and non-smooth settings. The authors propose to use the gradient clippings technique to train optimizers for both smooth problems and for the non -smooth setting. The main contribution of this paper is to prove that the convergence rate of the optimizers in the smooth setting is bounded by a factor of sqrt(1/\sqrt{T}) where T is the number of steps required to converge to a solution of the convex problem. This result is in contrast to previous work that assumes that the stochastically gradient is smooth. The paper also provides a theoretical analysis of the problem formulation using step-size schemes and provides a proof of the smoothness assumption.
2612,SP:b45f6966fcc07f3a33f70a57e72507b16fc7bb24,clipped SGD CONJUNCTION clipped SSTM algorithm. clipped SSTM algorithm CONJUNCTION clipped SGD. clipped SGD USED-FOR generalized smooth objectives. clipped SSTM algorithm USED-FOR generalized smooth objectives. Hölder - continuous gradients FEATURE-OF generalized smooth objectives. stepsize USED-FOR clipped SSTM algorithm. stepsize USED-FOR Hölder - continuity assumption. OtherScientificTerm is sub - Gaussian noise. Metric is convergence rates. ,This paper studies the connection between clipped SGD and the clipped SSTM algorithm for generalized smooth objectives with Hölder-continuous gradients. The main contribution of this paper is to show that the stepsize used in the clipped version of the SSTMs can be used to overcome the Hohl-continuity assumption. The paper also provides a theoretical analysis of the convergence rates under sub-Gaussian noise.
2613,SP:a22a893e25ce739dc757861741014764e78aa820,output length HYPONYM-OF forecast horizon. approach USED-FOR transformer. self - attention unit CONJUNCTION autocorrelation and decomposition units. autocorrelation and decomposition units CONJUNCTION self - attention unit. decomposition block USED-FOR time series. decomposition block USED-FOR seasonal and trend components. average pooling USED-FOR trend component. trend component USED-FOR seasonal. logsparse attention CONJUNCTION LSH. LSH CONJUNCTION logsparse attention. autocorrelation unit COMPARE attention. attention COMPARE autocorrelation unit. attention CONJUNCTION logsparse attention. logsparse attention CONJUNCTION attention. LSH CONJUNCTION probsparse attention. probsparse attention CONJUNCTION LSH. autocorrelation unit COMPARE logsparse attention. logsparse attention COMPARE autocorrelation unit. autocorrelation unit COMPARE probsparse attention. probsparse attention COMPARE autocorrelation unit. autocorrelation unit COMPARE LSH. LSH COMPARE autocorrelation unit. Task is long - horizon time series forecasting. Method is j - item rotated value embedding. ,"This paper addresses the problem of long-horizon time series forecasting, where the forecast horizon (i.e., output length) is a function of the number of items in the dataset. The authors propose an approach to train a transformer that learns a self-attention unit, as well as autocorrelation and decomposition units. The decomposition block for time series consists of seasonal and trend components. The seasonal component is based on average pooling, while the trend component is modeled using a j-item rotated value embedding. Experiments show that the autocorerelation unit outperforms attention, logsparse attention, LSH, and probsparse attention."
2614,SP:a22a893e25ce739dc757861741014764e78aa820,time series USED-FOR long - term forecasting. self - attention mechanism USED-FOR Transformer. entangled temporal dependencies FEATURE-OF long time series. auto - correlation mechanism CONJUNCTION series decomposition module. series decomposition module CONJUNCTION auto - correlation mechanism. series decomposition module USED-FOR Autoformer. auto - correlation mechanism USED-FOR Autoformer. ,This paper studies the problem of long-term forecasting of time series with entangled temporal dependencies. The authors propose a self-attention mechanism similar to Transformer. Autoformer is trained with an auto-correlation mechanism and a series decomposition module.
2615,SP:a22a893e25ce739dc757861741014764e78aa820,Autoformer USED-FOR long - term time series forecasting. auto - correlation mechanism USED-FOR sub - series similarity. OtherScientificTerm is series periodicity. Generic is method. ,This paper proposes Autoformer for long-term time series forecasting. The key idea is to use an auto-correlation mechanism to estimate the sub-series similarity between different time series based on the series periodicity. The proposed method is evaluated on a variety of datasets.
2616,SP:a22a893e25ce739dc757861741014764e78aa820,seasonality CONJUNCTION trend - cycle components. trend - cycle components CONJUNCTION seasonality. trend - cycle components PART-OF deep Transformer architecture. seasonality PART-OF deep Transformer architecture. seasonality PART-OF time series. auto - correlation mechanism USED-FOR self - attention in Transformers. method COMPARE SOTA methods. SOTA methods COMPARE method. Task is long - term forecasting problem of time series. Metric is computation efficiency. OtherScientificTerm is power spectral density. ,"This paper studies the long-term forecasting problem of time series. The authors propose a deep Transformer architecture that incorporates both seasonality and trend-cycle components to improve the computation efficiency. The self-attention in Transformers is based on an auto-correlation mechanism, where the power spectral density of the time series is computed. The proposed method is compared with SOTA methods."
2617,SP:a22a893e25ce739dc757861741014764e78aa820,approach USED-FOR long - term time - series forecasting. decomposition USED-FOR approach. series decomoposition block USED-FOR mean ( trend - cyclical ). auto - correlation component USED-FOR local structures. method COMPARE baseline approaches. baseline approaches COMPARE method. benchmark datasets EVALUATE-FOR baseline approaches. benchmark datasets EVALUATE-FOR method. Generic is it. OtherScientificTerm is seasonal part. ,This paper proposes an approach for long-term time-series forecasting based on decomposition. The main idea is to use a series decomoposition block to decompose the mean (trend-cyclical) and the variance of it into a seasonal part and an auto-correlation component to model the local structures. The method is evaluated on three benchmark datasets and compared to several baseline approaches.
2618,SP:eeb2c3348de291c5eacac5d9de7b6b84ca030ad5,dataset USED-FOR cryptic crosswords. reordering of letters CONJUNCTION extracting letters. extracting letters CONJUNCTION reordering of letters. reordering of letters HYPONYM-OF wordplay. extracting letters HYPONYM-OF wordplay. dataset USED-FOR puzzles. translation CONJUNCTION summarization. summarization CONJUNCTION translation. text - to - text transformer USED-FOR tasks. T5 HYPONYM-OF text - to - text transformer. T5 USED-FOR tasks. dataset EVALUATE-FOR T5. KNN HYPONYM-OF non - neural baselines. translation HYPONYM-OF tasks. summarization HYPONYM-OF tasks. wordplays FEATURE-OF tasks. cryptic crosswords dataset EVALUATE-FOR T5. curriculum learning USED-FOR problem. Method is pretraining strategy. OtherScientificTerm is adversarial splits. Generic is strategy. ,"This paper proposes a new dataset for solving cryptic crosswords. The dataset is designed for solving puzzles involving wordplay (e.g., reordering of letters, extracting letters, etc.). The authors propose a pretraining strategy where adversarial splits are randomly generated. The authors evaluate T5, a text-to-text transformer, on the new dataset on three tasks: translation, summarization, and wordplays. The results show that T5 outperforms other non-neural baselines such as KNN. The paper also presents an ablation study of the proposed strategy. Finally, the authors propose curriculum learning to solve the problem. The experimental results on the cryptic crossword dataset show the effectiveness of T5."
2619,SP:eeb2c3348de291c5eacac5d9de7b6b84ca030ad5,"cryptic crossword clues FEATURE-OF dataset. non - neural baselines CONJUNCTION neural baseline. neural baseline CONJUNCTION non - neural baselines. neural baseline USED-FOR dataset. non - neural baselines USED-FOR dataset. T5 transformer - based model HYPONYM-OF neural baseline. pre - finetuning stage USED-FOR model. model USED-FOR task. pre - finetuning stage USED-FOR T5 model. transformer - based models USED-FOR task. dataset USED-FOR complex reasoning tasks. cryptic crossword clues FEATURE-OF dataset. task EVALUATE-FOR T5 model. OtherScientificTerm are finetuning, and finetuning stage. Generic is baselines. ","This paper presents a new dataset with cryptic crossword clues and a neural baseline (a T5 transformer-based model). The dataset is designed for complex reasoning tasks, and the authors compare the performance of the T5 model on this task using the pre-finetuning stage and the neural baseline. The authors show that the finetuning helps the model to generalize better to new tasks. The paper also shows that the pre -finetune stage can help the model generalize to a new task with transformer -based models. The experiments are conducted on a variety of datasets and baselines. The results show that during the prefinetunating stage, the model is able to generalise better to unseen tasks."
2620,SP:eeb2c3348de291c5eacac5d9de7b6b84ca030ad5,"NLP systems USED-FOR compositional language. cryptic crossword clues USED-FOR NLP systems. dataset USED-FOR NLP systems. cryptic crossword clues FEATURE-OF dataset. curriculum approach USED-FOR synthetic tasks. model USED-FOR synthetic tasks. model PART-OF curriculum approach. augmented word descrambling task HYPONYM-OF synthetic tasks. composition CONJUNCTION generalization. generalization CONJUNCTION composition. word - initial disjoint data split EVALUATE-FOR generalization. word - initial disjoint data split USED-FOR composition. model COMPARE human solving strategies. human solving strategies COMPARE model. Generic is task. Task is Cryptic crossword puzzles task. OtherScientificTerm are cryptic clues, and wordplay part of clues. ","This paper presents a new dataset of cryptic crossword clues to train NLP systems to learn compositional language. The task is called Cryptic crossword puzzles task. The authors propose a curriculum approach to train a model on synthetic tasks (e.g., augmented word descrambling task). The key idea is to split the cryptic clues into wordplay part of clues and word-initial disjoint data split to improve composition and generalization. Experiments show that the proposed model outperforms human solving strategies."
2621,SP:eeb2c3348de291c5eacac5d9de7b6b84ca030ad5,"corpus USED-FOR NLP task. cryptic crossword clues USED-FOR NLP task. corpus USED-FOR cryptic crossword clues. literal ( synonym ) clue CONJUNCTION wordplay. wordplay CONJUNCTION literal ( synonym ) clue. literal ( synonym ) clue PART-OF Cryptic crossword clues. wordplay PART-OF Cryptic crossword clues. system USED-FOR surface, syntactic and semantic content. T5 model USED-FOR cryptic clue solving. Material is Cryptic wordplay. Task is NLP challenge. Method is curriculum training approach. ","This paper presents a corpus of cryptic crossword clues for an NLP task. Cryptic wordplay is a well-known NLP challenge, and the corpus contains both literal (synonym) and wordplay. The authors propose a curriculum training approach, where the system learns surface, syntactic and semantic content for cryptic clue solving using a T5 model."
2622,SP:eeb2c3348de291c5eacac5d9de7b6b84ca030ad5,"wordplay cipher PART-OF puzzles. creativity CONJUNCTION linguistic and world knowledge. linguistic and world knowledge CONJUNCTION creativity. linguistic and world knowledge USED-FOR task. creativity USED-FOR task. baselines CONJUNCTION curriculum learning approach. curriculum learning approach CONJUNCTION baselines. NLP systems USED-FOR compositional language. model USED-FOR tasks. cryptic clues USED-FOR NLP systems. dataset EVALUATE-FOR NLP systems. curriculum learning approach USED-FOR tasks. cryptic clues FEATURE-OF dataset. unscrambling words HYPONYM-OF tasks. Material is UK - style cryptic crossword puzzles. OtherScientificTerm are fluent natural language, and definition. Generic is benchmark. ","This paper presents a set of UK-style cryptic crossword puzzles. The puzzles consist of a wordplay cipher, and the goal is to solve the puzzles in a way that can be expressed in fluent natural language. This task is motivated by both creativity and linguistic and world knowledge. The authors evaluate NLP systems on this dataset using both baselines and a curriculum learning approach, and show that the model is able to solve these tasks (e.g., unscrambling words). The authors also present a new benchmark that is based on the definition of cryptic clues."
2623,SP:7693974b70806d9b67920b8ddd2335afc4883319,Vision Transformers USED-FOR vision tasks. scale USED-FOR representations. ViTs CONJUNCTION CNN. CNN CONJUNCTION ViTs. internal representation structure FEATURE-OF CNN. internal representation structure FEATURE-OF ViTs. Centered Kernel Alignment HYPONYM-OF representational similarity techniques. representational similarity techniques USED-FOR CNN. representational similarity techniques USED-FOR internal representation structure. Generic is they. Method is convolutions. OtherScientificTerm is inductive biases. ,"Vision Transformers are widely used for vision tasks, and they have been shown to be able to generalize well to unseen tasks. However, their internal representation structure is not well-studied. This paper proposes to learn representations at scale, which is an important aspect of ViTs and CNN. The authors propose to use representational similarity techniques, such as Centered Kernel Alignment, to learn the inner representation structure of CNN, and show that they can generalize better than ViTs. They also show that convolutions can be used to learn inductive biases."
2624,SP:7693974b70806d9b67920b8ddd2335afc4883319,convnet CONJUNCTION transformers. transformers CONJUNCTION convnet. transformers USED-FOR representation of features. convnet USED-FOR representation of features. datasets USED-FOR models. Method is Centered Kernel Alignment. ,"This paper proposes Centered Kernel Alignment, which is an extension of convnet and transformers to learn the representation of features using convnet. The authors show that the proposed models can be trained on datasets with different sizes and different datasets. "
2625,SP:7693974b70806d9b67920b8ddd2335afc4883319,Neural network architectures COMPARE CNNs. CNNs COMPARE Neural network architectures. ImageNet classification EVALUATE-FOR CNNs. self - attention COMPARE convolutions. convolutions COMPARE self - attention. Vision Transformer CONJUNCTION ViT. ViT CONJUNCTION Vision Transformer. Vision Transformer HYPONYM-OF Neural network architectures. ViT HYPONYM-OF Neural network architectures. ImageNet classification EVALUATE-FOR Neural network architectures. self - attention USED-FOR Neural network architectures. internal representation structure COMPARE CNNs. CNNs COMPARE internal representation structure. Vision Transformers COMPARE CNNs. CNNs COMPARE Vision Transformers. Vision Transformers USED-FOR internal representation structure. ViT - B/16 CONJUNCTION ViT - L/16. ViT - L/16 CONJUNCTION ViT - B/16. ViT - B/32 CONJUNCTION ViT - B/16. ViT - B/16 CONJUNCTION ViT - B/32. Vision Transformer variants CONJUNCTION ResNet variants. ResNet variants CONJUNCTION Vision Transformer variants. Centered Kernel Alignment USED-FOR internal representations. ViT - L/16 HYPONYM-OF Vision Transformer variants. ResNet50x1 HYPONYM-OF ResNet variants. ViT - B/32 HYPONYM-OF Vision Transformer variants. ViT - B/16 HYPONYM-OF Vision Transformer variants. skip connections CONJUNCTION spatial localization of information. spatial localization of information CONJUNCTION skip connections. local vs. global information aggregation FEATURE-OF similarity of representations. ViTs CONJUNCTION ResNets. ResNets CONJUNCTION ViTs. image processing strategies USED-FOR model families. Task is classification. ,"Neural network architectures such as Vision Transformer, ViT, and ResNet variants such as ResNet50x1 have been shown to outperform CNNs on ImageNet classification with self-attention instead of convolutions. However, the internal representation structure of Vision Transformers (ViT-B/16 and ViT-L/L/16) and CNNs (ViTs and ResNets) is not compared to CNNs. This paper proposes to use Centered Kernel Alignment to learn the internal representations of all three variants of the Vision Transformers. The authors propose to use skip connections, spatial localization of information, and global information aggregation to improve the similarity of representations (local vs. global) for classification. Finally, the authors propose several image processing strategies to further improve the performance of the proposed model families."
2626,SP:7693974b70806d9b67920b8ddd2335afc4883319,representation propagation analysis CONJUNCTION spatial localization analysis. spatial localization analysis CONJUNCTION representation propagation analysis. attention distance analysis CONJUNCTION effective receptive fields. effective receptive fields CONJUNCTION attention distance analysis. spatial localization analysis CONJUNCTION linear probes. linear probes CONJUNCTION spatial localization analysis. effective receptive fields CONJUNCTION representation propagation analysis. representation propagation analysis CONJUNCTION effective receptive fields. Vision Transformers CONJUNCTION ResNet networks. ResNet networks CONJUNCTION Vision Transformers. Centered kernel alignment CONJUNCTION attention distance analysis. attention distance analysis CONJUNCTION Centered kernel alignment. internal representations USED-FOR image classification. ResNet networks USED-FOR image classification. internal representations USED-FOR Vision Transformers. internal representations USED-FOR ResNet networks. Centered kernel alignment USED-FOR internal representations. attention distance analysis USED-FOR internal representations. tools USED-FOR ResNet networks. tools USED-FOR internal representations. Centered kernel alignment HYPONYM-OF tools. representation propagation analysis HYPONYM-OF tools. spatial localization analysis HYPONYM-OF tools. attention distance analysis HYPONYM-OF tools. effective receptive fields HYPONYM-OF tools. Generic is models. ,"This paper proposes three tools: Centered kernel alignment, attention distance analysis, effective receptive fields, and spatial localization analysis, as well as linear probes, to improve the internal representations of Vision Transformers and ResNet networks for image classification. The authors show that these three tools can improve internal representations for Vision Transformers, ResNet, and other models. "
2627,SP:7693974b70806d9b67920b8ddd2335afc4883319,"representations COMPARE those. those COMPARE representations. those COMPARE convolutional neural networks ( resnet ). convolutional neural networks ( resnet ) COMPARE those. convolutional neural networks ( resnet ) USED-FOR representations. vision transformers ( ViT ) USED-FOR representations. centered kernel alignment ( CKA ) USED-FOR representations. ViT USED-FOR representations. resnets USED-FOR lower - level and higher - level representations. residual connections USED-FOR transformers. ViT CONJUNCTION ResNets. ResNets CONJUNCTION ViT. ResNets USED-FOR spatial information. ViT USED-FOR spatial information. ViT COMPARE ResNets. ResNets COMPARE ViT. global information USED-FOR ViT. ViT USED-FOR inductive bias. local processing PART-OF convolutions. ViT COMPARE ResNet. ResNet COMPARE ViT. MLP Mixer COMPARE ResNet. ResNet COMPARE MLP Mixer. MLP Mixer COMPARE ViT. ViT COMPARE MLP Mixer. OtherScientificTerm are spatial positions, residual connection, and local information. Generic are model, and it. ","This paper proposes vision transformers (ViT), which learn representations similar to those learned by convolutional neural networks (resnet), but with centered kernel alignment (CKA). The authors argue that resnets learn both lower-level and higher-level representations, and that ViT learns representations that capture spatial positions. The authors show that transformers can be trained with residual connections, and show that the residual connection can be used to encode local information. They compare ViT and ResNets to show that ViTs can capture spatial information better than ResNet. They also show that local processing in convolutions can be incorporated into ViT to improve inductive bias. Finally, they compare MLP Mixer and ViT with ResNet, showing that the model can generalize better."
2628,SP:dfd740399e48b946f02efdec823b8975a900f6a3,"Thompson Sampling USED-FOR combinatorial semi - bandit. greedy approximation oracles USED-FOR Thompson Sampling. suboptimality of greedy oracles USED-FOR Thompson sampling. OtherScientificTerm are greedy regret, and almost matching upper bound. ",This paper studies Thompson Sampling in combinatorial semi-bandit with greedy approximation oracles. The authors show that Thompson sampling with suboptimality of greedy oracles converges to a greedy regret with an almost matching upper bound.
2629,SP:dfd740399e48b946f02efdec823b8975a900f6a3,"linear regret FEATURE-OF combinatorial TS ( CTS ) algorithm. approximation oracle USED-FOR combinatorial maximization. combinatorial maximization oracles USED-FOR CTS. greedy oracles USED-FOR CTS. greedy oracles HYPONYM-OF combinatorial maximization oracles. greedy regret FEATURE-OF CTS. matching lower bound USED-FOR CTS. matching lower bound USED-FOR small - scale problem. small - scale problem EVALUATE-FOR CTS. Method is Thompson - Sampling ( TS ). OtherScientificTerm are semi - bandit feedback, combinatorial problem, regret upper bounds, and suboptimality gap. ","This paper studies the linear regret of the combinatorial TS (CTS) algorithm, which is an extension of Thompson-Sampling (TS). The main idea is to use semi-bandit feedback, where an approximation oracle is used to approximate the true objective of combinatorical maximization. The authors show that CTS with greedy oracles can achieve a greedy regret of $O(1/\sqrt{T})$. The authors also provide a matching lower bound for CTS on a small-scale problem, which shows that the regret upper bounds do not depend on the suboptimality gap."
2630,SP:dfd740399e48b946f02efdec823b8975a900f6a3,"Combinatorial Thompson sampling ( CTS ) HYPONYM-OF Thompson sampling algorithm. Task is offline optimization problem. OtherScientificTerm is computational hardness. Generic is framework. Method are CTS, and almost matching upper bound. ",This paper studies an offline optimization problem where the computational hardness is not known. The authors propose a new Thompson sampling algorithm called Combinatorial Thompson sampling (CTS). The authors provide a theoretical analysis of the proposed framework and provide an almost matching upper bound. The experimental results show the effectiveness of CTS.
2631,SP:dfd740399e48b946f02efdec823b8975a900f6a3,greedy oracle USED-FOR Thompson sampling ( TS ). Gaussian priors USED-FOR TS. Beta priors USED-FOR TS. regret EVALUATE-FOR algorithm. algorithm USED-FOR multi - armed bandit problem. near - optimal regret FEATURE-OF multi - armed bandit problem. near - optimal regret FEATURE-OF algorithm. Task is combinatorial semi - bandit problem. ,"This paper studies the problem of Thompson sampling (TS) with a greedy oracle. In particular, the authors consider TS with Gaussian priors. Beta priors are used for TS. The regret of the proposed algorithm is shown to have near-optimal regret for a multi-armed bandit problem, which is a combinatorial semi-bandit problem."
2632,SP:dfd740399e48b946f02efdec823b8975a900f6a3,"Greedy oracle USED-FOR cumulative regret. Greedy oracle USED-FOR TS algorithm. lower bound USED-FOR approach. lower bound FEATURE-OF cumulative regret. probabilistic maximum coverage ( PMC ) CONJUNCTION online influence maximization ( OIM ). online influence maximization ( OIM ) CONJUNCTION probabilistic maximum coverage ( PMC ). probabilistic maximum coverage ( PMC ) PART-OF CMAB. TS based regret - minimisation algorithms USED-FOR single parameter reward distribution. CMAB setup USED-FOR TS. PMC CONJUNCTION OIM. OIM CONJUNCTION PMC. PMC HYPONYM-OF problems. OIM HYPONYM-OF problems. Greedy oracle USED-FOR optimisation methods. submodular maximisation HYPONYM-OF optimisation methods. TS USED-FOR CMAB setup. Greedy oracle USED-FOR TS. Greedy oracle USED-FOR CMAB setup. OtherScientificTerm are upper bound, and minimum spanning tree ( MST ). Task are MAB literature, and offline learning. ","This paper proposes a new TS algorithm based on a Greedy oracle to reduce the cumulative regret. The authors provide a lower bound for their approach and provide an upper bound. The main contribution of this paper is to provide a theoretical analysis of TS based regret-minimisation algorithms for a single parameter reward distribution. This is an important topic in the MAB literature, as TS has been extensively studied in the CMAB setup, which includes probabilistic maximum coverage (PMC) and online influence maximization (OIM). The authors show that TS with a modified version of TS with Greedy/acle can achieve better performance than other optimisation methods, such as submodular maximisation. The paper also provides an analysis of the minimum spanning tree (MST), which is a popular tool in offline learning."
2633,SP:3ca7fdaba9793a61a1f9d264a551fe895e55dd99,"Method are federated learning, and learning platform. Generic are model, and algorithm. ","This paper studies the problem of federated learning, where the goal is to train a model that is robust to changes in the number of clients. The authors propose a learning platform where each client has access to a small subset of the clients' data, and each client is able to update its own model according to the changes in their data. The algorithm is based on the idea that each client can update its model only if the other clients update their data as well. "
2634,SP:3ca7fdaba9793a61a1f9d264a551fe895e55dd99,coalition formation process USED-FOR federated learning. Generic is algorithm. Metric is total weighted error. OtherScientificTerm is price of anarchy. ,"This paper studies the coalition formation process in federated learning. The authors propose an algorithm that is able to minimize the total weighted error, which they call the price of anarchy. "
2635,SP:3ca7fdaba9793a61a1f9d264a551fe895e55dd99,"price of anarchy ( PoA ) FEATURE-OF federated learning. game theoretic model USED-FOR federated learning. average error rates USED-FOR notion of optimality. algorithm USED-FOR optimal solution. OtherScientificTerm are optimality, and PoA bounds. ",This paper studies the problem of federated learning with the price of anarchy (PoA) in a game theoretic model. The authors propose a notion of optimality based on the average error rates and show that the optimality can be expressed as a function of the number of players. They then propose an algorithm for finding the optimal solution and provide PoA bounds.
2636,SP:3ca7fdaba9793a61a1f9d264a551fe895e55dd99,"optimality CONJUNCTION maximizing social welfare. maximizing social welfare CONJUNCTION optimality. Task is federated learning problem. Method are global model, and Price of Anarchy analysis. ","This paper studies the federated learning problem, where each client has access to a global model and the goal is to achieve both optimality and maximizing social welfare. The authors provide a Price of Anarchy analysis and provide some theoretical results."
2637,SP:3ca7fdaba9793a61a1f9d264a551fe895e55dd99,stable coalition PART-OF federated learning. optimal algorithm USED-FOR weighted sum of errors. Task is mean estimation. OtherScientificTerm is closed - form solution. ,This paper studies the problem of mean estimation in federated learning with a stable coalition. The authors propose an optimal algorithm for estimating the weighted sum of errors. The main contribution of this paper is to provide a closed-form solution.
2638,SP:17088db004fbf4902c5c3d53e387d1b68f4d69a5,"feature invariance CONJUNCTION pose equivariance. pose equivariance CONJUNCTION feature invariance. loss terms USED-FOR feature invariance. loss terms USED-FOR pose equivariance. loss terms USED-FOR decomposition. model USED-FOR canonical pose. model USED-FOR point clouds. ShapeNet USED-FOR model. model USED-FOR pairwise registration. RRI features USED-FOR DeepGMR. features USED-FOR unsupervised clustering / classification. reconstruction CONJUNCTION canonicalization. canonicalization CONJUNCTION reconstruction. loss terms USED-FOR reconstruction. loss terms USED-FOR canonicalization. Method is capsule - based representation of 3D point clouds. Generic is representation. OtherScientificTerm are capsule, SE(3 ) transformations of the point cloud, and relative angle of rotation. Metric is reconstruction quality. ","This paper proposes a capsule-based representation of 3D point clouds. The proposed representation is based on ShapeNet, where the model is trained to reconstruct point clouds from the capsule. The authors propose two loss terms to improve the feature invariance and pose equivariance of the decomposition. The first loss term penalizes the SE(3) transformations of the point cloud, while the second term penalize the relative angle of rotation. The model is used for pairwise registration, and the features are used for unsupervised clustering/classification, similar to DeepGMR with RRI features. Experiments show that the proposed loss terms improve the reconstruction and canonicalization performance, while also improving the reconstruction quality."
2639,SP:17088db004fbf4902c5c3d53e387d1b68f4d69a5,capsule - based network architecture USED-FOR self - supervised representation. 3D point clouds USED-FOR self - supervised representation. canonical transformation USED-FOR alignment. it USED-FOR latent representation. transformation invariance FEATURE-OF object parts ( capsules ). transformation invariance USED-FOR latent representation. it USED-FOR transformation invariance. method USED-FOR 3D point cloud. ( unaligned ) ShapeNet dataset EVALUATE-FOR method. canonicalization accuracy CONJUNCTION stability. stability CONJUNCTION canonicalization accuracy. It COMPARE state - of - the - art methods. state - of - the - art methods COMPARE It. unsupervised classification accuracy EVALUATE-FOR latent representation. reconstruction accuracy CONJUNCTION canonicalization accuracy. canonicalization accuracy CONJUNCTION reconstruction accuracy. unsupervised classification accuracy EVALUATE-FOR It. stability EVALUATE-FOR It. canonicalization accuracy EVALUATE-FOR state - of - the - art methods. canonicalization accuracy EVALUATE-FOR It. reconstruction accuracy EVALUATE-FOR It. reconstruction accuracy EVALUATE-FOR state - of - the - art methods. Method is pre - canonicalized ( aligned ) 3D models. OtherScientificTerm is 3D pose / viewpoint annotations. ,"This paper proposes a capsule-based network architecture for self-supervised representation on 3D point clouds. The key idea is to use pre-canonized (aligned) 3D models and use a canonical transformation for alignment. The method is trained on the (unaligned) ShapeNet dataset and it learns a latent representation that is invariant to transformation invariance to object parts (capsules). It is evaluated on unsupervised classification accuracy, canonicalization accuracy, and stability. It outperforms state-of-the-art methods in terms of reconstruction accuracy, as well as the number of 3D pose/viewpoint annotations."
2640,SP:17088db004fbf4902c5c3d53e387d1b68f4d69a5,self - supervised manner USED-FOR Canonical Capsule decomposition of shape point clouds. per - capsule pose CONJUNCTION transformation - invariant descriptor. transformation - invariant descriptor CONJUNCTION per - capsule pose. method USED-FOR per - capsule pose. method USED-FOR transformation - invariant descriptor. method USED-FOR pairwise data. unaligned 3D point cloud dataset USED-FOR method. random SE(3 ) transformations USED-FOR method. random SE(3 ) transformations USED-FOR pairwise data. reconstruction CONJUNCTION canonicalization. canonicalization CONJUNCTION reconstruction. object - centric representation USED-FOR applications. canonicalization CONJUNCTION classification. classification CONJUNCTION canonicalization. capsule decomposition USED-FOR applications. capsule decomposition USED-FOR object - centric representation. reconstruction HYPONYM-OF applications. classification HYPONYM-OF applications. canonicalization HYPONYM-OF applications. ,"This paper proposes Canonical Capsule decomposition of shape point clouds in a self-supervised manner. The method is trained on an unaligned 3D point cloud dataset and uses random SE(3) transformations to obtain per-capsule pose and transformation-invariant descriptor for pairwise data. The capsule decomposition can be used to learn object-centric representation for applications such as reconstruction, canonicalization, and classification."
2641,SP:17088db004fbf4902c5c3d53e387d1b68f4d69a5,"self - supervised capsule architecture USED-FOR 3D point clouds. Canonical Capsules USED-FOR K - part decomposition. K - part decomposition USED-FOR point cloud. canonical capsules USED-FOR invariance / equivariance. canonicalization CONJUNCTION unsupervised classification. unsupervised classification CONJUNCTION canonicalization. method COMPARE state - of - the - art methods. state - of - the - art methods COMPARE method. 3D point cloud reconstruction CONJUNCTION canonicalization. canonicalization CONJUNCTION 3D point cloud reconstruction. canonicalization EVALUATE-FOR method. 3D point cloud reconstruction EVALUATE-FOR state - of - the - art methods. 3D point cloud reconstruction EVALUATE-FOR method. Material are pre - aligned datasets, and semantically aligned dataset. OtherScientificTerm is random rigid transformations. ","This paper proposes a self-supervised capsule architecture for 3D point clouds. Canonical Capsules are used to perform a K-part decomposition of a point cloud. The canonical capsules are designed to ensure invariance/equivariance to random rigid transformations. The method is evaluated on pre-aligned datasets and on a semantically aligned dataset. The proposed method outperforms state-of-the-art methods in terms of performance on the task of 3D Point cloud reconstruction, canonicalization, and unsupervised classification."
2642,SP:17088db004fbf4902c5c3d53e387d1b68f4d69a5,"self - supervised capsule based architecture USED-FOR 3D point cloud. canonicalization operation USED-FOR unbiased object - centric reasoning. known relative transformation USED-FOR equivariance. equivariance USED-FOR capsule pose equivariance. unsupervised classification CONJUNCTION canonicalization. canonicalization CONJUNCTION unsupervised classification. 3D autoencoding and reconstruction CONJUNCTION unsupervised classification. unsupervised classification CONJUNCTION 3D autoencoding and reconstruction. ShapeNet dataset EVALUATE-FOR unsupervised classification. ShapeNet dataset EVALUATE-FOR canonicalization. unsupervised classification EVALUATE-FOR framework. canonicalization EVALUATE-FOR framework. 3D autoencoding and reconstruction EVALUATE-FOR framework. ShapeNet dataset EVALUATE-FOR framework. OtherScientificTerm are canonical capsules, and capsule descriptor invariance. Generic is system. ","This paper proposes a self-supervised capsule based architecture for 3D point cloud. The canonicalization operation is used for unbiased object-centric reasoning. The capsule pose equivariance is achieved by equivariant to the known relative transformation between the canonical capsules and the ground truth capsules. The proposed system is evaluated on 3D autoencoding and reconstruction, unsupervised classification, canonicalization on the ShapeNet dataset, and capsule descriptor invariance."
2643,SP:34cc3466ff7786968f437007b6af7d9ffd4decc7,method USED-FOR conformal prediction intervals. histogram of the conditional distribution USED-FOR method. conditional coverage CONJUNCTION optimal length. optimal length CONJUNCTION conditional coverage. optimal length FEATURE-OF large samples. marginal coverage CONJUNCTION conditional coverage. conditional coverage CONJUNCTION marginal coverage. marginal coverage FEATURE-OF finite samples. conditional coverage FEATURE-OF intervals. marginal coverage FEATURE-OF intervals. optimal length FEATURE-OF intervals. benchmark data sets EVALUATE-FOR methods. OtherScientificTerm is skewness of the data. ,"This paper proposes a method to estimate conformal prediction intervals based on a histogram of the conditional distribution. The intervals are assumed to have marginal coverage for finite samples, conditional coverage for large samples, and optimal length for small samples. The methods are evaluated on three benchmark data sets, and the results show that the proposed methods are able to generalize to new data points without skewness of the data."
2644,SP:34cc3466ff7786968f437007b6af7d9ffd4decc7,conditional density estimate USED-FOR optimal prediction interval. split conformal USED-FOR conformalization technique. conformalization technique USED-FOR non - conformity score. method COMPARE methods. methods COMPARE method. conditional coverage EVALUATE-FOR methods. conditional coverage EVALUATE-FOR method. marginal coverage EVALUATE-FOR method. Method is nonconformity - score. ,"This paper proposes a non-conformity-score based on the split conformal, which is a conformalization technique that uses a conditional density estimate to estimate the optimal prediction interval. The proposed method is evaluated on marginal coverage and conditional coverage and compared to other methods."
2645,SP:34cc3466ff7786968f437007b6af7d9ffd4decc7,"extension USED-FOR conformal prediction. skewed data USED-FOR conformal prediction. methodology USED-FOR confidence sets. prior methods USED-FOR marginal * coverage. prior methods USED-FOR conditional * coverage. marginal * coverage CONJUNCTION conditional * coverage. conditional * coverage CONJUNCTION marginal * coverage. approximate conditional coverage CONJUNCTION asymptotic conditional coverage. asymptotic conditional coverage CONJUNCTION approximate conditional coverage. conformalization strategy USED-FOR approximate conditional coverage. Metric is conditional coverage. Method is Conformal prediction. OtherScientificTerm are response variable, finite samples, and conditional density. ","This paper proposes an extension to conformal prediction in the presence of skewed data. Conformal prediction is based on estimating the conditional coverage of a given response variable. The authors propose a methodology for estimating confidence sets for finite samples, where the conditional density is a function of the number of samples. They show that prior methods for estimating marginal* coverage and conditional* coverage do not generalize well. They propose a conformalization strategy to improve approximate conditional coverage and asymptotic conditional coverage."
2646,SP:34cc3466ff7786968f437007b6af7d9ffd4decc7,conformity score USED-FOR average interval lengths. conformity score USED-FOR conditional coverage. conditional histogram USED-FOR approximate oracle intervals. finite marginal coverage CONJUNCTION asymptotic conditional coverage. asymptotic conditional coverage CONJUNCTION finite marginal coverage. asymptotic conditional coverage EVALUATE-FOR method. finite marginal coverage EVALUATE-FOR method. Task is split conformal prediction. Method is conformal histogram regression ( CHR ). Generic is intervals. ,"This paper considers the problem of split conformal prediction. The authors propose conformal histogram regression (CHR), where approximate oracle intervals are derived from the conditional histogram. The conditional coverage is defined as the conformity score between the average interval lengths. The intervals are then used to estimate the marginal and asymptotic conditional coverage of the method. Experiments show that the proposed method can achieve finite marginal coverage as well as asymptonotic conditional Coverage."
2647,SP:34cc3466ff7786968f437007b6af7d9ffd4decc7,conformal score USED-FOR prediction intervals. method USED-FOR conditional distribution. conformal score USED-FOR method. method COMPARE quantile - based conformal methods. quantile - based conformal methods COMPARE method. OtherScientificTerm is features. ,This paper proposes a method to estimate the conditional distribution of a set of features using a conformal score that is used to estimate prediction intervals. The proposed method is compared to other quantile-based conformal methods.
2648,SP:d39075aff611dd54574e7ee1a1aeacce83fdf532,"invariant and null - orbit subspaces PART-OF decomposition of L^2. Task is kernel setting. Method is KRR. OtherScientificTerm are invariant subspace, and generalisation bounds. ","This paper studies the decomposition of L^2 into invariant and null-observable subspaces in the kernel setting. In particular, the authors consider KRR, where the invariant subspace is defined as a function of the number of samples. The authors provide generalisation bounds for this setting."
2649,SP:d39075aff611dd54574e7ee1a1aeacce83fdf532,invariance FEATURE-OF group action. group action USED-FOR avering. kernel ridge regression USED-FOR statistical model. invariance FEATURE-OF RKHSs. Metric is generalization benefit. Method is orbit - averaging. OtherScientificTerm is kernel. ,"This paper studies the invariance of the group action in avering. The authors propose a statistical model based on kernel ridge regression and show that the generalization benefit of the RKHSs is proportional to the number of iterations of orbit-averaging. In addition, the authors show that when the kernel is large enough, the invariancy of the kernel can be improved."
2650,SP:d39075aff611dd54574e7ee1a1aeacce83fdf532,"incorporating invariances USED-FOR kernel methods. approach USED-FOR invariance. orbit averaging USED-FOR invariance. sample complexity EVALUATE-FOR invariance. orbit averaging HYPONYM-OF linear operator. linear operator USED-FOR subspace of the original RKHS. sub - space restricted inner products USED-FOR low - rank approximations. Method are kernel ridge regression, invariance principle, kernel ridge regression learning, and orthogonal decomposition. Metric is excess risk. OtherScientificTerm is kernel ridge regression hypothesis. ","This paper studies the problem of incorporating invariances into kernel methods. In particular, the authors consider kernel ridge regression, where the invariance principle of the original RKHS is violated. The authors propose an approach to improve invariance in terms of sample complexity by using orbit averaging, a linear operator that maps the subspace of a kernel to a subspace corresponding to an orthogonal decomposition. They also propose low-rank approximations based on sub-space restricted inner products, which are shown to reduce the excess risk. Finally, they provide a theoretical analysis of the kernel ridge ridge regression hypothesis and provide a proof of the existence of the invariancy principle. "
2651,SP:d39075aff611dd54574e7ee1a1aeacce83fdf532,invariance PART-OF kernel. invariance USED-FOR theoretical generalization gap. invariant transformations USED-FOR operator. invariant transformations USED-FOR orbit - averaged functional. functional USED-FOR projection operator. orthogonal functions USED-FOR invariant functions. functional USED-FOR orthogonal functions. effective dimensionality FEATURE-OF orthogonal functions. generalization gap USED-FOR kernel ridge regression. effective dimensionality USED-FOR generalization gap. ,"This paper studies the theoretical generalization gap of kernel ridge regression with respect to invariance in the kernel. The authors propose an orbit-averaged functional based on invariant transformations of the operator, which is an extension of the orbit-approximated functional. The functional is used as a projection operator, and the authors show that orthogonal functions of the functional can be expressed as invariant functions with respect the effective dimensionality of the function. The paper also studies the generalization of the kernel ridge ridge regression in terms of effective dimensionalities."
2652,SP:d39075aff611dd54574e7ee1a1aeacce83fdf532,"orbit - averaging USED-FOR kernel ridge regression. d - dimensions FEATURE-OF unit sphere. Metric is expected risk. OtherScientificTerm are group action, and kernel. ",This paper studies the problem of kernel ridge regression with orbit-averaging. The authors consider the case where the unit sphere is of d-dimensions and the expected risk depends on the group action. The main contribution of this paper is to show that the kernel can be approximated in a way that minimizes the risk.
2653,SP:97fac361b69ed5871a60dc40e51900747a453df9,"approach USED-FOR prediction network. generative model USED-FOR neural network activations. prediction network CONJUNCTION generative model. generative model CONJUNCTION prediction network. pre - trained networks USED-FOR network. image benchmarks CONJUNCTION speech benchmarks. speech benchmarks CONJUNCTION image benchmarks. speech benchmarks EVALUATE-FOR paper. image benchmarks EVALUATE-FOR paper. OtherScientificTerm are activation, and intermediate layers. ","This paper proposes an approach to jointly train a prediction network and a generative model for neural network activations. The network is trained using pre-trained networks, where the activation is sampled from intermediate layers. The paper is evaluated on image benchmarks and speech benchmarks."
2654,SP:97fac361b69ed5871a60dc40e51900747a453df9,"generative model USED-FOR activations. decoded activations USED-FOR DecNN model. DecNN model COMPARE model. model COMPARE DecNN model. itself USED-FOR DecNN model. data augmentation USED-FOR This. ensemble network USED-FOR model. ReDecNN USED-FOR measuring uncertainty. OOD detection CONJUNCTION calibration. calibration CONJUNCTION OOD detection. measuring uncertainty CONJUNCTION OOD detection. OOD detection CONJUNCTION measuring uncertainty. ReDecNN USED-FOR OOD detection. ReDecNN USED-FOR calibration. assertion - like capability FEATURE-OF neural network. Generic are approach, and It. Task are Probing, Data Augmentation, and prediction task. Method are Composing networks, and Ensemble network. OtherScientificTerm are features, and decodable activations. ","This paper proposes an approach called ReDecNN for Probing. This is an extension of data augmentation, where a generative model is used to generate activations and then a DecNN model is trained on the decoded activations. This approach is called ""Data Augmentation"". It is similar to Composing networks, but instead of training a single model, an ensemble network is used, where the model itself is trained to be similar to the original model. The idea is to use the ensemble network as a way to augment the original DecNN with features that are not present in the original decodable activations (e.g. in the prediction task). Ensemble network is trained in a way that allows the model to generalize to unseen activations, which is an interesting idea. The authors also propose ReDecnn for measuring uncertainty, OOD detection, and calibration. The main contribution of this paper is that it introduces an assertion-like capability of the neural network."
2655,SP:97fac361b69ed5871a60dc40e51900747a453df9,"8 - layer MLP USED-FOR model. MLP activation USED-FOR generative model. ReDecNN USED-FOR DecNN. calibration CONJUNCTION fairness. fairness CONJUNCTION calibration. out - of - distribution detection CONJUNCTION calibration. calibration CONJUNCTION out - of - distribution detection. OtherScientificTerm are intermediate neural network activations, model input space, and intermediate layers. Task is training. Method is classifier. ","This paper proposes a generative model that uses an 8-layer MLP as an intermediate neural network activations. The model is trained using ReDecNN, where each layer is an MLP activation and the final model is a DecNN trained on top of the 8 layer MLP. During training, the intermediate layers are sampled from the model input space, and a classifier is trained to predict the output of the classifier. The paper studies out-of-distribution detection, calibration, fairness, etc."
2656,SP:97fac361b69ed5871a60dc40e51900747a453df9,"robustness CONJUNCTION interpretability. interpretability CONJUNCTION robustness. intermediate latent representations USED-FOR convolutional classifiers. model USED-FOR invertible intermediate hidden representations. reconstruction loss PART-OF overall loss calculation. regular MC Dropout USED-FOR uncertainty computations. constraints FEATURE-OF intermediate hidden layers. OtherScientificTerm are regularizer, intermediate hiddens, reconstructions, recursive depth, and hidden layers. Method are Monte Carlo Dropout, and classifier. ","This paper studies the problem of robustness and interpretability of convolutional classifiers with intermediate latent representations. The authors propose a model that learns invertible intermediate hidden representations, where the reconstruction loss is incorporated into the overall loss calculation. The main idea is to add a regularizer that encourages the intermediate hiddens to be close to the original reconstructions. This regularizer is called Monte Carlo Dropout, and the uncertainty computations are performed using regular MC Dropout. In addition, the authors propose to add recursive depth to the hidden layers to encourage the classifier to be invariant to constraints on the intermediate hidden layers."
2657,SP:97fac361b69ed5871a60dc40e51900747a453df9,generative model probes CONJUNCTION neural architecture. neural architecture CONJUNCTION generative model probes. neural architecture USED-FOR model. intermediate layers FEATURE-OF model. sensitive information USED-FOR model. self - composition USED-FOR ensemble. out - of - distribution detection CONJUNCTION calibration. calibration CONJUNCTION out - of - distribution detection. uncertainty measurements USED-FOR out - of - distribution detection. uncertainty measurements USED-FOR calibration. tree of connected classifiers USED-FOR ensemble. OtherScientificTerm is intermediate activation. Method is pretrained image models. ,"This paper proposes to use generative model probes and a neural architecture to train a model with intermediate layers. The idea is to use self-composition to train an ensemble with a tree of connected classifiers, where each classifier is responsible for an intermediate activation. This allows the model to learn sensitive information without the need of pretrained image models. Experiments on out-of-distribution detection and calibration are conducted using uncertainty measurements."
2658,SP:3f10ca1e7f8fef6cb0c5957ec2f0689fb9bed753,"Wasserstein barycenter estimation CONJUNCTION Nonparametric independence testing. Nonparametric independence testing CONJUNCTION Wasserstein barycenter estimation. Method is plug in "" barycentric mapping "" estimator. Metric is rates of convergence. OtherScientificTerm is discrete - discrete of semi - discrete. ","This paper proposes a plug in ""barycentric mapping"" estimator. The authors combine Wasserstein barycenter estimation with Nonparametric independence testing to improve rates of convergence. In particular, the authors consider the case where the discrete-discrete of semi-divergent is non-convex."
2659,SP:3f10ca1e7f8fef6cb0c5957ec2f0689fb9bed753,statistical behavior FEATURE-OF plug - in estimators. barycentric projections USED-FOR plug - in estimators. transport cost CONJUNCTION transport map. transport map CONJUNCTION transport cost. rate of convergence FEATURE-OF transport cost. rate of convergence FEATURE-OF transport map. curse of dimensionality FEATURE-OF plug - in estimator. smoothness of the densities USED-FOR rate of convergence. Method is kernel smoothed plug - in estimators. ,"This paper studies the statistical behavior of plug-in estimators based on barycentric projections. The authors show that the rate of convergence of the transport cost and the transport map depends on the smoothness of the densities, and that the curse of dimensionality of the plug-invariant estimator can be alleviated. The paper also provides a theoretical analysis of kernel smoothed plug-ins estimators."
2660,SP:3f10ca1e7f8fef6cb0c5957ec2f0689fb9bed753,absolutely continuous Lebesgue measures FEATURE-OF Monge map. Kantorovich barycentric map approximator USED-FOR empirical Monge maps. rates of convergence FEATURE-OF barycentric map. empirical plug - in estimators USED-FOR rates of convergence. empirical plug - in estimators USED-FOR barycentric map. Kernel density estimator USED-FOR smoothened distributions. statistical accuracy CONJUNCTION computational cost. computational cost CONJUNCTION statistical accuracy. OtherScientificTerm is Gaussian distributions. Metric is rate. ,"This paper studies absolutely continuous Lebesgue measures of the Monge map. The authors propose a Kantorovich barycentric map approximator for empirical Monge maps, which is a generalization of the Kantorovic barycentrism of Gaussian distributions. The rates of convergence of the barycentered map can be derived using empirical plug-in estimators. The rate of convergence is shown to be linear in the number of samples. Kernel density estimator for smoothened distributions is also proposed. The statistical accuracy and computational cost are discussed."
2661,SP:3f10ca1e7f8fef6cb0c5957ec2f0689fb9bed753,"Barycentric projection USED-FOR plug - in estimation. estimated marginal distributions USED-FOR OT problem. Barycentric projection USED-FOR estimated marginal distributions. Barycentric projection USED-FOR OT problem. empirical CDF CONJUNCTION kernel density estimator. kernel density estimator CONJUNCTION empirical CDF. plug - in estimators USED-FOR density of. kernel density estimator HYPONYM-OF plug - in estimators. empirical CDF HYPONYM-OF plug - in estimators. OtherScientificTerm are marginal distributions, and Conditional mean. Task is rate of estimation. ","This paper studies the Barycentric projection for plug-in estimation for the OT problem. The main idea is to use the estimated marginal distributions of the original OT problem to estimate the density of the marginal distributions. Conditional mean is then used to compute the rate of estimation. The authors show that the two plug-ins estimators for density of, the empirical CDF and the kernel density estimator, converge to the same solution."
2662,SP:3f10ca1e7f8fef6cb0c5957ec2f0689fb9bed753,IID samples USED-FOR distributions. probability distributions FEATURE-OF optimal transport map. Euclidean space FEATURE-OF probability distributions. dual representation of the 2 - Wasserstein distance USED-FOR upper bound. smoothness assumptions USED-FOR kernel estimates. Wasserstein barycenter estimation CONJUNCTION nonparametric independence testing. nonparametric independence testing CONJUNCTION Wasserstein barycenter estimation. nonparametric independence testing HYPONYM-OF applications. Wasserstein barycenter estimation HYPONYM-OF applications. Generic is upper bounds. OtherScientificTerm is estimation risk. Method is plugging in empirical distributions. ,"This paper studies the problem of estimating IID samples for distributions that are approximated by the optimal transport map over probability distributions in Euclidean space. The authors derive an upper bound on the estimation risk based on the dual representation of the 2-Wasserstein distance. Under smoothness assumptions on the kernel estimates, the authors derive two upper bounds that are consistent with the empirical results. In particular, they show that the upper bounds do not depend on plugging in empirical distributions, which is useful for applications such as Wasserstein barycenter estimation and nonparametric independence testing."
2663,SP:573fbdbe5857c4aace1dfc27e25b8d65a18c9b96,infinitely wide CNNs USED-FOR dataset distillation. infinitely wide CNNs USED-FOR tiny dataset. Fashion - MNIST CONJUNCTION SVHN. SVHN CONJUNCTION Fashion - MNIST. MNIST CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION MNIST. SVHN CONJUNCTION CIFAR-10/100 datasets. CIFAR-10/100 datasets CONJUNCTION SVHN. Generic is approach. Method is Kernel Inducing Points ( KIP ). ,"This paper proposes to use infinitely wide CNNs for dataset distillation. The proposed approach is based on Kernel Inducing Points (KIP) and is evaluated on MNIST, Fashion-MNIST, SVHN, and CIFAR-10/100 datasets."
2664,SP:573fbdbe5857c4aace1dfc27e25b8d65a18c9b96,"Kernel Inducing Points ( KIP ) method USED-FOR dataset distillation. Kernel Inducing Points ( KIP ) method USED-FOR kernel functions. KIP method USED-FOR test residual. support dataset USED-FOR kernel regression model. support dataset USED-FOR dataset. compressed dataset USED-FOR classification. classification CONJUNCTION training. training CONJUNCTION classification. compressed dataset USED-FOR training. kernels USED-FOR infinitely wide neural networks. infinitely wide neural networks PART-OF NTK. kernels USED-FOR KIP algorithm. infinite NTK convolution kernels COMPARE kernels. kernels COMPARE infinite NTK convolution kernels. MNIST CONJUNCTION CIFAR. CIFAR CONJUNCTION MNIST. infinite NTK convolution kernels USED-FOR classification tasks. kernels USED-FOR classification tasks. MNIST HYPONYM-OF classification tasks. CIFAR HYPONYM-OF classification tasks. orchestration system USED-FOR support dataset. GPUs USED-FOR orchestration system. kernel family USED-FOR dataset compression. kernel family USED-FOR compressed dataset. high dimensional statistics tools USED-FOR compressed dataset. Method are kernel computations, infinite - network kernels, and 3 layer convolutional neural networks. Generic are them, they, and kernel. ","This paper proposes a Kernel Inducing Points (KIP) method for kernel functions for dataset distillation. The KIP method first distills the test residual to a support dataset for a kernel regression model. Then, the support dataset is used to compress the original dataset into a compressed dataset for classification and training. The authors show that the KIP algorithm can be applied to infinitely wide neural networks in NTK, and that infinite NTK convolution kernels can be used instead of kernels for classification tasks (MNIST, CIFAR). The authors also show that kernel computations can be decomposed into infinite-network kernels and use them for training. Finally, the authors propose an orchestration system that uses GPUs to compress support dataset and use the compressed dataset to train 3 layer convolutional neural networks. They show that using this kernel family for dataset compression can lead to better performance than using high dimensional statistics tools. "
2665,SP:573fbdbe5857c4aace1dfc27e25b8d65a18c9b96,dataset distillation algorithms USED-FOR infinitely wide neural networks. LS USED-FOR infinitely wide neural networks. LS HYPONYM-OF dataset distillation algorithms. distributed framework USED-FOR distillation. images HYPONYM-OF synthesized data samples. ,"This paper proposes two dataset distillation algorithms, namely LS and LS, to train infinitely wide neural networks. The authors propose a distributed framework for distillation, where the synthesized data samples (e.g. images) are distributed across multiple nodes and the distillation is performed in a distributed manner. The paper is well-written and easy to follow."
2666,SP:573fbdbe5857c4aace1dfc27e25b8d65a18c9b96,KIP and LS methods USED-FOR dataset distillation. Nyugen 2021 USED-FOR dataset distillation. Nyugen 2021 USED-FOR KIP and LS methods. KIP CONJUNCTION LS. LS CONJUNCTION KIP. kernels USED-FOR LS. inf - width network limit USED-FOR KIP. kernel Ridge regression USED-FOR KIP. inf - width network limit USED-FOR LS. kernel Ridge regression USED-FOR LS. KIP CONJUNCTION LS. LS CONJUNCTION KIP. kernel setting CONJUNCTION neural network ( NN ) transfer setting. neural network ( NN ) transfer setting CONJUNCTION kernel setting. method CONJUNCTION theory. theory CONJUNCTION method. Generic is algorithms. OtherScientificTerm is spectral. Material is distilled data. ,This paper proposes KIP and LS methods based on Nyugen 2021 for dataset distillation. The proposed algorithms are based on the kernel Ridge regression of KIP with an inf-width network limit and LS with kernels. The kernel setting is combined with the neural network (NN) transfer setting. The authors show that the spectral of the distilled data is similar to that of the original data. The method and the theory are well-motivated.
2667,SP:573fbdbe5857c4aace1dfc27e25b8d65a18c9b96,dataset distillation USED-FOR deep learning training efficiency. MNIST CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION MNIST. CIFAR10 CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION CIFAR10. Fashion - MNIST CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION Fashion - MNIST. CIFAR-10 CONJUNCTION MNIST. MNIST CONJUNCTION CIFAR-10. CIFAR100 CONJUNCTION SVHN datasets. SVHN datasets CONJUNCTION CIFAR100. algorithm CONJUNCTION methods. methods CONJUNCTION algorithm. KERNEL RIDGE REGRESSION USED-FOR DATASET META - LEARNING. ,"This paper studies the problem of data distillation for improving deep learning training efficiency. The authors conduct experiments on MNIST, CIFAR-10, Fashion-MNIST, MNIST-Fashion, CifAR10-CIFAR100 and SVHN datasets. The results show that the proposed algorithm, DATASET META-LEARNING with KERNEL RIDGE REGRESSION, outperforms existing methods."
2668,SP:9837e0c68887cc1382aefd0ead01f72cde199e0d,"outlier unlabeled data USED-FOR semi - supervised learning. OVA classifier USED-FOR outliers. open - set soft - consistency regularization loss USED-FOR outlier detection. Generic are problem, and method. ",This paper studies the problem of outlier unlabeled data in semi-supervised learning. The authors propose an open-set soft-consistency regularization loss for outlier detection and propose an OVA classifier to detect outliers. Experimental results show the effectiveness of the proposed method.
2669,SP:9837e0c68887cc1382aefd0ead01f72cde199e0d,"OpenMatch USED-FOR open - set semi - supervised learning. FixMatch PART-OF OpenMatch. OVA - classifier USED-FOR consistent anomaly score distribution. self - supervised framework USED-FOR FixMatch. OVA - classifier USED-FOR it. self - supervised framework USED-FOR SOCR. model USED-FOR inliers. model USED-FOR FixMatch. OSSL tasks EVALUATE-FOR FixMatch. accuracy EVALUATE-FOR FixMatch. unlabeled data USED-FOR inliers. Method are outlier detector, and OpenMatch Framework. ","This paper proposes FixMatch, an extension of OpenMatch for open-set semi-supervised learning. In OpenMatch, FixMatch is an extension to the OpenMatch Framework, which is a generalization of the outlier detector. In particular, it uses an OVA-classifier to learn a consistent anomaly score distribution. In addition to SOCR, the authors also propose a new self supervised framework for FixMatch for SOCR. The proposed model is able to detect inliers from unlabeled data. The authors evaluate FixMatch on OSSL tasks and show that FixMatch can achieve better accuracy with fewer parameters."
2670,SP:9837e0c68887cc1382aefd0ead01f72cde199e0d,outliers PART-OF unlabeled data. classifiers USED-FOR unlabeled data. entropy minimization loss CONJUNCTION consistency regularization loss. consistency regularization loss CONJUNCTION entropy minimization loss. labeled data USED-FOR classification loss. unlabeled data USED-FOR OvA classifiers. entropy minimization loss USED-FOR OvA classifiers. consistency regularization loss USED-FOR OvA classifiers. recognition accuracy CONJUNCTION outlier detection accuracy. outlier detection accuracy CONJUNCTION recognition accuracy. ImageNet EVALUATE-FOR approach. CIFAR EVALUATE-FOR approach. approach COMPARE baselines. baselines COMPARE approach. CIFAR CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR. ImageNet EVALUATE-FOR baselines. CIFAR EVALUATE-FOR baselines. outlier detection accuracy EVALUATE-FOR baselines. recognition accuracy EVALUATE-FOR baselines. outlier detection accuracy EVALUATE-FOR approach. recognition accuracy EVALUATE-FOR approach. Task is detecting outliers. Generic is latter. ,"This paper addresses the problem of detecting outliers in unlabeled data. To this end, the authors propose to train OvA classifiers on the original classifiers to detect the unlabelized data. The classification loss is based on the labeled data, with an entropy minimization loss and a consistency regularization loss. The latter is designed to encourage the classifier to be consistent. The proposed approach is evaluated on CIFAR and ImageNet, where the proposed approach outperforms the baselines in terms of recognition accuracy and outlier detection accuracy."
2671,SP:9837e0c68887cc1382aefd0ead01f72cde199e0d,open - set semi - supervised learning setting EVALUATE-FOR model. OOD detector CONJUNCTION FixMatch. FixMatch CONJUNCTION OOD detector. OOD USED-FOR ignore outliers. augmentations USED-FOR unlabeled data. OOD USED-FOR unsupervised method. model USED-FOR open - set semi - supervised learning setting. Task is open - set semi - supervised learning. ,"This paper presents an open-set semi-supervised learning setting where the model is evaluated on a single dataset. The authors propose an unsupervised method that uses OOD to ignore outliers, which is similar to the OOD detector and FixMatch. They also propose augmentations to the unlabeled data. The paper is well-written and well-motivated. However, the paper is not well-structured and the experimental results are not convincing. The proposed model is not suitable for the open-sets semi-Supervised learning."
2672,SP:9837e0c68887cc1382aefd0ead01f72cde199e0d,model USED-FOR open - set semi - supervised learning. it USED-FOR classification. semi - supervised learning model USED-FOR it. one - versus - all classifiers CONJUNCTION FixMatch. FixMatch CONJUNCTION one - versus - all classifiers. FixMatch HYPONYM-OF semi - supervised learning engine. loss terms USED-FOR model. one - versus - all classifiers HYPONYM-OF loss terms. FixMatch HYPONYM-OF loss terms. loss term USED-FOR out - of - distribution score. proposal COMPARE state - of - the - art models. state - of - the - art models COMPARE proposal. Material is unlabeled data. OtherScientificTerm is Out of distribution samples. ,"This paper proposes a model for open-set semi-supervised learning, where the model is trained on unlabeled data, and it is used for classification. The model is based on a recent semimodelized version of a semi-regressive learning model called FixMatch. The proposed model uses two loss terms: one-versus-all classifiers and FixMatch, which is a recent and well-known semi-survised learning engine. Out of distribution samples are added to the training set, and a new loss term is introduced to reduce the out-of-distribution score. The experimental results show that the proposed proposal is able to outperform the state-of -the-art models."
2673,SP:eb86d33d5d47f1cfe2c66ca2c9f852229e32a32f,goal - conditioned RL CONJUNCTION model - based exploration. model - based exploration CONJUNCTION goal - conditioned RL. method USED-FOR goal - conditioned policies. goal - conditioned RL USED-FOR method. model - based exploration USED-FOR method. exploration policy USED-FOR model disagreement. exploration policy CONJUNCTION goal - conditioned policy. goal - conditioned policy CONJUNCTION exploration policy. exploration policy PART-OF model. goal - conditioned policy USED-FOR It. exploration policy USED-FOR It. These USED-FOR replay buffer. Deepmind Control suite CONJUNCTION simulated robotic manipulation. simulated robotic manipulation CONJUNCTION Deepmind Control suite. simulated robotic manipulation HYPONYM-OF benchmark tasks. Deepmind Control suite USED-FOR benchmark tasks. benchmark tasks EVALUATE-FOR approach. simulated robotic manipulation EVALUATE-FOR approach. Deepmind Control suite EVALUATE-FOR approach. Skew - Fit CONJUNCTION DIAYN. DIAYN CONJUNCTION Skew - Fit. DIAYN CONJUNCTION others. others CONJUNCTION DIAYN. others HYPONYM-OF methods. Skew - Fit HYPONYM-OF methods. DIAYN HYPONYM-OF methods. distance functions USED-FOR goal - conditioned RL component. ,"This paper proposes a method to train goal-conditioned policies using goal-constrained RL and model-based exploration. It consists of an exploration policy that aims to mitigate model disagreement and a goal-conditional policy that encourages the exploration policy to explore the entire model. The authors propose to use distance functions as the goal- conditioned RL component and use these as a replay buffer. The proposed approach is evaluated on several benchmark tasks from the Deepmind Control suite and simulated robotic manipulation. Results are compared to other methods such as Skew-Fit, DIAYN, and others."
2674,SP:eb86d33d5d47f1cfe2c66ca2c9f852229e32a32f,"model based exploration CONJUNCTION reachability. reachability CONJUNCTION model based exploration. model free and model based approaches USED-FOR problem. Task is model based RL setting. Method are task agnostic model, and world model. OtherScientificTerm are policies, images, and goal conditional mode. Metric is expected information gain. ","This paper studies the problem of model based RL setting where the goal is to learn a task agnostic model that can be applied to unseen tasks. The authors consider both model free and model based approaches to this problem. The main contribution of this paper is to study the relationship between model based exploration and reachability. In particular, the authors consider the case where the world model does not have access to the policies, but only to the images. In this case, the goal conditional mode can be used to estimate the expected information gain."
2675,SP:eb86d33d5d47f1cfe2c66ca2c9f852229e32a32f,achiever * USED-FOR goal state. * explorer * CONJUNCTION achiever *. achiever * CONJUNCTION * explorer *. agents PART-OF differentiable world model. * explorer * HYPONYM-OF differentiable world model. achiever * HYPONYM-OF agents. * explorer * HYPONYM-OF agents. * achiever * USED-FOR goal - reaching policy. loss function USED-FOR * achiever *. method COMPARE baselines. baselines COMPARE method. benchmarks EVALUATE-FOR method. OtherScientificTerm is uncertain states. ,"This paper proposes a differentiable world model that consists of two agents: *explorer* and *achiever*. The explorer aims to reach a goal state, while the achiever aims to find a goal-reaching policy that maximizes the likelihood of reaching the goal state. The *achievever* is trained with a loss function that encourages the agent to reach uncertain states. The proposed method is evaluated on several benchmarks and compared to several baselines."
2676,SP:eb86d33d5d47f1cfe2c66ca2c9f852229e32a32f,"model - based method USED-FOR universal goal - conditioned policies. model - based method CONJUNCTION ensemble disagreement - based exploration bonus. ensemble disagreement - based exploration bonus CONJUNCTION model - based method. distance metrics USED-FOR goal - conditioned policy. images USED-FOR goal - conditioned policy. Method are world model and policy learning methods, explorer, and world model. Generic are achiever, and approach. ","This paper proposes a model-based method for learning universal goal-conditioned policies with an ensemble disagreement-based exploration bonus, which is an extension of existing world model and policy learning methods. The key idea is to train an explorer that takes as input the current state of the world model, the current goal, and a set of state-of-the-art policies conditioned on that state. The achiever is trained in a supervised fashion, where the explorer is trained to explore the environment in a way that maximizes the distance between the agent’s current state and the goal state. This approach is based on the idea that the agent should be able to explore states that are similar to the current world model. The agent is trained using distance metrics between states and goals, and the agent is rewarded for exploring states that have similar distances to the goal states. The agents are trained using images from the environment."
2677,SP:eb86d33d5d47f1cfe2c66ca2c9f852229e32a32f,framework USED-FOR policies. policies USED-FOR visual goal oriented tasks. framework USED-FOR visual goal oriented tasks. achiever HYPONYM-OF second. second HYPONYM-OF policies. policies PART-OF framework. achiever HYPONYM-OF policies. first HYPONYM-OF policies. second PART-OF framework. explorer HYPONYM-OF policies. model based approach USED-FOR achiever. dynamics model USED-FOR trajectories. trajectories USED-FOR achiever. kitchen manipulation task CONJUNCTION block handling task. block handling task CONJUNCTION kitchen manipulation task. block handling task CONJUNCTION yoga task. yoga task CONJUNCTION block handling task. benchmark EVALUATE-FOR goal oriented polices. yoga task PART-OF benchmark. kitchen manipulation task HYPONYM-OF benchmark. block handling task HYPONYM-OF benchmark. OtherScientificTerm is novelty. ,"This paper proposes a framework for learning policies for visual goal oriented tasks. The framework consists of two policies: the first is an explorer and the second is an achiever. The achiever is trained using a model based approach, where the novelty is learned by generating trajectories using a dynamics model. The goal oriented polices are evaluated on a benchmark consisting of a kitchen manipulation task, block handling task, and a yoga task."
2678,SP:2db4aba9a370df67f786157f18cbaa4167c6a46d,Shapeshifter USED-FOR dense kernels. reshapes CONJUNCTION transposes. transposes CONJUNCTION reshapes. transposes USED-FOR reparameterization. Shapeshifter USED-FOR reparameterization. reshapes USED-FOR reparameterization. transposes PART-OF Shapeshifter. reshapes PART-OF Shapeshifter. OtherScientificTerm is lower - rank matrices. Task is machine translation. ,"This paper proposes Shapeshifter, a method for reparameterizing dense kernels. Shapeshifter combines reshapes and transposes to perform reparamiterization. The main idea is to use lower-rank matrices, which is an important problem in machine translation."
2679,SP:2db4aba9a370df67f786157f18cbaa4167c6a46d,"weight factorization USED-FOR method. NMT tasks EVALUATE-FOR Shapeshifter. parameter efficiency EVALUATE-FOR Shapeshifter. PHM HYPONYM-OF works. Method is low - rank factorization. Generic are matrix, and algorithm. OtherScientificTerm is Kronecker products. ","This paper proposes a method based on weight factorization. The authors propose a low-rank factorization of the matrix, where the matrix is a weighted sum of the Kronecker products of the parameters of the algorithm. The method is based on the idea of Shapeshifter, which is an extension of previous works such as PHM. Experiments on NMT tasks show that Shapeshifter improves parameter efficiency while maintaining good parameter efficiency."
2680,SP:2db4aba9a370df67f786157f18cbaa4167c6a46d,ShapeShifter USED-FOR large language models. approach USED-FOR large language models. approach USED-FOR reducing number of parameters. factorized matrix representations USED-FOR approach. factorized matrix representations USED-FOR reducing number of parameters. Kronecker products COMPARE low - rank factorizations. low - rank factorizations COMPARE Kronecker products. Kronecker products USED-FOR matrix. approach USED-FOR reduction in parameters. decomposition rank FEATURE-OF reduction in parameters. approach USED-FOR non - square matrices. decomposition USED-FOR rank. it USED-FOR square matrices. approach COMPARE large models. large models COMPARE approach. machine translation tasks EVALUATE-FOR approach. approach COMPARE model compression approaches. model compression approaches COMPARE approach. machine translation tasks EVALUATE-FOR model compression approaches. model compression approaches COMPARE large models. large models COMPARE model compression approaches. machine translation tasks EVALUATE-FOR large models. ,"This paper proposes ShapeShifter, an approach for reducing number of parameters in large language models using factorized matrix representations. The key idea is to use Kronecker products instead of low-rank factorizations to approximate the matrix. The proposed approach allows for reduction in parameters in terms of decomposition rank. In particular, the proposed approach can be applied to non-square matrices, and it can be extended to square matrices as well. Experiments on several machine translation tasks show that the approach outperforms other model compression approaches and large models."
2681,SP:2db4aba9a370df67f786157f18cbaa4167c6a46d,accuracy EVALUATE-FOR large language models. factorized representation CONJUNCTION distillation. distillation CONJUNCTION factorized representation. model compression CONJUNCTION factorized representation. factorized representation CONJUNCTION model compression. model compression HYPONYM-OF methods. distillation HYPONYM-OF methods. factorized representation HYPONYM-OF methods. factorized representation of matrices USED-FOR parameter space. Kronecker products USED-FOR factorize parameter space. expressiveness EVALUATE-FOR stacked layers of low rank matrices. accuracy EVALUATE-FOR method. Generic is large models. ,"This paper studies the problem of improving the accuracy of large language models using three methods: model compression, factorized representation, and distillation. First, the authors propose to factorize the parameter space using a factorization of matrices using Kronecker products. Second, they show that stacked layers of low rank matrices can improve the expressiveness of large models. Third, they demonstrate that the proposed method improves the accuracy."
2682,SP:2db4aba9a370df67f786157f18cbaa4167c6a46d,"algorithm USED-FOR Transformer - based models. sums of Kronecker products PART-OF factorization method. factorization method USED-FOR algorithm. English - French CONJUNCTION English - Romanian. English - Romanian CONJUNCTION English - French. English - German CONJUNCTION English - French. English - French CONJUNCTION English - German. transformer - based models USED-FOR translation. factorization USED-FOR stack of matrices. English - German USED-FOR transformer - based models. English - German USED-FOR translation. Method are Shapeshifter ’, and model compression techniques. Metric is expressiveness. OtherScientificTerm is weight matrices. ","This paper proposes a new algorithm for Transformer-based models based on a factorization method consisting of sums of Kronecker products. The authors call this algorithm ‘Shapeshifter’. The idea is to compress the stack of matrices using the factorization and then use model compression techniques to improve the expressiveness of the learned weights. The weight matrices are then used for training the model. Experiments are conducted on translation in English-German, English-French and English-Romanian."
2683,SP:0ff862542ada5b664d615c26e7a4a95b6cbe540e,"tree - structure information PART-OF transformer architecture. leaf - to - leaf "" relative paths CONJUNCTION leaf - to - root "" absolute paths. leaf - to - root "" absolute paths CONJUNCTION leaf - to - leaf "" relative paths. predicting function names HYPONYM-OF extreme code summarization. extreme code summarization EVALUATE-FOR proposal. programming languages EVALUATE-FOR non - transformer and transformer models. absolute paths CONJUNCTION pointer networks. pointer networks CONJUNCTION absolute paths. OtherScientificTerm are tree structures, and query. Method is transformer attention. ","This paper proposes to incorporate tree-structure information into the transformer architecture. The proposal is evaluated on extreme code summarization (e.g., predicting function names), where tree structures are represented as ""relative paths"", ""leaf-to-root"" absolute paths, and ""pointer networks"". The results are compared to non-transformer and transformer models on a variety of programming languages. The results show that transformer attention is effective, and that the query is more interpretable."
2684,SP:0ff862542ada5b664d615c26e7a4a95b6cbe540e,"neural network architecture USED-FOR code. neural network architecture USED-FOR TPTrans ( Tree Path Transformer ). attention calculation USED-FOR Transformer architecture. these PART-OF Transformer's attention module. path embeddings USED-FOR modification. baselines CONJUNCTION variants. variants CONJUNCTION baselines. variants PART-OF TPTrans. code summarization EVALUATE-FOR TPTrans. baselines PART-OF TPTrans. pointer network outputs USED-FOR model. OtherScientificTerm are syntax trees of programs, leaf nodes, relative path embeddings, absolute path embeddings, and relative and absolute paths. Material is programming languages. ","This paper proposes TPTrans (Tree Path Transformer), a neural network architecture for generating code from syntax trees of programs. The main idea is to replace the attention calculation in the standard Transformer architecture with an attention calculation on the path embeddings of the leaf nodes. This modification is based on the fact that the relative path embedding of a program can be replaced by absolute path embeds, and the authors propose to incorporate these into the Transformer's attention module. The model is trained with pointer network outputs. The authors evaluate TPTrans on code summarization, and compare with several baselines and variants of TPTrans. The results show that the model is able to generate code that can be easily summarized in programming languages."
2685,SP:0ff862542ada5b664d615c26e7a4a95b6cbe540e,"positional and relational encoding USED-FOR self attention primitive of Transformers. self attention primitive of Transformers USED-FOR source - code understanding. self attention USED-FOR relative relational information. position USED-FOR self - attention term. edges HYPONYM-OF relative relational information. embedding USED-FOR self attention. RNN USED-FOR TPTrans. TPTrans USED-FOR former. absolute path HYPONYM-OF RNN. TPTrans COMPARE prior work. prior work COMPARE TPTrans. Code Transformer HYPONYM-OF prior work. OtherScientificTerm are graph, code Parse Tree, relative path, parse - tree root, and positional self - attention term. Generic is latter. ","This paper proposes to use both positional and relational encoding to improve the self attention primitive of Transformers for source-code understanding. The former is based on TPTrans, which is an RNN (absolute path) that maps a graph to a code Parse Tree, while the latter is a parse-tree root that maps the embedding to self attention. Specifically, the positional self-attention term takes the position as input, and the relative relational information (e.g., edges) as output. Experiments show that TPTrans outperforms prior work such as Code Transformer."
2686,SP:0ff862542ada5b664d615c26e7a4a95b6cbe540e,"Transformer - based architecture USED-FOR embedding source code snippets. encoding of absolute and relative AST paths PART-OF self - attention. encoding of absolute and relative AST paths PART-OF Transformer - based architecture. approach COMPARE Transformer - based baselines. Transformer - based baselines COMPARE approach. approach COMPARE code2seq. code2seq COMPARE approach. Transformer - based baselines COMPARE code2seq. code2seq COMPARE Transformer - based baselines. path - only source code encoding USED-FOR code2seq. method summarization benchmark EVALUATE-FOR Transformer - based baselines. method summarization benchmark EVALUATE-FOR approach. Method are relative position encoding, position encoding framework, and self - attention computation. OtherScientificTerm is AST paths. Task is method summarization. ","This paper proposes a Transformer-based architecture for embedding source code snippets that combines the encoding of absolute and relative AST paths in self-attention. The relative position encoding is an extension of the position encoding framework, where AST paths are encoded in a way that is similar to the way that AST paths have been encoded in the past. The proposed approach is evaluated on a method summarization benchmark, where the proposed approach outperforms Transformer and other Transformer -based baselines, as well as code2seq, which uses path-only source code encoding. The authors also provide a theoretical analysis of the limitations of the self attention computation, and provide an ablation study of the effect of the relative position encoder and decoder on the performance of the proposed position encoding. Finally, the authors provide an empirical evaluation of the method summary performance of their proposed approach on a different version of a method summary benchmark."
2687,SP:0ff862542ada5b664d615c26e7a4a95b6cbe540e,technique USED-FOR modeling source code. data CONJUNCTION control flow. control flow CONJUNCTION data. ASTs CONJUNCTION data. data CONJUNCTION ASTs. modeling code COMPARE language. language COMPARE modeling code. sequence - based transformer USED-FOR method. graph structure PART-OF sequence - based transformer. attention weights USED-FOR graph structure. model USED-FOR code summarization. Material is source code. Method is GNNs. ,"This paper proposes a technique for modeling source code. The method is based on a sequence-based transformer, which incorporates a graph structure based on attention weights. The motivation is that modeling code is more challenging than modeling language, and that the source code can be represented as a sequence of ASTs, data, and control flow. The model is trained to perform code summarization, which is an important task for GNNs."
2688,SP:727bcd651b11b7d84dd2c2d535cc85402f9117d4,It USED-FOR self - attention mechanism. computational complexity EVALUATE-FOR self - attention mechanism. blocked and axial attention PART-OF It. computational complexity EVALUATE-FOR It. attention mechanism USED-FOR lower resolution blocks. technique USED-FOR condition image generation process. self - modulation USED-FOR condition image generation process. latent code USED-FOR technique. self - modulation HYPONYM-OF technique. latent code USED-FOR condition image generation process. Method is transformer based generative model. Metric is complexity. ,"This paper proposes a transformer based generative model. It combines blocked and axial attention to reduce the computational complexity of the self-attention mechanism. The attention mechanism is trained to focus on lower resolution blocks, which reduces the complexity. The technique, self-modulation, is applied to the condition image generation process with latent code."
2689,SP:727bcd651b11b7d84dd2c2d535cc85402f9117d4,"HiT HYPONYM-OF Transformer - based generator. Transformer - based generator USED-FOR high - resolution image synthesis. GANs USED-FOR HiT. GANs USED-FOR Transformer - based generator. multi - axis blocked self - attention USED-FOR local and global dependencies. full attention COMPARE multi - axis blocked self - attention. multi - axis blocked self - attention COMPARE full attention. cross - attention mechanism USED-FOR noise information. cross - attention mechanism USED-FOR HiT. Generic are generator, and method. ","This paper proposes HiT, a Transformer-based generator for high-resolution image synthesis with GANs. HiT uses a cross-attention mechanism to incorporate noise information into the generator, instead of using full attention as in multi-axis blocked self-attentive to model both local and global dependencies. Experiments show that the proposed method achieves state-of-the-art results."
2690,SP:727bcd651b11b7d84dd2c2d535cc85402f9117d4,Transformer - based generator USED-FOR high - resolution image generation. quadratic complexity FEATURE-OF self - attention operation. quadratic complexity FEATURE-OF Transformer - based generator. multi - axis blocked self - attention USED-FOR low - resolution stage. GANs USED-FOR Transformer - based generator. implicit neural function FEATURE-OF multi - layer perceptrons. multi - layer perceptrons USED-FOR high - resolution stage. Generic is method. ,"This paper proposes a Transformer-based generator for high-resolution image generation with quadratic complexity of the self-attention operation. The proposed method is based on the idea of multi-axis blocked self-Attention in the low-resolution stage, which is used in GANs. The authors also propose to use multi-layer perceptrons with an implicit neural function in the high resolution stage."
2691,SP:727bcd651b11b7d84dd2c2d535cc85402f9117d4,transformer - based generator USED-FOR high - resolution image generation. quadratic scaling problem FEATURE-OF attention operator. multi - axis blocked self - attention FEATURE-OF attention operator. multi - axis blocked self - attention USED-FOR quadratic scaling problem. cross - attention USED-FOR input and intermediate features. Material is ImageNet and FFHQ datasets. ,"This paper proposes a transformer-based generator for high-resolution image generation. The key idea is to solve the quadratic scaling problem of the attention operator with multi-axis blocked self-attention. To achieve this, the authors propose to use the concept of ""intermediate features"", which is based on the idea of cross-attraction between input and intermediate features. Experiments are conducted on ImageNet and FFHQ datasets."
2692,SP:727bcd651b11b7d84dd2c2d535cc85402f9117d4,"larger - scale ( more distant ) communication FEATURE-OF low - resolution deeper layers. MLPs USED-FOR higher - resolution layers. CelebA - HQ CONJUNCTION FFHQ. FFHQ CONJUNCTION CelebA - HQ. face image datasets EVALUATE-FOR attention mechanisms. Method are attention - based GAN generator architecture, and 128 ^ 2 unconditional ImageNet synthesis. OtherScientificTerm are latent code, and scene compositions. Generic is architecture. ","This paper proposes an attention-based GAN generator architecture. The key idea is to use MLPs to generate higher-resolution layers that allow for larger-scale (more distant) communication between low-resolution deeper layers. The authors demonstrate the effectiveness of the proposed architecture on CelebA-HQ and FFHQ, as well as on 128 ^ 2 unconditional ImageNet synthesis. The attention mechanisms are evaluated on two face image datasets, where the latent code is used to generate different scene compositions."
2693,SP:41a6753bc56eb16040600666a859294ae36cfa9c,"graphs FEATURE-OF geodesically convex halfspaces. Task is active learning setting. Generic are algorithm, and it. Metric is query complexity. ","This paper considers the active learning setting, where the goal is to learn a graph of geodesically convex halfspaces on graphs. The authors propose an algorithm that is efficient in terms of query complexity, but it is computationally expensive."
2694,SP:41a6753bc56eb16040600666a859294ae36cfa9c,query complexity EVALUATE-FOR bounds. convex analysis USED-FOR bounds. upper - bounds CONJUNCTION lower bound. lower bound CONJUNCTION upper - bounds. extreme vertices USED-FOR lower bound. bounds USED-FOR querying strategies. greedy and selective sampling approaches USED-FOR querying strategies. ground - truth communities PART-OF real - world graphs. halfspace assumption USED-FOR learning. cut - size USED-FOR bounds. Method is active learning algorithm. ,"This paper studies the query complexity of existing bounds on the number of vertices in a convex analysis. The authors derive upper-bounds and a lower bound for extreme vertices. These bounds can be used to develop new querying strategies based on greedy and selective sampling approaches. The bounds are derived by considering the cut-size of the vertices, and the authors propose an active learning algorithm that makes use of the halfspace assumption in learning. The experiments are conducted on real-world graphs with ground-truth communities."
2695,SP:41a6753bc56eb16040600666a859294ae36cfa9c,"query complexity EVALUATE-FOR geodesically convex halfspaces. graphs FEATURE-OF geodesically convex halfspaces. geodesically convex set FEATURE-OF graph. query complexity EVALUATE-FOR halfspaces. upper and lower bounds FEATURE-OF query complexity. weighted graph FEATURE-OF halfspaces. graph parameters USED-FOR halfspaces. minimum shortest cover CONJUNCTION graph diameter. graph diameter CONJUNCTION minimum shortest cover. graph diameter USED-FOR upper and almost matching lower bound. hull set CONJUNCTION graph diameter. graph diameter CONJUNCTION hull set. graph diameter USED-FOR upper bound. OtherScientificTerm are shortest path, halfspace, convex hull, convex hulls, treewidth, and separation axioms. ","This paper studies the query complexity of geodesically convex halfspaces on graphs. The authors prove upper and lower bounds on the query of such halfspace. The main idea is to consider a graph as a geodesic convex set, where the shortest path is a convex hull and the halfspace is a weighted graph. In this way, the authors show that the halfsaces on the weighted graph can be expressed as a function of the graph parameters. The upper and almost matching lower bound are derived based on the minimum shortest cover and the graph diameter, respectively. In particular, the upper bound depends on the hull set and graph diameter and the lower bound on the treewidth, which is a result of the separation axioms."
2696,SP:41a6753bc56eb16040600666a859294ae36cfa9c,"query complexity EVALUATE-FOR active learning geodesically convex halfspaces. hull size CONJUNCTION diameter. diameter CONJUNCTION hull size. diameter CONJUNCTION tree - width. tree - width CONJUNCTION diameter. first CONJUNCTION second. second CONJUNCTION first. second CONJUNCTION hull size. hull size CONJUNCTION second. second HYPONYM-OF worst - case bounds. first HYPONYM-OF worst - case bounds. second HYPONYM-OF natural graph parameters. natural graph parameters USED-FOR worst - case bounds. graphs USED-FOR latter. bounded tree - width FEATURE-OF graphs. convexity properties USED-FOR lower bounds. Radon number CONJUNCTION Tree - width. Tree - width CONJUNCTION Radon number. partial cubes CONJUNCTION weakly median graphs. weakly median graphs CONJUNCTION partial cubes. minor - free graphs CONJUNCTION partial cubes. partial cubes CONJUNCTION minor - free graphs. label propagation algorithm CONJUNCTION $ S^2 $ algorithm. $ S^2 $ algorithm CONJUNCTION label propagation algorithm. $ S^2 $ algorithm HYPONYM-OF baseline algorithms. label propagation algorithm HYPONYM-OF baseline algorithms. Material is graph. OtherScientificTerm are hypothesis - dependent upper bounds, smallest shortest path cover, and trees. Generic are algorithm, former, and bounds. ","This paper studies the query complexity of active learning geodesically convex halfspaces. The authors derive hypothesis-dependent upper bounds on the worst-case bounds on three natural graph parameters: the first, the second, and the second on the hull size, diameter, and tree-width. For the former, the authors assume that the graph is convex, and for the latter, they assume that graphs with bounded tree-wideness have bounded Radon number. The lower bounds are based on the convexity properties of the graph, and are obtained by minimizing the smallest shortest path cover between nodes in the graph. The paper compares the proposed bounds with two baseline algorithms: a label propagation algorithm and a $S^2$ algorithm, and shows that the former outperforms the latter on minor-free graphs, partial cubes, weakly median graphs, and trees."
2697,SP:41a6753bc56eb16040600666a859294ae36cfa9c,"connected undirected graphs USED-FOR binary classification. graph properties CONJUNCTION separation condition. separation condition CONJUNCTION graph properties. Task is active learning on graphs. OtherScientificTerm are geodesically convex set, graph, and VC dimension. Method is halfspace querying algorithm. ","This paper studies active learning on graphs. The authors consider the problem of binary classification on connected undirected graphs, where the input is a geodesically convex set and the output is a graph. In particular, they consider the case where the VC dimension of the graph is larger than the number of vertices. They consider both the graph properties and the separation condition, and propose a halfspace querying algorithm."
2698,SP:e880db33ba8c305ef1808a02325e2d2b7da95e68,"method USED-FOR feature encoder. feature encoder USED-FOR TAL models. image-/action - classification pretraining CONJUNCTION TAL training. TAL training CONJUNCTION image-/action - classification pretraining. encoder USED-FOR TAL task. spatially / temporally low fidelity data USED-FOR encoder. OtherScientificTerm are distribution - shift, and GPU memory limit. Method is pretraining routine. ","This paper proposes a method to train a feature encoder for TAL models. The idea is to use the same image-/action-classification pretraining and TAL training, but with a distribution-shift. The authors propose a pretraining routine where the encoder is trained on spatially/temporally low fidelity data, and then used on the TAL task with a GPU memory limit."
2699,SP:e880db33ba8c305ef1808a02325e2d2b7da95e68,low - fidelity video encoder optimization approach USED-FOR temporal action localization problem. low - fidelity video encoder optimization approach USED-FOR large memory constraints. large memory constraints FEATURE-OF temporal action localization problem. ActivityNet and HACS datasets EVALUATE-FOR approach. engineering solution USED-FOR approach. ,This paper proposes a low-fidelity video encoder optimization approach to tackle the temporal action localization problem with large memory constraints. The proposed approach is evaluated on ActivityNet and HACS datasets. The approach is based on an engineering solution.
2700,SP:e880db33ba8c305ef1808a02325e2d2b7da95e68,hardware constraint USED-FOR end - to - end optimization. method USED-FOR them. method USED-FOR TAL head. full - resolution videos USED-FOR TAL head. video encoder USED-FOR TAL models. video encoder USED-FOR task discrepancy problem. lofi setting COMPARE state - of - the - arts. state - of - the - arts COMPARE lofi setting. lofi setting COMPARE models. models COMPARE lofi setting. state - of - the - arts CONJUNCTION models. models CONJUNCTION state - of - the - arts. pretraining conditions FEATURE-OF models. method COMPARE baselines. baselines COMPARE method. Method is video encoding network. ,This paper proposes a new hardware constraint for end-to-end optimization of a video encoding network. The proposed method trains a TAL head on full-resolution videos and then fine-tunes them using the proposed method. The main contribution of the paper is the introduction of a new video encoder for training TAL models to address the task discrepancy problem. The experiments show that the proposed lofi setting outperforms the state-of-the-arts and models trained under different pretraining conditions. The experimental results also show the effectiveness of the method compared to the baselines.
2701,SP:e880db33ba8c305ef1808a02325e2d2b7da95e68,"video encoder optimization USED-FOR temporal action localization ( TAL ) models. method USED-FOR video encoder optimization. temporal and/or spatial resolution USED-FOR mini - batch construction. temporal and/or spatial resolution USED-FOR LoFi. ActivityNet CONJUNCTION HACS - v1.1. HACS - v1.1 CONJUNCTION ActivityNet. LoFi technique USED-FOR TAL models. HACS - v1.1 EVALUATE-FOR TAL models. ActivityNet EVALUATE-FOR TAL models. Method are TAL methods, and video encoder. OtherScientificTerm are GPU memory constraints, and GPU memory requirement. ","This paper proposes a method for video encoder optimization for temporal action localization (TAL) models. The key idea is to use temporal and/or spatial resolution in LoFi for mini-batch construction, which has been shown to improve the performance of TAL methods. The main contribution of this paper is the introduction of GPU memory constraints, which allows the GPU memory requirement to be reduced. Experiments on ActivityNet and HACS-v1.1 show that TAL models trained with the LoFi technique can achieve state-of-the-art performance. The paper also provides a thorough ablation study on the effect of the size of the video encoding."
2702,SP:e880db33ba8c305ef1808a02325e2d2b7da95e68,Models USED-FOR temporal action localization ( TAL ). video encoder CONJUNCTION TAL head. TAL head CONJUNCTION video encoder. video encoder PART-OF stages. TAL head PART-OF stages. auxiliary video clip classification dataset USED-FOR video encoder. End - to - end training USED-FOR encoder. mulgrid training USED-FOR end - to - end training. lower resolution video encoder CONJUNCTION TAL head. TAL head CONJUNCTION lower resolution video encoder. lower resolution video encoder USED-FOR end - to - end training. end - to - end training USED-FOR TAL head. mulgrid training USED-FOR solution. backbone networks CONJUNCTION TAL heads. TAL heads CONJUNCTION backbone networks. OtherScientificTerm is GPU RAM. Material is ActivityNet and HACS datasets. ,"Models for temporal action localization (TAL) are trained in two stages: (1) a video encoder trained on an auxiliary video clip classification dataset, and (2) a TAL head trained on the original video. End-to-end training of the encoder is performed using mulgrid training, while end-to -end training is performed with a lower resolution video encoding, and a larger version of the TAL. The proposed solution is evaluated on the ActivityNet and HACS datasets, where the GPU RAM is significantly reduced. Results show that the proposed backbone networks and TAL heads are effective."
2703,SP:f79e91e469a70b219cd4a2116d5f389842f265ec,regularized M - estimators CONJUNCTION distribution of the residual. distribution of the residual CONJUNCTION regularized M - estimators. derivatives CONJUNCTION distribution of the residual. distribution of the residual CONJUNCTION derivatives. derivatives FEATURE-OF regularized M - estimators. Method is adaptive criterion. Generic is models. ,This paper proposes an adaptive criterion to evaluate the performance of models. The authors propose to use regularized M-estimators with different derivatives and a different distribution of the residual. The main contribution of this paper is to provide a theoretical analysis of this adaptive criterion. 
2704,SP:f79e91e469a70b219cd4a2116d5f389842f265ec,"sparse M - estimation USED-FOR estimation. non - penalised loss function CONJUNCTION penalty function. penalty function CONJUNCTION non - penalised loss function. statistical criterion USED-FOR penalised M - estimation problem. quantities USED-FOR penalised estimator. derivative formulas USED-FOR penalised estimator. tuning parameter USED-FOR regularization. Task is robust linear regression. Generic is criterion. OtherScientificTerm are out - of - sample distance, and unknown quantities. ","This paper studies the problem of robust linear regression with sparse M-estimation, where the estimation is done by estimating the difference between a non-penalised loss function and a penalty function. The authors propose a statistical criterion for the penalised M - estimation problem. The criterion is based on the fact that the out-of-sample distance between the non-parametric and penalised estimator can be bounded by a set of unknown quantities. These quantities are then used to train a penalised estimationator based on derivative formulas. A tuning parameter is also proposed for regularization."
2705,SP:f79e91e469a70b219cd4a2116d5f389842f265ec,convex penalties USED-FOR M - estimators. distribution of residuals CONJUNCTION functions of residuals. functions of residuals CONJUNCTION distribution of residuals. algebraic forms USED-FOR derivatives. algebraic forms USED-FOR estimator. derivatives FEATURE-OF estimator. loss function CONJUNCTION penalty. penalty CONJUNCTION loss function. criterion USED-FOR out - of - sample error. penalty USED-FOR criterion. loss function USED-FOR criterion. tuning parameters FEATURE-OF penalty. covariance matrix USED-FOR criterion. out - of - sample error FEATURE-OF criterion. OtherScientificTerm is finite sample bounds. ,"This paper studies convex penalties for M-estimators. The authors consider the case where the estimator has algebraic forms for the derivatives of the distribution of residuals and the functions of the residuals. The main contribution of this paper is to derive finite sample bounds for the estimators. In particular, the authors derive a criterion for the out-of-sample error based on the covariance matrix and a loss function and a penalty with different tuning parameters."
2706,SP:f79e91e469a70b219cd4a2116d5f389842f265ec,"criterion USED-FOR out of sample error. Task are robust estimation, and robust community. Method is robust regularized linear regression model. Metric is data - driven criterion. ",This paper studies the problem of robust estimation. The authors propose a new data-driven criterion for estimating the out of sample error. The main contribution of the paper is the introduction of a new robust regularized linear regression model. The paper is well-written and well-motivated. The contribution of this paper is clearly stated and the contributions of the authors are well-supported by the robust community.
2707,SP:f79e91e469a70b219cd4a2116d5f389842f265ec,convex and gradient - Lipschitz loss function CONJUNCTION convex penalty. convex penalty CONJUNCTION convex and gradient - Lipschitz loss function. M - estimators USED-FOR linear models. convex and gradient - Lipschitz loss function USED-FOR M - estimators. loss function CONJUNCTION penalty function. penalty function CONJUNCTION loss function. criterion USED-FOR tuning parameters. criterion USED-FOR out - of - sample error. tuning parameters USED-FOR M - estimators. criterion USED-FOR M - estimators. penalty function HYPONYM-OF tuning parameters. loss function HYPONYM-OF tuning parameters. covariance matrix CONJUNCTION noise distribution. noise distribution CONJUNCTION covariance matrix. Generic is it. ,"This paper studies M-estimators for linear models with a convex and gradient-Lipschitz loss function and convex penalty. The authors propose a new criterion for out-of-sample error that can be used to evaluate the tuning parameters of the standard tuning parameters (loss function, penalty function, etc) of M-iterators in linear models. The criterion is based on the observation that the covariance matrix and the noise distribution of a function can be expressed as a function of the number of samples, and that it can be computed in terms of the ratio of the covariant matrix to the corresponding noise distribution."
2708,SP:be53bc4c064402489b644332ad9c17743502d73c,"attention coverage penalty USED-FOR seq2seq models. summarization benchmarks EVALUATE-FOR coverage loss. attention coverage losses USED-FOR beam search. coverage loss COMPARE attention coverage losses. attention coverage losses COMPARE coverage loss. OtherScientificTerm are expected coverage, beam hypotheses, and expected attention coverage constraints. ",This paper proposes a new attention coverage penalty for seq2seq models. The proposed coverage loss is evaluated on several summarization benchmarks and compared to other attention coverage losses for beam search. The authors show that the expected coverage of the beam hypotheses is better than the other expected attention coverage constraints.
2709,SP:be53bc4c064402489b644332ad9c17743502d73c,cross attention distribution USED-FOR beam search. cross attention distribution USED-FOR transformer ’s encoder - decoder framework. algorithm USED-FOR predict distribution. algorithm USED-FOR beam search. ,This paper proposes a transformer’s encoder-decoder framework that uses a cross attention distribution for beam search. The proposed algorithm is able to predict distribution and perform beam search efficiently.
2710,SP:be53bc4c064402489b644332ad9c17743502d73c,"beam search scoring procedure USED-FOR conditional generation. global attention feature component PART-OF beam search scoring procedure. cumulative local autoregressive attention USED-FOR augmentation. neural language generators USED-FOR cumulative global attention. regression USED-FOR training. cumulative local attention COMPARE global attention. global attention COMPARE cumulative local attention. scheme USED-FOR cumulative local attention. decomposable iterative manner USED-FOR scheme. length reward PART-OF scheme. pretrained summarization system USED-FOR summarization. beam search augmentation COMPARE beam search. beam search COMPARE beam search augmentation. approach USED-FOR global attention. Oracle "" global attention COMPARE prediction. prediction COMPARE Oracle "" global attention. beam search CONJUNCTION approach. approach CONJUNCTION beam search. OtherScientificTerm are beam, and attention. Method is decoder transformer. ","This paper proposes a beam search scoring procedure for conditional generation that incorporates a global attention feature component. The augmentation is based on cumulative local autoregressive attention, which is trained using neural language generators. During training, regression is used to compare the cumulative local attention with the global attention. The proposed scheme is a decomposable iterative manner, where the length reward of the beam is computed using the decoder transformer. The summarization is performed using a pretrained summarization system. Experiments show that the proposed beam search augmentation outperforms the original beam search and the proposed approach for global attention, and that ""Oracle"" global attention outperforms prediction."
2711,SP:be53bc4c064402489b644332ad9c17743502d73c,truncated beam size COMPARE vocabulary size. vocabulary size COMPARE truncated beam size. global attention distribution USED-FOR local decisions. global attention distribution USED-FOR global - aware beam search. global scoring function USED-FOR beam search - based generation. global scoring function USED-FOR regression task. regression task USED-FOR global attention distribution prediction. local optimality FEATURE-OF beam search. local optimality CONJUNCTION global optimal generation. global optimal generation CONJUNCTION local optimality. search strategy USED-FOR summarization. search strategy USED-FOR SOTA models. summarization EVALUATE-FOR SOTA models. PEGASUS HYPONYM-OF SOTA models. Generic is approach. ,This paper proposes a global-aware beam search that uses a global attention distribution to make local decisions. The key idea is to use a global scoring function for beam search-based generation as a regression task. The proposed approach is motivated by the observation that truncated beam size is more effective than vocabulary size. The authors also show that the proposed search strategy can improve the summarization performance of SOTA models such as PEGASUS. The main contribution of this paper is to combine local optimality in beam search with global optimal generation.
2712,SP:be53bc4c064402489b644332ad9c17743502d73c,method USED-FOR beam search. global scoring function USED-FOR beam search. step - wise global scoring function USED-FOR beam search. global attention distribution USED-FOR step - wise global scoring function. global - aware inference COMPARE summarization models. summarization models COMPARE global - aware inference. ,This paper proposes a method for beam search with a step-wise global scoring function based on the global attention distribution. The authors show that global-aware inference outperforms summarization models.
2713,SP:4c7d14ab3304cfbf083815aa6e6d9c0e0a5fba6f,"gauge equivariant transformer USED-FOR geometric learning. gauge equivariance USED-FOR orientation ambiguity. convolution or local aggregation operations USED-FOR gauge equivariance. convolution or local aggregation operations USED-FOR orientation ambiguity. parallel transport USED-FOR regular field. error bound FEATURE-OF gauge equivariance. shape classification ( SHREC ) CONJUNCTION human body segmentation. human body segmentation CONJUNCTION shape classification ( SHREC ). shape classification ( SHREC ) HYPONYM-OF datasets. human body segmentation HYPONYM-OF datasets. Method are gauge equivariant self - attention, and gauge equivariant layers. Material is mesh data. ","This paper proposes a gauge equivariant transformer for geometric learning. The main idea is to use the existing work of Gauge Equivariant self-attention, which uses parallel transport to represent the regular field as a regular field. The authors argue that the use of this work is motivated by the observation that gauge equivarianance can alleviate the problem of orientation ambiguity caused by convolution or local aggregation operations. Theoretical analysis of the error bound of the proposed work is provided, which is shown to be tight. Experiments are conducted on two datasets: shape classification (SHREC) and human body segmentation, and on mesh data. The experimental results show that the proposed gauge-equivariant layers perform better than existing work."
2714,SP:4c7d14ab3304cfbf083815aa6e6d9c0e0a5fba6f,"coordinates USED-FOR positional encoding. parallel transport method USED-FOR features. regular fields of cyclic groups CONJUNCTION parallel transport method. parallel transport method CONJUNCTION regular fields of cyclic groups. parallel transport method USED-FOR network. regular fields of cyclic groups USED-FOR network. transformer capacity USED-FOR SHREC and Human Segmentation. OtherScientificTerm are manifolds, global rotations, and local position vectors. Generic is fields. ","This paper proposes a network that uses regular fields of cyclic groups and a parallel transport method to extract features from manifolds. The idea is to use coordinates as a positional encoding, and then use global rotations to map the local position vectors to the generated fields. The transformer capacity is used in SHREC and Human Segmentation."
2715,SP:4c7d14ab3304cfbf083815aa6e6d9c0e0a5fba6f,"canonical coordinate system USED-FOR neighborhoods. parameter efficiency EVALUATE-FOR prior art. Task are attention, and SHREC and Human Body Segmentation tasks. OtherScientificTerm are global symmetry, gauge equivariance, intermediate layers, expressivity, and equivariance error. Method is gauge equivariant transformer. ","This paper studies the problem of attention. The authors propose to use a canonical coordinate system to represent neighborhoods as a function of global symmetry, which they call gauge equivariance. They show that this can be achieved by a gauge equivarianant transformer, where the intermediate layers are independent of each other and the expressivity of each layer is preserved. They demonstrate this in SHREC and Human Body Segmentation tasks. They also show that their method can achieve better parameter efficiency compared to prior art in terms of the number of parameters required to achieve the same performance. The paper also shows that their work can be used to reduce the amount of training data needed to achieve a certain level of expressivity, which can be done by minimizing the equivariant error."
2716,SP:4c7d14ab3304cfbf083815aa6e6d9c0e0a5fba6f,gauge - equivariant transformer ( GET ) USED-FOR manifolds. canonical coordinate systems USED-FOR rotation ambiguity. canonical coordinate systems USED-FOR surface. manifold FEATURE-OF self - attention. Taylor expansion USED-FOR equivariant constraints. regular field of cyclic groups USED-FOR feature fields. Taylor expansion USED-FOR gauge - equivariant self - attention. SHREC dataset CONJUNCTION Human Body Segmentation dataset. Human Body Segmentation dataset CONJUNCTION SHREC dataset. Human Body Segmentation dataset EVALUATE-FOR SOTA. SHREC dataset USED-FOR SOTA. SOTA EVALUATE-FOR method. 3D shape classification and segmentation EVALUATE-FOR method. Human Body Segmentation dataset EVALUATE-FOR method. Method is parallel transport approach. OtherScientificTerm is feature vectors. Generic is fields. ,"This paper proposes a parallel transport approach to learn self-attention on manifolds using a gauge-equivariant transformer (GET). The surface is modeled using canonical coordinate systems to avoid rotation ambiguity, and the surface is represented by a set of canonical coordinates of the manifold. The authors propose to use a Taylor expansion to enforce equivariant constraints on the feature fields, which are modeled as a regular field of cyclic groups. This allows the authors to learn a gauge of self-Attention on the manifold with respect to the Taylor expansion. The feature vectors of the fields are then used to train a parallel transformer. The proposed method is evaluated on 3D shape classification and segmentation using SOTA on the SHREC dataset and Human Body Segmentation dataset."
2717,SP:4c7d14ab3304cfbf083815aa6e6d9c0e0a5fba6f,"self - attention networks ( transformers ) CONJUNCTION equivariant neural networks. equivariant neural networks CONJUNCTION self - attention networks ( transformers ). manifolds CONJUNCTION meshes. meshes CONJUNCTION manifolds. self - attention USED-FOR meshes. self - attention USED-FOR manifolds. 2D manifolds FEATURE-OF 3D shapes. OtherScientificTerm are discretized versions of manifolds, canonical coordinate system, self - attention blocks, gauge, and gauge - equivariant value function. Task is gauge - equivariance. Generic is methods. ","This paper proposes to learn discretized versions of manifolds by combining self-attention networks (transformers) with equivariant neural networks. In particular, the authors propose to learn manifolds and meshes using the idea that the canonical coordinate system can be expressed as a function of the number of points in the manifold. The authors show that this can be achieved by using self - attention blocks, which can be seen as a form of gauge-equivariance. They also show that such methods can be extended to 3D shapes on 2D manifolds. Finally, they show that a gauge can be represented as a parameterized version of the standard gauge, which is then used to learn a gauge -equivariant value function."
2718,SP:19cd64baeb7db11b5ec066e6f8ccb4bc576d3588,"EM algorithm USED-FOR faster fitting of mixture models. exponential family USED-FOR generalized version. SPTNs CONJUNCTION mixture of real NVP flows. mixture of real NVP flows CONJUNCTION SPTNs. evaluation EVALUATE-FOR architectures. mixture of real NVP flows HYPONYM-OF architectures. SPTNs HYPONYM-OF architectures. Method are mixture models, and SGD approach. Generic is approach. ",This paper proposes an EM algorithm for faster fitting of mixture models. The generalized version is based on the exponential family. The proposed SGD approach is evaluated on two architectures: SPTNs and a mixture of real NVP flows. The evaluation shows that the proposed approach outperforms existing methods.
2719,SP:19cd64baeb7db11b5ec066e6f8ccb4bc576d3588,method USED-FOR mixture models. Expectation - Maximization ( EM ) CONJUNCTION Metropolis - Hastings ( MH ). Metropolis - Hastings ( MH ) CONJUNCTION Expectation - Maximization ( EM ). Metropolis - Hastings ( MH ) USED-FOR mixture components. Expectation - Maximization ( EM ) USED-FOR mixture components. MH USED-FOR mixture components. MH USED-FOR EM - based algorithm. it CONJUNCTION gradient - based optimization methods. gradient - based optimization methods CONJUNCTION it. it USED-FOR EM - based algorithm. sum - product networks ( SPNs ) CONJUNCTION mixtures of real - valued flows. mixtures of real - valued flows CONJUNCTION sum - product networks ( SPNs ). Gaussian mixture models ( GMMs ) CONJUNCTION sum - product networks ( SPNs ). sum - product networks ( SPNs ) CONJUNCTION Gaussian mixture models ( GMMs ). Metric is computational cost. ,"This paper proposes a method for learning mixture models. The main idea is to combine Expectation-Maximization (EM) and Metropolis-Hastings (MH) to learn mixture components. In particular, it combines it with gradient-based optimization methods and applies it to an EM-based algorithm based on MH. Empirical results on Gaussian mixture models (GMMs), sum-product networks (SPNs), and mixtures of real-valued flows are presented. The computational cost is also discussed."
2720,SP:19cd64baeb7db11b5ec066e6f8ccb4bc576d3588,"method USED-FOR scalably fitting mixture models. mixture components USED-FOR fitting. mixtures of exponential family distributions FEATURE-OF methods. method COMPARE large - K methods. large - K methods COMPARE method. method COMPARE stochastic gradient descent. stochastic gradient descent COMPARE method. Method are generic mixture models, EM algorithm, and Metropolis Hastings algorithm. Generic are model, applications, and algorithm. ","This paper proposes a method for scalably fitting mixture models. The authors consider generic mixture models, where the fitting is done by combining mixture components. In particular, the authors consider methods for mixtures of exponential family distributions. The EM algorithm is a generalization of the Metropolis Hastings algorithm. The proposed method is compared to stochastic gradient descent, and compared to large-K methods. The experiments show that the proposed model can be applied to a variety of applications, and the proposed algorithm is computationally efficient."
2721,SP:19cd64baeb7db11b5ec066e6f8ccb4bc576d3588,"Metropolis - Hasting ( MH ) USED-FOR likelihood component per datum. synthetic dataset CONJUNCTION UCI datasets. UCI datasets CONJUNCTION synthetic dataset. UCI datasets EVALUATE-FOR method. synthetic dataset EVALUATE-FOR method. Method are stochastic EM algorithm, and minibatch. ",This paper proposes a new stochastic EM algorithm. The authors propose Metropolis-Hasting (MH) to estimate the likelihood component per datum. The method is evaluated on a synthetic dataset and two UCI datasets. The experiments show that the proposed minibatch is more efficient than previous methods.
2722,SP:19cd64baeb7db11b5ec066e6f8ccb4bc576d3588,stochastic approximation USED-FOR EM algorithm. stochastic approximation USED-FOR mixture models. computational cost EVALUATE-FOR mixture models. stochastic approximation USED-FOR computational cost. Metropolis - Hasting ( MH ) sampler USED-FOR E - step. Metropolis - Hasting ( MH ) sampler USED-FOR latent variables. sum product   transform networks CONJUNCTION non - volume preserving flows. non - volume preserving flows CONJUNCTION sum product   transform networks. method COMPARE EM - based methods. EM - based methods COMPARE method. EM - based methods USED-FOR Gaussian mixture models. UCI datasets EVALUATE-FOR sum product   transform networks. synthetic data EVALUATE-FOR EM - based methods. ,"This paper proposes a stochastic approximation to the EM algorithm to reduce the computational cost of training mixture models. Specifically, the authors use a Metropolis-Hasting (MH) sampler to sample the latent variables during the E-step. Experiments on UCI datasets show that the proposed method outperforms other EM-based methods for Gaussian mixture models on synthetic data, as well as on sum product  transform networks and non-volume preserving flows."
2723,SP:aae8847c5e52d14820967ab39770ab4ae16df59c,"global sparsity constraints FEATURE-OF continuous minimization problem. continuous minimization problem USED-FOR training process. weight update CONJUNCTION structure parameter update. structure parameter update CONJUNCTION weight update. structure parameter update HYPONYM-OF steps. weight update HYPONYM-OF steps. variance - reduced policy gradient estimator USED-FOR structure parameter. problem USED-FOR engineering implementation. Method are sparse training method, and deep neural networks. Metric is save memory usage. OtherScientificTerm is memory usage. Task is optimization process. ",This paper proposes a sparse training method for deep neural networks. The training process is formulated as a continuous minimization problem with global sparsity constraints. The optimization process consists of two steps: weight update and a structure parameter update using a variance-reduced policy gradient estimator. The problem is well suited for engineering implementation and can save memory usage.
2724,SP:aae8847c5e52d14820967ab39770ab4ae16df59c,"Method is neural networks. OtherScientificTerm are channel - level sparsity, software, and sparsity. Generic are techniques, and technique. ","This paper studies the problem of training neural networks with channel-level sparsity. The authors propose two techniques to address this issue. The first technique is to train the software to be sparsity-agnostic. The second technique is a variant of this technique, where the software is trained with a fixed number of channels. "
2725,SP:aae8847c5e52d14820967ab39770ab4ae16df59c,inference USED-FOR pruning - while - training. Method is variational based pruning method. Generic is training. ,This paper proposes a variational based pruning method. The main idea is to use inference for pruning-while-training. The training is done in two stages: 1) pruning the weights and 2) fine-tuning the weights. 
2726,SP:aae8847c5e52d14820967ab39770ab4ae16df59c,channel - level sparse training algorithm USED-FOR forward and backward propagation process. channel sparsity USED-FOR forward and backward propagation process. channel sparsity USED-FOR channel - level sparse training algorithm. weight updating step CONJUNCTION mask updating step. mask updating step CONJUNCTION weight updating step. weight updating step PART-OF sparse training process. mask updating step PART-OF sparse training process. variance - reduced policy gradient estimator USED-FOR trainable pruning mask. forward and backward propagation USED-FOR sparse computation. Generic is problem. Method is backpropagation. ,This paper proposes a channel-level sparse training algorithm that leverages channel sparsity to speed up the forward and backward propagation process. The sparse training process consists of a weight updating step followed by a mask updating step. The authors propose a variance-reduced policy gradient estimator to learn a trainable pruning mask. Theoretical analysis of the problem is provided and empirical results are presented. The main contribution of the paper is to show that the forward-and-backward propagation can speed up sparse computation without backpropagation.
2727,SP:aae8847c5e52d14820967ab39770ab4ae16df59c,Network sparsity USED-FOR inference. sparsity FEATURE-OF forward and backward propagation steps. chain rule USED-FOR sparse structure. chain - rule based gradient step USED-FOR structure. reduced variance FEATURE-OF policy gradient estimator. forward passes PART-OF policy gradient estimator. forward passes PART-OF it. technique USED-FOR VGG19. accuracy EVALUATE-FOR networks. accuracy EVALUATE-FOR FLOPs. FLOPs EVALUATE-FOR networks. CIFAR-10 EVALUATE-FOR technique. CIFAR-10 EVALUATE-FOR VGG19. Method is channel - wise sparse network. ,"Network sparsity is an important issue for inference. This paper proposes a channel-wise sparse network where the forward and backward propagation steps are trained with sparsity. The structure of the structure is modeled as a chain-rule based gradient step, where the chain rule is used to learn sparse structure. The paper also proposes a policy gradient estimator with reduced variance that consists of two forward passes and two backward passes. The proposed technique is applied to VGG19 on CIFAR-10 and shows improved accuracy and lower FLOPs compared to existing networks."
2728,SP:e0aa68ab03a3ef396b0dc4be4190b328d72cfab0,"NEO - IS COMPARE NEIS. NEIS COMPARE NEO - IS. NEO - IS USED-FOR normalization constant. MCMC extension USED-FOR sampling. NEO - IS COMPARE methods. methods COMPARE NEO - IS. NEO - IS COMPARE MCMC variant. MCMC variant COMPARE NEO - IS. methods USED-FOR finding the marginal likelihood. finding the marginal likelihood CONJUNCTION Sampling. Sampling CONJUNCTION finding the marginal likelihood. Sampling CONJUNCTION Inpaiting. Inpaiting CONJUNCTION Sampling. methods USED-FOR Inpaiting. methods USED-FOR Sampling. posterior distribution USED-FOR Sampling. OtherScientificTerm are Non - Equilibrium IS ( NEIS ), and marginal likelihood. Method is importance sampler NEO - IS. ","This paper proposes a new importance sampler NEO-IS (NEO-IS), which is a generalization of Non-Equilibrium IS (NEIS) and is based on the MCMC extension to sampling. Compared to NEIS, the authors show that NEO-Is is able to approximate the normalization constant better than NEIS. The authors also show that the proposed MCMC variant is more efficient than the original NEIS and the proposed methods for finding the marginal likelihood, Sampling from the posterior distribution, and Inpaiting."
2729,SP:e0aa68ab03a3ef396b0dc4be4190b328d72cfab0,"orbits USED-FOR importance samples. negative log target CONJUNCTION friction term. friction term CONJUNCTION negative log target. friction term USED-FOR Hamilton dynamics. negative log target USED-FOR Hamilton dynamics. NEO - IS USED-FOR MCMC algorithm. iterative Sampling Importance Resampling ( SIR ) PART-OF NEO - IS framework. iterative Sampling Importance Resampling ( SIR ) USED-FOR NEO - MCMC. importance weight USED-FOR Orbits. algorithms USED-FOR VAE. Method are importance samplers, self - normalized version, and conformal Hamiltonian dynamics. OtherScientificTerm are normalized importance weights, map $ T$, and dissipative friction term. Generic is algorithm. ","This paper studies the problem of importance samplers. The authors propose a self-normalized version of the MCMC algorithm based on the NEO-IS framework, which is an extension of the iterative Sampling Importance Resampling (SIR) introduced in the original NEO-MCMC algorithm. The key idea is to replace the negative log target and the friction term in the Hamilton dynamics with a dissipative friction term. Orbits are then used to generate importance samples. The importance weight of the importance samples is computed by minimizing the difference between the normalized importance weights of the original and the self-normed version. The algorithm is shown to converge to a conformal Hamiltonian dynamics, and the authors show that the proposed algorithms can be applied to VAE."
2730,SP:e0aa68ab03a3ef396b0dc4be4190b328d72cfab0,algorithms USED-FOR sampling from complex distributions. algorithms USED-FOR normalizing constants. NEO USED-FOR sampling from complex distributions. NEO USED-FOR normalizing constants. NEO - IS HYPONYM-OF importance sampling estimator. importance sampling estimator USED-FOR normalizing constant. NEO - IS unbiased estimator CONJUNCTION iterated sampling- importance resampling methods. iterated sampling- importance resampling methods CONJUNCTION NEO - IS unbiased estimator. NEO - IS unbiased estimator USED-FOR normalizing constant. NEO - IS unbiased estimator USED-FOR NEO - MCMC. iterated sampling- importance resampling methods PART-OF NEO - MCMC. OtherScientificTerm is complex distributions. ,"This paper presents algorithms for computing normalizing constants using NEO for sampling from complex distributions. Specifically, the authors propose NEO-IS, an importance sampling estimator for computing the normalizing constant, which is a generalization of the previous NEO-Is unbiased estimator and iterated sampling-importance resampling methods. The main contribution of this paper is the introduction of NEO-MCMC, which combines the proposed NEO-IIS unbiased and the iterated learning of the NEO-IMPACT estimator. "
2731,SP:e0aa68ab03a3ef396b0dc4be4190b328d72cfab0,NEO - IS CONJUNCTION NEO - MCMC. NEO - MCMC CONJUNCTION NEO - IS. NEO - MCMC HYPONYM-OF sampling approaches. NEO - IS HYPONYM-OF sampling approaches. method USED-FOR sampling. method USED-FOR normalizing constant. normalizing constant USED-FOR unnormalized target distribution. method USED-FOR unnormalized target distribution. unnormalized distribution USED-FOR sampling. approaches USED-FOR Normalizing Constant Estimation. approaches USED-FOR Sampling tasks. Normalizing Constant Estimation CONJUNCTION Sampling tasks. Sampling tasks CONJUNCTION Normalizing Constant Estimation. Metric is convergence guarantees. ,This paper proposes a method for sampling from an unnormalized target distribution using a normalizing constant. The authors propose two sampling approaches: NEO-IS and NEO-MCMC. The proposed approaches are applied to Normalizing Constant Estimation and Sampling tasks. The convergence guarantees are provided.
2732,SP:e0aa68ab03a3ef396b0dc4be4190b328d72cfab0,Non - Equilibrium Importance Sampling ( NEIS ) USED-FOR NEO. discrete orbit USED-FOR NEO. Jacobian determinant USED-FOR probability distribution and sampling process. method USED-FOR partition function. method USED-FOR biased estimation. self - normalized IS HYPONYM-OF biased estimation. estimated partition function USED-FOR biased estimation. bias CONJUNCTION MSE. MSE CONJUNCTION bias. upper bound USED-FOR bias. upper bound USED-FOR MSE. orbit trajectory USED-FOR NEO. damped Hamiltonian system USED-FOR system. damped Hamiltonian system USED-FOR orbit trajectory. extra momentum USED-FOR system. NEO USED-FOR NEO - MCMC procedure. Sampling Importance Resampling USED-FOR NEO - MCMC procedure. NEO USED-FOR Sampling Importance Resampling. Generic is methods. ,"This paper proposes Non-Equilibrium Importance Sampling (NEIS) for NEO with discrete orbit. The proposed method estimates the partition function of the probability distribution and sampling process using a Jacobian determinant. The method is motivated by biased estimation (i.e., self-normalized IS) of the estimated partition function, which is a popular method for biased estimation in the literature. The authors provide an upper bound on the bias and MSE of the proposed method, and show that the proposed methods can be used to estimate the bias. The paper also proposes a new NEO-MCMC procedure based on the proposed NEO with Sampling Importance Resampling with NEO. The main idea is to use the damped Hamiltonian system as the orbit trajectory of the system with extra momentum."
2733,SP:506dca4f64f837e32958c3c43a0c68f194a36bb3,MBC property FEATURE-OF set encoding algorithms. Mini - Batch Consistency ( MBC ) HYPONYM-OF property. slot - based algorithm USED-FOR MBC. It USED-FOR representations. algorithm COMPARE Deep Sets. Deep Sets COMPARE algorithm. Deep Sets CONJUNCTION Set Transformer. Set Transformer CONJUNCTION Deep Sets. Task is set encoding. OtherScientificTerm is cardinality. Material is streaming data. Method is Slot Set Encoder ( SSE ). ,"This paper studies the problem of set encoding and proposes a new property called Mini-Batch Consistency (MBC) which is a property of existing set encoding algorithms. The authors propose a slot-based algorithm for MBC and show that the MBC property can be achieved by a simple algorithm called Slot Set Encoder (SSE). It is able to learn representations that are similar to Deep Sets and Set Transformer, while preserving the cardinality. The paper also shows that the proposed algorithm can be applied to streaming data."
2734,SP:506dca4f64f837e32958c3c43a0c68f194a36bb3,"Attention USED-FOR Transformers. slots USED-FOR method. Method is attention based set encoder. OtherScientificTerm are mini - batch consistency, and permutations. ","This paper proposes an attention based set encoder for Transformers. The proposed method is based on slots, where each slot corresponds to a set of permutations of the input set. The idea is to use mini-batch consistency to ensure that the permutations are similar to each other. "
2735,SP:506dca4f64f837e32958c3c43a0c68f194a36bb3,"permutation invariance CONJUNCTION equivariance. equivariance CONJUNCTION permutation invariance. equivariance FEATURE-OF it. permutation invariance FEATURE-OF it. method USED-FOR pairwise interactions. DeepSets COMPARE method. method COMPARE DeepSets. method COMPARE DeepSets. DeepSets COMPARE method. Mini - Batch Consistent Set Encoding FEATURE-OF DeepSets. sigmoid USED-FOR attention matrix. softmax USED-FOR attention matrix. softmax CONJUNCTION sigmoid. sigmoid CONJUNCTION softmax. slot attention USED-FOR encoding. method COMPARE methods. methods COMPARE method. methods USED-FOR set encoding. method USED-FOR set encoding. tasks EVALUATE-FOR set encoding. tasks EVALUATE-FOR methods. tasks EVALUATE-FOR method. OtherScientificTerm are memory, random subset, attention, self - attention, and GRU update of slots. Material is stream of data. Generic is encodings. ","This paper proposes a method to encode pairwise interactions in a stream of data. The method is similar to DeepSets with Mini-Batch Consistent Set Encoding, except that it has permutation invariance and equivariance. The encoding is based on slot attention, where the memory is used to store a random subset, and the attention is computed using a combination of softmax and sigmoid to approximate the attention matrix. The encodings are trained with self-attention, and a GRU update of slots is used. The proposed method is evaluated on a variety of tasks and compared to other methods for set encoding."
2736,SP:506dca4f64f837e32958c3c43a0c68f194a36bb3,mini - batch encodings USED-FOR set encoding. property USED-FOR set encoding functions. mini - batch consistency HYPONYM-OF property. encoder USED-FOR set encoding. encoding function USED-FOR mini - batch consistency. OtherScientificTerm is set elements. ,"This paper studies the properties of mini-batch encodings for the task of set encoding. The authors propose a new property for set encoding functions, called mini-batch consistency, which is defined as the consistency of the encoder with respect to the set elements. In particular, the authors show that the encoding function is invariant to the number of iterations."
2737,SP:506dca4f64f837e32958c3c43a0c68f194a36bb3,concept USED-FOR large - scale set encoding models. mini - batch processing of sets USED-FOR large - scale set encoding models. invariance CONJUNCTION equivariance. equivariance CONJUNCTION invariance. invariance FEATURE-OF set encoding models. MBC USED-FOR set encoding models. equivariance FEATURE-OF set encoding models. equivariance USED-FOR MBC. invariance USED-FOR MBC. slot set encoder HYPONYM-OF MBC - based set encoding mechanism. method COMPARE approaches. approaches COMPARE method. Method is mini - batch consistency ” ( MBC ). ,This paper proposes a concept called “mini-batch consistency” (MBC) for training large-scale set encoding models with mini-batch processing of sets. MBC aims to improve the invariance and equivariance of existing set encoders. The authors propose an MBC-based set encoding mechanism called slot set encoder and show that the proposed method outperforms existing approaches.
2738,SP:b2eafdb24fa081ae8b37525d70fb4bc2d54518dc,Nash - Q learning USED-FOR one. 1v1 Diplomacy EVALUATE-FOR method. Material is human data. Metric is 6v1 Diplomacy. ,This paper proposes a new method based on Nash-Q learning. The main idea is to use human data to train a new one that is similar to the one proposed in the paper. The method is evaluated on 1v1 Diplomacy and on 6v2 Diplomacy. The results show that the proposed method outperforms the existing methods.
2739,SP:b2eafdb24fa081ae8b37525d70fb4bc2d54518dc,"learning methods USED-FOR board game Diplomacy. equilibrium search method USED-FOR Nash. equilibrium search method USED-FOR method. value function USED-FOR Nash equilibrium. policy USED-FOR equilibrium. policy network USED-FOR search. Diplomacy - specific method USED-FOR graph structure of the game. transformer - based model USED-FOR learning to play Diplomacy. transformer - based model COMPARE Graph - Network approaches. Graph - Network approaches COMPARE transformer - based model. methods USED-FOR Diplomacy agent. Method is value iteration. OtherScientificTerm are large action space, Double Oracle, and 2 - player variant of Diplomacy. ","This paper proposes learning methods for the board game Diplomacy. The method is based on the equilibrium search method for Nash, where the value function is used to find the Nash equilibrium. The search is performed by a policy network, which is trained to maximize the return of the value iteration. The Diplomacy-specific method is designed to capture the graph structure of the game. The authors show that the proposed transformer-based model is better suited for learning to play Diplomacy compared to other Graph-Network approaches. They also show that their methods can be used to train a Diplomacy agent that is robust to large action space. Finally, the authors conduct experiments on Double Oracle, a 2-player variant of Diplomacy, and demonstrate the effectiveness of their methods."
2740,SP:b2eafdb24fa081ae8b37525d70fb4bc2d54518dc,"Diplomacy HYPONYM-OF 7 - player strategy game. simultaneous moves CONJUNCTION combinatorially large action space. combinatorially large action space CONJUNCTION simultaneous moves. 7 - player strategy game USED-FOR RL. dataset of human play USED-FOR imitation learning. websites FEATURE-OF dataset of human play. regret matching CONJUNCTION geographically local perturbations. geographically local perturbations CONJUNCTION regret matching. double oracle - like method CONJUNCTION regret matching. regret matching CONJUNCTION double oracle - like method. policy improvement algorithm USED-FOR competitive - quality play. Nash Q - learning CONJUNCTION double oracle - like method. double oracle - like method CONJUNCTION Nash Q - learning. Nash Q - learning USED-FOR policy improvement algorithm. DipNet USED-FOR 1v6 setting. DipNet HYPONYM-OF imitation baseline. DORA ) USED-FOR 1v1 human play. method COMPARE DORA ). DORA ) COMPARE method. method USED-FOR 1v1 human play. policy improvement method COMPARE SOTA ( SearchBot ). SOTA ( SearchBot ) COMPARE policy improvement method. imitation network USED-FOR policy improvement method. OtherScientificTerm are human - level play, and metagame. Method are policy improvement algorithms, transformer - based architecture, and DORA. Task is exploration problem. ","This paper studies RL in Diplomacy, a 7-player strategy game with simultaneous moves and a combinatorially large action space. The paper presents a dataset of human play from various websites, as well as an imitation baseline (DipNet for the 1v6 setting). The policy improvement algorithm is based on Nash Q-learning, a double oracle-like method, regret matching, and geographically local perturbations, and is designed to encourage competitive-quality play. The method (DORA) is shown to outperform SOTA (SearchBot) in 1v1 human play, and to be competitive with human-level play in 2v2 and 3v3 settings. The authors also propose a transformer-based architecture, which is similar to DORA. The main difference is that the policy improvement method is trained with an imitation network, and the exploration problem is modeled as a metagame."
2741,SP:b2eafdb24fa081ae8b37525d70fb4bc2d54518dc,"algorithm USED-FOR agents. human data CONJUNCTION reward shaping. reward shaping CONJUNCTION human data. algorithm USED-FOR large action space of Diplomacy. algorithm USED-FOR self - play. self - play USED-FOR agents. action exploration CONJUNCTION equilibrium approximation. equilibrium approximation CONJUNCTION action exploration. algorithm USED-FOR action exploration. algorithm USED-FOR equilibrium approximation. It USED-FOR action exploration. algorithm USED-FOR It. algorithm USED-FOR value iteration reinforcement learning. agent USED-FOR 2 - player variant of Diplomacy. agent USED-FOR 7 - player no - press Diplomacy. OtherScientificTerm are combinatorial action spaces, and policy proposals. Method are policy proposal network, and double oracle step. Generic are it, approach, and game. ","This paper proposes an algorithm for training agents to self-play in the large action space of Diplomacy, where agents are trained on both human data and reward shaping. It is an extension of an existing algorithm for value iteration reinforcement learning, where the goal is to learn a policy proposal network that is robust to changes in the combinatorial action spaces. The authors argue that this algorithm can be used for both action exploration and equilibrium approximation, and show that it can be applied to a variety of problems. They show that their approach is able to solve a 7-player no-press Diplomacy with a single agent trained on a 2-player variant of the game, and that their policy proposals are robust to change in the policy proposals of the other agents. They also show that a double oracle step is needed to make sure that the policy proposal of the agent is correct."
2742,SP:b2eafdb24fa081ae8b37525d70fb4bc2d54518dc,"approach USED-FOR agents. agents USED-FOR Diplomacy board game. DORA USED-FOR agents. action space FEATURE-OF it. OtherScientificTerm are human gameplay, two player ( France / Austria ) variant, and DORA player. Material is historical game data. Task is Diplomacy. ","This paper presents an approach to train agents to play a Diplomacy board game using DORA. The approach is inspired by human gameplay, where agents are trained on historical game data. The authors propose a two player (France/Austria) variant, where one agent is trained on the historical game and the other is trained to play on the new game. The main idea is to learn a DORTA player, and then use it to explore the action space of the game. This is an interesting direction to explore in the context of Diplomacy."
2743,SP:1ce1cef9988a07ccd2175a718b29ad23bc779429,"approach USED-FOR attention heads. $ H'$ heads PART-OF multi - head attention layer. $ H$ heads USED-FOR task. latent variables USED-FOR attention heads. strategies USED-FOR attention head selection. transformers CONJUNCTION speech - transformer. speech - transformer CONJUNCTION transformers. approach COMPARE baseline approaches. baseline approaches COMPARE approach. transformers USED-FOR MT. speech - transformer USED-FOR tasks. baseline approaches COMPARE baselines. baselines COMPARE baseline approaches. MT CONJUNCTION speech - transformer. speech - transformer CONJUNCTION MT. approach COMPARE baselines. baselines COMPARE approach. full parameter sharing USED-FOR baseline approaches. transformers HYPONYM-OF baseline approaches. speech - transformer HYPONYM-OF baseline approaches. transformers HYPONYM-OF full parameter sharing. speech - transformer HYPONYM-OF full parameter sharing. adapter modules USED-FOR baselines. multilingual ASR CONJUNCTION multi - domain ASR. multi - domain ASR CONJUNCTION multilingual ASR. multi - domain ASR CONJUNCTION multilingual speech translation. multilingual speech translation CONJUNCTION multi - domain ASR. multilingual Machine Translation CONJUNCTION multilingual ASR. multilingual ASR CONJUNCTION multilingual Machine Translation. multilingual speech translation CONJUNCTION multi - domain speech translation. multi - domain speech translation CONJUNCTION multilingual speech translation. multi - domain ASR CONJUNCTION multi - domain speech translation. multi - domain speech translation CONJUNCTION multi - domain ASR. multilingual ASR CONJUNCTION multilingual speech translation. multilingual speech translation CONJUNCTION multilingual ASR. tasks EVALUATE-FOR head - sharing approach. head - sharing approach COMPARE fully - shared baselines. fully - shared baselines COMPARE head - sharing approach. tasks EVALUATE-FOR fully - shared baselines. scalable adaptation USED-FOR neural machine translation. OtherScientificTerm is multi - head attention layers. Task are Subset selection, and Group selection. ","This paper proposes an approach to share attention heads across multiple heads in a multi-head attention layer, where $H'$ heads are shared across all $H$ heads in the multi - head attention layer. Subset selection and Group selection are two strategies for attention head selection. The attention heads are parameterized as latent variables, which are then shared across the multiple heads. The proposed approach is compared to several baseline approaches with full parameter sharing (transformers, speech-transformer, MT, transformers with adapter modules) on three tasks: multilingual Machine Translation, multilingual ASR, multi-domain ASR and multilingual speech translation. The results show that the proposed head-sharing approach outperforms fully-shared baselines on all three tasks. The paper also provides a theoretical analysis of scalable adaptation to neural machine translation."
2744,SP:1ce1cef9988a07ccd2175a718b29ad23bc779429,"speech recognition CONJUNCTION multi - domain speech translation. multi - domain speech translation CONJUNCTION speech recognition. multilingual translation CONJUNCTION speech recognition. speech recognition CONJUNCTION multilingual translation. tasks PART-OF multi - task learning setup. transformer model PART-OF multi - task learning setup. transformer model USED-FOR tasks. multilingual translation HYPONYM-OF multi - task learning setup. multilingual translation HYPONYM-OF tasks. speech recognition HYPONYM-OF multi - task learning setup. speech recognition HYPONYM-OF tasks. attention heads USED-FOR task. attention heads USED-FOR approach. multilingual translation CONJUNCTION speech translation. speech translation CONJUNCTION multilingual translation. tasks EVALUATE-FOR baselines. multilingual translation EVALUATE-FOR baselines. speech translation EVALUATE-FOR baselines. multilingual translation HYPONYM-OF tasks. speech translation HYPONYM-OF tasks. Task is negative interference. OtherScientificTerm are positive transfer, and negative transfer. ","This paper proposes a multi-task learning setup that combines a transformer model for tasks such as speech recognition, multilingual translation, and multi-domain speech translation. The approach is based on using attention heads for each task to reduce negative interference. The idea is that positive transfer is beneficial, while negative transfer is harmful. Experiments on three tasks (multilingual translation and speech translation) show improvements over baselines."
2745,SP:1ce1cef9988a07ccd2175a718b29ad23bc779429,transformer architecture USED-FOR multi - task learning. attention heads PART-OF multi - head attention. attention heads USED-FOR transformer architecture. neural module USED-FOR heads. heads USED-FOR model. gumbel - softmax USED-FOR neural module. Generic is this. ,"This paper proposes a transformer architecture for multi-task learning that combines attention heads from multi-head attention. The model uses these heads as a neural module, which is trained using gumbel-softmax. Experimental results show that this is effective."
2746,SP:1ce1cef9988a07ccd2175a718b29ad23bc779429,Multi - head attention USED-FOR Transformer models. Gumbel softmax USED-FOR attention heads. latent variables USED-FOR attention selection. OtherScientificTerm is shared and specialized attention heads. Generic is approach. ,"Multi-head attention has been proposed for Transformer models. The idea is to use Gumbel softmax to select attention heads. The attention selection is based on latent variables. The authors propose to use both shared and specialized attention heads, which is a novel approach."
2747,SP:1ce1cef9988a07ccd2175a718b29ad23bc779429,strategies USED-FOR approach. group selection HYPONYM-OF strategies. approach COMPARE transformers. transformers COMPARE approach. transformers CONJUNCTION adapters. adapters CONJUNCTION transformers. approach COMPARE adapters. adapters COMPARE approach. multi - lingual / domain settings EVALUATE-FOR approach. OtherScientificTerm is negative interference. Generic is models. ,"This paper proposes an approach based on two strategies: (1) group selection and (2) minimizing negative interference. The authors compare their approach with transformers and adapters on multi-lingual/domain settings, and show that their models perform better."
2748,SP:69c522cea4a150624bc709e1c12c0f65183c1b2a,covariance matrix FEATURE-OF features. test error FEATURE-OF random feature regression. asymptotic regime FEATURE-OF random feature regression. covariance shifts FEATURE-OF partial ordering. OtherScientificTerm is covariate shift. Method is overparameterization. ,"This paper studies the test error of random feature regression in the asymptotic regime when the covariance matrix of the features is large. The authors show that partial ordering with covariance shifts can lead to large test error, and that the covariate shift is due to overparameterization."
2749,SP:69c522cea4a150624bc709e1c12c0f65183c1b2a,kernel ridge regression USED-FOR covariate shift. asymptotic regime FEATURE-OF kernel ridge regression. bias CONJUNCTION variance. variance CONJUNCTION bias. limiting test error CONJUNCTION bias. bias CONJUNCTION limiting test error. OtherScientificTerm is hardness. ,"This paper studies kernel ridge regression in the asymptotic regime of covariate shift in the setting of hardness. The main result is that the limiting test error, the bias, and the variance can be reduced to zero when the hardness is small. "
2750,SP:69c522cea4a150624bc709e1c12c0f65183c1b2a,"high dimensional setting FEATURE-OF random feature regression. OtherScientificTerm are covariate shift, high - dimensional asymptotic limit, Gaussian covariates, and covariance matrix. Method is bias and variance decomposition. ","This paper studies random feature regression in a high dimensional setting, where the covariate shift can be very large. The authors prove a high-dimensional asymptotic limit for the variance of Gaussian covariates, and provide a bias and variance decomposition of the covariance matrix."
2751,SP:69c522cea4a150624bc709e1c12c0f65183c1b2a,covariate shift FEATURE-OF over - parameterized random features models. linear trend FEATURE-OF test error. clean and corrupted error FEATURE-OF test error. OtherScientificTerm is partial ordering over test errors. ,"This paper studies covariate shift in over-parameterized random features models. The authors show that the test error follows a linear trend between the clean and corrupted error, and that partial ordering over test errors can be observed."
2752,SP:69c522cea4a150624bc709e1c12c0f65183c1b2a,"Method is overparameterized models. OtherScientificTerm are covariate shift, and relative hardness. Material is empirical benchmarks. ","This paper studies the problem of overparameterized models, where the covariate shift between the training data and the test data can cause the relative hardness to decrease. The authors provide empirical benchmarks to demonstrate the effect of this phenomenon."
2753,SP:ff1b7a7a6295e8f40f3b5df5f6950ca9d33603e0,misspecification of the prior distribution FEATURE-OF Thompson sampling algorithm. canonical TS algorithm PART-OF n - Monte Carlo algorithm. regret difference FEATURE-OF misspecified prior. misspecified prior USED-FOR n - MC algorithm. total variation distance USED-FOR eps. total variation distance USED-FOR misspecification. Task is meta - learning setting. Method is meta - learning. ,"This paper studies the problem of misspecification of the prior distribution of the Thompson sampling algorithm in a meta-learning setting. In particular, the authors consider the canonical TS algorithm as an n-Monte Carlo algorithm and show that the regret difference of the misspecified prior of the n-MC algorithm can be bounded by the total variation distance between the eps. The authors also provide a theoretical analysis of the effect of the amount of mispecification on the performance of meta-learning."
2754,SP:ff1b7a7a6295e8f40f3b5df5f6950ca9d33603e0,"theory USED-FOR Bayesian bandit algorithms. misspecified prior USED-FOR Bayesian bandit algorithms. sample complexity EVALUATE-FOR Bayesian meta - learning. theory USED-FOR setting. Bayesian POMDPs HYPONYM-OF setting. Method are Thompson sampling, and sensitivity analysis. OtherScientificTerm are sensitivity bound, and Gaussian prior+Gaussian reward. ",This paper proposes a new theory for Bayesian bandit algorithms with misspecified prior. The main idea is to use Thompson sampling to estimate the sample complexity of Bayesian meta-learning. The sensitivity analysis is based on the fact that the sensitivity bound depends on the difference between the Gaussian prior+Gaussian reward. The theory is applied to a new setting called Bayesian POMDPs.
2755,SP:ff1b7a7a6295e8f40f3b5df5f6950ca9d33603e0,prior mis - specification FEATURE-OF bayesian bandit algorithms. Thompson sampling algorithms HYPONYM-OF algorithms. Task is meta learning. ,"This paper studies the problem of prior mis-specification in bayesian bandit algorithms. In particular, the authors focus on algorithms such as Thompson sampling algorithms, which have been widely used in meta learning. "
2756,SP:ff1b7a7a6295e8f40f3b5df5f6950ca9d33603e0,Thompson sampling ( TS ) CONJUNCTION posterior sampling bandit algorithms. posterior sampling bandit algorithms CONJUNCTION Thompson sampling ( TS ). prior misspecification FEATURE-OF posterior sampling bandit algorithms. prior misspecification USED-FOR Thompson sampling ( TS ). regret EVALUATE-FOR algorithms. misspecified prior USED-FOR algorithms. priors PART-OF meta - learning setting. model parameters FEATURE-OF bandit instances. unknown prior USED-FOR model parameters. total variation distance FEATURE-OF estimated priors. ,"This paper studies prior misspecification in Thompson sampling (TS) and posterior sampling bandit algorithms. The regret of these algorithms with misspecified prior is studied in a meta-learning setting, where the priors are assumed to be unknown. The authors show that bandit instances with unknown prior can be learned with model parameters that are close to the estimated priors in terms of total variation distance."
2757,SP:ff1b7a7a6295e8f40f3b5df5f6950ca9d33603e0,"mis - specified priors FEATURE-OF posterior - sampling - based bandit algorithms. bandit settings CONJUNCTION meta - learning settings. meta - learning settings CONJUNCTION bandit settings. Generic is algorithms. Method are Thompson sampling, and posterior - sample - based decision algorithms. OtherScientificTerm are posterior distribution, and knowledge gradient. Task is approximation of knowledge gradient. Metric is TS. ","This paper studies posterior-sampling-based bandit algorithms with mis-specified priors under bandit settings and meta-learning settings. The authors propose two algorithms: 1) Thompson sampling, where the posterior distribution is sampled from, and 2) posterior-sample-based decision algorithms, where knowledge gradient is estimated from the posterior. The main contribution of the paper is to show that the approximation of knowledge gradient can be achieved by minimizing TS."
2758,SP:3477b64480ed638b1c4e1f8aa73fc2e77666c89a,"exponential separation FEATURE-OF learning models. boosting techniques USED-FOR algorithm. algorithm USED-FOR exponential separation. algorithm USED-FOR learning models. equivalence query oracle USED-FOR counterexamples. OtherScientificTerm are equivalence - query oracle, and adversarial examples. Method is adversarially robust learning. ",This paper proposes an algorithm for learning models with exponential separation based on boosting techniques. The key idea is to use an equivalence query oracle to generate counterexamples and then use the equivalence-query oracle as a proxy to generate adversarial examples. This is an important problem in adversarially robust learning.
2759,SP:3477b64480ed638b1c4e1f8aa73fc2e77666c89a,"PAC model USED-FOR exponential separation. restricted adversary FEATURE-OF adversarial training. adversarial training USED-FOR EQ model. exponential separation USED-FOR robustness. robustness EVALUATE-FOR adversaries. Metric is query complexity. Method are interactive model, boosting procedure, randomized EQ model, and on - manifold ) adversarial training. OtherScientificTerm are EQ oracle, random counterexample, and VC - dimension. Task is adversarial robustness. ","This paper studies adversarial training with restricted adversary in the context of an EQ model. The authors propose a PAC model that learns an exponential separation between the query complexity of the interactive model and that of the original EQ oracle. They also propose a boosting procedure where the random counterexample is used to boost the VC-dimension of the randomized EQ model, which improves adversarial robustness. Finally, they evaluate the robustness of the adversaries on a variety of (on-manifold) adversarial testing environments."
2760,SP:3477b64480ed638b1c4e1f8aa73fc2e77666c89a,"query complexity EVALUATE-FOR query model. Equivalence - Query ( EQ ) model HYPONYM-OF query model. upper bound EVALUATE-FOR PAC learning. model of learning CONJUNCTION adversarial robustness. adversarial robustness CONJUNCTION model of learning. OtherScientificTerm are VC class, equivalence - query sample oracle, and on - manifold attacks. Generic is model. Method is learner. ","This paper studies the query complexity of a query model called the Equivalence-Query (EQ) model, which is a variant of the VC class. The authors provide an upper bound on PAC learning, which shows that the model is robust to on-manifold attacks. They also provide an equivalence-query sample oracle, which can be used as a proxy for the model of learning and adversarial robustness. "
2761,SP:3477b64480ed638b1c4e1f8aa73fc2e77666c89a,sample complexity EVALUATE-FOR PAC model. sample complexity CONJUNCTION query complexity. query complexity CONJUNCTION sample complexity. PAC model COMPARE interactive model. interactive model COMPARE PAC model. query complexity EVALUATE-FOR interactive model. sample complexity EVALUATE-FOR interactive model. PAC model COMPARE query complexity. query complexity COMPARE PAC model. Equivalence - Query - learning HYPONYM-OF interactive model. Metric is adversarial robustness. ,This paper studies adversarial robustness by comparing the sample complexity of the PAC model and the query complexity of an interactive model (Equivalence-Query-learning). 
2762,SP:3477b64480ed638b1c4e1f8aa73fc2e77666c89a,"EQ - learning HYPONYM-OF abstract learning model. sample complexity EVALUATE-FOR algorithm. Method is classifier. OtherScientificTerm are data distribution, and finite VC dimension. Generic is formalism. Material is on - manifold adversarial examples. ","This paper proposes a new abstract learning model called EQ-learning, which is an extension of the well-known Q-learning. The main idea is to learn a classifier that predicts the label of an instance of a class from a data distribution with a finite VC dimension. The formalism is well-motivated, and the sample complexity of the algorithm is shown to be O(1/\sqrt{T}) with respect to the number of samples. Experiments are conducted on on-manifold adversarial examples."
2763,SP:7520cc1203bb06bbe432e7cc679892e95258ed99,benchmark EVALUATE-FOR diverse source models. benchmark EVALUATE-FOR proxy PARC. Generic is source models. OtherScientificTerm is proxy. Method is crowd - sourced models. ,"This paper presents a new benchmark for diverse source models, which is based on the proxy PARC. The source models are selected based on their similarity to the proxy. The paper also provides a theoretical analysis of the effect of crowd-sourced models."
2764,SP:7520cc1203bb06bbe432e7cc679892e95258ed99,unified benchmark EVALUATE-FOR transferability estimation algorithms. algorithm USED-FOR one. Person correlation score EVALUATE-FOR one. ,This paper presents a unified benchmark for transferability estimation algorithms. The authors propose a new algorithm and evaluate the one on Person correlation score.
2765,SP:7520cc1203bb06bbe432e7cc679892e95258ed99,"model selection USED-FOR transfer learning. source or upstream models CONJUNCTION downstream or target data. downstream or target data CONJUNCTION source or upstream models. evaluation metric EVALUATE-FOR approaches. benchmark EVALUATE-FOR source models. PARC COMPARE algorithms. algorithms COMPARE PARC. Generic are model, methods, and approach. Method is Pairwise Annotation Representation Comparison ( PARC ). ","This paper studies the problem of model selection for transfer learning. The authors propose a new evaluation metric, Pairwise Annotation Representation Comparison (PARC), that compares the performance of source or upstream models with downstream or target data. PARC is based on the observation that when the source model is similar to the target model, the two methods converge to the same solution. The paper also proposes a new benchmark for evaluating source models and shows that PARC outperforms the existing algorithms. Finally, the authors provide a theoretical analysis of the proposed approach."
2766,SP:7520cc1203bb06bbe432e7cc679892e95258ed99,tools EVALUATE-FOR model selection methods. model selection methods COMPARE baselines. baselines COMPARE model selection methods. methods USED-FOR diverse model selection. method COMPARE methods. methods COMPARE method. method USED-FOR diverse model selection. PARC HYPONYM-OF method. Task is scalable diverse model selection ” task. ,"This paper presents a set of tools to evaluate model selection methods on a “scalable diverse model selection” task. The authors compare the performance of the proposed method, PARC, with a number of baselines and show that the proposed methods are better suited for diversemodel selection."
2767,SP:7520cc1203bb06bbe432e7cc679892e95258ed99,"method COMPARE RSA. RSA COMPARE method. dnn USED-FOR probe network. spearman correlation USED-FOR model selection score. RDM USED-FOR model selection score. PARC score HYPONYM-OF model selection score. LEEP CONJUNCTION RSA. RSA CONJUNCTION LEEP. RSA CONJUNCTION DDS. DDS CONJUNCTION RSA. method COMPARE LEEP. LEEP COMPARE method. method COMPARE DDS. DDS COMPARE method. method COMPARE RSA. RSA COMPARE method. fine - tuning accuracy EVALUATE-FOR method. Method are model selection method, and model zoo. Task is Pairwise Annotation Representation Comparison. ","This paper proposes a model selection method based on the idea of Pairwise Annotation Representation Comparison. The proposed method is similar to RSA, except that instead of using dnn as the probe network, the model selection score is based on spearman correlation. The authors also propose to use RDM to compute a new model selection, called PARC score. Experiments on a model zoo show that the proposed method outperforms LEEP, RSA, and DDS in terms of fine-tuning accuracy."
2768,SP:dcdb9c88f61ac3caf3da8255a7953c753cf048d1,"learning schemes USED-FOR compressing high - dimensional neural representations. empirical approach USED-FOR learning schemes. empirical approach USED-FOR compressing high - dimensional neural representations. binary codes USED-FOR compressing high - dimensional neural representations. binary "" codebook "" matrix B(C ) USED-FOR k - dimensional subspace. low - dimensional projector CONJUNCTION binary "" codebook "" matrix B(C ). binary "" codebook "" matrix B(C ) CONJUNCTION low - dimensional projector. d - dimensional output USED-FOR neural architecture. binary "" codebook "" matrix B(C ) PART-OF two - phase algorithm. low - dimensional projector PART-OF two - phase algorithm. L - dimensional vector USED-FOR g(x ). ERM USED-FOR weights. projector CONJUNCTION codebook. codebook CONJUNCTION projector. weights CONJUNCTION projector. projector CONJUNCTION weights. ERM USED-FOR projector. ERM USED-FOR codebook. codebook CONJUNCTION embedding. embedding CONJUNCTION codebook. natural decoding schemes USED-FOR classifiers. classifiers COMPARE baseline approaches. baseline approaches COMPARE classifiers. baseline approaches USED-FOR codebook. ImageNet-1 K USED-FOR classifiers. accuracy COMPARE ResNet50 baseline. ResNet50 baseline COMPARE accuracy. HashNet CONJUNCTION GreedyHash. GreedyHash CONJUNCTION HashNet. application USED-FOR out - of - distribution detection. application EVALUATE-FOR baselines. Method are high - dimensional neural representations, network, and embedding g. Metric is test accuracy. OtherScientificTerm are Boolean vector, B(C ), hash function, and out - of - distribution domain. Generic are algorithm, approach, and applications. Task is image retrieval. ","This paper presents an empirical approach to learning schemes for compressing high-dimensional neural representations into binary codes. The two-phase algorithm consists of a low-dimensional projector, a binary ""codebook"" matrix B(C) that maps a k-dimensional subspace to a Boolean vector, and a neural architecture that takes as input a d-dimensional output. ERM is used to compute the weights, the projector, the codebook, and the embedding g(x) which is an L-dimensional vector. The authors show that using natural decoding schemes, classifiers trained on ImageNet-1K can achieve better test accuracy than baseline approaches. They also show that the proposed algorithm can be applied to the out-of-distribution domain, where the output of the network can be decomposed into a hash function. The proposed approach is evaluated on two applications: 1) image retrieval, where classifiers are trained on HashNet and GreedyHash and the accuracy is compared to the ResNet50 baseline, and 2) application to out-sample detection."
2769,SP:dcdb9c88f61ac3caf3da8255a7953c753cf048d1,two - phase method USED-FOR low - dimensional binary codes. classification task USED-FOR two - phase method. classification task USED-FOR low - dimensional binary codes. surrogate classification task USED-FOR binary codes. Error - Correcting Output Codes approach USED-FOR instance codes. OtherScientificTerm is side - information. ,"This paper proposes a two-phase method for learning low-dimensional binary codes using a classification task. First, binary codes are generated using a surrogate classification task, and second, instance codes are learned using the Error-Correcting Output Codes approach, where side-information is added to the output."
2770,SP:dcdb9c88f61ac3caf3da8255a7953c753cf048d1,"small dimension k USED-FOR k - dimensional binary vector. method USED-FOR codewords. Straight - Through Estimator technique USED-FOR empirical risk minimization. codebook ( low - dimensional bianry code USED-FOR LLC method. Straight - Through Estimator technique USED-FOR LLC method. codebook USED-FOR ECOC framework. algorithm USED-FOR binary classification problems. optimization software USED-FOR binary classification. binary codebook USED-FOR taxonomy visualization. binary code classification USED-FOR applications. system USED-FOR multi - class classification. Method are binary vector representations, lookups, LLC, optimization, and heuristic. OtherScientificTerm are small - dimensional binary codes, side - information, length k vector, and binary code. Metric are accuracy, and classification accuracy. Task are optimization problem, and out - of - distribution detection. ","This paper proposes a method for learning codewords from binary vector representations. The proposed LLC method uses a codebook (low-dimensional bianry code) for empirical risk minimization, and uses the Straight-Through Estimator technique to estimate the k-dimensional binary vector with a small dimension k. The algorithm is applied to binary classification problems, where k is the number of binary codes, and the goal is to maximize the accuracy. The main idea is to use the codebook in the ECOC framework, which is used to generate binary codebook for taxonomy visualization. The lookups are done in a similar way as in LLC, but instead of computing the length k vector, they compute the length of the binary code. The optimization problem is formulated as a linear combination of two steps: (1) compute the side-information, and (2) compute a heuristic to minimize the distance between the output of the heuristic and the original binary code, which can be used for out-of-distribution detection. The authors show that this algorithm can be applied to a variety of binary classification problems, and demonstrate that their algorithm can achieve better classification accuracy than existing optimization software for binary classification. They also show that their system is able to generalize to multi-class classification. "
2771,SP:dcdb9c88f61ac3caf3da8255a7953c753cf048d1,"Low - dimensional binary codes USED-FOR large - scale ml tasks. retrieval HYPONYM-OF large - scale ml tasks. method USED-FOR learning semantially meaningful low - dimensional biary codes. LLC USED-FOR learning semantially meaningful low - dimensional biary codes. learning process USED-FOR sematic structure. LLC USED-FOR class and instance codes. OOD HYPONYM-OF downstream tasks. OtherScientificTerm are semantially meaningful low - dimensional biary codes, and side - information. ","Low-dimensional binary codes have been widely used in large-scale ml tasks such as retrieval. This paper proposes a method called LLC for learning semantially meaningful low-dimensional biary codes. The main idea is to learn a sematic structure during the learning process and then use this side-information to learn the class and instance codes. Experiments on downstream tasks (e.g., OOD) demonstrate the effectiveness of the proposed method."
2772,SP:dcdb9c88f61ac3caf3da8255a7953c753cf048d1,method USED-FOR low - dimensional binary codes. It USED-FOR classification. large scale number of classes FEATURE-OF classification. learning EVALUATE-FOR method. classification accuracy EVALUATE-FOR method. class code USED-FOR intuitive taxonomy. image retrieval CONJUNCTION out - of - distribution detection. out - of - distribution detection CONJUNCTION image retrieval. method USED-FOR out - of - distribution detection. method USED-FOR image retrieval. Material is ImageNet. ,This paper proposes a method for learning low-dimensional binary codes. It is motivated by the problem of classification with large scale number of classes. The method is evaluated in terms of classification accuracy and learning performance on ImageNet. The main contribution of the paper is to propose an intuitive taxonomy based on the class code. The proposed method is applied to image retrieval and out-of-distribution detection.
2773,SP:07def8c80d05f86402ce769313480b30cd99af43,"throughput ( FPS ) CONJUNCTION adversarila robustness. adversarila robustness CONJUNCTION throughput ( FPS ). GDWS USED-FOR regular convolution. error optimization strategy USED-FOR regular convolution. error optimization strategy USED-FOR GDWS. CIFAR-10 CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR-10. SVHN CONJUNCTION ImageNet. ImageNet CONJUNCTION SVHN. CNN's regular conv CONJUNCTION GDWS. GDWS CONJUNCTION CNN's regular conv. Metric are FPS, and adversarial robustness. ","This paper proposes GDWS, an error optimization strategy for regular convolution that aims to improve both throughput (FPS) and adversarila robustness. The authors compare CNN's regular conv and GDWS on CIFAR-10, SVHN, and ImageNet, and show improvements in FPS and adversarial robustness on all three datasets."
2774,SP:07def8c80d05f86402ce769313480b30cd99af43,post - training transformation USED-FOR pre - trained 2D convolutional layer. post - training transformation USED-FOR depth - wise separable convolution operation. minimizing error CONJUNCTION limiting computation complexity. limiting computation complexity CONJUNCTION minimizing error. optimization USED-FOR minimizing error. optimization USED-FOR limiting computation complexity. robustness CONJUNCTION frame per second. frame per second CONJUNCTION robustness. ,This paper proposes a post-training transformation to the pre-trained 2D convolutional layer to make the depth-wise separable convolution operation more efficient. The optimization is aimed at minimizing error and limiting computation complexity. The paper also provides theoretical analysis on robustness and frame per second.
2775,SP:07def8c80d05f86402ce769313480b30cd99af43,real - life hardware FEATURE-OF throughput. robustness EVALUATE-FOR method. throughput EVALUATE-FOR method. ImageNet datasets EVALUATE-FOR GDWS. Method is Generalized Depthwise - Separable ( GDWS ). ,This paper proposes a method called Generalized Depthwise-Separable (GDWS) to improve the robustness and throughput on real-life hardware. The proposed GDWS is evaluated on ImageNet datasets.
2776,SP:07def8c80d05f86402ce769313480b30cd99af43,"error / complexity EVALUATE-FOR algorithms. Method are depthwise - separable convolutions, GDWS, depthwise filters, pointwise convolution, and general convolutions. Metric is natural and robust accuracy. OtherScientificTerm is FPS. ","This paper proposes depthwise-separable convolutions, which is a generalization of GDWS. The main idea is to use depthwise filters instead of pointwise convolution, which allows for both natural and robust accuracy. The error/complexity of the proposed algorithms is shown to be linear in the FPS. The authors also show that general convolutions can be extended to the case where FPS is not linear in FPS."
2777,SP:07def8c80d05f86402ce769313480b30cd99af43,Generalized Depthwise - Separable ( GDWS ) convolutions USED-FOR images. efficiency CONJUNCTION robustness. robustness CONJUNCTION efficiency. channel distribution vectors USED-FOR 2D CNNs. vectorizations USED-FOR convolutional operations. singular value decomposition USED-FOR efficiency. benchmark data EVALUATE-FOR method. ,This paper proposes Generalized Depthwise-Separable (GDWS) convolutions for images. The main idea is to use channel distribution vectors for 2D CNNs to improve efficiency and robustness. The vectorizations are then used for convolutional operations. The efficiency is achieved by singular value decomposition. The proposed method is evaluated on benchmark data.
2778,SP:9e4d04b22ce4f986aabb747a42f40c827073e39e,model USED-FOR semi - template - based modeling. GraphRetro HYPONYM-OF single - step retrosynthesis model. USPTO-50k dataset USED-FOR semi - template - based modeling. synthons CONJUNCTION leaving groups. leaving groups CONJUNCTION synthons. graph neural network USED-FOR GraphRetro. wrong edit predictions CONJUNCTION synthon completions. synthon completions CONJUNCTION wrong edit predictions. Material is USPTO-50k. Generic is baselines. ,"This paper proposes GraphRetro, a single-step retrosynthesis model (GraphRetro) which is a model for semi-template-based modeling on the USPTO-50k dataset. The main idea is to use a graph neural network to learn the graph of the input, and then use the learned model to generate the final output. The authors conduct extensive experiments on several datasets from the standard USPto-50K, and show improvements over the baselines in terms of wrong edit predictions, synthon completions, and leaving groups."
2779,SP:9e4d04b22ce4f986aabb747a42f40c827073e39e,method USED-FOR single - step retrosynthesis prediction. method USED-FOR graph edits. method USED-FOR synthons. message passing network USED-FOR method. leaving groups USED-FOR synthons. message passing network USED-FOR graph edits. MPN USED-FOR leaving groups. ,This paper proposes a method for single-step retrosynthesis prediction. The method uses a message passing network to perform graph edits and to generate synthons using leaving groups generated by MPN.
2780,SP:9e4d04b22ce4f986aabb747a42f40c827073e39e,classification problem USED-FOR pre - computed leaving groups fragments. generation problem USED-FOR completion of incomplete molecular graphs. retrosynthesis EVALUATE-FOR model. Task is molecular graph generation. Material is USPTO-50k. OtherScientificTerm is leaving groups. ,This paper studies the problem of molecular graph generation. The authors propose to solve the generation problem for completion of incomplete molecular graphs by solving a classification problem for pre-computed leaving groups fragments. The proposed model is evaluated on USPTO-50k and on retrosynthesis. The experimental results show that the proposed leaving groups are more effective than the existing methods.
2781,SP:9e4d04b22ce4f986aabb747a42f40c827073e39e,neural networks USED-FOR retrosynthetic prediction model. algorithm USED-FOR prediction. leaving groups USED-FOR synthons. vocabulary of leaving groups USED-FOR synthons. algorithm COMPARE synthon - based retrosynthetic model. synthon - based retrosynthetic model COMPARE algorithm. method COMPARE algorithms. algorithms COMPARE method. ,This paper proposes a retrosynthetic prediction model based on neural networks. The authors propose an algorithm for prediction that uses the vocabulary of leaving groups to generate synthons. The proposed algorithm is shown to outperform the synthon-based retrosyndhetic model. The experiments show that the proposed method outperforms the existing algorithms.
2782,SP:9e4d04b22ce4f986aabb747a42f40c827073e39e,"strategy USED-FOR reactants generation. reactants generation USED-FOR retrosynthesis prediction. strategy USED-FOR retrosynthesis prediction. graph edits USED-FOR synthons. top-1 accuracy EVALUATE-FOR method. OtherScientificTerm are graph topology, chemical reaction, intermediate synthons, and molecule sub - graphs. ","This paper proposes a strategy for reactants generation for retrosynthesis prediction. The key idea is to learn a graph topology for each molecule in a chemical reaction, which is then used to generate intermediate synthons. The synthons are generated by making graph edits to the molecule sub-graphs. The method is evaluated on top-1 accuracy."
2783,SP:772277d969c95924755113c86663fb0e009f24cc,"HR covariate CONJUNCTION LR covariate Y. LR covariate Y CONJUNCTION HR covariate. Bayesian or frequentist approach USED-FOR RKHS. OtherScientificTerm are high - res ( HR ) covariates, GP posterior, and minimax optimal finite sample rates. Generic are datasets, and problem. ","This paper studies the problem of estimating high-res (HR) covariates for high-resolution (LR) and low-res covariates (RKHS) datasets. The authors propose a Bayesian or frequentist approach to RKHS, where the HR covariate and the LR covariate Y are sampled from the GP posterior, and the goal is to find minimax optimal finite sample rates. The problem is formulated in terms of minimizing the difference between the HR and LR covariates of the two datasets. "
2784,SP:772277d969c95924755113c86663fb0e009f24cc,"deconditional mean embeddings ( DMEs ) CONJUNCTION task transformed Gaussian processes ( TTGPs ). task transformed Gaussian processes ( TTGPs ) CONJUNCTION deconditional mean embeddings ( DMEs ). convergence rate EVALUATE-FOR deconditional mean operators. framework USED-FOR deconditional mean embeddings. vector - valued regressors USED-FOR deconditional mean operators ( DMOs ). DMEs USED-FOR refining low resolution spatial fields. high resolution information USED-FOR DMEs. high resolution information FEATURE-OF refining low resolution spatial fields. DMEs CONJUNCTION TTGPs. TTGPs CONJUNCTION DMEs. task dataset CONJUNCTION transformation dataset. transformation dataset CONJUNCTION task dataset. empirical estimator USED-FOR cross - covariance operator. bagged high resolution covariates USED-FOR empirical estimator. conditional mean shrinkage operator HYPONYM-OF conditional mean operator ( CMO ). variational formulation USED-FOR deconditional posterior. OtherScientificTerm are deconditional posteriors, reconstruction operator, low resolution spatial fields, aggregated targets, low resolution covariates, and bags of high resolution covariates. Generic is setup. Task is downscaling of atmospheric temperature ( CMIP6 ). ","This paper proposes a framework for learning deconditional mean embeddings (DMEs) and task transformed Gaussian processes (TTGPs). The authors propose to use vector-valued regressors to learn a class of deconditionally mean operators (DMOs) with a convergence rate of O(1/\sqrt{T}) where O(T) is the reconstruction operator. The authors show that DMEs and TTGPs can be used for refining low resolution spatial fields with high resolution information. The main contribution of the paper is to propose a variational formulation of the decondetric posterior, where the aggregated targets are sampled from low resolution covariates and the bags of highresolution covariates are sampled. The paper also proposes an empirical estimator of the cross-covariance operator, the conditional mean shrinkage operator, which is a conditional mean operator (CMO). Experiments are conducted on a task dataset and a transformation dataset. The experimental results show the effectiveness of the proposed setup on the task of downscaling of atmospheric temperature (CMIP6)."
2785,SP:772277d969c95924755113c86663fb0e009f24cc,"aggregate data USED-FOR Learning models. conditional mean process HYPONYM-OF Gaussian process model. conditional mean process USED-FOR un - aggregated latent function. Gaussian process model CONJUNCTION marginalization operater. marginalization operater CONJUNCTION Gaussian process model. Gaussian process model USED-FOR aggregated data. Generic is dataset. OtherScientificTerm are input - output pairs, and intermediate covariates. ","Learning models from aggregated data is an important problem. This paper proposes to use a Gaussian process model (i.e., conditional mean process) to approximate the un-aggregated latent function. The proposed dataset consists of a set of input-output pairs and the intermediate covariates are sampled from the data. The aggregated training data is modeled using a Gaussian process model and a marginalization operater."
2786,SP:772277d969c95924755113c86663fb0e009f24cc,GP model USED-FOR refining coarse - grained spatial data. GP model USED-FOR aggregated data. two - stage regression algorithm USED-FOR downscaling. synthetic and real - world datasets EVALUATE-FOR model. ,This paper proposes a GP model for refining coarse-grained spatial data. The main idea is to use the GP model to compress aggregated data and then use a two-stage regression algorithm for downscaling. The proposed model is evaluated on both synthetic and real-world datasets.
2787,SP:772277d969c95924755113c86663fb0e009f24cc,Method is Gaussian process approach. Generic is algorithm. OtherScientificTerm is resolution. ,This paper proposes a Gaussian process approach for estimating the resolution of a sequence of points. The algorithm is based on the observation that the resolution can be estimated by minimizing the sum of the difference between the input and the output. The paper also provides a theoretical analysis of the proposed algorithm.
2788,SP:59cfeb59cecac51fecff8f8ceb0266fc6ac22a05,click - through rate CONJUNCTION movie recommendation. movie recommendation CONJUNCTION click - through rate. movie recommendation HYPONYM-OF sparse features. click - through rate HYPONYM-OF sparse features. progressive search algorithm HYPONYM-OF techniques. progressive search algorithm USED-FOR approach. techniques USED-FOR approach. neural architecture search ( NAS ) USED-FOR approach. accuracy CONJUNCTION inference time. inference time CONJUNCTION accuracy. inference time EVALUATE-FOR approach. accuracy EVALUATE-FOR approach. Method is deep sparse networks ( DSNs ). ,"This paper proposes a new approach to deep sparse networks (DSNs) based on neural architecture search (NAS). The approach is based on two techniques: (1) a progressive search algorithm, and (2) the use of sparse features such as click-through rate and movie recommendation. The proposed approach is evaluated in terms of accuracy and inference time."
2789,SP:59cfeb59cecac51fecff8f8ceb0266fc6ac22a05,"NAS USED-FOR Deep Sparse Network ( DSN ) domain. DARTS based method USED-FOR encoding vector. low - rank ones PART-OF vector. Method are DSN, and AutoFIS. OtherScientificTerm are feature - interaction, interaction, search parameters, and search space. Material is DSN domain. ","This paper proposes NAS for the Deep Sparse Network (DSN) domain. The main idea is to use DARTS based method to learn an encoding vector that encodes the feature-interaction between a DSN and the input space. The key idea of the DSN domain is to encode the interaction as a vector of low-rank ones, and then use AutoFIS to map the input vector to the target space. This is done by minimizing the number of search parameters in the search space. "
2790,SP:59cfeb59cecac51fecff8f8ceb0266fc6ac22a05,"method USED-FOR interaction of the feature columns. method USED-FOR CTR prediction application. interaction of the feature columns PART-OF CTR prediction application. application USED-FOR deep sparse networks. iterative algorithm USED-FOR interaction tensor. it COMPARE methods. methods COMPARE it. Criteo, Avazu and ML1 M datasets EVALUATE-FOR it. Criteo, Avazu and ML1 M datasets EVALUATE-FOR methods. ","This paper proposes a method to model the interaction of the feature columns in the CTR prediction application. The proposed application can be applied to deep sparse networks. The authors propose an iterative algorithm to learn the interaction tensor and evaluate it on the Criteo, Avazu and ML1M datasets."
2791,SP:59cfeb59cecac51fecff8f8ceb0266fc6ac22a05,neural architecture search approach USED-FOR deep sparse networks. PROFIT USED-FOR deep sparse networks. PROFIT HYPONYM-OF neural architecture search approach. distilled low - rank search space CONJUNCTION progressive search algorithm. progressive search algorithm CONJUNCTION distilled low - rank search space. progressive search algorithm USED-FOR PROFIT. distilled low - rank search space USED-FOR PROFIT. lower orders USED-FOR progressive search algorithm. benchmark datasets EVALUATE-FOR method. ,"This paper proposes PROFIT, a neural architecture search approach for deep sparse networks. The proposed method is based on a distilled low-rank search space and a progressive search algorithm based on lower orders. The method is evaluated on several benchmark datasets."
2792,SP:59cfeb59cecac51fecff8f8ceb0266fc6ac22a05,NAS USED-FOR deep sparse prediction. low - rank approximation of the full space USED-FOR distilled search space. progressive differentiable search USED-FOR sparse prediction. progressive differentiable search USED-FOR order - priority. order - priority FEATURE-OF sparse prediction. Metric is search cost. Generic is method. ,This paper proposes NAS for deep sparse prediction. The distilled search space is a low-rank approximation of the full space. The authors propose a progressive differentiable search for sparse prediction with order-priority to reduce the search cost. Experiments show the effectiveness of the proposed method.
2793,SP:23a2171eab71c4fd3754791ca2aac9be87411cdb,"algorithm USED-FOR components. regularization HYPONYM-OF components. Method are regularized fine - tuning, and fine - tuning. Metric is PAC - Bayes generalization bound. OtherScientificTerm is noise stability. Generic is model. ","This paper studies the problem of regularized fine-tuning. The authors propose an algorithm that learns two components: (1) regularization and (2) noise stability. The PAC-Bayes generalization bound is established for both of these components, and the authors provide a theoretical analysis of the effect of the two components. The main contribution of the paper is the analysis of how the noise stability of the model is affected by the choice of the regularization. The paper also provides an empirical study of the impact of the choice for the two two components on the final performance of the final model. In particular, the authors show that the regularized version of the algorithm is more robust to noise stability than the non-regularized version. This is an interesting finding, as the authors point out, as it is not clear how much noise stability is lost during fine-tune."
2794,SP:23a2171eab71c4fd3754791ca2aac9be87411cdb,distance - based regularisation USED-FOR fine - tuning. PAC - Bayesian analysis USED-FOR distance - based regularisation. heuristics USED-FOR label noise. means USED-FOR hyperparameters. hyperparameters USED-FOR distance - based regularisation. label noise FEATURE-OF fine - tuning. approaches USED-FOR fine - tuning. method COMPARE approaches. approaches COMPARE method. OtherScientificTerm is constraints. ,"This paper proposes a distance-based regularisation based on PAC-Bayesian analysis for fine-tuning under label noise. The main idea is to use heuristics to deal with label noise, and to use means to choose hyperparameters for the distance. The method is compared to other approaches to fine-tune under label-induced constraints."
2795,SP:23a2171eab71c4fd3754791ca2aac9be87411cdb,regularization method USED-FOR training and fine - tuning large neural networks. maximum change in network parameters USED-FOR fine - tuning. method COMPARE approaches. approaches COMPARE method. Generic is constraint. ,This paper proposes a new regularization method for training and fine-tuning large neural networks. The main idea is to impose a constraint on the maximum change in network parameters during fine-tune. The method is evaluated on several datasets and compared to existing approaches.
2796,SP:23a2171eab71c4fd3754791ca2aac9be87411cdb,"PAC - Bayes bound USED-FOR generalization of a fine - tuned network. label correction CONJUNCTION label removal. label removal CONJUNCTION label correction. layerwise regularization CONJUNCTION label correction. label correction CONJUNCTION layerwise regularization. fine - tuning algorithm USED-FOR image classification tasks. artificially - added label noise FEATURE-OF those. those HYPONYM-OF image classification tasks. Method are neural networks, fine - tuned network, and finetuning. ","This paper studies the generalization of a fine-tuned network under the PAC-Bayes bound. The authors show that neural networks can be finetuned in a way that leads to better generalization. The main contribution of this paper is to study the effect of layerwise regularization, label correction, and label removal. They show that finetuning can lead to improved generalization performance on image classification tasks, especially those with artificially-added label noise. They also provide a theoretical analysis of the finetune effect."
2797,SP:23a2171eab71c4fd3754791ca2aac9be87411cdb,fine - tuning USED-FOR task. model USED-FOR task. model USED-FOR fine - tuning. noise stability FEATURE-OF pre - trained model. noise stability HYPONYM-OF terms. PAC - Bayes generalization bound USED-FOR fine - tuning. algorithm USED-FOR fine - tuning. regularization term PART-OF algorithm. transfer learning CONJUNCTION few shot classification tasks. few shot classification tasks CONJUNCTION transfer learning. transfer learning EVALUATE-FOR algorithm. few shot classification tasks EVALUATE-FOR algorithm. few shot classification tasks HYPONYM-OF benchmark tasks. benchmark tasks EVALUATE-FOR algorithm. transfer learning HYPONYM-OF benchmark tasks. Method is generalization analysis. ,"This paper proposes an algorithm for fine-tuning a pre-trained model for a new task using two terms: (1) noise stability of the pre-trained model, and (2) a PAC-Bayes generalization bound on the performance of the fine -tuned model. The authors also propose a regularization term in the algorithm. The proposed algorithm is evaluated on two benchmark tasks: transfer learning and few shot classification tasks. The generalization analysis is provided."
2798,SP:f2a77f93bdc0401bbd6162a16fba25b9f90530e2,lower bound USED-FOR structured best - arm identification problem. algorithm USED-FOR asymptotic optimality. sample size FEATURE-OF asymptotic optimality. ,This paper proposes a new lower bound for the structured best-arm identification problem. The algorithm is able to achieve asymptotic optimality with a small sample size.
2799,SP:f2a77f93bdc0401bbd6162a16fba25b9f90530e2,"minimum CVaR CONJUNCTION VaR. VaR CONJUNCTION minimum CVaR. VaR FEATURE-OF arm. minimum CVaR USED-FOR arm. \delta - correct algorithm USED-FOR heavy - tailed arm distributions. anytime - valid confidence interval COMPARE truncation - based intervals. truncation - based intervals COMPARE anytime - valid confidence interval. anytime - valid confidence interval USED-FOR CVaR estimation. asymptotic sample complexity EVALUATE-FOR algorithm. OtherScientificTerm are asymptotic lower bound, and delta. ","This paper proposes a \delta-correct algorithm for heavy-tailed arm distributions, where the minimum CVaR and the VaR of an arm are assumed to be close to each other. The authors provide an asymptotic lower bound on the number of samples required for the algorithm to converge to the optimal delta. They also provide a theoretical analysis of the asymmetric sample complexity of the algorithm. Finally, they show that the anytime-valid confidence interval for CVAR estimation is equivalent to truncation-based intervals."
2800,SP:f2a77f93bdc0401bbd6162a16fba25b9f90530e2,"multi armed bandits USED-FOR arm. VaR FEATURE-OF arm. problem USED-FOR finance and clinical trials. problem USED-FOR risk sensitivity. arm identification USED-FOR problem. bounded $ ( 1+\epsilon)$ moments FEATURE-OF distributions. exploratory transform USED-FOR arm sampling distribution. exploratory transform PART-OF algorithm. empirical reward distribution USED-FOR arm. empirical minimum CVaR FEATURE-OF arm. algorithm USED-FOR arms. termination condition USED-FOR algorithm. optimal sample complexity EVALUATE-FOR algorithm. asymptotic convergence FEATURE-OF track - and - stop exploration protocol. track - and - stop exploration protocol USED-FOR algorithm. asymptotic convergence EVALUATE-FOR algorithm. OtherScientificTerm are mean, arm reward distributions, canonical SPEF, empirical distribution, error threshold, Bernoulli arms, and error parameter delta. Task are parametric case, VaR problem, and CVaR problem. Method is GLRT. ","This paper studies the problem of identifying an arm in multi armed bandits. This problem is well-motivated in finance and clinical trials, where the goal is to minimize risk sensitivity. The problem is formulated as an arm identification problem where the mean and variance of the arm reward distributions are assumed to be bounded $(1+\epsilon)$ moments. In the parametric case, the authors consider a variant of the VaR problem in which the arm is assumed to have VaR. The authors propose an algorithm that combines an exploratory transform to the arm sampling distribution and a canonical SPEF to approximate the empirical distribution. The algorithm is trained to identify arms that have an empirical minimum CVaR, and the authors show that the algorithm converges to the optimal sample complexity under a termination condition. The asymptotic convergence of the algorithm is shown using a track-and-stop exploration protocol, which is similar to GLRT, except that the error threshold is replaced by Bernoulli arms. The error parameter delta is also shown to be the same as in GLRT."
2801,SP:f2a77f93bdc0401bbd6162a16fba25b9f90530e2,weighted sum of CVaR CONJUNCTION mean. mean CONJUNCTION weighted sum of CVaR. CVaR CONJUNCTION weighted sum of CVaR. weighted sum of CVaR CONJUNCTION CVaR. multi - armed bandit best arm identification problem USED-FOR arm. VaR CONJUNCTION CVaR. CVaR CONJUNCTION VaR. VaR FEATURE-OF minimum tail risk. minimum tail risk FEATURE-OF arm. CVaR HYPONYM-OF minimum tail risk. lower bound USED-FOR problem. optimal $ \delta$-correct algorithm COMPARE lower bound. lower bound COMPARE optimal $ \delta$-correct algorithm. arm distributions USED-FOR CVaR. sample complexity EVALUATE-FOR optimal $ \delta$-correct algorithm. time computational complexity CONJUNCTION sample complexity. sample complexity CONJUNCTION time computational complexity. Generic is solution. ,"This paper studies the multi-armed bandit best arm identification problem, where each arm is represented by a weighted sum of CVaR and a mean. The minimum tail risk of an arm is defined as the sum of VaR, CVAR, and the mean. A lower bound for the problem is derived, and a solution is proposed. Experiments show that the optimal $\delta$-correct algorithm outperforms the lower bound in terms of both time computational complexity and sample complexity. The paper also provides a theoretical analysis of the effect of the choice of arm distributions on the performance of the optimal $\delta$."
2802,SP:f2a77f93bdc0401bbd6162a16fba25b9f90530e2,fixed confidence setting FEATURE-OF CVaR arm identification. conic combinations of mean and CVaR CONJUNCTION VaR based criteria. VaR based criteria CONJUNCTION conic combinations of mean and CVaR. They USED-FOR conic combinations of mean and CVaR. They USED-FOR VaR based criteria. lower bound CONJUNCTION algorithm. algorithm CONJUNCTION lower bound. expected sample complexity EVALUATE-FOR algorithm. OtherScientificTerm is reals. ,"This paper studies CVaR arm identification in a fixed confidence setting. They consider conic combinations of mean and CVaRs, as well as VaR based criteria. They provide a lower bound on the expected sample complexity and an algorithm that can be applied to reals."
2803,SP:765942c86da1594b33268df6d0d15c682bc7eaa6,inductive bias ( IB ) USED-FOR vision tasks. vision tasks USED-FOR vision transformers ( ViT ). inductive bias ( IB ) USED-FOR vision transformers ( ViT ). reduction cell CONJUNCTION parallel convolution branch. parallel convolution branch CONJUNCTION reduction cell. dilation FEATURE-OF parallel convolution branches. reduction cell USED-FOR features. dilation USED-FOR reduction cell. ImageNet EVALUATE-FOR baselines. Task is downstream classification tasks. ,This paper proposes to use inductive bias (IB) to train vision transformers (ViT) on vision tasks. The main idea is to use a reduction cell and a parallel convolution branch that are trained with dilation. The features extracted from the reduction cell are then used to train the features on downstream classification tasks. Experiments on ImageNet show improvements over baselines.
2804,SP:765942c86da1594b33268df6d0d15c682bc7eaa6,reduction cell ( RC ) CONJUNCTION normal cell ( NC ). normal cell ( NC ) CONJUNCTION reduction cell ( RC ). basic cells USED-FOR vanilla vision transformer structure. normal cell ( NC ) HYPONYM-OF basic cells. reduction cell ( RC ) HYPONYM-OF basic cells. NCs USED-FOR locality and global dependencies. locality and global dependencies FEATURE-OF token sequence. paralleled attention module CONJUNCTION convolutional layers. convolutional layers CONJUNCTION paralleled attention module. feed - forward network ( FFN ) USED-FOR cells. convolutional layers CONJUNCTION feed - forward network ( FFN ). feed - forward network ( FFN ) CONJUNCTION convolutional layers. paralleled attention module CONJUNCTION feed - forward network ( FFN ). feed - forward network ( FFN ) CONJUNCTION paralleled attention module. paralleled attention module USED-FOR cells. downstream tasks EVALUATE-FOR network design. ImageNet EVALUATE-FOR network design. ImageNet CONJUNCTION downstream tasks. downstream tasks CONJUNCTION ImageNet. OtherScientificTerm is RCs. ,"This paper proposes to replace the vanilla vision transformer structure with two basic cells: a reduction cell (RC) and a normal cell (NC). The NCs are designed to capture both locality and global dependencies in the token sequence. The cells are trained with a paralleled attention module, convolutional layers, and a feed-forward network (FFN). The proposed network design is evaluated on ImageNet and downstream tasks."
2805,SP:765942c86da1594b33268df6d0d15c682bc7eaa6,"local features encoding CONJUNCTION scale invariance. scale invariance CONJUNCTION local features encoding. inductive biases USED-FOR images. vision transformer model USED-FOR inductive biases. scale invariance HYPONYM-OF images. local features encoding HYPONYM-OF images. local features encoding HYPONYM-OF inductive biases. large - scale training data USED-FOR It. architecture USED-FOR inductive biases. pyramid effect FEATURE-OF reduction cell. dilated convolutions USED-FOR pyramid effect. dilated convolutions USED-FOR reduction cell. Convolution CONJUNCTION attention. attention CONJUNCTION Convolution. attention USED-FOR global relationships. convolution USED-FOR local relationships. pyramid structure USED-FOR scale invariance. pyramid structure CONJUNCTION attention. attention CONJUNCTION pyramid structure. diluted convolutions USED-FOR pyramid structure. Method are Data Efficient ViT, and pertained CNN. OtherScientificTerm is normal cell. ","This paper proposes a vision transformer model to learn inductive biases for images, such as local features encoding and scale invariance. It is based on large-scale training data and is trained on large datasets. The proposed architecture is able to learn the inductive bias for two types of images: 1) Local features encoding, 2) Scale invariance and 3) Data Efficient ViT. The pyramid effect of the reduction cell is mitigated by dilated convolutions to reduce the pyramid effect in the normal cell. Convolution and attention are used to model global relationships, while convolution is used to represent local relationships, and attention is used for global relationships. The paper also proposes a pertained CNN, which is trained in a similar fashion to the original pertained network."
2806,SP:765942c86da1594b33268df6d0d15c682bc7eaa6,architecture USED-FOR intrinsic inductive bias IB. architecture USED-FOR vision transformer architecture VITAE. intrinsic inductive bias IB USED-FOR vision transformer architecture VITAE. convolutional dillated layers USED-FOR architecture. architecture USED-FOR scale and locality IB. Generic is method. ,This paper proposes a new architecture for intrinsic inductive bias IB for vision transformer architecture VITAE. The proposed architecture is based on convolutional dillated layers. The authors claim that the proposed architecture can improve scale and locality IB. The method is evaluated on several datasets.
2807,SP:765942c86da1594b33268df6d0d15c682bc7eaa6,locality CONJUNCTION scale - invariance. scale - invariance CONJUNCTION locality. architecture changes USED-FOR inductive biases. scale - invariance HYPONYM-OF inductive biases. locality HYPONYM-OF inductive biases. scale - invariance HYPONYM-OF architecture changes. Reduction Cells CONJUNCTION Normal Cells. Normal Cells CONJUNCTION Reduction Cells. Reduction Cells USED-FOR It. CNN baselines COMPARE Transformer - based ImageNet models. Transformer - based ImageNet models COMPARE CNN baselines. Transformer - based ImageNet models COMPARE ViTAE model. ViTAE model COMPARE Transformer - based ImageNet models. Top-1 accuracy EVALUATE-FOR ViTAE model. ,"This paper proposes two architecture changes to improve the inductive biases, namely locality and scale-invariance. It uses Reduction Cells and Normal Cells. The experimental results show that the proposed ViTAE model achieves better Top-1 accuracy than CNN baselines and Transformer-based ImageNet models."
2808,SP:5e3572a386f890c5864437985cf63b13844f338f,adversarial fine - tuning method USED-FOR language model fine - tuning. information - theoretic perspective USED-FOR method. ablation study USED-FOR hyperparameters. approach USED-FOR hyperparameters. Method is RIFT. Generic is baselines. ,"This paper proposes an adversarial fine-tuning method for language model fine-tune. The proposed method is motivated from an information-theoretic perspective. The authors propose an ablation study to evaluate the hyperparameters of the proposed approach, RIFT. The experimental results show improvements over the baselines."
2809,SP:5e3572a386f890c5864437985cf63b13844f338f,adversarial training USED-FOR fine - tuning. adversarial training USED-FOR pre - trained language model. fine - tuning USED-FOR pre - trained language model. Informative Fine - Tuning ( RIFT ) USED-FOR model. pre - trained model USED-FOR features. method COMPARE baseline models. baseline models COMPARE method. sentiment analysis and natural language inference tasks EVALUATE-FOR method. sentiment analysis and natural language inference tasks EVALUATE-FOR baseline models. Genetic and PWWS attacks EVALUATE-FOR method. Genetic and PWWS attacks EVALUATE-FOR baseline models. OtherScientificTerm is catastrophic forgetting. ,This paper proposes to use adversarial training for fine-tuning of a pre-trained language model. Informative Fine-Tuning (RIFT) is used to fine-tune the model to avoid catastrophic forgetting. The authors also propose to use the features from the pre-trained model to improve the performance. The proposed method is evaluated on sentiment analysis and natural language inference tasks and compared to baseline models on Genetic and PWWS attacks.
2810,SP:5e3572a386f890c5864437985cf63b13844f338f,pre - trained language models USED-FOR adversarial robustness. objective model COMPARE pre - trained model. pre - trained model COMPARE objective model. adversarial training HYPONYM-OF robust training methods. RIFT HYPONYM-OF fine - tuning method. fine - tuning method USED-FOR mutual information. information - theoretical perspective USED-FOR RIFT. RIFT COMPARE baselines. baselines COMPARE RIFT. baselines USED-FOR generalization. RIFT USED-FOR generalization. I(S;T|Y ) COMPARE I(S;T ). I(S;T ) COMPARE I(S;T|Y ). Task is catastrophic forgetting problem. ,"This paper studies adversarial robustness of pre-trained language models. In particular, the authors focus on the catastrophic forgetting problem and propose two robust training methods: (1) adversarial training, where the objective model is trained to be more robust than the pre-pre-trained model, and (2) mutual information, which is a fine-tuning method that aims to maximize mutual information between the two models. RIFT is an information-theoretical perspective, and the authors show that RIFT can improve generalization over baselines. The authors also show that I(S;T|Y) is better than I(T|T) in terms of generalization performance."
2811,SP:5e3572a386f890c5864437985cf63b13844f338f,method USED-FOR finetuning pretrained NLP models. finetuning USED-FOR catastrophic forgetting. finetuning COMPARE finetuning of models. finetuning of models COMPARE finetuning. pretrained weights USED-FOR features. accuracy EVALUATE-FOR finetuning. Generic is model. Method is regularizing adversarial finetuning. ,"This paper proposes a method for finetuning pretrained NLP models to prevent catastrophic forgetting. The authors argue that the proposed method, called regularizing adversarial finetune, can be used to improve the performance of the model by regularizing the weights of the features learned by the pretrained weights. The experiments show that the benefits of regularizing weights can be seen by comparing the accuracy of the finetuned models with the original training data."
2812,SP:5e3572a386f890c5864437985cf63b13844f338f,them USED-FOR adversarial attacks. pretrained NLP models USED-FOR adversarial fine - tuning. conditional mutual information FEATURE-OF feature vectors. RIFT USED-FOR adversarial training. regularization term USED-FOR conditional mutual information. pretrained model USED-FOR feature vectors. regularization term USED-FOR RIFT. RIFT COMPARE baselines methods. baselines methods COMPARE RIFT. adversarial robustness EVALUATE-FOR baselines methods. L2 penalty CONJUNCTION regularizer. regularizer CONJUNCTION L2 penalty. adversarial robustness EVALUATE-FOR RIFT. Generic is method. OtherScientificTerm is catastrophic forgetting. ,This paper proposes a method called RIFT for adversarial fine-tuning of pretrained NLP models to make them more robust to adversarial attacks. RIFT uses a regularization term that penalizes the conditional mutual information between the feature vectors of the pretrained model and the target model during adversarial training to prevent catastrophic forgetting. Experiments show that RIFT outperforms the baselines methods in terms of adversarial robustness when combined with an L2 penalty and a regularizer.
2813,SP:167a8b7e0173bffc5f08a9c2f378fe7bdf837da3,Stochastic Anderson Mixing ( SAM ) scheme USED-FOR nonconvex optimization problems. SAM CONJUNCTION SAM. SAM CONJUNCTION SAM. SAM CONJUNCTION variance reduction. variance reduction CONJUNCTION SAM. smoothness CONJUNCTION unbiased gradient estimate. unbiased gradient estimate CONJUNCTION smoothness. bounded variance assumptions FEATURE-OF unbiased gradient estimate. SAM HYPONYM-OF scheme. SAM HYPONYM-OF scheme. CIFAR CONJUNCTION Penn Treebank. Penn Treebank CONJUNCTION CIFAR. MNIST CONJUNCTION CIFAR. CIFAR CONJUNCTION MNIST. MNIST EVALUATE-FOR baselines. Penn Treebank EVALUATE-FOR baselines. CIFAR EVALUATE-FOR baselines. OtherScientificTerm is asymptotic and non - asymptotic convergence. ,"This paper proposes a Stochastic Anderson Mixing (SAM) scheme for solving nonconvex optimization problems. SAM is a general scheme that combines SAM with variance reduction. The authors provide asymptotic and non-asymptotic convergence results for both smoothness and unbiased gradient estimate under bounded variance assumptions. Experimental results on MNIST, CIFAR, and Penn Treebank are provided to compare with baselines."
2814,SP:167a8b7e0173bffc5f08a9c2f378fe7bdf837da3,stochastic Anderson mixing method USED-FOR non - convex stochastic optimization problems. method USED-FOR deep learning problems. Method is convergence theory. Generic is methods. ,"This paper proposes a stochastic Anderson mixing method for solving non-convex stochastically optimization problems. The method is well-motivated by convergence theory and has been applied to deep learning problems in the past. However, the proposed methods are computationally expensive. "
2815,SP:167a8b7e0173bffc5f08a9c2f378fe7bdf837da3,Anderson mixing USED-FOR stochastic optimization problems. Anderson mixing USED-FOR neural networks. strategies USED-FOR noise and uncertainty. variance reduction CONJUNCTION preconditioned mixing. preconditioned mixing CONJUNCTION variance reduction. adaptive regularization CONJUNCTION variance reduction. variance reduction CONJUNCTION adaptive regularization. Anderson mixing USED-FOR stochastic settings. damped projection CONJUNCTION adaptive regularization. adaptive regularization CONJUNCTION damped projection. strategies USED-FOR Anderson mixing. preconditioned mixing PART-OF strategies. damped projection HYPONYM-OF strategies. adaptive regularization HYPONYM-OF strategies. variance reduction HYPONYM-OF strategies. ,"This paper studies the problem of Anderson mixing in stochastic optimization problems, which is an important problem for neural networks. The authors propose three strategies to mitigate the noise and uncertainty in Anderson mixing: damped projection, adaptive regularization, variance reduction, and preconditioned mixing. The main contribution of this paper is to study the effect of different strategies on the Anderson mixing performance in the standard and more complex settings, and to show that Anderson mixing can be applied to more complex and challenging settings."
2816,SP:167a8b7e0173bffc5f08a9c2f378fe7bdf837da3,"algorithms USED-FOR variance reduction. Method are stochastic Anderson Mixing algorithm, and convergence analysis. Task is theoretical convergence analysis. ","This paper studies the stochastic Anderson Mixing algorithm. The authors propose algorithms for variance reduction and provide theoretical convergence analysis. The paper is well-written and easy to follow. However, the convergence analysis is not well-motivated. "
2817,SP:167a8b7e0173bffc5f08a9c2f378fe7bdf837da3,Anderson acceleration USED-FOR fixed point iterations. Anderson acceleration USED-FOR non convex optimization problem. deterministic   version USED-FOR algorithm. O(epsilon^{-2 } ) sample complexity EVALUATE-FOR algorithm. Method is SAM. ,"This paper studies the problem of Anderson acceleration for fixed point iterations in a non convex optimization problem. The authors propose a deterministic  version of the proposed algorithm, SAM, which achieves O(epsilon^{-2}) sample complexity."
2818,SP:fe9c80cc5615705ef844d59b56413779c8d54a06,noise USED-FOR measurement matrix. method USED-FOR linear inverse problem. annealed Langevin dynamics framework USED-FOR black - box prior. annealed Langevin dynamics framework USED-FOR score function. Langevin noise term USED-FOR conditional score function. heuristic USED-FOR step size calculation. Newton's method USED-FOR optimization. Newton's method USED-FOR heuristic. super resolution CONJUNCTION compressive sensing. compressive sensing CONJUNCTION super resolution. image deblurring CONJUNCTION super resolution. super resolution CONJUNCTION image deblurring. linear inverse problems EVALUATE-FOR method. image deblurring HYPONYM-OF linear inverse problems. compressive sensing HYPONYM-OF linear inverse problems. super resolution HYPONYM-OF linear inverse problems. imposed statistical model FEATURE-OF residual. Method is minimum MSE Gaussian denoiser. Generic is task. ,"This paper proposes a method for solving a linear inverse problem. The score function is based on an annealed Langevin dynamics framework as a black-box prior. The Langevin noise term is used to approximate the conditional score function. The noise is used as a measurement matrix. A heuristic is used for the step size calculation based on Newton's method for optimization. The residual is modeled as an imposed statistical model and the minimum MSE Gaussian denoiser is used. The method is evaluated on linear inverse problems such as image deblurring, super resolution, and compressive sensing. The results show that the proposed method can achieve state-of-the-art performance on the task."
2819,SP:fe9c80cc5615705ef844d59b56413779c8d54a06,algorithm USED-FOR noisy linear inverse problems. SNIPS USED-FOR noisy linear inverse problems. approximate MMSE estimator USED-FOR algorithm. pre - trained Gaussian denoisers USED-FOR approximate MMSE estimator. synthetically annealed version of Langevin dynamics USED-FOR MCMC sampling. MCMC sampling USED-FOR estimator. ,This paper proposes an algorithm called SNIPS for solving noisy linear inverse problems using an approximate MMSE estimator based on pre-trained Gaussian denoisers. The estimator is based on MCMC sampling based on a synthetically annealed version of Langevin dynamics.
2820,SP:fe9c80cc5615705ef844d59b56413779c8d54a06,"Langevin dynamics USED-FOR conditional sampling. Langevin dynamics USED-FOR conditional sampling. annealed Langevin dynamics USED-FOR convergence. OtherScientificTerm are annealing, annealing noise, likelihood distribution, and functional form. Generic is estimator. ",This paper studies the convergence of the annealed Langevin dynamics for conditional sampling. The main idea is to use annealing to make the estimator more robust to the presence or absence of an additional noise. This is achieved by adding an additional term to the likelihood distribution that encourages the likelihood to take a functional form. 
2821,SP:fe9c80cc5615705ef844d59b56413779c8d54a06,technique USED-FOR noisy linear inverse problems. annealed Langevin dynamics USED-FOR technique. spectral space FEATURE-OF linear degradation operator. technique USED-FOR Langevin dynamics. hessian USED-FOR coordinate wise step sizes. linear degradation operator FEATURE-OF Langevin dynamics. spectral space FEATURE-OF Langevin dynamics. coordinate wise step sizes USED-FOR method. inpainting CONJUNCTION super - résolution. super - résolution CONJUNCTION inpainting. compress sensing CONJUNCTION inpainting. inpainting CONJUNCTION compress sensing. method USED-FOR compress sensing. method USED-FOR inpainting. method USED-FOR super - résolution. ,"This paper proposes a technique for solving noisy linear inverse problems based on annealed Langevin dynamics. The proposed technique can be applied to Langevin Dynamics in spectral space with a linear degradation operator. The method uses coordinate wise step sizes based on the hessian. Experiments on compress sensing, inpainting, and super-resolution show the effectiveness of the proposed method."
2822,SP:fe9c80cc5615705ef844d59b56413779c8d54a06,approach USED-FOR noisy inverse problems. approach USED-FOR noisy inverse problems. super - resolution CONJUNCTION compressive sensing. compressive sensing CONJUNCTION super - resolution. image deblurring CONJUNCTION super - resolution. super - resolution CONJUNCTION image deblurring. image deblurring HYPONYM-OF noisy inverse problems. compressive sensing HYPONYM-OF noisy inverse problems. super - resolution HYPONYM-OF noisy inverse problems. synthetic data EVALUATE-FOR approach. ,"This paper presents an approach to solve noisy inverse problems, such as image deblurring, super-resolution, and compressive sensing. The approach is evaluated on synthetic data."
2823,SP:b04caddcb2dc9e9b365a76fdbf3d3eb4efcdffd9,social media data USED-FOR detecting online drug trafficking. meta - learning algorithm USED-FOR model optimization. Method is graph convolutional networks. Material is social media. ,"This paper studies the problem of detecting online drug trafficking from social media data. The authors propose a meta-learning algorithm for model optimization, which is based on graph convolutional networks. Experiments on social media show promising results."
2824,SP:b04caddcb2dc9e9b365a76fdbf3d3eb4efcdffd9,"post content CONJUNCTION relational structure information. relational structure information CONJUNCTION post content. heterogeneous graph USED-FOR drug trafficking system. social media FEATURE-OF relational structure information. relational structure information USED-FOR heterogeneous graph. post content USED-FOR heterogeneous graph. graph structure learning PART-OF it. meta - learning CONJUNCTION knowledge distillation. knowledge distillation CONJUNCTION meta - learning. meta - learning USED-FOR model parameters. it USED-FOR drug traffickers. knowledge distillation USED-FOR model parameters. it USED-FOR model parameters. knowledge distillation USED-FOR it. social media USED-FOR drug traffickers. meta - learning USED-FOR it. it EVALUATE-FOR framework. Method are MetaHG, and HG. ","This paper proposes MetaHG, a framework that combines graph structure learning with meta-learning. Specifically, it combines post content with relational structure information from social media to form a heterogeneous graph for a drug trafficking system. The authors show that it can be used to train drug traffickers using social media, and that it is able to learn model parameters using meta - learning and knowledge distillation. Experiments on HG show that the proposed framework is effective, and it is evaluated on a variety of datasets."
2825,SP:b04caddcb2dc9e9b365a76fdbf3d3eb4efcdffd9,limited labeled samples USED-FOR model training. model USED-FOR challenges. sparse graph structure HYPONYM-OF challenges. Instagram USED-FOR large social media dataset. Method is heterogeneous graph learning model. ,"This paper proposes a heterogeneous graph learning model. The idea is to use limited labeled samples for model training. The model is designed to tackle challenges such as sparse graph structure. Experiments are conducted on Instagram, a large social media dataset."
2826,SP:b04caddcb2dc9e9b365a76fdbf3d3eb4efcdffd9,MetaHG USED-FOR illicit drug trafficker detection. R - GCN USED-FOR illicit drug trafficker detection. social media USED-FOR illicit drug trafficker detection. R - GCN USED-FOR MetaHG. methods USED-FOR MetaHG. self - supervised learning(AS - SSL ) CONJUNCTION knowledge distillation(KD ). knowledge distillation(KD ) CONJUNCTION self - supervised learning(AS - SSL ). metrics EVALUATE-FOR models. meta learning(MAML ) CONJUNCTION self - supervised learning(AS - SSL ). self - supervised learning(AS - SSL ) CONJUNCTION meta learning(MAML ). knowledge distillation(KD ) USED-FOR MetaHG. self - supervised learning(AS - SSL ) USED-FOR MetaHG. meta learning(MAML ) HYPONYM-OF methods. knowledge distillation(KD ) HYPONYM-OF methods. self - supervised learning(AS - SSL ) HYPONYM-OF methods. dataset USED-FOR task. Instagram USED-FOR dataset. ,"This paper proposes MetaHG, an extension of R-GCN for the task of illicit drug trafficker detection on social media. The authors propose three methods to improve the performance of metaHG: meta learning(MAML), self-supervised learning(AS-SSL), and knowledge distillation(KD). The authors evaluate their models on a variety of metrics and compare their performance with other models. They also propose a new dataset based on Instagram for this task."
2827,SP:b04caddcb2dc9e9b365a76fdbf3d3eb4efcdffd9,"model USED-FOR drug - trafficker profiles. Instagram USED-FOR drug - trafficker profiles. users CONJUNCTION keywords. keywords CONJUNCTION users. keywords CONJUNCTION posts. posts CONJUNCTION keywords. multiple regularizer terms PART-OF algorithm. keywords USED-FOR heterogeneous graph. users USED-FOR heterogeneous graph. posts USED-FOR heterogeneous graph. model COMPARE baselines. baselines COMPARE model. dataset EVALUATE-FOR model. user visualization CONJUNCTION case study. case study CONJUNCTION user visualization. user visualization HYPONYM-OF experiments. case study HYPONYM-OF experiments. OtherScientificTerm are regular cross - entropy loss, and graph. Method are node representations, and out - of - domain model. ","This paper proposes a model for drug-trafficker profiles on Instagram. The algorithm consists of multiple regularizer terms: 1) a regular cross-entropy loss that encourages the node representations to be similar, 2) an out-of-domain model that models the graph as a heterogeneous graph of users, keywords, and posts. The model is evaluated on a large dataset and compared to several baselines. Experiments include user visualization and a case study."
2828,SP:242da1384f48260d58a0e7949438611c05079197,"ReLU activation USED-FOR neural networks. neural networks USED-FOR representation of continuous functions. ReLU activation USED-FOR representation of continuous functions. depth - d neural net USED-FOR class of functions. architectures USED-FOR function classes. ReLU nets USED-FOR continuous piecewise linear functions. ReLU net USED-FOR CPWL. ReLU nets USED-FOR ReLU(d ). representational capabilities EVALUATE-FOR ReLU nets. polyhedral complexes USED-FOR CPWL functions. networks USED-FOR CPWL functions. Method are neural nets, depth - d+1 neural nets, and depth O(logn ) nets. OtherScientificTerm are layers, width constraints, hidden layers, max functions, max function, ReLU(k ), and max. ","This paper studies the use of ReLU activation in neural networks for learning the representation of continuous functions. In particular, the authors consider the case where the depth-d neural net is trained to represent a class of functions that are continuous piecewise linear (CPWL) and ReLU(d). The authors show that ReLU nets can be used to represent continuous pieceswise linear functions, and propose two architectures for these function classes. First, they show that a ReLU net for CPWL can be trained to approximate the ReLU of d with respect to the width constraints (i.e., the width of the hidden layers). Second, they demonstrate that depth O(logn) neural nets can also be trained in a similar way, but with width constraints on the number of hidden layers. Finally, they compare the representational capabilities of these two architectures and show that the networks are able to approximate a variety of CPWW functions with polyhedral complexes. The authors also show that depth-D+1 neural nets are not limited to the case of max functions, but can be extended to max functions as well. The max function can be represented as a function of depth d, and the max function is represented by a function that is a linear combination of the width and depth of the layers. The paper also shows that the max can be approximated by ReLU (k) and the width is bounded by the max."
2829,SP:242da1384f48260d58a0e7949438611c05079197,"depth USED-FOR representing real functions. ReLU networks USED-FOR depth. ReLU networks USED-FOR representing real functions. OtherScientificTerm are real functions, and expressivity. Method is neural nets. ",This paper studies the depth of ReLU networks for representing real functions. The authors argue that the depth is important for the expressivity of neural nets.
2830,SP:242da1384f48260d58a0e7949438611c05079197,"ReLU networks USED-FOR continuous function. hidden layer PART-OF ReLU networks. ReLU network USED-FOR piecewise continuous functions. layers PART-OF network. finite depth FEATURE-OF ReLU network functions. Method are universal approximation theorem, and approximations. OtherScientificTerm are hidden layers, describable functions, candidate function, and piecewise linear function. Generic is networks. ","This paper studies the universal approximation theorem, which states that ReLU networks can approximate any continuous function with a single hidden layer. The authors prove that the ReLU network can approximate piecewise continuous functions with the same number of hidden layers. This is an interesting result, as it is known that the number of layers in the network is finite, and that the network functions can be approximated with finite depth. In particular, the authors show that the describable functions of the network are equivalent to the candidate function, which is a piecewise linear function. They also show that these approximations can be used to approximate functions that are not describable by the networks."
2831,SP:242da1384f48260d58a0e7949438611c05079197,"exact representations USED-FOR ReLU networks. depth logarithmic in dimension USED-FOR piecewise linear functions. relu network USED-FOR affine function. OtherScientificTerm are continuous piecewise linear functions, Max(2^k ), and max of 2^k affine functions. Method is depth k network. ","This paper studies exact representations for ReLU networks. The authors consider continuous piecewise linear functions with depth logarithmic in dimension, where depth k is the number of pieces. Max(2^k) is the max of 2^k affine functions, and the authors show that a depth k network is equivalent to a relu network for an affine function."
2832,SP:242da1384f48260d58a0e7949438611c05079197,ReLU activations USED-FOR fully connected neural network. fully connected neural network USED-FOR class of functions. hidden layers PART-OF fully connected network. NN USED-FOR continuous piecewise linear functions. ,This paper proposes a fully connected neural network with ReLU activations for a class of functions. The fully connected network consists of two hidden layers. The NN is trained for continuous piecewise linear functions.
2833,SP:8d5741aedf3125e0e790a58ec3ce81a4e2ea4dcb,min - max operation USED-FOR adversarial attack settings. universal adversarial attacks CONJUNCTION robust attacks. robust attacks CONJUNCTION universal adversarial attacks. method USED-FOR generating model ensemble attacks. generating model ensemble attacks CONJUNCTION universal adversarial attacks. universal adversarial attacks CONJUNCTION generating model ensemble attacks. method USED-FOR robust attacks. method USED-FOR universal adversarial attacks. data transformations USED-FOR robust attacks. method USED-FOR applications. ,"This paper proposes a min-max operation for adversarial attack settings. The proposed method is applied to the problem of generating model ensemble attacks, universal adversarial attacks, and robust attacks with data transformations. The experiments show the effectiveness of the proposed method in various applications."
2834,SP:8d5741aedf3125e0e790a58ec3ce81a4e2ea4dcb,"universal attack CONJUNCTION robust attack. robust attack CONJUNCTION universal attack. model ensemble attack CONJUNCTION universal attack. universal attack CONJUNCTION model ensemble attack. convergence rate CONJUNCTION attacking ability. attacking ability CONJUNCTION convergence rate. universal attack CONJUNCTION robust attack. robust attack CONJUNCTION universal attack. ensemble attack CONJUNCTION universal attack. universal attack CONJUNCTION ensemble attack. robust attack HYPONYM-OF tasks. ensemble attack HYPONYM-OF tasks. universal attack HYPONYM-OF tasks. Method are APGDA attack method, and APGDA. ","This paper proposes an APGDA attack method. The authors study the convergence rate, the attacking ability, and the generalization properties of APGADA. The paper considers three different tasks: model ensemble attack, universal attack, and robust attack. The main contribution of the paper is to provide a theoretical analysis of the convergence and attack properties of the APGA. "
2835,SP:8d5741aedf3125e0e790a58ec3ce81a4e2ea4dcb,min - max framework USED-FOR problems. Universal Perturbation CONJUNCTION adversarial attack. adversarial attack CONJUNCTION Universal Perturbation. Ensemble attack over multiple models CONJUNCTION Universal Perturbation. Universal Perturbation CONJUNCTION Ensemble attack over multiple models. Ensemble attack over multiple models HYPONYM-OF problems. adversarial attack HYPONYM-OF problems. Universal Perturbation HYPONYM-OF problems. first - order approach USED-FOR problem. min - max approach COMPARE empirical risk minimization. empirical risk minimization COMPARE min - max approach. ,"This paper proposes a min-max framework for solving problems such as Ensemble attack over multiple models, Universal Perturbation, and adversarial attack. The authors propose a first-order approach to solve the problem and show that the min-min approach outperforms empirical risk minimization."
2836,SP:8d5741aedf3125e0e790a58ec3ce81a4e2ea4dcb,min - max problem USED-FOR multi - domain robust optimization. min - max problem USED-FOR generating adversarial attacks. min - max formulation USED-FOR attacks. attacks USED-FOR ensemble of models. min - max formulation USED-FOR ensemble of models. min - max USED-FOR adversarial attacks scenarios. min - max USED-FOR robust models. robust models USED-FOR multiple $ \ell_p$ attacks. approach COMPARE baseline. baseline COMPARE approach. ,"This paper studies the problem of generating adversarial attacks using the min-max problem in multi-domain robust optimization. The authors propose to use the existing attacks to train an ensemble of models with the same max-max formulation, which can be used to generate new attacks. They show that using the proposed min-min for adversarial attack scenarios can lead to robust models that are robust to multiple $ell_p$ attacks. The experimental results show that the proposed approach outperforms the baseline."
2837,SP:8d5741aedf3125e0e790a58ec3ce81a4e2ea4dcb,min - max framework APGDA USED-FOR models. models USED-FOR adversarial robustness. APGDA USED-FOR model ensemble attack. model ensemble attack CONJUNCTION universal attack. universal attack CONJUNCTION model ensemble attack. universal attack CONJUNCTION robust attack. robust attack CONJUNCTION universal attack. APGDA USED-FOR universal attack. APGDA USED-FOR robust attack. data transformations USED-FOR robust attack. Generic is method. ,"This paper proposes a min-max framework APGDA to train models for adversarial robustness. The proposed method can be applied to both model ensemble attack and universal attack. In particular, the proposed method is able to train a robust attack with data transformations."
2838,SP:9dcb74bfdbc4aa1e27f5d2adb6d2abf475e9324d,"sparse PCA CONJUNCTION tensor PCA. tensor PCA CONJUNCTION sparse PCA. poly - time efficient algorithm CONJUNCTION exponential exhaustive search. exponential exhaustive search CONJUNCTION poly - time efficient algorithm. lower bounds USED-FOR computational model. low - degree polynomials USED-FOR computational model. Task is sparse tensor PCA. Generic is algorithm. OtherScientificTerm are sparsity, signal dimension, and constant tensor order. ",This paper studies the problem of sparse tensor PCA. The authors propose a poly-time efficient algorithm and an exponential exhaustive search. The algorithm is based on the idea that sparsity is a function of the signal dimension. The paper provides lower bounds for the computational model that can be expressed as low-degree polynomials with constant tensor order. 
2839,SP:9dcb74bfdbc4aa1e27f5d2adb6d2abf475e9324d,"Complexity EVALUATE-FOR Sparse Tensor PCA. sparse principal component analysis CONJUNCTION tensor principal component analysis. tensor principal component analysis CONJUNCTION sparse principal component analysis. Sparse Tensor PCA "" problem HYPONYM-OF problems. tensor principal component analysis HYPONYM-OF problems. sparse principal component analysis HYPONYM-OF problems. algorithms CONJUNCTION computational lower bounds. computational lower bounds CONJUNCTION algorithms. computational lower bounds USED-FOR problem. algorithms USED-FOR problem. OtherScientificTerm is single - spike "" tensor. ","Complexity of Sparse Tensor PCA is a well-studied problem. This paper studies two problems: sparse principal component analysis (i.e., the ""single-spike"" tensor) and tensor principal component decomposition (ii. the ""Sparse TensorPCA"" problem). The authors provide algorithms and computational lower bounds for this problem. "
2840,SP:9dcb74bfdbc4aa1e27f5d2adb6d2abf475e9324d,algorithms USED-FOR sparse tensor PCA problems. polynomial - time algorithm CONJUNCTION exponential - time search algorithm. exponential - time search algorithm CONJUNCTION polynomial - time algorithm. polynomial - time algorithm USED-FOR smooth interpolation. exponential - time search algorithm USED-FOR smooth interpolation. smooth interpolation USED-FOR algorithms. lower signal / noise ratio regime FEATURE-OF distinct sparse signal cases. distinct sparse signal cases FEATURE-OF sparce PCA. lower signal / noise ratio regime FEATURE-OF sparce PCA. Lower bound analysis USED-FOR algorithm. ,This paper proposes two algorithms for solving sparse tensor PCA problems based on smooth interpolation using a polynomial-time algorithm and an exponential-time search algorithm. The authors show that the proposed algorithms can be used to solve sparce PCA in the lower signal/noise ratio regime for distinct sparse signal cases. Lower bound analysis is provided for the proposed algorithm.
2841,SP:9dcb74bfdbc4aa1e27f5d2adb6d2abf475e9324d,"recovery USED-FOR symmetric sparse tensor estimation problem. computational complexity EVALUATE-FOR recovery. degree of sparsity CONJUNCTION signal strength. signal strength CONJUNCTION degree of sparsity. signal strength CONJUNCTION integer parameter. integer parameter CONJUNCTION signal strength. Necessary conditions USED-FOR testing problem. polynomial of bounded degree USED-FOR planted model. planted model COMPARE null mull of only Gaussian noise. null mull of only Gaussian noise COMPARE planted model. L2 test USED-FOR distinguishability. OtherScientificTerm are matrices, tensors, sparsity constraints, and ambient dimension. Method are single spike model, brute force search, and multiple spike model. ","This paper studies recovery for the symmetric sparse tensor estimation problem with respect to the computational complexity of recovery in the setting of matrices. The authors consider the single spike model, where the tensors are assumed to be symmetric. The testing problem is formulated as a testing problem under the following conditions: degree of sparsity, signal strength, and integer parameter. In particular, the authors consider a setting where the planted model is a polynomial of bounded degree and the ambient dimension is bounded. Theoretical results are provided for the case where the sparsity constraints are not satisfied. In this setting, a brute force search is used to find a planted model that is at least as good as the null mull of only Gaussian noise. In addition, a L2 test for distinguishability is also provided. Finally, a multiple spike model is considered."
2842,SP:9dcb74bfdbc4aa1e27f5d2adb6d2abf475e9324d,"statistical constraints CONJUNCTION runtime constraints. runtime constraints CONJUNCTION statistical constraints. statistical constraints FEATURE-OF signal - to - noise ratio. runtime constraints FEATURE-OF estimation algorithm. information - theoretic lower bound FEATURE-OF low - degree polynomials. PCA CONJUNCTION tensor PCA. tensor PCA CONJUNCTION PCA. bound FEATURE-OF low - degree polynomials. OtherScientificTerm are unit norm, and Gaussian noise tensor. Generic is algorithm. Method are sparse PCA, planted model, and pure noise model. ","This paper studies the problem of estimating the signal-to-noise ratio under statistical constraints and runtime constraints in an estimation algorithm. The authors propose a new algorithm, sparse PCA, where the unit norm is a Gaussian noise tensor, and the planted model is a pure noise model. They provide an information-theoretic lower bound on the low-degree polynomials of this bound for both PCA and tensor PCA."
2843,SP:660137b0f84e47c06dc2bee1c95b299c67e4cb67,Cartesian coordinates USED-FOR deep implicit networks. embeddings USED-FOR spatial frequencies. SIREN CONJUNCTION FFN. FFN CONJUNCTION SIREN. SIREN HYPONYM-OF Positional Encodings. spatial grid USED-FOR they. embeddings USED-FOR method. method COMPARE methods. methods COMPARE method. Generic is network. OtherScientificTerm is maximal spatial frequency. Material is image with large smooth regions. ,"Cartesian coordinates have been widely used in deep implicit networks, but they are typically represented as a spatial grid. This paper proposes to use Positional Encodings (such as SIREN and FFN) instead. The proposed method uses embeddings to represent spatial frequencies, which are then used to train a network to predict the maximal spatial frequency of an image with large smooth regions. Experiments show that the proposed method outperforms existing methods."
2844,SP:660137b0f84e47c06dc2bee1c95b299c67e4cb67,"implicit function approximation USED-FOR image and mesh generation. Method are progressive low resolution, and FFN. OtherScientificTerm is ` masking `. ",This paper studies implicit function approximation for image and mesh generation in the context of progressive low resolution. The main contribution of this paper is to propose to use `masking` instead of FFN in order to reduce the number of parameters required for FFN to converge. 
2845,SP:660137b0f84e47c06dc2bee1c95b299c67e4cb67,Fourier features USED-FOR Position Encoding models. iterative feedback algorithm USED-FOR images. iterative feedback algorithm USED-FOR solution. OtherScientificTerm is bandwidth parameter. Task is 2D silhouette reconstruction task. ,This paper proposes to use Fourier features to train Position Encoding models. The proposed solution is based on an iterative feedback algorithm that generates images from a fixed set of points and then updates the bandwidth parameter based on the current position of the points. The authors conduct experiments on a 2D silhouette reconstruction task.
2846,SP:660137b0f84e47c06dc2bee1c95b299c67e4cb67,"Implicit neural representations USED-FOR compressed representations of data. approach USED-FOR compressed representations of data. data science CONJUNCTION physics. physics CONJUNCTION data science. method USED-FOR optimization. method USED-FOR implicit neural representations. implicit neural representations USED-FOR optimization. MLPs USED-FOR low - spatial frequencies. frequency distributions FEATURE-OF dataset. Material is image. OtherScientificTerm are ( x, y ) coordinates, and RGB pixel values. ","Implicit neural representations are an important approach for learning compressed representations of data, which is an important problem in both data science and physics. This paper proposes a method for learning implicit neural representations for optimization. The idea is to train MLPs to predict low-spatial frequencies of an image, which are then used to compute (x,y) coordinates for the RGB pixel values of the image. The authors show that the resulting dataset has similar frequency distributions."
2847,SP:660137b0f84e47c06dc2bee1c95b299c67e4cb67,"bandwidths USED-FOR positional encodings. positional encodings USED-FOR implicit functions. positional encodings USED-FOR method. images CONJUNCTION 3d occupancy. 3d occupancy CONJUNCTION images. 3d occupancy CONJUNCTION mesh deformations. mesh deformations CONJUNCTION 3d occupancy. images HYPONYM-OF domains. mesh deformations HYPONYM-OF domains. 3d occupancy HYPONYM-OF domains. Generic are technique, and it. OtherScientificTerm are maximum frequency, and appearance of artifacts. Metric is quality criterion. ","This paper proposes a technique to improve the performance of implicit functions by using positional encodings with different bandwidths. The method is based on the idea of using different spatial locations of the data points in the input space to learn positional encodes for implicit functions. The technique is simple and effective, and it can be applied to a variety of domains such as images, 3d occupancy and mesh deformations. The main contribution of the paper is that the maximum frequency of the encoder and decoder can be tuned to minimize the appearance of artifacts, which is a quality criterion. "
2848,SP:b03063fa82d76db341076e5f282176f4c007a202,entropy regularized extra - gradient method USED-FOR zero sum games. algorithms USED-FOR equilibrium. equilibrium FEATURE-OF regularized game. algorithms USED-FOR approximate equilibria. algorithms USED-FOR Markov zero sum games. unregularized game CONJUNCTION Markov zero sum games. Markov zero sum games CONJUNCTION unregularized game. ,This paper proposes an entropy regularized extra-gradient method for zero sum games. The authors provide algorithms for finding the equilibrium of a regularized game and approximate equilibria for Markov zerosum games. Experiments are conducted on both an unregularized game as well as Markov two-player games.
2849,SP:b03063fa82d76db341076e5f282176f4c007a202,two - player zero - sum matrix games CONJUNCTION zero - sum Markov games. zero - sum Markov games CONJUNCTION two - player zero - sum matrix games. entropy regularization FEATURE-OF games. two - player zero - sum matrix games HYPONYM-OF games. zero - sum Markov games HYPONYM-OF games. Predictive Update ( PU ) CONJUNCTION Optimistic Multiplicative Weights Update ( OMWU ). Optimistic Multiplicative Weights Update ( OMWU ) CONJUNCTION Predictive Update ( PU ). mirror descent approach USED-FOR regularized objective. KL distance USED-FOR mirror descent approach. Optimistic Multiplicative Weights Update ( OMWU ) HYPONYM-OF first - order descent / ascent algorithms. Predictive Update ( PU ) HYPONYM-OF first - order descent / ascent algorithms. optimistic predictions USED-FOR first - order descent / ascent algorithms. Optimistic Multiplicative Weights Update ( OMWU ) HYPONYM-OF optimistic predictions. Predictive Update ( PU ) HYPONYM-OF optimistic predictions. mirror descent approach USED-FOR regularized matrix games. algorithms USED-FOR Nash equilibrium. Nash equilibrium FEATURE-OF regularized game. infinity norm CONJUNCTION optimality gap. optimality gap CONJUNCTION infinity norm. KL distance CONJUNCTION infinity norm. infinity norm CONJUNCTION KL distance. optimality gap CONJUNCTION duality gap. duality gap CONJUNCTION optimality gap. KL distance HYPONYM-OF measures. duality gap HYPONYM-OF measures. infinity norm HYPONYM-OF measures. optimality gap HYPONYM-OF measures. measures USED-FOR linear convergence. last - iterate convergence USED-FOR It. PU CONJUNCTION OMWU. OMWU CONJUNCTION PU. OMWU USED-FOR $ Q$-value functions. PU USED-FOR $ Q$-value functions. optimal $ Q$-value function FEATURE-OF regularized game. infinite norm FEATURE-OF optimal $ Q$-value function. PU USED-FOR regularized Markov games. OMWU USED-FOR regularized Markov games. convergence rate COMPARE linear rate. linear,"This paper studies two types of games with entropy regularization: two-player zero-sum matrix games and two-sum Markov games, which are both games with two players. The authors propose two first-order descent/ascent algorithms based on optimistic predictions: Predictive Update (PU) and Optimistic Multiplicative Weights Update (OMWU) for regularized matrix games, as well as a mirror descent approach to regularized objective based on KL distance and optimality gap. The algorithms aim to converge to a Nash equilibrium of the regularized game. It is based on last-iterate convergence, and the authors show that the convergence rate of PU and OMWU converges linearly to the optimal $Q$-value function with infinite norm and duality gap."
2850,SP:b03063fa82d76db341076e5f282176f4c007a202,mirror descent USED-FOR entropy - regularized competitive games. Linear convergence rates EVALUATE-FOR algorithm. algorithm USED-FOR subproblem. subproblem PART-OF entropy - regularized Markov game. linear convergence rate FEATURE-OF regularized equilibrium. ,This paper studies mirror descent in entropy-regularized competitive games. Linear convergence rates of the proposed algorithm are provided for a subproblem in an entropy-reinforced Markov game. The linear convergence rate of the regularized equilibrium is also provided.
2851,SP:b03063fa82d76db341076e5f282176f4c007a202,"zero - sum ( ZS ) matrix games CONJUNCTION ZS Markov games. ZS Markov games CONJUNCTION zero - sum ( ZS ) matrix games. entropic regularization CONJUNCTION optimistic - type methods. optimistic - type methods CONJUNCTION entropic regularization. optimistic - type methods USED-FOR ZS Markov games. optimistic - type methods USED-FOR zero - sum ( ZS ) matrix games. entropic regularization USED-FOR zero - sum ( ZS ) matrix games. entropic regularization USED-FOR ZS Markov games. strongly - concave / strongly - concave problem USED-FOR game. simplex USED-FOR ZS matrix games. Mirror - prox / extragradient CONJUNCTION optimistic mirror descent. optimistic mirror descent CONJUNCTION Mirror - prox / extragradient. performance measures EVALUATE-FOR methods. multiplicative updates USED-FOR optimistic mirror descent. Task are regularized problem, and ZS infinite - horizon Markov games. OtherScientificTerm are strong convexity / concavity, and regularization parameter. ","This paper studies the convergence of zero-sum (ZS) matrix games and ZS Markov games with entropic regularization and optimistic-type methods in the context of both ZS matrix games with a simplex and with a strong convexity/concave problem, as well as in the case of ZS infinite-horizon Markovs. The authors propose a new regularized problem for the case where the game can be viewed as a strongly-convex/strongly-concavave problem. Mirror-prox/extragradient and optimistic mirror descent with multiplicative updates are considered. The proposed methods are evaluated on a variety of performance measures and compared to existing methods in terms of the number of iterations and the choice of the regularization parameter."
2852,SP:b03063fa82d76db341076e5f282176f4c007a202,"game theory CONJUNCTION GANs. GANs CONJUNCTION game theory. methods USED-FOR approximate Nash equilibrium. last - iterate convergence guarantees USED-FOR saddle - point optimization. game theory HYPONYM-OF saddle - point optimization. GANs HYPONYM-OF saddle - point optimization. methods USED-FOR regularized version of the payoff. quantal response equilibrium USED-FOR approximate Nash equilibrium. OtherScientificTerm are equilibrium uniqueness guarantee, regularization, and QRE. Task is Markov games. ","This paper studies the last-iterate convergence guarantees for saddle-point optimization, specifically in game theory and GANs. The authors propose two methods for finding an approximate Nash equilibrium with a quantal response equilibrium. The first method, QRE, provides an equilibrium uniqueness guarantee. The second method, regularization, provides a regularized version of the payoff. Both methods are applied to Markov games."
2853,SP:862223b8bd4c275f96c7e41c92daaa2ca2906194,"transformer - based image super - resolution method USED-FOR screen content images. transformer USED-FOR mapping. scale token USED-FOR magnification factor. scale token PART-OF It. OtherScientificTerm are thin edges, coordinates, and rgb values. Method is training and testing datatests. ",This paper proposes a transformer-based image super-resolution method for screen content images. It adds a scale token to the magnification factor to encourage thin edges to be close to each other. The mapping is performed using the transformer and the coordinates are computed using rgb values. Both training and testing datatests are provided.
2854,SP:862223b8bd4c275f96c7e41c92daaa2ca2906194,transformers PART-OF network architecture. CNN USED-FOR feature maps. CNN USED-FOR architecture. implicit transformer USED-FOR receptive field. MLP USED-FOR implicit position encoding. Natural images USED-FOR SR. SR techniques USED-FOR screen content. Task is SCISR problem. ,This paper proposes a new network architecture that incorporates transformers. The proposed architecture is based on a CNN that generates feature maps. The implicit transformer is used to encode the receptive field and the MLP is used for implicit position encoding. Natural images are used to train SR. Experiments are conducted on the SCISR problem and SR techniques are applied to screen content.
2855,SP:862223b8bd4c275f96c7e41c92daaa2ca2906194,implicit transformer USED-FOR continuous image super - resolution. transformer - like formulation CONJUNCTION scale token. scale token CONJUNCTION transformer - like formulation. scale token CONJUNCTION implicit position encoding. implicit position encoding CONJUNCTION scale token. query ( high - resolution ) coordinate CONJUNCTION key ( low - resolution ) coordinate. key ( low - resolution ) coordinate CONJUNCTION query ( high - resolution ) coordinate. key ( low - resolution ) coordinate CONJUNCTION scale token. scale token CONJUNCTION key ( low - resolution ) coordinate. implicit transformer USED-FOR transformation weight. query ( high - resolution ) coordinate USED-FOR transformation weight. pixel features USED-FOR pixel intensity. CNN backbone USED-FOR pixel features. transformation weights USED-FOR pixel features. implicit position encoding USED-FOR pixel intensity. screen content images EVALUATE-FOR continuous super - resolution methods. Method is adaptive weighting scheme. ,"This paper proposes an implicit transformer for continuous image super-resolution, which combines a transformer-like formulation, a scale token, and an implicit position encoding. The transformation weight of the implicit transformer is based on the query (high-resolution) coordinate, the key (low-resolution), and the scale token. An adaptive weighting scheme is also proposed. The pixel features of the CNN backbone are used to compute the pixel intensity using the transformation weights. Experiments on screen content images demonstrate the effectiveness of the proposed continuous super-resolution methods."
2856,SP:862223b8bd4c275f96c7e41c92daaa2ca2906194,arbitrary screen content image super - resolution COMPARE natural image super - resolution. natural image super - resolution COMPARE arbitrary screen content image super - resolution. datasets USED-FOR scenario. arbitrary scaling FEATURE-OF screen content images. implicit transformer CONJUNCTION implicit position encoding. implicit position encoding CONJUNCTION implicit transformer. LIIF USED-FOR ITSRN. implicit transformer PART-OF ITSRN. implicit position encoding PART-OF ITSRN. Generic is schemes. ,"This paper studies the problem of arbitrary screen content image super-resolution, which is in contrast to natural image super -resolution. The authors consider a scenario where the screen content images are subject to arbitrary scaling, and propose two datasets for this scenario. The proposed schemes are based on LIIF and ITSRN with an implicit transformer and an implicit position encoding."
2857,SP:862223b8bd4c275f96c7e41c92daaa2ca2906194,"method USED-FOR super - resolution of screen content images. transformer USED-FOR method. datasets USED-FOR image quality assessment. datasets EVALUATE-FOR method. Method are implicit transformer, and implicit neural representation. Generic is modules. ","This paper proposes a method for super-resolution of screen content images based on transformer. The method is evaluated on two datasets for image quality assessment. The main contribution of the paper is the implicit transformer, which is an extension of implicit neural representation. The authors also propose two additional modules to improve the performance."
2858,SP:3751625929b707ced417c3eb10064e4917866048,"generative models USED-FOR interventional distributions. sum - product networks ( SPN ) HYPONYM-OF generative models. SPNs HYPONYM-OF models. gate functions USED-FOR tractable inference. gate functions USED-FOR models. gate functions USED-FOR SPNs. interventional SPNs ( iSPN ) HYPONYM-OF conditional SPNs. OtherScientificTerm are conditional distributions, causal graph, and interventional distribution. ","This paper studies the problem of learning generative models for interventional distributions, specifically sum-product networks (SPN). SPNs are models that use gate functions for tractable inference. The authors consider conditional SPNs, i.e., interventional SPNs (iSPN), where the conditional distributions are drawn from a causal graph, and the interventional distribution is sampled from this graph."
2859,SP:3751625929b707ced417c3eb10064e4917866048,interventional Sum - Product Networks ’ USED-FOR causal inference framework. deep learning techniques USED-FOR complex nonparametric functions. multivariate conditional probability distributions FEATURE-OF complex nonparametric functions. deep learning techniques USED-FOR It. SPNs USED-FOR It. small synthetic data sets EVALUATE-FOR It. Task is causal inference. ,"This paper proposes a causal inference framework based on ‘interventional Sum-Product Networks’. It leverages deep learning techniques to model complex nonparametric functions with multivariate conditional probability distributions. It is based on SPNs and is evaluated on small synthetic data sets. The paper is well-written and well-motivated, and the contribution of the paper is clear. However, there are some issues that need to be addressed in order to make causal inference more practical."
2860,SP:3751625929b707ced417c3eb10064e4917866048,sum product networks USED-FOR interventional distributions. iSPN HYPONYM-OF sum product networks. graph USED-FOR estimates of arbitrary causal quantities. computational complexity EVALUATE-FOR model. model COMPARE software packages. software packages COMPARE model. software packages USED-FOR causal effect estimation. running time EVALUATE-FOR model. Method is SPN. OtherScientificTerm is mutilated distribution. ,"This paper proposes iSPN, which is a generalization of sum product networks for interventional distributions. The main idea of SPN is to use a graph to provide estimates of arbitrary causal quantities. The computational complexity of the proposed model is compared to other software packages for causal effect estimation. The running time of the model is also compared to existing methods for estimating a mutilated distribution."
2861,SP:3751625929b707ced417c3eb10064e4917866048,tractable method USED-FOR estimating causal effects. interventional sum - product networks HYPONYM-OF tractable method. interventional sum - product networks USED-FOR estimating causal effects. iSPNs HYPONYM-OF universal function approximators. iSPNs USED-FOR estimates of interventional queries. collection of synthetic data generating processes USED-FOR iSPNs. interventional data USED-FOR iSPNs. collection of synthetic data generating processes USED-FOR estimates of interventional queries. interventional data USED-FOR estimates of interventional queries. ,"This paper proposes a tractable method, i.e., interventional sum-product networks, for estimating causal effects. iSPNs are universal function approximators and can be used to generate estimates of interventional queries from interventional data using a collection of synthetic data generating processes."
2862,SP:3751625929b707ced417c3eb10064e4917866048,this USED-FOR interventional distributions. interventional sum - product networks HYPONYM-OF sum - product networks ( SPNs ). causal graph USED-FOR SPNs. tractable probabilistic models USED-FOR causality. method COMPARE methods. methods COMPARE method. ,"This paper proposes a causal graph for SPNs, i.e., interventional sum-product networks (SPNs). The authors show that this can be used to model interventional distributions. The authors also provide tractable probabilistic models for causality. The experimental results show that the proposed method outperforms existing methods."
2863,SP:c857ff674ca05c1d949337cb885f056b82d981d6,deep Markov factor analysis ( DMFA ) HYPONYM-OF generative model. spatial inductive assumptions USED-FOR temporal dynamics. Markov property USED-FOR chain of low dimensional temporal embeddings. Markov property CONJUNCTION spatial inductive assumptions. spatial inductive assumptions CONJUNCTION Markov property. temporal dynamics FEATURE-OF functional magnetic resonance imaging ( fMRI ). Markov property USED-FOR generative model. DMFA USED-FOR fMRI data. synthetic and application data EVALUATE-FOR DMFA. Metric is low temporal embedding. OtherScientificTerm is subject and cognitive state variability. ,"This paper proposes deep Markov factor analysis (DMFA), a generative model that leverages the Markov property of a chain of low dimensional temporal embeddings and spatial inductive assumptions on temporal dynamics in functional magnetic resonance imaging (fMRI). DMFA is applied to fMRI data and evaluated on both synthetic and application data. The results show that the low temporal embedding helps to reduce subject and cognitive state variability."
2864,SP:c857ff674ca05c1d949337cb885f056b82d981d6,"radial basis function USED-FOR spatial component. stochastic gradient descent USED-FOR Variational inference. DMFA USED-FOR subject specific atlases. DMFA COMPARE methods. methods COMPARE DMFA. held - out log - likelihood EVALUATE-FOR DMFA. held - out log - likelihood EVALUATE-FOR methods. Task are Concurrent Temporal and Spatial Analysis of fMRI Data, Topographic Factor Analysis, and topographical factor analysis. Generic is approach. Method are spatial and temporal components, temporal components, Pre - trained model, and pre - trained model. OtherScientificTerm is Markov property. Material are Autism dataset, Depression dataset, MDD, and musical stimuli. ","This paper addresses the problem of Concurrent Temporal and Spatial Analysis of fMRI Data. The proposed approach, DMFA, decomposes the data into spatial and temporal components. The spatial component is modeled as a radial basis function and the temporal components are modeled as the Markov property. Variational inference is performed using stochastic gradient descent. Experiments on the Autism dataset and the Depression dataset show that DMFA is able to generate subject specific atlases that outperform existing methods in terms of held-out log-likelihood. Pre-trained model is also shown to be able to generalize to MDD and to musical stimuli. The main contribution of the paper is the Topographic Factor Analysis, which is an extension of topographical factor analysis. The authors also provide a theoretical analysis of the effect of the size of the pre-training model."
2865,SP:c857ff674ca05c1d949337cb885f056b82d981d6,"DMFA USED-FOR spatiotemporal BOLD fMRI time series. time - varying latent factors USED-FOR time - varying spatial maps. spatial mixtures of RBFs PART-OF time - varying spatial maps. time - varying latent factors USED-FOR DMFA. Latent time series USED-FOR grouping. Latent time series USED-FOR categorical clustering variable. categorical clustering variable USED-FOR grouping. variational approach USED-FOR Optimization. ABIDE autism data set CONJUNCTION Depression study data set. Depression study data set CONJUNCTION ABIDE autism data set. ABIDE autism data set HYPONYM-OF simulated data set. Method are Deep Markov Factor Analysis ( DMFA ), and deep neural networks. OtherScientificTerm is Bayesian diagram. Task is clustering of relevant clinical outcomes. ","This paper proposes Deep Markov Factor Analysis (DMFA), a variant of DMFA for spatiotemporal BOLD fMRI time series. DMFA uses time-varying latent factors to generate time-variating spatial maps (i.e., spatial mixtures of RBFs) that are then used to train deep neural networks. Latent time series are used as a categorical clustering variable for grouping, and a Bayesian diagram is used to represent the clustering of relevant clinical outcomes. Optimization is performed using a variational approach, and experiments are conducted on a simulated data set (the ABIDE autism data set and a Depression study data set)."
2866,SP:c857ff674ca05c1d949337cb885f056b82d981d6,deep Markov factor analysis ( DMFA ) USED-FOR temporal dynamics. deep Markov factor analysis ( DMFA ) USED-FOR functional magnetic resonance imaging data. functional magnetic resonance imaging data USED-FOR temporal dynamics. method USED-FOR fMRI data. DMFA ) USED-FOR fMRI data. method USED-FOR validation of fMRI related hypotheses. method USED-FOR DMFA ). low dimensional temporal embedding FEATURE-OF fMRI data. OtherScientificTerm is nonlinear and complex temporal dynamics of neural processes. Material is imaging data. ,This paper proposes a deep Markov factor analysis (DMFA) for temporal dynamics in functional magnetic resonance imaging data. The method is applied to fMRI data with low dimensional temporal embedding and is shown to be effective for the validation of fMRI related hypotheses. The paper is well-motivated by the nonlinear and complex temporal dynamics of neural processes and the importance of imaging data in understanding these dynamics.
2867,SP:c857ff674ca05c1d949337cb885f056b82d981d6,Markov process USED-FOR temporal dynamics. high spatial dimensions CONJUNCTION low - dimensional feature space. low - dimensional feature space CONJUNCTION high spatial dimensions. temporal dynamics PART-OF fMRI dataset. Markov process USED-FOR deep Markov factor analysis ( DMFA ). DMFA USED-FOR low dimensional temporal embedding. DMFA USED-FOR fMRI responses. subject or cognitive state USED-FOR DMFA. subject or cognitive state USED-FOR low dimensional temporal embedding. NTFA CONJUNCTION HTFA. HTFA CONJUNCTION NTFA. DMFA COMPARE state - of - the - art techniques. state - of - the - art techniques COMPARE DMFA. synthetic and real fMRI datasets EVALUATE-FOR DMFA. HTFA HYPONYM-OF state - of - the - art techniques. NTFA HYPONYM-OF state - of - the - art techniques. ,"This paper proposes deep Markov factor analysis (DMFA), which uses a Markov process to model temporal dynamics in an fMRI dataset with high spatial dimensions and a low-dimensional feature space. DMFA learns a low dimensional temporal embedding based on subject or cognitive state, which is then used to model fMRI responses. Experiments on both synthetic and real fMRI datasets show that DMFA outperforms state-of-the-art techniques such as NTFA and HTFA."
2868,SP:855dcaa42868a29a14619d63221169495ed5dd54,prior CONJUNCTION divergence term. divergence term CONJUNCTION prior. approach USED-FOR density. approach HYPONYM-OF continuous normalizing - flow methods. neural network USED-FOR divergence term. MF COMPARE CNFs. CNFs COMPARE MF. MF USED-FOR flow models. ODE USED-FOR sampling. MF COMPARE CNFs. CNFs COMPARE MF. MF HYPONYM-OF universal density approximator. sample quality CONJUNCTION training complexity. training complexity CONJUNCTION sample quality. density estimation CONJUNCTION sample quality. sample quality CONJUNCTION density estimation. density estimation EVALUATE-FOR MF. sample quality EVALUATE-FOR MF. training complexity EVALUATE-FOR MF. Method is flow - based generative models. OtherScientificTerm is general curved surfaces. ,"This paper proposes a new approach to estimating density in flow-based generative models, which is inspired by continuous normalizing-flow methods, where the prior and the divergence term are modeled by a neural network. The sampling is modeled as an ODE, and MF is a universal density approximator that can be applied to flow models with general curved surfaces. Experimental results show that MF outperforms CNFs in terms of density estimation, sample quality, and training complexity."
2869,SP:855dcaa42868a29a14619d63221169495ed5dd54,Dacorogna - Moser transport USED-FOR continuous normalizing flow ( CNF ). toy data CONJUNCTION implicit surfaces. implicit surfaces CONJUNCTION toy data. flat torus USED-FOR toy data. boundaryless manifolds EVALUATE-FOR idea. flat torus HYPONYM-OF boundaryless manifolds. implicit surfaces HYPONYM-OF boundaryless manifolds. Generic is training. Method is numerical integration. OtherScientificTerm is divergence of the mass flow rate. ,"This paper proposes a continuous normalizing flow (CNF) based on Dacorogna-Moser transport. The idea is evaluated on boundaryless manifolds (flat torus, toy data and implicit surfaces). The training is based on numerical integration, where the divergence of the mass flow rate is estimated."
2870,SP:855dcaa42868a29a14619d63221169495ed5dd54,continuous normalizing flow ( CNF ) USED-FOR manifold - valued data. Moser Flow ( MF ) USED-FOR density. divergence PART-OF neural network. manifold FEATURE-OF probability distribution. inference CONJUNCTION generation. generation CONJUNCTION inference. approach COMPARE approaches. approaches COMPARE approach. model USED-FOR sampling. ODE solver USED-FOR model. ODE solver USED-FOR sampling. tori CONJUNCTION Stanford bunny. Stanford bunny CONJUNCTION tori. 2 - spheres FEATURE-OF datasets. 2 - spheres EVALUATE-FOR approach. datasets EVALUATE-FOR approach. OtherScientificTerm is model density. ,"This paper proposes a continuous normalizing flow (CNF) for manifold-valued data. The density is modeled using Moser Flow (MF). The divergence of the neural network is modeled as a probability distribution over the manifold. The model is trained using an ODE solver, and the model is used for sampling and inference. The proposed approach is evaluated on two datasets on 2-spheres, tori and Stanford bunny, and compared to other approaches."
2871,SP:855dcaa42868a29a14619d63221169495ed5dd54,"Moser Flow HYPONYM-OF Continuous Normalizing Flows. Continuous Normalizing Flows USED-FOR Riemannian Manifolds. approach USED-FOR vector field. approach COMPARE Riemmanian CNF's. Riemmanian CNF's COMPARE approach. vector field USED-FOR normalizing equation. differential geometry USED-FOR approach. solver USED-FOR ODE. Moser Flows USED-FOR arbitrary distributions. Moser Flows USED-FOR Euclidean Submanifolds. Induced metric USED-FOR Moser Flows. paper USED-FOR Continuous Normalizing Flow research. OtherScientificTerm are volume form, and manifolds of interest. Generic is method. ","This paper proposes a method called Moser Flow, which is a variant of Continuous Normalizing Flows for Riemannian Manifolds. The proposed approach is based on differential geometry, where the vector field of the normalizing equation is modeled as a function of the volume form. The authors show that the proposed approach outperforms Riemmanian CNF's. The main contribution of the paper is the introduction of a solver to solve the ODE, which allows the method to be applied to manifolds of interest. In particular, the authors apply Moser Flows to Euclidean Submanifolds using the Induced metric, and to arbitrary distributions. The paper is well-motivated and well-written, and is an important contribution to Continuous Normalization Flow research."
2872,SP:855dcaa42868a29a14619d63221169495ed5dd54,"divergence USED-FOR neural network. Riemannian manifold FEATURE-OF continuous normalizing flow. Moser theorem USED-FOR divergence. divergence USED-FOR continuous normalizing flow. toy distributions CONJUNCTION real world earth and climate dataset. real world earth and climate dataset CONJUNCTION toy distributions. they COMPARE procedures. procedures COMPARE they. toy distributions EVALUATE-FOR method. real world earth and climate dataset EVALUATE-FOR method. method COMPARE FFJORD. FFJORD COMPARE method. FFJORD USED-FOR complex two dimensional density. method USED-FOR complex two dimensional density. Generic are approach, and flow. ","This paper proposes a new approach to learning a continuous normalizing flow on a Riemannian manifold using a divergence based on the Moser theorem. This divergence can be used to train a neural network. The flow can be viewed as a function of the dimension of the input space. The method is evaluated on toy distributions as well as a real world earth and climate dataset and they are compared to existing procedures. The proposed method is compared to FFJORD, a method for learning a complex two dimensional density."
2873,SP:545554de09d17df77d6169a5cc8f36022ecb355c,nonlinear ICA approaches USED-FOR blind source separation problem ( BSS ). causal discovery HYPONYM-OF independent causal mechanisms ( ICM ). ICM USED-FOR BSS. condition USED-FOR identifiability. condition USED-FOR information theoretic and geometric interpretations. orthogonality condition FEATURE-OF Jacobian. spurious solutions USED-FOR model. Method is IMA formulation. Material is toy examples. ,"This paper studies nonlinear ICA approaches to the blind source separation problem (BCS) in the context of independent causal mechanisms (ICM), i.e., causal discovery. In particular, the authors extend the IMA formulation to the case of BSS with ICM. The main contribution of the paper is the introduction of an orthogonality condition on the Jacobian of the model, which allows for information theoretic and geometric interpretations of the condition for identifiability. Experiments on toy examples demonstrate that the proposed model is robust to spurious solutions."
2874,SP:545554de09d17df77d6169a5cc8f36022ecb355c,regularization scheme USED-FOR identifiability. Jacobian matrix FEATURE-OF mixing functions. degeneracies PART-OF nonlinear ICA. Task is causal inference. ,"This paper proposes a regularization scheme to improve identifiability in the setting of causal inference. In particular, the authors consider mixing functions in the form of a Jacobian matrix. The main contribution of this paper is to study degeneracies in nonlinear ICA."
2875,SP:545554de09d17df77d6169a5cc8f36022ecb355c,"non - linear ICA USED-FOR Blind Source Separation. criterion USED-FOR non - linear ICA. causal discovery literature FEATURE-OF Independence of Causal Mechanisms. Independence of Causal Mechanisms USED-FOR criterion. evaluation metric CONJUNCTION regularizer. regularizer CONJUNCTION evaluation metric. identifiability FEATURE-OF non - linear ICA. OtherScientificTerm are gradients, and mixing mechanism. Generic are constraint, and it. ","This paper proposes a new criterion for non-linear ICA for Blind Source Separation based on the idea of Independence of Causal Mechanisms in the causal discovery literature. Specifically, the authors propose a new evaluation metric and a new regularizer that encourages gradients to be independent of the mixing mechanism. This constraint is motivated by the fact that it allows for identifiability in the case of non-leaky ICA."
2876,SP:545554de09d17df77d6169a5cc8f36022ecb355c,"non - spurious solutions USED-FOR nonlinear blind source separation. framework USED-FOR non - spurious solutions. independent mechanism analysis "" ( IMA ) USED-FOR non - spurious solutions. concepts USED-FOR framework. causality USED-FOR concepts. independent causal mechanisms HYPONYM-OF causality. latent variables USED-FOR nonlinear mixing. restricted nonlinear ICA model USED-FOR IMA. OtherScientificTerm is Jacobian. ","This paper proposes a framework called ""independent mechanism analysis"" (IMA) to find non-spurious solutions for nonlinear blind source separation. The framework is based on two concepts: (1) causality (independent causal mechanisms) and (2) nonlinear mixing (using latent variables). The IMA is trained using a restricted nonlinear ICA model, where the Jacobian is assumed to be independent."
2877,SP:545554de09d17df77d6169a5cc8f36022ecb355c,identifiability FEATURE-OF nonlinear ICA. it USED-FOR nonlinear ICA. principle USED-FOR constraint of mixture functions. modified independent causal mechanism principle USED-FOR causal discovery literature. identifiability FEATURE-OF nonlinear ICA. modified independent causal mechanism principle USED-FOR principle. modified independent causal mechanism principle USED-FOR constraint of mixture functions. OtherScientificTerm is non - identifiable cases. ,"This paper proposes a new principle based on the modified independent causal mechanism principle in the causal discovery literature, and applies it to the problem of nonlinear ICA with respect to identifiability. The main contribution of this paper is to show that the constraint of mixture functions can be derived from the principle, and that non-identifiable cases can be considered."
2878,SP:7df49c554d6c9fca370f049279ef7324b6f79de9,"Annealed Importance Sampling ( AIS ) CONJUNCTION Langevin / HMC MCMC. Langevin / HMC MCMC CONJUNCTION Annealed Importance Sampling ( AIS ). Annealed Importance Sampling ( AIS ) USED-FOR variational inference method. Langevin / HMC MCMC USED-FOR variational inference method. HMC dynamics USED-FOR intermediate bridging densities. HMC dynamics USED-FOR approximate samples. posterior distribution FEATURE-OF approximate samples. bridging densities HYPONYM-OF hard - to - set parameters. Uncorrected Hamiltonian Annealing ( UHA ) HYPONYM-OF method. OtherScientificTerm are lower bound, and log evidence. ","This paper proposes a variational inference method based on Annealed Importance Sampling (AIS) and Langevin/HMC MCMC. The main idea is to use HMC dynamics to approximate intermediate bridging densities, which are hard-to-set parameters. The paper provides a lower bound on the number of samples required to approximate the posterior distribution. The method is called Uncorrected Hamiltonian Annealing (UHA), which is based on log evidence."
2879,SP:7df49c554d6c9fca370f049279ef7324b6f79de9,uncorrected HMC kernel USED-FOR AIS - type variational scheme. step size CONJUNCTION momentum covariance. momentum covariance CONJUNCTION step size. reparameterization gradients USED-FOR algorithm tuning parameters. momentum covariance CONJUNCTION annealing schedule. annealing schedule CONJUNCTION momentum covariance. annealing schedule HYPONYM-OF algorithm tuning parameters. momentum covariance HYPONYM-OF algorithm tuning parameters. step size HYPONYM-OF algorithm tuning parameters. reparameterization gradients USED-FOR ELBO. Generic is method. ,"This paper proposes an AIS-type variational scheme based on an uncorrected HMC kernel. The authors propose to use reparameterization gradients to approximate the ELBO for several algorithm tuning parameters such as step size, momentum covariance, and annealing schedule. Experiments are conducted to show the effectiveness of the proposed method."
2880,SP:7df49c554d6c9fca370f049279ef7324b6f79de9,"UHA HYPONYM-OF Hamiltonian AIS. ( re)sampling momenta CONJUNCTION leapfrog integration. leapfrog integration CONJUNCTION ( re)sampling momenta. accept - reject step PART-OF HMC update. leapfrog integration PART-OF UHA step. ( re)sampling momenta PART-OF UHA step. step size CONJUNCTION damping coefficients. damping coefficients CONJUNCTION step size. damping coefficients USED-FOR momentum resampling. proposal distribution CONJUNCTION step size. step size CONJUNCTION proposal distribution. modification USED-FOR parameters. damping coefficients HYPONYM-OF parameters. proposal distribution HYPONYM-OF parameters. step size HYPONYM-OF parameters. VI CONJUNCTION HMC. HMC CONJUNCTION VI. models EVALUATE-FOR UHA. UHA COMPARE HMC. HMC COMPARE UHA. UHA COMPARE VI. VI COMPARE UHA. ELBO EVALUATE-FOR UHA. OtherScientificTerm are AIS ratio, and ratios of momentum distributions. Generic is it. ","This paper proposes UHA, a variant of Hamiltonian AIS called UHA. The UHA step consists of (re)sampling momenta and leapfrog integration. The accept-reject step of the standard HMC update is replaced by an additional modification to the AIS ratio. This modification is motivated by the observation that the ratio between the proposal distribution and the step size of momentum resampling depends on the ratio of the two ratios of momentum distributions. The authors argue that this modification can be used to improve the performance of the parameters used in the original proposal distribution (e.g., step size, damping coefficients) and to make it more interpretable. Experiments on several models show that UHA outperforms VI and HMC in terms of ELBO."
2881,SP:7df49c554d6c9fca370f049279ef7324b6f79de9,uncorrected HMC kernels USED-FOR AIS. uncorrected HMC kernels USED-FOR log normalisation constant. HMC kernels HYPONYM-OF uncorrected HMC kernels. step - size FEATURE-OF numerical integration. reparameterisation gradient USED-FOR parameters. tuning parameters USED-FOR UHA. HMC USED-FOR AIS. inference tasks EVALUATE-FOR tuning parameters. tuning parameters COMPARE methods. methods COMPARE tuning parameters. inference tasks EVALUATE-FOR methods. HMC USED-FOR methods. UHA COMPARE IW. IW COMPARE UHA. VAE training EVALUATE-FOR UHA. tuned parameters USED-FOR UHA. OtherScientificTerm is accept - reject steps. ,"This paper proposes to use uncorrected HMC kernels (i.e., HMM kernels) as the log normalisation constant for AIS. The authors argue that the step-size of numerical integration can be reduced by reducing the number of accept-reject steps. To this end, the authors propose to use a reparameterisation gradient to tune the parameters. The proposed tuning parameters for UHA are evaluated on several inference tasks and compared to other methods that use HMC for tuning parameters in AIS and show that UHA outperforms IW on VAE training."
2882,SP:7df49c554d6c9fca370f049279ef7324b6f79de9,Hamiltonian dynamics kernels USED-FOR annealed important sampling ( AIS ). accept - reject operations PART-OF kernel. momentum variables FEATURE-OF density ratios. non - differentiable operations PART-OF Hamiltonian dynamics. approach HYPONYM-OF fully differentiable AIS method. Uncorrected Hamiltonian Annealing ( UHA ) HYPONYM-OF fully differentiable AIS method. Method is UHA. OtherScientificTerm is differentiability. ,"This paper studies the problem of annealed important sampling (AIS) with Hamiltonian dynamics kernels. The authors propose a new approach called Uncorrected Hamiltonian Annealing (UHA), which is a fully differentiable AIS method. The main idea of UHA is to replace the accept-reject operations in the kernel with non-differentiable operations, which allow for differentiability of the density ratios of the momentum variables."
2883,SP:b0bf070e8d7eefdfc45f236e9ecb9edfb4816e0a,"computing Lipschitzness bounds USED-FOR certified robustness. Lipschitz constant FEATURE-OF network. approach USED-FOR local Lipschitz bounds. Lipschitz constant FEATURE-OF global bounds. interval bounds USED-FOR hidden layers. interval bounds USED-FOR local Lipschitz bounds. worst - case loss USED-FOR certifiably robust models. Lipschitz bounds USED-FOR worst - case loss. OtherScientificTerm are Lipschitzness bounds, tighter Lipschitz constant, weight matrices, sparsity - inducing regularization, and upper and lower activation bounds. Generic is method. Method are ReLU activation, and Lipschitzness - based baselines. ","This paper considers the problem of computing Lipschitzness bounds for certified robustness. The authors propose a method based on the observation that the global bounds of the Lipschtitz constant of the network can be reduced to a smaller version of the original global bounds when the network is trained on a tighter subset of data points. The main contribution of the paper is that the authors propose an approach to compute local Lipsscitz bounds based on interval bounds for the hidden layers. The key idea is to use a tighter version of ReLU activation, where the weight matrices are replaced with a sparsity-inducing regularization. The upper and lower activation bounds are obtained by minimizing the difference between the upper and the lower bound of the upper bound, which is then used to compute the worst-case loss for certifiably robust models. The experiments show that the proposed method is able to achieve state-of-the-art results compared to Lipsshitzness-based baselines."
2884,SP:b0bf070e8d7eefdfc45f236e9ecb9edfb4816e0a,"trainable local Lipschitz upper - bound COMPARE global Lipschitz upper - bound. global Lipschitz upper - bound COMPARE trainable local Lipschitz upper - bound. trainable local Lipschitz upper - bound USED-FOR neural network. Lipschitz constant USED-FOR global Lipschitz upper - bound. weight matrices CONJUNCTION piece - wise linear activation functions. piece - wise linear activation functions CONJUNCTION weight matrices. ReLU USED-FOR Lipschitz upper - bound. ReLU USED-FOR algorithms. ReLU USED-FOR local Lipschitz bound. image classification tasks EVALUATE-FOR algorithms. Metric is global Lipschitz bound. OtherScientificTerm are activation functions, upper threshold, and constant region. ","This paper proposes a trainable local Lipschitz upper-bound for a neural network, which is an improvement over the global Lipscheditz bound. The main idea is to train a local version of a global neural network by maximizing the local version, where the weights of the activation functions are independent of each other. This is achieved by minimizing the Lipschnitz constant, which can be viewed as an upper threshold for the global version. The global version of Lipsching constant is then used to approximate the global of the Lipchitz constant. The paper then proposes two algorithms based on ReLU to optimize the local versions of the ReLU, which are trained with weight matrices and piece-wise linear activation functions. The proposed algorithms are evaluated on image classification tasks, and the results show that the local variant of ReLU is able to achieve a better upper bound on the local local version (in terms of the constant region) than the global one."
2885,SP:b0bf070e8d7eefdfc45f236e9ecb9edfb4816e0a,"local Lipschitz bound USED-FOR robustness. local Lipschitz bound FEATURE-OF neural network output. invariances FEATURE-OF ReLU activations. ReLU activations PART-OF network. invariances USED-FOR bound. OtherScientificTerm are small perturbations, and clipping threshold. ",This paper proposes a local Lipschitz bound for robustness to small perturbations to the neural network output. The bound is based on the invariances of ReLU activations in the network. The authors also propose a clipping threshold to ensure the robustness.
2886,SP:b0bf070e8d7eefdfc45f236e9ecb9edfb4816e0a,"method USED-FOR certified robustness. local Lipschitz constants FEATURE-OF models. zero - activation of ReLU USED-FOR method. method USED-FOR Lipschitz constants. Lipschitz constants FEATURE-OF weight matrix. method COMPARE methods. methods COMPARE method. robust accuracy CONJUNCTION certified robust accuracy. certified robust accuracy CONJUNCTION robust accuracy. clean accuracy CONJUNCTION robust accuracy. robust accuracy CONJUNCTION clean accuracy. robust accuracy EVALUATE-FOR PGD. certified robust accuracy EVALUATE-FOR method. robust accuracy EVALUATE-FOR methods. robust accuracy EVALUATE-FOR method. clean accuracy EVALUATE-FOR methods. clean accuracy EVALUATE-FOR method. OtherScientificTerm are feature maps, and ReLU. ","This paper proposes a method for certified robustness based on zero-activation of ReLU. The method is motivated by the observation that the local Lipschitz constants of models tend to be highly correlated with the feature maps. The authors propose a method to estimate the Lipsscitz constants in the weight matrix using ReLU and show that the proposed method outperforms existing methods in terms of clean accuracy, robust accuracy, and certified robust accuracy. In addition, the authors show that PGD can achieve comparable or better robust accuracy with their method."
2887,SP:b0bf070e8d7eefdfc45f236e9ecb9edfb4816e0a,local Lipschitz bound USED-FOR certified robust neural networks. local Lipschitz constant USED-FOR provable tighter bound. local Lipschitz constant COMPARE global Lipschitz bound. global Lipschitz bound COMPARE local Lipschitz constant. provable tighter bound COMPARE global Lipschitz bound. global Lipschitz bound COMPARE provable tighter bound. hinge loss USED-FOR pre - activation states. certification method USED-FOR training. ReLU function CONJUNCTION hinge loss. hinge loss CONJUNCTION ReLU function. method COMPARE certified robust training methods. certified robust training methods COMPARE method. ,"This paper proposes a provable tighter bound based on the local Lipschitz bound for certified robust neural networks. In particular, the authors show that when the number of parameters of the ReLU function and the hinge loss for the pre-activation states is small, the provable tight bound is as tight as the global Lipshitz bound. The authors also propose a certification method for training and show that the proposed method outperforms other certified robust training methods."
2888,SP:f6314bfd897cb996de2eaabf0d3037f41da467f3,"scalable method USED-FOR conformal Bayesian prediction. modified posterior predictive density USED-FOR scalable method. add - one - in "" importance sampling USED-FOR modified posterior predictive density. estimation USED-FOR non - extreme miscoverage levels. Method is sparse and hierarchical models. OtherScientificTerm are credible intervals, approximate posterior samples, and novelty. Generic is method. ","This paper proposes a scalable method for conformal Bayesian prediction based on a modified posterior predictive density based on ""add-one-in"" importance sampling. The method is applied to both sparse and hierarchical models, where the estimation is done at non-extreme miscoverage levels. The key idea is to use credible intervals to estimate approximate posterior samples, which are then used to estimate novelty."
2889,SP:f6314bfd897cb996de2eaabf0d3037f41da467f3,"Bayesian model USED-FOR predictive intervals. Bayesian model USED-FOR approach. frequentist coverage FEATURE-OF predictive intervals. Bayesian methods USED-FOR conformal inference. partially exchangeable data USED-FOR approach. Method are conformal prediction, and conjugate models. Generic is baseline approach. ","This paper proposes a new approach to conformal inference based on Bayesian methods. The approach is based on partially exchangeable data, and uses a Bayesian model to estimate predictive intervals with frequentist coverage. The main contribution of this paper is to propose a new way of conformal prediction, which does not rely on conjugate models. The experimental results show improvement over the baseline approach."
2890,SP:f6314bfd897cb996de2eaabf0d3037f41da467f3,approach USED-FOR constructing prediction intervals. approximately correct frequentist coverage FEATURE-OF constructing prediction intervals. Bayesian posterior predictive density USED-FOR conformal prediction technique. importance sampling approximation USED-FOR posterior predictive density. algorithm USED-FOR well - calibrated prediction intervals. misspecification FEATURE-OF Bayesian model. hierachical models USED-FOR extension. Method is misspecified models. OtherScientificTerm is Bayesian posterior. Generic is method. ,This paper presents an approach to constructing prediction intervals with approximately correct frequentist coverage. The authors propose a conformal prediction technique that uses the Bayesian posterior predictive density as an importance sampling approximation. The main contribution of the paper is that the authors propose an extension to hierachical models that does not rely on misspecified models. This allows the algorithm to learn well-calibrated prediction intervals that do not depend on the misspecification of a Bayesian model. Empirical results demonstrate the effectiveness of the proposed method.
2891,SP:f6314bfd897cb996de2eaabf0d3037f41da467f3,"Bayesian and importance sampling techniques USED-FOR conformal prediction framework. Bayesian posterior predictive density USED-FOR conformity score. weighted importance sampling USED-FOR predictive distribution. exchangeable models CONJUNCTION partial exchangeability. partial exchangeability CONJUNCTION exchangeable models. OtherScientificTerm are complete exchangeability, and hyperpriors. Method are flat hierarchical structure, and hierarchical models. ","This paper proposes a conformal prediction framework based on Bayesian and importance sampling techniques. The conformity score is derived from the Bayesian posterior predictive density. The predictive distribution is learned using weighted importance sampling. The authors argue that complete exchangeability is not possible due to the flat hierarchical structure of hierarchical models. To address this issue, the authors propose to use hyperpriors, which allow for exchangeable models and partial exchangeability."
2892,SP:f6314bfd897cb996de2eaabf0d3037f41da467f3,"conditionally exchangeable data USED-FOR hierarchical data. model misspecification CONJUNCTION distribution assumptions. distribution assumptions CONJUNCTION model misspecification. finite sample correct coverage FEATURE-OF conformal interval. Method are conformal Bayesian computation approach, conformal prediction method, and importance sampling. Generic is model. ","This paper proposes a conformal Bayesian computation approach for hierarchical data with unconditionally exchangeable data. The main contribution of the paper is the conformal prediction method, which is based on importance sampling. The authors provide theoretical analysis on the model misspecification and distribution assumptions. The paper also provides a finite sample correct coverage for the proposed conformal interval."
2893,SP:1e86c162b8e8d652a0590b66aa5f7c363955cc5b,"stable and smooth denoisers USED-FOR regularization. RED CONJUNCTION PnP. PnP CONJUNCTION RED. PnP HYPONYM-OF denoisers. RED HYPONYM-OF denoisers. neural networks USED-FOR scalar - valued potential function g. gradient FEATURE-OF scalar - valued potential function g. denoisers PART-OF optimization schemes. denoisers USED-FOR symmetric Jacobians. denoisers USED-FOR MAP estimation. backtracking step size FEATURE-OF optimization schemes. conv layers CONJUNCTION nonlinear activations. nonlinear activations CONJUNCTION conv layers. non - negative weights CONJUNCTION convex activations. convex activations CONJUNCTION non - negative weights. conv layers USED-FOR GraDnCNN. convex activations USED-FOR DnICNNs. non - negative weights USED-FOR DnICNNs. nonlinear activations PART-OF GraDnCNN. GraDnCNN USED-FOR network. Gaussian deblurring CONJUNCTION image superresolution. image superresolution CONJUNCTION Gaussian deblurring. denoising / recovery USED-FOR Gaussian deblurring. DnICNN USED-FOR denoising / recovery. denoising / recovery USED-FOR image superresolution. regularizer USED-FOR inverse problems. denoisers USED-FOR inverse problems. gaussian image deblurring CONJUNCTION superresolution. superresolution CONJUNCTION gaussian image deblurring. denoisers USED-FOR regularizer. neural networks USED-FOR gradients of smooth potential functions. RED USED-FOR superresolution. neural networks USED-FOR denoisers. OtherScientificTerm are denoiser residual, g, and Lipschitz continuity. Metric is convergence. ","This paper studies the use of stable and smooth denoisers for regularization. In particular, the authors consider two types of densoisers: RED and PnP, which are both symmetric Jacobians, and propose two optimization schemes with backtracking step size that use these denoiser for MAP estimation. The first network, GraDnCNN, uses conv layers, nonlinear activations, and convex activations to train DnICNNs with non-negative weights, and uses neural networks to approximate the gradient of a scalar-valued potential function g with respect to the denoiser residual. The authors show that the gradients of smooth potential functions can be approximated using neural networks, and that the convergence is guaranteed by the Lipschitz continuity of g. In addition, they show that using denoising/recovery with DnicNN and DnIQNN can improve the performance on Gaussian deblurring, image superresolution, and denoised versions of RED for superresolution. Finally, they propose a regularizer for inverse problems that uses these two kinds of regularizer."
2894,SP:1e86c162b8e8d652a0590b66aa5f7c363955cc5b,"image denoiser USED-FOR prior. prior USED-FOR regularization. denosing ( RED ) USED-FOR regularization. PnP / RED USED-FOR contractive, nonexpansive, and/or firmly nonexpansive denoisers. denoisers USED-FOR PnP / RED convergence. gradients FEATURE-OF potential functions. gradients FEATURE-OF denoisers. RED FEATURE-OF denoisers. deep neural network USED-FOR potential function. function USED-FOR denoising. gradient USED-FOR function. convex function USED-FOR denoiser. potential function USED-FOR denoiser. procedure USED-FOR potential function. procedure USED-FOR denoiser. potential - driven denoiser USED-FOR RED - SD algorithm. image deblurring CONJUNCTION superresolution. superresolution CONJUNCTION image deblurring. PnP - PGD CONJUNCTION RED - SD. RED - SD CONJUNCTION PnP - PGD. denoisers USED-FOR superresolution. denoisers USED-FOR image deblurring. denoisers COMPARE PnP - PGD. PnP - PGD COMPARE denoisers. RED - SD USED-FOR image deblurring. RED - SD USED-FOR superresolution. denoisers COMPARE RED - SD. RED - SD COMPARE denoisers. Task is optimization. Generic is network. OtherScientificTerm is activation functions. ","This paper proposes a new image denoiser that can be used as a prior for regularization with denosing (RED). PnP/RED have been shown to converge to contractive, nonexpansive, and/or firmly non-discriminative denoisers. This paper proposes to use the gradients of potential functions with respect to the input image as a proxy for the corresponding gradients for the denoiser in order to improve the Pp/RED convergence. The main idea is to use a deep neural network as a potential function for the potential function and then use the gradient of the function for denoising. This procedure is called potential-driven denoisers. The network is parameterized as a convex function, and the activation functions are defined as the sum of the parameters of the potential functions. The potential function is then used to train the denoiter. The proposed RED-SD algorithm is based on the use of a potential-based, but not necessarily a denoised, potential function. Experiments are conducted on image deblurring, superresolution, and image-to-image learning tasks. The results show that using the proposed denoers improves the performance compared to PNP-PGD, RED-SDA, and RED-D. The experiments also show that the proposed densoisers outperform the PnPs and REDs in the experiments."
2895,SP:1e86c162b8e8d652a0590b66aa5f7c363955cc5b,"image denoising based CNNs USED-FOR regularizers. explicit regularization USED-FOR optimization algorithm. gradient of smooth potential functions USED-FOR regularizers. smoothness USED-FOR denoisers. smooth activation functions USED-FOR differential continuity. MSE loss USED-FOR residual denoising fashion. smooth activation functions PART-OF denoisers. back - tracking USED-FOR convergence. steepest descent variant of RED USED-FOR algorithm. gradient USED-FOR steepest descent variant of RED. gaussian deblur CONJUNCTION super - resolution. super - resolution CONJUNCTION gaussian deblur. PnP CONJUNCTION RED. RED CONJUNCTION PnP. method COMPARE methods. methods COMPARE method. image inverse problems EVALUATE-FOR methods. image inverse problems EVALUATE-FOR method. RED USED-FOR methods. PnP USED-FOR methods. super - resolution HYPONYM-OF image inverse problems. gaussian deblur HYPONYM-OF image inverse problems. Method are regularization by denoising ( RED ), explicit regularizer, and optimization theory. ","This paper proposes a new optimization algorithm based on explicit regularization by denoising (RED), which is a generalization of the regularization based CNNs to regularizers based on the gradient of smooth potential functions. The main idea is to use the smoothness of the denoisers as an explicit regularizer to improve the convergence of the optimization algorithm. In particular, the authors propose to use smooth activation functions in the original denoiser to enforce differential continuity and to use MSE loss to approximate the residual denoizing fashion. The authors also propose a steepest descent variant of RED that uses the gradient from the original algorithm as the gradient and use back-tracking to improve convergence. Experiments on image inverse problems such as gaussian deblur and super-resolution show that the proposed method outperforms the existing methods based on PnP and RED. "
2896,SP:1e86c162b8e8d652a0590b66aa5f7c363955cc5b,"RED algorithm USED-FOR inverse problems. explicit potential function USED-FOR gradient. gradient USED-FOR neural - net denoiser. explicit potential function USED-FOR neural - net denoiser. RED CONJUNCTION plug - and - play ( PnP)-type algorithms. plug - and - play ( PnP)-type algorithms CONJUNCTION RED. convergence EVALUATE-FOR plug - and - play ( PnP)-type algorithms. convergence EVALUATE-FOR RED. explicit cost function USED-FOR RED algorithm. adaptive stepsize selection USED-FOR RED. scheme HYPONYM-OF adaptive stepsize selection. PSNR and SSIM performance EVALUATE-FOR RED and PnP algorithms. OtherScientificTerm are Jacobian, and explicit cost. Metric is stability. ","This paper proposes a RED algorithm for inverse problems, which uses an explicit potential function to compute the gradient of a neural-net denoiser. The explicit cost function is defined as the difference between the Jacobian of the gradient with respect to the explicit cost. The authors compare the convergence of RED and plug-and-play (PNP)-type algorithms and show that RED converges faster with adaptive stepsize selection (e.g., the scheme proposed in [1]). The authors also compare the PSNR and SSIM performance of the proposed RED and PnP algorithms, and show improved stability."
2897,SP:1e86c162b8e8d652a0590b66aa5f7c363955cc5b,"strict conditions FEATURE-OF denoiser. explicit regularizers USED-FOR denoisers. RED USED-FOR explicit regularizers. CNN HYPONYM-OF denoisers. denoiser CONJUNCTION regularizer. regularizer CONJUNCTION denoiser. deep denoisers USED-FOR RED algorithms. smooth activation functions USED-FOR CNN. CNN USED-FOR smooth potential functional. smooth potential functional USED-FOR regularizer. gradient USED-FOR noise residual. potential functional USED-FOR denoiser. noise residual USED-FOR AWGN. method USED-FOR gradient - based RED update. back - tracking trick USED-FOR framework. gradient USED-FOR gradient - based RED update. nonconvex optimization analysis USED-FOR convergence analysis. Method are regularization by denoising ( RED ), and explicit RED regularizer. Generic is they. OtherScientificTerm is objective function. ","This paper studies regularization by denoising (RED). RED is a generalization of explicit regularizers for denoisers (e.g., CNN) that satisfy strict conditions. RED algorithms have been shown to converge to a point where they converge to the optimal objective function. However, RED algorithms are limited to deep densoisers. In this paper, the authors propose to extend the explicit RED regularizer to the case where the denoiser and the regularizer are smooth potential functional, which is achieved by using smooth activation functions of CNN. The proposed framework is based on the back-tracking trick, and the method is used to compute a gradient-based RED update using the gradient of the noise residual of AWGN. The convergence analysis is performed using nonconvex optimization analysis."
2898,SP:da92e936f88b3842ca82c2914413b129ca35890f,"RhythmicNet USED-FOR musical soundtrack. human movements FEATURE-OF musical soundtrack. beat CONJUNCTION visual motions. visual motions CONJUNCTION beat. musical rhythm CONJUNCTION beat. beat CONJUNCTION musical rhythm. musical rhythm CONJUNCTION visual motions. visual motions CONJUNCTION musical rhythm. Video2Rhythm CONJUNCTION Rhythm2Drum. Rhythm2Drum CONJUNCTION Video2Rhythm. Rhythm2Drum CONJUNCTION Drum2Music. Drum2Music CONJUNCTION Rhythm2Drum. parts PART-OF It. Rhythm2Drum PART-OF It. Video2Rhythm PART-OF It. Drum2Music PART-OF It. Video2Rhythm HYPONYM-OF parts. Drum2Music HYPONYM-OF parts. Rhythm2Drum HYPONYM-OF parts. OtherScientificTerm are sync sounds of audio sources, and body movements. ","This paper proposes RhythmicNet, which aims to learn a musical soundtrack inspired by human movements. It consists of three parts: Video2Rhythm, Rhythm2Drum, and Rhythm2Music, which is composed of a combination of parts that are composed of musical rhythm, beat, and visual motions. It is trained by sampling from the sync sounds of audio sources, which are then combined with body movements. "
2899,SP:da92e936f88b3842ca82c2914413b129ca35890f,Video2Rhythm USED-FOR rhythmic nature of free body movements. Rhythm2Drum module USED-FOR drum. Material is human motion videos. OtherScientificTerm is piano. Method is Drum2Music. ,"This paper proposes Video2Rhythm, which aims to capture the rhythmic nature of free body movements in human motion videos. The key idea is to use the Rhythm2Drum module to generate a drum that can be played on a piano. Experiments show that the proposed Drum2Music can achieve state-of-the-art results."
2900,SP:da92e936f88b3842ca82c2914413b129ca35890f,"approach USED-FOR rhythmic music. human movement video USED-FOR approach. Video2Rhythm USED-FOR rhythm. Rhytm2Drum USED-FOR rhythmic patterns. supervised learning paradigm USED-FOR steps. Generic is model. OtherScientificTerm are drum beats, and multi - instrument song. Method is Drum2Music. ","This paper presents an approach to learning rhythmic music from a human movement video. The proposed model, Video2Rhythm, learns to predict the rhythm of a sequence of steps using a supervised learning paradigm. Rhytm2Drum learns to generate rhythmic patterns that are similar to the drum beats in a multi-instrument song, and Drum2Music learns to imitate the sounds of an instrument."
2901,SP:da92e936f88b3842ca82c2914413b129ca35890f,method USED-FOR musical soundtracks. musical soundtracks USED-FOR silent videos of human activity. Video2Rhythm CONJUNCTION Rhythm2Drum. Rhythm2Drum CONJUNCTION Video2Rhythm. Rhythm2Drum CONJUNCTION Drum2Music. Drum2Music CONJUNCTION Rhythm2Drum. parts PART-OF method. Drum2Music HYPONYM-OF parts. Video2Rhythm HYPONYM-OF parts. Rhythm2Drum HYPONYM-OF parts. sequential modules USED-FOR musical soundtrack. Video2Rhythm component USED-FOR musical beats. It USED-FOR rhythm. U - net USED-FOR velocity and offset prediction. transformer - based encoder - decoder architecture CONJUNCTION U - net. U - net CONJUNCTION transformer - based encoder - decoder architecture. Rhythm2Drum component USED-FOR drum pattern. transformer - based encoder - decoder architecture USED-FOR GrooveVAE. transformer - based encoder - decoder architecture USED-FOR drum pattern. transformer - based encoder - decoder architecture USED-FOR Rhythm2Drum component. piano CONJUNCTION guitar accompaniment. guitar accompaniment CONJUNCTION piano. Drum2Music component USED-FOR musical soundtrack. guitar accompaniment PART-OF drum track. guitar accompaniment USED-FOR musical soundtrack. guitar accompaniment USED-FOR Drum2Music component. piano USED-FOR Drum2Music component. Method is RhythmNet. Material is silent video. Generic is components. ,"This paper presents a method for generating musical soundtracks for silent videos of human activity. The method consists of three parts: Video2Rhythm, Rhythm2Drum, and Drum2Music. The Video 2Rhythm component generates musical beats for each frame of a silent video. It predicts the rhythm of each frame using a U-net that is trained for velocity and offset prediction. In RhythmNet, the musical soundtrack is generated using sequential modules. In GrooveVAE, a transformer-based encoder-decoder architecture is used to generate the drum pattern of the Drum2Drrum component, and a new drum track is generated by adding a piano and a guitar accompaniment to the drum track. The three components are trained separately."
2902,SP:da92e936f88b3842ca82c2914413b129ca35890f,guitar chords CONJUNCTION drum beat. drum beat CONJUNCTION guitar chords. piano roll CONJUNCTION guitar chords. guitar chords CONJUNCTION piano roll. method USED-FOR rhythmic sound. instruments PART-OF rhythmic sound. piano roll HYPONYM-OF instruments. drum beat HYPONYM-OF instruments. guitar chords HYPONYM-OF instruments. keypoint estimation network CONJUNCTION velocity features. velocity features CONJUNCTION keypoint estimation network. keypoint estimation network USED-FOR human action dynamics. velocity features USED-FOR human action dynamics. features USED-FOR rhythmic pattern. rhythmic pattern USED-FOR drum beats. piano roll CONJUNCTION guitar chords. guitar chords CONJUNCTION piano roll. drum beat sequence USED-FOR late - stage generation. late - stage generation USED-FOR piano roll. guitar chords USED-FOR late - stage generation. opinion scores EVALUATE-FOR architecture. OtherScientificTerm is human activity. Generic is intermediate representations. ,"This paper proposes a method to generate a rhythmic sound consisting of three instruments: piano roll, guitar chords, and drum beat. The key idea is to use a keypoint estimation network and velocity features to model human action dynamics. These features are then used to learn a new rhythmic pattern for drum beats, which is then used for late-stage generation of piano roll and guitar chords based on the drum beat sequence. The proposed architecture is evaluated on a variety of datasets with different opinion scores and is shown to be able to generate sounds that are consistent with human activity. The intermediate representations are also evaluated."
2903,SP:0f7ff312a242a553dc9ecf35b421e58fb2d50a26,"stages PART-OF RL ( both online and offline ) algorithms. policy evaluation HYPONYM-OF stages. Method is Q - function. Task are policy improvement, and offline RL. OtherScientificTerm is policy. ","This paper proposes two stages in RL (both online and offline) algorithms: (1) policy evaluation, where the Q-function is used to evaluate the quality of the learned policy, and (2) policy improvement, where a policy is updated based on the current state of the policy. The paper is well-written and well-motivated. However, the paper suffers from a lack of comparison to prior work on offline RL."
2904,SP:0f7ff312a242a553dc9ecf35b421e58fb2d50a26,"off - policy evaluation USED-FOR offline RL. one - step methods COMPARE multistep and iterative ones. multistep and iterative ones COMPARE one - step methods. Task are Offline RL, and Off - Policy Evaluation. Method are offline RL methods, and multistep and iterative offline RL methods. ",Offline RL: Off-Policy Evaluation. This paper focuses on the problem of off-policy evaluation in offline RL. The authors compare one-step methods with multistep and iterative ones. The main finding is that the one step offline RL methods are more efficient than the other two. The paper also provides a theoretical analysis of the differences between the multisteep and the iterative offlineRL methods.
2905,SP:0f7ff312a242a553dc9ecf35b421e58fb2d50a26,"1 - step heuristic USED-FOR Q - function based offline RL methods. Extrapolation Error PART-OF Deep Offline Reinforcement Learning. Generic are method, and approach. ",This paper proposes a 1-step heuristic for Q-function based offline RL methods. The method is based on the idea of Extrapolation Error in Deep Offline Reinforcement Learning. The paper provides a theoretical analysis of the proposed approach.
2906,SP:0f7ff312a242a553dc9ecf35b421e58fb2d50a26,"policy improvement step COMPARE iterative benchmarks. iterative benchmarks COMPARE policy improvement step. iterative policy improvement USED-FOR offline reinforcement learning. D4RL benchmark EVALUATE-FOR policy improvement step. D4RL benchmark EVALUATE-FOR iterative benchmarks. regularization hyperparameters USED-FOR variants. MSE FEATURE-OF estimated Q function. Gridworld environments EVALUATE-FOR multi - step overestimation. OtherScientificTerm are multi - step ”, noise signal, and propagation of signal. Method is multi - step algorithms. ","This paper studies the problem of iterative policy improvement in offline reinforcement learning, where the policy improvement step is compared to iterative benchmarks on the D4RL benchmark. The authors propose two variants of multi-step algorithms with different regularization hyperparameters, and show that “multi-step overestimation” occurs when the MSE of the estimated Q function is larger than the noise signal. They also show that the propagation of signal is affected by the number of steps. Finally, the authors conduct experiments on Gridworld environments to demonstrate the multi-steep overestimation."
2907,SP:0f7ff312a242a553dc9ecf35b421e58fb2d50a26,"baseline USED-FOR offline - RL paradigm. D4RL benchmark EVALUATE-FOR one - step of constrained policy improvement. baseline COMPARE methods. methods COMPARE baseline. regularization FEATURE-OF behavior policy. learning FEATURE-OF behavior policy. task CONJUNCTION gridworld experiment. gridworld experiment CONJUNCTION task. task EVALUATE-FOR off policy evaluation. multi - step approaches COMPARE one - step methods. one - step methods COMPARE multi - step approaches. Task is policy evaluation. Method are offline - RL algorithms, and off - policy evaluation. Generic is algorithms. ","This paper presents a baseline for the offline-RL paradigm, which is evaluated on the D4RL benchmark for one-step of constrained policy improvement. Compared to the baseline, the proposed methods are more efficient. The paper also provides a theoretical analysis of policy evaluation in the context of the existing state-of-the-art offline-RRL algorithms. In particular, the authors show that the learned behavior policy is subject to some form of regularization, and that the learning of the behavior policy does not depend on the current state of the algorithms. Finally, the paper provides an empirical evaluation of off-policy evaluation on a simple task and a gridworld experiment. The results show that multi-step approaches perform better than one-stage methods."
2908,SP:0346eba4f587acbe3492d039066f1737360fd870,"convex relaxation USED-FOR low - rank matrix recovery problems. convex relaxation USED-FOR optimization problems. projection USED-FOR spectraherdon. projection USED-FOR projected subgradient descent. projected subgradient descent USED-FOR problems. projection USED-FOR problems. pointwise maximum FEATURE-OF smooth functions. pointwise maximum FEATURE-OF non - smooth objective function. projected extragradient method USED-FOR saddle point problem. saddle point problem USED-FOR optimization problem. low - rank projection USED-FOR projection. OtherScientificTerm are positive semidefinite matrices, full SVD, and generalized strict complementarity ” condition. Generic is algorithm. Method is low - rank SVD. ","This paper studies optimization problems with a convex relaxation for low-rank matrix recovery problems. The main idea is to use a projection for spectraherdon, which can be used in the context of projected subgradient descent for these problems. In particular, the authors consider the case of positive semidefinite matrices, where the pointwise maximum of the smooth functions is larger than that of the non-smooth objective function. In this case, the optimization problem can be viewed as a saddle point problem, and the authors propose a projected extragradient method to solve the saddle point objective. The authors show that the proposed algorithm is equivalent to the full SVD under the “generalized strict complementarity” condition, and that the projection can be seen as a low-ranking projection of the original low-ranked SVD."
2909,SP:0346eba4f587acbe3492d039066f1737360fd870,extragradient method USED-FOR low - rank and nonsmooth matrix optimization problems. method USED-FOR optimal solution. ,This paper proposes an extension of an existing extragradient method for solving low-rank and nonsmooth matrix optimization problems. The method is shown to converge to an optimal solution.
2910,SP:0346eba4f587acbe3492d039066f1737360fd870,method USED-FOR nonsmooth problems. method USED-FOR smooth low - rank matrix optimization problems. smooth saddle - point problem USED-FOR nonsmooth problem. algorithm USED-FOR saddle - point problem. method USED-FOR problem. OtherScientificTerm is auxiliary parameters. ,"This paper proposes a method for solving nonsmooth problems. The method is applied to solving smooth low-rank matrix optimization problems. In particular, the authors propose to solve a nonsmoothed problem as a smooth saddle-point problem. The authors propose a new algorithm for solving the saddle-points problem and show that the proposed method can solve the problem in a way that does not require auxiliary parameters. The paper is well-written and easy to follow."
2911,SP:0346eba4f587acbe3492d039066f1737360fd870,"SVDs COMPARE low - rank SVDs. low - rank SVDs COMPARE SVDs. SVDs USED-FOR algorithms. projected sub - gradient descent HYPONYM-OF algorithms. SVDs USED-FOR smooth counterpart of the same problem. replacement USED-FOR non - smooth objective functions. low - rank SVDs USED-FOR non - smooth functions. smooth function CONJUNCTION point - wise maximum of affine functions. point - wise maximum of affine functions CONJUNCTION smooth function. low0rank SVDs COMPARE full SVDs. full SVDs COMPARE low0rank SVDs. saddle point structure FEATURE-OF non - smooth functions. low - rank SVDs USED-FOR non - smooth objectives. low - rank SVDs COMPARE full SVDs. full SVDs COMPARE low - rank SVDs. Task is convex, non - smooth objective function. Generic is problem. Metric are computational complexity, and O(1 / t ) convergence. OtherScientificTerm is initialization. ","This paper studies the problem of solving a convex, non-smooth objective function, where the objective is a function of a smooth function and a point-wise maximum of affine functions. The authors prove that SVDs are equivalent to the smooth counterpart of the same problem, and show that algorithms based on the learned algorithms (such as projected sub-gradient descent) can be viewed as a replacement for the original low-rank version of the original objective functions. They also show that the computational complexity of the problem can be reduced to O(1/t) convergence when the initialization is fixed. Finally, they show that for non-slooth objectives with saddle point structure, the low-ranks of low0rank SVDS are more efficient than full SVD."
2912,SP:0346eba4f587acbe3492d039066f1737360fd870,strict complementarity USED-FOR nonsmooth problems. maximum of smooth ones CONJUNCTION minimax form. minimax form CONJUNCTION maximum of smooth ones. maximum of smooth ones USED-FOR nonsmooth problems. low - rank projections USED-FOR projected extragradient method. method USED-FOR classical low - rank approximation problems. Task is nonsmooth and low - rank matrix optimization problems. Method is low - rank projected subgradient steps. ,"This paper studies nonsmooth and low-rank matrix optimization problems. The authors propose a projected extragradient method based on low-ranks projections, which is motivated by the observation that strict complementarity is necessary for solving nonssmooth problems with maximum of smooth ones and minimax form. The proposed method can be applied to both classical low-ratio approximation problems and is shown to be able to solve both classical and non-classical low-rate approximation problems. In particular, the authors show that the proposed method converges to the optimal solution of the original problem in a few high-rank projected subgradient steps."
2913,SP:d39f1d77d9919f897ccf82958b71be8798523923,Robinson's decomposition of CATE USED-FOR binary $ T$. separable assumption ( A3 ) FEATURE-OF outcome structural equation ( $ Y$ ). synthetic experiments CONJUNCTION real data analysis. real data analysis CONJUNCTION synthetic experiments. Method is CATE estimation approach. OtherScientificTerm is graph. ,"This paper proposes a CATE estimation approach. The main idea is to use Robinson's decomposition of CATE for binary $T$ as a separable assumption (A3) on the outcome structural equation ($Y$), which is then used to derive a graph $Y$ which is used to estimate $T$. The paper provides both synthetic experiments and real data analysis."
2914,SP:d39f1d77d9919f897ccf82958b71be8798523923,CATE estimation method USED-FOR graph - structured treatment. Robinson Decomposition USED-FOR CATE estimation method. Robinson Decomposition USED-FOR It. small world datasets EVALUATE-FOR GIN. ,This paper proposes a novel CATE estimation method for graph-structured treatment. It is based on the idea of Robinson Decomposition. Experiments on small world datasets demonstrate the effectiveness of GIN.
2915,SP:d39f1d77d9919f897ccf82958b71be8798523923,graph intervention network ( GIN ) USED-FOR conditional average causal effects. Robinson decomposition USED-FOR GIN. GIN USED-FOR training. synthetic outcome functions FEATURE-OF datasets. synthetic outcome functions EVALUATE-FOR method. datasets EVALUATE-FOR method. OtherScientificTerm is graph - structured. ,"This paper proposes a graph intervention network (GIN) to model conditional average causal effects. The GIN is based on the Robinson decomposition and is graph-structured. During training, the GIN takes as input the current state of the graph and predicts the next state. The proposed method is evaluated on two datasets with synthetic outcome functions."
2916,SP:d39f1d77d9919f897ccf82958b71be8798523923,"R - learner USED-FOR graph - treatment setting. Task is estimating conditional average treatment effect. OtherScientificTerm is graph. Method are Robinson decomposition, and two - stage algorithm. Generic is algorithm. ","This paper studies the problem of estimating conditional average treatment effect in a graph-treatment setting with an R-learner. The authors propose a two-stage algorithm, where the first stage decomposes the graph into a set of nodes, and the second stage uses the Robinson decomposition to estimate the effect of each node. The proposed algorithm is evaluated on a variety of datasets."
2917,SP:d39f1d77d9919f897ccf82958b71be8798523923,representation USED-FOR method. R - learner USED-FOR method. graph structure USED-FOR graph neural network. graph neural network USED-FOR representation. R - learner USED-FOR nuisance components. propensity features CONJUNCTION CATE. CATE CONJUNCTION propensity features. R - learner COMPARE method. method COMPARE R - learner. gradient - based methods USED-FOR method. gradient - based methods USED-FOR CATE. OtherScientificTerm is binary treatment. ,"This paper proposes a method that uses an R-learner to extract nuisance components from a representation learned by a graph neural network with a graph structure. The R-learner is trained in a way that is similar to the method proposed in [1], but with a binary treatment. The authors compare the proposed method with gradient-based methods on propensity features and CATE."
2918,SP:eeb42a1e48857f976a647eb8d86d25c9012962d5,discrete case FEATURE-OF causal inference. matrix equations USED-FOR probability axioms. graphically driven formulae CONJUNCTION matrix multiplications. matrix multiplications CONJUNCTION graphically driven formulae. pseudoinverse USED-FOR intermediary criteria. OtherScientificTerm is proxy variable - based identification conditions. ,"This paper studies causal inference in the discrete case, where the underlying probability axioms can be expressed as matrix equations. The main contribution of this paper is the introduction of proxy variable-based identification conditions, which are based on graphically driven formulae and matrix multiplications. The pseudoinverse is used to define intermediary criteria, which is then used to derive the final objective."
2919,SP:eeb42a1e48857f976a647eb8d86d25c9012962d5,proxies / surrogates USED-FOR identifiability. matrix equations USED-FOR causal effect identifiability. identification paradigms USED-FOR identifiability. notation USED-FOR synthesis. algorithm USED-FOR identification. matrix equations paradigm USED-FOR algorithm. approach COMPARE approaches. approaches COMPARE approach. Method is canonical ID algorithm. Generic is it. ,"This paper proposes a canonical ID algorithm, which is based on the matrix equations paradigm for causal effect identifiability. The key idea is to use proxies/surrogates as identifiable approximations of the original matrix equations in order to improve the identibility performance. The paper is well-motivated and well-written. The identification paradigms are well-studied in the literature, and the paper is clearly written and easy to follow. The synthesis is done using a simple notation, and it is clear that the proposed algorithm can be used for identification. The proposed approach is compared to other approaches and the results are promising."
2920,SP:eeb42a1e48857f976a647eb8d86d25c9012962d5,unobserved confounders CONJUNCTION observable variables. observable variables CONJUNCTION unobserved confounders. proxy variables USED-FOR second. methods USED-FOR approach. Task is graph based identification. Generic is first. OtherScientificTerm is graph. ,This paper addresses the problem of graph based identification. The authors propose two methods for this approach. The first is to use proxy variables to estimate the relationship between the unobserved confounders and the observable variables. The second is to learn a graph that is independent of the underlying variables.
2921,SP:eeb42a1e48857f976a647eb8d86d25c9012962d5,graphical criteria CONJUNCTION matrix equations. matrix equations CONJUNCTION graphical criteria. graphical criteria USED-FOR approach. matrix equations USED-FOR approach. graphical constraints PART-OF causal graph. graphical constraints USED-FOR matrix equations of probability distributions. intermediary pseudoinverse criteria USED-FOR causal effect. inverse of a matrix USED-FOR intermediary pseudoinverse criteria. Task is causal effect identification problem. Method is graphical and matrical approaches. OtherScientificTerm is proxy - based criteria. ,"This paper studies the causal effect identification problem and proposes an approach that combines graphical criteria and matrix equations of probability distributions. The graphical constraints are added to the causal graph and the matrix equations are used to derive the graphical constraints. The intermediary pseudoinverse criteria is defined as the inverse of a matrix, which is then used to estimate a causal effect. Both graphical and matrical approaches are considered. The proxy-based criteria are also considered."
2922,SP:eeb42a1e48857f976a647eb8d86d25c9012962d5,"graphical and matrical approaches PART-OF graphical identification approach. matrix chain - rule and inversion criteria USED-FOR proxy - based approaches. C - factorization USED-FOR causal effect identification algorithm. Generic are algorithm, and algorithms. ",This paper proposes a graphical identification approach that combines both graphical and matrical approaches. The authors propose proxy-based approaches based on matrix chain-rule and inversion criteria. They also propose a causal effect identification algorithm based on C-factorization. The proposed algorithm is evaluated on several datasets and compared to existing algorithms.
2923,SP:db15860d08418f6bc792c2ade2eade32840a12b8,Dual Curriculum Design ( DCD ) HYPONYM-OF environment generation framework. teacher agents CONJUNCTION student agent. student agent CONJUNCTION teacher agents. student agent PART-OF framework. teacher agents PART-OF framework. grid world domain CONJUNCTION CarRacing domain. CarRacing domain CONJUNCTION grid world domain. grid world domain EVALUATE-FOR method. CarRacing domain EVALUATE-FOR method. method COMPARE baselines. baselines COMPARE method. grid world domain EVALUATE-FOR baselines. CarRacing domain EVALUATE-FOR baselines. PAIRD HYPONYM-OF baselines. Task is Prioritized Level Replay ( PLR ). OtherScientificTerm is student's agent's regret. ,This paper proposes a new environment generation framework called Dual Curriculum Design (DCD) which is an extension of Prioritized Level Replay (PLR). The proposed framework consists of two teacher agents and a student agent. The teacher agents are trained to maximize the student's agent's regret. The proposed method is evaluated on a grid world domain and a CarRacing domain and compared to baselines such as PAIRD.
2924,SP:db15860d08418f6bc792c2ade2eade32840a12b8,distribution of environments USED-FOR learning agent. levels / environments USED-FOR policy. co - evolving teachers PART-OF Dual Curriculum Design. replay teacher HYPONYM-OF co - evolving teachers. PLR CONJUNCTION REPAIRED robustness guarantees. REPAIRED robustness guarantees CONJUNCTION PLR. theory CONJUNCTION REPAIRED robustness guarantees. REPAIRED robustness guarantees CONJUNCTION theory. PLR FEATURE-OF theory. emergent complexity CONJUNCTION scalability. scalability CONJUNCTION emergent complexity. zero - shot generalization CONJUNCTION emergent complexity. emergent complexity CONJUNCTION zero - shot generalization. scalability EVALUATE-FOR method. emergent complexity EVALUATE-FOR method. zero - shot generalization EVALUATE-FOR method. Method is unsupervised environment design. ,"This paper studies unsupervised environment design, where a learning agent is trained on a distribution of environments. The goal is to learn a policy that can generalize to new levels/environments. The authors propose Dual Curriculum Design with co-evolving teachers (e.g., replay teacher). The authors combine theory with PLR and REPAIRED robustness guarantees. The proposed method is evaluated on zero-shot generalization, emergent complexity and scalability."
2925,SP:db15860d08418f6bc792c2ade2eade32840a12b8,"environment selection scheme USED-FOR policy. unsupervised environment design HYPONYM-OF problem. curriculum learning setting USED-FOR policy. approaches PART-OF method. PAIRED HYPONYM-OF approaches. two - teacher dual curriculum game USED-FOR method. method COMPARE PAIRED. PAIRED COMPARE method. Method are generator, and sampling scheme. ","This paper studies a problem called unsupervised environment design, i.e., the problem of learning a policy in an environment selection scheme without access to the generator. The authors propose a curriculum learning setting where the policy is trained in a two-teacher dual curriculum game, where the generator is trained to generate the environment and the teacher is trained with a sampling scheme. The proposed method is a combination of two approaches: (1) PAIRED and (2) a new method based on a modified version of the two-tutor Dual curriculum game. Experiments show that the proposed method outperforms PAIRED."
2926,SP:db15860d08418f6bc792c2ade2eade32840a12b8,"unsupervised environment design CONJUNCTION prioritized experience replay. prioritized experience replay CONJUNCTION unsupervised environment design. curriculum design CONJUNCTION unsupervised environment design. unsupervised environment design CONJUNCTION curriculum design. them PART-OF dual curriculum game. approaches USED-FOR games. equilibria FEATURE-OF joint game. equilibria CONJUNCTION equilibria. equilibria CONJUNCTION equilibria. replay - enhanced PAIRED USED-FOR UED technique. PLR USED-FOR UED technique. replay - enhanced PAIRED HYPONYM-OF algorithms. mazes CONJUNCTION car racing domains. car racing domains CONJUNCTION mazes. car racing domains EVALUATE-FOR approaches. mazes EVALUATE-FOR approaches. OtherScientificTerm are experienced trajectories, and PLR teacher. ","This paper proposes to combine curriculum design, unsupervised environment design, and prioritized experience replay to solve the problem of learning from experienced trajectories. The authors propose to combine them into a dual curriculum game, where the goal is to find the equilibria of the joint game. The proposed approaches are applied to three games: mazes, car racing domains, and two algorithms: (1) replay-enhanced PAIRED, which is a UED technique based on PLR, and (2) a PLR teacher, which learns from past experience. The approaches are evaluated on mazes and car racing environments."
2927,SP:db15860d08418f6bc792c2ade2eade32840a12b8,"REPAIRED HYPONYM-OF PLR - based replay mechanism. environment distribution USED-FOR buffer. maze environment CONJUNCTION car - racing tracks. car - racing tracks CONJUNCTION maze environment. approaches COMPARE PLR. PLR COMPARE approaches. approaches COMPARE baselines. baselines COMPARE approaches. PLR COMPARE baselines. baselines COMPARE PLR. Generic is framework. Method is Dual Curriculum Design. OtherScientificTerm are Nash Equilibrium, policy - gradient updates, and PLR buffer. ","This paper proposes REPAIRED, a PLR-based replay mechanism. The framework is inspired by Dual Curriculum Design, where the buffer is sampled from an environment distribution, and the goal is to find a Nash Equilibrium where the policy-gradient updates converge to a fixed point in the PLR buffer. Experiments are conducted on a maze environment and two car-racing tracks, and compared with other approaches and PLR. Results show that the proposed approaches outperform the baselines."
2928,SP:9ed528da4b67f22678303cfd975aafe678db6411,shuffled model of DP USED-FOR multi - armed bandits. algorithms USED-FOR centralized model. regret EVALUATE-FOR algorithms. private summation algorithm USED-FOR arm elimination algorithm. ,This paper studies the problem of multi-armed bandits with a shuffled model of DP. The authors propose two algorithms to solve the centralized model and show that the regret of the proposed algorithms is O(1/\sqrt{T})$. The authors also propose an arm elimination algorithm based on a private summation algorithm.
2929,SP:9ed528da4b67f22678303cfd975aafe678db6411,shuffle model of DP USED-FOR multi - armed bandits. shuffle DP CONJUNCTION Bernoulli rewards. Bernoulli rewards CONJUNCTION shuffle DP. algorithms USED-FOR bandits. SDP - AE CONJUNCTION VB - SDP - AE. VB - SDP - AE CONJUNCTION SDP - AE. SDP - AE HYPONYM-OF algorithms. VB - SDP - AE HYPONYM-OF algorithms. shuffle DP USED-FOR bandits. Bernoulli rewards USED-FOR bandits. Distribution - dependent and independent regret bounds EVALUATE-FOR algorithms. VB - SDP - AE COMPARE centralised DP. centralised DP COMPARE VB - SDP - AE. additive regret EVALUATE-FOR VB - SDP - AE. OtherScientificTerm is privacy. ,"This paper studies the shuffle model of DP for multi-armed bandits. The authors propose two algorithms for bandits based on shuffle DP and Bernoulli rewards. Distribution-dependent and independent regret bounds are provided for the proposed algorithms, SDP-AE and VB-SDP -AE. The main contribution of the paper is the theoretical analysis of the effect of privacy. In particular, the authors show that the additive regret of the proposed VB–SDP–AE is better than the centralised DP."
2930,SP:9ed528da4b67f22678303cfd975aafe678db6411,"approximate differential privacy FEATURE-OF multi - armed bandit problem. centralized pure DP model CONJUNCTION centralized approximate DP model. centralized approximate DP model CONJUNCTION centralized pure DP model. centralized approximate DP model CONJUNCTION local pure DP model. local pure DP model CONJUNCTION centralized approximate DP model. multi - armed bandits USED-FOR models of privacy. centralized approximate DP model HYPONYM-OF models of privacy. centralized pure DP model HYPONYM-OF models of privacy. local pure DP model HYPONYM-OF models of privacy. shuffle model of privacy USED-FOR multi - armed bandit problem. reward USED-FOR shuffle DP mechanism. it USED-FOR multiplicative $ 1/\epsilon^2 $ dependence. regret bound USED-FOR local DP model. bandit literature CONJUNCTION differential privacy literature. differential privacy literature CONJUNCTION bandit literature. shuffle DP algorithm USED-FOR MAB. private binary summation USED-FOR shuffle DP algorithm. private binary summation PART-OF shuffle DP literature. non - private arm elimination algorithm USED-FOR algorithms. static batch sizes USED-FOR algorithm. dynamic batch sizes USED-FOR algorithm. Generic is model. OtherScientificTerm is ad. Method are central DP model, and differential algorithms. Metric is regret. ","This paper studies the approximate differential privacy of a multi-armed bandit problem. The authors consider models of privacy in multi-arm bandits, including a centralized pure DP model, a centralized approximate DP model and a local pureDP model. The shuffle model of privacy is used in the multi-arms bandit (MAB) problem, where the reward for the shuffle DP mechanism is a function of the number of arms in the model. In particular, the authors show that the regret bound for the local DP model is $O(1/\epsilon^2)$, where $1$ is the ad and $2$ is a central DP model. They also show that a shuffle DP algorithm with private binary summation is equivalent to MAB, and show that it has a multiplicative $1/2$ dependence. Finally, they propose two algorithms based on a non-private arm elimination algorithm, which are shown to have a regret of $O(\sqrt{T})$. They show that their algorithm can be trained with static batch sizes, and with dynamic batch sizes. They compare their results with those of the bandit literature and the differential privacy literature, showing that their differential algorithms have a lower regret."
2931,SP:9ed528da4b67f22678303cfd975aafe678db6411,shuffle model of differential privacy USED-FOR multi - armed bandit setting. regret guarantees FEATURE-OF central DP model. privacy guarantees FEATURE-OF bandit algorithm. regret guarantees EVALUATE-FOR local model. optimal error guarantees FEATURE-OF private binary summation mechanism. private binary summation mechanism USED-FOR arm - elimination style algorithms. multiplicative dependence FEATURE-OF constant batch size policy. Generic is policy. OtherScientificTerm is additive dependence. ,"This paper studies the multi-armed bandit setting under the shuffle model of differential privacy. The authors show that the regret guarantees of the central DP model can be used to derive the privacy guarantees of a bandit algorithm. The regret guarantees for the local model can also be derived. In addition, the authors provide optimal error guarantees for a private binary summation mechanism for arm-elimination style algorithms. Finally, they show that for a constant batch size policy with multiplicative dependence on the number of arms, the policy converges to a solution with additive dependence."
2932,SP:9ed528da4b67f22678303cfd975aafe678db6411,shuffled model of differential privacy USED-FOR multi - armed bandit problem. multi - armed bandit setting FEATURE-OF shuffled differential privacy ( SDP ). SDP algorithm COMPARE multi - armed bandit algorithm. multi - armed bandit algorithm COMPARE SDP algorithm. multi - armed bandit algorithm USED-FOR central privacy model. regret guarantees FEATURE-OF SDP algorithm. ,"This paper studies the multi-armed bandit problem with a shuffled model of differential privacy. The shuffled differential privacy (SDP) is studied in a multi-arm bandit setting. The authors show that the regret guarantees of the SDP algorithm are better than that of the standard multimodal (multi-armed) bandit algorithm. In addition, the authors provide a theoretical analysis of the central privacy model of the multi-(multi-arm, multi-bandit) algorithm."
2933,SP:de2523a5fdebda3573f1063447a7818bf3ed6333,"calibrated regression USED-FOR decision making framework. algorithm USED-FOR distributional prediction. OtherScientificTerm are threshold decisions, Bayes optimal decision, predicted distribution, and decision rule. Method are threshold calibration, and Threshold calibration. Metric are reliability gap, and predicted expected loss. ",This paper proposes a decision making framework based on calibrated regression. The key idea is to use threshold decisions as a proxy for the Bayes optimal decision. Threshold calibration is defined as the difference between the predicted distribution and the true distribution. The authors propose an algorithm for distributional prediction and show that threshold calibration can reduce the reliability gap between the true and the predicted expected loss when the true decision rule is known.
2934,SP:de2523a5fdebda3573f1063447a7818bf3ed6333,"threshold calibration USED-FOR practical hospital scheduling problem. average calibration CONJUNCTION distribution calibration. distribution calibration CONJUNCTION average calibration. algorithm USED-FOR threshold calibration. reduced reliability gap CONJUNCTION true decision loss. true decision loss CONJUNCTION reduced reliability gap. Method are threshold calibration method, and CDF prediction. Material is real data sets. ","This paper studies the problem of threshold calibration in the practical hospital scheduling problem. The authors propose a new threshold calibration method, which is based on the idea that the CDF prediction should be independent of the average calibration and the distribution calibration. The proposed algorithm is able to achieve a state-of-the-art threshold calibration with a reduced reliability gap and a true decision loss. Experiments are conducted on real data sets."
2935,SP:de2523a5fdebda3573f1063447a7818bf3ed6333,regression model USED-FOR conditional CDFs. definition USED-FOR threshold calibration. calibrated quantiles FEATURE-OF conditional CDFs. conditional CDFs USED-FOR definition. distribution calibration USED-FOR regression. quantile calibration USED-FOR regression. loss function CONJUNCTION decision rule. decision rule CONJUNCTION loss function. threshold - calibrated results USED-FOR reliability gap. isotonic regression USED-FOR quantiles. quantile levels CONJUNCTION thresholds. thresholds CONJUNCTION quantile levels. approach USED-FOR quantile levels. isotonic regression USED-FOR approach. calibration errors FEATURE-OF thresholds. uncalibrated models CONJUNCTION calibration maps. calibration maps CONJUNCTION uncalibrated models. uncalibrated models PART-OF calibrated model. calibration maps PART-OF calibrated model. cost - sensitive setup FEATURE-OF real datasets. method COMPARE uncalibrated model. uncalibrated model COMPARE method. method COMPARE quantile calibrated model. quantile calibrated model COMPARE method. quantile calibrated model CONJUNCTION distribution calibrated model. distribution calibrated model CONJUNCTION quantile calibrated model. method COMPARE distribution calibrated model. distribution calibrated model COMPARE method. uncalibrated model CONJUNCTION quantile calibrated model. quantile calibrated model CONJUNCTION uncalibrated model. OtherScientificTerm is CDFs. Method is averaged calibration. Generic is calibration definitions. ,"This paper proposes a new definition of threshold calibration for conditional CDFs with calibrated quantiles. The main idea is to use a regression model to estimate the conditionalCDFs and then use distribution calibration for regression. The loss function and the decision rule are the same, but the CDFs are trained with averaged calibration. The authors show that threshold-calibrated results can reduce the reliability gap between the uncalibrated models and the calibration maps of the calibrated model. The proposed approach uses isotonic regression to estimate quantile levels and thresholds with calibration errors. Experiments on real datasets with a cost-sensitive setup show that the proposed method outperforms the original unaltered model, the quantile calibrated model, and the distribution calibrated model when the calibration definitions are different."
2936,SP:de2523a5fdebda3573f1063447a7818bf3ed6333,decision - making perspective FEATURE-OF calibrated regression. average calibration HYPONYM-OF calibrated regression. distribution calibration USED-FOR decision - making. threshold USED-FOR calibration. algorithm USED-FOR threshold calibration. OtherScientificTerm is regression predictions. ,"This paper studies the decision-making perspective of calibrated regression (i.e., average calibration) from the perspective of distribution calibration. The main contribution of this paper is to propose a new algorithm for threshold calibration, where the threshold is used to evaluate the calibration of the regression predictions."
2937,SP:de2523a5fdebda3573f1063447a7818bf3ed6333,"calibrating "" forecasts USED-FOR downstream decision - making tasks. decision loss HYPONYM-OF decision rule. average calibration HYPONYM-OF calibration. average calibration USED-FOR estimation of the decision loss. algorithm USED-FOR threshold - calibrated forecasts. accuracy EVALUATE-FOR decision loss estimates. threshold - calibration USED-FOR algorithm. accuracy EVALUATE-FOR threshold - calibration. decision loss estimates EVALUATE-FOR threshold - calibrated forecasts. finite data sample USED-FOR algorithm. accuracy EVALUATE-FOR threshold - calibrated forecasts. finite data sample USED-FOR threshold - calibrated forecasts. synthetic downstream decision - making setup FEATURE-OF real - world data. real - world data EVALUATE-FOR algorithm. average and distribution calibration USED-FOR decision loss estimates. distribution CONJUNCTION threshold. threshold CONJUNCTION distribution. threshold CONJUNCTION average calibration. average calibration CONJUNCTION threshold. Method is threshold calibration. ","This paper studies the problem of ""calibrating"" forecasts for downstream decision-making tasks. The authors propose a new type of calibration called ""average calibration"" for the estimation of the decision loss, which is a decision rule. The proposed algorithm is able to obtain threshold-calibrated forecasts with high accuracy on a finite data sample. The algorithm is evaluated on real-world data from a synthetic downstream decision making setup. The main contribution of the paper is the analysis of the relationship between the average and distribution calibration for decision loss estimates. The analysis is based on the fact that the threshold calibration is a function of both the distribution and the threshold."
2938,SP:f55160db59c6f3e85f6e1ea0ec32c1a0982fbc48,"Wasserstein distance USED-FOR method. technique USED-FOR baseline models. few - shot classification CONJUNCTION personalized dialogue modeling. personalized dialogue modeling CONJUNCTION few - shot classification. personalized dialogue modeling CONJUNCTION multi - target domain adaptation. multi - target domain adaptation CONJUNCTION personalized dialogue modeling. Method is Argmax Centroids. OtherScientificTerm are random function, and particles. ","This paper proposes Argmax Centroids, a method based on the Wasserstein distance between a random function and a set of particles. The technique is applied to several baseline models and applied to few-shot classification, personalized dialogue modeling, and multi-target domain adaptation. "
2939,SP:f55160db59c6f3e85f6e1ea0ec32c1a0982fbc48,method USED-FOR argmax distribution. dialogue system task CONJUNCTION domain adaptation task. domain adaptation task CONJUNCTION dialogue system task. few shot learning task CONJUNCTION dialogue system task. dialogue system task CONJUNCTION few shot learning task. toy dataset CONJUNCTION few shot learning task. few shot learning task CONJUNCTION toy dataset. few shot learning task PART-OF data. toy dataset PART-OF data. dialogue system task PART-OF data. domain adaptation task PART-OF data. Generic is approximation. OtherScientificTerm is Wasserstein distance. ,"This paper proposes a method to approximate the argmax distribution. The approximation is based on the Wasserstein distance between the input and the target distribution. Experiments are conducted on a toy dataset, a few shot learning task, a dialogue system task, and a domain adaptation task."
2940,SP:f55160db59c6f3e85f6e1ea0ec32c1a0982fbc48,"approach USED-FOR deep learning applications. approach USED-FOR bootstrap. personalized dialogue systems CONJUNCTION multi - target domain adaptation. multi - target domain adaptation CONJUNCTION personalized dialogue systems. few shot image classification CONJUNCTION personalized dialogue systems. personalized dialogue systems CONJUNCTION few shot image classification. method USED-FOR multi - target domain adaptation. method USED-FOR personalized dialogue systems. method USED-FOR few shot image classification. few - shot classification CONJUNCTION meta learning tasks. meta learning tasks CONJUNCTION few - shot classification. algorithm USED-FOR SOTA. algorithm USED-FOR few - shot classification. algorithm USED-FOR meta learning tasks. SOTA USED-FOR few - shot classification. OtherScientificTerm are argmax distribution p *, and target distribution p *. Method is Monte Carlo sampling. ","This paper presents an approach to bootstrap for deep learning applications. The method is applied to few shot image classification, personalized dialogue systems, and multi-target domain adaptation. The algorithm is shown to improve the SOTA for few-shot classification and meta learning tasks. The main contribution of the paper is to propose to sample from the argmax distribution p* instead of the target distribution p*. This is achieved by Monte Carlo sampling."
2941,SP:f55160db59c6f3e85f6e1ea0ec32c1a0982fbc48,centroid points USED-FOR argmax distribution. objective function USED-FOR centroid points. objective function USED-FOR argmax distribution. empirical distribution CONJUNCTION ground - truth distribution. ground - truth distribution CONJUNCTION empirical distribution. bound of Wasserstein distance FEATURE-OF empirical distribution. empirical distribution FEATURE-OF centroids. bound of Wasserstein distance USED-FOR method. Argmax centroids USED-FOR machine learning tasks. mult - task learning tasks EVALUATE-FOR method. ,This paper proposes a new objective function to estimate centroid points for the argmax distribution. The method is based on the bound of Wasserstein distance between the empirical distribution of the centroids and the ground-truth distribution. Argmax centroidids are then used for various machine learning tasks. The proposed method is evaluated on mult-task learning tasks and shows promising results.
2942,SP:f55160db59c6f3e85f6e1ea0ec32c1a0982fbc48,"method USED-FOR argmax distribution. method USED-FOR random function. argmax distribution FEATURE-OF random function. optimized centroids USED-FOR method. application EVALUATE-FOR method. method USED-FOR meta - learning and multi - task learning. model USED-FOR task. parameters USED-FOR task. inference CONJUNCTION task. task CONJUNCTION inference. method USED-FOR random function. argmax distribution CONJUNCTION real distribution. real distribution CONJUNCTION argmax distribution. few - shot supervised learning CONJUNCTION personalized dialogue systems. personalized dialogue systems CONJUNCTION few - shot supervised learning. personalized dialogue systems CONJUNCTION multi - target domain adaptation. multi - target domain adaptation CONJUNCTION personalized dialogue systems. method COMPARE baselines. baselines COMPARE method. numerical experiments EVALUATE-FOR baselines. multi - target domain adaptation EVALUATE-FOR method. personalized dialogue systems EVALUATE-FOR method. few - shot supervised learning EVALUATE-FOR method. multi - target domain adaptation HYPONYM-OF numerical experiments. numerical experiments EVALUATE-FOR method. few - shot supervised learning HYPONYM-OF numerical experiments. personalized dialogue systems HYPONYM-OF numerical experiments. Task is approximating the argmax distribution. Method is bootstrap. Generic are applications, and it. OtherScientificTerm is loss function. Metric is Wasserstein distance. ","This paper proposes a method for approximating the argmax distribution of a random function with optimized centroids. The proposed method is applied to meta-learning and multi-task learning, where the goal is to learn a model that can generalize to a new task with different parameters. The authors propose a bootstrap that can be applied to both these applications, and show that it is able to generalize well to unseen tasks. The method is trained to approximate the random function, which is then used for inference and the new task. The paper also proposes a loss function that minimizes the Wasserstein distance between the learned argmax and the real distribution. Experiments on several numerical experiments (e.g., few-shot supervised learning, personalized dialogue systems, multi-target domain adaptation) show that the proposed method outperforms the baselines."
2943,SP:ef342e3c6a16e898a49b700a9fd4f0ea6a069dcc,online setting CONJUNCTION preference - free setting. preference - free setting CONJUNCTION online setting. multi - objective reinforcement learning ( MORL ) USED-FOR preference - free setting. online setting FEATURE-OF multi - objective reinforcement learning ( MORL ). tabular episodic MDPs USED-FOR model - based algorithms. optimal regret CONJUNCTION sample complexity bound. sample complexity bound CONJUNCTION optimal regret. optimal regret EVALUATE-FOR algorithms. sample complexity bound EVALUATE-FOR algorithms. OtherScientificTerm is preference vector. ,"This paper studies multi-objective reinforcement learning (MORL) in both the online setting and the preference-free setting. The authors propose model-based algorithms based on tabular episodic MDPs, where the preference vector is sampled from a distribution over time. The proposed algorithms achieve optimal regret and a sample complexity bound."
2944,SP:ef342e3c6a16e898a49b700a9fd4f0ea6a069dcc,"online, episodic and tabular setting FEATURE-OF multi - objective RL. adversarial chosen weight vectors CONJUNCTION learning approach. learning approach CONJUNCTION adversarial chosen weight vectors. UCBVI USED-FOR learning approach. learning approach PART-OF MO - UCBVI algorithm. adversarial chosen weight vectors PART-OF MO - UCBVI algorithm. algorithm COMPARE state - of - the - art. state - of - the - art COMPARE algorithm. reward - free exploration problem EVALUATE-FOR framework. OtherScientificTerm are adversarial chosen weight vector, sublinear - time regret, and optimal policy. ","This paper studies multi-objective RL in the online, episodic and tabular setting. The authors propose a new MO-UCBVI algorithm that combines adversarial chosen weight vectors with a learning approach based on UCBVI. The key idea is to learn an adversarial weight vector that maximizes the sublinear-time regret of the optimal policy. The proposed framework is evaluated on a reward-free exploration problem, where the proposed algorithm outperforms the state-of-the-art."
2945,SP:ef342e3c6a16e898a49b700a9fd4f0ea6a069dcc,objective vector CONJUNCTION preference vector. preference vector CONJUNCTION objective vector. d - dimensional vectors USED-FOR reward function. preference vector HYPONYM-OF d - dimensional vectors. objective vector HYPONYM-OF d - dimensional vectors. algorithms USED-FOR preference vectors. sub - linear regrets EVALUATE-FOR algorithms. ,"This paper studies the problem of learning d-dimensional vectors for the reward function, i.e., the objective vector and the preference vector. The authors propose two algorithms for learning preference vectors with sub-linear regrets."
2946,SP:ef342e3c6a16e898a49b700a9fd4f0ea6a069dcc,"unknown transition USED-FOR multi - objective RL ( MORL ). d - dimensional preference vectors USED-FOR reward function. preference vector USED-FOR optimal policies. lower bound USED-FOR PFE. OtherScientificTerm are weight vector, policy, PAC - bound, and H factor loose. Task are online MORL setting, preference - free exploration ( PFE ) setting, and learning and planning phases. ","This paper studies multi-objective RL (MORL) with unknown transition. In the online MORL setting, the reward function is a function of d-dimensional preference vectors, where the weight vector is the distance between the policy and the preference vector. The paper considers the preference-free exploration (PFE) setting where the policy is assumed to be a linear combination of the two preference vectors. The authors derive a PAC-bound for PFE, which shows that optimal policies can be obtained by maximizing the H factor loose during the learning and planning phases. They also derive a lower bound for PFR."
2947,SP:ef342e3c6a16e898a49b700a9fd4f0ea6a069dcc,multi - objective and online setting FEATURE-OF reinforcement learning problem. OtherScientificTerm is optimal solutions. ,This paper studies the reinforcement learning problem in the multi-objective and online setting. The authors show that optimal solutions can be obtained in both cases.
2948,SP:aa84981dd503ec34d9f06aa6e5f680e267f82b04,"model - agnostic explanations model USED-FOR dialogue response generation task. local explanation of response generation ( LERG ) USED-FOR dialogue response generation task. local explanation of response generation ( LERG ) HYPONYM-OF model - agnostic explanations model. approach USED-FOR explicit and implicit relations. OtherScientificTerm are sequence - to - sequence natrual, explanations, and human response. Generic is task. Task is sequence prediction. ","This paper proposes a model-agnostic explanations model, called local explanation of response generation (LERG), for dialogue response generation task. The task is framed as sequence-to-sequence natrual, where the explanations are given in the form of a human response. The proposed approach is able to model both explicit and implicit relations, which is useful for sequence prediction."
2949,SP:aa84981dd503ec34d9f06aa6e5f680e267f82b04,"LERG USED-FOR importance scores. dialogue response generation model USED-FOR LERG. dialogue response generation model USED-FOR importance scores. It USED-FOR sequence prediction. It USED-FOR explanations. intra - response consistency CONJUNCTION causal cause identification. causal cause identification CONJUNCTION intra - response consistency. unbiased approximation CONJUNCTION intra - response consistency. intra - response consistency CONJUNCTION unbiased approximation. DailyDialog EVALUATE-FOR approach. Task are model - agnostic explanations of dialogue response generation, and text generation. Method is local explanation of response generation ( LERG ). OtherScientificTerm is human response. ","This paper addresses the problem of model-agnostic explanations of dialogue response generation. The authors propose a local explanation of response generation (LERG), which is based on the fact that the human response is not always available. LERG is trained using a dialogue response generator model to generate importance scores. It is also used for sequence prediction. It can be used to generate explanations for text generation. Experiments on DailyDialog demonstrate the effectiveness of the proposed approach in terms of unbiased approximation, intra-response consistency, and causal cause identification."
2950,SP:aa84981dd503ec34d9f06aa6e5f680e267f82b04,method USED-FOR dialog response generation process. metrics EVALUATE-FOR explanations. perplexity change EVALUATE-FOR former. OtherScientificTerm is importance scores. Generic is latter. ,This paper proposes a method to improve the dialog response generation process. The main idea is to use importance scores to measure the importance of different explanations in terms of different metrics. The former measures perplexity change and the latter measures importance of explanations.
2951,SP:aa84981dd503ec34d9f06aa6e5f680e267f82b04,sequence generation models USED-FOR method. properties FEATURE-OF method. automatic evaluations CONJUNCTION user studies. user studies CONJUNCTION automatic evaluations. user studies EVALUATE-FOR method. automatic evaluations EVALUATE-FOR method. explanation method COMPARE methods. methods COMPARE explanation method. explanation method COMPARE random baselines. random baselines COMPARE explanation method. random baselines COMPARE methods. methods COMPARE random baselines. Method is LERG. OtherScientificTerm is model perplexity difference. Task is Human evaluations. ,"This paper proposes a method called LERG, which is based on sequence generation models. The method is motivated by the properties of LERG. The authors show that LERG is able to explain the model perplexity difference between the explanation method and random baselines. The proposed method is evaluated on automatic evaluations and user studies. Human evaluations show that the proposed explanation method outperforms the other methods as well as random baseline."
2952,SP:aa84981dd503ec34d9f06aa6e5f680e267f82b04,local explanation of sequence generation models USED-FOR dialogue response generation setting. Automatic and human evaluations EVALUATE-FOR LERG. LERG COMPARE baselines. baselines COMPARE LERG. Automatic and human evaluations EVALUATE-FOR baselines. local explanation method USED-FOR dialogue response generation. explanation method USED-FOR generation models. automatic evaluations CONJUNCTION human studies. human studies CONJUNCTION automatic evaluations. human studies EVALUATE-FOR dialogue model explanation quality. automatic evaluations EVALUATE-FOR dialogue model explanation quality. Method is local explanation of response generation ( LERG ). OtherScientificTerm is perturbations. ,This paper proposes a local explanation of sequence generation models for the dialogue response generation setting. The authors propose a new local explanation method for the context-aware dialogue model generation (LERG) based on the observation that generation models trained with the proposed explanation method are more robust to perturbations. LERG is evaluated on both automatic and human evaluations and compared to baselines. The results on automatic evaluations and human studies show improvements in dialogue model explanation quality.
2953,SP:965413b1726617006317bbbec55673dd5d21812a,gradient compression CONJUNCTION error compensation. error compensation CONJUNCTION gradient compression. gradient compression USED-FOR accelerated training algorithms. error compensation USED-FOR accelerated training algorithms. gradient compression CONJUNCTION error compensation. error compensation CONJUNCTION gradient compression. error compensation PART-OF Katyusha algorithm. gradient compression PART-OF Katyusha algorithm. algorithm USED-FOR SG and VR term. algorithm USED-FOR error compensation. quantization error USED-FOR error compensation. convergence rate CONJUNCTION iteration complexity. iteration complexity CONJUNCTION convergence rate. convergence rate EVALUATE-FOR ECKL. iteration complexity EVALUATE-FOR ECKL. ECKL USED-FOR smooth convex functions. it COMPARE DIANA. DIANA COMPARE it. logistic regression EVALUATE-FOR it. logistic regression EVALUATE-FOR DIANA. ,"This paper proposes to combine gradient compression and error compensation in the Katyusha algorithm to improve the performance of accelerated training algorithms. Specifically, the proposed algorithm compresses the SG and VR term and provides error compensation based on the quantization error. The convergence rate and iteration complexity of ECKL for smooth convex functions are analyzed. Empirical results show that it outperforms DIANA on logistic regression."
2954,SP:965413b1726617006317bbbec55673dd5d21812a,error compensation variant USED-FOR loopless Katyusha. error compensation variant PART-OF ECLK. ECLK HYPONYM-OF loopless Katyusha. contraction operators USED-FOR accelerated linear convergence rate. logistic regression problem USED-FOR binary classification. logistic regression problem EVALUATE-FOR ECLK. OtherScientificTerm is Theoretical convergence analysis. ,"This paper proposes ECLK, an error compensation variant of the classic loopless Katyusha. Theoretical convergence analysis is provided. The authors also provide an accelerated linear convergence rate with contraction operators. Experiments on a logistic regression problem for binary classification are provided to show the effectiveness and efficiency of the proposed algorithm."
2955,SP:965413b1726617006317bbbec55673dd5d21812a,accelerated methods USED-FOR communication - efficient learning applications. O(\sqrt{L/\mu } ) rate EVALUATE-FOR error - compensated methods. loopless Katyusha strategy USED-FOR O(\sqrt{L/\mu } ) rate. loopless Katyusha strategy USED-FOR error - compensated methods. stochastic gradient information USED-FOR loopless Katyusha strategy. stochastic gradient information USED-FOR O(\sqrt{L/\mu } ) rate. stochastic gradient information USED-FOR error - compensated methods. ECLK COMPARE error compensation methods. error compensation methods COMPARE ECLK. rate EVALUATE-FOR error compensation methods. theoretical iteration complexity EVALUATE-FOR ECLK. rate EVALUATE-FOR ECLK. Generic is methods. ,"This paper studies accelerated methods for communication-efficient learning applications. The authors propose a loopless Katyusha strategy to improve the O(\sqrt{L/\mu}) rate of error-composed methods using stochastic gradient information. The proposed methods, called ECLK, have theoretical iteration complexity of O(n^2) and O(N^3) and are shown to have a better rate than existing error compensation methods."
2956,SP:965413b1726617006317bbbec55673dd5d21812a,error compensated based Katyusha method USED-FOR finite - sum convex problems. method COMPARE full precision accelerated counterpart. full precision accelerated counterpart COMPARE method. asymptotic convergence rate EVALUATE-FOR full precision accelerated counterpart. asymptotic convergence rate EVALUATE-FOR method. method USED-FOR contractive compressor. ,This paper proposes an error compensated based Katyusha method for finite-sum convex problems. The proposed method achieves better asymptotic convergence rate than its full precision accelerated counterpart. The authors also propose a contractive compressor based on the proposed method.
2957,SP:965413b1726617006317bbbec55673dd5d21812a,"communication efficient algorithms USED-FOR SGD. approximate global minimum USED-FOR objective function. average loss FEATURE-OF objective function. coordinator USED-FOR estimator. Nesterov momentum USED-FOR contractive compressors. stochastic functions USED-FOR gradient. Nesterov momentum FEATURE-OF accelerated gradient - methods. top $ k$ estimator HYPONYM-OF stochastic functions. Generic are model, and approach. OtherScientificTerm is local smooth convex loss function. Metric are communication, and rate of convergence. ","This paper studies the problem of communication efficient algorithms for SGD. The authors propose a model where the objective function is an approximate global minimum of a local smooth convex loss function. The estimator is trained with a coordinator, and the communication between the coordinator and the client is used to estimate the gradient of the gradient using stochastic functions (such as the top $k$ estimator in accelerated gradient-methods with Nesterov momentum in contractive compressors). The authors show that the proposed approach converges linearly with the rate of convergence."
2958,SP:27c58dad7fa7743a8ff56fad863aa0dae823dccb,"local plasticity USED-FOR readout weights. backpropagation USED-FOR Liquid State Machines ( LSM ). local plasticity USED-FOR recurrent neural networks. biologically inspired model USED-FOR self - organization. reservoir of integrate - and - fire neurons PART-OF model. dynamical variable USED-FOR STDP learning rates. MNIST USED-FOR neuromorphic computing. MNIST and N - MNIST dataset USED-FOR networks. SLM model COMPARE LSM models. LSM models COMPARE SLM model. astrocyte - dependent STDP plasticity FEATURE-OF SLM model. OtherScientificTerm are critical phase transition, critical transition, synaptic efficacies, and astrocytes. ","This paper proposes to use local plasticity in the readout weights of recurrent neural networks with backpropagation, similar to Liquid State Machines (LSM). The authors propose a biologically inspired model for self-organization, where a reservoir of integrate-and-fire neurons is incorporated into the model, and the critical phase transition is modeled as a function of the critical transition. The authors train networks on the MNIST and N-MNIST dataset, which is a popular dataset for neuromorphic computing. They also propose a dynamical variable to control the STDP learning rates, which can be used as a measure of synaptic efficacies. They show that the proposed SLM model exhibits astrocyte-dependent STDP plasticity compared to other LSM models, and show that astrocetes are more likely to be active during critical phase."
2959,SP:27c58dad7fa7743a8ff56fad863aa0dae823dccb,connections PART-OF liquid state machine ( LSM ). way USED-FOR connections. STDP USED-FOR neuron model. depression term HYPONYM-OF STDP. model COMPARE LSMs. LSMs COMPARE model. Generic is it. OtherScientificTerm is edge of chaos ( EOC ). ,"This paper proposes a new way to model connections in a liquid state machine (LSM). The main idea is to use a neuron model trained with STDP (e.g., depression term). The authors show that it is able to model the edge of chaos (ECO). The experiments show that the proposed model outperforms existing LSMs."
2960,SP:27c58dad7fa7743a8ff56fad863aa0dae823dccb,accuracy EVALUATE-FOR LSMs. stability EVALUATE-FOR LSMs. liquid state machine ( LSM ) CONJUNCTION biologically inspired astrocyte model ( NALSM ). biologically inspired astrocyte model ( NALSM ) CONJUNCTION liquid state machine ( LSM ). accuracy CONJUNCTION stability. stability CONJUNCTION accuracy. neuronal dynamics PART-OF LSM. biologically inspired astrocyte model ( NALSM ) USED-FOR neuronal dynamics. edge - of - chaos FEATURE-OF LSM. NALSM USED-FOR spiking model. accuracy EVALUATE-FOR spiking model. LSM CONJUNCTION STDP - based LSM approaches. STDP - based LSM approaches CONJUNCTION LSM. MNIST and N - MNIST dataset EVALUATE-FOR NALSM. NALSM COMPARE LSM. LSM COMPARE NALSM. NALSM COMPARE STDP - based LSM approaches. STDP - based LSM approaches COMPARE NALSM. NALSM COMPARE spiking neuron models. spiking neuron models COMPARE NALSM. STDP - based LSM approaches CONJUNCTION spiking neuron models. spiking neuron models CONJUNCTION STDP - based LSM approaches. multi - layer SNNs HYPONYM-OF spiking neuron models. OtherScientificTerm is re - tuning parameters. ,"This paper investigates the relationship between accuracy and stability of LSMs. The authors propose a combination of a liquid state machine (LSM) and a biologically inspired astrocyte model (NALSM) to model neuronal dynamics in LSM. NALSM is used to train a spiking model that achieves higher accuracy and better stability than LSM and STDP-based LSM approaches. Experiments on the MNIST and N-MNIST dataset show that NalSM can achieve comparable performance to LSM without re-tuning parameters. In addition, the authors compare the performance of LSM, STDP, and spiking neuron models (e.g., multi-layer SNNs)."
2961,SP:27c58dad7fa7743a8ff56fad863aa0dae823dccb,"mechanism USED-FOR liquid ( reservoir ) layer. edge - of - chaos dynamics regime FEATURE-OF liquid ( reservoir ) layer. astrocytes USED-FOR synaptic plasticity. feedbacks USED-FOR edge - of - chaos property. It COMPARE LSM - based methods. LSM - based methods COMPARE It. LSM - based methods USED-FOR real - world tasks. edge - of - chaos property FEATURE-OF liquid state machine ( LSM ). It USED-FOR real - world tasks. feedbacks USED-FOR It. OtherScientificTerm are feedback, network activity, biological observation, and liquid activity. ","This paper proposes a mechanism to learn a liquid (reservoir) layer in the edge-of-chaos dynamics regime. It leverages feedbacks to enforce the edge of-chaotic property of the liquid state machine (LSM) and is able to generalize better than other LSM-based methods on real-world tasks. The authors show that the learned feedback can be used to model the behavior of astrocytes in synaptic plasticity. They also show that this feedback is correlated with network activity, which is in contrast to biological observation that the liquid activity is not correlated with the network activity."
2962,SP:27c58dad7fa7743a8ff56fad863aa0dae823dccb,"modulating STDP HYPONYM-OF astrocytes. Method is LSM. Generic are network, and It. Material is MNIST and N - MNIST data sets. ","This paper proposes an extension of LSM to the case where the network is trained on a large number of data points. It is called modulating STDP, which is a variant of astrocytes, i.e., modulating the output of the original LSM. The authors conduct experiments on MNIST and N-MNIST data sets to demonstrate the effectiveness of the modulated STDP."
2963,SP:64ccd697d3c11d7d8947ef1b06c61d94b6a2e575,quantity imbalance CONJUNCTION topology imbalance. topology imbalance CONJUNCTION quantity imbalance. graphs USED-FOR imbalanced classification. re - weight nodes USED-FOR imbalanced data distribution. label propagation algorithm USED-FOR topological position. topological position FEATURE-OF labeled nodes. label propagation algorithm USED-FOR labeled nodes. benchmark datasets EVALUATE-FOR model. ,"This paper studies the problem of imbalanced classification in graphs, which is caused by both quantity imbalance and topology imbalance. The authors propose to re-weight nodes to account for the imbalanced data distribution and propose a label propagation algorithm to determine the topological position of the labeled nodes. The proposed model is evaluated on several benchmark datasets."
2964,SP:64ccd697d3c11d7d8947ef1b06c61d94b6a2e575,node representation USED-FOR topology - imbalance problem. graph - structured data USED-FOR node representation. graph - structured data USED-FOR topology - imbalance problem. label propagation algorithm CONJUNCTION Personalized PageRank matrix. Personalized PageRank matrix CONJUNCTION label propagation algorithm. ReNode algorithm USED-FOR semi - supervised training of GNN models. Totoro FEATURE-OF semi - supervised training of GNN models. Totoro FEATURE-OF ReNode algorithm. ReNode USED-FOR TINL problem. Generic is problem. ,This paper addresses the topology-imbalance problem with graph-structured data to learn node representation. The authors propose a label propagation algorithm and a Personalized PageRank matrix to solve the problem. The ReNode algorithm is applied to semi-supervised training of GNN models on Totoro. The experiments show that ReNode can solve the TINL problem.
2965,SP:64ccd697d3c11d7d8947ef1b06c61d94b6a2e575,imbalance HYPONYM-OF ML. graph - specific problem USED-FOR topological imbalance. method USED-FOR reweighting. ReNode HYPONYM-OF method. ReNode HYPONYM-OF reweighting. method USED-FOR topological imbalance. ,"This paper studies the problem of imbalance in ML, which is a graph-specific problem. The authors propose a method for reweighting, called ReNode, to address this problem. They show that the proposed method can reduce the topological imbalance."
2966,SP:64ccd697d3c11d7d8947ef1b06c61d94b6a2e575,"topological imbalance problem FEATURE-OF node classification problem. asymmetric topological structure FEATURE-OF semi - supervised learning process. asymmetric topological structure USED-FOR imbalance. it USED-FOR GNN. Task are label propagation problem, and imbalance issue. OtherScientificTerm are Node reweighting loss, and distance weighted conflicts. ",This paper studies the topological imbalance problem in the node classification problem. The imbalance is caused by the asymmetric topological structure of the semi-supervised learning process. The label propagation problem is the main cause of the imbalance issue. Node reweighting loss is proposed to address the problem and it is applied to GNN. The paper also studies the problem of distance weighted conflicts.
2967,SP:64ccd697d3c11d7d8947ef1b06c61d94b6a2e575,"topological difference of labelled nodes PART-OF graph. topology imbalance HYPONYM-OF problem. It USED-FOR node classifiers. topology imbalance problem FEATURE-OF node classification tasks. OtherScientificTerm are decision boundary, and topological importance. Method is annealing method. Generic is baselines. ","This paper studies a problem called topology imbalance, i.e., the topological difference of labelled nodes in a graph. It is an important problem for node classifiers, as it can lead to node classification tasks that suffer from a significant imbalance in the decision boundary. This paper proposes an annealing method to address this issue. The main contribution of this paper is to show that the importance of topological importance can be reduced by reducing the number of nodes in the graph. Experiments are conducted on several datasets and compared with several baselines."
2968,SP:ec12f0a05db75ac15ad22b34cdc2a0142bc2c72f,"d - dimensional lattice FEATURE-OF piecewise constant signals. lattice partition recovery HYPONYM-OF problem. one - sided consistency FEATURE-OF DCART. merging stage USED-FOR partition recovery. OtherScientificTerm are noise, over - partitioning, dyadic rectangles, and minimax lower bound. Generic are method, and methodology. Method is dyadic classification and regression tree ( DCART ). ","This paper studies a problem called lattice partition recovery, where piecewise constant signals are partitioned into a d-dimensional lattice. The authors propose a method called dyadic classification and regression tree (DCART), which is based on the observation that DCART suffers from one-sided consistency, which can lead to noise and over-partitioning. To address this issue, the authors propose to use dyadic rectangles, and derive a minimax lower bound on the partition recovery using a merging stage. Experimental results demonstrate the effectiveness of the proposed methodology."
2969,SP:ec12f0a05db75ac15ad22b34cdc2a0142bc2c72f,"$ d$-dimensional lattice FEATURE-OF piecewise constant signal. scalable DCART algorithm USED-FOR algorithm. vanilla DCART algorithm USED-FOR partitions. one - sided consistency FEATURE-OF vanilla DCART algorithm. DCART algorithm USED-FOR partition of the ground truth signal. vanilla DCART algorithm PART-OF two - step algorithm. two - step algorithm USED-FOR over - partitioning. OtherScientificTerm are additive Gaussian noise, partition of the lattice, One - sided consistency, and constant signal. Generic is estimator. Method is 2 step estimator. ",This paper studies the problem of learning a piecewise constant signal on a $d$-dimensional lattice with additive Gaussian noise. The authors propose an algorithm based on the scalable DCART algorithm. The main contribution of the paper is to propose a new estimator that is more scalable than the existing 2 step estimator. One-sided consistency is achieved by replacing the vanilla DCART with a two-step algorithm that learns partitions with one-sided consistent with the partition of the ground truth signal. This allows the authors to avoid over-partitioning in the two-steps of the proposed algorithm. 
2970,SP:ec12f0a05db75ac15ad22b34cdc2a0142bc2c72f,"OtherScientificTerm are piecewise constant function, noisy measurements, bisections of partitions, one - sided consistency, definitional uniqueness, and order - matching lower bound. Method are dyadic CART ( DCART ) algorithm, DCART, and two - step estimator. Metric is Hausdorff distance. ","This paper proposes a dyadic CART (DCART) algorithm for estimating a piecewise constant function that is invariant to noisy measurements. DCART is based on a two-step estimator, where the bisections of partitions are assumed to have one-sided consistency. The authors prove an order-matching lower bound on the Hausdorff distance between the true and noisy values of the function, which is a function of the definitional uniqueness."
2971,SP:ec12f0a05db75ac15ad22b34cdc2a0142bc2c72f,"Dyadic CART ( DCART ) algorithm USED-FOR partition recovery. vanilla DCART algorithm USED-FOR partition. DCART algorithm USED-FOR partition. dyadic cells USED-FOR partition. multivariate piecewise polynomials CONJUNCTION bounded variation functions. bounded variation functions CONJUNCTION multivariate piecewise polynomials. optimal decision trees USED-FOR bounded variation functions. Material is noisy signal. OtherScientificTerm are Gaussian noise vector, and rectangular partition. Method are DCART estimator, vanilla DCART, and DCART. ","This paper proposes Dyadic CART (DCART) algorithm for partition recovery from a noisy signal. The main idea is to use the vanilla DCART algorithm to recover a partition from a set of dyadic cells, where each cell corresponds to a Gaussian noise vector. The authors show that the DCART estimator converges to the optimal solution of a rectangular partition when the number of cells is bounded by a constant factor. They also show that for multivariate piecewise polynomials and bounded variation functions with optimal decision trees, DCART can converge to a solution that is at least as good as the vanilla version of DCART."
2972,SP:ec12f0a05db75ac15ad22b34cdc2a0142bc2c72f,noisy high dimensional piecewise constant signal USED-FOR recovery of constancy regions. DCART estimator USED-FOR rectangular partition. estimator USED-FOR ground truth partition. DCARTAM HYPONYM-OF variant. Hausdorff FEATURE-OF ground truth partition. DCARTAM COMPARE TV - based estimator. TV - based estimator COMPARE DCARTAM. DCART COMPARE TV - based estimator. TV - based estimator COMPARE DCART. DCART COMPARE DCARTAM. DCARTAM COMPARE DCART. metrics EVALUATE-FOR DCART. metrics EVALUATE-FOR TV - based estimator. metrics EVALUATE-FOR DCARTAM. estimator CONJUNCTION cardinality of the ground truth partition. cardinality of the ground truth partition CONJUNCTION estimator. DCARTAM COMPARE DCART. DCART COMPARE DCARTAM. DCARTAM COMPARE TV - based estimator. TV - based estimator COMPARE DCARTAM. Method is two steps estimator. Metric is Hausdorff distance. Generic is distance. ,"This paper studies the recovery of constancy regions from a noisy high dimensional piecewise constant signal. The authors propose a two steps estimator. First, the DCART estimator is applied to a rectangular partition. Second, a variant called DCARTAM is proposed. The proposed estimator estimates the ground truth partition in terms of Hausdorff distance. The distance is defined as the sum of the difference between the estimator and the cardinality of the groundtruth partition. Experiments on several metrics are conducted to compare DCART with the TV-based estimator, and show that DCARTam outperforms DCART by a large margin."
2973,SP:3c65b3e69a024431cafdc1b4bfbccd432de69faf,"implicit and explicit counterfactual maximum likelihood estimation HYPONYM-OF solutions. natural language inference CONJUNCTION image captioning task. image captioning task CONJUNCTION natural language inference. human evaluation CONJUNCTION automatic evaluation. automatic evaluation CONJUNCTION human evaluation. natural language inference EVALUATE-FOR methods. image captioning task EVALUATE-FOR methods. human evaluation EVALUATE-FOR methods. OtherScientificTerm are spurious correlations, and observed confounders. ","This paper proposes two solutions: implicit and explicit counterfactual maximum likelihood estimation. The proposed methods are evaluated on both natural language inference and an image captioning task, with both human evaluation and automatic evaluation. The results show that the proposed methods do not suffer from spurious correlations, but instead suffer from observed confounders."
2974,SP:3c65b3e69a024431cafdc1b4bfbccd432de69faf,training objective USED-FOR spurious effect of confounding variables. counterfactual maximum likelihood estimation ( CMLE ) HYPONYM-OF training objective. upperbound formulation USED-FOR interventional log - likelihood. explicit CMLE HYPONYM-OF upperbound formulation. natural language inference ( NLI ) CONJUNCTION image captioning. image captioning CONJUNCTION natural language inference ( NLI ). image captioning USED-FOR intervention modeling tasks. natural language inference ( NLI ) USED-FOR intervention modeling tasks. natural language inference ( NLI ) USED-FOR method. image captioning USED-FOR method. human - perceived performance EVALUATE-FOR method. ,"This paper proposes a new training objective, counterfactual maximum likelihood estimation (CMLE), to mitigate the spurious effect of confounding variables. The authors propose an upperbound formulation for interventional log-likelihood, called explicit CMLE. The proposed method is applied to both natural language inference (NLI) and image captioning for intervention modeling tasks. The method is evaluated on human-perceived performance."
2975,SP:3c65b3e69a024431cafdc1b4bfbccd432de69faf,"CATE estimation USED-FOR maximum likelihood. methodology USED-FOR maximum likelihood. methodology USED-FOR CATE estimation. Implicit CMLE CONJUNCTION Explicit CMLE. Explicit CMLE CONJUNCTION Implicit CMLE. Explicit CMLE HYPONYM-OF algorithms. Implicit CMLE HYPONYM-OF algorithms. CATE generalization bounds USED-FOR implicit variant. natural language inference CONJUNCTION image captioning tasks. image captioning tasks CONJUNCTION natural language inference. methods USED-FOR natural language inference. methods USED-FOR image captioning tasks. methodology USED-FOR causally - inspired prediction. OtherScientificTerm are interventional distributions, counterfactual sampling distribution, and spurious correlations. Method are Counterfactual MLE ( CMLE ), Shalit's CATE generalization bounds paper, and deep learning. Generic is variant. Task is shortcut learning. ","This paper proposes a methodology for CATE estimation for maximum likelihood, which is an extension of Counterfactual MLE (CMLE), which is a methodology to estimate maximum likelihood for interventional distributions. The authors propose two algorithms: Implicit CMLE and Explicit CMLE. The implicit variant is based on Shalit's CATE generalization bounds, and the authors show that the counterfactual sampling distribution is robust to spurious correlations. The proposed methods are applied to natural language inference and image captioning tasks, and are shown to be effective for causally-inspired prediction. The paper also shows that the proposed variant can be used for shortcut learning, and is shown to generalize well to deep learning."
2976,SP:3c65b3e69a024431cafdc1b4bfbccd432de69faf,"upper bounds USED-FOR CMLE objective. Implicit MLE HYPONYM-OF upper bounds. Wasserstein distance USED-FOR representations. Explicit MLE HYPONYM-OF upper bounds. observational data USED-FOR Monte Carlo evaluations. Image Captioning ( IC ) HYPONYM-OF spurious correlations. Natural Language Inference ( NLI ) HYPONYM-OF spurious correlations. Natural Language Inference ( NLI ) HYPONYM-OF tasks. Image Captioning ( IC ) HYPONYM-OF tasks. Method are Counterfactual Maximum Likelihood Objective CMLE, and statistical model. OtherScientificTerm are interventional distribution, causal link, spurious features, and interventional distributions. Generic is model. Material is counterfactual examples. ","This paper proposes a new statistical model, Counterfactual Maximum Likelihood Objective CMLE, which is motivated by the observation that Monte Carlo evaluations on observational data often fail to capture spurious correlations (e.g., in Natural Language Inference (NLI) and in Image Captioning (IC)). The authors propose two upper bounds for the CMLE objective: (1) Implicit MLE, where representations are learned by minimizing a Wasserstein distance between an interventional distribution and a causal link, and (2) Explicit MLE where the model is trained on counterfactual examples, where the spurious features are learned to approximate the interventional distributions."
2977,SP:3c65b3e69a024431cafdc1b4bfbccd432de69faf,counterfactual maximum likelihood estimation USED-FOR deep learning models. Implicit CMLE CONJUNCTION Explicit CMLE. Explicit CMLE CONJUNCTION Implicit CMLE. algorithms USED-FOR causal predictions of DL models. observational data USED-FOR DL models. Implicit CMLE HYPONYM-OF algorithms. Explicit CMLE HYPONYM-OF algorithms. method COMPARE regular MLE method. regular MLE method COMPARE method. real data sets EVALUATE-FOR regular MLE method. real data sets EVALUATE-FOR method. OtherScientificTerm is spurious correlation relationships. ,"This paper studies the problem of counterfactual maximum likelihood estimation for deep learning models. The authors propose two algorithms, Implicit CMLE and Explicit CMLE, for causal predictions of DL models trained on observational data. They show that the proposed method outperforms the regular MLE method on two real data sets. They also show that spurious correlation relationships can be avoided."
2978,SP:c5a59c8d6db0f5491721aaaef182609c360930d3,multitask learning approach USED-FOR gradient conflict. computer vision CONJUNCTION multitask reinforcement learning. multitask reinforcement learning CONJUNCTION computer vision. multitask reinforcement learning HYPONYM-OF settings. computer vision HYPONYM-OF settings. Method is gradient update. OtherScientificTerm is pareto optimality. ,"This paper proposes a multitask learning approach to address the gradient conflict. The authors consider two settings: (1) computer vision and (2) multitask reinforcement learning. In the first setting, the gradient update is done in parallel, which leads to pareto optimality. "
2979,SP:c5a59c8d6db0f5491721aaaef182609c360930d3,"competition between tasks USED-FOR shared resources. gradient updates USED-FOR shared resources. CAGrad HYPONYM-OF algorithm. algorithm USED-FOR average loss function. network's loss HYPONYM-OF average loss function. gradient descent CONJUNCTION MGDA algorithm. MGDA algorithm CONJUNCTION gradient descent. algorithm USED-FOR MGDA algorithm. algorithm USED-FOR gradient descent. Method are Multi - task learning ( MTL ), and sub - optimal optimization. OtherScientificTerm are gradients, worst local improvement, and average loss. Generic is method. ","Multi-task learning (MTL) is an important problem in the context of competition between tasks to find shared resources through gradient updates. The authors propose an algorithm called CAGrad, which is an algorithm that learns an average loss function (i.e., the network's loss) that minimizes the difference between the gradients of the two tasks. The method is motivated by the observation that sub-optimal optimization can lead to the worst local improvement, and the authors propose to use the average loss as a proxy for the gradient descent of the MGDA algorithm."
2980,SP:c5a59c8d6db0f5491721aaaef182609c360930d3,"method USED-FOR conflicting gradients. gradient modification USED-FOR method. model - wide gradient direction $ d$ USED-FOR shared parameters. dual objective USED-FOR per - task loss weights. gradient update USED-FOR shared parameters. Method is multi - task learning paradigms. OtherScientificTerm are per - task gradients, and worst local improvement. ","This paper proposes a method to deal with conflicting gradients using gradient modification. The main idea is to use a model-wide gradient direction $d$ to update shared parameters during gradient update. The authors propose a dual objective to optimize per-task loss weights, where the worst local improvement is computed for each task. The proposed method is applicable to multi-task learning paradigms."
2981,SP:c5a59c8d6db0f5491721aaaef182609c360930d3,"average task gradient USED-FOR first - order multi - task learning method. gradient descent methods USED-FOR single tasks. gradient descent methods USED-FOR computer vision. computer vision CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION computer vision. gradient descent methods USED-FOR reinforcement learning. single tasks PART-OF computer vision. gradient descent methods USED-FOR average task loss. single tasks USED-FOR reinforcement learning. gradient descent CONJUNCTION MGDA. MGDA CONJUNCTION gradient descent. CAGrad COMPARE gradient descent. gradient descent COMPARE CAGrad. optimization problem USED-FOR update direction. method USED-FOR deep network parameters. OtherScientificTerm are minimum local single - task improvement, conflicting gradients, single tasks gradients, task losses, and pareto stationary point. Task is multi - task learning. Generic is methods. Method are Conflict - Averse Gradient descent ( CAGrad ), and multi - task learning methods. ","This paper proposes a first-order multi-task learning method based on the average task gradient. The main idea is to use gradient descent methods for single tasks in computer vision and reinforcement learning. The authors argue that the current methods do not achieve minimum local single-task improvement due to conflicting gradients. To address this issue, the authors propose Conflict-Averse Gradient descent (CAGrad) and show that CAGrad outperforms gradient descent and MGDA when the task losses are close to a pareto stationary point. The proposed method can also be applied to deep network parameters. "
2982,SP:c5a59c8d6db0f5491721aaaef182609c360930d3,solution USED-FOR multi - task learning. solution USED-FOR update vector. update vector USED-FOR worst task loss relative improvement. OtherScientificTerm is average loss gradient. ,"This paper proposes a solution for multi-task learning. The proposed solution is to learn an update vector for the worst task loss relative improvement, which is the average loss gradient of the two tasks. The paper is well-written and easy to follow."
2983,SP:000cbfda2e26fdcfee50a628799a73b6886cfccc,simplicity prior USED-FOR large language models. GPT-3 HYPONYM-OF large language models. prior HYPONYM-OF simplicity prior. witness set USED-FOR learner. random test set USED-FOR learner. human learners CONJUNCTION inductive program learners. inductive program learners CONJUNCTION human learners. GPT learners CONJUNCTION human learners. human learners CONJUNCTION GPT learners. accuracy EVALUATE-FOR GPT learners. accuracy EVALUATE-FOR inductive program learners. accuracy EVALUATE-FOR human learners. test sets EVALUATE-FOR inductive program learners. Material is P3 language. ,"This paper proposes a new simplicity prior, called prior, for large language models (e.g., GPT-3), which is an extension of the simplicity prior used in the P3 language. The key idea is to train a learner on a random test set, where the learner is trained on a witness set, and the witness set is used to train an inductive program. The authors evaluate the accuracy of GPT learners, human learners, and inductive programs on three test sets, and show that the accuracy is comparable to human learners."
2984,SP:000cbfda2e26fdcfee50a628799a73b6886cfccc,humans CONJUNCTION language models. language models CONJUNCTION humans. language models CONJUNCTION inductive programming systems. inductive programming systems CONJUNCTION language models. machine teaching USED-FOR inductive programming systems. machine teaching USED-FOR humans. simplicity prior USED-FOR P3 language. simplicity prior USED-FOR Concepts. humans CONJUNCTION computational approaches. computational approaches CONJUNCTION humans. simplicity prior USED-FOR language models. GPT HYPONYM-OF language models. ,"This paper studies the relationship between humans, language models, and inductive programming systems learned from machine teaching. The authors propose a simplicity prior for the P3 language, and show that concepts learned from the simplicity prior can be used to improve the performance of language models (e.g., GPT). The authors also show that humans and language models and computational approaches can be combined to improve performance."
2985,SP:000cbfda2e26fdcfee50a628799a73b6886cfccc,"external patterns CONJUNCTION internal patterns. internal patterns CONJUNCTION external patterns. common - sense or world - knowledge HYPONYM-OF external patterns. ABAB HYPONYM-OF internal patterns. teaching problem USED-FOR this. program induction systems CONJUNCTION humans. humans CONJUNCTION program induction systems. LMs CONJUNCTION program induction systems. program induction systems CONJUNCTION LMs. LMs COMPARE humans. humans COMPARE LMs. humans USED-FOR few - shot settings. LMs USED-FOR few - shot settings. OtherScientificTerm are patterns, and few - shot setting. Material is few - show setting. Generic are latter, model, and it. ","This paper studies the problem of learning patterns in a few-shot setting, i.e., in the few-show setting, where the goal is to learn a model that can generalize to unseen tasks. This is a teaching problem, and this is formulated as a learning problem, where external patterns (e.g. common-sense or world-knowledge) and internal patterns (i.e. ABAB) are considered, and the latter is used to train the model. The paper compares LMs, program induction systems, and humans on a variety of few-shots settings, and shows that LMs perform better than humans in most of these settings. In particular, it is shown that it is able to generalize well to unseen problems."
2986,SP:000cbfda2e26fdcfee50a628799a73b6886cfccc,"exploitation of rule simplicity PART-OF models. human participants HYPONYM-OF models. minimal example set USED-FOR system. Method is language models. OtherScientificTerm are rule, and simplicity. Task is exploitation of simplicity. Generic is systems. ","This paper studies the exploitation of rule simplicity in models (e.g., human participants) trained on language models. The main idea is to train a system on a minimal example set, where the rule is assumed to be easy to follow. The exploitation of simplicity is a well-studied problem in the literature, and this paper is one of the first attempts to study exploitation in systems trained on a small set of examples. "
2987,SP:000cbfda2e26fdcfee50a628799a73b6886cfccc,humans CONJUNCTION GPT * language models. GPT * language models CONJUNCTION humans. GPT * language models CONJUNCTION inductive programming systems. inductive programming systems CONJUNCTION GPT * language models. few - shot learning abilities EVALUATE-FOR GPT * language models. few - shot learning abilities EVALUATE-FOR humans. machine teaching point of view USED-FOR few - shot learning abilities. minimal witness set USED-FOR Models. complexity FEATURE-OF P3 programs. simplicity FEATURE-OF GPT models. GPT models COMPARE ones. ones COMPARE GPT models. OtherScientificTerm is program. Method is GPT-3 models. ,"This paper studies the few-shot learning abilities of humans, GPT* language models, and inductive programming systems from a machine teaching point of view. Models are trained with a minimal witness set, and the complexity of P3 programs is studied. The authors show that the simplicity of GPT models is due to the fact that the program can be viewed as a linear combination of the GPT-3 models. They also compare the performance of the proposed GPT model with existing ones."
2988,SP:ba01895bf1aa07a0630b8c41fc0e91effb34b4cf,feature representations USED-FOR robust and non - robust features. convolution layer HYPONYM-OF feature representations. layer USED-FOR feature maps. information bottleneck method USED-FOR robust and non robust features. adversarial examples USED-FOR robust network. adversarial class label USED-FOR Non - robust features. Method is information bottleneck. Metric is accuracy. OtherScientificTerm is propagating non - robust features. ,"This paper proposes an information bottleneck method to distinguish between robust and non-robust features based on feature representations (i.e., the convolution layer). The main idea is to train a robust network on adversarial examples and then train a second layer that maps the feature maps to the adversarial class label. The idea is that the information bottleneck will improve the accuracy of the trained network by propagating non-observable features."
2989,SP:ba01895bf1aa07a0630b8c41fc0e91effb34b4cf,robust ones CONJUNCTION non - robust ones. non - robust ones CONJUNCTION robust ones. intermediate features PART-OF DNN. robust ones HYPONYM-OF intermediate features. non - robust ones HYPONYM-OF intermediate features. visualization USED-FOR non - robust ones. semantic information FEATURE-OF non - robust ones. AE generation algorithm USED-FOR non - robust feature. Task is AE. OtherScientificTerm is common attack baselines. ,"This paper studies the problem of AE, where the intermediate features of a DNN are either robust ones or non-robust ones. The authors show that non-robotically robust ones are more likely to contain semantic information, as demonstrated by a visualization. The paper then proposes an AE generation algorithm that generates a non-probabilistic version of the AE feature that is robust to common attack baselines."
2990,SP:ba01895bf1aa07a0630b8c41fc0e91effb34b4cf,Information Bottleneck ( IB ) USED-FOR robust and non - robust features. Information Bottleneck ( IB ) USED-FOR neural networks ( NNs ). robust and non - robust features PART-OF neural networks ( NNs ). attack mechanism USED-FOR gradient of non - robust features. OtherScientificTerm is distilled features. Task is adversarial prediction. ,This paper proposes to use Information Bottleneck (IB) to separate the robust and non-robust features in neural networks (NNs). The authors also propose an attack mechanism to minimize the gradient of non-observable features. The distilled features are then used for adversarial prediction.
2991,SP:ba01895bf1aa07a0630b8c41fc0e91effb34b4cf,adversarial prediction USED-FOR non - robust feature. information bottleneck USED-FOR intermediate representation. non - robust features gradient USED-FOR attack mechanism. Generic is approach. ,This paper proposes an attack mechanism based on non-robust features gradient. The proposed approach is based on adversarial prediction of the non-robot feature. The intermediate representation is learned using an information bottleneck.
2992,SP:ba01895bf1aa07a0630b8c41fc0e91effb34b4cf,adversarial examples USED-FOR robust and non - robust features. method USED-FOR robust and non - robust features. information bottleneck USED-FOR method. distilled features CONJUNCTION adversarial prediction. adversarial prediction CONJUNCTION distilled features. attack USED-FOR gradient of non - robust features. ,This paper proposes a method to extract both robust and non-robust features from adversarial examples using an information bottleneck. The key idea is to combine distilled features with adversarial prediction. The authors also propose an attack to reduce the gradient of non-observable features.
2993,SP:ed67b2664359799a11cebb9eaba6da74ff1dd977,support vector proliferation USED-FOR independent feature models. super - linear lower bound USED-FOR support vector proliferation. sharp phase transition FEATURE-OF Gaussian feature models. Method is SVM. ,"This paper proposes a super-linear lower bound on support vector proliferation for independent feature models. The main result is that for Gaussian feature models with sharp phase transition, the SVM converges to a stationary point."
2994,SP:ed67b2664359799a11cebb9eaba6da74ff1dd977,"solution USED-FOR hard - margin SVM classifier. solution USED-FOR ordinary linear regression problem. SVM USED-FOR SVP. OtherScientificTerm are support vector proliferation, support vectors, lower bounds, and transition region. Generic is it. ","This paper proposes a solution to the ordinary linear regression problem that can be used to train a hard-margin SVM classifier. The main contribution of this paper is to study the problem of support vector proliferation. In particular, the authors show that the support vectors of an SVP trained with an SVM can be partitioned into two parts: (1) support vectors that are close to each other and (2) support vector that are far away from each other. The authors also provide lower bounds on the number of support vectors in the transition region and show that it can be bounded."
2995,SP:ed67b2664359799a11cebb9eaba6da74ff1dd977,"minimum-$\ell_p$-norm hard - margin SVM CONJUNCTION minimum-$\ell_p$-norm interpolation. minimum-$\ell_p$-norm interpolation CONJUNCTION minimum-$\ell_p$-norm hard - margin SVM. minimum-$\ell_p$-norm hard - margin SVM USED-FOR linear classification. minimum-$\ell_p$-norm interpolation USED-FOR linear classification. Task are support vector proliferation ( SVP ), and SVP. OtherScientificTerm is Gaussian inputs. Generic is distributions. ",This paper studies support vector proliferation (SVP) in the setting where Gaussian inputs are available. The authors propose a minimum-$\ell_p$-norm hard-margin SVM and a simple minimum-${\ell_{p}-norm interpolation for linear classification. The main contribution of this paper is to provide a theoretical analysis of SVP and to provide some theoretical guarantees for the convergence of these distributions.
2996,SP:ed67b2664359799a11cebb9eaba6da74ff1dd977,super - linear lower bound USED-FOR dimension of support vector proliferation ( SVP ). super - linear lower bound FEATURE-OF support vector proliferation ( SVP ). anisotropic subgaussians USED-FOR features. asymptotic threshold FEATURE-OF phase transition. phase transition FEATURE-OF isotropic Gaussian distribution. $ \ell_1$-SVMs USED-FOR SVP phase transition. Material is synthetic data sets. ,"This paper provides a super-linear lower bound on the dimension of support vector proliferation (SVP) using anisotropic subgaussians. The authors show that the asymptotic threshold of the phase transition of an isotropic Gaussian distribution is bounded by $\ell_1$-SVMs, and that the SVP phase transition can be approximated by $\ell_2$-sVMs. The paper also provides experimental results on synthetic data sets."
2997,SP:ed67b2664359799a11cebb9eaba6da74ff1dd977,"SVM estimator COMPARE interpolating least - norm linear regression. interpolating least - norm linear regression COMPARE SVM estimator. overparametrisation USED-FOR generalised linear models. anisotropic model USED-FOR SVP. isotropic Gaussian data USED-FOR SVP. OtherScientificTerm are support vectors, sub - Gaussian features, overparameterized regimes, and loss function. Method is data model. Task is Artificial Intelligence and Statistics. ","This paper proposes a new SVM estimator that is more efficient than interpolating least-norm linear regression. The main idea is to use overparametrisation to train generalised linear models, where the support vectors are assumed to be Gaussian and the sub-Gaussian features are Gaussian. The authors propose an anisotropic model for SVP on isotropic Gaussian data, and show that in overparameterized regimes, the loss function converges to a point where the data model converges. The paper is well-written and well-motivated, and the results are interesting. However, there are a few issues that need to be addressed in order to be accepted by the community of Artificial Intelligence and Statistics."
2998,SP:99f226a63902863c429cb7baefab09626d13921e,online learning of policies USED-FOR MDPs. system USED-FOR policy. episodic setting COMPARE system. system COMPARE episodic setting. navigation constraints FEATURE-OF online setting. algorithm USED-FOR policy. lower bounds USED-FOR algorithm. Metric is sample complexity. ,"This paper studies the online learning of policies in MDPs. The online setting is subject to navigation constraints, which is different from the episodic setting in which the system learns a policy. The authors provide lower bounds on the sample complexity of the algorithm that learns the policy."
2999,SP:99f226a63902863c429cb7baefab09626d13921e,Markov Decision Processes USED-FOR policy identification ( BPI ) problem. sample complexity EVALUATE-FOR lower bound. OtherScientificTerm is navigation constraints. Method is generative model setting. Generic is algorithm. ,"This paper studies the policy identification (BPI) problem in Markov Decision Processes. The authors consider the case where navigation constraints are imposed on the agent, and the generative model setting is used. They derive a lower bound on the sample complexity of the algorithm, which is shown to be tight."
3000,SP:99f226a63902863c429cb7baefab09626d13921e,best - policy identification USED-FOR discounted MDPs. online interaction FEATURE-OF discounted MDPs. online interaction USED-FOR best - policy identification. \delta - correct algorithm USED-FOR problem. sample complexity EVALUATE-FOR \delta - correct algorithm. information - theoretic lower bound FEATURE-OF sample complexity. information - theoretic lower bound USED-FOR \delta - correct algorithm. lower bound USED-FOR non - convex optimization problem. convex relaxation USED-FOR non - convex optimization problem. template USED-FOR pure exploration strategies. algorithm USED-FOR pure exploration strategies. forced exploration HYPONYM-OF asymptotic lower bounds. template USED-FOR algorithm. asymptotic lower bounds USED-FOR algorithm. Task is relaxed optimization problem. ,"This paper studies the problem of best-policy identification in discounted MDPs with online interaction. The authors propose a \delta-correct algorithm to solve this problem with an information-theoretic lower bound on the sample complexity. This lower bound can be extended to a non-convex optimization problem with a convex relaxation, which is a relaxed optimization problem. In addition, the authors propose an algorithm based on a template to learn pure exploration strategies, and provide asymptotic lower bounds (e.g., forced exploration)."
3001,SP:99f226a63902863c429cb7baefab09626d13921e,"best policy identification ( BPI ) USED-FOR finite MDPs. problem - dependent sample complexity bounds USED-FOR best policy identification ( BPI ). convex relaxation USED-FOR relaxed lower bound. upper bound COMPARE relaxed lower bound. relaxed lower bound COMPARE upper bound. non - convex optimization problem USED-FOR lower bound. navigation constraints FEATURE-OF problem - dependent lower bound. sample complexity EVALUATE-FOR problem - dependent lower bound. OtherScientificTerm is infinite horizon discounted criterion. Method are MDP, and generative model. Generic is algorithm. ","This paper studies problem-dependent sample complexity bounds for best policy identification (BPI) in finite MDPs. The main result is an upper bound on the infinite horizon discounted criterion and a relaxed lower bound based on a convex relaxation. The lower bound is derived as a non-convex optimization problem, where the MDP is assumed to be a generative model, and the algorithm is trained to identify the optimal policy. The sample complexity of the problem-dependant lower bound depends on the navigation constraints."
3002,SP:99f226a63902863c429cb7baefab09626d13921e,discounted infinite MDP USED-FOR best policy identification problem. lower bound USED-FOR BPI problem. model - based algorithm MDP - NaS USED-FOR optimal policy. OtherScientificTerm is generator. Task is online setting of MDP. Method is MDP. ,"This paper studies the best policy identification problem in a discounted infinite MDP. The main contribution of this paper is to derive a lower bound for the BPI problem. The authors also propose a model-based algorithm MDP-NaS to identify the optimal policy in an online setting of MDP, where the generator is not allowed to observe the state of the MDP during training."
3003,SP:de4a0f5a464aa3311445cc25c4915cf0c4d975c3,"approach USED-FOR embeddings. knowledge base USED-FOR approach. embedding method USED-FOR logic embedding tasks. Generic are representation, and model. OtherScientificTerm are cones, and sector cones. ","This paper presents an approach to learning embeddings from a knowledge base. The representation is learned by sampling from a set of cones, which are then used to train a model. The authors propose an embedding method for logic embedding tasks, which is based on the concept of sector cones."
3004,SP:de4a0f5a464aa3311445cc25c4915cf0c4d975c3,"embedded representation USED-FOR response entities. knowledge graph ( KG ) query systems USED-FOR embedded representation. embedded representation USED-FOR geometric region. embedding representations USED-FOR response entities. multidimensional boxes CONJUNCTION rectangles. rectangles CONJUNCTION multidimensional boxes. multidimensional boxes FEATURE-OF regions. rectangles HYPONYM-OF regions. negation USED-FOR boxes. query operators FEATURE-OF cone family. query graph structures FEATURE-OF benchmarks. Material is Query2box. OtherScientificTerm are Cartesian space of cones, and disjunction. ","This paper proposes to learn an embedded representation of a geometric region using knowledge graph (KG) query systems, which can then be used to represent response entities in the context of embedding representations for response entities. The authors propose to represent the regions as multidimensional boxes, rectangles, and other types of regions, which are represented as a Cartesian space of cones. The boxes are represented using negation, and the rectangles are represented by disjunction. The query operators of the cone family are defined in terms of the query operators. Experiments are conducted on two benchmarks with different query graph structures, Query2box and QueryNet."
3005,SP:de4a0f5a464aa3311445cc25c4915cf0c4d975c3,"region - based embeddings USED-FOR framework. Cartesian product of 2D cones USED-FOR embedding space. 0 - aperture cones USED-FOR Entities. projection CONJUNCTION intersection. intersection CONJUNCTION projection. cones USED-FOR complement operation. union CONJUNCTION complement. complement CONJUNCTION union. intersection CONJUNCTION union. union CONJUNCTION intersection. cone - based region embeddings USED-FOR intersection. Material is knowledge bases. OtherScientificTerm are 2D space, Cartesian product space, and geometric concepts. Task is FOL query embedding. Generic is implementation. ","This paper proposes a framework for learning FOL query embedding using region-based embeddings. Entities are represented as 0-apartment cones, and the embedding space is modeled as a Cartesian product of 2D cones. The cones are used to represent the complement operation, which is defined as the sum of the projection, intersection, union, and complement. The core idea of the paper is to use the cone-based region embedding to model the intersection and union in a 2D space instead of in the Cartesian products space. The paper is well motivated and well-written. The implementation is well-motivated and the experiments are well-done. The experiments are conducted on a variety of knowledge bases and demonstrate the effectiveness of the proposed geometric concepts."
3006,SP:de4a0f5a464aa3311445cc25c4915cf0c4d975c3,geometric embeddings USED-FOR query answering. cone embeddings USED-FOR query answering. cone embeddings HYPONYM-OF geometric embeddings. embeddings of queries USED-FOR Query answering. first order logic USED-FOR embeddings of queries. disjunction CONJUNCTION negation. negation CONJUNCTION disjunction. conjunction CONJUNCTION disjunction. disjunction CONJUNCTION conjunction. probabilistic query based model HYPONYM-OF embeddings. Geometric based embeddings CONJUNCTION probabilistic models. probabilistic models CONJUNCTION Geometric based embeddings. geometric embeddings USED-FOR negation. OtherScientificTerm is Universal Quantification. ,"Query answering with embeddings of queries based on first order logic is an important problem that has been studied in the literature for a long time. In this paper, the authors propose to use a geometric embedding of queries (concave embedding, e.g. cones) for the purpose of query answering. The main contribution of this paper is the use of geometric embeds (e.g., cones) to perform query answering, specifically, the application of cone embeds for query answering to the problem of combining conjunction, disjunction, negation, etc. The authors also propose a probabilistic query based model that can be used to combine these two types of embeddents. The paper is well written and easy to follow. The experimental results show the effectiveness of geometric based embeds of queries and the probabilism models. The experiments are well done and the paper is clearly written.   "
3007,SP:de4a0f5a464aa3311445cc25c4915cf0c4d975c3,"embedding model USED-FOR multi - hop reasoning over knowledge graphs. intersection CONJUNCTION projection and complement operators. projection and complement operators CONJUNCTION intersection. projection and complement operators HYPONYM-OF neural logic operators. intersection HYPONYM-OF neural logic operators. DNF technique USED-FOR operators. model USED-FOR union. DNF technique USED-FOR model. DNF technique USED-FOR union. surface ( sphere cap ) PART-OF n - sphere. OtherScientificTerm are 1 - sphere, union operators, and independent arc segments. ","This paper proposes a new embedding model for multi-hop reasoning over knowledge graphs. The model learns to represent a union using a DNF technique that learns operators such as intersection, projection and complement operators. The union is represented as an n-sphere with a surface (sphere cap) and a number of independent arc segments. The number of nodes in the union is proportional to the size of the 1-spheres."
3008,SP:773b5b6d31e6899da395933eb7f9e25a6e50c406,continuous state and action spaces FEATURE-OF dynamic programming. state and action space USED-FOR VI. classical value iteration USED-FOR VI. method COMPARE CVI. CVI COMPARE method. algorithm USED-FOR VI. conjugate domain FEATURE-OF VI. VI USED-FOR CVI. algorithm USED-FOR CVI. dual space FEATURE-OF problem. summation USED-FOR problem. CVI COMPARE VI. VI COMPARE CVI. complexity EVALUATE-FOR CVI. complexity EVALUATE-FOR VI. OtherScientificTerm is dual of the objective function. ,"This paper studies dynamic programming in continuous state and action spaces in the context of dynamic programming, where the dual of the objective function is not known. The authors propose a method, VI, which extends classical value iteration to the state andaction space. The proposed method is compared to CVI, which is a variant of VI in the conjugate domain, and CVI in the dual space, and the proposed algorithm is shown to outperform VI in CVI. In addition, the authors show that the complexity of the proposed VI is comparable to that of CVI with the addition of summation to the problem."
3009,SP:773b5b6d31e6899da395933eb7f9e25a6e50c406,"transition function CONJUNCTION cost functions. cost functions CONJUNCTION transition function. state space CONJUNCTION decision space. decision space CONJUNCTION state space. convex duality USED-FOR minimization problem. convex conjugate of c1 CONJUNCTION convex conjugate. convex conjugate CONJUNCTION convex conjugate of c1. convex conjugate of c1 USED-FOR optimization problem. convex conjugate USED-FOR optimization problem. convex conjugate functions USED-FOR algorithm. Method is value iteration. OtherScientificTerm are f2(u ), c1(x ), disturbance w, inverted pendulum, discretized disturbances, value function, f1(x ), and error bounds. Generic are problems, and problem. ","This paper studies the problem of value iteration in the setting where the state space is a state space and the decision space is the transition function and the cost functions are discretized disturbances. The authors consider the minimization problem in terms of convex duality, where f2(u) and f1(x) are convex functions of the input x and the output w. The problem is formulated as a convex version of the inverted pendulum, where the input w is a disturbance w, and the outputs w are a transition function w and a set of cost functions w. In this setting, the authors consider two problems: (1) the problem can be viewed as an optimization problem over the convex conjugate of c1, and (2) the optimization problem can also be considered as a function of the original convex function f1 and the new convex conveugate c1. The main contribution of this paper is to provide error bounds for both of these problems. In particular, the error bounds are shown for the case where the value function is a function f(x, w) and the loss w is an exponential function of f1. In addition, the algorithm can be seen as an extension of the work of [1], in which the authors show that the algorithm is equivalent to an algorithm that uses two convex convolutions of the same function, and that the loss can be expressed as the sum of the conveconjugate functions of f(w) and c1(w)."
3010,SP:773b5b6d31e6899da395933eb7f9e25a6e50c406,"method USED-FOR optimal control problems. value iteration ( VI ) algorithm USED-FOR method. convex duality USED-FOR value iteration ( VI ) algorithm. slow $ \inf$ operation COMPARE addition operation. addition operation COMPARE slow $ \inf$ operation. time complexity EVALUATE-FOR algorithm. convergence CONJUNCTION time complexity. time complexity CONJUNCTION convergence. convergence EVALUATE-FOR algorithm. naive "" VI algorithm COMPARE algorithm. algorithm COMPARE naive "" VI algorithm. OtherScientificTerm are Legendre - Fenchel transform, and infimal convolution. Method is MATLAB. ","This paper proposes a method for solving optimal control problems based on the value iteration (VI) algorithm with convex duality. The main idea is to use the Legendre-Fenchel transform, where the slow $\inf$ operation is replaced by the addition operation. The authors prove convergence and time complexity of the proposed algorithm, which is shown to be faster than the ""naive"" VI algorithm. The experiments are conducted on MATLAB, and the authors show that the infimal convolution is effective."
3011,SP:773b5b6d31e6899da395933eb7f9e25a6e50c406,value iteration algorithm ( CVI ) USED-FOR optimal control of stochastic system. continuous state space FEATURE-OF optimal control of stochastic system. value iteration step USED-FOR algorithm. algorithm USED-FOR stochastic system. stochastic system CONJUNCTION discretization. discretization CONJUNCTION stochastic system. CVI - d COMPARE classical value iteration ( VI ). classical value iteration ( VI ) COMPARE CVI - d. CVI COMPARE CVI - d. CVI - d COMPARE CVI. noisy inverted pendulum experiment USED-FOR classical value iteration ( VI ). Metric is one - step computational complexity. ,"This paper proposes a value iteration algorithm (CVI) for the optimal control of stochastic system in continuous state space. The algorithm is based on the value iteration step, and the authors prove that the one-step computational complexity of the algorithm is polynomial in the number of iterations. The authors also prove that CVI-d is equivalent to classical value iteration (VI) in the noisy inverted pendulum experiment, which is a nice proof of convergence of the proposed algorithm to the optimal state of a stochastically system and discretization."
3012,SP:773b5b6d31e6899da395933eb7f9e25a6e50c406,"biconjugate operations USED-FOR reformulation. conjugate operators USED-FOR algorithm. approach COMPARE value iteration. value iteration COMPARE approach. approach COMPARE variant. variant COMPARE approach. value iteration CONJUNCTION variant. variant CONJUNCTION value iteration. variant USED-FOR dynamic discretization grids. Method is approximate value iteration method. OtherScientificTerm are regulatory assumptions, dual space, and DP value function. ","This paper proposes an approximate value iteration method that does not require any regulatory assumptions. The algorithm uses conjugate operators to approximate the reformulation, which is done in a dual space, where the DP value function is sampled from the dual space. The proposed approach is compared to value iteration and a variant for dynamic discretization grids."
3013,SP:7cd593ccba4830f3383a92ef6266224cc7699706,"Transformer USED-FOR multi - modal representation. multi - modal data USED-FOR video, audio, and text data. approach USED-FOR representation learning. Transformer + contrastive learning USED-FOR approach. Transformer + contrastive learning USED-FOR representation learning. noisy association PART-OF multiple instances learning way. AudioSet USED-FOR training. audio event classification CONJUNCTION zero - shot video retrieval. zero - shot video retrieval CONJUNCTION audio event classification. zero - shot video retrieval CONJUNCTION image classification. image classification CONJUNCTION zero - shot video retrieval. action recognition CONJUNCTION audio event classification. audio event classification CONJUNCTION action recognition. independent feature extractors COMPARE Vision Transformers. Vision Transformers COMPARE independent feature extractors. independent feature extractors USED-FOR Multi - modal data. positional encoding USED-FOR 3D positional encoding. horizontal, vertical, and temporal axes FEATURE-OF 3D positional encoding. relativeness FEATURE-OF horizontal, vertical, and temporal axes. relativeness FEATURE-OF 3D positional encoding. Method is contrastive learning. OtherScientificTerm are semantic granularity, and DropToken. Task is downstream tasks. ","This paper proposes a Transformer for multi-modal representation for video, audio, and text data. The proposed approach is based on Transformer + contrastive learning, which is an existing approach for representation learning. The key idea is to add a noisy association to the multiple instances learning way, and to use AudioSet for training. The authors also propose to use a different form of contrastive learn, where the semantic granularity of each instance is controlled by DropToken. The experimental results on action recognition, audio event classification, zero-shot video retrieval, and image classification show that the proposed independent feature extractors outperform Vision Transformers, and that the 3D positional encoding has better relativeness to horizontal, vertical, and temporal axes. The paper also provides some ablation studies on downstream tasks."
3014,SP:7cd593ccba4830f3383a92ef6266224cc7699706,modality - specific or agnostic transformer encoder CONJUNCTION multimodal projection head. multimodal projection head CONJUNCTION modality - specific or agnostic transformer encoder. linear projection CONJUNCTION modality - specific or agnostic transformer encoder. modality - specific or agnostic transformer encoder CONJUNCTION linear projection. linear projection PART-OF VATT. modality - specific or agnostic transformer encoder PART-OF VATT. multimodal projection head PART-OF VATT. NCE loss CONJUNCTION MIL - NCE loss. MIL - NCE loss CONJUNCTION NCE loss. MIL - NCE loss USED-FOR text - video features. NCE loss USED-FOR video - audio feature learning. NCE loss USED-FOR VATT. MIL - NCE loss USED-FOR VATT. DropToken USED-FOR training. video action recognition CONJUNCTION audio event classification. audio event classification CONJUNCTION video action recognition. HowTo1 M and AudioSet datasets USED-FOR pretraining. VATT USED-FOR finetuning tasks. video action recognition HYPONYM-OF finetuning tasks. audio event classification HYPONYM-OF finetuning tasks. OtherScientificTerm is video - audio - text. ,"This paper proposes VATT, which consists of a linear projection, a modality-specific or agnostic transformer encoder, and a multimodal projection head. VATT uses the NCE loss for video-audio feature learning and the MIL-NCE loss for text-video features. DropToken is used for training and pretraining on the HowTo1M and AudioSet datasets. Experiments show that VATT can achieve state-of-the-art performance on finetuning tasks such as video action recognition and audio event classification."
3015,SP:7cd593ccba4830f3383a92ef6266224cc7699706,transformer - based architecture USED-FOR representations. video - audio - text triplet data USED-FOR representations. modality - specific transformers CONJUNCTION modality - agnostic transformers. modality - agnostic transformers CONJUNCTION modality - specific transformers. downstream tasks EVALUATE-FOR representation. Task is manual data annotation. ,"This paper proposes a transformer-based architecture for learning representations on video-audio-text triplet data. The authors propose to combine modality-specific transformers with modality - agnostic transformers. The proposed representation is evaluated on several downstream tasks, including manual data annotation."
3016,SP:7cd593ccba4830f3383a92ef6266224cc7699706,"It USED-FOR video, audio, text representation. contrastive learning framework USED-FOR video, audio, text representation. contrastive learning framework USED-FOR It. Visual / Audio Transformer encoders CONJUNCTION modality - agnostic Transformers. modality - agnostic Transformers CONJUNCTION Visual / Audio Transformer encoders. Visual / Audio Transformer encoders USED-FOR architecture. MMV USED-FOR architecture. Method is multimodal self - supervised learning. OtherScientificTerm is joint embedding spaces. Task is video(-text ) tasks. ","This paper proposes multimodal self-supervised learning. It uses a contrastive learning framework to learn a video, audio, text representation. The proposed architecture is based on MMV, Visual/Audio Transformer encoders, and modality-agnostic Transformers. The joint embedding spaces are trained on video(-text) tasks."
3017,SP:7cd593ccba4830f3383a92ef6266224cc7699706,framework USED-FOR self - supervised multimodal representations. unlabeled data CONJUNCTION Transformer backbones. Transformer backbones CONJUNCTION unlabeled data. unlabeled data USED-FOR self - supervised multimodal representations. Transformer backbones USED-FOR framework. unlabeled data USED-FOR framework. modality - specific or modality - agnostic HYPONYM-OF Transformer encoders. text - to - video retrieval CONJUNCTION image classification. image classification CONJUNCTION text - to - video retrieval. action recognition CONJUNCTION audio event classification. audio event classification CONJUNCTION action recognition. audio event classification CONJUNCTION text - to - video retrieval. text - to - video retrieval CONJUNCTION audio event classification. action recognition HYPONYM-OF tasks. image classification HYPONYM-OF tasks. text - to - video retrieval HYPONYM-OF tasks. audio event classification HYPONYM-OF tasks. ,"This paper proposes a framework for learning self-supervised multimodal representations from unlabeled data and Transformer backbones. The key idea is to train Transformer encoders (modality-specific or modality-agnostic) on a set of tasks (action recognition, audio event classification, text-to-video retrieval, image classification, etc.). "
3018,SP:97f533426dce73d27768dd7afc2ddf035cf21e61,"Nystrom approximation USED-FOR attention matrix. Nystrom approximation USED-FOR method. attention matrix PART-OF PSD matrix. Performer CONJUNCTION Reformer. Reformer CONJUNCTION Performer. method COMPARE attention. attention COMPARE method. BigBird CONJUNCTION Performer. Performer CONJUNCTION BigBird. Reformer HYPONYM-OF attention. BigBird HYPONYM-OF attention. Performer HYPONYM-OF attention. Method are Skyformer, Nystromformer, and Nystrom method. Generic is It. ","This paper proposes a method called Skyformer that uses Nystrom approximation to approximate the attention matrix in the PSD matrix. It is based on the idea of Nystromformer, which is an extension of the Nystrom method. Experiments show that the proposed method outperforms other types of attention such as BigBird, Performer and Reformer."
3019,SP:97f533426dce73d27768dd7afc2ddf035cf21e61,"approach USED-FOR attention. approach USED-FOR Transformer. attention USED-FOR Transformer. normalization term USED-FOR softmax attention. normalization term PART-OF Kernelized attention. accuracy EVALUATE-FOR Nystrom approximation. accuracy EVALUATE-FOR approach. Nystromformer USED-FOR similarity. approach USED-FOR kernels. Nystromformer COMPARE approach. approach COMPARE Nystromformer. Method are Nystrom method approach, attention mechanism, and kernelized attention. Metric is convergence. ","This paper proposes an approach to improve the performance of attention in Transformer. The main idea is to extend the Nystrom method approach by adding a normalization term to the softmax attention. Kernelized attention is a special case of Nystrom approximation, and the authors show that the proposed approach can improve the accuracy of the Transformer by an order of magnitude. The authors also show that their approach can approximate kernels more accurately than Nystromformer, and that the similarity between the kernels can be improved by adding the normalized term to kernelized attention. Finally, the authors provide a theoretical analysis of the convergence of their approach."
3020,SP:97f533426dce73d27768dd7afc2ddf035cf21e61,modified Nyström method USED-FOR Kernelized Attention. Skyformer USED-FOR Kernelized Attention. Skyformer HYPONYM-OF modified Nyström method. spectral norm FEATURE-OF approximation error. Skyformer COMPARE vanilla self - attention. vanilla self - attention COMPARE Skyformer. OtherScientificTerm is computational costs. ,"This paper proposes Skyformer, a modified Nyström method for Kernelized Attention. The main idea is to reduce the approximation error by minimizing the spectral norm of the error. The authors show that Skyformer outperforms vanilla self-attention in terms of computational costs."
3021,SP:97f533426dce73d27768dd7afc2ddf035cf21e61,self - attention USED-FOR transformer models. Gaussian Kernels USED-FOR self - attention. Gaussian Kernels USED-FOR transformer models. attention method COMPARE softmax self - attention. softmax self - attention COMPARE attention method. accuracy EVALUATE-FOR softmax self - attention. accuracy EVALUATE-FOR attention method. Nystrom sketch USED-FOR kernel matrix. Nystrom sketch USED-FOR kernel based attention. spectral norm FEATURE-OF approximation. ,This paper proposes to use Gaussian Kernels as self-attention for transformer models. The proposed attention method is shown to outperform the softmax self-algorithm in terms of accuracy. The kernel based attention is based on the Nystrom sketch of the kernel matrix. The spectral norm of the approximation is also analyzed.
3022,SP:97f533426dce73d27768dd7afc2ddf035cf21e61,"self - attention mechanism USED-FOR soft max structure. Transformers FEATURE-OF soft max structure. Transformers FEATURE-OF self - attention mechanism. Gaussian kernel evaluation USED-FOR soft max structure. quadratic computational complexity EVALUATE-FOR Kernel approximation strategies. Nyström approximation HYPONYM-OF approximation strategy. Gaussian kernel USED-FOR PSD kernel matrix. low - rank approximation USED-FOR kernelized attention score. relative spectral norm USED-FOR low - rank approximation. matrix inversion USED-FOR Nyström approximation. numerical stability FEATURE-OF GPU. conjugate gradient method COMPARE Schulz type iteration. Schulz type iteration COMPARE conjugate gradient method. matrix products USED-FOR Schulz type iteration. Empirical evaluation COMPARE naive Nyström approximation. naive Nyström approximation COMPARE Empirical evaluation. tasks COMPARE naive Nyström approximation. naive Nyström approximation COMPARE tasks. LRA benchmark EVALUATE-FOR naive Nyström approximation. tasks EVALUATE-FOR Empirical evaluation. LRA benchmark EVALUATE-FOR Empirical evaluation. LRA benchmark EVALUATE-FOR tasks. Generic are model, and matrix. OtherScientificTerm are kernel matrix, PSD, empirical kernel matrix, and floating point divisions. ","This paper proposes a self-attention mechanism with Transformers to approximate the soft max structure of a PSD kernel matrix with Gaussian kernel evaluation. Kernel approximation strategies have quadratic computational complexity and the authors propose a new approximation strategy, Nyström approximation, which is based on matrix inversion. The main idea of the proposed model is to compute the kernel matrix as a function of the PSD and then compute a low-rank approximation to the kernelized attention score based on the relative spectral norm of the matrix. Empirical evaluation on the LRA benchmark shows that the proposed conjugate gradient method performs better than the Schulz type iteration with matrix products. The numerical stability of the GPU is also improved by adding floating point divisions."
3023,SP:a6f1094a4c9f38df38c9710b9dcd6299f430fae2,"Augmented Policy Cloning ( APC ) approach USED-FOR expert behavior. virtual, perturbed states CONJUNCTION expert actions. expert actions CONJUNCTION virtual, perturbed states. expert actions USED-FOR virtual states. virtual, perturbed states PART-OF expert trajectory data. it USED-FOR transfer high - DoF behaviors. it USED-FOR policy cloning. policy cloning CONJUNCTION transfer high - DoF behaviors. transfer high - DoF behaviors CONJUNCTION policy cloning. data efficiency EVALUATE-FOR policy cloning. data efficiency EVALUATE-FOR it. image - based data augmentation method USED-FOR method. method COMPARE BC. BC COMPARE method. method COMPARE Naive ABC. Naive ABC COMPARE method. BC CONJUNCTION Naive ABC. Naive ABC CONJUNCTION BC. ","This paper proposes an Augmented Policy Cloning (APC) approach to learn expert behavior by augmenting expert trajectory data with virtual, perturbed states and expert actions. The method is based on an image-based data augmentation method, and it is shown to improve the data efficiency of policy cloning and transfer high-DoF behaviors. Experiments show that the proposed method outperforms BC and Naive ABC."
3024,SP:a6f1094a4c9f38df38c9710b9dcd6299f430fae2,expert policies USED-FOR RL context. approach USED-FOR RL context. expert policies USED-FOR approach. state - action pairs USED-FOR policy learning algorithm. Augmented Policy Cloning ( APC ) COMPARE Behaviour Cloning. Behaviour Cloning COMPARE Augmented Policy Cloning ( APC ). Augmented Policy Cloning ( APC ) USED-FOR cloned policy. cloned policy COMPARE Behaviour Cloning. Behaviour Cloning COMPARE cloned policy. Material is expert demonstrations. OtherScientificTerm is Gaussian noise. Method is expert policy. ,"This paper presents an approach to use expert policies in RL context. The key idea is to use the state-action pairs of a policy learning algorithm to generate new state-actions pairs based on the expert demonstrations. The paper compares Augmented Policy Cloning (APC) with Behaviour Cloning, where the cloned policy is trained with Gaussian noise and the expert policy is used to generate a new state."
3025,SP:a6f1094a4c9f38df38c9710b9dcd6299f430fae2,"data - augmentation technique USED-FOR policy learning. augmented policy cloning ( APC ) HYPONYM-OF data - augmentation technique. parametric experts USED-FOR policy learning. student policy USED-FOR high Degrees of Freedom environment. Generic are It, and augmentation. ","This paper proposes augmented policy cloning (APC), a data-augmentation technique for policy learning with parametric experts. It is based on the idea that the student policy can be trained in a high Degrees of Freedom environment without the need for augmentation."
3026,SP:a6f1094a4c9f38df38c9710b9dcd6299f430fae2,method USED-FOR student policy. data augmentation USED-FOR student policy. data augmentation USED-FOR method. Method is policy cloning method. OtherScientificTerm is expert policy. Task is RL. ,"This paper proposes a policy cloning method. The method uses data augmentation to train a student policy that can be used as a substitute for the expert policy. The idea is interesting and interesting. However, the novelty of this paper is limited. The paper is not well-motivated and the experiments are not very convincing. It is not clear to me why this is a good idea in RL."
3027,SP:a6f1094a4c9f38df38c9710b9dcd6299f430fae2,"parametric expert policy USED-FOR non - interactive imitation learning setting. action USED-FOR agent. IL USED-FOR warmstarting. imitation learning tasks CONJUNCTION RL tasks. RL tasks CONJUNCTION imitation learning tasks. method COMPARE Behavior cloning. Behavior cloning COMPARE method. method COMPARE approach. approach COMPARE method. Behavior cloning CONJUNCTION approach. approach CONJUNCTION Behavior cloning. IL USED-FOR RL tasks. OtherScientificTerm are logged expert data ’s states, expert ’s policy, logged states, and actions. ","This paper proposes a parametric expert policy for a non-interactive imitation learning setting where logged expert data’s states are available. The agent is trained to take an action based on the logged states, and the agent is evaluated on a set of imitation learning tasks and RL tasks using IL for warmstarting. The proposed method is compared to Behavior cloning and an approach that does not use logged states but only actions."
3028,SP:3660d1d4a8e8f281880781ba32df7b678b705f9c,techniques USED-FOR unadversarial images. technique USED-FOR unadversarial patches. unadversarial textures USED-FOR 3D meshes. robustness EVALUATE-FOR pre - trained classifier. other USED-FOR unadversarial textures. 3D meshes USED-FOR 3D meshes. ImageNet CONJUNCTION CIFAR. CIFAR CONJUNCTION ImageNet. CIFAR EVALUATE-FOR unadversarial patches. ImageNet EVALUATE-FOR unadversarial patches. simulated data CONJUNCTION small scale real world experiment. small scale real world experiment CONJUNCTION simulated data. simulated data EVALUATE-FOR unadversarial textures. small scale real world experiment EVALUATE-FOR unadversarial textures. Method is computer vision deep learning model. Generic is patches. ,"This paper proposes two techniques to generate unadversarial images. One technique is to generate a set of patches that are similar to a pre-trained computer vision deep learning model, and the other is to learn unadsarvely textures for 3D meshes that can be used to train a 3D mesh. The idea is to improve the robustness of the pre-trainable classifier by training the patches to be similar to the original image. Experiments on ImageNet and CIFAR demonstrate the effectiveness of the proposed techniques. On simulated data and a small scale real world experiment, the authors show that the proposed patches are more robust to different types of attacks."
3029,SP:3660d1d4a8e8f281880781ba32df7b678b705f9c,techniques USED-FOR textures. techniques USED-FOR patches. patches CONJUNCTION textures. textures CONJUNCTION patches. adversarial examples literature USED-FOR techniques. clean CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION clean CIFAR-10. ImageNet CONJUNCTION robustness benchmarks. robustness benchmarks CONJUNCTION ImageNet. ImageNet - C ( common perturbations ) HYPONYM-OF robustness benchmarks. gradient - based methods USED-FOR patches / textures. QR codes CONJUNCTION smaller - sized images. smaller - sized images CONJUNCTION QR codes. patches / textures COMPARE baselines. baselines COMPARE patches / textures. smaller - sized images CONJUNCTION predefined fixed patterns. predefined fixed patterns CONJUNCTION smaller - sized images. predefined fixed patterns HYPONYM-OF baselines. smaller - sized images HYPONYM-OF baselines. QR codes HYPONYM-OF baselines. OtherScientificTerm is fixed - model setting. ,"This paper proposes techniques to learn patches and textures from the adversarial examples literature. The authors conduct experiments on clean CIFAR-10, ImageNet, and robustness benchmarks such as ImageNet-C (common perturbations). The authors show that gradient-based methods can learn patches/texture in a fixed-model setting. They also show that the learned patches/trajectories outperform baselines such as QR codes, smaller-sized images, and predefined fixed patterns."
3030,SP:3660d1d4a8e8f281880781ba32df7b678b705f9c,"adversarial patches HYPONYM-OF adversarial examples. small patch CONJUNCTION object texture. object texture CONJUNCTION small patch. prediction accuracy EVALUATE-FOR small patch. gradient descent USED-FOR small patch. blur CONJUNCTION fog. fog CONJUNCTION blur. simulated image corruptions EVALUATE-FOR method. regression task EVALUATE-FOR method. classification accuracy EVALUATE-FOR method. blur HYPONYM-OF simulated image corruptions. fog HYPONYM-OF simulated image corruptions. patch method USED-FOR classification. OtherScientificTerm are object textures, and real physical objects. Metric is classifier accuracy. Material is corrupted images. Method is classifier. ","This paper studies adversarial examples, i.e., adversarial patches that are generated from corrupted images. In particular, the authors study the prediction accuracy of a small patch (or object texture) generated by gradient descent. The method is evaluated on simulated image corruptions (blur, fog, etc) and on a regression task. The classification accuracy of the proposed method is shown to be better than the state-of-the-art in terms of classifier accuracy. The authors also show that the proposed patch method can be used for classification on real physical objects."
3031,SP:3660d1d4a8e8f281880781ba32df7b678b705f9c,"pre - trained model USED-FOR classification task. in - domain performance CONJUNCTION robustness. robustness CONJUNCTION in - domain performance. Method is deep models. Material is images. OtherScientificTerm are patch, texture of specific objects, and data corruptions. ","This paper studies the problem of in-domain performance and robustness to data corruptions in deep models. In particular, the authors consider images where a pre-trained model is trained to perform a classification task on a patch of the image, but the texture of specific objects is not known to the model. The authors show that the performance of deep models can be degraded when the textures of objects are not known."
3032,SP:3660d1d4a8e8f281880781ba32df7b678b705f9c,"CNN based classification CONJUNCTION regression models. regression models CONJUNCTION CNN based classification. CNN based classification HYPONYM-OF computer vision system. regression models HYPONYM-OF computer vision system. marker systems USED-FOR computer vision. approaches USED-FOR patches. patches CONJUNCTION textures. textures CONJUNCTION patches. textures USED-FOR surface of 3D objects. approaches USED-FOR textures. adversarial perturbations USED-FOR approaches. approaches USED-FOR system. corruptions FEATURE-OF robustness. robustness EVALUATE-FOR system. robustness EVALUATE-FOR approaches. non - modified objects COMPARE baselines. baselines COMPARE non - modified objects. Method is detection algorithm. Generic is it. Material are synthetic 2D and 3D data, and real objects. OtherScientificTerm is physical objects. ","This paper proposes a detection algorithm for objects that have been modified in such a way that it is robust to adversarial perturbations. The detection algorithm is based on the idea of ""marker systems"", which are commonly used in computer vision such as CNN based classification and regression models. These marker systems have been shown to be effective in many applications of computer vision, such as the recent advances in CNN based classifying, regression models, etc. The paper proposes two approaches to detect patches and textures on the surface of 3D objects, which are used to train a system that is robust against corruptions. Experiments are conducted on synthetic 2D and 3D data, as well as on real objects. The results show that the non-modified objects are more robust than the baselines."
3033,SP:4c12852373f5f113bd47dce3e2434c5e7d61a202,"data augmentation USED-FOR RL. augmentation USED-FOR erroneous bootstrapping. augmented and un - augmented data USED-FOR Q objective. un - augmented data USED-FOR actor. data augmentation strategies USED-FOR RL. refinements USED-FOR data augmentation strategies. Generic are techniques, tasks, method, and baselines. OtherScientificTerm are policy, and networks. ","This paper studies the problem of data augmentation in RL. The authors argue that augmentation can lead to erroneous bootstrapping, and propose two techniques to address this issue. First, the authors propose to use both augmented and un-augmented data for the Q objective, where the augmented data is used to train the actor, and the un - augmented data are used to fine-tune the policy. Second, they propose two refinements to the existing data augmentation strategies in RL, which are shown to improve the performance of the proposed method compared to the baselines. Finally, they show that the proposed techniques can be applied to a variety of tasks where the networks are not fully trained."
3034,SP:4c12852373f5f113bd47dce3e2434c5e7d61a202,"data augmentation USED-FOR high - variance Q - targets. augmentation USED-FOR Q - value estimation. unaugmented data USED-FOR actor - critic algorithms. augmentation USED-FOR bootstrapping. unaugmented data USED-FOR actor. DeepMind control suite CONJUNCTION variants. variants CONJUNCTION DeepMind control suite. raw pixel inputs USED-FOR DeepMind control suite. raw pixel inputs USED-FOR variants. data augmentation methods COMPARE SVEA method. SVEA method COMPARE data augmentation methods. DrQ HYPONYM-OF data augmentation methods. DistractingCS CONJUNCTION robotic manipulation environments. robotic manipulation environments CONJUNCTION DistractingCS. robotic manipulation environments FEATURE-OF Generalization. DistractingCS FEATURE-OF Generalization. Vision Transformers architecture USED-FOR method. Method is SVEA. Generic are objective, and components. OtherScientificTerm are augmented and original observations, and parameter sharing. ","This paper proposes to use data augmentation to address high-variance Q-targets. The main idea is to use unaugmented data to train actor-critic algorithms and use the augmentation for Q-value estimation. The proposed method, called SVEA, is based on the Vision Transformers architecture and is trained using raw pixel inputs from the DeepMind control suite and variants. The authors compare the performance of the proposed data augmentation methods, DrQ, with the SVEE method and show that the proposed objective is more robust to both augmented and original observations. Generalization results are reported on DistractingCS and robotic manipulation environments and show better performance with parameter sharing."
3035,SP:4c12852373f5f113bd47dce3e2434c5e7d61a202,technique USED-FOR reinforcement learning. robustness CONJUNCTION sample - efficiency. sample - efficiency CONJUNCTION robustness. sample - efficiency EVALUATE-FOR reinforcement learning. robustness EVALUATE-FOR reinforcement learning. technique USED-FOR robustness. sample - efficiency EVALUATE-FOR technique. technique USED-FOR algorithm. Q - learning subroutine PART-OF algorithm. modification USED-FOR learning. modification USED-FOR Q - targets. ,This paper proposes a technique to improve the robustness and sample-efficiency of reinforcement learning. The proposed algorithm consists of a Q-learning subroutine. The main contribution of the paper is a modification to the Q-targets to speed up learning.
3036,SP:4c12852373f5f113bd47dce3e2434c5e7d61a202,visual inputs USED-FOR image transformation. visual inputs USED-FOR value - based deep reinforcement learning agents. state representations USED-FOR visual variations. Control tasks EVALUATE-FOR method. benchmarks USED-FOR generalization. tasks COMPARE domains. domains COMPARE tasks. domains CONJUNCTION benchmarks. benchmarks CONJUNCTION domains. Atari games HYPONYM-OF domains. Natural Atari HYPONYM-OF benchmarks. Natural Atari HYPONYM-OF generalization. OtherScientificTerm is high dimensional input observations. Task is generalization of learned skills. Generic is prior methods. Method is simplified approach ( SVEA ). Material is CoinRun / ProcGen. ,"This paper studies the problem of generalization of value-based deep reinforcement learning agents to visual inputs for image transformation. In particular, the authors focus on high dimensional input observations, and propose a simplified approach (SVEA) to address this issue. The proposed method is evaluated on Control tasks, where the visual variations are modeled using state representations. The experiments show that the proposed method outperforms prior methods on CoinRun/ProcGen, and shows better generalization performance on other tasks and domains (e.g., Atari games) and benchmarks (Natural Atari)."
3037,SP:4c12852373f5f113bd47dce3e2434c5e7d61a202,DrQ HYPONYM-OF image - based RL algorithm. training stability EVALUATE-FOR DrQ. image augmentation USED-FOR DrQ. data augmentation USED-FOR image - based RL algorithm. augmented and un - augmented observations USED-FOR critic. benchmarks EVALUATE-FOR method. OtherScientificTerm is actor's inputs. ,"This paper proposes DrQ, an image-based RL algorithm with data augmentation, to improve training stability. The key idea is to augment the actor's inputs with augmented and un-augmented observations, which are then used to train the critic. The method is evaluated on several benchmarks."
3038,SP:f8ca9d92c45adc4512381035856b445029e3080a,method USED-FOR finite sum problem. expectation function FEATURE-OF local model. federated learning USED-FOR finite sum problem. momentum variance - reduced estimator USED-FOR local updates. STORM estimator USED-FOR stochastic optimization. momentum variance - reduced estimator USED-FOR STEM algorithm. momentum updates USED-FOR local and server updates. momentum updates PART-OF STEM. STEM COMPARE sample complexity. sample complexity COMPARE STEM. log factor FEATURE-OF lower bound communication complexity. lower bound communication complexity EVALUATE-FOR it. communication frequency CONJUNCTION minibatch size. minibatch size CONJUNCTION communication frequency. STEM COMPARE SCAFFOLD. SCAFFOLD COMPARE STEM. STEM COMPARE FedAvg. FedAvg COMPARE STEM. FedAvg CONJUNCTION SCAFFOLD. SCAFFOLD CONJUNCTION FedAvg. neural network training EVALUATE-FOR STEM. Generic is estimators. OtherScientificTerm is server - side update. ,"This paper proposes a method for solving a finite sum problem in federated learning with respect to the expectation function of the local model. The proposed STEM algorithm uses a momentum variance-reduced estimator for local updates, which is a variant of the STORM estimator used in stochastic optimization. The momentum updates in STEM are used for both local and server updates, where the server-side update is computed by minimizing the difference between the two estimators. The authors show that it has a lower bound communication complexity of a log factor that depends on the communication frequency and minibatch size. Experiments on neural network training show that the proposed STEM outperforms FedAvg, SCAFFOLD."
3039,SP:f8ca9d92c45adc4512381035856b445029e3080a,two - step momentum method USED-FOR federated learning. sample and communication complexities EVALUATE-FOR first - order stochastic FL algorithms. method USED-FOR first - order stochastic FL algorithms. sample and communication complexities EVALUATE-FOR method. synchronization interval CONJUNCTION batch size. batch size CONJUNCTION synchronization interval. ,This paper proposes a two-step momentum method for federated learning. The proposed method is shown to reduce the sample and communication complexities of first-order stochastic FL algorithms. The paper also provides theoretical analysis on the synchronization interval and batch size.
3040,SP:f8ca9d92c45adc4512381035856b445029e3080a,FedAvg method USED-FOR non - convex federated learning. unified framework USED-FOR convergence analysis. momentum extension USED-FOR non - convex federated learning. momentum extension CONJUNCTION unified framework. unified framework CONJUNCTION momentum extension. FedAvg method USED-FOR momentum extension. local update frequencies CONJUNCTION local minibatch sizes. local minibatch sizes CONJUNCTION local update frequencies. near - tight sample and communication complexities EVALUATE-FOR method. CIFAR-10 and MNIST image classification tasks EVALUATE-FOR method. ,This paper proposes a momentum extension to the FedAvg method for non-convex federated learning and a unified framework for convergence analysis. The proposed method is evaluated on CIFAR-10 and MNIST image classification tasks with near-tight sample and communication complexities with respect to local update frequencies and local minibatch sizes.
3041,SP:f8ca9d92c45adc4512381035856b445029e3080a,momentum - assisted stochastic gradient directions USED-FOR Stochastic Two - Sided Momentum algorithm. communication complexity EVALUATE-FOR method. near optimal sample complexity EVALUATE-FOR method. minibatch sizes CONJUNCTION local update frequency. local update frequency CONJUNCTION minibatch sizes. ,This paper proposes a Stochastic Two-Sided Momentum algorithm with momentum-assisted stochastic gradient directions. The proposed method achieves near optimal sample complexity with a communication complexity of O(1/\sqrt{T}) and O(n^2) for minibatch sizes and local update frequency.
3042,SP:f8ca9d92c45adc4512381035856b445029e3080a,"STEM USED-FOR federated learning. sample gradient smoothness condition USED-FOR STEM. momentum - based techniques USED-FOR STEM algorithm. communication complexity EVALUATE-FOR STEM algorithm. optimal sample complexity EVALUATE-FOR STEM algorithm. OtherScientificTerm are non - convex functions, and batch size. ","This paper studies the problem of federated learning using STEM, which is an extension of the sample gradient smoothness condition in the case of non-convex functions. The main contribution of this paper is to extend momentum-based techniques to the formulation of the standard STEM algorithm. The authors show that the optimal sample complexity of the proposed STEM algorithm is O(1/\sqrt{n}) with respect to the batch size, and that the communication complexity of this STEM algorithm can be reduced to O(n^{-1/n} with the same communication complexity."
3043,SP:bd3eecb81a17af010f2d3555434990855c1810f2,generalization bound CONJUNCTION covariance of perturbed gradient descent. covariance of perturbed gradient descent CONJUNCTION generalization bound. information - theoretic ) bound USED-FOR stochastic optimization methods. optimization USED-FOR neural networks. trace constraint FEATURE-OF covariance of noise. trace constraint USED-FOR upper bound. ,"This paper provides a generalization bound and a covariance of perturbed gradient descent for stochastic optimization methods, which is an interesting (information-theoretic) bound. The main contribution of this paper is to provide an upper bound on the number of iterations of optimization for neural networks. The upper bound is based on a trace constraint on the covariance to the noise."
3044,SP:bd3eecb81a17af010f2d3555434990855c1810f2,"SGLD algorithms USED-FOR generalization. information - theoretic techniques USED-FOR generalization. Gaussian noise PART-OF SGLD update. covariance USED-FOR Gaussian noise. trace FEATURE-OF SGLD. Generic are it, and bound. OtherScientificTerm are noise, and prior distribution. Method is generalization bound. ","This paper studies the generalization of SGLD algorithms using information-theoretic techniques. Specifically, the authors propose a new generalization bound and prove that it is tight. The main idea is to add Gaussian noise to the SGLL update using the covariance of the noise. The authors show that this bound is tight when the prior distribution is Gaussian. Moreover, they show that the trace of the current iteration of the original version of the same class of sGLD converges to the same trace as the original one."
3045,SP:bd3eecb81a17af010f2d3555434990855c1810f2,"covariance structure FEATURE-OF noise term. optimal noise covariance COMPARE empirical gradient covariance. empirical gradient covariance COMPARE optimal noise covariance. SGLD USED-FOR optimal noise covariance. expected gradient covariance USED-FOR optimal noise covariance. SGD noise COMPARE isotropic noise. isotropic noise COMPARE SGD noise. OtherScientificTerm are constraint, data - dependent priors, and generalization bound. ","This paper studies the covariance structure of the noise term in the presence of a constraint. The authors show that the optimal noise covariance of SGLD is a function of the expected gradient covariance, and that the constraint is not strictly related to the data-dependent priors. They also provide a generalization bound that shows that optimal noise covarianance can be expressed as the expected standard deviation of the gradient with respect to the constraint, which is in contrast to the empirical gradient covariances. Finally, they show that SGD noise is more stable than isotropic noise."
3046,SP:bd3eecb81a17af010f2d3555434990855c1810f2,isentropic noise CONJUNCTION data - dependent priors. data - dependent priors CONJUNCTION isentropic noise. Method is information - theoretic analysis of SGLD. OtherScientificTerm is prior distribution covariance. ,This paper presents an information-theoretic analysis of SGLD. The main contribution of the paper is a theoretical analysis of the relationship between isentropic noise and data-dependent priors. The authors show that the prior distribution covariance is a function of the number of samples.
3047,SP:bd3eecb81a17af010f2d3555434990855c1810f2,optimization of the information theoretical generalization bounds FEATURE-OF Stochastic Gradient Langevin Dynamics ( SGLD ). noise covariance COMPARE empirical gradient covariance. empirical gradient covariance COMPARE noise covariance. noise covariance USED-FOR SGLD. updating rule CONJUNCTION noise covariance. noise covariance CONJUNCTION updating rule. noise covariance USED-FOR SGLD. expected gradient covariance USED-FOR noise covariance. OtherScientificTerm is information theoretical generalization bounds. ,"This paper studies the optimization of the information theoretical generalization bounds of Stochastic Gradient Langevin Dynamics (SGLD). The authors show that the updating rule and the noise covariance of SGLD are equivalent to the empirical gradient covariance. The authors then propose to use the expected gradient covance to estimate the noise covance. Finally, the authors provide some experimental results to support their theoretical findings."
3048,SP:19fbd1a381598538662417a4a1885ba4ac04f5f8,flow prediction method USED-FOR motion bits. polynomial approximation USED-FOR flow prediction method. Task is learning - based video coding. Generic is algorithm. ,This paper addresses the problem of learning-based video coding. The authors propose a flow prediction method for motion bits based on polynomial approximation. The algorithm is well motivated and the experimental results are promising.
3049,SP:19fbd1a381598538662417a4a1885ba4ac04f5f8,VLVC HYPONYM-OF neural video compression codec. 3D motion compensation structure USED-FOR model. prediction modes FEATURE-OF model. spatio - temporal interpolation USED-FOR 3D motion compensation structure. low - delay or random - access settings HYPONYM-OF prediction modes. datasets EVALUATE-FOR VLVC codec. rate - distortion EVALUATE-FOR VLVC codec. OtherScientificTerm is reference frames. Method is neural video codecs. Metric is MS - SSIM. ,"This paper proposes a neural video compression codec called VLVC. The proposed model is based on a 3D motion compensation structure based on spatio-temporal interpolation. The prediction modes of the proposed model are low-delay or random-access settings, where the reference frames are not available. The experimental results on two datasets show that the proposed VVVC codec is able to reduce the rate-distortion by a significant margin compared to state-of-the-art neural video codecs. The MS-SSIM is also shown to be improved."
3050,SP:19fbd1a381598538662417a4a1885ba4ac04f5f8,"multi - frame references USED-FOR learned video coding. voxel flows USED-FOR motion compensation. voxel flows USED-FOR multiple optical flows. weighted map USED-FOR weighted trilinear warping. multiple backward optical flow USED-FOR single optical flow. multiple backward optical flow USED-FOR polynominal motion modeling. flow reversal layer USED-FOR forward flow. softmax spaltting USED-FOR motion compensation. forward flow USED-FOR motion compensation. forward flow CONJUNCTION softmax spaltting. softmax spaltting CONJUNCTION forward flow. Residual coding USED-FOR feature domain. OtherScientificTerm are predcition, and backward flow. Method are hybrid video coding system, and inter coding. ","This paper addresses the problem of learned video coding with multi-frame references. The authors propose a hybrid video coding system, where voxel flows are used for motion compensation and multiple optical flows for polynominal motion modeling. The forward flow is trained with a flow reversal layer, and the weighted map is used for weighted trilinear warping. Residual coding is used in the feature domain, where the predcition of the forward flow and softmax spaltting are applied to the motion compensation. The backward flow is used to reconstruct the original video. The inter coding is performed in a supervised fashion."
3051,SP:19fbd1a381598538662417a4a1885ba4ac04f5f8,"voxel flow COMPARE pixel warping idea. pixel warping idea COMPARE voxel flow. voxel flow USED-FOR neural video compression method. RD EVALUATE-FOR methods. compression method COMPARE methods. methods COMPARE compression method. single reference frame USED-FOR methods. OtherScientificTerm are volume of reference frames, flow, and GOP. Generic is method. ","This paper proposes a neural video compression method based on voxel flow instead of pixel warping idea. The main idea is to reduce the volume of reference frames by using a flow, which is similar to GOP. The proposed method is evaluated on a variety of datasets and compared to other methods based on single reference frame on RD. The results show that the proposed compression method outperforms other methods."
3052,SP:19fbd1a381598538662417a4a1885ba4ac04f5f8,"it COMPARE video compression method. video compression method COMPARE it. prediction mode CONJUNCTION fixed network. fixed network CONJUNCTION prediction mode. 3D motion vector fields CONJUNCTION trilinear warping. trilinear warping CONJUNCTION 3D motion vector fields. motion compensation PART-OF versatile compression. 3D motion vector fields USED-FOR motion compensation. trilinear warping USED-FOR motion compensation. Motion estimation USED-FOR prediction modes. network USED-FOR motion fields. unified polynomial function USED-FOR network. network USED-FOR multiple reference frame prediction. unified polynomial function USED-FOR motion fields. flow prediction USED-FOR reduction of computation. Method are Learned video compression, learned video compression framework, and framework design. Generic is field. Metric is rate distortion. OtherScientificTerm is voxel flow transmission. ","This paper proposes a new learned video compression framework, and compares it with the state-of-the-art video compression method. The proposed versatile compression combines motion compensation with 3D motion vector fields and trilinear warping. Motion estimation is used for both prediction modes and fixed network. A unified polynomial function is used to train the network for multiple reference frame prediction. The flow prediction is also used for reduction of computation. The experimental results show that the proposed field is able to reduce the rate distortion of voxel flow transmission, and the proposed framework design is well motivated."
3053,SP:ba790fdcf2deef1a1b5e1961c7c4a28dd0218420,"Task is multitask online convex optimization. Method is strongly convex regularizer. OtherScientificTerm are task relatedness, and task relatedness matrix. Material is real data. ","This paper studies the problem of multitask online convex optimization. Specifically, the authors propose a strongly convex regularizer that encourages the task relatedness to be close to the target task. The main idea is to learn a task-dependent matrix of the task-relatedness matrix, which is then used to train the regularizer. Experiments are conducted on real data."
3054,SP:ba790fdcf2deef1a1b5e1961c7c4a28dd0218420,Bregman divergences CONJUNCTION positively weighted task - interaction matrix. positively weighted task - interaction matrix CONJUNCTION Bregman divergences. convex losses CONJUNCTION Bregman divergences. Bregman divergences CONJUNCTION convex losses. convex losses USED-FOR multi - task variant of online mirror descent. positively weighted task - interaction matrix USED-FOR multi - task variant of online mirror descent. Task is online multi - task learning setting. OtherScientificTerm is regret guarantees. Method is independent mirror descent algorithms. Generic is algorithms. ,"This paper studies the online multi-task learning setting, where the goal is to obtain regret guarantees. The authors propose a multi-tasking variant of online mirror descent with convex losses, Bregman divergences, and a positively weighted task-interaction matrix. The paper also provides a theoretical analysis of the performance of independent mirror descent algorithms. Finally, the authors provide empirical evaluations of the proposed algorithms."
3055,SP:ba790fdcf2deef1a1b5e1961c7c4a28dd0218420,"regret EVALUATE-FOR it. OGD CONJUNCTION EG. EG CONJUNCTION OGD. Method are MT - OMD, and closed - form updates. ","This paper proposes a new version of MT-OMD, called MT-ODM, and evaluates it in terms of regret. In particular, the authors compare OGD and EG and show that OGD performs better than EG. The authors also show that closed-form updates perform better."
3056,SP:ba790fdcf2deef1a1b5e1961c7c4a28dd0218420,"MT - OMD USED-FOR multi - task online learning. convex loss functions USED-FOR MT - OMD. Method is naive optimization. OtherScientificTerm are task similarity, distance of reference vectors, and regret bound. ","This paper proposes MT-OMD for multi-task online learning with convex loss functions. Compared to naive optimization, where the task similarity is assumed to be a function of the distance of reference vectors, the authors propose to use the distance between the reference vectors as a measure of task similarity. The authors provide a regret bound of $O(1/\sqrt{T})$ for the case where the distance is small."
3057,SP:ba790fdcf2deef1a1b5e1961c7c4a28dd0218420,"Task is multitask online convex optimization ( OCO ). OtherScientificTerm are convex decision set, time horizon, regret bound, and task variance. Metric are multitask regret, and multitask regret bound. Method are OMD, and multitask OMD ( MT - OMD ). ","This paper studies the problem of multitask online convex optimization (OCO), where the goal is to minimize a convex decision set over a time horizon. The authors propose a new multitask regret bound, which is based on the fact that the OMD is linear in the time horizon, and that the regret bound depends only on the task variance. The paper also proposes a multitask OMD (MT-OMD), which is a generalization of the original multitask loss."
3058,SP:75f80e4e7836a7575e60de7f055820c6c7065fcb,methods USED-FOR underdamped Langevin diffusion processes. sum - decomposable strongly convex potential FEATURE-OF underdamped Langevin diffusion processes. Accelerated ULD - MCMC algorithm CONJUNCTION variance - reduced variants. variance - reduced variants CONJUNCTION Accelerated ULD - MCMC algorithm. dimensionality CONJUNCTION component number. component number CONJUNCTION dimensionality. information - based lower bound USED-FOR estimating ULD. information - based lower bound USED-FOR gradient complexity. component number CONJUNCTION target accuracy. target accuracy CONJUNCTION component number. ALUM COMPARE algorithms. algorithms COMPARE ALUM. algorithms USED-FOR estimation ULD processes. algorithms COMPARE algorithms. algorithms COMPARE algorithms. algorithms COMPARE ALUM. ALUM COMPARE algorithms. OtherScientificTerm is upper bound. ,"This paper proposes methods for estimating underdamped Langevin diffusion processes with a sum-decomposable strongly convex potential. The authors propose an Accelerated ULD-MCMC algorithm and two variance-reduced variants. The main contribution of the paper is to derive an information-based lower bound for the gradient complexity for estimating ULD that takes into account the dimensionality, component number, and target accuracy. The upper bound is proved to be tight. The experiments show that the proposed algorithms for estimation ULD processes outperform ALUM and other algorithms."
3059,SP:75f80e4e7836a7575e60de7f055820c6c7065fcb,"algorithms USED-FOR upper bound. algorithms USED-FOR sampling error. algorithms USED-FOR approximation error. sampling error FEATURE-OF strongly - log - concave sampling problem. upper bound FEATURE-OF approximation error. algorithms USED-FOR strongly - log - concave sampling problem. approximation error USED-FOR Underdamped Langevin Diffusion ( ULD ) process. strongly - log - concave distribution FEATURE-OF continuous time ULD process. algorithm USED-FOR ULD approximation problem. iteration complexity EVALUATE-FOR algorithm. SVRG - ALUM CONJUNCTION SAGA - ALUM. SAGA - ALUM CONJUNCTION SVRG - ALUM. SVRG - ALUM HYPONYM-OF algorithms. SAGA - ALUM HYPONYM-OF algorithms. gradient complexity EVALUATE-FOR ULD approximation problem. lower bound FEATURE-OF gradient complexity. lower bound FEATURE-OF ULD approximation problem. gradient complexity EVALUATE-FOR Variance reduction modifications of ALUM. Variance reduction modifications of ALUM USED-FOR sum - decomposable setting. stochastic gradient oracle USED-FOR sum - decomposable setting. OtherScientificTerm are invariant distribution, sum - decomposable function, accuracy parameter, gradient evaluations, and sampling / discretization error. Generic are process, and distribution. Task are full gradient setting, and stochastic gradient oracle setting. Method is RMM algorithm. ","This paper proposes two algorithms for computing the upper bound of the approximation error of the Underdamped Langevin Diffusion (ULD) process under an invariant distribution. In particular, the authors consider a continuous time ULD process with a strongly-concave distribution, and propose two algorithms to compute the sampling error of this strongly-log-consistency sampling problem. The first algorithm, SVRG-ALUM, has an iteration complexity of $O(1/\sqrt{T})$, where $T$ is a sum-decomposable function. The second algorithm, ALUM, is an algorithm for solving the ULD approximation problem with a lower bound of $\Omega(T)$. Variance reduction modifications of ALUM are applied to the full gradient setting, as well as to the stochastic gradient oracle setting, where the accuracy parameter is a function of the number of gradient evaluations, and the sampling/discretization error is the sum of the parameters of the RMM algorithm. "
3060,SP:75f80e4e7836a7575e60de7f055820c6c7065fcb,"algorithm USED-FOR stochastic setting. algorithm USED-FOR ULD. gradient FEATURE-OF potential. estimators USED-FOR gradient. overall query and iteration complexity EVALUATE-FOR algorithm. lower bound USED-FOR stochastic setting. OtherScientificTerm are strongly log - concave smooth measure, Underdamped Langevin Diffusion ( ULD ), Gradient queries, and $ N$ smooth functions. Task is approximation problem. ","This paper studies the problem of estimating the gradient of a strongly log-concave smooth measure under the setting of Underdamped Langevin Diffusion (ULD). The authors propose an algorithm to approximate ULD in a stochastic setting with overall query and iteration complexity of $O(\sqrt{T})$. Gradient queries can be expressed as $N$ smooth functions, and estimators for this gradient of the potential are provided. The authors provide a lower bound on the number of queries required to approximate the gradient in the stochastically setting, and provide a theoretical analysis of the approximation problem."
3061,SP:75f80e4e7836a7575e60de7f055820c6c7065fcb,underdamped Langevin diffusion USED-FOR strongly - convex potential. finite sum of smooth components PART-OF underdamped Langevin diffusion. gradient USED-FOR RMM. ALUM USED-FOR full gradient setting. optimal asymptotic complexity EVALUATE-FOR ALUM. gradient USED-FOR ALUM. VR - ALUM methods COMPARE methods. methods COMPARE VR - ALUM methods. gradient complexity EVALUATE-FOR VR - ALUM methods. VR - ALUM methods USED-FOR stochastic gradient setting. gradient complexity EVALUATE-FOR methods. methods USED-FOR approximation problem. OtherScientificTerm is lower bound. ,"This paper studies the underdamped Langevin diffusion for strongly-convex potential, which consists of a finite sum of smooth components. The authors prove a lower bound on the optimal asymptotic complexity of ALUM in the full gradient setting, and show that ALUM can be approximated with the gradient of the RMM. In addition, the authors show that VR-ALUM methods for the stochastic gradient setting have a lower gradient complexity than existing methods for this approximation problem."
3062,SP:75f80e4e7836a7575e60de7f055820c6c7065fcb,randomized algorithm USED-FOR trajectory of underdamped Langevin dynamics. variance reduction method USED-FOR error over sum - decomposable problems. variance reduction method USED-FOR It. Method is RMM method. OtherScientificTerm is information theoretic lower bound. Generic is method. ,This paper proposes a randomized algorithm for estimating the trajectory of underdamped Langevin dynamics. It is based on a variance reduction method that is applied to error over sum-decomposable problems. The authors provide an information theoretic lower bound on the variance of the RMM method and provide a theoretical analysis of the proposed method.
3063,SP:22822f378c3fbc15b77eb736194b1ce7f0585072,model USED-FOR continual learning. parameter regularization methods CONJUNCTION gradient projection methods. gradient projection methods CONJUNCTION parameter regularization methods. bayesian perspective USED-FOR regularization. precision matrix USED-FOR gradient. precision matrix USED-FOR update rule. trust - region optimization USED-FOR gradient projection. stimulus - response dataset CONJUNCTION stroke MNIST. stroke MNIST CONJUNCTION stimulus - response dataset. stroke MNIST USED-FOR sequential problems. stimulus - response dataset USED-FOR sequential problems. OtherScientificTerm is Laplace distribution. Method is online updates. ,"This paper proposes a model for continual learning based on the Laplace distribution. The authors combine parameter regularization methods with gradient projection methods. The regularization is derived from a bayesian perspective, and the update rule is based on a precision matrix that is used to estimate the gradient. The gradient projection is performed using trust-region optimization. The experiments are conducted on sequential problems on a stimulus-response dataset and stroke MNIST, and online updates are performed."
3064,SP:22822f378c3fbc15b77eb736194b1ce7f0585072,"sequential task learning FEATURE-OF catastrophic forgetting. recurrent neural networks USED-FOR catastrophic forgetting. regularization - based family FEATURE-OF continual learning approaches. SOTA techniques HYPONYM-OF continual learning approaches. formalism USED-FOR probabilistic learning setting. supervised learning CONJUNCTION recurrent models. recurrent models CONJUNCTION supervised learning. supervised learning USED-FOR it. neural networks USED-FOR formalism. neural networks USED-FOR probabilistic learning setting. NCL COMPARE approaches. approaches COMPARE NCL. controlled experimental settings FEATURE-OF NCL framework. Task is continual learning setting. Method are Natural Continual Learning ( NCL ), and biological learning. ","This paper studies the problem of catastrophic forgetting in sequential task learning with recurrent neural networks. The continual learning setting is well-studied, and continual learning approaches such as SOTA techniques fall into a regularization-based family. This paper proposes a new formalism for the probabilistic learning setting based on neural networks, called Natural Continual Learning (NCL). The authors show that it can be combined with supervised learning and recurrent models. The NCL framework is evaluated in controlled experimental settings, and NCL is shown to outperform existing approaches. The authors also provide some theoretical analysis to support their claims, and provide some empirical results on biological learning."
3065,SP:22822f378c3fbc15b77eb736194b1ce7f0585072,NCL USED-FOR recurrent neural network ( RNN ). NCL HYPONYM-OF regularization based continual learning approach. Laplace approximation USED-FOR approximate posterior. Laplace approximation USED-FOR NCL. approximate posterior USED-FOR NCL. Bayesian continual learning USED-FOR NCL. NCL COMPARE baseline methods. baseline methods COMPARE NCL. methods USED-FOR trajectories of convergence. convex and non - convex loss landscapes USED-FOR NCL. trust region based regularization method USED-FOR gradient. curvature of prior distribution USED-FOR trust region based regularization method. precision matrix USED-FOR curvature of prior distribution. precision matrix USED-FOR trust region based regularization method. technique COMPARE projection based methods. projection based methods COMPARE technique. NCL COMPARE baselines. baselines COMPARE NCL. stimulus - response and MNIST tasks EVALUATE-FOR baselines. stimulus - response and MNIST tasks EVALUATE-FOR NCL. Generic is task. ,This paper proposes a regularization based continual learning approach called NCL for recurrent neural network (RNN). NCL is based on Bayesian continual learning and uses a Laplace approximation to approximate posterior. NCL can be applied to both convex and non-convex loss landscapes and is shown to outperform baseline methods in terms of trajectories of convergence. The paper also proposes a trust region based regularization method that penalizes the gradient based on the curvature of prior distribution using a precision matrix. The proposed technique is compared to projection based methods and compared to several baselines on both stimulus-response and MNIST tasks. The experimental results on the first task show that NCL outperforms the baselines.
3066,SP:22822f378c3fbc15b77eb736194b1ce7f0585072,weight regularization CONJUNCTION gradient projection. gradient projection CONJUNCTION weight regularization. gradient projection USED-FOR non - interfering subspaces. weight regularization USED-FOR NCL. gradient projection PART-OF NCL. Fisher information matrix USED-FOR trust region optimization. trust region optimization USED-FOR Gradient projection. Fisher information matrix USED-FOR Gradient projection. Kronecker - factor approximation USED-FOR method. recurrent neural network ( RNN ) architectures USED-FOR continual learning approaches. Method is Natural Continual Learning ( NCL ). ,"This paper proposes Natural Continual Learning (NCL), which combines weight regularization and gradient projection to find non-interrupting subspaces in NCL. Gradient projection is based on trust region optimization with the Fisher information matrix, which is used as a trust region estimation. The proposed method uses a Kronecker-factor approximation. The method is motivated by the observation that recurrent neural network (RNN) architectures have been shown to be effective continual learning approaches, and that NCL is a natural extension of this idea."
3067,SP:22822f378c3fbc15b77eb736194b1ce7f0585072,extension USED-FOR Bayesian continual learning. prior precision matrix USED-FOR trust region. method USED-FOR RNNs. neuroscience related tasks CONJUNCTION stroke MNIST datasets. stroke MNIST datasets CONJUNCTION neuroscience related tasks. method COMPARE baselines. baselines COMPARE method. stroke MNIST datasets EVALUATE-FOR baselines. neuroscience related tasks EVALUATE-FOR baselines. stroke MNIST datasets EVALUATE-FOR method. neuroscience related tasks EVALUATE-FOR method. Generic is model. ,This paper proposes an extension to Bayesian continual learning. The proposed method trains RNNs by learning a prior precision matrix to represent the trust region of the model. The method is evaluated on neuroscience related tasks and stroke MNIST datasets and compared to baselines.
3068,SP:26de056be14962312c759be5d284ef235d660f9c,"data manifold of interest PART-OF high - dimensional space. upsampling ” padding layer FEATURE-OF square flows. square flows PART-OF injective ( or rectangular ) flows. volume change term USED-FOR injective flows. numerical linear algebra methods USED-FOR volume change computation. it USED-FOR model. out of distribution detection score EVALUATE-FOR it. FashionMNItST USED-FOR model. Task is constructing injective normalizing flows. OtherScientificTerm are injective normalizing flows, low - dimensional space, and volume сhanging term. Generic is approaches. Material are synthetic and real - life data, and MNIST data samples. ","This paper studies the problem of constructing injective normalizing flows. The authors consider injective (or rectangular) flows consisting of square flows with a “upsampling” padding layer, where the data manifold of interest is a high-dimensional space and the volume change term is applied to the injective flows. This volume change computation is typically done using numerical linear algebra methods, which are computationally expensive. In this paper, the authors propose two approaches to address this issue. First, they use FashionMNItST and show that it can improve the model’s out of distribution detection score by a factor of 2.2. Second, they introduce a volume сhanging term that encourages the volume of the original flow to be in a low-dimensional (rather than a high dimensional) space. They evaluate their model on both synthetic and real-life data, as well as on MNIST data samples."
3069,SP:26de056be14962312c759be5d284ef235d660f9c,"low dimensional manifold FEATURE-OF data probability function. unbiased stochastic estimator USED-FOR log det. forward mode USED-FOR map differential. Task is normalizing flow. OtherScientificTerm are manifold assumption, volume change term, and manifold dimension. ","This paper studies the problem of normalizing flow under the manifold assumption. In particular, the authors consider the case where the data probability function is on a low dimensional manifold. The authors propose an unbiased stochastic estimator for the log det, where the volume change term depends on the manifold dimension and the forward mode is used to estimate the map differential."
3070,SP:26de056be14962312c759be5d284ef235d660f9c,"cost function USED-FOR NFs. latent / data spaces USED-FOR NFs. differential geometric variant USED-FOR density estimation. change - of - variables formula FEATURE-OF differential geometric variant. Method are Normalizing Flows ( NFs ), generative models, and Rectangular "" NFs ( RNFs ). Metric is pushforward measure. OtherScientificTerm is invertibility requirement. ","This paper studies Normalizing Flows (NFs), a generalization of the cost function for NFs. NFs are trained on latent/data spaces, where the pushforward measure is a function of the dimension of the latent space. The authors propose a differential geometric variant of the density estimation based on the change-of-variables formula, which they call ""Rectangular"" NFs (RNFs). The authors show that the invertibility requirement of RNFs does not hold for generative models."
3071,SP:26de056be14962312c759be5d284ef235d660f9c,"methods USED-FOR estimating densities. rectangular normalizing flows USED-FOR methods. Rectangular normalizing flows COMPARE squared normalizing flows. squared normalizing flows COMPARE Rectangular normalizing flows. low ( d ) dimensional manifold FEATURE-OF density of data. rectangular flows USED-FOR densities. tori CONJUNCTION spheres. spheres CONJUNCTION tori. low dimensional manifolds FEATURE-OF densities. density estimation USED-FOR injective flows. ML USED-FOR density estimation. stochastic gradient estimates USED-FOR other. methods COMPARE method. method COMPARE methods. OtherScientificTerm are injective instead of bijective flows, manifold, Jacobian - transpose - Jacobian ”, and memory. Generic is One. ","This paper proposes two methods for estimating densities using rectangular normalizing flows, which are injective instead of bijective flows. The main idea is to estimate the density of data on a low (d) dimensional manifold, where the manifold is defined as a “Jacobian-transpose-Jacobian”, which is a function of the dimension of the input manifold. Rectangular normalizing functions are more efficient than squared normalizingflows, and densities on low dimensional manifolds such as tori and spheres can be estimated using rectangular flows. One uses ML to perform density estimation for injective flows, while the other uses stochastic gradient estimates. The authors compare the proposed methods to a few existing methods and show that the proposed method performs better. They also show that their method is memory-efficient."
3072,SP:26de056be14962312c759be5d284ef235d660f9c,manifold CONJUNCTION density. density CONJUNCTION manifold. manifold matching CONJUNCTION density matching. density matching CONJUNCTION manifold matching. two - step training procedure USED-FOR manifold matching. two - step training procedure USED-FOR density matching. technique USED-FOR Jacobian - transpose - Jacobian term. Jacobian - transpose - Jacobian term FEATURE-OF joint training objective. joint training USED-FOR two - step training. simulated data CONJUNCTION MNIST / FMNIST. MNIST / FMNIST CONJUNCTION simulated data. MNIST / FMNIST EVALUATE-FOR method. simulated data EVALUATE-FOR method. Method is rectangular flows. ,"This paper proposes a two-step training procedure for manifold matching and density matching. The main idea is to use rectangular flows, where the manifold and the density are learned jointly. The joint training objective is defined as a Jacobian-transpose-Jacobian term, which is derived from the technique proposed in [1]. The proposed method is evaluated on simulated data and MNIST/FMNIST."
3073,SP:395dae632dab83f3f61bdf67eabe4d351492798c,"biologically motivated mechanism USED-FOR network of slow - responding neurons. biologically motivated mechanism USED-FOR computation. network of slow - responding neurons USED-FOR computation. latent state space FEATURE-OF energy function. energy function USED-FOR neural dynamics. model USED-FOR fast computation. fast computation CONJUNCTION learning. learning CONJUNCTION fast computation. model USED-FOR learning. synthetic data EVALUATE-FOR model. model USED-FOR real data. cortical microcircuits USED-FOR real data. cortical microcircuits USED-FOR model. OtherScientificTerm are response lag, phase space, and timescale. Method is Latent Equilibrium "" framework. ","This paper proposes a biologically motivated mechanism to train a network of slow-responding neurons for computation. The neural dynamics are modeled as an energy function in the latent state space, which can be used to reduce the response lag. The authors propose a ""Latent Equilibrium"" framework, which is based on the fact that the phase space of the neurons can be represented as a timescale. The model is evaluated on synthetic data and on real data with cortical microcircuits, and the model is shown to be effective for fast computation and learning."
3074,SP:395dae632dab83f3f61bdf67eabe4d351492798c,"elements PART-OF neural network hierarchy. inference CONJUNCTION learning. learning CONJUNCTION inference. "" learning "" phase USED-FOR weight update. relaxation "" phase CONJUNCTION "" learning "" phase. "" learning "" phase CONJUNCTION relaxation "" phase. relaxation "" phase USED-FOR computation. relaxation USED-FOR weight update. continuous - time leaky neuronal dynamics CONJUNCTION continuous active local plasticity. continuous active local plasticity CONJUNCTION continuous - time leaky neuronal dynamics. error backpropagation USED-FOR deep cortical networks. method COMPARE BP. BP COMPARE method. MINST EVALUATE-FOR method. OtherScientificTerm are physical and biological elements, time lag, teaching signals, weight update rate, membrane potential, latent equilibrium space, relaxation state, and recurrent computations. Generic are solutions, and model. Method is prospective energy function. ","This paper studies the role of different elements in the neural network hierarchy: the physical and biological elements. In particular, the authors propose two solutions: (1) a ""relaxation"" phase to reduce computation and (2) an ""learning"" phase that alternates between inference and learning to reduce the time lag between the weight update and the teaching signals. The authors propose to use error backpropagation to train deep cortical networks with continuous-time leaky neuronal dynamics and continuous active local plasticity, where the membrane potential is modeled as a latent equilibrium space, and the weights of the model are sampled from the relaxation state. The proposed method is evaluated on MINST, and compared to BP."
3075,SP:395dae632dab83f3f61bdf67eabe4d351492798c,"plasticity rule USED-FOR learning. plasticity rule CONJUNCTION backprop. backprop CONJUNCTION plasticity rule. plasticity rule USED-FOR neuron model. slow ) neural dynamics FEATURE-OF neuron model. model USED-FOR networks. tasks EVALUATE-FOR networks. OtherScientificTerm are physical substrates, and slow dynamics. Generic is network. ",This paper proposes a new neuron model with (slow) neural dynamics based on a combination of a plasticity rule and backprop to speed up learning. The model is shown to improve the performance of existing networks on a variety of tasks with physical substrates. The authors also provide a theoretical analysis of the slow dynamics of the network.
3076,SP:395dae632dab83f3f61bdf67eabe4d351492798c,"framework USED-FOR quasi - instantaneous inference. Latent Equilibrium USED-FOR quasi - instantaneous inference. benchmark datasets EVALUATE-FOR prediction quality. prediction quality EVALUATE-FOR models. framework USED-FOR models. benchmark datasets EVALUATE-FOR models. framework USED-FOR models of cortical networks. OtherScientificTerm are network depth, slow neuronal elements, back - propagation, and heterogeneous substrates. Task is learning without phased plasticity. ","This paper proposes a framework called Latent Equilibrium for quasi-instantaneous inference. The framework is applied to models of cortical networks and is shown to improve the prediction quality on several benchmark datasets. The authors argue that this is due to network depth, slow neuronal elements, back-propagation, and heterogeneous substrates. The paper also discusses the problem of learning without phased plasticity."
3077,SP:395dae632dab83f3f61bdf67eabe4d351492798c,framework USED-FOR inference and learning in networks of slow components. leaky integrator dynamics USED-FOR disentangled neuron and synapse dynamics. leaky integrator dynamics USED-FOR biological neurons. constant - energy manifold FEATURE-OF neuronal dynamics. Latent Equilibrium HYPONYM-OF constant - energy manifold. robustness EVALUATE-FOR LE. MNIST classification task EVALUATE-FOR robustness. MNIST classification task EVALUATE-FOR LE. OtherScientificTerm is relaxation. ,"This paper proposes a framework for inference and learning in networks of slow components. The main idea is to use leaky integrator dynamics to model disentangled neuron and synapse dynamics in biological neurons. This relaxation is motivated by the observation that neuronal dynamics lie on a constant-energy manifold, called Latent Equilibrium. Experiments on the MNIST classification task demonstrate the effectiveness of LE in terms of robustness."
3078,SP:b937901e3230b14e36975fbab0658a52bdac4977,"architecture USED-FOR message passing GNNs. message passing GNN USED-FOR subgraph. message passing GNN USED-FOR node attributes. based GNN HYPONYM-OF message passing GNN. model COMPARE MPNNs. MPNNs COMPARE model. architecture USED-FOR regular graphs. NGNNs USED-FOR regular graphs. Method are GNNs, GNN, and nested GNNs ( NGNNs ). OtherScientificTerm are unidentifability of nodes, and random regular graph. Material is graphs. ","This paper proposes a new architecture for message passing GNNs, which is a generalization of the existing architecture for regular graphs. Specifically, the authors propose a new message based GNN, called a message-passing GNN (based GNN), which takes as input a GNN and outputs a subgraph of the original GNN. The node attributes of the subgraph are encoded as the output of the message passed by the GNN with respect to the node attributes. The authors show that the proposed model outperforms MPNNs in terms of the unidentifability of nodes in the original graph, and in graphs with a random regular graph. They also show that their architecture can be applied to regular graphs with NGNNs (NGNNs)."
3079,SP:b937901e3230b14e36975fbab0658a52bdac4977,"Nested Graph Neural Networks ( NGNNs ) HYPONYM-OF graph neural network. nested subgraph COMPARE nested subtree. nested subtree COMPARE nested subgraph. model COMPARE 1 - WL. 1 - WL COMPARE model. representations USED-FOR node. GNN USED-FOR nested subgraph. GNN USED-FOR representation. global pooling USED-FOR graph - level embeddings. it COMPARE higher - order GNN variants. higher - order GNN variants COMPARE it. NGNN COMPARE higher - order GNN variants. higher - order GNN variants COMPARE NGNN. NGNN COMPARE 1 - WL expressivity. 1 - WL expressivity COMPARE NGNN. message - passing GNNs USED-FOR NGNN. message - passing GNNs USED-FOR 1 - WL expressivity. NGNN USED-FOR n - sized r - regular graphs. ogb - molhiv CONJUNCTION ogb - molpcba. ogb - molpcba CONJUNCTION ogb - molhiv. QM9 CONJUNCTION ogb - molhiv. ogb - molhiv CONJUNCTION QM9. ogb - molpcba CONJUNCTION TU datasets. TU datasets CONJUNCTION ogb - molpcba. TU datasets HYPONYM-OF graph classification and regression. graph classification and regression EVALUATE-FOR NGNN. QM9 HYPONYM-OF graph classification and regression. ogb - molhiv HYPONYM-OF graph classification and regression. ogb - molpcba HYPONYM-OF graph classification and regression. ogb - molpcba CONJUNCTION QM9 subtasks. QM9 subtasks CONJUNCTION ogb - molpcba. QM9 subtasks EVALUATE-FOR method. ogb - molpcba EVALUATE-FOR method. Method are message passing approaches, and node representations. Metric is computational complexity. OtherScientificTerm are subgraph, and graph. ","This paper proposes a new graph neural network, called Nested Graph Neural Networks (NGNNs). Unlike previous message passing approaches, the authors propose to learn representations for each node separately, which reduces the computational complexity. Each node is represented by a representation learned by a GNN, which is similar to a nested subgraph but different from a nested subtree. The node representations are shared across all nodes in the graph, and global pooling is used to learn graph-level embeddings. The authors evaluate the proposed NGNN on several graph classification and regression tasks, including QM9, ogb-molhiv, and ogb - molpcba and TU datasets, and show that it outperforms higher-order GNN variants and 1-WL expressivity with message-passing GNNs on n-sized r-regular graphs. The proposed method is evaluated on OGB-molpcba, Ogb-molhiv and QM09 subtasks."
3080,SP:b937901e3230b14e36975fbab0658a52bdac4977,global pooling layer USED-FOR graph - level embedding. GNN USED-FOR node representations. GNN USED-FOR subgraphs. GNN USED-FOR framework. framework USED-FOR GNNs. it COMPARE real - world datasets. real - world datasets COMPARE it. framework USED-FOR k - regular graphs. Method is message passing based neural network. ,"This paper proposes a message passing based neural network. The proposed framework uses a GNN to learn node representations and subgraphs, and a global pooling layer to learn graph-level embedding. The experimental results show that the proposed framework can learn k-regular graphs, and that it outperforms real-world datasets."
3081,SP:b937901e3230b14e36975fbab0658a52bdac4977,subgraph COMPARE subtree. subtree COMPARE subgraph. subgraph USED-FOR Nested GNN. subgraph COMPARE subtree. subtree COMPARE subgraph. NGNN CONJUNCTION GNN. GNN CONJUNCTION NGNN. whole - graph representation USED-FOR tasks. whole - graph representation CONJUNCTION node representation. node representation CONJUNCTION whole - graph representation. graph classification HYPONYM-OF tasks. Generic is method. ,"This paper proposes Nested GNN, which uses a subgraph instead of a subtree. The authors argue that the whole-graph representation and the node representation can be used for different tasks such as graph classification. The proposed method can be combined with NGNN or GNN."
3082,SP:b937901e3230b14e36975fbab0658a52bdac4977,limited expressive power FEATURE-OF GNNs. rooted subtrees USED-FOR GNNs. rooted subgraphs USED-FOR nodes. expressive power FEATURE-OF graph isomorphism testing. method USED-FOR GNNs. GNNs USED-FOR representation power. method USED-FOR representation power. GNNs USED-FOR graph classification and regression tasks. OtherScientificTerm is 2 - WL. ,"This paper studies the expressive power of GNNs with limited expressive power in graph isomorphism testing. The authors propose a method to increase the representation power of the GNN with the help of rooted subtrees, where nodes are represented as rooted subgraphs. The method is applied to graph classification and regression tasks, and is shown to improve the performance by a significant margin over 2-WL."
3083,SP:7b8284aa82022ce73802bfc57238b0d82031b226,"framework USED-FOR learning and inference. probabilistic models with structured latent spaces USED-FOR learning and inference. variational bound USED-FOR importance sampler proposals. importance sampling USED-FOR variational bound. IWAE CONJUNCTION VSMC - like methods. VSMC - like methods CONJUNCTION IWAE. It USED-FOR VSMC - like methods. It USED-FOR IWAE. importance sampling USED-FOR VSMC - like methods. methods USED-FOR global bound. local pairwise divergences FEATURE-OF target distributions. methods COMPARE NVI. NVI COMPARE methods. local pairwise divergences USED-FOR NVI. OtherScientificTerm are structured latent spaces, and gradient variance. Method is stochastic optimization. Task is parallel inference. ","This paper proposes a framework for learning and inference in probabilistic models with structured latent spaces. The main contribution is to derive a variational bound for importance sampler proposals based on importance sampling. It is applied to IWAE and VSMC-like methods. The authors show that the proposed methods converge to a global bound with respect to the gradient variance, which is in contrast to NVI, which relies on local pairwise divergences between the target distributions. They also show that stochastic optimization can be applied to parallel inference."
3084,SP:7b8284aa82022ce73802bfc57238b0d82031b226,"nested importance sampling CONJUNCTION variational inference. variational inference CONJUNCTION nested importance sampling. nested importance sampling PART-OF nested variational inference ( NVI ) framework. variational inference PART-OF nested variational inference ( NVI ) framework. nesting FEATURE-OF KL divergences. heuristics USED-FOR state - space model. amortized inference USED-FOR hierarchical deep generative model. annealing path USED-FOR multimodal distribution. Generic is framework. OtherScientificTerm is intermediate densities. Method are importance sampler, and NVI. ","This paper proposes a nested variational inference (NVI) framework that combines nested importance sampling with variational inferential inference. The framework is motivated by the observation that the nesting of KL divergences can lead to different intermediate densities. To address this issue, the authors propose to use heuristics to learn a state-space model. The main contribution of the paper is the use of amortized inference to train a hierarchical deep generative model. In particular, the importance sampler is used to train the NVI. The authors also propose an annealing path to approximate a multimodal distribution."
3085,SP:7b8284aa82022ce73802bfc57238b0d82031b226,local objectives USED-FOR forward and backward edge marginals. local objectives USED-FOR forward and reverse proposal distributions. variational and nested inference USED-FOR inference procedure. resampling USED-FOR sample diversity. Method is nested inference. ,"This paper proposes to use local objectives to estimate the forward and backward edge marginals of forward and reverse proposal distributions. The inference procedure is based on variational and nested inference, where nested inference is used to sample from the posterior. The authors also propose resampling to increase sample diversity."
3086,SP:7b8284aa82022ce73802bfc57238b0d82031b226,SMC CONJUNCTION annealed importance sampling. annealed importance sampling CONJUNCTION SMC. method USED-FOR intermediate densities. Generic is objective. Task is lower - variance gradient estimates. ,"This paper proposes a new objective for estimating the intermediate densities of the data points. The objective is motivated by the problem of lower-variance gradient estimates. The authors propose to combine SMC with annealed importance sampling. The method is shown to be able to estimate the intermediate density of the training data points, which is an important contribution to the community."
3087,SP:7b8284aa82022ce73802bfc57238b0d82031b226,"f - divergence variational inference CONJUNCTION nested importance sampling. nested importance sampling CONJUNCTION f - divergence variational inference. f - divergence variational inference PART-OF inference and approximate weighted sampling framework. nested importance sampling PART-OF inference and approximate weighted sampling framework. weight preserving operators USED-FOR approximation of an unnormalized target distribution. weight preserving operators USED-FOR proposal. weight preserving operators USED-FOR variation sampling density. nesting level FEATURE-OF f - divergence. f - divergence USED-FOR transformations. OtherScientificTerm are unnormalized target distribution, and sampling distribution. Method is reverse transformations. ","This paper proposes an inference and approximate weighted sampling framework that combines f-divergence variational inference with nested importance sampling. The proposal uses weight preserving operators to approximate the variation sampling density of an unnormalized target distribution. The transformations are based on f-derivergence at the nesting level, where the sampling distribution is approximated with reverse transformations."
3088,SP:f3792f82b28727a7a198c6eac9511391d2045a5f,"algorithm HYPONYM-OF certified algorithms. instance optimal bound USED-FOR Lipschitz continuous    functions. zero - order queries CONJUNCTION certificate of optimality. certificate of optimality CONJUNCTION zero - order queries. unknown    Lipschitz constant FEATURE-OF Lipschitz continuous function. algorithm CONJUNCTION lower bound. lower bound CONJUNCTION algorithm. OtherScientificTerm are global solutions, possibly non - convex    constraints, global optimum, dimension dependent constants, Lipschitz constant, and upper bounded. ","This paper studies the instance optimal bound for Lipschitz continuous   functions, which are certified algorithms with global solutions and possibly non-convex   constraints. In particular, the authors consider the case of zero-order queries and certificate of optimality, where the global optimum is a function of dimension dependent constants. The authors show that the upper bounded of this upper bounded depends on the dimension of the function and on the unknown   Lipsichitz constant of the   original   function. The algorithm and the lower bound are then compared."
3089,SP:f3792f82b28727a7a198c6eac9511391d2045a5f,"mild geometric assumptions CONJUNCTION noise - free zeroth - order oracle. noise - free zeroth - order oracle CONJUNCTION mild geometric assumptions. sample complexity EVALUATE-FOR \varepsilon - optimal point. certified DOO algorithm USED-FOR error certification. sample complexity EVALUATE-FOR sum of packing numbers. packing number FEATURE-OF shell. Task are zeroth - order Lipschitz optimization, and certification. OtherScientificTerm are mild geometric assumption, upper bound, and instance - dependent lower bound. Generic is algorithm. ","This paper studies zeroth-order Lipschitz optimization under mild geometric assumptions and a noise-free zerth-order oracle. Under the mild geometric assumption, the authors show that the sample complexity of the \varepsilon-optimal point of the certified DOO algorithm for error certification can be reduced to the sum of packing numbers. The authors also provide an instance-dependent lower bound that shows that if the packing number of the shell is small enough, then the algorithm will converge to a point where the upper bound can be obtained. "
3090,SP:f3792f82b28727a7a198c6eac9511391d2045a5f,"complexity EVALUATE-FOR certifiable zeroth - order optimization of Lipschitz functions. optimal oracle complexity EVALUATE-FOR problem. OtherScientificTerm are oracle access, error parameter, and oracle queries. ",This paper studies the complexity of certifiable zeroth-order optimization of Lipschitz functions. The authors show that the optimal oracle complexity of this problem is O(1/\sqrt{T}) where T is the number of oracle access and T is an error parameter. They also provide a theoretical analysis of the oracle queries.
3091,SP:f3792f82b28727a7a198c6eac9511391d2045a5f,Certified DOO algorithm USED-FOR Lipschitz function. Certified DOO algorithm HYPONYM-OF zero - order method. compact feasible set FEATURE-OF Lipschitz function. convergence rate CONJUNCTION instance - dependent lower bound. instance - dependent lower bound CONJUNCTION convergence rate. instance - dependent lower bound USED-FOR certified algorithms. convergence rate EVALUATE-FOR Certified DOO algorithm. Certified DOO algorithm CONJUNCTION instance - dependent lower bound. instance - dependent lower bound CONJUNCTION Certified DOO algorithm. lower bound COMPARE upper bound. upper bound COMPARE lower bound. OtherScientificTerm is logarithm term. ,"This paper proposes a new zero-order method called Certified DOO algorithm for computing the Lipschitz function in a compact feasible set. The authors prove the convergence rate and instance-dependent lower bound for certified algorithms. The lower bound is tighter than the upper bound of [1], and the logarithm term is also tighter."
3092,SP:f3792f82b28727a7a198c6eac9511391d2045a5f,"1 - d FEATURE-OF optimal sample complexity. sample complexity bound FEATURE-OF algorithm. sample complexity EVALUATE-FOR DOO algorithm. lower bound USED-FOR certifiable algorithms. lower bound COMPARE upper bound. upper bound COMPARE lower bound. Task is certifiable zeroth - order Lipschitz function optimization. OtherScientificTerm are error scales, sample complexity upper bound, and Lipschitz constant. Method is c. DOO algorithm. ","This paper studies the problem of certifiable zeroth-order Lipschitz function optimization. In particular, the authors study the sample complexity bound of the algorithm. The authors show that the optimal sample complexity is 1-d, where d is the error scales and c is the number of iterations. The sample complexity of the DOO algorithm is also shown to be optimal. The lower bound for certifiable algorithms is a lower bound of c, which is the same as the upper bound. The main contribution of this paper is to provide a sample complexity upper bound that does not depend on c. The upper bound is an upper bound of d, which can be obtained by computing the Lipschnitz constant."
3093,SP:6e8134eeaf524db765a6186f3de74e936243f8d4,method USED-FOR adversarial examples. accuracy EVALUATE-FOR classification system. attack USED-FOR system. approach COMPARE label adversarial attack. label adversarial attack COMPARE approach. MC dropout CONJUNCTION SelectiveNet. SelectiveNet CONJUNCTION MC dropout. ensembles CONJUNCTION MC dropout. MC dropout CONJUNCTION ensembles. uncertainty - aware models HYPONYM-OF neural network models. SelectiveNet HYPONYM-OF uncertainty - aware models. MC dropout HYPONYM-OF uncertainty - aware models. ensembles HYPONYM-OF uncertainty - aware models. OtherScientificTerm is gradient. ,"This paper proposes a method to generate adversarial examples that can be used to improve the accuracy of a classification system. The proposed approach is similar to the label adversarial attack, except that the attack can be applied to the entire system, rather than just the gradient. The authors demonstrate the effectiveness of the proposed method on several neural network models, including uncertainty-aware models such as ensembles, MC dropout, and SelectiveNet."
3094,SP:6e8134eeaf524db765a6186f3de74e936243f8d4,"accuracy EVALUATE-FOR network. SelectiveNets CONJUNCTION Ensemble methods. Ensemble methods CONJUNCTION SelectiveNets. MC - dropout CONJUNCTION SelectiveNets. SelectiveNets CONJUNCTION MC - dropout. white - box and black - box attacks USED-FOR uncertainty estimation methods. Ensemble methods HYPONYM-OF uncertainty estimation methods. MC - dropout HYPONYM-OF uncertainty estimation methods. SelectiveNets HYPONYM-OF uncertainty estimation methods. Task are adversarial attacks, and attacking networks. Generic are attacks, networks, and method. ","This paper studies the problem of adversarial attacks, where the goal is to minimize the accuracy of the network. The authors propose to use white-box and black-box attacks to attack uncertainty estimation methods such as MC-dropout, SelectiveNets, and Ensemble methods. The proposed attacks are shown to be effective in attacking networks that are not trained to be robust to the proposed method."
3095,SP:6e8134eeaf524db765a6186f3de74e936243f8d4,white - box and black - box regime FEATURE-OF Attacks. minimal perturbations USED-FOR uncertainty estimation methods. Generic is attack. Method is DNN. Metric is accuracy. OtherScientificTerm is black - box regime. ,"This paper proposes a novel attack on the DNN. Attacks are presented in both the white-box and black-box regime. The authors show that under minimal perturbations, uncertainty estimation methods can be trained to be robust against the proposed attack. They also show that the accuracy of the attack can be improved by a factor of at least 1.5. The paper is well-written and easy to follow. The main contribution of the paper is the theoretical analysis of the black -box regime, which is very interesting."
3096,SP:6e8134eeaf524db765a6186f3de74e936243f8d4,"network USED-FOR uncertainty estimation. adversarial attacks COMPARE attack technique. attack technique COMPARE adversarial attacks. attack technique USED-FOR network. Method are uncertainty estimation attack technique, and DNN. OtherScientificTerm is network ’s capacity. Generic is attack. Metric are accuracy, and model ’s accuracy. ","This paper proposes an uncertainty estimation attack technique that aims to reduce the network’s capacity to perform uncertainty estimation. Compared to adversarial attacks, the proposed attack technique is able to attack a network that is trained for uncertainty estimation, while maintaining the accuracy of the original DNN. The authors show that their attack is effective in reducing the network's capacity to estimate the uncertainty of a given input. They also show that the attack can be applied to any model that is not trained for accuracy."
3097,SP:6e8134eeaf524db765a6186f3de74e936243f8d4,neural network USED-FOR uncertainty estimation. greedy heuristic USED-FOR FGSM. AURC CONJUNCTION NLL. NLL CONJUNCTION AURC. NLL CONJUNCTION Brier Score. Brier Score CONJUNCTION NLL. uncertainty estimation metrics EVALUATE-FOR neural networks. neural networks USED-FOR uncertainty. Brier Score HYPONYM-OF uncertainty estimation metrics. AURC HYPONYM-OF uncertainty estimation metrics. NLL HYPONYM-OF uncertainty estimation metrics. uncertainty estimation metrics EVALUATE-FOR method. softmax - based or MC dropout HYPONYM-OF neural networks. Metric is accuracy. ,"This paper proposes to use a neural network for uncertainty estimation. The main idea is to use FGSM with a greedy heuristic. The method is evaluated on several uncertainty estimation metrics, including AURC, NLL, and Brier Score. The authors show that neural networks trained with softmax-based or MC dropout are able to estimate uncertainty with high accuracy."
3098,SP:c5a5bf6e0bdebf5170c8fe3fedd2f3438e39cd21,streaming Stochastic Block Model ( SBM ) USED-FOR null model. null model USED-FOR streaming community detection algorithms. streaming Stochastic Block Model ( SBM ) USED-FOR streaming community detection algorithms. growing graphs USED-FOR streaming community detection algorithms. online algorithm USED-FOR community membership. Belief Propagation ( BP ) USED-FOR online algorithm. It COMPARE offline variant BP. offline variant BP COMPARE It. accuracy EVALUATE-FOR offline variant BP. accuracy EVALUATE-FOR It. ,This paper proposes to use streaming Stochastic Block Model (SBM) as a null model to train streaming community detection algorithms on growing graphs. The online algorithm based on Belief Propagation (BP) is used to estimate community membership. It outperforms the offline variant BP in terms of accuracy.
3099,SP:c5a5bf6e0bdebf5170c8fe3fedd2f3438e39cd21,"vertices PART-OF graph. stochastic block model USED-FOR graph. algorithm USED-FOR community detection. StreamBP ) USED-FOR community detection. algorithm COMPARE belief propagation algorithm ( OfflineBP ). belief propagation algorithm ( OfflineBP ) COMPARE algorithm. graph EVALUATE-FOR belief propagation algorithm ( OfflineBP ). theoretical guarantees EVALUATE-FOR algorithm. StreamBP and OfflineBP algorithms CONJUNCTION alternatives. alternatives CONJUNCTION StreamBP and OfflineBP algorithms. voting algorithms HYPONYM-OF alternatives. OtherScientificTerm are random order, and side information. Method are $ R$-local algorithms, and StreamBP ( StreamBP * ). Generic is algorithms. Material are synthetic and real - world datasets, and real - world datasets. ","This paper proposes a new algorithm (StreamBP) for community detection, which is based on the observation that the vertices of a graph can be represented as a stochastic block model, and that the random order of the blocks can be interpreted as a function of $R$-local algorithms. The authors prove theoretical guarantees for the proposed algorithm and compare their algorithm with a belief propagation algorithm (OfflineBP) on a graph, and show that the proposed algorithms converge faster. The paper also compares the performance of the proposed StreamBP and OfflineBP algorithms with alternatives such as voting algorithms, and shows that both algorithms are competitive on both synthetic and real-world datasets. Finally, the authors provide some theoretical analysis of the side information in their algorithm, and provide some experimental results on real-life datasets."
3100,SP:c5a5bf6e0bdebf5170c8fe3fedd2f3438e39cd21,"stochastic block model USED-FOR community detection. local streaming algorithm COMPARE local algorithm. local algorithm COMPARE local streaming algorithm. belief propagation algorithm COMPARE offline BP. offline BP COMPARE belief propagation algorithm. accuracy EVALUATE-FOR offline BP. accuracy EVALUATE-FOR belief propagation algorithm. algorithm COMPARE StreamBP. StreamBP COMPARE algorithm. OtherScientificTerm are nodes, noise, and side information. Method is Offline BP. ","This paper proposes a stochastic block model for community detection, where nodes are connected to each other via a local streaming algorithm. The local algorithm is shown to outperform the local algorithm in terms of accuracy. Offline BP is also shown to improve accuracy by a large margin, especially when noise is added to the side information. The proposed algorithm is also compared to StreamBP."
3101,SP:c5a5bf6e0bdebf5170c8fe3fedd2f3438e39cd21,stochastic block models USED-FOR recovering the communities. bounded radius FEATURE-OF information. information USED-FOR recovering the communities. belief propagation COMPARE offline BP algorithm. offline BP algorithm COMPARE belief propagation. reconstruction accuracy EVALUATE-FOR offline BP algorithm. side information USED-FOR recovery. side information USED-FOR belief propagation. side information USED-FOR offline BP algorithm. reconstruction accuracy EVALUATE-FOR belief propagation. synthetic and real networks EVALUATE-FOR algorithms. OtherScientificTerm is communities. ,This paper studies the problem of recovering the communities from stochastic block models using information with bounded radius. The authors show that belief propagation with side information improves the reconstruction accuracy compared to the offline BP algorithm with no side information for recovery. The proposed algorithms are evaluated on both synthetic and real networks.
3102,SP:c5a5bf6e0bdebf5170c8fe3fedd2f3438e39cd21,"streaming stochastic block model ( StSBM ) HYPONYM-OF stochastic block model. asymptotic behavior FEATURE-OF model. R - local streaming algorithm COMPARE random guessing. random guessing COMPARE R - local streaming algorithm. streaming algorithm COMPARE offline version algorithm. offline version algorithm COMPARE streaming algorithm. OtherScientificTerm are sparse regime, and side information. Material is synthetic and real - world datasets. ","This paper proposes a streaming stochastic block model (StSBM), which is a variant of the standard, non-parametric, and non-convex (or non-Gaussian) streaming algorithm. The authors prove the asymptotic behavior of the model in the sparse regime, and show that the R-local streaming algorithm outperforms random guessing when the side information is sparse. They also show that their streaming algorithm is equivalent to the offline version algorithm, and provide experimental results on both synthetic and real-world datasets."
3103,SP:b1163857a6b06047c3531ab762642fcbed6dd294,"regularization USED-FOR linear mapping. parameter space FEATURE-OF l2 regularization. diagonal networks CONJUNCTION convolutional networks. convolutional networks CONJUNCTION diagonal networks. fully connected deep networks CONJUNCTION diagonal networks. diagonal networks CONJUNCTION fully connected deep networks. convolutional networks CONJUNCTION residual nets. residual nets CONJUNCTION convolutional networks. fully connected deep networks HYPONYM-OF architectures. residual nets HYPONYM-OF architectures. convolutional networks HYPONYM-OF architectures. diagonal networks HYPONYM-OF architectures. architectures USED-FOR regularizer. structural requirements FEATURE-OF network. \ell_p norm USED-FOR induced regularizer. architectures USED-FOR \ell_{p, q } group norms. Method are linear neural nets, and linear net. OtherScientificTerm are induced regularization, and \ell_p norms. ","This paper studies the regularization of linear neural nets. The authors show that the l2 regularization in the parameter space can be viewed as an induced regularization, which is a linear mapping between the input and output of the linear net. This regularizer can be applied to a variety of architectures, including fully connected deep networks, diagonal networks, convolutional networks, and residual nets. In particular, they show that \ell_{p, q} group norms can be induced using different architectures, and that the \ell_p norm can be used as the induced regularizer. They also show that when the structural requirements of the network are different, \ell-p norms can also be induced."
3104,SP:b1163857a6b06047c3531ab762642fcbed6dd294,representation cost EVALUATE-FOR linear neural networks. predictor space FEATURE-OF \ell_2 $ regularization. fully connected network USED-FOR trace norm. fully connected network CONJUNCTION diagonal network. diagonal network CONJUNCTION fully connected network. diagonal network CONJUNCTION convolutional network. convolutional network CONJUNCTION diagonal network. convolutional network USED-FOR discrete Fourier transform. linear neural network architecture USED-FOR regularization effects. ,"This paper studies the representation cost of linear neural networks. The authors propose a \ell_2$ regularization in the predictor space. The regularization effects are modeled with a linear neural network architecture, where a fully connected network is used to estimate the trace norm, a diagonal network is applied to compute the discrete Fourier transform, and a convolutional network is employed to reconstruct the input."
3105,SP:b1163857a6b06047c3531ab762642fcbed6dd294,diagonal networks CONJUNCTION convolutional networks. convolutional networks CONJUNCTION diagonal networks. fully - connected networks CONJUNCTION diagonal networks. diagonal networks CONJUNCTION fully - connected networks. convolutional networks CONJUNCTION residual networks. residual networks CONJUNCTION convolutional networks. complexity measure EVALUATE-FOR diagonal networks. fully - connected networks USED-FOR complexity measure. full or limited filter width FEATURE-OF convolutional networks. architectures USED-FOR $ k$-support norms. Method is linear neural network. OtherScientificTerm is $ l_2 $ regularization. Metric is complexity measures. ,"This paper studies the complexity of a linear neural network. The authors propose a complexity measure for fully-connected networks, diagonal networks, convolutional networks with full or limited filter width, and residual networks. They show that these architectures converge to $\epsilon$-support norms with $l_2$ regularization. They also show that their complexity measures are tight."
3106,SP:b1163857a6b06047c3531ab762642fcbed6dd294,penalty USED-FOR functional space. l^2 regularization USED-FOR parameters. architecture USED-FOR representation cost. representation cost USED-FOR bias. Method is linear networks. ,"This paper proposes a penalty for bias in linear networks that penalizes the functional space. The main idea is to use an l^2 regularization of the parameters, where the parameters are learned by minimizing the variance of the weights of linear networks. The authors also propose an architecture to reduce the representation cost to reduce bias."
3107,SP:b1163857a6b06047c3531ab762642fcbed6dd294,"representation costs FEATURE-OF linear networks. induced complexity measures EVALUATE-FOR representation costs. diagonal nets CONJUNCTION convolutional nets. convolutional nets CONJUNCTION diagonal nets. convolutional nets CONJUNCTION residual nets. residual nets CONJUNCTION convolutional nets. fullly - connected nets CONJUNCTION diagonal nets. diagonal nets CONJUNCTION fullly - connected nets. representation cost CONJUNCTION dual norm. dual norm CONJUNCTION representation cost. representation costs PART-OF architecture. fullly - connected nets HYPONYM-OF representation costs. diagonal nets HYPONYM-OF representation costs. residual nets HYPONYM-OF architecture. convolutional nets HYPONYM-OF architecture. fullly - connected nets HYPONYM-OF architecture. diagonal nets HYPONYM-OF architecture. representation cost USED-FOR neural network. representation cost USED-FOR complexity measures. group $ l_{p, q}$ norms CONJUNCTION $ k$-support norms. $ k$-support norms CONJUNCTION group $ l_{p, q}$ norms. architectures USED-FOR group $ l_{p, q}$ norms. architectures USED-FOR $ k$-support norms. representation cost EVALUATE-FOR homogeneous parametrization. Method are shallow networks, elastic nets, and neural network representation cost. Generic is network. ","This paper studies the representation costs of linear networks with different representation costs. The authors consider the induced complexity measures of these representation costs, which are defined as the sum of the representation cost and the dual norm of a given architecture (fullly-connected nets, diagonal nets, convolutional nets, residual nets, etc.). The authors show that shallow networks have similar representation costs as elastic nets, and that the neural network representation cost can be viewed as a function of the number of layers of the network. They also show that the complexity measures can be interpreted as a representation cost of a neural network. Finally, they show that for homogeneous parametrization, the proposed representation cost is the same for group $l_{p, q}$ norms, $k$-support norms, and $k$. "
3108,SP:c9c7fc5288e24a54531b7063c028d307279fe2ef,approach USED-FOR knowledge base completion. ,This paper proposes an approach for knowledge base completion. The idea is interesting and well motivated. The paper is well-written and well-motivated. 
3109,SP:c9c7fc5288e24a54531b7063c028d307279fe2ef,"incomplete knowledge bases USED-FOR non - parametric reasoning method. Method is multi - hop reasoning. OtherScientificTerm are reasoning paths, and outgoing relation. Material is knowledge base. ","This paper proposes a non-parametric reasoning method based on incomplete knowledge bases. The idea is to use multi-hop reasoning, where the reasoning paths are sampled from a knowledge base, and the outgoing relation between the two paths is learned."
3110,SP:f63e4ed39d577b50eab4f4b6d08ef912a69840ef,pre - training strategies USED-FOR entity linking. AIDA and TAC - KBP baselines USED-FOR pre - training strategies. noise USED-FOR pre - training. noise USED-FOR entity candidate selection strategies. model USED-FOR entity disambiguation. 4 - layer transformer USED-FOR language representation. MLP final layer USED-FOR disambiguation. 4 - layer transformer PART-OF model. MLP final layer PART-OF 4 - layer transformer. transformer architecture USED-FOR entity linking. methods USED-FOR entity linking. Method is context selection methods. ,"This paper proposes pre-training strategies for entity linking based on AIDA and TAC-KBP baselines. The key idea is to use noise during pre-train to improve the entity candidate selection strategies. The authors also propose a model for entity disambiguation that consists of a 4-layer transformer for language representation and an MLP final layer for disambigenation. The transformer architecture is well-suited to entity linking, and the proposed methods can be used to improve entity linking performance. The paper also provides a thorough analysis of the context selection methods."
3111,SP:f63e4ed39d577b50eab4f4b6d08ef912a69840ef,MLP USED-FOR embedding. architecture USED-FOR entity disambiguation. fenceposts USED-FOR MLP. transformer USED-FOR entity resolution. Wikipedia USED-FOR entity resolution. Wikipedia USED-FOR transformer. transformer USED-FOR it. Wikipedia USED-FOR model. CoNLL CONJUNCTION TAC - KBP. TAC - KBP CONJUNCTION CoNLL. OtherScientificTerm is BERT. Metric is dot product. Method is BERT model. Generic is hyperparameters. ,This paper proposes a new architecture for entity disambiguation based on fenceposts. The model is trained on Wikipedia using a transformer that takes as input the entity resolution from Wikipedia and outputs an embedding using an MLP. The authors compare the performance of CoNLL and TAC-KBP with BERT and show that the dot product of the embedding and embedding of the entity is better than the BERT model. The hyperparameters are also compared.
3112,SP:eaeee88e0717cda8d6f3d8ff83ebe594eba44f29,"models COMPARE models. models COMPARE models. active learning USED-FOR uncertainty estimation methods. initialization schemes USED-FOR subset model. mutual information USED-FOR acquisition function. mutual information USED-FOR initialization schemes. build - up approach USED-FOR acquisition functions. Top-1 accuracy EVALUATE-FOR ensemble models. ImageNet EVALUATE-FOR ensemble models. ImageNet EVALUATE-FOR Top-1 accuracy. ensembling schemes USED-FOR ensemble models. ImageNet data USED-FOR subset model. ensembling scheme USED-FOR acquisition models. ensembles USED-FOR acquisition models. Task is Monte Carlo estimation of model uncertainty. Method are build - up "" approach, subset models, and acquisition model. Generic are that, method, and model. OtherScientificTerm are variation ratio, and random seeds. ","This paper studies Monte Carlo estimation of model uncertainty in the context of ensemble models. The authors propose a ""build-up"" approach to this problem, which is similar to that of active learning in uncertainty estimation methods. The key difference is that instead of using random seeds, the authors propose to use subset models, where the subset model is trained using different initialization schemes based on mutual information between the training data and the acquisition function. This build-up approach allows the authors to train acquisition functions that are more robust to variations in the variation ratio of the model. Experiments on Top-1 accuracy on ImageNet show that models trained with ensembling schemes achieve better performance than models trained without ensembles. The main contribution of this paper is to propose a method for learning a subset model from ImageNet data that is robust to variation in the acquisition model."
3113,SP:eaeee88e0717cda8d6f3d8ff83ebe594eba44f29,"Generic are method, baselines, and approach. Metric is accuracy. Material are CIFAR100, and ImageNet. Task is ICLR. ",This paper proposes a method to improve the performance of ICLR. The proposed method is based on the observation that the accuracy of CIFAR100 can be improved when the number of samples is increased. The authors compare their approach to baselines and show that their approach can improve performance on ImageNet. They also show that they can improve the accuracy on CifAR100 by a large margin. 
3114,SP:4a1cce61f12c68846c507130bd055b3444ac8101,"concurrent routing COMPARE sequential routing. sequential routing COMPARE concurrent routing. gradient CONJUNCTION layerNorm. layerNorm CONJUNCTION gradient. Cifar10 CONJUNCTION Cifar100. Cifar100 CONJUNCTION Cifar10. Method are dynamic routing, norm normalization, and layerNorm normalization. OtherScientificTerm is squash function. ","This paper proposes dynamic routing, which is a variant of concurrent routing that is different from sequential routing. The main idea is to replace the gradient and the layerNorm with a new norm normalization. This is done by adding a squash function to the original gradient and a new layerNorm normalization to the gradient. Experiments are conducted on Cifar10 and CifAR100."
3115,SP:4a1cce61f12c68846c507130bd055b3444ac8101,routing mechanism USED-FOR capsule networks. capsule structure CONJUNCTION restnet backbone. restnet backbone CONJUNCTION capsule structure. capsule structure USED-FOR real world data sets. network COMPARE capsule networks. capsule networks COMPARE network. overlapping digits FEATURE-OF augmented MNIST dataset. augmented MNIST dataset EVALUATE-FOR structure. ,This paper proposes a new routing mechanism for capsule networks. The capsule structure and the restnet backbone are well-studied in real world data sets. The proposed structure is evaluated on an augmented MNIST dataset with overlapping digits. The experimental results show that the proposed network is able to achieve comparable performance to the state-of-the-art capsule networks while being more efficient.
3116,SP:99ca283c579152bc44b19c21392aeb7f6b76231b,grid search algorithm USED-FOR hyperparameters. hyperparameters PART-OF deep neural nets training. grid search algorithm USED-FOR deep neural nets training. learning rate CONJUNCTION drop out. drop out CONJUNCTION learning rate. drop out HYPONYM-OF hyperparameters. learning rate HYPONYM-OF hyperparameters. parallel tempering USED-FOR statistical physics. parallel tempering USED-FOR hyperparameters. OtherScientificTerm is noise. Method is grid search. ,"This paper proposes a grid search algorithm to find hyperparameters in deep neural nets training. The hyperparameter considered are the learning rate and drop out. The grid search is based on the observation that the hyperparametrized parameters are more sensitive to noise. The authors propose to use parallel tempering, which has been used in statistical physics, to find the optimal hyperparamets."
3117,SP:99ca283c579152bc44b19c21392aeb7f6b76231b,paradigm USED-FOR hyperparameter search. hyperparameter space CONJUNCTION parameter space. parameter space CONJUNCTION hyperparameter space. way USED-FOR hyperparameter space. parameter optimization USED-FOR hyperparameter search. joint space FEATURE-OF non - local paths. physics USED-FOR parallel tempering. hyperparameter USED-FOR parameter learning. dropout rate CONJUNCTION learning rate. learning rate CONJUNCTION dropout rate. batch size CONJUNCTION dropout rate. dropout rate CONJUNCTION batch size. noise FEATURE-OF training process. inverse temperature FEATURE-OF Langevin diffusion. learning rate HYPONYM-OF hyperparameters. batch size HYPONYM-OF hyperparameters. dropout rate HYPONYM-OF hyperparameters. Task is separation. Method is langevin chain. ,"This paper proposes a new paradigm for hyperparameter search based on parameter optimization. The main idea is to learn a way to partition the space of hyperparameters into a joint space and a parameter space. The joint space is used to represent non-local paths and the parameters of the joint space are used for separation. The parallel tempering is based on physics and the authors propose to use the langevin chain. The Langevin diffusion has inverse temperature. The authors also show that the choice of the hyperparametrieter for parameter learning affects the parameter learning. The paper also shows that the noise in the training process can affect the performance of the parameters (batch size, dropout rate, learning rate)."
3118,SP:beba754d96cc441712a5413c41e98863c8abf605,reinforcement learning methods USED-FOR neural machine translation. MRT USED-FOR expected reward. REINFORCE USED-FOR peakier distribution. realistic or dummy constant rewards USED-FOR REINFORCE. BLEU scores EVALUATE-FOR rewards. REINFORCE CONJUNCTION MRT. MRT CONJUNCTION REINFORCE. Method is on - policy RL approaches. ,"This paper studies reinforcement learning methods for neural machine translation. The authors propose REINFORCE and MRT, which are on-policy RL approaches where the expected reward is modeled as a function of the current state of the art MRT. The main idea is to learn a peakier distribution of the expected rewards using REINFRECE with realistic or dummy constant rewards. The rewards are evaluated using BLEU scores. "
3119,SP:beba754d96cc441712a5413c41e98863c8abf605,"reinforcement learning method USED-FOR neural sequence - to - sequence models. local ( let alone global ) optima FEATURE-OF reward function. reinforcement learning method USED-FOR contrastive minimum risk training ( CMRT ). REINFORCE algorithm COMPARE CMRT. CMRT COMPARE REINFORCE algorithm. REINFORCE algorithm USED-FOR NMT models. Method are NMT, and REINFORCE / CMRT. Generic are baseline model, and model. OtherScientificTerm is REINFORCE. ","This paper proposes a reinforcement learning method for neural sequence-to-sequence models, called contrastive minimum risk training (CMRT), which aims to find local (let alone global) optima for the reward function. The REINFORCE algorithm is similar to the CMRT, but is trained on NMT instead of NMT models. The main difference between REINForCE/CMRR and the baseline model is that the model is trained in a supervised fashion."
3120,SP:366b68d2490ea7569c74dc66ec0f83daa029ddd9,"action value function CONJUNCTION value function. value function CONJUNCTION action value function. unknown reward function CONJUNCTION transition probability. transition probability CONJUNCTION unknown reward function. asymptotic properties FEATURE-OF action value function. asymptotic properties FEATURE-OF value function. Generic is estimator. OtherScientificTerm are empirical Bellman operator, and Confidence intervals. ",This paper proposes an estimator for estimating the value of an action value function and a value function with unknown reward function and transition probability. The estimator is based on the empirical Bellman operator. Confidence intervals are derived and the asymptotic properties of the value function are analyzed.
3121,SP:366b68d2490ea7569c74dc66ec0f83daa029ddd9,"reinforcement learning USED-FOR inference problem. estimated optimal value function CONJUNCTION Q - function. Q - function CONJUNCTION estimated optimal value function. estimations USED-FOR method. method COMPARE algorithms. algorithms COMPARE method. UCRL HYPONYM-OF algorithms. OtherScientificTerm are exploration policy, unique and non - unique optimal policy cases, and non - unique case. Method is Q - OCBA. ","This paper studies the inference problem in reinforcement learning, where the goal is to find an exploration policy that maximizes the value of the current state. The authors consider both unique and non-unique optimal policy cases, and propose a method called Q-OCBA that uses estimations of the estimated optimal value function and the Q-function of the state. They show that their method outperforms existing algorithms such as UCRL in both the unique and the non-uniqueness case."
3122,SP:d922459581c3295ff315fda6e59b9f7e9147f22d,collaborative generated hashing ( CGH ) method USED-FOR hash funcations. content data USED-FOR collaborative generated hashing ( CGH ) method. generative step CONJUNCTION inference. inference CONJUNCTION generative step. balanced and uncorrelated constraints USED-FOR inference. Generic is approach. Metric is accuracy. OtherScientificTerm is warm - start and cold - start recommendations. ,This paper proposes a collaborative generated hashing (CGH) method for hash funcations based on content data. The approach consists of two steps: a generative step that generates a set of hash points and an inference based on balanced and uncorrelated constraints. The goal is to improve the accuracy of the generated hash points by making warm-start and cold-start recommendations.
3123,SP:d922459581c3295ff315fda6e59b9f7e9147f22d,"computational efficiency EVALUATE-FOR candidate selection. binary codes representation USED-FOR candidate selection. binary code representation USED-FOR transformation. constraints USED-FOR balanced and uncorrelated transformations. Hamming distance computations USED-FOR inference. Task is user and item recommendations. OtherScientificTerm are collaborative and content information, and binary codes. Generic is representations. Method are generative step, and kNN - based techniques. ","This paper addresses the problem of user and item recommendations, where both collaborative and content information is available. The paper proposes to improve computational efficiency of candidate selection with binary codes representation to improve candidate selection. The key idea is to use a binary code representation to represent each transformation as a representation of a transformation, and to use these representations as constraints to learn balanced and uncorrelated transformations. The generative step is done using kNN-based techniques. Hamming distance computations are used for inference, and binary codes are used to represent the transformation."
3124,SP:c2a5551f229211c9aa4c43686b517fcde82bbccf,"real - world data EVALUATE-FOR methods. methods COMPARE methods. methods COMPARE methods. real - world data EVALUATE-FOR methods. methods USED-FOR scenario. Method are adversarial transfer learning network, and adversarial networks. Task is drug response prediction. Material are cell line data, patient data, and Patient data. ","This paper proposes an adversarial transfer learning network to transfer information from cell line data to patient data. This is an important problem in drug response prediction, as adversarial networks can transfer information between cell lines. The proposed methods are evaluated on real-world data, and compared to existing methods for this scenario. Patient data is also compared."
3125,SP:c2a5551f229211c9aa4c43686b517fcde82bbccf,adversarial domain adaptation USED-FOR input space. multi - task learning USED-FOR output space. adversarial domain adaptation CONJUNCTION multi - task learning. multi - task learning CONJUNCTION adversarial domain adaptation. multi - task learning USED-FOR adversarial inductive transfer learning. adversarial domain adaptation USED-FOR adversarial inductive transfer learning. Method is transfer learning. OtherScientificTerm is input and output spaces. ,This paper studies adversarial inductive transfer learning with adversarial domain adaptation in the input space and multi-task learning in the output space. The main contribution of this paper is to study the problem of transfer learning from the input and output spaces. 
3126,SP:a27f975266e990b2ab4a0ab8db1588e945d0300a,dynamics model USED-FOR combining model based and model free RL algorithms. Baysian neural network USED-FOR dynamics of the environment. anchored ensemble of neural networks USED-FOR Bayesian inference process. ensemble of dynamics model USED-FOR PPO[1 ] based agent. real data CONJUNCTION model generated data. model generated data CONJUNCTION real data. dynamics model USED-FOR agent. Task is model based reinforcement learning ( RL ). Method is model free algorithm. Generic is model. ,This paper studies the problem of combining model based and model free RL algorithms using a dynamics model. The main idea is to use a Baysian neural network to model the dynamics of the environment and then use an anchored ensemble of neural networks to perform the Bayesian inference process. The PPO[1] based agent is trained with an ensemble of dynamics model and the agent is evaluated on both real data and model generated data. The results show that the model free algorithm outperforms the model based algorithm.
3127,SP:a27f975266e990b2ab4a0ab8db1588e945d0300a,MBPGE USED-FOR policy. model - free + model - based algorithm USED-FOR policy. models USED-FOR policy gradient algorithm. MBPGE HYPONYM-OF model - free + model - based algorithm. policy gradient algorithm USED-FOR policy. true Bayesian distribution USED-FOR they. randomized anchorized MAP USED-FOR true Bayesian distribution. randomized anchorized MAP USED-FOR they. rollouts USED-FOR policy training. learned dynamics USED-FOR distributional shift. learned dynamics USED-FOR policy training. rollouts USED-FOR they. ,"This paper proposes MBPGE, a model-free + model-based algorithm that learns a policy using models to train a policy gradient algorithm. Specifically, they use a randomized anchorized MAP instead of a true Bayesian distribution, and they use rollouts during policy training to mitigate distributional shift."
3128,SP:2aaddb6dda434b49487857d99c9d143e2f54d350,defense method USED-FOR capsule networks. CapNets COMPARE CNNs. CNNs COMPARE CapNets. CapNets USED-FOR white - box and black - box settings. human perception FEATURE-OF visualizations of adversarial examples. CapNets USED-FOR visualizations of adversarial examples. corrupted MNIST dataset EVALUATE-FOR defense method. defense method USED-FOR out - of - distribution detector. ,"This paper proposes a defense method for capsule networks. The authors argue that CapNets are more robust than CNNs in both white-box and black-box settings, and demonstrate that the visualizations of adversarial examples in human perception can be produced by CapNETS. The proposed defense method is evaluated on corrupted MNIST dataset, where the authors show that the proposed method can be used as an out-of-distribution detector."
3129,SP:2aaddb6dda434b49487857d99c9d143e2f54d350,class - conditional capsule networks USED-FOR detecting and generating adversarial images. class - conditional image reconstruction USED-FOR method. classification and reconstruction loss EVALUATE-FOR reconstructive attack. FashionMNIST CONJUNCTION SVHN. SVHN CONJUNCTION FashionMNIST. MNIST CONJUNCTION FashionMNIST. FashionMNIST CONJUNCTION MNIST. SVHN CONJUNCTION CIFAR-10 dataset. CIFAR-10 dataset CONJUNCTION SVHN. defense CONJUNCTION reconstructive attack method. reconstructive attack method CONJUNCTION defense. Method is defense method. ,"This paper proposes a defense method based on class-conditional capsule networks for detecting and generating adversarial images. The method is based on the class-conditioned image reconstruction. The authors evaluate the classification and reconstruction loss of the proposed reconstructive attack on MNIST, FashionMNIST, SVHN, and the CIFAR-10 dataset. The results show that the proposed defense and the reconstructed attack method are effective."
3130,SP:da88bfbe3f59ce1a24522aa5e74c9472b079664a,"Edge of Chaos initialization USED-FOR NTK. training CONJUNCTION generalization. generalization CONJUNCTION training. condition number FEATURE-OF NTK. Edge of Chaos initialization CONJUNCTION activations. activations CONJUNCTION Edge of Chaos initialization. activations CONJUNCTION residual connections. residual connections CONJUNCTION activations. convergence rate FEATURE-OF constant kernel. Method are deep neural networks, and neural network. OtherScientificTerm is depth L. ","This paper studies the convergence of deep neural networks. The authors consider the problem of training and generalization when the depth L of the neural network is large. They show that the NTK with the same condition number converges to a constant kernel with a convergence rate of O(1/\sqrt{T}) with Edge of Chaos initialization, activations, and residual connections."
3131,SP:da88bfbe3f59ce1a24522aa5e74c9472b079664a,"limiting behavior FEATURE-OF neural tangent kernels. piecewise smooth activations COMPARE ReLU. ReLU COMPARE piecewise smooth activations. convergence EVALUATE-FOR activations. edge of caos'behavior USED-FOR residual networks. Method is limit kernels. OtherScientificTerm are edge of caos'initialization, propagation, and initialization. ","This paper studies the limiting behavior of neural tangent kernels. The authors show that piecewise smooth activations converge faster than ReLU, and that the limit kernels can be seen as a function of the edge of caos' initialization. They also show that the propagation of activations can be viewed as an extension of the initialization. Finally, they show that residual networks can be represented as an application of the edges of the caos's behavior."
3132,SP:dd59b897384c52c20d62be73fc33184c8c226f4b,dependency trees USED-FOR self - supervised method. contrastive learning USED-FOR self - supervised method. linguistic knowledge USED-FOR sentence embeddings. sentence representation CONJUNCTION dependency tree representations. dependency tree representations CONJUNCTION sentence representation. bi - LSTM USED-FOR dependency tree representations. bi - LSTM USED-FOR sentence representation. Tree LSTM USED-FOR dependency tree representations. negative log - likelihood loss USED-FOR softmax classifier. Method is sentence embedding method. ,This paper proposes a self-supervised method based on dependency trees using contrastive learning. The main idea is to leverage linguistic knowledge to learn sentence embeddings. The sentence embedding method is based on a bi-LSTM to learn the sentence representation and dependency tree representations using Tree LSTM. The softmax classifier is trained with a negative log-likelihood loss.
3133,SP:dd59b897384c52c20d62be73fc33184c8c226f4b,"tree LSTM USED-FOR linguistic knowledge. context sentence CONJUNCTION negative samples. negative samples CONJUNCTION context sentence. model USED-FOR context sentence. surface information prediction CONJUNCTION syntactic and semantic tasks. syntactic and semantic tasks CONJUNCTION surface information prediction. probing tasks CONJUNCTION qualitative analysis. qualitative analysis CONJUNCTION probing tasks. probing tasks CONJUNCTION syntactic and semantic tasks. syntactic and semantic tasks CONJUNCTION probing tasks. probing tasks USED-FOR surface information prediction. logistic regression model USED-FOR downstream tasks. sentence embeddings USED-FOR logistic regression model. Method are self - supervised sentence embedding approach, and contrastive framework. Material is plain text. ","This paper proposes a self-supervised sentence embedding approach. The authors propose a tree LSTM to extract linguistic knowledge from plain text using a contrastive framework. The model learns a context sentence and a set of negative samples for each sentence, and then uses the model to predict the next context sentence. Experiments are conducted on surface information prediction, syntactic and semantic tasks, probing tasks, and qualitative analysis. A logistic regression model is used for downstream tasks based on sentence embeddings."
3134,SP:980babd58fc2ea5f40bb22b3a9a09737f14f3f18,"domain adaptation USED-FOR task. fine - tuning USED-FOR pre - trained models. fine - tuning USED-FOR domain adaptation. fine - tuning USED-FOR task. BERT HYPONYM-OF pre - trained models. domain adaptation USED-FOR tasks. sentiment analysis HYPONYM-OF tasks. Material are financial domains, and financial domain. ","This paper studies the problem of fine-tuning pre-trained models (e.g., BERT) for a new task based on domain adaptation. The authors focus on the financial domains and show that domain adaptation can be applied to other tasks such as sentiment analysis. In particular, the authors show that in the financial domain, domain adaptation is beneficial."
3135,SP:980babd58fc2ea5f40bb22b3a9a09737f14f3f18,"BERT USED-FOR financial sentiment analysis. BERT COMPARE state - of - the - art. state - of - the - art COMPARE BERT. in - domain data USED-FOR BERT. in - domain dataset USED-FOR pre - training. Method are language model pre - training, and fine - tuning. Material are unsupervised large corpus, and new domain. OtherScientificTerm is catastrophic forgetting. ","This paper studies the problem of language model pre-training. The authors propose to use BERT for financial sentiment analysis, where BERT is trained on an unsupervised large corpus and fine-tuned on a new domain. The results show that BERT outperforms the state-of-the-art in terms of performance on the new domain, but suffers from catastrophic forgetting. To address this issue, the authors suggest to use an in-domain dataset for pre-train BERT."
3136,SP:31c9c3a693922d5c3448e80ade920391dce261f9,pitch information USED-FOR singing voice generation approaches. GAN USED-FOR singing voice waveforms. Task is unconditional singing voice generation. Material is dataset of singing voice data. ,"This paper addresses the problem of unconditional singing voice generation. In particular, the authors propose to use pitch information to improve the performance of singing voice generating approaches. The authors also propose a GAN to generate singing voice waveforms. Experiments are conducted on a dataset of singing singing voice data."
3137,SP:31c9c3a693922d5c3448e80ade920391dce261f9,singing voice synthesis HYPONYM-OF problem. neural network architecture USED-FOR problem. OtherScientificTerm is score / lyrics supervision. ,This paper studies a problem called singing voice synthesis. The authors propose a neural network architecture to solve this problem without score/lyrics supervision. 
3138,SP:99d41c8285fd0270ff16e915ef03187a0a7005b0,"neural network architectures USED-FOR adversarial attacks. defense techniques USED-FOR adversarial attacks. adversarial training HYPONYM-OF defense techniques. core tensor PART-OF Tucker decomposition. Tucker format FEATURE-OF low rank tensors. low rank tensors FEATURE-OF network's weight matrices. OtherScientificTerm are random perturbation, and sparsity. Generic are network, approach, and it. ","This paper studies the problem of defending neural network architectures against adversarial attacks. In particular, the authors propose two defense techniques to defend against such attacks: (1) adversarial training, where a random perturbation is applied to the weights of the network, and (2) sparsity, where the network's weight matrices are modeled as low rank tensors in the Tucker format. The core tensor of the Tucker decomposition is decomposed into two parts. The authors show that the proposed approach is robust to perturbations, and demonstrate that it can be applied to a variety of datasets."
3139,SP:99d41c8285fd0270ff16e915ef03187a0a7005b0,"randomization - based tensorization framework USED-FOR robust network learning. randomization USED-FOR factor matrices. randomly sampled sketching matrices USED-FOR randomization. low - rank tensors USED-FOR network parameters. randomly sampled sketching matrices USED-FOR factor matrices. OtherScientificTerm are subspace, weight matrix, and sparsity. ","This paper proposes a randomization-based tensorization framework for robust network learning. The key idea is to use randomly sampled sketching matrices as the randomization for factor matrices, and to use low-rank tensors as the network parameters. The idea is that if the subspace is large enough, then the weight matrix should be sparse, and if the sparsity is small, then sparsity should be small."
3140,SP:762729b64c1c1494de0f7410ea3662da61e93b6d,clustering attention - based approach USED-FOR unsmoothness. graph attention mechanism USED-FOR CGT model. Transformers USED-FOR NLP tasks. Material is spatio - temporal data. OtherScientificTerm is unsmooth boundaries. Generic is baselines. ,"This paper proposes a clustering attention-based approach to tackle unsmoothness in spatio-temporal data. Specifically, the authors propose a CGT model based on the graph attention mechanism, which is able to distinguish between the unsmooth boundaries. Transformers have been widely used in NLP tasks, and the authors show that the proposed baselines outperform the existing methods."
3141,SP:762729b64c1c1494de0f7410ea3662da61e93b6d,neural network architecture USED-FOR spacial and temporal unsmoothness problem. clustering modules USED-FOR spacial regions. CGT HYPONYM-OF model. clustering modules USED-FOR model. encoder - decoder structure USED-FOR CGT. encoder - decoder structure USED-FOR model. temporal patterns USED-FOR clustering modules. additivity - preserved multi - view position encoding USED-FOR temporal relationships. additivity - preserved multi - view position encoding USED-FOR temporal unsmoothness. real ride - hailing datasets EVALUATE-FOR method. ,"This paper proposes a novel neural network architecture to tackle the spacial and temporal unsmoothness problem. The proposed model, CGT, uses an encoder-decoder structure similar to that of CGT and uses clustering modules to represent different spacial regions based on temporal patterns. The authors also propose an additivity-preserved multi-view position encoding to preserve temporal relationships and address the issue of temporal unssmoothness. The method is evaluated on real ride-hailing datasets."
3142,SP:81d7c60d0d12eb268d7edeebe86422991a1d4997,experience replay CONJUNCTION target network. target network CONJUNCTION experience replay. target network USED-FOR DQN. experience replay USED-FOR DQN. algorithmic and statistical errors FEATURE-OF neural FQI algorithm. Method is deep Q - learning. ,This paper studies the problem of deep Q-learning and proposes a neural FQI algorithm with algorithmic and statistical errors. The main idea is to use experience replay and target network to improve the performance of DQN by combining experience replay with the target network. 
3143,SP:81d7c60d0d12eb268d7edeebe86422991a1d4997,fitted Q - iteration USED-FOR off - policy reinforcement learning setting. neural networks USED-FOR value function class of interest. fitted Q iteration USED-FOR near - optimal policy. Holder smoothness FEATURE-OF transition dynamics. Method is deep Q - learning. OtherScientificTerm is two - player zero - sum stochastic games. ,"This paper studies the fitted Q-iteration in the off-policy reinforcement learning setting, where the value function class of interest is modeled by neural networks. The authors show that fitted Q iteration converges to a near-optimal policy, which is in contrast to deep Q-learning, which does not converge to the optimal policy. Holder smoothness of the transition dynamics is studied, and experiments are conducted on two-player zero-sum stochastic games."
3144,SP:a558ffa1706ef78893528c8c23e2295a79824d2f,compositionality FEATURE-OF self - attention models. Transformer HYPONYM-OF self - attention models. non - linear function USED-FOR semantic mutation. non - linear function USED-FOR it. PhraseTransformer COMPARE Transformer. Transformer COMPARE PhraseTransformer. BLEU score EVALUATE-FOR Transformer. machine translation and PoS tagging tasks EVALUATE-FOR PhraseTransformer. machine translation and PoS tagging tasks EVALUATE-FOR Transformer. BLEU score EVALUATE-FOR PhraseTransformer. OtherScientificTerm is hypernode. ,"This paper studies compositionality of self-attention models such as Transformer. Specifically, it uses a non-linear function to model semantic mutation, which is a function of the hypernode. Experiments on machine translation and PoS tagging tasks show that PhraseTransformer outperforms Transformer in terms of BLEU score."
3145,SP:a558ffa1706ef78893528c8c23e2295a79824d2f,network USED-FOR representations of phrases. algorithm USED-FOR applications. translation CONJUNCTION pos tagging. pos tagging CONJUNCTION translation. algorithm USED-FOR pos tagging. algorithm USED-FOR translation. pos tagging HYPONYM-OF applications. translation HYPONYM-OF applications. method COMPARE transformer. transformer COMPARE method. Task is NLP. ,This paper proposes a network that learns representations of phrases. The proposed algorithm is applied to two applications: translation and pos tagging. The method is compared to a transformer and compared to several other methods in NLP.
3146,SP:622b0593972296a95b630a4ece1e959b60fec56c,modular neural architecture USED-FOR algorithm induction. controller policy USED-FOR distribution. distribution FEATURE-OF modules. modules CONJUNCTION input / output locations. input / output locations CONJUNCTION modules. memory tape FEATURE-OF input / output locations. input / output locations FEATURE-OF distribution. REINFORCE USED-FOR controller. ,This paper proposes a modular neural architecture for algorithm induction. The controller policy learns a distribution over modules and input/output locations on memory tape. REINFORCE is used to train the controller.
3147,SP:622b0593972296a95b630a4ece1e959b60fec56c,Modular Algorithm Induction Network ( MAIN ) USED-FOR algorithms. modules USED-FOR algorithmic tasks. it COMPARE baselines. baselines COMPARE it. tasks EVALUATE-FOR MAIN. reinforcement learning USED-FOR MAIN. ,"This paper proposes Modular Algorithm Induction Network (MAIN) to learn algorithms that are modular in nature. The modules are designed to solve algorithmic tasks in a modular fashion. MAIN is trained with reinforcement learning and evaluated on a variety of tasks, where it outperforms the baselines."
3148,SP:d668cc809e4f6b5f3330cf75cb5f71693a123c07,"cross entropy HYPONYM-OF training loss. Method are neural network, and Monte Carlo Arithmetic ( MCA ). OtherScientificTerm is floating - point arithmetic. Metric is classification accuracy. ","This paper proposes a new training loss called cross entropy, which is a modification of the training loss of a neural network. The main idea is based on Monte Carlo Arithmetic (MCA), which is an extension of floating-point arithmetic. The authors show that cross entropy can be used to improve the classification accuracy."
3149,SP:d668cc809e4f6b5f3330cf75cb5f71693a123c07,scalable method USED-FOR neural networks. floating point rounding errors FEATURE-OF neural networks. Monte Carlo arithmetic USED-FOR scalable method. AlexNet CONJUNCTION ResNet. ResNet CONJUNCTION AlexNet. networks USED-FOR quantization. loss of significance metric EVALUATE-FOR networks. process USED-FOR loss of significance metric. AlexNet HYPONYM-OF architectures. ResNet HYPONYM-OF architectures. ,"This paper proposes a scalable method based on Monte Carlo arithmetic to reduce the floating point rounding errors of neural networks. The authors evaluate the performance of two architectures, AlexNet and ResNet, on the loss of significance metric obtained by the proposed process. The results show that networks trained for quantization perform better."
3150,SP:eda1d368aa3b4d806020c4c430a173d1ddd13d0d,"policy optimization HYPONYM-OF objective. model learning objective HYPONYM-OF objective. Method is model - based reinforcement learning methods. Generic are issue, method, and model. OtherScientificTerm is objective mismatch. ","This paper studies the problem of objective mismatch in model-based reinforcement learning methods. The issue is well-motivated and well-studied. In particular, the authors point out that there are two types of objective: policy optimization (i.e., the model learning objective) and the objective mismatch between the learned model and the training data. The authors then propose a method to address the issue and show that the proposed method can achieve state-of-the-art performance."
3151,SP:eda1d368aa3b4d806020c4c430a173d1ddd13d0d,"NLL CONJUNCTION reward. reward CONJUNCTION NLL. models USED-FOR rewards. NLLs FEATURE-OF models. Method are probabilistic model ( of dynamics ), and reweighting trick. OtherScientificTerm is model ( log - likelihood ). ",This paper proposes a probabilistic model (of dynamics) where the model (log-likelihood) is a weighted combination of the NLL and the reward. The authors propose a reweighting trick to make the model more interpretable. They show that models with NLLs and rewards trained on the same data can be reweighted in different ways.
3152,SP:63c452f2b2cbfeea0b45831bd7dc1ac26883fd9f,"feature representations USED-FOR adversarial attack. OtherScientificTerm are intermediate feature space information, and noise. Metric is transferability. ",This paper studies adversarial attack on feature representations. The authors propose to remove intermediate feature space information in order to improve the transferability of adversarial attacks. The main contribution of this paper is to show that adding additional noise to the original input is not necessary to improve transferability.
3153,SP:63c452f2b2cbfeea0b45831bd7dc1ac26883fd9f,adversarial attack USED-FOR targeted blackbox model. approaches COMPARE approaches. approaches COMPARE approaches. regularization USED-FOR approaches. output layer USED-FOR approaches. intermediate features USED-FOR approaches. intermediate layer USED-FOR adversarial example. binary classifier USED-FOR adversarial examples. features USED-FOR binary classifier. ,"This paper proposes an adversarial attack against a targeted blackbox model. The proposed approaches are based on regularization, where the output layer is trained to be adversarial, and the intermediate features are used to generate the adversarial example. The adversarial examples are generated by a binary classifier trained on the features."
3154,SP:a7a2ded35804c381603a1196c7f7893fdf796c05,"Wasserstein Distances USED-FOR higher - level functions of policies. WD - based TRPO CONJUNCTION distributional RL. distributional RL CONJUNCTION WD - based TRPO. OtherScientificTerm are behaviors, policy, behavioral embeddings, and behavioral mapping. Generic is behavior. Method is Wasserstein Distance. ","This paper proposes to use Wasserstein Distances to model higher-level functions of policies. Specifically, the authors propose to model behaviors as a function of the distance between the learned policy and the environment. The behavior is modeled as a Wesselstein Distance between the behavioral embeddings of the policy and its environment, and the behavioral mapping between the embedding of the environment and the policy is modeled using the learned behavior. Experiments are conducted on WD-based TRPO and distributional RL."
3155,SP:a7a2ded35804c381603a1196c7f7893fdf796c05,Wasserstein distances USED-FOR regularized policy optimization ( PO ) method. SGD USED-FOR WD. Evolution Strategies HYPONYM-OF policy search approaches. OpenAI Gym FEATURE-OF control tasks. ,"This paper proposes a regularized policy optimization (PO) method based on Wasserstein distances. The main idea is to use SGD to approximate WD, which is an extension of SGD. The authors also propose two policy search approaches: Evolution Strategies and Evolution Strategies. Experiments are conducted on control tasks on OpenAI Gym."
3156,SP:ef1c6403597c3a6083c1ad4256449325ac99416c,"gradient descent methods USED-FOR deep neural network. hyper - parameter USED-FOR adaptive step size. maximum learning rate HYPONYM-OF hyper - parameter. Polyak step - size USED-FOR non - convex setting. adaptive learning - rates USED-FOR interpolation with gradients ( ALI - G ). adaptive learning - rates HYPONYM-OF stochastic extension of the Polyak step - size. convergence guarantees USED-FOR ALI - G. ALI - G USED-FOR convex setting. ALI - G CONJUNCTION SGD. SGD CONJUNCTION ALI - G. SGD COMPARE ALI - G. ALI - G COMPARE SGD. objective loss USED-FOR ALI - G. Adam CONJUNCTION DFW. DFW CONJUNCTION Adam. DFW CONJUNCTION L4Adam. L4Adam CONJUNCTION DFW. Adagrad CONJUNCTION Adam. Adam CONJUNCTION Adagrad. L4Adam CONJUNCTION SGD. SGD CONJUNCTION L4Adam. object recognition CONJUNCTION natural language processing task. natural language processing task CONJUNCTION object recognition. differentiable neural computer CONJUNCTION object recognition. object recognition CONJUNCTION differentiable neural computer. algorithm COMPARE methods. methods COMPARE algorithm. SGD USED-FOR differentiable neural computer. SGD USED-FOR natural language processing task. SGD USED-FOR object recognition. SGD HYPONYM-OF methods. Adagrad HYPONYM-OF methods. object recognition HYPONYM-OF methods. L4Adam HYPONYM-OF methods. DFW HYPONYM-OF methods. Adam HYPONYM-OF methods. ALI - G COMPARE SGD. SGD COMPARE ALI - G. schedule learning rate USED-FOR SGD. schedule learning rate USED-FOR ALI - G. Method are stochastic gradient descent ( SGD ), and neural networks. OtherScientificTerm is learning rates. ","This paper studies gradient descent methods for training a deep neural network. The authors propose a stochastic extension of the Polyak step-size to the non-convex setting called adaptive learning-rates for interpolation with gradients (ALI-G), which is an extension of adaptive learning - rates, which is the stochastically extension of a Polyak increase in the step size. The adaptive step size is defined as a hyper-parameter, such as the maximum learning rate. The paper provides convergence guarantees for ALI-GI and SGD in the convex setting, and shows that the objective loss of ALI -G converges linearly with respect to SGD. The algorithm is compared with other methods (Adagrad, Adam, DFW, L4Adam, SGD) on a differentiable neural computer, object recognition, and natural language processing task. The results show that the proposed algorithm outperforms the other methods in all cases. In addition, the authors show that ALI with a schedule learning rate outperforms SGD with the same number of iterations. "
3157,SP:ef1c6403597c3a6083c1ad4256449325ac99416c,"adaptive learning rate method USED-FOR optimization of deep neural networks. Polyak update rule USED-FOR stochastic updates. Polyak update rule USED-FOR simplification. zero minimal training loss USED-FOR simplification. benchmarks EVALUATE-FOR complex architectures. Method are over - parameterized DNNs, and SGD learning rates. OtherScientificTerm is convex settings. ","This paper proposes an adaptive learning rate method for the optimization of deep neural networks. The main idea is to use the Polyak update rule for stochastic updates, which allows for simplification with zero minimal training loss. The authors show that over-parameterized DNNs can be efficiently optimized with SGD learning rates, especially in convex settings. Experiments are conducted on several benchmarks to evaluate the performance of complex architectures."
3158,SP:6e24a1e0aff73db6ae8558f114b644965e287e36,horizontal and top - down connections USED-FOR perceptual grouping. network structure USED-FOR horizontal and top - down connections. network structure USED-FOR perceptual grouping. Gestalt cues CONJUNCTION object - based strategies. object - based strategies CONJUNCTION Gestalt cues. datasets USED-FOR object - based strategies. datasets USED-FOR Gestalt cues. connections PART-OF network. Generic is dataset. Material is cABC dataset. Method is cABC. ,This paper proposes a new network structure that combines horizontal and top-down connections to improve perceptual grouping. The authors propose two datasets for Gestalt cues and object-based strategies. The first dataset is the cABC dataset. The second dataset is a modified version of cABC. The connections in the second network are the same as in the first network.
3159,SP:6e24a1e0aff73db6ae8558f114b644965e287e36,neural networks USED-FOR computational model of the brain. strategies USED-FOR visual challenges. Task is neuroscience and perception literature. Method is neural architectures. ,"This paper is an interesting contribution to the neuroscience and perception literature. The authors propose to use neural networks as a computational model of the brain, which is an important direction in neuroscience and the perception literature, and propose several neural architectures to do so. They also propose strategies to tackle visual challenges. "
3160,SP:7a0db1e8804defc5c04e0f4dd345272c6df1ff77,sparsity FEATURE-OF neural networks. scale - invariant regularizer ( DeepHoyer ) USED-FOR sparsity. Hoyer measure USED-FOR scale - invariant regularizer ( DeepHoyer ). It USED-FOR element - wise sparsity. DeepHoyer USED-FOR pruned models. Hoyer measure USED-FOR sparsity. DeepHoyer USED-FOR DNN training. Method is Hoyer - Square. ,This paper proposes a scale-invariant regularizer (DeepHoyer) to reduce the sparsity of neural networks. It aims to reduce element-wise sparsity by using the Hoyer measure. DeepHoyer can be applied to pruned models and is shown to reduce sparsity during DNN training. The paper also proposes a variant of Hoyer-Square to further improve the performance.
3161,SP:7a0db1e8804defc5c04e0f4dd345272c6df1ff77,"l1 regularization COMPARE go - to strategy. go - to strategy COMPARE l1 regularization. l1 / l2 ratio HYPONYM-OF Hoyer regularization. Hoyer regularization USED-FOR sparse solutions. minima structure FEATURE-OF Hoyer regularization. LeNet CONJUNCTION AlexNet. AlexNet CONJUNCTION LeNet. AlexNet CONJUNCTION ResNet. ResNet CONJUNCTION AlexNet. MNIST CONJUNCTION CIFAR. CIFAR CONJUNCTION MNIST. CIFAR CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR. regularizations USED-FOR deep networks. ResNet HYPONYM-OF datasets. deep networks USED-FOR it. datasets EVALUATE-FOR it. ImageNet HYPONYM-OF datasets. MNIST HYPONYM-OF datasets. regularizations USED-FOR it. CIFAR HYPONYM-OF datasets. ResNet HYPONYM-OF regularizations. ResNet HYPONYM-OF deep networks. LeNet HYPONYM-OF regularizations. AlexNet HYPONYM-OF regularizations. LeNet HYPONYM-OF deep networks. AlexNet HYPONYM-OF deep networks. Method are sparse neural networks, l0 regularization, and structured Hoyer regularization. OtherScientificTerm is regularization. ","This paper studies sparse neural networks. The authors argue that the l1 regularization (i.e., l1/l2 ratio) is a better go-to strategy for sparse solutions than the traditional go -to strategy, i.e. l0 regularization. They argue that Hoyer regularization with minima structure (e.g., l2/l1 ratio) can lead to sparse solutions, and propose a structured version of this regularization, which they call structured Hoyer. They evaluate it on a variety of datasets (MNIST, CIFAR, ImageNet, etc.) with different regularizations (LeNet, AlexNet, ResNet, and others) and show that it can be used to train deep networks (i.,e., LeNet and AlexNet)."
3162,SP:5ec05ac5d72e8e0b39b15a0cd7b2f5a64e861024,"adaptive gradient methods USED-FOR “ data dependent ” logarithmic regret bounds. SAdam HYPONYM-OF Adam. SC - RMSprop HYPONYM-OF RMSprop. SC - RMSprop USED-FOR SAdam. SAdam COMPARE adaptive gradient methods. adaptive gradient methods COMPARE SAdam. optimizing strongly convex functions CONJUNCTION deep networks. deep networks CONJUNCTION optimizing strongly convex functions. SAdam COMPARE SGD. SGD COMPARE SAdam. adaptive gradient methods CONJUNCTION SGD. SGD CONJUNCTION adaptive gradient methods. optimizing strongly convex functions EVALUATE-FOR SAdam. Task is online convex optimization. OtherScientificTerm are class of loss functions, and “ data independent ” logarithmic bound. ","This paper studies the problem of online convex optimization and proposes a new class of loss functions, called adaptive gradient methods, which can provide “data dependent” logarithmic regret bounds. In particular, the authors propose SAdam, a variant of Adam based on SC-RMSprop, which is an extension of RMSprop. The authors show that SAdam is equivalent to adaptive and SGD in terms of performance on optimizing strongly convex functions and deep networks, and provide an “uniform” and “noisy” (i.e. data independent) “dataset-dependent” linear regret bound."
3163,SP:5ec05ac5d72e8e0b39b15a0cd7b2f5a64e861024,SAdam HYPONYM-OF Adam. controlled step size FEATURE-OF strong convexity. OtherScientificTerm is data - dependent O(log T ) regret bound. Generic is algorithm. ,"This paper proposes a new variant of Adam, called SAdam, which achieves a data-dependent O(log T) regret bound. The algorithm is motivated by the observation that strong convexity with controlled step size can be achieved with a small number of steps."
3164,SP:9f89501e6319280b4a14b674632a300805aa485c,optimisation USED-FOR BERT models. block matrices USED-FOR attention layers. block matrices USED-FOR optimisation. memory footprint CONJUNCTION processing   time. processing   time CONJUNCTION memory footprint. memory consumption FEATURE-OF BERT. ,This paper proposes a novel optimisation for BERT models based on block matrices in the attention layers. The main motivation is to reduce the memory footprint and processing  time of BERT. Experiments are conducted to demonstrate the effectiveness of the proposed memory consumption.
3165,SP:9f89501e6319280b4a14b674632a300805aa485c,"memory use EVALUATE-FOR model. block size USED-FOR model. OtherScientificTerm are attention matrix, and memory usage. Task is training. ",This paper studies the memory use of a model trained with a fixed block size. The authors propose to use the attention matrix as a proxy for the size of the block size to evaluate the performance of the model. They show that this can reduce the memory usage during training.
3166,SP:0f04fc2e7966f4ba53909654fc0e8b90fc405f2a,generalization error CONJUNCTION mean / variance of the test accuracy. mean / variance of the test accuracy CONJUNCTION generalization error. score USED-FOR pruning. E[BN ] HYPONYM-OF score. generalization error CONJUNCTION test accuracy mean / variance. test accuracy mean / variance CONJUNCTION generalization error. VGG11 CONJUNCTION ResNet18. ResNet18 CONJUNCTION VGG11. ResNet18 CONJUNCTION Conv4 models. Conv4 models CONJUNCTION ResNet18. small score weights USED-FOR VGG11. test accuracy CONJUNCTION generalization gap. generalization gap CONJUNCTION test accuracy. generalization gap EVALUATE-FOR pruning small score weights. noise injection USED-FOR pruning. pruning HYPONYM-OF pruning. ,"This paper proposes a new score called E[BN] for pruning, which is a weighted combination of the generalization error and the mean/variance of the test accuracy. The authors conduct experiments on VGG11, ResNet18, and Conv4 models and show that pruning small score weights leads to a significant drop in both test accuracy and generalization gap. They also show that the effect of noise injection on the performance of pruning."
3167,SP:0f04fc2e7966f4ba53909654fc0e8b90fc405f2a,model accuracy CONJUNCTION generalization risk. generalization risk CONJUNCTION model accuracy. generalization risk EVALUATE-FOR neural network pruning. model accuracy EVALUATE-FOR neural network pruning. Method is pruning methods. OtherScientificTerm is pruning. ,This paper studies the relationship between model accuracy and generalization risk in neural network pruning. The authors propose two pruning methods and show that their pruning results are correlated with the model accuracy. They also provide some theoretical analysis to support their findings.
3168,SP:dba3f5ec3af2a4a67ed4fc36b0f37fe556354177,word - to - vector PART-OF NLP. pretrained encoder USED-FOR search space. pretrained encoder USED-FOR dense and continuous architecture - embedding space. architecture - embedding encoder CONJUNCTION decoder. decoder CONJUNCTION architecture - embedding encoder. Auto - Encoder HYPONYM-OF self - supervision learning. self - supervision learning USED-FOR decoder. it USED-FOR reinforcement learning based Neural Architecture Search(NAS ). architecture - embedding space FEATURE-OF reinforcement learning based Neural Architecture Search(NAS ). ,This paper studies the problem of word-to-vector in NLP. The authors propose a dense and continuous architecture-embedding space based on a pretrained encoder that encodes the search space and a decoder based on self-supervision learning (Auto-Encoder). The authors apply it to reinforcement learning based Neural Architecture Search(NAS) in the architecture-encoding space.
3169,SP:dba3f5ec3af2a4a67ed4fc36b0f37fe556354177,"auto - encoder USED-FOR neural architecture. agent controller USED-FOR reinforcement learning. encoder USED-FOR agent controller. continuous low - dimensional embedding space USED-FOR neural architecture. Method is Neural Architecture Search. OtherScientificTerm are decoder, controller, embedding space, and generalization gap. Metric are reward, and validation accuracy. ","This paper proposes Neural Architecture Search, which aims to find a neural architecture with a continuous low-dimensional embedding space using an auto-encoder. The encoder is used to train an agent controller for reinforcement learning, where the agent controller is trained to predict the reward. The decoder is then used to search for the best encoder to train the controller. The goal is to minimize the generalization gap between the encoder and the controller, which is measured in terms of validation accuracy. The authors show that if the controller can find a good encoder that is close to the optimal encoder, then the agent will be able to generalize better to unseen environments. The paper also shows that the encoding space can be decomposed into multiple subspaces, which can be used to further reduce the size of the training set."
3170,SP:e2e5bebccc76a51df3cb8b64572720da97174604,Homotopy Training Algorithm ( HTA ) USED-FOR neural network optimization problems. simplified problems USED-FOR HTA. continuous homotopy path USED-FOR HTA. synthetic data CONJUNCTION CIFAR-10 dataset. CIFAR-10 dataset CONJUNCTION synthetic data. ,This paper proposes a Homotopy Training Algorithm (HTA) for neural network optimization problems. HTA is based on simplified problems and is trained on a continuous homotopy path. Experiments are conducted on synthetic data and CIFAR-10 dataset.
3171,SP:e2e5bebccc76a51df3cb8b64572720da97174604,"homotopy - based continuation method USED-FOR neural networks. method USED-FOR homotopy function. extreme case FEATURE-OF optimization problem. optimization problem PART-OF homotopy function. continuous path USED-FOR optimization problem. homotopy parameter USED-FOR continuous path. convex case FEATURE-OF solution path. synthetic and real datasets EVALUATE-FOR method. OtherScientificTerm are parameter space, and convergence. Material is non - convex case. ","This paper proposes a homotopy-based continuation method for training neural networks. The proposed method learns a continuous path to the solution of an optimization problem in the extreme case, where the parameter space is continuous. The continuous path is defined as a function of the homotology parameter. The authors show that the solution path in the convex case converges to a solution in the non-convex case. They also provide theoretical analysis on the convergence of the proposed method on both synthetic and real datasets."
3172,SP:5d9517fa62cd97b94ff45f645e100a8ad631e281,higher - order interactions FEATURE-OF transformer block. 2 - simplicial attention USED-FOR scalar triple product. 2 - simplicial attention USED-FOR dot product. 2 - simplicial attention USED-FOR weighted average of tensor products of value vectors. bridge BoxWorld environment EVALUATE-FOR representation power. representation power EVALUATE-FOR architecture. bridge BoxWorld environment EVALUATE-FOR architecture. OtherScientificTerm is weighted average of value vectors. ,This paper proposes a transformer block with higher-order interactions. The authors propose to use a weighted average of tensor products of value vectors using 2-simplicial attention on the scalar triple product and on the dot product. The proposed architecture is evaluated on the bridge BoxWorld environment to show better representation power.
3173,SP:5d9517fa62cd97b94ff45f645e100a8ad631e281,"attention USED-FOR 2 - simplex. attention weights USED-FOR head. binary relation FEATURE-OF head. model USED-FOR edge = 1 - simplex. Task is Simplicialization of attention. OtherScientificTerm are 2 - simplexes, and arity-3 relation. ","This paper proposes Simplicialization of attention to 2-simplex. Specifically, the authors propose to learn attention for a 2-simplex, where the head is a binary relation to the arity-3 relation, and the attention weights for the head are learned. The model is trained to predict edge = 1-superplex and edge = 2-sparse. "
3174,SP:f66721bf3eccf2e36444c2c41303e97745f10f0e,"conditional variational autoencoder ( CVAE ) USED-FOR geometry of 2D rotations of objects. latent representation USED-FOR method. it USED-FOR hyperspherical latent space. latent representation USED-FOR rotations. OtherScientificTerm are 2D rotation, ordinal latent variable, and latent variable. ","This paper proposes a conditional variational autoencoder (CVAE) to model the geometry of 2D rotations of objects. The proposed method uses a latent representation to represent the rotations, and then uses it to represent a hyperspherical latent space, where each 2D rotation is represented as an ordinal latent variable. The latent variable is then used to predict the next rotation."
3175,SP:f66721bf3eccf2e36444c2c41303e97745f10f0e,semi - supervised approach USED-FOR rotation of objects. image FEATURE-OF rotation of objects. supervised loss CONJUNCTION unsupervised loss. unsupervised loss CONJUNCTION supervised loss. supervised loss USED-FOR CVAE. unsupervised loss USED-FOR CVAE. CVAE USED-FOR approach. Generic is network. ,This paper proposes a semi-supervised approach for the rotation of objects in an image. The approach is based on CVAE with a supervised loss and an unsupervised loss. The network is trained in a supervised fashion.
3176,SP:87dc93d26ad5ad4a8dccde1780b5b127f391cfd6,curriculum USED-FOR large - scale multi - agent learning. ,"This paper proposes a curriculum for large-scale multi-agent learning. The paper is well-written and well-motivated. However, there are some issues that need to be addressed."
3177,SP:87dc93d26ad5ad4a8dccde1780b5b127f391cfd6,"method USED-FOR scaling multi - agent reinforcement learning. evolution USED-FOR scaling multi - agent reinforcement learning. evolution USED-FOR method. particle world set of environments USED-FOR games. Method are EPC ), and crossover. ","This paper proposes a method for scaling multi-agent reinforcement learning using evolution. The method (EPC) is based on the idea of crossover, i.e., learning to adapt to a new environment as the environment changes. This is achieved by playing games on a particle world set of environments."
3178,SP:0ea5b3247ce031f25b98cf7d42bd4290020fbed2,it HYPONYM-OF multimodule neural pipeline. Task is abstract diagrammatic reasoning. Generic is model architecture. OtherScientificTerm is diagram subsets. ,"This paper addresses the problem of abstract diagrammatic reasoning. The authors propose a model architecture that is based on a multimodule neural pipeline, i.e., it is a multi-modal neural pipeline. The main idea is to learn a set of diagram subsets, which are then fed into the model architecture. "
3179,SP:0ea5b3247ce031f25b98cf7d42bd4290020fbed2,"them CONJUNCTION graph network. graph network CONJUNCTION them. region proposal USED-FOR method. method USED-FOR object level representation. aggregation function USED-FOR gated graph networks. gated graph networks USED-FOR approach. transformer network USED-FOR spatial attention. transformer network USED-FOR approach. object level representation USED-FOR spatial attention. Task is Raven Progressive Matrices ( RPM ) reasoning. Method are multiplexed graph networks, and WREN method. Generic are architecture, and network. OtherScientificTerm is node embeddings. ","This paper studies Raven Progressive Matrices (RPM) reasoning with multiplexed graph networks. The proposed method is based on region proposal and learns an object level representation by combining them with a graph network. The approach uses gated graph networks with an aggregation function and uses a transformer network for spatial attention. The architecture is similar to the WREN method, except that instead of using node embeddings as in WREN, the network is trained on the entire graph."
3180,SP:9bcb840f867f1a7108aa22a7bb14c348fda52eb0,adaptive thermostat Monte Carlo sampler USED-FOR feedforward neural networks. momentum CONJUNCTION noisy. noisy CONJUNCTION momentum. approach USED-FOR momentum. noisy USED-FOR model parameter. momentum FEATURE-OF model parameter. noisy USED-FOR approach. fixup CONJUNCTION weight normalization. weight normalization CONJUNCTION fixup. SELU CONJUNCTION fixup. fixup CONJUNCTION SELU. ResNet++ HYPONYM-OF ResNet. batchnorm / dropout PART-OF ResNet. early stopping CONJUNCTION stochastic regularization. stochastic regularization CONJUNCTION early stopping. stochastic regularization CONJUNCTION learning rate schedules. learning rate schedules CONJUNCTION stochastic regularization. OtherScientificTerm is hyperparameter setup. ,"This paper proposes an adaptive thermostat Monte Carlo sampler for feedforward neural networks. The proposed approach aims to balance momentum and noisy in the model parameter. The authors propose a hyperparameter setup that combines SELU, fixup, and weight normalization. Experiments on ResNet++ (ResNet with batchnorm/dropout) demonstrate the effectiveness of early stopping, stochastic regularization, and learning rate schedules."
3181,SP:9bcb840f867f1a7108aa22a7bb14c348fda52eb0,"MCMC algorithm ( ATMC ) USED-FOR posterior distribution of neural network weights. Bayesian inference USED-FOR uncertainty - calibrated models. Bayesian inference USED-FOR deep learning. overfitting CONJUNCTION uncertainty - calibrated models. uncertainty - calibrated models CONJUNCTION overfitting. Generic are approach, and method. ",This paper proposes a novel MCMC algorithm (ATMC) for estimating the posterior distribution of neural network weights. The approach is motivated by the observation that Bayesian inference in deep learning can lead to overfitting and uncertainty-calibrated models. The method is well-motivated and well motivated.
3182,SP:8cf0614f0fbd3756453304703d00776cfc9a4b9f,method USED-FOR deep neural networks. models USED-FOR image recognition datasets. image recognition datasets EVALUATE-FOR it. models USED-FOR it. OtherScientificTerm is early - bird. Generic is model. ,This paper proposes a method for training deep neural networks. The method is called early-bird and it is evaluated on several image recognition datasets using a variety of models. The idea is to train a model on a small number of datasets and then train the rest of the models on the remaining datasets.
3183,SP:8cf0614f0fbd3756453304703d00776cfc9a4b9f,"sparsity pattern FEATURE-OF lottery ticket. low - cost training USED-FOR sparsity pattern. sparsity USED-FOR it. network USED-FOR it. sparsity FEATURE-OF network. OtherScientificTerm are lottery ticket hypothesis, and initialization. Generic is method. ","This paper studies the lottery ticket hypothesis and proposes to use low-cost training to learn the sparsity pattern of a lottery ticket. In particular, it uses sparsity to train a network with high sparsity and then uses this sparsity as an initialization. The method is evaluated on a variety of datasets."
3184,SP:8aeece75c839643a02d2b3b5f3aca7cb76cf1d35,adversarial learning framework USED-FOR hidden features. prior distributions USED-FOR hidden features. training strategy USED-FOR GAN. it USED-FOR adversarial perturbations. framework USED-FOR adversarial perturbations. Generic is algorithm. ,"This paper proposes an adversarial learning framework for learning hidden features from prior distributions. The main contribution of the paper is to propose a new training strategy for GAN. The proposed algorithm is simple and effective, and it can be applied to adversarial perturbations. "
3185,SP:8aeece75c839643a02d2b3b5f3aca7cb76cf1d35,regularization technique USED-FOR adversarial robustness. Embedding Regularization HYPONYM-OF regularization technique. generative adversarial networks ( GAN ) USED-FOR inference. generative adversarial networks ( GAN ) USED-FOR latent space. aggregated posterior FEATURE-OF hidden space vector. inference USED-FOR latent space. prior distribution USED-FOR aggregated posterior. strategy CONJUNCTION adversarial training. adversarial training CONJUNCTION strategy. adversarial accuracy EVALUATE-FOR adversarial training. benchmark datasets EVALUATE-FOR adversarial training. adversarial accuracy EVALUATE-FOR strategy. benchmark datasets EVALUATE-FOR strategy. ,"This paper proposes Embedding Regularization, a new regularization technique for improving adversarial robustness. The key idea is to use generative adversarial networks (GANs) to perform inference on the latent space during inference. The inference is then used to compute the aggregated posterior of the hidden space vector from the prior distribution. The authors evaluate the effectiveness of the proposed strategy and adversarial training on several benchmark datasets."
3186,SP:efd68097f47dbfdd0208573071686a62240d1b12,"joint learning algorithm USED-FOR tasks. NER CONJUNCTION RE. RE CONJUNCTION NER. joint learning algorithm USED-FOR NER. NER HYPONYM-OF tasks. RE HYPONYM-OF tasks. BERT model USED-FOR word vectors. BERT model USED-FOR model. it USED-FOR tasks. network branches USED-FOR tasks. joint learning USED-FOR tasks. Generic are first branch, second branch, and branch. ","This paper proposes a joint learning algorithm for tasks such as NER and RE. The model is based on a BERT model that predicts word vectors. The first branch is trained to predict the next word, while the second branch is used to generate the new word. The authors show that the proposed branch is able to generalize well and that it can be applied to a variety of tasks with different network branches. They also show that joint learning is effective for these tasks."
3187,SP:efd68097f47dbfdd0208573071686a62240d1b12,"named entity recognition ( NER ) CONJUNCTION relation extraction ( RE ). relation extraction ( RE ) CONJUNCTION named entity recognition ( NER ). end - to - end joint model USED-FOR named entity recognition ( NER ). end - to - end joint model USED-FOR relation extraction ( RE ). pre - trained language models USED-FOR end - to - end joint model. NER output USED-FOR RE. Generic is model. OtherScientificTerm are BERT, and handcrafted features. ",This paper proposes an end-to-end joint model for named entity recognition (NER) and relation extraction (RE) using pre-trained language models. The model is based on BERT [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30] [31] [32] [33] [34] [35] [36] [37] [38] [39] [40] [41] [42] [43] [44] [45] [46] [47] [48] [49] [50] [51] [53] [54] [55] [56] [57] [58] [59] [60] [63] [64] [65] [67] [68] [69] [70] [71] [72] [73] [74] [75] [76] [77] [78] [79] [84] [85] [86] [87] [88] [89] [90] [91] [92] [93] [94] [95] [104] [105] [106] [107] [114] [115] [116] [127] [133] [134] [135] [136] [137] [138] [139] [140] [141] [145] [155] [156] [167] [147] [168] [169] [175] [176] [177] [178] [179] [180] [174] [185] [197] [198] [201] [205] [206] [209] [237] [213] [228] [255] [276] [265] [287] [278] [347] [349] [344] [355] [346] [353] [428] [435] [425] [449] [426] [427] [424] [445] [436] [432] [447] [457] [459] [417] [474]
3188,SP:8fd4f3f8615c0a7a76ec7bfe996d2ead803f7828,triplet loss USED-FOR convex relaxation of the ordinal embedding problem. feed - forward neural network USED-FOR loss. optimization capability CONJUNCTION parallelism. parallelism CONJUNCTION optimization capability. deep network USED-FOR optimization capability. parallelism USED-FOR deep network. GPUs USED-FOR parallelism. Generic is network. OtherScientificTerm is binary codes. Task is real - world task. ,"This paper proposes a triple loss for solving a convex relaxation of the ordinal embedding problem. The loss is modeled as a feed-forward neural network, where each layer of the network takes as input a sequence of binary codes, and outputs the output of the next layer. This allows the deep network to have both optimization capability and parallelism on GPUs, which is useful in a real-world task."
3189,SP:8fd4f3f8615c0a7a76ec7bfe996d2ead803f7828,features USED-FOR informative representations. neural networks USED-FOR features. RGB values HYPONYM-OF low - level input representations. instance ID HYPONYM-OF low - level input representations. triplet comparisons USED-FOR IDs. triplet comparisons USED-FOR representations. large real world datasets EVALUATE-FOR it. Method is feature representations. OtherScientificTerm is ordinal embeddings. Generic is technique. ,"This paper proposes to learn feature representations that are informative. The idea is to use neural networks to learn features that can be used to learn informative representations. Specifically, low-level input representations such as RGB values and instance ID are considered. The representations are learned using triplet comparisons between IDs, which are then used as ordinal embeddings. The technique is evaluated on several large real world datasets and it is shown to be effective."
3190,SP:12e7f417a7ef1ccafccff5ffb3f8f11cd2c05b20,"neural network USED-FOR method. data sampling USED-FOR gradient flow. REINFORCE USED-FOR gradient flow. method COMPARE approaches. approaches COMPARE method. OtherScientificTerm are off - the - target distributions, and reward. ","This paper proposes a method that uses a neural network to sample from off-the-target distributions. The method, REINFORCE, learns gradient flow from data sampling, where the reward is a function of the number of samples sampled from the target distribution. The proposed method is evaluated on a variety of datasets and compared to existing approaches."
3191,SP:12e7f417a7ef1ccafccff5ffb3f8f11cd2c05b20,meta learning approach USED-FOR reinforcement learning tasks. data valuation USED-FOR reinforcement learning tasks. data valuation USED-FOR meta learning approach. network CONJUNCTION regular predictor network. regular predictor network CONJUNCTION network. data value estimator HYPONYM-OF network. data value estimation USED-FOR predictor. Generic is construction. Material is unreliable and corrupted data. ,"This paper proposes a meta learning approach based on data valuation for reinforcement learning tasks. The proposed construction is based on the observation that unreliable and corrupted data can lead to poor performance. To address this issue, the authors propose a network called data value estimator and a regular predictor network. The predictor is trained using data value estimation."
3192,SP:e2c3374629cfd654b7b35e88507e65646d70470e,"ResNet CONJUNCTION DenseNet. DenseNet CONJUNCTION ResNet. ResNet HYPONYM-OF vanilla feedforward. ResNet CONJUNCTION DenseNet. DenseNet CONJUNCTION ResNet. OtherScientificTerm are gradient norm, mean and variance of forward activations, variance of the layer gradient norm, layer gradient norm, and gradient. Method is vanilla feedforward network. ","This paper studies the gradient norm of a vanilla feedforward network (ResNet and DenseNet). The authors show that the mean and variance of forward activations are independent of the variance of the layer gradient norm. The authors also show that if the gradient of a layer is large enough, then the gradient can be bounded."
3193,SP:e2c3374629cfd654b7b35e88507e65646d70470e,"residual and dense net type connections USED-FOR moments of per - layer gradients. random initialization FEATURE-OF moments of per - layer gradients. residual networks CONJUNCTION densely connected networks. densely connected networks CONJUNCTION residual networks. bounds USED-FOR residual networks. bounds USED-FOR densely connected networks. skip connections FEATURE-OF densely connected networks. skip connections FEATURE-OF architectures. Method are duality, and vanilla networks. OtherScientificTerm are square norm of Jacobian, randomness of random initialization, connections, gradients, initialization scales, and gradient norm. ","This paper studies the moments of per-layer gradients with random initialization with residual and dense net type connections. The authors derive bounds for residual networks and densely connected networks with skip connections, and show that the duality holds for vanilla networks. In particular, the authors prove that the square norm of Jacobian does not depend on the randomness of random initialization, but rather on the number of connections. In addition, they show that gradients can be expressed in terms of initialization scales, and that the gradient norm can be written as a function of the width of the network. Finally, they demonstrate that different architectures with different skip connections have different gradients."
3194,SP:4463645f1a9abfbf472935d9eb3342919aa4e0f4,method USED-FOR code optimization. method USED-FOR neural networks. code optimization USED-FOR neural networks. tuning knobs PART-OF code template. cost model USED-FOR reinforcement learning. reinforcement learning USED-FOR tuning knobs. tuning knobs USED-FOR search task. search task USED-FOR it. hardware cost measurements USED-FOR cost model. Method is RL controller. ,"This paper proposes a method for code optimization for neural networks. Specifically, it uses a search task to find tuning knobs in the code template using reinforcement learning with a cost model based on hardware cost measurements. The RL controller is trained on a set of benchmarks."
3195,SP:4463645f1a9abfbf472935d9eb3342919aa4e0f4,"adaptive sampling CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION adaptive sampling. optimizing compiler USED-FOR DNN's. optimizing compiler USED-FOR search of optimal code. adaptive sampling USED-FOR optimizing compiler. reinforcement learning USED-FOR optimizing compiler. adaptive sampling USED-FOR DNN's. reinforcement learning USED-FOR DNN's. PPO USED-FOR code optimization "" search "" policy. K - mean clustering USED-FOR adaptive sampling. adaptive sampling USED-FOR compilation time. K - mean clustering USED-FOR compilation proposals. RL USED-FOR search strategy. search strategy COMPARE random search. random search COMPARE search strategy. RL COMPARE random search. random search COMPARE RL. simulated annealing HYPONYM-OF random search. ","This paper proposes an optimizing compiler that combines adaptive sampling and reinforcement learning to improve the performance of DNN's. The code optimization ""search"" policy is based on PPO. The adaptive sampling is used to reduce the compilation time by using K-mean clustering of the compilation proposals. The proposed search strategy is shown to outperform random search (e.g., simulated annealing)."
3196,SP:df8483206bb88debeb24b04eb31e016368792a84,Gaussian perturbations USED-FOR smoothed classifier. Method is randomized smoothing approach. ,This paper proposes a randomized smoothing approach. The main idea is to use Gaussian perturbations to train a smoothed classifier. The idea is interesting and the experimental results are promising. 
3197,SP:df8483206bb88debeb24b04eb31e016368792a84,top - k predictions COMPARE top-1 predictions. top-1 predictions COMPARE top - k predictions. certifiable bounds FEATURE-OF adversarial perturbations. \ell_2 radius FEATURE-OF adversarial perturbations. certifiable radius FEATURE-OF \ell_2 perturbations. ,This paper provides certifiable bounds on the \ell_2 radius of adversarial perturbations with certifiable radius for top-k predictions compared to top-1 predictions. 
3198,SP:84a83ee258d5bc613b7d73045477018b8a56c56d,"distribution of the per - sample gradients USED-FOR generalization. generalization gap EVALUATE-FOR model. generalization gap EVALUATE-FOR neural network. generalization gap EVALUATE-FOR GSNR quantity. GSNR quantity USED-FOR neural network. gradient descent USED-FOR neural network. DNNs CONJUNCTION gradient descent. gradient descent CONJUNCTION DNNs. shallow models CONJUNCTION learning techniques. learning techniques CONJUNCTION shallow models. DNNs COMPARE shallow models. shallow models COMPARE DNNs. GNSR FEATURE-OF DNNs. random labels USED-FOR GSNR. OtherScientificTerm are per - sample gradients, and Gradient Signal to Noise ratio ( GSNR ). Method is MNIST. ","This paper studies the distribution of the per-sample gradients in order to improve generalization. The authors propose a new quantity called Gradient Signal to Noise ratio (GSNR), which measures the generalization gap between a model and a neural network trained with gradient descent. They compare the GNSR of DNNs and gradient descent with shallow models and learning techniques on MNIST and show that the GSNR can be improved with random labels."
3199,SP:84a83ee258d5bc613b7d73045477018b8a56c56d,one - step generalization ratio HYPONYM-OF quantity. GSNR CONJUNCTION quantity. quantity CONJUNCTION GSNR. OSGR CONJUNCTION GSNR. GSNR CONJUNCTION OSGR. quantity USED-FOR generalization. GSNR USED-FOR neural network. real labels CONJUNCTION random labels. random labels CONJUNCTION real labels. Cifar10 USED-FOR neural network. real labels USED-FOR neural network. relation USED-FOR learning of features. expected gradient CONJUNCTION learning of features. learning of features CONJUNCTION expected gradient. Generic is approximations. ,"This paper proposes a new quantity called one-step generalization ratio (OSGR), which is a combination of GSNR and a quantity that measures generalization. The authors propose approximations of the two quantities. The neural network is trained on Cifar10 with real labels and random labels. The relation between the expected gradient and the learning of features is studied."
3200,SP:fb726f0fea2ed1a009b3aacf74ac149bcf988cdd,Generic is approach. Material is knowledge graphs. Method is Query2Box. OtherScientificTerm is knowledge graph. ,This paper proposes an approach to learning knowledge graphs. The approach is based on the idea of Query2Box. The main idea is to learn a knowledge graph and then use the knowledge graph as the input to query the query box. The paper is well-written and easy to follow.
3201,SP:fb726f0fea2ed1a009b3aacf74ac149bcf988cdd,"method USED-FOR complex logical queries. large incomplete knowledge bases ( KB ) USED-FOR complex logical queries. boxes or hyper - rectangles HYPONYM-OF regions. set based operators USED-FOR logical queries. Generic are it, and queries. Method is logical and, or and existential operator. OtherScientificTerm is vector space. ","This paper proposes a method for solving complex logical queries in large incomplete knowledge bases (KB). The method is called logical and, or and existential operator, and it is based on the idea that logical queries can be expressed as set based operators. The queries are represented as regions (e.g., boxes or hyper-rectangles) in vector space."
3202,SP:c8bbdbf038ddec801c931ae9399b8c16b08428bc,"SGD algorithm COMPARE SGD algorithms. SGD algorithms COMPARE SGD algorithm. consistent gradient estimators USED-FOR SGD algorithm. unbiased gradient estimators USED-FOR SGD algorithms. convergence properties EVALUATE-FOR SGD algorithm. embeddings USED-FOR nodes. learning embeddings USED-FOR graph problems. embeddings USED-FOR downstream task. embeddings USED-FOR node classification. nodes PART-OF graph. learning embeddings USED-FOR graph problems. Consistent gradient estimators USED-FOR graph problems. estimators USED-FOR SGD. OtherScientificTerm are unbiased gradients, neighbours, node, and neighbours - of - neighbours. Method is consistent estimators. ","This paper studies the convergence properties of the SGD algorithm with consistent gradient estimators compared to other SGD algorithms with unbiased gradients. In particular, the authors consider the case where the neighbours of a node are non-convex (i.e., neighbours that are independent of the node) and the neighbours-of-neighbourhoods are not. The authors show that the unbiased gradient estimator of SGD with consistent estimators converges to a solution that has the same convergence properties as the original SGD. They also show that learning embeddings for nodes in a graph can be used for a downstream task, e.g., for node classification, and show that this is a generalization of the work of Chen et al. (2018). The authors also provide a theoretical analysis of the consistency of the estimators used in SGD and provide a proof of the convergence of the consistent estimator in the setting of graph problems. "
3203,SP:c8bbdbf038ddec801c931ae9399b8c16b08428bc,"consistent estimators COMPARE unbiased one. unbiased one COMPARE consistent estimators. learning graph representations USED-FOR problem. Task is stochastic optimization. OtherScientificTerm are consistent gradient, and sample size. Metric is convergence rates. ","This paper studies the problem of stochastic optimization, where the goal is to minimize a consistent gradient. The problem is formulated as learning graph representations, and the authors show that consistent estimators can converge faster than the unbiased one. The authors also show convergence rates when the sample size is small."
3204,SP:d53ee573b8083ecf891d4d560eb8a54c30c5cb3a,"It USED-FOR sub - networks. latency CONJUNCTION memory. memory CONJUNCTION latency. network USED-FOR sub - networks. sub - networks USED-FOR resource constraints. memory HYPONYM-OF resource constraints. latency HYPONYM-OF resource constraints. Method are Once - for - all net, neural network, and retraining. ","This paper proposes a Once-for-all net. It trains sub-networks of the same network to satisfy different resource constraints (latency, memory, etc). The main idea is to train a neural network to be able to adapt to new tasks without retraining."
3205,SP:d53ee573b8083ecf891d4d560eb8a54c30c5cb3a,"architectures USED-FOR specialized resource constraint deployment scenarios. weight sharing USED-FOR small networks. small networks PART-OF large network. prediction based NAS method USED-FOR performance / inference prediction module. Generic are two - step approach, and method. Method is progressive shrinking. OtherScientificTerm is sub architecture. ","This paper proposes a two-step approach to shrink the size of a large network. The first step is progressive shrinking, where small networks are added to the large network via weight sharing. The second step is to learn new architectures for specialized resource constraint deployment scenarios. The proposed method is based on a prediction based NAS method, where a performance/inference prediction module is trained on top of the learned sub architecture."
3206,SP:1be944b5f82d33ab1feb5639792a4c06b8f0c85a,neural module network USED-FOR reading comprehension. symbolic reasoning USED-FOR neural module network. symbolic reasoning USED-FOR reading comprehension. differentiable neural modules USED-FOR reasoning. arithmetics CONJUNCTION sorting. sorting CONJUNCTION arithmetics. differentiable neural modules USED-FOR operations. sorting CONJUNCTION counting. counting CONJUNCTION sorting. counting HYPONYM-OF operations. arithmetics HYPONYM-OF operations. sorting HYPONYM-OF operations. modules USED-FOR complex reasoning. parser supervision CONJUNCTION intermediate output supervision. intermediate output supervision CONJUNCTION parser supervision. information extraction loss CONJUNCTION parser supervision. parser supervision CONJUNCTION information extraction loss. intermediate output supervision HYPONYM-OF auxiliary loss. information extraction loss HYPONYM-OF auxiliary loss. parser supervision HYPONYM-OF auxiliary loss. model COMPARE state - of - the - art models. state - of - the - art models COMPARE model. DROP EVALUATE-FOR model. Generic is module. Method is parser. Task is weak supervision. OtherScientificTerm is auxiliary losses. ,"This paper proposes a neural module network that uses symbolic reasoning to improve reading comprehension. The reasoning is performed with differentiable neural modules that can be used to perform different kinds of operations such as arithmetics, sorting, and counting. The modules can be trained to perform complex reasoning. The module is trained in a way that the parser does not require weak supervision. The auxiliary losses include an information extraction loss, a parser supervision, and an intermediate output supervision. Experiments show that the proposed model can achieve better DROP compared to state-of-the-art models."
3207,SP:1be944b5f82d33ab1feb5639792a4c06b8f0c85a,deep learning approach USED-FOR symbolic reasoning. Neural Module Networks USED-FOR explicit reasoning steps. semantic parsing of the question CONJUNCTION resolution. resolution CONJUNCTION semantic parsing of the question. semantic parsing of the question PART-OF process. resolution PART-OF process. MNMs USED-FOR resolution. BERT pretrained model USED-FOR Auxiliary tasks. model COMPARE SOTA. SOTA COMPARE model. ,This paper presents a deep learning approach for symbolic reasoning. The main idea is to use Neural Module Networks to perform explicit reasoning steps. The process consists of semantic parsing of the question and resolution using MNMs. Auxiliary tasks are performed using a BERT pretrained model. The proposed model is compared with SOTA.
3208,SP:319922e4a316a9b9e76504f806d30ea3bffa3f99,"pruning / sparsification FEATURE-OF randomly initialized neural networks. connection sensitivity HYPONYM-OF metric. metric USED-FOR pruning method. layer - to - layer Jacobian matrices USED-FOR pruned networks. distribution of singular values FEATURE-OF layer - to - layer Jacobian matrices. methods USED-FOR LDI. OtherScientificTerm are layerwise dynamical isometry'( LDI ), dynamical isometry, and signal propagation. Method is sparse networks. ","This paper studies pruning/sparsification of randomly initialized neural networks. The authors propose a new pruning method based on a new metric called 'connection sensitivity', which is based on the 'layerwise dynamical isometry' (LDI). LDI is defined as the distribution of singular values of layer-to-layer Jacobian matrices of pruned networks, which is a special case of the 'dynamic isometry'. The authors show that LDI can be learned using existing methods, and demonstrate that sparse networks can be pruned and sparsified using signal propagation."
3209,SP:319922e4a316a9b9e76504f806d30ea3bffa3f99,"initialization USED-FOR connection - sensitivity - based pruning. normalized magnitude of gradients HYPONYM-OF pruning criterion. connection sensitivity ( CS ) HYPONYM-OF pruning criterion. Jacobians CONJUNCTION gradient. gradient CONJUNCTION Jacobians. distribution property USED-FOR layerwise dynamic isometry. distribution property FEATURE-OF nonlinear network. layerwise dynamic isometry USED-FOR faithful signal propagation. distribution property USED-FOR faithful signal propagation. initialization setup USED-FOR pruning. orthogonal initialization and approximation USED-FOR dynamic isometry. Method are signal propagation theory, and linear networks. OtherScientificTerm are CS, gradients, and orthogonally initial weights. ","This paper studies the initialization for connection-sensitivity-based pruning, which is a popular pruning criterion in signal propagation theory (e.g., normalized magnitude of gradients). In particular, connection sensitivity (CS) is a well-studied and well-motivated pruning criteria. The authors show that CS is a function of the Jacobians and the gradient of the input signal, and that the gradients of a nonlinear network satisfy the same distribution property as the distribution property of a layerwise dynamic isometry, which leads to faithful signal propagation. In contrast to linear networks, the authors propose an initialization setup for pruning with orthogonal initialization and approximation, which allows for more efficient and efficient pruning. In addition, they show that orthogonally initial weights can be used to approximate the dynamics of the dynamics."
3210,SP:d5899cba36329d863513b91c2db57675086abc49,dense layers COMPARE sparsely - connected linear layers. sparsely - connected linear layers COMPARE dense layers. approach USED-FOR topology. sparse layers USED-FOR random weights. Method is sparse network architecture. Material is resource - constrained platforms. OtherScientificTerm is gradient vanishing. ,"This paper proposes a sparse network architecture to address the problem of gradient vanishing in resource-constrained platforms. The authors argue that dense layers are more efficient than sparsely-connected linear layers, and propose an approach to learn a topology that is more robust to gradient vanishing. The main idea is to use sparse layers to learn random weights, which can be used to train a network with sparse network."
3211,SP:d5899cba36329d863513b91c2db57675086abc49,"dense layers COMPARE sparse linear layers. sparse linear layers COMPARE dense layers. Xavier initialization scheme USED-FOR deep stacks of sparse layers. OtherScientificTerm are sparse matrices, dense matrix D, topology of the sparse matrices, fixed topology, and random dense matrix. ","This paper studies deep stacks of sparse layers with sparse matrices. The authors show that dense layers are more efficient than sparse linear layers. The main contribution of this paper is the introduction of the Xavier initialization scheme for deep stacks, where the dense matrix D is a function of the topology of the dense matrices, and the fixed topology can be replaced by a random dense matrix. "
3212,SP:b05a6a0f05dcc63a7e17233f20c49c465c46d194,gated recurrent neural networks USED-FOR signal processing / long term propagation problem. mean field theory USED-FOR signal processing / long term propagation problem. initialization strategy USED-FOR time scale. state - to - state Jacobians USED-FOR time scale. Method is recurrent neural networks. Generic is system. ,This paper studies the signal processing/long term propagation problem in gate-and-fire recurrent neural networks using mean field theory. The authors propose an initialization strategy that scales the time scale based on the state-to-state Jacobians of the system. Experiments are conducted to demonstrate the effectiveness of the proposed system.
3213,SP:b05a6a0f05dcc63a7e17233f20c49c465c46d194,"recurrent neural network ( GRUs CONJUNCTION LSTMs. LSTMs CONJUNCTION recurrent neural network ( GRUs. randomized initializations USED-FOR recurrent neural network ( GRUs. statistical thermodynamics USED-FOR mean field approximations. analyzing signal propagation USED-FOR approach. statistical thermodynamics USED-FOR analyzing signal propagation. statistical thermodynamics USED-FOR approach. toy datasets EVALUATE-FOR approach. Generic are networks, and model. ",This paper studies the problem of training recurrent neural network (GRUs and LSTMs) with randomized initializations. The authors propose an approach based on analyzing signal propagation using statistical thermodynamics to derive mean field approximations. The proposed approach is evaluated on toy datasets and compared to several existing networks. The results show that the proposed model can achieve state-of-the-art performance.
3214,SP:7b65eb83b0d3149f788ab11b1ab9057b440ddd57,solution USED-FOR disease density estimation. satellite scene images USED-FOR disease density estimation. intra - class diversity CONJUNCTION inter - class similarity. inter - class similarity CONJUNCTION intra - class diversity. siamese networks USED-FOR features. siamese networks USED-FOR solution. approach USED-FOR post - classification smoothing. Generic is applications. ,This paper proposes a solution for disease density estimation from satellite scene images. The proposed solution is based on siamese networks that learn features based on intra-class diversity and inter-class similarity. The approach is applied to post-classification smoothing and is shown to be effective in several applications.
3215,SP:7b65eb83b0d3149f788ab11b1ab9057b440ddd57,L2 distance FEATURE-OF features. threshold USED-FOR features. threshold FEATURE-OF L2 distance. average pooling of the feature vectors USED-FOR coupling. method USED-FOR estimating crowding population. estimating crowding population CONJUNCTION diseases density. diseases density CONJUNCTION estimating crowding population. method USED-FOR diseases density. satellite images USED-FOR estimating crowding population. satellite images USED-FOR diseases density. Method is coupled inference. Material is satellite imagery. ,This paper proposes a method for estimating crowding population and diseases density from satellite images using coupled inference. The coupling is based on an average pooling of the feature vectors and a threshold on the L2 distance between the features. Experiments are conducted on satellite imagery.
3216,SP:99c10e038939aa88fc112db10fe801b42360c8dc,pre - trained semantic segmentation network USED-FOR semantically adaptive filters. semantically adaptive filters USED-FOR self - supervised monocular depth estimation. two - stage training heuristic USED-FOR depth estimation. infinite depth values USED-FOR SfM - based supervision framework. two - stage training heuristic USED-FOR dynamic objects. depth estimation USED-FOR dynamic objects. KITTI benchmark EVALUATE-FOR approach. approach COMPARE state - of - the - art. state - of - the - art COMPARE approach. OtherScientificTerm is small apparent motion. ,This paper proposes self-supervised monocular depth estimation with semantically adaptive filters based on a pre-trained semantic segmentation network. The authors propose a SfM-based supervision framework with infinite depth values and a two-stage training heuristic for depth estimation for dynamic objects with small apparent motion. The proposed approach is evaluated on the KITTI benchmark and compared with the state-of-the-art.
3217,SP:99c10e038939aa88fc112db10fe801b42360c8dc,pixel - adaptive convolutions USED-FOR semantic labels. pixel - adaptive convolutions USED-FOR self - supervised monocular depth estimation. semantic labels USED-FOR self - supervised monocular depth estimation. pretrained network USED-FOR semantic features. two - stage training process USED-FOR SfM predictions. networks EVALUATE-FOR method. KITTY dataset EVALUATE-FOR networks. KITTY dataset EVALUATE-FOR method. ,"This paper proposes to use pixel-adaptive convolutions to learn semantic labels for self-supervised monocular depth estimation. The authors propose a two-stage training process for SfM predictions. First, a pretrained network is used to learn the semantic features. Second, the proposed method is evaluated on two networks on the KITTY dataset."
3218,SP:e98ec7fd9c27eabd7f5bf3429f984034c2d355a2,approach USED-FOR attack. Task is data poisoning type of attack. Generic is attacks. Method is classifier. ,This paper studies a data poisoning type of attack. The authors propose a new approach to attack that is based on the observation that most existing attacks do not work well when the classifier is corrupted. 
3219,SP:e98ec7fd9c27eabd7f5bf3429f984034c2d355a2,"certifiable defense USED-FOR data poisoning attacks. randomized smoothing approach USED-FOR certifiable defense. smoothing procedure USED-FOR dataset. noisy datasets USED-FOR classifiers. model USED-FOR classifier. linear regression USED-FOR classifier. pre - trained feature extractor USED-FOR linear regression. pre - trained feature extractor USED-FOR classifier. Generic are technique, and method. ","This paper proposes a randomized smoothing approach for certifiable defense against data poisoning attacks. The proposed technique is based on the observation that classifiers trained on noisy datasets tend to overfit to noisy datasets. The authors propose a smoothing procedure for each dataset, where the smoothed version of the model is used to train a classifier using linear regression with a pre-trained feature extractor. Experiments show the effectiveness of the proposed method."
3220,SP:795cdeb7e4f7285f2c1ac9b9a0fbac3039201ed5,differential privacy ’s stability properties USED-FOR anomaly and backdoor attack detection. expected loss EVALUATE-FOR algorithm. differentially private learning algorithm USED-FOR outlier. expected loss EVALUATE-FOR differentially private learning algorithm. distribution USED-FOR algorithm. differential privacy USED-FOR outliers. differential privacy USED-FOR backdoor attack detection. Generic is assumption. Method is uniformly asymptotic empirical risk minimization. OtherScientificTerm is randomness of the learning algorithm. ,"This paper studies anomaly and backdoor attack detection under differential privacy’s stability properties. The main assumption made in this paper is that uniformly asymptotic empirical risk minimization does not depend on the randomness of the learning algorithm. Under this assumption, the expected loss of a differentially private learning algorithm to detect an outlier under the same distribution as the original algorithm is shown to be polynomial in the number of samples. The paper also shows that differential privacy can be used to detect outliers for backdoor attack Detection."
3221,SP:795cdeb7e4f7285f2c1ac9b9a0fbac3039201ed5,"differential privacy ( DP ) USED-FOR outlier and novelty detection. Differential privacy HYPONYM-OF privacy metric. OtherScientificTerm are poisoned data, outliers, and poisoned samples. Generic is model. ","This paper proposes a new privacy metric called Differential privacy (DP) for outlier and novelty detection. The main idea is to use poisoned data to detect outliers, and to train the model to distinguish between the poisoned samples and the original data."
3222,SP:a5f0e531afd970144169823971d2d039bff752fb,"calibration USED-FOR regression problem. calibration metric USED-FOR regression. metrics USED-FOR calibration. ECE calibration metric USED-FOR classification problem. calibration USED-FOR regression problem. mean square error USED-FOR uncertainty. Generic is metric. OtherScientificTerm are ECE idea, uncertainty bins, network output uncertainty, and reliability diagram. Metric is RMSE. ","This paper proposes a new calibration metric for regression. The proposed metric is based on the ECE calibration metric, which is used in the classification problem. The main idea is to use the mean square error between the uncertainty bins and the network output uncertainty. The authors also propose two metrics for measuring calibration for regression problem: RMSE and reliability diagram."
3223,SP:a5f0e531afd970144169823971d2d039bff752fb,"uncertainty calibration diagnostics CONJUNCTION re - calibration methods. re - calibration methods CONJUNCTION uncertainty calibration diagnostics. re - calibration methods USED-FOR neural network regression. replacement diagnostic USED-FOR uncertainty calibration quality. network USED-FOR parametric distribution. direct "" uncertainty modeling HYPONYM-OF uncertainty prediction schemes. network USED-FOR uncertainty prediction schemes. recalibration method USED-FOR schemes. Generic is diagnostic. OtherScientificTerm are Gaussian, and empirical uncertainty. Method is neural network. ","This paper studies uncertainty calibration diagnostics and re-calibration methods for neural network regression. The authors propose a replacement diagnostic for the uncertainty calibration quality. The diagnostic is based on the fact that the uncertainty prediction schemes (e.g., ""direct"" uncertainty modeling), which use a network to predict a parametric distribution over a Gaussian, are prone to overestimating the empirical uncertainty of the neural network. To address this issue, the authors propose two schemes that use a recalibration method."
3224,SP:c422afd1df1ac98e23235830585dd0d45513064c,fine - tune technique USED-FOR BERT models. BERT models USED-FOR form and content information. fine - tune technique USED-FOR form and content information. textual data USED-FOR form and content information. R and S embeddings PART-OF BERT model. R & S USED-FOR information in text. structural positions CONJUNCTION content - bearing   symbols. content - bearing   symbols CONJUNCTION structural positions. content - bearing   symbols PART-OF positions. Method is structural parsing. ,This paper proposes a fine-tuning technique to improve the performance of BERT models on form and content information extracted from textual data. The key idea is to replace the R and S embeddings in the BERT model with information in text extracted from structural parsing. The structural positions and content-bearing  symbols are extracted from the positions.
3225,SP:c422afd1df1ac98e23235830585dd0d45513064c,one CONJUNCTION other. other CONJUNCTION one. one HYPONYM-OF disentangling layer ( TPR ). attention USED-FOR other. LSTMs USED-FOR one. Generic is layer. ,"This paper proposes a disentangling layer (TPR), which consists of two parts: one based on LSTMs, and the other based on attention. The idea is to disentangle the layer into two parts."
3226,SP:117b19c4163cb3d08eda6bc7af0d48ed815b519e,method USED-FOR hierarchical RL. PPO CONJUNCTION GAE. GAE CONJUNCTION PPO. GAE USED-FOR model. PPO USED-FOR model. multi agent locomotion tasks EVALUATE-FOR model. Method is high - level and low - level controller. OtherScientificTerm is low - level controller. ,"This paper proposes a method for hierarchical RL, where a high-level and low-level controller are jointly trained. The model is based on PPO and GAE, and the model is evaluated on multi agent locomotion tasks. The results show that the proposed method is able to achieve state-of-the-art performance. The authors also provide a theoretical analysis of the dynamics of the high- level controller and the dynamics in the lower-level controllers."
3227,SP:117b19c4163cb3d08eda6bc7af0d48ed815b519e,"collisions CONJUNCTION collaboration. collaboration CONJUNCTION collisions. collisions HYPONYM-OF multi - agent settings. physically simulated environment USED-FOR multi - agent settings. collaboration HYPONYM-OF multi - agent settings. physics simulation of humanoid robots USED-FOR it. Method are multi - agent hierarchical reinforcement learning algorithm, MARL, and hierarchical RL. ","This paper proposes a multi-agent hierarchical reinforcement learning algorithm, MARL, which is based on the idea of hierarchical RL. Specifically, it takes a physics simulation of humanoid robots and applies it to a physically simulated environment to tackle the problem of dealing with two types of multi-agents: collisions and collaboration. "
3228,SP:928640a19b0a0b1e1dc0d1b07cc99e1d51a4d817,"embedding USED-FOR CNN. Euclidean space FEATURE-OF graph. pooling method COMPARE pooling techniques. pooling techniques COMPARE pooling method. real data EVALUATE-FOR pooling method. real data EVALUATE-FOR pooling techniques. method USED-FOR GNNs. Method are graph neural networks, and DeepWalk. OtherScientificTerm are nodes, and local structure. Task is local aggregation of information. ","This paper proposes a new pooling method for training graph neural networks. The key idea is to learn an embedding for each node of a CNN, which maps the graph to a Euclidean space. This embedding is then used to train a CNN that aggregates information from all nodes in the embedding. The authors show that the proposed pooling methods outperform existing pooling techniques on real data. The method is also applied to GNNs and is shown to improve the local aggregation of information. In addition, the authors demonstrate that the method DeepWalk can be applied to a variety of datasets, which shows that the local structure of the data can be improved."
3229,SP:928640a19b0a0b1e1dc0d1b07cc99e1d51a4d817,"node attributes PART-OF graph. layers of graph convolution USED-FOR node attributes. layers of graph convolution USED-FOR graph neural network. sampling USED-FOR median based quantization. Method are graph embedding algorithm, and graph convolution. OtherScientificTerm is embedding. ",This paper proposes a new graph embedding algorithm. The key idea is to train a graph neural network with multiple layers of graph convolution to capture node attributes in the graph. The authors propose to use median based quantization based on sampling from the embedding. The experiments show the effectiveness of the proposed method.
3230,SP:465adf302cd8b7e6b449271a91d1d2fad844aa4d,"F pooling USED-FOR Frequency Pooling. anti - aliasing properties FEATURE-OF pooling operation. discrete Fourier transform ( DFT ) USED-FOR spectrum domain. inverse DFT USED-FOR time domain. FFT CONJUNCTION auto differentiation frameworks. auto differentiation frameworks CONJUNCTION FFT. FFT USED-FOR method. auto differentiation frameworks USED-FOR method. method COMPARE models. models COMPARE method. CIFAR-100 FEATURE-OF Resnet / Desnet. Resnet / Desnet EVALUATE-FOR method. Material are 1D/2D signal, and ImageNet. OtherScientificTerm is high - frequencies. ","This paper proposes Frequency Pooling with F pooling, which is a variant of Frequency Pooled. The proposed pooling operation has anti-aliasing properties. The authors propose a discrete Fourier transform (DFT) for the spectrum domain and an inverse DFT for the time domain. The method is based on FFT and auto differentiation frameworks. Experiments are conducted on Resnet/Desnet on CIFAR-100 and ImageNet and show that the proposed method outperforms existing models. The main contribution of the paper is to propose to pool a 1D/2D signal into a sequence of high-frequency."
3231,SP:465adf302cd8b7e6b449271a91d1d2fad844aa4d,convolutional neural networks ( CNN ) USED-FOR image classification. pooling operation PART-OF convolutional neural networks ( CNN ). frequency pooling ( F - pooling ) HYPONYM-OF pooling operation. F - pooling USED-FOR orignal signal. convolutional neural networks USED-FOR image classifiation. CIFAR-100 CONJUNCTION ImageNet dataset. ImageNet dataset CONJUNCTION CIFAR-100. F - pooling COMPARE convolutional neural networks. convolutional neural networks COMPARE F - pooling. ImageNet dataset EVALUATE-FOR convolutional neural networks. CIFAR-100 EVALUATE-FOR convolutional neural networks. CIFAR-100 USED-FOR image classifiation. matrix multiplications USED-FOR F - pooling. Task is signal processing. OtherScientificTerm is shift - equivalent functions. ,"This paper proposes a new pooling operation in convolutional neural networks (CNN) for image classification, called frequency pooling (F-pooling). F-pools the orignal signal to reduce the variance in signal processing. The authors propose to use shift-equivalent functions and show that F-piling with matrix multiplications can improve the performance of convolutionally neural networks for image classifiation on CIFAR-100 and ImageNet dataset."
3232,SP:77f0f3779f9bdeb75ea5744ab494942a4943117b,data augmentation USED-FOR policies. randomly parameterized convolutional layer USED-FOR data augmentation. dropout CONJUNCTION L2 regularization. L2 regularization CONJUNCTION dropout. L2 regularization CONJUNCTION batch normalization. batch normalization CONJUNCTION L2 regularization. batch normalization CONJUNCTION policy gradient method. policy gradient method CONJUNCTION batch normalization. RL benchmarks EVALUATE-FOR PPO. PPO HYPONYM-OF regularization techniques. batch normalization HYPONYM-OF regularization techniques. dropout HYPONYM-OF regularization techniques. L2 regularization HYPONYM-OF regularization techniques. it COMPARE methods. methods COMPARE it. it USED-FOR visual bias. it USED-FOR computer vision problem. visual bias PART-OF computer vision problem. OtherScientificTerm is observation spaces. Generic is method. ,"This paper proposes to use data augmentation to train policies using a randomly parameterized convolutional layer. The method is motivated by the observation spaces and the fact that existing regularization techniques such as dropout, L2 regularization, batch normalization, and policy gradient method fail to generalize well to new observation spaces. Experiments on several RL benchmarks demonstrate the effectiveness of PPO. In addition, it is shown that it can reduce visual bias in the computer vision problem compared to existing methods."
3233,SP:77f0f3779f9bdeb75ea5744ab494942a4943117b,random convolutions USED-FOR deep RL agents. random convolutions USED-FOR observation space. loss term USED-FOR features of perturbed and unperturbed observations. loss term USED-FOR learning of invariant features. method COMPARE regularization. regularization COMPARE method. OtherScientificTerm is invariant features. ,This paper studies the problem of training deep RL agents with random convolutions in the observation space. The authors propose a new loss term that penalizes the features of perturbed and unperturbed observations to encourage the learning of invariant features. Experiments show that the proposed method outperforms other forms of regularization.
3234,SP:31772a9122ec998c7c829bc4813f6147cdc30145,Salient Attributes USED-FOR Network Explanation ( SANE ). method USED-FOR image similarity models explanation. them CONJUNCTION saliency map. saliency map CONJUNCTION them. OtherScientificTerm is similarity score. ,"This paper proposes a method for image similarity models explanation, called Network Explanation (SANE) based on Salient Attributes. The key idea is to combine them with the saliency map, which is a similarity score between the source and target images. "
3235,SP:31772a9122ec998c7c829bc4813f6147cdc30145,saliency map generator CONJUNCTION attribute predictor. attribute predictor CONJUNCTION saliency map generator. approach USED-FOR image similarity models. method USED-FOR attribute. saliency map CONJUNCTION attribute activations. attribute activations CONJUNCTION saliency map. image similarity FEATURE-OF saliency map. SANE USED-FOR training. ,This paper proposes an approach to improve image similarity models by combining a saliency map generator and an attribute predictor. The proposed method predicts an attribute based on a combination of the saliency of the image similarity between the image and the attribute. The training is based on SANE.
3236,SP:50f9dcac485552f2925839151da4dd8d82e35fcc,2D COMPARE 1D representation. 1D representation COMPARE 2D. 2D USED-FOR normalising flow. reformulation USED-FOR one. Task is text - to - speech synthesis. Generic is approach. ,This paper addresses the problem of text-to-speech synthesis. The authors propose a new approach to normalising flow in 2D instead of 1D representation. The main idea is to use 2D as the normalising flows instead of the original 1D. The proposed one is based on a reformulation of the one proposed in [1]. 
3237,SP:50f9dcac485552f2925839151da4dd8d82e35fcc,2 - D matrix USED-FOR high dimensional 1 - D raw waveform. method USED-FOR autoregressive flow. row dimension USED-FOR Autoregressive flow. OtherScientificTerm is Log - likelihood. Material is high - fidelity speech. ,This paper proposes a method for learning autoregressive flow with row dimension in the form of a 2-D matrix for a high dimensional 1-D raw waveform. Log-likelihood is used to estimate the likelihood of the input. The method is applied to high-fidelity speech.
3238,SP:963e85369978dddcd9e3130bc11453696066bbf3,graph translation USED-FOR graph translator. adversarial training framework USED-FOR graph translator. discriminator USED-FOR translated graph. discriminator USED-FOR translator. approach COMPARE baselines. baselines COMPARE approach. Material is synthetic and real - world datasets. ,This paper proposes an adversarial training framework for graph translator for graph translation. The key idea is to train a discriminator to discriminate between the original and translated graph. The proposed approach is evaluated on both synthetic and real-world datasets and compared with several baselines.
3239,SP:963e85369978dddcd9e3130bc11453696066bbf3,"deep generative models USED-FOR graphs. graph translator CONJUNCTION conditional graph discriminator. conditional graph discriminator CONJUNCTION graph translator. conditional graph discriminator PART-OF architecture. graph translator PART-OF architecture. GAN approach USED-FOR conditional distribution. graph encoders USED-FOR graph translator. encoder - decoder COMPARE encoder / decoders. encoder / decoders COMPARE encoder - decoder. computational complexities EVALUATE-FOR methods. graph characteristics USED-FOR architecture. method COMPARE state - of - the - art. state - of - the - art COMPARE method. Method is deep graph generative model. Metric are scalability, and computational complexity. OtherScientificTerm is hacker detection. ","This paper proposes a deep graph generative model that aims to improve the scalability and reduce the computational complexity of existing deep generative models for graphs. The proposed architecture consists of a graph translator and a conditional graph discriminator. The graph translator is based on existing graph encoders, while the conditional distribution is modeled using a GAN approach. The authors compare the performance of the encoder-decoder with other encoder/decoders and show that the proposed methods have similar computational complexities, but the proposed method is more scalable than the state-of-the-art. The main contribution of this paper is the introduction of a new architecture that takes into account the graph characteristics. The paper also provides a theoretical analysis of the impact of the proposed architecture on the vulnerability of the network to hacker detection."
3240,SP:962caffd236630c4079bfc7292403c1cc6861c3b,sorting CONJUNCTION tree traversal. tree traversal CONJUNCTION sorting. gating functions USED-FOR recurrent neural networks. recurrent neural networks CONJUNCTION feed - forward layers of Transformer. feed - forward layers of Transformer CONJUNCTION recurrent neural networks. it USED-FOR tasks. gating functions USED-FOR feed - forward layers of Transformer. toy tasks CONJUNCTION tasks. tasks CONJUNCTION toy tasks. it USED-FOR toy tasks. tasks HYPONYM-OF tasks. toy tasks HYPONYM-OF tasks. machine translation HYPONYM-OF tasks. sorting HYPONYM-OF toy tasks. tree traversal HYPONYM-OF toy tasks. data - driven function USED-FOR depth of recursion. LSTM CONJUNCTION Transformer. Transformer CONJUNCTION LSTM. approach COMPARE vanilla LSTM. vanilla LSTM COMPARE approach. approach USED-FOR Transformer. approach USED-FOR LSTM. approach COMPARE Transformer. Transformer COMPARE approach. Transformer COMPARE Transformer. Transformer COMPARE Transformer. Transformer COMPARE vanilla LSTM. vanilla LSTM COMPARE Transformer. vanilla LSTM CONJUNCTION Transformer. Transformer CONJUNCTION vanilla LSTM. OtherScientificTerm is gating function. ,"This paper proposes to use gating functions to train recurrent neural networks and feed-forward layers of Transformer, and apply it to a variety of tasks, including toy tasks (sorting, tree traversal, etc.), as well as tasks like machine translation. The key idea is to use a data-driven function to estimate the depth of recursion, which is then used to train the gating function. The proposed approach is compared to LSTM and Transformer."
3241,SP:962caffd236630c4079bfc7292403c1cc6861c3b,METAGROSS HYPONYM-OF neural sequence modelling unit. unit USED-FOR recursive parametrization of   gating functions. gated RNN paradigm USED-FOR recursive parametrization of   gating functions. gated - RNNs USED-FOR vanishing gradient problems. gated - RNNs USED-FOR learning long - range dependencies in sequences. hierarchically - structured data USED-FOR learning. inductive bias USED-FOR tasks. configuration USED-FOR inductive bias. Generic is method. OtherScientificTerm is parametrization. ,"This paper proposes METAGROSS, a neural sequence modelling unit. The proposed unit learns recursive parametrization of  gating functions using the gated RNN paradigm. The method is motivated by the fact that gated-RNNs have been shown to be effective in solving vanishing gradient problems, as well as in learning long-range dependencies in sequences. The authors also point out that learning with hierarchically-structured data is challenging, and propose a configuration that allows for inductive bias to be applied to these tasks. Experiments are conducted to demonstrate the effectiveness of the proposed parametrized functions."
3242,SP:d03aa0318f0d24a5b7c7817dfc7fba47ebec11cd,self - supervised objective USED-FOR speech recognition. local prior matching ( LMP ) USED-FOR speech recognition. local prior matching ( LMP ) HYPONYM-OF self - supervised objective. unlabeled speech data USED-FOR approach. supervised pretrained model USED-FOR it. unlabeled data USED-FOR it. WER EVALUATE-FOR LPM. Method is self - supervised approach. ,"This paper proposes a self-supervised objective called local prior matching (LMP) for speech recognition. The approach is based on unlabeled speech data, and it is trained with a supervised pretrained model. Experiments show that LPM can achieve better WER than the state-of-the-art. The paper also provides a theoretical analysis of the proposed self -supervised approach."
3243,SP:d03aa0318f0d24a5b7c7817dfc7fba47ebec11cd,ASRs USED-FOR distillation approach. model CONJUNCTION student model. student model CONJUNCTION model. supervised data USED-FOR model. student model HYPONYM-OF ASRs. model HYPONYM-OF ASRs. Librispeech data USED-FOR unsupervised data. Librispeech data USED-FOR ASR models. Librispeech LM data USED-FOR LM. approach COMPARE baseline model. baseline model COMPARE approach. Librispeech subset USED-FOR baseline model. Method is distillation. ,"This paper proposes a distillation approach based on ASRs, i.e., a model trained with supervised data and a student model trained on unsupervised data. The main idea is to use ASR models trained on Librispeech data as a substitute for the unstructured data. This distillation is done in two steps: 1) distill the LM from the LibrisPEech LM data and 2) train the LM on top of the original LM. Experiments show that the proposed approach outperforms the baseline model trained only on the original and the new version of the Libripeech subset."
3244,SP:e6af249608633f1776b608852a00946a5c09a357,method USED-FOR classifier. fairness criterion CONJUNCTION robustness. robustness CONJUNCTION fairness criterion. robustness FEATURE-OF data poisoning. accuracy CONJUNCTION fairness. fairness CONJUNCTION accuracy. method COMPARE classifiers. classifiers COMPARE method. inaccuracy FEATURE-OF poisoned data. accuracy EVALUATE-FOR classifiers. fairness EVALUATE-FOR classifiers. inaccuracy EVALUATE-FOR method. fairness EVALUATE-FOR method. Material is synthetic and real benchmark data sets. ,This paper proposes a method to train a classifier that is robust to data poisoning. The authors propose a fairness criterion and a robustness against data poisoning based on the observation that poisoned data can lead to higher inaccuracy. The method is evaluated on both synthetic and real benchmark data sets and compared to other classifiers in terms of accuracy and fairness.
3245,SP:e6af249608633f1776b608852a00946a5c09a357,adversarial fair training CONJUNCTION adversarial robust training. adversarial robust training CONJUNCTION adversarial fair training. classifier CONJUNCTION adversaries. adversaries CONJUNCTION classifier. Material is clean hold - out dataset. ,This paper proposes to combine adversarial fair training with adversarial robust training. The idea is to train a classifier and adversaries on the same clean hold-out dataset. The paper is well-written and well-motivated.
3246,SP:6306417f5a300629ec856495781515c6af05a363,"method USED-FOR point - based learning. hybrid Eulerian - Lagrangian fluid simulation method USED-FOR method. MLPs USED-FOR particle based features. simulation algorithm USED-FOR learning problem. MLP USED-FOR particle based velocity. OtherScientificTerm are Eulerian grid, grid, grid quantities, and particles. Task is classification task. Method is flow solver. ","This paper proposes a method for point-based learning based on a hybrid Eulerian-Lagrangian fluid simulation method. Specifically, the learning problem is formulated as a simulation algorithm where each particle is represented as a grid and the grid quantities are sampled from a flow solver. The particle based velocity is modeled as an MLP and the particle based features are modeled as MLPs. The classification task is performed by sampling from the particles."
3247,SP:6306417f5a300629ec856495781515c6af05a363,PIC / FLIP scheme USED-FOR learning problem of 3D object detection and segmentation. PIC / FLIP scheme USED-FOR Computational Fluid Dynamics. GCN(graph convolutions ) CONJUNCTION Point nets. Point nets CONJUNCTION GCN(graph convolutions ). Eulerian formulation USED-FOR data representation. intrinsic CNNs USED-FOR localized neighborhood information. extrinsic CNNs USED-FOR global features. GCN(graph convolutions ) HYPONYM-OF intrinsic CNNs. Point nets HYPONYM-OF intrinsic CNNs. Vox net HYPONYM-OF extrinsic CNNs. extrinsic CNNs CONJUNCTION intrinsic CNNs. intrinsic CNNs CONJUNCTION extrinsic CNNs. PIC / FLIP scheme USED-FOR CFD. PIC / FLIP scheme USED-FOR problem. Method is Lagrangian formulation. ,"This paper proposes a PIC/FLIP scheme for the learning problem of 3D object detection and segmentation, which is inspired by Computational Fluid Dynamics. The main idea is to use an Eulerian formulation of the data representation, instead of the Lagrangian formulation used in previous work. The authors show that intrinsic CNNs such as GCN(graph convolutions) and Point nets can capture localized neighborhood information, while extrinsic CNNs like Vox net are able to capture global features. The experiments show that the proposed PIC / FLIP scheme can improve the performance of CFD and can be applied to any problem."
3248,SP:0561a2174d7334e078a49ae8859a36e4d74f9b5b,robustness properties FEATURE-OF gradient clipping. Gradient clipping USED-FOR privacy preserving. Gradient clipping HYPONYM-OF optimization technique. gradient clipping USED-FOR label noise. variant USED-FOR classification calibration. gradient clipping USED-FOR variant. label noise USED-FOR variant. ,"This paper studies the robustness properties of gradient clipping. Gradient clipping is an optimization technique that aims at privacy preserving. In this paper, the authors propose a variant that uses gradient clipping to remove label noise for classification calibration."
3249,SP:0561a2174d7334e078a49ae8859a36e4d74f9b5b,gradient clipping USED-FOR stochastic gradient descent. gradient clipping CONJUNCTION robustness. robustness CONJUNCTION gradient clipping. label noise FEATURE-OF robustness. gradient clipping USED-FOR symmetric label noise. label noise robustness EVALUATE-FOR gradient clipping ( cl - clipping ). synthetic datasets CONJUNCTION classification benchmarks. classification benchmarks CONJUNCTION synthetic datasets. ,"This paper studies the effect of gradient clipping (cl-clipping) on stochastic gradient descent. The main contribution of this paper is to study the relationship between gradient clipping and robustness to label noise. The authors show that gradient clipping is effective for symmetric label noise, and that the label noise robustness is improved with gradient clipping. The experiments are conducted on synthetic datasets and classification benchmarks."
3250,SP:414b06d86e132357a54eb844036b78a232571301,"imitation learning method USED-FOR state distributions. imitator dynamics COMPARE expert dynamics. expert dynamics COMPARE imitator dynamics. state distributions COMPARE state - action distributions. state - action distributions COMPARE state distributions. VAE CONJUNCTION inverse dynamics model. inverse dynamics model CONJUNCTION VAE. expert demonstrations USED-FOR VAE. Wasserstein distance FEATURE-OF trajectory distributions. objectives USED-FOR method. method COMPARE inverse reinforcement learning and behavior cloning approaches. inverse reinforcement learning and behavior cloning approaches COMPARE method. OtherScientificTerm are local objective, imitator, and imitator and expert dynamics. Task is global alignment of states. ","This paper proposes an imitation learning method to learn state distributions that are closer to the imitator dynamics than to the expert dynamics. Specifically, the authors train a VAE on expert demonstrations and an inverse dynamics model, where the local objective is to minimize the Wasserstein distance between the trajectory distributions of the imimator and the expert. The authors then use these objectives to train the method and compare the proposed method with inverse reinforcement learning and behavior cloning approaches. The main contribution of this paper is the global alignment of states, which is an important problem in practice."
3251,SP:414b06d86e132357a54eb844036b78a232571301,dynamics CONJUNCTION dynamics of the imitator. dynamics of the imitator CONJUNCTION dynamics. global alignment PART-OF approach. state - predictive VAE and inverse dynamics model USED-FOR approach. approach COMPARE imitation learning methods. imitation learning methods COMPARE approach. method USED-FOR dynamics. Task is imitation learning. OtherScientificTerm is local alignment. ,This paper addresses the problem of imitation learning where the dynamics and dynamics of the imitator are different. The authors propose an approach that combines global alignment with local alignment. The approach is based on a state-predictive VAE and inverse dynamics model. Experiments show that the proposed approach outperforms other imitation learning methods in terms of dynamics.
3252,SP:91761d68086330ce378507c152e72218ed7b2196,extention method USED-FOR SGD. pseudo - residual targets USED-FOR gradient boosting problem. extention method USED-FOR deep gradient boosting ( DGB ). deep gradient boosting ( DGB ) HYPONYM-OF SGD. DGB USED-FOR real CNNs. input normalization layer USED-FOR DGB. model COMPARE model. model COMPARE model. CIFAR-10 CONJUNCTION ImageNet recognition. ImageNet recognition CONJUNCTION CIFAR-10. normalization layer USED-FOR CNNs. model COMPARE BN. BN COMPARE model. BN USED-FOR model. CIFAR-10 EVALUATE-FOR BN. ImageNet recognition EVALUATE-FOR model. CIFAR-10 EVALUATE-FOR model. ImageNet recognition EVALUATE-FOR model. CIFAR-10 EVALUATE-FOR model. Method is back - propagation procedure. OtherScientificTerm is convolution kernels. ,"This paper proposes an extension method to SGD called deep gradient boosting (DGB), which is a variant of SGD that uses pseudo-residual targets to solve the gradient boosting problem. DGB can be applied to real CNNs by adding an additional input normalization layer to the DGB. The authors also propose a back-propagation procedure to ensure that the convolution kernels are close to each other. Experiments on CIFAR-10 and ImageNet recognition show that the proposed model outperforms BN."
3253,SP:91761d68086330ce378507c152e72218ed7b2196,idea USED-FOR back propagated gradients. chain rule USED-FOR pseudo residual targets. chain rule USED-FOR gradient boosting problem. pseudo residual targets USED-FOR gradient boosting problem. chain rule USED-FOR idea. chain rule USED-FOR back propagated gradients. boosting problem USED-FOR weight update. linear base learner USED-FOR boosting problem. diagonal terms PART-OF matrix inversion. computational cost EVALUATE-FOR boosting problem. ,"This paper proposes an idea to compute back propagated gradients using a chain rule to generate pseudo residual targets for the gradient boosting problem. The boosting problem is formulated as a linear base learner, where the diagonal terms of the matrix inversion are used to compute the weight update. The authors show that the boosting problem can reduce the computational cost by a factor of 2."
3254,SP:7709a8b907c5642479e7b6fb0b362efc4ead63ce,DARTS HYPONYM-OF neural architecture search ( NAS ) method. DARTS USED-FOR extension. extension USED-FOR DARTS. method USED-FOR edges. Metric is memory cost. OtherScientificTerm is random subset. ,"This paper proposes an extension to DARTS, a neural architecture search (NAS) method. The main idea is to reduce the memory cost by adding a random subset of edges to the training set. The method is trained to find edges that are close to the target architecture."
3255,SP:7709a8b907c5642479e7b6fb0b362efc4ead63ce,training efficiency EVALUATE-FOR DARTS. channels USED-FOR connection. edge normalization USED-FOR optimization problem. CIFAR-10 CONJUNCTION IamgeNet. IamgeNet CONJUNCTION CIFAR-10. ImageNet EVALUATE-FOR approach. CIFAR-10 EVALUATE-FOR approach. IamgeNet EVALUATE-FOR approach. OtherScientificTerm is memory and computing overheads. Generic is components. Method is Partial channel connection. ,"This paper aims to improve the training efficiency of DARTS by reducing the memory and computing overheads. The authors propose two components: (1) Partial channel connection, where the channels are used to train the connection, and (2) edge normalization to solve the optimization problem. The proposed approach is evaluated on ImageNet, CIFAR-10, and IamgeNet."
3256,SP:724870046e990376990ba9f73d63d331f61788d7,"loss function USED-FOR critic training. critic training USED-FOR DDPG. model USED-FOR gradients. Task is optimal control problems. OtherScientificTerm are gradient of the dynamics, prediction error, and gradient. ","This paper studies optimal control problems where the goal is to minimize the gradient of the dynamics. The authors propose a new loss function for critic training in DDPG. The main idea is to use the prediction error as a measure of the difference between the gradients of the model and the true gradient, which is then used as the loss function in critic training. "
3257,SP:724870046e990376990ba9f73d63d331f61788d7,"RL method USED-FOR continuous control problems. DDPG HYPONYM-OF RL method. It COMPARE model - based approaches. model - based approaches COMPARE It. differentiable models of the dynamics USED-FOR it. MPC HYPONYM-OF model - based approaches. method USED-FOR DDPG. simulator gradients USED-FOR method. Bellman error objective USED-FOR critic. OtherScientificTerm are dynamics, critic values, and gradients of the critic. ",This paper proposes a new RL method for continuous control problems called DDPG. It is based on differentiable models of the dynamics and is compared to other model-based approaches such as MPC. The method is trained on simulator gradients and is able to solve the problem of solving the DDPGs in a supervised manner. The critic is trained using a Bellman error objective and the dynamics are learned by minimizing the difference between the critic values and the gradients of the critic.
3258,SP:be0202a28bcca68edb0abe4d1c0ba1af265211e3,"approach USED-FOR hierarchical reinforcement learning approach. recurrent VAE USED-FOR graph decomposition of the state space. hidden state CONJUNCTION hidden state+observation. hidden state+observation CONJUNCTION hidden state. hard Kumaraswamy distribution FEATURE-OF approximate posterior. hidden state USED-FOR decoder. nodes PART-OF world graph. high - level policy CONJUNCTION classical planning. classical planning CONJUNCTION high - level policy. OtherScientificTerm are graph, binary variable, and sparse reward. Generic are algorithm, and reconstruction. Method are recurrent binary VAE, and inference network. ","This paper presents an approach to a hierarchical reinforcement learning approach. The algorithm is based on a recurrent VAE that learns a graph decomposition of the state space, where each node in the graph is associated with a binary variable, and the goal is to learn a high-level policy that can be combined with classical planning. The main idea of the algorithm is to use a recurrent binary VAE, where the approximate posterior is a hard Kumaraswamy distribution over the hidden state and hidden state+observation of the decoder. The inference network is trained to reconstruct the nodes in the world graph, and then the reconstruction is repeated until the sparse reward is reached."
3259,SP:be0202a28bcca68edb0abe4d1c0ba1af265211e3,approach USED-FOR waypoint states. states PART-OF hierarchical RL approach. RL domains FEATURE-OF waypoint states. binary latent variable VAE USED-FOR waypoint states. HRL algorithm USED-FOR large RL domains. policies COMPARE baseline approaches. baseline approaches COMPARE policies. grid world tasks EVALUATE-FOR policies. grid world tasks EVALUATE-FOR baseline approaches. ,This paper proposes a new approach to learn waypoint states in RL domains by combining states from a hierarchical RL approach. The authors propose a binary latent variable VAE to model the waypoints. The proposed HRL algorithm can be applied to large RL domains. Experiments on grid world tasks show that the proposed policies outperform baseline approaches.
3260,SP:e8a3a0f77dab336ce50c9dc941f7350173916e04,"identifying concise equations USED-FOR functional relations. selection layer USED-FOR sparse connections. sparse connections PART-OF network. OtherScientificTerm are concise equations, and base functions. Generic are functions, and method. ",This paper focuses on identifying concise equations for functional relations. The authors propose to use sparse connections in the network as a selection layer to select sparse connections that are close to the base functions. These functions are then used to find the sparse connections. Experimental results show the effectiveness of the proposed method.
3261,SP:e8a3a0f77dab336ce50c9dc941f7350173916e04,White Box Network ( WBN ) USED-FOR function blocks. selection layer USED-FOR function block. function block HYPONYM-OF neural network. MNIST and CIFAR classification tasks EVALUATE-FOR positive transfer. PathNet style setting USED-FOR neural network. OtherScientificTerm is function priors. ,"This paper proposes a White Box Network (WBN) to learn function blocks. The neural network is a function block with a selection layer, and the function priors are learned in a PathNet style setting. Experiments on MNIST and CIFAR classification tasks demonstrate positive transfer."
3262,SP:b7f4fda6497a1c20fd57f029be5f1b2e2780e227,expert policy USED-FOR GCSL. control problems EVALUATE-FOR approach. OtherScientificTerm is goal conditioned policies. Generic is algorithm. Method is goal - conditioned policy. ,"This paper studies the problem of learning goal conditioned policies. The authors propose an algorithm called GCSL, where the goal-conditioned policy is trained to maximize the probability that the expert policy will converge to a goal. The approach is evaluated on several control problems."
3263,SP:b7f4fda6497a1c20fd57f029be5f1b2e2780e227,imitation learning USED-FOR method. policy USED-FOR state - action pairs. method USED-FOR goal - achieving tasks. Task is RL environment. Generic is algorithm. ,"This paper proposes a method based on imitation learning to learn to solve tasks in an RL environment. Specifically, the algorithm learns a policy to predict state-action pairs, and then uses this policy to solve goal-achieving tasks. The method is applied to a variety of goal-accomplishing tasks."
3264,SP:1c7cf7417825208feac9fe3b3488a51ad1e72270,"Byzantine failures FEATURE-OF security of distributed optimization algorithm. parameter - server USED-FOR asynchronous SGD algorithm. It USED-FOR scenario. repeated and unbounded Byzantine failures FEATURE-OF scenario. Method are distributed optimization algorithm, neural network models, and stochastic line search ideas. Generic is failures. OtherScientificTerm is gradients. ","This paper studies the problem of security of distributed optimization algorithm under Byzantine failures. It considers a scenario with repeated and unbounded Byzantine failures, where the parameter-server of an asynchronous SGD algorithm fails to converge to the optimal solution. The authors show that these failures are not caused by the neural network models, but by the stochastic line search ideas. They also show that the gradients of the server can be corrupted."
3265,SP:1c7cf7417825208feac9fe3b3488a51ad1e72270,"Zeno++ USED-FOR SGD. Zeno++ server USED-FOR worker gradients. reference ” gradient USED-FOR Zeno++ server. reference ” gradient USED-FOR worker gradients. Method is distributed asynchronous SGD. OtherScientificTerm are Byzantine failures, gradients, and worker gradient. ","This paper studies distributed asynchronous SGD. The authors propose to use Zeno++ for SGD, which is a generalization of SGD with Byzantine failures. The main idea is to use a “reference” gradient to update the worker gradients of the Zeno++) server. The gradients are computed by computing the difference between the worker gradient of the server and the reference gradient. "
3266,SP:d16ed9bd4193d99774840783347137e938955b87,cAdv HYPONYM-OF unrestricted adversarial attack methods. texture FEATURE-OF image. unrestricted adversarial attack methods USED-FOR texture. unrestricted adversarial attack methods USED-FOR image. optimization objectives USED-FOR adversarial examples. parametrized colorization techniques CONJUNCTION texture transfer method. texture transfer method CONJUNCTION parametrized colorization techniques. methods COMPARE defense methods. defense methods COMPARE methods. methods COMPARE transferrable accross models. transferrable accross models COMPARE methods. defense methods CONJUNCTION transferrable accross models. transferrable accross models CONJUNCTION defense methods. Method is semantic technique. Task is user study. OtherScientificTerm is C&W attack. ,"This paper proposes a semantic technique, called cAdv, to extract the texture of an image using unrestricted adversarial attack methods (e.g., cAdv) that aim to extract texture from an image. The main contribution of the paper is a user study that compares the performance of the proposed methods with existing defense methods and transferrable accross models. The paper also proposes a set of optimization objectives to generate adversarial examples that are more robust to the C&W attack. Experiments are conducted using parametrized colorization techniques and a texture transfer method."
3267,SP:d16ed9bd4193d99774840783347137e938955b87,generating adversarial examples HYPONYM-OF adversarial attacks. one HYPONYM-OF adversarial attacks. other HYPONYM-OF adversarial attacks. network weights PART-OF pre - trained colourisation network. cross - entropy EVALUATE-FOR one. network weights USED-FOR cross - entropy. cross - entropy CONJUNCTION loss. loss CONJUNCTION cross - entropy. loss USED-FOR texture differences. loss EVALUATE-FOR latter. cross - entropy EVALUATE-FOR latter. OtherScientificTerm is textures of the original images. Method is classifier. ,"This paper proposes two adversarial attacks, one based on generating adversarial examples, and the other based on changing the network weights of a pre-trained colourisation network. The first one is based on reducing the cross-entropy of the original images, while the latter is motivated by reducing the texture differences between the original and the generated images. The idea is that by changing the weights of the classifier, the texture of the generated image can be further reduced."
3268,SP:f4f7dd96b7865fe2d4c6bddf82875f0c9377c3b4,"reinforcement learning agent USED-FOR external memory. method USED-FOR reinforcement learning agent. method USED-FOR few - shot setting. Method are Learning to Control ( LTC ), and dense - sparse memory design. OtherScientificTerm is plasticity. ","This paper proposes Learning to Control (LTC), a method to train a reinforcement learning agent to learn external memory. The method is applied to the few-shot setting, where the goal is to improve the plasticity of the learned representations. The authors propose a dense-sparse memory design to achieve this goal."
3269,SP:f4f7dd96b7865fe2d4c6bddf82875f0c9377c3b4,sparse memory USED-FOR memory network architecture. memory entry USED-FOR policy pi_theta. policy pi_theta USED-FOR memory. policy gradient USED-FOR policy pi_theta. meta - learning setting FEATURE-OF online NER task. online NER task EVALUATE-FOR model. OtherScientificTerm is policy's certainty. Metric is entopy. ,"This paper proposes a memory network architecture with sparse memory. The memory is stored in a policy pi_theta, which is trained to maximize the policy's certainty. Each memory entry is used to update the policy pi_{theta} using the policy gradient. The model is evaluated on an online NER task in a meta-learning setting, where the entopy is evaluated."
3270,SP:f4f7dd96b7865fe2d4c6bddf82875f0c9377c3b4,"nearest neighbor memory address USED-FOR generalization. memory augmented networks USED-FOR few - shot learning. LRU - like procedure USED-FOR memory. RNN approach USED-FOR NER. Memory Augmented Networks CONJUNCTION Matching Networks. Matching Networks CONJUNCTION Memory Augmented Networks. few - shot learning USED-FOR NER. approach USED-FOR RNN approach. Method are memory address mapping, and policy - gradient RL. OtherScientificTerm are memory key, memory locations, action space, reward, and memory address assignment distribution. ","This paper proposes to use nearest neighbor memory address as a proxy for generalization. The memory address mapping is an extension of memory augmented networks for few-shot learning. In particular, the memory is mapped to an LRU-like procedure, where the memory key is sampled from the memory locations of the current state and action space. The authors show that this approach can be used to improve the RNN approach for NER with few -shot learning, which is similar to policy-gradient RL. The main difference is that the reward is learned from a memory address assignment distribution instead of the original one. The paper also shows that Memory Augmented Networks and Matching Networks can be combined to improve performance."
3271,SP:2e9235485b79d0b22ec8b565b19bfa26804ccbe1,"method USED-FOR primitives. primitives USED-FOR robotic movements. dataset of demonstrations USED-FOR tasks. dataset of demonstrations USED-FOR method. transformer network USED-FOR trajectory. transformer network USED-FOR latent variables. transformer network USED-FOR algorithm. dynamic time warping USED-FOR reconstruction. Task is robotic tasks. OtherScientificTerm are motor programs, and joint space. Method are LSTM network, and LSTM. ","This paper proposes a method to learn primitives for robotic movements from a dataset of demonstrations for a variety of tasks. The proposed algorithm is based on a transformer network that maps the trajectory to latent variables, which are then used to train an LSTM network. The reconstruction is done by dynamic time warping, where the learned motor programs are mapped to a joint space, and the learned trajectories are used to update the LstM. Experiments are conducted on several robotic tasks."
3272,SP:2e9235485b79d0b22ec8b565b19bfa26804ccbe1,"approach USED-FOR extracting reusable motor primitives. task demonstrations USED-FOR approach. task demonstrations USED-FOR extracting reusable motor primitives. deep encoder network USED-FOR approach. decoder network USED-FOR trajectory segments. regularization term USED-FOR network. OtherScientificTerm are reusable motor primitives, robot's configuration space, and motor primitives. Generic is networks. Method is robotic planning algorithm. ","This paper presents an approach to extracting reusable motor primitives from task demonstrations. The approach is based on a deep encoder network that encodes the robot's configuration space, and a decoder network which decodes the trajectory segments. The networks are trained in a supervised fashion. The network is also trained with a regularization term that encourages the network to focus on the most important parts of the trajectory, i.e., the most relevant parts of a trajectory, rather than the rest of the trajectories. Experiments show that the proposed robotic planning algorithm is able to find the most valuable parts of trajectories that are most important to the motor primitive."
3273,SP:2e9235485b79d0b22ec8b565b19bfa26804ccbe1,unlabeled actions USED-FOR middle - level motor task primitives. loss function FEATURE-OF LSTMs. LSTMs USED-FOR decomposition of motor tasks. MIME dataset EVALUATE-FOR approach. OtherScientificTerm is recomposed task. ,This paper proposes to learn middle-level motor task primitives from unlabeled actions. The main idea is to use LSTMs with a loss function that encourages the decomposition of motor tasks. The approach is evaluated on the MIME dataset. The results show that the proposed approach leads to better performance on the recomposed task.
3274,SP:c7c37aeebec7f33c1015f1fa3dd2a36d7b437d1c,"transfer PART-OF RL. action space CONJUNCTION reward space. reward space CONJUNCTION action space. reward space CONJUNCTION discount factor. discount factor CONJUNCTION reward space. state space CONJUNCTION action space. action space CONJUNCTION state space. hidden parameter USED-FOR environment's transition dynamics. OtherScientificTerm are distribution of environments, transition dynamics, and observability. Generic is agent. ","This paper studies transfer in RL, where the agent has access to a distribution of environments, a state space, an action space, a reward space, and a discount factor. The authors propose to model the transition dynamics of the environment as a hidden parameter, which can be used to estimate the environment's transition dynamics, which is then used to train the agent to transfer to new environments. They show that this can lead to improved transfer performance, especially when the agent is trained in an environment with high observability."
3275,SP:c7c37aeebec7f33c1015f1fa3dd2a36d7b437d1c,latent encoding USED-FOR transition dynamics. probe policy USED-FOR short trajectories. policy USED-FOR tasks. dynamics FEATURE-OF tasks. Method is universal policy. OtherScientificTerm is encoding. ,"This paper proposes a universal policy that learns to predict the future trajectories of a probe policy. The core idea is to learn a latent encoding that encodes the transition dynamics of the probe policy, which is then used to predict short trajectories. The encoding is learned in an unsupervised manner, and the policy is then applied to new tasks with different dynamics."
3276,SP:c7c37aeebec7f33c1015f1fa3dd2a36d7b437d1c,"strategy USED-FOR single - trajectory transfer. reinforcement learned policy USED-FOR single - trajectory transfer. approach PART-OF few - shot supervised - learning community. OtherScientificTerm are lower dimensional latent variable, and latent variable. ","This paper proposes a strategy for single-trajectory transfer with reinforcement learned policy. The key idea is to learn a lower dimensional latent variable for each trajectory, and then use the latent variable to predict the trajectory of the next trajectory. The approach is well-motivated and has been widely adopted in the few-shot supervised-learning community."
3277,SP:f2f1aff9a5b91d748b24fee0155367f650401aab,"AlphaGo CONJUNCTION AlphaZero 2HNN. AlphaZero 2HNN CONJUNCTION AlphaGo. 3HNN USED-FOR action - value Q function. fixed dataset USED-FOR network. updating dataset USED-FOR MCTS iterations. updating dataset USED-FOR 3HNN. MCTS iterations USED-FOR 3HNN. Learning speed EVALUATE-FOR 3HNN. 3HNN COMPARE 2HNN. 2HNN COMPARE 3HNN. 3HNN COMPARE that. that COMPARE 3HNN. Learning speed EVALUATE-FOR 2HNN. Learning speed COMPARE that. that COMPARE Learning speed. that COMPARE 2HNN. 2HNN COMPARE that. game of Hex EVALUATE-FOR approach. positions CONJUNCTION random positions. random positions CONJUNCTION positions. strong agent ’s games FEATURE-OF positions. positions HYPONYM-OF test datasets. random positions HYPONYM-OF test datasets. Method are AlphaZero learning paradigm, and 3HNN architecture. Generic are architecture, and datasets. OtherScientificTerm are loss, and threshold expansion parameter. ","This paper proposes an extension of the AlphaZero learning paradigm to the 3HNN architecture. The main difference between AlphaGo and AlphaZero 2HNN is that instead of training a network on a fixed dataset, the authors propose to train the network on an updating dataset, where the network is trained using MCTS iterations on the updating dataset. The authors propose a new architecture, called 3-HNN, which learns an action-value Q function that is parameterized by a 3-layer model of the loss. Learning speed is compared to that of the original AlphaGo as well as that of AlphaZero. The proposed approach is evaluated on the game of Hex and on a game of Go, where they show that the proposed approach outperforms the original 2HN. They also compare the performance of the proposed three-layer 3HN architecture with that of a standard 2-layer 1HNN and show that in the case of a threshold expansion parameter, the proposed method is able to achieve better performance than that of standard 2hNN. They evaluate their approach on two test datasets: positions from strong agent’s games, and random positions from random positions."
3278,SP:f2f1aff9a5b91d748b24fee0155367f650401aab,three - head network USED-FOR AlphaZero - like training. three - head network USED-FOR policy. it USED-FOR AlphaZero training. AlphaZero training USED-FOR game of Hex 9x9. it USED-FOR game of Hex 9x9. fixed dataset USED-FOR supervised learning. supervised learning USED-FOR three - head network. ,"This paper proposes a three-head network for AlphaZero-like training, where a policy is trained with a fixed number of heads. The authors show that the three head network can be trained with supervised learning on a fixed dataset, and apply it to AlphaZero training in the game of Hex 9x9."
3279,SP:f2f1aff9a5b91d748b24fee0155367f650401aab,three - head neural network architecture CONJUNCTION training loss. training loss CONJUNCTION three - head neural network architecture. architecture USED-FOR Hex. one CONJUNCTION other. other CONJUNCTION one. datasets EVALUATE-FOR evaluation. benzene USED-FOR randomly sampled but perfectly labelled examples. MoHex 2.0 USED-FOR near - optimal players. randomly sampled but perfectly labelled examples USED-FOR other. other HYPONYM-OF datasets. one HYPONYM-OF datasets. state - value errors CONJUNCTION action - value errors. action - value errors CONJUNCTION state - value errors. action - value errors CONJUNCTION policy prediction accuracies. policy prediction accuracies CONJUNCTION action - value errors. Task is Hex game. ,"This paper proposes a three-head neural network architecture and a training loss to improve the performance of the classic Hex game. The proposed architecture is motivated by the observation that the near-optimal players in MoHex 2.0 are trained with the same architecture. The evaluation is performed on two datasets: one with randomly sampled but perfectly labelled examples from benzene, and the other with only a small number of randomly sampled examples. The results show improvements in terms of state-value errors, action -value errors and policy prediction accuracies."
3280,SP:89d6d55107b6180109affe7522265c751640ad96,algorithm USED-FOR transferring policies. source optimal policy USED-FOR MDP. method USED-FOR simulated robotic tasks ( Mujoco ). Method is transition model. OtherScientificTerm is trajectory distribution. ,"This paper proposes an algorithm for transferring policies from source to target environments. The main idea is to learn a transition model, where the source optimal policy is the MDP, and the target environment is the trajectory distribution. The method is applied to simulated robotic tasks (Mujoco)."
3281,SP:89d6d55107b6180109affe7522265c751640ad96,reinforcement learning USED-FOR transferring policies. policy USED-FOR optimal policy. low sample complexity EVALUATE-FOR policy. policy adaptation mechanism USED-FOR algorithm. Generic is framework. OtherScientificTerm is state transition. ,"This paper studies the problem of transferring policies using reinforcement learning. The main contribution of this paper is to propose a framework for learning a policy with low sample complexity that can then be used to learn an optimal policy. The algorithm is based on a policy adaptation mechanism, where the state transition between the learned policy and the original policy is assumed to be smooth."
3282,SP:89d6d55107b6180109affe7522265c751640ad96,"source policy CONJUNCTION source dynamics. source dynamics CONJUNCTION source policy. KL divergence FEATURE-OF trajectory likelihood. target policy CONJUNCTION target dynamics. target dynamics CONJUNCTION target policy. target policy FEATURE-OF trajectory likelihood. source policy USED-FOR trajectory likelihood. target policy USED-FOR state distribution. state distribution FEATURE-OF optimal source policy. target policy COMPARE optimal source policy. optimal source policy COMPARE target policy. method COMPARE warm - started RL. warm - started RL COMPARE method. MuJoCo locomotion robots EVALUATE-FOR method. varying physics FEATURE-OF MuJoCo locomotion robots. OtherScientificTerm are policy, and state transition function. ",This paper proposes to learn a policy that maximizes the trajectory likelihood between the target policy and the source dynamics by maximizing the KL divergence between the trajectories of the source policy and target dynamics. The key idea is to learn the state transition function by minimizing the distance between the state distribution of the optimal source policy with respect to a target policy. Experiments on MuJoCo locomotion robots with varying physics show that the proposed method outperforms warm-started RL.
3283,SP:626021101836a635ad2d896bd66951aff31aa846,scale - equivariant steerable convolutional neural networks USED-FOR translation and scaling symmetry. translation and scaling symmetry FEATURE-OF representation. pooling CONJUNCTION nonlinearity. nonlinearity CONJUNCTION pooling. scale - convolution blocks PART-OF network. OtherScientificTerm is scale - equivariant. Method is scale - equivariant network. ,"This paper proposes scale-equivariant steerable convolutional neural networks that preserve the translation and scaling symmetry of the representation. The key idea is to make the network scale-extivariant by pooling the scale-convolution blocks of the network. The authors show that this makes the network more efficient and more scalable. They also show that pooling and nonlinearity are not necessary to make scale-elevatorant. Finally, the authors provide a theoretical analysis of the properties of scale-ellivariant network."
3284,SP:626021101836a635ad2d896bd66951aff31aa846,method USED-FOR scale equivariance. scale equivariance PART-OF convolutional networks. method USED-FOR convolutional networks. steerable filters USED-FOR method. continuous scale and translation space USED-FOR discretized implementation. continuous scale and translation space USED-FOR theory. steerable basis elements USED-FOR discretized implementation. number of layers CONJUNCTION image scale. image scale CONJUNCTION number of layers. image scale CONJUNCTION scale interactions. scale interactions CONJUNCTION image scale. MNIST - scale EVALUATE-FOR method. MNIST - scale and STL-10 EVALUATE-FOR method. Method is STL-10. ,"This paper proposes a method to improve scale equivariance in convolutional networks using steerable filters. The theory is based on continuous scale and translation space, and the discretized implementation uses steerable basis elements. The method is evaluated on MNIST-scale and STL-10, where the authors show improvements in terms of number of layers, image scale, and scale interactions."
3285,SP:626021101836a635ad2d896bd66951aff31aa846,framework ( SESN ) USED-FOR deep networks. scale equivariance CONJUNCTION translation invariance. translation invariance CONJUNCTION scale equivariance. scale equivariance FEATURE-OF deep networks. scale - translation group FEATURE-OF group convolution. group convolution USED-FOR formulation. continuous basis functions USED-FOR Filters. image classification accuracy EVALUATE-FOR approach. Generic is framework. OtherScientificTerm is baselines. ,This paper proposes a new framework (SESN) for training deep networks with scale equivariance and translation invariance. The formulation is based on group convolution with a scale-translation group. Filters are trained with continuous basis functions. The proposed approach is evaluated on image classification accuracy and compared to several baselines.
3286,SP:6316f750b8c69e55e61926c34e3ba5acbd7228ad,"applications USED-FOR 3D scanners. point clouds USED-FOR 3D shapes. decoder USED-FOR inverse mapping. encoder USED-FOR lower - dimensional latent space. decoder USED-FOR method. encoder USED-FOR method. Earth Mover's Distance USED-FOR training loss. Hausdorff distance loss USED-FOR partial matching. adversarial ( min - max ) strategy USED-FOR training. simulated and real data EVALUATE-FOR method. It COMPARE state of the art. state of the art COMPARE It. supervised settings it COMPARE PCN approach. PCN approach COMPARE supervised settings it. state of the art USED-FOR unsupervised settings. It COMPARE supervised settings it. supervised settings it COMPARE It. It COMPARE PCN approach. PCN approach COMPARE It. It USED-FOR unsupervised settings. Task are reconstructing 3D shapes, adversarial formulation of the reconstruction, and supervised settings. Method are GANs, and encoder / decoder. Material is partially - observed noisy one. ","This paper considers the problem of reconstructing 3D shapes from point clouds. This is an important problem for applications to 3D scanners and other applications to reconstruct 3D objects. The paper proposes a method that uses an encoder/decoder to map a point cloud to a lower-dimensional latent space, and a decoder to learn an inverse mapping between the original point cloud and the partially-observed noisy one. The training is done using an adversarial (min-max) strategy, where the training loss is based on Earth Mover's Distance, and the partial matching is performed using a Hausdorff distance loss. The method is evaluated on simulated and real data, and compared to the state of the art in both unsupervised settings and supervised settings. The results show that the proposed adversarial formulation of the reconstruction is more effective than the standard PCN approach. "
3287,SP:6316f750b8c69e55e61926c34e3ba5acbd7228ad,"Task are point cloud completion, and autonomous driving applications. Generic is setting. ","This paper studies the problem of point cloud completion, which is an important problem in autonomous driving applications. The authors propose a novel setting where the goal is to complete the task in a single pass. The paper is well-written and easy to follow."
3288,SP:6316f750b8c69e55e61926c34e3ba5acbd7228ad,"method USED-FOR 3D point clouds. automatically completing 3D scans USED-FOR method. real data COMPARE synthetic data. synthetic data COMPARE real data. synthetic data USED-FOR it. real data USED-FOR it. generative adversarial network ( GAN ) USED-FOR complete point clouds. 3D scanning USED-FOR noisy or partial point clouds. noisy or partial point clouds USED-FOR complete point clouds. discriminator USED-FOR mappings of noisy input. encoded clean shapes CONJUNCTION mappings of noisy input. mappings of noisy input CONJUNCTION encoded clean shapes. discriminator USED-FOR encoded clean shapes. point clouds HYPONYM-OF mappings of noisy input. synthetic data point clouds HYPONYM-OF encoded clean shapes. real - life data 3D scans USED-FOR point clouds. Generic are It, and generator. OtherScientificTerm are paired data samples, and manifold of clean shapes X_c. ","This paper proposes a method for generating 3D point clouds from automatically completing 3D scans. It is based on the observation that it is easier to learn from real data than synthetic data. The authors propose a generative adversarial network (GAN) that generates complete point clouds using noisy or partial point clouds generated by 3D scanning. The generator is trained on paired data samples, where each sample is sampled from a manifold of clean shapes X_c, and the discriminator is trained to distinguish between encoded clean shapes (i.e., synthetic data point clouds) and mappings of noisy input (e.g., point clouds produced from real-life data 3d scans)."
3289,SP:270c679b322f69a943bf7f6b938dc1bf663d3c6f,generative models USED-FOR authentication / anomaly detection systems. sensor data USED-FOR authentication / anomaly detection systems. maxmin game USED-FOR scenario. closed form solution USED-FOR multivariate Gaussian data. Nash equilibrium FEATURE-OF game. authentication EVALUATE-FOR state of the art methods. OtherScientificTerm is decision rule. Generic is algorithm. Task is data augmentation. ,"This paper studies the problem of training generative models for authentication/anomaly detection systems on sensor data. The authors consider the scenario of a maxmin game, where the agent has access to a set of data points and the goal is to find a decision rule that maximizes the probability of the data point being detected. The main contribution of the paper is to provide a closed form solution for multivariate Gaussian data, and to prove that the game converges to a Nash equilibrium. The paper also provides a theoretical analysis of the algorithm, and provides some experimental results on authentication against state of the art methods. The results are interesting, but the paper needs some work on data augmentation."
3290,SP:270c679b322f69a943bf7f6b938dc1bf663d3c6f,"threat model USED-FOR generative impersonation attacks. minimax game USED-FOR threat model. objective USED-FOR attack. Gan - in - the - middle attack HYPONYM-OF attack. VoxCeleb2 EVALUATE-FOR approach. Material is leaked images. OtherScientificTerm are registration images, Nash equilibrium, optimal strategies, and data dimension. Method is GANs. ","This paper proposes a new threat model for generative impersonation attacks based on a minimax game. The proposed attack is a variant of the Gan-in-the-middle attack, where the objective is to generate a set of fake registration images from the leaked images. The authors show that the Nash equilibrium of the optimal strategies is a function of the data dimension, which is an important property of GANs. The approach is evaluated on VoxCeleb2."
3291,SP:270c679b322f69a943bf7f6b938dc1bf663d3c6f,"zero - sum game USED-FOR problem. algorithm USED-FOR faked input. Task is attack - defense problem. OtherScientificTerm are closed form of the optimal strategies, registration, fake inputs, sufficient statistics, and leaked observations. Material is Gaussian case. Metric is success rate. Method is learning algorithm. ","This paper studies the attack-defense problem, which is a zero-sum game where the goal is to find a closed form of the optimal strategies that maximizes the probability of success. The problem can be viewed as a variant of the Gaussian case, where the registration is done by generating fake inputs. The authors propose an algorithm that generates faked input, and show that the success rate of the proposed learning algorithm is O(1/\sqrt{T}) where T is the number of fake inputs and T is sufficient statistics. The paper also provides a theoretical analysis of the effect of leaking observations."
3292,SP:a7e7619667892806a6f4038cbe4b1c6cd0eec0ed,robust and standard accuracies EVALUATE-FOR adversarial training. standard accuracy CONJUNCTION sensible adversarial accuracy. sensible adversarial accuracy CONJUNCTION standard accuracy. adversarial training algorithm USED-FOR sensible adversarial risk. adversarial and standard accuracies EVALUATE-FOR models. technique USED-FOR models. OtherScientificTerm is adversarial risk. Method is Bayes optimal classifier. ,"This paper studies the robust and standard accuracies of adversarial training. The authors propose a new adversarial risk that aims to balance the standard accuracy and the sensible adversarial accuracy. The adversarial loss is defined as the difference between the Bayes optimal classifier and the one that maximizes the adversarial and standard accuracy of the model. The paper also proposes an adversarial learning algorithm that minimizes the ""Sensible adversarial Risk"" (SAR). The authors show that the proposed technique can improve the performance of existing models in terms of both the robustness of the models as well as the standard and adversarial accuracies."
3293,SP:a7e7619667892806a6f4038cbe4b1c6cd0eec0ed,"robustness CONJUNCTION standard accuracy. standard accuracy CONJUNCTION robustness. standard accuracy EVALUATE-FOR adversarial learning. robustness EVALUATE-FOR adversarial learning. Method is adversarial example framework. Metric is natural accuracy. Generic are model, algorithm, and it. ","This paper proposes an adversarial example framework to evaluate the trade-off between robustness and standard accuracy in adversarial learning. The main idea is to use natural accuracy as a proxy for the robustness of the model. The authors show that natural accuracy is a good proxy for robustness, and show that the proposed algorithm can be used to evaluate it empirically."
3294,SP:a7e7619667892806a6f4038cbe4b1c6cd0eec0ed,"minimizing robust risk USED-FOR Bayes - optimal classifier. minimizing robust risk USED-FOR adversary. adversary FEATURE-OF robust risk. robust accuracy EVALUATE-FOR CIFAR-10. robust accuracy EVALUATE-FOR modification. CIFAR-10 EVALUATE-FOR modification. Method are variation of adversarial training / robust optimization, and gradient clipping. OtherScientificTerm is perturbation set. Generic is variant. ","This paper proposes a variation of adversarial training/robust optimization, where the perturbation set is randomly generated and the adversary is tasked with minimizing robust risk to the Bayes-optimal classifier. The proposed variant is called gradient clipping. The authors evaluate the proposed modification on CIFAR-10 and show improved robust accuracy."
3295,SP:9ca0b8d270e3fea3ba8f88c8f1ba50d8a8f7e4b8,adversarial training USED-FOR method. cyclic learning scheme USED-FOR networks. adversarial training USED-FOR AFD. adversarial training COMPARE direct method. direct method COMPARE adversarial training. accuracy EVALUATE-FOR adversarial training. L1 distance USED-FOR direct method. online distillation methods EVALUATE-FOR method. Task is online knowledge distillation problem. Method is Online Adversarial Feature map Distillation ). Material is CIFAR100. ,This paper studies the online knowledge distillation problem and proposes a method called OADM (Online Adversarial Feature map Distillation). The proposed method is based on adversarial training for AFD. The authors propose a cyclic learning scheme to train the networks. The experiments on CIFAR100 show that the proposed method achieves better accuracy than the direct method based on L1 distance. The experimental results on other online distillation methods are also presented.
3296,SP:9ca0b8d270e3fea3ba8f88c8f1ba50d8a8f7e4b8,feature map information USED-FOR logits. GAN USED-FOR feature map information. feature map information USED-FOR online knowledge distillation. direct feature map alignment COMPARE algorithm. algorithm COMPARE direct feature map alignment. algorithm USED-FOR feature maps. adversarial game USED-FOR big and small nets. Generic is nets. ,"This paper proposes to use feature map information from a GAN to align logits for online knowledge distillation. Compared to direct feature map alignment, the proposed algorithm aligns feature maps in a more efficient way. The authors conduct experiments on both big and small nets trained in an adversarial game and show that the proposed nets outperform existing methods."
3297,SP:9ca0b8d270e3fea3ba8f88c8f1ba50d8a8f7e4b8,feature map level FEATURE-OF Knowledge Distillation ( KD ). task COMPARE KD. KD COMPARE task. network USED-FOR network. logit - based KD USED-FOR approaches. Generic is networks. Method is feature map level KD. ,"This paper studies Knowledge Distillation (KD) at the feature map level. In particular, the authors propose a new task that is different from standard KD in the sense that the network is trained on a subset of the original network instead of the entire network. The authors propose two approaches based on logit-based KD and show that the proposed networks are able to generalize better to new tasks. The main contribution of this paper is to propose a novel task, which is a variant of feature map-level KD. "
3298,SP:e43fc8747f823be6497224696adb92d45150b02d,"embeddings USED-FOR sentiments. word embeddings USED-FOR expressing sentiments. Glove CONJUNCTION word2vec. word2vec CONJUNCTION Glove. word embeddings COMPARE methods. methods COMPARE word embeddings. models USED-FOR word embeddings. word2vec HYPONYM-OF methods. Glove HYPONYM-OF methods. Method are GloVe word embedding model, maximum likelihood estimation, bayesian estimation, and estimation methods. OtherScientificTerm are extension term, and sentiment information. Material is Stanford sentiment tree ( SST ) corpus. ","This paper proposes a GloVe word embedding model, which is based on maximum likelihood estimation. The authors argue that word embeddings for expressing sentiments can be used as an extension term to express sentiment information. They show that the proposed models can produce better word embeds than existing methods such as Glove and word2vec. They also show that Bayesian estimation can be improved by adding an extra extension term that allows for bayesian estimation of the likelihood of the embedding. They evaluate the proposed estimation methods on the Stanford sentiment tree (SST) corpus and show that their estimation methods outperform existing methods."
3299,SP:e43fc8747f823be6497224696adb92d45150b02d,method USED-FOR word embedding. sentiment information USED-FOR method. probability of positive sentiment PART-OF loss function. D - GloVe USED-FOR method. word - level sentiment analysis CONJUNCTION sentence - level sentiment analysis. sentence - level sentiment analysis CONJUNCTION word - level sentiment analysis. word similarity CONJUNCTION word - level sentiment analysis. word - level sentiment analysis CONJUNCTION word similarity. word similarity HYPONYM-OF experiments. sentence - level sentiment analysis HYPONYM-OF experiments. word - level sentiment analysis HYPONYM-OF experiments. method COMPARE baseline methods. baseline methods COMPARE method. low - frequency sentence setting EVALUATE-FOR baseline methods. low - frequency sentence setting EVALUATE-FOR method. ,"This paper proposes a method for word embedding based on sentiment information. The method is based on D-GloVe and incorporates the probability of positive sentiment into the loss function. Experiments are conducted on word similarity, word-level sentiment analysis, and sentence - level sentiment analysis. Results show that the proposed method outperforms baseline methods in a low-frequency sentence setting."
3300,SP:e43fc8747f823be6497224696adb92d45150b02d,sentiment information PART-OF word embedding model. maximum likelihood estimation CONJUNCTION maximum posterior estimation. maximum posterior estimation CONJUNCTION maximum likelihood estimation. maximum posterior estimation USED-FOR framework. maximum likelihood estimation USED-FOR framework. word similarity CONJUNCTION low frequency embeddings. low frequency embeddings CONJUNCTION word similarity. prior knowledge PART-OF model. Bayesian inference CONJUNCTION prior knowledge. prior knowledge CONJUNCTION Bayesian inference. ,This paper proposes a word embedding model that incorporates sentiment information. The proposed framework is based on maximum likelihood estimation and maximum posterior estimation. The authors propose to combine word similarity with low frequency embeddings. The model combines Bayesian inference with prior knowledge.
3301,SP:72d32a2ae382f63e055ab3eafcc9276b10fba985,early stopping USED-FOR optimization. Gradient Descent USED-FOR Label Noise. Label Noise USED-FOR Overparameterized Neural Networks. Early Stopping USED-FOR Gradient Descent. Method is early stop. ,This paper studies the problem of early stopping in optimization. The authors propose to use Gradient Descent with Early Stopping to reduce the Label Noise in Overparameterized Neural Networks. The early stop is shown to be effective.
3302,SP:72d32a2ae382f63e055ab3eafcc9276b10fba985,training approach COMPARE state - of - art methods. state - of - art methods COMPARE training approach. label noise datasets EVALUATE-FOR training approach. state - of - art approaches COMPARE method. method COMPARE state - of - art approaches. early - stopping and safe set USED-FOR method. prestopping idea USED-FOR approaches. Generic is two stage method. Metric is minimum validation error. OtherScientificTerm is maximal safe set. ,"This paper proposes a two stage method to reduce the minimum validation error. The training approach is evaluated on two label noise datasets and compared to state-of-art methods. The method is based on the idea of early-stopping and safe set, which is an extension of the prestopping idea that has been used in previous approaches. The main contribution of this paper is to propose a maximal safe set."
3303,SP:72d32a2ae382f63e055ab3eafcc9276b10fba985,training strategy USED-FOR robustness. training strategy USED-FOR label noise. dataset USED-FOR neural network. it USED-FOR maximal safe set. validation error EVALUATE-FOR network. network USED-FOR maximal safe set. tradictional   self - training USED-FOR semi - superivsed learning. Co - training USED-FOR domain adaptation. co - training USED-FOR domain adaptation. training strategy COMPARE tradictional   self - training. tradictional   self - training COMPARE training strategy. training strategy COMPARE co - training. co - training COMPARE training strategy. tradictional   self - training USED-FOR domain adaptation. tradictional   self - training CONJUNCTION co - training. co - training CONJUNCTION tradictional   self - training. Method is prestopping. ,"This paper proposes a new training strategy to improve robustness to label noise. The key idea is to train a neural network on a dataset and then perform prestopping, where the validation error of the network is used to train it on a maximal safe set. The authors compare the proposed training strategy with tradictional  self-training for semi-superivsed learning and co-training in domain adaptation."
3304,SP:8316872d8b388587bf25f724c80155b25b6cb68e,pixel data HYPONYM-OF dataset. it USED-FOR policy. Generic is task. ,This paper proposes a new dataset called pixel data. The idea is that it can be used to train a policy that can generalize to unseen data points. The task is very interesting and the paper is well-written. 
3305,SP:8316872d8b388587bf25f724c80155b25b6cb68e,images CONJUNCTION videos. videos CONJUNCTION images. model USED-FOR RL policy. model USED-FOR actions ’ representations. datasets of unstructured information USED-FOR actions ’ representations. images HYPONYM-OF datasets of unstructured information. videos HYPONYM-OF datasets of unstructured information. action representations USED-FOR RL policy. model COMPARE baselines. baselines COMPARE model. Task is generalization of reinforcement learning policies. Method is reinforcement learning policies. ,"This paper addresses the problem of generalization of reinforcement learning policies. The authors propose a model to learn actions’ representations from datasets of unstructured information (e.g., images and videos). The authors show that the learned action representations can be used to train an RL policy. The proposed model outperforms several baselines."
3306,SP:8316872d8b388587bf25f724c80155b25b6cb68e,generalization of discrete action policies USED-FOR task. policy USED-FOR tasks. general understanding USED-FOR policy. HVAE USED-FOR characteristics. generalization USED-FOR policy. risk minimization USED-FOR generalization. OtherScientificTerm is action's characteristics. ,This paper studies the generalization of discrete action policies to a new task. The authors propose to use a general understanding of the action's characteristics to train a policy that can generalize to new tasks. They use HVAE to model these characteristics and show that generalization can be achieved using risk minimization.
3307,SP:f534d51192eaacc6cb6bfd365e6d959d9dd498b2,"methods USED-FOR word embedding matrices. method USED-FOR p - dimensional embedding. rnq parameters USED-FOR representation. t x q matrices FEATURE-OF tensor product. tensor product USED-FOR method. tensor product USED-FOR d x p embedding matrix. Algorithms USED-FOR full p - dimensional representations. dot products USED-FOR p - dimensional representations. Method is d x p embedding matrices. OtherScientificTerm are vocabulary size, embedding size, and tensor product of n q - dimensional embeddings. ","This paper proposes methods for learning word embedding matrices. The proposed method learns a p-dimensional embedding of a word using a tensor product of t x q matrices, where the representation is parameterized by rnq parameters. The method is motivated by the fact that the d x p embedding matrix can be represented as a t-dimensional product of a d x q embedding, which is a more efficient method for learning p-dimensionality. Algorithms for learning full p-dimensions have been proposed in the literature, which are usually based on dot products. However, dot products are computationally expensive, which limits the applicability of the proposed method. This paper proposes to use the tensor products of n q dimensional embeddings. The authors argue that this is due to the large vocabulary size, which makes it difficult to compute the embedding size. "
3308,SP:f534d51192eaacc6cb6bfd365e6d959d9dd498b2,"word2ket USED-FOR storing word embeddings. tensor products USED-FOR word2ket. tensor products USED-FOR storing word embeddings. tensor product USED-FOR d - dimensional vector. time cost EVALUATE-FOR word lookup. GPU HYPONYM-OF processor memory. machine translation CONJUNCTION question answering. question answering CONJUNCTION machine translation. summarization CONJUNCTION machine translation. machine translation CONJUNCTION summarization. question answering HYPONYM-OF tasks. summarization HYPONYM-OF tasks. machine translation HYPONYM-OF tasks. OtherScientificTerm are linear operators, and vocabulary size. Generic are applications, and model. ","This paper proposes word2ket, which uses tensor products for storing word embeddings. The key idea is to use a d-dimensional vector as a tensor product, and then use linear operators to map the vector to a vector. The authors show that this can reduce the time cost of word lookup by up to 1.5x, which is a significant speedup compared to using processor memory (e.g. GPU). The authors also show that their model can be applied to tasks such as summarization, machine translation, and question answering. The experiments are conducted on three different applications and show that the model can achieve state-of-the-art performance while reducing the vocabulary size."
3309,SP:f534d51192eaacc6cb6bfd365e6d959d9dd498b2,"methods USED-FOR neural NLP models. large word embedding matrix USED-FOR neural NLP models. nlp tasks EVALUATE-FOR method. Metric is memory footprint. OtherScientificTerm are quantum entanglement, and word embeddings. ",This paper proposes methods to train neural NLP models with large word embedding matrix. The main idea is to reduce the memory footprint by minimizing the quantum entanglement between the word embeddings. The method is evaluated on nlp tasks.
3310,SP:3df499068ffe6c995457c2174f987cb0ae3c2551,behavioral repertoire imitation learning ( BRIL ) USED-FOR policy. BRIL USED-FOR collection. context - dependent policy USED-FOR BRIL. BRIL USED-FOR context variable. user ’s knowledge USED-FOR BRIL. t - SNE HYPONYM-OF dimensionality reduction method. dimensionality reduction method USED-FOR feature space. state - context input variable CONJUNCTION action output variable. action output variable CONJUNCTION state - context input variable. action output variable USED-FOR supervised learning ( behavior cloning ). state - context input variable USED-FOR supervised learning ( behavior cloning ). supervised learning ( behavior cloning ) USED-FOR policy. StarCraft environment EVALUATE-FOR method. BRIL COMPARE baselines. baselines COMPARE BRIL. behavior cloning CONJUNCTION behavior cloning. behavior cloning CONJUNCTION behavior cloning. BRIL COMPARE behavior cloning. behavior cloning COMPARE BRIL. clustered demonstrations USED-FOR behavior cloning. diverse demonstrations EVALUATE-FOR behavior cloning. behavior cloning HYPONYM-OF baselines. behavior cloning HYPONYM-OF baselines. ,"This paper proposes behavioral repertoire imitation learning (BRIL), which learns a policy to imitate a set of behaviors from a collection using a context-dependent policy. BRIL learns a context variable based on the user’s knowledge and uses this context variable to augment the collection. The feature space is learned using a dimensionality reduction method (e.g., t-SNE, t-NSE) to reduce the dimensionality of the feature space. The policy is trained using supervised learning (behavior cloning) with both a state-context input variable and an action output variable. The method is evaluated on the StarCraft environment and compared to two baselines: behavior cloning using clustered demonstrations and behavior cloning with diverse demonstrations."
3311,SP:3df499068ffe6c995457c2174f987cb0ae3c2551,"dimensionality reduction technique USED-FOR latent space of strategies. feature vector USED-FOR method. dimensionality reduction technique USED-FOR method. reduced representation of the current strategy USED-FOR policy model. behavioral cloning USED-FOR BRIL. StarCraft II EVALUATE-FOR BRIL. BRIL model COMPARE base imitation learning model. base imitation learning model COMPARE BRIL model. strategy information USED-FOR base imitation learning model. Method is policy. OtherScientificTerm are space of strategies, and built - in AI. ","This paper proposes a method that uses a dimensionality reduction technique to reduce the latent space of strategies using a feature vector. The policy model is trained with a reduced representation of the current strategy, and the policy is trained using behavioral cloning. The proposed method, BRIL, is evaluated on StarCraft II, where it is shown that the BRIL model outperforms a base imitation learning model that does not use strategy information. The paper also shows that BRIL can be used in conjunction with behavioral cloning to improve the performance of the policy. The authors also show that their method can be applied to other games as well. Finally, the authors show that the learned policy can generalize to unseen environments and explore the space of different strategies, which is a promising direction for built-in AI."
3312,SP:3df499068ffe6c995457c2174f987cb0ae3c2551,"Behavioral Repertoire Imitation Learning ( BRIL ) USED-FOR policy. behavior inputs USED-FOR policy. imitation learning USED-FOR policy. behavior cloning USED-FOR NN. behavior cloned policy COMPARE behavior cloned policies. behavior cloned policies COMPARE behavior cloned policy. Method are t - SNE, and BRIL policy. OtherScientificTerm is behavior labels. ","This paper proposes Behavioral Repertoire Imitation Learning (BRIL) to learn a policy from behavior inputs using imitation learning. The main idea is to use t-SNE, which is an extension of behavior cloning in NN, to train a policy with behavior labels. The authors compare the performance of a behavior cloned policy with a set of behavior-cloned policies, and show that the behavior CLoned policy performs better than the other two types of behavior cloning policies. In addition, the authors show that a BRIL policy is able to generalize to unseen environments."
3313,SP:db15d3cc3e95173ca6d4fd88313d89a739d1c910,"weight magnitudes USED-FOR training. learning capability FEATURE-OF complex pattern. OtherScientificTerm are saturation, lottery tickets, and computation - heavy iterative pruning. Method is gradual pruning. ","This paper studies the effect of different weight magnitudes on training performance. The authors argue that saturation occurs when the number of lottery tickets is large, and that gradual pruning is the best way to address this issue. They also argue that computation-heavy iterative pruning can lead to poor performance. Finally, they argue that the learning capability of a complex pattern can be improved if the weights are small."
3314,SP:db15d3cc3e95173ca6d4fd88313d89a739d1c910,"weight magnitude FEATURE-OF model. memorization capacity analysis USED-FOR early wining tickets. CIFAR 10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR 10. ImageNet EVALUATE-FOR ResNet architectures. CIFAR 10 EVALUATE-FOR ResNet architectures. OtherScientificTerm are structure of winning lottery tickets, structure of the early winning tickets, lottery tickets, and model saturation. Method are SGD optimization, pruning, and iterative pruning. ","This paper provides a memorization capacity analysis for early wining tickets, which is motivated by the structure of winning lottery tickets. The authors show that the weight magnitude of the model is proportional to the size of the lottery tickets, and that SGD optimization can lead to model saturation. To address this issue, the authors propose pruning the weights of the early winning tickets and propose iterative pruning. Experiments on CIFAR 10 and ImageNet are conducted to evaluate the ResNet architectures."
3315,SP:db15d3cc3e95173ca6d4fd88313d89a739d1c910,sparse sub - networks COMPARE large model. large model COMPARE sparse sub - networks. sparse sub - networks PART-OF dense large models. accuracy EVALUATE-FOR large model. accuracy EVALUATE-FOR sparse sub - networks. pruning strategies USED-FOR sparse models. Material is lottery ticket hypothesis. Generic is sub - networks. Method is dense model. OtherScientificTerm is pruning. ,"This paper studies sparse sub-networks in dense large models and shows that sparse sub - networks can outperform a large model in terms of accuracy. The paper is motivated by the lottery ticket hypothesis, which states that the probability of winning a lottery ticket is proportional to the number of tickets drawn from the lottery tickets. The authors show that sparse models with different pruning strategies outperform sparse models that do not prune the sub-nets. The main contribution of this paper is to show that pruning the dense model can lead to better performance."
3316,SP:06d2a46282e34302050e81a1be8a2627acb159ee,neural network architecture USED-FOR image classification. binary tree FEATURE-OF features. method COMPARE multiple baselines. multiple baselines COMPARE method. image classification EVALUATE-FOR method. OtherScientificTerm is product of probabilities. ,"This paper proposes a novel neural network architecture for image classification. The key idea is to decompose the features into a binary tree, where the product of probabilities is computed. The method is evaluated on image classification and compared with multiple baselines."
3317,SP:06d2a46282e34302050e81a1be8a2627acb159ee,out - of - distribution samples USED-FOR neural network classifiers. PR subnets USED-FOR product relationship. it USED-FOR over - fitting. UDN USED-FOR unknown samples. UDN COMPARE baselines. baselines COMPARE UDN. Method is dot product of regular networks. ,"This paper studies the problem of learning neural network classifiers from out-of-distribution samples. The authors propose to use the dot product of regular networks, where the product relationship is modeled as PR subnets. They show that UDN can be used to handle unknown samples, and that it can avoid over-fitting. They also compare UDN with several baselines."
3318,SP:06d2a46282e34302050e81a1be8a2627acb159ee,"binary tree USED-FOR product relationship. fully connected layer CONJUNCTION binary tree. binary tree CONJUNCTION fully connected layer. softmax HYPONYM-OF method. binary tree USED-FOR output subnet. fully connected layer USED-FOR output subnet. split nodes USED-FOR probability distribution. split nodes PART-OF binary tree. classifier USED-FOR classification decision. uniform probability distribution FEATURE-OF subnets. information theory based regularization USED-FOR method. CIFAR-10 CONJUNCTION MNIST. MNIST CONJUNCTION CIFAR-10. SVHN EVALUATE-FOR method. CIFAR-10 EVALUATE-FOR method. MNIST EVALUATE-FOR method. MNIST CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION MNIST. CIFAR-100 EVALUATE-FOR method. CIFAR-100 EVALUATE-FOR SVHN. rejection accuracy EVALUATE-FOR method. classification accuracy EVALUATE-FOR method. Generic is ones. OtherScientificTerm are max path, and complex predictions. Task is generalization. ","This paper proposes a method called softmax, which is a combination of a fully connected layer and a binary tree to model the product relationship between the input and output. The output subnet is represented as a split node of the binary tree, where the split nodes represent the probability distribution of the input. The input subnets are assumed to have a uniform probability distribution. The classifier is used to make the classification decision. The proposed method is based on information theory based regularization. The method is evaluated on SVHN, CIFAR-10, MNIST, and a modified version of MNIST. The experiments show that the proposed method improves the rejection accuracy as well as the classification accuracy. The main contribution of this paper is to propose a method that can generalize to larger datasets than the existing ones. The authors also propose to use the max path as a way to reduce the number of samples required for generalization to complex predictions."
3319,SP:fa3e729469e74cac44745008fe65c01cc97c9820,"variance FEATURE-OF auxiliary variables. Method are variational inference, and mean field variational inference. OtherScientificTerm are variational posterior, and limited expressivity of mean field variational inference. Generic is model. Task is uncertainty prediction. ","This paper studies the problem of variational inference, where the variational posterior can be interpreted as a function of the variance of the auxiliary variables. In particular, the authors focus on the case where the model is trained on the mean field of the data. The authors point out that the model can be seen as an extension of mean field variational inferential inference, which is a generalization of the work of [1] and [2]. The authors also point out the limitations of the model in terms of uncertainty prediction, which they argue is a limitation of the limited expressivity of the current state of the art."
3320,SP:fa3e729469e74cac44745008fe65c01cc97c9820,method USED-FOR flexible variational posterior distributions. iterative locale refinements USED-FOR mean - field approximation. auxiliary variables USED-FOR iterative locale refinements. auxiliary variables USED-FOR method. Bayesian neural nets USED-FOR variational inference ( VI ). method USED-FOR Bayesian neural nets. method USED-FOR variational inference ( VI ). mean - field approach CONJUNCTION baselines. baselines CONJUNCTION mean - field approach. classification and regression tasks EVALUATE-FOR mean - field approach. classification and regression tasks EVALUATE-FOR baselines. OtherScientificTerm is Gaussian latent variables. ,This paper proposes a method for learning flexible variational posterior distributions. The method uses auxiliary variables as iterative locale refinements to improve the mean-field approximation. The proposed method is applied to Bayesian neural nets for variational inference (VI) and can be applied to Gaussian latent variables as well. Experiments on classification and regression tasks show that the proposed means-field approach outperforms baselines.
3321,SP:fa3e729469e74cac44745008fe65c01cc97c9820,way USED-FOR variational posterior. auxiliary variables USED-FOR q(w ). Material is regression and classification benchmark datasets. Metric is training time. ,This paper proposes a way to learn a variational posterior. The main idea is to add auxiliary variables to the q(w) to reduce the training time. Experiments are conducted on regression and classification benchmark datasets.
3322,SP:8e20d28a2a3a6f8f0b6a29a09a10fb8c7a011e86,neural program synthesis CONJUNCTION image captioning. image captioning CONJUNCTION neural program synthesis. image captioning HYPONYM-OF structured prediction problems. neural program synthesis HYPONYM-OF structured prediction problems. ASR - K HYPONYM-OF ARS estimator. action space FEATURE-OF tilmestep. variants CONJUNCTION binary tree version. binary tree version CONJUNCTION variants. ASR - K HYPONYM-OF variants. Method is gradient ARSM estimator. OtherScientificTerm is binary tree. ,"This paper studies the gradient ARSM estimator. The authors consider two structured prediction problems: neural program synthesis and image captioning. They propose two variants of the ASR-K, an ARS estimator, and a binary tree version, where the tilmestep in the action space is represented as the binary tree."
3323,SP:8e20d28a2a3a6f8f0b6a29a09a10fb8c7a011e86,neural program synthesis CONJUNCTION image captioning. image captioning CONJUNCTION neural program synthesis. algorithm USED-FOR unbiased stochastic gradient estimation. algorithm USED-FOR reinforcement learning of sequence generation tasks. image captioning HYPONYM-OF reinforcement learning of sequence generation tasks. neural program synthesis HYPONYM-OF reinforcement learning of sequence generation tasks. multiple rollouts USED-FOR gradient variance. correlated Monte Carlo rollouts USED-FOR method. ,"This paper proposes an algorithm for unbiased stochastic gradient estimation for reinforcement learning of sequence generation tasks, such as neural program synthesis and image captioning. The method is based on correlated Monte Carlo rollouts, where multiple rollouts are used to reduce the gradient variance."
3324,SP:8e20d28a2a3a6f8f0b6a29a09a10fb8c7a011e86,reinforcement learning - based algorithm USED-FOR contextual sequence generation. augment - REINFORCE - swap - merge ) HYPONYM-OF gradient estimates. MIXER algorithm USED-FOR algorithm. runtime complexity EVALUATE-FOR algorithm. binary tree - based hierarchical softmax USED-FOR algorithm. MS COCO dataset USED-FOR image captioning. Karel dataset CONJUNCTION MS COCO dataset. MS COCO dataset CONJUNCTION Karel dataset. Karel dataset USED-FOR neural program synthesis. neural program synthesis EVALUATE-FOR algorithm. image captioning EVALUATE-FOR algorithm. MS COCO dataset EVALUATE-FOR algorithm. Karel dataset EVALUATE-FOR algorithm. Generic is it. ,"This paper proposes a reinforcement learning-based algorithm for contextual sequence generation. The algorithm is based on the MIXER algorithm, but it uses gradient estimates (i.e., augment-REINFORCE-swap-merge) instead of a binary tree-based hierarchical softmax. The runtime complexity of the proposed algorithm is shown to be O(1/\sqrt{T}) where O(T) is the number of samples. Experiments on the Karel dataset and the MS COCO dataset for image captioning and neural program synthesis are conducted to evaluate the algorithm."
3325,SP:ab51af66e626b1b03bbf0de7a5237370e941925c,resource allocation game CONJUNCTION Stackelberg game. Stackelberg game CONJUNCTION resource allocation game. security game CONJUNCTION resource allocation game. resource allocation game CONJUNCTION security game. security game USED-FOR problem. Stackelberg game USED-FOR problem. stochastic - shortest - path MDP USED-FOR attacker's planning problem. Task is goal recognition control. OtherScientificTerm is defender's objective. ,"This paper studies the problem of goal recognition control. The problem is framed as a combination of a security game, a resource allocation game, and a Stackelberg game. The attacker's planning problem is formulated as a stochastic-shortest-path MDP, where the defender's objective is to maximize the shortest path between the current state and the goal state."
3326,SP:ab51af66e626b1b03bbf0de7a5237370e941925c,"agent USED-FOR action space. Method is reinforcement learning. OtherScientificTerm are value function, and worst case distinctiveness. ",This paper studies the problem of reinforcement learning in the setting where the agent has access to an action space and the value function is unknown. The authors propose to use the worst case distinctiveness as a measure of the agent’s ability to generalize to unseen states. 
3327,SP:ab51af66e626b1b03bbf0de7a5237370e941925c,non - optimal agents CONJUNCTION stochastic actions. stochastic actions CONJUNCTION non - optimal agents. Method is goal recognition framework. Generic is model. Task is GRD problem. OtherScientificTerm is interdictions. ,"This paper proposes a novel goal recognition framework. The model is motivated by the GRD problem, where the interdictions between non-optimal agents and stochastic actions are considered. "
3328,SP:e9d173bdf0b650fd093226cfb4607032c905cf61,batch size USED-FOR BigGAN. FID HYPONYM-OF GANs. computing power USED-FOR large batches. Generic is method. OtherScientificTerm is small batches. ,"This paper proposes a method called BigGAN to reduce the batch size of GANs (e.g., FID). The main idea is to use computing power for large batches instead of small batches. "
3329,SP:e9d173bdf0b650fd093226cfb4607032c905cf61,core - set selection USED-FOR GANs. sampled sets of datapoints USED-FOR minibatch size. features PART-OF image. Core - Set selection PART-OF method. Core - Set selection PART-OF GAN training. Generic is technique. OtherScientificTerm is random projections. ,This paper proposes a technique called Core-Set selection for training GANs. The method consists of two steps: 1) Core-set selection is an existing method in GAN training and 2) the authors propose to use sampled sets of datapoints to reduce the minibatch size. The technique is based on the idea that the features of an image should be sampled from random projections. 
3330,SP:e9d173bdf0b650fd093226cfb4607032c905cf61,"core - set selection USED-FOR active learning. method USED-FOR large mini - batches. Inception activations USED-FOR low - dimensional embedding. images HYPONYM-OF high - dimensional data. core - set selection USED-FOR GAN training. timing CONJUNCTION memory usage. memory usage CONJUNCTION timing. core - set selection USED-FOR mode collapse. synthetic dataset EVALUATE-FOR mode collapse. timing EVALUATE-FOR core - set selection. memory usage EVALUATE-FOR core - set selection. synthetic dataset EVALUATE-FOR core - set selection. it USED-FOR anomaly detection. Method are GANs, and core - sets. OtherScientificTerm is mini - batch sizes. ","This paper studies the problem of core-set selection in active learning. Inception activations are used to learn a low-dimensional embedding for high-dimensional data (e.g. images). The authors propose a method for training large mini-batches. The core-sets are selected in a way that minimizes the impact of mode collapse on GAN training. The authors evaluate the impact on timing and memory usage on a synthetic dataset, and show that the choice of mini-batch sizes does not have a significant impact on the performance. In addition, the authors show that it can be used for anomaly detection."
3331,SP:f174ef07670a31a3ce647910c59040a19ea52d7a,"mutual information USED-FOR optimality. RNN USED-FOR MI. Method are recurrent neural networks ( RNNs ), and RNNs. Generic is it. OtherScientificTerm is maximum likelihood training objective. ","This paper studies recurrent neural networks (RNNs) from the perspective of maximising the mutual information between the weights of the RNNs to improve optimality. In particular, the authors propose a maximum likelihood training objective (MI) and show that it can be viewed as an extension of the maximum likelihood of the original RNN. The authors also show that the MI can be used to train an RNN to be more efficient."
3332,SP:f174ef07670a31a3ce647910c59040a19ea52d7a,ability USED-FOR RNNs. past information USED-FOR RNNs. mutual - information estimators USED-FOR mini - batch settings. RNNs USED-FOR past information. Gaussian noise USED-FOR hidden states. suboptimal mutual - information FEATURE-OF internal representations. stochastic training USED-FOR LSTM. stochastic training USED-FOR RNNs. stochastic training USED-FOR applications. synthetizing hand - drawn sketches HYPONYM-OF applications. Method is RNN. ,"This paper studies the ability of RNNs to learn past information from past information using stochastic training. The authors propose to use mutual-information estimators in mini-batch settings, where the hidden states are masked with Gaussian noise, and the RNN is trained to estimate the past information. The main contribution of this paper is to show that RNN can learn to estimate past information with stochastically training, and that the internal representations exhibit suboptimal mutual-inference. The paper also proposes to use this insight to improve the performance of an LSTM by using a variant of the stochedastic training for applications such as synthetizing hand-drawn sketches."
3333,SP:f174ef07670a31a3ce647910c59040a19ea52d7a,"RNNs USED-FOR next - step prediction. next - step prediction USED-FOR IBL. RNNs USED-FOR IBL. stochastic representation USED-FOR IBL. additive Gaussian noise USED-FOR stochastic representation. additive Gaussian noise USED-FOR deterministic ) hidden state h. Method are Information Bottleneck Lagrangian ( IBL ), and approximate computations. ","This paper proposes the Information Bottleneck Lagrangian (IBL), which is a generalization of the previous work on RNNs for next-step prediction for IBL. The main idea is to use a stochastic representation of the (deterministic) hidden state h with additive Gaussian noise, which is then used to approximate the IBL with the help of the RNN. The approximate computations are shown to be computationally efficient."
3334,SP:f87a75fa12ddeb7538c4522d025e679f2c6dd237,"greedy operator USED-FOR approximated value function. delusional bias FEATURE-OF Q - learning. greedy operator USED-FOR policy representation. consistent algorithm USED-FOR small and finite state spaces. algorithm USED-FOR overcoming delusional bias. large state spaces FEATURE-OF delusional bias. smooth penalty term USED-FOR approximate consistency. smooth penalty term PART-OF Q - learning objective. heuristic methods USED-FOR search. heuristic methods USED-FOR Atari domains. OtherScientificTerm are Bellman backup, and Q - function approximators. ","This paper studies the problem of overcoming delusional bias in Q-learning with greedy operator on the approximated value function. The authors propose a consistent algorithm for both small and finite state spaces, and show that the algorithm is effective in overcoming delusions bias in large state spaces. They also propose to add a smooth penalty term to approximate consistency to the Q-learning objective, which they call Bellman backup. Finally, they apply heuristic methods to speed up the search in Atari domains and demonstrate the effectiveness of their Q-function approximators."
3335,SP:f87a75fa12ddeb7538c4522d025e679f2c6dd237,"solution USED-FOR delusional bias. delusional bias FEATURE-OF Deep Q - learning. max_a PART-OF Q - learning. heuristics USED-FOR search. Bellman error HYPONYM-OF scores. exponentiated Q - values USED-FOR sampling. sampling USED-FOR heuristics. consistency penalty CONJUNCTION approximate search. approximate search CONJUNCTION consistency penalty. DQN CONJUNCTION DDQN. DDQN CONJUNCTION DQN. they EVALUATE-FOR components. approximate search HYPONYM-OF components. consistency penalty HYPONYM-OF components. DDQN EVALUATE-FOR algorithm. DQN EVALUATE-FOR algorithm. OtherScientificTerm are Delusional bias, backed - up values, policy, non - delusional Q - functions, penalty term, and realizable policy class. Task is search problem. Method is greedy policy. ","This paper proposes a solution to the problem of delusional bias in Deep Q-learning. Delusional bias refers to the tendency of the learned policy to ignore the backed-up values. The authors consider the search problem in the setting where the policy is trained with non-delusional Q-functions. The main contribution of this paper is to introduce a penalty term that penalizes the deviation of the greedy policy from a realizable policy class (max_a) when max_a is a function of the number of samples in Q-Learning. This penalty term is defined in terms of the Bellman error, which is defined as the difference between the scores of the policy and the true policy (i.e., the true value of the agent). The authors then propose two heuristics for search based on sampling from exponentiated Q-values. They evaluate their algorithm on DQN and DDQN, and they show that they outperform existing methods in terms both of the two components: consistency penalty and approximate search."
3336,SP:f87a75fa12ddeb7538c4522d025e679f2c6dd237,"policy - consistent backups CONJUNCTION regression - based function approximation. regression - based function approximation CONJUNCTION policy - consistent backups. regression - based function approximation USED-FOR Q - learning. deep Q - learning USED-FOR delusional bias problem. soft consistency penalty USED-FOR delusional bias problem. soft consistency penalty USED-FOR exact consistency testing. OtherScientificTerm are regressors, and consistency condition. Generic are penalty, and model. Task is MSBE problem. ","This paper studies the delusional bias problem in deep Q-learning with policy-consistent backups and regression-based function approximation. The authors propose a soft consistency penalty for exact consistency testing, which penalizes regressors that do not satisfy the consistency condition. The proposed penalty is applied to both the training and testing phases of the model, and is shown to be effective in the MSBE problem."
3337,SP:827b0d2e2e3cf434c02b7f221bb9b2e0388e48b8,"generative latent variable model USED-FOR unsupervised scene decomposition. SPACE USED-FOR unsupervised scene decomposition. SPACE HYPONYM-OF generative latent variable model. model USED-FOR generating background. mixture model USED-FOR model. one HYPONYM-OF hierarchical mixture model. hierarchical mixture model USED-FOR model. ELBO USED-FOR model. boundary loss CONJUNCTION ELBO. ELBO CONJUNCTION boundary loss. 3D - room dataset CONJUNCTION Atari. Atari CONJUNCTION 3D - room dataset. Atari EVALUATE-FOR proposed. 3D - room dataset EVALUATE-FOR proposed. Task is generating foreground. OtherScientificTerm are binary latent variable, and bounding box separation. ","This paper proposes SPACE, a generative latent variable model for unsupervised scene decomposition. The model is trained for generating background, and for generating foreground, the model uses a hierarchical mixture model (one for binary latent variable and one for latent variable with bounding box separation). The boundary loss and ELBO are used to train the model. The proposed is evaluated on a 3D-room dataset and Atari."
3338,SP:827b0d2e2e3cf434c02b7f221bb9b2e0388e48b8,foreground - background probabilistic modeling framework USED-FOR unsupervised scene decomposition. background segments PART-OF scene background. foreground objects CONJUNCTION background segments. background segments CONJUNCTION foreground objects. framework USED-FOR scene foreground - background interactions. foreground objects PART-OF scene foreground - background interactions. chain rules USED-FOR foreground objects. chain rules USED-FOR scene foreground - background interactions. chain rules USED-FOR background segments. Atari environments CONJUNCTION 3D - Rooms. 3D - Rooms CONJUNCTION Atari environments. 3D - Rooms HYPONYM-OF synthetic datasets. Atari environments HYPONYM-OF synthetic datasets. method COMPARE baseline methods. baseline methods COMPARE method. baseline methods USED-FOR background segments. method USED-FOR background segments. Task is probabilistic scene decomposition. ,"This paper proposes a new foreground-background probabilistic modeling framework for unsupervised scene decomposition. The framework learns to model scene foreground - background interactions using chain rules to represent foreground objects and background segments in the scene background. Experiments are conducted on two synthetic datasets: Atari environments and 3D-Rooms. Results show that the proposed method is able to learn more complex background segments than baseline methods. The paper is well-written and well-motivated, and the paper is clearly written and easy to follow. However, there are some issues that need to be addressed in order to make this work applicable to the problem of probabilistically scene decomposing."
3339,SP:827b0d2e2e3cf434c02b7f221bb9b2e0388e48b8,"foreground / background separation CONJUNCTION object bounding box prediction. object bounding box prediction CONJUNCTION foreground / background separation. generative latent variable models USED-FOR scene decomposition. object bounding box prediction HYPONYM-OF scene decomposition. foreground / background separation HYPONYM-OF scene decomposition. parallel spatial attention mechanism USED-FOR architecture. Generic are SPACE, space, and approach. Task is foreground / background segmentation. OtherScientificTerm are foreground, object bounding box predictions, and Montezuma's Revenge. ","This paper addresses the problem of foreground/background segmentation. The authors propose to use generative latent variable models to perform scene decomposition (foreground/background separation and object bounding box prediction) in a space called SPACE. The architecture is based on a parallel spatial attention mechanism, where the foreground and background are sampled from the same space, and the foreground, background, and object bindinging box predictions are sampled sequentially. The proposed approach is inspired by Montezuma's Revenge."
3340,SP:73d7d614378cbb6a8d7347dca790675674e0eadb,EHP operation USED-FOR CNN compression method. depthwise separable convolution USED-FOR CNNs. it USED-FOR rank - k approach. EHP USED-FOR depthwise separable convolution. accuracy EVALUATE-FOR rank - k approach. benchmark datasets EVALUATE-FOR method. OtherScientificTerm is operation equivalence. ,"This paper proposes a new CNN compression method based on the EHP operation. In particular, it extends the rank-k approach by adding EHP to the depthwise separable convolution used in CNNs to improve the accuracy. The authors also show that the operation equivalence can be achieved. The proposed method is evaluated on several benchmark datasets."
3341,SP:73d7d614378cbb6a8d7347dca790675674e0eadb,depthwise separable convolution USED-FOR convolution methods. FALCON USED-FOR convolution kernel. TA / efficiency tradeoff EVALUATE-FOR FALCON. FALCON based method COMPARE state - of - the - art methods. state - of - the - art methods COMPARE FALCON based method. TA EVALUATE-FOR FALCON based method. efficiency EVALUATE-FOR FALCON based method. Method is fast and lightweight convolution. ,"This paper proposes a fast and lightweight convolution based on depthwise separable convolution, which is a popular technique in convolution methods. FALCON learns the convolution kernel in an end-to-end manner, which leads to fast and efficient convolution. The authors demonstrate the TA/efficiency tradeoff of the proposed method by comparing the TA and efficiency performance of their proposed method with the state-of-the-art methods."
3342,SP:73d7d614378cbb6a8d7347dca790675674e0eadb,"Falcon CONJUNCTION rank - k Falcon. rank - k Falcon CONJUNCTION Falcon. rank - k Falcon HYPONYM-OF model compression method. Falcon HYPONYM-OF model compression method. rank - k Falcon layer USED-FOR model. Falcon CONJUNCTION rank - k Falcon layer. rank - k Falcon layer CONJUNCTION Falcon. convolution layer CONJUNCTION Falcon. Falcon CONJUNCTION convolution layer. Falcon USED-FOR convolution kernel K. Method are CNN type of models, depthwise convolution kernel D, and DP. OtherScientificTerm is D+P's memory. Metric is memory saving. ","This paper proposes a new model compression method called Falcon and rank-k Falcon, which is a combination of Falcon and a rank-K Falcon. The main idea is to compress the CNN type of models by adding a depthwise convolution kernel D to each layer of the convolution layer, and then using D+P's memory to reduce the size of the model. The authors propose to use DP as a proxy for the number of parameters in D and show that using DP results in a significant memory saving. In addition, the authors also propose to combine a convolutionlayer from Falcon with a rank -k Falcon layer to further reduce the model's size."
3343,SP:35d45ed014320d8dff22f3531f805d15fa91dafb,normalization technique USED-FOR small batch sizes. group and batch normalizations USED-FOR normalization technique. Method is batch - normalization improvements. ,This paper proposes a new normalization technique for small batch sizes based on group and batch normalizations. The authors also propose batch-normalization improvements to improve the efficiency of the training process.
3344,SP:35d45ed014320d8dff22f3531f805d15fa91dafb,techniques USED-FOR Batch Normalization. medium batch size CONJUNCTION weight decay. weight decay CONJUNCTION medium batch size. inference example weighing CONJUNCTION medium batch size. medium batch size CONJUNCTION inference example weighing. inference example weighing HYPONYM-OF techniques. weight decay HYPONYM-OF techniques. medium batch size HYPONYM-OF techniques. batch sizes FEATURE-OF deep models. Method is batch and group normalization. ,"This paper proposes three techniques for Batch Normalization: inference example weighing, medium batch size, and weight decay. The authors argue that batch and group normalization should be combined to improve the performance of deep models with larger batch sizes. "
3345,SP:35d45ed014320d8dff22f3531f805d15fa91dafb,techniques USED-FOR deep network model. Batch Normalization ( BN ) USED-FOR techniques. weight decay USED-FOR regularizing convolution weights training. normalization methods CONJUNCTION weight decay. weight decay CONJUNCTION normalization methods. Method is multi - gpu training. ,This paper proposes techniques based on Batch Normalization (BN) to regularize the deep network model. The main contribution of this paper is to study the effect of different normalization methods and weight decay on regularizing convolution weights training. The experiments are conducted on multi-gpu training.
3346,SP:39d187474524c6b7de1ce6fd811ec53edae0a8fc,generative models USED-FOR sensitive user data. method USED-FOR sensitive user data. generative models USED-FOR method. differential privacy ( DP ) techniques USED-FOR privacy. federated learning ( FL ) setting USED-FOR scheme. generative models USED-FOR direct inspection of user data. OtherScientificTerm is aggregate updates. Generic is models. ,"This paper proposes a method to protect sensitive user data using generative models. The proposed scheme is based on the federated learning (FL) setting, where aggregate updates are shared among all models. To ensure privacy, differential privacy (DP) techniques are used. The authors also propose to use generative model for direct inspection of user data."
3347,SP:39d187474524c6b7de1ce6fd811ec53edae0a8fc,"hypothesis generation CONJUNCTION labeling. labeling CONJUNCTION hypothesis generation. data wrangling CONJUNCTION hypothesis generation. hypothesis generation CONJUNCTION data wrangling. model class selection CONJUNCTION validation. validation CONJUNCTION model class selection. data wrangling HYPONYM-OF modeling tasks. validation HYPONYM-OF modeling tasks. labeling HYPONYM-OF modeling tasks. hypothesis generation HYPONYM-OF modeling tasks. model class selection HYPONYM-OF modeling tasks. generative model USED-FOR data distribution. differentiable privacy guarantees FEATURE-OF federated learning methods. federated learning methods USED-FOR generative model. Task is real world federate learning problems. OtherScientificTerm are user level data privacy, features, and user privacy. ","This paper studies real world federate learning problems, where the modeling tasks include data wrangling, hypothesis generation, labeling, model class selection, and validation. The authors propose a generative model that uses federated learning methods with differentiable privacy guarantees to model the data distribution. The main contribution of this paper is to study user level data privacy, which is an important problem as features are not always available to the server and the user privacy is not guaranteed. "
3348,SP:39d187474524c6b7de1ce6fd811ec53edae0a8fc,"differentially private federated learning method USED-FOR GAN. differentially private federated learning method USED-FOR data bugging situations. privacy protection USED-FOR data bugging situations. method USED-FOR generator. centralised server USED-FOR generator. text and image modeling FEATURE-OF examples. OtherScientificTerm are discriminators, and DP - protected. Task is debugging data related issues. ","This paper proposes a differentially private federated learning method for GAN to tackle data bugging situations with privacy protection. The method trains a generator on a centralised server and trains discriminators that are DP-protected. Experiments on text and image modeling are conducted to demonstrate the effectiveness of the proposed method. The paper is well written and easy to follow. However, there is a lack of discussion on debugging data related issues."
3349,SP:1931ec4c3cd0dbb411cf1bc0f9776b7e26e3ad78,"motion interpolation USED-FOR techniques. technique USED-FOR physical realism. online motion generator USED-FOR human figures. Dynamics filter USED-FOR online motion generator. Task are computer animation of characters, and dynamics filtering. OtherScientificTerm is plausible transitions. ","This paper studies the problem of computer animation of characters, and proposes two techniques based on motion interpolation. Dynamics filter is used to train an online motion generator for human figures, which is a technique for improving physical realism. The main contribution of the paper is the introduction of dynamics filtering, which allows for more plausible transitions."
3350,SP:1931ec4c3cd0dbb411cf1bc0f9776b7e26e3ad78,local and global models PART-OF semi - parametric approach. approach USED-FOR local motion feature. approach USED-FOR motion sequence. bi - directional composition USED-FOR global motion composition. approach COMPARE baselines. baselines COMPARE approach. GAN CONJUNCTION VAE. VAE CONJUNCTION GAN. approach COMPARE GAN. GAN COMPARE approach. VAE HYPONYM-OF baselines. GAN HYPONYM-OF baselines. ,This paper proposes a semi-parametric approach that combines local and global models. The proposed approach first learns a local motion feature and then learns a global motion sequence based on bi-directional composition. Experiments show that the proposed approach outperforms baselines such as GAN and VAE.
3351,SP:1931ec4c3cd0dbb411cf1bc0f9776b7e26e3ad78,composable semi - parametric modeling USED-FOR generating long - range diverse and distinctive behaviors. memory bank USED-FOR motion patterns. non - parametric part HYPONYM-OF memory bank. deep neural networks USED-FOR high quality and smooth motion generation. deep neural networks PART-OF parametric part. non - parametric method CONJUNCTION parametric method. parametric method CONJUNCTION non - parametric method. non - parametric method CONJUNCTION rich pattern and diversities. rich pattern and diversities CONJUNCTION non - parametric method. ideas COMPARE approaches. approaches COMPARE ideas. datasets EVALUATE-FOR approaches. datasets EVALUATE-FOR ideas. ,"This paper proposes composable semi-parametric modeling for generating long-range diverse and distinctive behaviors. The parametric part consists of deep neural networks for high quality and smooth motion generation, while the non-parallel part is a memory bank for motion patterns. The authors propose a combination of the proposed ideas with a parametric method, a non -parametric method and a rich pattern and diversities. The proposed ideas are evaluated on three datasets and compared with existing approaches."
3352,SP:3a09bdf2e5a17d271f890fd28113202afb9ae761,"method USED-FOR hierarchical importance attribution. Method is BERT. OtherScientificTerm are non - additivity, non - linear function, and context independence. ","This paper proposes a method for hierarchical importance attribution based on BERT. The key idea is to use non-additivity, i.e. to use a non-linear function that is independent of the context. The authors argue that this allows for context independence."
3353,SP:3a09bdf2e5a17d271f890fd28113202afb9ae761,deep models USED-FOR compositional semantics. deep models USED-FOR hierarchical explanations. additive feature attribution CONJUNCTION context decomposition. context decomposition CONJUNCTION additive feature attribution. non - additivity CONJUNCTION context independence. context independence CONJUNCTION non - additivity. importance attribution scores USED-FOR hierarchical explanations. non - additivity HYPONYM-OF importance attribution scores. approach USED-FOR context - independent importance. sampling step CONJUNCTION contextual decomposition pipeline. contextual decomposition pipeline CONJUNCTION sampling step. contextual decomposition pipeline CONJUNCTION occlusion pipeline. occlusion pipeline CONJUNCTION contextual decomposition pipeline. sampling step USED-FOR score attribution approaches. approach COMPARE prior approaches. prior approaches COMPARE approach. attribution scores COMPARE prior approaches. prior approaches COMPARE attribution scores. human annotations COMPARE prior approaches. prior approaches COMPARE human annotations. human annotations USED-FOR attribution scores. approach USED-FOR attribution scores. OtherScientificTerm is surrounding word contexts. Method is language model. ,"This paper proposes to use deep models to model compositional semantics in hierarchical explanations using importance attribution scores (i.e., additive feature attribution and context decomposition). The authors argue that existing score attribution approaches rely on a sampling step, a contextual decomposition pipeline, and an occlusion pipeline. The authors propose an approach to model context-independent importance, which is a combination of non-additivity and context independence. In particular, the authors propose to sample from surrounding word contexts and use a language model to predict the importance of each word. The proposed approach is compared to prior approaches and compared to human annotations. The results show that the proposed approach produces better attribution scores than prior approaches."
3354,SP:3a09bdf2e5a17d271f890fd28113202afb9ae761,"hierarchical decomposition method USED-FOR natural language. mathematical formulation USED-FOR hierarchical decomposition method. contextual decomposition algorithm USED-FOR method. sampling method USED-FOR occlusion algorithm. Stanford Sentiment Treebank-2 CONJUNCTION Yelp Sentiment Polarity. Yelp Sentiment Polarity CONJUNCTION Stanford Sentiment Treebank-2. method COMPARE baselines. baselines COMPARE method. Yelp Sentiment Polarity CONJUNCTION TACRED relation extraction dataset. TACRED relation extraction dataset CONJUNCTION Yelp Sentiment Polarity. hierarchical explanations COMPARE baselines. baselines COMPARE hierarchical explanations. sentiment datasets EVALUATE-FOR LSTM and BERT models. Stanford Sentiment Treebank-2 HYPONYM-OF sentiment datasets. TACRED relation extraction dataset HYPONYM-OF sentiment datasets. Yelp Sentiment Polarity HYPONYM-OF sentiment datasets. LSTM and BERT models EVALUATE-FOR method. sentiment datasets EVALUATE-FOR method. Generic are formulation, and model. OtherScientificTerm is sampling step. ","This paper proposes a hierarchical decomposition method for natural language based on a mathematical formulation. The formulation is well-motivated and the proposed method is based on the contextual decomposition algorithm. The authors also propose a sampling method for the occlusion algorithm and show that the sampling step is computationally efficient. The method is evaluated on two sentiment datasets (Stanford Sentiment Treebank-2, Yelp Sentiment Polarity, and TACRED relation extraction dataset) and compared with two baselines that do not use hierarchical explanations. The proposed method outperforms both LSTM and BERT models."
3355,SP:9ec1740e58d1b07a6b1c6130ec7e23c370efb701,"method USED-FOR saliency map. them PART-OF saliency map. prior layers PART-OF saliency map. CNN USED-FOR saliency map. prior layers USED-FOR them. LOVI HYPONYM-OF heatmap. HSV USED-FOR heatmap. Method are SMOE, and gradient - based method. OtherScientificTerm are scale blocks, and backward passes. Generic is it. ","This paper proposes a method called SMOE, which is a gradient-based method for learning a saliency map of a given input image. The key idea of SMOE is to decompose the input image into scale blocks and combine them with the prior layers of the saliency model produced by a CNN. The main idea is to use HSV to generate a heatmap (e.g., LOVI) that is similar to the original heatmap produced by HSV, but instead of using it as the input, the authors propose to use it as an input. The authors also propose to do backward passes to make sure that the scale blocks are close to each other."
3356,SP:9ec1740e58d1b07a6b1c6130ec7e23c370efb701,"method USED-FOR saliency maps. information theoretic measure USED-FOR method. Generic are network, measure, approaches, and methods. ",This paper proposes a method for learning saliency maps based on an information theoretic measure. The idea is to train a network to predict the saliency of a given input image. The measure is then used to estimate the likelihood of the input image being saliency. The authors show that the proposed approaches outperform existing methods.
3357,SP:9ec1740e58d1b07a6b1c6130ec7e23c370efb701,approach USED-FOR saliency maps. SMOE scale USED-FOR saliency maps. neural network USED-FOR saliency maps. approach COMPARE approaches. approaches COMPARE approach. gradients USED-FOR approaches. output activation tensors PART-OF deep network. SMOE scale HYPONYM-OF operator. operator USED-FOR output activation tensors. operator USED-FOR global saliency map. Generic is it. OtherScientificTerm is backward passes. Method is gradient - based approaches. ,"This paper presents an approach to learn saliency maps from a neural network. The approach is similar to existing approaches that use gradients. The difference is that the output activation tensors of a deep network are parameterized by an operator called SMOE scale, which can be used to learn the saliency map. The idea is interesting and the paper is well-written. However, it suffers from a lack of comparison to existing gradient-based approaches. The paper also suffers from the lack of comparisons to backward passes."
3358,SP:7f11fa931f4085f7227cc870eba4a3aac4b1bf42,"approach USED-FOR non - autoregressive translation ( NAT ). predicting positions COMPARE word identities. word identities COMPARE predicting positions. predicting positions USED-FOR approach. heuristic search method USED-FOR position supervision. heuristic search method USED-FOR nearest neighbors. nearest neighbors PART-OF embedding space. nearest neighbors USED-FOR position supervision. OtherScientificTerm are word order, and positions. ",This paper proposes an approach for non-autoregressive translation (NAT) based on predicting positions instead of word identities. The key idea is to use a heuristic search method to find nearest neighbors in the embedding space to provide position supervision. The nearest neighbors are then used to predict the word order and the positions.
3359,SP:7f11fa931f4085f7227cc870eba4a3aac4b1bf42,"position USED-FOR latent variable. word order FEATURE-OF output word order. decoder USED-FOR position mapping. Task are non - autoregressive translation ( NAT ), and translation and paraphrase task. Method is predicting word order supervision. ","This paper studies the problem of non-autoregressive translation (NAT), where the latent variable is a position, and the goal is to predict the output word order in terms of word order. The authors propose to use a decoder to perform the position mapping, and to use predicting word order supervision. The experiments are conducted on a translation and paraphrase task."
3360,SP:7f11fa931f4085f7227cc870eba4a3aac4b1bf42,non - autoregressive model USED-FOR conditioned text generation. discrete latent variables USED-FOR generation order. discrete latent variables USED-FOR non - autoregressive decoder. machine translation CONJUNCTION paraphrase generation. paraphrase generation CONJUNCTION machine translation. paraphrase generation EVALUATE-FOR non - autoregressive models. machine translation EVALUATE-FOR non - autoregressive models. Method is marginal inference. ,"This paper proposes a non-autoregressive model for conditioned text generation. The authors propose to use discrete latent variables to model the generation order, and use marginal inference to estimate the generation of the next sentence. In addition, the authors also propose a novel non-auto-regressive decoder that uses discrete latent variable to model generation order. Experiments on machine translation and paraphrase generation show the effectiveness of the proposed non-Autoregressive models."
3361,SP:e1c40112901b6ff905ae0e221fd3df4f545acd08,random paths USED-FOR generator. Task is generative model analysis. OtherScientificTerm is features. ,"This paper presents a generative model analysis. The main idea is to use random paths to train the generator, which is then used to generate the features. The paper is well-written and easy to follow."
3362,SP:e1c40112901b6ff905ae0e221fd3df4f545acd08,approach USED-FOR variability. approach USED-FOR GAN generation. generator architecture USED-FOR randomization. randomization FEATURE-OF routing. random input vector USED-FOR generation. Gaussian distribution USED-FOR random input vector. OtherScientificTerm is layer. Method is forward propagation. ,This paper proposes an approach to reduce variability in GAN generation. The key idea is to use a generator architecture that learns randomization in routing. The generation is done using a random input vector drawn from a Gaussian distribution. The randomization of the layer is learned by forward propagation.
3363,SP:e1c40112901b6ff905ae0e221fd3df4f545acd08,"random dropout USED-FOR generator. dropout USED-FOR activations. Method are generator of GANs, and single component. OtherScientificTerm is random input vector. ","This paper studies the generator of GANs. The authors propose to use random dropout to train the generator. The idea is to train a single component that takes as input a random input vector and outputs a single output vector. Then, the dropout is applied to the activations."
3364,SP:1fec5468baaccb4a956399a829b62ac47494a6ac,CNNs USED-FOR spherical data. architecture COMPARE frameworks. frameworks COMPARE architecture. graph USED-FOR discretization of a sphere. discretization of a sphere USED-FOR architecture. discretization of a sphere USED-FOR frameworks. rotation equivalence FEATURE-OF sphere. computational cost EVALUATE-FOR model. ,"This paper studies CNNs for spherical data. The proposed architecture is similar to existing frameworks that use discretization of a sphere as a graph. The main difference is that instead of discretizing a sphere with rotation equivalence, the authors propose to discretize the sphere with respect to the rotation of the sphere. The authors show that the proposed model can reduce the computational cost of the model."
3365,SP:1fec5468baaccb4a956399a829b62ac47494a6ac,convolution USED-FOR spherical neural network. graph CNN formulation CONJUNCTION pooling strategy. pooling strategy CONJUNCTION graph CNN formulation. pooling strategy USED-FOR hierarchical pixelations of the sphere. graph of connected pixels USED-FOR discretized sphere. climate event segmentation CONJUNCTION uneven sampling. uneven sampling CONJUNCTION climate event segmentation. cosmological mode classification CONJUNCTION climate event segmentation. climate event segmentation CONJUNCTION cosmological mode classification. 3D object recognition CONJUNCTION cosmological mode classification. cosmological mode classification CONJUNCTION 3D object recognition. sampling flexibility CONJUNCTION rotation equivariance. rotation equivariance CONJUNCTION sampling flexibility. computational efficiency CONJUNCTION sampling flexibility. sampling flexibility CONJUNCTION computational efficiency. OtherScientificTerm is geodesic distance. ,"This paper proposes a spherical neural network with convolution. The authors propose a graph CNN formulation and a pooling strategy to learn hierarchical pixelations of the sphere. The discretized sphere is represented as a graph of connected pixels. The geodesic distance between the nodes of the graph is computed. Experiments are conducted on 3D object recognition, cosmological mode classification, climate event segmentation, and uneven sampling. Results show improvements in computational efficiency, sampling flexibility, and rotation equivariance."
3366,SP:1fec5468baaccb4a956399a829b62ac47494a6ac,graphical representation CONJUNCTION graph - convolutions. graph - convolutions CONJUNCTION graphical representation. method USED-FOR learning over spherical data. graph - convolutions USED-FOR method. graphical representation USED-FOR method. method USED-FOR equivariance. rotations FEATURE-OF equivariance. distance - based similarity measure USED-FOR graph. equivariance FEATURE-OF representation. it USED-FOR non - uniform data. problems EVALUATE-FOR DeepSphere. OtherScientificTerm is sphere. ,This paper proposes a method for learning over spherical data using a graphical representation and graph-convolutions. The method aims to improve the equivariance to rotations in the sphere. The authors propose a distance-based similarity measure to measure the similarity between a graph and its neighbors. They evaluate DeepSphere on three problems and show that it can generalize to non-uniform data.
3367,SP:ae544fa9abd539e0c2e77fdb5541f5c5194feb9f,compression risk FEATURE-OF domain - invariant representations. compression risks CONJUNCTION adaptability. adaptability CONJUNCTION compression risks. compression risks EVALUATE-FOR Learning domain - invariant representations. gamma(H ) USED-FOR compression risk. domain discrepancy CONJUNCTION compression. compression CONJUNCTION domain discrepancy. source error CONJUNCTION domain discrepancy. domain discrepancy CONJUNCTION source error. Learning weighted representations USED-FOR source error. invariance CONJUNCTION compression. compression CONJUNCTION invariance. Learning weighted representations USED-FOR compression. Learning weighted representations USED-FOR domain discrepancy. ,"Learning domain-invariant representations has been shown to reduce compression risk and adaptability. This paper proposes to use gamma(H) to reduce the compression risk. Learning weighted representations can reduce the source error, domain discrepancy, and compression. The paper is well-written and well-motivated. The authors clearly discuss the connection between invariance, compression, and domain discrepancy."
3368,SP:ae544fa9abd539e0c2e77fdb5541f5c5194feb9f,theoretical frameworks USED-FOR unsupervised domain adaptation. unsupervised domain adaptation USED-FOR learning invariant representation. theoretical frameworks USED-FOR learning invariant representation. Generic is bound. OtherScientificTerm is compression information. Metric is adaptability. Method is weighting representations. ,This paper presents theoretical frameworks for unsupervised domain adaptation for learning invariant representation. The main result of the paper is a bound on how much compression information is needed to achieve adaptability. The paper also provides a theoretical analysis of weighting representations.
3369,SP:ae544fa9abd539e0c2e77fdb5541f5c5194feb9f,theoretical framework USED-FOR domain adaptation. upper bound USED-FOR hypothesis space. adaptability term PART-OF classical domain adaptation theory. hypothesis space FEATURE-OF adaptability term. upper bound USED-FOR adaptability term. upper bound USED-FOR adaptability term. Generic is theory. ,"This paper presents a theoretical framework for domain adaptation. The main contribution of the paper is to provide an upper bound on the hypothesis space of the adaptability term in classical domain adaptation theory. The paper is well-written and easy to follow, and the theory is clearly presented."
3370,SP:39126802d517f93bdcbc47708a6aa1ed13bf2800,"algorithm USED-FOR attribute of GUI elements. rasterized design images USED-FOR algorithm. rasterized design images USED-FOR attribute of GUI elements. color CONJUNCTION padding. padding CONJUNCTION color. border width CONJUNCTION color. color CONJUNCTION border width. image USED-FOR attributes. padding HYPONYM-OF attributes. border width HYPONYM-OF attributes. color HYPONYM-OF attributes. OtherScientificTerm are UI element, rendering, and image pixel - perfect. Method are convolutional DNN, and policy \pi. ","This paper presents an algorithm for learning the attribute of GUI elements from rasterized design images. The key idea is to learn the attributes of the image, i.e., border width, color, padding, etc. and then to render the UI element using a convolutional DNN. The rendering is done in a way that the image pixel-perfect, and the policy \pi is learned."
3371,SP:39126802d517f93bdcbc47708a6aa1ed13bf2800,"Siamese networks CONJUNCTION imitation learning. imitation learning CONJUNCTION Siamese networks. approach USED-FOR reverse - engineering webpages. Siamese networks USED-FOR approach. imitation learning USED-FOR approach. synthetic data USED-FOR reverse - engineer training. attribute refinement PART-OF elements. imitation learning USED-FOR attribute refinement. Generic are it, and step. ","This paper presents an approach to reverse-engineering webpages using Siamese networks and imitation learning. The key idea is to use synthetic data for reverse-engine training, and to use it as a training set for the next step. The two elements are attribute refinement, which is based on imitation learning, and the authors conduct extensive experiments to demonstrate the effectiveness of their approach."
3372,SP:39126802d517f93bdcbc47708a6aa1ed13bf2800,"synthetic datasets USED-FOR model. black box rendering engine USED-FOR synthetic datasets. pixel based metrics CONJUNCTION mean squared error. mean squared error CONJUNCTION pixel based metrics. Generic are approach, and them. OtherScientificTerm are user interface, interface, and attribute space. Material is real - world datasets. ","This paper presents an approach to learning a model from synthetic datasets generated by a black box rendering engine. The approach is based on the observation that the user interface can be seen as a function of the attribute space. The paper proposes to learn an interface that maps the input to an attribute space, and then use them to train a model on the synthetic datasets. The authors provide pixel based metrics, as well as mean squared error, to evaluate the performance of the learned model. Experiments on real-world datasets are conducted to show the effectiveness of the proposed approach."
3373,SP:1c5d31363faf2b8c43f2698ad426bfffcc02ad03,"adversarially trained networks USED-FOR intermediate representations. strategies USED-FOR robust transfer. robust model USED-FOR robust transfer. CIFAR10 CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION CIFAR10. ImageNet CONJUNCTION CIFAR10/100. CIFAR10/100 CONJUNCTION ImageNet. Task are robustly transfer learning, and transfer. Method are linear layer, and lifelong learning strategies. ","This paper studies the problem of robustly transfer learning, where adversarially trained networks are used to learn intermediate representations. The authors propose two strategies for robust transfer. First, the authors propose a linear layer, where the weights of the linear layer are shared across all layers. Second, they propose lifelong learning strategies, where each layer is trained to be robust to different types of transfer. Experiments are conducted on ImageNet, CIFAR10, CifAR100, and CIFR10/100. Results show that the proposed strategies are effective at robust transfer with a robust model."
3374,SP:1c5d31363faf2b8c43f2698ad426bfffcc02ad03,"adversarial robustness EVALUATE-FOR transfer learning. strategies COMPARE baselines. baselines COMPARE strategies. Method are deep neural network classifier, and robust classifier. ",This paper studies adversarial robustness in transfer learning. The authors propose to train a deep neural network classifier that is robust to perturbations in the input data. The robust classifier is trained on a set of examples from the source data and the target data. Experiments show that the proposed strategies outperform the baselines.
3375,SP:1c5d31363faf2b8c43f2698ad426bfffcc02ad03,"validation accuracy CONJUNCTION robustness. robustness CONJUNCTION validation accuracy. robustness FEATURE-OF adversarial attacks. CIFAR task EVALUATE-FOR adversarial attacks. robustness EVALUATE-FOR training. validation accuracy EVALUATE-FOR training. ImageNet - based models USED-FOR low - data regime. ImageNet - based models USED-FOR transfer. feature extractor USED-FOR deeper networks. validation accuracy EVALUATE-FOR robust - trained source models. Method are transfer learning, and Learning without Forgetting strategies. Task are transfer learning task, and transfer learning setting. OtherScientificTerm is robust features. ","This paper studies the problem of transfer learning, where the goal is to improve the validation accuracy and robustness to adversarial attacks on the CIFAR task. The authors propose to use ImageNet-based models for transfer in the low-data regime, which is an important problem in the transfer learning setting. The main contribution of this paper is to propose Learning without Forgetting strategies, which aims to learn a feature extractor that can be used to extract deeper networks that are more robust to robust features. This is a transfer learning task, and the authors show that training with robust-trained source models leads to better validation accuracy as well as better robustness on the original training set."
3376,SP:06a047ae70a1a25dc6e8f317d6e492e211ad17ce,neural iterated learning algorithm USED-FOR dominance of high compositional language. dominance of high compositional language PART-OF multi - agent communication game. language USED-FOR agent. topological similarity FEATURE-OF language. topological similarity FEATURE-OF zero - shot performance. pre - training strategies USED-FOR neural agent. high compositional language FEATURE-OF neural agent. Method is referential game. ,This paper proposes a neural iterated learning algorithm that aims to address the dominance of high compositional language in a multi-agent communication game. The agent is trained using a language with high topological similarity to zero-shot performance. This is achieved by playing a referential game in which the agent has to choose a language that maximizes its similarity to the target language. The authors also propose pre-training strategies to improve the performance of the neural agent trained with high composition of language.
3377,SP:06a047ae70a1a25dc6e8f317d6e492e211ad17ce,neural agents USED-FOR compositional language. supervised learning phase USED-FOR randomly - initialized speaker and listener. supervised learning phase CONJUNCTION self - play phase. self - play phase CONJUNCTION supervised learning phase. supervised learning phase PART-OF phases. phase HYPONYM-OF phases. self - play phase HYPONYM-OF phases. speaker ’s language USED-FOR dataset. procedure USED-FOR compositional languages. hyperparameters USED-FOR procedure. symbolic referential game USED-FOR compositional languages. Task is emergence of compositional language. Method is iterated learning method. ,"This paper studies the emergence of compositional language by neural agents. The authors propose an iterated learning method that consists of two phases: a supervised learning phase for a randomly-initialized speaker and listener, and a self-play phase where the speaker’s language is used to generate a dataset. The proposed procedure is able to learn compositional languages from a symbolic referential game with different hyperparameters."
3378,SP:06a047ae70a1a25dc6e8f317d6e492e211ad17ce,two - players games USED-FOR language emergence. neural iterated learning model USED-FOR Compositional languages. neural iterated learning model USED-FOR comopsitional languages. OtherScientificTerm is compositional languages. Generic is they. ,"This paper studies the problem of language emergence in two-players games. Compositional languages are learned using a neural iterated learning model. The authors argue that compositional languages can be seen as a special case of two-player games, and that they can be used to study language emergence."
3379,SP:add48154b31c13f48aef740e665f23694fa83681,approach USED-FOR fitting Markov Random Fields ( MRFs ). MRF structures USED-FOR algorithm. OtherScientificTerm is Markov Random Fields ( MRFs ). Task is learning. Method is MRF models. ,"This paper presents an approach to fitting Markov Random Fields (MRFs) in the context of learning. The proposed algorithm is based on MRF structures, and is motivated by the fact that the MRF models have been shown to perform poorly in practice. "
3380,SP:add48154b31c13f48aef740e665f23694fa83681,"variational distributions USED-FOR model. variational distributions USED-FOR inference of latent variables. algorithm USED-FOR MRFs. MRFs USED-FOR inference. MRFs USED-FOR learning. inference CONJUNCTION learning. learning CONJUNCTION inference. algorithm USED-FOR inference. model USED-FOR inference of latent variables. algorithm USED-FOR learning. variational distributions USED-FOR partition function. NVIL USED-FOR partition function. minimax operation USED-FOR it. adversial training HYPONYM-OF GAN. digits dataset CONJUNCTION Anneal importance sampling. Anneal importance sampling CONJUNCTION digits dataset. OtherScientificTerm are NLL, and log partition function. ","This paper proposes an algorithm to learn MRFs for inference and learning. The model is used for inference of latent variables using variational distributions. The partition function is learned using NVIL, where the NLL is a log partition function and it is parameterized as a minimax operation. The authors show that adversial training, a GAN, can be learned using this algorithm. Experiments are conducted on digits dataset and Anneal importance sampling."
3381,SP:add48154b31c13f48aef740e665f23694fa83681,black - box style learning algorithm USED-FOR Markov Random Fields ( MRF ). variational approximations USED-FOR positive phase. variational approximations USED-FOR log likelihood objective function. positive phase FEATURE-OF log likelihood objective function. variational approach CONJUNCTION variational approximations. variational approximations CONJUNCTION variational approach. approach COMPARE variational approach. variational approach COMPARE approach. modeling of the latent variable prior PART-OF variational approximations. approximating distribution USED-FOR modeling of the latent variable prior. variational approximations USED-FOR negative phase. OtherScientificTerm is latent variable prior. ,This paper proposes a black-box style learning algorithm for Markov Random Fields (MRF). The proposed approach is a combination of a variational approach and variational approximations to the log likelihood objective function in the positive phase. The modeling of the latent variable prior is done by approximating distribution. The negative phase is approximated with variational approximation.
3382,SP:b875f6417663e43dded41b6a6f1b9ab49ad954a2,"HER USED-FOR image - based domain. filter transitions USED-FOR false negative rewards. simulated 2d and 3d reaching CONJUNCTION rope task. rope task CONJUNCTION simulated 2d and 3d reaching. simulated 2d and 3d reaching EVALUATE-FOR learning. method USED-FOR real - world robot. OtherScientificTerm are epsilon - ball, and positive and negative rewards. Method is goal relabeling. Material is images. ","This paper proposes HER for the image-based domain. The key idea is to use the epsilon-ball as the goal, and use goal relabeling to distinguish between positive and negative rewards. The paper also proposes to use filter transitions to prevent false negative rewards from appearing in the images. Experiments on simulated 2d and 3d reaching and a rope task demonstrate the effectiveness of the learning. The proposed method can be applied to a real-world robot."
3383,SP:b875f6417663e43dded41b6a6f1b9ab49ad954a2,"goal - conditioned RL USED-FOR self - supervised reinforcement learning. relabeling trick USED-FOR binary rewards. relabeling trick USED-FOR it. relabeling USED-FOR tricks. Generic are approach, and one. ","This paper studies the problem of self-supervised reinforcement learning with goal-conditioned RL in the context of goal-conditional RL. The approach is not new, but it uses the relabeling trick to learn binary rewards, which is an interesting idea. However, it is not the first one to use relabeled to learn tricks. "
3384,SP:b875f6417663e43dded41b6a6f1b9ab49ad954a2,reward function USED-FOR rewards. images USED-FOR reward function. images USED-FOR rewards. Task is goal conditioned reinforcement learning. Material is vision. ,"This paper studies goal conditioned reinforcement learning in the context of vision. Specifically, the authors propose a reward function that uses images as rewards, where the reward function is conditioned on the quality of the images. "
3385,SP:8ae78a6640be13e511242eab64101f74ebc4b30a,"methods USED-FOR verification problems. cross - nonlinearity CONJUNCTION cross - position dependency. cross - position dependency CONJUNCTION cross - nonlinearity. verification problems USED-FOR Transformers. robustness bounds COMPARE naive Interval Bound Propagation. naive Interval Bound Propagation COMPARE robustness bounds. Method are robustness verification techniques, and sentiment analysis. ","This paper proposes methods to solve verification problems for Transformers using existing methods. The main contribution of this paper is to provide robustness verification techniques that are robust to cross-nonlinearity, cross-position dependency, and cross-interval dependency. The robustness bounds are compared to naive Interval Bound Propagation. The authors also provide a sentiment analysis to support their claims."
3386,SP:8ae78a6640be13e511242eab64101f74ebc4b30a,CROWN framework USED-FOR robustness verification. robustness verification USED-FOR transformers. MLP CONJUNCTION CNNs. CNNs CONJUNCTION MLP. CNNs CONJUNCTION RNNs. RNNs CONJUNCTION CNNs. CROWN framework USED-FOR architectures. propagating linear bounds USED-FOR CROWN framework. MLP HYPONYM-OF architectures. RNNs HYPONYM-OF architectures. CNNs HYPONYM-OF architectures. cross - nonlinearities CONJUNCTION cross - position dependencies. cross - position dependencies CONJUNCTION cross - nonlinearities. backward propagation of bounds USED-FOR CROWN. forward propagation of bounds CONJUNCTION back - propagation of bounds. back - propagation of bounds CONJUNCTION forward propagation of bounds. forward propagation of bounds USED-FOR self attention layers. computational complexity EVALUATE-FOR method. forward propagation USED-FOR loose bounds. mixed approach ( forward - backward ) COMPARE backward method. backward method COMPARE mixed approach ( forward - backward ). OtherScientificTerm is Transformers. ,"This paper proposes a CROWN framework for robustness verification for transformers. The authors propose propagating linear bounds for different architectures (MLP, CNNs, RNNs, etc) and architectures with cross-nonlinearities, cross-position dependencies, and cross-time dependencies. CROWN is based on the idea of backward propagation of bounds, which is similar to the back-propagation of bounds used in CROWN. In addition, the authors propose to use forward propagation of the self attention layers, which uses forward propagation to propagate loose bounds. The proposed method is shown to reduce the computational complexity by a factor of $\Omega(\sqrt{T})$. The authors compare the proposed mixed approach (forward-backward) with the backward method and show that the proposed method performs better. Transformers are evaluated on a variety of datasets."
3387,SP:8ae78a6640be13e511242eab64101f74ebc4b30a,algorithm USED-FOR transformers. algorithm USED-FOR robustness. robustness EVALUATE-FOR transformers. self - attention layers FEATURE-OF transformers. cross nonlinearity CONJUNCTION cross position dependency. cross position dependency CONJUNCTION cross nonlinearity. lower bounds COMPARE Interval Boundary Propagation ( IBP ) method. Interval Boundary Propagation ( IBP ) method COMPARE lower bounds. backward propagation USED-FOR Interval Boundary Propagation ( IBP ) method. this USED-FOR tight bounds. multiplication ( xy ) CONJUNCTION division ( x / y ). division ( x / y ) CONJUNCTION multiplication ( xy ). bounds USED-FOR multiplication ( xy ). bounds USED-FOR division ( x / y ). tight bounds USED-FOR self - attention layer computations. bounds USED-FOR tight bounds. they COMPARE IBP method. IBP method COMPARE they. computation time EVALUATE-FOR IBP method. it CONJUNCTION backward process. backward process CONJUNCTION it. forward process CONJUNCTION backward process. backward process CONJUNCTION forward process. computation time EVALUATE-FOR they. ,"This paper proposes a new algorithm for improving the robustness of transformers with self-attention layers. The main contribution of this paper is to derive tight bounds on the self attention layer computations using bounds on multiplication (xy), division (x/y), and cross position dependency. These lower bounds are compared to the Interval Boundary Propagation (IBP) method with backward propagation. The results show that they outperform the IBP method in terms of computation time when combined with forward process and backward process."
3388,SP:92cb7b1e88f3c8883ae6123c19e1ba24622464e6,"world knowledge USED-FOR pretraining approach. pretrained models USED-FOR NLP tasks. question answering CONJUNCTION fine - grained entity typing. fine - grained entity typing CONJUNCTION question answering. zero - shot fact completion CONJUNCTION question answering. question answering CONJUNCTION zero - shot fact completion. pretrained model COMPARE baselines. baselines COMPARE pretrained model. objective PART-OF masked LM objective. objective EVALUATE-FOR pretrained model. masked LM objective EVALUATE-FOR pretrained model. tasks EVALUATE-FOR baselines. tasks EVALUATE-FOR pretrained model. zero - shot fact completion HYPONYM-OF tasks. fine - grained entity typing HYPONYM-OF tasks. question answering HYPONYM-OF tasks. Material are Wikipedia text, and Wikidata. OtherScientificTerm is context change. ","This paper proposes a pretraining approach based on world knowledge, where the pretrained models are used to solve NLP tasks. The authors propose to use Wikipedia text as a starting point, and then use Wikidata as a second source of world knowledge. The pretrained model is evaluated on three tasks: zero-shot fact completion, question answering, and fine-grained entity typing. The proposed objective is added to the masked LM objective, and compared to two baselines on the three tasks. "
3389,SP:92cb7b1e88f3c8883ae6123c19e1ba24622464e6,"binary classification task USED-FOR target. MLM loss USED-FOR binary classification task. tasks EVALUATE-FOR model. Method is entity centric text embeddings. Task is knowledge pre - training. OtherScientificTerm is adversarial "" target. ","This paper proposes entity centric text embeddings to address the problem of knowledge pre-training. The authors propose to use a binary classification task with MLM loss as a target to train the model. The idea is to use the ""adversarial"" target as a way to improve the performance of the model on a variety of tasks."
3390,SP:92cb7b1e88f3c8883ae6123c19e1ba24622464e6,"BERT HYPONYM-OF pre - training of language models. Method is language models. Material are English Wikipedia, and Wikidata. Generic is it. Task is binary prediction task. ","This paper proposes BERT, a pre-training of language models, which is a variant of BERT. BERT is an extension of the work of [1], which pre-train language models on English Wikipedia. The main difference is that it is based on a binary prediction task, where the goal is to predict the correct answer to a binary question. The authors conduct experiments on Wikidata, and show that BERT outperforms [1]."
3391,SP:4395d6f3e197df478eee84e092539dc370babd97,labeled dataset USED-FOR unlabeled dataset. unknown categories PART-OF unlabeled dataset. self - supervised learning USED-FOR representations. robust rank - based metric USED-FOR estimates of similarity / dissimilarity. consistency - based regularization USED-FOR optimization. robust rank - based metric CONJUNCTION consistency - based regularization. consistency - based regularization CONJUNCTION robust rank - based metric. robust rank - based metric USED-FOR optimization. labeled / unlabeled losses USED-FOR Joint optimization / refinement. ImageNet CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION ImageNet. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. OmniGlot CONJUNCTION ImageNet. ImageNet CONJUNCTION OmniGlot. CIFAR-100 CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR-100. OmniGlot HYPONYM-OF datasets. SVHN HYPONYM-OF datasets. CIFAR-100 HYPONYM-OF datasets. ImageNet HYPONYM-OF datasets. CIFAR-10 HYPONYM-OF datasets. Task is unsupervised object discovery. Generic is task. Material is labeled data. OtherScientificTerm is labeled classes. ,"This paper addresses the problem of unsupervised object discovery, where the unlabeled dataset contains unknown categories. This task is challenging because of the lack of labeled data. To address this problem, the authors propose to use self-supervised learning to learn representations that are similar to the labeled classes. Joint optimization/refinement is performed with labeled/unlabeled losses, using a robust rank-based metric to provide estimates of similarity/dissimilarity, and a consistency-based regularization to guide optimization. Experiments are conducted on several datasets (OmniGlot, ImageNet, CIFAR-10, and SVHN)."
3392,SP:4395d6f3e197df478eee84e092539dc370babd97,"supervised manners USED-FOR multi - stage training framework. multi - stage training framework USED-FOR robust feature extractor. self - supervised learning USED-FOR network. labelled data USED-FOR it. activation knowledge FEATURE-OF labelled classes. activation knowledge USED-FOR rank statistics. pseudo labels USED-FOR network. method COMPARE SOTA. SOTA COMPARE method. Task is clustering unseen classes. OtherScientificTerm are activated dimensions, and rank. ","This paper proposes a multi-stage training framework in supervised manners to train a robust feature extractor for the task of clustering unseen classes. The network is trained using self-supervised learning, where it is trained on labelled data. The activation knowledge of the labelled classes is used to compute the rank statistics, which are then used to train the network with pseudo labels. The activated dimensions of the pseudo labels are used to estimate the rank. The proposed method is compared with SOTA."
3393,SP:4395d6f3e197df478eee84e092539dc370babd97,labeled and unlabeled data USED-FOR self - supervised learning. self - supervised learning USED-FOR features. rank statistic USED-FOR similarity. knowledge USED-FOR joint supervised - unsupervised objective. Generic is methodology. Material is unlabeled dataset. ,This paper studies the problem of self-supervised learning on both labeled and unlabeled data. The main contribution of the paper is to propose a methodology to estimate the similarity between the labels and features learned by self-survised learning. The similarity is estimated using a rank statistic. The authors also propose a joint supervised-unsupervised objective based on this knowledge. Experiments are conducted on the labeled dataset as well as the unlabeled dataset.
3394,SP:068c4e93c135968aef2637d2bfcba727a3c0f001,"agents USED-FOR visual planning. method USED-FOR agents. graph HYPONYM-OF topological map. semi parametric topological memory HYPONYM-OF work. conditional variational auto - encoder USED-FOR graph. random rollouts USED-FOR graph. rollouts USED-FOR connectivity predictor. connectivity predictor USED-FOR graph. contrastive loss USED-FOR rollouts. contrastive loss USED-FOR connectivity predictor. context vector USED-FOR connectivity network. planner CONJUNCTION policy. policy CONJUNCTION planner. inverse model USED-FOR policy. OtherScientificTerm are maze, nodes, edges, accessibility, context image, and connectivity probabilities. Task is Training. ","This paper proposes a method for training agents for visual planning. The work is similar to semi parametric topological memory, where a topological map (e.g. a graph) is learned using a conditional variational auto-encoder. The key difference is that instead of using random rollouts for the graph, the authors propose to use rollouts to train a connectivity predictor, which is trained using a contrastive loss. The connectivity network is trained on a context vector, where each node is associated with a maze, and edges are associated with accessibility. Training is done in two stages: First, a context image is generated, and the connectivity probabilities are used to train the planner and the policy using an inverse model."
3395,SP:068c4e93c135968aef2637d2bfcba727a3c0f001,"Conditional Variational Autoencoder USED-FOR it. model USED-FOR hallucinated states. Dijkstra USED-FOR edges. Method are visual planning approach, and Contrastive Predictive Coding ( CPC ) model. ","This paper proposes a visual planning approach that is based on the Contrastive Predictive Coding (CPC) model. In particular, it uses the Conditional Variational Autoencoder (CACD) and uses the Dijkstra as the edges. The model is trained to predict hallucinated states."
3396,SP:068c4e93c135968aef2637d2bfcba727a3c0f001,"hallucinated nodes CONJUNCTION energy cost function. energy cost function CONJUNCTION hallucinated nodes. hallucinated nodes USED-FOR HTM. semiparametric topological memory method USED-FOR HTM. energy cost function USED-FOR HTM. CVAE USED-FOR hallucination. contrastive loss USED-FOR energy cost function. graph USED-FOR top view planning problems. Generic are approach, and method. OtherScientificTerm is robustness score. ","This paper proposes a semiparametric topological memory method for training HTM with hallucinated nodes and energy cost function. The proposed approach is based on CVAE, where hallucination is modeled as a contrastive loss, and the robustness score is computed as a function of the number of hallucination. The method is applied to top view planning problems on a graph."
3397,SP:907d92896eda706e1526debb5a87b41bb1e978e0,"Generic are model, models, method, and it. OtherScientificTerm are real world scenarios, and adversarial samples. ","This paper proposes a new model for learning adversarial samples. The idea is to train a model to predict the likelihood of an adversarial sample. The authors argue that models trained on adversarial examples should be able to generalize well to real world scenarios. The method is well motivated and the experiments show that it can generalize to a wide range of environments. However, the authors do not provide any theoretical analysis of the proposed method, and it is not clear to me how the method can be used in practice. "
3398,SP:907d92896eda706e1526debb5a87b41bb1e978e0,image net HYPONYM-OF benchmark   datasets. Generic is algorithm. ,This paper proposes an algorithm for learning to predict the label of an image. The authors provide a theoretical analysis of the algorithm and provide some empirical results on two benchmark datasets (image net and image-based) to support their claims. 
3399,SP:907d92896eda706e1526debb5a87b41bb1e978e0,Generic is algorithm. OtherScientificTerm is dataset - specific spurious bias. Material is real world samples. ,This paper proposes a new algorithm to detect the presence of dataset-specific spurious bias. Experiments are conducted on real world samples.
3400,SP:82777947d2377efa897c6905261f5375b29a4c19,distribution of support image embeddings USED-FOR model. Task is few - shot classification. Generic is approaches. Method is batch norm. OtherScientificTerm is embedding. ,"This paper addresses the problem of few-shot classification, where the model is trained on a distribution of support image embeddings. The authors propose two approaches: (1) a batch norm that encourages the embedding to be similar to the support image, and (2) a model that learns to distinguish between support and non-support images."
3401,SP:82777947d2377efa897c6905261f5375b29a4c19,"technique USED-FOR prototypical networks. distance COMPARE norm of the query embedding. norm of the query embedding COMPARE distance. distance EVALUATE-FOR prototype. distribution CONJUNCTION centroid. centroid CONJUNCTION distribution. multivariate gaussian USED-FOR distribution. multivariate gaussian USED-FOR centroid. prototype USED-FOR random negative samples. Task is 1 - way few shot classification task. Generic are it, and proposal. ","This paper proposes a technique for learning prototypical networks. The main idea is to use a 1-way few shot classification task, where the distance between the prototype and the norm of the query embedding is used to evaluate the distance of the prototype to random negative samples. To do this, a multivariate gaussian is used for the distribution and centroid. The proposal is well motivated and the experimental results show that it is effective."
3402,SP:82777947d2377efa897c6905261f5375b29a4c19,prototypical networks USED-FOR few - shot one - classification problems. prototypical networks USED-FOR few - shot learning problems. prototypical networks USED-FOR method. prototypical networks USED-FOR embedding function. distance metric USED-FOR class structure. embedding space FEATURE-OF distance metric. models USED-FOR one - class problems. ,This paper proposes a method that uses prototypical networks to solve few-shot one-classification problems. The key idea is to learn an embedding function that is a weighted combination of the parameters of the prototypical network. This embedding space is then used to learn a distance metric that captures the class structure. Experiments show that the proposed models can be used to solve one class problems.
3403,SP:4a6df2b39643f548dab806a0b128fe5a3ce4dadc,"topology of the graph USED-FOR large scale setting. topological graph CONJUNCTION feature graph. feature graph CONJUNCTION topological graph. eigenvalues USED-FOR graph -- remove edges. eigenvalues USED-FOR graph laplacian. graph laplacian USED-FOR graph -- remove edges. topological graph USED-FOR linear combination. feature graph USED-FOR linear combination. unsupervised learning technique USED-FOR coarsened graph. iterative procedures USED-FOR embedded representation. Generic are way, approach, and Cheap procedures. OtherScientificTerm are graph, graph space, and adjacency "" matrix. ","This paper proposes a way to learn the topology of the graph in a large scale setting. The key idea is to learn a linear combination of a topological graph and a feature graph. The graph -- remove edges is modeled as a graph laplacian with eigenvalues computed from the graph space. The coarsened graph is learned using an unsupervised learning technique. The approach is based on the observation that the ""adjacency"" matrix of the topological and feature graph can be learned using iterative procedures. Cheap procedures are used to learn an embedded representation."
3404,SP:4a6df2b39643f548dab806a0b128fe5a3ce4dadc,"framework USED-FOR unsupervised graph embedding methods. embeddings USED-FOR graph nodes. coarsened embeddings USED-FOR embeddings. feature information USED-FOR graph topology. coarsened graph USED-FOR embeddings. adjacency matrix COMPARE adjacency matrix. adjacency matrix COMPARE adjacency matrix. node features FEATURE-OF nearest neighbor graph. coarsened graph USED-FOR embeddings. spectral approach USED-FOR graph. high - frequency information PART-OF features. Method is GraphZoom. OtherScientificTerm are graph with feature information, and nodes. ","This paper proposes a framework for unsupervised graph embedding methods. The key idea is to use coarsened embeddings from the nearest neighbor graph with feature information to approximate the graph topology, and then use the coarsensed embeds to generate embeddets for the graph nodes. The main idea of GraphZoom is to replace the adjacency matrix of the closest neighbor graph by a new, smaller adjacence matrix, which is a function of the distance between node features and the nearest neighbors of the nodes. A spectral approach is used to represent the graph as a set of features that contain high-frequency information."
3405,SP:4a6df2b39643f548dab806a0b128fe5a3ce4dadc,multi - level graph - coarsening approach USED-FOR unsupervised graph embedding models. Graph Coarsening USED-FOR graph size. Graph Fusion USED-FOR attribute similarity graph. approach PART-OF pipeline. Graph embedding CONJUNCTION Embedding refinement. Embedding refinement CONJUNCTION Graph embedding. steps PART-OF approach. network topology FEATURE-OF attribute similarity graph. steps PART-OF pipeline. Graph embedding HYPONYM-OF steps. Graph Coarsening HYPONYM-OF steps. Graph Fusion HYPONYM-OF steps. Graph Fusion PART-OF pipeline. models USED-FOR Graph embedding. pipeline USED-FOR scaling. graph coarsening and refinement based approach USED-FOR pipeline. graph coarsening and refinement based approach USED-FOR scaling. node classification task EVALUATE-FOR approach. Method is learning components. ,"This paper proposes a multi-level graph-coarsening approach for unsupervised graph embedding models. The proposed pipeline consists of two steps: Graph Coarsening to reduce the graph size and Graph Fusion to learn an attribute similarity graph over the network topology. Graph embedding and Embedding refinement are learned using the same models. In addition to learning components, the proposed pipeline also performs scaling using a graph coarsening and refinement based approach. The approach is evaluated on a node classification task."
3406,SP:e33a92e3a6acc668fa2022237e6d947b2eb8bd76,cliques CONJUNCTION graphs. graphs CONJUNCTION cliques. predictive models CONJUNCTION cliques. cliques CONJUNCTION predictive models. Task is image analysis / synthesis. Generic is similarity. Method is autoregressive model. ,"This paper studies the problem of image analysis/synthesis. The authors focus on the similarity between predictive models, cliques, graphs, etc. They propose an autoregressive model that learns to predict the similarity of a set of images. The paper is well-written and easy to follow. "
3407,SP:e33a92e3a6acc668fa2022237e6d947b2eb8bd76,approach USED-FOR image generation. autoregressive model USED-FOR image pixels. autoregressive model USED-FOR approach. models USED-FOR generative models. models USED-FOR image coding and compression settings. PixelCNN HYPONYM-OF generative models. Generic is model. Method is copy and adjustment models. Task is image - to - image translation. ,This paper presents an approach to image generation using an autoregressive model that maps image pixels to a model that predicts the output of the model. The authors use copy and adjustment models and show that these models can be used to train generative models such as PixelCNN in both image coding and compression settings. They also show that their approach can be applied to image-to-image translation.
3408,SP:e33a92e3a6acc668fa2022237e6d947b2eb8bd76,"image generation benchmark EVALUATE-FOR model. technique USED-FOR Image to Image translation. Method are autoregressive modeling, and copy and adjustment mechanism. OtherScientificTerm is sub - pixel ( channel values ). ","This paper proposes a technique for Image to Image translation based on autoregressive modeling. The key idea is to use a copy and adjustment mechanism, where the sub-pixel (channel values) of the original image are updated as the model is trained on an image generation benchmark. "
3409,SP:4224604c2650710cdf5be3ab8acc67c891944bed,behavior agnostic off - policy evaluation DualDice USED-FOR optimization framework. GenDice HYPONYM-OF optimization framework. DualDice COMPARE GenDice. GenDice COMPARE DualDice. distribution correction USED-FOR GenDice. algorithm COMPARE DualDice. DualDice COMPARE algorithm. it USED-FOR offline page rank problem. OtherScientificTerm is behavior - agnostic settings. ,"This paper proposes a new optimization framework called GenDice based on the behavior agnostic off-policy evaluation DualDice. The main difference between the proposed algorithm and the existing dualDice is the distribution correction, which is used to improve the performance of GenDICE. The authors also show that it can be used to solve the offline page rank problem, which has been shown to be useful in behavior-agnostic settings."
3410,SP:4224604c2650710cdf5be3ab8acc67c891944bed,estimator USED-FOR stationary distribution. stationary distribution FEATURE-OF Markov chain. stationary distribution COMPARE empirical data distribution. empirical data distribution COMPARE stationary distribution. fixed point solution USED-FOR operators. ratio HYPONYM-OF fixed point solution. ratio USED-FOR operators. method USED-FOR behavior - agnostic and undiscounted case. ,"This paper proposes a new estimator for the stationary distribution of a Markov chain. The stationary distribution is different from the empirical data distribution. The authors propose to use a fixed point solution (e.g., ratio) for the operators. The method is applied to both behavior-agnostic and undiscounted case."
3411,SP:4224604c2650710cdf5be3ab8acc67c891944bed,"framework USED-FOR off - policy value estimation. variational representation of $ f$-divergence USED-FOR method. Task is infinite horizon RL tasks. OtherScientificTerm are parametric density ratio, and density ratio. Material is unknown behavior policy data. ","This paper proposes a framework for off-policy value estimation in infinite horizon RL tasks. The proposed method is based on a variational representation of $f$-divergence, which is defined as a parametric density ratio between the current state and the past state. The density ratio is then used to estimate the value of an unknown behavior policy data."
3412,SP:235998cafe7b558b6f6cf6c49b689ce84004af5d,"causal "" features COMPARE ones. ones COMPARE causal "" features. spurious correlations FEATURE-OF ones. causal features COMPARE spurious or irrelevant features. spurious or irrelevant features COMPARE causal features. Task is natural language machine learning tasks. Generic is approach. ","This paper proposes a new approach to learn ""causal"" features instead of ""irrelevant"" features, i.e., ones with spurious correlations. The authors argue that causal features are more informative than spurious or irrelevant features, which is an important problem in natural language machine learning tasks. The approach is well-motivated and well motivated. "
3413,SP:235998cafe7b558b6f6cf6c49b689ce84004af5d,"textual datasets USED-FOR sentiment analysis. Method are learning methods, and retrained algorithms. Generic is datasets. ","This paper studies the problem of sentiment analysis on textual datasets. The authors argue that existing learning methods are not robust to changes in the datasets, and that retrained algorithms should be able to generalize to new datasets. "
3414,SP:235998cafe7b558b6f6cf6c49b689ce84004af5d,"models USED-FOR NLP tasks. method USED-FOR SNLI. dataset COMPARE counterfactually - augmented one. counterfactually - augmented one COMPARE dataset. IMDB sentiment dataset EVALUATE-FOR method. Method are human - in - the - loop method, and counterfactual augmentation. Generic is process. ","This paper proposes a human-in-the-loop method for training models for NLP tasks. The proposed method, SNLI, is trained on the IMDB sentiment dataset, where the dataset is different from the counterfactually-augmented one. The main idea of the proposed process is to train a model on the original dataset and then perform counterfactual augmentation on the new dataset."
3415,SP:b720eb5b6e44473a9392cc572af89270019d4c42,"orientation tuning CONJUNCTION spatial frequency sensitivity. spatial frequency sensitivity CONJUNCTION orientation tuning. spatial frequency sensitivity FEATURE-OF features. orientation tuning FEATURE-OF features. features PART-OF human visual cortex. metrics USED-FOR loss - function. metrics EVALUATE-FOR features. OtherScientificTerm are CNN - features, and basic patterns. Method is CNNs. ","This paper proposes to evaluate CNN-features in terms of orientation tuning, spatial frequency sensitivity, and other metrics to evaluate the quality of the features in human visual cortex. These metrics are then used as a loss-function, which is then used to train CNNs. The results show that CNN-samples are more sensitive to basic patterns, and that these basic patterns are more informative."
3416,SP:b720eb5b6e44473a9392cc572af89270019d4c42,convolutional neural networks ( CNNs ) features USED-FOR perceptual quality comparisons. spatial frequency CONJUNCTION orientation selectivity. orientation selectivity CONJUNCTION spatial frequency. orientation selectivity FEATURE-OF CNN features. spatial frequency FEATURE-OF CNN features. Perceptual Efficacy ( PE ) Score USED-FOR spatial frequency. Perceptual Efficacy ( PE ) Score USED-FOR orientation selectivity. Perceptual Efficacy ( PE ) Score USED-FOR analysis. CNN features USED-FOR perceptual loss. human image quality judgement USED-FOR perceptual loss. PE score FEATURE-OF CNN features. Material is human image quality judgements. ,"This paper presents an analysis of how convolutional neural networks (CNNs) features affect perceptual quality comparisons. The analysis is based on the Perceptual Efficacy (PE) Score, which measures the spatial frequency and orientation selectivity of CNN features. The authors show that CNN features with high PE score are more sensitive to human image quality judgements, and that the perceptual loss induced by CNN features is higher than the perceptual losses induced by human images."
3417,SP:b720eb5b6e44473a9392cc572af89270019d4c42,"properties FEATURE-OF human visual system. properties FEATURE-OF DNN feature. orientation selectivity HYPONYM-OF properties. human Contrast Sensitivity Function ( CSF ) EVALUATE-FOR properties. Perceptual Efficacy ( PE ) HYPONYM-OF composite score. OtherScientificTerm are deep neural network ( DNN ) features, perceptual severity of image distortions, visual frequency, and feature ’s response. ",This paper studies the properties of deep neural network (DNN) features. The authors propose a composite score called Perceptual Efficacy (PE) that measures the perceptual severity of image distortions. The properties of a DNN feature are evaluated using the human Contrast Sensitivity Function (CSF). The authors show that properties such as orientation selectivity and visual frequency are correlated with the feature’s response.
3418,SP:e2c726a1c3e3ecbec198c4dd804a4298aacec3ad,"Understanding drug - drug interactions ( DDI ) PART-OF drug development and prescription management. graph energy neural network ( GENN ) USED-FOR DDI prediction. Decagon model COMPARE model. model COMPARE Decagon model. energy function USED-FOR model. baselines COMPARE approach. approach COMPARE baselines. prediction accuracy EVALUATE-FOR approach. OtherScientificTerm are missing DDI data, and DDI types. ","Understanding drug-drug interactions (DDI) is an important problem in drug development and prescription management due to missing DDI data. This paper proposes a graph energy neural network (GENN) for DDI prediction. Compared to the Decagon model, the proposed model uses an energy function that is independent of the DDI types. The experimental results show that the proposed approach outperforms the baselines in terms of prediction accuracy."
3419,SP:e2c726a1c3e3ecbec198c4dd804a4298aacec3ad,framework USED-FOR correlated drug - drug interaction. structured prediction energy networks ( SPEN ) USED-FOR framework. energy function USED-FOR dependency structure. MLP USED-FOR graph energy. nodes embeddings USED-FOR MLP. graph convolutional network USED-FOR nodes embeddings. edge information PART-OF node embedding. test inference network CONJUNCTION cost - augmented training network. cost - augmented training network CONJUNCTION test inference network. cost - augmented training network USED-FOR semi - supervised setting. test inference network PART-OF method. DDI datasets EVALUATE-FOR baseline methods. OtherScientificTerm is neighborhood information. ,"This paper proposes a framework for predicting correlated drug-drug interaction based on structured prediction energy networks (SPEN). The main idea is to use an energy function to model the dependency structure between the two drugs. The proposed method consists of a test inference network and a cost-augmented training network for the semi-supervised setting. The nodes embeddings are generated by a graph convolutional network, and the MLP is trained to predict the graph energy. The edge information in the node embedding is used to represent the neighborhood information. The experimental results on DDI datasets show improvements over baseline methods."
3420,SP:e2c726a1c3e3ecbec198c4dd804a4298aacec3ad,graph neural network USED-FOR link type correlation. drugs CONJUNCTION interaction. interaction CONJUNCTION drugs. link prediction task USED-FOR drug - to - drug interaction prediction problem. energy - based formulation USED-FOR graph neural network. method COMPARE feedforward GNNs. feedforward GNNs COMPARE method. DDI prediction datasets EVALUATE-FOR feedforward GNNs. DDI prediction datasets EVALUATE-FOR method. OtherScientificTerm is graph edges. ,This paper studies the link prediction task for the drug-to-drug interaction prediction problem. The authors propose a graph neural network based on energy-based formulation to model the link type correlation between drugs and interaction. The proposed method is evaluated on two DDI prediction datasets and compared to feedforward GNNs. The results show that the proposed method can generalize better to unseen graph edges.
3421,SP:1f2a27579404aa165303789fdce9b3ed54f7b0c6,vector quantized neural network discritization method CONJUNCTION future time step prediction. future time step prediction CONJUNCTION vector quantized neural network discritization method. speech features USED-FOR speech recognition models. representations COMPARE speech features. speech features COMPARE representations. these USED-FOR BERT model. BERT model USED-FOR Discrete representations. these USED-FOR Discrete representations. ,This paper proposes a vector quantized neural network discritization method and future time step prediction. Discrete representations are learned from these and used in the BERT model. The authors show that these representations are more expressive than speech features used in speech recognition models.
3422,SP:1f2a27579404aa165303789fdce9b3ed54f7b0c6,quantized representations USED-FOR speech. CPC USED-FOR Audio. wav2vec COMPARE CPC. CPC COMPARE wav2vec. binary cross - entropy loss COMPARE InfoNCE softmax - cross entropy loss. InfoNCE softmax - cross entropy loss COMPARE binary cross - entropy loss. wav2vec USED-FOR Audio. binary cross - entropy loss USED-FOR wav2vec. gumbel softmax / VQ codebook USED-FOR vector quantization. Generic is approach. ,"This paper proposes to use quantized representations for speech. The proposed approach, wav2vec, is similar to CPC for Audio, but uses a binary cross-entropy loss instead of the InfoNCE softmax-cross entropy loss. The vector quantization is based on the gumbel softmax/VQ codebook."
3423,SP:1f2a27579404aa165303789fdce9b3ed54f7b0c6,discrete representations USED-FOR self - supervised learning. it USED-FOR NLP. Method is discrete vs. continuous representation. OtherScientificTerm is discrete. ,"This paper studies the problem of learning discrete representations for self-supervised learning. The main contribution of this paper is to compare the performance of learning a discrete vs. learning a continuous representation. This is a very interesting topic and it is very relevant to NLP. The paper is well-written and easy to follow. However, there are a few issues that need to be addressed."
3424,SP:f9c5b74b8bea5161d33676d9290d7b9d7e81d7b6,reinforcement learning ( RL ) based recommendation algorithm USED-FOR users'implicit feedback. actor - critic RL paradigm USED-FOR ranking - oriented loss / objective function. ranking - oriented loss / objective function FEATURE-OF collaborative filtering. implicit feedback FEATURE-OF collaborative filtering. actor network USED-FOR metric. critic network USED-FOR ranking - oriented metric. feature - based critic COMPARE one. one COMPARE feature - based critic. ,This paper proposes a reinforcement learning (RL) based recommendation algorithm for users' implicit feedback. The authors propose a ranking-oriented loss/objective function for collaborative filtering with implicit feedback based on the actor-critic RL paradigm. The metric is learned using an actor network and a critic network. The feature-based critic is compared to the original one.
3425,SP:f9c5b74b8bea5161d33676d9290d7b9d7e81d7b6,two - level architecture USED-FOR ranking solution. ranking solution USED-FOR collaborative filtering task in recommender systems. critic USED-FOR ranking - based metric. actor - critic approach PART-OF solution. RL USED-FOR critic. actor CONJUNCTION critic. critic CONJUNCTION actor. Method is actor ( VAE framework. OtherScientificTerm is actor parameters. Generic is approach. ,"This paper proposes a two-level architecture for a ranking solution for collaborative filtering task in recommender systems. The proposed solution consists of an actor-critic approach, where the actor (VAE framework) is trained using RL, and the critic is trained on a ranking-based metric. The actor parameters are learned jointly with the critic. The approach is evaluated on several datasets."
3426,SP:f9c5b74b8bea5161d33676d9290d7b9d7e81d7b6,latent variable models ( LVMs ) USED-FOR collaborative filtering. implicit feedback USED-FOR latent variable models ( LVMs ). variational autoencoders USED-FOR nonlinear LVMs. multinomial likelihood USED-FOR they. NDCG @N CONJUNCTION RECALL @N. RECALL @N CONJUNCTION NDCG @N. training objective COMPARE evaluation objective. evaluation objective COMPARE training objective. actor - critic RL approach USED-FOR nonlinear LVM. actor - critic RL approach USED-FOR NDCG loss. actor USED-FOR approximate objective. critic model USED-FOR NDCG. 3 - features USED-FOR critic. Material is large - scale datasets. Generic is method ( RaCT ). ,"This paper studies the problem of collaborative filtering with latent variable models (LVMs) trained with implicit feedback. The authors propose variational autoencoders for nonlinear LVMs, where they are trained with multinomial likelihood. NDCG @N and RECALL @N are trained on large-scale datasets. The training objective is similar to the evaluation objective, but the authors propose an actor-critic RL approach for the nonlinear LVM, where the approximate objective is learned by the actor and the critic model is trained on 3-features. The proposed method (RaCT) is evaluated on two datasets."
3427,SP:2444a83ae08181b125a325d893789f074d6db8ee,compositional TD methods USED-FOR Composite Q - learning algorithm. approach USED-FOR method. method COMPARE Q - learning. Q - learning COMPARE method. data efficiency EVALUATE-FOR Q - learning. approach COMPARE Q - learning. Q - learning COMPARE approach. data efficiency EVALUATE-FOR method. data efficiency EVALUATE-FOR approach. compositional idea USED-FOR off - policy critic. its COMPARE Q - learning. Q - learning COMPARE its. its USED-FOR deep RL domains. tabular domain EVALUATE-FOR its. off - policy critic USED-FOR deep RL domains. tabular domain EVALUATE-FOR Q - learning. compositional idea USED-FOR deep RL domains. ,This paper proposes a Composite Q-learning algorithm based on compositional TD methods. The proposed method is based on an existing approach to improve the efficiency of the method. The authors show that the proposed method improves the data efficiency compared to Q-Learning on the tabular domain and its performance on deep RL domains using the compositional idea as an off-policy critic.
3428,SP:2444a83ae08181b125a325d893789f074d6db8ee,"single step bootstrapping USED-FOR Q - learning. Q - learning formalism USED-FOR single step bootstrapping. Q function USED-FOR behavior. Q function COMPARE vanilla Q - learning. vanilla Q - learning COMPARE Q function. OtherScientificTerm are n - step returns, n - step fixed horizon, and gamma discounted Q function. Method is bootstrapping. ","This paper studies the problem of single step bootstrapping in Q-learning using the Q - learning formalism. In particular, the authors consider the case where the n-step returns are linear in the number of steps. The authors show that the behavior of the Q function can be approximated by a gamma discounted Q function, where gamma is defined as a function of the nth step fixed horizon, and show that this Q function converges to a fixed point faster than vanilla Q-Learning. The paper also provides a theoretical analysis of the dynamics of the behavior in the case when the n step returns are not linear. Finally, the paper provides an empirical study of the effect of the gamma-discounted Q function on the performance of bootstrapped."
3429,SP:2444a83ae08181b125a325d893789f074d6db8ee,short - term truncated value function CONJUNCTION long - term shifted value function. long - term shifted value function CONJUNCTION short - term truncated value function. short term truncated returns COMPARE tail of the returns. tail of the returns COMPARE short term truncated returns. components PART-OF value function. short - term truncated value function HYPONYM-OF components. long - term shifted value function HYPONYM-OF components. truncated value function CONJUNCTION shifted value function. shifted value function CONJUNCTION truncated value function. temporal difference formulations USED-FOR truncated value function. temporal difference formulations USED-FOR shifted value function. approaches USED-FOR off - policy case. algorithm COMPARE approaches. approaches COMPARE algorithm. MuJoCo tasks CONJUNCTION tabular domain. tabular domain CONJUNCTION MuJoCo tasks. tabular domain EVALUATE-FOR algorithm. tabular domain EVALUATE-FOR approaches. MuJoCo tasks EVALUATE-FOR algorithm. MuJoCo tasks EVALUATE-FOR approaches. ,"This paper proposes to replace the value function with two components: a short-term truncated value function and a long-term shifted value function. The authors argue that the short term truncated returns are more efficient than the tail of the returns. To this end, the authors propose to use temporal difference formulations to approximate the truncated values function and the shift value function respectively. The proposed algorithm is evaluated on MuJoCo tasks and the tabular domain, and compared to existing approaches for the off-policy case."
3430,SP:64564b09bd68e7af17845019193825794f08e99b,vison - based SAC USED-FOR visual goals. vison - based SAC USED-FOR approach. VAE USED-FOR compressed state. random pertubation controller CONJUNCTION VAE. VAE CONJUNCTION random pertubation controller. random pertubation controller USED-FOR robot. random pertubation controller USED-FOR compressed state. Task is robotic system. OtherScientificTerm is manual reset. Method is reward engineering. Generic is it. ,"This paper proposes an approach based on vison-based SAC to solve visual goals using a robotic system. The robot is trained with a random pertubation controller and a VAE to generate a compressed state, which is then used to train a robot with a manual reset. The approach is based on reward engineering, where the goal is to find a state that maximizes the likelihood of the robot achieving it."
3431,SP:64564b09bd68e7af17845019193825794f08e99b,"approaches USED-FOR real - world RL. robotics USED-FOR real - world RL. variational autoencoder USED-FOR images. variational autoencoder USED-FOR low dimensional space. low dimensional space FEATURE-OF images. OtherScientificTerm are raw sensory inputs, minimal reward design effort, and manual resetting. Method is perturbation policy. ",This paper proposes two approaches to tackle real-world RL in robotics using robotics. The first approach is to use a variational autoencoder to generate images in a low dimensional space with raw sensory inputs. The second is to learn a perturbation policy that aims to maximize the likelihood of the perturbed image being generated by a minimal reward design effort. This is achieved by performing manual resetting.
3432,SP:64564b09bd68e7af17845019193825794f08e99b,"RL approaches USED-FOR real world robotic systems. episodic learning USED-FOR algorithms. external feedback USED-FOR handcrafted reward functions. approaches PART-OF system. approach COMPARE systems. systems COMPARE approach. Material are raw sensory data, and real robotic system. ","This paper presents an empirical study of RL approaches to training real world robotic systems using raw sensory data. The algorithms are based on episodic learning, where handcrafted reward functions are learned using external feedback. The authors show that the proposed approaches can be integrated into a system, and show that their approach can outperform existing systems. The paper also presents a simulation of a real robotic system to demonstrate the effectiveness of their approach."
3433,SP:ee4d59fa9487ecdcd663a4a7833689d1754aac7c,sample complexity EVALUATE-FOR adversarially robust learning. unlabeled samples USED-FOR adversarially robust learning. algorithm USED-FOR robust classifier. labeled samples CONJUNCTION unlabeled samples. unlabeled samples CONJUNCTION labeled samples. labeled samples USED-FOR robust classifier. VAT algorithm USED-FOR deep networks. algorithm COMPARE adversarial training. adversarial training COMPARE algorithm. labeled samples USED-FOR adversarial training. ,"This paper studies the sample complexity of adversarially robust learning with labeled samples and unlabeled samples. The authors propose a new algorithm to train a robust classifier with both labeled samples as well as with only the labeled samples. They apply the VAT algorithm to deep networks and show that the proposed algorithm can outperform adversarial training with only a small number of labeled samples, which is a significant improvement over the state of the art."
3434,SP:ee4d59fa9487ecdcd663a4a7833689d1754aac7c,unlabeled data USED-FOR robust generalization. labeled sample complexity EVALUATE-FOR robust setting. algorithm USED-FOR robust test accuracy. MNIST and CIFAR datasets EVALUATE-FOR algorithm. MNIST and CIFAR datasets EVALUATE-FOR it. Method is toy model. Material is sufficient unlabeled data. ,"This paper studies the problem of robust generalization with unlabeled data. The authors propose a toy model where the labeled sample complexity of the robust setting is bounded by the number of sufficient unlabeled data, and propose an algorithm to improve the robust test accuracy. The algorithm is evaluated on MNIST and CIFAR datasets, and it shows promising results."
3435,SP:ee4d59fa9487ecdcd663a4a7833689d1754aac7c,"generalization error CONJUNCTION stability term. stability term CONJUNCTION generalization error. stability term FEATURE-OF robust generalization error. separator USED-FOR symmetric 2 gaussian mixture data. unlabeled data USED-FOR robust generalization. Mnist CONJUNCTION Cifar. Cifar CONJUNCTION Mnist. regularization COMPARE PGD adversarial training. PGD adversarial training COMPARE regularization. Task are adversarial robustness, classification problem, and model prediction. Generic is problem. Material is labeled examples. ","This paper studies adversarial robustness. The authors consider the classification problem and propose a separator for symmetric 2 gaussian mixture data to reduce the robust generalization error and the stability term. They show that this problem can be solved by regularizing the unlabeled data with the labeled examples. They also show that the proposed regularization improves the performance over PGD adversarial training on Mnist and Cifar. Finally, they show that model prediction can be improved with the regularization."
3436,SP:8d92aa968c590a352cb34c9fa1dbe77dff19519f,"advantage estimation USED-FOR actor - critic RL algorithms. 1 - step returns CONJUNCTION 2 - step returns. 2 - step returns CONJUNCTION 1 - step returns. 1 - step returns USED-FOR advantage. 2 - step returns USED-FOR advantage. OtherScientificTerm are minimum advantage, and regulatory focus. ","This paper studies the problem of advantage estimation in actor-critic RL algorithms. In particular, the authors propose to use 1-step returns and 2-step return to estimate the advantage. The main idea is to use the minimum advantage as a proxy for the regulatory focus. The paper is well-written and easy to follow."
3437,SP:8d92aa968c590a352cb34c9fa1dbe77dff19519f,"Method are risk - aware reinforcement learning, and Risk control. OtherScientificTerm are high variance states, and low variance states. Generic is algorithm. ","This paper studies risk-aware reinforcement learning. Risk control is an important problem in the real world, where high variance states are more likely to be encountered than low variance states. The authors propose an algorithm that aims to mitigate this issue. "
3438,SP:8d92aa968c590a352cb34c9fa1dbe77dff19519f,advantage function estimates USED-FOR policy gradient methods. exponentially weighted average USED-FOR policy gradients. estimates USED-FOR exponentially weighted average. order statistics USED-FOR policy gradient. regulatory ratio USED-FOR policy gradients. order statistic USED-FOR averaged advantage estimate. order statistic USED-FOR regulatory ratio. hyper - parameter USED-FOR optimistic case ( max advantage ). sparse and dense rewards CONJUNCTION discrete and continuous actions. discrete and continuous actions CONJUNCTION sparse and dense rewards. discrete and continuous actions CONJUNCTION fully observable and partially observable environments. fully observable and partially observable environments CONJUNCTION discrete and continuous actions. order statistics CONJUNCTION regulatory ratio. regulatory ratio CONJUNCTION order statistics. regulatory ratio USED-FOR policy. order statistics USED-FOR policy. sparse and dense rewards HYPONYM-OF domains. discrete and continuous actions HYPONYM-OF domains. fully observable and partially observable environments HYPONYM-OF domains. OtherScientificTerm is overtly optimistic estimates. ,"This paper studies the advantage function estimates for policy gradient methods. The main idea is to use these estimates as an exponentially weighted average for the policy gradients. The order statistic of the averaged advantage estimate is then used to compute the regulatory ratio of the policy gradient. This hyper-parameter is used in the optimistic case (max advantage) and in the pessimistic case (min advantage). The authors show empirically that using these order statistics and regulatory ratio can improve the policy performance in a variety of domains, including sparse and dense rewards, discrete and continuous actions, fully observable and partially observable environments. The authors also show that using overtly optimistic estimates can improve performance."
3439,SP:2162408ce2a3267724b5f8f0abec41d4dc714220,"method COMPARE quantization method baselines. quantization method baselines COMPARE method. computational savings EVALUATE-FOR quantization method baselines. computational savings EVALUATE-FOR method. ImageNet CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION ImageNet. CIFAR10 CONJUNCTION PTB. PTB CONJUNCTION CIFAR10. ImageNet HYPONYM-OF datasets. PTB HYPONYM-OF datasets. CIFAR10 HYPONYM-OF datasets. CNN and RNN - based neural nets HYPONYM-OF models. Method are quantization technique, and inference technique. ","This paper proposes a new quantization technique. The proposed method achieves significant computational savings compared to the existing quantization method baselines. The authors conduct experiments on three datasets: ImageNet, CIFAR10, and PTB. The experiments are conducted on two models: CNN and RNN-based neural nets and show that the proposed inference technique is effective."
3440,SP:2162408ce2a3267724b5f8f0abec41d4dc714220,"precision USED-FOR numerical representation of the network. accuracy CONJUNCTION speed. speed CONJUNCTION accuracy. precision USED-FOR method. Generic is network. OtherScientificTerm are threshold value, threshold, full precision, and reduced precision. ",This paper proposes a method that uses precision as a numerical representation of the network. The method is motivated by the trade-off between accuracy and speed. The key idea is to use a threshold value that is proportional to the difference between the full precision and the reduced precision. 
3441,SP:2162408ce2a3267724b5f8f0abec41d4dc714220,"bitwise operations USED-FOR networks. threshold Delta USED-FOR output activation. Method is Precision Gating. OtherScientificTerm are neural network activations, and average bitwidth. ","This paper proposes Precision Gating, which aims to reduce the variance in the number of neural network activations during training. The main idea is to use bitwise operations to train the networks, where the output activation is determined by the threshold Delta. The authors show that the average bitwidth of the weights of the networks can be reduced to a fixed number of bits."
3442,SP:0c2c9b80c087389168acdd42af15877fb499449b,"one CONJUNCTION other. other CONJUNCTION one. mixed source - target data USED-FOR one. branches PART-OF architecture. other HYPONYM-OF branches. one HYPONYM-OF branches. co - teaching USED-FOR branch. Task are wildly unsupervised domain adaptation, and training. Method is Pseudo - labeling. Generic is process. ","This paper addresses the problem of wildly unsupervised domain adaptation. The authors propose an architecture that consists of two branches: one based on mixed source-target data, and the other based on co-teaching. Pseudo-labeling is used in the first stage of the training. The second stage of training is based on the learned representations of the source and target domains. This process is repeated until convergence."
3443,SP:0c2c9b80c087389168acdd42af15877fb499449b,"problem setting USED-FOR domain adaptation field. UDA methods USED-FOR real - world data. WUDA problem COMPARE ones. ones COMPARE WUDA problem. Material are clean labeled source data, and clean source data. Task is unsupervised domain adaptation ( WUDA ). ","This paper studies the problem setting in the domain adaptation field, where the source domain is clean labeled source data, and the target domain is not clean source data. The paper focuses on the problem of unsupervised domain adaptation (WUDA), where the goal is to adapt the source data to the target target domain. The WUDA problem is different from existing ones in that UDA methods are applied to real-world data."
3444,SP:0c2c9b80c087389168acdd42af15877fb499449b,"method USED-FOR unsupervised domain adaptation. labeled source and unlabeled target set USED-FOR problem. branches PART-OF butterfly network. OtherScientificTerm are pseudo - labels, and optimization objective. Method is checking ” mechanism. ","This paper proposes a method for unsupervised domain adaptation. The problem is formulated as a combination of a labeled source and unlabeled target set, where the source and the target are pseudo-labels. The authors propose a “checking” mechanism where the optimization objective is to minimize the difference between the labels of the two source and target domains. The proposed method is based on a butterfly network, which consists of two branches."
3445,SP:f3cc10ce2f77aeb2a6a3bae5631602452c14d403,"actor and critic USED-FOR deterministic autoencoder. models COMPARE SAC. SAC COMPARE models. SAC USED-FOR raw state. encoder USED-FOR tasks. deterministic autoencoders COMPARE beta - VAE autoencoder. beta - VAE autoencoder COMPARE deterministic autoencoders. end - to - end learning USED-FOR autoencoder. beta - VAE autoencoder COMPARE end - to - end learning. end - to - end learning COMPARE beta - VAE autoencoder. deterministic autoencoders USED-FOR approach. Material are image - based environment, and DeepMind control suite. Method is SAC - AE. ","This paper proposes a deterministic autoencoder with an actor and critic. Compared to existing models, SAC learns to predict the raw state of an image in an image-based environment. The encoder is trained for different tasks, and the authors compare their approach to deterministic and beta-VAE autoencoders, and show that SAC-AE outperforms both. The authors also show that end-to-end learning can improve the performance of the autoencaoder. Finally, the authors conduct experiments on the DeepMind control suite."
3446,SP:f3cc10ce2f77aeb2a6a3bae5631602452c14d403,approach USED-FOR soft actor - critic ( SAC ) algorithm. soft actor - critic ( SAC ) algorithm USED-FOR proprioceptive state spaces. higher - dimensional visual state spaces FEATURE-OF proprioceptive state spaces. encoder - decoder structure USED-FOR image reconstruction loss. encoder - decoder structure USED-FOR SAC's learning objectives. critic CONJUNCTION policy. policy CONJUNCTION critic. encoder - decoder architecture CONJUNCTION critic. critic CONJUNCTION encoder - decoder architecture. encoder USED-FOR encoder - decoder architecture. image reconstruction CONJUNCTION critic learning. critic learning CONJUNCTION image reconstruction. pixel - based SAC CONJUNCTION D4PG. D4PG CONJUNCTION pixel - based SAC. proprioceptive SAC CONJUNCTION pixel - based SAC. pixel - based SAC CONJUNCTION proprioceptive SAC. approach COMPARE proprioceptive SAC. proprioceptive SAC COMPARE approach. approach COMPARE pixel - based SAC. pixel - based SAC COMPARE approach. D4PG CONJUNCTION SLAC. SLAC CONJUNCTION D4PG. DeepMind control suite EVALUATE-FOR tasks. pixel - based SAC HYPONYM-OF tasks. SLAC HYPONYM-OF tasks. tasks EVALUATE-FOR approach. DeepMind control suite EVALUATE-FOR approach. method COMPARE model - based baselines. model - based baselines COMPARE method. method COMPARE raw pixel - based SAC. raw pixel - based SAC COMPARE method. model - based baselines COMPARE raw pixel - based SAC. raw pixel - based SAC COMPARE model - based baselines. information capacity CONJUNCTION generalization. generalization CONJUNCTION information capacity. Method is Q - critic. OtherScientificTerm is encoder weights. ,"This paper proposes an approach to train a soft actor-critic (SAC) algorithm to learn proprioceptive state spaces in higher-dimensional visual state spaces. SAC's learning objectives are formulated as an encoder-decoder structure that minimizes the image reconstruction loss and jointly learns a critic and a policy. The encoder and decoder architecture is based on the same encoder, which is trained in parallel. The paper proposes a variant of Q-critics, where the encoder weights are learned jointly with the critic. The proposed approach is evaluated on three tasks from the DeepMind control suite: image reconstruction, critic learning, and pixel-based SAC (D4PG, SLAC). The proposed method outperforms all model-based baselines on all three tasks, and the proposed approach shows better performance than the original proprietary SAC, the pixel-by-pixel SAC and the original pixel-free SAC. In addition, the proposed method shows better information capacity and better generalization."
3447,SP:f3cc10ce2f77aeb2a6a3bae5631602452c14d403,method USED-FOR model - free RL. image observations USED-FOR method. feature learning CONJUNCTION policy learning. policy learning CONJUNCTION feature learning. policy and value function USED-FOR autoencoder. deterministic regularized autoencoder COMPARE stochastic variational autoencoder. stochastic variational autoencoder COMPARE deterministic regularized autoencoder. model - based and model - free methods USED-FOR RL. method COMPARE model - based and model - free methods. model - based and model - free methods COMPARE method. image observations USED-FOR model - based and model - free methods. control tasks EVALUATE-FOR method. image observations USED-FOR RL. ,This paper proposes a method for model-free RL based on image observations. The key idea is to combine feature learning with policy learning. The autoencoder is trained with a policy and value function. The authors compare the performance of the deterministic regularized autoencaver with the stochastic variational autoencalcoder. The proposed method is evaluated on several control tasks and compared with model-based and model -free methods for RL with image observations
3448,SP:917bc9151a5829e97efd9bd0d0b2a3d1771b3265,dropout configurations USED-FOR mini - batch. dropout configurations FEATURE-OF neural net. neural net USED-FOR it. dropout masks HYPONYM-OF dropout configurations. ensembling COMPARE dropout. dropout COMPARE ensembling. method COMPARE dropout. dropout COMPARE method. validation accuracies EVALUATE-FOR dropout. validation accuracies EVALUATE-FOR method. Method is ensemble of dropout. OtherScientificTerm is training loss. ,"This paper proposes an ensemble of dropout. Specifically, it trains a neural net with different dropout configurations (e.g., dropout masks, etc) for a mini-batch. The training loss is fixed. The experiments show that the proposed ensembling outperforms dropout in terms of validation accuracies."
3449,SP:917bc9151a5829e97efd9bd0d0b2a3d1771b3265,"variation HYPONYM-OF Dropout. dropout samples USED-FOR loss function. dropout masks CONJUNCTION shared weights. shared weights CONJUNCTION dropout masks. run time EVALUATE-FOR naive approach. naive approach COMPARE dropout. dropout COMPARE naive approach. method USED-FOR model. validation error EVALUATE-FOR model. Metric are faster convergence, and computational time. OtherScientificTerm is forward - passes. Generic is network. ","This paper proposes a variant of Dropout, i.e., using dropout samples as a loss function to achieve faster convergence. The authors argue that the proposed dropout masks and shared weights can reduce the number of forward-passes and thus reduce the computational time of the network. The experiments show that their proposed method can improve the performance of the model in terms of validation error and run time compared to the naive approach without dropout."
3450,SP:917bc9151a5829e97efd9bd0d0b2a3d1771b3265,dropout technique COMPARE dropout. dropout COMPARE dropout technique. dropout technique USED-FOR training. error rates EVALUATE-FOR dropout. dropout technique USED-FOR generalization. training and validation sets EVALUATE-FOR dropout. CIFAR-100 CONJUNCTION IMAGENET. IMAGENET CONJUNCTION CIFAR-100. IMAGENET CONJUNCTION SVHN. SVHN CONJUNCTION IMAGENET. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. dropout samples CONJUNCTION dropout ratio. dropout ratio CONJUNCTION dropout samples. datasets FEATURE-OF image classification tasks. CIFAR-10 HYPONYM-OF datasets. IMAGENET HYPONYM-OF datasets. CIFAR-100 HYPONYM-OF datasets. SVHN HYPONYM-OF datasets. ,"This paper proposes a new dropout technique for training that is more efficient than standard dropout in terms of both error rates and generalization. The authors evaluate the effectiveness of dropout on both training and validation sets. The experiments are conducted on a variety of image classification tasks on different datasets (CIFAR-10, CifAR-100, IMAGENET, SVHN, etc). The authors show that dropout samples and dropout ratio are the most important factors for improving performance."
3451,SP:5ad4b9e837e08d995b545b0b2734bc8fa4fafc43,Label Sensitive Gate ( LSG ) structure USED-FOR model. model USED-FOR disentangled filters. supervised manner USED-FOR disentangled filters. training path USED-FOR Label - Sensitive Gate path. filters USED-FOR class - sensitive features. ,This paper proposes a new model based on the Label Sensitive Gate (LSG) structure. The proposed model learns disentangled filters in a supervised manner. The Label-Sensitive Gate path is defined as a training path where the filters are used to extract class-sensitive features.
3452,SP:5ad4b9e837e08d995b545b0b2734bc8fa4fafc43,"Label Sensitive Gate ( LSG ) structure USED-FOR models. models USED-FOR disentangled filters. model USED-FOR features. network parameters CONJUNCTION sparse gate matrix. sparse gate matrix CONJUNCTION network parameters. Method is DNN model. OtherScientificTerm are sparse LSG structure, and class - specific filters. ","This paper proposes to use the Label Sensitive Gate (LSG) structure to train models that can learn disentangled filters. The key idea is to train a DNN model with a sparse LSG structure, where the network parameters and the sparse gate matrix are learned separately, and then the model is used to extract features from the learned features. This allows the model to learn class-specific filters that can be used in the training process."
3453,SP:5ad4b9e837e08d995b545b0b2734bc8fa4fafc43,method USED-FOR convolutional neural network ( CNN ). method USED-FOR interpretability. interpretability EVALUATE-FOR convolutional neural network ( CNN ). gating function USED-FOR filters. sparsity of the filters CONJUNCTION localization accuracy. localization accuracy CONJUNCTION sparsity of the filters. CIFAR10 EVALUATE-FOR localization accuracy. Method is CNN filters. Generic is model. ,"This paper proposes a method to improve the interpretability of a convolutional neural network (CNN). The method is based on the observation that CNN filters tend to be sparse. To address this issue, the authors propose a gating function that encourages the filters to be sparser. The proposed model is evaluated on CIFAR10, where the sparsity of the filters and the localization accuracy are compared."
3454,SP:c9a512b6bc59aacbec2d5608284e29a7746172cf,"value function factorization CONJUNCTION communication learning. communication learning CONJUNCTION value function factorization. decentralized Q functions CONJUNCTION communication messages. communication messages CONJUNCTION decentralized Q functions. one CONJUNCTION other. other CONJUNCTION one. framework USED-FOR value function factorization. framework USED-FOR communication learning. one HYPONYM-OF regularizers. other HYPONYM-OF regularizers. method USED-FOR dropping non - informative messages. sensor and hallway tasks EVALUATE-FOR approach. decentralized StarCraft II benchmark EVALUATE-FOR method. approach COMPARE approaches. approaches COMPARE approach. OtherScientificTerm are non - informative messages, and communication. ","This paper proposes a framework for combining value function factorization and communication learning. The authors propose two regularizers, one that encourages the Q functions to be more informative and the other that encourages communication to be less informative. The proposed method is evaluated on sensor and hallway tasks and is shown to be effective at dropping non-informative messages. The method is also evaluated on the decentralized StarCraft II benchmark. The experimental results show that the proposed approach outperforms existing approaches."
3455,SP:c9a512b6bc59aacbec2d5608284e29a7746172cf,method USED-FOR multi - agent reinforcement learning. nearly decomposable value functions USED-FOR method. local ( agent specific ) value function CONJUNCTION global value function. global value function CONJUNCTION local ( agent specific ) value function. communication need FEATURE-OF multi - agent setup. variational inference tools USED-FOR method. ,This paper proposes a method for multi-agent reinforcement learning based on nearly decomposable value functions. The proposed method combines a local (agent specific) value function with a global value function. The method is based on variational inference tools. The authors also address the communication need in multi-agents setup.
3456,SP:c9a512b6bc59aacbec2d5608284e29a7746172cf,"finding almost - decentralized value functions USED-FOR collaborative multi - agent RL problem. OtherScientificTerm are almost - decentralized value functions, mutual information, and redundant messages. Method is entropy regularization. ",This paper studies the problem of finding almost-decentralized value functions for a collaborative multi-agent RL problem. The authors propose to use entropy regularization to encourage the mutual information between agents to be high-dimensional and to avoid redundant messages.
3457,SP:562f1a50f80d760a4be35095cd795cdb0f69a890,programming puzzles HYPONYM-OF programs. programs USED-FOR AI systems. GAN - like generation USED-FOR adaptive method of puzzle generation. Task is puzzle generation. Generic is puzzles. ,"This paper studies the problem of programming puzzles, which are commonly used programs in AI systems. The authors propose an adaptive method of puzzle generation based on GAN-like generation. The main contribution of this paper is to propose a new way of solving puzzles."
3458,SP:562f1a50f80d760a4be35095cd795cdb0f69a890,"method USED-FOR generating hard puzzles. trainable puzzle solver USED-FOR generating hard puzzles. trainable puzzle solver USED-FOR method. neural solver HYPONYM-OF discriminator. Task is symbolic and deep learning based AI. Generic are approach, and generator. ","This paper presents a method for generating hard puzzles using a trainable puzzle solver. This is an important problem in both symbolic and deep learning based AI, and this paper proposes an approach to tackle this problem. The key idea is to train a discriminator (i.e., a neural solver) to discriminate between the generated and the generated solutions. The generator is trained on the generated solution, and the discriminator is trained to distinguish between the two solutions."
3459,SP:562f1a50f80d760a4be35095cd795cdb0f69a890,"boolean programs USED-FOR puzzles. Method are trainable'puzzle'program synthesizer, and program solver. OtherScientificTerm is syntax. ","This paper proposes a trainable 'puzzle' program synthesizer. The puzzles are formulated as boolean programs, and the authors propose a program solver to solve them. The syntax is similar to that of [1] and [2]."
3460,SP:627b515cc893ff33914dff255f5d6e136441d2e2,approach USED-FOR hierarchical RL problem. primitives PART-OF policy. primitive policies USED-FOR tasks. scheme COMPARE flat and hierarchical policies. flat and hierarchical policies COMPARE scheme. generalization EVALUATE-FOR scheme. OtherScientificTerm is primitive. Generic is they. ,"This paper proposes an approach to solving the hierarchical RL problem. The key idea is to replace the primitives in the policy with a set of primitive policies that can be applied to different tasks. The primitive is defined as a function of the number of tasks, and the authors show that they can generalize better than flat and hierarchical policies in terms of generalization."
3461,SP:627b515cc893ff33914dff255f5d6e136441d2e2,information bottleneck USED-FOR task decomposition. information bottleneck USED-FOR policy primitives. task decomposition USED-FOR policy primitives. hierarchical reinforcement learning USED-FOR policy primitives. higher - level meta - policy USED-FOR policy primitives. OtherScientificTerm is maximizing rewards. ,This paper proposes to use an information bottleneck to guide the task decomposition of policy primitives in hierarchical reinforcement learning. The idea is to train a higher-level meta-policy that learns to decompose the learned policies into a set of lower-level policies that can be used for maximizing rewards.
3462,SP:627b515cc893ff33914dff255f5d6e136441d2e2,"encoder CONJUNCTION decoder. decoder CONJUNCTION encoder. encoder PART-OF primitive. decoder PART-OF primitive. meta - policy USED-FOR primitives. reward maximization CONJUNCTION information content. information content CONJUNCTION reward maximization. Method are policy design, and RL. OtherScientificTerm are policy, and temporally extended actions. ","This paper studies policy design in the context of RL. The authors propose a primitive that consists of an encoder, a decoder, and a meta-policy. The policy design is based on the idea that primitives can be learned as part of a meta - policy, and that the policy can be extended to take temporally extended actions. This is motivated by the idea of reward maximization and information content in RL."
3463,SP:4d135a76ab151dd0adcf92c5ed8d3c717d256520,"reward prediction model USED-FOR latent representation. latent representation USED-FOR model - based reinforcement learning. method USED-FOR latent dynamics model. multi - step reward prediction USED-FOR latent dynamics model. observation reconstruction USED-FOR latent space. planning performance guarantees USED-FOR approximate latent reward prediction models. Method is MPC. Generic are this, and model. OtherScientificTerm is action sequences. ","This paper proposes a reward prediction model to learn a latent representation for model-based reinforcement learning. The method learns a latent dynamics model using multi-step reward prediction, which is similar to MPC. The key difference is that this is done in an unsupervised way, where the latent space is learned using observation reconstruction, and the action sequences are sampled from the model. The authors provide planning performance guarantees for approximate latent reward prediction models."
3464,SP:4d135a76ab151dd0adcf92c5ed8d3c717d256520,technique USED-FOR model based RL / planning. latent dynamics models USED-FOR technique. reward prediction USED-FOR latent model. reward prediction CONJUNCTION state reconstruction. state reconstruction CONJUNCTION reward prediction. state reconstruction USED-FOR latent model. reward prediction USED-FOR latent model. OtherScientificTerm is state reconstruction loss. ,This paper proposes a technique for model based RL/planning based on latent dynamics models. The key idea is to use reward prediction and state reconstruction to train a latent model. The state reconstruction loss is a weighted sum of the reward and the state reconstruction.
3465,SP:4d135a76ab151dd0adcf92c5ed8d3c717d256520,algorithm USED-FOR planning. latent reward prediction USED-FOR algorithm. reward function USED-FOR reward. forward dynamics function CONJUNCTION reward function. reward function CONJUNCTION forward dynamics function. forward dynamics function USED-FOR dynamical system in latent state space. forward dynamics function USED-FOR model. reward function USED-FOR model. encoder USED-FOR model. functions USED-FOR objective. mean - squared error USED-FOR objective. multi - pendulum CONJUNCTION multi - cheetah. multi - cheetah CONJUNCTION multi - pendulum. multi - cheetah HYPONYM-OF specific RL domains. multi - pendulum HYPONYM-OF specific RL domains. OtherScientificTerm is latent state. Generic is method. ,"This paper proposes an algorithm for planning based on latent reward prediction. The model is trained with an encoder that predicts the forward dynamics function of a dynamical system in latent state space, and a reward function that is used to compute the reward. The objective is based on the mean-squared error between the forward state and the reward function. The authors show that the proposed method can achieve state-of-the-art performance on two specific RL domains: multi-pendulum and multi-cheetah."
3466,SP:03c61ba3d6fe01bd0bc3469cd408c370527d9d69,ALBERT HYPONYM-OF BERT architecture. embedding factorization CONJUNCTION sharing layers. sharing layers CONJUNCTION embedding factorization. sharing layers CONJUNCTION sentence ordering. sentence ordering CONJUNCTION sharing layers. Generic is tasks. ,"This paper presents ALBERT, a BERT architecture. The authors propose to use embedding factorization, sharing layers, and sentence ordering to improve the performance. The experiments are conducted on a variety of tasks."
3467,SP:03c61ba3d6fe01bd0bc3469cd408c370527d9d69,"free parameters CONJUNCTION memory footprint. memory footprint CONJUNCTION free parameters. free parameters FEATURE-OF complexity. sentence order prediction USED-FOR training. OtherScientificTerm are BERT, computation steps, embedding matrix, and hyperparameters. Generic are strategies, and matrices. Method are layer - wise parameter sharing, and lamb optimizer. Task is NLP / NLU tasks. ","This paper proposes two strategies to reduce the complexity of BERT. The first strategy is layer-wise parameter sharing, where the embedding matrix is shared across layers. The second strategy is to share the computation steps between layers. Both strategies are based on the observation that the number of layers in BERT can be reduced by sharing the matrices. The main contribution of this paper is to study the effect of reducing the complexity in terms of free parameters and memory footprint. The authors propose a lamb optimizer, which is trained on sentence order prediction during training. They show that the hyperparameters can be shared between layers, and show that this can lead to better performance on NLP/NLU tasks."
3468,SP:03c61ba3d6fe01bd0bc3469cd408c370527d9d69,ALBERT HYPONYM-OF pre - trained BERT - like model. factorized embedding parameterization CONJUNCTION cross - layer parameter sharing. cross - layer parameter sharing CONJUNCTION factorized embedding parameterization. cross - layer parameter sharing CONJUNCTION intern - sentence coherence loss. intern - sentence coherence loss CONJUNCTION cross - layer parameter sharing. model size CONJUNCTION memory consumption. memory consumption CONJUNCTION model size. next sentence prediction ( NSP ) task PART-OF BERT. memory consumption FEATURE-OF BERT. auxiliary task PART-OF sentence - order prediction ( SOP ). model size FEATURE-OF BERT. ALBERT COMPARE BERT - large. BERT - large COMPARE ALBERT. GLUE CONJUNCTION RACE. RACE CONJUNCTION GLUE. RACE CONJUNCTION SQuAD. SQuAD CONJUNCTION RACE. ALBERT USED-FOR GLUE. ALBERT USED-FOR RACE. ALBERT USED-FOR SQuAD. ,"This paper proposes ALBERT, a pre-trained BERT-like model. The authors propose a factorized embedding parameterization, cross-layer parameter sharing, and an intern-sentence coherence loss. The main contribution of the paper is the introduction of the auxiliary task of sentence-order prediction (SOP), which is an auxiliary task to the standard BERT that consists of the next sentence prediction (NSP) task. The model size and memory consumption of BERT are discussed, and the authors compare the performance of the proposed AlBERT with the state-of-the-art on GLUE, RACE, and SQuAD. The results show that the performance gain of ALBERt is comparable to that of the original BERT - large."
3469,SP:5dd50f3e6cef6b82192a1d37b35469dc7fb443ce,unified architecture USED-FOR multi - task learning. image CONJUNCTION text. text CONJUNCTION image. text CONJUNCTION videos. videos CONJUNCTION text. tasks USED-FOR compressed model. modalities FEATURE-OF tasks. videos HYPONYM-OF modalities. image HYPONYM-OF modalities. text HYPONYM-OF tasks. text HYPONYM-OF modalities. peripheral networks USED-FOR feature representations. peripheral networks USED-FOR domain specific input. domain specific input USED-FOR feature representations. Transformer networks USED-FOR spatio - temporal information. attention based encoder - decoder model CONJUNCTION Transformer networks. Transformer networks CONJUNCTION attention based encoder - decoder model. central neural processor CONJUNCTION attention based encoder - decoder model. attention based encoder - decoder model CONJUNCTION central neural processor. video captioning CONJUNCTION video question answering. video question answering CONJUNCTION video captioning. Generic is models. ,"This paper proposes a unified architecture for multi-task learning. The compressed model is trained on a variety of tasks across different modalities (image, text, videos). The central neural processor, attention based encoder-decoder model, and Transformer networks are used to encode spatio-temporal information. The peripheral networks are trained to extract feature representations from the domain specific input. Experiments on video captioning and video question answering demonstrate the effectiveness of the proposed models."
3470,SP:5dd50f3e6cef6b82192a1d37b35469dc7fb443ce,"Central Neural Processor HYPONYM-OF model. text CONJUNCTION image. image CONJUNCTION text. model USED-FOR text. model USED-FOR image. Method are OmniNet "" architecture, and transformer. OtherScientificTerm is Dedicated modality - specific "" input peripherals. ","This paper proposes a ""OmniNet"" architecture. The model, Central Neural Processor, is a model that takes as input both text and image. Dedicated modality-specific ""input peripherals"" are added to the transformer. "
3471,SP:5dd50f3e6cef6b82192a1d37b35469dc7fb443ce,text CONJUNCTION videos. videos CONJUNCTION text. images CONJUNCTION text. text CONJUNCTION images. learning architecture USED-FOR tasks. OmniNet- based on transformer USED-FOR tasks. modalities FEATURE-OF tasks. OmniNet- based on transformer USED-FOR learning architecture. videos HYPONYM-OF modalities. text HYPONYM-OF modalities. images HYPONYM-OF modalities. temporal information CONJUNCTION spatial information. spatial information CONJUNCTION temporal information. spatio - temporal cache mechanism USED-FOR temporal information. spatio - temporal cache mechanism USED-FOR spatial information. framework USED-FOR asynchronous multi - task learning. pre - trained neural networks USED-FOR modalities. pre - trained neural networks USED-FOR framework. pre - trained neural networks USED-FOR asynchronous multi - task learning. peripheral networks PART-OF model. peripheral networks USED-FOR shared format / space. CPU USED-FOR central neural processor. temporal encoding CONJUNCTION spatial encoding. spatial encoding CONJUNCTION temporal encoding. self - attention CONJUNCTION RNN. RNN CONJUNCTION self - attention. RNN USED-FOR temporal encoding. RNN USED-FOR spatial encoding. central neural processor USED-FOR temporal encoding. central neural processor USED-FOR spatial encoding. self - attention USED-FOR temporal encoding. self - attention USED-FOR spatial encoding. RNN USED-FOR central neural processor. self - attention USED-FOR central neural processor. temporal and spatial caches USED-FOR encoding components. spatial temporal decoder USED-FOR tasks. caches USED-FOR spatial temporal decoder. Method is OmiNet. ,"This paper proposes a learning architecture based on OmniNet- based on transformer for tasks in different modalities (e.g., images, text, videos). The authors propose a framework for asynchronous multi-task learning using pre-trained neural networks for each modalities. The central neural processor of the model is a CPU that computes the temporal information and spatial information using a spatio-temporal cache mechanism. The self-attention and the RNN are used for temporal encoding and spatial encoding respectively. The peripheral networks are used to learn a shared format/space. The encoding components are trained using temporal and spatial caches. The spatial temporal decoder is trained using the caches and is used for all tasks. The authors conduct extensive experiments to demonstrate the effectiveness of OmiNet."
3472,SP:6c8e2dd1d6224dffb95dbf729b159f00bfb05721,influence functions USED-FOR pointwise confidence intervals. pointwise confidence intervals USED-FOR regression models. marginal error term CONJUNCTION local variability error term. local variability error term CONJUNCTION marginal error term. coverage CONJUNCTION discrimination. discrimination CONJUNCTION coverage. discrimination EVALUATE-FOR method. coverage EVALUATE-FOR method. OtherScientificTerm is confidence interval. ,This paper proposes to use influence functions to learn pointwise confidence intervals for regression models. The confidence interval is defined as the sum of a marginal error term and a local variability error term. The method is evaluated on both coverage and discrimination.
3473,SP:6c8e2dd1d6224dffb95dbf729b159f00bfb05721,confidence intervals USED-FOR deep neural networks. guaranteed coverage FEATURE-OF confidence intervals. local uncertainty estimate USED-FOR algorithm. jackknife confidence interval estimate USED-FOR algorithm. algorithm COMPARE methods. methods COMPARE algorithm. toy and real - world examples EVALUATE-FOR algorithm. toy and real - world examples EVALUATE-FOR methods. coverage EVALUATE-FOR algorithm. coverage EVALUATE-FOR methods. Method is discriminative jackknife. ,This paper studies the problem of estimating confidence intervals for deep neural networks with guaranteed coverage. The authors propose an algorithm based on a local uncertainty estimate and a jackknife confidence interval estimate. The discriminative jackknife is used to estimate the confidence intervals. The proposed algorithm is evaluated on both toy and real-world examples and compared to other methods in terms of coverage.
3474,SP:6c8e2dd1d6224dffb95dbf729b159f00bfb05721,"discriminative jackknife ( DJ ) USED-FOR estimates of predictive uncertainty. DJ USED-FOR frequentist confidence intervals. posthoc procedure USED-FOR frequentist confidence intervals. posthoc procedure USED-FOR DJ. Task is machine learning. Generic is model. OtherScientificTerm are DJ confidence intervals, and higher order influence functions. ",This paper proposes discriminative jackknife (DJ) to improve estimates of predictive uncertainty in machine learning. The key idea is to use DJ as a posthoc procedure to compute frequentist confidence intervals for the model. The DJ confidence intervals are then used to estimate higher order influence functions.
3475,SP:057a035c4eeeb5fe985b20d0266126d66d9d243f,method USED-FOR generative adversarial network. high resolution videos USED-FOR generative adversarial network. discriminator PART-OF Adversarial Networks. spatial and temporal discriminator PART-OF discriminator. average pooling USED-FOR temporal discriminator. method COMPARE state - of - the - art methods. state - of - the - art methods COMPARE method. metrics EVALUATE-FOR state - of - the - art methods. metrics EVALUATE-FOR method. non - conditional setting FEATURE-OF Kinetic-600 dataset. Generic is network. ,"This paper proposes a method for training a generative adversarial network on high resolution videos. The proposed network is based on Adversarial Networks, where the discriminator is a combination of a spatial and temporal discriminator. The temporal adversinator is trained using average pooling. The method is evaluated on the Kinetic-600 dataset in a non-conditional setting and compared with state-of-the-art methods on several metrics."
3476,SP:057a035c4eeeb5fe985b20d0266126d66d9d243f,image - level spatial discriminator CONJUNCTION video - level temporal discriminator. video - level temporal discriminator CONJUNCTION image - level spatial discriminator. IS CONJUNCTION FVD quantitative metrics. FVD quantitative metrics CONJUNCTION IS. FID CONJUNCTION IS. IS CONJUNCTION FID. FID CONJUNCTION FVD quantitative metrics. FVD quantitative metrics CONJUNCTION FID. FVD quantitative metrics EVALUATE-FOR DVD - GAN. IS EVALUATE-FOR DVD - GAN. FID EVALUATE-FOR DVD - GAN. video generation works COMPARE DVD - GAN. DVD - GAN COMPARE video generation works. DVD - GAN HYPONYM-OF model. UCF-101 and Kinetics-600 dataset EVALUATE-FOR model. UCF-101 and Kinetics-600 dataset EVALUATE-FOR DVD - GAN. Task is video generation. ,"This paper studies the problem of video generation and proposes a model called DVD-GAN, which consists of an image-level spatial discriminator and a video-level temporal discriminator. The proposed model is evaluated on the UCF-101 and Kinetics-600 dataset and compared with other video generation works. The results show that the proposed model achieves better IS, FID, and FVD quantitative metrics."
3477,SP:057a035c4eeeb5fe985b20d0266126d66d9d243f,"class - conditional GAN model USED-FOR video generation. DVD - GAN HYPONYM-OF class - conditional GAN model. ConvGRU modules CONJUNCTION ResNet blocks. ResNet blocks CONJUNCTION ConvGRU modules. ConvGRU modules USED-FOR generator. ResNet blocks USED-FOR generator. single latent variable USED-FOR generator. discriminator PART-OF dual discriminator. image discriminator HYPONYM-OF discriminator. dual discriminator USED-FOR model. large - scale Kinetics-600 dataset USED-FOR model. large - scale Kinetics dataset USED-FOR large GAN model. model COMPARE video prediction model. video prediction model COMPARE model. Method are MoCoGAN model, and video discriminator. ","This paper proposes a new class-conditional GAN model for video generation, called DVD-GAN. The proposed model is based on a dual discriminator consisting of an image discriminator and a video discriminator. The generator is trained with ConvGRU modules and ResNet blocks, and the discriminator is trained on a single latent variable. The model is evaluated on a large-scale Kinetics-600 dataset and compared to the MoCoGAN model. The results show that the proposed model outperforms the video prediction model."
3478,SP:a89ee8eb2f60d9d522993a57d656f0ef726d86d6,"spatial attention USED-FOR method. Generic are task, and model. Method are FSL, and classifier. OtherScientificTerm is base classes. ","This paper proposes a method based on spatial attention, where the task is to find a set of classes that are similar to the base classes. The method is based on the idea of FSL. The idea is that the classifier should be able to generalize well to unseen classes. This is achieved by training the model on a large number of classes. "
3479,SP:a89ee8eb2f60d9d522993a57d656f0ef726d86d6,realistic setting USED-FOR few - shot learning. large - scale dataset USED-FOR pre - trained model. pre - trained model USED-FOR representations. pre - trained model COMPARE dataset. dataset COMPARE pre - trained model. it USED-FOR spatial attention map. Generic is model. Task is few - shot classification problem. ,"This paper studies the problem of few-shot learning in a realistic setting. The main idea is to train a pre-trained model on a large-scale dataset to learn representations that are similar to the original dataset. The model is trained in a way that it learns a spatial attention map, which is then used to solve a few -shot classification problem."
3480,SP:a89ee8eb2f60d9d522993a57d656f0ef726d86d6,"model USED-FOR prior knowledge. model USED-FOR algorithm. few - shot classification accuracy EVALUATE-FOR pre - trained model. Task are few - shot few - shot learning, base training tasks, and classification. Method is few - shot learning model. Generic is it. OtherScientificTerm are discriminative prediction, and pre - trained domain ’s classes. ","This paper studies the problem of few-shot little-shot learning, where a pre-trained model is trained to achieve a few percent improvement in the few -shot classification accuracy. The authors propose an algorithm that uses the learned model as prior knowledge to improve the performance of the pre-training model. The key idea is to train a few - shot learning model such that it performs discriminative prediction on the base training tasks, and then perform classification on the new training data. The main idea is that the new data should not be too different from the original training data, but should be similar to the original data. To do so, the authors propose to sample from a subset of the training data and then use the samples from this subset to train the model. This is done by sampling from a set of pre-trainable classes and then using the samples to predict the class of the next data point. The goal is to learn a few classes that are close to the target domain’s classes. "
3481,SP:d236f0b38414442af00b9be5e5d39e138f0069a2,"Xu et al. ’s semantic loss USED-FOR generator. Xu et al. ’s semantic loss USED-FOR Generative Adversarial Neural Networks. Super Mario Bros.-style levels CONJUNCTION molecules. molecules CONJUNCTION Super Mario Bros.-style levels. problem domains USED-FOR generation of constrained images. molecules HYPONYM-OF problem domains. constraints FEATURE-OF semantic loss. Method are GAN, and Constrained Adversarial Network. Generic is it. Material is constrained images. ","This paper proposes to use Xu et al.’s semantic loss as a generator in Generative Adversarial Neural Networks (GANs). The main idea is to use GAN as a constraint on the output of a GAN, and to use it as a way to train the generator. The authors propose to use the constraints of the semantic loss to learn the generator, and propose a Constrained Adversary Network, which is trained to generate constrained images. The generation of constrained images is a very important problem in many problem domains, such as Super Mario Bros.-style levels and molecules. "
3482,SP:d236f0b38414442af00b9be5e5d39e138f0069a2,penalty term USED-FOR Constrained Adversarial Networks ( CAN ). structural constraints PART-OF Constrained Adversarial Networks ( CAN ). penalty term USED-FOR structural constraints. semantic loss USED-FOR logical constraints. semantic loss USED-FOR penalty term. CAN COMPARE GAN. GAN COMPARE CAN. OtherScientificTerm is hard constraints. Generic is they. ,This paper proposes a new penalty term for adding structural constraints to Constrained Adversarial Networks (CAN). The proposed penalty term is based on a semantic loss that penalizes logical constraints. The authors argue that hard constraints should be penalized because they are more difficult to enforce. The experimental results show that the proposed CAN outperforms GAN.
3483,SP:d236f0b38414442af00b9be5e5d39e138f0069a2,"method USED-FOR generative adversarial networks. method USED-FOR generating structured objectives. generative adversarial networks USED-FOR generating structured objectives. term USED-FOR generation of invalid samples. term PART-OF GAN objective. term USED-FOR solution. OtherScientificTerm are semantic loss, and log probability. ",This paper proposes a method for training generative adversarial networks for generating structured objectives. The proposed solution is based on adding a term to the GAN objective to prevent generation of invalid samples. The semantic loss is defined as the difference between the log probability of the generated sample and the true sample.
3484,SP:110f0b86431f0a93cf48e08fe445e32172a37eae,"hinges PART-OF piece - wise linear activation unit. adversarial attacks FEATURE-OF robustness. Generic are unit, and method. OtherScientificTerm is activation. ","This paper proposes a piece-wise linear activation unit that consists of a set of hinges, each of which corresponds to a different piece of the unit. The authors claim that the proposed method is robust to adversarial attacks in terms of robustness to different types of perturbations of the activation. "
3485,SP:110f0b86431f0a93cf48e08fe445e32172a37eae,activation function USED-FOR S - APL. piecewise linear function USED-FOR activation function. S - APL USED-FOR neural networks. trainability CONJUNCTION robustness. robustness CONJUNCTION trainability. robustness EVALUATE-FOR neural networks. S - APL USED-FOR trainability. adversarial examples USED-FOR neural networks. robustness EVALUATE-FOR S - APL. OtherScientificTerm is APL activation function. ,"This paper proposes a new activation function for S-APL, which is a piecewise linear function. The authors show that the proposed APL activation function can be used to improve the trainability and robustness of neural networks trained on adversarial examples."
3486,SP:110f0b86431f0a93cf48e08fe445e32172a37eae,activation function USED-FOR deep neural networks. S - APL USED-FOR deep neural networks. S - APL HYPONYM-OF activation function. APL activation USED-FOR It. It COMPARE activation functions. activation functions COMPARE It. linear pieces FEATURE-OF It. ReLU HYPONYM-OF activation functions. S - APL COMPARE activation functions. activation functions COMPARE S - APL. networks EVALUATE-FOR activation functions. MNIST / CIFAR-10 / CIFAR-100 datasets EVALUATE-FOR activation functions. neural networks USED-FOR adversarial attacks. activation USED-FOR neural networks. OtherScientificTerm is x - axis. ,This paper proposes a new activation function called S-APL for deep neural networks. It is based on the APL activation and consists of linear pieces along the x-axis. It outperforms other activation functions such as ReLU. The authors evaluate the proposed activation functions on MNIST/CIFAR-10/CCF-100 datasets and compare their performance with other networks. The results show that the proposed new activation can improve the performance of neural networks against adversarial attacks.
3487,SP:4787aff0fb84beb13cde0d40c32d3a743d8e4082,"dirichlet prior USED-FOR uncertainty in predictions. dirichlet prior USED-FOR multinomial distribution. blackbox DL models USED-FOR multinomial distribution. dirichlet parameters CONJUNCTION model parameters. model parameters CONJUNCTION dirichlet parameters. NLP and CV domains EVALUATE-FOR method. Generic are prior, it, and model. Task is transfer learning tasks. ","This paper proposes a dirichlet prior to reduce the uncertainty in predictions. The prior is based on the fact that the multinomial distribution of blackbox DL models can be represented as a function of the number of samples. The authors propose to use it as an additional parameter to the model, and show that this can improve performance on transfer learning tasks. The proposed method is evaluated on NLP and CV domains, and the authors show that the combination of the proposed prior and the model parameters leads to better performance."
3488,SP:4787aff0fb84beb13cde0d40c32d3a743d8e4082,"method USED-FOR wrapper ” model. wrapper ” model USED-FOR multiclass predictor. wrapper model USED-FOR Dirichlet distribution. Task is estimate of model uncertainty. Generic is base model. OtherScientificTerm are categorical distribution, sampled predictive entropy, and decision. Method is wrapper. ","This paper proposes a method to train a “wrapper” model for a multiclass predictor. The idea is to use the estimate of model uncertainty as a proxy for the uncertainty of the base model. The wrapper model is trained on a Dirichlet distribution, where the categorical distribution is sampled and the sampled predictive entropy is used as a surrogate for the decision."
3489,SP:4787aff0fb84beb13cde0d40c32d3a743d8e4082,model USED-FOR selective prediction. black - box classification model USED-FOR model. model USED-FOR Dirichlet distribution. categorical distribution FEATURE-OF Dirichlet distribution. auxiliary model USED-FOR black - box and concentration parameter. L1 regularization term USED-FOR concentration parameter. L1 regularization term USED-FOR model. Dirichlet USED-FOR categorical distributions. Method is pre - trained models. OtherScientificTerm is average of sampled categorical distributions. ,This paper proposes a model for selective prediction based on a black-box classification model. The model is trained to approximate a Dirichlet distribution over a categorical distribution. The main idea is to use an auxiliary model to estimate both the black-Box and concentration parameter. The proposed model uses an L1 regularization term on the concentration parameter to encourage the model to generalize better to categorical distributions that are not sampled by the pre-trained models. The authors also propose to use the average of sampled categorical samples.
3490,SP:1207bf6cf93737d63e1a7cc1ff3a99bf9d6098f9,"input weight block FEATURE-OF input weights. bias term block FEATURE-OF bias term. square norm FEATURE-OF gradients. average square norm USED-FOR adaptation. Method are blockwise adaptivity, and linear threshold unit. ","This paper studies blockwise adaptivity, where the input weights are partitioned into an input weight block and a bias term block. The adaptation is based on the average square norm of the gradients with respect to a linear threshold unit."
3491,SP:1207bf6cf93737d63e1a7cc1ff3a99bf9d6098f9,BAG HYPONYM-OF generalization of AdaGrad. BAGM HYPONYM-OF BAG. BAG COMPARE adaptive gradient methods. adaptive gradient methods COMPARE BAG. Convergence rate EVALUATE-FOR algorithm. Metric is uniform stability. ,"This paper proposes BAG, a generalization of AdaGrad. BAGM is a variant of BAG. Convergence rate of the proposed algorithm is shown to be O(1/\sqrt{T}) where O(T) is the number of iterations. The authors also show that BAG is more stable than adaptive gradient methods, and that uniform stability is achieved."
3492,SP:1207bf6cf93737d63e1a7cc1ff3a99bf9d6098f9,"uniform stability CONJUNCTION generalization. generalization CONJUNCTION uniform stability. convergence FEATURE-OF non - convex optimization. regret FEATURE-OF online convex optimization. adaptivity COMPARE coordinate - wise adaptivity. coordinate - wise adaptivity COMPARE adaptivity. block level COMPARE coordinate - wise adaptivity. coordinate - wise adaptivity COMPARE block level. block level EVALUATE-FOR adaptivity. approach COMPARE alternatives. alternatives COMPARE approach. simulated and real - world problems EVALUATE-FOR alternatives. simulated and real - world problems EVALUATE-FOR approach. Method is adaptive gradient approaches. OtherScientificTerm are step - size, and blocks of coordinates. ","This paper studies the convergence of non-convex optimization in terms of uniform stability and generalization. In particular, the authors study adaptive gradient approaches and show that online convex optimization with regret converges to a fixed point with respect to the step-size. The authors also show that adaptivity at the block level is better than coordinate-wise adaptivity, and that blocks of coordinates are more stable. The proposed approach is evaluated on simulated and real-world problems and compared to alternatives."
3493,SP:0334d79349e9fb8ca32751b7ad29f82e00a5381c,"temporal ordering CONJUNCTION short- and long term reasoning. short- and long term reasoning CONJUNCTION temporal ordering. models COMPARE models. models COMPARE models. temporal dependencies USED-FOR models. dataset USED-FOR models. spatiotemporal reasoning USED-FOR models. Material are synthetic video understanding dataset, and action recognition datasets. OtherScientificTerm is scene biases. ","This paper presents a synthetic video understanding dataset. The dataset is designed to test whether models trained with spatiotemporal reasoning can generalize well to unseen scenes. The authors show that models trained on the dataset are able to generalize better than models trained without temporal dependencies (e.g. temporal ordering, short- and long term reasoning). The authors also show that scene biases can be leveraged to improve performance on action recognition datasets."
3494,SP:0334d79349e9fb8ca32751b7ad29f82e00a5381c,synthetically generated dataset USED-FOR video understanding tasks. CATER HYPONYM-OF synthetically generated dataset. CATER USED-FOR video understanding tasks. primitive actions CONJUNCTION compositional actions. compositional actions CONJUNCTION primitive actions. dataset USED-FOR videos of primitive actions. CLEVR USED-FOR dataset. motions of primitive 3D objects USED-FOR dataset. compositional action classification CONJUNCTION long - term temporal reasoning. long - term temporal reasoning CONJUNCTION compositional action classification. real video data USED-FOR frame aggregation - based methods. Task is 3D object localization tasks. ,"This paper presents CATER, a synthetically generated dataset for video understanding tasks. The dataset is built on top of CLEVR and consists of videos of primitive actions, compositional actions, and motions of primitive 3D objects. The authors conduct extensive experiments on 3D object localization tasks and show promising results on compositional action classification and long-term temporal reasoning. In addition, the authors also show that frame aggregation-based methods can be trained on real video data."
3495,SP:0334d79349e9fb8ca32751b7ad29f82e00a5381c,synthetic dataset ( CATER ) USED-FOR video understanding. static scenes CONJUNCTION object structures. object structures CONJUNCTION static scenes. spatial - temporal video models USED-FOR temporal dimension. video datasets USED-FOR object structures. static scenes FEATURE-OF video datasets. tasks USED-FOR temporal understanding. models USED-FOR temporal reasoning. Material is observable synthetic dataset. Method is video understanding models. ,This paper presents a synthetic dataset (CATER) for video understanding. The goal of this work is to provide an observable synthetic dataset that can be used to train video understanding models. The authors propose to use spatial-temporal video models to model the temporal dimension of video datasets with static scenes and object structures. They also propose two tasks for temporal understanding and show that the proposed models can perform temporal reasoning.
3496,SP:b637c75acbe9d0152384b632f2e92a0d248cb720,non - adversarial terms PART-OF discriminator. classifiers USED-FOR discriminator. non - adversarial terms PART-OF Boundary Calibration'GAN. relative acurracy HYPONYM-OF classifiers. Method is GANs. Task is classification tasks. Generic is methods. ,This paper proposes a 'Boundary Calibration' GAN that adds non-adversarial terms to the discriminator using classifiers such as relative acurracy. The authors argue that GANs should be able to generalize better to new classification tasks. The proposed methods are empirically validated on a variety of datasets.
3497,SP:b637c75acbe9d0152384b632f2e92a0d248cb720,method USED-FOR model compatibility. model compatibility FEATURE-OF GANs. method USED-FOR GANs. maximum mean discrepancy FEATURE-OF datasets. real dataset USED-FOR classifier. GAN - generated samples USED-FOR classifier. model compatibility FEATURE-OF produces generator. Task is generation procedure. Metric is visual quality. ,"This paper proposes a method to improve model compatibility of GANs. The main idea is to train a classifier on a real dataset with GAN-generated samples, and then train a generation procedure on the generated samples. The authors show that the maximum mean discrepancy between the two datasets can be reduced to 0.5 and 1.5, respectively, which can improve the visual quality of the generated images. They also show that a produces generator with high model compatibility can be trained with low model compatibility."
3498,SP:b637c75acbe9d0152384b632f2e92a0d248cb720,"Method are GAN, classifiers, and multi pretrained classifiers. OtherScientificTerm is real data distribution. ",This paper studies the problem of training a GAN to generalize well to unseen data points. The authors propose to train the classifiers on the real data distribution and then train multi pretrained classifiers to predict the true data distribution. This is an interesting idea and the paper is well-written. 
3499,SP:93616e31fa1dc64d130c0c44cbb73c0412b24a97,defense network USED-FOR learning. human - designed algorithm COMPARE learning model. learning model COMPARE human - designed algorithm. learning model USED-FOR attacker. human - designed algorithm USED-FOR attacker. defense EVALUATE-FOR framework. framework USED-FOR imitation learning. OtherScientificTerm is attacker network. ,This paper proposes a defense network for learning. The main idea is to train an attacker network to imitate a human-designed algorithm instead of the learning model used by the attacker. The proposed framework is evaluated on defense and imitation learning.
3500,SP:93616e31fa1dc64d130c0c44cbb73c0412b24a97,min - max training framework USED-FOR adversarial robustness. gradient - based attack USED-FOR inner maximization. neural network USED-FOR attack. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. method USED-FOR CW. CIFAR-100 EVALUATE-FOR method. CIFAR-10 EVALUATE-FOR method. Metric is clean accuracy. ,"This paper proposes a min-max training framework for improving adversarial robustness. The main idea is to use a gradient-based attack on the inner maximization of a neural network, which is designed to maximize the clean accuracy. The proposed method is evaluated on CIFAR-10 and CifAR-100 and shown to improve CW performance."
3501,SP:93616e31fa1dc64d130c0c44cbb73c0412b24a97,"attacker "" network HYPONYM-OF neural network. training scheme USED-FOR joint training. min - max problem USED-FOR joint training. method COMPARE adversarial training. adversarial training COMPARE method. gradient information USED-FOR attacker network. adversarial training USED-FOR CIFAR-10/100. CIFAR-10/100 EVALUATE-FOR method. ","This paper proposes a new neural network, called ""attacker"" network. The authors propose a training scheme for joint training with a min-max problem. The attacker network is trained with gradient information. The proposed method is compared to adversarial training on CIFAR-10/100."
3502,SP:0a66c3540383c76689258d2fffe0571ed944c1e7,MDPs USED-FOR IRL setting. learning of constraints USED-FOR IRL setting. learning of constraints USED-FOR MDPs. constrained MDP FEATURE-OF likelihood of demonstrations. algorithm USED-FOR constraints. OtherScientificTerm is features. Generic is approach. ,"This paper studies the problem of learning of constraints in MDPs in the IRL setting. Specifically, the authors consider the likelihood of demonstrations in a constrained MDP. The authors propose an algorithm that learns the constraints in an unsupervised manner, where the features of the MDP are not available to the agent. The proposed approach is evaluated on a variety of datasets."
3503,SP:0a66c3540383c76689258d2fffe0571ed944c1e7,"OtherScientificTerm are likelihood, and constraints. Method is MaxEnt IRL methods. Generic are problem, algorithm, and method. ","This paper studies the problem of estimating the likelihood of an action in the presence of constraints. The authors propose a new algorithm for this problem, which is motivated by MaxEnt IRL methods. The method is based on the observation that the likelihood can be reduced to zero when the constraints are relaxed. The proposed method is shown to be computationally efficient."
3504,SP:0a66c3540383c76689258d2fffe0571ed944c1e7,"constraints USED-FOR MDP. state - action pairs FEATURE-OF constraints. objective USED-FOR maximization of the demonstration likelihood. constraint FEATURE-OF maximization of the demonstration likelihood. greedy iterative constraint inference USED-FOR constraints. algorithm USED-FOR feature occupancy. feature occupancy USED-FOR constraints. algorithm USED-FOR constraints. greedy iterative constraint inference USED-FOR feature occupancy. GridWorld EVALUATE-FOR method. Method are inverse constraint learning method, and inverse cost learning. OtherScientificTerm are reward function, and normalization constant. Material is tabular setting. Generic is optimization. Task is maximum coverage problem. ","This paper proposes an inverse constraint learning method. The main idea is to learn constraints for an MDP that are independent of state-action pairs. The objective is to optimize the maximization of the demonstration likelihood under the constraint. The authors propose to use inverse cost learning, where the reward function is a function of the number of states and actions. In the tabular setting, the authors propose an algorithm that learns the feature occupancy of the constraints using greedy iterative constraint inference. The optimization is similar to the maximum coverage problem, except that the authors add a normalization constant. The proposed method is evaluated on GridWorld."
3505,SP:2a5fba69a6287b87a19bcd745d2e4326bbb723de,human visual model USED-FOR artificial neural network architecture. ( human vision ) model USED-FOR human visual illusions. contextual ones HYPONYM-OF human visual illusions. human visual model USED-FOR artificial neural networks. formulation USED-FOR top - down connections. constraints PART-OF human visual model. ,"This paper proposes a new artificial neural network architecture based on a human visual model. The authors use a (human vision) model to model human visual illusions such as contextual ones, and propose a new formulation of top-down connections that can be used to train artificial neural networks. The main contribution of the paper is the incorporation of constraints into the human vision model."
3506,SP:2a5fba69a6287b87a19bcd745d2e4326bbb723de,complex hierarchical recurrent model USED-FOR contour detection. organization of cortical circuits USED-FOR complex hierarchical recurrent model. it USED-FOR tilt illusion. transfer - learning orientation estimation USED-FOR it. cell segmentation ( SNEMI3D ) datasets EVALUATE-FOR model. transfer - learning orientation estimation USED-FOR tilt illusion. ,This paper proposes a complex hierarchical recurrent model for contour detection based on the organization of cortical circuits. The model is evaluated on cell segmentation (SNEMI3D) datasets and it is shown to be able to detect the tilt illusion using transfer-learning orientation estimation.
3507,SP:2a5fba69a6287b87a19bcd745d2e4326bbb723de,"they USED-FOR limitations of perception. visual illusions HYPONYM-OF human visual system. neural networks USED-FOR visual data. robustness EVALUATE-FOR human visual system. circuits USED-FOR neural networks. neural circuits USED-FOR visual illusions. computational solution USED-FOR visual illusions. architecture COMPARE SOTA convolutional architectures. SOTA convolutional architectures COMPARE architecture. contour detection tasks EVALUATE-FOR SOTA convolutional architectures. contour detection tasks EVALUATE-FOR architecture. \gamma - networks HYPONYM-OF neural network architecture. Method is recurrent network architecture. OtherScientificTerm are features, and visual system. ","This paper proposes a recurrent network architecture that learns to predict the next layer of a circuit in a recurrent fashion. The authors argue that neural networks trained on visual data trained on circuits can be useful as they can be used to overcome limitations of perception and improve the robustness of the human visual system, e.g. visual illusions. The main contribution of this paper is that the authors propose a computational solution for visual illusions that can be learned using neural circuits. The proposed architecture, called \gamma-networks, is shown to outperform SOTA convolutional architectures on contour detection tasks. The paper also shows that the features learned by the proposed architecture can be transferred to other neural network architecture. "
3508,SP:4f094a3f7eeb302738c2b482fbaca56e34ac6a99,context - aware neural network ( conCNN ) USED-FOR object detection. context semantics USED-FOR object detection. context semantics PART-OF context - aware neural network ( conCNN ). context - aware module PART-OF R - CNN detection framework. context - aware module PART-OF approach. context - aware module USED-FOR Conditional Random Fields ( CRF ) model. stack of common CNN operations USED-FOR context - aware module. stack of common CNN operations USED-FOR Conditional Random Fields ( CRF ) model. mean - field approach USED-FOR paper. Material is COCO dataset. ,This paper proposes a novel context-aware neural network (conCNN) that incorporates the context semantics for object detection. The proposed approach consists of a standard R-CNN detection framework that includes a context -aware module that takes as input a stack of common CNN operations and outputs a Conditional Random Fields (CRF) model. The paper is based on a mean-field approach and is evaluated on the COCO dataset.
3509,SP:4f094a3f7eeb302738c2b482fbaca56e34ac6a99,contextual reasoning module USED-FOR object detection. approach USED-FOR object detection. approach USED-FOR contextual reasoning module. end - to - end modules USED-FOR algorithm. NIPS 2011 USED-FOR algorithm. R - CNN USED-FOR detection framework. Relation Module HYPONYM-OF baseline. OtherScientificTerm is small objects. ,This paper proposes an approach to train a contextual reasoning module for object detection. The proposed algorithm is based on NIPS 2011 and consists of two end-to-end modules. The detection framework is implemented using R-CNN and is trained on small objects. The experimental results are compared to a baseline called Relation Module.
3510,SP:4f094a3f7eeb302738c2b482fbaca56e34ac6a99,"CRF - based context module USED-FOR CNN - based object detectors. RCNN HYPONYM-OF two - stage region - based detector. context module USED-FOR two - stage region - based detector. node PART-OF CRF. neural network layers USED-FOR Message passing. object detector outputs CONJUNCTION box overlap. box overlap CONJUNCTION object detector outputs. box overlap CONJUNCTION co - occurrence of class labels. co - occurrence of class labels CONJUNCTION box overlap. box overlap USED-FOR Potentials. object detector outputs USED-FOR Potentials. co - occurrence of class labels USED-FOR Potentials. OtherScientificTerm are classification head, and classification label. Method is RPN. Task is MS COCO object detection task. ","This paper proposes a CRF-based context module for CNN-based object detectors. The context module is a two-stage region-based detector (RCNN). Message passing is performed by two neural network layers, one for each node in the CRF, and the other for each classification head. Potentials are generated from object detector outputs, box overlap, and co-occurrence of class labels. The RPN is evaluated on MS COCO object detection task."
3511,SP:74d63293d2f8a41a14743bfcd8939fca5e804fdb,training CONJUNCTION inference. inference CONJUNCTION training. BatchNorm USED-FOR inference. BatchNorm USED-FOR training. training CONJUNCTION inference. inference CONJUNCTION training. data distributions USED-FOR training. data distributions USED-FOR inference. min - max rescaling COMPARE normalization. normalization COMPARE min - max rescaling. mean CONJUNCTION running mean. running mean CONJUNCTION mean. denominator USED-FOR inference. running mean USED-FOR inference. running mean FEATURE-OF denominator. running mean USED-FOR running average. mean USED-FOR running average. robustness CONJUNCTION accuracy. accuracy CONJUNCTION robustness. accuracy EVALUATE-FOR clean data. robustness EVALUATE-FOR clean data. OtherScientificTerm is adversarial perturbations. Generic is alternative. Method is RobustNorm. ,"This paper proposes an alternative to BatchNorm for both training and inference on data distributions that are not robust to adversarial perturbations. Instead of using min-max rescaling, the authors propose to use normalization. The main idea is to use the mean and running mean of the denominator for inference and use the running average as the mean. The authors show that the proposed alternative, RobustNorm, improves both robustness and accuracy on clean data."
3512,SP:74d63293d2f8a41a14743bfcd8939fca5e804fdb,"test accuracy EVALUATE-FOR BatchNorm. normalization method COMPARE BatchNorm. BatchNorm COMPARE normalization method. attack methods USED-FOR normalization method. unperturbed datasets EVALUATE-FOR BatchNorm. test accuracy EVALUATE-FOR normalization method. Method are Robust Normalization, and Normalization. OtherScientificTerm is adversarial vulnerability. ","This paper proposes Robust Normalization, a new normalization method that aims to improve the test accuracy of BatchNorm on unperturbed datasets. The main contribution of this paper is to propose a new type of attack methods to attack the proposed normalization and show that the proposed Normalization is more robust to adversarial vulnerability. The paper also provides a theoretical analysis of the effect of different attack methods on the performance of the normalization proposed by the paper."
3513,SP:74d63293d2f8a41a14743bfcd8939fca5e804fdb,tracking part PART-OF BatchNorm. BatchNorm USED-FOR adversarial vulnerability. BatchNorm COMPARE that. that COMPARE BatchNorm. robustness EVALUATE-FOR networks. test accuracy EVALUATE-FOR clean images. RobustNorm COMPARE BatchNorm. BatchNorm COMPARE RobustNorm. RobustNorm USED-FOR natural and adversarial scenarios. BatchNorm USED-FOR natural and adversarial scenarios. ,"This paper proposes to replace the tracking part of BatchNorm with a more robust version, called RobustNorm, which aims to reduce adversarial vulnerability and improve robustness of networks. Compared to that, the authors show that RobustNNorm is able to achieve better test accuracy on clean images, and that BatchNNorm can achieve better performance on both natural and adversarial scenarios."
3514,SP:f16d3e61eda162dfee39396abbd594425f47f625,first HYPONYM-OF regularization methods. auxiliary variable USED-FOR noise. second USED-FOR noise. auxiliary variable USED-FOR second. NTK - theory USED-FOR noisily labeled dataset. NTK - theory USED-FOR clean dataset. noisily labeled dataset USED-FOR clean dataset. 2 - layer NN CONJUNCTION CNN. CNN CONJUNCTION 2 - layer NN. regularization methods USED-FOR 2 - layer NN. CNN HYPONYM-OF regularization methods. Material is noisily labeled data. OtherScientificTerm is Euclidean norm. ,"This paper proposes two regularization methods: the first is to add an auxiliary variable to remove the noise from the noisily labeled data, and the second is to use NTK-theoretic results to generate a clean dataset from the original, non-noisy labeled dataset. The authors compare the performance of 2-layer NN and CNN with the two regularizations, and show that the Euclidean norm is the best one."
3515,SP:f16d3e61eda162dfee39396abbd594425f47f625,classification problem HYPONYM-OF learning with noisy labels. generalization bounds USED-FOR kernel ridge regression solutions. regularization techniques COMPARE kernel ridge regression. kernel ridge regression COMPARE regularization techniques. Method is neural networks. OtherScientificTerm is neural tangent kernel regime. ,"This paper studies the classification problem, i.e., learning with noisy labels, in neural networks. The authors provide generalization bounds for kernel ridge regression solutions, and extend the results to the neural tangent kernel regime. They also show that the proposed regularization techniques are more effective than the original kernels."
3516,SP:f16d3e61eda162dfee39396abbd594425f47f625,early stopping USED-FOR training of noisily labeled samples. regularizing HYPONYM-OF regularization methods. distance USED-FOR regularizing. kernel ridge regression HYPONYM-OF regularization methods. regularization methods USED-FOR NTK regime. gradient descent USED-FOR regularization methods. generalization bound EVALUATE-FOR solution. solution USED-FOR clean data distribution. noisy label USED-FOR solution. OtherScientificTerm is auxiliary variable. ,"This paper studies the problem of early stopping during training of noisily labeled samples. The authors propose two regularization methods for the NTK regime: (1) kernel ridge regression, where the distance between the true label and the auxiliary variable is minimized, and (2) regularizing with gradient descent. The proposed solution has a generalization bound of O(1/\sqrt{T}) and is shown to converge to a clean data distribution with a noisy label."
3517,SP:67335658ec9de6ba3fa352ca4de073ac51f2f703,pixel - shift based data augmentation CONJUNCTION average pooling operations. average pooling operations CONJUNCTION pixel - shift based data augmentation. average pooling operations USED-FOR CNN - NNGP / NTK based ridge regression. pixel - shift based data augmentation USED-FOR CNN - NNGP / NTK based ridge regression. average pooling COMPARE global average pooling. global average pooling COMPARE average pooling. data pre - processing step USED-FOR CNN - NNGP / NTK based ridge regression. Metric is classification accuracy. Material is CIFAR-10. Method is learned representations. ,"This paper proposes to use pixel-shift based data augmentation and average pooling operations for CNN-NNGP/NTK based ridge regression. The authors show that the proposed data pre-processing step significantly improves the classification accuracy on CIFAR-10. Moreover, the authors show the advantage of using the averaged pooling instead of the global average pooleding. Finally, the learned representations are shown to be more interpretable."
3518,SP:67335658ec9de6ba3fa352ca4de073ac51f2f703,CNTK CONJUNCTION CNN - GP. CNN - GP CONJUNCTION CNTK. CNN - GP USED-FOR They. CNTK USED-FOR They. representations COMPARE supervised neural networks. supervised neural networks COMPARE representations. Generic is architectures. Task is learning. OtherScientificTerm is classification layer. ,This paper proposes two architectures for learning. They are based on CNTK and CNN-GP. The authors show that the learned representations are better than supervised neural networks. The main contribution of the paper is the introduction of a classification layer. The paper also provides a theoretical analysis of the performance of the proposed architectures.
3519,SP:67335658ec9de6ba3fa352ca4de073ac51f2f703,"CNN - GP CONJUNCTION CNTKs. CNTKs CONJUNCTION CNN - GP. Material are CIFAR-10 dataset, Fashion - MNIST, and CIFAR-10. Method are Local Average Pooling ( LAP ) layers, flip data augmentation, feature extractor, and neural networks. OtherScientificTerm are Global Average Pooling ( GAP ), and Pooling layer. Metric is classification accuracy. ","This paper proposes to augment CNN-GP and CNTKs with Local Average Pooling (LAP) layers, which is a variant of the Global AveragePooling (GAP) layer proposed in [1]. The authors propose to use flip data augmentation, where a feature extractor is used to augment the weights of the Pooling layer. The experiments are conducted on the CIFAR-10 dataset and Fashion-MNIST. The results show that the proposed neural networks are able to improve classification accuracy by a significant margin. "
3520,SP:4ffab7f7f9fc09fdf59602228d231c6f6330fb98,"MCTS USED-FOR root node ( Q_MCTS ). MCTS USED-FOR estimated Q - values. MCTS USED-FOR SAVE. estimated Q - values USED-FOR SAVE. It USED-FOR amortized value network Q_theta. linear combination of Q - learning loss CONJUNCTION cross - entropy loss. cross - entropy loss CONJUNCTION linear combination of Q - learning loss. softmax(Q_MCTS ) CONJUNCTION softmax(Q_theta ). softmax(Q_theta ) CONJUNCTION softmax(Q_MCTS ). linear combination of Q - learning loss USED-FOR It. linear combination of Q - learning loss USED-FOR amortized value network Q_theta. SAVE USED-FOR MCTS. Q - function PART-OF MCTS. Q - function USED-FOR SAVE. it USED-FOR SAVE. SAVE COMPARE baseline algorithms. baseline algorithms COMPARE SAVE. Method is Search with Amortized Value Estimates ( SAVE ). OtherScientificTerm are policy, and search budget. ","This paper proposes Search with Amortized Value Estimates (SAVE), which is an extension of the work of [1]. SAVE uses MCTS to train a root node (Q_MCTS) to estimate the estimated Q-values for a policy. It trains an amortized value network Q_theta with a linear combination of Q-learning loss and a cross-entropy loss. It uses a combination of softmax(Q_mCTS, Q_ta) and softmax(-Q_a) to compute the Q-function of Q_a, which is then used to train the root node. The authors show that SAVE is able to improve the performance of the MMTS by incorporating the Q -function of the original Q_function into MCTC, and that it can be used to improve SAVE by a factor of 1.5 when the search budget is $O(1/\sqrt{T})$. The authors compare SAVE with several baseline algorithms and show that the proposed SAVE outperforms the baseline algorithms."
3521,SP:4ffab7f7f9fc09fdf59602228d231c6f6330fb98,Q learning CONJUNCTION MCTS. MCTS CONJUNCTION Q learning. MCTS PART-OF SAVE. Q learning PART-OF SAVE. Q values USED-FOR Q function. Q values USED-FOR selection and backup phase of MCTS. Q values USED-FOR MCTS. Q learning CONJUNCTION MCTS. MCTS CONJUNCTION Q learning. SAVE COMPARE generic UCT. generic UCT COMPARE SAVE. PUCT CONJUNCTION Q learning. Q learning CONJUNCTION PUCT. generic UCT CONJUNCTION PUCT. PUCT CONJUNCTION generic UCT. SAVE COMPARE Q learning. Q learning COMPARE SAVE. SAVE COMPARE PUCT. PUCT COMPARE SAVE. ,"This paper proposes SAVE, a combination of Q learning and MCTS. The main idea is to use the Q values of the Q function during the selection and backup phase of MCTC. Experiments show that the proposed SAVE outperforms generic UCT, PUCT, Q learning, etc."
3522,SP:4ffab7f7f9fc09fdf59602228d231c6f6330fb98,"model - free RL CONJUNCTION model - based search. model - based search CONJUNCTION model - free RL. model - free RL PART-OF approach. MCTS HYPONYM-OF model - based search. model - based search PART-OF approach. Q - learning HYPONYM-OF model - free RL. loss function USED-FOR value function. value estimates PART-OF SAVE. Expert Iteration HYPONYM-OF approaches. search USED-FOR value estimates. OtherScientificTerm are AlphaZero, and root node. ","This paper proposes an approach that combines model-free RL (Q-learning and Q-learning) with model-based search (e.g., MCTS). The authors propose a new loss function to approximate the value function, which they call AlphaZero. The authors show that the root node of AlphaZero can be used as a proxy for the value estimates in SAVE. They also show that this search can produce value estimates that are close to the true value estimates. They compare their approach with other approaches such as Expert Iteration."
3523,SP:ab451cc0ec221864a5da532eceba0f021f30def4,"it USED-FOR 3D visualization. right and left view HYPONYM-OF stereoscopic view. Task are stereoscopic view synthesis, and generating a stereoscopic view. ","This paper studies the problem of stereoscopic view synthesis. The main contribution of this paper is to propose a new way of generating a stereooscopic view (i.e., a right and left view) and apply it to 3D visualization. The paper is well-written and easy to follow."
3524,SP:ab451cc0ec221864a5da532eceba0f021f30def4,method USED-FOR stereoscopic view synthesis. neural network model USED-FOR viewpoint. neural network model PART-OF method. t - shaped kernel USED-FOR novel view synthesis. adaptive dilations USED-FOR kernels. ,This paper proposes a method for stereoscopic view synthesis that combines a neural network model that predicts the viewpoint and a novel view synthesis based on a t-shaped kernel. The kernels are trained with adaptive dilations.
3525,SP:ab451cc0ec221864a5da532eceba0f021f30def4,"visually pleasing image generation CONJUNCTION low metric errors. low metric errors CONJUNCTION visually pleasing image generation. low metric errors FEATURE-OF datasets. datasets EVALUATE-FOR visually pleasing image generation. Method is deep learning method. OtherScientificTerm are translated viewpoints, and fixed baseline. Generic are approach, and problem. ",This paper proposes a deep learning method that learns to generate images from translated viewpoints. The approach is based on the observation that visually pleasing image generation and low metric errors can be achieved on standard datasets with fixed baseline. The paper provides a theoretical analysis of the problem and provides empirical evidence that this problem can be solved.
3526,SP:e308cf28f7bd5d8e6c36517e2845298ccd401f5d,"information - theoretic training scheme USED-FOR Variational Auto - Encoders ( VAEs ). scheme USED-FOR disentanglement problem of VAEs. Method is over - capacity encoder. OtherScientificTerm are KL divergence, prior, and latent representations. Generic is them. ","This paper proposes an information-theoretic training scheme for Variational Auto-Encoders (VAEs). The proposed scheme aims to address the disentanglement problem of VAEs, where the over-capacity encoder can be seen as a function of the KL divergence between the prior and the latent representations. The authors propose to train them in an unsupervised manner, and show that this leads to better performance."
3527,SP:e308cf28f7bd5d8e6c36517e2845298ccd401f5d,"Capacity Constrained InfoMax USED-FOR learning principle. learning principle USED-FOR Variational InfoMax AutoEncoder ( VIMAE ). Capacity Constrained InfoMax USED-FOR Variational InfoMax AutoEncoder ( VIMAE ). InfoVAE CONJUNCTION β - VAE. β - VAE CONJUNCTION InfoVAE. VAE CONJUNCTION β - VAE. β - VAE CONJUNCTION VAE. noise CONJUNCTION generalization. generalization CONJUNCTION noise. VIMAE COMPARE VAE. VAE COMPARE VIMAE. VIMAE COMPARE β - VAE. β - VAE COMPARE VIMAE. robustness CONJUNCTION generalization. generalization CONJUNCTION robustness. noise FEATURE-OF robustness. entropy EVALUATE-FOR β - VAE. robustness EVALUATE-FOR β - VAE. OtherScientificTerm are encoding information, network capacity, representations, and information bottleneck idea. ","This paper proposes Variational InfoMax AutoEncoder (VIMAE), which uses Capacity Constrained InfoMax as a learning principle to improve the efficiency of encoding information. The authors compare InfoVAE, β-VAE and VIMAE in terms of entropy, robustness to noise, generalization, etc. They also compare the performance with VAE, and show that the network capacity does not affect the quality of the representations. The paper also provides a theoretical analysis of the information bottleneck idea."
3528,SP:e308cf28f7bd5d8e6c36517e2845298ccd401f5d,Wasserstein Autoencoders CONJUNCTION InfoVAE. InfoVAE CONJUNCTION Wasserstein Autoencoders. OtherScientificTerm is variational autoencoding objectives. Generic is objectives. ,"This paper proposes variational autoencoding objectives, which can be viewed as a generalization of the Wasserstein Autoencoders and InfoVAE. The authors provide a theoretical analysis of these objectives and provide some empirical results."
3529,SP:ef4a0c82cc364b797fba0ba86a91d9945b66a193,"node ’s local distribution over attributes USED-FOR attributed network embedding method. neighborhood nodes USED-FOR multi - scale embedding approach. pooled and multi - scale versions HYPONYM-OF embedding methods. OtherScientificTerm are neighborhood attribute distribution, and node - feature pointwise mutual information matrix. Generic is model. ","This paper proposes a new attributed network embedding method based on a node’s local distribution over attributes. The proposed multi-scale embedding approach is based on the neighborhood nodes, where each node has its own neighborhood attribute distribution, and each node is assigned a node-feature pointwise mutual information matrix. The model is evaluated on a variety of datasets, and compared to existing embedding methods (both pooled and multi -scale versions)."
3530,SP:ef4a0c82cc364b797fba0ba86a91d9945b66a193,attribute distribution USED-FOR embedding algorithms. multi - scale version of AE USED-FOR multi - scale attribute information. algorithms USED-FOR PMI matrix. algorithms USED-FOR interpretability. interpretability FEATURE-OF PMI matrix. transfer learning CONJUNCTION regression. regression CONJUNCTION transfer learning. regression CONJUNCTION link prediction. link prediction CONJUNCTION regression. node classification CONJUNCTION transfer learning. transfer learning CONJUNCTION node classification. link prediction HYPONYM-OF scenarios. node classification HYPONYM-OF scenarios. regression HYPONYM-OF scenarios. transfer learning HYPONYM-OF scenarios. Method is multi - scaling. ,"This paper proposes a multi-scale version of AE to incorporate multi-Scale attribute information. The key idea is to use the attribute distribution to train embedding algorithms. The multi-scaling is motivated by the observation that the interpretability of the PMI matrix can be improved by algorithms that are trained to maximize interpretability. Experiments are conducted on several scenarios including node classification, transfer learning, regression, and link prediction."
3531,SP:ef4a0c82cc364b797fba0ba86a91d9945b66a193,"attribute distributions USED-FOR Skip - gram style embedding algorithms. local neighborhoods FEATURE-OF attribute distributions. random - walk way USED-FOR node feature. Node features USED-FOR node content. OtherScientificTerm are randomly selected node features, features, and random features. ","This paper studies the attribute distributions of Skip-gram style embedding algorithms in local neighborhoods. The authors propose a random-walk way to learn a node feature, which is based on randomly selected node features. Node features are used to represent node content, and the features are learned in a way that is similar to random features."
3532,SP:70d92189aedeb4148b61b987d97a3c15898dd834,formula USED-FOR IB phase transitions. information bottleneck ( IB ) objective FEATURE-OF phase transition problem. algorithm USED-FOR phase transition points. IB USED-FOR classification. OtherScientificTerm is phase transition. Method is neural networks. ,"This paper proposes a new formula for estimating IB phase transitions. The phase transition problem is formulated as an information bottleneck (IB) objective, and the authors propose an algorithm to estimate phase transition points. The authors show that the phase transition can be estimated using neural networks, and that the IB can be used for classification."
3533,SP:70d92189aedeb4148b61b987d97a3c15898dd834,"beta parameter FEATURE-OF IB. Task are information bottleneck ( IB ) principle, phase transition phenomenon, and generalization. OtherScientificTerm are phase transition betas, LHS, and encoding function. ","This paper studies the information bottleneck (IB) principle. The authors study the phase transition phenomenon and show that the beta parameter of the IB is a function of the number of phase transition betas. They also show that if the LHS is large enough, then the encoding function is also large enough. Finally, the authors show that generalization can be achieved when the beta is small."
3534,SP:70d92189aedeb4148b61b987d97a3c15898dd834,"information bottleneck FEATURE-OF phase transition. algorithm USED-FOR IB phase transitions. algorithm USED-FOR MNIST and CIFAR10 dataset. OtherScientificTerm are IB loss landscape, and Fisher information. ",This paper proposes an algorithm for IB phase transitions. The main idea is to model the phase transition as an information bottleneck. The authors show that the IB loss landscape can be viewed as a function of Fisher information. The proposed algorithm is applied to the MNIST and CIFAR10 dataset.
3535,SP:fecfd5e98540e2d146a726f94802d96472455111,"advantage estimate USED-FOR reinforcement learning. Bayes theorem USED-FOR ratio. Method are classifier, unbiased advantage estimator, and Monte - Carlo estimator. OtherScientificTerm are dependency factor, expectation, and S_t. ","This paper studies the advantage estimate in reinforcement learning. The main idea is to estimate the ratio between a classifier’s performance and an unbiased advantage estimator. The ratio is derived from the Bayes theorem, where the dependency factor is defined as the difference between the expectation of the classifier and the true classifier. The paper then proposes a Monte-Carlo estimator, where S_t is a function of the number of samples."
3536,SP:fecfd5e98540e2d146a726f94802d96472455111,advantage estimator USED-FOR reinforcement learning. importance sampling USED-FOR advantage estimator. form USED-FOR lower - variance estimator. control variate USED-FOR importance sampling estimator. importance sampling estimator CONJUNCTION estimator. estimator CONJUNCTION importance sampling estimator. control variate USED-FOR estimator. ,This paper proposes a new advantage estimator for reinforcement learning based on importance sampling. The proposed form can be used as a lower-variance estimator. The main idea is to combine the importance sampling estimator with an estimator based on a control variate.
3537,SP:fecfd5e98540e2d146a726f94802d96472455111,independence property USED-FOR variance of advantage function. control variate USED-FOR variance. dependency model of reward function USED-FOR variance. control variate USED-FOR dependency model of reward function. algorithm USED-FOR policy. control variate technique USED-FOR algorithm. PPO USED-FOR algorithm. PPO USED-FOR policy. ,This paper studies the independence property of the variance of advantage function. The variance is modeled as a dependency model of reward function using a control variate. The authors propose an algorithm based on the controlvariate technique to learn a policy using PPO.
3538,SP:f0d84396e0ede7969d3f3f55549d214f20daf1b0,doubly robust estimator USED-FOR off - policy policy evaluation. estimate of the state density ratio COMPARE long products of action importance weights. long products of action importance weights COMPARE estimate of the state density ratio. estimate of the state density ratio USED-FOR infinite horizon technique. infinite horizon technique USED-FOR doubly robust estimator. method CONJUNCTION Lagrangian duality. Lagrangian duality CONJUNCTION method. Metric is doubly robust estimator's bias. OtherScientificTerm is stationary distribution ratio estimate. Method is infinite horizon estimator. ,"This paper proposes a doubly robust estimator for off-policy policy evaluation. The authors propose an infinite horizon technique based on the estimate of the state density ratio instead of the long products of action importance weights, which is shown to be more robust to perturbations of the stationary distribution ratio estimate. In addition, the authors propose a method to reduce the variance of the estimator's bias by combining the proposed method with Lagrangian duality. Experiments demonstrate the effectiveness of the proposed infinite horizon estimator."
3539,SP:f0d84396e0ede7969d3f3f55549d214f20daf1b0,"approach USED-FOR reducing bias. stationary state distributions USED-FOR reducing variance. stationary state distributions FEATURE-OF off - policy evaluation. doubly robust method USED-FOR reducing bias. value function USED-FOR it. state distribution ratio CONJUNCTION estimation of the value function. estimation of the value function CONJUNCTION state distribution ratio. estimation of the value function USED-FOR approach. Method is low variance, low bias OPE. ","This paper proposes a doubly robust method for reducing bias. The key idea is to use stationary state distributions for reducing variance in off-policy evaluation. The proposed approach is based on the state distribution ratio and the estimation of the value function, and it is shown to converge to a value function that is low variance, low bias OPE."
3540,SP:f0d84396e0ede7969d3f3f55549d214f20daf1b0,algorithm USED-FOR off - policy evaluation problem. reinforcement learning USED-FOR off - policy evaluation problem. value function learning method CONJUNCTION stationary distribution ratio estimators. stationary distribution ratio estimators CONJUNCTION value function learning method. value function learning method PART-OF It. stationary distribution ratio estimators PART-OF It. double robustness EVALUATE-FOR method. control domains EVALUATE-FOR algorithm. Generic is estimator. Method is value function or ratio estimator. ,This paper proposes an algorithm for solving the off-policy evaluation problem in reinforcement learning. It combines a value function learning method with stationary distribution ratio estimators. The estimator is trained to be close to the true value function or ratio estimator. The proposed method achieves double robustness on two control domains.
3541,SP:73f8dddb09333a739c609cc324a1e813d29f8874,method USED-FOR adaptation. few - shot learning setting FEATURE-OF adaptation. metric - softmax loss USED-FOR features. exponent term PART-OF softmax loss. Gaussian kernel - based radial basis function USED-FOR exponent term. Task - Adaptive Transformation HYPONYM-OF task adaptation process. affine transformation USED-FOR Task - Adaptive Transformation. support set USED-FOR method. affine transformation USED-FOR support and query image sets. OtherScientificTerm is probability score calculating function. Task is feature learning process. Material is training / base set. ,"This paper proposes a method for adaptation in the few-shot learning setting. The main idea is to use a metric-softmax loss to estimate the features and then use a Gaussian kernel-based radial basis function to replace the exponent term in the softmax loss. The authors also propose a probability score calculating function to reduce the variance of the feature learning process. The proposed method is trained on a support set and on a query set. Task-Adaptive Transformation is a task adaptation process that uses an affine transformation to generate support and query image sets, respectively, from the training/base set."
3542,SP:73f8dddb09333a739c609cc324a1e813d29f8874,"metric - learning perspective USED-FOR few - shot learning. Guassian kernel RBF USED-FOR metric - softmax "" loss. class templates / weights USED-FOR Guassian kernel RBF. softmax + cross - entropy loss HYPONYM-OF softmax loss. loss USED-FOR training. it USED-FOR discriminative feature. ","This paper studies few-shot learning from a metric-learning perspective. The authors propose a new ""metric-softmax"" loss based on the Guassian kernel RBF with different class templates/weights. The proposed softmax loss is called the softmax + cross-entropy loss. This loss can be used during training and it is shown to be a discriminative feature."
3543,SP:73f8dddb09333a739c609cc324a1e813d29f8874,"metric - softmax loss USED-FOR meta - training dataset. linear transformation USED-FOR few - shot training data. linear transformation USED-FOR features. mini - Imagenet CONJUNCTION CUB-200 - 2011 datasets. CUB-200 - 2011 datasets CONJUNCTION mini - Imagenet. CUB-200 - 2011 datasets EVALUATE-FOR 5 - way-5 - shot testing. mini - Imagenet EVALUATE-FOR 5 - way-5 - shot testing. Method is few - shot image classification algorithm. OtherScientificTerm are episodic updates, and metric soft - max loss. ","This paper proposes a few-shot image classification algorithm. The key idea is to use episodic updates, where the features are learned using a linear transformation on the few -shot training data, and the metric-softmax loss is applied to the meta-training dataset. Experiments on 5-way-5-shot testing on mini-Imagenet and CUB-200-2011 datasets demonstrate the effectiveness of the proposed metric soft-max loss."
3544,SP:cde2a84c463cdab9b19fcbdaf1cfe20d0187dcfa,accuracy EVALUATE-FOR numerical solvers. simulated reference data USED-FOR neural network. simulations of partial differential equations HYPONYM-OF numerical solvers. neural network USED-FOR numerical solver. square loss CONJUNCTION task specific regularization. task specific regularization CONJUNCTION square loss. square loss USED-FOR approximation scheme. volume preservation HYPONYM-OF task specific regularization. Generic is unsupervised version. Method is differentiable numberical solver. ,"This paper aims to improve the accuracy of numerical solvers (e.g., simulations of partial differential equations) by training a neural network on simulated reference data. The approximation scheme is based on square loss and task specific regularization such as volume preservation. An unsupervised version is also proposed, which is a differentiable numberical solver."
3545,SP:cde2a84c463cdab9b19fcbdaf1cfe20d0187dcfa,"NN USED-FOR numerical solvers of PDEs. NN USED-FOR low - res correction. correction prediction CONJUNCTION differentiable PDE solver. differentiable PDE solver CONJUNCTION correction prediction. Task are fluid flow simulation, high resolution simulation, and approximating fluid flow simulation. Generic is approaches. OtherScientificTerm are low - resolution correction, and temporal regularization. ","This paper studies the problem of low-res correction of numerical solvers of PDEs using NN. This is an important problem in fluid flow simulation, where high resolution simulation is not feasible. The authors propose two approaches to address this problem: 1) low-resolution correction and 2) temporal regularization. Theoretical analysis is provided on the relationship between correction prediction and a differentiable PDE solver. Experiments are conducted on approximating fluid flow simulations."
3546,SP:cde2a84c463cdab9b19fcbdaf1cfe20d0187dcfa,Numerical solvers USED-FOR partial differential equations. function domain FEATURE-OF high dimensional grid. model USED-FOR PDE solver. data - driven way USED-FOR residuals. NN USED-FOR correction function. supervised and unsupervised manners USED-FOR NN. smoke rising simulation dataset USED-FOR method. OtherScientificTerm is high resolution results. Method is temporal regularization method. ,"Numerical solvers for partial differential equations have been shown to produce high resolution results. This paper proposes a temporal regularization method to reduce the variance of the residuals in a data-driven way. The proposed method is based on the smoke rising simulation dataset, where the high dimensional grid is a function domain and the model is used as a PDE solver. The NN is trained in both supervised and unsupervised manners to approximate the correction function."
3547,SP:e6534cd49bdc266dbeb111682ad37ef9d666e31e,"Online Continual Compression HYPONYM-OF problem. Generative methods USED-FOR continual learning. stacked quantization modules ( SQM ) HYPONYM-OF hierarchical variant. hierarchical variant PART-OF VQ - VAE model. OtherScientificTerm is catastrophic forgetting. Method are generative model, generator, and SQM. Task is Generative Replay. ","Generative methods have been shown to be effective in continual learning in the face of catastrophic forgetting. However, the problem of Online Continual Compression is a very challenging problem, as the generative model may not be able to adapt to new tasks. This paper proposes a hierarchical variant of the VQ-VAE model, stacked quantization modules (SQM), which is a hierarchical version of the original VQ model. The idea is to train a generator that computes the parameters of the SQM sequentially, and then use the generated samples to update the generator. The authors conduct experiments on Generative Replay and Generative Recall."
3548,SP:e6534cd49bdc266dbeb111682ad37ef9d666e31e,Stacked Quantization Modules ( SQM ) USED-FOR Online Continual Compression. VQ - VAE framework USED-FOR Stacked Quantization Modules ( SQM ). online continual image classification benchmarks EVALUATE-FOR SQM. ,This paper proposes Stacked Quantization Modules (SQM) for Online Continual Compression based on the VQ-VAE framework. The proposed SQM is evaluated on online continual image classification benchmarks.
3549,SP:e6534cd49bdc266dbeb111682ad37ef9d666e31e,ever - growing data USED-FOR long - term learning scenario. limited storage USED-FOR long - term learning scenario. ever - growing data USED-FOR limited storage. memory horizons CONJUNCTION reduced catastrophic forgetting. reduced catastrophic forgetting CONJUNCTION memory horizons. resolutions CONJUNCTION memory horizons. memory horizons CONJUNCTION resolutions. resolutions FEATURE-OF online compression system. memory horizons FEATURE-OF online compression system. reduced catastrophic forgetting FEATURE-OF online compression system. reservoir sampling USED-FOR architecture. Method is Quantization Modules. Generic is them. ,"This paper proposes Quantization Modules to address the problem of limited storage in the long-term learning scenario with ever-growing data. The authors propose an online compression system with different resolutions, memory horizons, and reduced catastrophic forgetting. The proposed architecture is based on reservoir sampling, which is an extension of reservoir sampling. Theoretical analysis is provided to show the advantages of the proposed architecture. Empirical results are provided to support the theoretical analysis of the performance of QuantizationModules and show that the proposed algorithm outperforms them."
3550,SP:4ada8234990b4dbcdecb6bafeb6f509263661ae8,"Task is multi - label prediction. Generic is method. OtherScientificTerm are joint embedding space, label space, common space, and co - embedding space. Method is KNN. ","This paper studies the problem of multi-label prediction. The proposed method is based on the idea that the joint embedding space should be similar to the label space, while the co-embedding space is different from the common space. The main contribution of the paper is the proposed KNN."
3551,SP:4ada8234990b4dbcdecb6bafeb6f509263661ae8,share embedding space USED-FOR multi - label problems. encoder USED-FOR label outputs. instance embedding CONJUNCTION label embedding. label embedding CONJUNCTION instance embedding. instance embedding USED-FOR training stage. embedding space USED-FOR inference. ,"This paper proposes a share embedding space for multi-label problems. During the training stage, an encoder is used to generate label outputs for each instance. During inference, the embedding of the instance embedding and the label embedding are shared."
3552,SP:4ada8234990b4dbcdecb6bafeb6f509263661ae8,"metric learning approach USED-FOR multi - label classification problem. k - NN USED-FOR It. it USED-FOR training. squared Euclidean distance EVALUATE-FOR it. image datasets CONJUNCTION text datasets. text datasets CONJUNCTION image datasets. image datasets EVALUATE-FOR multi - label learning algorithms. text datasets EVALUATE-FOR multi - label learning algorithms. OtherScientificTerm are features, and label embedding. ","This paper proposes a metric learning approach for the multi-label classification problem. It uses k-NN to estimate the distance between features and labels, and then uses it during training to compute the squared Euclidean distance between the features and the label embedding. Experiments on image datasets and text datasets are conducted to evaluate the performance of the proposed multiple-label learning algorithms."
3553,SP:d81a0edd94cc0b32734c42f1fb65d7070f963f86,"theoretical model USED-FOR delayed gradients. asynchronous training FEATURE-OF delayed gradients. It HYPONYM-OF model. momentum formulation USED-FOR stability. stability bounds USED-FOR SGD. stability bounds USED-FOR SGD. learning rate FEATURE-OF training speed. Method are differential equation, and sychronization. OtherScientificTerm is wall - clock time. Metric is accuracy. ",This paper proposes a theoretical model for delayed gradients during asynchronous training. It is a model that takes the momentum formulation of the differential equation as input and uses sychronization to update the weights in wall-clock time. The authors provide stability bounds for SGD and show that the learning rate of the training speed is proportional to the accuracy.
3554,SP:d81a0edd94cc0b32734c42f1fb65d7070f963f86,asynchrony USED-FOR model training. generalization gap EVALUATE-FOR models. A - SGD USED-FOR models. momentum USED-FOR asynchronous training. shifted - momentum USED-FOR asynchronous training. momentum USED-FOR shifted - momentum. OtherScientificTerm is local minimum points. ,"This paper studies the effect of asynchronous training on the generalization gap of models trained with A-SGD. The authors propose to use shifted-momentum, which is based on momentum, to perform asynchronous training. They show that the local minimum points of the shifted momentum can improve the performance of the models."
3555,SP:d81a0edd94cc0b32734c42f1fb65d7070f963f86,"dynamical system USED-FOR A - SGD. stability FEATURE-OF system. momentum PART-OF A - SGD. shifted momentum USED-FOR stability. shifted momentum USED-FOR momentum values. - SGD USED-FOR large batch synchronous training. learning rate FEATURE-OF - SGD. OtherScientificTerm are delayed gradients, learning rates, and threshold. ","This paper studies the stability of A-SGD in a dynamic system with delayed gradients. In particular, the authors show that momentum in the original A-GSD is not stable, and that the momentum values can be computed with shifted momentum to improve stability of the system. The authors also show that the learning rates of the original and shifted momentum versions of the same algorithm converge to the same threshold, and show that -SGD with the same learning rate converges to a similar threshold in large batch synchronous training."
3556,SP:05587c2ba9ff9bf3574604a60f614dd807c95e22,hindsight modelling HYPONYM-OF model - based reinforcement learning method. value function USED-FOR method. model USED-FOR embedding. embedding of value relevant information USED-FOR method. Method is value model. Task is value prediction. ,"This paper presents Hindsight modelling, a model-based reinforcement learning method. The method is based on a value function, where the value model is trained to predict the future value of the current state. The authors propose a method that learns an embedding of value relevant information using the model. This embedding is then used for value prediction."
3557,SP:05587c2ba9ff9bf3574604a60f614dd807c95e22,representation USED-FOR RL. hindsight model - based approach USED-FOR representation. approximator USED-FOR features. approximator USED-FOR approach. Task is training. OtherScientificTerm is optimal value functions. ,"This paper proposes a hindsight model-based approach to learn a representation for RL. The approach is based on an approximator of the features, which is used during training to estimate the optimal value functions."
3558,SP:05587c2ba9ff9bf3574604a60f614dd807c95e22,method USED-FOR value function learning. hindsight value function USED-FOR expected return. biased gradient estimator USED-FOR policy gradient methods. hindsight value function HYPONYM-OF biased gradient estimator. Method is Value - Driven Hindsight Modelling. Task is Q - Learning. ,This paper proposes a method for value function learning called Value-Driven Hindsight Modelling. The main idea is to use a biased gradient estimator (such as the hindsight value function used in policy gradient methods) to estimate the expected return. The paper provides theoretical analysis and empirical results on Q-Learning.
3559,SP:6388fb91f2eaac02d9406672760a237f78735452,adversarial attack problem USED-FOR graph classification problem. graph convolutional networks USED-FOR graph classification problem. edges USED-FOR attack. rewiring operation USED-FOR attack. graph edge number CONJUNCTION average degree. average degree CONJUNCTION graph edge number. RL based learning method USED-FOR policy. rewiring operation USED-FOR policy. method COMPARE baselines. baselines COMPARE method. method COMPARE methods. methods COMPARE method. baselines COMPARE methods. methods COMPARE baselines. method USED-FOR attack. social network data USED-FOR attack. social network data EVALUATE-FOR method. OtherScientificTerm is graph eigenvalues. Method is Rewiring. ,This paper studies an adversarial attack problem for graph classification problem with graph convolutional networks. The attack is based on rewiring the edges of the original graph. Rewiring is done by computing the graph eigenvalues and the graph edge number and the average degree. The authors propose an RL based learning method to learn a policy that is trained with the rewired operation. The proposed method is evaluated on social network data and compared with several baselines.
3560,SP:6388fb91f2eaac02d9406672760a237f78735452,edge PART-OF graph. adversarial attack setting USED-FOR graphs. graph rewiring operation HYPONYM-OF adversarial attack setting. reinforcement learning based approach USED-FOR attack strategy. with COMPARE method. method COMPARE with. with COMPARE baseline methods. baseline methods COMPARE with. method CONJUNCTION baseline methods. baseline methods CONJUNCTION method. Generic is attack. OtherScientificTerm is arbitrary edges. ,"This paper proposes an adversarial attack setting for graphs, i.e., the graph rewiring operation. The attack strategy is based on a reinforcement learning based approach, where each edge in the graph is replaced by an arbitrary edge. Experiments show that the proposed method outperforms with and baseline methods."
3561,SP:6388fb91f2eaac02d9406672760a237f78735452,"Reinforcement learning USED-FOR rewiring operation. eigenvalues FEATURE-OF graph's Laplacian matrix. eigenvalues EVALUATE-FOR rewiring operation. graph's Laplacian matrix EVALUATE-FOR rewiring operation. Method is ReWatt method. OtherScientificTerm are graph, edge, and edges. ","This paper proposes a rewiring operation based on Reinforcement learning. The main idea is to use the ReWatt method, which is based on the observation that the eigenvalues of a graph's Laplacian matrix can be computed by rewinding an edge. The paper then proposes to rewire the edges of the original graph, and then use the rewired operation to compute the new eigenvectors of the graph."
3562,SP:233b12d422d0ac40026efdf7aab9973181902d70,"SURE estimator HYPONYM-OF network training regularizer. MRI CONJUNCTION EDX. EDX CONJUNCTION MRI. image reconstruction algorithms USED-FOR MRI. image reconstruction algorithms USED-FOR EDX. architecture COMPARE image reconstruction algorithms. image reconstruction algorithms COMPARE architecture. Method are CNN autoencoders, and bagging / boosting technique. ","This paper proposes a network training regularizer called SURE estimator, which aims to improve the performance of CNN autoencoders. The main idea is to use a bagging/boosting technique, where the weights of the weights are randomly sampled from the training set. Experiments show that the proposed architecture outperforms existing image reconstruction algorithms for MRI and EDX."
3563,SP:233b12d422d0ac40026efdf7aab9973181902d70,"encoder decoder setup USED-FOR linear deblurring problem. Stein's unbiased risk estimator USED-FOR problem. RELU activations FEATURE-OF convolutional neural network. convolutional neural network USED-FOR encoder. boosted loss function USED-FOR nonboosted "" loss function. Method are boosting estimators, and models ( boosting ). Generic is procedure. ","This paper studies an encoder decoder setup for a linear deblurring problem. The problem is formulated as a variant of Stein's unbiased risk estimator. The encoder is trained with a convolutional neural network with RELU activations. The ""boosted"" loss function is a modified version of the standard boosted loss function. The authors show that boosting estimators can be used to improve the performance of models (boosting). The authors also provide a theoretical analysis of the proposed procedure."
3564,SP:233b12d422d0ac40026efdf7aab9973181902d70,piecewise linear close form expression USED-FOR Stein ’s unbiased risk estimator. formulation USED-FOR Encoder - decoder convolutional neural network. bagging USED-FOR this. Task is inverse problems. ,"This paper proposes a piecewise linear close form expression for Stein’s unbiased risk estimator. This formulation can be used to train an Encoder-decoder convolutional neural network. The authors show that this can be done with bagging, which is an important problem in inverse problems."
3565,SP:978555e8eced096b92b37a91fc16b60f7d99c2b6,"class imbalance CONJUNCTION out - of - distribution tasks. out - of - distribution tasks CONJUNCTION class imbalance. task imbalance CONJUNCTION class imbalance. class imbalance CONJUNCTION task imbalance. meta learning algorithms USED-FOR task imbalance. meta learning algorithms USED-FOR class imbalance. task - dependent learning rate decaying factor USED-FOR large tasks. meta - knowledge USED-FOR small task. class - specific scaling factor USED-FOR class - specific gradient. class - specific scaling factor USED-FOR class imbalance. task - dependent variables USED-FOR meta - knowledge. task - dependent variables USED-FOR out - of - distribution tasks. variational inference USED-FOR model parameters. benchmark datasets EVALUATE-FOR approach. Generic is small tasks. Method is task - specific training. OtherScientificTerm are scaling factor, and small class. ","This paper proposes to address the problem of task imbalance, class imbalance, and out-of-distribution tasks with meta learning algorithms. In particular, the authors propose to use a task-dependent learning rate decaying factor for large tasks and a class-specific scaling factor for small tasks to mitigate the class imbalance and class imbalance. The main idea is to use meta-knowledge from small task to small task for task-specific training, where the scaling factor is learned for each small class. The authors also propose to learn meta-dependent variables for both small and large tasks using variational inference to learn the model parameters. The proposed approach is evaluated on several benchmark datasets."
3566,SP:978555e8eced096b92b37a91fc16b60f7d99c2b6,Bayesian approach USED-FOR meta learning. prior CONJUNCTION inference network. inference network CONJUNCTION prior. task - specific balancing variables CONJUNCTION inference network. inference network CONJUNCTION task - specific balancing variables. prior USED-FOR task - specific balancing variables. task - specific balancing variables PART-OF approach. inference network PART-OF approach. amortized inference scheme USED-FOR model. meta - learning objective loss CONJUNCTION lower bound of probabilistic model marginal likelihood. lower bound of probabilistic model marginal likelihood CONJUNCTION meta - learning objective loss. model USED-FOR meta - learning objective loss. OtherScientificTerm is imbalanced class distribution. ,"This paper proposes a Bayesian approach to meta learning. The approach consists of task-specific balancing variables, a prior, and an inference network. The model is trained using an amortized inference scheme, where the imbalanced class distribution is used to estimate the meta-learning objective loss and the lower bound of probabilistic model marginal likelihood."
3567,SP:978555e8eced096b92b37a91fc16b60f7d99c2b6,"gradient - based meta - learning models USED-FOR few - shot classification. variables USED-FOR task adaptation. OtherScientificTerm are task imbalance, class imbalance, distribution, and meta - learned knowledge. ","This paper studies gradient-based meta-learning models for few-shot classification. The main contribution of this paper is to study the problem of task imbalance, i.e., the class imbalance between the training data and the test data. The authors show that when the distribution of the data is large enough, the meta-learned knowledge can be adapted to the new task. They also show that the choice of variables for task adaptation can help with task adaptation."
3568,SP:2395947721c4a337701a7c61cd4ba5c0e38fcc9b,RL problem USED-FOR IL. reward FEATURE-OF RL problem. image - based tasks ( Atari ) CONJUNCTION continuous control tasks ( Mujoco ). continuous control tasks ( Mujoco ) CONJUNCTION image - based tasks ( Atari ). GAIL CONJUNCTION supervised BC approach. supervised BC approach CONJUNCTION GAIL. algorithm COMPARE GAIL. GAIL COMPARE algorithm. algorithm COMPARE supervised BC approach. supervised BC approach COMPARE algorithm. continuous control tasks ( Mujoco ) EVALUATE-FOR algorithm. image - based tasks ( Atari ) EVALUATE-FOR algorithm. Method is SQUIL. Task is behavioral cloning ( BC ). OtherScientificTerm is expert actions. ,"This paper proposes SQUIL, an algorithm for behavioral cloning (BC) that learns to imitate expert actions by solving an RL problem with reward. The proposed algorithm is evaluated on image-based tasks (Atari) and continuous control tasks (Mujoco), and compared to GAIL and a supervised BC approach."
3569,SP:2395947721c4a337701a7c61cd4ba5c0e38fcc9b,"reinforcement learning USED-FOR imitation learning approach. RL problem USED-FOR imitation learning problem. regularized behavior cloning USED-FOR approach. imitation learning problems EVALUATE-FOR approach. approach PART-OF RL methods. OtherScientificTerm are reward, out - of - distribution states, and compounding errors. ","This paper proposes an imitation learning approach based on reinforcement learning. The main idea is to formulate the imitation learning problem as an RL problem, where the reward is a function of how well the agent is able to imitate out-of-distribution states. The proposed approach is based on regularized behavior cloning and is evaluated on imitation learning problems. The results show that the proposed approach can be integrated into existing RL methods without compounding errors."
3570,SP:2395947721c4a337701a7c61cd4ba5c0e38fcc9b,method USED-FOR imitation learning. method COMPARE GAIL. GAIL COMPARE method. Soft Q Learning USED-FOR Soft Q Imitation Learning ( SQIL ). Soft Q Learning USED-FOR approach. replay buffers USED-FOR it. behavioral cloning USED-FOR SQIL. ,"This paper proposes a method for imitation learning based on Soft Q Learning, Soft Q Imitation Learning (SQIL), which is a method that is similar to GAIL. However, it does not use replay buffers and instead uses replay buffers. The main difference between SQIL and GAIL is that SQIL is based on behavioral cloning."
3571,SP:37a4825aaeb899187b957d9ed9ae657617f4a055,deep network USED-FOR point cloud sequence super - resolution / upsampling. pointNet CONJUNCTION PU - net. PU - net CONJUNCTION pointNet. training loss USED-FOR temporal coherence. OtherScientificTerm is temporal incoherence. Method is point cloud shape representation. Generic is method. Material is sequence data. ,This paper proposes a deep network for point cloud sequence super-resolution/upsampling. The method is motivated by the observation that temporal incoherence in the point cloud shape representation can lead to poor performance on sequence data. The authors propose a training loss to encourage temporal coherence between pointNet and PU-net.
3572,SP:37a4825aaeb899187b957d9ed9ae657617f4a055,temporally stable features USED-FOR point clouds. learning temporally stable features USED-FOR upsampling point clouds. approaches USED-FOR geometry - oriented tasks. semantic labeling CONJUNCTION geometry - oriented tasks. geometry - oriented tasks CONJUNCTION semantic labeling. approaches USED-FOR semantic labeling. geometry - oriented tasks CONJUNCTION point - based graphics. point - based graphics CONJUNCTION geometry - oriented tasks. Learning point - based descriptors PART-OF vision and graphics meetings. normal estimation HYPONYM-OF geometry - oriented tasks. fourth dimension FEATURE-OF features. Method is point - based descriptors. ,"This paper studies the problem of learning temporally stable features for upsampling point clouds. The authors propose approaches for semantic labeling, geometry-oriented tasks (e.g., normal estimation) and point-based graphics. The paper also presents a theoretical analysis of the properties of the learned features in the fourth dimension, which is an important aspect of learning the feature of a point cloud. The experiments demonstrate the effectiveness of the proposed approaches and the usefulness of learning point-related descriptors in vision and graphics meetings."
3573,SP:37a4825aaeb899187b957d9ed9ae657617f4a055,"temporally stable representations USED-FOR point - based data sets. temporally stable representations USED-FOR super - resolution. temporally stable representations USED-FOR varying size and dynamic point sets. siamese network setup USED-FOR temporal loss calculation. temporal loss function USED-FOR temporally coherent point set generation. temporal loss function CONJUNCTION siamese network setup. siamese network setup CONJUNCTION temporal loss function. estimated point cloud COMPARE super - resolution point cloud. super - resolution point cloud COMPARE estimated point cloud. EMD USED-FOR temporal loss. ground truth acceleration CONJUNCTION estimated velocity. estimated velocity CONJUNCTION ground truth acceleration. permutation invariant loss terms CONJUNCTION siamese training setup. siamese training setup CONJUNCTION permutation invariant loss terms. siamese training setup CONJUNCTION generator architecture. generator architecture CONJUNCTION siamese training setup. mode collapse USED-FOR temporal point networks. generator architecture USED-FOR output variance. dynamic adjustments of the output size USED-FOR output variance. OtherScientificTerm are variable input and output size, and spatial loss. Generic are loss functions, and task. ","This paper studies the problem of learning temporally stable representations for point-based data sets with varying size and dynamic point sets. The authors propose a temporal loss function for temporally coherent point set generation and a siamese network setup for temporal loss calculation. The temporal loss is based on EMD, where the parameters are the ground truth acceleration and the estimated velocity, respectively, and the variable input and output size is the number of points in the data set. The estimated point cloud is then compared to the super-resolution point cloud, which is then used to compute the spatial loss. The loss functions are trained with permutation invariant loss terms, siamesed training setup, and generator architecture to reduce the output variance caused by dynamic adjustments of the output size. Experimental results show that the proposed temporal point networks are robust to mode collapse and can generalize to a new task."
3574,SP:af54d04f219d381208c049b8a9c59b8cdd1783e0,side information USED-FOR alignment problem. entropic optimal transport ( Sinkhorn ) USED-FOR alignment problem. approach COMPARE state - of - the - art. state - of - the - art COMPARE approach. single - cell RNA - seq CONJUNCTION marriage - matching. marriage - matching CONJUNCTION single - cell RNA - seq. state - of - the - art USED-FOR applications. marriage - matching CONJUNCTION MNIST. MNIST CONJUNCTION marriage - matching. applications EVALUATE-FOR approach. MNIST HYPONYM-OF applications. single - cell RNA - seq HYPONYM-OF applications. marriage - matching HYPONYM-OF applications. Generic is it. Method is optimal transport framework. ,"This paper proposes entropic optimal transport (Sinkhorn) to solve the alignment problem with side information. In particular, it extends the optimal transport framework to include side information as well. The proposed approach is evaluated on two applications: single-cell RNA-seq and marriage-matching on MNIST and compared to the state-of-the-art."
3575,SP:af54d04f219d381208c049b8a9c59b8cdd1783e0,way USED-FOR optimal transport ( OT ) cost. subset correspondence information USED-FOR way. neural network USED-FOR transport cost. L2 regularizer USED-FOR transport plan. Euclidean distance CONJUNCTION Procrustes - based OT. Procrustes - based OT CONJUNCTION Euclidean distance. transport cost COMPARE Euclidean distance. Euclidean distance COMPARE transport cost. transport cost COMPARE Procrustes - based OT. Procrustes - based OT COMPARE transport cost. Material is real - world datasets ( RNA sequence ). ,This paper proposes a way to estimate the optimal transport (OT) cost based on subset correspondence information. The authors propose a neural network that estimates the transport cost and then uses an L2 regularizer to optimize the transport plan. Experiments are conducted on real-world datasets (RNA sequence) and compare the proposed transport cost with Euclidean distance and Procrustes-based OT.
3576,SP:af54d04f219d381208c049b8a9c59b8cdd1783e0,"cost function USED-FOR optimal transport. gradient - based method USED-FOR cost function. gradient - based method USED-FOR dataset alignment. Sinkhorn - Knopp iterative procedure USED-FOR optimal transport plan. Sinkhorn - Knopp iterative procedure USED-FOR algorithm. parameterized cost function USED-FOR Sinkhorn transport plan. side information USED-FOR loss function. loss function USED-FOR cost function. synthetic and real datasets EVALUATE-FOR method. method COMPARE approaches. approaches COMPARE method. synthetic and real datasets EVALUATE-FOR approaches. OtherScientificTerm are Sinkhorn iteration, and iterative updates. ","This paper proposes a gradient-based method for learning a cost function for optimal transport. The algorithm is based on the Sinkhorn-Knopp iterative procedure, which is used for dataset alignment. The authors propose a parameterized cost function that can be used to approximate the optimal transport plan. The cost function is learned by minimizing a loss function that takes into account the side information, and is then used during the Sinkshorn iteration. The method is evaluated on both synthetic and real datasets, and compared to other approaches. The results show that the iterative updates are effective."
3577,SP:fc86b06a367f6790c76b89ec3bfe4cb8627c540a,"fully unsupervised approach USED-FOR anomaly detection. one - class SVM CONJUNCTION one - class NN. one - class NN CONJUNCTION one - class SVM. method COMPARE baseline model. baseline model COMPARE method. one - class SVM HYPONYM-OF baseline model. one - class NN HYPONYM-OF baseline model. OtherScientificTerm is anomaly one. Method are autoencoders, and latent space clustering. ",This paper presents a fully unsupervised approach for anomaly detection. The main idea is to train autoencoders to distinguish between the anomaly one and the true one. The method is compared to a baseline model (one-class SVM and one-class NN) and compared with a few other methods. The results show that the latent space clustering is effective.
3578,SP:fc86b06a367f6790c76b89ec3bfe4cb8627c540a,framework USED-FOR anomaly detection. autoencoders CONJUNCTION reconstruction error. reconstruction error CONJUNCTION autoencoders. them USED-FOR autoencoder. clustering USED-FOR method. autoencoders USED-FOR method. reconstruction error USED-FOR method. ,This paper proposes a framework for anomaly detection. The method is based on clustering and uses autoencoders and reconstruction error to train them to train an autoencoder.
3579,SP:fc86b06a367f6790c76b89ec3bfe4cb8627c540a,"normal data CONJUNCTION anomaly data. anomaly data CONJUNCTION normal data. reconstruction errors USED-FOR normal data samples. normal data USED-FOR AE. Method are unsupervised anomaly detection method, and auto - encoder ( AE ). OtherScientificTerm are normal data subset, and normal samples. ",This paper proposes an unsupervised anomaly detection method. The main idea is to train an auto-encoder (AE) that is able to distinguish between normal data and anomaly data. The AE is trained on normal data with reconstruction errors and on anomaly data without reconstruction errors. The authors show that the AE can detect anomalies in the normal data subset and that normal samples are more likely to be anomalous.
3580,SP:9e831d3595c15ca34cadb3c4a5b02230593b4ccb,"two - step optimization process USED-FOR constrained policy optimization algorithm. PCPO algorithm USED-FOR constraint violation. constrained policy optimization algorithm USED-FOR control tasks. constrained policy optimization algorithm COMPARE baselines. baselines COMPARE constrained policy optimization algorithm. OtherScientificTerm are policies, and constraint set. ",This paper proposes a constrained policy optimization algorithm with a two-step optimization process for control tasks. The first step of the PCPO algorithm is to minimize the constraint violation. The second step is to learn policies that satisfy the constraint set. Experiments show that the proposed constrained policy optimizing algorithm outperforms the baselines.
3581,SP:9e831d3595c15ca34cadb3c4a5b02230593b4ccb,"algorithm USED-FOR CMDPs. stages PART-OF algorithm. reward improvement CONJUNCTION constraint violation. constraint violation CONJUNCTION reward improvement. bounds FEATURE-OF reward improvement. bounds FEATURE-OF constraint violation. KL divergence CONJUNCTION L2 norm. L2 norm CONJUNCTION KL divergence. KL divergence HYPONYM-OF projection metrics. L2 norm HYPONYM-OF projection metrics. projection metrics USED-FOR convergence. it COMPARE CPO and lagrangian based approaches. CPO and lagrangian based approaches COMPARE it. traffic management tasks EVALUATE-FOR it. control tasks EVALUATE-FOR it. safety constraints FEATURE-OF mujoco environments. traffic management tasks EVALUATE-FOR algorithm. control tasks EVALUATE-FOR algorithm. traffic management tasks HYPONYM-OF control tasks. mujoco environments HYPONYM-OF control tasks. Method are Projection based Constrained Policy Optimization, and unconstrained update. OtherScientificTerm are policy, and constraint set. ","This paper proposes Projection based Constrained Policy Optimization, an algorithm for CMDPs that consists of two stages: (1) an unconstrained update to the policy, and (2) an update to a constraint set. The authors provide bounds on the reward improvement and constraint violation. The convergence is achieved using two projection metrics: KL divergence and L2 norm. The algorithm is evaluated on several control tasks, including mujoco environments with safety constraints, and it outperforms CPO and lagrangian based approaches."
3582,SP:9e831d3595c15ca34cadb3c4a5b02230593b4ccb,technique USED-FOR constraints. technique USED-FOR Markov Decision Processes ( MDP ). constraints FEATURE-OF Markov Decision Processes ( MDP ). Euclidean distance CONJUNCTION KL - divergence. KL - divergence CONJUNCTION Euclidean distance. variants PART-OF method. KL - divergence HYPONYM-OF variants. Euclidean distance USED-FOR projection. Generic is problem. Task is projection problem. Method is Taylor - expanded variant. ,This paper proposes a technique to impose constraints on Markov Decision Processes (MDP). The method consists of two variants: Euclidean distance and KL-divergence. The first problem is the projection problem. The second is the Taylor-expanded variant.
3583,SP:068a0bb2497373acad5f70e66c61b71465b2de3d,"word2vec PART-OF word embedding. alpha HYPONYM-OF hyperparameter. low rank transformation mechanism USED-FOR embedding. hyperparameter alpha USED-FOR word embedding. Method are matrix factorization framework, and word embedding schemas. OtherScientificTerm is distance structure. ","This paper proposes a matrix factorization framework for word embedding, where word2vec is used as a hyperparameter, instead of alpha, which is commonly used in the literature. The key idea is to use a low rank transformation mechanism to transform the embedding into a matrix, where the distance structure of the matrix is determined by the distance between the word and the matrix. The authors propose to use word embeddings schemas based on the learned embedding schemas, and propose a new way to parameterize word 2vec. The main contribution of this paper is to introduce a new type of hyperparameters alpha to the word embeding. "
3584,SP:068a0bb2497373acad5f70e66c61b71465b2de3d,implicit alpha parameter USED-FOR word embeddings. co - occurrence matrix USED-FOR word embeddings. alpha FEATURE-OF co - occurrence matrix. OtherScientificTerm is singular values. Generic is method. ,"This paper proposes an implicit alpha parameter for word embeddings. Specifically, the alpha of the co-occurrence matrix is used to compute the word embedding. The alpha is defined as the difference between the singular values of the word and its nearest neighbors. The method is evaluated on a variety of datasets."
3585,SP:068a0bb2497373acad5f70e66c61b71465b2de3d,"embedding methods USED-FOR matrix factorization. analogy structure FEATURE-OF word embeddings. Task are learning word embeddings, embedding transformation process, and word similarity tasks. OtherScientificTerm are alpha parameter, embeddings, and alpha. ","This paper studies the problem of learning word embeddings with analogy structure. The authors propose to use embedding methods for matrix factorization, where the alpha parameter is learned to be a function of the embedding transformation process. In particular, the authors show that the alpha can be used as a proxy for the similarity between word similarity tasks. They also show that when the alpha is large enough, the embeds can be transformed to be similar to each other."
3586,SP:b2d099c78b48aab509ab64027ca49e9a47079fc0,"random projection forests USED-FOR similarity measurements. projections USED-FOR them. random transformations USED-FOR Tree levels. average tree - distance USED-FOR beta - similarity. OtherScientificTerm are forest, and leaf - node. ","This paper proposes to use random projection forests for similarity measurements. Tree levels are trained with random transformations, and then the projections are used to estimate them. The beta-similarity is estimated using the average tree-distance between the leaf-node and the original forest."
3587,SP:b2d099c78b48aab509ab64027ca49e9a47079fc0,beta - similarity HYPONYM-OF similarity measure. X - Projection trees HYPONYM-OF approximate version. X independent random projection directions USED-FOR approximate version. X - Forest HYPONYM-OF X - Projection trees. random projections USED-FOR X - Projection trees. kernel k - means CONJUNCTION DBSCAN. DBSCAN CONJUNCTION kernel k - means. DBSCAN CONJUNCTION Spectral clustering. Spectral clustering CONJUNCTION DBSCAN. clustering accuracy EVALUATE-FOR it. it USED-FOR methods. it USED-FOR beta similarity. clustering accuracy EVALUATE-FOR beta similarity. Spectral clustering HYPONYM-OF methods. kernel k - means HYPONYM-OF methods. DBSCAN HYPONYM-OF methods. OtherScientificTerm is RP trees. Method is PR trees. ,"This paper proposes a new similarity measure called beta-similarity, which is an approximate version of the standard similarity measure between RP trees. The approximate version is based on X independent random projection directions and is called X-Projection trees (X-Forest, X-Forest), which are based on random projections. The authors show that it can be used to compare beta similarity between different methods such as kernel k-means, DBSCAN, and Spectral clustering, and that it improves the clustering accuracy of beta similarity compared to the standard PR trees."
3588,SP:b2d099c78b48aab509ab64027ca49e9a47079fc0,"method USED-FOR RP trees. projection vectors USED-FOR RP tree. OtherScientificTerm are similarity, and trees. ",This paper proposes a method for learning RP trees. The main idea is to use projection vectors to represent the RP tree as a function of the similarity between the input and the output of the projection vectors. The authors show that the proposed method is able to learn trees that are close to each other.
3589,SP:77b8bed08af8be8af0c65a72a6e22cfb02645d02,"Markov chain Monte Carlo ( MCMC ) CONJUNCTION variational inference ( VI ). variational inference ( VI ) CONJUNCTION Markov chain Monte Carlo ( MCMC ). variational inference ( VI ) USED-FOR approximate inference. Markov chain Monte Carlo ( MCMC ) USED-FOR approximate inference. VI CONJUNCTION MCMC. MCMC CONJUNCTION VI. synthetic benchmarks CONJUNCTION generative modeling of MNIST. generative modeling of MNIST CONJUNCTION synthetic benchmarks. generative modeling of MNIST EVALUATE-FOR formulation. synthetic benchmarks EVALUATE-FOR formulation. OtherScientificTerm is optimization objective. Metric is Kullback - Leibler ( KL ) divergence. Method are MCMC + VI, and Bayesian neural networks. ",This paper proposes to combine Markov chain Monte Carlo (MCMC) with variational inference (VI) for approximate inference. The main idea is to replace the optimization objective with the Kullback-Leibler (KL) divergence between VI and MCMC. The authors show that the proposed MCMC + VI can be used to train Bayesian neural networks. Experiments on synthetic benchmarks and generative modeling of MNIST demonstrate the effectiveness of the proposed formulation.
3590,SP:77b8bed08af8be8af0c65a72a6e22cfb02645d02,method USED-FOR deep learning. uncertainty modelling USED-FOR deep learning. Bayesian inference arguments USED-FOR method. variational inference USED-FOR It. Generic is it. ,"This paper proposes a method for deep learning based on uncertainty modelling. The method is based on Bayesian inference arguments, and it uses variational inference."
3591,SP:77b8bed08af8be8af0c65a72a6e22cfb02645d02,hybrid method USED-FOR VI. hybrid method USED-FOR MCMC. MCMC CONJUNCTION VI. VI CONJUNCTION MCMC. parametric procedure USED-FOR ﬁnite - length MCMC / HMC chain. ELBO USED-FOR constrained and tractable objective. MNIST EVALUATE-FOR VAE. VAE EVALUATE-FOR techniques. synthetic datasets EVALUATE-FOR techniques. Metric is VI - motivated objective. OtherScientificTerm is intractable entropy. ,"This paper proposes a hybrid method for MCMC and VI. Specifically, the authors propose a parametric procedure for constructing a ﬁnite-length MCMC/HMC chain. The authors also propose a novel VI-motivated objective, which is based on minimizing an ELBO to obtain a constrained and tractable objective with intractable entropy. The proposed techniques are evaluated on synthetic datasets and on VAE on MNIST."
3592,SP:64f2744e938bd62cd47c1066dc404a42134953da,"MissDeepCausal method USED-FOR treatment effect estimation. incomplete covariates matrix USED-FOR treatment effect estimation. Variational AutoEncoders ( VAE ) USED-FOR latent confounders. Variational AutoEncoders ( VAE ) USED-FOR It. OtherScientificTerm are incomplete covariates, complex non - linear relationships, observed incomplete covariates, and Average Treatment Effect ( ATE ). Method are MIWAE, and doubly robust estimator. ","This paper proposes the MissDeepCausal method for treatment effect estimation with incomplete covariates matrix. It uses Variational AutoEncoders (VAE) to model latent confounders, which are often complex non-linear relationships. The authors propose MIWAE, which is a doubly robust estimator of the Average Treatment Effect (ATE). The authors also provide some experimental results to support their claims."
3593,SP:64f2744e938bd62cd47c1066dc404a42134953da,"matrix factorization techniques USED-FOR unbiased estimator. unbiased estimator USED-FOR Average Treatment Effect. matrix factorization techniques USED-FOR Average Treatment Effect. assumptions FEATURE-OF matrix factorization techniques. ignorability assumptions FEATURE-OF doubly robust estimators. Task is average treatment effect estimation. OtherScientificTerm is unobserved confounder Z. Method are low rank model, and linear model. ","This paper studies the problem of average treatment effect estimation under the assumption that there is an unobserved confounder Z. The authors propose to use matrix factorization techniques to obtain an unbiased estimator of the Average Treatment Effect under certain assumptions. The main contribution of this paper is to show that doubly robust estimators with ignorability assumptions can be obtained under the same assumptions as the low rank model, and that the linear model can also be obtained."
3594,SP:64f2744e938bd62cd47c1066dc404a42134953da,"deep latent - factor models USED-FOR causal inference. noisy proxys USED-FOR missingness. latent - factor model USED-FOR multiple imputation. multiple imputation USED-FOR doubly - robust causal treatment effect estimators. latent - factor model USED-FOR doubly - robust causal treatment effect estimators. missing at random assumption USED-FOR it. doubly robust estimator USED-FOR multiple imputation strategy. latent confounders USED-FOR linear - regression model. latent confounders USED-FOR approach. imputation strategies USED-FOR doubly - robust estimator. approaches COMPARE imputation strategies. imputation strategies COMPARE approaches. OtherScientificTerm are missing values, imputation, and confounders. ","This paper studies the problem of causal inference in deep latent-factor models, where missing values are assumed to be noisy proxys. The authors propose doubly-robust causal treatment effect estimators based on multiple imputation, which is a variant of the latent-factor model used in prior work for multiple imputations. In particular, it is based on the missing at random assumption, and the authors propose a doubly robust estimator that can be used as the basis for the multiple imputing strategy. The proposed approach is motivated by the latent confounders in the linear-regression model, where the imputation is performed on a subset of the data points that have been added to the data. The paper shows that the proposed approaches outperform the existing imputation strategies in terms of the number of confounder and the performance of the doubly -robust estimator."
3595,SP:971d0d94adf5113ee1bef8df9ea7dbd508cf4cbc,"reinforcement learning method USED-FOR mapping. Method are reinforcement learning policies, feed - forward neural network, and gradient - based method. Generic is network. ",This paper studies the problem of learning reinforcement learning policies that can be mapped to unseen states. The authors propose a new reinforcement learning method to learn this mapping. The main idea is to train a feed-forward neural network that maps the input to the output of the network. This is a gradient-based method.
3596,SP:971d0d94adf5113ee1bef8df9ea7dbd508cf4cbc,"neural architecture search USED-FOR constructing compact RL policies. ENAS and ES methods USED-FOR optimisation. It USED-FOR optimisation. ENAS and ES methods USED-FOR It. fixed Toeplitz structure USED-FOR compact policies. ES USED-FOR shared weights. ENAS USED-FOR partitioning. reward - compression outcomes EVALUATE-FOR baseline alternatives. OtherScientificTerm is compact RL policies. Method are chromatic network ” architecture, and RL network. Material is continuous control benchmarks. ","This paper addresses the problem of constructing compact RL policies using neural architecture search. It uses ENAS and ES methods for optimisation. The authors propose a “chromatic network” architecture, which is based on partitioning the RL network into a fixed Toeplitz structure, and then using ES to compute the shared weights. Experiments are conducted on continuous control benchmarks, and show improved reward-compression outcomes compared to baseline alternatives."
3597,SP:971d0d94adf5113ee1bef8df9ea7dbd508cf4cbc,"neural architecture search USED-FOR approaches. approaches USED-FOR policy networks. autoregressive RNN USED-FOR partitioning. REINFORCE USED-FOR partitioning. weights COMPARE distribution. distribution COMPARE weights. ES USED-FOR gradient approximation. weights USED-FOR weights. OtherScientificTerm are fixed size weight matrix, and random partitioning. Generic is network. Method is ENAS. Task is policy network compression. ","This paper proposes two approaches for training policy networks using neural architecture search. The first approach, REINFORCE, uses an autoregressive RNN to perform partitioning with a fixed size weight matrix. The second approach, ENAS, uses ES to perform gradient approximation on the weights instead of the distribution. Both approaches are based on the idea of random partitioning, where the weights of the network are randomly partitioned. The authors demonstrate the effectiveness of both approaches in policy network compression."
3598,SP:18aaba3423e81e9437b509d1a5e24836ef5635f6,"learnable group transform ( LGT ) HYPONYM-OF learnable time - series pre - processing. transform USED-FOR coefficents. time - warping CONJUNCTION wavelet - style transforms. wavelet - style transforms CONJUNCTION time - warping. Method are wavelet transform, and LGT. Material is time - series signal. OtherScientificTerm are affine group, and mother wavelet. ","This paper proposes learnable group transform (LGT), a new form of learnable time-series pre-processing. The main idea is to use a wavelet transform, where the input is an affine group, and the transform is used to generate coefficents. The authors show that LGT can be applied to a variety of settings, including time-warping, wavelet-style transforms, etc. They also show that the learned transform can be used to pre-process a time-sender signal. Finally, the authors provide a theoretical analysis of the behavior of the learned LGT, showing that the trained LGT is able to generalize to unseen data points. The paper also provides some experimental results to support the theoretical analysis. In particular, it is shown that the learnable LGT generalizes better to unseen points than to the original mother wavelet."
3599,SP:18aaba3423e81e9437b509d1a5e24836ef5635f6,joint learning algorithm USED-FOR them. common learning approach USED-FOR time - series. spectral domain USED-FOR them. time domain FEATURE-OF learning algorithms. OtherScientificTerm is basis functions. Material is speech. ,"This paper proposes a common learning approach for time-series, where the basis functions of the time series are given by a joint learning algorithm, and the goal is to learn them in the spectral domain. This is an interesting direction for learning algorithms in the time domain, which is an important topic in speech."
3600,SP:18aaba3423e81e9437b509d1a5e24836ef5635f6,group action FEATURE-OF mother wavelet. dilation and/or rotation of a mother wavelet USED-FOR Wavelet Transform. supervisedly learn operators USED-FOR mother wavelet. Generic is construction. OtherScientificTerm is Euclidean group. ,"This paper proposes Wavelet Transform, which is based on dilation and/or rotation of a mother wavelet. The construction is motivated by the observation that the group action of a given motherwavelet can be represented as a function of a Euclidean group. The authors propose to use supervisedly learn operators to learn the parameters of the mother wavelets."
3601,SP:3058e6bc5e8c62af325c214c9e1436d6cdf09204,"hyperbolic representations of the graph nodes USED-FOR graph convolutional network ( GCN ). Hyperbolic Neural Networks USED-FOR GCN. right matrix multiplication PART-OF GCN. Mobius matrix - vector multiplication USED-FOR GCN. Mobius matrix - vector multiplication USED-FOR right matrix multiplication. computational geometry CONJUNCTION machine learning community. machine learning community CONJUNCTION computational geometry. left matrix multiplication PART-OF GCN. computational geometry FEATURE-OF Ungar's ( 2010 ) weighted barycenter. Ungar's ( 2010 ) weighted barycenter USED-FOR left matrix multiplication. toy problem CONJUNCTION semi - supervised node classification. semi - supervised node classification CONJUNCTION toy problem. semi - supervised node classification EVALUATE-FOR method. toy problem EVALUATE-FOR method. Method is latent representations of the nodes. OtherScientificTerm are Poincare disks, weighted linear combination, Poincare disk, and weighted barycenter. ","This paper proposes a graph convolutional network (GCN) based on hyperbolic representations of the graph nodes. The proposed GCN is a generalization of Hyperbolic Neural Networks. The main idea is to replace the left matrix multiplication of GCN with Mobius matrix-vector multiplication, which replaces the right matrix multiplication in GCN by Ungar's (2010) weighted barycenter, which is well-known in the computational geometry and machine learning community. The authors propose to use Poincare disks to represent the nodes as a weighted linear combination of the weights of the nodes on the Poencare disk and the latent representations of each node. The method is evaluated on a toy problem and semi-supervised node classification."
3602,SP:3058e6bc5e8c62af325c214c9e1436d6cdf09204,"non - Euclidean spaces USED-FOR GCNs. embeddings USED-FOR non - Euclidean models. these USED-FOR GCN operations. OtherScientificTerm are Euclidean space, non - Euclidean settings, and complex spaces. Material is hyperbolic space. Method is mixed - curvature product formalism. ","This paper studies GCNs in non-Euclidean spaces. In Euclidean space, the embeddings of the learned GCN models are linear in the number of parameters. In non-euclidean settings, these are non-linear. The authors propose a mixed-curvature product formalism, where the parameters of the hyperbolic space are concatenated with those of the other complex spaces, and these are used as GCN operations."
3603,SP:3058e6bc5e8c62af325c214c9e1436d6cdf09204,non - Euclidean spaces USED-FOR representation learning. closed - form formulae USED-FOR distances. Euclidean representations CONJUNCTION Riemannian manifolds. Riemannian manifolds CONJUNCTION Euclidean representations. distances CONJUNCTION gradients. gradients CONJUNCTION distances. closed - form formulae USED-FOR gradients. Euclidean space USED-FOR representations. OtherScientificTerm is constant curvature geometries. ,"This paper studies representation learning in non-Euclidean spaces. The authors propose closed-form formulae for distances and gradients, which can be used to compare Euclidean representations and Riemannian manifolds. They show that the representations learned in Euclideane space are not invariant to constant curvature geometries."
